{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7795", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7795/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7795/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7795/events", "html_url": "https://github.com/pytorch/pytorch/issues/7795", "id": 325860334, "node_id": "MDU6SXNzdWUzMjU4NjAzMzQ=", "number": 7795, "title": "[feature request] Add cudaification API for distributions", "user": {"login": "Balandat", "id": 1605878, "node_id": "MDQ6VXNlcjE2MDU4Nzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1605878?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Balandat", "html_url": "https://github.com/Balandat", "followers_url": "https://api.github.com/users/Balandat/followers", "following_url": "https://api.github.com/users/Balandat/following{/other_user}", "gists_url": "https://api.github.com/users/Balandat/gists{/gist_id}", "starred_url": "https://api.github.com/users/Balandat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Balandat/subscriptions", "organizations_url": "https://api.github.com/users/Balandat/orgs", "repos_url": "https://api.github.com/users/Balandat/repos", "events_url": "https://api.github.com/users/Balandat/events{/privacy}", "received_events_url": "https://api.github.com/users/Balandat/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-05-23T20:25:49Z", "updated_at": "2018-05-24T20:30:31Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Right now it's possible to have a distribution \"live\" on the GPU, so that you can sample to directly get cuda tensors. You'll have to do something like the following though:</p>\n<pre><code>N = Normal(0.0, 1.0)\nN.loc = N.loc.cuda()\nN.scale = N.scale.cuda()\n</code></pre>\n<p>Alternatively, you can directly initialize the parameters on the GPU:</p>\n<pre><code>N_cuda = Normal(torch.tensor([0.0], device='cuda'), torch.tensor([1.0], device='cuda'))\n</code></pre>\n<p>Both of these methods are somewhat verbose and cumbersome. It would be great having something simpler like the following:</p>\n<pre><code>    Ncuda = Normal(0.0, 1.0).cuda()\n</code></pre>\n<p>Basically, one would just have to make sure that <code>.cuda()</code> gets called on all relevant parameters of the respective distribution.</p>\n<p>Some background on my use case: I'm using distributions to create more abstract notions of priors, which I register inside some Modules. Ideally, I'd just be able to call <code>.cuda()</code> on the top Module and have everything moved to GPU, like is the case with most modules - the above feature request is the basis for that.</p>", "body_text": "Right now it's possible to have a distribution \"live\" on the GPU, so that you can sample to directly get cuda tensors. You'll have to do something like the following though:\nN = Normal(0.0, 1.0)\nN.loc = N.loc.cuda()\nN.scale = N.scale.cuda()\n\nAlternatively, you can directly initialize the parameters on the GPU:\nN_cuda = Normal(torch.tensor([0.0], device='cuda'), torch.tensor([1.0], device='cuda'))\n\nBoth of these methods are somewhat verbose and cumbersome. It would be great having something simpler like the following:\n    Ncuda = Normal(0.0, 1.0).cuda()\n\nBasically, one would just have to make sure that .cuda() gets called on all relevant parameters of the respective distribution.\nSome background on my use case: I'm using distributions to create more abstract notions of priors, which I register inside some Modules. Ideally, I'd just be able to call .cuda() on the top Module and have everything moved to GPU, like is the case with most modules - the above feature request is the basis for that.", "body": "Right now it's possible to have a distribution \"live\" on the GPU, so that you can sample to directly get cuda tensors. You'll have to do something like the following though:\r\n\r\n    N = Normal(0.0, 1.0)\r\n    N.loc = N.loc.cuda()\r\n    N.scale = N.scale.cuda()\r\n\r\nAlternatively, you can directly initialize the parameters on the GPU:\r\n\r\n    N_cuda = Normal(torch.tensor([0.0], device='cuda'), torch.tensor([1.0], device='cuda'))\r\n\r\nBoth of these methods are somewhat verbose and cumbersome. It would be great having something simpler like the following:\r\n\r\n        Ncuda = Normal(0.0, 1.0).cuda()\r\n\r\nBasically, one would just have to make sure that `.cuda()` gets called on all relevant parameters of the respective distribution. \r\n\r\nSome background on my use case: I'm using distributions to create more abstract notions of priors, which I register inside some Modules. Ideally, I'd just be able to call `.cuda()` on the top Module and have everything moved to GPU, like is the case with most modules - the above feature request is the basis for that. "}