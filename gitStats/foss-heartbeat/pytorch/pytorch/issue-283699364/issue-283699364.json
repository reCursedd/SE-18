{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4284", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4284/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4284/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4284/events", "html_url": "https://github.com/pytorch/pytorch/issues/4284", "id": 283699364, "node_id": "MDU6SXNzdWUyODM2OTkzNjQ=", "number": 4284, "title": "BatchNorm doesn't use CuDNN for backwards in evaluation mode", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-20T21:24:06Z", "updated_at": "2017-12-21T00:46:21Z", "closed_at": "2017-12-21T00:46:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In principle we should be able to use the CuDNN kernel by passing nullptr for save_mean and save_std. However, when I do this, gradcheck fails.</p>\n<pre><code>diff --git a/aten/src/ATen/native/cudnn/BatchNorm.cpp b/aten/src/ATen/native/cudnn/BatchNorm.cpp\nindex 14730ee..ab8690e 100644\n--- a/aten/src/ATen/native/cudnn/BatchNorm.cpp\n+++ b/aten/src/ATen/native/cudnn/BatchNorm.cpp\n@@ -146,7 +146,10 @@ std::tuple&lt;Tensor, Tensor, Tensor&gt; cudnn_batch_norm_backward(\n   CheckedFrom c = \"cudnn_batch_norm_backward\";\n   setCuDNNStreamToCurrent();\n \n-  checkAllDefined(c, {input, grad_output, weight, save_mean, save_var});\n+  checkAllDefined(c, {input, grad_output, weight});\n+  if (training) {\n+    checkAllDefined(c, {save_mean, save_var});\n+  }\n   checkAllSameGPU(c, {input, grad_output, weight, save_mean, save_var});\n   if (input-&gt;type().scalarType() == ScalarType::Half) {\n     checkScalarType(c, weight, ScalarType::Float);\n@@ -160,8 +163,11 @@ std::tuple&lt;Tensor, Tensor, Tensor&gt; cudnn_batch_norm_backward(\n   checkDimRange(c, input, 2, 6 /* exclusive */);\n   checkSameSize(c, input, grad_output);\n   auto num_features = input-&gt;size(1);\n-  for (auto t : {weight, save_mean, save_var}) {\n-    checkNumel(c, t, num_features);\n+  checkNumel(c, weight, num_features);\n+  if (training) {\n+    for (auto t : {save_mean, save_var}) {\n+      checkNumel(c, t, num_features);\n+    }\n   }\n \n   cudnnBatchNormMode_t mode;\n@@ -197,8 +203,8 @@ std::tuple&lt;Tensor, Tensor, Tensor&gt; cudnn_batch_norm_backward(\n     grad_weight_t.data_ptr(),\n     grad_bias_t.data_ptr(),\n     epsilon,\n-    save_mean-&gt;data_ptr(),\n-    save_var-&gt;data_ptr()));\n+    training ? save_mean-&gt;data_ptr() : nullptr,\n+    training ? save_var-&gt;data_ptr() : nullptr));\n \n   return std::tuple&lt;Tensor,Tensor,Tensor&gt;{grad_input_t, grad_weight_t, grad_bias_t};\n }\nzation.cpp a/torch/csrc/autograd/functions/batch_normalization.cpp b/torch/csrc/autograd/functions/batch_normal \nindex d01f755..833d643 100644\n--- a/torch/csrc/autograd/functions/batch_normalization.cpp\n+++ b/torch/csrc/autograd/func\n                              tions/batch_normalization.cpp\n@@ -107,7 +107,7 @@ auto BatchNormBackward::apply(const variable_list&amp; grad_outputs) -&gt; variable_lis\n   use_cudnn = (input.type().backend() == at::kCUDA\n                &amp;&amp; (input.type().scalarType() != at::kHalf\n                || weight.type().scalarType() == at::kFloat)\n-               &amp;&amp; weight.defined() &amp;&amp; bias.defined() &amp;&amp; training\n+               &amp;&amp; weight.defined() &amp;&amp; bias.defined()\n                &amp;&amp; input.size(0) &lt;= 131070\n                &amp;&amp; cudnn_enabled &amp;&amp; CUDNN_VERSION &gt;= 5110L);\n #endif\n</code></pre>\n<p>I also tried tracking down the history but the <code>training</code> special case seems to have been here since the very beginning of Pytorch.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a></p>", "body_text": "In principle we should be able to use the CuDNN kernel by passing nullptr for save_mean and save_std. However, when I do this, gradcheck fails.\ndiff --git a/aten/src/ATen/native/cudnn/BatchNorm.cpp b/aten/src/ATen/native/cudnn/BatchNorm.cpp\nindex 14730ee..ab8690e 100644\n--- a/aten/src/ATen/native/cudnn/BatchNorm.cpp\n+++ b/aten/src/ATen/native/cudnn/BatchNorm.cpp\n@@ -146,7 +146,10 @@ std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(\n   CheckedFrom c = \"cudnn_batch_norm_backward\";\n   setCuDNNStreamToCurrent();\n \n-  checkAllDefined(c, {input, grad_output, weight, save_mean, save_var});\n+  checkAllDefined(c, {input, grad_output, weight});\n+  if (training) {\n+    checkAllDefined(c, {save_mean, save_var});\n+  }\n   checkAllSameGPU(c, {input, grad_output, weight, save_mean, save_var});\n   if (input->type().scalarType() == ScalarType::Half) {\n     checkScalarType(c, weight, ScalarType::Float);\n@@ -160,8 +163,11 @@ std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(\n   checkDimRange(c, input, 2, 6 /* exclusive */);\n   checkSameSize(c, input, grad_output);\n   auto num_features = input->size(1);\n-  for (auto t : {weight, save_mean, save_var}) {\n-    checkNumel(c, t, num_features);\n+  checkNumel(c, weight, num_features);\n+  if (training) {\n+    for (auto t : {save_mean, save_var}) {\n+      checkNumel(c, t, num_features);\n+    }\n   }\n \n   cudnnBatchNormMode_t mode;\n@@ -197,8 +203,8 @@ std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(\n     grad_weight_t.data_ptr(),\n     grad_bias_t.data_ptr(),\n     epsilon,\n-    save_mean->data_ptr(),\n-    save_var->data_ptr()));\n+    training ? save_mean->data_ptr() : nullptr,\n+    training ? save_var->data_ptr() : nullptr));\n \n   return std::tuple<Tensor,Tensor,Tensor>{grad_input_t, grad_weight_t, grad_bias_t};\n }\nzation.cpp a/torch/csrc/autograd/functions/batch_normalization.cpp b/torch/csrc/autograd/functions/batch_normal \nindex d01f755..833d643 100644\n--- a/torch/csrc/autograd/functions/batch_normalization.cpp\n+++ b/torch/csrc/autograd/func\n                              tions/batch_normalization.cpp\n@@ -107,7 +107,7 @@ auto BatchNormBackward::apply(const variable_list& grad_outputs) -> variable_lis\n   use_cudnn = (input.type().backend() == at::kCUDA\n                && (input.type().scalarType() != at::kHalf\n                || weight.type().scalarType() == at::kFloat)\n-               && weight.defined() && bias.defined() && training\n+               && weight.defined() && bias.defined()\n                && input.size(0) <= 131070\n                && cudnn_enabled && CUDNN_VERSION >= 5110L);\n #endif\n\nI also tried tracking down the history but the training special case seems to have been here since the very beginning of Pytorch.\nCC @ngimel", "body": "In principle we should be able to use the CuDNN kernel by passing nullptr for save_mean and save_std. However, when I do this, gradcheck fails.\r\n\r\n```\r\ndiff --git a/aten/src/ATen/native/cudnn/BatchNorm.cpp b/aten/src/ATen/native/cudnn/BatchNorm.cpp\r\nindex 14730ee..ab8690e 100644\r\n--- a/aten/src/ATen/native/cudnn/BatchNorm.cpp\r\n+++ b/aten/src/ATen/native/cudnn/BatchNorm.cpp\r\n@@ -146,7 +146,10 @@ std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(\r\n   CheckedFrom c = \"cudnn_batch_norm_backward\";\r\n   setCuDNNStreamToCurrent();\r\n \r\n-  checkAllDefined(c, {input, grad_output, weight, save_mean, save_var});\r\n+  checkAllDefined(c, {input, grad_output, weight});\r\n+  if (training) {\r\n+    checkAllDefined(c, {save_mean, save_var});\r\n+  }\r\n   checkAllSameGPU(c, {input, grad_output, weight, save_mean, save_var});\r\n   if (input->type().scalarType() == ScalarType::Half) {\r\n     checkScalarType(c, weight, ScalarType::Float);\r\n@@ -160,8 +163,11 @@ std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(\r\n   checkDimRange(c, input, 2, 6 /* exclusive */);\r\n   checkSameSize(c, input, grad_output);\r\n   auto num_features = input->size(1);\r\n-  for (auto t : {weight, save_mean, save_var}) {\r\n-    checkNumel(c, t, num_features);\r\n+  checkNumel(c, weight, num_features);\r\n+  if (training) {\r\n+    for (auto t : {save_mean, save_var}) {\r\n+      checkNumel(c, t, num_features);\r\n+    }\r\n   }\r\n \r\n   cudnnBatchNormMode_t mode;\r\n@@ -197,8 +203,8 @@ std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(\r\n     grad_weight_t.data_ptr(),\r\n     grad_bias_t.data_ptr(),\r\n     epsilon,\r\n-    save_mean->data_ptr(),\r\n-    save_var->data_ptr()));\r\n+    training ? save_mean->data_ptr() : nullptr,\r\n+    training ? save_var->data_ptr() : nullptr));\r\n \r\n   return std::tuple<Tensor,Tensor,Tensor>{grad_input_t, grad_weight_t, grad_bias_t};\r\n }\r\nzation.cpp a/torch/csrc/autograd/functions/batch_normalization.cpp b/torch/csrc/autograd/functions/batch_normal \r\nindex d01f755..833d643 100644\r\n--- a/torch/csrc/autograd/functions/batch_normalization.cpp\r\n+++ b/torch/csrc/autograd/func\r\n                              tions/batch_normalization.cpp\r\n@@ -107,7 +107,7 @@ auto BatchNormBackward::apply(const variable_list& grad_outputs) -> variable_lis\r\n   use_cudnn = (input.type().backend() == at::kCUDA\r\n                && (input.type().scalarType() != at::kHalf\r\n                || weight.type().scalarType() == at::kFloat)\r\n-               && weight.defined() && bias.defined() && training\r\n+               && weight.defined() && bias.defined()\r\n                && input.size(0) <= 131070\r\n                && cudnn_enabled && CUDNN_VERSION >= 5110L);\r\n #endif\r\n```\r\n\r\nI also tried tracking down the history but the `training` special case seems to have been here since the very beginning of Pytorch.\r\n\r\nCC @ngimel"}