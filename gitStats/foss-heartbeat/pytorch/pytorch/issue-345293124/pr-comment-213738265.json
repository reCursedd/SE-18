{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213738265", "pull_request_review_id": 150636273, "id": 213738265, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMzczODI2NQ==", "diff_hunk": "@@ -1,57 +1,127 @@\n #include \"ATen/cuda/CUDAStream.h\"\n #include \"ATen/cuda/CUDAContext.h\"\n #include \"ATen/cuda/Exceptions.h\"\n+#include \"ATen/DeviceGuard.h\"\n #include \"ATen/Error.h\"\n \n #include <mutex>\n #include <atomic>\n+#include <cstdint>\n+#include <deque>\n+#include <vector>\n \n // Internal implementation is entirely hidden\n+// Note: CUDAStreamInternals doubles for a THCStream\n struct CUDAStreamInternals {\n-  bool is_destructible;\n-  std::atomic<int> refcount;\n-  int64_t device; // Note: cudaGetDevice works with int32_t, not int64_t\n-  cudaStream_t stream;\n+  CUDAStreamInternals() = default;\n+\n+  ~CUDAStreamInternals() {\n+    if (stream) cudaStreamDestroy(stream);\n+  }\n+\n+  int64_t device = -1; \n+  cudaStream_t stream = nullptr;\n };\n \n namespace at {\n namespace cuda {\n \n namespace detail {\n \n-  /*\n-  * Stream state\n-  */\n-  static constexpr cudaStream_t DEFAULT_STREAM = 0;\n-\n-  static std::once_flag init_flag;\n+  // Global stream state and constants\n   static int64_t num_gpus;\n-  static CUDAStreamInternals* default_streams;\n+  static constexpr int STREAMS_PER_POOL = 32;\n+  static constexpr unsigned int DEFAULT_FLAGS = cudaStreamNonBlocking;\n+  static int HIGH_PRIORITY = 0;\n+  static int LOW_PRIORITY = 0;\n+  \n+  // Default streams\n+  static std::once_flag init_flag;\n+  static std::vector<CUDAStreamInternals> default_streams;\n+\n+  // Non-default streams\n+  static std::deque<std::once_flag> device_flags;\n+  static std::deque<std::atomic<int>> low_priority_counters;\n+  static std::deque<std::atomic<int>> high_priority_counters;", "path": "aten/src/ATen/cuda/CUDAStream.cpp", "position": null, "original_position": 56, "commit_id": "3c0c30659be4528f10b94829b64f8532c3e61bb1", "original_commit_id": "47920408ab3d205b1817b9be4865fc8d0b2402ba", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "You can store those types in a vector, as long as you don't expand it at runtime. I've used vectors of mutexes in the past and they worked fine.", "created_at": "2018-08-29T16:00:22Z", "updated_at": "2018-11-23T15:50:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/9938#discussion_r213738265", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9938", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213738265"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9938#discussion_r213738265"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9938"}}, "body_html": "<p>You can store those types in a vector, as long as you don't expand it at runtime. I've used vectors of mutexes in the past and they worked fine.</p>", "body_text": "You can store those types in a vector, as long as you don't expand it at runtime. I've used vectors of mutexes in the past and they worked fine.", "in_reply_to_id": 213398365}