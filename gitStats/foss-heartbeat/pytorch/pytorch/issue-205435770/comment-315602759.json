{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/315602759", "html_url": "https://github.com/pytorch/pytorch/issues/686#issuecomment-315602759", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/686", "id": 315602759, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTYwMjc1OQ==", "user": {"login": "kedarpathak", "id": 30209670, "node_id": "MDQ6VXNlcjMwMjA5Njcw", "avatar_url": "https://avatars0.githubusercontent.com/u/30209670?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kedarpathak", "html_url": "https://github.com/kedarpathak", "followers_url": "https://api.github.com/users/kedarpathak/followers", "following_url": "https://api.github.com/users/kedarpathak/following{/other_user}", "gists_url": "https://api.github.com/users/kedarpathak/gists{/gist_id}", "starred_url": "https://api.github.com/users/kedarpathak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kedarpathak/subscriptions", "organizations_url": "https://api.github.com/users/kedarpathak/orgs", "repos_url": "https://api.github.com/users/kedarpathak/repos", "events_url": "https://api.github.com/users/kedarpathak/events{/privacy}", "received_events_url": "https://api.github.com/users/kedarpathak/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-16T11:24:50Z", "updated_at": "2017-07-16T11:24:50Z", "author_association": "NONE", "body_html": "<p>class Generator(nn.Module):</p>\n<pre><code>def __init__(self, input_size, output_size, hidden_dims):\n\n    super(Generator, self).__init__()\n\n    self.layers = []\n\n    \n\n    prev_dim = input_size\n\n    for hidden_dim in hidden_dims:\n\n        self.layers.append(nn.Linear(prev_dim, hidden_dim))\n\n        self.layers.append(nn.ReLU(True))\n\n        prev_dim = hidden_dim\n\n    self.layers.append(nn.Linear(prev_dim, output_size))\n\n    \n\n    self.layer_module = ListModule(*self.layers)\n\n    \n\ndef forward(self, x):\n\n    out = x\n\n    for layer in self.layers:\n\n        out = layer(out)\n\n    return out\n</code></pre>\n<p>\u200b</p>\n<p>class Discriminator(nn.Module):</p>\n<pre><code>def __init__(self, input_size, output_size, hidden_dims):\n\n    super(Discriminator, self).__init__()\n\n    self.layers = []\n\n    \n\n    prev_dim = input_size\n\n    for idx, hidden_dim in enumerate(hidden_dims):\n\n        self.layers.append(nn.Linear(prev_dim, hidden_dim))\n\n        self.layers.append(nn.ReLU(True))\n\n        prev_dim = hidden_dim\n\n        \n\n    self.layers.append(nn.Linear(prev_dim, output_size))\n\n    self.layers.append(nn.Sigmoid())\n\n    \n\n    self.layer_module = ListModule(*self.layers)\n</code></pre>\n<p>\u200b</p>\n<pre><code>def forward(self, x):\n\n    out = x\n\n    for layer in self.layers:\n\n        out = layer(out)\n\n    return out.view(-1, 1)\n</code></pre>\n<p>\u200b</p>\n<h1>network</h1>\n<p>hidden_dim = 128</p>\n<p>g_num_layer = 3</p>\n<p>d_num_layer = 5</p>\n<p>\u200b</p>\n<p>G_AB = Generator(2, 2, [hidden_dim] * g_num_layer)</p>\n<p>G_BA = Generator(2, 2, [hidden_dim] * g_num_layer)</p>\n<p>\u200b</p>\n<p>D_A = Discriminator(2, 1, [hidden_dim] * d_num_layer)</p>\n<p>D_B = Discriminator(2, 1, [hidden_dim] * d_num_layer)</p>\n<p>\u200b</p>\n<p>G_AB.cuda()</p>\n<p>G_BA.cuda()</p>\n<p>D_A.cuda()</p>\n<p>D_B.cuda()</p>\n<p>\u200b</p>\n<h1>optimizer</h1>\n<p>lr = 0.0002</p>\n<p>beta1 = 0.5</p>\n<p>beta2 = 0.999</p>\n<p>\u200b</p>\n<p>d = nn.MSELoss()</p>\n<p>bce = nn.BCELoss()</p>\n<p>\u200b</p>\n<p>optimizer_d = torch.optim.Adam(</p>\n<pre><code>chain(D_A.parameters(), D_B.parameters()), lr=lr, betas=(beta1, beta2))\n</code></pre>\n<p>optimizer_g = torch.optim.Adam(</p>\n<pre><code>chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(beta1, beta2))\n</code></pre>\n<p>\u200b</p>\n<h1>training</h1>\n<p>num_epoch = 50000</p>\n<p>\u200b</p>\n<p>real_label = 1</p>\n<p>fake_label = 0</p>\n<p>\u200b</p>\n<p>real_tensor = Variable(torch.FloatTensor(batch_size).cuda())</p>\n<p>_ = real_tensor.data.fill_(real_label)</p>\n<p>print(real_tensor.sum())</p>\n<p>\u200b</p>\n<p>fake_tensor = Variable(torch.FloatTensor(batch_size).cuda())</p>\n<p>_ = fake_tensor.data.fill_(fake_label)</p>\n<p>print(fake_tensor.sum())</p>\n<hr>\n<p>RuntimeError                              Traceback (most recent call last)<br>\n in ()<br>\n77<br>\n78 real_tensor = Variable(torch.FloatTensor(batch_size).cuda())<br>\n---&gt; 79 _ = real_tensor.data.fill_(real_label)<br>\n80 print(real_tensor.sum())<br>\n81</p>\n<p>RuntimeError: cuda runtime error (8) : invalid device function at /py/conda-bld/pytorch_1493677666423/work/torch/lib/THC/generic/THCTensorMath.cu:15</p>", "body_text": "class Generator(nn.Module):\ndef __init__(self, input_size, output_size, hidden_dims):\n\n    super(Generator, self).__init__()\n\n    self.layers = []\n\n    \n\n    prev_dim = input_size\n\n    for hidden_dim in hidden_dims:\n\n        self.layers.append(nn.Linear(prev_dim, hidden_dim))\n\n        self.layers.append(nn.ReLU(True))\n\n        prev_dim = hidden_dim\n\n    self.layers.append(nn.Linear(prev_dim, output_size))\n\n    \n\n    self.layer_module = ListModule(*self.layers)\n\n    \n\ndef forward(self, x):\n\n    out = x\n\n    for layer in self.layers:\n\n        out = layer(out)\n\n    return out\n\n\u200b\nclass Discriminator(nn.Module):\ndef __init__(self, input_size, output_size, hidden_dims):\n\n    super(Discriminator, self).__init__()\n\n    self.layers = []\n\n    \n\n    prev_dim = input_size\n\n    for idx, hidden_dim in enumerate(hidden_dims):\n\n        self.layers.append(nn.Linear(prev_dim, hidden_dim))\n\n        self.layers.append(nn.ReLU(True))\n\n        prev_dim = hidden_dim\n\n        \n\n    self.layers.append(nn.Linear(prev_dim, output_size))\n\n    self.layers.append(nn.Sigmoid())\n\n    \n\n    self.layer_module = ListModule(*self.layers)\n\n\u200b\ndef forward(self, x):\n\n    out = x\n\n    for layer in self.layers:\n\n        out = layer(out)\n\n    return out.view(-1, 1)\n\n\u200b\nnetwork\nhidden_dim = 128\ng_num_layer = 3\nd_num_layer = 5\n\u200b\nG_AB = Generator(2, 2, [hidden_dim] * g_num_layer)\nG_BA = Generator(2, 2, [hidden_dim] * g_num_layer)\n\u200b\nD_A = Discriminator(2, 1, [hidden_dim] * d_num_layer)\nD_B = Discriminator(2, 1, [hidden_dim] * d_num_layer)\n\u200b\nG_AB.cuda()\nG_BA.cuda()\nD_A.cuda()\nD_B.cuda()\n\u200b\noptimizer\nlr = 0.0002\nbeta1 = 0.5\nbeta2 = 0.999\n\u200b\nd = nn.MSELoss()\nbce = nn.BCELoss()\n\u200b\noptimizer_d = torch.optim.Adam(\nchain(D_A.parameters(), D_B.parameters()), lr=lr, betas=(beta1, beta2))\n\noptimizer_g = torch.optim.Adam(\nchain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(beta1, beta2))\n\n\u200b\ntraining\nnum_epoch = 50000\n\u200b\nreal_label = 1\nfake_label = 0\n\u200b\nreal_tensor = Variable(torch.FloatTensor(batch_size).cuda())\n_ = real_tensor.data.fill_(real_label)\nprint(real_tensor.sum())\n\u200b\nfake_tensor = Variable(torch.FloatTensor(batch_size).cuda())\n_ = fake_tensor.data.fill_(fake_label)\nprint(fake_tensor.sum())\n\nRuntimeError                              Traceback (most recent call last)\n in ()\n77\n78 real_tensor = Variable(torch.FloatTensor(batch_size).cuda())\n---> 79 _ = real_tensor.data.fill_(real_label)\n80 print(real_tensor.sum())\n81\nRuntimeError: cuda runtime error (8) : invalid device function at /py/conda-bld/pytorch_1493677666423/work/torch/lib/THC/generic/THCTensorMath.cu:15", "body": "class Generator(nn.Module):\r\n\r\n    def __init__(self, input_size, output_size, hidden_dims):\r\n\r\n        super(Generator, self).__init__()\r\n\r\n        self.layers = []\r\n\r\n        \r\n\r\n        prev_dim = input_size\r\n\r\n        for hidden_dim in hidden_dims:\r\n\r\n            self.layers.append(nn.Linear(prev_dim, hidden_dim))\r\n\r\n            self.layers.append(nn.ReLU(True))\r\n\r\n            prev_dim = hidden_dim\r\n\r\n        self.layers.append(nn.Linear(prev_dim, output_size))\r\n\r\n        \r\n\r\n        self.layer_module = ListModule(*self.layers)\r\n\r\n        \r\n\r\n    def forward(self, x):\r\n\r\n        out = x\r\n\r\n        for layer in self.layers:\r\n\r\n            out = layer(out)\r\n\r\n        return out\r\n\r\n\u200b\r\n\r\nclass Discriminator(nn.Module):\r\n\r\n    def __init__(self, input_size, output_size, hidden_dims):\r\n\r\n        super(Discriminator, self).__init__()\r\n\r\n        self.layers = []\r\n\r\n        \r\n\r\n        prev_dim = input_size\r\n\r\n        for idx, hidden_dim in enumerate(hidden_dims):\r\n\r\n            self.layers.append(nn.Linear(prev_dim, hidden_dim))\r\n\r\n            self.layers.append(nn.ReLU(True))\r\n\r\n            prev_dim = hidden_dim\r\n\r\n            \r\n\r\n        self.layers.append(nn.Linear(prev_dim, output_size))\r\n\r\n        self.layers.append(nn.Sigmoid())\r\n\r\n        \r\n\r\n        self.layer_module = ListModule(*self.layers)\r\n\r\n\u200b\r\n\r\n    def forward(self, x):\r\n\r\n        out = x\r\n\r\n        for layer in self.layers:\r\n\r\n            out = layer(out)\r\n\r\n        return out.view(-1, 1)\r\n\r\n\u200b\r\n\r\n# network\r\n\r\nhidden_dim = 128\r\n\r\ng_num_layer = 3\r\n\r\nd_num_layer = 5\r\n\r\n\u200b\r\n\r\nG_AB = Generator(2, 2, [hidden_dim] * g_num_layer)\r\n\r\nG_BA = Generator(2, 2, [hidden_dim] * g_num_layer)\r\n\r\n\u200b\r\n\r\nD_A = Discriminator(2, 1, [hidden_dim] * d_num_layer)\r\n\r\nD_B = Discriminator(2, 1, [hidden_dim] * d_num_layer)\r\n\r\n\u200b\r\n\r\nG_AB.cuda()\r\n\r\nG_BA.cuda()\r\n\r\nD_A.cuda()\r\n\r\nD_B.cuda()\r\n\r\n\u200b\r\n\r\n# optimizer\r\n\r\nlr = 0.0002\r\n\r\nbeta1 = 0.5\r\n\r\nbeta2 = 0.999\r\n\r\n\u200b\r\n\r\nd = nn.MSELoss()\r\n\r\nbce = nn.BCELoss()\r\n\r\n\u200b\r\n\r\noptimizer_d = torch.optim.Adam(\r\n\r\n    chain(D_A.parameters(), D_B.parameters()), lr=lr, betas=(beta1, beta2))\r\n\r\noptimizer_g = torch.optim.Adam(\r\n\r\n    chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(beta1, beta2))\r\n\r\n\u200b\r\n\r\n# training\r\n\r\nnum_epoch = 50000\r\n\r\n\u200b\r\n\r\nreal_label = 1\r\n\r\nfake_label = 0\r\n\r\n\u200b\r\n\r\nreal_tensor = Variable(torch.FloatTensor(batch_size).cuda())\r\n\r\n_ = real_tensor.data.fill_(real_label)\r\n\r\nprint(real_tensor.sum())\r\n\r\n\u200b\r\n\r\nfake_tensor = Variable(torch.FloatTensor(batch_size).cuda())\r\n\r\n_ = fake_tensor.data.fill_(fake_label)\r\n\r\nprint(fake_tensor.sum())\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-8-b559c2ac8db1> in <module>()\r\n     77 \r\n     78 real_tensor = Variable(torch.FloatTensor(batch_size).cuda())\r\n---> 79 _ = real_tensor.data.fill_(real_label)\r\n     80 print(real_tensor.sum())\r\n     81 \r\n\r\nRuntimeError: cuda runtime error (8) : invalid device function at /py/conda-bld/pytorch_1493677666423/work/torch/lib/THC/generic/THCTensorMath.cu:15\r\n\r\n"}