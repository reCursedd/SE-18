{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/400338207", "html_url": "https://github.com/pytorch/pytorch/issues/1333#issuecomment-400338207", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1333", "id": 400338207, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDMzODIwNw==", "user": {"login": "arogozhnikov", "id": 6318811, "node_id": "MDQ6VXNlcjYzMTg4MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6318811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arogozhnikov", "html_url": "https://github.com/arogozhnikov", "followers_url": "https://api.github.com/users/arogozhnikov/followers", "following_url": "https://api.github.com/users/arogozhnikov/following{/other_user}", "gists_url": "https://api.github.com/users/arogozhnikov/gists{/gist_id}", "starred_url": "https://api.github.com/users/arogozhnikov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arogozhnikov/subscriptions", "organizations_url": "https://api.github.com/users/arogozhnikov/orgs", "repos_url": "https://api.github.com/users/arogozhnikov/repos", "events_url": "https://api.github.com/users/arogozhnikov/events{/privacy}", "received_events_url": "https://api.github.com/users/arogozhnikov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-26T14:51:29Z", "updated_at": "2018-07-20T10:28:54Z", "author_association": "NONE", "body_html": "<p>As a workaround (to avoid wasting memory) - pad from both sides in convolution, then take a slice of output.</p>\n<p>Update: see <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1828968\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/calclavia\">@calclavia</a> answer, which already implements this. Below is implementation as a module</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">CausalConv1d</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Conv1d</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,\n                 <span class=\"pl-smi\">in_channels</span>,\n                 <span class=\"pl-smi\">out_channels</span>,\n                 <span class=\"pl-smi\">kernel_size</span>,\n                 <span class=\"pl-smi\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                 <span class=\"pl-smi\">dilation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                 <span class=\"pl-smi\">groups</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                 <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">self</span>.__padding <span class=\"pl-k\">=</span> (kernel_size <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> dilation\n\n        <span class=\"pl-c1\">super</span>(CausalConv1d, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>(\n            in_channels,\n            out_channels,\n            <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>kernel_size,\n            <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span>stride,\n            <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.__padding,\n            <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation,\n            <span class=\"pl-v\">groups</span><span class=\"pl-k\">=</span>groups,\n            <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>bias)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">super</span>(CausalConv1d, <span class=\"pl-c1\">self</span>).forward(<span class=\"pl-c1\">input</span>)\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.__padding <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-k\">return</span> result[:, :, :<span class=\"pl-k\">-</span><span class=\"pl-c1\">self</span>.__padding]\n        <span class=\"pl-k\">return</span> result</pre></div>", "body_text": "As a workaround (to avoid wasting memory) - pad from both sides in convolution, then take a slice of output.\nUpdate: see @calclavia answer, which already implements this. Below is implementation as a module\nclass CausalConv1d(torch.nn.Conv1d):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 dilation=1,\n                 groups=1,\n                 bias=True):\n        self.__padding = (kernel_size - 1) * dilation\n\n        super(CausalConv1d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=self.__padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias)\n\n    def forward(self, input):\n        result = super(CausalConv1d, self).forward(input)\n        if self.__padding != 0:\n            return result[:, :, :-self.__padding]\n        return result", "body": "As a workaround (to avoid wasting memory) - pad from both sides in convolution, then take a slice of output. \r\n\r\nUpdate: see @calclavia answer, which already implements this. Below is implementation as a module\r\n\r\n```python\r\nclass CausalConv1d(torch.nn.Conv1d):\r\n    def __init__(self,\r\n                 in_channels,\r\n                 out_channels,\r\n                 kernel_size,\r\n                 stride=1,\r\n                 dilation=1,\r\n                 groups=1,\r\n                 bias=True):\r\n        self.__padding = (kernel_size - 1) * dilation\r\n\r\n        super(CausalConv1d, self).__init__(\r\n            in_channels,\r\n            out_channels,\r\n            kernel_size=kernel_size,\r\n            stride=stride,\r\n            padding=self.__padding,\r\n            dilation=dilation,\r\n            groups=groups,\r\n            bias=bias)\r\n\r\n    def forward(self, input):\r\n        result = super(CausalConv1d, self).forward(input)\r\n        if self.__padding != 0:\r\n            return result[:, :, :-self.__padding]\r\n        return result\r\n```"}