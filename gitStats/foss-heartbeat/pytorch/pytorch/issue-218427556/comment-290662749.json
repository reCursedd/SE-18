{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/290662749", "html_url": "https://github.com/pytorch/pytorch/issues/1150#issuecomment-290662749", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1150", "id": 290662749, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MDY2Mjc0OQ==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-31T09:19:07Z", "updated_at": "2017-03-31T09:19:07Z", "author_association": "COLLABORATOR", "body_html": "<p>Hi,</p>\n<p>In the doc of the function (<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/cuda/comm.py#L14\">here</a>) it says: <code>Note that it should be like (src, dst1, dst2, ...)</code> for the list of devices that you give. So it assumes that either the tensor is on cpu and it sends to all device or that the first device that you give him is the one where your tensor is currently in.</p>\n<p>Maybe it would be worth raising a proper error message when this supposition is not met?</p>", "body_text": "Hi,\nIn the doc of the function (here) it says: Note that it should be like (src, dst1, dst2, ...) for the list of devices that you give. So it assumes that either the tensor is on cpu and it sends to all device or that the first device that you give him is the one where your tensor is currently in.\nMaybe it would be worth raising a proper error message when this supposition is not met?", "body": "Hi,\r\n\r\nIn the doc of the function ([here](https://github.com/pytorch/pytorch/blob/master/torch/cuda/comm.py#L14)) it says: `Note that it should be like (src, dst1, dst2, ...)` for the list of devices that you give. So it assumes that either the tensor is on cpu and it sends to all device or that the first device that you give him is the one where your tensor is currently in.\r\n\r\nMaybe it would be worth raising a proper error message when this supposition is not met?"}