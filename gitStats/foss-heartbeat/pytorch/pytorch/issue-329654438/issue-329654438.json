{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8177", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8177/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8177/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8177/events", "html_url": "https://github.com/pytorch/pytorch/pull/8177", "id": 329654438, "node_id": "MDExOlB1bGxSZXF1ZXN0MTkyODU5MDU0", "number": 8177, "title": "[WIP] Better support for literals in the jit", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-05T23:01:21Z", "updated_at": "2018-11-23T15:45:03Z", "closed_at": "2018-06-08T16:09:14Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8177", "html_url": "https://github.com/pytorch/pytorch/pull/8177", "diff_url": "https://github.com/pytorch/pytorch/pull/8177.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8177.patch"}, "body_html": "<p>As shown in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"322386096\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7504\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7504/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7504\">#7504</a></p>\n<p>Very WIP (there are many edge cases to work out; a lot of the tests do not pass; have not added new tests)</p>\n<p>General approach:</p>\n<ul>\n<li>Add <code>FloatType</code> and <code>IntType</code> to <code>type.h</code> and assign these to constant literals (python scalars)</li>\n<li>Propagate these types on basic math (+-*/) on python scalars</li>\n<li>When adding a tensor <code>t</code> and a python scalar <code>1</code> (<code>x+1</code>), the scalar is casted using <code>aten::view_as</code> to the type of the tensor (dtype <em>and</em> backend) to match current PyTorch behavior.</li>\n</ul>\n<p>For example, the following code:</p>\n<pre><code>@torch.jit.script\ndef fn(x):\n    a = 17\n    b = -3.14\n    c = a + b\n    return x + c\n</code></pre>\n<p>compiles to:</p>\n<pre><code>In [4]: fn.graph\nOut[4]:\ngraph(%x : Dynamic) {\n  %a : int = prim::Constant[value={17}]()\n  %2 : float = prim::Constant[value={3.14}]()\n  %b : float = aten::neg(%2)  # propagating float type\n  %5 : Dynamic = aten::type_as(%a, %b)        # in preparation for int + float -&gt; float \n  %6 : Dynamic = aten::type_as(%b, %b)        # I guess this should be a no-op\n  %c : float = aten::add[alpha={1}](%5, %6)   # int + float -&gt; float\n  %9 : Dynamic = aten::type_as(%c, %x)        # in preparation for Tensor + float -&gt; Tensor\n  %10 : Dynamic = aten::add[alpha={1}](%x, %9)  # Tensor + Tensor -&gt; Tensor\n  return (%10);\n}\n</code></pre>", "body_text": "As shown in #7504\nVery WIP (there are many edge cases to work out; a lot of the tests do not pass; have not added new tests)\nGeneral approach:\n\nAdd FloatType and IntType to type.h and assign these to constant literals (python scalars)\nPropagate these types on basic math (+-*/) on python scalars\nWhen adding a tensor t and a python scalar 1 (x+1), the scalar is casted using aten::view_as to the type of the tensor (dtype and backend) to match current PyTorch behavior.\n\nFor example, the following code:\n@torch.jit.script\ndef fn(x):\n    a = 17\n    b = -3.14\n    c = a + b\n    return x + c\n\ncompiles to:\nIn [4]: fn.graph\nOut[4]:\ngraph(%x : Dynamic) {\n  %a : int = prim::Constant[value={17}]()\n  %2 : float = prim::Constant[value={3.14}]()\n  %b : float = aten::neg(%2)  # propagating float type\n  %5 : Dynamic = aten::type_as(%a, %b)        # in preparation for int + float -> float \n  %6 : Dynamic = aten::type_as(%b, %b)        # I guess this should be a no-op\n  %c : float = aten::add[alpha={1}](%5, %6)   # int + float -> float\n  %9 : Dynamic = aten::type_as(%c, %x)        # in preparation for Tensor + float -> Tensor\n  %10 : Dynamic = aten::add[alpha={1}](%x, %9)  # Tensor + Tensor -> Tensor\n  return (%10);\n}", "body": "As shown in https://github.com/pytorch/pytorch/issues/7504\r\n\r\nVery WIP (there are many edge cases to work out; a lot of the tests do not pass; have not added new tests)\r\n\r\nGeneral approach:\r\n- Add `FloatType` and `IntType` to `type.h` and assign these to constant literals (python scalars)\r\n- Propagate these types on basic math (+-*/) on python scalars\r\n- When adding a tensor `t` and a python scalar `1` (`x+1`), the scalar is casted using `aten::view_as` to the type of the tensor (dtype _and_ backend) to match current PyTorch behavior.\r\n\r\nFor example, the following code:\r\n```\r\n@torch.jit.script\r\ndef fn(x):\r\n    a = 17\r\n    b = -3.14\r\n    c = a + b\r\n    return x + c\r\n```\r\ncompiles to:\r\n```\r\nIn [4]: fn.graph\r\nOut[4]:\r\ngraph(%x : Dynamic) {\r\n  %a : int = prim::Constant[value={17}]()\r\n  %2 : float = prim::Constant[value={3.14}]()\r\n  %b : float = aten::neg(%2)  # propagating float type\r\n  %5 : Dynamic = aten::type_as(%a, %b)        # in preparation for int + float -> float \r\n  %6 : Dynamic = aten::type_as(%b, %b)        # I guess this should be a no-op\r\n  %c : float = aten::add[alpha={1}](%5, %6)   # int + float -> float\r\n  %9 : Dynamic = aten::type_as(%c, %x)        # in preparation for Tensor + float -> Tensor\r\n  %10 : Dynamic = aten::add[alpha={1}](%x, %9)  # Tensor + Tensor -> Tensor\r\n  return (%10);\r\n}\r\n```"}