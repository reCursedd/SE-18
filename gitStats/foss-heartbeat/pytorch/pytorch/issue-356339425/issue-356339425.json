{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11192", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11192/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11192/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11192/events", "html_url": "https://github.com/pytorch/pytorch/issues/11192", "id": 356339425, "node_id": "MDU6SXNzdWUzNTYzMzk0MjU=", "number": 11192, "title": "Cannot Backward Twice Through Distribution", "user": {"login": "purboo", "id": 3917986, "node_id": "MDQ6VXNlcjM5MTc5ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/3917986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/purboo", "html_url": "https://github.com/purboo", "followers_url": "https://api.github.com/users/purboo/followers", "following_url": "https://api.github.com/users/purboo/following{/other_user}", "gists_url": "https://api.github.com/users/purboo/gists{/gist_id}", "starred_url": "https://api.github.com/users/purboo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/purboo/subscriptions", "organizations_url": "https://api.github.com/users/purboo/orgs", "repos_url": "https://api.github.com/users/purboo/repos", "events_url": "https://api.github.com/users/purboo/events{/privacy}", "received_events_url": "https://api.github.com/users/purboo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-09-03T02:26:56Z", "updated_at": "2018-09-03T08:58:45Z", "closed_at": "2018-09-03T03:12:30Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Cannot backward twice through distribution (see example below).</p>\n<h2>Code example</h2>\n<p>It is ok to backward through two different loss:</p>\n<pre><code>t = torch.tensor(1.0, requires_grad=True)\nl1 = 2 * t\nl2 = 3 * t\n\nl1.backward() # ok\nl2.backward() # ok\n</code></pre>\n<p>However, it is not OK to backward through two different loss through distribution:</p>\n<pre><code>p = torch.tensor(0.2, requires_grad=True)\npi = Bernoulli(p)\n\nl1 = 2 * pi.log_prob(torch.tensor(1.0)) \nl2 = 3 * pi.log_prob(torch.tensor(2.0))\nl1.backward() # ok\nl2.backward() # Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n</code></pre>\n<p>What is the reason behind this?</p>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: Pytorch</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>OS: Arch Linux</li>\n<li>PyTorch version: 0.4.1.post2</li>\n<li>Python version: 3.7.0</li>\n<li>CUDA/cuDNN version: 9.0</li>\n<li>GPU models and configuration: Maxwell Titan X</li>\n<li>GCC version (if compiling from source): 8.1.0</li>\n</ul>", "body_text": "Issue description\nCannot backward twice through distribution (see example below).\nCode example\nIt is ok to backward through two different loss:\nt = torch.tensor(1.0, requires_grad=True)\nl1 = 2 * t\nl2 = 3 * t\n\nl1.backward() # ok\nl2.backward() # ok\n\nHowever, it is not OK to backward through two different loss through distribution:\np = torch.tensor(0.2, requires_grad=True)\npi = Bernoulli(p)\n\nl1 = 2 * pi.log_prob(torch.tensor(1.0)) \nl2 = 3 * pi.log_prob(torch.tensor(2.0))\nl1.backward() # ok\nl2.backward() # Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n\nWhat is the reason behind this?\nSystem Info\n\nPyTorch or Caffe2: Pytorch\nHow you installed PyTorch (conda, pip, source): pip\nOS: Arch Linux\nPyTorch version: 0.4.1.post2\nPython version: 3.7.0\nCUDA/cuDNN version: 9.0\nGPU models and configuration: Maxwell Titan X\nGCC version (if compiling from source): 8.1.0", "body": "## Issue description\r\n\r\nCannot backward twice through distribution (see example below).\r\n\r\n## Code example\r\n\r\nIt is ok to backward through two different loss:\r\n\r\n```\r\nt = torch.tensor(1.0, requires_grad=True)\r\nl1 = 2 * t\r\nl2 = 3 * t\r\n\r\nl1.backward() # ok\r\nl2.backward() # ok\r\n```\r\n\r\nHowever, it is not OK to backward through two different loss through distribution:\r\n\r\n```\r\np = torch.tensor(0.2, requires_grad=True)\r\npi = Bernoulli(p)\r\n\r\nl1 = 2 * pi.log_prob(torch.tensor(1.0)) \r\nl2 = 3 * pi.log_prob(torch.tensor(2.0))\r\nl1.backward() # ok\r\nl2.backward() # Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\r\n```\r\n\r\nWhat is the reason behind this? \r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: Pytorch\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- OS: Arch Linux\r\n- PyTorch version: 0.4.1.post2\r\n- Python version: 3.7.0\r\n- CUDA/cuDNN version: 9.0\r\n- GPU models and configuration: Maxwell Titan X\r\n- GCC version (if compiling from source): 8.1.0"}