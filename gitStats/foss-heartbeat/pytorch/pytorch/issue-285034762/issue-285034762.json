{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4402", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4402/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4402/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4402/events", "html_url": "https://github.com/pytorch/pytorch/issues/4402", "id": 285034762, "node_id": "MDU6SXNzdWUyODUwMzQ3NjI=", "number": 4402, "title": "Some computational efficiency improvements in derivatives", "user": {"login": "vishwakftw", "id": 23639302, "node_id": "MDQ6VXNlcjIzNjM5MzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/23639302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishwakftw", "html_url": "https://github.com/vishwakftw", "followers_url": "https://api.github.com/users/vishwakftw/followers", "following_url": "https://api.github.com/users/vishwakftw/following{/other_user}", "gists_url": "https://api.github.com/users/vishwakftw/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishwakftw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishwakftw/subscriptions", "organizations_url": "https://api.github.com/users/vishwakftw/orgs", "repos_url": "https://api.github.com/users/vishwakftw/repos", "events_url": "https://api.github.com/users/vishwakftw/events{/privacy}", "received_events_url": "https://api.github.com/users/vishwakftw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-12-29T05:06:56Z", "updated_at": "2018-01-04T13:45:10Z", "closed_at": "2018-01-04T13:45:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I was going through the <code>derivatives.yaml</code> for the derivative of existing functions in <code>torch</code>, and feel like some of them are a bit inefficient. Some of them are listed below:</p>\n<ul>\n<li><code>reciprocal</code> : From <code>grad / -(self * self)</code> to <code>- grad * result * result</code></li>\n<li><code>sqrt</code>: From <code>grad * self.pow(-0.5) / 2</code> to <code>grad * self.rsqrt() * 0.5</code> or <code>grad / (2 * result)</code></li>\n<li><code>acos</code>: From <code>grad * -((-self * self + 1).sqrt().reciprocal())</code> to <code>grad * -(self.rsqrt(self * self + 1))</code> and similarly for <code>asin</code></li>\n<li><code>div(Tensor self, Tensor other)</code>: Modify <code>other</code>'s derivative from <code>-grad * self / (other * other)</code> to <code>-grad * result / other</code></li>\n<li><code>tan</code>: From <code>grad / self.cos().pow(2)</code> to <code>grad * (1 + result.pow(2))</code></li>\n<li><code>atan</code>: From <code>grad * (self * self + 1).reciprocal()</code> to <code>grad / (self * self + 1)</code></li>\n<li><code>std</code>: From <code>grad * (2 * result).reciprocal()</code> to <code>grad / (2 * result)</code></li>\n<li><code>addcdiv</code>: From <code>-grad * value * tensor1 / (tensor2 * tensor2)</code> to <code>grad * (self - result) / tensor2</code></li>\n</ul>\n<p>Here I am assuming that a divide operation is costlier than a multiply operation.</p>\n<p>Let me know your views about these ops, and I will issue a PR if required.</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>reciprocal</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>sqrt</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>acos</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>asin</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> <code>div</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>tan</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>atan</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> <code>std</code></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> <code>addcdiv</code></li>\n</ul>", "body_text": "I was going through the derivatives.yaml for the derivative of existing functions in torch, and feel like some of them are a bit inefficient. Some of them are listed below:\n\nreciprocal : From grad / -(self * self) to - grad * result * result\nsqrt: From grad * self.pow(-0.5) / 2 to grad * self.rsqrt() * 0.5 or grad / (2 * result)\nacos: From grad * -((-self * self + 1).sqrt().reciprocal()) to grad * -(self.rsqrt(self * self + 1)) and similarly for asin\ndiv(Tensor self, Tensor other): Modify other's derivative from -grad * self / (other * other) to -grad * result / other\ntan: From grad / self.cos().pow(2) to grad * (1 + result.pow(2))\natan: From grad * (self * self + 1).reciprocal() to grad / (self * self + 1)\nstd: From grad * (2 * result).reciprocal() to grad / (2 * result)\naddcdiv: From -grad * value * tensor1 / (tensor2 * tensor2) to grad * (self - result) / tensor2\n\nHere I am assuming that a divide operation is costlier than a multiply operation.\nLet me know your views about these ops, and I will issue a PR if required.\n\n reciprocal\n sqrt\n acos\n asin\n div\n tan\n atan\n std\n addcdiv", "body": "I was going through the `derivatives.yaml` for the derivative of existing functions in `torch`, and feel like some of them are a bit inefficient. Some of them are listed below:\r\n\r\n- `reciprocal` : From `grad / -(self * self)` to `- grad * result * result`\r\n- `sqrt`: From `grad * self.pow(-0.5) / 2` to `grad * self.rsqrt() * 0.5` or `grad / (2 * result)`\r\n- `acos`: From `grad * -((-self * self + 1).sqrt().reciprocal())` to `grad * -(self.rsqrt(self * self + 1))` and similarly for `asin`\r\n- `div(Tensor self, Tensor other)`: Modify `other`'s derivative from `-grad * self / (other * other)` to `-grad * result / other`\r\n- `tan`: From `grad / self.cos().pow(2)` to `grad * (1 + result.pow(2))`\r\n- `atan`: From `grad * (self * self + 1).reciprocal()` to `grad / (self * self + 1)`\r\n- `std`: From `grad * (2 * result).reciprocal()` to `grad / (2 * result)`\r\n- `addcdiv`: From `-grad * value * tensor1 / (tensor2 * tensor2)` to `grad * (self - result) / tensor2`\r\n\r\nHere I am assuming that a divide operation is costlier than a multiply operation.\r\n\r\nLet me know your views about these ops, and I will issue a PR if required. \r\n\r\n- [x] `reciprocal`\r\n- [x] `sqrt`\r\n- [x] `acos`\r\n- [x] `asin`\r\n- [ ] `div`\r\n- [x] `tan`\r\n- [x] `atan`\r\n- [x] `std`\r\n- [ ] `addcdiv`\r\n  "}