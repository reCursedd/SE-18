{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1011", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1011/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1011/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1011/events", "html_url": "https://github.com/pytorch/pytorch/issues/1011", "id": 214613477, "node_id": "MDU6SXNzdWUyMTQ2MTM0Nzc=", "number": 1011, "title": "Masked copy returns a broken gradient for one of the arguments", "user": {"login": "Smerity", "id": 32325, "node_id": "MDQ6VXNlcjMyMzI1", "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Smerity", "html_url": "https://github.com/Smerity", "followers_url": "https://api.github.com/users/Smerity/followers", "following_url": "https://api.github.com/users/Smerity/following{/other_user}", "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}", "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions", "organizations_url": "https://api.github.com/users/Smerity/orgs", "repos_url": "https://api.github.com/users/Smerity/repos", "events_url": "https://api.github.com/users/Smerity/events{/privacy}", "received_events_url": "https://api.github.com/users/Smerity/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-16T06:51:13Z", "updated_at": "2017-03-19T23:01:10Z", "closed_at": "2017-03-19T01:28:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When using <code>masked_copy</code> the variable that is copied from is not given a correct (full sized) gradient.</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\n\nx = Variable(torch.rand(4), requires_grad=True)\ny = Variable(torch.ones(4), requires_grad=True)\nm = Variable(torch.ByteTensor([1, 1, 0, 1]))\n\nz = x.masked_copy(m, 2 * y)\n\nprint('x', x)\nprint('y', y)\nprint('z', z)\n\nloss = z.sum()\nloss.backward()\nprint('xg', x.grad)\nprint('yg', y.grad)\n\ny.data.add_(y.grad.data)\n</code></pre>\n<p>The above code results in <code>RuntimeError: inconsistent tensor size at /Users/soumith/code/pytorch-builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:827</code> as <code>y</code> is a torch.FloatTensor of size 4 and <code>y.grad</code> is a torch.FloatTensor of size 3.</p>\n<p>This appears to be an issue with <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/16a133ed9abfc8d6c0c574586451c4e600a6c2a6/torch/autograd/_functions/tensor.py#L403\">pytorch/torch/autograd/_functions/tensor.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 403\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/16a133ed9abfc8d6c0c574586451c4e600a6c2a6\">16a133e</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L403\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"403\"></td>\n          <td id=\"LC403\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> grad_tensor2 <span class=\"pl-k\">=</span> grad_output.masked_select(mask) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n which specifically uses <code>grad_tensor2 = grad_output.masked_select(mask) </code> which returns only the number of elements in the mask that are active.</p>\n<p>If we change that line to <code>grad_tensor2 = grad_output.clone().masked_fill_(mask.eq(0), 0)</code>, the code works.</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\n\nclass MaskedCopy(torch.autograd.function.InplaceFunction):\n\n    def forward(self, tensor1, mask, tensor2):\n        assert not self.needs_input_grad[1], \"MaskedCopy can't differentiate \" \\\n            \"the mask\"\n        if not self.inplace:\n            tensor1 = tensor1.clone()\n        else:\n            self.mark_dirty(tensor1)\n        self.save_for_backward(mask)\n        return tensor1.masked_copy_(mask, tensor2)\n\n    def backward(self, grad_output):\n        mask, = self.saved_tensors\n        grad_tensor1 = grad_tensor2 = None\n        if self.needs_input_grad[0]:\n            grad_tensor1 = grad_output.clone().masked_fill_(mask, 0)\n        if self.needs_input_grad[2]:\n            grad_tensor2 = grad_output.clone().masked_fill_(mask.eq(0), 0)\n        return grad_tensor1, None, grad_tensor2\n\nx = Variable(torch.rand(4), requires_grad=True)\ny = Variable(torch.ones(4), requires_grad=True)\nm = Variable(torch.ByteTensor([1, 1, 0, 1]))\n\nz = MaskedCopy()(x, m, 2 * y)\n\nprint('x', x)\nprint('y', y)\nprint('z', z)\n\nloss = z.sum()\nloss.backward()\nprint('xg', x.grad)\nprint('yg', y.grad)\n\ny.data.add_(y.grad.data)\n</code></pre>\n<p>Was there a reason that the gradient for the second argument, <code>y</code>, was supposed to be sparse?</p>\n<p>(nudge <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> as he was the last one to touch that line :))</p>", "body_text": "When using masked_copy the variable that is copied from is not given a correct (full sized) gradient.\nimport torch\nfrom torch.autograd import Variable\n\nx = Variable(torch.rand(4), requires_grad=True)\ny = Variable(torch.ones(4), requires_grad=True)\nm = Variable(torch.ByteTensor([1, 1, 0, 1]))\n\nz = x.masked_copy(m, 2 * y)\n\nprint('x', x)\nprint('y', y)\nprint('z', z)\n\nloss = z.sum()\nloss.backward()\nprint('xg', x.grad)\nprint('yg', y.grad)\n\ny.data.add_(y.grad.data)\n\nThe above code results in RuntimeError: inconsistent tensor size at /Users/soumith/code/pytorch-builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:827 as y is a torch.FloatTensor of size 4 and y.grad is a torch.FloatTensor of size 3.\nThis appears to be an issue with \n  \n    \n      pytorch/torch/autograd/_functions/tensor.py\n    \n    \n         Line 403\n      in\n      16a133e\n    \n    \n    \n    \n\n        \n          \n           grad_tensor2 = grad_output.masked_select(mask) \n        \n    \n  \n\n which specifically uses grad_tensor2 = grad_output.masked_select(mask)  which returns only the number of elements in the mask that are active.\nIf we change that line to grad_tensor2 = grad_output.clone().masked_fill_(mask.eq(0), 0), the code works.\nimport torch\nfrom torch.autograd import Variable\n\nclass MaskedCopy(torch.autograd.function.InplaceFunction):\n\n    def forward(self, tensor1, mask, tensor2):\n        assert not self.needs_input_grad[1], \"MaskedCopy can't differentiate \" \\\n            \"the mask\"\n        if not self.inplace:\n            tensor1 = tensor1.clone()\n        else:\n            self.mark_dirty(tensor1)\n        self.save_for_backward(mask)\n        return tensor1.masked_copy_(mask, tensor2)\n\n    def backward(self, grad_output):\n        mask, = self.saved_tensors\n        grad_tensor1 = grad_tensor2 = None\n        if self.needs_input_grad[0]:\n            grad_tensor1 = grad_output.clone().masked_fill_(mask, 0)\n        if self.needs_input_grad[2]:\n            grad_tensor2 = grad_output.clone().masked_fill_(mask.eq(0), 0)\n        return grad_tensor1, None, grad_tensor2\n\nx = Variable(torch.rand(4), requires_grad=True)\ny = Variable(torch.ones(4), requires_grad=True)\nm = Variable(torch.ByteTensor([1, 1, 0, 1]))\n\nz = MaskedCopy()(x, m, 2 * y)\n\nprint('x', x)\nprint('y', y)\nprint('z', z)\n\nloss = z.sum()\nloss.backward()\nprint('xg', x.grad)\nprint('yg', y.grad)\n\ny.data.add_(y.grad.data)\n\nWas there a reason that the gradient for the second argument, y, was supposed to be sparse?\n(nudge @apaszke as he was the last one to touch that line :))", "body": "When using `masked_copy` the variable that is copied from is not given a correct (full sized) gradient.\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nx = Variable(torch.rand(4), requires_grad=True)\r\ny = Variable(torch.ones(4), requires_grad=True)\r\nm = Variable(torch.ByteTensor([1, 1, 0, 1]))\r\n\r\nz = x.masked_copy(m, 2 * y)\r\n\r\nprint('x', x)\r\nprint('y', y)\r\nprint('z', z)\r\n\r\nloss = z.sum()\r\nloss.backward()\r\nprint('xg', x.grad)\r\nprint('yg', y.grad)\r\n\r\ny.data.add_(y.grad.data)\r\n```\r\n\r\nThe above code results in `RuntimeError: inconsistent tensor size at /Users/soumith/code/pytorch-builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:827` as `y` is a torch.FloatTensor of size 4 and `y.grad` is a torch.FloatTensor of size 3.\r\n\r\nThis appears to be an issue with https://github.com/pytorch/pytorch/blob/16a133ed9abfc8d6c0c574586451c4e600a6c2a6/torch/autograd/_functions/tensor.py#L403 which specifically uses `grad_tensor2 = grad_output.masked_select(mask) ` which returns only the number of elements in the mask that are active.\r\n\r\nIf we change that line to `grad_tensor2 = grad_output.clone().masked_fill_(mask.eq(0), 0)`, the code works.\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nclass MaskedCopy(torch.autograd.function.InplaceFunction):\r\n\r\n    def forward(self, tensor1, mask, tensor2):\r\n        assert not self.needs_input_grad[1], \"MaskedCopy can't differentiate \" \\\r\n            \"the mask\"\r\n        if not self.inplace:\r\n            tensor1 = tensor1.clone()\r\n        else:\r\n            self.mark_dirty(tensor1)\r\n        self.save_for_backward(mask)\r\n        return tensor1.masked_copy_(mask, tensor2)\r\n\r\n    def backward(self, grad_output):\r\n        mask, = self.saved_tensors\r\n        grad_tensor1 = grad_tensor2 = None\r\n        if self.needs_input_grad[0]:\r\n            grad_tensor1 = grad_output.clone().masked_fill_(mask, 0)\r\n        if self.needs_input_grad[2]:\r\n            grad_tensor2 = grad_output.clone().masked_fill_(mask.eq(0), 0)\r\n        return grad_tensor1, None, grad_tensor2\r\n\r\nx = Variable(torch.rand(4), requires_grad=True)\r\ny = Variable(torch.ones(4), requires_grad=True)\r\nm = Variable(torch.ByteTensor([1, 1, 0, 1]))\r\n\r\nz = MaskedCopy()(x, m, 2 * y)\r\n\r\nprint('x', x)\r\nprint('y', y)\r\nprint('z', z)\r\n\r\nloss = z.sum()\r\nloss.backward()\r\nprint('xg', x.grad)\r\nprint('yg', y.grad)\r\n\r\ny.data.add_(y.grad.data)\r\n```\r\n\r\nWas there a reason that the gradient for the second argument, `y`, was supposed to be sparse?\r\n\r\n(nudge @apaszke as he was the last one to touch that line :))"}