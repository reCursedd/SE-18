{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149991028", "pull_request_review_id": 75469712, "id": 149991028, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0OTk5MTAyOA==", "diff_hunk": "@@ -2864,38 +2864,17 @@ def test_cosine_similarity(self):\n         self.assertTrue(gradcheck(lambda x, y: F.cosine_similarity(x, y, dim=-1), (input1, input2)))\n \n     def test_grid_sample(self):\n-        # test known input on CPU\n-        input = Variable(torch.arange(1, 11).view(1, 1, 2, 5))\n-        grid = Variable(torch.Tensor(\n-            [[-1, -0.5, 0, 0.2, 1],\n-             [-1, -0.333, 0, 0.5, 1],\n-             [-1, -0.5, 0, 0.3333, 1],\n-             [-1, -0.2, 0, 0.2, 1]]).view(1, 2, 5, 2))\n-        output = F.grid_sample(input, grid)\n-        groundtruth = torch.Tensor(\n-            [[2.2500, 6.0000000000, 5.0000, 4.8340, 9.0000],\n-             [2.2500, 6.333250045, 5.0000, 5.1000, 8.4000]]).view(1, 1, 2, 5)\n-        self.assertEqual(output.data, groundtruth)\n+        def test_cpu_against_cuda(N, C, H, W, padding_mode):", "path": "test/test_nn.py", "position": 16, "original_position": 16, "commit_id": "3cac879c7dab4925dc9d8bf4419ea2616cd60d3f", "original_commit_id": "3cac879c7dab4925dc9d8bf4419ea2616cd60d3f", "user": {"login": "alykhantejani", "id": 687194, "node_id": "MDQ6VXNlcjY4NzE5NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/687194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alykhantejani", "html_url": "https://github.com/alykhantejani", "followers_url": "https://api.github.com/users/alykhantejani/followers", "following_url": "https://api.github.com/users/alykhantejani/following{/other_user}", "gists_url": "https://api.github.com/users/alykhantejani/gists{/gist_id}", "starred_url": "https://api.github.com/users/alykhantejani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alykhantejani/subscriptions", "organizations_url": "https://api.github.com/users/alykhantejani/orgs", "repos_url": "https://api.github.com/users/alykhantejani/repos", "events_url": "https://api.github.com/users/alykhantejani/events{/privacy}", "received_events_url": "https://api.github.com/users/alykhantejani/received_events", "type": "User", "site_admin": false}, "body": "nit:Might be cleaner to just move the cuda tests into their own function i.e \r\n```python\r\n@unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\r\ndef test_grid_sample_cuda(self):\r\n    def test_cpu_against_cuda(N, C, H, W, padding_mode):\r\n            def test_shape(N, C, IH, IW, H, W, padding_mode):\r\n                ....\r\n    for padding_mode in ['zeros', 'border']:\r\n        N = random.randint(1, 8)\r\n        C = random.randint(1, 8)\r\n        H = random.randint(1, 8)\r\n        W = random.randint(1, 8)\r\n        test_cpu_against_cuda(N, C, H, W, padding_mode)\r\n```\r\n\r\nBut as the version in master bundles them into one test I guess its fine this way too", "created_at": "2017-11-09T15:19:30Z", "updated_at": "2018-11-23T15:36:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/3599#discussion_r149991028", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3599", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149991028"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3599#discussion_r149991028"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3599"}}, "body_html": "<p>nit:Might be cleaner to just move the cuda tests into their own function i.e</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@unittest.skipIf</span>(<span class=\"pl-k\">not</span> <span class=\"pl-c1\">TEST_CUDA</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CUDA unavailable<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_grid_sample_cuda</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_cpu_against_cuda</span>(<span class=\"pl-smi\">N</span>, <span class=\"pl-smi\">C</span>, <span class=\"pl-smi\">H</span>, <span class=\"pl-smi\">W</span>, <span class=\"pl-smi\">padding_mode</span>):\n            <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_shape</span>(<span class=\"pl-smi\">N</span>, <span class=\"pl-smi\">C</span>, <span class=\"pl-smi\">IH</span>, <span class=\"pl-smi\">IW</span>, <span class=\"pl-smi\">H</span>, <span class=\"pl-smi\">W</span>, <span class=\"pl-smi\">padding_mode</span>):\n                <span class=\"pl-c1\">...</span>.\n    <span class=\"pl-k\">for</span> padding_mode <span class=\"pl-k\">in</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>zeros<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>border<span class=\"pl-pds\">'</span></span>]:\n        N <span class=\"pl-k\">=</span> random.randint(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">8</span>)\n        C <span class=\"pl-k\">=</span> random.randint(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">8</span>)\n        H <span class=\"pl-k\">=</span> random.randint(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">8</span>)\n        W <span class=\"pl-k\">=</span> random.randint(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">8</span>)\n        test_cpu_against_cuda(N, C, H, W, padding_mode)</pre></div>\n<p>But as the version in master bundles them into one test I guess its fine this way too</p>", "body_text": "nit:Might be cleaner to just move the cuda tests into their own function i.e\n@unittest.skipIf(not TEST_CUDA, \"CUDA unavailable\")\ndef test_grid_sample_cuda(self):\n    def test_cpu_against_cuda(N, C, H, W, padding_mode):\n            def test_shape(N, C, IH, IW, H, W, padding_mode):\n                ....\n    for padding_mode in ['zeros', 'border']:\n        N = random.randint(1, 8)\n        C = random.randint(1, 8)\n        H = random.randint(1, 8)\n        W = random.randint(1, 8)\n        test_cpu_against_cuda(N, C, H, W, padding_mode)\nBut as the version in master bundles them into one test I guess its fine this way too"}