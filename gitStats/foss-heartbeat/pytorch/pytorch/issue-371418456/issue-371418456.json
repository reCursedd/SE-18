{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12812", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12812/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12812/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12812/events", "html_url": "https://github.com/pytorch/pytorch/issues/12812", "id": 371418456, "node_id": "MDU6SXNzdWUzNzE0MTg0NTY=", "number": 12812, "title": "Pytorch BatchNorm2D Unstable", "user": {"login": "todpole3", "id": 4227871, "node_id": "MDQ6VXNlcjQyMjc4NzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4227871?v=4", "gravatar_id": "", "url": "https://api.github.com/users/todpole3", "html_url": "https://github.com/todpole3", "followers_url": "https://api.github.com/users/todpole3/followers", "following_url": "https://api.github.com/users/todpole3/following{/other_user}", "gists_url": "https://api.github.com/users/todpole3/gists{/gist_id}", "starred_url": "https://api.github.com/users/todpole3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/todpole3/subscriptions", "organizations_url": "https://api.github.com/users/todpole3/orgs", "repos_url": "https://api.github.com/users/todpole3/repos", "events_url": "https://api.github.com/users/todpole3/events{/privacy}", "received_events_url": "https://api.github.com/users/todpole3/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-18T08:34:41Z", "updated_at": "2018-10-19T05:49:07Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>I was tuning the following customed nn module:</p>\n<pre lang=\"class\" data-meta=\"ConvE(nn.Module):\"><code>    def __init__(self, args, num_entities):\n        super(ConvE, self).__init__()\n        self.entity_dim = args.entity_dim\n        self.relation_dim = args.relation_dim\n        assert(args.emb_2D_d1 * args.emb_2D_d2 == args.entity_dim)\n        assert(args.emb_2D_d1 * args.emb_2D_d2 == args.relation_dim)\n        self.emb_2D_d1 = args.emb_2D_d1\n        self.emb_2D_d2 = args.emb_2D_d2\n        self.num_out_channels = args.num_out_channels\n        self.w_d = args.kernel_size\n        self.HiddenDropout = nn.Dropout(args.hidden_dropout_rate)\n        self.FeatureDropout = nn.Dropout(args.feat_dropout_rate)\n\n        # stride = 1, padding = 0, dilation = 1, groups = 1\n        self.conv1 = nn.Conv2d(1, self.num_out_channels, (self.w_d, self.w_d), 1, 0)\n        self.bn0 = nn.BatchNorm2d(1)\n        self.bn1 = nn.BatchNorm2d(self.num_out_channels)\n        self.bn2 = nn.BatchNorm1d(self.entity_dim)\n        self.register_parameter('b', nn.Parameter(torch.zeros(num_entities)))\n        h_out = 2 * self.emb_2D_d1 - self.w_d + 1\n        w_out = self.emb_2D_d2 - self.w_d + 1\n        self.feat_dim = self.num_out_channels * h_out * w_out\n        self.fc = nn.Linear(self.feat_dim, self.entity_dim)\n\n    def forward(self, e1, r, kg):\n        E1 = kg.get_entity_embeddings(e1).view(-1, 1, self.emb_2D_d1, self.emb_2D_d2)\n        R = kg.get_relation_embeddings(r).view(-1, 1, self.emb_2D_d1, self.emb_2D_d2)\n        E2 = kg.get_all_entity_embeddings()\n\n        stacked_inputs = torch.cat([E1, R], 2)\n        stacked_inputs = self.bn0(stacked_inputs)\n\n        X = self.conv1(stacked_inputs)\n        X = self.bn1(X)                            # &lt;= problem line\n        X = F.relu(X)\n        X = self.FeatureDropout(X)\n        X = X.view(-1, self.feat_dim)\n        X = self.fc(X)\n        X = self.HiddenDropout(X)\n        X = self.bn2(X)\n        X = F.relu(X)\n        X = torch.mm(X, E2.transpose(1, 0))\n        X += self.b.expand_as(X)\n\n        S = F.sigmoid(X)\n        return S```\n\nIf I include the \"problem line\" in code, the model performance became very unstable and the loss declines slowly. If I commented out the line, the model performance becomes normal.\n\nThe strange part is that it used to be fine to include this line in my code and this problem caught my attention after I switched to 0.4.0+.\n\nI wonder if it is caused by any changed in the BatchNorm2d operator.\n\nBelow the pink curve is the result obtained w/ the problem line and the orange curve is obtained w/o it.\n![screen shot 2018-10-18 at 1 32 19 am](https://user-images.githubusercontent.com/4227871/47141498-af5e4e80-d275-11e8-8a90-356829ce8768.png)\n\nThe experiment code repo can be found here:\nhttps://github.com/salesforce/MultiHopKG/blob/master/src/emb/fact_network.py#L131\n</code></pre>", "body_text": "I was tuning the following customed nn module:\n    def __init__(self, args, num_entities):\n        super(ConvE, self).__init__()\n        self.entity_dim = args.entity_dim\n        self.relation_dim = args.relation_dim\n        assert(args.emb_2D_d1 * args.emb_2D_d2 == args.entity_dim)\n        assert(args.emb_2D_d1 * args.emb_2D_d2 == args.relation_dim)\n        self.emb_2D_d1 = args.emb_2D_d1\n        self.emb_2D_d2 = args.emb_2D_d2\n        self.num_out_channels = args.num_out_channels\n        self.w_d = args.kernel_size\n        self.HiddenDropout = nn.Dropout(args.hidden_dropout_rate)\n        self.FeatureDropout = nn.Dropout(args.feat_dropout_rate)\n\n        # stride = 1, padding = 0, dilation = 1, groups = 1\n        self.conv1 = nn.Conv2d(1, self.num_out_channels, (self.w_d, self.w_d), 1, 0)\n        self.bn0 = nn.BatchNorm2d(1)\n        self.bn1 = nn.BatchNorm2d(self.num_out_channels)\n        self.bn2 = nn.BatchNorm1d(self.entity_dim)\n        self.register_parameter('b', nn.Parameter(torch.zeros(num_entities)))\n        h_out = 2 * self.emb_2D_d1 - self.w_d + 1\n        w_out = self.emb_2D_d2 - self.w_d + 1\n        self.feat_dim = self.num_out_channels * h_out * w_out\n        self.fc = nn.Linear(self.feat_dim, self.entity_dim)\n\n    def forward(self, e1, r, kg):\n        E1 = kg.get_entity_embeddings(e1).view(-1, 1, self.emb_2D_d1, self.emb_2D_d2)\n        R = kg.get_relation_embeddings(r).view(-1, 1, self.emb_2D_d1, self.emb_2D_d2)\n        E2 = kg.get_all_entity_embeddings()\n\n        stacked_inputs = torch.cat([E1, R], 2)\n        stacked_inputs = self.bn0(stacked_inputs)\n\n        X = self.conv1(stacked_inputs)\n        X = self.bn1(X)                            # <= problem line\n        X = F.relu(X)\n        X = self.FeatureDropout(X)\n        X = X.view(-1, self.feat_dim)\n        X = self.fc(X)\n        X = self.HiddenDropout(X)\n        X = self.bn2(X)\n        X = F.relu(X)\n        X = torch.mm(X, E2.transpose(1, 0))\n        X += self.b.expand_as(X)\n\n        S = F.sigmoid(X)\n        return S```\n\nIf I include the \"problem line\" in code, the model performance became very unstable and the loss declines slowly. If I commented out the line, the model performance becomes normal.\n\nThe strange part is that it used to be fine to include this line in my code and this problem caught my attention after I switched to 0.4.0+.\n\nI wonder if it is caused by any changed in the BatchNorm2d operator.\n\nBelow the pink curve is the result obtained w/ the problem line and the orange curve is obtained w/o it.\n![screen shot 2018-10-18 at 1 32 19 am](https://user-images.githubusercontent.com/4227871/47141498-af5e4e80-d275-11e8-8a90-356829ce8768.png)\n\nThe experiment code repo can be found here:\nhttps://github.com/salesforce/MultiHopKG/blob/master/src/emb/fact_network.py#L131", "body": "I was tuning the following customed nn module:\r\n\r\n```class ConvE(nn.Module):\r\n    def __init__(self, args, num_entities):\r\n        super(ConvE, self).__init__()\r\n        self.entity_dim = args.entity_dim\r\n        self.relation_dim = args.relation_dim\r\n        assert(args.emb_2D_d1 * args.emb_2D_d2 == args.entity_dim)\r\n        assert(args.emb_2D_d1 * args.emb_2D_d2 == args.relation_dim)\r\n        self.emb_2D_d1 = args.emb_2D_d1\r\n        self.emb_2D_d2 = args.emb_2D_d2\r\n        self.num_out_channels = args.num_out_channels\r\n        self.w_d = args.kernel_size\r\n        self.HiddenDropout = nn.Dropout(args.hidden_dropout_rate)\r\n        self.FeatureDropout = nn.Dropout(args.feat_dropout_rate)\r\n\r\n        # stride = 1, padding = 0, dilation = 1, groups = 1\r\n        self.conv1 = nn.Conv2d(1, self.num_out_channels, (self.w_d, self.w_d), 1, 0)\r\n        self.bn0 = nn.BatchNorm2d(1)\r\n        self.bn1 = nn.BatchNorm2d(self.num_out_channels)\r\n        self.bn2 = nn.BatchNorm1d(self.entity_dim)\r\n        self.register_parameter('b', nn.Parameter(torch.zeros(num_entities)))\r\n        h_out = 2 * self.emb_2D_d1 - self.w_d + 1\r\n        w_out = self.emb_2D_d2 - self.w_d + 1\r\n        self.feat_dim = self.num_out_channels * h_out * w_out\r\n        self.fc = nn.Linear(self.feat_dim, self.entity_dim)\r\n\r\n    def forward(self, e1, r, kg):\r\n        E1 = kg.get_entity_embeddings(e1).view(-1, 1, self.emb_2D_d1, self.emb_2D_d2)\r\n        R = kg.get_relation_embeddings(r).view(-1, 1, self.emb_2D_d1, self.emb_2D_d2)\r\n        E2 = kg.get_all_entity_embeddings()\r\n\r\n        stacked_inputs = torch.cat([E1, R], 2)\r\n        stacked_inputs = self.bn0(stacked_inputs)\r\n\r\n        X = self.conv1(stacked_inputs)\r\n        X = self.bn1(X)                            # <= problem line\r\n        X = F.relu(X)\r\n        X = self.FeatureDropout(X)\r\n        X = X.view(-1, self.feat_dim)\r\n        X = self.fc(X)\r\n        X = self.HiddenDropout(X)\r\n        X = self.bn2(X)\r\n        X = F.relu(X)\r\n        X = torch.mm(X, E2.transpose(1, 0))\r\n        X += self.b.expand_as(X)\r\n\r\n        S = F.sigmoid(X)\r\n        return S```\r\n\r\nIf I include the \"problem line\" in code, the model performance became very unstable and the loss declines slowly. If I commented out the line, the model performance becomes normal.\r\n\r\nThe strange part is that it used to be fine to include this line in my code and this problem caught my attention after I switched to 0.4.0+.\r\n\r\nI wonder if it is caused by any changed in the BatchNorm2d operator.\r\n\r\nBelow the pink curve is the result obtained w/ the problem line and the orange curve is obtained w/o it.\r\n![screen shot 2018-10-18 at 1 32 19 am](https://user-images.githubusercontent.com/4227871/47141498-af5e4e80-d275-11e8-8a90-356829ce8768.png)\r\n\r\nThe experiment code repo can be found here:\r\nhttps://github.com/salesforce/MultiHopKG/blob/master/src/emb/fact_network.py#L131\r\n"}