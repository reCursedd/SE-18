{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12230", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12230/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12230/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12230/events", "html_url": "https://github.com/pytorch/pytorch/issues/12230", "id": 365607801, "node_id": "MDU6SXNzdWUzNjU2MDc4MDE=", "number": 12230, "title": "Bug in masked_fill_ for non contiguous tensors", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2018-10-01T19:27:53Z", "updated_at": "2018-10-16T16:34:32Z", "closed_at": "2018-10-16T16:34:32Z", "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>The in-place <code>masked_fill_</code> seems to give incorrect results on PyTorch master for non-contiguous tensors.</p>\n<h2>To Reproduce</h2>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">import</span> torch\n\nIn [<span class=\"pl-c1\">2</span>]: torch.<span class=\"pl-c1\">__version__</span>\nOut[<span class=\"pl-c1\">2</span>]: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>1.0.0a0+572132f<span class=\"pl-pds\">'</span></span>\n\nIn [<span class=\"pl-c1\">3</span>]: mask <span class=\"pl-k\">=</span> torch.tensor([[[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>]], [[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>]]], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.uint8)\n\nIn [<span class=\"pl-c1\">4</span>]: mask.shape\nOut[<span class=\"pl-c1\">4</span>]: torch.Size([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n\nIn [<span class=\"pl-c1\">5</span>]: x <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\n\nIn [<span class=\"pl-c1\">6</span>]: x, mask <span class=\"pl-k\">=</span> torch.broadcast_tensors(x, mask)\n\nIn [<span class=\"pl-c1\">7</span>]: mask.shape\nOut[<span class=\"pl-c1\">7</span>]: torch.Size([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>])\n\nIn [<span class=\"pl-c1\">8</span>]: x.is_contiguous()\nOut[<span class=\"pl-c1\">8</span>]: <span class=\"pl-c1\">False</span>\n\nIn [<span class=\"pl-c1\">9</span>]: x.masked_fill(mask, <span class=\"pl-c1\">0</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> This should be the correct behavior</span>\nOut[<span class=\"pl-c1\">9</span>]: \ntensor([[[ <span class=\"pl-c1\">0.0000</span>,  <span class=\"pl-c1\">0.9482</span>],\n         [ <span class=\"pl-c1\">0.0000</span>,  <span class=\"pl-c1\">0.0719</span>]],\n\n        [[ <span class=\"pl-c1\">0.6267</span>,  <span class=\"pl-c1\">0.0000</span>],\n         [<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6296</span>,  <span class=\"pl-c1\">0.0000</span>]]])\n\nIn [<span class=\"pl-c1\">10</span>]: x.masked_fill_(mask, <span class=\"pl-c1\">0</span>)   <span class=\"pl-c\"><span class=\"pl-c\">#</span> Incorrect result</span>\nOut[<span class=\"pl-c1\">10</span>]: \ntensor([[[<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.9482</span>],\n         [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0719</span>]],\n\n        [[<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.9482</span>],\n         [<span class=\"pl-c1\">0.0000</span>, <span class=\"pl-c1\">0.0719</span>]]])</pre></div>\n<p>We can retrieve the correct behavior by calling <code>.contiguous()</code> on the expanded tensor before doing a <code>masked_fill_</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">11</span>]: y <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\n\nIn [<span class=\"pl-c1\">12</span>]: y, mask <span class=\"pl-k\">=</span> torch.broadcast_tensors(y, mask)\n\nIn [<span class=\"pl-c1\">13</span>]: y <span class=\"pl-k\">=</span> y.contiguous()\n\nIn [<span class=\"pl-c1\">14</span>]: y.masked_fill_(mask, <span class=\"pl-c1\">0</span>)\nOut[<span class=\"pl-c1\">14</span>]: \ntensor([[[ <span class=\"pl-c1\">0.0000</span>,  <span class=\"pl-c1\">0.0579</span>],\n         [ <span class=\"pl-c1\">0.0000</span>,  <span class=\"pl-c1\">0.4561</span>]],\n\n        [[<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.0155</span>,  <span class=\"pl-c1\">0.0000</span>],\n         [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1.7522</span>,  <span class=\"pl-c1\">0.0000</span>]]])</pre></div>\n<h2>Environment</h2>\n<pre><code>  $ python collect_env.py \nCollecting environment information...\nPyTorch version: 1.0.0a0+572132f\nIs debug build: No\nCUDA used to build PyTorch: 9.1.85\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.12.2\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration: \nGPU 0: GeForce GTX 1080\nGPU 1: GeForce GTX 1080\n\nNvidia driver version: 390.30\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\n\nVersions of relevant libraries:\n[pip] numpy (1.15.1)\n[pip] torch (1.0.0a0+572132f)\n[pip] torchfile (0.1.0)\n[pip] torchvision (0.2.1)\n[conda] magma-cuda91              2.3.0                         1    pytorch\n[conda] torch                     1.0.0a0+572132f           &lt;pip&gt;\n[conda] torchfile                 0.1.0                     &lt;pip&gt;\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n</code></pre>", "body_text": "\ud83d\udc1b Bug\nThe in-place masked_fill_ seems to give incorrect results on PyTorch master for non-contiguous tensors.\nTo Reproduce\nIn [1]: import torch\n\nIn [2]: torch.__version__\nOut[2]: '1.0.0a0+572132f'\n\nIn [3]: mask = torch.tensor([[[1, 0]], [[0, 1]]], dtype=torch.uint8)\n\nIn [4]: mask.shape\nOut[4]: torch.Size([2, 1, 2])\n\nIn [5]: x = torch.randn(2, 2)\n\nIn [6]: x, mask = torch.broadcast_tensors(x, mask)\n\nIn [7]: mask.shape\nOut[7]: torch.Size([2, 2, 2])\n\nIn [8]: x.is_contiguous()\nOut[8]: False\n\nIn [9]: x.masked_fill(mask, 0)  # This should be the correct behavior\nOut[9]: \ntensor([[[ 0.0000,  0.9482],\n         [ 0.0000,  0.0719]],\n\n        [[ 0.6267,  0.0000],\n         [-0.6296,  0.0000]]])\n\nIn [10]: x.masked_fill_(mask, 0)   # Incorrect result\nOut[10]: \ntensor([[[0.0000, 0.9482],\n         [0.0000, 0.0719]],\n\n        [[0.0000, 0.9482],\n         [0.0000, 0.0719]]])\nWe can retrieve the correct behavior by calling .contiguous() on the expanded tensor before doing a masked_fill_:\nIn [11]: y = torch.randn(2, 2)\n\nIn [12]: y, mask = torch.broadcast_tensors(y, mask)\n\nIn [13]: y = y.contiguous()\n\nIn [14]: y.masked_fill_(mask, 0)\nOut[14]: \ntensor([[[ 0.0000,  0.0579],\n         [ 0.0000,  0.4561]],\n\n        [[-0.0155,  0.0000],\n         [-1.7522,  0.0000]]])\nEnvironment\n  $ python collect_env.py \nCollecting environment information...\nPyTorch version: 1.0.0a0+572132f\nIs debug build: No\nCUDA used to build PyTorch: 9.1.85\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.12.2\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration: \nGPU 0: GeForce GTX 1080\nGPU 1: GeForce GTX 1080\n\nNvidia driver version: 390.30\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\n\nVersions of relevant libraries:\n[pip] numpy (1.15.1)\n[pip] torch (1.0.0a0+572132f)\n[pip] torchfile (0.1.0)\n[pip] torchvision (0.2.1)\n[conda] magma-cuda91              2.3.0                         1    pytorch\n[conda] torch                     1.0.0a0+572132f           <pip>\n[conda] torchfile                 0.1.0                     <pip>\n[conda] torchvision               0.2.1                     <pip>", "body": "## \ud83d\udc1b Bug\r\n\r\nThe in-place `masked_fill_` seems to give incorrect results on PyTorch master for non-contiguous tensors.\r\n\r\n## To Reproduce\r\n\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: torch.__version__\r\nOut[2]: '1.0.0a0+572132f'\r\n\r\nIn [3]: mask = torch.tensor([[[1, 0]], [[0, 1]]], dtype=torch.uint8)\r\n\r\nIn [4]: mask.shape\r\nOut[4]: torch.Size([2, 1, 2])\r\n\r\nIn [5]: x = torch.randn(2, 2)\r\n\r\nIn [6]: x, mask = torch.broadcast_tensors(x, mask)\r\n\r\nIn [7]: mask.shape\r\nOut[7]: torch.Size([2, 2, 2])\r\n\r\nIn [8]: x.is_contiguous()\r\nOut[8]: False\r\n\r\nIn [9]: x.masked_fill(mask, 0)  # This should be the correct behavior\r\nOut[9]: \r\ntensor([[[ 0.0000,  0.9482],\r\n         [ 0.0000,  0.0719]],\r\n\r\n        [[ 0.6267,  0.0000],\r\n         [-0.6296,  0.0000]]])\r\n\r\nIn [10]: x.masked_fill_(mask, 0)   # Incorrect result\r\nOut[10]: \r\ntensor([[[0.0000, 0.9482],\r\n         [0.0000, 0.0719]],\r\n\r\n        [[0.0000, 0.9482],\r\n         [0.0000, 0.0719]]])\r\n```\r\n\r\nWe can retrieve the correct behavior by calling `.contiguous()` on the expanded tensor before doing a `masked_fill_`:\r\n\r\n```python\r\nIn [11]: y = torch.randn(2, 2)\r\n\r\nIn [12]: y, mask = torch.broadcast_tensors(y, mask)\r\n\r\nIn [13]: y = y.contiguous()\r\n\r\nIn [14]: y.masked_fill_(mask, 0)\r\nOut[14]: \r\ntensor([[[ 0.0000,  0.0579],\r\n         [ 0.0000,  0.4561]],\r\n\r\n        [[-0.0155,  0.0000],\r\n         [-1.7522,  0.0000]]])\r\n```\r\n\r\n## Environment\r\n\r\n```\r\n  $ python collect_env.py \r\nCollecting environment information...\r\nPyTorch version: 1.0.0a0+572132f\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.1.85\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080\r\nGPU 1: GeForce GTX 1080\r\n\r\nNvidia driver version: 390.30\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.3\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.1)\r\n[pip] torch (1.0.0a0+572132f)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] magma-cuda91              2.3.0                         1    pytorch\r\n[conda] torch                     1.0.0a0+572132f           <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n\r\n"}