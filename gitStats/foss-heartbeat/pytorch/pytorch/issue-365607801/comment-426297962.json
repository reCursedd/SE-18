{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/426297962", "html_url": "https://github.com/pytorch/pytorch/issues/12230#issuecomment-426297962", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12230", "id": 426297962, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjI5Nzk2Mg==", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-02T14:35:57Z", "updated_at": "2018-10-02T14:35:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> : I mistakenly thought that this was some regression in PyTorch master, because our unit test only fails on master; but I verified that there has been no change in behavior. The reason why our test is failing now is because <code>expanded_tensor * scalar</code> which previously copied the data into a new contiguous tensor, now copies it into a non contiguous tensor on linux. See below.</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">1</span>]: <span class=\"pl-k\">import</span> torch\n\nIn [<span class=\"pl-c1\">2</span>]: t <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\n\nIn [<span class=\"pl-c1\">3</span>]: t <span class=\"pl-k\">=</span> t.expand([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>])\n\nIn [<span class=\"pl-c1\">4</span>]: t.is_contiguous()\nOut[<span class=\"pl-c1\">4</span>]: <span class=\"pl-c1\">False</span>\n\nIn [<span class=\"pl-c1\">5</span>]: u <span class=\"pl-k\">=</span> t <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2.0</span>\n\nIn [<span class=\"pl-c1\">6</span>]: u.is_contiguous()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> = True on release </span>\nOut[<span class=\"pl-c1\">6</span>]: <span class=\"pl-c1\">False</span></pre></div>\n<p>Note that this in itself doesn't violate any expectations, other than the non intuitive results on expanded tensor operations that might follow. To get around this we are calling <code>.contiguous</code> after the scalar multiplication. Please feel free to close this issue (or follow up), as you might see fit!</p>", "body_text": "@fmassa : I mistakenly thought that this was some regression in PyTorch master, because our unit test only fails on master; but I verified that there has been no change in behavior. The reason why our test is failing now is because expanded_tensor * scalar which previously copied the data into a new contiguous tensor, now copies it into a non contiguous tensor on linux. See below.\nIn [1]: import torch\n\nIn [2]: t = torch.randn(2, 2)\n\nIn [3]: t = t.expand([2, 2, 2])\n\nIn [4]: t.is_contiguous()\nOut[4]: False\n\nIn [5]: u = t * 2.0\n\nIn [6]: u.is_contiguous()  # = True on release \nOut[6]: False\nNote that this in itself doesn't violate any expectations, other than the non intuitive results on expanded tensor operations that might follow. To get around this we are calling .contiguous after the scalar multiplication. Please feel free to close this issue (or follow up), as you might see fit!", "body": "@fmassa : I mistakenly thought that this was some regression in PyTorch master, because our unit test only fails on master; but I verified that there has been no change in behavior. The reason why our test is failing now is because `expanded_tensor * scalar` which previously copied the data into a new contiguous tensor, now copies it into a non contiguous tensor on linux. See below. \r\n\r\n```python\r\nIn [1]: import torch\r\n\r\nIn [2]: t = torch.randn(2, 2)\r\n\r\nIn [3]: t = t.expand([2, 2, 2])\r\n\r\nIn [4]: t.is_contiguous()\r\nOut[4]: False\r\n\r\nIn [5]: u = t * 2.0\r\n\r\nIn [6]: u.is_contiguous()  # = True on release \r\nOut[6]: False\r\n```\r\n\r\nNote that this in itself doesn't violate any expectations, other than the non intuitive results on expanded tensor operations that might follow. To get around this we are calling `.contiguous` after the scalar multiplication. Please feel free to close this issue (or follow up), as you might see fit! "}