{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193890159", "pull_request_review_id": 126958108, "id": 193890159, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5Mzg5MDE1OQ==", "diff_hunk": "@@ -0,0 +1,186 @@\n+import math\n+import multiprocessing\n+import sys\n+import tempfile\n+import unittest\n+from functools import wraps\n+\n+import torch\n+import torch.c10d as c10d", "path": "test/test_c10d.py", "position": null, "original_position": 9, "commit_id": "d6b04766ae9a33157480a696af057a709d12bcfe", "original_commit_id": "83bf660bb0e8a043d4cdecc12bac187f2234bd97", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "body": "c10 will also be top level, so I prefer this to also be top level.\r\n\r\nThen when we are at parity we can make torch.distributed use stuff from torch.c10d, and eventually perhaps move it again. I'm not married to any particular location but like to keep it separate from torch.distributed while it's a work in progress.", "created_at": "2018-06-07T21:10:27Z", "updated_at": "2018-11-23T15:45:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/8119#discussion_r193890159", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8119", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193890159"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8119#discussion_r193890159"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8119"}}, "body_html": "<p>c10 will also be top level, so I prefer this to also be top level.</p>\n<p>Then when we are at parity we can make torch.distributed use stuff from torch.c10d, and eventually perhaps move it again. I'm not married to any particular location but like to keep it separate from torch.distributed while it's a work in progress.</p>", "body_text": "c10 will also be top level, so I prefer this to also be top level.\nThen when we are at parity we can make torch.distributed use stuff from torch.c10d, and eventually perhaps move it again. I'm not married to any particular location but like to keep it separate from torch.distributed while it's a work in progress.", "in_reply_to_id": 193674041}