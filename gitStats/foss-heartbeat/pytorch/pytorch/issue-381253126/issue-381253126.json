{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14033", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14033/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14033/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14033/events", "html_url": "https://github.com/pytorch/pytorch/issues/14033", "id": 381253126, "node_id": "MDU6SXNzdWUzODEyNTMxMjY=", "number": 14033, "title": "[Torchscript/C++] C++ model is two times slower than Python model", "user": {"login": "andrewbo29", "id": 2898875, "node_id": "MDQ6VXNlcjI4OTg4NzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/2898875?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewbo29", "html_url": "https://github.com/andrewbo29", "followers_url": "https://api.github.com/users/andrewbo29/followers", "following_url": "https://api.github.com/users/andrewbo29/following{/other_user}", "gists_url": "https://api.github.com/users/andrewbo29/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewbo29/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewbo29/subscriptions", "organizations_url": "https://api.github.com/users/andrewbo29/orgs", "repos_url": "https://api.github.com/users/andrewbo29/repos", "events_url": "https://api.github.com/users/andrewbo29/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewbo29/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-11-15T16:58:14Z", "updated_at": "2018-11-19T18:44:40Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi folks! I'm converting my Pytorch model on C++ via Torchscipt. But when I tested the inference time, it turned out that my C++ model is two times slower than my original Python model.<br>\nWhat could be the reason for this behavior?  Or am I mistaken somewhere?</p>\n<p>Ubuntu 16.04, CUDA: 9.1, cuDNN: 7.1.3<br>\nNet is a Wide ResNet 50-2.</p>\n<p>Converting in Torchscript code:</p>\n<div class=\"highlight highlight-source-python\"><pre>net <span class=\"pl-k\">=</span> MyNet()\nnet.load_state_dict(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model.pth<span class=\"pl-pds\">'</span></span>)\nnet.train(<span class=\"pl-c1\">False</span>)\nexample <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)\n\n<span class=\"pl-k\">with</span> torch.no_grad():\n    traced_script_module <span class=\"pl-k\">=</span> torch.jit.trace(net, example)\n\ntraced_script_module.save(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model.pt<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>Python inference time test code:</p>\n<div class=\"highlight highlight-source-python\"><pre>net <span class=\"pl-k\">=</span> MyNet()\nnet.load_state_dict(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model.pth<span class=\"pl-pds\">'</span></span>)\nnet.train(<span class=\"pl-c1\">False</span>)\nnet.cuda()\n\ntest_number <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1010</span>\nbatch_num <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\nskip_numbers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\nbatch <span class=\"pl-k\">=</span> torch.randn(batch_num, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>).cuda()\n\ntimes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(test_number):\n    start <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">round</span>(time.time() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>))\n    <span class=\"pl-k\">with</span> torch.no_grad():\n        _, __ <span class=\"pl-k\">=</span> net(batch)\n    torch.cuda.synchronize()\n    end <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">round</span>(time.time() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1000</span>))\n    \n    elapsed_mseconds <span class=\"pl-k\">=</span> end <span class=\"pl-k\">-</span> start\n\n    <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> skip_numbers:\n        times <span class=\"pl-k\">+=</span> elapsed_mseconds\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Mean time: <span class=\"pl-c1\">{}</span> msec<span class=\"pl-pds\">'</span></span>.format(times <span class=\"pl-k\">/</span> (test_number <span class=\"pl-k\">-</span> skip_numbers)))</pre></div>\n<p>C++ inference time test code:</p>\n<div class=\"highlight highlight-source-c++\"><pre>#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>torch/torch.h<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>torch/script.h<span class=\"pl-pds\">&gt;</span></span>\n\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>ATen/ATen.h<span class=\"pl-pds\">&gt;</span></span>\n\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>iostream<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>memory<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>chrono<span class=\"pl-pds\">&gt;</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">&lt;</span>ctime<span class=\"pl-pds\">&gt;</span></span>\n\n<span class=\"pl-k\">void</span> <span class=\"pl-en\">time_test</span>(<span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* model_name) {\n    std::shared_ptr&lt;torch::jit::script::Module&gt; module = <span class=\"pl-c1\">torch::jit::load</span>(model_name);\n    module-&gt;<span class=\"pl-c1\">to</span>(at::<span class=\"pl-c1\">kCUDA</span>);\n\n    <span class=\"pl-k\">int</span> batch_num = <span class=\"pl-c1\">8</span>;\n    <span class=\"pl-k\">int</span> test_number = <span class=\"pl-c1\">1010</span>;\n    <span class=\"pl-k\">int</span> skip_numbers = <span class=\"pl-c1\">10</span>;\n\n    std::vector&lt;torch::jit::IValue&gt; batch;\n    batch.<span class=\"pl-c1\">push_back</span>(<span class=\"pl-c1\">torch::randn</span>({batch_num, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>}).<span class=\"pl-c1\">to</span>(at::<span class=\"pl-c1\">kCUDA</span>));\n\n    std::chrono::time_point&lt;std::chrono::system_clock&gt; start, end;\n\n    <span class=\"pl-k\">int</span> times = <span class=\"pl-c1\">0</span>;\n\n    <span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> j = <span class=\"pl-c1\">0</span>; j &lt; test_number; ++j) {\n        start = <span class=\"pl-c1\">std::chrono::system_clock::now</span>();\n        <span class=\"pl-k\">auto</span> output = module-&gt;<span class=\"pl-c1\">forward</span>(batch);\n\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> synchronizing</span>\n        <span class=\"pl-k\">auto</span> out_elems = output.<span class=\"pl-c1\">toTuple</span>()-&gt;<span class=\"pl-c1\">elements</span>();\n        <span class=\"pl-k\">auto</span> embed = out_elems[<span class=\"pl-c1\">1</span>].<span class=\"pl-c1\">toTensor</span>();\n        embed.<span class=\"pl-c1\">to</span>(at::<span class=\"pl-c1\">kCPU</span>);\n\n        end = <span class=\"pl-c1\">std::chrono::system_clock::now</span>();\n\n        <span class=\"pl-k\">long</span> elapsed_mseconds = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(end-start).<span class=\"pl-c1\">count</span>();\n\n        <span class=\"pl-k\">if</span> (j &gt; skip_numbers) {\n            times += elapsed_mseconds;\n        }\n    }\n\n    std::cout &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Mean time: <span class=\"pl-pds\">\"</span></span> &lt;&lt; times / (test_number - skip_numbers) &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">\"</span> msec<span class=\"pl-pds\">\"</span></span> &lt;&lt; <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>;\n}\n\n<span class=\"pl-k\">int</span> <span class=\"pl-en\">main</span>(<span class=\"pl-k\">int</span> argc, <span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* argv[]) {\n    <span class=\"pl-c1\">time_test</span>(argv[<span class=\"pl-c1\">1</span>]);\n}</pre></div>\n<p>As a result I got 25 msec for Python and 51 msec for C++.</p>", "body_text": "Hi folks! I'm converting my Pytorch model on C++ via Torchscipt. But when I tested the inference time, it turned out that my C++ model is two times slower than my original Python model.\nWhat could be the reason for this behavior?  Or am I mistaken somewhere?\nUbuntu 16.04, CUDA: 9.1, cuDNN: 7.1.3\nNet is a Wide ResNet 50-2.\nConverting in Torchscript code:\nnet = MyNet()\nnet.load_state_dict('model.pth')\nnet.train(False)\nexample = torch.rand(1, 3, 224, 224)\n\nwith torch.no_grad():\n    traced_script_module = torch.jit.trace(net, example)\n\ntraced_script_module.save('model.pt')\nPython inference time test code:\nnet = MyNet()\nnet.load_state_dict('model.pth')\nnet.train(False)\nnet.cuda()\n\ntest_number = 1010\nbatch_num = 8\nskip_numbers = 10\n\nbatch = torch.randn(batch_num, 3, 224, 224).cuda()\n\ntimes = 0\n\nfor i in range(test_number):\n    start = int(round(time.time() * 1000))\n    with torch.no_grad():\n        _, __ = net(batch)\n    torch.cuda.synchronize()\n    end = int(round(time.time() * 1000))\n    \n    elapsed_mseconds = end - start\n\n    if i > skip_numbers:\n        times += elapsed_mseconds\n\nprint('Mean time: {} msec'.format(times / (test_number - skip_numbers)))\nC++ inference time test code:\n#include <torch/torch.h>\n#include <torch/script.h>\n\n#include <ATen/ATen.h>\n\n#include <iostream>\n#include <memory>\n#include <chrono>\n#include <ctime>\n\nvoid time_test(const char* model_name) {\n    std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(model_name);\n    module->to(at::kCUDA);\n\n    int batch_num = 8;\n    int test_number = 1010;\n    int skip_numbers = 10;\n\n    std::vector<torch::jit::IValue> batch;\n    batch.push_back(torch::randn({batch_num, 3, 224, 224}).to(at::kCUDA));\n\n    std::chrono::time_point<std::chrono::system_clock> start, end;\n\n    int times = 0;\n\n    for (int j = 0; j < test_number; ++j) {\n        start = std::chrono::system_clock::now();\n        auto output = module->forward(batch);\n\n        // synchronizing\n        auto out_elems = output.toTuple()->elements();\n        auto embed = out_elems[1].toTensor();\n        embed.to(at::kCPU);\n\n        end = std::chrono::system_clock::now();\n\n        long elapsed_mseconds = std::chrono::duration_cast<std::chrono::milliseconds>(end-start).count();\n\n        if (j > skip_numbers) {\n            times += elapsed_mseconds;\n        }\n    }\n\n    std::cout << \"Mean time: \" << times / (test_number - skip_numbers) << \" msec\" << '\\n';\n}\n\nint main(int argc, const char* argv[]) {\n    time_test(argv[1]);\n}\nAs a result I got 25 msec for Python and 51 msec for C++.", "body": "Hi folks! I'm converting my Pytorch model on C++ via Torchscipt. But when I tested the inference time, it turned out that my C++ model is two times slower than my original Python model.\r\nWhat could be the reason for this behavior?  Or am I mistaken somewhere?\r\n\r\nUbuntu 16.04, CUDA: 9.1, cuDNN: 7.1.3\r\nNet is a Wide ResNet 50-2.\r\n\r\nConverting in Torchscript code:\r\n```python\r\nnet = MyNet()\r\nnet.load_state_dict('model.pth')\r\nnet.train(False)\r\nexample = torch.rand(1, 3, 224, 224)\r\n\r\nwith torch.no_grad():\r\n    traced_script_module = torch.jit.trace(net, example)\r\n\r\ntraced_script_module.save('model.pt')\r\n```\r\n\r\nPython inference time test code:\r\n```python\r\nnet = MyNet()\r\nnet.load_state_dict('model.pth')\r\nnet.train(False)\r\nnet.cuda()\r\n\r\ntest_number = 1010\r\nbatch_num = 8\r\nskip_numbers = 10\r\n\r\nbatch = torch.randn(batch_num, 3, 224, 224).cuda()\r\n\r\ntimes = 0\r\n\r\nfor i in range(test_number):\r\n    start = int(round(time.time() * 1000))\r\n    with torch.no_grad():\r\n        _, __ = net(batch)\r\n    torch.cuda.synchronize()\r\n    end = int(round(time.time() * 1000))\r\n    \r\n    elapsed_mseconds = end - start\r\n\r\n    if i > skip_numbers:\r\n        times += elapsed_mseconds\r\n\r\nprint('Mean time: {} msec'.format(times / (test_number - skip_numbers)))\r\n```\r\n\r\nC++ inference time test code:\r\n```C++\r\n#include <torch/torch.h>\r\n#include <torch/script.h>\r\n\r\n#include <ATen/ATen.h>\r\n\r\n#include <iostream>\r\n#include <memory>\r\n#include <chrono>\r\n#include <ctime>\r\n\r\nvoid time_test(const char* model_name) {\r\n    std::shared_ptr<torch::jit::script::Module> module = torch::jit::load(model_name);\r\n    module->to(at::kCUDA);\r\n\r\n    int batch_num = 8;\r\n    int test_number = 1010;\r\n    int skip_numbers = 10;\r\n\r\n    std::vector<torch::jit::IValue> batch;\r\n    batch.push_back(torch::randn({batch_num, 3, 224, 224}).to(at::kCUDA));\r\n\r\n    std::chrono::time_point<std::chrono::system_clock> start, end;\r\n\r\n    int times = 0;\r\n\r\n    for (int j = 0; j < test_number; ++j) {\r\n        start = std::chrono::system_clock::now();\r\n        auto output = module->forward(batch);\r\n\r\n        // synchronizing\r\n        auto out_elems = output.toTuple()->elements();\r\n        auto embed = out_elems[1].toTensor();\r\n        embed.to(at::kCPU);\r\n\r\n        end = std::chrono::system_clock::now();\r\n\r\n        long elapsed_mseconds = std::chrono::duration_cast<std::chrono::milliseconds>(end-start).count();\r\n\r\n        if (j > skip_numbers) {\r\n            times += elapsed_mseconds;\r\n        }\r\n    }\r\n\r\n    std::cout << \"Mean time: \" << times / (test_number - skip_numbers) << \" msec\" << '\\n';\r\n}\r\n\r\nint main(int argc, const char* argv[]) {\r\n    time_test(argv[1]);\r\n}\r\n```\r\n\r\nAs a result I got 25 msec for Python and 51 msec for C++."}