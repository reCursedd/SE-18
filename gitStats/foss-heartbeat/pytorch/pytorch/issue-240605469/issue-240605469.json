{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1982", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1982/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1982/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1982/events", "html_url": "https://github.com/pytorch/pytorch/issues/1982", "id": 240605469, "node_id": "MDU6SXNzdWUyNDA2MDU0Njk=", "number": 1982, "title": "[BUG] Different results on sum for Variable and Tensor of batch size 1", "user": {"login": "ethanluoyc", "id": 6040760, "node_id": "MDQ6VXNlcjYwNDA3NjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/6040760?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ethanluoyc", "html_url": "https://github.com/ethanluoyc", "followers_url": "https://api.github.com/users/ethanluoyc/followers", "following_url": "https://api.github.com/users/ethanluoyc/following{/other_user}", "gists_url": "https://api.github.com/users/ethanluoyc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ethanluoyc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ethanluoyc/subscriptions", "organizations_url": "https://api.github.com/users/ethanluoyc/orgs", "repos_url": "https://api.github.com/users/ethanluoyc/repos", "events_url": "https://api.github.com/users/ethanluoyc/events{/privacy}", "received_events_url": "https://api.github.com/users/ethanluoyc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-07-05T10:23:08Z", "updated_at": "2017-07-05T10:33:06Z", "closed_at": "2017-07-05T10:33:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p><strong>pytorch version:</strong> '0.1.12_2'</p>\n<p>It is not so easy to describe the exact bug here, but I think it is sufficient to provide an example in which strange results can be reproduced.</p>\n<p>Use the serialised version of Tensor in the attachment. Then run this snippet (also included in the attachment)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nx_restored <span class=\"pl-k\">=</span> Variable(torch.load(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x.pth<span class=\"pl-pds\">'</span></span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nx_res_repeated <span class=\"pl-k\">=</span> x_restored.repeat(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c1\">print</span>(x_restored.eq(x_res_repeated[<span class=\"pl-c1\">0</span>]).data.numpy().sum())\n<span class=\"pl-c1\">print</span>(x_restored.eq(x_res_repeated[<span class=\"pl-c1\">0</span>]).data.sum())\n<span class=\"pl-c1\">print</span>(x_restored.eq(x_res_repeated[<span class=\"pl-c1\">0</span>]).sum())\n</pre></div>\n<p>The output</p>\n<pre><code>3200\n3200\nVariable containing:\n 128\n[torch.ByteTensor of size 1]\n</code></pre>\n<p>I would expect the summation to return the same results regardless of whether we operate on numpy arrays, pytorch tensors or <code>Variable</code> holding a <code>Tensor</code> in the <code>data</code> field. (Which means it should return a Variable holding a Tensor with the right capacity to store the summed value.<br>\nHowever, this is unfortunately not the case.</p>\n<p><a href=\"https://github.com/pytorch/pytorch/files/1124295/bug_reproduce.zip\">bug_reproduce.zip</a></p>", "body_text": "pytorch version: '0.1.12_2'\nIt is not so easy to describe the exact bug here, but I think it is sufficient to provide an example in which strange results can be reproduced.\nUse the serialised version of Tensor in the attachment. Then run this snippet (also included in the attachment)\nimport torch\nfrom torch.autograd import Variable\n\nx_restored = Variable(torch.load('x.pth'), requires_grad=False)\nx_res_repeated = x_restored.repeat(10, 1)\n\nprint(x_restored.eq(x_res_repeated[0]).data.numpy().sum())\nprint(x_restored.eq(x_res_repeated[0]).data.sum())\nprint(x_restored.eq(x_res_repeated[0]).sum())\n\nThe output\n3200\n3200\nVariable containing:\n 128\n[torch.ByteTensor of size 1]\n\nI would expect the summation to return the same results regardless of whether we operate on numpy arrays, pytorch tensors or Variable holding a Tensor in the data field. (Which means it should return a Variable holding a Tensor with the right capacity to store the summed value.\nHowever, this is unfortunately not the case.\nbug_reproduce.zip", "body": "**pytorch version:** '0.1.12_2'\r\n\r\nIt is not so easy to describe the exact bug here, but I think it is sufficient to provide an example in which strange results can be reproduced.\r\n\r\nUse the serialised version of Tensor in the attachment. Then run this snippet (also included in the attachment)\r\n\r\n```python\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nx_restored = Variable(torch.load('x.pth'), requires_grad=False)\r\nx_res_repeated = x_restored.repeat(10, 1)\r\n\r\nprint(x_restored.eq(x_res_repeated[0]).data.numpy().sum())\r\nprint(x_restored.eq(x_res_repeated[0]).data.sum())\r\nprint(x_restored.eq(x_res_repeated[0]).sum())\r\n\r\n```\r\nThe output\r\n```\r\n3200\r\n3200\r\nVariable containing:\r\n 128\r\n[torch.ByteTensor of size 1]\r\n```\r\nI would expect the summation to return the same results regardless of whether we operate on numpy arrays, pytorch tensors or `Variable` holding a `Tensor` in the `data` field. (Which means it should return a Variable holding a Tensor with the right capacity to store the summed value.\r\nHowever, this is unfortunately not the case.\r\n\r\n[bug_reproduce.zip](https://github.com/pytorch/pytorch/files/1124295/bug_reproduce.zip)\r\n\r\n"}