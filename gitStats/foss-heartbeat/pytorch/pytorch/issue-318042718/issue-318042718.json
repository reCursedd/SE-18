{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6991", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6991/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6991/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6991/events", "html_url": "https://github.com/pytorch/pytorch/issues/6991", "id": 318042718, "node_id": "MDU6SXNzdWUzMTgwNDI3MTg=", "number": 6991, "title": "clamp makes grad None", "user": {"login": "theFool32", "id": 3063690, "node_id": "MDQ6VXNlcjMwNjM2OTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/3063690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/theFool32", "html_url": "https://github.com/theFool32", "followers_url": "https://api.github.com/users/theFool32/followers", "following_url": "https://api.github.com/users/theFool32/following{/other_user}", "gists_url": "https://api.github.com/users/theFool32/gists{/gist_id}", "starred_url": "https://api.github.com/users/theFool32/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/theFool32/subscriptions", "organizations_url": "https://api.github.com/users/theFool32/orgs", "repos_url": "https://api.github.com/users/theFool32/repos", "events_url": "https://api.github.com/users/theFool32/events{/privacy}", "received_events_url": "https://api.github.com/users/theFool32/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-26T13:59:17Z", "updated_at": "2018-04-26T14:06:28Z", "closed_at": "2018-04-26T14:05:49Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>It seems <code>torch.clamp</code> makes <code>grad</code> of a tensor with <code>requires_grad=True</code> <code>None</code><br>\nI don't know whether it is a bug or I misuse it.<br>\nThanks.</p>\n<h2>Code example</h2>\n<pre><code># snippet1\na = torch.rand(1, requires_grad=True)\na = torch.clamp(a, 0, 1)\nz = 2 * a\nz.backward()\nprint(a.requires_grad) # True\nprint(a.grad) # None\n\n# snippet2\na = torch.rand(1, requires_grad=True)\na = torch.clamp(a, 0, 1)\nz = 2 * a\nz.backward()\nprint(a.requires_grad) # True\nprint(a.grad) # tensor([ 2.])\n\n</code></pre>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: None</p>\n<p>OS: Arch Linux<br>\nGCC version: (GCC) 7.3.1 20180406<br>\nCMake version: version 3.10.3</p>\n<p>Python version: 3.6<br>\nIs CUDA available: No<br>\nCUDA runtime version: No CUDA<br>\nGPU models and configuration: No CUDA<br>\nNvidia driver version: No CUDA<br>\ncuDNN version: No CUDA</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.14.2)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchvision (0.2.0)<br>\n[conda] Could not collect</p>", "body_text": "Issue description\nIt seems torch.clamp makes grad of a tensor with requires_grad=True None\nI don't know whether it is a bug or I misuse it.\nThanks.\nCode example\n# snippet1\na = torch.rand(1, requires_grad=True)\na = torch.clamp(a, 0, 1)\nz = 2 * a\nz.backward()\nprint(a.requires_grad) # True\nprint(a.grad) # None\n\n# snippet2\na = torch.rand(1, requires_grad=True)\na = torch.clamp(a, 0, 1)\nz = 2 * a\nz.backward()\nprint(a.requires_grad) # True\nprint(a.grad) # tensor([ 2.])\n\n\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: None\nOS: Arch Linux\nGCC version: (GCC) 7.3.1 20180406\nCMake version: version 3.10.3\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nVersions of relevant libraries:\n[pip3] numpy (1.14.2)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.0)\n[conda] Could not collect", "body": "## Issue description\r\n\r\nIt seems `torch.clamp` makes `grad` of a tensor with `requires_grad=True` `None`\r\nI don't know whether it is a bug or I misuse it.\r\nThanks.\r\n\r\n## Code example\r\n```\r\n# snippet1\r\na = torch.rand(1, requires_grad=True)\r\na = torch.clamp(a, 0, 1)\r\nz = 2 * a\r\nz.backward()\r\nprint(a.requires_grad) # True\r\nprint(a.grad) # None\r\n\r\n# snippet2\r\na = torch.rand(1, requires_grad=True)\r\na = torch.clamp(a, 0, 1)\r\nz = 2 * a\r\nz.backward()\r\nprint(a.requires_grad) # True\r\nprint(a.grad) # tensor([ 2.])\r\n\r\n```\r\n\r\n## System Info\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 7.3.1 20180406\r\nCMake version: version 3.10.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.0)\r\n[conda] Could not collect\r\n"}