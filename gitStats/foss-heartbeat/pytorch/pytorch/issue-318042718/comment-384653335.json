{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/384653335", "html_url": "https://github.com/pytorch/pytorch/issues/6991#issuecomment-384653335", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6991", "id": 384653335, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDY1MzMzNQ==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-26T14:05:49Z", "updated_at": "2018-04-26T14:06:28Z", "author_association": "MEMBER", "body_html": "<p>The reason why in your first snippet the grad is <code>None</code> is because you are modifying the leaf variable <code>a</code> and you overwrite it with the result of <code>a.clamp</code>. This means that <code>a</code> now is an intermediate variable, and the gradient gets freed once it's not needed anymore.<br>\nYou should instead do:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nb <span class=\"pl-k\">=</span> torch.clamp(a, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\nz <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> b\nz.backward()\n<span class=\"pl-c1\">print</span>(a.requires_grad) <span class=\"pl-c\"><span class=\"pl-c\">#</span> True</span>\n<span class=\"pl-c1\">print</span>(a.grad) <span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor([ 2.])</span></pre></div>", "body_text": "The reason why in your first snippet the grad is None is because you are modifying the leaf variable a and you overwrite it with the result of a.clamp. This means that a now is an intermediate variable, and the gradient gets freed once it's not needed anymore.\nYou should instead do:\na = torch.rand(1, requires_grad=True)\nb = torch.clamp(a, 0, 1)\nz = 2 * b\nz.backward()\nprint(a.requires_grad) # True\nprint(a.grad) # tensor([ 2.])", "body": "The reason why in your first snippet the grad is `None` is because you are modifying the leaf variable `a` and you overwrite it with the result of `a.clamp`. This means that `a` now is an intermediate variable, and the gradient gets freed once it's not needed anymore.\r\nYou should instead do:\r\n```python\r\na = torch.rand(1, requires_grad=True)\r\nb = torch.clamp(a, 0, 1)\r\nz = 2 * b\r\nz.backward()\r\nprint(a.requires_grad) # True\r\nprint(a.grad) # tensor([ 2.])\r\n```"}