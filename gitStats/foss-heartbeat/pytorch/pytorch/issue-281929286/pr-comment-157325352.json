{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157325352", "pull_request_review_id": 83956690, "id": 157325352, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzMyNTM1Mg==", "diff_hunk": "@@ -584,5 +586,77 @@ Tensor RoiPooling2d_backward_cpu(\n   throw std::runtime_error(\"not implemented\");\n }\n \n+\n+\n+// TODO Replace this with more accurate digamma().\n+template <typename Scalar>\n+static inline Scalar digamma_one(Scalar x) {\n+  const Scalar eps = x * 1e-2;\n+  return (std::lgamma(x + eps) - std::lgamma(x - eps)) / (eps + eps);\n+}\n+\n+/** Computes the reparameterized gradient -(d/dalpha cdf(x;alpha)) / pdf(x;alpha)\n+    for random number x drawn from a standard Gamma distribution Gamma(alpha).\n+*/\n+template <typename Scalar>\n+static inline Scalar standard_gamma_grad_one(Scalar x, Scalar alpha) {\n+  // Use an asymptotic approximation for small x.\n+  if (x < 0.2f) {\n+    const Scalar a0 = 1 / alpha;\n+    const Scalar a1 = 1 / (alpha + 1);\n+    const Scalar a2 = 1 / (alpha + 2);\n+    const Scalar pow_x_alpha = std::pow(x, alpha);\n+    const Scalar gamma_pdf = std::pow(x, alpha - 1) * std::exp(-x);\n+    const Scalar gamma_cdf = pow_x_alpha * (a0 - x*a1 + 0.5f*x*x*a2);\n+    const Scalar gamma_cdf_alpha = (std::log(x) - digamma_one(alpha)) * gamma_cdf\n+        - pow_x_alpha * (a0*a0 - x*a1*a1 + 0.5f*x*x*a2*a2);\n+    const Scalar result = -gamma_cdf_alpha / gamma_pdf;\n+    return std::isnan(result) ? 0 : result;\n+  }\n+\n+  // Use an asymptotic approximation for large alpha.\n+  if (alpha > 50.0f) {\n+    return std::sqrt(x / alpha);\n+  }\n+\n+  // Use a bivariate rational approximation to the reparameterized gradient.\n+  const Scalar u = std::log(x / alpha);\n+  const Scalar v = std::log(alpha);\n+  static const Scalar coef_uv[3][8] = {\n+    {0.16028008, -0.088064309, 0.019630876, -0.0016920282,\n+     1.0, 0.36659853, 0.10843863, 0.0066895454},\n+    {0.521894, 0.16095838, 0.06237597, 0.0023884253,\n+     0.083457714, 0.0073297628, -0.0059299053, -0.00093720389},\n+    {-0.0031143957, -0.012143877, -0.0057656484, -0.00064847254,\n+     0.0087262576, -0.00022820524, 1.8871047e-05, 9.6307964e-06},\n+  };\n+  Scalar coef_v[8];\n+  for (int i = 0; i < 8; ++ i) {\n+    coef_v[i] = coef_uv[0][i] + u * (coef_uv[1][i] + u * coef_uv[2][i]);\n+  }\n+  const Scalar p = coef_v[0] + v * (coef_v[1] + v * (coef_v[2] + v * coef_v[3]));\n+  const Scalar q = coef_v[4] + v * (coef_v[5] + v * (coef_v[6] + v * coef_v[7]));\n+  return std::exp(p / q);\n+}\n+\n+template <typename Scalar>\n+struct StandardGammaGradOp {\n+  void operator()(Scalar& ret_val, const Scalar& self_val, const Scalar &alpha_val, bool& early_exit)\n+  {\n+    ret_val = standard_gamma_grad_one(self_val, alpha_val);\n+  }\n+\n+  static void apply(Tensor& ret, const Tensor& self, const Tensor& alpha) {\n+    StandardGammaGradOp<Scalar> op;\n+    CPU_tensor_apply3<Scalar, StandardGammaGradOp<Scalar>>(ret, self, alpha, op);\n+  }\n+};\n+\n+Tensor _standard_gamma_grad(const Tensor& self, const Tensor& alpha) {\n+  Tensor ret = self.type().tensor(self.sizes());\n+  dispatch_cpu_floating_types<StandardGammaGradOp>(self.type(), \"_standard_gamma_grad\", ret, self, alpha);", "path": "aten/src/ATen/native/NativeFunctions.cpp", "position": null, "original_position": 83, "commit_id": "196bdb8fd5e5819c66bbef8a9e5a57aedf5a7ec1", "original_commit_id": "6fe366fd1187dab09efe19941cdc793f9feee8a7", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "On (2) I'm not sure I understand; if we want the op to be able to be a lambda, isn't this basically as simple as we can get?", "created_at": "2017-12-16T00:22:50Z", "updated_at": "2018-11-23T15:37:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157325352", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4161", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157325352"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157325352"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4161"}}, "body_html": "<p>On (2) I'm not sure I understand; if we want the op to be able to be a lambda, isn't this basically as simple as we can get?</p>", "body_text": "On (2) I'm not sure I understand; if we want the op to be able to be a lambda, isn't this basically as simple as we can get?", "in_reply_to_id": 157128948}