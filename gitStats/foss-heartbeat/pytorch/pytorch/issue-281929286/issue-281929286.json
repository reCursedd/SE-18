{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4161", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4161/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4161/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4161/events", "html_url": "https://github.com/pytorch/pytorch/pull/4161", "id": 281929286, "node_id": "MDExOlB1bGxSZXF1ZXN0MTU4MjMwMTI4", "number": 4161, "title": "Support CPU Apply in ATen and implement standard_gamma using it", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2017-12-13T23:13:49Z", "updated_at": "2018-11-23T15:37:30Z", "closed_at": "2017-12-18T20:45:02Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4161", "html_url": "https://github.com/pytorch/pytorch/pull/4161", "diff_url": "https://github.com/pytorch/pytorch/pull/4161.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4161.patch"}, "body_html": "<p>Main changes in this PR:</p>\n<ol>\n<li>\n<p>Added a TH_APPLY-style templatized function for CPU apply calls (currently only 2 and 3 tensor argument versions are supported, but more are easy to add).  In fact, this is basically identical to TH_APPLY, except it uses ATen functions and the API is a template instead of a macro.  The template takes an operation that is performed on the data (and an indicator to signal early termination); i.e. you don't need to know that x_data is a pointer to the current data location of x.</p>\n</li>\n<li>\n<p>Refactors the ATen dispatch code to easily generate dispatch code for different subsets of the scalar types.  This is in preference to the template_scalar path, which requires valid specialization of each scalar type.  Valid specializations are  particularly annoying with CUDA because you most likely can't put the specializations in a header so need to write some sort of for-all-scalar-type macro to get the correct specializations.  Currently, we only generate dispatch_all (all scalar types, the equivalent existed already), and dispatch_cpu_floating_types (which is used by standard_gamma).</p>\n</li>\n<li>\n<p>Implements standard_gamma using the above changes as a proof of concept (this is an arbitrary choice, it was the latest apply macro to be committed).  The forward is bound via Declarations.yaml, the backward via the Apply template, and then they are hooked together in derivatives.yaml.  This eliminates needing to change TH at all going forward, which means one can write idiomatic C++ instead of the TH-style macros (e.g. TH_MATH_NAME).</p>\n</li>\n</ol>", "body_text": "Main changes in this PR:\n\n\nAdded a TH_APPLY-style templatized function for CPU apply calls (currently only 2 and 3 tensor argument versions are supported, but more are easy to add).  In fact, this is basically identical to TH_APPLY, except it uses ATen functions and the API is a template instead of a macro.  The template takes an operation that is performed on the data (and an indicator to signal early termination); i.e. you don't need to know that x_data is a pointer to the current data location of x.\n\n\nRefactors the ATen dispatch code to easily generate dispatch code for different subsets of the scalar types.  This is in preference to the template_scalar path, which requires valid specialization of each scalar type.  Valid specializations are  particularly annoying with CUDA because you most likely can't put the specializations in a header so need to write some sort of for-all-scalar-type macro to get the correct specializations.  Currently, we only generate dispatch_all (all scalar types, the equivalent existed already), and dispatch_cpu_floating_types (which is used by standard_gamma).\n\n\nImplements standard_gamma using the above changes as a proof of concept (this is an arbitrary choice, it was the latest apply macro to be committed).  The forward is bound via Declarations.yaml, the backward via the Apply template, and then they are hooked together in derivatives.yaml.  This eliminates needing to change TH at all going forward, which means one can write idiomatic C++ instead of the TH-style macros (e.g. TH_MATH_NAME).", "body": "Main changes in this PR:\r\n1) Added a TH_APPLY-style templatized function for CPU apply calls (currently only 2 and 3 tensor argument versions are supported, but more are easy to add).  In fact, this is basically identical to TH_APPLY, except it uses ATen functions and the API is a template instead of a macro.  The template takes an operation that is performed on the data (and an indicator to signal early termination); i.e. you don't need to know that x_data is a pointer to the current data location of x.\r\n\r\n2) Refactors the ATen dispatch code to easily generate dispatch code for different subsets of the scalar types.  This is in preference to the template_scalar path, which requires valid specialization of each scalar type.  Valid specializations are  particularly annoying with CUDA because you most likely can't put the specializations in a header so need to write some sort of for-all-scalar-type macro to get the correct specializations.  Currently, we only generate dispatch_all (all scalar types, the equivalent existed already), and dispatch_cpu_floating_types (which is used by standard_gamma).\r\n\r\n3) Implements standard_gamma using the above changes as a proof of concept (this is an arbitrary choice, it was the latest apply macro to be committed).  The forward is bound via Declarations.yaml, the backward via the Apply template, and then they are hooked together in derivatives.yaml.  This eliminates needing to change TH at all going forward, which means one can write idiomatic C++ instead of the TH-style macros (e.g. TH_MATH_NAME)."}