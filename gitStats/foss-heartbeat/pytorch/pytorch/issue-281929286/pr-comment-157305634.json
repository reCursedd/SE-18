{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157305634", "pull_request_review_id": 83933590, "id": 157305634, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzMwNTYzNA==", "diff_hunk": "@@ -0,0 +1,271 @@\n+#pragma once\n+\n+#include <sstream>\n+\n+namespace at {\n+\n+/*\n+ * The basic strategy for apply is as follows:\n+ *\n+ * 1. Starting with the outermost index, loop until we reach a dimension where the\n+ * data is no longer contiguous, i.e. the stride at that dimension is not equal to\n+ * the size of the tensor defined by the outer dimensions. Let's call this outer\n+ * (contiguous) tensor A. Note that if the Tensor is contiguous, then A is equal\n+ * to the entire Tensor. Let's call the inner tensor B.\n+ *\n+ * 2. We loop through the indices in B, starting at its outermost dimension. For\n+ * example, if B is a 2x2 matrix, then we do:\n+ *\n+ * B[0][0]\n+ * B[0][1]\n+ * B[1][0]\n+ * B[1][1]\n+ *\n+ * We set the offset into the underlying storage as (storageOffset + stride_B * index_B),\n+ * i.e. basically we compute the offset into the storage as we would normally for a\n+ * Tensor. But because we are guaranteed the subsequent data is contiguous in memory, we\n+ * can simply loop for sizeof(A) iterations and perform the operation, without having to\n+ * follow the order described by the strides of A.\n+ *\n+ * 3. As an optimization, we merge dimensions of A that are contiguous in memory. For\n+ * example, if A is a 3x3x3x3 tensor narrowed from a 3x3x4x3 tensor, then the first two\n+ * dimensions can be merged for the purposes of APPLY, reducing the number of nested\n+ * loops.\n+ */\n+\n+static inline void check_correct_backend(const Tensor &t, unsigned int pos) {\n+  if (t.type().backend() != Backend::CPU) {\n+    runtime_error(\"Expected tensor at position %d to have CPU Backend, but has %s Backend\",\n+                  pos, toString(t.type().backend()));\n+  }\n+}\n+\n+static inline void check_correct_backend(const Tensor& t1, const Tensor &t2) {\n+  check_correct_backend(t1, 1);\n+  check_correct_backend(t2, 2);\n+}\n+\n+static inline void check_correct_backend(const Tensor& t1, const Tensor &t2, const Tensor &t3) {\n+  check_correct_backend(t1, 1);\n+  check_correct_backend(t2, 2);\n+  check_correct_backend(t3, 3);\n+}\n+\n+#define __ATH_TENSOR_APPLYX_PREAMBLE(TYPE, ATENSOR, DIM, ALLOW_CONTIGUOUS) \\\n+  TYPE *ATENSOR##_data = NULL; \\\n+  int64_t *ATENSOR##_counter = NULL, *ATENSOR##_sizes = NULL, *ATENSOR##_strides = NULL, *ATENSOR##_dimOffset = NULL; \\\n+  int64_t ATENSOR##_stride = 0, ATENSOR##_size = 0, ATENSOR##_dim = 0, ATENSOR##_i; \\\n+  int ATENSOR##_contiguous = ALLOW_CONTIGUOUS && DIM < 0; \\\n+\\\n+  if(ATENSOR.dim() == 0) \\\n+    TH_TENSOR_APPLY_hasFinished = true; \\\n+  else \\\n+  { \\\n+    ATENSOR##_data = ATENSOR.data<TYPE>(); \\\n+    ATENSOR##_size = 1; \\\n+    ATENSOR##_stride = 1; \\\n+    for(ATENSOR##_i = ATENSOR.dim() - 1; ATENSOR##_i >= 0; ATENSOR##_i--) { \\\n+      if(ATENSOR.sizes()[ATENSOR##_i] != 1) { \\\n+        if(ATENSOR.strides()[ATENSOR##_i] == ATENSOR##_size && ATENSOR##_i != DIM) \\\n+          ATENSOR##_size *= ATENSOR.sizes()[ATENSOR##_i]; \\\n+        else{ \\\n+          ATENSOR##_contiguous = 0; \\\n+          break; \\\n+        } \\\n+      } \\\n+    } \\\n+    if (!ATENSOR##_contiguous) { \\\n+      /* Find the dimension of contiguous sections */ \\\n+      ATENSOR##_dim = 1; \\\n+      for(ATENSOR##_i = ATENSOR.dim() - 2; ATENSOR##_i >= 0; ATENSOR##_i--) \\\n+      { \\\n+        if(ATENSOR.strides()[ATENSOR##_i] != ATENSOR.strides()[ATENSOR##_i+1] * ATENSOR.sizes()[ATENSOR##_i+1] || ATENSOR##_i == DIM || ATENSOR##_i+1 == DIM) \\\n+          ATENSOR##_dim++; \\\n+      } \\\n+      /* Allocate an array of 3*dim elements, where dim is the number of contiguous sections */ \\\n+      ATENSOR##_counter = new int64_t[3*ATENSOR##_dim]; \\\n+      ATENSOR##_sizes = ATENSOR##_counter + ATENSOR##_dim; \\\n+      ATENSOR##_strides = ATENSOR##_counter + 2*ATENSOR##_dim; \\\n+      TH_TENSOR_dim_index = ATENSOR##_dim-1; \\\n+      ATENSOR##_dimOffset = (DIM == ATENSOR.dim()-1) ? &ATENSOR##_i : &ATENSOR##_counter[DIM]; \\\n+      ATENSOR##_sizes[TH_TENSOR_dim_index] = ATENSOR.sizes()[ATENSOR.dim()-1]; \\\n+      ATENSOR##_strides[TH_TENSOR_dim_index] = ATENSOR.strides()[ATENSOR.dim()-1]; \\\n+      /* ATENSOR##_counter tracks where we are in the storage. The offset into the */ \\\n+      /* storage is given by storage_offset + (i * j), where i is the stride */ \\\n+      /* vector and j is tensor_counter vector. This sets the starting position for the loop. */ \\\n+      for(ATENSOR##_i = ATENSOR##_dim-1; ATENSOR##_i >= 0; --ATENSOR##_i) { \\\n+        ATENSOR##_counter[ATENSOR##_i] = 0; \\\n+      } \\\n+      for(ATENSOR##_i = ATENSOR.dim()-2; ATENSOR##_i >= 0; --ATENSOR##_i) { \\\n+        if (ATENSOR.strides()[ATENSOR##_i] == ATENSOR.strides()[ATENSOR##_i+1] * ATENSOR.sizes()[ATENSOR##_i+1] && ATENSOR##_i != DIM && ATENSOR##_i+1 != DIM) { \\\n+          ATENSOR##_sizes[TH_TENSOR_dim_index] = ATENSOR.sizes()[ATENSOR##_i] * ATENSOR##_sizes[TH_TENSOR_dim_index]; \\\n+          if (DIM != ATENSOR.dim()-1 && ATENSOR##_i < DIM) \\\n+            ATENSOR##_dimOffset--; \\\n+        } else { \\\n+          --TH_TENSOR_dim_index; \\\n+          ATENSOR##_sizes[TH_TENSOR_dim_index] = ATENSOR.sizes()[ATENSOR##_i]; \\\n+          ATENSOR##_strides[TH_TENSOR_dim_index] = ATENSOR.strides()[ATENSOR##_i]; \\\n+        } \\\n+      } \\\n+      /* Size of the inner most section */ \\\n+      ATENSOR##_size = ATENSOR##_sizes[ATENSOR##_dim-1]; \\\n+      /* Stride of the inner most section */ \\\n+      ATENSOR##_stride = ATENSOR##_strides[ATENSOR##_dim-1]; \\\n+    } \\\n+  } \\\n+  ATENSOR##_i = 0;\n+\n+#define  __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(ATENSOR, ALWAYS_UPDATE) \\\n+  if(ATENSOR##_i == ATENSOR##_size || ALWAYS_UPDATE) \\\n+  { \\\n+    if(ATENSOR##_contiguous) \\\n+      break; \\\n+\\\n+    if(ATENSOR##_dim == 1) \\\n+       break; \\\n+\\\n+    /* Reset pointer to beginning of loop */ \\\n+    ATENSOR##_data -= ATENSOR##_size*ATENSOR##_stride; \\\n+    for(ATENSOR##_i = ATENSOR##_dim-2; ATENSOR##_i >= 0; ATENSOR##_i--) \\\n+    { \\\n+      ATENSOR##_counter[ATENSOR##_i]++; \\\n+      /* Jump ahread by the stride of this dimension */ \\\n+      ATENSOR##_data += ATENSOR##_strides[ATENSOR##_i]; \\\n+\\\n+      if(ATENSOR##_counter[ATENSOR##_i]  == ATENSOR##_sizes[ATENSOR##_i]) \\\n+      { \\\n+        if(ATENSOR##_i == 0) \\\n+        { \\\n+          TH_TENSOR_APPLY_hasFinished = true; \\\n+          break; \\\n+        } \\\n+          else \\\n+        { \\\n+          /* Reset the pointer to the beginning of the chunk defined by this dimension */ \\\n+          ATENSOR##_data -= ATENSOR##_counter[ATENSOR##_i]*ATENSOR##_strides[ATENSOR##_i]; \\\n+          ATENSOR##_counter[ATENSOR##_i] = 0; \\\n+        } \\\n+      } \\\n+      else \\\n+        break; \\\n+    } \\\n+    ATENSOR##_i = 0; \\\n+  } \\\n+\n+#define ATH_TENSOR_APPLY2_D(TYPE, ATENSOR1, ATENSOR2, DIM, CODE) \\\n+{ \\\n+  bool TH_TENSOR_APPLY_hasFinished = false; \\\n+  int64_t TH_TENSOR_dim_index = 0; \\\n+  __ATH_TENSOR_APPLYX_PREAMBLE(TYPE, ATENSOR1, DIM, 1) \\\n+  __ATH_TENSOR_APPLYX_PREAMBLE(TYPE, ATENSOR2, DIM, 1) \\\n+\\\n+  auto t1_numel = ATENSOR1.numel(); \\\n+  auto t2_numel = ATENSOR2.numel(); \\\n+  if(t1_numel != t2_numel) {                                    \\\n+    std::ostringstream oss; \\\n+    oss << \"inconsistent tensor size, expected \" << ATENSOR1.sizes() << \" and \" << ATENSOR2.sizes() \\\n+        << \" to have the same number of elements, but got \" << t1_numel << \" and \" << t2_numel << \" elements respectively\"; \\\n+    throw std::runtime_error(oss.str()); \\\n+  }                                                                   \\\n+  while(!TH_TENSOR_APPLY_hasFinished) \\\n+  { \\\n+    /* Loop through the inner most region of the Tensor */ \\\n+    for(; ATENSOR1##_i < ATENSOR1##_size && ATENSOR2##_i < ATENSOR2##_size; ATENSOR1##_i++, ATENSOR2##_i++, ATENSOR1##_data += ATENSOR1##_stride, ATENSOR2##_data += ATENSOR2##_stride) /* 0 et pas TENSOR##_dim! */ \\\n+    { \\\n+      CODE \\\n+    } \\\n+    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(ATENSOR1, 0) \\\n+    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(ATENSOR2, 0) \\\n+  } \\\n+  if(ATENSOR1##_counter != NULL) \\\n+    delete [] ATENSOR1##_counter; \\\n+  if(ATENSOR2##_counter != NULL) \\\n+    delete [] ATENSOR2##_counter; \\\n+}\n+\n+template <typename ScalarType, typename Op>\n+void CPU_tensor_apply2_dim(Tensor& tensor1, Tensor& tensor2, int64_t dim, Op op) {\n+  check_correct_backend(tensor1, tensor2);\n+  bool TH_TENSOR_APPLY_hasFinished = false;\n+  int64_t TH_TENSOR_dim_index = 0;\n+  __ATH_TENSOR_APPLYX_PREAMBLE(ScalarType, tensor1, dim, 1)\n+  __ATH_TENSOR_APPLYX_PREAMBLE(ScalarType, tensor2, dim, 1)\n+  auto t1_numel = tensor1.numel();\n+  auto t2_numel = tensor2.numel();\n+  if(t1_numel != t2_numel) {\n+    std::ostringstream oss;\n+    oss << \"inconsistent tensor size, expected \" << tensor1.sizes() << \" and \" << tensor2.sizes()\n+        << \" to have the same number of elements, but got \" << t1_numel << \" and \" << t2_numel << \" elements respectively\";\n+    throw std::runtime_error(oss.str());\n+  }\n+  while(!TH_TENSOR_APPLY_hasFinished)\n+  {\n+    /* Loop through the inner most region of the Tensor */\n+    for(; tensor1_i < tensor1_size && tensor2_i < tensor2_size; tensor1_i++, tensor2_i++, tensor1_data += tensor1_stride, tensor2_data += tensor2_stride)\n+    {\n+      op(*tensor1_data, *tensor2_data, TH_TENSOR_APPLY_hasFinished);\n+    }\n+    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(tensor1, 0)\n+    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(tensor2, 0)\n+  }\n+  if(tensor1_counter != NULL)\n+    delete [] tensor1_counter;\n+  if(tensor2_counter != NULL)\n+    delete [] tensor2_counter;\n+}\n+\n+template<typename ScalarType, typename Op>\n+void CPU_tensor_apply2(Tensor tensor1, Tensor tensor2, Op op) {\n+  CPU_tensor_apply2_dim<ScalarType, Op>(tensor1, tensor2, -1, op);\n+}\n+\n+template <typename ScalarType, typename Op>\n+void CPU_tensor_apply3_dim(Tensor &tensor1, Tensor& tensor2, Tensor& tensor3, int64_t dim, Op op) {", "path": "aten/src/ATen/CPUApplyUtils.h", "position": null, "original_position": 223, "commit_id": "196bdb8fd5e5819c66bbef8a9e5a57aedf5a7ec1", "original_commit_id": "6fe366fd1187dab09efe19941cdc793f9feee8a7", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "seems plausible; I want to have some reasonable benchmarking in place before I embark on that; I was doing things on the python level but this only supports Variables, the older version only Tensors (unless you use the python function and swallow that overhead).  So I'll just do the benchmarking at the C++ level.", "created_at": "2017-12-15T21:44:11Z", "updated_at": "2018-11-23T15:37:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157305634", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4161", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157305634"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157305634"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4161"}}, "body_html": "<p>seems plausible; I want to have some reasonable benchmarking in place before I embark on that; I was doing things on the python level but this only supports Variables, the older version only Tensors (unless you use the python function and swallow that overhead).  So I'll just do the benchmarking at the C++ level.</p>", "body_text": "seems plausible; I want to have some reasonable benchmarking in place before I embark on that; I was doing things on the python level but this only supports Variables, the older version only Tensors (unless you use the python function and swallow that overhead).  So I'll just do the benchmarking at the C++ level.", "in_reply_to_id": 157113535}