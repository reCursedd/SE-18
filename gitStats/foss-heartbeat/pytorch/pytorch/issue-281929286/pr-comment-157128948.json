{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157128948", "pull_request_review_id": 83721980, "id": 157128948, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzEyODk0OA==", "diff_hunk": "@@ -584,5 +586,77 @@ Tensor RoiPooling2d_backward_cpu(\n   throw std::runtime_error(\"not implemented\");\n }\n \n+\n+\n+// TODO Replace this with more accurate digamma().\n+template <typename Scalar>\n+static inline Scalar digamma_one(Scalar x) {\n+  const Scalar eps = x * 1e-2;\n+  return (std::lgamma(x + eps) - std::lgamma(x - eps)) / (eps + eps);\n+}\n+\n+/** Computes the reparameterized gradient -(d/dalpha cdf(x;alpha)) / pdf(x;alpha)\n+    for random number x drawn from a standard Gamma distribution Gamma(alpha).\n+*/\n+template <typename Scalar>\n+static inline Scalar standard_gamma_grad_one(Scalar x, Scalar alpha) {\n+  // Use an asymptotic approximation for small x.\n+  if (x < 0.2f) {\n+    const Scalar a0 = 1 / alpha;\n+    const Scalar a1 = 1 / (alpha + 1);\n+    const Scalar a2 = 1 / (alpha + 2);\n+    const Scalar pow_x_alpha = std::pow(x, alpha);\n+    const Scalar gamma_pdf = std::pow(x, alpha - 1) * std::exp(-x);\n+    const Scalar gamma_cdf = pow_x_alpha * (a0 - x*a1 + 0.5f*x*x*a2);\n+    const Scalar gamma_cdf_alpha = (std::log(x) - digamma_one(alpha)) * gamma_cdf\n+        - pow_x_alpha * (a0*a0 - x*a1*a1 + 0.5f*x*x*a2*a2);\n+    const Scalar result = -gamma_cdf_alpha / gamma_pdf;\n+    return std::isnan(result) ? 0 : result;\n+  }\n+\n+  // Use an asymptotic approximation for large alpha.\n+  if (alpha > 50.0f) {\n+    return std::sqrt(x / alpha);\n+  }\n+\n+  // Use a bivariate rational approximation to the reparameterized gradient.\n+  const Scalar u = std::log(x / alpha);\n+  const Scalar v = std::log(alpha);\n+  static const Scalar coef_uv[3][8] = {\n+    {0.16028008, -0.088064309, 0.019630876, -0.0016920282,\n+     1.0, 0.36659853, 0.10843863, 0.0066895454},\n+    {0.521894, 0.16095838, 0.06237597, 0.0023884253,\n+     0.083457714, 0.0073297628, -0.0059299053, -0.00093720389},\n+    {-0.0031143957, -0.012143877, -0.0057656484, -0.00064847254,\n+     0.0087262576, -0.00022820524, 1.8871047e-05, 9.6307964e-06},\n+  };\n+  Scalar coef_v[8];\n+  for (int i = 0; i < 8; ++ i) {\n+    coef_v[i] = coef_uv[0][i] + u * (coef_uv[1][i] + u * coef_uv[2][i]);\n+  }\n+  const Scalar p = coef_v[0] + v * (coef_v[1] + v * (coef_v[2] + v * coef_v[3]));\n+  const Scalar q = coef_v[4] + v * (coef_v[5] + v * (coef_v[6] + v * coef_v[7]));\n+  return std::exp(p / q);\n+}\n+\n+template <typename Scalar>\n+struct StandardGammaGradOp {\n+  void operator()(Scalar& ret_val, const Scalar& self_val, const Scalar &alpha_val, bool& early_exit)\n+  {\n+    ret_val = standard_gamma_grad_one(self_val, alpha_val);\n+  }\n+\n+  static void apply(Tensor& ret, const Tensor& self, const Tensor& alpha) {\n+    StandardGammaGradOp<Scalar> op;\n+    CPU_tensor_apply3<Scalar, StandardGammaGradOp<Scalar>>(ret, self, alpha, op);\n+  }\n+};\n+\n+Tensor _standard_gamma_grad(const Tensor& self, const Tensor& alpha) {\n+  Tensor ret = self.type().tensor(self.sizes());\n+  dispatch_cpu_floating_types<StandardGammaGradOp>(self.type(), \"_standard_gamma_grad\", ret, self, alpha);", "path": "aten/src/ATen/native/NativeFunctions.cpp", "position": null, "original_position": 83, "commit_id": "196bdb8fd5e5819c66bbef8a9e5a57aedf5a7ec1", "original_commit_id": "6fe366fd1187dab09efe19941cdc793f9feee8a7", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "Ok, I think this is getting more complicated than it needs to be because dispatch was already very complicated.\r\nI think we can make all of this simpler by basically simplifying the dispatch macros to only care about switching over scalar type. The old dispatch can be deleted since it is not used as far as I know.\r\n\r\nFor instance, here it would be much simpler if.\r\n(1) We used a function EnsureBackend({self, alpha}, at::kCPU) rather than rely on folding it into one function.\r\n(2) We use templated functions rather the classes. The reason for the functions was partial specialization. If you need partial specialization, then just write your own switch statement.\r\n(3) We get rid of the generated dispatch code, this was needed because of the cross product of (backend x scalar_type) that made this exceedingly verbose. If we just dispatch on scalar type, each option can be written directly in Dispatch.h.", "created_at": "2017-12-15T05:58:40Z", "updated_at": "2018-11-23T15:37:24Z", "html_url": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157128948", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4161", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157128948"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157128948"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4161"}}, "body_html": "<p>Ok, I think this is getting more complicated than it needs to be because dispatch was already very complicated.<br>\nI think we can make all of this simpler by basically simplifying the dispatch macros to only care about switching over scalar type. The old dispatch can be deleted since it is not used as far as I know.</p>\n<p>For instance, here it would be much simpler if.<br>\n(1) We used a function EnsureBackend({self, alpha}, at::kCPU) rather than rely on folding it into one function.<br>\n(2) We use templated functions rather the classes. The reason for the functions was partial specialization. If you need partial specialization, then just write your own switch statement.<br>\n(3) We get rid of the generated dispatch code, this was needed because of the cross product of (backend x scalar_type) that made this exceedingly verbose. If we just dispatch on scalar type, each option can be written directly in Dispatch.h.</p>", "body_text": "Ok, I think this is getting more complicated than it needs to be because dispatch was already very complicated.\nI think we can make all of this simpler by basically simplifying the dispatch macros to only care about switching over scalar type. The old dispatch can be deleted since it is not used as far as I know.\nFor instance, here it would be much simpler if.\n(1) We used a function EnsureBackend({self, alpha}, at::kCPU) rather than rely on folding it into one function.\n(2) We use templated functions rather the classes. The reason for the functions was partial specialization. If you need partial specialization, then just write your own switch statement.\n(3) We get rid of the generated dispatch code, this was needed because of the cross product of (backend x scalar_type) that made this exceedingly verbose. If we just dispatch on scalar type, each option can be written directly in Dispatch.h."}