{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157250521", "pull_request_review_id": 83867647, "id": 157250521, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzI1MDUyMQ==", "diff_hunk": "@@ -0,0 +1,271 @@\n+#pragma once\n+\n+#include <sstream>\n+\n+namespace at {\n+\n+/*\n+ * The basic strategy for apply is as follows:\n+ *\n+ * 1. Starting with the outermost index, loop until we reach a dimension where the\n+ * data is no longer contiguous, i.e. the stride at that dimension is not equal to\n+ * the size of the tensor defined by the outer dimensions. Let's call this outer\n+ * (contiguous) tensor A. Note that if the Tensor is contiguous, then A is equal\n+ * to the entire Tensor. Let's call the inner tensor B.\n+ *\n+ * 2. We loop through the indices in B, starting at its outermost dimension. For\n+ * example, if B is a 2x2 matrix, then we do:\n+ *\n+ * B[0][0]\n+ * B[0][1]\n+ * B[1][0]\n+ * B[1][1]\n+ *\n+ * We set the offset into the underlying storage as (storageOffset + stride_B * index_B),\n+ * i.e. basically we compute the offset into the storage as we would normally for a\n+ * Tensor. But because we are guaranteed the subsequent data is contiguous in memory, we\n+ * can simply loop for sizeof(A) iterations and perform the operation, without having to\n+ * follow the order described by the strides of A.\n+ *\n+ * 3. As an optimization, we merge dimensions of A that are contiguous in memory. For\n+ * example, if A is a 3x3x3x3 tensor narrowed from a 3x3x4x3 tensor, then the first two\n+ * dimensions can be merged for the purposes of APPLY, reducing the number of nested\n+ * loops.\n+ */\n+\n+static inline void check_correct_backend(const Tensor &t, unsigned int pos) {\n+  if (t.type().backend() != Backend::CPU) {\n+    runtime_error(\"Expected tensor at position %d to have CPU Backend, but has %s Backend\",\n+                  pos, toString(t.type().backend()));\n+  }\n+}\n+\n+static inline void check_correct_backend(const Tensor& t1, const Tensor &t2) {\n+  check_correct_backend(t1, 1);\n+  check_correct_backend(t2, 2);\n+}\n+\n+static inline void check_correct_backend(const Tensor& t1, const Tensor &t2, const Tensor &t3) {\n+  check_correct_backend(t1, 1);\n+  check_correct_backend(t2, 2);\n+  check_correct_backend(t3, 3);\n+}\n+\n+#define __ATH_TENSOR_APPLYX_PREAMBLE(TYPE, ATENSOR, DIM, ALLOW_CONTIGUOUS) \\\n+  TYPE *ATENSOR##_data = NULL; \\\n+  int64_t *ATENSOR##_counter = NULL, *ATENSOR##_sizes = NULL, *ATENSOR##_strides = NULL, *ATENSOR##_dimOffset = NULL; \\\n+  int64_t ATENSOR##_stride = 0, ATENSOR##_size = 0, ATENSOR##_dim = 0, ATENSOR##_i; \\\n+  int ATENSOR##_contiguous = ALLOW_CONTIGUOUS && DIM < 0; \\\n+\\\n+  if(ATENSOR.dim() == 0) \\\n+    TH_TENSOR_APPLY_hasFinished = true; \\\n+  else \\\n+  { \\\n+    ATENSOR##_data = ATENSOR.data<TYPE>(); \\\n+    ATENSOR##_size = 1; \\\n+    ATENSOR##_stride = 1; \\\n+    for(ATENSOR##_i = ATENSOR.dim() - 1; ATENSOR##_i >= 0; ATENSOR##_i--) { \\\n+      if(ATENSOR.sizes()[ATENSOR##_i] != 1) { \\\n+        if(ATENSOR.strides()[ATENSOR##_i] == ATENSOR##_size && ATENSOR##_i != DIM) \\\n+          ATENSOR##_size *= ATENSOR.sizes()[ATENSOR##_i]; \\\n+        else{ \\\n+          ATENSOR##_contiguous = 0; \\\n+          break; \\\n+        } \\\n+      } \\\n+    } \\\n+    if (!ATENSOR##_contiguous) { \\\n+      /* Find the dimension of contiguous sections */ \\\n+      ATENSOR##_dim = 1; \\\n+      for(ATENSOR##_i = ATENSOR.dim() - 2; ATENSOR##_i >= 0; ATENSOR##_i--) \\\n+      { \\\n+        if(ATENSOR.strides()[ATENSOR##_i] != ATENSOR.strides()[ATENSOR##_i+1] * ATENSOR.sizes()[ATENSOR##_i+1] || ATENSOR##_i == DIM || ATENSOR##_i+1 == DIM) \\\n+          ATENSOR##_dim++; \\\n+      } \\\n+      /* Allocate an array of 3*dim elements, where dim is the number of contiguous sections */ \\\n+      ATENSOR##_counter = new int64_t[3*ATENSOR##_dim]; \\\n+      ATENSOR##_sizes = ATENSOR##_counter + ATENSOR##_dim; \\\n+      ATENSOR##_strides = ATENSOR##_counter + 2*ATENSOR##_dim; \\\n+      TH_TENSOR_dim_index = ATENSOR##_dim-1; \\\n+      ATENSOR##_dimOffset = (DIM == ATENSOR.dim()-1) ? &ATENSOR##_i : &ATENSOR##_counter[DIM]; \\\n+      ATENSOR##_sizes[TH_TENSOR_dim_index] = ATENSOR.sizes()[ATENSOR.dim()-1]; \\\n+      ATENSOR##_strides[TH_TENSOR_dim_index] = ATENSOR.strides()[ATENSOR.dim()-1]; \\\n+      /* ATENSOR##_counter tracks where we are in the storage. The offset into the */ \\\n+      /* storage is given by storage_offset + (i * j), where i is the stride */ \\\n+      /* vector and j is tensor_counter vector. This sets the starting position for the loop. */ \\\n+      for(ATENSOR##_i = ATENSOR##_dim-1; ATENSOR##_i >= 0; --ATENSOR##_i) { \\\n+        ATENSOR##_counter[ATENSOR##_i] = 0; \\\n+      } \\\n+      for(ATENSOR##_i = ATENSOR.dim()-2; ATENSOR##_i >= 0; --ATENSOR##_i) { \\\n+        if (ATENSOR.strides()[ATENSOR##_i] == ATENSOR.strides()[ATENSOR##_i+1] * ATENSOR.sizes()[ATENSOR##_i+1] && ATENSOR##_i != DIM && ATENSOR##_i+1 != DIM) { \\\n+          ATENSOR##_sizes[TH_TENSOR_dim_index] = ATENSOR.sizes()[ATENSOR##_i] * ATENSOR##_sizes[TH_TENSOR_dim_index]; \\\n+          if (DIM != ATENSOR.dim()-1 && ATENSOR##_i < DIM) \\\n+            ATENSOR##_dimOffset--; \\\n+        } else { \\\n+          --TH_TENSOR_dim_index; \\\n+          ATENSOR##_sizes[TH_TENSOR_dim_index] = ATENSOR.sizes()[ATENSOR##_i]; \\\n+          ATENSOR##_strides[TH_TENSOR_dim_index] = ATENSOR.strides()[ATENSOR##_i]; \\\n+        } \\\n+      } \\\n+      /* Size of the inner most section */ \\\n+      ATENSOR##_size = ATENSOR##_sizes[ATENSOR##_dim-1]; \\\n+      /* Stride of the inner most section */ \\\n+      ATENSOR##_stride = ATENSOR##_strides[ATENSOR##_dim-1]; \\\n+    } \\\n+  } \\\n+  ATENSOR##_i = 0;\n+\n+#define  __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(ATENSOR, ALWAYS_UPDATE) \\\n+  if(ATENSOR##_i == ATENSOR##_size || ALWAYS_UPDATE) \\\n+  { \\\n+    if(ATENSOR##_contiguous) \\\n+      break; \\\n+\\\n+    if(ATENSOR##_dim == 1) \\\n+       break; \\\n+\\\n+    /* Reset pointer to beginning of loop */ \\\n+    ATENSOR##_data -= ATENSOR##_size*ATENSOR##_stride; \\\n+    for(ATENSOR##_i = ATENSOR##_dim-2; ATENSOR##_i >= 0; ATENSOR##_i--) \\\n+    { \\\n+      ATENSOR##_counter[ATENSOR##_i]++; \\\n+      /* Jump ahread by the stride of this dimension */ \\\n+      ATENSOR##_data += ATENSOR##_strides[ATENSOR##_i]; \\\n+\\\n+      if(ATENSOR##_counter[ATENSOR##_i]  == ATENSOR##_sizes[ATENSOR##_i]) \\\n+      { \\\n+        if(ATENSOR##_i == 0) \\\n+        { \\\n+          TH_TENSOR_APPLY_hasFinished = true; \\\n+          break; \\\n+        } \\\n+          else \\\n+        { \\\n+          /* Reset the pointer to the beginning of the chunk defined by this dimension */ \\\n+          ATENSOR##_data -= ATENSOR##_counter[ATENSOR##_i]*ATENSOR##_strides[ATENSOR##_i]; \\\n+          ATENSOR##_counter[ATENSOR##_i] = 0; \\\n+        } \\\n+      } \\\n+      else \\\n+        break; \\\n+    } \\\n+    ATENSOR##_i = 0; \\\n+  } \\\n+\n+#define ATH_TENSOR_APPLY2_D(TYPE, ATENSOR1, ATENSOR2, DIM, CODE) \\\n+{ \\\n+  bool TH_TENSOR_APPLY_hasFinished = false; \\\n+  int64_t TH_TENSOR_dim_index = 0; \\\n+  __ATH_TENSOR_APPLYX_PREAMBLE(TYPE, ATENSOR1, DIM, 1) \\\n+  __ATH_TENSOR_APPLYX_PREAMBLE(TYPE, ATENSOR2, DIM, 1) \\\n+\\\n+  auto t1_numel = ATENSOR1.numel(); \\\n+  auto t2_numel = ATENSOR2.numel(); \\\n+  if(t1_numel != t2_numel) {                                    \\\n+    std::ostringstream oss; \\\n+    oss << \"inconsistent tensor size, expected \" << ATENSOR1.sizes() << \" and \" << ATENSOR2.sizes() \\\n+        << \" to have the same number of elements, but got \" << t1_numel << \" and \" << t2_numel << \" elements respectively\"; \\\n+    throw std::runtime_error(oss.str()); \\\n+  }                                                                   \\\n+  while(!TH_TENSOR_APPLY_hasFinished) \\\n+  { \\\n+    /* Loop through the inner most region of the Tensor */ \\\n+    for(; ATENSOR1##_i < ATENSOR1##_size && ATENSOR2##_i < ATENSOR2##_size; ATENSOR1##_i++, ATENSOR2##_i++, ATENSOR1##_data += ATENSOR1##_stride, ATENSOR2##_data += ATENSOR2##_stride) /* 0 et pas TENSOR##_dim! */ \\\n+    { \\\n+      CODE \\\n+    } \\\n+    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(ATENSOR1, 0) \\\n+    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(ATENSOR2, 0) \\\n+  } \\\n+  if(ATENSOR1##_counter != NULL) \\\n+    delete [] ATENSOR1##_counter; \\\n+  if(ATENSOR2##_counter != NULL) \\\n+    delete [] ATENSOR2##_counter; \\\n+}\n+\n+template <typename ScalarType, typename Op>\n+void CPU_tensor_apply2_dim(Tensor& tensor1, Tensor& tensor2, int64_t dim, Op op) {\n+  check_correct_backend(tensor1, tensor2);\n+  bool TH_TENSOR_APPLY_hasFinished = false;\n+  int64_t TH_TENSOR_dim_index = 0;\n+  __ATH_TENSOR_APPLYX_PREAMBLE(ScalarType, tensor1, dim, 1)\n+  __ATH_TENSOR_APPLYX_PREAMBLE(ScalarType, tensor2, dim, 1)\n+  auto t1_numel = tensor1.numel();\n+  auto t2_numel = tensor2.numel();\n+  if(t1_numel != t2_numel) {\n+    std::ostringstream oss;\n+    oss << \"inconsistent tensor size, expected \" << tensor1.sizes() << \" and \" << tensor2.sizes()\n+        << \" to have the same number of elements, but got \" << t1_numel << \" and \" << t2_numel << \" elements respectively\";\n+    throw std::runtime_error(oss.str());\n+  }\n+  while(!TH_TENSOR_APPLY_hasFinished)\n+  {\n+    /* Loop through the inner most region of the Tensor */\n+    for(; tensor1_i < tensor1_size && tensor2_i < tensor2_size; tensor1_i++, tensor2_i++, tensor1_data += tensor1_stride, tensor2_data += tensor2_stride)\n+    {\n+      op(*tensor1_data, *tensor2_data, TH_TENSOR_APPLY_hasFinished);\n+    }\n+    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(tensor1, 0)\n+    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(tensor2, 0)\n+  }\n+  if(tensor1_counter != NULL)\n+    delete [] tensor1_counter;\n+  if(tensor2_counter != NULL)\n+    delete [] tensor2_counter;\n+}\n+\n+template<typename ScalarType, typename Op>\n+void CPU_tensor_apply2(Tensor tensor1, Tensor tensor2, Op op) {\n+  CPU_tensor_apply2_dim<ScalarType, Op>(tensor1, tensor2, -1, op);\n+}\n+\n+template <typename ScalarType, typename Op>", "path": "aten/src/ATen/CPUApplyUtils.h", "position": null, "original_position": 222, "commit_id": "196bdb8fd5e5819c66bbef8a9e5a57aedf5a7ec1", "original_commit_id": "6fe366fd1187dab09efe19941cdc793f9feee8a7", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "ScalarType is the name of a concrete `enum` in ATen. This ScalarType is not the same thing. Let's use a different name for the template parameter (even `real` is better).", "created_at": "2017-12-15T17:02:43Z", "updated_at": "2018-11-23T15:37:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157250521", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4161", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157250521"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157250521"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4161"}}, "body_html": "<p>ScalarType is the name of a concrete <code>enum</code> in ATen. This ScalarType is not the same thing. Let's use a different name for the template parameter (even <code>real</code> is better).</p>", "body_text": "ScalarType is the name of a concrete enum in ATen. This ScalarType is not the same thing. Let's use a different name for the template parameter (even real is better)."}