{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157252133", "pull_request_review_id": 83867647, "id": 157252133, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzI1MjEzMw==", "diff_hunk": "@@ -584,5 +586,77 @@ Tensor RoiPooling2d_backward_cpu(\n   throw std::runtime_error(\"not implemented\");\n }\n \n+\n+\n+// TODO Replace this with more accurate digamma().\n+template <typename Scalar>\n+static inline Scalar digamma_one(Scalar x) {\n+  const Scalar eps = x * 1e-2;\n+  return (std::lgamma(x + eps) - std::lgamma(x - eps)) / (eps + eps);\n+}\n+\n+/** Computes the reparameterized gradient -(d/dalpha cdf(x;alpha)) / pdf(x;alpha)\n+    for random number x drawn from a standard Gamma distribution Gamma(alpha).\n+*/\n+template <typename Scalar>\n+static inline Scalar standard_gamma_grad_one(Scalar x, Scalar alpha) {\n+  // Use an asymptotic approximation for small x.\n+  if (x < 0.2f) {\n+    const Scalar a0 = 1 / alpha;\n+    const Scalar a1 = 1 / (alpha + 1);\n+    const Scalar a2 = 1 / (alpha + 2);\n+    const Scalar pow_x_alpha = std::pow(x, alpha);\n+    const Scalar gamma_pdf = std::pow(x, alpha - 1) * std::exp(-x);\n+    const Scalar gamma_cdf = pow_x_alpha * (a0 - x*a1 + 0.5f*x*x*a2);\n+    const Scalar gamma_cdf_alpha = (std::log(x) - digamma_one(alpha)) * gamma_cdf\n+        - pow_x_alpha * (a0*a0 - x*a1*a1 + 0.5f*x*x*a2*a2);\n+    const Scalar result = -gamma_cdf_alpha / gamma_pdf;\n+    return std::isnan(result) ? 0 : result;\n+  }\n+\n+  // Use an asymptotic approximation for large alpha.\n+  if (alpha > 50.0f) {\n+    return std::sqrt(x / alpha);\n+  }\n+\n+  // Use a bivariate rational approximation to the reparameterized gradient.\n+  const Scalar u = std::log(x / alpha);\n+  const Scalar v = std::log(alpha);\n+  static const Scalar coef_uv[3][8] = {\n+    {0.16028008, -0.088064309, 0.019630876, -0.0016920282,\n+     1.0, 0.36659853, 0.10843863, 0.0066895454},\n+    {0.521894, 0.16095838, 0.06237597, 0.0023884253,\n+     0.083457714, 0.0073297628, -0.0059299053, -0.00093720389},\n+    {-0.0031143957, -0.012143877, -0.0057656484, -0.00064847254,\n+     0.0087262576, -0.00022820524, 1.8871047e-05, 9.6307964e-06},\n+  };\n+  Scalar coef_v[8];\n+  for (int i = 0; i < 8; ++ i) {\n+    coef_v[i] = coef_uv[0][i] + u * (coef_uv[1][i] + u * coef_uv[2][i]);\n+  }\n+  const Scalar p = coef_v[0] + v * (coef_v[1] + v * (coef_v[2] + v * coef_v[3]));\n+  const Scalar q = coef_v[4] + v * (coef_v[5] + v * (coef_v[6] + v * coef_v[7]));\n+  return std::exp(p / q);\n+}\n+\n+template <typename Scalar>\n+struct StandardGammaGradOp {\n+  void operator()(Scalar& ret_val, const Scalar& self_val, const Scalar &alpha_val, bool& early_exit)", "path": "aten/src/ATen/native/NativeFunctions.cpp", "position": null, "original_position": 70, "commit_id": "196bdb8fd5e5819c66bbef8a9e5a57aedf5a7ec1", "original_commit_id": "6fe366fd1187dab09efe19941cdc793f9feee8a7", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I'm not a fan of early_exit: I don't think it will work well with parallelization strategies on the CPU. Currently, the default for apply functions is for serial execution. If we care about CPU performance, the default should be parallel, which means the API shouldn't expose serial semantics.", "created_at": "2017-12-15T17:09:40Z", "updated_at": "2018-11-23T15:37:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157252133", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4161", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157252133"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4161#discussion_r157252133"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4161"}}, "body_html": "<p>I'm not a fan of early_exit: I don't think it will work well with parallelization strategies on the CPU. Currently, the default for apply functions is for serial execution. If we care about CPU performance, the default should be parallel, which means the API shouldn't expose serial semantics.</p>", "body_text": "I'm not a fan of early_exit: I don't think it will work well with parallelization strategies on the CPU. Currently, the default for apply functions is for serial execution. If we care about CPU performance, the default should be parallel, which means the API shouldn't expose serial semantics.", "in_reply_to_id": 157106645}