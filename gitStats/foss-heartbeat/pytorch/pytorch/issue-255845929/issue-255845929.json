{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2655", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2655/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2655/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2655/events", "html_url": "https://github.com/pytorch/pytorch/issues/2655", "id": 255845929, "node_id": "MDU6SXNzdWUyNTU4NDU5Mjk=", "number": 2655, "title": "[Feature request] detach (fix?) a nn.module", "user": {"login": "ruotianluo", "id": 16023153, "node_id": "MDQ6VXNlcjE2MDIzMTUz", "avatar_url": "https://avatars2.githubusercontent.com/u/16023153?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ruotianluo", "html_url": "https://github.com/ruotianluo", "followers_url": "https://api.github.com/users/ruotianluo/followers", "following_url": "https://api.github.com/users/ruotianluo/following{/other_user}", "gists_url": "https://api.github.com/users/ruotianluo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ruotianluo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ruotianluo/subscriptions", "organizations_url": "https://api.github.com/users/ruotianluo/orgs", "repos_url": "https://api.github.com/users/ruotianluo/repos", "events_url": "https://api.github.com/users/ruotianluo/events{/privacy}", "received_events_url": "https://api.github.com/users/ruotianluo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-09-07T07:50:29Z", "updated_at": "2017-10-02T10:50:01Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Suppose, we have a siamese network.</p>\n<pre><code>out1 = net(in1)\nout2 = net(in2)\n</code></pre>\n<p>However, we want the gradient of parameters in <code>net</code> only affected by the first branch (note that suppose we still want the gradient of <code>in1</code> and <code>in2</code>),<br>\nthen what we should do here is:</p>\n<pre><code>out1 = net(in1)\nfor p in net.parameters(): p.requires_grad=False\nout2 =net(in2)\nfor p in net.parameters(): p.requires_grad=True\nfn(out1, out2).backward()\n</code></pre>\n<p>So, can we have a <code>detach</code> function for nn.module:</p>\n<p>So, when we don't want to update a net, we can do:<br>\n<code>net.detach_()</code><br>\n(Here, detach_() just set all the parameters requires_grad to be False)</p>\n<p>If we want to do the example I mention above, we could do:</p>\n<pre><code>out1 = net(in1)\nout2 = net.detach()(in2)\nfn(out1, out2).backward()\n</code></pre>\n<p>(here, detach() is a clone of the input network but with all parameters requires_grad to be False)</p>\n<p>(I give the name detach because it's essentially detach the module Parameters)</p>", "body_text": "Suppose, we have a siamese network.\nout1 = net(in1)\nout2 = net(in2)\n\nHowever, we want the gradient of parameters in net only affected by the first branch (note that suppose we still want the gradient of in1 and in2),\nthen what we should do here is:\nout1 = net(in1)\nfor p in net.parameters(): p.requires_grad=False\nout2 =net(in2)\nfor p in net.parameters(): p.requires_grad=True\nfn(out1, out2).backward()\n\nSo, can we have a detach function for nn.module:\nSo, when we don't want to update a net, we can do:\nnet.detach_()\n(Here, detach_() just set all the parameters requires_grad to be False)\nIf we want to do the example I mention above, we could do:\nout1 = net(in1)\nout2 = net.detach()(in2)\nfn(out1, out2).backward()\n\n(here, detach() is a clone of the input network but with all parameters requires_grad to be False)\n(I give the name detach because it's essentially detach the module Parameters)", "body": "Suppose, we have a siamese network.\r\n```\r\nout1 = net(in1)\r\nout2 = net(in2)\r\n```\r\n\r\nHowever, we want the gradient of parameters in `net` only affected by the first branch (note that suppose we still want the gradient of `in1` and `in2`),\r\nthen what we should do here is:\r\n```\r\nout1 = net(in1)\r\nfor p in net.parameters(): p.requires_grad=False\r\nout2 =net(in2)\r\nfor p in net.parameters(): p.requires_grad=True\r\nfn(out1, out2).backward()\r\n```\r\n\r\n\r\nSo, can we have a `detach` function for nn.module:\r\n\r\nSo, when we don't want to update a net, we can do:\r\n`net.detach_()`\r\n(Here, detach_() just set all the parameters requires_grad to be False)\r\n\r\nIf we want to do the example I mention above, we could do:\r\n```\r\nout1 = net(in1)\r\nout2 = net.detach()(in2)\r\nfn(out1, out2).backward()\r\n```\r\n(here, detach() is a clone of the input network but with all parameters requires_grad to be False)\r\n\r\n(I give the name detach because it's essentially detach the module Parameters)"}