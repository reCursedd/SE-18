{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4282", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4282/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4282/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4282/events", "html_url": "https://github.com/pytorch/pytorch/issues/4282", "id": 283671040, "node_id": "MDU6SXNzdWUyODM2NzEwNDA=", "number": 4282, "title": "RuntimeError for indexing with high dimensional tensor only when using cuda", "user": {"login": "sytrus-in-github", "id": 12224616, "node_id": "MDQ6VXNlcjEyMjI0NjE2", "avatar_url": "https://avatars0.githubusercontent.com/u/12224616?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sytrus-in-github", "html_url": "https://github.com/sytrus-in-github", "followers_url": "https://api.github.com/users/sytrus-in-github/followers", "following_url": "https://api.github.com/users/sytrus-in-github/following{/other_user}", "gists_url": "https://api.github.com/users/sytrus-in-github/gists{/gist_id}", "starred_url": "https://api.github.com/users/sytrus-in-github/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sytrus-in-github/subscriptions", "organizations_url": "https://api.github.com/users/sytrus-in-github/orgs", "repos_url": "https://api.github.com/users/sytrus-in-github/repos", "events_url": "https://api.github.com/users/sytrus-in-github/events{/privacy}", "received_events_url": "https://api.github.com/users/sytrus-in-github/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-20T19:30:15Z", "updated_at": "2018-04-24T13:48:27Z", "closed_at": "2018-04-24T12:20:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi there!</p>\n<p>I have been working with some high dimensional pytorch variables, and have come across several times the following error when doing indexing operation with cuda:</p>\n<pre><code>RuntimeError: Assertion `ndim &lt;= MAX_ADVINDEX_CALC_DIMS' failed.\n</code></pre>\n<p>What is annoying about this error is that the same code can work with cpu with no problem but not with cuda ...</p>\n<p>This problem can be avoided with the workaround of first reshaping the variable to lower dimensions, then do the corresponding indexing, then switch back to the original shape, like, for example, this workaround for flipping a tensor:</p>\n<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"190314353\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/229\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/229/hovercard?comment_id=350041662&amp;comment_type=issue_comment\" href=\"https://github.com/pytorch/pytorch/issues/229#issuecomment-350041662\">#229 (comment)</a></p>\n<p>So there are my questions:</p>\n<ul>\n<li>Why does this assertion have to exist?</li>\n<li>Can this constraint somehow be handled internally so that indexing with high dimensional tensor can work with cuda and have the same behavior than the cpu case?</li>\n</ul>\n<p>Thanks!</p>", "body_text": "Hi there!\nI have been working with some high dimensional pytorch variables, and have come across several times the following error when doing indexing operation with cuda:\nRuntimeError: Assertion `ndim <= MAX_ADVINDEX_CALC_DIMS' failed.\n\nWhat is annoying about this error is that the same code can work with cpu with no problem but not with cuda ...\nThis problem can be avoided with the workaround of first reshaping the variable to lower dimensions, then do the corresponding indexing, then switch back to the original shape, like, for example, this workaround for flipping a tensor:\n#229 (comment)\nSo there are my questions:\n\nWhy does this assertion have to exist?\nCan this constraint somehow be handled internally so that indexing with high dimensional tensor can work with cuda and have the same behavior than the cpu case?\n\nThanks!", "body": "Hi there!\r\n\r\nI have been working with some high dimensional pytorch variables, and have come across several times the following error when doing indexing operation with cuda:\r\n```\r\nRuntimeError: Assertion `ndim <= MAX_ADVINDEX_CALC_DIMS' failed.\r\n```\r\nWhat is annoying about this error is that the same code can work with cpu with no problem but not with cuda ...\r\n\r\nThis problem can be avoided with the workaround of first reshaping the variable to lower dimensions, then do the corresponding indexing, then switch back to the original shape, like, for example, this workaround for flipping a tensor:\r\n\r\n[https://github.com/pytorch/pytorch/issues/229#issuecomment-350041662](https://github.com/pytorch/pytorch/issues/229#issuecomment-350041662)  \r\n\r\nSo there are my questions:\r\n\r\n- Why does this assertion have to exist? \r\n- Can this constraint somehow be handled internally so that indexing with high dimensional tensor can work with cuda and have the same behavior than the cpu case?\r\n\r\nThanks!"}