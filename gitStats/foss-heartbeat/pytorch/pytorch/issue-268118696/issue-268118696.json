{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3266", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3266/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3266/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3266/events", "html_url": "https://github.com/pytorch/pytorch/issues/3266", "id": 268118696, "node_id": "MDU6SXNzdWUyNjgxMTg2OTY=", "number": 3266, "title": "Wrong memory order with pin_memory() over transposed tensors", "user": {"login": "ozancaglayan", "id": 330946, "node_id": "MDQ6VXNlcjMzMDk0Ng==", "avatar_url": "https://avatars0.githubusercontent.com/u/330946?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ozancaglayan", "html_url": "https://github.com/ozancaglayan", "followers_url": "https://api.github.com/users/ozancaglayan/followers", "following_url": "https://api.github.com/users/ozancaglayan/following{/other_user}", "gists_url": "https://api.github.com/users/ozancaglayan/gists{/gist_id}", "starred_url": "https://api.github.com/users/ozancaglayan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ozancaglayan/subscriptions", "organizations_url": "https://api.github.com/users/ozancaglayan/orgs", "repos_url": "https://api.github.com/users/ozancaglayan/repos", "events_url": "https://api.github.com/users/ozancaglayan/events{/privacy}", "received_events_url": "https://api.github.com/users/ozancaglayan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-24T17:25:23Z", "updated_at": "2017-10-25T11:19:06Z", "closed_at": "2017-10-25T11:19:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hello,</p>\n<p>I was experimenting with a custom <code>collate_fn</code> which receives a batch of different sized integer sequences and pads them to form 2D tensors. I noticed a weird issue for which I created the following MWE:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\nt <span class=\"pl-k\">=</span> torch.FloatTensor(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>)).view((<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">5</span>))\n<span class=\"pl-c1\">print</span>(t.t())\n<span class=\"pl-c1\">print</span>(t.t().pin_memory())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Output</span>\n <span class=\"pl-c1\">0</span>  <span class=\"pl-c1\">5</span>\n <span class=\"pl-c1\">1</span>  <span class=\"pl-c1\">6</span>\n <span class=\"pl-c1\">2</span>  <span class=\"pl-c1\">7</span>\n <span class=\"pl-c1\">3</span>  <span class=\"pl-c1\">8</span>\n <span class=\"pl-c1\">4</span>  <span class=\"pl-c1\">9</span>\n[torch.FloatTensor of size <span class=\"pl-ii\">5x2</span>]\n\n <span class=\"pl-c1\">0</span>  <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">2</span>  <span class=\"pl-c1\">3</span>\n <span class=\"pl-c1\">4</span>  <span class=\"pl-c1\">5</span>\n <span class=\"pl-c1\">6</span>  <span class=\"pl-c1\">7</span>\n <span class=\"pl-c1\">8</span>  <span class=\"pl-c1\">9</span>\n[torch.FloatTensor of size <span class=\"pl-ii\">5x2</span>]</pre></div>\n<p>It's probably because of the different memory ordering on Python's side and CUDA's allocator.<br>\nTested on K40 and GTX1080 with driver 375.39, CUDA8.</p>", "body_text": "Hello,\nI was experimenting with a custom collate_fn which receives a batch of different sized integer sequences and pads them to form 2D tensors. I noticed a weird issue for which I created the following MWE:\nimport torch\n\nt = torch.FloatTensor(range(10)).view((2,5))\nprint(t.t())\nprint(t.t().pin_memory())\n\n# Output\n 0  5\n 1  6\n 2  7\n 3  8\n 4  9\n[torch.FloatTensor of size 5x2]\n\n 0  1\n 2  3\n 4  5\n 6  7\n 8  9\n[torch.FloatTensor of size 5x2]\nIt's probably because of the different memory ordering on Python's side and CUDA's allocator.\nTested on K40 and GTX1080 with driver 375.39, CUDA8.", "body": "Hello,\r\n\r\nI was experimenting with a custom `collate_fn` which receives a batch of different sized integer sequences and pads them to form 2D tensors. I noticed a weird issue for which I created the following MWE:\r\n\r\n```python\r\nimport torch\r\n\r\nt = torch.FloatTensor(range(10)).view((2,5))\r\nprint(t.t())\r\nprint(t.t().pin_memory())\r\n\r\n# Output\r\n 0  5\r\n 1  6\r\n 2  7\r\n 3  8\r\n 4  9\r\n[torch.FloatTensor of size 5x2]\r\n\r\n 0  1\r\n 2  3\r\n 4  5\r\n 6  7\r\n 8  9\r\n[torch.FloatTensor of size 5x2]\r\n```\r\n\r\nIt's probably because of the different memory ordering on Python's side and CUDA's allocator.\r\nTested on K40 and GTX1080 with driver 375.39, CUDA8."}