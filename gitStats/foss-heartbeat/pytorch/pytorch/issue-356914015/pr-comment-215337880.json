{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215337880", "pull_request_review_id": 152579436, "id": 215337880, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTMzNzg4MA==", "diff_hunk": "@@ -0,0 +1,205 @@\n+#include \"ATen/native/cpu/DistanceOpsKernel.h\"\n+\n+#include <numeric>\n+#include <iterator>\n+#include <algorithm>\n+\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/Parallel.h\"\n+#include \"ATen/cpu/vml.h\"\n+\n+namespace at { namespace native { namespace {\n+\n+template<typename scalar_t>\n+struct PDist {\n+  using Vec = vec256::Vec256<scalar_t>;\n+\n+  static inline Vec sign(Vec val) {\n+    return vec256::min(vec256::max(Vec(0), val.ceil()), Vec(1)) +\n+      vec256::min(vec256::max(Vec(-1), val.floor()), Vec(0));\n+  }\n+\n+  // Zero norm\n+  struct zdist_calc {\n+    static inline Vec map(const Vec& diff, const Vec& p) { return vec256::min(diff.abs().ceil(), Vec(1)); }\n+    static inline Vec red(const Vec& agg, const Vec& up) { return agg + up; }\n+    static inline scalar_t finish(const scalar_t agg, const scalar_t p) { return agg; }\n+  };\n+\n+  // One norm\n+  struct odist_calc {\n+    static inline Vec map(const Vec& diff, const Vec& p) { return diff; }\n+    static inline Vec red(const Vec& agg, const Vec& up) { return agg + up; }\n+    static inline scalar_t finish(const scalar_t agg, const scalar_t p) { return agg; }\n+    static inline Vec backward(const Vec& diff, const scalar_t grad, const scalar_t dist, const Vec& p) { return Vec(grad) * sign(diff); }\n+  };\n+\n+  // Special general pnorm derivative if p is less than two\n+  struct lttdist_calc {\n+    static inline Vec backward(const Vec& diff, const scalar_t grad, const scalar_t dist, const Vec& p) { return dist == 0.0 ? Vec(0) : sign(diff) * diff.abs().pow(p - Vec(1)) * Vec(grad) / Vec(dist).pow(p - Vec(1)); }\n+  };\n+\n+  // Two norm\n+  struct tdist_calc {\n+    // TODO This can probably use fused add multiply to get better perf\n+    static inline Vec map(const Vec& diff, const Vec& p) { return diff * diff; }\n+    static inline Vec red(const Vec& agg, const Vec& up) { return agg + up; }\n+    static inline scalar_t finish(const scalar_t agg, const scalar_t p) { return std::sqrt(agg); }\n+    static inline Vec backward(const Vec& diff, const scalar_t grad, const scalar_t dist, const Vec& p) { return dist == 0.0 ? Vec(0) : Vec(grad) * diff / Vec(dist); }\n+  };\n+\n+  // General p norm\n+  struct pdist_calc {\n+    static inline Vec map(const Vec& diff, const Vec& p) { return diff.pow(p); }\n+    static inline Vec red(const Vec& agg, const Vec& up) { return agg + up; }\n+    static inline scalar_t finish(const scalar_t agg, const scalar_t p) { return std::pow(agg, 1.0 / p); }\n+    static inline Vec backward(const Vec& diff, const scalar_t grad, const scalar_t dist, const Vec& p) { return dist == 0.0 ? Vec(0) : diff * diff.abs().pow(p - Vec(2)) * Vec(grad) / Vec(dist).pow(p - Vec(1)); }\n+  };\n+\n+  // Info norm\n+  struct idist_calc {\n+    static inline Vec map(const Vec& diff, const Vec& p) { return diff; }\n+    static inline Vec red(const Vec& agg, const Vec& up) { return vec256::max(agg, up); }\n+    static inline scalar_t finish(const scalar_t agg, const scalar_t p) { return agg; }\n+    static inline Vec backward(const Vec& diff, const scalar_t grad, const scalar_t dist, const Vec& p) { return Vec(grad) * sign(diff) * (Vec(1) - vec256::min(Vec(1), (diff.abs() - Vec(dist)).abs().ceil())); }\n+  };\n+\n+  template <typename F>\n+  static void run_parallel(Tensor& result, const Tensor& self, const scalar_t p) {\n+    const scalar_t * const self_start = self.data<scalar_t>();\n+    const scalar_t * const self_end = self_start + self.numel();\n+    int64_t n = self.size(0);\n+    int64_t m = self.size(1);\n+\n+    scalar_t * const res_start = result.data<scalar_t>();\n+    int64_t combs = result.numel(); // n * (n - 1) / 2\n+    const Vec pvec(p);\n+\n+    parallel_for(0, combs, internal::GRAIN_SIZE / (16 * m), [=, &pvec](int64_t k, int64_t end) {\n+      float n2 = n - .5;\n+      // The -1 accounts for floating point truncation issues\n+      int64_t i = static_cast<int64_t>((n2 - std::sqrt(n2 * n2 - 2 * k - 1)));\n+      int64_t j = k - n * i + i * (i + 1) / 2 + i + 1;\n+\n+      const scalar_t * self_i = self_start + i * m;\n+      const scalar_t * self_j = self_start + j * m;\n+      scalar_t * res = res_start + k;\n+      const scalar_t * const res_end = res_start + end;\n+\n+      while (res != res_end) {\n+        *res = F::finish(vec256::map2_reduce_all<scalar_t>(\n+          [&pvec](Vec a, Vec b) { return F::map((a - b).abs(), pvec); },\n+          F::red, self_i, self_j, m), p);\n+\n+        res += 1;\n+        self_j += m;\n+        if (self_j == self_end) {\n+          self_i += m;\n+          self_j = self_i + m;\n+        }\n+      }\n+    });\n+  }\n+\n+  // Assumes self is nonempty and 2D\n+  static void apply(Tensor& result, const Tensor& self, const scalar_t p) {\n+    if (p == 0.0) {", "path": "aten/src/ATen/native/cpu/DistanceOpsKernel.cpp", "position": null, "original_position": 106, "commit_id": "fb1ad6aafbc16862ca2973c08f91e84683964805", "original_commit_id": "e49da239055adf80fe06f01811d189dd8431276f", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "body": "Do your tests cover all these branches?", "created_at": "2018-09-05T16:21:09Z", "updated_at": "2018-11-23T15:50:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/11230#discussion_r215337880", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11230", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215337880"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11230#discussion_r215337880"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11230"}}, "body_html": "<p>Do your tests cover all these branches?</p>", "body_text": "Do your tests cover all these branches?"}