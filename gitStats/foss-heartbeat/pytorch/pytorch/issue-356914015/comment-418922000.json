{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/418922000", "html_url": "https://github.com/pytorch/pytorch/pull/11230#issuecomment-418922000", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11230", "id": 418922000, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODkyMjAwMA==", "user": {"login": "erikbrinkman", "id": 858926, "node_id": "MDQ6VXNlcjg1ODkyNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/858926?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erikbrinkman", "html_url": "https://github.com/erikbrinkman", "followers_url": "https://api.github.com/users/erikbrinkman/followers", "following_url": "https://api.github.com/users/erikbrinkman/following{/other_user}", "gists_url": "https://api.github.com/users/erikbrinkman/gists{/gist_id}", "starred_url": "https://api.github.com/users/erikbrinkman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erikbrinkman/subscriptions", "organizations_url": "https://api.github.com/users/erikbrinkman/orgs", "repos_url": "https://api.github.com/users/erikbrinkman/repos", "events_url": "https://api.github.com/users/erikbrinkman/events{/privacy}", "received_events_url": "https://api.github.com/users/erikbrinkman/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-06T00:16:55Z", "updated_at": "2018-09-06T00:16:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a> The default dispatch is still slower than it was before introducing this change. The cause isn't clear other than it seems that the compiler is not eliding all of the Vec256 operations. If the answer is that lack of AVX is an uncommon use case and we don't care about performance there than this seems ready to push. If the answer is that we still want this to be <em>reasonably</em> fast than there are two ways to fix it: making the dispatch dispatch to the old code, or introducing tricks to get the compiler to figure out how to handle vec256_base functions appropriately. The later seems far outside the scope of this PR.</p>\n<p>In response to timing for various sizes single threaded, usually an average of 7 runs. These two graphs are with the other dimension set to 2048. Note that for changing the number of rows, the computation grows by n^2. The width of each bar corresponds to the uncertainty. The scaling on each seems appropriate.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/858926/45127838-2fe64700-b12f-11e8-8d52-e086a0b474eb.png\"><img src=\"https://user-images.githubusercontent.com/858926/45127838-2fe64700-b12f-11e8-8d52-e086a0b474eb.png\" alt=\"num_cols\" style=\"max-width:100%;\"></a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/858926/45127843-34aafb00-b12f-11e8-936b-981114af2a5f.png\"><img src=\"https://user-images.githubusercontent.com/858926/45127843-34aafb00-b12f-11e8-936b-981114af2a5f.png\" alt=\"num_rows\" style=\"max-width:100%;\"></a></p>", "body_text": "@cpuhrsch The default dispatch is still slower than it was before introducing this change. The cause isn't clear other than it seems that the compiler is not eliding all of the Vec256 operations. If the answer is that lack of AVX is an uncommon use case and we don't care about performance there than this seems ready to push. If the answer is that we still want this to be reasonably fast than there are two ways to fix it: making the dispatch dispatch to the old code, or introducing tricks to get the compiler to figure out how to handle vec256_base functions appropriately. The later seems far outside the scope of this PR.\nIn response to timing for various sizes single threaded, usually an average of 7 runs. These two graphs are with the other dimension set to 2048. Note that for changing the number of rows, the computation grows by n^2. The width of each bar corresponds to the uncertainty. The scaling on each seems appropriate.", "body": "@cpuhrsch The default dispatch is still slower than it was before introducing this change. The cause isn't clear other than it seems that the compiler is not eliding all of the Vec256 operations. If the answer is that lack of AVX is an uncommon use case and we don't care about performance there than this seems ready to push. If the answer is that we still want this to be _reasonably_ fast than there are two ways to fix it: making the dispatch dispatch to the old code, or introducing tricks to get the compiler to figure out how to handle vec256_base functions appropriately. The later seems far outside the scope of this PR.\r\n\r\nIn response to timing for various sizes single threaded, usually an average of 7 runs. These two graphs are with the other dimension set to 2048. Note that for changing the number of rows, the computation grows by n^2. The width of each bar corresponds to the uncertainty. The scaling on each seems appropriate.\r\n\r\n![num_cols](https://user-images.githubusercontent.com/858926/45127838-2fe64700-b12f-11e8-8d52-e086a0b474eb.png)\r\n\r\n![num_rows](https://user-images.githubusercontent.com/858926/45127843-34aafb00-b12f-11e8-936b-981114af2a5f.png)\r\n"}