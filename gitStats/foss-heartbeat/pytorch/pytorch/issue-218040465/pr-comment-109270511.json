{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109270511", "pull_request_review_id": 30367946, "id": 109270511, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTI3MDUxMQ==", "diff_hunk": "@@ -190,7 +196,11 @@ def forward(fn, input, hx, weight, output, hy):\n         handle = cudnn.get_handle()\n         fn.datatype = cudnn._typemap[input.type()]\n         is_input_packed = fn.batch_sizes is not None\n-\n+        if is_input_packed:\n+            warnings.warn(\n+                \"Warning: persistent algorithm not supported for variable length input.\"\n+                \"Switching to standard\")\n+            fn.persistent = False  # persistent algo is not supported for variable length input", "path": "torch/backends/cudnn/rnn.py", "position": 41, "original_position": 41, "commit_id": "738ebd1be1c0444730a96ce650e85496a0f8444e", "original_commit_id": "738ebd1be1c0444730a96ce650e85496a0f8444e", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "body": "Wow, this is easily twice as fast for my current use case (the whole model -- the LSTMs are probably 5x faster). Thanks Natalia! Now we just need more P100s... :)\r\n(Also, I'm okay with setting the flag every time but defaulting to use persistent mode for cases where it's obviously better is probably worth it)", "created_at": "2017-04-01T00:22:43Z", "updated_at": "2018-11-23T15:32:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/1141#discussion_r109270511", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1141", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109270511"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1141#discussion_r109270511"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1141"}}, "body_html": "<p>Wow, this is easily twice as fast for my current use case (the whole model -- the LSTMs are probably 5x faster). Thanks Natalia! Now we just need more P100s... :)<br>\n(Also, I'm okay with setting the flag every time but defaulting to use persistent mode for cases where it's obviously better is probably worth it)</p>", "body_text": "Wow, this is easily twice as fast for my current use case (the whole model -- the LSTMs are probably 5x faster). Thanks Natalia! Now we just need more P100s... :)\n(Also, I'm okay with setting the flag every time but defaulting to use persistent mode for cases where it's obviously better is probably worth it)", "in_reply_to_id": 109042245}