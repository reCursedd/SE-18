{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109049467", "pull_request_review_id": 30126822, "id": 109049467, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTA0OTQ2Nw==", "diff_hunk": "@@ -190,7 +196,11 @@ def forward(fn, input, hx, weight, output, hy):\n         handle = cudnn.get_handle()\n         fn.datatype = cudnn._typemap[input.type()]\n         is_input_packed = fn.batch_sizes is not None\n-\n+        if is_input_packed:\n+            warnings.warn(\n+                \"Warning: persistent algorithm not supported for variable length input.\"\n+                \"Switching to standard\")\n+            fn.persistent = False  # persistent algo is not supported for variable length input", "path": "torch/backends/cudnn/rnn.py", "position": 41, "original_position": 41, "commit_id": "738ebd1be1c0444730a96ce650e85496a0f8444e", "original_commit_id": "738ebd1be1c0444730a96ce650e85496a0f8444e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think both defaulting to non-persistent and picking persistent kernels for batch sizes where we're pretty sure that it's going to be faster are both ok. Do these kernels really have such unpredictable perf?\r\n\r\nWe don't have any function for querying the cc.\r\n\r\nI don't think that the overhead is going to be significant, as far as I know most commonly used sequences are no longer than a few thousand steps, so if we were to use the fixed bin size of 50, we'd have around 100 evaluations. That seems reasonable to me, but I agree that 50 is quite arbitrary and it would be better to design it in a way that better fits the perf characteristics.\r\n\r\nAn example would be to put all sequence lengths for which we're 99% sure that it's slower in a single bin or even exclude that form benchmarks, use smaller bins on the boundary where persistent kernels might be advantageous, and gradually enlarge the bin size once we enter the region where the sequences are very long and we're pretty sure that persistent RNNs will help.", "created_at": "2017-03-30T22:07:54Z", "updated_at": "2018-11-23T15:32:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/1141#discussion_r109049467", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1141", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109049467"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1141#discussion_r109049467"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1141"}}, "body_html": "<p>I think both defaulting to non-persistent and picking persistent kernels for batch sizes where we're pretty sure that it's going to be faster are both ok. Do these kernels really have such unpredictable perf?</p>\n<p>We don't have any function for querying the cc.</p>\n<p>I don't think that the overhead is going to be significant, as far as I know most commonly used sequences are no longer than a few thousand steps, so if we were to use the fixed bin size of 50, we'd have around 100 evaluations. That seems reasonable to me, but I agree that 50 is quite arbitrary and it would be better to design it in a way that better fits the perf characteristics.</p>\n<p>An example would be to put all sequence lengths for which we're 99% sure that it's slower in a single bin or even exclude that form benchmarks, use smaller bins on the boundary where persistent kernels might be advantageous, and gradually enlarge the bin size once we enter the region where the sequences are very long and we're pretty sure that persistent RNNs will help.</p>", "body_text": "I think both defaulting to non-persistent and picking persistent kernels for batch sizes where we're pretty sure that it's going to be faster are both ok. Do these kernels really have such unpredictable perf?\nWe don't have any function for querying the cc.\nI don't think that the overhead is going to be significant, as far as I know most commonly used sequences are no longer than a few thousand steps, so if we were to use the fixed bin size of 50, we'd have around 100 evaluations. That seems reasonable to me, but I agree that 50 is quite arbitrary and it would be better to design it in a way that better fits the perf characteristics.\nAn example would be to put all sequence lengths for which we're 99% sure that it's slower in a single bin or even exclude that form benchmarks, use smaller bins on the boundary where persistent kernels might be advantageous, and gradually enlarge the bin size once we enter the region where the sequences are very long and we're pretty sure that persistent RNNs will help.", "in_reply_to_id": 109042245}