{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109050887", "pull_request_review_id": 30128238, "id": 109050887, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTA1MDg4Nw==", "diff_hunk": "@@ -190,7 +196,11 @@ def forward(fn, input, hx, weight, output, hy):\n         handle = cudnn.get_handle()\n         fn.datatype = cudnn._typemap[input.type()]\n         is_input_packed = fn.batch_sizes is not None\n-\n+        if is_input_packed:\n+            warnings.warn(\n+                \"Warning: persistent algorithm not supported for variable length input.\"\n+                \"Switching to standard\")\n+            fn.persistent = False  # persistent algo is not supported for variable length input", "path": "torch/backends/cudnn/rnn.py", "position": 41, "original_position": 41, "commit_id": "738ebd1be1c0444730a96ce650e85496a0f8444e", "original_commit_id": "738ebd1be1c0444730a96ce650e85496a0f8444e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Damn so even slight changes in the sequence length (e.g. from a power of 2 to sth else) can have a dramatic effect on the perf? Is there really no way to predict that? Maybe we should just roll out our own benchmarks and hardcode the results... How frequent are cuBLAS releases?", "created_at": "2017-03-30T22:16:17Z", "updated_at": "2018-11-23T15:32:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/1141#discussion_r109050887", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1141", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109050887"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1141#discussion_r109050887"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1141"}}, "body_html": "<p>Damn so even slight changes in the sequence length (e.g. from a power of 2 to sth else) can have a dramatic effect on the perf? Is there really no way to predict that? Maybe we should just roll out our own benchmarks and hardcode the results... How frequent are cuBLAS releases?</p>", "body_text": "Damn so even slight changes in the sequence length (e.g. from a power of 2 to sth else) can have a dramatic effect on the perf? Is there really no way to predict that? Maybe we should just roll out our own benchmarks and hardcode the results... How frequent are cuBLAS releases?", "in_reply_to_id": 109042245}