{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2343", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2343/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2343/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2343/events", "html_url": "https://github.com/pytorch/pytorch/issues/2343", "id": 248751413, "node_id": "MDU6SXNzdWUyNDg3NTE0MTM=", "number": 2343, "title": "weight_norm on GRU followed by .cuda causes an assert", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-08-08T15:18:40Z", "updated_at": "2017-08-08T15:18:40Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>I haven't looked into this in depth but this looks fishy:</p>\n<pre><code>import torch.nn as nn\nfrom torch.nn.utils.weight_norm import weight_norm\na = nn.GRU(100, 20)\nb = weight_norm(a, name='weight_hh_l0')\nb = weight_norm(b, name='weight_ih_l0')\nb.cuda()\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 147, in cuda\n    return self._apply(lambda t: t.cuda(device_id))\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.py\", line 116, in _apply\n    self.flatten_parameters()\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.py\", line 107, in flatten_parameters\n    rnn._copyParams(all_weights, params)\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/rnn.py\", line 186, in _copyParams\n    assert param_from.type() == param_to.type()\n</code></pre>\n<p>More here: <a href=\"https://discuss.pytorch.org/t/built-in-weight-norm-on-rnn/5905\" rel=\"nofollow\">https://discuss.pytorch.org/t/built-in-weight-norm-on-rnn/5905</a></p>", "body_text": "I haven't looked into this in depth but this looks fishy:\nimport torch.nn as nn\nfrom torch.nn.utils.weight_norm import weight_norm\na = nn.GRU(100, 20)\nb = weight_norm(a, name='weight_hh_l0')\nb = weight_norm(b, name='weight_ih_l0')\nb.cuda()\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 147, in cuda\n    return self._apply(lambda t: t.cuda(device_id))\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.py\", line 116, in _apply\n    self.flatten_parameters()\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.py\", line 107, in flatten_parameters\n    rnn._copyParams(all_weights, params)\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/rnn.py\", line 186, in _copyParams\n    assert param_from.type() == param_to.type()\n\nMore here: https://discuss.pytorch.org/t/built-in-weight-norm-on-rnn/5905", "body": "I haven't looked into this in depth but this looks fishy:\r\n```\r\nimport torch.nn as nn\r\nfrom torch.nn.utils.weight_norm import weight_norm\r\na = nn.GRU(100, 20)\r\nb = weight_norm(a, name='weight_hh_l0')\r\nb = weight_norm(b, name='weight_ih_l0')\r\nb.cuda()\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 147, in cuda\r\n    return self._apply(lambda t: t.cuda(device_id))\r\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.py\", line 116, in _apply\r\n    self.flatten_parameters()\r\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/nn/modules/rnn.py\", line 107, in flatten_parameters\r\n    rnn._copyParams(all_weights, params)\r\n  File \"/home/didoyang/anaconda2/lib/python2.7/site-packages/torch/backends/cudnn/rnn.py\", line 186, in _copyParams\r\n    assert param_from.type() == param_to.type()\r\n```\r\n\r\nMore here: https://discuss.pytorch.org/t/built-in-weight-norm-on-rnn/5905"}