{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393098019", "html_url": "https://github.com/pytorch/pytorch/issues/7801#issuecomment-393098019", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7801", "id": 393098019, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzA5ODAxOQ==", "user": {"login": "ivankreso", "id": 2056432, "node_id": "MDQ6VXNlcjIwNTY0MzI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2056432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ivankreso", "html_url": "https://github.com/ivankreso", "followers_url": "https://api.github.com/users/ivankreso/followers", "following_url": "https://api.github.com/users/ivankreso/following{/other_user}", "gists_url": "https://api.github.com/users/ivankreso/gists{/gist_id}", "starred_url": "https://api.github.com/users/ivankreso/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ivankreso/subscriptions", "organizations_url": "https://api.github.com/users/ivankreso/orgs", "repos_url": "https://api.github.com/users/ivankreso/repos", "events_url": "https://api.github.com/users/ivankreso/events{/privacy}", "received_events_url": "https://api.github.com/users/ivankreso/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-30T09:40:42Z", "updated_at": "2018-05-30T09:40:42Z", "author_association": "NONE", "body_html": "<p>I don't understand why does it need to sync between GPUs at every checkpoint? The only reason I can think of is sync between BatchNorm mean and variance but that would cause slowdown without checkpointing also.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13488275\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prigoyal\">@prigoyal</a> DenseNet has (BN+Relu+Conv) units which are then wrapped in np.utils.checkpoint. If I got this right checkpointing feature will then recompute BN and Relu during backward pass but not Conv and only the output of every Conv layer is cached (input to BN is not cached because torch.cat is inside checkpoint function in the code below). This looks optimal to me and my model requires ~ 2x less memory with only 10% slower training time.</p>\n<pre><code>def _bn_function_factory(norm, relu, conv):\n    def bn_function(*inputs):\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = conv(relu(norm(concated_features)))\n        return bottleneck_output\n\n    return bn_function\n</code></pre>", "body_text": "I don't understand why does it need to sync between GPUs at every checkpoint? The only reason I can think of is sync between BatchNorm mean and variance but that would cause slowdown without checkpointing also.\n@prigoyal DenseNet has (BN+Relu+Conv) units which are then wrapped in np.utils.checkpoint. If I got this right checkpointing feature will then recompute BN and Relu during backward pass but not Conv and only the output of every Conv layer is cached (input to BN is not cached because torch.cat is inside checkpoint function in the code below). This looks optimal to me and my model requires ~ 2x less memory with only 10% slower training time.\ndef _bn_function_factory(norm, relu, conv):\n    def bn_function(*inputs):\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = conv(relu(norm(concated_features)))\n        return bottleneck_output\n\n    return bn_function", "body": "I don't understand why does it need to sync between GPUs at every checkpoint? The only reason I can think of is sync between BatchNorm mean and variance but that would cause slowdown without checkpointing also.\r\n\r\n@prigoyal DenseNet has (BN+Relu+Conv) units which are then wrapped in np.utils.checkpoint. If I got this right checkpointing feature will then recompute BN and Relu during backward pass but not Conv and only the output of every Conv layer is cached (input to BN is not cached because torch.cat is inside checkpoint function in the code below). This looks optimal to me and my model requires ~ 2x less memory with only 10% slower training time.\r\n\r\n```\r\ndef _bn_function_factory(norm, relu, conv):\r\n    def bn_function(*inputs):\r\n        concated_features = torch.cat(inputs, 1)\r\n        bottleneck_output = conv(relu(norm(concated_features)))\r\n        return bottleneck_output\r\n\r\n    return bn_function\r\n```\r\n"}