{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4540", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4540/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4540/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4540/events", "html_url": "https://github.com/pytorch/pytorch/issues/4540", "id": 286897650, "node_id": "MDU6SXNzdWUyODY4OTc2NTA=", "number": 4540, "title": "Cuda out of memory but gpu memory is utilized about a half", "user": {"login": "SebyakinAndrei", "id": 12896581, "node_id": "MDQ6VXNlcjEyODk2NTgx", "avatar_url": "https://avatars0.githubusercontent.com/u/12896581?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SebyakinAndrei", "html_url": "https://github.com/SebyakinAndrei", "followers_url": "https://api.github.com/users/SebyakinAndrei/followers", "following_url": "https://api.github.com/users/SebyakinAndrei/following{/other_user}", "gists_url": "https://api.github.com/users/SebyakinAndrei/gists{/gist_id}", "starred_url": "https://api.github.com/users/SebyakinAndrei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SebyakinAndrei/subscriptions", "organizations_url": "https://api.github.com/users/SebyakinAndrei/orgs", "repos_url": "https://api.github.com/users/SebyakinAndrei/repos", "events_url": "https://api.github.com/users/SebyakinAndrei/events{/privacy}", "received_events_url": "https://api.github.com/users/SebyakinAndrei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-01-08T21:44:22Z", "updated_at": "2018-02-10T11:24:32Z", "closed_at": "2018-02-10T11:24:31Z", "author_association": "NONE", "body_html": "<p>I'm running slightly modified code of EDSR baseline network from here: <a href=\"https://github.com/thstkdgus35/EDSR-PyTorch\">https://github.com/thstkdgus35/EDSR-PyTorch</a><br>\nThe training on first epoch goes well, but on the test stage when loading a model it raises an error:</p>\n<pre><code>THCudaCheck FAIL file=d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu line=58 error=2 : out of memory\nTraceback (most recent call last):\n  File \"main.py\", line 17, in &lt;module&gt;\n    t.test()\n  File \"D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\trainer.py\", line 100, in test\n    output = _test_forward(input)\n  File \"D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\trainer.py\", line 91, in _test_forward\n    return self.model(x)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\model\\EDSR.py\", line 50, in forward\n    x = self.tail(res)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 67, in forward\n    input = module(input)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 67, in forward\n    input = module(input)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 277, in forward\n    self.padding, self.dilation, self.groups)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\", line 90, in conv2d\n    return f(input, weight, bias)\nRuntimeError: cuda runtime error (2) : out of memory at d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu:58\n</code></pre>\n<p>But what is strange is that gpu memory at that moment is utilised about a half. Here's nvidia-smi output (at the moment of peak memory usage):</p>\n<pre><code>C:\\Program Files\\NVIDIA Corporation\\NVSMI&gt;nvidia-smi -i 0 -q -d MEMORY\n\n==============NVSMI LOG==============\n\nTimestamp                           : Mon Jan 08 23:49:46 2018\nDriver Version                      : 388.13\n\nAttached GPUs                       : 1\nGPU 00000000:02:00.0\n    FB Memory Usage\n        Total                       : 6144 MiB\n        Used                        : 3657 MiB\n        Free                        : 2487 MiB\n    BAR1 Memory Usage\n        Total                       : 256 MiB\n        Used                        : 229 MiB\n        Free                        : 27 MiB\n</code></pre>\n<p>In test function I also use volatile=True option. What could be the problem?<br>\nOS: Win10<br>\nGPU: 1060 6G</p>", "body_text": "I'm running slightly modified code of EDSR baseline network from here: https://github.com/thstkdgus35/EDSR-PyTorch\nThe training on first epoch goes well, but on the test stage when loading a model it raises an error:\nTHCudaCheck FAIL file=d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu line=58 error=2 : out of memory\nTraceback (most recent call last):\n  File \"main.py\", line 17, in <module>\n    t.test()\n  File \"D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\trainer.py\", line 100, in test\n    output = _test_forward(input)\n  File \"D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\trainer.py\", line 91, in _test_forward\n    return self.model(x)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\model\\EDSR.py\", line 50, in forward\n    x = self.tail(res)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 67, in forward\n    input = module(input)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 67, in forward\n    input = module(input)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 277, in forward\n    self.padding, self.dilation, self.groups)\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\", line 90, in conv2d\n    return f(input, weight, bias)\nRuntimeError: cuda runtime error (2) : out of memory at d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu:58\n\nBut what is strange is that gpu memory at that moment is utilised about a half. Here's nvidia-smi output (at the moment of peak memory usage):\nC:\\Program Files\\NVIDIA Corporation\\NVSMI>nvidia-smi -i 0 -q -d MEMORY\n\n==============NVSMI LOG==============\n\nTimestamp                           : Mon Jan 08 23:49:46 2018\nDriver Version                      : 388.13\n\nAttached GPUs                       : 1\nGPU 00000000:02:00.0\n    FB Memory Usage\n        Total                       : 6144 MiB\n        Used                        : 3657 MiB\n        Free                        : 2487 MiB\n    BAR1 Memory Usage\n        Total                       : 256 MiB\n        Used                        : 229 MiB\n        Free                        : 27 MiB\n\nIn test function I also use volatile=True option. What could be the problem?\nOS: Win10\nGPU: 1060 6G", "body": "I'm running slightly modified code of EDSR baseline network from here: https://github.com/thstkdgus35/EDSR-PyTorch\r\nThe training on first epoch goes well, but on the test stage when loading a model it raises an error:\r\n```\r\nTHCudaCheck FAIL file=d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu line=58 error=2 : out of memory\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 17, in <module>\r\n    t.test()\r\n  File \"D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\trainer.py\", line 100, in test\r\n    output = _test_forward(input)\r\n  File \"D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\trainer.py\", line 91, in _test_forward\r\n    return self.model(x)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"D:\\Projects\\SR\\EDSR-PyTorch-master\\code\\model\\EDSR.py\", line 50, in forward\r\n    x = self.tail(res)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 67, in forward\r\n    input = module(input)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 67, in forward\r\n    input = module(input)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 277, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File \"C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\", line 90, in conv2d\r\n    return f(input, weight, bias)\r\nRuntimeError: cuda runtime error (2) : out of memory at d:\\pytorch\\pytorch\\torch\\lib\\thc\\generic/THCStorage.cu:58\r\n```\r\nBut what is strange is that gpu memory at that moment is utilised about a half. Here's nvidia-smi output (at the moment of peak memory usage):\r\n```\r\nC:\\Program Files\\NVIDIA Corporation\\NVSMI>nvidia-smi -i 0 -q -d MEMORY\r\n\r\n==============NVSMI LOG==============\r\n\r\nTimestamp                           : Mon Jan 08 23:49:46 2018\r\nDriver Version                      : 388.13\r\n\r\nAttached GPUs                       : 1\r\nGPU 00000000:02:00.0\r\n    FB Memory Usage\r\n        Total                       : 6144 MiB\r\n        Used                        : 3657 MiB\r\n        Free                        : 2487 MiB\r\n    BAR1 Memory Usage\r\n        Total                       : 256 MiB\r\n        Used                        : 229 MiB\r\n        Free                        : 27 MiB\r\n```\r\nIn test function I also use volatile=True option. What could be the problem?\r\nOS: Win10\r\nGPU: 1060 6G\r\n\r\n\r\n  \r\n  "}