{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/401831514", "html_url": "https://github.com/pytorch/pytorch/issues/9066#issuecomment-401831514", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9066", "id": 401831514, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTgzMTUxNA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-02T14:50:48Z", "updated_at": "2018-07-02T14:50:48Z", "author_association": "MEMBER", "body_html": "<p>Yeah. We never trace the internals of <code>IndexBackward</code> or <code>IndexBackwardBackward</code>. We should just have a JIT pass that merges that viewing ops in the forward (and so their backward will get fused automatically, even if it's going to be differentiated by autograd). If you care about this optimization for 2nd order derivatives, then we'd need to add symbolic AD formulas for viewing ops, but since their backward involves mutation, we'd need to add that to the JIT IR first.</p>", "body_text": "Yeah. We never trace the internals of IndexBackward or IndexBackwardBackward. We should just have a JIT pass that merges that viewing ops in the forward (and so their backward will get fused automatically, even if it's going to be differentiated by autograd). If you care about this optimization for 2nd order derivatives, then we'd need to add symbolic AD formulas for viewing ops, but since their backward involves mutation, we'd need to add that to the JIT IR first.", "body": "Yeah. We never trace the internals of `IndexBackward` or `IndexBackwardBackward`. We should just have a JIT pass that merges that viewing ops in the forward (and so their backward will get fused automatically, even if it's going to be differentiated by autograd). If you care about this optimization for 2nd order derivatives, then we'd need to add symbolic AD formulas for viewing ops, but since their backward involves mutation, we'd need to add that to the JIT IR first."}