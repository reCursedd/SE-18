{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4076", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4076/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4076/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4076/events", "html_url": "https://github.com/pytorch/pytorch/issues/4076", "id": 280241524, "node_id": "MDU6SXNzdWUyODAyNDE1MjQ=", "number": 4076, "title": "Unexpected behavior when using max() with cuda.ByteTensor", "user": {"login": "Feynman27", "id": 12432112, "node_id": "MDQ6VXNlcjEyNDMyMTEy", "avatar_url": "https://avatars1.githubusercontent.com/u/12432112?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Feynman27", "html_url": "https://github.com/Feynman27", "followers_url": "https://api.github.com/users/Feynman27/followers", "following_url": "https://api.github.com/users/Feynman27/following{/other_user}", "gists_url": "https://api.github.com/users/Feynman27/gists{/gist_id}", "starred_url": "https://api.github.com/users/Feynman27/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Feynman27/subscriptions", "organizations_url": "https://api.github.com/users/Feynman27/orgs", "repos_url": "https://api.github.com/users/Feynman27/repos", "events_url": "https://api.github.com/users/Feynman27/events{/privacy}", "received_events_url": "https://api.github.com/users/Feynman27/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-07T19:01:19Z", "updated_at": "2018-02-15T22:41:20Z", "closed_at": "2018-02-15T22:41:20Z", "author_association": "NONE", "body_html": "<p>Cross-ref from <a href=\"https://discuss.pytorch.org/t/unexpected-behavior-when-using-max-with-cuda-bytetensor/10901\" rel=\"nofollow\">https://discuss.pytorch.org/t/unexpected-behavior-when-using-max-with-cuda-bytetensor/10901</a>:</p>\n<p>I'm seeing some unexpected behavior using a <code>cuda.ByteTensor</code> with <code>tensor.max()</code>.  I'm creating a binary mask from 2 <code>cuda.FloatTensors</code> (y,x) using <code>y==x</code>. This works fine. But when trying to retrieve the argmax using something like <code>(y==x).max(dim=1)[1]</code>, it only returns the expected behavior on a cpu. I can only get the expected result on the gpu by converting the <code>cuda.ByteTensor</code> to a <code>cuda.FloatTensor</code> before calling the <code>max</code> operator: <code>(y==x).type(cuda.FloatTensor).max(dim=1)[1]</code>. Below I've provided a minimal example.</p>\n<div class=\"highlight highlight-source-python\"><pre>Python <span class=\"pl-c1\">2.7</span>.13 <span class=\"pl-k\">|</span>Continuum Analytics, Inc.<span class=\"pl-k\">|</span> (default, Dec <span class=\"pl-c1\">20</span> <span class=\"pl-c1\">2016</span>, <span class=\"pl-c1\">23</span>:<span class=\"pl-c1\">0<span class=\"pl-ii\">9</span></span>:<span class=\"pl-c1\">15</span>) \n[<span class=\"pl-c1\">GCC</span> <span class=\"pl-c1\">4.4</span>.7 <span class=\"pl-c1\">20120313</span> (Red Hat <span class=\"pl-c1\">4.4</span>.7<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)] on linux2\nType <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>help<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>copyright<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>credits<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>license<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">for</span> more information.\nAnaconda <span class=\"pl-k\">is</span> brought to you by Continuum Analytics.\nPlease check out: http:<span class=\"pl-k\">//</span>continuum.io<span class=\"pl-k\">/</span>thanks <span class=\"pl-k\">and</span> https:<span class=\"pl-k\">//</span>anaconda.org\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">print</span>(torch.<span class=\"pl-c1\">__version__</span>)\n<span class=\"pl-c1\">0.2</span>.0_4\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.Tensor( [ [<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>] ]).cuda()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">type</span>(x)\n<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.cuda.FloatTensor<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> b_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">==</span> x\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">type</span>(b_x)\n<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.cuda.ByteTensor<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> b_x\n\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n[torch.cuda.ByteTensor of size <span class=\"pl-ii\">5x1</span> (<span class=\"pl-c1\">GPU</span> <span class=\"pl-c1\">0</span>)]\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> b_x.max(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n(\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n[torch.cuda.ByteTensor of size <span class=\"pl-c1\">5</span> (<span class=\"pl-c1\">GPU</span> <span class=\"pl-c1\">0</span>)]\n, \n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">1</span>\n[torch.cuda.LongTensor of size <span class=\"pl-c1\">5</span> (<span class=\"pl-c1\">GPU</span> <span class=\"pl-c1\">0</span>)]\n)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.Tensor( [ [<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>] ])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">type</span>(x)\n<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.FloatTensor<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> b_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">==</span> x\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">type</span>(b_x)\n<span class=\"pl-k\">&lt;</span><span class=\"pl-k\">class</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.ByteTensor<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> b_x.max(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n(\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n[torch.ByteTensor of size <span class=\"pl-c1\">5</span>]\n, \n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n[torch.LongTensor of size <span class=\"pl-c1\">5</span>]\n)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.Tensor( [ [<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>] ]).cuda()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> b_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">==</span> x\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> b_x.type(torch.cuda.FloatTensor).max(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n(\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n[torch.cuda.FloatTensor of size <span class=\"pl-c1\">5</span> (<span class=\"pl-c1\">GPU</span> <span class=\"pl-c1\">0</span>)]\n, \n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n <span class=\"pl-c1\">0</span>\n[torch.cuda.LongTensor of size <span class=\"pl-c1\">5</span> (<span class=\"pl-c1\">GPU</span> <span class=\"pl-c1\">0</span>)]\n)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> </pre></div>", "body_text": "Cross-ref from https://discuss.pytorch.org/t/unexpected-behavior-when-using-max-with-cuda-bytetensor/10901:\nI'm seeing some unexpected behavior using a cuda.ByteTensor with tensor.max().  I'm creating a binary mask from 2 cuda.FloatTensors (y,x) using y==x. This works fine. But when trying to retrieve the argmax using something like (y==x).max(dim=1)[1], it only returns the expected behavior on a cpu. I can only get the expected result on the gpu by converting the cuda.ByteTensor to a cuda.FloatTensor before calling the max operator: (y==x).type(cuda.FloatTensor).max(dim=1)[1]. Below I've provided a minimal example.\nPython 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:09:15) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n>>> import torch\n>>> print(torch.__version__)\n0.2.0_4\n>>> x = torch.Tensor( [ [0], [1], [1], [0], [0] ]).cuda()\n>>> type(x)\n<class 'torch.cuda.FloatTensor'>\n>>> b_x = 1 == x\n>>> type(b_x)\n<class 'torch.cuda.ByteTensor'>\n>>> b_x\n\n 0\n 1\n 1\n 0\n 0\n[torch.cuda.ByteTensor of size 5x1 (GPU 0)]\n\n>>> b_x.max(dim=1)\n(\n 0\n 1\n 1\n 0\n 0\n[torch.cuda.ByteTensor of size 5 (GPU 0)]\n, \n 1\n 0\n 0\n 1\n 1\n[torch.cuda.LongTensor of size 5 (GPU 0)]\n)\n>>> x = torch.Tensor( [ [0], [1], [1], [0], [0] ])\n>>> type(x)\n<class 'torch.FloatTensor'>\n>>> b_x = 1 == x\n>>> type(b_x)\n<class 'torch.ByteTensor'>\n>>> b_x.max(dim=1)\n(\n 0\n 1\n 1\n 0\n 0\n[torch.ByteTensor of size 5]\n, \n 0\n 0\n 0\n 0\n 0\n[torch.LongTensor of size 5]\n)\n>>> x = torch.Tensor( [ [0], [1], [1], [0], [0] ]).cuda()\n>>> b_x = 1 == x\n>>> b_x.type(torch.cuda.FloatTensor).max(dim=1)\n(\n 0\n 1\n 1\n 0\n 0\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\n, \n 0\n 0\n 0\n 0\n 0\n[torch.cuda.LongTensor of size 5 (GPU 0)]\n)\n>>>", "body": "Cross-ref from [https://discuss.pytorch.org/t/unexpected-behavior-when-using-max-with-cuda-bytetensor/10901](https://discuss.pytorch.org/t/unexpected-behavior-when-using-max-with-cuda-bytetensor/10901):\r\n\r\nI'm seeing some unexpected behavior using a ```cuda.ByteTensor``` with ```tensor.max()```.  I'm creating a binary mask from 2 ```cuda.FloatTensors``` (y,x) using ```y==x```. This works fine. But when trying to retrieve the argmax using something like ```(y==x).max(dim=1)[1]```, it only returns the expected behavior on a cpu. I can only get the expected result on the gpu by converting the ```cuda.ByteTensor``` to a ```cuda.FloatTensor``` before calling the ```max``` operator: ```(y==x).type(cuda.FloatTensor).max(dim=1)[1]```. Below I've provided a minimal example.  \r\n```python\r\nPython 2.7.13 |Continuum Analytics, Inc.| (default, Dec 20 2016, 23:09:15) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nAnaconda is brought to you by Continuum Analytics.\r\nPlease check out: http://continuum.io/thanks and https://anaconda.org\r\n>>> import torch\r\n>>> print(torch.__version__)\r\n0.2.0_4\r\n>>> x = torch.Tensor( [ [0], [1], [1], [0], [0] ]).cuda()\r\n>>> type(x)\r\n<class 'torch.cuda.FloatTensor'>\r\n>>> b_x = 1 == x\r\n>>> type(b_x)\r\n<class 'torch.cuda.ByteTensor'>\r\n>>> b_x\r\n\r\n 0\r\n 1\r\n 1\r\n 0\r\n 0\r\n[torch.cuda.ByteTensor of size 5x1 (GPU 0)]\r\n\r\n>>> b_x.max(dim=1)\r\n(\r\n 0\r\n 1\r\n 1\r\n 0\r\n 0\r\n[torch.cuda.ByteTensor of size 5 (GPU 0)]\r\n, \r\n 1\r\n 0\r\n 0\r\n 1\r\n 1\r\n[torch.cuda.LongTensor of size 5 (GPU 0)]\r\n)\r\n>>> x = torch.Tensor( [ [0], [1], [1], [0], [0] ])\r\n>>> type(x)\r\n<class 'torch.FloatTensor'>\r\n>>> b_x = 1 == x\r\n>>> type(b_x)\r\n<class 'torch.ByteTensor'>\r\n>>> b_x.max(dim=1)\r\n(\r\n 0\r\n 1\r\n 1\r\n 0\r\n 0\r\n[torch.ByteTensor of size 5]\r\n, \r\n 0\r\n 0\r\n 0\r\n 0\r\n 0\r\n[torch.LongTensor of size 5]\r\n)\r\n>>> x = torch.Tensor( [ [0], [1], [1], [0], [0] ]).cuda()\r\n>>> b_x = 1 == x\r\n>>> b_x.type(torch.cuda.FloatTensor).max(dim=1)\r\n(\r\n 0\r\n 1\r\n 1\r\n 0\r\n 0\r\n[torch.cuda.FloatTensor of size 5 (GPU 0)]\r\n, \r\n 0\r\n 0\r\n 0\r\n 0\r\n 0\r\n[torch.cuda.LongTensor of size 5 (GPU 0)]\r\n)\r\n>>> \r\n```\r\n"}