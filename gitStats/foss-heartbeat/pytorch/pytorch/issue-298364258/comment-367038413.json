{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367038413", "html_url": "https://github.com/pytorch/pytorch/issues/5301#issuecomment-367038413", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5301", "id": 367038413, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzAzODQxMw==", "user": {"login": "mxlei01", "id": 7399324, "node_id": "MDQ6VXNlcjczOTkzMjQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/7399324?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mxlei01", "html_url": "https://github.com/mxlei01", "followers_url": "https://api.github.com/users/mxlei01/followers", "following_url": "https://api.github.com/users/mxlei01/following{/other_user}", "gists_url": "https://api.github.com/users/mxlei01/gists{/gist_id}", "starred_url": "https://api.github.com/users/mxlei01/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mxlei01/subscriptions", "organizations_url": "https://api.github.com/users/mxlei01/orgs", "repos_url": "https://api.github.com/users/mxlei01/repos", "events_url": "https://api.github.com/users/mxlei01/events{/privacy}", "received_events_url": "https://api.github.com/users/mxlei01/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-20T16:40:48Z", "updated_at": "2018-02-20T16:40:48Z", "author_association": "NONE", "body_html": "<p>This is all of the output:</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-56-bea3acbf0a84&gt; in &lt;module&gt;()\n     32         for feature, label in data.DataLoader(get_concat_dataset_dnn(\"train\"),\n     33                                               batch_size=dnn_minibatch_size, shuffle=True, collate_fn=transform_variable_dnn,\n---&gt; 34                                               num_workers=1, pin_memory=pin_memory):\n     35             # Step 1. Pytorch accumulates gradients, need to clear them out before each instance.\n     36             dnn.zero_grad()\n\n~/python3_virtualenv/python3_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    273         while True:\n    274             assert (not self.shutdown and self.batches_outstanding &gt; 0)\n--&gt; 275             idx, batch = self._get_batch()\n    276             self.batches_outstanding -= 1\n    277             if idx != self.rcvd_idx:\n\n~/python3_virtualenv/python3_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _get_batch(self)\n    252                 raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\n    253         else:\n--&gt; 254             return self.data_queue.get()\n    255 \n    256     def __next__(self):\n\n/usr/lib/python3.6/queue.py in get(self, block, timeout)\n    162             elif timeout is None:\n    163                 while not self._qsize():\n--&gt; 164                     self.not_empty.wait()\n    165             elif timeout &lt; 0:\n    166                 raise ValueError(\"'timeout' must be a non-negative number\")\n\n/usr/lib/python3.6/threading.py in wait(self, timeout)\n    293         try:    # restore state no matter what (e.g., KeyboardInterrupt)\n    294             if timeout is None:\n--&gt; 295                 waiter.acquire()\n    296                 gotit = True\n    297             else:\n\n~/python3_virtualenv/python3_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py in handler(signum, frame)\n    173         # This following call uses `waitid` with WNOHANG from C side. Therefore,\n    174         # Python can still get and update the process status successfully.\n--&gt; 175         _error_if_any_worker_fails()\n    176         if previous_handler is not None:\n    177             previous_handler(signum, frame)\n\nRuntimeError: DataLoader worker (pid 39637) exited unexpectedly with exit code 1.\n</code></pre>\n<p>The amount of data in each line is less than 1KB.</p>", "body_text": "This is all of the output:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-56-bea3acbf0a84> in <module>()\n     32         for feature, label in data.DataLoader(get_concat_dataset_dnn(\"train\"),\n     33                                               batch_size=dnn_minibatch_size, shuffle=True, collate_fn=transform_variable_dnn,\n---> 34                                               num_workers=1, pin_memory=pin_memory):\n     35             # Step 1. Pytorch accumulates gradients, need to clear them out before each instance.\n     36             dnn.zero_grad()\n\n~/python3_virtualenv/python3_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    273         while True:\n    274             assert (not self.shutdown and self.batches_outstanding > 0)\n--> 275             idx, batch = self._get_batch()\n    276             self.batches_outstanding -= 1\n    277             if idx != self.rcvd_idx:\n\n~/python3_virtualenv/python3_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _get_batch(self)\n    252                 raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\n    253         else:\n--> 254             return self.data_queue.get()\n    255 \n    256     def __next__(self):\n\n/usr/lib/python3.6/queue.py in get(self, block, timeout)\n    162             elif timeout is None:\n    163                 while not self._qsize():\n--> 164                     self.not_empty.wait()\n    165             elif timeout < 0:\n    166                 raise ValueError(\"'timeout' must be a non-negative number\")\n\n/usr/lib/python3.6/threading.py in wait(self, timeout)\n    293         try:    # restore state no matter what (e.g., KeyboardInterrupt)\n    294             if timeout is None:\n--> 295                 waiter.acquire()\n    296                 gotit = True\n    297             else:\n\n~/python3_virtualenv/python3_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py in handler(signum, frame)\n    173         # This following call uses `waitid` with WNOHANG from C side. Therefore,\n    174         # Python can still get and update the process status successfully.\n--> 175         _error_if_any_worker_fails()\n    176         if previous_handler is not None:\n    177             previous_handler(signum, frame)\n\nRuntimeError: DataLoader worker (pid 39637) exited unexpectedly with exit code 1.\n\nThe amount of data in each line is less than 1KB.", "body": "This is all of the output:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-56-bea3acbf0a84> in <module>()\r\n     32         for feature, label in data.DataLoader(get_concat_dataset_dnn(\"train\"),\r\n     33                                               batch_size=dnn_minibatch_size, shuffle=True, collate_fn=transform_variable_dnn,\r\n---> 34                                               num_workers=1, pin_memory=pin_memory):\r\n     35             # Step 1. Pytorch accumulates gradients, need to clear them out before each instance.\r\n     36             dnn.zero_grad()\r\n\r\n~/python3_virtualenv/python3_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    273         while True:\r\n    274             assert (not self.shutdown and self.batches_outstanding > 0)\r\n--> 275             idx, batch = self._get_batch()\r\n    276             self.batches_outstanding -= 1\r\n    277             if idx != self.rcvd_idx:\r\n\r\n~/python3_virtualenv/python3_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _get_batch(self)\r\n    252                 raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\r\n    253         else:\r\n--> 254             return self.data_queue.get()\r\n    255 \r\n    256     def __next__(self):\r\n\r\n/usr/lib/python3.6/queue.py in get(self, block, timeout)\r\n    162             elif timeout is None:\r\n    163                 while not self._qsize():\r\n--> 164                     self.not_empty.wait()\r\n    165             elif timeout < 0:\r\n    166                 raise ValueError(\"'timeout' must be a non-negative number\")\r\n\r\n/usr/lib/python3.6/threading.py in wait(self, timeout)\r\n    293         try:    # restore state no matter what (e.g., KeyboardInterrupt)\r\n    294             if timeout is None:\r\n--> 295                 waiter.acquire()\r\n    296                 gotit = True\r\n    297             else:\r\n\r\n~/python3_virtualenv/python3_env/lib/python3.6/site-packages/torch/utils/data/dataloader.py in handler(signum, frame)\r\n    173         # This following call uses `waitid` with WNOHANG from C side. Therefore,\r\n    174         # Python can still get and update the process status successfully.\r\n--> 175         _error_if_any_worker_fails()\r\n    176         if previous_handler is not None:\r\n    177             previous_handler(signum, frame)\r\n\r\nRuntimeError: DataLoader worker (pid 39637) exited unexpectedly with exit code 1.\r\n```\r\n\r\nThe amount of data in each line is less than 1KB."}