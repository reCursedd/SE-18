{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13070", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13070/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13070/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13070/events", "html_url": "https://github.com/pytorch/pytorch/pull/13070", "id": 373654981, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI1NTMzMTUz", "number": 13070, "title": "Refactor Random Number Generators in ATen", "user": {"login": "syed-ahmed", "id": 8906225, "node_id": "MDQ6VXNlcjg5MDYyMjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/8906225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syed-ahmed", "html_url": "https://github.com/syed-ahmed", "followers_url": "https://api.github.com/users/syed-ahmed/followers", "following_url": "https://api.github.com/users/syed-ahmed/following{/other_user}", "gists_url": "https://api.github.com/users/syed-ahmed/gists{/gist_id}", "starred_url": "https://api.github.com/users/syed-ahmed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syed-ahmed/subscriptions", "organizations_url": "https://api.github.com/users/syed-ahmed/orgs", "repos_url": "https://api.github.com/users/syed-ahmed/repos", "events_url": "https://api.github.com/users/syed-ahmed/events{/privacy}", "received_events_url": "https://api.github.com/users/syed-ahmed/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-24T20:04:59Z", "updated_at": "2018-11-23T15:55:21Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/13070", "html_url": "https://github.com/pytorch/pytorch/pull/13070", "diff_url": "https://github.com/pytorch/pytorch/pull/13070.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/13070.patch"}, "body_html": "<h2>Summary:</h2>\n<p>The purpose of this PR is to refactor Random Number Generator (RNG) design in ATen. Currently, RNGs in PyTorch has an assymetrical design, i.e. CPU Generators use an ATen class, whereas CUDA Generators use legacy THC code (<code>THCRNGState, THCState, THCRandom_Init</code> etc.). Moreover, the concept of generators in ATen aren't clear from its current design. This PR does the following:</p>\n<ul>\n<li>Clarifies generator concepts and creates a unified generator class that can serve multiple backends (CPU, CUDA etc.).</li>\n<li>Refactors CUDA kernels with ATen generator logic and removes THC generator code.</li>\n<li>Refactors CPU generator to use <code>std::mt19937_64</code> instead of custom mt19937 in TH.</li>\n<li>Removes <code>curandStateMTGP32</code> and <code>curandStatePhilox4_32_10</code> and replaces with native Philox4_32_10 implementation.</li>\n<li>Misc:\n<ul>\n<li>Fixes bug in <code>incr_n</code> function of Philox engine from fusion compiler (bug also exists currently in curand) where <code>nhi</code> wasn't being checked for overflow.</li>\n<li>Fixes improper scaling of random number to produce uniform distribution in fusion compiler (i.e. random numbers were sampled from <code>{0-2^24}</code> and then divided by 2^24 to avoid bias that was caused by dividing with 2^32).</li>\n<li>Fixes uses of arbitrary increment number for <code>next_philox_seed</code> in <code>Distributions.cu</code> with proper philox offset calculation.</li>\n<li>Fixes hardcoded generator related code in several python files used for code generation, such as <code>function_wrapper.py</code> etc.</li>\n<li>Fixes generator front-end python bindings to include device kwarg and removes <code>default_generator</code> python module.</li>\n<li>Removes creation of generator from Types.</li>\n<li>Updates documentations and comments and adds documentation for <code>torch.Generator</code> api.</li>\n</ul>\n</li>\n</ul>\n<h2>Guide for Reviewers</h2>\n<p>Unfortunately, this PR touches more files than it originally planned on. Hence, following is a small guide on what I think should help you review the ~100 files in this PR.</p>\n<h4><em>Definitions</em></h4>\n<ul>\n<li><strong>Generator</strong>: An object which manages the state of the algorithm that produces pseudo random numbers. For instance, Generator class in ATen is responsible for handling seeds, creation of generators, etc.</li>\n<li><strong>Engine</strong>: An object which contains the implementation of the algorithm that produces pseudo random numbers. For instance, for the CPU side, we are using a 64 bit Mersenne Twister (<code>std::mt19937_64</code>) and for the CUDA side we are using a Philox Engine (<code>Philox4_32_10</code>).</li>\n<li><strong>CUDA Grids, Blocks etc.</strong>: <a href=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy\" rel=\"nofollow\">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy</a></li>\n</ul>\n<h4>Current State of Generators</h4>\n<h5><em>What is the deal with <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/templates/GeneratorDerived.h\">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/templates/GeneratorDerived.h</a>?</em></h5>\n<ul>\n<li>This template file takes in the <code>$</code> variables from <code>gen.py</code> to produce <code>CPUGenerator.h</code> and <code>CUDAGenerator.h</code>. This file forces the two sides (CPU and CUDA) to have the same API, however, we have a contradiction when we pass <code>THGenerator</code> as a member only for the CPU side, but it is not present in the CUDAGenerator. Then it begs the question that, if we are to diverge from these two sides to be same, why not modify on two separate files, <code>CPUGenerator.h</code> and <code>CUDAGenerator.h</code>, rather than modifying these two files through a complicated <code>gen.py</code> script. <strong>tl-dr</strong>: I have removed this logic and it turns out we don't need to maintain two Generator headers since, we have defined an engine and a generator to be independent objects.</li>\n</ul>\n<h5><em>Generator State vs Generator</em></h5>\n<ul>\n<li>CPUGenerator in ATen currently has a <code>THGenerator</code> object. This <code>THGenerator</code> objects are used as inputs to some kernels in <code>Declarations.cwrap</code>. In addition, <code>function_wrapper.py</code> processes these declarations from what it looks like - swapping <code>THGenerator</code> with <code>Generator</code>. As a result of this mix-match, <code>THGenerator</code> and <code>CPUGenerator</code> are basically treated as the same thing (this was also noted somewhere in one of the linked issues).</li>\n<li><code>CUDAGenerator</code> in ATen doesn't have the funky replacement of <code>THCGenerator</code> with Generator. Instead, <code>function_wrapper.py</code> and <code>ProcessorSpecificPlugin.py</code> processes the <code>Declarations.cwrap</code> to remove <code>Generator</code> input in function signatures for the CUDA side. As a result, <code>CUDAGenerator.cpp</code> just makes direct calls to <code>THCRandom</code> functions and doesn't really serve a purpose.</li>\n<li>In TH/THC, the design is consistent. That is, for the CPU side, there is <code>THGenerator</code> and for the CUDA side, there is <code>THCGenerator</code>. Both of these structs have the same definition as how we are defining what a Generator is. In addition, both of these structs only sets/gets things for their engines. Currently TH/THC both uses Mersenne Twister engines by default. TH uses custom implementation of mersenne twister and THC initializes <code>curandStateMTGP32</code>. Some kernels in ATen, uses <code>curandStatePhilox4_32_10</code> while using the <code>THCRandom_getGenerator()</code> to feed the <code>THCGenerator</code> and use it in the <code>curandStatePhilox4_32_10</code> - (uses seed and philox_seed_offset from that struct). <strong>tl-dr</strong>: In this PR, I have removed all usage of <code>THCGenerator</code> and <code>THGenerator</code> and tried to improve the symmetry of the design for both sides.</li>\n</ul>\n<h5><em>Generator registry, default generators, torch.Generator api and how front-end calls the backend</em></h5>\n<ul>\n<li>Currently <code>Context.h</code> creates a <code>generator_registry</code> which is an array of unique pointers to <code>Generator</code> objects (the size of which is equivalent the number of <code>COMPILE_TIME_MAX_DEVICE_TYPES</code> from <code>DeviceType.h</code>. There is one for CUDA, initialized through the <code>initCUDAGenerator</code> in <code>CUDAHooks</code>, there is one for CPU, initialized in <code>Context.cpp</code>. This <code>generator_registry</code> is meant to hold singletons, i.e. the default generator which is used by apis like <code>torch.randn</code>, <code>torch.randperm</code> etc. It is to be noted, there is a need for having default generators if a user intends to use the running state of an engine throughout their application. Currently, a default generator is instantiated in the front end for the CPU side (residing in <code>torch/csrc/Module.cpp</code>) and is added as <code>default_generator</code> python module, which is then imported in <code>torch/random.py</code>. For the cuda side, there is no module like this and we are just calling <code>THCRandom</code> functions as methods of the cuda module (torch/csrc/cuda/Module.cpp) which are used in <code>torch/cuda/random.py</code>. Lastly, <code>Generator.h</code>, <code>Generator.cpp</code> in <code>torch/csrc/</code> creates a <code>torch.Generator</code> module, which gives the user the ability to create a Generator object other than the default generator. The purpose of this api seems to give the user the ability to \"fork\" RNG states. That is, if someone wanted to run their <code>torch.randn</code> function without incrementing the state of the <code>default_generator</code>, they could do that by using a <code>torch.Generator</code> object as an input to the function. <strong>tl-dr</strong>: In this PR, I extended the functionality of the <code>Generator.cpp</code> module to have a <code>device</code> keyword and <code>default</code> keyword, such that creation of default/non-default generators for different device and their backends can happen through the same python module (rather than doing funky <code>default_generator</code> module and also releasing a unique pointer through Types in <code>torch/csrc/Generator.cpp</code>).</li>\n</ul>\n<h5><em>Philox Offset Calculation</em></h5>\n<ul>\n<li><code>Distributions.cu</code> uses arbitrary (may be found through experimentation?) numbers like 20 and 10, when incrementing <code>philox_seed_offset</code>, however, there is way to calculate this number before launching a kernel, by figuring out the number of elements per thread using the grid size, block size and loop unrolling number (as demonstrated by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> in <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Dropout.cu#L108\">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Dropout.cu#L108</a>). The kernels in <code>Distributions.cu</code> utilize <code>CUDApplyUtils.cuh</code> which does loop unrolling through the <code>step</code> variable and and tries to ensure that a kernel works on one element per thread. However, for <code>CUDA_VERSION &lt; 9000</code>, <code>CUDApplyUtils.cuh</code> adjusts <code>grid_size</code> to run with full occupancy (check the <code>grid.x</code> assignment in that file) and hence, runs a kernel in a grid-stride manner (that is, it's not one element per thread). Hence, an arbitrary philox offset increment could mean that the kernels in <code>Distribution.cu</code> might be broken, however, that might not necessarily be the case if the increment is high enough (but then you might ask how high is high enough)? Hence, <strong>tl-dr</strong>: In this PR, I included a <code>incrementPhiloxOffset</code> function which calculates an increment number given some inputs and I use this function in all the kernels requiring the offset increment.</li>\n</ul>\n<h5><em>Mersenne Twister concerns</em></h5>\n<p>Mersenne twister engine has 19937 states. [This article](- <a href=\"http://www.pcg-random.org/posts/cpp-seeding-surprises.html\" rel=\"nofollow\">http://www.pcg-random.org/posts/cpp-seeding-surprises.html</a>) explains why it's a bad idea to seed a mersenne twister with a single 32 bit. <strong>tl-dr</strong>:  In this PR, I have kept the same functionality as was before, seeding a Mersenne twister engine but from the std library. The bias problems of seeding this engine still remains.</p>\n<h4>Core Files to Review</h4>\n<ul>\n<li><code>aten/src/ATen/core/Generator.h</code>: Re-defines backend API for Generators. Unifies CPU and CUDA Generator structs, i.e. <code>GeneratorState</code> is equivalent to the union of <code>THCGenerator</code> and <code>THGenerator</code>.</li>\n<li><code>aten/src/ATen/core/Generator.cpp</code>: Implements Generator.h.</li>\n<li><code>aten/src/ATen/cuda/PhiloxRNGEngine.h</code>: Creates a header file out of Runtian's Philox RNG implementation in fusion compiler. Adds some device functions for common distributions.</li>\n<li><code>aten/src/ATen/native/cuda/Distributions.cu</code>: Corrects <code>next_philox_seed</code> usages. Please review grid size calculations, since it is being done differently in <code>CUDAApplyUtils.cuh</code></li>\n<li><code>aten/src/ATen/test/cpu_generator_test.cpp</code>: Adds unit tests and shows usage for using Generators in CPU.</li>\n<li><code>aten/src/ATen/test/cuda_generator_test.cpp</code>: Adds unit tests and shows usage for using Generators in CUDA.</li>\n<li><code>torch/src/Generator.cpp</code>: Extends torch.Generator api to have <code>device</code> and <code>default</code> keywords.</li>\n<li><code>torch/cuda/random.py</code>: Modifies the front-end cuda random.py with new Generator python module. Keeps functionality same as before, but calls to backend is different.</li>\n</ul>\n<p>I have added individual comments to rest of the files in this PR.</p>\n<p>The core functionality was verified through PyTorch's test suite. Some tests needed some adjustments with manual seeding and increasing their tolerance values.</p>\n<p>Here are some plots of some common distributions after the patch: <a href=\"https://gist.github.com/syed-ahmed/fb580ea89bd0b36572395db61573a20f\">https://gist.github.com/syed-ahmed/fb580ea89bd0b36572395db61573a20f</a></p>\n<p>Requesting review from: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7799218\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mcarilli\">@mcarilli</a> and any other code owners.</p>\n<h4>Future work related to this PR:</h4>\n<ul>\n<li><strong>Templatized and CPU version of Philox</strong>: Currently we are utilizing cuda vectors which can't be used in host. Hence, we need a vector in CPU to hold Philox counter and state variables. Moreover, Philox4_32_10 is found to be good in an older GPU generation (GTX 580). We may find new keys and round numbers that may work better for newer GPUs. Hence, having a templatized Philox might give a way to experiment with different parameters of Philox.</li>\n<li><strong>Do we need mersenne twister for cpu side if there is a Philox cpu implementation?</strong>: While mersenne twister has better statistical properties than Philox, having philox on the cpu side would mean we will get multi threading on the cpu generator. Another issue with mersenne twister is deterministically seeding it. Since, we would want to maintain a <code>manual_seed</code> function which takes in 64-bit seed, it makes sense to have an engine which doesn't have as large a state than the <code>mt19937</code> (see the bias problem mentioned above). On the design side, if we get rid of mersenne twister on the CPU side, we won't be able to use std random functions and distributions (which might not be a big deal, since TH has most of the random functions we need, already implemented, currently kept as device functions in PhiloxRNGEngine.h) or we could switch to a 64-bit std::random engine, such as knuth's lcg. On the contrary, the philox engine state consists of only two 64-bit keys. Hence, we could also avoid bias problems by using philox cpu engine.</li>\n<li><strong>Moving random tensors to ATen</strong>: Use Sam's PR (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"348073646\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10273\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/10273/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/10273\">#10273</a>) as a guide to how to move the tensors and use the new usages from this PR.</li>\n<li><strong>Implement poisson kernel for CUDA</strong>: Currently the only instance of curand is curand_poisson. Curand poisson uses a combination of three algorithms (knuth, normal approximation and gammainc approximation) to give poisson randoms. Can we find a better way to do this?</li>\n</ul>\n<h4>PR and Issues resolution:</h4>\n<ul>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357773117\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11340\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/11340/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/11340\">#11340</a></li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"230477497\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1614\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1614/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1614\">#1614</a></li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"363951593\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12083\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/12083/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/12083\">#12083</a></li>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"379978037\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13867\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/13867/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/13867\">#13867</a></li>\n</ul>\n<h4>Helpful background material:</h4>\n<ul>\n<li><a href=\"https://isocpp.org/files/papers/n3551.pdf\" rel=\"nofollow\">https://isocpp.org/files/papers/n3551.pdf</a></li>\n<li><a href=\"http://www.thesalmons.org/john/random123/papers/random123sc11.pdf\" rel=\"nofollow\">http://www.thesalmons.org/john/random123/papers/random123sc11.pdf</a></li>\n<li><a href=\"https://kristerw.blogspot.com/2017/05/seeding-stdmt19937-random-number-engine.html\" rel=\"nofollow\">https://kristerw.blogspot.com/2017/05/seeding-stdmt19937-random-number-engine.html</a></li>\n</ul>", "body_text": "Summary:\nThe purpose of this PR is to refactor Random Number Generator (RNG) design in ATen. Currently, RNGs in PyTorch has an assymetrical design, i.e. CPU Generators use an ATen class, whereas CUDA Generators use legacy THC code (THCRNGState, THCState, THCRandom_Init etc.). Moreover, the concept of generators in ATen aren't clear from its current design. This PR does the following:\n\nClarifies generator concepts and creates a unified generator class that can serve multiple backends (CPU, CUDA etc.).\nRefactors CUDA kernels with ATen generator logic and removes THC generator code.\nRefactors CPU generator to use std::mt19937_64 instead of custom mt19937 in TH.\nRemoves curandStateMTGP32 and curandStatePhilox4_32_10 and replaces with native Philox4_32_10 implementation.\nMisc:\n\nFixes bug in incr_n function of Philox engine from fusion compiler (bug also exists currently in curand) where nhi wasn't being checked for overflow.\nFixes improper scaling of random number to produce uniform distribution in fusion compiler (i.e. random numbers were sampled from {0-2^24} and then divided by 2^24 to avoid bias that was caused by dividing with 2^32).\nFixes uses of arbitrary increment number for next_philox_seed in Distributions.cu with proper philox offset calculation.\nFixes hardcoded generator related code in several python files used for code generation, such as function_wrapper.py etc.\nFixes generator front-end python bindings to include device kwarg and removes default_generator python module.\nRemoves creation of generator from Types.\nUpdates documentations and comments and adds documentation for torch.Generator api.\n\n\n\nGuide for Reviewers\nUnfortunately, this PR touches more files than it originally planned on. Hence, following is a small guide on what I think should help you review the ~100 files in this PR.\nDefinitions\n\nGenerator: An object which manages the state of the algorithm that produces pseudo random numbers. For instance, Generator class in ATen is responsible for handling seeds, creation of generators, etc.\nEngine: An object which contains the implementation of the algorithm that produces pseudo random numbers. For instance, for the CPU side, we are using a 64 bit Mersenne Twister (std::mt19937_64) and for the CUDA side we are using a Philox Engine (Philox4_32_10).\nCUDA Grids, Blocks etc.: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy\n\nCurrent State of Generators\nWhat is the deal with https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/templates/GeneratorDerived.h?\n\nThis template file takes in the $ variables from gen.py to produce CPUGenerator.h and CUDAGenerator.h. This file forces the two sides (CPU and CUDA) to have the same API, however, we have a contradiction when we pass THGenerator as a member only for the CPU side, but it is not present in the CUDAGenerator. Then it begs the question that, if we are to diverge from these two sides to be same, why not modify on two separate files, CPUGenerator.h and CUDAGenerator.h, rather than modifying these two files through a complicated gen.py script. tl-dr: I have removed this logic and it turns out we don't need to maintain two Generator headers since, we have defined an engine and a generator to be independent objects.\n\nGenerator State vs Generator\n\nCPUGenerator in ATen currently has a THGenerator object. This THGenerator objects are used as inputs to some kernels in Declarations.cwrap. In addition, function_wrapper.py processes these declarations from what it looks like - swapping THGenerator with Generator. As a result of this mix-match, THGenerator and CPUGenerator are basically treated as the same thing (this was also noted somewhere in one of the linked issues).\nCUDAGenerator in ATen doesn't have the funky replacement of THCGenerator with Generator. Instead, function_wrapper.py and ProcessorSpecificPlugin.py processes the Declarations.cwrap to remove Generator input in function signatures for the CUDA side. As a result, CUDAGenerator.cpp just makes direct calls to THCRandom functions and doesn't really serve a purpose.\nIn TH/THC, the design is consistent. That is, for the CPU side, there is THGenerator and for the CUDA side, there is THCGenerator. Both of these structs have the same definition as how we are defining what a Generator is. In addition, both of these structs only sets/gets things for their engines. Currently TH/THC both uses Mersenne Twister engines by default. TH uses custom implementation of mersenne twister and THC initializes curandStateMTGP32. Some kernels in ATen, uses curandStatePhilox4_32_10 while using the THCRandom_getGenerator() to feed the THCGenerator and use it in the curandStatePhilox4_32_10 - (uses seed and philox_seed_offset from that struct). tl-dr: In this PR, I have removed all usage of THCGenerator and THGenerator and tried to improve the symmetry of the design for both sides.\n\nGenerator registry, default generators, torch.Generator api and how front-end calls the backend\n\nCurrently Context.h creates a generator_registry which is an array of unique pointers to Generator objects (the size of which is equivalent the number of COMPILE_TIME_MAX_DEVICE_TYPES from DeviceType.h. There is one for CUDA, initialized through the initCUDAGenerator in CUDAHooks, there is one for CPU, initialized in Context.cpp. This generator_registry is meant to hold singletons, i.e. the default generator which is used by apis like torch.randn, torch.randperm etc. It is to be noted, there is a need for having default generators if a user intends to use the running state of an engine throughout their application. Currently, a default generator is instantiated in the front end for the CPU side (residing in torch/csrc/Module.cpp) and is added as default_generator python module, which is then imported in torch/random.py. For the cuda side, there is no module like this and we are just calling THCRandom functions as methods of the cuda module (torch/csrc/cuda/Module.cpp) which are used in torch/cuda/random.py. Lastly, Generator.h, Generator.cpp in torch/csrc/ creates a torch.Generator module, which gives the user the ability to create a Generator object other than the default generator. The purpose of this api seems to give the user the ability to \"fork\" RNG states. That is, if someone wanted to run their torch.randn function without incrementing the state of the default_generator, they could do that by using a torch.Generator object as an input to the function. tl-dr: In this PR, I extended the functionality of the Generator.cpp module to have a device keyword and default keyword, such that creation of default/non-default generators for different device and their backends can happen through the same python module (rather than doing funky default_generator module and also releasing a unique pointer through Types in torch/csrc/Generator.cpp).\n\nPhilox Offset Calculation\n\nDistributions.cu uses arbitrary (may be found through experimentation?) numbers like 20 and 10, when incrementing philox_seed_offset, however, there is way to calculate this number before launching a kernel, by figuring out the number of elements per thread using the grid size, block size and loop unrolling number (as demonstrated by @ngimel in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Dropout.cu#L108). The kernels in Distributions.cu utilize CUDApplyUtils.cuh which does loop unrolling through the step variable and and tries to ensure that a kernel works on one element per thread. However, for CUDA_VERSION < 9000, CUDApplyUtils.cuh adjusts grid_size to run with full occupancy (check the grid.x assignment in that file) and hence, runs a kernel in a grid-stride manner (that is, it's not one element per thread). Hence, an arbitrary philox offset increment could mean that the kernels in Distribution.cu might be broken, however, that might not necessarily be the case if the increment is high enough (but then you might ask how high is high enough)? Hence, tl-dr: In this PR, I included a incrementPhiloxOffset function which calculates an increment number given some inputs and I use this function in all the kernels requiring the offset increment.\n\nMersenne Twister concerns\nMersenne twister engine has 19937 states. [This article](- http://www.pcg-random.org/posts/cpp-seeding-surprises.html) explains why it's a bad idea to seed a mersenne twister with a single 32 bit. tl-dr:  In this PR, I have kept the same functionality as was before, seeding a Mersenne twister engine but from the std library. The bias problems of seeding this engine still remains.\nCore Files to Review\n\naten/src/ATen/core/Generator.h: Re-defines backend API for Generators. Unifies CPU and CUDA Generator structs, i.e. GeneratorState is equivalent to the union of THCGenerator and THGenerator.\naten/src/ATen/core/Generator.cpp: Implements Generator.h.\naten/src/ATen/cuda/PhiloxRNGEngine.h: Creates a header file out of Runtian's Philox RNG implementation in fusion compiler. Adds some device functions for common distributions.\naten/src/ATen/native/cuda/Distributions.cu: Corrects next_philox_seed usages. Please review grid size calculations, since it is being done differently in CUDAApplyUtils.cuh\naten/src/ATen/test/cpu_generator_test.cpp: Adds unit tests and shows usage for using Generators in CPU.\naten/src/ATen/test/cuda_generator_test.cpp: Adds unit tests and shows usage for using Generators in CUDA.\ntorch/src/Generator.cpp: Extends torch.Generator api to have device and default keywords.\ntorch/cuda/random.py: Modifies the front-end cuda random.py with new Generator python module. Keeps functionality same as before, but calls to backend is different.\n\nI have added individual comments to rest of the files in this PR.\nThe core functionality was verified through PyTorch's test suite. Some tests needed some adjustments with manual seeding and increasing their tolerance values.\nHere are some plots of some common distributions after the patch: https://gist.github.com/syed-ahmed/fb580ea89bd0b36572395db61573a20f\nRequesting review from: @ezyang @SsnL @mruberry @ngimel @mcarilli and any other code owners.\nFuture work related to this PR:\n\nTemplatized and CPU version of Philox: Currently we are utilizing cuda vectors which can't be used in host. Hence, we need a vector in CPU to hold Philox counter and state variables. Moreover, Philox4_32_10 is found to be good in an older GPU generation (GTX 580). We may find new keys and round numbers that may work better for newer GPUs. Hence, having a templatized Philox might give a way to experiment with different parameters of Philox.\nDo we need mersenne twister for cpu side if there is a Philox cpu implementation?: While mersenne twister has better statistical properties than Philox, having philox on the cpu side would mean we will get multi threading on the cpu generator. Another issue with mersenne twister is deterministically seeding it. Since, we would want to maintain a manual_seed function which takes in 64-bit seed, it makes sense to have an engine which doesn't have as large a state than the mt19937 (see the bias problem mentioned above). On the design side, if we get rid of mersenne twister on the CPU side, we won't be able to use std random functions and distributions (which might not be a big deal, since TH has most of the random functions we need, already implemented, currently kept as device functions in PhiloxRNGEngine.h) or we could switch to a 64-bit std::random engine, such as knuth's lcg. On the contrary, the philox engine state consists of only two 64-bit keys. Hence, we could also avoid bias problems by using philox cpu engine.\nMoving random tensors to ATen: Use Sam's PR (#10273) as a guide to how to move the tensors and use the new usages from this PR.\nImplement poisson kernel for CUDA: Currently the only instance of curand is curand_poisson. Curand poisson uses a combination of three algorithms (knuth, normal approximation and gammainc approximation) to give poisson randoms. Can we find a better way to do this?\n\nPR and Issues resolution:\n\n#11340\n#1614\n#12083\n#13867\n\nHelpful background material:\n\nhttps://isocpp.org/files/papers/n3551.pdf\nhttp://www.thesalmons.org/john/random123/papers/random123sc11.pdf\nhttps://kristerw.blogspot.com/2017/05/seeding-stdmt19937-random-number-engine.html", "body": "## Summary:\r\nThe purpose of this PR is to refactor Random Number Generator (RNG) design in ATen. Currently, RNGs in PyTorch has an assymetrical design, i.e. CPU Generators use an ATen class, whereas CUDA Generators use legacy THC code (`THCRNGState, THCState, THCRandom_Init` etc.). Moreover, the concept of generators in ATen aren't clear from its current design. This PR does the following:\r\n- Clarifies generator concepts and creates a unified generator class that can serve multiple backends (CPU, CUDA etc.).\r\n- Refactors CUDA kernels with ATen generator logic and removes THC generator code.\r\n- Refactors CPU generator to use `std::mt19937_64` instead of custom mt19937 in TH.\r\n- Removes `curandStateMTGP32` and `curandStatePhilox4_32_10` and replaces with native Philox4_32_10 implementation.\r\n- Misc:\r\n  - Fixes bug in `incr_n` function of Philox engine from fusion compiler (bug also exists currently in curand) where `nhi` wasn't being checked for overflow.\r\n  - Fixes improper scaling of random number to produce uniform distribution in fusion compiler (i.e. random numbers were sampled from `{0-2^24}` and then divided by 2^24 to avoid bias that was caused by dividing with 2^32).\r\n  - Fixes uses of arbitrary increment number for `next_philox_seed` in `Distributions.cu` with proper philox offset calculation.\r\n  - Fixes hardcoded generator related code in several python files used for code generation, such as `function_wrapper.py` etc.\r\n  - Fixes generator front-end python bindings to include device kwarg and removes `default_generator` python module.\r\n  - Removes creation of generator from Types.\r\n  - Updates documentations and comments and adds documentation for `torch.Generator` api.\r\n\r\n## Guide for Reviewers\r\nUnfortunately, this PR touches more files than it originally planned on. Hence, following is a small guide on what I think should help you review the ~100 files in this PR.\r\n#### _Definitions_\r\n- **Generator**: An object which manages the state of the algorithm that produces pseudo random numbers. For instance, Generator class in ATen is responsible for handling seeds, creation of generators, etc.\r\n- **Engine**: An object which contains the implementation of the algorithm that produces pseudo random numbers. For instance, for the CPU side, we are using a 64 bit Mersenne Twister (`std::mt19937_64`) and for the CUDA side we are using a Philox Engine (`Philox4_32_10`).\r\n- **CUDA Grids, Blocks etc.**: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy\r\n#### Current State of Generators\r\n##### _What is the deal with https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/templates/GeneratorDerived.h?_\r\n- This template file takes in the `$` variables from `gen.py` to produce `CPUGenerator.h` and `CUDAGenerator.h`. This file forces the two sides (CPU and CUDA) to have the same API, however, we have a contradiction when we pass `THGenerator` as a member only for the CPU side, but it is not present in the CUDAGenerator. Then it begs the question that, if we are to diverge from these two sides to be same, why not modify on two separate files, `CPUGenerator.h` and `CUDAGenerator.h`, rather than modifying these two files through a complicated `gen.py` script. **tl-dr**: I have removed this logic and it turns out we don't need to maintain two Generator headers since, we have defined an engine and a generator to be independent objects.\r\n##### _Generator State vs Generator_\r\n- CPUGenerator in ATen currently has a `THGenerator` object. This `THGenerator` objects are used as inputs to some kernels in `Declarations.cwrap`. In addition, `function_wrapper.py` processes these declarations from what it looks like - swapping `THGenerator` with `Generator`. As a result of this mix-match, `THGenerator` and `CPUGenerator` are basically treated as the same thing (this was also noted somewhere in one of the linked issues).\r\n- `CUDAGenerator` in ATen doesn't have the funky replacement of `THCGenerator` with Generator. Instead, `function_wrapper.py` and `ProcessorSpecificPlugin.py` processes the `Declarations.cwrap` to remove `Generator` input in function signatures for the CUDA side. As a result, `CUDAGenerator.cpp` just makes direct calls to `THCRandom` functions and doesn't really serve a purpose.\r\n- In TH/THC, the design is consistent. That is, for the CPU side, there is `THGenerator` and for the CUDA side, there is `THCGenerator`. Both of these structs have the same definition as how we are defining what a Generator is. In addition, both of these structs only sets/gets things for their engines. Currently TH/THC both uses Mersenne Twister engines by default. TH uses custom implementation of mersenne twister and THC initializes `curandStateMTGP32`. Some kernels in ATen, uses `curandStatePhilox4_32_10` while using the `THCRandom_getGenerator()` to feed the `THCGenerator` and use it in the `curandStatePhilox4_32_10` - (uses seed and philox_seed_offset from that struct). **tl-dr**: In this PR, I have removed all usage of `THCGenerator` and `THGenerator` and tried to improve the symmetry of the design for both sides.\r\n##### _Generator registry, default generators, torch.Generator api and how front-end calls the backend_\r\n- Currently `Context.h` creates a `generator_registry` which is an array of unique pointers to `Generator` objects (the size of which is equivalent the number of `COMPILE_TIME_MAX_DEVICE_TYPES` from `DeviceType.h`. There is one for CUDA, initialized through the `initCUDAGenerator` in `CUDAHooks`, there is one for CPU, initialized in `Context.cpp`. This `generator_registry` is meant to hold singletons, i.e. the default generator which is used by apis like `torch.randn`, `torch.randperm` etc. It is to be noted, there is a need for having default generators if a user intends to use the running state of an engine throughout their application. Currently, a default generator is instantiated in the front end for the CPU side (residing in `torch/csrc/Module.cpp`) and is added as `default_generator` python module, which is then imported in `torch/random.py`. For the cuda side, there is no module like this and we are just calling `THCRandom` functions as methods of the cuda module (torch/csrc/cuda/Module.cpp) which are used in `torch/cuda/random.py`. Lastly, `Generator.h`, `Generator.cpp` in `torch/csrc/` creates a `torch.Generator` module, which gives the user the ability to create a Generator object other than the default generator. The purpose of this api seems to give the user the ability to \"fork\" RNG states. That is, if someone wanted to run their `torch.randn` function without incrementing the state of the `default_generator`, they could do that by using a `torch.Generator` object as an input to the function. **tl-dr**: In this PR, I extended the functionality of the `Generator.cpp` module to have a `device` keyword and `default` keyword, such that creation of default/non-default generators for different device and their backends can happen through the same python module (rather than doing funky `default_generator` module and also releasing a unique pointer through Types in `torch/csrc/Generator.cpp`). \r\n##### _Philox Offset Calculation_\r\n- `Distributions.cu` uses arbitrary (may be found through experimentation?) numbers like 20 and 10, when incrementing `philox_seed_offset`, however, there is way to calculate this number before launching a kernel, by figuring out the number of elements per thread using the grid size, block size and loop unrolling number (as demonstrated by @ngimel in https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Dropout.cu#L108). The kernels in `Distributions.cu` utilize `CUDApplyUtils.cuh` which does loop unrolling through the `step` variable and and tries to ensure that a kernel works on one element per thread. However, for `CUDA_VERSION < 9000`, `CUDApplyUtils.cuh` adjusts `grid_size` to run with full occupancy (check the `grid.x` assignment in that file) and hence, runs a kernel in a grid-stride manner (that is, it's not one element per thread). Hence, an arbitrary philox offset increment could mean that the kernels in `Distribution.cu` might be broken, however, that might not necessarily be the case if the increment is high enough (but then you might ask how high is high enough)? Hence, **tl-dr**: In this PR, I included a `incrementPhiloxOffset` function which calculates an increment number given some inputs and I use this function in all the kernels requiring the offset increment. \r\n##### _Mersenne Twister concerns_\r\nMersenne twister engine has 19937 states. [This article](- http://www.pcg-random.org/posts/cpp-seeding-surprises.html) explains why it's a bad idea to seed a mersenne twister with a single 32 bit. **tl-dr**:  In this PR, I have kept the same functionality as was before, seeding a Mersenne twister engine but from the std library. The bias problems of seeding this engine still remains. \r\n\r\n#### Core Files to Review\r\n- `aten/src/ATen/core/Generator.h`: Re-defines backend API for Generators. Unifies CPU and CUDA Generator structs, i.e. `GeneratorState` is equivalent to the union of `THCGenerator` and `THGenerator`.\r\n- `aten/src/ATen/core/Generator.cpp`: Implements Generator.h.\r\n- `aten/src/ATen/cuda/PhiloxRNGEngine.h`: Creates a header file out of Runtian's Philox RNG implementation in fusion compiler. Adds some device functions for common distributions.\r\n- `aten/src/ATen/native/cuda/Distributions.cu`: Corrects `next_philox_seed` usages. Please review grid size calculations, since it is being done differently in `CUDAApplyUtils.cuh`\r\n- `aten/src/ATen/test/cpu_generator_test.cpp`: Adds unit tests and shows usage for using Generators in CPU.\r\n- `aten/src/ATen/test/cuda_generator_test.cpp`: Adds unit tests and shows usage for using Generators in CUDA.\r\n- `torch/src/Generator.cpp`: Extends torch.Generator api to have `device` and `default` keywords.\r\n- `torch/cuda/random.py`: Modifies the front-end cuda random.py with new Generator python module. Keeps functionality same as before, but calls to backend is different.\r\n\r\nI have added individual comments to rest of the files in this PR.\r\n\r\nThe core functionality was verified through PyTorch's test suite. Some tests needed some adjustments with manual seeding and increasing their tolerance values.\r\n\r\nHere are some plots of some common distributions after the patch: https://gist.github.com/syed-ahmed/fb580ea89bd0b36572395db61573a20f\r\n\r\nRequesting review from: @ezyang @SsnL @mruberry @ngimel @mcarilli and any other code owners.\r\n\r\n#### Future work related to this PR:\r\n- **Templatized and CPU version of Philox**: Currently we are utilizing cuda vectors which can't be used in host. Hence, we need a vector in CPU to hold Philox counter and state variables. Moreover, Philox4_32_10 is found to be good in an older GPU generation (GTX 580). We may find new keys and round numbers that may work better for newer GPUs. Hence, having a templatized Philox might give a way to experiment with different parameters of Philox.\r\n- **Do we need mersenne twister for cpu side if there is a Philox cpu implementation?**: While mersenne twister has better statistical properties than Philox, having philox on the cpu side would mean we will get multi threading on the cpu generator. Another issue with mersenne twister is deterministically seeding it. Since, we would want to maintain a `manual_seed` function which takes in 64-bit seed, it makes sense to have an engine which doesn't have as large a state than the `mt19937` (see the bias problem mentioned above). On the design side, if we get rid of mersenne twister on the CPU side, we won't be able to use std random functions and distributions (which might not be a big deal, since TH has most of the random functions we need, already implemented, currently kept as device functions in PhiloxRNGEngine.h) or we could switch to a 64-bit std::random engine, such as knuth's lcg. On the contrary, the philox engine state consists of only two 64-bit keys. Hence, we could also avoid bias problems by using philox cpu engine. \r\n- **Moving random tensors to ATen**: Use Sam's PR (https://github.com/pytorch/pytorch/pull/10273) as a guide to how to move the tensors and use the new usages from this PR.\r\n- **Implement poisson kernel for CUDA**: Currently the only instance of curand is curand_poisson. Curand poisson uses a combination of three algorithms (knuth, normal approximation and gammainc approximation) to give poisson randoms. Can we find a better way to do this?\r\n\r\n#### PR and Issues resolution:\r\n- https://github.com/pytorch/pytorch/issues/11340\r\n- https://github.com/pytorch/pytorch/issues/1614\r\n- https://github.com/pytorch/pytorch/issues/12083\r\n- https://github.com/pytorch/pytorch/issues/13867\r\n\r\n#### Helpful background material:\r\n- https://isocpp.org/files/papers/n3551.pdf\r\n- http://www.thesalmons.org/john/random123/papers/random123sc11.pdf\r\n- https://kristerw.blogspot.com/2017/05/seeding-stdmt19937-random-number-engine.html "}