{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234006821", "pull_request_review_id": 175554806, "id": 234006821, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNDAwNjgyMQ==", "diff_hunk": "@@ -221,8 +253,21 @@ Tensor& bernoulli_tensor_cuda_(Tensor &self, const Tensor& p_, Generator* gen) {\n \n Tensor& bernoulli_scalar_cuda_(Tensor &self, double p, Generator* gen) {\n   AT_CHECK(0 <= p && p <= 1, \"bernoulli_ expects p to be in [0, 1], but got p=\", p);\n+  auto gen_ = at::detail::checkGeneratorWithDefault(gen, kCUDA);\n+  uint64_t block_size = 512; // AT_APPLY_THREADS_PER_BLOCK in CUDAApplyUtils.cuh\n+  uint64_t step = 4;\n+  uint64_t total_elements = self.numel();\n+  // grid calculation from getApplyGrid() in CUDAApplyUtils.cuh\n+  uint64_t grid_size = (total_elements + (block_size * step) - 1) / (block_size * step);\n+  #if CUDA_VERSION < 9000\n+    if (!self.is_contiguous()) {\n+      uint64_t blocks_per_sm = 4; // AT_APPLY_BLOCKS_PER_SM in CUDAApplyUtils.cuh", "path": "aten/src/ATen/native/cuda/Distributions.cu", "position": 281, "original_position": 297, "commit_id": "a4279c73de70eb5a87260df7f5ede98c05f4e320", "original_commit_id": "992d302ff90714182d533472c2a9f090ad79d3d8", "user": {"login": "syed-ahmed", "id": 8906225, "node_id": "MDQ6VXNlcjg5MDYyMjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/8906225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syed-ahmed", "html_url": "https://github.com/syed-ahmed", "followers_url": "https://api.github.com/users/syed-ahmed/followers", "following_url": "https://api.github.com/users/syed-ahmed/following{/other_user}", "gists_url": "https://api.github.com/users/syed-ahmed/gists{/gist_id}", "starred_url": "https://api.github.com/users/syed-ahmed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syed-ahmed/subscriptions", "organizations_url": "https://api.github.com/users/syed-ahmed/orgs", "repos_url": "https://api.github.com/users/syed-ahmed/repos", "events_url": "https://api.github.com/users/syed-ahmed/events{/privacy}", "received_events_url": "https://api.github.com/users/syed-ahmed/received_events", "type": "User", "site_admin": false}, "body": "I wanted to make it this function `unsigned int blocks_per_sm = at::cuda::getCurrentDeviceProperties()->maxThreadsPerMultiProcessor/block_size;` but CUDAApplyUtils.cuh currently hard codes blocks_per_sm and block_size and launches kernels with those hardcoded values. Hence, if the value of `at::cuda::getCurrentDeviceProperties()->maxThreadsPerMultiProcessor` doesn't result into a blocks_per_sm of 4, the offset calculation would be off and that's why hardcoded here. I could have modified CUDAApplyUtils.cuh but wanted to avoid doing it for this PR, but if you think it's necessary, I'll do it.", "created_at": "2018-11-15T21:03:47Z", "updated_at": "2018-11-23T15:54:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/13070#discussion_r234006821", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13070", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234006821"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13070#discussion_r234006821"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13070"}}, "body_html": "<p>I wanted to make it this function <code>unsigned int blocks_per_sm = at::cuda::getCurrentDeviceProperties()-&gt;maxThreadsPerMultiProcessor/block_size;</code> but CUDAApplyUtils.cuh currently hard codes blocks_per_sm and block_size and launches kernels with those hardcoded values. Hence, if the value of <code>at::cuda::getCurrentDeviceProperties()-&gt;maxThreadsPerMultiProcessor</code> doesn't result into a blocks_per_sm of 4, the offset calculation would be off and that's why hardcoded here. I could have modified CUDAApplyUtils.cuh but wanted to avoid doing it for this PR, but if you think it's necessary, I'll do it.</p>", "body_text": "I wanted to make it this function unsigned int blocks_per_sm = at::cuda::getCurrentDeviceProperties()->maxThreadsPerMultiProcessor/block_size; but CUDAApplyUtils.cuh currently hard codes blocks_per_sm and block_size and launches kernels with those hardcoded values. Hence, if the value of at::cuda::getCurrentDeviceProperties()->maxThreadsPerMultiProcessor doesn't result into a blocks_per_sm of 4, the offset calculation would be off and that's why hardcoded here. I could have modified CUDAApplyUtils.cuh but wanted to avoid doing it for this PR, but if you think it's necessary, I'll do it.", "in_reply_to_id": 233685181}