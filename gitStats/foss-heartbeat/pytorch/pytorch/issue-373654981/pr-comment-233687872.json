{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233687872", "pull_request_review_id": 175157819, "id": 233687872, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzY4Nzg3Mg==", "diff_hunk": "@@ -148,51 +134,84 @@ void bernoulli_scalar_cuda_kernel(\n   // The template argument `4` below indicates that we want to operate on four\n   // element at each time. See NOTE [ CUDA_tensor_applyN helpers ] for details.\n   at::cuda::CUDA_tensor_apply1<scalar_t, 4>(\n-      ret, [seeds, p] __device__(\n-        int n, scalar_t& v1, scalar_t& v2, scalar_t& v3, scalar_t& v4) {\n-        curandStatePhilox4_32_10_t state;\n-        curand_init(\n-            seeds.first,\n-            blockIdx.x * blockDim.x + threadIdx.x,\n-            seeds.second,\n-            &state);\n-        float4 rand = curand_uniform4(&state);\n-        switch (n) {\n-          case 4: {\n-            v4 = static_cast<scalar_t>(rand.w <= p);\n-            // fallthrough\n-          }\n-          case 3: {\n-            v3 = static_cast<scalar_t>(rand.z <= p);\n-            // fallthrough\n-          }\n-          case 2: {\n-            v2 = static_cast<scalar_t>(rand.y <= p);\n-            // fallthrough\n-          }\n-          case 1: {\n-            v1 = static_cast<scalar_t>(rand.x <= p);\n-          }\n+      ret, \n+      [seeds, p] __device__(\n+      int n, scalar_t& v1, scalar_t& v2, scalar_t& v3, scalar_t& v4) {\n+      at::cuda::Philox4_32_10 engine(\n+                                seeds.first,\n+                                blockIdx.x * blockDim.x + threadIdx.x,\n+                                seeds.second);\n+      auto w = at::cuda::standard_uniform_distribution(engine);\n+      auto z = at::cuda::standard_uniform_distribution(engine);\n+      auto y = at::cuda::standard_uniform_distribution(engine);\n+      auto x = at::cuda::standard_uniform_distribution(engine);\n+      switch (n) {\n+        case 4: {\n+          v4 = static_cast<scalar_t>(w <= p);\n+          // fallthrough\n+        }\n+        case 3: {\n+          v3 = static_cast<scalar_t>(z <= p);\n+          // fallthrough\n+        }\n+        case 2: {\n+          v2 = static_cast<scalar_t>(y <= p);\n+          // fallthrough\n+        }\n+        case 1: {\n+          v1 = static_cast<scalar_t>(x <= p);\n         }\n       }\n-    );\n+    }\n+  );\n }\n \n } // namespace\n \n namespace at { namespace native {\n Tensor _s_poisson_cuda(const Tensor& lambda, Generator* gen) {\n   Tensor ret = at::empty(lambda.sizes(), lambda.options());\n+  auto gen_ = at::detail::checkGeneratorWithDefault(gen, kCUDA);\n+  uint64_t block_size = 512; // AT_APPLY_THREADS_PER_BLOCK in CUDAApplyUtils.cuh\n+  uint64_t step = 1;\n+  uint64_t total_elements = ret.numel();\n+  // grid calculation from getApplyGrid() in CUDAApplyUtils.cuh\n+  uint64_t grid_size = (total_elements + (block_size * step) - 1) / (block_size * step);\n+  #if CUDA_VERSION < 9000\n+    if (!ret.is_contiguous()) {\n+      uint64_t blocks_per_sm = 4; // AT_APPLY_BLOCKS_PER_SM in CUDAApplyUtils.cuh\n+      grid_size = std::min((unsigned int)(at::cuda::getCurrentDeviceProperties()->multiProcessorCount) * blocks_per_sm , grid_size);\n+    }\n+  #endif\n+  // the philox offset calculation here is not exact and the multiplier 20 is just\n+  // a guess. This is because curand_poisson is using algorithms which use while\n+  // loops and hence, we cannot predict the number of engine calls that is made\n+  // by the philox engine.\n+  auto seeds = gen_->incrementPhiloxOffset(total_elements, grid_size, block_size, step, 20);\n   AT_DISPATCH_FLOATING_TYPES_AND_HALF(ret.type(), \"poisson\", [&] {\n-    poisson_cuda_kernel<scalar_t>(ret, lambda, next_philox_seed(gen, 20));\n+    poisson_cuda_kernel<scalar_t>(ret, lambda, seeds);\n   });\n   return ret;\n }\n \n Tensor _s_gamma_cuda(const Tensor& alpha, Generator* gen) {\n   Tensor ret = at::empty(alpha.sizes(), alpha.options());\n+  auto gen_ = at::detail::checkGeneratorWithDefault(gen, kCUDA);\n+  uint64_t block_size = 512; // AT_APPLY_THREADS_PER_BLOCK in CUDAApplyUtils.cuh\n+  uint64_t step = 1;\n+  uint64_t total_elements = ret.numel();\n+  // grid calculation from getApplyGrid() in CUDAApplyUtils.cuh\n+  uint64_t grid_size = (total_elements + (block_size * step) - 1) / (block_size * step);\n+  #if CUDA_VERSION < 9000\n+    if (!ret.is_contiguous()) {\n+      uint64_t blocks_per_sm = 4; // AT_APPLY_BLOCKS_PER_SM in CUDAApplyUtils.cuh\n+      grid_size = std::min((unsigned int)(at::cuda::getCurrentDeviceProperties()->multiProcessorCount) * blocks_per_sm , grid_size);\n+    }\n+  #endif\n+  // number of engine() calls is 3 - 1 for uniform and 2 for normal\n+  auto seeds = gen_->incrementPhiloxOffset(total_elements, grid_size, block_size, step, 3);", "path": "aten/src/ATen/native/cuda/Distributions.cu", "position": null, "original_position": 253, "commit_id": "a4279c73de70eb5a87260df7f5ede98c05f4e320", "original_commit_id": "992d302ff90714182d533472c2a9f090ad79d3d8", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "It's much more than 3, if you look at sample_gamma, it loops over many attempts, I'm not sure that even original 10 was enough. I'd ask on pytorch #distributions what would be an appropriate number to put here, may be 10 came from some experiments. ", "created_at": "2018-11-15T01:49:22Z", "updated_at": "2018-11-23T15:54:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/13070#discussion_r233687872", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13070", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233687872"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13070#discussion_r233687872"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13070"}}, "body_html": "<p>It's much more than 3, if you look at sample_gamma, it loops over many attempts, I'm not sure that even original 10 was enough. I'd ask on pytorch #distributions what would be an appropriate number to put here, may be 10 came from some experiments.</p>", "body_text": "It's much more than 3, if you look at sample_gamma, it loops over many attempts, I'm not sure that even original 10 was enough. I'd ask on pytorch #distributions what would be an appropriate number to put here, may be 10 came from some experiments."}