{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233685181", "pull_request_review_id": 175157819, "id": 233685181, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzY4NTE4MQ==", "diff_hunk": "@@ -221,8 +253,21 @@ Tensor& bernoulli_tensor_cuda_(Tensor &self, const Tensor& p_, Generator* gen) {\n \n Tensor& bernoulli_scalar_cuda_(Tensor &self, double p, Generator* gen) {\n   AT_CHECK(0 <= p && p <= 1, \"bernoulli_ expects p to be in [0, 1], but got p=\", p);\n+  auto gen_ = at::detail::checkGeneratorWithDefault(gen, kCUDA);\n+  uint64_t block_size = 512; // AT_APPLY_THREADS_PER_BLOCK in CUDAApplyUtils.cuh\n+  uint64_t step = 4;\n+  uint64_t total_elements = self.numel();\n+  // grid calculation from getApplyGrid() in CUDAApplyUtils.cuh\n+  uint64_t grid_size = (total_elements + (block_size * step) - 1) / (block_size * step);\n+  #if CUDA_VERSION < 9000\n+    if (!self.is_contiguous()) {\n+      uint64_t blocks_per_sm = 4; // AT_APPLY_BLOCKS_PER_SM in CUDAApplyUtils.cuh", "path": "aten/src/ATen/native/cuda/Distributions.cu", "position": 281, "original_position": 297, "commit_id": "a4279c73de70eb5a87260df7f5ede98c05f4e320", "original_commit_id": "992d302ff90714182d533472c2a9f090ad79d3d8", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "why is this hardcoded? It is a function of max threads per sm and block_size. Same for every place where blocks_per_sm is hardcoded. ", "created_at": "2018-11-15T01:33:01Z", "updated_at": "2018-11-23T15:54:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/13070#discussion_r233685181", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13070", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233685181"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13070#discussion_r233685181"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13070"}}, "body_html": "<p>why is this hardcoded? It is a function of max threads per sm and block_size. Same for every place where blocks_per_sm is hardcoded.</p>", "body_text": "why is this hardcoded? It is a function of max threads per sm and block_size. Same for every place where blocks_per_sm is hardcoded."}