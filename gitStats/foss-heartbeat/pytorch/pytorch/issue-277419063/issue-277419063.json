{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3924", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3924/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3924/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3924/events", "html_url": "https://github.com/pytorch/pytorch/issues/3924", "id": 277419063, "node_id": "MDU6SXNzdWUyNzc0MTkwNjM=", "number": 3924, "title": "Loss function nn.MSELoss() divides by wrong number for averaging", "user": {"login": "msieb1", "id": 32754403, "node_id": "MDQ6VXNlcjMyNzU0NDAz", "avatar_url": "https://avatars2.githubusercontent.com/u/32754403?v=4", "gravatar_id": "", "url": "https://api.github.com/users/msieb1", "html_url": "https://github.com/msieb1", "followers_url": "https://api.github.com/users/msieb1/followers", "following_url": "https://api.github.com/users/msieb1/following{/other_user}", "gists_url": "https://api.github.com/users/msieb1/gists{/gist_id}", "starred_url": "https://api.github.com/users/msieb1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/msieb1/subscriptions", "organizations_url": "https://api.github.com/users/msieb1/orgs", "repos_url": "https://api.github.com/users/msieb1/repos", "events_url": "https://api.github.com/users/msieb1/events{/privacy}", "received_events_url": "https://api.github.com/users/msieb1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-11-28T15:09:37Z", "updated_at": "2017-12-01T21:32:31Z", "closed_at": "2017-12-01T21:32:31Z", "author_association": "NONE", "body_html": "<p>The function divides by the entire number of elements and not just batch sample size.<br>\nFor example, using</p>\n<pre><code>loss_f1 = nn.MSELoss()\nloss_f2 = nn.MSELoss(size_average=False)\n\nx = autograd.Variable(torch.FloatTensor([    [1,2,3], [1,2,3]   ]))\n\ny = autograd.Variable(torch.FloatTensor([    [2,4,6], [3,4,7]   ]))\n\n# sample size of current batch = 2\n# dimension of features = 3\n</code></pre>\n<p>shows the problem.</p>\n<pre><code>\n# loss no averaging/no division...just plain summing up\nloss_2 = loss_f2(x, y) # = 38\n\n# averaged loss over batch sample size\nloss_1 = loss_f1(x, y) # = 6.3333\n\n# Apparently division by 6 was done in the second averaging case.\n</code></pre>\n<p>The documentation describes the implemented MSE loss as</p>\n<blockquote>\n<p>loss(x,y)=1/n (sum(||x-y||**2)</p>\n</blockquote>\n<p>where n is the sample size of the current batch.<br>\nHowever, apparently the expression does not get divided by n (which would be 2 in our example), but by 2*3=6, i.e. the entire number of elements per training batch.</p>\n<p>Is this intended? From a theoretical standpoint it should be 2, shouldn't it?</p>", "body_text": "The function divides by the entire number of elements and not just batch sample size.\nFor example, using\nloss_f1 = nn.MSELoss()\nloss_f2 = nn.MSELoss(size_average=False)\n\nx = autograd.Variable(torch.FloatTensor([    [1,2,3], [1,2,3]   ]))\n\ny = autograd.Variable(torch.FloatTensor([    [2,4,6], [3,4,7]   ]))\n\n# sample size of current batch = 2\n# dimension of features = 3\n\nshows the problem.\n\n# loss no averaging/no division...just plain summing up\nloss_2 = loss_f2(x, y) # = 38\n\n# averaged loss over batch sample size\nloss_1 = loss_f1(x, y) # = 6.3333\n\n# Apparently division by 6 was done in the second averaging case.\n\nThe documentation describes the implemented MSE loss as\n\nloss(x,y)=1/n (sum(||x-y||**2)\n\nwhere n is the sample size of the current batch.\nHowever, apparently the expression does not get divided by n (which would be 2 in our example), but by 2*3=6, i.e. the entire number of elements per training batch.\nIs this intended? From a theoretical standpoint it should be 2, shouldn't it?", "body": "The function divides by the entire number of elements and not just batch sample size.\r\nFor example, using\r\n```\r\nloss_f1 = nn.MSELoss()\r\nloss_f2 = nn.MSELoss(size_average=False)\r\n\r\nx = autograd.Variable(torch.FloatTensor([    [1,2,3], [1,2,3]   ]))\r\n\r\ny = autograd.Variable(torch.FloatTensor([    [2,4,6], [3,4,7]   ]))\r\n\r\n# sample size of current batch = 2\r\n# dimension of features = 3\r\n```\r\nshows the problem.\r\n```\r\n\r\n# loss no averaging/no division...just plain summing up\r\nloss_2 = loss_f2(x, y) # = 38\r\n\r\n# averaged loss over batch sample size\r\nloss_1 = loss_f1(x, y) # = 6.3333\r\n\r\n# Apparently division by 6 was done in the second averaging case.\r\n```\r\n\r\nThe documentation describes the implemented MSE loss as\r\n\r\n> loss(x,y)=1/n (sum(||x-y||**2)\r\n\r\nwhere n is the sample size of the current batch.\r\nHowever, apparently the expression does not get divided by n (which would be 2 in our example), but by 2*3=6, i.e. the entire number of elements per training batch.\r\n\r\nIs this intended? From a theoretical standpoint it should be 2, shouldn't it?\r\n"}