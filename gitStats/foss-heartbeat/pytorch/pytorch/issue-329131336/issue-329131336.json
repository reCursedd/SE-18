{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8117", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8117/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8117/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8117/events", "html_url": "https://github.com/pytorch/pytorch/pull/8117", "id": 329131336, "node_id": "MDExOlB1bGxSZXF1ZXN0MTkyNDY0MzEz", "number": 8117, "title": "Migrated hardshrink() to ATen and deprecated nn.Hardshrink()", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-06-04T16:33:52Z", "updated_at": "2018-11-23T15:48:17Z", "closed_at": "2018-06-14T20:42:22Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8117", "html_url": "https://github.com/pytorch/pytorch/pull/8117", "diff_url": "https://github.com/pytorch/pytorch/pull/8117.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8117.patch"}, "body_html": "<h2>Summary:</h2>\n<ol>\n<li>Added hardshrink() to ATen (CPU + GPU)</li>\n<li>Removed nn.Hardshrink(), but keeping file \"aten/src/THNN/generic/HardShrink.c\"</li>\n<li>Reusing nn.Hardshrink() tests in test_nn and including CUDA tests. Not sure test_nn is a good place for hardshrink() since it is no longer under nn</li>\n<li>Added tests in test_torch</li>\n</ol>\n<p>This replaces <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"324587815\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7695\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7695/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/7695\">#7695</a>, <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #4154.\">fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"281855896\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4154\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4154/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4154\">#4154</a></p>\n<h2>Future work:</h2>\n<ol>\n<li>not supporting default lambd=0.5 in torch.hardshrink() due to a Scalar bug <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"332430590\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8484\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8484/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/8484\">#8484</a></li>\n</ol>\n<h2>Performance:</h2>\n<h3>CPU forward:</h3>\n<p>Previous impl:</p>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>).fill_(<span class=\"pl-c1\">0.3</span>).requires_grad_()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">origfn</span>(<span class=\"pl-smi\">data</span>):\n  f <span class=\"pl-k\">=</span> torch.nn.Hardshrink(<span class=\"pl-c1\">0.3</span>)\n  large_out <span class=\"pl-k\">=</span> f(data)\n<span class=\"pl-k\">%</span>timeit origfn(large_data) \n<span class=\"pl-c\"><span class=\"pl-c\">#</span>###################</span>\n<span class=\"pl-c1\">100</span> loops, best of <span class=\"pl-c1\">3</span>: <span class=\"pl-c1\">2.15</span> ms per loop</pre></div>\n<p>Current impl:</p>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>).fill_(<span class=\"pl-c1\">0.3</span>).requires_grad_()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">newfn</span>(<span class=\"pl-smi\">data</span>):\n    large_out <span class=\"pl-k\">=</span> data.hardshrink(<span class=\"pl-c1\">0.3</span>)\n<span class=\"pl-k\">%</span>timeit newfn(large_data)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>###################</span>\n<span class=\"pl-c1\">100</span> loops, best of <span class=\"pl-c1\">3</span>: <span class=\"pl-c1\">2.62</span> ms per loop</pre></div>\n<h3>CPU backward:</h3>\n<p>Previous impl:</p>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>).fill_(<span class=\"pl-c1\">0.3</span>).requires_grad_()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">origfn_backward</span>(<span class=\"pl-smi\">data</span>):\n    f <span class=\"pl-k\">=</span> torch.nn.Hardshrink(<span class=\"pl-c1\">0.3</span>)\n    large_out <span class=\"pl-k\">=</span> f(data)\n    large_out.sum().backward()\n<span class=\"pl-k\">%</span>timeit origfn_backward(large_data)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>###################</span>\n<span class=\"pl-c1\">100</span> loops, best of <span class=\"pl-c1\">3</span>: <span class=\"pl-c1\">5.2</span> ms per loop</pre></div>\n<p>Current impl:</p>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>).fill_(<span class=\"pl-c1\">0.3</span>).requires_grad_()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">newfn_backward</span>(<span class=\"pl-smi\">data</span>):\n    large_out <span class=\"pl-k\">=</span> data.hardshrink(<span class=\"pl-c1\">0.3</span>)\n    large_out.sum().backward()\n<span class=\"pl-k\">%</span>timeit newfn_backward(large_data)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>###################</span>\n<span class=\"pl-c1\">100</span> loops, best of <span class=\"pl-c1\">3</span>: <span class=\"pl-c1\">6.47</span> ms per loop</pre></div>\n<h3>CUDA:</h3>\n<p>Current impl:</p>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>cuda).fill_(<span class=\"pl-c1\">0.3</span>)\n<span class=\"pl-k\">%</span>timeit data.hardshrink(<span class=\"pl-c1\">0.3</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>###################</span>\n<span class=\"pl-c1\">10000</span> loops, best of <span class=\"pl-c1\">3</span>: <span class=\"pl-c1\">97.1</span> \u00b5s per loop</pre></div>\n<p>Benchmark:</p>\n<div class=\"highlight highlight-source-python\"><pre>large_data <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>cuda).fill_(<span class=\"pl-c1\">0.3</span>)\n<span class=\"pl-k\">%</span>timeit data.mul_(<span class=\"pl-c1\">2</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>###################</span>\n<span class=\"pl-c1\">10000</span> loops, best of <span class=\"pl-c1\">3</span>: <span class=\"pl-c1\">51.6</span> \u00b5s per loop</pre></div>", "body_text": "Summary:\n\nAdded hardshrink() to ATen (CPU + GPU)\nRemoved nn.Hardshrink(), but keeping file \"aten/src/THNN/generic/HardShrink.c\"\nReusing nn.Hardshrink() tests in test_nn and including CUDA tests. Not sure test_nn is a good place for hardshrink() since it is no longer under nn\nAdded tests in test_torch\n\nThis replaces #7695, fixes #4154\nFuture work:\n\nnot supporting default lambd=0.5 in torch.hardshrink() due to a Scalar bug #8484\n\nPerformance:\nCPU forward:\nPrevious impl:\nlarge_data = torch.zeros(1000, 1000).fill_(0.3).requires_grad_()\ndef origfn(data):\n  f = torch.nn.Hardshrink(0.3)\n  large_out = f(data)\n%timeit origfn(large_data) \n####################\n100 loops, best of 3: 2.15 ms per loop\nCurrent impl:\nlarge_data = torch.zeros(1000, 1000).fill_(0.3).requires_grad_()\ndef newfn(data):\n    large_out = data.hardshrink(0.3)\n%timeit newfn(large_data)\n####################\n100 loops, best of 3: 2.62 ms per loop\nCPU backward:\nPrevious impl:\nlarge_data = torch.zeros(1000, 1000).fill_(0.3).requires_grad_()\ndef origfn_backward(data):\n    f = torch.nn.Hardshrink(0.3)\n    large_out = f(data)\n    large_out.sum().backward()\n%timeit origfn_backward(large_data)\n####################\n100 loops, best of 3: 5.2 ms per loop\nCurrent impl:\nlarge_data = torch.zeros(1000, 1000).fill_(0.3).requires_grad_()\ndef newfn_backward(data):\n    large_out = data.hardshrink(0.3)\n    large_out.sum().backward()\n%timeit newfn_backward(large_data)\n####################\n100 loops, best of 3: 6.47 ms per loop\nCUDA:\nCurrent impl:\nlarge_data = torch.zeros(1000, 1000, device=cuda).fill_(0.3)\n%timeit data.hardshrink(0.3)\n####################\n10000 loops, best of 3: 97.1 \u00b5s per loop\nBenchmark:\nlarge_data = torch.zeros(1000, 1000, device=cuda).fill_(0.3)\n%timeit data.mul_(2)\n####################\n10000 loops, best of 3: 51.6 \u00b5s per loop", "body": "## Summary:\r\n1. Added hardshrink() to ATen (CPU + GPU)\r\n2. Removed nn.Hardshrink(), but keeping file \"aten/src/THNN/generic/HardShrink.c\"\r\n3. Reusing nn.Hardshrink() tests in test_nn and including CUDA tests. Not sure test_nn is a good place for hardshrink() since it is no longer under nn\r\n4. Added tests in test_torch\r\n\r\nThis replaces #7695, fixes #4154\r\n\r\n## Future work:\r\n1. not supporting default lambd=0.5 in torch.hardshrink() due to a Scalar bug #8484\r\n\r\n\r\n## Performance:\r\n\r\n### CPU forward:\r\n\r\nPrevious impl:\r\n```python\r\nlarge_data = torch.zeros(1000, 1000).fill_(0.3).requires_grad_()\r\ndef origfn(data):\r\n  f = torch.nn.Hardshrink(0.3)\r\n  large_out = f(data)\r\n%timeit origfn(large_data) \r\n####################\r\n100 loops, best of 3: 2.15 ms per loop\r\n```\r\n\r\nCurrent impl:\r\n```python\r\nlarge_data = torch.zeros(1000, 1000).fill_(0.3).requires_grad_()\r\ndef newfn(data):\r\n    large_out = data.hardshrink(0.3)\r\n%timeit newfn(large_data)\r\n####################\r\n100 loops, best of 3: 2.62 ms per loop\r\n```\r\n\r\n### CPU backward:\r\n\r\nPrevious impl:\r\n```python\r\nlarge_data = torch.zeros(1000, 1000).fill_(0.3).requires_grad_()\r\ndef origfn_backward(data):\r\n    f = torch.nn.Hardshrink(0.3)\r\n    large_out = f(data)\r\n    large_out.sum().backward()\r\n%timeit origfn_backward(large_data)\r\n####################\r\n100 loops, best of 3: 5.2 ms per loop\r\n```\r\n\r\nCurrent impl:\r\n```python\r\nlarge_data = torch.zeros(1000, 1000).fill_(0.3).requires_grad_()\r\ndef newfn_backward(data):\r\n    large_out = data.hardshrink(0.3)\r\n    large_out.sum().backward()\r\n%timeit newfn_backward(large_data)\r\n####################\r\n100 loops, best of 3: 6.47 ms per loop\r\n```\r\n\r\n### CUDA:\r\n\r\nCurrent impl:\r\n```python\r\nlarge_data = torch.zeros(1000, 1000, device=cuda).fill_(0.3)\r\n%timeit data.hardshrink(0.3)\r\n####################\r\n10000 loops, best of 3: 97.1 \u00b5s per loop\r\n```\r\n\r\nBenchmark:\r\n```python\r\nlarge_data = torch.zeros(1000, 1000, device=cuda).fill_(0.3)\r\n%timeit data.mul_(2)\r\n####################\r\n10000 loops, best of 3: 51.6 \u00b5s per loop\r\n```"}