{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396686695", "html_url": "https://github.com/pytorch/pytorch/pull/8117#issuecomment-396686695", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8117", "id": 396686695, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjY4NjY5NQ==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-12T18:21:10Z", "updated_at": "2018-06-12T18:21:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For reasons I don't understand, this PR also allows nn.Hardshrink() to work in both of CPU and CUDA. For instance:</p>\n<pre><code>cuda = torch.device('cuda')\ndata = torch.zeros(2,2, device=cuda).fill_(0.3)\nf = torch.nn.Hardshrink(0.3)\nf(data)\n-------------\ntensor([[ 0.,  0.],\n        [ 0.,  0.]], device='cuda:0')\n</code></pre>\n<p>Is nn.Hardshrink() borrowing the implementation from torch.hardshrink()? Did I miss something?</p>", "body_text": "For reasons I don't understand, this PR also allows nn.Hardshrink() to work in both of CPU and CUDA. For instance:\ncuda = torch.device('cuda')\ndata = torch.zeros(2,2, device=cuda).fill_(0.3)\nf = torch.nn.Hardshrink(0.3)\nf(data)\n-------------\ntensor([[ 0.,  0.],\n        [ 0.,  0.]], device='cuda:0')\n\nIs nn.Hardshrink() borrowing the implementation from torch.hardshrink()? Did I miss something?", "body": "For reasons I don't understand, this PR also allows nn.Hardshrink() to work in both of CPU and CUDA. For instance:\r\n\r\n```\r\ncuda = torch.device('cuda')\r\ndata = torch.zeros(2,2, device=cuda).fill_(0.3)\r\nf = torch.nn.Hardshrink(0.3)\r\nf(data)\r\n-------------\r\ntensor([[ 0.,  0.],\r\n        [ 0.,  0.]], device='cuda:0')\r\n```\r\n\r\nIs nn.Hardshrink() borrowing the implementation from torch.hardshrink()? Did I miss something?"}