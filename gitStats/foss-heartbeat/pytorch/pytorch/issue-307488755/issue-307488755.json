{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5929", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5929/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5929/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5929/events", "html_url": "https://github.com/pytorch/pytorch/issues/5929", "id": 307488755, "node_id": "MDU6SXNzdWUzMDc0ODg3NTU=", "number": 5929, "title": "RuntimeError: cuda runtime error (2) : out of memory when using loss function during my TRAIN loop.", "user": {"login": "BinbinBian", "id": 12493662, "node_id": "MDQ6VXNlcjEyNDkzNjYy", "avatar_url": "https://avatars3.githubusercontent.com/u/12493662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BinbinBian", "html_url": "https://github.com/BinbinBian", "followers_url": "https://api.github.com/users/BinbinBian/followers", "following_url": "https://api.github.com/users/BinbinBian/following{/other_user}", "gists_url": "https://api.github.com/users/BinbinBian/gists{/gist_id}", "starred_url": "https://api.github.com/users/BinbinBian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BinbinBian/subscriptions", "organizations_url": "https://api.github.com/users/BinbinBian/orgs", "repos_url": "https://api.github.com/users/BinbinBian/repos", "events_url": "https://api.github.com/users/BinbinBian/events{/privacy}", "received_events_url": "https://api.github.com/users/BinbinBian/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-22T02:58:19Z", "updated_at": "2018-03-22T09:44:47Z", "closed_at": "2018-03-22T09:44:47Z", "author_association": "NONE", "body_html": "<p>Implementating of CVPR2017 paper \"<a href=\"http://openaccess.thecvf.com/content_cvpr_2017/papers/Krause_A_Hierarchical_Approach_CVPR_2017_paper.pdf\" rel=\"nofollow\">A Hierarchical Approach for Generating Descriptive Image Paragraphs</a>\"</p>\n<p>I can run my model on cpu well with batch size =8<br>\nbut When I am trying to run my model on <em>multi GPU</em>(4 gpu devices), during the <em>TRAIN</em>  loop, when I call the loss function, it results in a out of memory error.<br>\nEven I run the model with  a small batch_size =4, it run out of memory.</p>\n<p>The train loop  code of the model I am running:</p>\n<p>def forward(self, image,  instrs_vec,reference_vec):<br>\n# image: size=(batch,3,224,224)<br>\n# instrs_vec: size=(batch,20,20)<br>\n#S_max=N_max=20</p>\n<pre><code>    batch_size = image.size(0)\n   \n   #vgg16 encoder\n    fc_features, att_features = self.Encoder(image,14)\n  \n    att_features = norm(att_features) #TODO\n    sent_state = self.init_hidden(batch_size)\n    word_log_softmax_outputs = []\n    sent_logistic_outputs =[]\n    for i in range(self.S_max):\n        if i ==0:\n            procedure = self.init_procedure(batch_size)  # (batch_size , 512)TODO\n        else:\n            procedure = self.get_procedure(pre_procedure_words,self.self_att_type)\n        sent_output, sent_state = self.sent_core(procedure , fc_features, att_features, sent_state)\n        topic = self.h2topic(sent_output)\n        sent_logistic = F.log_softmax(self.sent_logistic(sent_output))\n        sent_logistic_outputs.append(sent_logistic)\n        pre_procedure_words_list = []\n        word_state = (topic.unsqueeze(0),topic.unsqueeze(0))\n        for j in range(self.N_max+2):\n            if j == 0:\n                xt = topic\n            else:\n                it = instrs_vec[:, i,j - 1].clone()\n                xt = self.embed(it)\n            word_output, word_state = self.word_LSTM(xt.unsqueeze(0), word_state)\n            word_logit = self.logit(self.dropout(word_output.squeeze(0)))  #TODO  DROPOUT?\n            output = F.log_softmax(word_logit)\n            if j&gt;0:\n                word_log_softmax_outputs.append(output)\n            pre_procedure_words_list.append(word_output.squeeze(0))\n        pre_procedure_words = torch.stack(pre_procedure_words_list,1)\n    word_log_softmax_outputs = torch.cat([_.unsqueeze(1) for _ in word_log_softmax_outputs], 1).contiguous()\n    sent_logistic_outputs = torch.cat([_.unsqueeze(1) for _ in sent_logistic_outputs], 1).contiguous()\n    return word_log_softmax_outputs,sent_logistic_outputs\n</code></pre>\n<p>for i, data in enumerate(train_loader):<br>\nif use_gpu:<br>\ntorch.cuda.synchronize()<br>\ndata_time.update(time.time() - end)<br>\ntmp = [data[0], data[1], data[2], data[3], data[4], data[5], data[6]]<br>\nif use_gpu:<br>\ntmp = [Variable(_ ).cuda( ) for _ in tmp]<br>\nelse:<br>\ntmp = [Variable(_ ) for _ in tmp]<br>\nimg, instrs_vec, instrs_mask, instrs_num, reference_vec, reference_mask, classes = tmp<br>\n# compute output<br>\nword_log_softmax_outputs, sent_logistic_outputs = model(img, instrs_vec, reference_vec)<br>\n# compute loss<br>\nwordRNN_loss = criterion[0](word_log_softmax_outputs, instrs_vec[:, :, 1:], instrs_mask[:, :, 1:])<br>\nsentRNN_loss = criterion[1](sent_logistic_outputs, instrs_num)</p>\n<pre><code>    wordRNN_losses.update(wordRNN_loss.data[0], data[0].size(0))\n    sentRNN_losses.update(sentRNN_loss.data[0], data[0].size(0))\n    if opts.semantic_reg:  #TODO\n        class_loss = criterion[2]\n        # combined loss\n        loss = sentRNN_loss * opts.lambda_sent + \\\n               wordRNN_loss * opts.lambda_word + \\\n               class_loss * opts.lambda_class\n        # measure performance and record losses\n        class_losses.update(class_loss.data[0], data[0].size(0))\n    else:\n        loss = sentRNN_loss * opts.lambda_sent + wordRNN_loss * opts.lambda_word\n    # compute gradient and do Adam step\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n</code></pre>\n<ul>\n<li>OS:linux</li>\n<li>PyTorch version:0.3.1</li>\n<li>How you installed PyTorch (conda, pip, source):pip</li>\n<li>Python version:python2.7</li>\n<li>CUDA/cuDNN version:8</li>\n<li>GPU models and configuration:<br>\n-GPU :TITAN X</li>\n</ul>\n<p><strong>Error output:</strong></p>\n<p>Traceback (most recent call last):<br>\nFile \"train.v1.py\", line 408, in <br>\nmain()<br>\nFile \"train.v1.py\", line 176, in main<br>\ntrain(train_loader, model, criterion, optimizer, epoch)<br>\nFile \"train.v1.py\", line 282, in train<br>\nloss.backward()<br>\nFile \"/home/bbbian/local/anaconda/lib/python2.7/site-packages/torch/autograd/variable.py\", line 167, in backward<br>\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)<br>\nFile \"/home/bbbian/local/anaconda/lib/python2.7/site-packages/torch/autograd/<strong>init</strong>.py\", line 99, in backward<br>\nvariables, grad_variables, retain_graph)<br>\nRuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58</p>", "body_text": "Implementating of CVPR2017 paper \"A Hierarchical Approach for Generating Descriptive Image Paragraphs\"\nI can run my model on cpu well with batch size =8\nbut When I am trying to run my model on multi GPU(4 gpu devices), during the TRAIN  loop, when I call the loss function, it results in a out of memory error.\nEven I run the model with  a small batch_size =4, it run out of memory.\nThe train loop  code of the model I am running:\ndef forward(self, image,  instrs_vec,reference_vec):\n# image: size=(batch,3,224,224)\n# instrs_vec: size=(batch,20,20)\n#S_max=N_max=20\n    batch_size = image.size(0)\n   \n   #vgg16 encoder\n    fc_features, att_features = self.Encoder(image,14)\n  \n    att_features = norm(att_features) #TODO\n    sent_state = self.init_hidden(batch_size)\n    word_log_softmax_outputs = []\n    sent_logistic_outputs =[]\n    for i in range(self.S_max):\n        if i ==0:\n            procedure = self.init_procedure(batch_size)  # (batch_size , 512)TODO\n        else:\n            procedure = self.get_procedure(pre_procedure_words,self.self_att_type)\n        sent_output, sent_state = self.sent_core(procedure , fc_features, att_features, sent_state)\n        topic = self.h2topic(sent_output)\n        sent_logistic = F.log_softmax(self.sent_logistic(sent_output))\n        sent_logistic_outputs.append(sent_logistic)\n        pre_procedure_words_list = []\n        word_state = (topic.unsqueeze(0),topic.unsqueeze(0))\n        for j in range(self.N_max+2):\n            if j == 0:\n                xt = topic\n            else:\n                it = instrs_vec[:, i,j - 1].clone()\n                xt = self.embed(it)\n            word_output, word_state = self.word_LSTM(xt.unsqueeze(0), word_state)\n            word_logit = self.logit(self.dropout(word_output.squeeze(0)))  #TODO  DROPOUT?\n            output = F.log_softmax(word_logit)\n            if j>0:\n                word_log_softmax_outputs.append(output)\n            pre_procedure_words_list.append(word_output.squeeze(0))\n        pre_procedure_words = torch.stack(pre_procedure_words_list,1)\n    word_log_softmax_outputs = torch.cat([_.unsqueeze(1) for _ in word_log_softmax_outputs], 1).contiguous()\n    sent_logistic_outputs = torch.cat([_.unsqueeze(1) for _ in sent_logistic_outputs], 1).contiguous()\n    return word_log_softmax_outputs,sent_logistic_outputs\n\nfor i, data in enumerate(train_loader):\nif use_gpu:\ntorch.cuda.synchronize()\ndata_time.update(time.time() - end)\ntmp = [data[0], data[1], data[2], data[3], data[4], data[5], data[6]]\nif use_gpu:\ntmp = [Variable(_ ).cuda( ) for _ in tmp]\nelse:\ntmp = [Variable(_ ) for _ in tmp]\nimg, instrs_vec, instrs_mask, instrs_num, reference_vec, reference_mask, classes = tmp\n# compute output\nword_log_softmax_outputs, sent_logistic_outputs = model(img, instrs_vec, reference_vec)\n# compute loss\nwordRNN_loss = criterion[0](word_log_softmax_outputs, instrs_vec[:, :, 1:], instrs_mask[:, :, 1:])\nsentRNN_loss = criterion[1](sent_logistic_outputs, instrs_num)\n    wordRNN_losses.update(wordRNN_loss.data[0], data[0].size(0))\n    sentRNN_losses.update(sentRNN_loss.data[0], data[0].size(0))\n    if opts.semantic_reg:  #TODO\n        class_loss = criterion[2]\n        # combined loss\n        loss = sentRNN_loss * opts.lambda_sent + \\\n               wordRNN_loss * opts.lambda_word + \\\n               class_loss * opts.lambda_class\n        # measure performance and record losses\n        class_losses.update(class_loss.data[0], data[0].size(0))\n    else:\n        loss = sentRNN_loss * opts.lambda_sent + wordRNN_loss * opts.lambda_word\n    # compute gradient and do Adam step\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n\nOS:linux\nPyTorch version:0.3.1\nHow you installed PyTorch (conda, pip, source):pip\nPython version:python2.7\nCUDA/cuDNN version:8\nGPU models and configuration:\n-GPU :TITAN X\n\nError output:\nTraceback (most recent call last):\nFile \"train.v1.py\", line 408, in \nmain()\nFile \"train.v1.py\", line 176, in main\ntrain(train_loader, model, criterion, optimizer, epoch)\nFile \"train.v1.py\", line 282, in train\nloss.backward()\nFile \"/home/bbbian/local/anaconda/lib/python2.7/site-packages/torch/autograd/variable.py\", line 167, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\nFile \"/home/bbbian/local/anaconda/lib/python2.7/site-packages/torch/autograd/init.py\", line 99, in backward\nvariables, grad_variables, retain_graph)\nRuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58", "body": "Implementating of CVPR2017 paper \"[A Hierarchical Approach for Generating Descriptive Image Paragraphs](http://openaccess.thecvf.com/content_cvpr_2017/papers/Krause_A_Hierarchical_Approach_CVPR_2017_paper.pdf)\"\r\n\r\nI can run my model on cpu well with batch size =8\r\nbut When I am trying to run my model on _multi GPU_(4 gpu devices), during the _TRAIN_  loop, when I call the loss function, it results in a out of memory error. \r\nEven I run the model with  a small batch_size =4, it run out of memory.\r\n\r\n\r\nThe train loop  code of the model I am running:\r\n\r\n\r\n def forward(self, image,  instrs_vec,reference_vec):\r\n        # image: size=(batch,3,224,224)\r\n        # instrs_vec: size=(batch,20,20)\r\n        #S_max=N_max=20\r\n\r\n        batch_size = image.size(0)\r\n       \r\n       #vgg16 encoder\r\n        fc_features, att_features = self.Encoder(image,14)\r\n      \r\n        att_features = norm(att_features) #TODO\r\n        sent_state = self.init_hidden(batch_size)\r\n        word_log_softmax_outputs = []\r\n        sent_logistic_outputs =[]\r\n        for i in range(self.S_max):\r\n            if i ==0:\r\n                procedure = self.init_procedure(batch_size)  # (batch_size , 512)TODO\r\n            else:\r\n                procedure = self.get_procedure(pre_procedure_words,self.self_att_type)\r\n            sent_output, sent_state = self.sent_core(procedure , fc_features, att_features, sent_state)\r\n            topic = self.h2topic(sent_output)\r\n            sent_logistic = F.log_softmax(self.sent_logistic(sent_output))\r\n            sent_logistic_outputs.append(sent_logistic)\r\n            pre_procedure_words_list = []\r\n            word_state = (topic.unsqueeze(0),topic.unsqueeze(0))\r\n            for j in range(self.N_max+2):\r\n                if j == 0:\r\n                    xt = topic\r\n                else:\r\n                    it = instrs_vec[:, i,j - 1].clone()\r\n                    xt = self.embed(it)\r\n                word_output, word_state = self.word_LSTM(xt.unsqueeze(0), word_state)\r\n                word_logit = self.logit(self.dropout(word_output.squeeze(0)))  #TODO  DROPOUT?\r\n                output = F.log_softmax(word_logit)\r\n                if j>0:\r\n                    word_log_softmax_outputs.append(output)\r\n                pre_procedure_words_list.append(word_output.squeeze(0))\r\n            pre_procedure_words = torch.stack(pre_procedure_words_list,1)\r\n        word_log_softmax_outputs = torch.cat([_.unsqueeze(1) for _ in word_log_softmax_outputs], 1).contiguous()\r\n        sent_logistic_outputs = torch.cat([_.unsqueeze(1) for _ in sent_logistic_outputs], 1).contiguous()\r\n        return word_log_softmax_outputs,sent_logistic_outputs\r\n\r\n\r\n for i, data in enumerate(train_loader):\r\n        if use_gpu:\r\n            torch.cuda.synchronize()\r\n        data_time.update(time.time() - end)\r\n        tmp = [data[0], data[1], data[2], data[3], data[4], data[5], data[6]]\r\n        if use_gpu:\r\n            tmp = [Variable(_ ).cuda( ) for _ in tmp]\r\n        else:\r\n            tmp = [Variable(_ ) for _ in tmp]\r\n        img, instrs_vec, instrs_mask, instrs_num, reference_vec, reference_mask, classes = tmp\r\n        # compute output\r\n        word_log_softmax_outputs, sent_logistic_outputs = model(img, instrs_vec, reference_vec)\r\n        # compute loss\r\n        wordRNN_loss = criterion[0](word_log_softmax_outputs, instrs_vec[:, :, 1:], instrs_mask[:, :, 1:])\r\n        sentRNN_loss = criterion[1](sent_logistic_outputs, instrs_num)\r\n\r\n        wordRNN_losses.update(wordRNN_loss.data[0], data[0].size(0))\r\n        sentRNN_losses.update(sentRNN_loss.data[0], data[0].size(0))\r\n        if opts.semantic_reg:  #TODO\r\n            class_loss = criterion[2]\r\n            # combined loss\r\n            loss = sentRNN_loss * opts.lambda_sent + \\\r\n                   wordRNN_loss * opts.lambda_word + \\\r\n                   class_loss * opts.lambda_class\r\n            # measure performance and record losses\r\n            class_losses.update(class_loss.data[0], data[0].size(0))\r\n        else:\r\n            loss = sentRNN_loss * opts.lambda_sent + wordRNN_loss * opts.lambda_word\r\n        # compute gradient and do Adam step\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n- OS:linux\r\n- PyTorch version:0.3.1\r\n- How you installed PyTorch (conda, pip, source):pip\r\n- Python version:python2.7\r\n- CUDA/cuDNN version:8\r\n- GPU models and configuration: \r\n -GPU :TITAN X\r\n\r\n **Error output:**\r\n\r\nTraceback (most recent call last):\r\n  File \"train.v1.py\", line 408, in <module>\r\n    main()\r\n  File \"train.v1.py\", line 176, in main\r\n    train(train_loader, model, criterion, optimizer, epoch)\r\n  File \"train.v1.py\", line 282, in train\r\n    loss.backward()\r\n  File \"/home/bbbian/local/anaconda/lib/python2.7/site-packages/torch/autograd/variable.py\", line 167, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"/home/bbbian/local/anaconda/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 99, in backward\r\n    variables, grad_variables, retain_graph)\r\nRuntimeError: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:58\r\n\r\n\r\n"}