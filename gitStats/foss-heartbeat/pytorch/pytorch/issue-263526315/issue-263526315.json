{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3009", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3009/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3009/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3009/events", "html_url": "https://github.com/pytorch/pytorch/issues/3009", "id": 263526315, "node_id": "MDU6SXNzdWUyNjM1MjYzMTU=", "number": 3009, "title": "CUDA Memory Leak in torch.qr", "user": {"login": "chrischoy", "id": 5080549, "node_id": "MDQ6VXNlcjUwODA1NDk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5080549?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chrischoy", "html_url": "https://github.com/chrischoy", "followers_url": "https://api.github.com/users/chrischoy/followers", "following_url": "https://api.github.com/users/chrischoy/following{/other_user}", "gists_url": "https://api.github.com/users/chrischoy/gists{/gist_id}", "starred_url": "https://api.github.com/users/chrischoy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chrischoy/subscriptions", "organizations_url": "https://api.github.com/users/chrischoy/orgs", "repos_url": "https://api.github.com/users/chrischoy/repos", "events_url": "https://api.github.com/users/chrischoy/events{/privacy}", "received_events_url": "https://api.github.com/users/chrischoy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2017-10-06T18:01:51Z", "updated_at": "2017-10-07T04:35:06Z", "closed_at": "2017-10-07T04:35:06Z", "author_association": "NONE", "body_html": "<p>It seems like the function <code>torch.qr</code> leaks memory.<br>\nI'm using <code>torch.__version__</code>  0.2.0_4 on anaconda python 3.6 CUDA 8.0 using the standard conda installation.</p>\n<pre><code>import torch\nB = torch.rand(4000, 1000).cuda()   # nvidia-smi output 298MiB\n_ = torch.qr(B)   # 388MiB\nfor i in range(100): torch.qr(B)  # Constantly increases until 1941MiB\n</code></pre>\n<p>So the memory footprint increases up to 1.9G though each <code>qr</code> takes about 100M.<br>\ncompared to <code>svd</code> where it takes up more memory but doesn't leak memory and the memory foot print fluctuates between 414M and 432M and back to 414M.</p>\n<pre><code>import torch\nB = torch.rand(4000, 1000).cuda()   # nvidia-smi output 298MiB\n_ = torch.svd(B)   # 399MiB\nfor i in range(100): torch.svd(B)  # fluctuates between 414 to 432 and back to 414\n</code></pre>\n<p>Aside from that, is there a reason for using Magma instead of cuBLAS?</p>", "body_text": "It seems like the function torch.qr leaks memory.\nI'm using torch.__version__  0.2.0_4 on anaconda python 3.6 CUDA 8.0 using the standard conda installation.\nimport torch\nB = torch.rand(4000, 1000).cuda()   # nvidia-smi output 298MiB\n_ = torch.qr(B)   # 388MiB\nfor i in range(100): torch.qr(B)  # Constantly increases until 1941MiB\n\nSo the memory footprint increases up to 1.9G though each qr takes about 100M.\ncompared to svd where it takes up more memory but doesn't leak memory and the memory foot print fluctuates between 414M and 432M and back to 414M.\nimport torch\nB = torch.rand(4000, 1000).cuda()   # nvidia-smi output 298MiB\n_ = torch.svd(B)   # 399MiB\nfor i in range(100): torch.svd(B)  # fluctuates between 414 to 432 and back to 414\n\nAside from that, is there a reason for using Magma instead of cuBLAS?", "body": "It seems like the function `torch.qr` leaks memory.\r\nI'm using `torch.__version__`  0.2.0_4 on anaconda python 3.6 CUDA 8.0 using the standard conda installation.\r\n\r\n```\r\nimport torch\r\nB = torch.rand(4000, 1000).cuda()   # nvidia-smi output 298MiB\r\n_ = torch.qr(B)   # 388MiB\r\nfor i in range(100): torch.qr(B)  # Constantly increases until 1941MiB\r\n```\r\n\r\nSo the memory footprint increases up to 1.9G though each `qr` takes about 100M.\r\ncompared to `svd` where it takes up more memory but doesn't leak memory and the memory foot print fluctuates between 414M and 432M and back to 414M.\r\n\r\n```\r\nimport torch\r\nB = torch.rand(4000, 1000).cuda()   # nvidia-smi output 298MiB\r\n_ = torch.svd(B)   # 399MiB\r\nfor i in range(100): torch.svd(B)  # fluctuates between 414 to 432 and back to 414\r\n```\r\n\r\nAside from that, is there a reason for using Magma instead of cuBLAS?"}