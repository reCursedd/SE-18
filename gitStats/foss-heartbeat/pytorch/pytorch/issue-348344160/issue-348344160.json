{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10299", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10299/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10299/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10299/events", "html_url": "https://github.com/pytorch/pytorch/issues/10299", "id": 348344160, "node_id": "MDU6SXNzdWUzNDgzNDQxNjA=", "number": 10299, "title": "Hard-coded rpaths on OSX build can lead to hard to debug hysteresis problems", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-07T14:22:53Z", "updated_at": "2018-08-07T14:22:53Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>When <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"344569939\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9836\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9836/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9836\">#9836</a> was merged, it passed CI, both in PR and on master. A few hours later, all builds started failing (though somewhat intermittently). This PR was initially ruled out as cause of the outage, because it didn't fail on master. But it in fact was the root cause.</p>\n<p>The problem is that the new codepath incorrectly relied on build products existing from <em>a different</em> build workspace, unrelated to the build in question. We determined this by comparing the machines for which builds were passing (by Tuesday, this was only high-sierra-20) and the machines which were failing, and there was a key difference between their directory structure:</p>\n<pre><code>jenkins-worker-macos-high-sierra-20:pytorch-builds administrator$ find . -name libtorch.1.dylib\n./cpp-build/caffe2/lib/libtorch.1.dylib\n./cpp-build/install/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/build/lib/libtorch.1.dylib    # THE KEY FILE\n./pytorch-macos-10.13-py3-build/build/lib.macosx-10.7-x86_64-3.6/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/torch/lib/tmp_install/lib/libtorch.1.dylib\n\njenkins-worker-macos-high-sierra-19:pytorch-builds administrator$ find . -name libtorch.1.dylib\n./cpp-build/caffe2/lib/libtorch.1.dylib\n./cpp-build/install/lib/libtorch.1.dylib\n./pytorch-macos-10.13-cuda9.2-cudnn7-py3-build/build/lib.macosx-10.7-x86_64-3.6/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-cuda9.2-cudnn7-py3-build/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-cuda9.2-cudnn7-py3-build/torch/lib/tmp_install/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/build/lib.macosx-10.7-x86_64-3.6/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/torch/lib/tmp_install/lib/libtorch.1.dylib\n</code></pre>\n<p>Further inspection of the <code>test_api</code> binary reveals that it has its RPATH hardcoded to this directory:</p>\n<pre><code>$ otool -l /var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/test_binaries/build/bin/test_api\n...\nLoad command 21\n          cmd LC_RPATH\n      cmdsize 104\n         path /private/var/lib/jenkins/workspace/pytorch-builds/pytorch-macos-10.13-py3-build/build/lib (offset 12)\n</code></pre>\n<p>In my estimation, while <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"344569939\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9836\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/9836/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/9836\">#9836</a> is the PR that \"broke the build\", there are few structural problems which contributed to the problem:</p>\n<ol>\n<li>Our OS X builds have inadequate sandboxing. In this particular case, a sandbox rule that prevented reads to workspaces that were not the current workspace would have immediately smoked out the problem. But there is independently a lack of good hygiene: for example, the OS X builds are installing into a global Miniconda environment <code>/var/lib/jenkins/pytorch-ci-env/miniconda3</code>, which is itself a footgun waiting to be unloaded.</li>\n<li>We do not appear to have testing which directly checks that the RPATHs of executables we distribute \"make sense\". We should determine what we <em>expect</em> the RPATH of various libraries and executables, and write tests which directly interrogate the rpath (e.g., using otool) and check that this is indeed the case. Under no circumstances would an RPATH that contains a hard coded build directory be what we would \"expect\".</li>\n</ol>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4063635\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yf225\">@yf225</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1388690\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/anderspapitto\">@anderspapitto</a></p>", "body_text": "When #9836 was merged, it passed CI, both in PR and on master. A few hours later, all builds started failing (though somewhat intermittently). This PR was initially ruled out as cause of the outage, because it didn't fail on master. But it in fact was the root cause.\nThe problem is that the new codepath incorrectly relied on build products existing from a different build workspace, unrelated to the build in question. We determined this by comparing the machines for which builds were passing (by Tuesday, this was only high-sierra-20) and the machines which were failing, and there was a key difference between their directory structure:\njenkins-worker-macos-high-sierra-20:pytorch-builds administrator$ find . -name libtorch.1.dylib\n./cpp-build/caffe2/lib/libtorch.1.dylib\n./cpp-build/install/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/build/lib/libtorch.1.dylib    # THE KEY FILE\n./pytorch-macos-10.13-py3-build/build/lib.macosx-10.7-x86_64-3.6/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/torch/lib/tmp_install/lib/libtorch.1.dylib\n\njenkins-worker-macos-high-sierra-19:pytorch-builds administrator$ find . -name libtorch.1.dylib\n./cpp-build/caffe2/lib/libtorch.1.dylib\n./cpp-build/install/lib/libtorch.1.dylib\n./pytorch-macos-10.13-cuda9.2-cudnn7-py3-build/build/lib.macosx-10.7-x86_64-3.6/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-cuda9.2-cudnn7-py3-build/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-cuda9.2-cudnn7-py3-build/torch/lib/tmp_install/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/build/lib.macosx-10.7-x86_64-3.6/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/torch/lib/libtorch.1.dylib\n./pytorch-macos-10.13-py3-build/torch/lib/tmp_install/lib/libtorch.1.dylib\n\nFurther inspection of the test_api binary reveals that it has its RPATH hardcoded to this directory:\n$ otool -l /var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/test_binaries/build/bin/test_api\n...\nLoad command 21\n          cmd LC_RPATH\n      cmdsize 104\n         path /private/var/lib/jenkins/workspace/pytorch-builds/pytorch-macos-10.13-py3-build/build/lib (offset 12)\n\nIn my estimation, while #9836 is the PR that \"broke the build\", there are few structural problems which contributed to the problem:\n\nOur OS X builds have inadequate sandboxing. In this particular case, a sandbox rule that prevented reads to workspaces that were not the current workspace would have immediately smoked out the problem. But there is independently a lack of good hygiene: for example, the OS X builds are installing into a global Miniconda environment /var/lib/jenkins/pytorch-ci-env/miniconda3, which is itself a footgun waiting to be unloaded.\nWe do not appear to have testing which directly checks that the RPATHs of executables we distribute \"make sense\". We should determine what we expect the RPATH of various libraries and executables, and write tests which directly interrogate the rpath (e.g., using otool) and check that this is indeed the case. Under no circumstances would an RPATH that contains a hard coded build directory be what we would \"expect\".\n\nCC @yf225 @anderspapitto", "body": "When #9836 was merged, it passed CI, both in PR and on master. A few hours later, all builds started failing (though somewhat intermittently). This PR was initially ruled out as cause of the outage, because it didn't fail on master. But it in fact was the root cause.\r\n\r\nThe problem is that the new codepath incorrectly relied on build products existing from *a different* build workspace, unrelated to the build in question. We determined this by comparing the machines for which builds were passing (by Tuesday, this was only high-sierra-20) and the machines which were failing, and there was a key difference between their directory structure:\r\n\r\n```\r\njenkins-worker-macos-high-sierra-20:pytorch-builds administrator$ find . -name libtorch.1.dylib\r\n./cpp-build/caffe2/lib/libtorch.1.dylib\r\n./cpp-build/install/lib/libtorch.1.dylib\r\n./pytorch-macos-10.13-py3-build/build/lib/libtorch.1.dylib    # THE KEY FILE\r\n./pytorch-macos-10.13-py3-build/build/lib.macosx-10.7-x86_64-3.6/torch/lib/libtorch.1.dylib\r\n./pytorch-macos-10.13-py3-build/torch/lib/libtorch.1.dylib\r\n./pytorch-macos-10.13-py3-build/torch/lib/tmp_install/lib/libtorch.1.dylib\r\n\r\njenkins-worker-macos-high-sierra-19:pytorch-builds administrator$ find . -name libtorch.1.dylib\r\n./cpp-build/caffe2/lib/libtorch.1.dylib\r\n./cpp-build/install/lib/libtorch.1.dylib\r\n./pytorch-macos-10.13-cuda9.2-cudnn7-py3-build/build/lib.macosx-10.7-x86_64-3.6/torch/lib/libtorch.1.dylib\r\n./pytorch-macos-10.13-cuda9.2-cudnn7-py3-build/torch/lib/libtorch.1.dylib\r\n./pytorch-macos-10.13-cuda9.2-cudnn7-py3-build/torch/lib/tmp_install/lib/libtorch.1.dylib\r\n./pytorch-macos-10.13-py3-build/build/lib.macosx-10.7-x86_64-3.6/torch/lib/libtorch.1.dylib\r\n./pytorch-macos-10.13-py3-build/torch/lib/libtorch.1.dylib\r\n./pytorch-macos-10.13-py3-build/torch/lib/tmp_install/lib/libtorch.1.dylib\r\n```\r\n\r\nFurther inspection of the `test_api` binary reveals that it has its RPATH hardcoded to this directory:\r\n\r\n```\r\n$ otool -l /var/lib/jenkins/pytorch-ci-env/miniconda3/lib/python3.6/site-packages/torch/test_binaries/build/bin/test_api\r\n...\r\nLoad command 21\r\n          cmd LC_RPATH\r\n      cmdsize 104\r\n         path /private/var/lib/jenkins/workspace/pytorch-builds/pytorch-macos-10.13-py3-build/build/lib (offset 12)\r\n```\r\n\r\nIn my estimation, while #9836 is the PR that \"broke the build\", there are few structural problems which contributed to the problem:\r\n\r\n1. Our OS X builds have inadequate sandboxing. In this particular case, a sandbox rule that prevented reads to workspaces that were not the current workspace would have immediately smoked out the problem. But there is independently a lack of good hygiene: for example, the OS X builds are installing into a global Miniconda environment `/var/lib/jenkins/pytorch-ci-env/miniconda3`, which is itself a footgun waiting to be unloaded.\r\n2. We do not appear to have testing which directly checks that the RPATHs of executables we distribute \"make sense\". We should determine what we *expect* the RPATH of various libraries and executables, and write tests which directly interrogate the rpath (e.g., using otool) and check that this is indeed the case. Under no circumstances would an RPATH that contains a hard coded build directory be what we would \"expect\".\r\n\r\nCC @yf225 @anderspapitto "}