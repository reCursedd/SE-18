{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424882359", "html_url": "https://github.com/pytorch/pytorch/issues/12006#issuecomment-424882359", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12006", "id": 424882359, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDg4MjM1OQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T21:53:38Z", "updated_at": "2018-09-26T21:54:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So by making swapping the 0th and 2nd dimension of weight if the first is larger, I get this down a bit, but not remarkably so. (I also moved BN to ATen/native, but that shouldn't matter that much, I think).</p>\n<pre><code>batch_size 5\nforward: 0.018\nbackward: 0.016\ntotal: 0.034\nbatch_size 6\nforward: 0.006\nbackward: 0.019\ntotal: 0.025\nbatch_size 7\nforward: 0.038\nbackward: 0.059\ntotal: 0.097\nbatch_size 8\nforward: 0.045\nbackward: 0.068\ntotal: 0.114\n</code></pre>\n<p>So I think there might be more to do.</p>", "body_text": "So by making swapping the 0th and 2nd dimension of weight if the first is larger, I get this down a bit, but not remarkably so. (I also moved BN to ATen/native, but that shouldn't matter that much, I think).\nbatch_size 5\nforward: 0.018\nbackward: 0.016\ntotal: 0.034\nbatch_size 6\nforward: 0.006\nbackward: 0.019\ntotal: 0.025\nbatch_size 7\nforward: 0.038\nbackward: 0.059\ntotal: 0.097\nbatch_size 8\nforward: 0.045\nbackward: 0.068\ntotal: 0.114\n\nSo I think there might be more to do.", "body": "So by making swapping the 0th and 2nd dimension of weight if the first is larger, I get this down a bit, but not remarkably so. (I also moved BN to ATen/native, but that shouldn't matter that much, I think).\r\n```\r\nbatch_size 5\r\nforward: 0.018\r\nbackward: 0.016\r\ntotal: 0.034\r\nbatch_size 6\r\nforward: 0.006\r\nbackward: 0.019\r\ntotal: 0.025\r\nbatch_size 7\r\nforward: 0.038\r\nbackward: 0.059\r\ntotal: 0.097\r\nbatch_size 8\r\nforward: 0.045\r\nbackward: 0.068\r\ntotal: 0.114\r\n```\r\nSo I think there might be more to do."}