{"url": "https://api.github.com/repos/pytorch/pytorch/issues/841", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/841/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/841/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/841/events", "html_url": "https://github.com/pytorch/pytorch/issues/841", "id": 210142362, "node_id": "MDU6SXNzdWUyMTAxNDIzNjI=", "number": 841, "title": "Just one cpu core in use, until I use numpy...", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-02-24T19:52:40Z", "updated_at": "2017-10-29T23:50:10Z", "closed_at": "2017-02-24T21:38:55Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I followed the installation instructions for linux that you have on GitHub, including the optional dependencies part.</p>\n<p>I run the following code:</p>\n<pre><code>import torch\na = torch.rand(10000,100000)\nb = torch.rand(100000,10000)\nc = a.mm(b)\n</code></pre>\n<p>However, when I run that script in a Linux machine where I installed python with Anaconda, and I also installed mkl and anaconda accelerate, that script uses just one core. I have tried compiling from source, and also installing pytorch with \"conda install\", and also not installing the accelerate library, but it never uses more than one core during that script.</p>\n<p>However, if I first run this script, which uses all cores:</p>\n<pre lang=\"import\" data-meta=\"numpy as np\"><code>a = np.random.rand(10000,100000)\nb = np.random.rand(100000,10000)\nc = a.dot(b)\n</code></pre>\n<p>And afterwards I run again the torch one, then it uses all cores of the machine.</p>\n<p>What am I missing? Do I have to set some flag?</p>\n<p>I have also observed that, if I don't run the numpy script first, when I train a neural network, it seems to use all cores at some point when computing the gradients, but not during the forward pass.</p>\n<p>This is what the compiler says while compiling:</p>\n<pre><code>-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\n--   Library mkl_intel_lp64: /home/user/local_installs/anaconda3/lib/libmkl_intel_lp64.so\n--   Library mkl_gnu_thread: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m]\n--   Library mkl_intel_lp64: /home/user/local_installs/anaconda3/lib/libmkl_intel_lp64.so\n--   Library mkl_intel_thread: /home/user/local_installs/anaconda3/lib/libmkl_intel_thread.so\n--   Library mkl_core: /home/user/local_installs/anaconda3/lib/libmkl_core.so\n--   Library pthread: /usr/lib64/libpthread.so\n--   Library m: /usr/lib64/libm.so\n-- Looking for cblas_sgemm\n-- Looking for cblas_sgemm - not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m]\n--   Library mkl_intel_lp64: /home/user/local_installs/anaconda3/lib/libmkl_intel_lp64.so\n--   Library mkl_sequential: /home/user/local_installs/anaconda3/lib/libmkl_sequential.so\n--   Library mkl_core: /home/user/local_installs/anaconda3/lib/libmkl_core.so\n--   Library m: /usr/lib64/libm.so\n-- Looking for cblas_sgemm\n-- Looking for cblas_sgemm - found\n-- MKL library found\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\n-- Performing Test BLAS_F2C_FLOAT_WORKS\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\n-- Found a library with BLAS API (mkl).\n-- Found a library with LAPACK API. (mkl)\n</code></pre>", "body_text": "Hi,\nI followed the installation instructions for linux that you have on GitHub, including the optional dependencies part.\nI run the following code:\nimport torch\na = torch.rand(10000,100000)\nb = torch.rand(100000,10000)\nc = a.mm(b)\n\nHowever, when I run that script in a Linux machine where I installed python with Anaconda, and I also installed mkl and anaconda accelerate, that script uses just one core. I have tried compiling from source, and also installing pytorch with \"conda install\", and also not installing the accelerate library, but it never uses more than one core during that script.\nHowever, if I first run this script, which uses all cores:\na = np.random.rand(10000,100000)\nb = np.random.rand(100000,10000)\nc = a.dot(b)\n\nAnd afterwards I run again the torch one, then it uses all cores of the machine.\nWhat am I missing? Do I have to set some flag?\nI have also observed that, if I don't run the numpy script first, when I train a neural network, it seems to use all cores at some point when computing the gradients, but not during the forward pass.\nThis is what the compiler says while compiling:\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m]\n--   Library mkl_gf: not found\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\n--   Library mkl_intel_lp64: /home/user/local_installs/anaconda3/lib/libmkl_intel_lp64.so\n--   Library mkl_gnu_thread: not found\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m]\n--   Library mkl_intel_lp64: /home/user/local_installs/anaconda3/lib/libmkl_intel_lp64.so\n--   Library mkl_intel_thread: /home/user/local_installs/anaconda3/lib/libmkl_intel_thread.so\n--   Library mkl_core: /home/user/local_installs/anaconda3/lib/libmkl_core.so\n--   Library pthread: /usr/lib64/libpthread.so\n--   Library m: /usr/lib64/libm.so\n-- Looking for cblas_sgemm\n-- Looking for cblas_sgemm - not found\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m]\n--   Library mkl_intel: not found\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m]\n--   Library mkl_intel: not found\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m]\n--   Library mkl_gf_lp64: not found\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m]\n--   Library mkl_gf: not found\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m]\n--   Library mkl_intel_lp64: /home/user/local_installs/anaconda3/lib/libmkl_intel_lp64.so\n--   Library mkl_sequential: /home/user/local_installs/anaconda3/lib/libmkl_sequential.so\n--   Library mkl_core: /home/user/local_installs/anaconda3/lib/libmkl_core.so\n--   Library m: /usr/lib64/libm.so\n-- Looking for cblas_sgemm\n-- Looking for cblas_sgemm - found\n-- MKL library found\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\n-- Performing Test BLAS_F2C_FLOAT_WORKS\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\n-- Found a library with BLAS API (mkl).\n-- Found a library with LAPACK API. (mkl)", "body": "Hi,\r\n\r\nI followed the installation instructions for linux that you have on GitHub, including the optional dependencies part.\r\n\r\nI run the following code:\r\n\r\n```\r\nimport torch\r\na = torch.rand(10000,100000)\r\nb = torch.rand(100000,10000)\r\nc = a.mm(b)\r\n```\r\n\r\nHowever, when I run that script in a Linux machine where I installed python with Anaconda, and I also installed mkl and anaconda accelerate, that script uses just one core. I have tried compiling from source, and also installing pytorch with \"conda install\", and also not installing the accelerate library, but it never uses more than one core during that script.\r\n\r\nHowever, if I first run this script, which uses all cores:\r\n```import numpy as np\r\na = np.random.rand(10000,100000)\r\nb = np.random.rand(100000,10000)\r\nc = a.dot(b)\r\n```\r\n\r\nAnd afterwards I run again the torch one, then it uses all cores of the machine.\r\n\r\nWhat am I missing? Do I have to set some flag?\r\n\r\nI have also observed that, if I don't run the numpy script first, when I train a neural network, it seems to use all cores at some point when computing the gradients, but not during the forward pass.\r\n\r\nThis is what the compiler says while compiling:\r\n\r\n```\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_intel_thread - mkl_core - pthread - m]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_gnu_thread - mkl_core - pthread - m]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_gf - mkl_intel_thread - mkl_core - pthread - m]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_gnu_thread - mkl_core - pthread - m]\r\n--   Library mkl_intel_lp64: /home/user/local_installs/anaconda3/lib/libmkl_intel_lp64.so\r\n--   Library mkl_gnu_thread: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_intel_thread - mkl_core - pthread - m]\r\n--   Library mkl_intel_lp64: /home/user/local_installs/anaconda3/lib/libmkl_intel_lp64.so\r\n--   Library mkl_intel_thread: /home/user/local_installs/anaconda3/lib/libmkl_intel_thread.so\r\n--   Library mkl_core: /home/user/local_installs/anaconda3/lib/libmkl_core.so\r\n--   Library pthread: /usr/lib64/libpthread.so\r\n--   Library m: /usr/lib64/libm.so\r\n-- Looking for cblas_sgemm\r\n-- Looking for cblas_sgemm - not found\r\n-- Checking for [mkl_intel - mkl_gnu_thread - mkl_core - pthread - m]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_intel - mkl_intel_thread - mkl_core - pthread - m]\r\n--   Library mkl_intel: not found\r\n-- Checking for [mkl_gf_lp64 - mkl_sequential - mkl_core - m]\r\n--   Library mkl_gf_lp64: not found\r\n-- Checking for [mkl_gf - mkl_sequential - mkl_core - m]\r\n--   Library mkl_gf: not found\r\n-- Checking for [mkl_intel_lp64 - mkl_sequential - mkl_core - m]\r\n--   Library mkl_intel_lp64: /home/user/local_installs/anaconda3/lib/libmkl_intel_lp64.so\r\n--   Library mkl_sequential: /home/user/local_installs/anaconda3/lib/libmkl_sequential.so\r\n--   Library mkl_core: /home/user/local_installs/anaconda3/lib/libmkl_core.so\r\n--   Library m: /usr/lib64/libm.so\r\n-- Looking for cblas_sgemm\r\n-- Looking for cblas_sgemm - found\r\n-- MKL library found\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Found a library with BLAS API (mkl).\r\n-- Found a library with LAPACK API. (mkl)\r\n```"}