{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1715", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1715/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1715/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1715/events", "html_url": "https://github.com/pytorch/pytorch/pull/1715", "id": 233426603, "node_id": "MDExOlB1bGxSZXF1ZXN0MTIzODczODI0", "number": 1715, "title": "Add DistributedDataParallel", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-06-04T11:02:54Z", "updated_at": "2018-11-23T15:33:45Z", "closed_at": "2017-06-13T02:21:13Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1715", "html_url": "https://github.com/pytorch/pytorch/pull/1715", "diff_url": "https://github.com/pytorch/pytorch/pull/1715.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1715.patch"}, "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">import</span> torchvision\n\ndist.init_process_group(<span class=\"pl-v\">backend</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>gloo<span class=\"pl-pds\">'</span></span>)\n\nmodel <span class=\"pl-k\">=</span> torchvision.models.resnet50().cuda()\nmodel <span class=\"pl-k\">=</span> torch.nn.DistributedDataParallel(model) <span class=\"pl-c\"><span class=\"pl-c\">#</span> prepend Distributed</span>\n\ndataset <span class=\"pl-k\">=</span> <span class=\"pl-c1\">...</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> so each process sees only a subset of the whole dataset</span>\nsampler <span class=\"pl-k\">=</span> torch.utils.data.DistributedSampler(dataset)\ndata_loader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n        <span class=\"pl-c1\">...</span>, <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>sampler)\n\n<span class=\"pl-k\">for</span> batch, target <span class=\"pl-k\">in</span> data_loader:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> optimize model, log, etc.</span></pre></div>", "body_text": "import torch\nimport torch.distributed as dist\nimport torchvision\n\ndist.init_process_group(backend='gloo')\n\nmodel = torchvision.models.resnet50().cuda()\nmodel = torch.nn.DistributedDataParallel(model) # prepend Distributed\n\ndataset = ...\n# so each process sees only a subset of the whole dataset\nsampler = torch.utils.data.DistributedSampler(dataset)\ndata_loader = torch.utils.data.DataLoader(\n        ..., sampler=sampler)\n\nfor batch, target in data_loader:\n    # optimize model, log, etc.", "body": "```python\r\nimport torch\r\nimport torch.distributed as dist\r\nimport torchvision\r\n\r\ndist.init_process_group(backend='gloo')\r\n\r\nmodel = torchvision.models.resnet50().cuda()\r\nmodel = torch.nn.DistributedDataParallel(model) # prepend Distributed\r\n\r\ndataset = ...\r\n# so each process sees only a subset of the whole dataset\r\nsampler = torch.utils.data.DistributedSampler(dataset)\r\ndata_loader = torch.utils.data.DataLoader(\r\n        ..., sampler=sampler)\r\n\r\nfor batch, target in data_loader:\r\n    # optimize model, log, etc.\r\n```"}