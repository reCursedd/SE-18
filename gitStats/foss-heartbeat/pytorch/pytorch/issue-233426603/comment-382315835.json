{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/382315835", "html_url": "https://github.com/pytorch/pytorch/pull/1715#issuecomment-382315835", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1715", "id": 382315835, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjMxNTgzNQ==", "user": {"login": "acgtyrant", "id": 3921062, "node_id": "MDQ6VXNlcjM5MjEwNjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3921062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acgtyrant", "html_url": "https://github.com/acgtyrant", "followers_url": "https://api.github.com/users/acgtyrant/followers", "following_url": "https://api.github.com/users/acgtyrant/following{/other_user}", "gists_url": "https://api.github.com/users/acgtyrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/acgtyrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acgtyrant/subscriptions", "organizations_url": "https://api.github.com/users/acgtyrant/orgs", "repos_url": "https://api.github.com/users/acgtyrant/repos", "events_url": "https://api.github.com/users/acgtyrant/events{/privacy}", "received_events_url": "https://api.github.com/users/acgtyrant/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-18T08:57:35Z", "updated_at": "2018-04-18T08:57:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Hi, as far as I understand, the <code>DistributedDataParallel</code> <code>nccl.reduce</code> all gradients from all machines to the machine whose rank is 0(device to device), and every machine <code>comm.reduce</code> all gradients from its GPUs to the device 0 respectively again. After every machine updates the module in device 0 respectively, the module in device 0 will <code>_sync_params</code> the parameters to others devices respectively in the forward function. Right?</p>\n<p>I am suprised that the <code>DistributedDataParallel</code> does not broadcast the parameters from one machine to all machines like DataParallel broadcasting the parameters from device 0 to all devices, I think this means that the traning modules in different machines are not same.</p>", "body_text": "@apaszke Hi, as far as I understand, the DistributedDataParallel nccl.reduce all gradients from all machines to the machine whose rank is 0(device to device), and every machine comm.reduce all gradients from its GPUs to the device 0 respectively again. After every machine updates the module in device 0 respectively, the module in device 0 will _sync_params the parameters to others devices respectively in the forward function. Right?\nI am suprised that the DistributedDataParallel does not broadcast the parameters from one machine to all machines like DataParallel broadcasting the parameters from device 0 to all devices, I think this means that the traning modules in different machines are not same.", "body": "@apaszke Hi, as far as I understand, the `DistributedDataParallel` `nccl.reduce` all gradients from all machines to the machine whose rank is 0(device to device), and every machine `comm.reduce` all gradients from its GPUs to the device 0 respectively again. After every machine updates the module in device 0 respectively, the module in device 0 will `_sync_params` the parameters to others devices respectively in the forward function. Right?\r\n\r\nI am suprised that the `DistributedDataParallel` does not broadcast the parameters from one machine to all machines like DataParallel broadcasting the parameters from device 0 to all devices, I think this means that the traning modules in different machines are not same."}