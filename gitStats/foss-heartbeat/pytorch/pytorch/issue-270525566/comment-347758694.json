{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/347758694", "html_url": "https://github.com/pytorch/pytorch/pull/3435#issuecomment-347758694", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3435", "id": 347758694, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Nzc1ODY5NA==", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T05:39:58Z", "updated_at": "2017-11-29T05:39:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a></p>\n<p>Added two commits: The first one used the correct PySequence Fast for everyone in the module.cpp and added the \"r\" back. Also changed and tested and made sure that the invalid backend error can be thrown.</p>\n<p>The second one removed the limitation that each group is bound to a given device sequence. But I don't feel comfortable to cache many NCCL communicators. So the current design is, for each process group, we only cache a single NCCL communicator (say with device sequence 0,1,2,3,4,5,6,7) If there is a new collective NCCL call with a new device sequence (say 1,0,2,3,4,5,6,7). We will destroy the existing cached NCCL communicator (with sequence 0,1,2,3,4,5,6,7), rebuild a new communicator and cache this one instead. This design is pretty clean and simple. I tested with different new device sequence and made sure it worked. This shouldn't be a release blocker for now</p>\n<p>For the NCCL deadlock, it's not super clear to me why but it happens on several consecutive broadcast calls using the same communicator. If the communicator is rebuilt, it will work (as the current workaround).</p>", "body_text": "@apaszke\nAdded two commits: The first one used the correct PySequence Fast for everyone in the module.cpp and added the \"r\" back. Also changed and tested and made sure that the invalid backend error can be thrown.\nThe second one removed the limitation that each group is bound to a given device sequence. But I don't feel comfortable to cache many NCCL communicators. So the current design is, for each process group, we only cache a single NCCL communicator (say with device sequence 0,1,2,3,4,5,6,7) If there is a new collective NCCL call with a new device sequence (say 1,0,2,3,4,5,6,7). We will destroy the existing cached NCCL communicator (with sequence 0,1,2,3,4,5,6,7), rebuild a new communicator and cache this one instead. This design is pretty clean and simple. I tested with different new device sequence and made sure it worked. This shouldn't be a release blocker for now\nFor the NCCL deadlock, it's not super clear to me why but it happens on several consecutive broadcast calls using the same communicator. If the communicator is rebuilt, it will work (as the current workaround).", "body": "@apaszke  \r\n\r\nAdded two commits: The first one used the correct PySequence Fast for everyone in the module.cpp and added the \"r\" back. Also changed and tested and made sure that the invalid backend error can be thrown.\r\n\r\nThe second one removed the limitation that each group is bound to a given device sequence. But I don't feel comfortable to cache many NCCL communicators. So the current design is, for each process group, we only cache a single NCCL communicator (say with device sequence 0,1,2,3,4,5,6,7) If there is a new collective NCCL call with a new device sequence (say 1,0,2,3,4,5,6,7). We will destroy the existing cached NCCL communicator (with sequence 0,1,2,3,4,5,6,7), rebuild a new communicator and cache this one instead. This design is pretty clean and simple. I tested with different new device sequence and made sure it worked. This shouldn't be a release blocker for now\r\n\r\nFor the NCCL deadlock, it's not super clear to me why but it happens on several consecutive broadcast calls using the same communicator. If the communicator is rebuilt, it will work (as the current workaround)."}