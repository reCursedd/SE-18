{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148461342", "pull_request_review_id": 73704858, "id": 148461342, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0ODQ2MTM0Mg==", "diff_hunk": "@@ -198,6 +241,34 @@ def broadcast(tensor, src, group=group.WORLD):\n     return torch._C._dist_broadcast(tensor, src, group)\n \n \n+def all_reduce_multigpu(tensor_list, op=reduce_op.SUM, group=group.WORLD):\n+    \"\"\"Reduces the tensor data across all machines in such a way that all get\n+    the final result. This function reduces a number of tensors on every node,\n+    while each tensor resides on different GPUs\n+    Therefore, the input tensor in the tensor list needs to be GPU tensors.\n+    Also, each tensor in the tensor list needs to reside on a different GPU.\n+\n+    After the call, all ``tensor``s in the tensor list  is going to be bitwise\n+    identical in all processes.\n+\n+    Only nccl backend is currently supported\n+    tensors should only be GPU tensors\n+\n+    Arguments:\n+        tensor list (List[Tensor]): List of input and output tensors of\n+            the collective. The function operates in-place and requires that\n+            each tensor to be a GPU tensor on different GPUs.\n+        op (optional): One of the values from ``torch.distributed.reduce_op``\n+            enum.  Specifies an operation used for element-wise reductions.\n+        group (optional): Group of the collective.\n+    \"\"\"\n+    assert torch.distributed._initialized == _INITIALIZED_PG, \\\n+        \"collective only supported in process-group mode\"\n+    assert torch.distributed._backend == \"nccl\", \\", "path": "torch/distributed/__init__.py", "position": null, "original_position": 111, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "029262b0414ad5b6bfa9af5c92e03b3160822708", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I don't think that's the right way to go with backend checking. All data channels other than NCCL should just raise an error when the vector overload is used, so you don't have to do this here", "created_at": "2017-11-02T07:56:06Z", "updated_at": "2018-11-23T15:35:52Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r148461342", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/148461342"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r148461342"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p>I don't think that's the right way to go with backend checking. All data channels other than NCCL should just raise an error when the vector overload is used, so you don't have to do this here</p>", "body_text": "I don't think that's the right way to go with backend checking. All data channels other than NCCL should just raise an error when the vector overload is used, so you don't have to do this here"}