{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153432731", "pull_request_review_id": 79428071, "id": 153432731, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzQzMjczMQ==", "diff_hunk": "@@ -0,0 +1,232 @@\n+#pragma once\n+\n+#include \"../DataChannel.hpp\"\n+#include \"DataChannelUtils.hpp\"\n+\n+#include <nccl.h>\n+\n+#include <utility>\n+#include <memory>\n+#include <string>\n+#include <unordered_map>\n+#include <vector>\n+\n+\n+#define NCCL_CHECK(cmd) do {                                  \\\n+  ncclResult_t error = cmd;                                   \\\n+  if (error != ncclSuccess) {                                 \\\n+    std::string err = \"NCCL error in: \" +                     \\\n+                      std::string(__FILE__) + \":\" +           \\\n+                      std::to_string(__LINE__) + \", \" +       \\\n+                      std::string(ncclGetErrorString(error)); \\\n+    throw std::runtime_error(err);                            \\\n+  }                                                           \\\n+} while (0)\n+\n+\n+namespace thd {\n+\n+// Type aliasing\n+using NcclResourcePair =\n+  std::pair<std::vector<ncclComm_t>*, std::vector<cudaEvent_t>*>;\n+\n+struct DataChannelNccl : DataChannel {\n+\n+  // Nothing to implement\n+  struct RequestNccl : DataChannel::Request {};\n+\n+  // Wrapper on the pair of NCCL resources\n+  class NcclResources {\n+\n+  public:\n+\n+    NcclResources() = default;\n+    NcclResources(std::unique_ptr<std::vector<ncclComm_t>>&& ncclComm,\n+                  std::unique_ptr<std::vector<cudaEvent_t>>&& event):\n+\n+      _commEventPair(std::pair<std::unique_ptr<std::vector<ncclComm_t>>,\n+                               std::unique_ptr<std::vector<cudaEvent_t>>>\n+                               (std::move(ncclComm), std::move(event))) {}\n+    // Delete copy and assignment ctors\n+    NcclResources(const NcclResources&) = delete;\n+    NcclResources& operator=(const NcclResources&) = delete;\n+\n+    // Move ctors by default\n+    NcclResources(NcclResources&&) = default;\n+    NcclResources& operator=(NcclResources&&) = default;\n+\n+    // Nccl Communicator Getter\n+    std::vector<ncclComm_t>* ncclComms() {\n+      return _commEventPair.first.get();\n+    }\n+\n+    // Nccl CUDA event Getter\n+    std::vector<cudaEvent_t>* ncclCudaEvents() {\n+      return _commEventPair.second.get();\n+    }\n+\n+  private:\n+\n+    std::pair<std::unique_ptr<std::vector<ncclComm_t>>,\n+              std::unique_ptr<std::vector<cudaEvent_t>>> _commEventPair;\n+  };\n+\n+\n+  // Constructor\n+  DataChannelNccl(InitMethod::Config config, int timeout = -1);\n+  virtual ~DataChannelNccl();\n+\n+  bool init() override;\n+  void destroy() override;\n+\n+  rank_type getRank() override;\n+  rank_type getNumProcesses() override;\n+\n+  void allReduce(std::vector<at::Tensor>& input,\n+                 std::vector<at::Tensor>& output,\n+                 THDReduceOp operation,\n+                 THDGroup = THDGroupWORLD) override;\n+\n+  void allReduce(at::Tensor& data,\n+                 THDReduceOp operation,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void allGather(std::vector<at::Tensor>& input,\n+                 std::vector<at::Tensor>& output,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void allGather(std::vector<at::Tensor>& output,\n+                 at::Tensor& input,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void reduce(std::vector<at::Tensor>& input,\n+              THDReduceOp operation,\n+              rank_type dstRank,\n+              THDGroup groupId = THDGroupWORLD) override;\n+\n+  void reduce(at::Tensor& data,\n+              THDReduceOp operation,\n+              rank_type dstRank,\n+              THDGroup groupId = THDGroupWORLD) override;\n+\n+  void broadcast(std::vector<at::Tensor>& data,\n+                 rank_type srcRank,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void broadcast(at::Tensor& data,\n+                 rank_type srcRank,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void barrier(THDGroup groupId = THDGroupWORLD) override;\n+\n+  THDGroup newGroup(const std::vector<rank_type>& ranks) override;\n+\n+  void destroyGroup(THDGroup groupId = THDGroupWORLD) override;\n+\n+  // Not supported functions\n+  void gather(std::vector<at::Tensor>& output,\n+              at::Tensor& input,\n+              rank_type dstRank,\n+              THDGroup groupId = THDGroupWORLD) override;\n+\n+  void scatter(std::vector<at::Tensor>& input,\n+               at::Tensor& output,\n+               rank_type srcRank,\n+               THDGroup groupId = THDGroupWORLD) override;\n+\n+  void send(Scalar& data, rank_type dstRank) override;\n+\n+  void send(at::Tensor& data, rank_type dstRank) override;\n+\n+  void receive(Scalar& data, rank_type srcRank) override;\n+\n+  rank_type receive(at::Tensor& data) override;\n+\n+  void receive(at::Tensor& data, rank_type srcRank) override;\n+\n+  RequestNccl* isend(at::Tensor& data, rank_type dstRank) override;\n+\n+  RequestNccl* ireceive(at::Tensor& data, rank_type srcRank) override;\n+\n+private:\n+\n+  // Current process' rank\n+  rank_type _rank;\n+  // Number of processes in network\n+  rank_type _numProcesses;\n+\n+  // Accept waiting timeout in milliseconds, optional\n+  int _timeout;\n+  // Master's address\n+  std::string _masterAddr;\n+  // Master's port\n+  port_type _masterPort;\n+  // Socket on which the master is listening\n+  int _masterListeningSocket;\n+  // Sockets on which the master is sending to each slave\n+  std::vector<int> _masterSendingSockets;\n+  // Slave socket\n+  int _slaveSocket;\n+\n+  // Number of GPUs on each node\n+  int _numGPUs;\n+  // Mutex for Nccl Data Channel\n+  std::mutex _mutex;\n+\n+  /**\n+   * The GPU devices each group have used. The GPU devices are stored in a\n+   * device sequence.\n+   *\n+   * e.g. If the group only uses device 0, then the value of\n+   *      the used device string stored (value of the hashmap) would be \"0\".\n+   *\n+   *      If the group uses device 0 - 7 and the each tensor of the\n+   *      input tensor list is on device, 0, 1, 2, 3, 4, 5, 6, 7 separately,\n+   *      then the value of the used device string stored would be\n+   *      \"0,1,2,3,4,5,6,7\"\n+   *\n+   *      If the group uses device 0 - 7 and the each tensor of the\n+   *      input tensor list is on device, 0, 4, 5, 6, 7, 1, 2, 3 separately,\n+   *      then the value of the used device string  stored would be\n+   *      \"0,4,5,6,7,1,2,3\"\n+   *\n+   *      Note that the order of the device for the tensor list matters and\n+   *      each group can only be associated with one used device string\n+   */\n+  std::unordered_map<THDGroup, std::string> _groupDevices;", "path": "torch/lib/THD/base/data_channels/DataChannelNccl.hpp", "position": null, "original_position": 196, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "e2ee0c8f846ebc17f4b0cb1233c7bedbdd76d048", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "The problem with this approach is that it doesn't really fit the interface we expose - a group is just a way to have multiple collectives running concurrently. I think a better solution, would be to store a communicator per group and per combination of devices used within this group. That would be equivalent and would satisfy the interface, without resorting to clearing caches, etc. It's not like this will create any more groups than if you didn't do this - it's just that certain communicators will belong to a single group instead of multiple.\r\n\r\nI'm fine with merging this, but we definitely won't make this backend public until it's fixed.\r\n\r\nDo we finally know what's causing the deadlocks and why do we even need to destroy communicators periodically?", "created_at": "2017-11-28T09:18:48Z", "updated_at": "2018-11-23T15:36:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r153432731", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153432731"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r153432731"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p>The problem with this approach is that it doesn't really fit the interface we expose - a group is just a way to have multiple collectives running concurrently. I think a better solution, would be to store a communicator per group and per combination of devices used within this group. That would be equivalent and would satisfy the interface, without resorting to clearing caches, etc. It's not like this will create any more groups than if you didn't do this - it's just that certain communicators will belong to a single group instead of multiple.</p>\n<p>I'm fine with merging this, but we definitely won't make this backend public until it's fixed.</p>\n<p>Do we finally know what's causing the deadlocks and why do we even need to destroy communicators periodically?</p>", "body_text": "The problem with this approach is that it doesn't really fit the interface we expose - a group is just a way to have multiple collectives running concurrently. I think a better solution, would be to store a communicator per group and per combination of devices used within this group. That would be equivalent and would satisfy the interface, without resorting to clearing caches, etc. It's not like this will create any more groups than if you didn't do this - it's just that certain communicators will belong to a single group instead of multiple.\nI'm fine with merging this, but we definitely won't make this backend public until it's fixed.\nDo we finally know what's causing the deadlocks and why do we even need to destroy communicators periodically?", "in_reply_to_id": 152942368}