{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153364077", "pull_request_review_id": 79278128, "id": 153364077, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzM2NDA3Nw==", "diff_hunk": "@@ -172,6 +181,10 @@ def gather(self, outputs, output_device):\n         return gather(outputs, output_device, dim=self.dim)\n \n     def train(self, mode=True):\n+        # Destroy the default group's NCCL communicator cache, which will be\n+        # recreated at the later call\n+        if dist._backend == \"nccl\":\n+            dist.destroy_group()", "path": "torch/nn/parallel/distributed.py", "position": null, "original_position": 41, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "e2ee0c8f846ebc17f4b0cb1233c7bedbdd76d048", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "When user set model.train() (after one epoch of model.eval()) and model.eval() (after one epoch of model.train()). (Note that model.eval() will call model.train() as well), the NCCL communicator cache needed to be cleared to avoid the deadlock. So this is needed\r\n\r\nSamewise, I will use dist._clear_group_cache() instead ", "created_at": "2017-11-28T00:27:39Z", "updated_at": "2018-11-23T15:36:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r153364077", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153364077"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r153364077"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p>When user set model.train() (after one epoch of model.eval()) and model.eval() (after one epoch of model.train()). (Note that model.eval() will call model.train() as well), the NCCL communicator cache needed to be cleared to avoid the deadlock. So this is needed</p>\n<p>Samewise, I will use dist._clear_group_cache() instead</p>", "body_text": "When user set model.train() (after one epoch of model.eval()) and model.eval() (after one epoch of model.train()). (Note that model.eval() will call model.train() as well), the NCCL communicator cache needed to be cleared to avoid the deadlock. So this is needed\nSamewise, I will use dist._clear_group_cache() instead", "in_reply_to_id": 152941629}