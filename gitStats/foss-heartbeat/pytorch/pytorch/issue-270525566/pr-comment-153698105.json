{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153698105", "pull_request_review_id": 79741699, "id": 153698105, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzY5ODEwNQ==", "diff_hunk": "@@ -0,0 +1,232 @@\n+#pragma once\n+\n+#include \"../DataChannel.hpp\"\n+#include \"DataChannelUtils.hpp\"\n+\n+#include <nccl.h>\n+\n+#include <utility>\n+#include <memory>\n+#include <string>\n+#include <unordered_map>\n+#include <vector>\n+\n+\n+#define NCCL_CHECK(cmd) do {                                  \\\n+  ncclResult_t error = cmd;                                   \\\n+  if (error != ncclSuccess) {                                 \\\n+    std::string err = \"NCCL error in: \" +                     \\\n+                      std::string(__FILE__) + \":\" +           \\\n+                      std::to_string(__LINE__) + \", \" +       \\\n+                      std::string(ncclGetErrorString(error)); \\\n+    throw std::runtime_error(err);                            \\\n+  }                                                           \\\n+} while (0)\n+\n+\n+namespace thd {\n+\n+// Type aliasing\n+using NcclResourcePair =\n+  std::pair<std::vector<ncclComm_t>*, std::vector<cudaEvent_t>*>;\n+\n+struct DataChannelNccl : DataChannel {\n+\n+  // Nothing to implement\n+  struct RequestNccl : DataChannel::Request {};\n+\n+  // Wrapper on the pair of NCCL resources\n+  class NcclResources {\n+\n+  public:\n+\n+    NcclResources() = default;\n+    NcclResources(std::unique_ptr<std::vector<ncclComm_t>>&& ncclComm,\n+                  std::unique_ptr<std::vector<cudaEvent_t>>&& event):\n+\n+      _commEventPair(std::pair<std::unique_ptr<std::vector<ncclComm_t>>,\n+                               std::unique_ptr<std::vector<cudaEvent_t>>>\n+                               (std::move(ncclComm), std::move(event))) {}\n+    // Delete copy and assignment ctors\n+    NcclResources(const NcclResources&) = delete;\n+    NcclResources& operator=(const NcclResources&) = delete;\n+\n+    // Move ctors by default\n+    NcclResources(NcclResources&&) = default;\n+    NcclResources& operator=(NcclResources&&) = default;\n+\n+    // Nccl Communicator Getter\n+    std::vector<ncclComm_t>* ncclComms() {\n+      return _commEventPair.first.get();\n+    }\n+\n+    // Nccl CUDA event Getter\n+    std::vector<cudaEvent_t>* ncclCudaEvents() {\n+      return _commEventPair.second.get();\n+    }\n+\n+  private:\n+\n+    std::pair<std::unique_ptr<std::vector<ncclComm_t>>,\n+              std::unique_ptr<std::vector<cudaEvent_t>>> _commEventPair;\n+  };\n+\n+\n+  // Constructor\n+  DataChannelNccl(InitMethod::Config config, int timeout = -1);\n+  virtual ~DataChannelNccl();\n+\n+  bool init() override;\n+  void destroy() override;\n+\n+  rank_type getRank() override;\n+  rank_type getNumProcesses() override;\n+\n+  void allReduce(std::vector<at::Tensor>& input,\n+                 std::vector<at::Tensor>& output,\n+                 THDReduceOp operation,\n+                 THDGroup = THDGroupWORLD) override;\n+\n+  void allReduce(at::Tensor& data,\n+                 THDReduceOp operation,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void allGather(std::vector<at::Tensor>& input,\n+                 std::vector<at::Tensor>& output,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void allGather(std::vector<at::Tensor>& output,\n+                 at::Tensor& input,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void reduce(std::vector<at::Tensor>& input,\n+              THDReduceOp operation,\n+              rank_type dstRank,\n+              THDGroup groupId = THDGroupWORLD) override;\n+\n+  void reduce(at::Tensor& data,\n+              THDReduceOp operation,\n+              rank_type dstRank,\n+              THDGroup groupId = THDGroupWORLD) override;\n+\n+  void broadcast(std::vector<at::Tensor>& data,\n+                 rank_type srcRank,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void broadcast(at::Tensor& data,\n+                 rank_type srcRank,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void barrier(THDGroup groupId = THDGroupWORLD) override;\n+\n+  THDGroup newGroup(const std::vector<rank_type>& ranks) override;\n+\n+  void destroyGroup(THDGroup groupId = THDGroupWORLD) override;\n+\n+  // Not supported functions\n+  void gather(std::vector<at::Tensor>& output,\n+              at::Tensor& input,\n+              rank_type dstRank,\n+              THDGroup groupId = THDGroupWORLD) override;\n+\n+  void scatter(std::vector<at::Tensor>& input,\n+               at::Tensor& output,\n+               rank_type srcRank,\n+               THDGroup groupId = THDGroupWORLD) override;\n+\n+  void send(Scalar& data, rank_type dstRank) override;\n+\n+  void send(at::Tensor& data, rank_type dstRank) override;\n+\n+  void receive(Scalar& data, rank_type srcRank) override;\n+\n+  rank_type receive(at::Tensor& data) override;\n+\n+  void receive(at::Tensor& data, rank_type srcRank) override;\n+\n+  RequestNccl* isend(at::Tensor& data, rank_type dstRank) override;\n+\n+  RequestNccl* ireceive(at::Tensor& data, rank_type srcRank) override;\n+\n+private:\n+\n+  // Current process' rank\n+  rank_type _rank;\n+  // Number of processes in network\n+  rank_type _numProcesses;\n+\n+  // Accept waiting timeout in milliseconds, optional\n+  int _timeout;\n+  // Master's address\n+  std::string _masterAddr;\n+  // Master's port\n+  port_type _masterPort;\n+  // Socket on which the master is listening\n+  int _masterListeningSocket;\n+  // Sockets on which the master is sending to each slave\n+  std::vector<int> _masterSendingSockets;\n+  // Slave socket\n+  int _slaveSocket;\n+\n+  // Number of GPUs on each node\n+  int _numGPUs;\n+  // Mutex for Nccl Data Channel\n+  std::mutex _mutex;\n+\n+  /**\n+   * The GPU devices each group have used. The GPU devices are stored in a\n+   * device sequence.\n+   *\n+   * e.g. If the group only uses device 0, then the value of\n+   *      the used device string stored (value of the hashmap) would be \"0\".\n+   *\n+   *      If the group uses device 0 - 7 and the each tensor of the\n+   *      input tensor list is on device, 0, 1, 2, 3, 4, 5, 6, 7 separately,\n+   *      then the value of the used device string stored would be\n+   *      \"0,1,2,3,4,5,6,7\"\n+   *\n+   *      If the group uses device 0 - 7 and the each tensor of the\n+   *      input tensor list is on device, 0, 4, 5, 6, 7, 1, 2, 3 separately,\n+   *      then the value of the used device string  stored would be\n+   *      \"0,4,5,6,7,1,2,3\"\n+   *\n+   *      Note that the order of the device for the tensor list matters and\n+   *      each group can only be associated with one used device string\n+   */\n+  std::unordered_map<THDGroup, std::string> _groupDevices;", "path": "torch/lib/THD/base/data_channels/DataChannelNccl.hpp", "position": null, "original_position": 196, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "e2ee0c8f846ebc17f4b0cb1233c7bedbdd76d048", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "Updates, now this is fixed and works. Instead of storing multiple communicators per group per device sequence. We only cache one communicator, if a new device sequence comes in, we destroy the previous communicator.  (The device sequence change shouldn't happen that often anyway in a training process) ", "created_at": "2017-11-29T05:44:29Z", "updated_at": "2018-11-23T15:36:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r153698105", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153698105"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r153698105"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p>Updates, now this is fixed and works. Instead of storing multiple communicators per group per device sequence. We only cache one communicator, if a new device sequence comes in, we destroy the previous communicator.  (The device sequence change shouldn't happen that often anyway in a training process)</p>", "body_text": "Updates, now this is fixed and works. Instead of storing multiple communicators per group per device sequence. We only cache one communicator, if a new device sequence comes in, we destroy the previous communicator.  (The device sequence change shouldn't happen that often anyway in a training process)", "in_reply_to_id": 152942368}