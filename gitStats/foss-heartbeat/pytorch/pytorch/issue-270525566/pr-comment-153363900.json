{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153363900", "pull_request_review_id": 79278128, "id": 153363900, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzM2MzkwMA==", "diff_hunk": "@@ -97,6 +97,11 @@ def __init__(self, module, device_ids=None, output_device=None, dim=0):\n         for p in self.module.state_dict().values():\n             dist.broadcast(p, 0)\n \n+        # Destroy the default group's NCCL communicator cache, which will be\n+        # recreated at the later call\n+        if dist._backend == \"nccl\":\n+            dist.destroy_group()", "path": "torch/nn/parallel/distributed.py", "position": null, "original_position": 16, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "e2ee0c8f846ebc17f4b0cb1233c7bedbdd76d048", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "1, what I really would like to do here is to clear the NCCL communicator and CUDA events Cache of the default group here to avoid the NCCL deadlock. This is needed to make it work and destroy_group() does that. Actually we don't even need destroy_group function, but just the _clear_group_cache() function instead, which will not remove the groupId from the hashmap.\r\n\r\n2, OK, I removed the destroy_group function since we don't really need it, instead, I made the non-user function _clear_group_cache that will actually clear the cache and used it here.", "created_at": "2017-11-28T00:26:28Z", "updated_at": "2018-11-23T15:36:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r153363900", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153363900"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r153363900"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p>1, what I really would like to do here is to clear the NCCL communicator and CUDA events Cache of the default group here to avoid the NCCL deadlock. This is needed to make it work and destroy_group() does that. Actually we don't even need destroy_group function, but just the _clear_group_cache() function instead, which will not remove the groupId from the hashmap.</p>\n<p>2, OK, I removed the destroy_group function since we don't really need it, instead, I made the non-user function _clear_group_cache that will actually clear the cache and used it here.</p>", "body_text": "1, what I really would like to do here is to clear the NCCL communicator and CUDA events Cache of the default group here to avoid the NCCL deadlock. This is needed to make it work and destroy_group() does that. Actually we don't even need destroy_group function, but just the _clear_group_cache() function instead, which will not remove the groupId from the hashmap.\n2, OK, I removed the destroy_group function since we don't really need it, instead, I made the non-user function _clear_group_cache that will actually clear the cache and used it here.", "in_reply_to_id": 152941495}