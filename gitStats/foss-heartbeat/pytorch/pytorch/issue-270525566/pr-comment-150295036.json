{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150295036", "pull_request_review_id": 75823688, "id": 150295036, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDI5NTAzNg==", "diff_hunk": "@@ -320,3 +395,47 @@ def _process_batch():\n         with torch.cuda.device(device_ids[0]):\n             while True:\n                 _process_batch()  # just to have a clear scope\n+\n+    @staticmethod\n+    def _reduction_thread_fn_nccl(queue, group_id, device_ids, default_streams):\n+        \"\"\"\n+        Reduction thread function specifically for NCCL, the reduction will use\n+        default CUDA streams\n+        \"\"\"\n+        def _process_batch():\n+            dev_grad_batch, dev_events, job_event = queue.get()\n+            dev_coalesced = []\n+            # Coalesce the tensors on all devices and start a local reduction\n+            for dev_id, grad_batch, event, default_stream in \\\n+                    zip(device_ids,\n+                        dev_grad_batch,\n+                        dev_events,\n+                        default_streams):\n+                with torch.cuda.device(dev_id), \\\n+                        torch.cuda.stream(default_stream):\n+                    default_stream.wait_event(event)\n+                    coalesced = _flatten_dense_tensors(grad_batch)\n+                    dev_coalesced.append(coalesced)\n+\n+            # TODO: remove nccl.reduce with\n+            #       dist.all_reduce_multigpus\n+            nccl.reduce(dev_coalesced, root=0, streams=default_streams)", "path": "torch/nn/parallel/distributed.py", "position": null, "original_position": 205, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "18f92006e8b7b6efff22489a775eb1941e0091e9", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "Once I move to dist. allreduce_multigpu, why do you still think we need to do broadcast parameters in _sync_params? Aren't we supposed to delete it? \r\n\r\nallreduce_multigpu will let every single GPU to have the averaged gradients. And all the rest left to do at the beginning of each forward is to update the parameters on every GPU with the optimizer on 8 GPUs.\r\n\r\nSo a desired flow is that we don't broadcast anything in forward (maybe buffer?, but not parameters), and at the reduction thread, only use a single step of dist.all_reduce_multigpu(), which will use all 4 IB interfaces.  This would be targeting the best collective op performance.\r\n\r\nFor these new functions, I have my own script of testing it and I have already tested the functionality of the new functions.  As I put in the TODO in this PR, I will write the unittest for this too later. That's why NCCL backend is marked as experimental only. ", "created_at": "2017-11-10T17:32:54Z", "updated_at": "2018-11-23T15:36:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r150295036", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150295036"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r150295036"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p>Once I move to dist. allreduce_multigpu, why do you still think we need to do broadcast parameters in _sync_params? Aren't we supposed to delete it?</p>\n<p>allreduce_multigpu will let every single GPU to have the averaged gradients. And all the rest left to do at the beginning of each forward is to update the parameters on every GPU with the optimizer on 8 GPUs.</p>\n<p>So a desired flow is that we don't broadcast anything in forward (maybe buffer?, but not parameters), and at the reduction thread, only use a single step of dist.all_reduce_multigpu(), which will use all 4 IB interfaces.  This would be targeting the best collective op performance.</p>\n<p>For these new functions, I have my own script of testing it and I have already tested the functionality of the new functions.  As I put in the TODO in this PR, I will write the unittest for this too later. That's why NCCL backend is marked as experimental only.</p>", "body_text": "Once I move to dist. allreduce_multigpu, why do you still think we need to do broadcast parameters in _sync_params? Aren't we supposed to delete it?\nallreduce_multigpu will let every single GPU to have the averaged gradients. And all the rest left to do at the beginning of each forward is to update the parameters on every GPU with the optimizer on 8 GPUs.\nSo a desired flow is that we don't broadcast anything in forward (maybe buffer?, but not parameters), and at the reduction thread, only use a single step of dist.all_reduce_multigpu(), which will use all 4 IB interfaces.  This would be targeting the best collective op performance.\nFor these new functions, I have my own script of testing it and I have already tested the functionality of the new functions.  As I put in the TODO in this PR, I will write the unittest for this too later. That's why NCCL backend is marked as experimental only.", "in_reply_to_id": 150059235}