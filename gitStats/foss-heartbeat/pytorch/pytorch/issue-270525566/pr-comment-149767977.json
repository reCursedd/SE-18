{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149767977", "pull_request_review_id": 75214607, "id": 149767977, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0OTc2Nzk3Nw==", "diff_hunk": "@@ -242,7 +274,13 @@ def _queue_reduction(self, bucket_idx):\n \n         # Queue the reduction and make sure backward waits for it\n         event = threading.Event()\n-        self._reduction_queues[bucket_idx].put((dev_buckets, dev_events, event))\n+\n+        if dist._backend == \"nccl\":\n+            # NCCL backend, all buckets will share a single reduction queue\n+            self._reduction_queues[0].put((dev_buckets, dev_events, event))", "path": "torch/nn/parallel/distributed.py", "position": null, "original_position": 76, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "18f92006e8b7b6efff22489a775eb1941e0091e9", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "@csarofeen Every reduction bucket will write to this single queue when any of the bucket is full (in line 257). Among all processes on different nodes, the order for each bucket to become full is deterministic since the way we divide the gradients into bucket is the same across multiple processes. Therefore, the single queue and single reduction thread makes sure that the order in which each bucket got all_reduced is the same for each process.", "created_at": "2017-11-08T19:11:54Z", "updated_at": "2018-11-23T15:36:13Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r149767977", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149767977"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r149767977"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22205833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/csarofeen\">@csarofeen</a> Every reduction bucket will write to this single queue when any of the bucket is full (in line 257). Among all processes on different nodes, the order for each bucket to become full is deterministic since the way we divide the gradients into bucket is the same across multiple processes. Therefore, the single queue and single reduction thread makes sure that the order in which each bucket got all_reduced is the same for each process.</p>", "body_text": "@csarofeen Every reduction bucket will write to this single queue when any of the bucket is full (in line 257). Among all processes on different nodes, the order for each bucket to become full is deterministic since the way we divide the gradients into bucket is the same across multiple processes. Therefore, the single queue and single reduction thread makes sure that the order in which each bucket got all_reduced is the same for each process."}