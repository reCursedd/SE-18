{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149785808", "pull_request_review_id": 75236208, "id": 149785808, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0OTc4NTgwOA==", "diff_hunk": "@@ -242,7 +274,13 @@ def _queue_reduction(self, bucket_idx):\n \n         # Queue the reduction and make sure backward waits for it\n         event = threading.Event()\n-        self._reduction_queues[bucket_idx].put((dev_buckets, dev_events, event))\n+\n+        if dist._backend == \"nccl\":\n+            # NCCL backend, all buckets will share a single reduction queue\n+            self._reduction_queues[0].put((dev_buckets, dev_events, event))", "path": "torch/nn/parallel/distributed.py", "position": null, "original_position": 76, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "18f92006e8b7b6efff22489a775eb1941e0091e9", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "body": "\"the order for each bucket to become full is deterministic\" This is not true. Even though your buckets are made up of all the same gradients, backwards calls are asynchronous which means the order in which the gradients finish are not deterministic. If gradient 3 is submitted before gradient 4 but gradient 4 takes much less time to compute, gradient 4 can finish before gradient 3.\r\nI believe logic needs to be added to enforce the all_reduce of `bucket_idx-1` has been submitted before the all_reduce of `bucket_idx`.", "created_at": "2017-11-08T20:20:47Z", "updated_at": "2018-11-23T15:36:13Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r149785808", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149785808"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r149785808"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p>\"the order for each bucket to become full is deterministic\" This is not true. Even though your buckets are made up of all the same gradients, backwards calls are asynchronous which means the order in which the gradients finish are not deterministic. If gradient 3 is submitted before gradient 4 but gradient 4 takes much less time to compute, gradient 4 can finish before gradient 3.<br>\nI believe logic needs to be added to enforce the all_reduce of <code>bucket_idx-1</code> has been submitted before the all_reduce of <code>bucket_idx</code>.</p>", "body_text": "\"the order for each bucket to become full is deterministic\" This is not true. Even though your buckets are made up of all the same gradients, backwards calls are asynchronous which means the order in which the gradients finish are not deterministic. If gradient 3 is submitted before gradient 4 but gradient 4 takes much less time to compute, gradient 4 can finish before gradient 3.\nI believe logic needs to be added to enforce the all_reduce of bucket_idx-1 has been submitted before the all_reduce of bucket_idx.", "in_reply_to_id": 149767977}