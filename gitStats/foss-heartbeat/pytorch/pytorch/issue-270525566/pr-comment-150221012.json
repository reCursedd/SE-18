{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150221012", "pull_request_review_id": 75739895, "id": 150221012, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDIyMTAxMg==", "diff_hunk": "@@ -0,0 +1,731 @@\n+#include \"../Cuda.hpp\"\n+#include \"../../../csrc/utils/auto_gpu.h\"\n+#include \"DataChannelNccl.hpp\"\n+#include \"DataChannelUtils.hpp\"\n+\n+#include <cuda.h>\n+#include <THC/THC.h>\n+\n+#include <unistd.h>\n+\n+#include <cstdint>\n+#include <stdexcept>\n+#include <unordered_set>\n+#include <sstream>\n+\n+namespace thd {\n+\n+namespace {\n+\n+\n+std::unordered_map<THDReduceOp, ncclRedOp_t> ncclOp = {\n+  {THDReduceOp::THDReduceMIN, ncclMin},\n+  {THDReduceOp::THDReduceMAX, ncclMax},\n+  {THDReduceOp::THDReduceSUM, ncclSum},\n+  {THDReduceOp::THDReducePRODUCT, ncclProd},\n+};\n+\n+\n+std::unordered_map<at::ScalarType, ncclDataType_t> ncclDatatype = {\n+  {at::kChar, ncclInt8},\n+  {at::kByte, ncclUint8},\n+  {at::kFloat, ncclFloat},\n+  {at::kDouble, ncclDouble},\n+  {at::kInt, ncclInt32},\n+  {at::kLong, ncclInt64},\n+};\n+\n+\n+// Helper function that gets the data type and issues error if not supported\n+static ncclDataType_t _getNcclDataType(at::ScalarType type) {\n+  if (ncclDatatype.find(type) == ncclDatatype.end()) {\n+    throw std::runtime_error(\"Unsupported data type for NCCL\");\n+  }\n+  return ncclDatatype[type];\n+}\n+\n+\n+// Helper function that gets the device list to determine the CUDA devices\n+std::vector<int> getDevicesList(const std::string& deviceSeq) {\n+\n+  std::stringstream ss(deviceSeq);\n+  std::string device;\n+  std::vector<int> devices;\n+  while (std::getline(ss, device, ',')) {\n+    devices.push_back(stoi(device));\n+  }\n+  return devices;\n+}\n+\n+} // namespace\n+\n+\n+// RequestNccl\n+DataChannelNccl::RequestNccl::RequestNccl(QueueWorker::Request&& request)\n+ : _request(std::move(request)) {\n+}\n+\n+\n+DataChannelNccl::RequestNccl::~RequestNccl() {}\n+\n+\n+bool DataChannelNccl::RequestNccl::isCompleted() {\n+  return _request.isCompleted();\n+}\n+\n+void DataChannelNccl::RequestNccl::wait() {\n+  _request.wait();\n+}\n+\n+// End of RequestNccl\n+\n+// DataChannelNccl\n+DataChannelNccl::DataChannelNccl(InitMethod::Config config, int timeout)\n+  : _rank(config.rank)\n+  , _numProcesses(config.world_size)\n+  , _timeout(timeout)\n+  , _masterListeningSocket(-1)\n+{\n+  if (_rank == 0) {\n+    _masterListeningSocket = config.master.listen_socket;\n+  } else {\n+    _masterAddr = config.worker.master_addr;\n+    _masterPort = config.worker.master_port;\n+  }\n+}\n+\n+\n+// Use the socket to broadcast NCCL ID\n+void DataChannelNccl::broadcastUniqueNcclId(ncclUniqueId* srcNcclId,\n+                                            ncclUniqueId* dstNcclId) {\n+  // Send the unique NCCL id to every rank\n+  if (_rank == 0) {\n+    std::vector<int> sockets(_numProcesses - 1);\n+    for (rank_type i = 0; i < _numProcesses - 1; ++i) {\n+      std::tie(sockets[i], std::ignore) = accept(_masterListeningSocket,\n+                                                 _timeout);\n+    }\n+    for (auto socket : sockets) {\n+      ResourceGuard socket_guard([socket]() { ::close(socket); });\n+      send_bytes<uint8_t>(socket,\n+                          reinterpret_cast<uint8_t*>(srcNcclId),\n+                          NCCL_UNIQUE_ID_BYTES);\n+    }\n+  } else {\n+    int socket;\n+    try {\n+      socket = connect(_masterAddr, _masterPort, true, _timeout);\n+    } catch (...) {\n+      std::string errStr = \"Rank: \" + std::to_string(_rank) + \" cannot \"\n+                           \"connect to the master: \" + _masterAddr + \":\" +\n+                           std::to_string(_masterPort);\n+      throw std::runtime_error(errStr);\n+    }\n+    ResourceGuard socket_guard([socket]() { ::close(socket); });\n+    recv_bytes<uint8_t>(socket,\n+                        reinterpret_cast<uint8_t*>(dstNcclId),\n+                        NCCL_UNIQUE_ID_BYTES);\n+  }\n+}\n+\n+\n+// Destructor that closes the master's listening socket\n+DataChannelNccl::~DataChannelNccl() {\n+  // Destroy the master listening socket\n+  if (_masterListeningSocket != -1) {\n+    ::close(_masterListeningSocket);\n+    _masterListeningSocket = -1;\n+  }\n+  /**\n+   * Note that destructor will be called after cudaruntime being unloaded since\n+   * DataChannel is a global variable.\n+   */\n+}\n+\n+\n+// Destroy the data channel\n+void DataChannelNccl::destroy() {\n+\n+  std::unique_lock<std::mutex> channelLock(_mutex);\n+\n+  // Destroy the master listening socket\n+  if (_masterListeningSocket != -1) {\n+    ::close(_masterListeningSocket);\n+    _masterListeningSocket = -1;\n+  }\n+\n+  // Guard GPU device\n+  AutoGPU gpuGuard;\n+\n+  // Destroy the CUDA and NCCL resources\n+  for (auto& itemPair : _ncclCommsAndEvents) {\n+\n+    auto devices = getDevicesList(_groupDevices[itemPair.first]);\n+\n+    // Destroy the CUDA events\n+    size_t idx = 0;\n+    for (auto& event : *(itemPair.second.second)) {\n+      gpuGuard.setDevice(devices[idx++]);\n+      THCudaCheck(cudaEventSynchronize(event));\n+      THCudaCheck(cudaEventDestroy(event));", "path": "torch/lib/THD/base/data_channels/DataChannelNccl.cpp", "position": null, "original_position": 170, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "18f92006e8b7b6efff22489a775eb1941e0091e9", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think the point of these changes would be to make the PR smaller and simpler to review. But we can fix that later if you don't want to do it now", "created_at": "2017-11-10T12:15:20Z", "updated_at": "2018-11-23T15:36:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r150221012", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150221012"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r150221012"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p>I think the point of these changes would be to make the PR smaller and simpler to review. But we can fix that later if you don't want to do it now</p>", "body_text": "I think the point of these changes would be to make the PR smaller and simpler to review. But we can fix that later if you don't want to do it now", "in_reply_to_id": 150053204}