{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/342675070", "html_url": "https://github.com/pytorch/pytorch/pull/3435#issuecomment-342675070", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3435", "id": 342675070, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjY3NTA3MA==", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-08T00:59:58Z", "updated_at": "2017-11-08T01:06:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> Addressed your comments,</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22205833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/csarofeen\">@csarofeen</a> I made distributed.py use a single reduction thread since all the NCCL calls are asynchronous, using multiple threads wouldn't provide much perf gain for us. Also, it would be pretty tricky to get each thread execute in a certain order.  This simple one thread model works without NCCL deadlock issues as I tested.  So each reduction bucket will write the reduction request to a single queue, which will be call NCCL backend in the order which the gradients become available. The single queue will be assigned to a dedicated single reduction thread to process the NCCL reduction one after another while maintaining the NCCL call order.</p>", "body_text": "@ngimel Addressed your comments,\n@csarofeen I made distributed.py use a single reduction thread since all the NCCL calls are asynchronous, using multiple threads wouldn't provide much perf gain for us. Also, it would be pretty tricky to get each thread execute in a certain order.  This simple one thread model works without NCCL deadlock issues as I tested.  So each reduction bucket will write the reduction request to a single queue, which will be call NCCL backend in the order which the gradients become available. The single queue will be assigned to a dedicated single reduction thread to process the NCCL reduction one after another while maintaining the NCCL call order.", "body": "@ngimel Addressed your comments, \r\n\r\n@csarofeen I made distributed.py use a single reduction thread since all the NCCL calls are asynchronous, using multiple threads wouldn't provide much perf gain for us. Also, it would be pretty tricky to get each thread execute in a certain order.  This simple one thread model works without NCCL deadlock issues as I tested.  So each reduction bucket will write the reduction request to a single queue, which will be call NCCL backend in the order which the gradients become available. The single queue will be assigned to a dedicated single reduction thread to process the NCCL reduction one after another while maintaining the NCCL call order."}