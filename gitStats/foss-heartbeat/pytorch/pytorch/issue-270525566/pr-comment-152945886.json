{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152945886", "pull_request_review_id": 78879805, "id": 152945886, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mjk0NTg4Ng==", "diff_hunk": "@@ -0,0 +1,232 @@\n+#pragma once\n+\n+#include \"../DataChannel.hpp\"\n+#include \"DataChannelUtils.hpp\"\n+\n+#include <nccl.h>\n+\n+#include <utility>\n+#include <memory>\n+#include <string>\n+#include <unordered_map>\n+#include <vector>\n+\n+\n+#define NCCL_CHECK(cmd) do {                                  \\\n+  ncclResult_t error = cmd;                                   \\\n+  if (error != ncclSuccess) {                                 \\\n+    std::string err = \"NCCL error in: \" +                     \\\n+                      std::string(__FILE__) + \":\" +           \\\n+                      std::to_string(__LINE__) + \", \" +       \\\n+                      std::string(ncclGetErrorString(error)); \\\n+    throw std::runtime_error(err);                            \\\n+  }                                                           \\\n+} while (0)\n+\n+\n+namespace thd {\n+\n+// Type aliasing\n+using NcclResourcePair =\n+  std::pair<std::vector<ncclComm_t>*, std::vector<cudaEvent_t>*>;\n+\n+struct DataChannelNccl : DataChannel {\n+\n+  // Nothing to implement\n+  struct RequestNccl : DataChannel::Request {};\n+\n+  // Wrapper on the pair of NCCL resources\n+  class NcclResources {\n+\n+  public:\n+\n+    NcclResources() = default;\n+    NcclResources(std::unique_ptr<std::vector<ncclComm_t>>&& ncclComm,\n+                  std::unique_ptr<std::vector<cudaEvent_t>>&& event):\n+\n+      _commEventPair(std::pair<std::unique_ptr<std::vector<ncclComm_t>>,\n+                               std::unique_ptr<std::vector<cudaEvent_t>>>\n+                               (std::move(ncclComm), std::move(event))) {}\n+    // Delete copy and assignment ctors\n+    NcclResources(const NcclResources&) = delete;\n+    NcclResources& operator=(const NcclResources&) = delete;\n+\n+    // Move ctors by default\n+    NcclResources(NcclResources&&) = default;\n+    NcclResources& operator=(NcclResources&&) = default;\n+\n+    // Nccl Communicator Getter\n+    std::vector<ncclComm_t>* ncclComms() {\n+      return _commEventPair.first.get();\n+    }\n+\n+    // Nccl CUDA event Getter\n+    std::vector<cudaEvent_t>* ncclCudaEvents() {\n+      return _commEventPair.second.get();\n+    }\n+\n+  private:\n+\n+    std::pair<std::unique_ptr<std::vector<ncclComm_t>>,\n+              std::unique_ptr<std::vector<cudaEvent_t>>> _commEventPair;\n+  };\n+\n+\n+  // Constructor\n+  DataChannelNccl(InitMethod::Config config, int timeout = -1);\n+  virtual ~DataChannelNccl();\n+\n+  bool init() override;\n+  void destroy() override;\n+\n+  rank_type getRank() override;\n+  rank_type getNumProcesses() override;\n+\n+  void allReduce(std::vector<at::Tensor>& input,\n+                 std::vector<at::Tensor>& output,\n+                 THDReduceOp operation,\n+                 THDGroup = THDGroupWORLD) override;\n+\n+  void allReduce(at::Tensor& data,\n+                 THDReduceOp operation,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void allGather(std::vector<at::Tensor>& input,\n+                 std::vector<at::Tensor>& output,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void allGather(std::vector<at::Tensor>& output,\n+                 at::Tensor& input,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void reduce(std::vector<at::Tensor>& input,\n+              THDReduceOp operation,\n+              rank_type dstRank,\n+              THDGroup groupId = THDGroupWORLD) override;\n+\n+  void reduce(at::Tensor& data,\n+              THDReduceOp operation,\n+              rank_type dstRank,\n+              THDGroup groupId = THDGroupWORLD) override;\n+\n+  void broadcast(std::vector<at::Tensor>& data,\n+                 rank_type srcRank,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void broadcast(at::Tensor& data,\n+                 rank_type srcRank,\n+                 THDGroup groupId = THDGroupWORLD) override;\n+\n+  void barrier(THDGroup groupId = THDGroupWORLD) override;\n+\n+  THDGroup newGroup(const std::vector<rank_type>& ranks) override;\n+\n+  void destroyGroup(THDGroup groupId = THDGroupWORLD) override;\n+\n+  // Not supported functions\n+  void gather(std::vector<at::Tensor>& output,\n+              at::Tensor& input,\n+              rank_type dstRank,\n+              THDGroup groupId = THDGroupWORLD) override;\n+\n+  void scatter(std::vector<at::Tensor>& input,\n+               at::Tensor& output,\n+               rank_type srcRank,\n+               THDGroup groupId = THDGroupWORLD) override;\n+\n+  void send(Scalar& data, rank_type dstRank) override;\n+\n+  void send(at::Tensor& data, rank_type dstRank) override;\n+\n+  void receive(Scalar& data, rank_type srcRank) override;\n+\n+  rank_type receive(at::Tensor& data) override;\n+\n+  void receive(at::Tensor& data, rank_type srcRank) override;\n+\n+  RequestNccl* isend(at::Tensor& data, rank_type dstRank) override;\n+\n+  RequestNccl* ireceive(at::Tensor& data, rank_type srcRank) override;\n+\n+private:\n+\n+  // Current process' rank\n+  rank_type _rank;\n+  // Number of processes in network\n+  rank_type _numProcesses;\n+\n+  // Accept waiting timeout in milliseconds, optional\n+  int _timeout;\n+  // Master's address\n+  std::string _masterAddr;\n+  // Master's port\n+  port_type _masterPort;\n+  // Socket on which the master is listening\n+  int _masterListeningSocket;\n+  // Sockets on which the master is sending to each slave\n+  std::vector<int> _masterSendingSockets;", "path": "torch/lib/THD/base/data_channels/DataChannelNccl.hpp", "position": null, "original_position": 167, "commit_id": "9400fd54da0d3b3a27b93cc4af8bb4c7a29b47d2", "original_commit_id": "e2ee0c8f846ebc17f4b0cb1233c7bedbdd76d048", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "It would be useful to add a comment that these are not sorted by ranks and can be in arbitrary order", "created_at": "2017-11-24T11:07:02Z", "updated_at": "2018-11-23T15:36:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/3435#discussion_r152945886", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3435", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152945886"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3435#discussion_r152945886"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3435"}}, "body_html": "<p>It would be useful to add a comment that these are not sorted by ranks and can be in arbitrary order</p>", "body_text": "It would be useful to add a comment that these are not sorted by ranks and can be in arbitrary order"}