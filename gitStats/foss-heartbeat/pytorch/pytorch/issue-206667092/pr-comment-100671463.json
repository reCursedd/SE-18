{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/100671463", "pull_request_review_id": 21389477, "id": 100671463, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMDY3MTQ2Mw==", "diff_hunk": "@@ -303,45 +265,101 @@ def _check_container_source(container_type, source_file, original_source):\n                    .format(torch.typename(container_type), msg))\n             warnings.warn(msg, SourceChangeWarning)\n \n+    def legacy_load(f):\n+        deserialized_objects = {}\n+\n+        def persistent_load(saved_id):\n+            if isinstance(saved_id, tuple):\n+                # Ignore containers that don't have any sources saved\n+                if all(saved_id[1:]):\n+                    _check_container_source(*saved_id)\n+                return saved_id[0]\n+            return deserialized_objects[int(saved_id)]\n+\n+        with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \\\n+                mkdtemp() as tmpdir:\n+\n+            tar.extract('storages', path=tmpdir)\n+            with open(os.path.join(tmpdir, 'storages'), 'rb', 0) as f:\n+                num_storages = pickle_module.load(f)\n+                for i in range(num_storages):\n+                    args = pickle_module.load(f)\n+                    key, location, storage_type = args\n+                    obj = storage_type._new_with_file(f)\n+                    obj = restore_location(obj, location)\n+                    deserialized_objects[key] = obj\n+\n+                storage_views = pickle_module.load(f)\n+                for target_cdata, root_cdata, offset, size in storage_views:\n+                    root = deserialized_objects[root_cdata]\n+                    deserialized_objects[target_cdata] = root[offset:offset + size]\n+\n+            tar.extract('tensors', path=tmpdir)\n+            with open(os.path.join(tmpdir, 'tensors'), 'rb', 0) as f:\n+                num_tensors = pickle_module.load(f)\n+                for i in range(num_tensors):\n+                    args = pickle_module.load(f)\n+                    key, storage_id, original_tensor_type = args\n+                    storage = deserialized_objects[storage_id]\n+                    tensor_type = storage_to_tensor_type(storage)\n+                    tensor = tensor_type._new_with_metadata_file(f, storage)\n+                    deserialized_objects[key] = tensor\n+\n+            pickle_file = tar.extractfile('pickle')\n+            unpickler = pickle_module.Unpickler(pickle_file)\n+            unpickler.persistent_load = persistent_load\n+            result = unpickler.load()\n+            return result\n+\n+    deserialized_storages = {}\n+\n     def persistent_load(saved_id):\n-        if isinstance(saved_id, tuple):\n+        assert isinstance(saved_id, tuple)\n+        typename = saved_id[0]\n+        data = saved_id[1:]\n+\n+        if typename == 'module':\n             # Ignore containers that don't have any sources saved\n-            if all(saved_id[1:]):\n-                _check_container_source(*saved_id)\n-            return saved_id[0]\n-        return deserialized_objects[int(saved_id)]\n-\n-    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \\\n-            mkdtemp() as tmpdir:\n-\n-        tar.extract('storages', path=tmpdir)\n-        with open(os.path.join(tmpdir, 'storages'), 'rb', 0) as f:\n-            num_storages = pickle_module.load(f)\n-            for i in range(num_storages):\n-                args = pickle_module.load(f)\n-                key, location, storage_type = args\n-                obj = storage_type._new_with_file(f)\n-                obj = restore_location(obj, location)\n-                deserialized_objects[key] = obj\n-\n-            storage_views = pickle_module.load(f)\n-            for target_cdata, root_cdata, offset, size in storage_views:\n-                root = deserialized_objects[root_cdata]\n-                deserialized_objects[target_cdata] = root[offset:offset + size]\n-\n-        tar.extract('tensors', path=tmpdir)\n-        with open(os.path.join(tmpdir, 'tensors'), 'rb', 0) as f:\n-            num_tensors = pickle_module.load(f)\n-            for i in range(num_tensors):\n-                args = pickle_module.load(f)\n-                key, storage_id, original_tensor_type = args\n-                storage = deserialized_objects[storage_id]\n-                tensor_type = storage_to_tensor_type(storage)\n-                tensor = tensor_type._new_with_metadata_file(f, storage)\n-                deserialized_objects[key] = tensor\n-\n-        pickle_file = tar.extractfile('pickle')\n-        unpickler = pickle_module.Unpickler(pickle_file)\n-        unpickler.persistent_load = persistent_load\n-        result = unpickler.load()\n-        return result\n+            if all(data[1:]):\n+                _check_container_source(*data)\n+            return data[0]\n+        elif typename == 'storage':\n+            data_type, key, location, size, is_view, offset, view_size = data\n+            if key not in deserialized_storages:\n+                deserialized_storages[key] = restore_location(\n+                    data_type(size), location)\n+            data = deserialized_storages[key]\n+            if is_view:\n+                data = data[offset:offset + view_size]\n+            return data\n+        else:\n+            raise RuntimeError(\"Unknown saved id type: %s\" % saved_id[0])\n+\n+    # try the legacy loader first, which only works if f is a tarfile\n+    try:\n+        return legacy_load(f)\n+    except tarfile.TarError:\n+        pass\n+\n+    f.seek(0)\n+    magic_number = pickle_module.load(f)\n+    if magic_number != MAGIC_NUMBER:\n+        raise RuntimeError(\"Invalid magic number; corrupt file?\")\n+    protocol_version = pickle_module.load(f)\n+    if protocol_version != PROTOCOL_VERSION:", "path": "torch/serialization.py", "position": 345, "original_position": 316, "commit_id": "35301cb6db948aa78de659c45904ff87007009f3", "original_commit_id": "7b4ec7f0831a3971a71167d8bec63e1384e2a416", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Why? Doesn't pickle detect the protocol automatically?", "created_at": "2017-02-11T17:03:49Z", "updated_at": "2018-11-23T15:32:24Z", "html_url": "https://github.com/pytorch/pytorch/pull/713#discussion_r100671463", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/713", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/100671463"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/713#discussion_r100671463"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/713"}}, "body_html": "<p>Why? Doesn't pickle detect the protocol automatically?</p>", "body_text": "Why? Doesn't pickle detect the protocol automatically?", "in_reply_to_id": 100670941}