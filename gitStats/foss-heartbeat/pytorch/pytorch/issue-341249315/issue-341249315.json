{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9447", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9447/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9447/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9447/events", "html_url": "https://github.com/pytorch/pytorch/issues/9447", "id": 341249315, "node_id": "MDU6SXNzdWUzNDEyNDkzMTU=", "number": 9447, "title": "Get rid of Storage View, IPC will not preserve storage structure", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-07-14T16:44:17Z", "updated_at": "2018-07-16T22:41:29Z", "closed_at": "2018-07-16T22:41:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Storage views are currently used to implement CUDA IPC with the caching allocator. The basic problem is that you can only do IPC with a chunk of memory returned by cudaMalloc, but when you are using the caching CUDA allocator, lots of logically distinct allocations are together on the same cudaMalloc block, and you have no choice but to IPC the whole thing. A storage view was used to implement this situation, since you gave the entire CUDA caching allocator block a storage, and then you had the actual storages be views on this storage.</p>\n<p>This is not a good reason to add something as complicated as storage views to PyTorch. So the new plan is this:</p>\n<ol>\n<li>When you transfer a CUDA tensor across IPC, its storage/offsets change: instead of being the same storages they were before, you get the entire CUDA allocation storage.</li>\n<li>The transferred tensors are non-resizable (this is true in practice today, since resizing would break sharing, but it is even more important after IPC, because a resize could cause you to clobber other allocations)</li>\n</ol>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, who I flew this plan by previously.</p>\n<p>This should fix <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"298649315\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5311\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5311/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/5311\">#5311</a></p>", "body_text": "Storage views are currently used to implement CUDA IPC with the caching allocator. The basic problem is that you can only do IPC with a chunk of memory returned by cudaMalloc, but when you are using the caching CUDA allocator, lots of logically distinct allocations are together on the same cudaMalloc block, and you have no choice but to IPC the whole thing. A storage view was used to implement this situation, since you gave the entire CUDA caching allocator block a storage, and then you had the actual storages be views on this storage.\nThis is not a good reason to add something as complicated as storage views to PyTorch. So the new plan is this:\n\nWhen you transfer a CUDA tensor across IPC, its storage/offsets change: instead of being the same storages they were before, you get the entire CUDA allocation storage.\nThe transferred tensors are non-resizable (this is true in practice today, since resizing would break sharing, but it is even more important after IPC, because a resize could cause you to clobber other allocations)\n\nCC @apaszke, who I flew this plan by previously.\nThis should fix #5311", "body": "Storage views are currently used to implement CUDA IPC with the caching allocator. The basic problem is that you can only do IPC with a chunk of memory returned by cudaMalloc, but when you are using the caching CUDA allocator, lots of logically distinct allocations are together on the same cudaMalloc block, and you have no choice but to IPC the whole thing. A storage view was used to implement this situation, since you gave the entire CUDA caching allocator block a storage, and then you had the actual storages be views on this storage.\r\n\r\nThis is not a good reason to add something as complicated as storage views to PyTorch. So the new plan is this:\r\n\r\n1. When you transfer a CUDA tensor across IPC, its storage/offsets change: instead of being the same storages they were before, you get the entire CUDA allocation storage.\r\n2. The transferred tensors are non-resizable (this is true in practice today, since resizing would break sharing, but it is even more important after IPC, because a resize could cause you to clobber other allocations)\r\n\r\nCC @apaszke, who I flew this plan by previously.\r\n\r\nThis should fix #5311"}