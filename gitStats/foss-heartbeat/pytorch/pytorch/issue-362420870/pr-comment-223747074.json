{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223747074", "pull_request_review_id": 162938843, "id": 223747074, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzc0NzA3NA==", "diff_hunk": "@@ -0,0 +1,103 @@\n+#pragma once\n+\n+#include <torch/tensor.h>\n+\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+#include <vector>\n+\n+namespace torch {\n+namespace data {\n+namespace detail {\n+namespace sequencers {\n+\n+/// A `Sequencer` accepts a function that yields the next result of a\n+/// `DataLoader` and then has the opportunity to influence the order in which\n+/// these results are returned. The `NoSequencer` does not enforce any\n+/// sequencing and returns any result directly. The `OrderedSequencer` instead\n+/// buffers results internally to return them in order of their sequence number.\n+template <typename Result>\n+struct Sequencer {\n+  using ResultProducer = std::function<optional<Result>()>;\n+  virtual ~Sequencer() = default;\n+  virtual optional<Result> next(ResultProducer next_result) = 0;\n+};\n+\n+/// A `Sequencer` that does not enforce any ordering. It is effectively the\n+/// identity function.\n+template <typename Result>\n+struct NoSequencer final : public Sequencer<Result> {\n+  using typename Sequencer<Result>::ResultProducer;\n+  optional<Result> next(ResultProducer next_result) override {\n+    return next_result();\n+  }\n+};\n+\n+/// A `Sequencer` that buffers results and returns them in order of their\n+/// sequence number. The `OrderedSequencer` maintains an internal, monotonically\n+/// incrementing counter for the next sequence number it expects. If it receives\n+/// a result with a higher sequence number, it will buffer it for later (when\n+/// the sequence number reaches that of this result). Otherwise, if the sequence\n+/// numbers match, the result is returned.\n+///\n+/// Implementation note: The `OrderedSequencer` is implemented with a fixed-size\n+/// buffer. Let `m` be the maximum number of jobs in the data loader's queue and\n+/// `s` be the current sequence number. Assume `m` jobs are scheduled in the\n+/// `DataLoader`. Any new result is stored at index `job.sqn mod m` in the\n+/// `OrderedSequencer`. Why are we sure sequence numbers of new jobs will not\n+/// collide with sequence numbers of buffered jobs? The `OrderedSequencer` will\n+/// not return from `next()` until it receives the result with sqn `s`. This\n+/// means no new jobs can be scheduled in the `DataLoader` in the meantime,\n+/// which enforces that as long as sqn `s` has not been received, `s + m` (which\n+/// would cause a collision in the fixed-size buffer) will not yet be scheduled.\n+template <typename Result>\n+struct OrderedSequencer : public Sequencer<Result> {\n+  using typename Sequencer<Result>::ResultProducer;\n+\n+  /// Constructs the `OrderedSequencer` with the maximum number of results it\n+  /// will ever hold at one point in time.\n+  explicit OrderedSequencer(size_t max_jobs) : buffer_(max_jobs) {}\n+\n+  /// Buffers results until the next one in the expected order is received.\n+  optional<Result> next(ResultProducer next_result) override {\n+    // If we already have the result for the next sqn, return it.\n+    if (auto& maybe_result = buffer(next_sequence_number_)) {\n+      auto result = std::move(*maybe_result);\n+      buffer(next_sequence_number_++).reset();\n+      return result;\n+    }\n+    // Otherwise wait for the next result.\n+    while (true) {\n+      auto result = next_result();\n+      if (!result) {\n+        break;\n+      }\n+      // If it was not nullopt and the sequence numbers match, return it\n+      // directly and bump the sequence number.\n+      if (result->sequence_number == next_sequence_number_) {\n+        ++next_sequence_number_;\n+        return result;\n+      }\n+      // Stash the result for later.\n+      buffer(result->sequence_number) = std::move(result);", "path": "torch/csrc/api/include/torch/data/detail/sequencers.h", "position": 93, "original_position": 83, "commit_id": "2fabdad63c0bc48b26af6bf8d2e74513b09c97da", "original_commit_id": "0cf5d795bd8b96ac542039a8f612df1ce84602f2", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can we please assert that this entry was empty? It's important to ensure that the overwriting I mentioned above doesn't happen.", "created_at": "2018-10-09T15:18:00Z", "updated_at": "2018-11-23T15:52:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/11918#discussion_r223747074", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11918", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223747074"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11918#discussion_r223747074"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11918"}}, "body_html": "<p>Can we please assert that this entry was empty? It's important to ensure that the overwriting I mentioned above doesn't happen.</p>", "body_text": "Can we please assert that this entry was empty? It's important to ensure that the overwriting I mentioned above doesn't happen."}