{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223746551", "pull_request_review_id": 162938843, "id": 223746551, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzc0NjU1MQ==", "diff_hunk": "@@ -0,0 +1,103 @@\n+#pragma once\n+\n+#include <torch/tensor.h>\n+\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+#include <vector>\n+\n+namespace torch {\n+namespace data {\n+namespace detail {\n+namespace sequencers {\n+\n+/// A `Sequencer` accepts a function that yields the next result of a\n+/// `DataLoader` and then has the opportunity to influence the order in which\n+/// these results are returned. The `NoSequencer` does not enforce any\n+/// sequencing and returns any result directly. The `OrderedSequencer` instead\n+/// buffers results internally to return them in order of their sequence number.\n+template <typename Result>\n+struct Sequencer {\n+  using ResultProducer = std::function<optional<Result>()>;\n+  virtual ~Sequencer() = default;\n+  virtual optional<Result> next(ResultProducer next_result) = 0;\n+};\n+\n+/// A `Sequencer` that does not enforce any ordering. It is effectively the\n+/// identity function.\n+template <typename Result>\n+struct NoSequencer final : public Sequencer<Result> {\n+  using typename Sequencer<Result>::ResultProducer;\n+  optional<Result> next(ResultProducer next_result) override {\n+    return next_result();\n+  }\n+};\n+\n+/// A `Sequencer` that buffers results and returns them in order of their\n+/// sequence number. The `OrderedSequencer` maintains an internal, monotonically\n+/// incrementing counter for the next sequence number it expects. If it receives\n+/// a result with a higher sequence number, it will buffer it for later (when\n+/// the sequence number reaches that of this result). Otherwise, if the sequence\n+/// numbers match, the result is returned.\n+///\n+/// Implementation note: The `OrderedSequencer` is implemented with a fixed-size\n+/// buffer. Let `m` be the maximum number of jobs in the data loader's queue and\n+/// `s` be the current sequence number. Assume `m` jobs are scheduled in the\n+/// `DataLoader`. Any new result is stored at index `job.sqn mod m` in the\n+/// `OrderedSequencer`. Why are we sure sequence numbers of new jobs will not\n+/// collide with sequence numbers of buffered jobs? The `OrderedSequencer` will\n+/// not return from `next()` until it receives the result with sqn `s`. This\n+/// means no new jobs can be scheduled in the `DataLoader` in the meantime,\n+/// which enforces that as long as sqn `s` has not been received, `s + m` (which\n+/// would cause a collision in the fixed-size buffer) will not yet be scheduled.", "path": "torch/csrc/api/include/torch/data/detail/sequencers.h", "position": 61, "original_position": 53, "commit_id": "2fabdad63c0bc48b26af6bf8d2e74513b09c97da", "original_commit_id": "0cf5d795bd8b96ac542039a8f612df1ce84602f2", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Unless someone calls `prefetch(1000000)` which you're not checking against \ud83d\ude09 You'd simply start overwriting buffer entries this way.", "created_at": "2018-10-09T15:16:52Z", "updated_at": "2018-11-23T15:52:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/11918#discussion_r223746551", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11918", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223746551"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11918#discussion_r223746551"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11918"}}, "body_html": "<p>Unless someone calls <code>prefetch(1000000)</code> which you're not checking against <g-emoji class=\"g-emoji\" alias=\"wink\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f609.png\">\ud83d\ude09</g-emoji> You'd simply start overwriting buffer entries this way.</p>", "body_text": "Unless someone calls prefetch(1000000) which you're not checking against \ud83d\ude09 You'd simply start overwriting buffer entries this way."}