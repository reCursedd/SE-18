{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223740742", "pull_request_review_id": 162938843, "id": 223740742, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzc0MDc0Mg==", "diff_hunk": "@@ -0,0 +1,301 @@\n+#pragma once\n+\n+#include <torch/data/data_loader_options.h>\n+#include <torch/data/detail/data_shuttle.h>\n+#include <torch/data/detail/sequencers.h>\n+#include <torch/data/iterator.h>\n+#include <torch/data/samplers/random.h>\n+#include <torch/data/worker_exception.h>\n+\n+#include <torch/csrc/utils/memory.h>\n+#include <torch/csrc/utils/variadic.h>\n+\n+#include <ATen/Error.h>\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+#include <exception>\n+#include <memory>\n+#include <thread>\n+#include <type_traits>\n+#include <utility>\n+#include <vector>\n+\n+namespace torch {\n+namespace data {\n+template <typename Dataset, typename Sampler>\n+class DataLoader {\n+ public:\n+  using Batch = typename Dataset::BatchType;\n+  using IndexBatch = std::vector<size_t>;\n+\n+  /// Constructs a new `DataLoader` from a `dataset` to sample from, `options`\n+  /// to configure the `DataLoader` with, and a `sampler` that specifies the\n+  /// sampling strategy.\n+  DataLoader(Dataset dataset, DataLoaderOptions options, Sampler sampler)\n+      : options_(std::move(options)),\n+        dataset_size_(dataset.size()),\n+        sampler_(std::move(sampler)),\n+        sequencer_(new_sequencer()) {\n+    // clang-format off\n+    AT_CHECK(\n+        options_.batch_size <= dataset.size(),\n+        \"Batch size (was \", options_.batch_size, \") \",\n+        \"must not be larger than the dataset size (was \",\n+        dataset.size(), \")\");\n+    // clang-format on\n+\n+    if (options_.workers > 0) {\n+      for (size_t w = 0; w < options_.workers; ++w) {\n+        // Here we copy the dataset into the worker thread closure. Each worker\n+        // has its own copy of the dataset. This means the dataset must be\n+        // trivially copiable, or else we don't expect more than one worker to\n+        // be in use.\n+        workers_.emplace_back(\n+            [this, dataset] { this->worker_thread(std::move(dataset)); });\n+      }\n+    } else {\n+      main_thread_dataset_ = torch::make_unique<Dataset>(std::move(dataset));\n+    }\n+  }\n+\n+  ~DataLoader() {\n+    join();\n+  }\n+\n+  /// Returns an iterator into the `DataLoader`. The lifetime of the iterator is\n+  /// bound to the `DataLoader`. In C++ standards language, the category of the\n+  /// iterator is `OutputIterator`. See\n+  /// https://en.cppreference.com/w/cpp/named_req/OutputIterator for what this\n+  /// means. In short: you may increment the iterator and dereference it, but\n+  /// cannot go back, or step forward more than one position at a time. When the\n+  /// `DataLoader` is exhausted, it will compare equal with the special\n+  /// \"sentinel\" iterator returned by `DataLoader::end()`. Most of the time, you\n+  /// should only use range-for loops to loop over the `DataLoader`, but\n+  /// standard algorithms like `std::copy(dataloader.begin(), dataloader.end(),\n+  /// output_iterator)`  are supported too.\n+  Iterator<Batch> begin() {\n+    reset();\n+    return Iterator<Batch>(torch::make_unique<detail::ValidIterator<Batch>>(\n+        [this] { return this->next(); }));\n+  }\n+\n+  /// Returns a special \"sentinel\" iterator that compares equal with a\n+  /// non-sentinel iterator once the `DataLoader` is exhausted.\n+  Iterator<Batch> end() {\n+    return Iterator<Batch>(\n+        torch::make_unique<detail::SentinelIterator<Batch>>());\n+  }\n+\n+  /// Joins the `DataLoader`'s worker threads and drains internal queues.\n+  void join() {\n+    if (joined_) {", "path": "torch/csrc/api/include/torch/data/dataloader.h", "position": 98, "original_position": 92, "commit_id": "2fabdad63c0bc48b26af6bf8d2e74513b09c97da", "original_commit_id": "0cf5d795bd8b96ac542039a8f612df1ce84602f2", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Not sure how much do we care about thread safety here? Might be at least worth documenting this.", "created_at": "2018-10-09T15:03:29Z", "updated_at": "2018-11-23T15:52:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/11918#discussion_r223740742", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11918", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223740742"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11918#discussion_r223740742"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11918"}}, "body_html": "<p>Not sure how much do we care about thread safety here? Might be at least worth documenting this.</p>", "body_text": "Not sure how much do we care about thread safety here? Might be at least worth documenting this."}