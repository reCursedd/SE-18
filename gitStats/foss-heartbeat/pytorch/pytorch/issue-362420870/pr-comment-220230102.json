{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220230102", "pull_request_review_id": 158568107, "id": 220230102, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMDIzMDEwMg==", "diff_hunk": "@@ -0,0 +1,233 @@\n+#pragma once\n+\n+#include <torch/data/detail/data-shuttle.h>\n+#include <torch/data/detail/sequencers.h>\n+#include <torch/data/options.h>\n+#include <torch/data/samplers/random.h>\n+\n+#include <torch/csrc/utils/memory.h>\n+#include <torch/csrc/utils/variadic.h>\n+\n+#include <ATen/Error.h>\n+#include <ATen/optional.h>\n+\n+#include <cstddef>\n+#include <thread>\n+#include <type_traits>\n+#include <utility>\n+\n+namespace torch {\n+namespace data {\n+template <\n+    typename DatasetType,\n+    typename SamplerType = samplers::RandomSampler<>>\n+class DataLoader {\n+ public:\n+  using Batch = typename DatasetType::BatchType;\n+  using IndexBatch = typename SamplerType::IndexBatchType;\n+\n+  DataLoader(\n+      DatasetType dataset,\n+      DataLoaderOptions options,\n+      SamplerType sampler)\n+      : options_(options.coalesce()),\n+        dataset_size_(dataset.size()),\n+        sampler_(std::move(sampler)) {\n+    using namespace detail::sequencers;\n+\n+    // clang-format off\n+    AT_CHECK(\n+        options_.batch_size_ <= dataset.size(),\n+        \"Batch size (was \", options.batch_size_, \") \",\n+        \"must not be larger than the dataset size (was \",\n+        dataset.size(), \")\");\n+    // clang-format on\n+\n+    if (options_.enforce_ordering_) {\n+      sequencer_ =\n+          torch::make_unique<OrderedSequencer<Result>>(*options_.max_jobs_);\n+    } else {\n+      sequencer_ = torch::make_unique<NoSequencer<Result>>();\n+    }\n+\n+    if (options_.workers_ == 0) {\n+      dataset_ = std::move(dataset);\n+    }\n+\n+    for (size_t w = 0; w < options_.workers_; ++w) {\n+      // Here we copy the dataset into the worker thread closure. Each worker\n+      // has its own copy of the dataset. This means the dataset must be\n+      // trivially copiable, or else we don't expect more than one worker to\n+      // be in use.\n+      workers_.emplace_back(\n+          [this, dataset] { this->worker_thread(std::move(dataset)); });\n+    }\n+\n+    if (!options_.defer_prefetch_) {\n+      prefetch();\n+    }\n+  }\n+\n+  ~DataLoader() {\n+    join();\n+  }\n+\n+  void reset(bool prefetch = true) {\n+    shuttle_.drain();\n+    sampler_.reset();\n+    sequence_number_ = 0;\n+    if (prefetch) {\n+      this->prefetch();\n+    }\n+  }\n+\n+  void prefetch(size_t requested_jobs) {\n+    while (requested_jobs-- > 0) {\n+      if (auto index_batch = get_index_batch()) {\n+        push_job(std::move(*index_batch));\n+      } else {\n+        shuttle_.exhausted();\n+      }\n+    }\n+  }\n+\n+  void prefetch() {\n+    prefetch(*options_.max_jobs_);\n+  }\n+\n+  at::optional<Batch> next() {\n+    at::optional<Batch> batch;\n+    if (options_.workers_ > 0) {\n+      // sequencer_->next(...).map(|r| r.batch)\n+      auto result = sequencer_->next(\n+          [this] { return shuttle_.pop_result(options_.timeout_); });\n+      if (result) {\n+        batch = std::move(result->batch);\n+      }\n+      prefetch(1);\n+    } else if (auto index_batch = get_index_batch()) {\n+      batch = dataset_->batch(std::move(*index_batch));\n+    }\n+    return batch;\n+  }\n+\n+  template <typename LoopFunction>\n+  void loop(LoopFunction function) {\n+    while (auto maybe_batch = next()) {\n+      function(*maybe_batch);\n+    }\n+    reset();\n+  }\n+\n+  void join() {\n+    if (joined_) {\n+      return;\n+    }\n+    shuttle_.drain();\n+    // Send one 'quit' message per worker. Since a worker dies (exits its\n+    // thread) after receiving this message, each `QuitWorker()` message will be\n+    // read by exactly one worker.\n+    for (size_t w = 0; w < options_.workers_; ++w) {\n+      push_job(QuitWorker());\n+    }\n+    for (auto& worker : workers_) {\n+      worker.join();\n+    }\n+    joined_ = true;\n+  }\n+\n+  const DataLoaderOptions& options() const noexcept {\n+    return options_;\n+  }\n+\n+  size_t dataset_size() const noexcept {\n+    return dataset_size_;\n+  }\n+\n+ private:\n+  struct Sequenced {\n+    Sequenced() = default;\n+    Sequenced(size_t sqn) : sequence_number(sqn) {}\n+    size_t sequence_number;\n+  };\n+\n+  struct QuitWorker {};\n+  // Job = Enum(QuitWorker, IndexBatch)\n+  struct Job : Sequenced {\n+    Job() = default;\n+    Job(QuitWorker q, size_t sqn) : Sequenced(sqn), quit(q) {}\n+    Job(IndexBatch&& i, size_t sqn)\n+        : Sequenced(sqn), index_batch(std::move(i)) {}\n+    at::optional<QuitWorker> quit;", "path": "torch/csrc/api/include/torch/data/dataloader.h", "position": null, "original_position": 161, "commit_id": "2fabdad63c0bc48b26af6bf8d2e74513b09c97da", "original_commit_id": "d87492e397ef6bf55e2fc8782dfaefdd5aba9920", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This is just a boolean", "created_at": "2018-09-25T15:02:18Z", "updated_at": "2018-11-23T15:51:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/11918#discussion_r220230102", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11918", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220230102"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11918#discussion_r220230102"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11918"}}, "body_html": "<p>This is just a boolean</p>", "body_text": "This is just a boolean"}