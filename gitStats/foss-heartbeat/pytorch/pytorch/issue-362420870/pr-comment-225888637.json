{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225888637", "pull_request_review_id": 165584111, "id": 225888637, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNTg4ODYzNw==", "diff_hunk": "@@ -0,0 +1,49 @@\n+#pragma once\n+\n+#include <torch/data/example.h>\n+#include <torch/data/transforms/collate.h>\n+#include <torch/tensor.h>\n+\n+#include <utility>\n+#include <vector>\n+\n+namespace torch {\n+namespace data {\n+namespace transforms {\n+\n+template <typename T = Example<>>\n+struct Stack;\n+\n+/// A `Collation` for `Example<Tensor, Tensor>` types that stacks all data\n+/// tensors into one tensor, and all target (label) tensors into one tensor.\n+template <>\n+struct Stack<Example<>> : public Collation<Example<>> {", "path": "torch/csrc/api/include/torch/data/transforms/stack.h", "position": 20, "original_position": 20, "commit_id": "2fabdad63c0bc48b26af6bf8d2e74513b09c97da", "original_commit_id": "0cf5d795bd8b96ac542039a8f612df1ce84602f2", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "If it were to follow the python interface, it would stack a `vector<int>` into a `Tensor` of `int64_t`.\r\nIf you imagine an `int` as a scalar tensor, then it makes sense to return a 1d tensor.", "created_at": "2018-10-17T11:40:29Z", "updated_at": "2018-11-23T15:53:08Z", "html_url": "https://github.com/pytorch/pytorch/pull/11918#discussion_r225888637", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11918", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225888637"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11918#discussion_r225888637"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11918"}}, "body_html": "<p>If it were to follow the python interface, it would stack a <code>vector&lt;int&gt;</code> into a <code>Tensor</code> of <code>int64_t</code>.<br>\nIf you imagine an <code>int</code> as a scalar tensor, then it makes sense to return a 1d tensor.</p>", "body_text": "If it were to follow the python interface, it would stack a vector<int> into a Tensor of int64_t.\nIf you imagine an int as a scalar tensor, then it makes sense to return a 1d tensor.", "in_reply_to_id": 223752653}