{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/376366927", "html_url": "https://github.com/pytorch/pytorch/pull/6031#issuecomment-376366927", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6031", "id": 376366927, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjM2NjkyNw==", "user": {"login": "Jiaming-Liu", "id": 16099575, "node_id": "MDQ6VXNlcjE2MDk5NTc1", "avatar_url": "https://avatars3.githubusercontent.com/u/16099575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jiaming-Liu", "html_url": "https://github.com/Jiaming-Liu", "followers_url": "https://api.github.com/users/Jiaming-Liu/followers", "following_url": "https://api.github.com/users/Jiaming-Liu/following{/other_user}", "gists_url": "https://api.github.com/users/Jiaming-Liu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jiaming-Liu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jiaming-Liu/subscriptions", "organizations_url": "https://api.github.com/users/Jiaming-Liu/orgs", "repos_url": "https://api.github.com/users/Jiaming-Liu/repos", "events_url": "https://api.github.com/users/Jiaming-Liu/events{/privacy}", "received_events_url": "https://api.github.com/users/Jiaming-Liu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-27T01:38:06Z", "updated_at": "2018-03-27T02:45:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Just to make it easier to understand:</p>\n<div class=\"highlight highlight-source-python\"><pre>optim <span class=\"pl-k\">=</span> torch.optim.Adam(<span class=\"pl-c1\">set</span>(p <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> model.parameters()))</pre></div>\n<p>should be avoided.</p>\n<p>A reasonable use case:</p>\n<div class=\"highlight highlight-source-python\"><pre>biases <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>(param <span class=\"pl-k\">for</span> name, param <span class=\"pl-k\">in</span> model.named_parameters() <span class=\"pl-k\">if</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">in</span> name)\nweights <span class=\"pl-k\">=</span> [p <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> model.parameters() <span class=\"pl-k\">if</span> p <span class=\"pl-k\">not</span> <span class=\"pl-k\">in</span> biases]  <span class=\"pl-c\"><span class=\"pl-c\">#</span> make biases a set to accelerate `in`</span>\ngroups <span class=\"pl-k\">=</span> [\n    <span class=\"pl-c1\">dict</span>(<span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>weights, <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">weight_decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5e-4</span>),\n    <span class=\"pl-c1\">dict</span>(<span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>biases, <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.2</span>, <span class=\"pl-v\">weight_decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n]\noptim <span class=\"pl-k\">=</span> torch.optim.Adam(groups)</pre></div>\n<p>This might raise error after <code>optim.load_state_dict()</code>, and it is very hard to debug.</p>\n<pre><code>Traceback (most recent call last):\n  File \"xxxxxxxxxxxxxx.py\", line 29, in &lt;module&gt;\n    optim.step()\n  File \"/xxxxxxxxxxxxxx/site-packages/torch/optim/adam.py\", line 69, in step\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\nRuntimeError: inconsistent tensor size, expected xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre>", "body_text": "Just to make it easier to understand:\noptim = torch.optim.Adam(set(p for p in model.parameters()))\nshould be avoided.\nA reasonable use case:\nbiases = set(param for name, param in model.named_parameters() if 'bias' in name)\nweights = [p for p in model.parameters() if p not in biases]  # make biases a set to accelerate `in`\ngroups = [\n    dict(params=weights, lr=0.1, weight_decay=5e-4),\n    dict(params=biases, lr=0.2, weight_decay=0)\n]\noptim = torch.optim.Adam(groups)\nThis might raise error after optim.load_state_dict(), and it is very hard to debug.\nTraceback (most recent call last):\n  File \"xxxxxxxxxxxxxx.py\", line 29, in <module>\n    optim.step()\n  File \"/xxxxxxxxxxxxxx/site-packages/torch/optim/adam.py\", line 69, in step\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\nRuntimeError: inconsistent tensor size, expected xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx", "body": "Just to make it easier to understand:\r\n``` Python\r\noptim = torch.optim.Adam(set(p for p in model.parameters()))\r\n```\r\nshould be avoided.\r\n\r\nA reasonable use case:\r\n``` Python\r\nbiases = set(param for name, param in model.named_parameters() if 'bias' in name)\r\nweights = [p for p in model.parameters() if p not in biases]  # make biases a set to accelerate `in`\r\ngroups = [\r\n    dict(params=weights, lr=0.1, weight_decay=5e-4),\r\n    dict(params=biases, lr=0.2, weight_decay=0)\r\n]\r\noptim = torch.optim.Adam(groups)\r\n```\r\nThis might raise error after `optim.load_state_dict()`, and it is very hard to debug.\r\n```\r\nTraceback (most recent call last):\r\n  File \"xxxxxxxxxxxxxx.py\", line 29, in <module>\r\n    optim.step()\r\n  File \"/xxxxxxxxxxxxxx/site-packages/torch/optim/adam.py\", line 69, in step\r\n    exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\nRuntimeError: inconsistent tensor size, expected xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\n```"}