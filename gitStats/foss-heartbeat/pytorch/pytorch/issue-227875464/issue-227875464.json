{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1537", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1537/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1537/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1537/events", "html_url": "https://github.com/pytorch/pytorch/issues/1537", "id": 227875464, "node_id": "MDU6SXNzdWUyMjc4NzU0NjQ=", "number": 1537, "title": "GPU out of memory when data is on RAM", "user": {"login": "wddabc", "id": 5722427, "node_id": "MDQ6VXNlcjU3MjI0Mjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5722427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wddabc", "html_url": "https://github.com/wddabc", "followers_url": "https://api.github.com/users/wddabc/followers", "following_url": "https://api.github.com/users/wddabc/following{/other_user}", "gists_url": "https://api.github.com/users/wddabc/gists{/gist_id}", "starred_url": "https://api.github.com/users/wddabc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wddabc/subscriptions", "organizations_url": "https://api.github.com/users/wddabc/orgs", "repos_url": "https://api.github.com/users/wddabc/repos", "events_url": "https://api.github.com/users/wddabc/events{/privacy}", "received_events_url": "https://api.github.com/users/wddabc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-05-11T04:30:19Z", "updated_at": "2018-02-17T18:54:49Z", "closed_at": "2017-05-11T16:37:51Z", "author_association": "NONE", "body_html": "<p>Hi, I'm running the following snippet</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> numpy\ntorch.set_default_tensor_type(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>torch.FloatTensor<span class=\"pl-pds\">\"</span></span>)\nw <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5000</span>\nh <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4000</span>\n<span class=\"pl-k\">assert</span> torch.cuda.is_available()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>large_mtx=numpy.random.rand(10000, 10000)</span>\nallocation <span class=\"pl-k\">=</span> time.time()\na <span class=\"pl-k\">=</span> torch.rand(w,h)\nb <span class=\"pl-k\">=</span> torch.rand(h,w)\na_cu <span class=\"pl-k\">=</span> a.cuda()\nb_cu <span class=\"pl-k\">=</span> b.cuda()\nstart <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Allocation <span class=\"pl-pds\">\"</span></span>, allocation <span class=\"pl-k\">-</span> start)\nc <span class=\"pl-k\">=</span> a_cu.mm(b_cu)\ncu_blas <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Torch cuBlas <span class=\"pl-pds\">\"</span></span>, cu_blas <span class=\"pl-k\">-</span> allocation)</pre></div>\n<p>on my laptop, which works fine. However, when I uncomment the <code>large_mtx=numpy.random.rand(10000, 10000)</code>, which declares some CPU memory and run again. I got:</p>\n<blockquote>\n<p>THCudaCheck FAIL file=/Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory<br>\nTraceback (most recent call last):<br>\nFile \"mem_test.py\", line 16, in <br>\nc = a_cu.mm(b_cu)<br>\nRuntimeError: cuda runtime error (2) : out of memory at /Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu:66</p>\n</blockquote>\n<p>Looks like pytorch tries to allocate that numpy array into GPU.</p>\n<p>I'm using pytorch 0.1.11+4e693d1, cuda version 8.0, macOs 10.12.4</p>\n<p><strong>Update</strong>. when I comment that line back, it no longer works and throws error on a different line:</p>\n<blockquote>\n<p>THCudaCheck FAIL file=/Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory<br>\nTraceback (most recent call last):<br>\nFile \"mem_test.py\", line 13, in <br>\nb_cu = b.cuda()<br>\nFile \"/Study/tools/anaconda2/lib/python2.7/site-packages/torch/_utils.py\", line 65, in <em>cuda<br>\nreturn new_type(self.size()).copy</em>(self, async)<br>\nRuntimeError: cuda runtime error (2) : out of memory at /Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu:66</p>\n</blockquote>\n<p>Looks like the GPU memory leaks on the previous run and never recycled.</p>", "body_text": "Hi, I'm running the following snippet\nimport time\nimport torch\nimport numpy\ntorch.set_default_tensor_type(\"torch.FloatTensor\")\nw = 5000\nh = 4000\nassert torch.cuda.is_available()\n#large_mtx=numpy.random.rand(10000, 10000)\nallocation = time.time()\na = torch.rand(w,h)\nb = torch.rand(h,w)\na_cu = a.cuda()\nb_cu = b.cuda()\nstart = time.time()\nprint(\"Allocation \", allocation - start)\nc = a_cu.mm(b_cu)\ncu_blas = time.time()\nprint(\"Torch cuBlas \", cu_blas - allocation)\non my laptop, which works fine. However, when I uncomment the large_mtx=numpy.random.rand(10000, 10000), which declares some CPU memory and run again. I got:\n\nTHCudaCheck FAIL file=/Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\nTraceback (most recent call last):\nFile \"mem_test.py\", line 16, in \nc = a_cu.mm(b_cu)\nRuntimeError: cuda runtime error (2) : out of memory at /Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu:66\n\nLooks like pytorch tries to allocate that numpy array into GPU.\nI'm using pytorch 0.1.11+4e693d1, cuda version 8.0, macOs 10.12.4\nUpdate. when I comment that line back, it no longer works and throws error on a different line:\n\nTHCudaCheck FAIL file=/Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\nTraceback (most recent call last):\nFile \"mem_test.py\", line 13, in \nb_cu = b.cuda()\nFile \"/Study/tools/anaconda2/lib/python2.7/site-packages/torch/_utils.py\", line 65, in cuda\nreturn new_type(self.size()).copy(self, async)\nRuntimeError: cuda runtime error (2) : out of memory at /Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu:66\n\nLooks like the GPU memory leaks on the previous run and never recycled.", "body": "Hi, I'm running the following snippet\r\n```python\r\nimport time\r\nimport torch\r\nimport numpy\r\ntorch.set_default_tensor_type(\"torch.FloatTensor\")\r\nw = 5000\r\nh = 4000\r\nassert torch.cuda.is_available()\r\n#large_mtx=numpy.random.rand(10000, 10000)\r\nallocation = time.time()\r\na = torch.rand(w,h)\r\nb = torch.rand(h,w)\r\na_cu = a.cuda()\r\nb_cu = b.cuda()\r\nstart = time.time()\r\nprint(\"Allocation \", allocation - start)\r\nc = a_cu.mm(b_cu)\r\ncu_blas = time.time()\r\nprint(\"Torch cuBlas \", cu_blas - allocation)\r\n```\r\non my laptop, which works fine. However, when I uncomment the `large_mtx=numpy.random.rand(10000, 10000)`, which declares some CPU memory and run again. I got:\r\n\r\n> THCudaCheck FAIL file=/Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\r\n> Traceback (most recent call last):\r\n>   File \"mem_test.py\", line 16, in <module>\r\n>     c = a_cu.mm(b_cu)\r\n> RuntimeError: cuda runtime error (2) : out of memory at /Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu:66\r\n\r\nLooks like pytorch tries to allocate that numpy array into GPU. \r\n\r\nI'm using pytorch 0.1.11+4e693d1, cuda version 8.0, macOs 10.12.4\r\n\r\n**Update**. when I comment that line back, it no longer works and throws error on a different line:\r\n\r\n> THCudaCheck FAIL file=/Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\r\n> Traceback (most recent call last):\r\n>   File \"mem_test.py\", line 13, in <module>\r\n>     b_cu = b.cuda()\r\n>   File \"/Study/tools/anaconda2/lib/python2.7/site-packages/torch/_utils.py\", line 65, in _cuda\r\n>     return new_type(self.size()).copy_(self, async)\r\n> RuntimeError: cuda runtime error (2) : out of memory at /Study/tools/pytorch/torch/lib/THC/generic/THCStorage.cu:66\r\n\r\nLooks like the GPU memory leaks on the previous run and never recycled."}