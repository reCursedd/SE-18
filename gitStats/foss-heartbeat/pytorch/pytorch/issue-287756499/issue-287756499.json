{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4605", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4605/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4605/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4605/events", "html_url": "https://github.com/pytorch/pytorch/issues/4605", "id": 287756499, "node_id": "MDU6SXNzdWUyODc3NTY0OTk=", "number": 4605, "title": "Warning on infinite acos gradients?", "user": {"login": "stefdoerr", "id": 7935362, "node_id": "MDQ6VXNlcjc5MzUzNjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7935362?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefdoerr", "html_url": "https://github.com/stefdoerr", "followers_url": "https://api.github.com/users/stefdoerr/followers", "following_url": "https://api.github.com/users/stefdoerr/following{/other_user}", "gists_url": "https://api.github.com/users/stefdoerr/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefdoerr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefdoerr/subscriptions", "organizations_url": "https://api.github.com/users/stefdoerr/orgs", "repos_url": "https://api.github.com/users/stefdoerr/repos", "events_url": "https://api.github.com/users/stefdoerr/events{/privacy}", "received_events_url": "https://api.github.com/users/stefdoerr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-11T11:40:52Z", "updated_at": "2018-01-11T12:21:40Z", "closed_at": "2018-01-11T11:49:40Z", "author_association": "NONE", "body_html": "<p>Just a suggestion. Should there maybe be some kind of warning thrown in the case we get <code>acos</code> gradients of <code>-inf</code>? I understand that the behavior is correct but as the <code>inf</code> propagates through the graph you end up with a <code>NaN</code> that can take a decent bit of debugging to detect in a larger program.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nangle <span class=\"pl-k\">=</span> torch.autograd.Variable(torch.FloatTensor([<span class=\"pl-c1\">0</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ncos <span class=\"pl-k\">=</span> torch.cos(angle)\nacos <span class=\"pl-k\">=</span> torch.acos(cos)\n<span class=\"pl-c1\">print</span>(torch.autograd.grad(acos, cos, <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>])\n<span class=\"pl-c1\">print</span>(torch.autograd.grad(acos, angle)[<span class=\"pl-c1\">0</span>])</pre></div>", "body_text": "Just a suggestion. Should there maybe be some kind of warning thrown in the case we get acos gradients of -inf? I understand that the behavior is correct but as the inf propagates through the graph you end up with a NaN that can take a decent bit of debugging to detect in a larger program.\nimport torch\nangle = torch.autograd.Variable(torch.FloatTensor([0]), requires_grad=True)\ncos = torch.cos(angle)\nacos = torch.acos(cos)\nprint(torch.autograd.grad(acos, cos, retain_graph=True)[0])\nprint(torch.autograd.grad(acos, angle)[0])", "body": "Just a suggestion. Should there maybe be some kind of warning thrown in the case we get `acos` gradients of `-inf`? I understand that the behavior is correct but as the `inf` propagates through the graph you end up with a `NaN` that can take a decent bit of debugging to detect in a larger program.\r\n\r\n```python\r\nimport torch\r\nangle = torch.autograd.Variable(torch.FloatTensor([0]), requires_grad=True)\r\ncos = torch.cos(angle)\r\nacos = torch.acos(cos)\r\nprint(torch.autograd.grad(acos, cos, retain_graph=True)[0])\r\nprint(torch.autograd.grad(acos, angle)[0])\r\n```"}