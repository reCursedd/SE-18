{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4534", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4534/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4534/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4534/events", "html_url": "https://github.com/pytorch/pytorch/issues/4534", "id": 286813719, "node_id": "MDU6SXNzdWUyODY4MTM3MTk=", "number": 4534, "title": "nn.BatchNorm1d fails with batch size 1 on the new PyTorch 0.3", "user": {"login": "ccsasuke", "id": 1481766, "node_id": "MDQ6VXNlcjE0ODE3NjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1481766?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ccsasuke", "html_url": "https://github.com/ccsasuke", "followers_url": "https://api.github.com/users/ccsasuke/followers", "following_url": "https://api.github.com/users/ccsasuke/following{/other_user}", "gists_url": "https://api.github.com/users/ccsasuke/gists{/gist_id}", "starred_url": "https://api.github.com/users/ccsasuke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ccsasuke/subscriptions", "organizations_url": "https://api.github.com/users/ccsasuke/orgs", "repos_url": "https://api.github.com/users/ccsasuke/repos", "events_url": "https://api.github.com/users/ccsasuke/events{/privacy}", "received_events_url": "https://api.github.com/users/ccsasuke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-01-08T17:02:51Z", "updated_at": "2018-11-06T16:09:21Z", "closed_at": "2018-01-08T17:15:02Z", "author_association": "NONE", "body_html": "<p>My code was running Okay on PyTorch 0.2, but ran into this error when forwarding through a BatchNorm1d layer on 0.3:</p>\n<p><code>ValueError: Expected more than 1 value per channel when training, got input size [1, 128]</code></p>\n<p>I looked into the code and found that functional.batch_norm() added a check in 0.3:</p>\n<pre><code>def batch_norm(input, running_mean, running_var, weight=None, bias=None,\n               training=False, momentum=0.1, eps=1e-5):\n    if training:\n        size = list(input.size())\n        if reduce(mul, size[2:], size[0]) == 1:\n            raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))\n    f = torch._C._functions.BatchNorm(running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled)\n    return f(input, weight, bias)\n</code></pre>\n<p>So each of my samples is a 1d vector of size 128, and some of the batches may happen to have size 1. Therefore, my input size can be [1, 128].<br>\nOn these (valid) batches, however, <code>reduce(mul, size[2:], size[0])</code> will return 1 and fails the check.</p>", "body_text": "My code was running Okay on PyTorch 0.2, but ran into this error when forwarding through a BatchNorm1d layer on 0.3:\nValueError: Expected more than 1 value per channel when training, got input size [1, 128]\nI looked into the code and found that functional.batch_norm() added a check in 0.3:\ndef batch_norm(input, running_mean, running_var, weight=None, bias=None,\n               training=False, momentum=0.1, eps=1e-5):\n    if training:\n        size = list(input.size())\n        if reduce(mul, size[2:], size[0]) == 1:\n            raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))\n    f = torch._C._functions.BatchNorm(running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled)\n    return f(input, weight, bias)\n\nSo each of my samples is a 1d vector of size 128, and some of the batches may happen to have size 1. Therefore, my input size can be [1, 128].\nOn these (valid) batches, however, reduce(mul, size[2:], size[0]) will return 1 and fails the check.", "body": "My code was running Okay on PyTorch 0.2, but ran into this error when forwarding through a BatchNorm1d layer on 0.3:\r\n\r\n`ValueError: Expected more than 1 value per channel when training, got input size [1, 128]`\r\n\r\nI looked into the code and found that functional.batch_norm() added a check in 0.3:\r\n\r\n```\r\ndef batch_norm(input, running_mean, running_var, weight=None, bias=None,\r\n               training=False, momentum=0.1, eps=1e-5):\r\n    if training:\r\n        size = list(input.size())\r\n        if reduce(mul, size[2:], size[0]) == 1:\r\n            raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))\r\n    f = torch._C._functions.BatchNorm(running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled)\r\n    return f(input, weight, bias)\r\n```\r\n\r\nSo each of my samples is a 1d vector of size 128, and some of the batches may happen to have size 1. Therefore, my input size can be [1, 128].\r\nOn these (valid) batches, however, `reduce(mul, size[2:], size[0])` will return 1 and fails the check.\r\n  "}