{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/356033424", "html_url": "https://github.com/pytorch/pytorch/issues/4534#issuecomment-356033424", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4534", "id": 356033424, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjAzMzQyNA==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-08T17:23:00Z", "updated_at": "2018-01-08T17:23:00Z", "author_association": "MEMBER", "body_html": "<p>It will fail. Don't train on batches of size 1 if you use feature-wise batch normalization. (Inference is fine on batch-size 1). Skip over the left-over batch.</p>\n<p>Batch normalization computes:</p>\n<pre><code>y = (x - mean(x)) / (std(x) + eps)\n</code></pre>\n<p>If you have one sample per batch then mean(x) = x, and the output will be entirely zero (ignoring the bias). You can't use that for learning.</p>", "body_text": "It will fail. Don't train on batches of size 1 if you use feature-wise batch normalization. (Inference is fine on batch-size 1). Skip over the left-over batch.\nBatch normalization computes:\ny = (x - mean(x)) / (std(x) + eps)\n\nIf you have one sample per batch then mean(x) = x, and the output will be entirely zero (ignoring the bias). You can't use that for learning.", "body": "It will fail. Don't train on batches of size 1 if you use feature-wise batch normalization. (Inference is fine on batch-size 1). Skip over the left-over batch.\r\n\r\nBatch normalization computes:\r\n\r\n```\r\ny = (x - mean(x)) / (std(x) + eps)\r\n```\r\n\r\nIf you have one sample per batch then mean(x) = x, and the output will be entirely zero (ignoring the bias). You can't use that for learning."}