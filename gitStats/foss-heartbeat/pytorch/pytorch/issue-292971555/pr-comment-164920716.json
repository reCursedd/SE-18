{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164920716", "pull_request_review_id": 92770119, "id": 164920716, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDkyMDcxNg==", "diff_hunk": "@@ -0,0 +1,154 @@\n+import math\n+from numbers import Number\n+\n+import torch\n+from torch.autograd import Variable\n+from torch.distributions import constraints\n+from torch.distributions.distribution import Distribution\n+from torch.distributions.utils import broadcast_all, lazy_property\n+\n+\n+def _get_batch_shape(bmat, bvec):\n+    \"\"\"\n+    Given a batch of matrices and a batch of vectors, compute the combined `batch_shape`.\n+    \"\"\"\n+    try:\n+        vec_shape = torch._C._infer_size(bvec.shape, bmat.shape[:-1])\n+    except RuntimeError:\n+        raise ValueError(\"Incompatible batch shapes: vector {}, matrix {}\".format(bvec.shape, bmat.shape))\n+    return torch.Size(vec_shape[:-1])\n+\n+\n+def _batch_mv(bmat, bvec):\n+    \"\"\"\n+    Performs a batched matrix-vector product, with an arbitrary batch shape.\n+    \"\"\"\n+    batch_shape = bvec.shape[:-1]\n+    event_dim = bvec.shape[-1]\n+    bmat = bmat.expand(batch_shape + (event_dim, event_dim))\n+    if batch_shape != bmat.shape[:-2]:\n+        raise ValueError(\"Batch shapes do not match: matrix {}, vector {}\".format(bmat.shape, bvec.shape))\n+    bvec = bvec.unsqueeze(-1)", "path": "torch/distributions/multivariate_normal.py", "position": null, "original_position": 31, "commit_id": "34e77b6002e95a3ff46320473772fb7c99479980", "original_commit_id": "4ea67b2456f617fa119fe3eb6aa109e8a278571a", "user": {"login": "tbrx", "id": 1396856, "node_id": "MDQ6VXNlcjEzOTY4NTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1396856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tbrx", "html_url": "https://github.com/tbrx", "followers_url": "https://api.github.com/users/tbrx/followers", "following_url": "https://api.github.com/users/tbrx/following{/other_user}", "gists_url": "https://api.github.com/users/tbrx/gists{/gist_id}", "starred_url": "https://api.github.com/users/tbrx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tbrx/subscriptions", "organizations_url": "https://api.github.com/users/tbrx/orgs", "repos_url": "https://api.github.com/users/tbrx/repos", "events_url": "https://api.github.com/users/tbrx/events{/privacy}", "received_events_url": "https://api.github.com/users/tbrx/received_events", "type": "User", "site_admin": false}, "body": "To call `torch.bmm`, as I understand it we need both arguments to be 3D tensors. If the batch_shape is multidimensional, then this flattens it down. It's more complicated here than elsewhere because the matrix `bmat` might be missing leading dimensions of the batch shape.\r\n\r\nI wanted to avoid pre-emptively calling `.expand` on the `scale_tril` matrix, since in the `_batch_mahalanobis` helper (line 71) we split along the batch dimension, call `torch.inverse`, and then call `torch.stack`. \r\n\r\nThis whole problem comes about because the leading dimensions of the`loc` and `scale_tril` parameters might not be the same. If the batch size is large (e.g. because loc has a batch dimension) but there is only a single covariance matrix, then we don't mistakenly end up inverting the single covariance matrix once for every item in the batch. (Originally we had also discussed prohibiting any batching of the `covariance_matrix` or `scale_tril` parameter.)\r\n\r\nIf I understand correctly, I think we could probably remove this entire workaround after #4612.\r\n", "created_at": "2018-01-31T00:19:04Z", "updated_at": "2018-11-23T15:38:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/4950#discussion_r164920716", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4950", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164920716"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4950#discussion_r164920716"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4950"}}, "body_html": "<p>To call <code>torch.bmm</code>, as I understand it we need both arguments to be 3D tensors. If the batch_shape is multidimensional, then this flattens it down. It's more complicated here than elsewhere because the matrix <code>bmat</code> might be missing leading dimensions of the batch shape.</p>\n<p>I wanted to avoid pre-emptively calling <code>.expand</code> on the <code>scale_tril</code> matrix, since in the <code>_batch_mahalanobis</code> helper (line 71) we split along the batch dimension, call <code>torch.inverse</code>, and then call <code>torch.stack</code>.</p>\n<p>This whole problem comes about because the leading dimensions of the<code>loc</code> and <code>scale_tril</code> parameters might not be the same. If the batch size is large (e.g. because loc has a batch dimension) but there is only a single covariance matrix, then we don't mistakenly end up inverting the single covariance matrix once for every item in the batch. (Originally we had also discussed prohibiting any batching of the <code>covariance_matrix</code> or <code>scale_tril</code> parameter.)</p>\n<p>If I understand correctly, I think we could probably remove this entire workaround after <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"287930038\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4612\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4612/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4612\">#4612</a>.</p>", "body_text": "To call torch.bmm, as I understand it we need both arguments to be 3D tensors. If the batch_shape is multidimensional, then this flattens it down. It's more complicated here than elsewhere because the matrix bmat might be missing leading dimensions of the batch shape.\nI wanted to avoid pre-emptively calling .expand on the scale_tril matrix, since in the _batch_mahalanobis helper (line 71) we split along the batch dimension, call torch.inverse, and then call torch.stack.\nThis whole problem comes about because the leading dimensions of theloc and scale_tril parameters might not be the same. If the batch size is large (e.g. because loc has a batch dimension) but there is only a single covariance matrix, then we don't mistakenly end up inverting the single covariance matrix once for every item in the batch. (Originally we had also discussed prohibiting any batching of the covariance_matrix or scale_tril parameter.)\nIf I understand correctly, I think we could probably remove this entire workaround after #4612.", "in_reply_to_id": 164910338}