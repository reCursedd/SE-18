{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5169", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5169/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5169/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5169/events", "html_url": "https://github.com/pytorch/pytorch/issues/5169", "id": 296065383, "node_id": "MDU6SXNzdWUyOTYwNjUzODM=", "number": 5169, "title": "Unnecessary memcopies emitted by autograd engine", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}, {"id": 679952992, "node_id": "MDU6TGFiZWw2Nzk5NTI5OTI=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/performance", "name": "performance", "color": "f9d0c4", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-02-10T02:26:48Z", "updated_at": "2018-04-20T15:51:38Z", "closed_at": "2018-04-20T15:51:38Z", "author_association": "CONTRIBUTOR", "body_html": "<ul>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: master</li>\n<li>How you installed PyTorch (conda, pip, source):source</li>\n<li>Python version:3.6</li>\n<li>CUDA/cuDNN version:9.0</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n</ul>\n<p>Using .view that actually changes dimensions in a model introduces 4(!) extra memcopies of the size of gradOutput in backward pass.  Minimum reproducer below, in the first loop where input is 3d and view is flattening it to 2D, there are 4 memcopies after softmax backward, in the second loop where view is a noop there is none. Running with emitted nvtx shows that the first memcopy comes from AsStridedBackward, 3 extra ones come from autogradCopySlices, it is possibly related to  <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"278567986\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3970\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3970/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3970\">#3970</a>. This is a regression from 0.3 that did not have those memcopies regardless of the view. It is also likely that this issue affects memory usage, as we are not able to fit the models on master that used to fit on 0.3.  cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(256,256)\n       \n    def  forward(self, input, target):\n        input = self.fc(input)\n        input = input.view(-1, input.size(-1))\n        loss = F.cross_entropy(input, target, size_average=False)\n        return loss\n\n\ninput =torch.randn(20,400,256).cuda()\ntarget = torch.ones(8000).cuda().long()\nmodel = Model().cuda()    \nfor i in range(5):\n    loss = model(torch.autograd.Variable(input, requires_grad = True), torch.autograd.Variable(target))\n    loss.backward()\n\ninput =torch.randn(8000,256).cuda()\nfor i in range(5):\n    loss = model(torch.autograd.Variable(input, requires_grad = True), torch.autograd.Variable(target))\n    loss.backward()\n</code></pre>", "body_text": "OS: Ubuntu 16.04\nPyTorch version: master\nHow you installed PyTorch (conda, pip, source):source\nPython version:3.6\nCUDA/cuDNN version:9.0\nGPU models and configuration:\nGCC version (if compiling from source):\n\nUsing .view that actually changes dimensions in a model introduces 4(!) extra memcopies of the size of gradOutput in backward pass.  Minimum reproducer below, in the first loop where input is 3d and view is flattening it to 2D, there are 4 memcopies after softmax backward, in the second loop where view is a noop there is none. Running with emitted nvtx shows that the first memcopy comes from AsStridedBackward, 3 extra ones come from autogradCopySlices, it is possibly related to  #3970. This is a regression from 0.3 that did not have those memcopies regardless of the view. It is also likely that this issue affects memory usage, as we are not able to fit the models on master that used to fit on 0.3.  cc @colesbury\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = torch.nn.Linear(256,256)\n       \n    def  forward(self, input, target):\n        input = self.fc(input)\n        input = input.view(-1, input.size(-1))\n        loss = F.cross_entropy(input, target, size_average=False)\n        return loss\n\n\ninput =torch.randn(20,400,256).cuda()\ntarget = torch.ones(8000).cuda().long()\nmodel = Model().cuda()    \nfor i in range(5):\n    loss = model(torch.autograd.Variable(input, requires_grad = True), torch.autograd.Variable(target))\n    loss.backward()\n\ninput =torch.randn(8000,256).cuda()\nfor i in range(5):\n    loss = model(torch.autograd.Variable(input, requires_grad = True), torch.autograd.Variable(target))\n    loss.backward()", "body": "- OS: Ubuntu 16.04\r\n- PyTorch version: master\r\n- How you installed PyTorch (conda, pip, source):source\r\n- Python version:3.6\r\n- CUDA/cuDNN version:9.0\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n\r\nUsing .view that actually changes dimensions in a model introduces 4(!) extra memcopies of the size of gradOutput in backward pass.  Minimum reproducer below, in the first loop where input is 3d and view is flattening it to 2D, there are 4 memcopies after softmax backward, in the second loop where view is a noop there is none. Running with emitted nvtx shows that the first memcopy comes from AsStridedBackward, 3 extra ones come from autogradCopySlices, it is possibly related to  #3970. This is a regression from 0.3 that did not have those memcopies regardless of the view. It is also likely that this issue affects memory usage, as we are not able to fit the models on master that used to fit on 0.3.  cc @colesbury \r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.fc = torch.nn.Linear(256,256)\r\n       \r\n    def  forward(self, input, target):\r\n        input = self.fc(input)\r\n        input = input.view(-1, input.size(-1))\r\n        loss = F.cross_entropy(input, target, size_average=False)\r\n        return loss\r\n\r\n\r\ninput =torch.randn(20,400,256).cuda()\r\ntarget = torch.ones(8000).cuda().long()\r\nmodel = Model().cuda()    \r\nfor i in range(5):\r\n    loss = model(torch.autograd.Variable(input, requires_grad = True), torch.autograd.Variable(target))\r\n    loss.backward()\r\n\r\ninput =torch.randn(8000,256).cuda()\r\nfor i in range(5):\r\n    loss = model(torch.autograd.Variable(input, requires_grad = True), torch.autograd.Variable(target))\r\n    loss.backward()\r\n```\r\n\r\n"}