{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/365772460", "html_url": "https://github.com/pytorch/pytorch/issues/5169#issuecomment-365772460", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5169", "id": 365772460, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTc3MjQ2MA==", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-14T22:54:57Z", "updated_at": "2018-02-14T23:12:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Redefining nn/functional.py linear as (as you recommend in 2):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">linear</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">weight</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n   <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>                                                                                                                                                                                                    </span>\n<span class=\"pl-s\">   Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.                                                                                                                            </span>\n<span class=\"pl-s\">                                                                                                                                                                                                           </span>\n<span class=\"pl-s\">   Shape:                                                                                                                                                                                                  </span>\n<span class=\"pl-s\">       - Input: :math:`(N, *, in\\_features)` where `*` means any number of                                                                                                                                </span>\n<span class=\"pl-s\">         additional dimensions                                                                                                                                                                            </span>\n<span class=\"pl-s\">       - Weight: :math:`(out\\_features, in\\_features)`                                                                                                                                                    </span>\n<span class=\"pl-s\">       - Bias: :math:`(out\\_features)`                                                                                                                                                                    </span>\n<span class=\"pl-s\">       - Output: :math:`(N, *, out\\_features)`                                                                                                                                                            </span>\n<span class=\"pl-s\">   <span class=\"pl-pds\">\"\"\"</span></span>\n\n   <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.contiguous()\n   sizes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.size()[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n   <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">input</span>.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n   <span class=\"pl-k\">if</span> <span class=\"pl-c1\">input</span>.dim() <span class=\"pl-k\">==</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">and</span> bias <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n       <span class=\"pl-c\"><span class=\"pl-c\">#</span> fused op is marginally faster                                                                                                                                                                    </span>\n       output <span class=\"pl-k\">=</span> torch.addmm(bias, <span class=\"pl-c1\">input</span>, weight.t())\n       <span class=\"pl-k\">return</span> output.view(<span class=\"pl-k\">*</span>sizes, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n   output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.matmul(weight.t())\n   <span class=\"pl-k\">if</span> bias <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n       output <span class=\"pl-k\">+=</span> bias\n   <span class=\"pl-k\">return</span> output</pre></div>\n<p>Was enough to avoid the mem copies in fairseq. Should we just route though to addmm for now until someone tackles 1?</p>", "body_text": "Redefining nn/functional.py linear as (as you recommend in 2):\ndef linear(input, weight, bias=None):\n   \"\"\"                                                                                                                                                                                                    \n   Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.                                                                                                                            \n                                                                                                                                                                                                           \n   Shape:                                                                                                                                                                                                  \n       - Input: :math:`(N, *, in\\_features)` where `*` means any number of                                                                                                                                \n         additional dimensions                                                                                                                                                                            \n       - Weight: :math:`(out\\_features, in\\_features)`                                                                                                                                                    \n       - Bias: :math:`(out\\_features)`                                                                                                                                                                    \n       - Output: :math:`(N, *, out\\_features)`                                                                                                                                                            \n   \"\"\"\n\n   input = input.contiguous()\n   sizes = input.size()[:-1]\n   input = input.view(-1, input.size(-1))\n   if input.dim() == 2 and bias is not None:\n       # fused op is marginally faster                                                                                                                                                                    \n       output = torch.addmm(bias, input, weight.t())\n       return output.view(*sizes, -1)\n\n   output = input.matmul(weight.t())\n   if bias is not None:\n       output += bias\n   return output\nWas enough to avoid the mem copies in fairseq. Should we just route though to addmm for now until someone tackles 1?", "body": "Redefining nn/functional.py linear as (as you recommend in 2):\r\n```.py\r\ndef linear(input, weight, bias=None):\r\n   \"\"\"                                                                                                                                                                                                    \r\n   Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.                                                                                                                            \r\n                                                                                                                                                                                                           \r\n   Shape:                                                                                                                                                                                                  \r\n       - Input: :math:`(N, *, in\\_features)` where `*` means any number of                                                                                                                                \r\n         additional dimensions                                                                                                                                                                            \r\n       - Weight: :math:`(out\\_features, in\\_features)`                                                                                                                                                    \r\n       - Bias: :math:`(out\\_features)`                                                                                                                                                                    \r\n       - Output: :math:`(N, *, out\\_features)`                                                                                                                                                            \r\n   \"\"\"\r\n\r\n   input = input.contiguous()\r\n   sizes = input.size()[:-1]\r\n   input = input.view(-1, input.size(-1))\r\n   if input.dim() == 2 and bias is not None:\r\n       # fused op is marginally faster                                                                                                                                                                    \r\n       output = torch.addmm(bias, input, weight.t())\r\n       return output.view(*sizes, -1)\r\n\r\n   output = input.matmul(weight.t())\r\n   if bias is not None:\r\n       output += bias\r\n   return output\r\n```\r\nWas enough to avoid the mem copies in fairseq. Should we just route though to addmm for now until someone tackles 1?"}