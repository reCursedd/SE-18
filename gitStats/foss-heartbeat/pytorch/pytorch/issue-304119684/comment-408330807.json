{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/408330807", "html_url": "https://github.com/pytorch/pytorch/issues/5694#issuecomment-408330807", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5694", "id": 408330807, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODMzMDgwNw==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-27T07:01:58Z", "updated_at": "2018-07-27T07:01:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Contrary to what the title suggests, the crash resulting from the types, it is about division by 0 for long.</p>\n<p>To for the scalar case, this is easily caught by checking == 0 in div_DEFAULT (and possibly others) and made into an exception (or should it be a warning as in NumPy?).<br>\nFor the tensor case, that also has this, this is not all that efficient, so we might need to have a signal handler, have that set a flag and check that flag upon exit.</p>\n<p>So two questions</p>\n<ul>\n<li>What should be the escalation level? (Exception or Warning)</li>\n<li>Is signal handler + checking OK? Or is there a completely other way to do it?</li>\n</ul>\n<p>I think the question of whether float * Int Tensor should be float or int is separate.</p>", "body_text": "Contrary to what the title suggests, the crash resulting from the types, it is about division by 0 for long.\nTo for the scalar case, this is easily caught by checking == 0 in div_DEFAULT (and possibly others) and made into an exception (or should it be a warning as in NumPy?).\nFor the tensor case, that also has this, this is not all that efficient, so we might need to have a signal handler, have that set a flag and check that flag upon exit.\nSo two questions\n\nWhat should be the escalation level? (Exception or Warning)\nIs signal handler + checking OK? Or is there a completely other way to do it?\n\nI think the question of whether float * Int Tensor should be float or int is separate.", "body": "Contrary to what the title suggests, the crash resulting from the types, it is about division by 0 for long.\r\n\r\nTo for the scalar case, this is easily caught by checking == 0 in div_DEFAULT (and possibly others) and made into an exception (or should it be a warning as in NumPy?).\r\nFor the tensor case, that also has this, this is not all that efficient, so we might need to have a signal handler, have that set a flag and check that flag upon exit.\r\n\r\nSo two questions\r\n- What should be the escalation level? (Exception or Warning)\r\n- Is signal handler + checking OK? Or is there a completely other way to do it?\r\n\r\nI think the question of whether float * Int Tensor should be float or int is separate.\r\n"}