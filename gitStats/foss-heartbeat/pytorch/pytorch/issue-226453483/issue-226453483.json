{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1479", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1479/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1479/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1479/events", "html_url": "https://github.com/pytorch/pytorch/issues/1479", "id": 226453483, "node_id": "MDU6SXNzdWUyMjY0NTM0ODM=", "number": 1479, "title": "use view on variable cause backward fail to assign grad?", "user": {"login": "Frefreak", "id": 7122156, "node_id": "MDQ6VXNlcjcxMjIxNTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/7122156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Frefreak", "html_url": "https://github.com/Frefreak", "followers_url": "https://api.github.com/users/Frefreak/followers", "following_url": "https://api.github.com/users/Frefreak/following{/other_user}", "gists_url": "https://api.github.com/users/Frefreak/gists{/gist_id}", "starred_url": "https://api.github.com/users/Frefreak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Frefreak/subscriptions", "organizations_url": "https://api.github.com/users/Frefreak/orgs", "repos_url": "https://api.github.com/users/Frefreak/repos", "events_url": "https://api.github.com/users/Frefreak/events{/privacy}", "received_events_url": "https://api.github.com/users/Frefreak/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-05T01:49:01Z", "updated_at": "2017-05-05T01:58:19Z", "closed_at": "2017-05-05T01:52:59Z", "author_association": "NONE", "body_html": "<p>Hi, I notice that if I use <code>view</code> on a variable, the variable's <code>grad</code> would always be <code>None</code>. I'm new to this library and couldn't find any doc mentions this? Is this the expected behavior?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\na <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>])).view(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>)\nb <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">6</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>).view(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> view applied to variable makes b.grad None</span>\nc <span class=\"pl-k\">=</span> a.mm(b).sum()\nc.backward()\n<span class=\"pl-c1\">print</span>(a.requires_grad)\n<span class=\"pl-c1\">print</span>(b.requires_grad)\n<span class=\"pl-c1\">print</span>(b.grad)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">20</span>)\n\n<span class=\"pl-k\">del</span> a, b, c\n\na <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>])).view(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>)\nb <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">6</span>]).view(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> view applied to tensor works</span>\nc <span class=\"pl-k\">=</span> a.mm(b).sum()\nc.backward()\n<span class=\"pl-c1\">print</span>(a.requires_grad)\n<span class=\"pl-c1\">print</span>(b.requires_grad)\n<span class=\"pl-c1\">print</span>(b.grad)</pre></div>\n<p>output:</p>\n<pre><code>False\nTrue\nNone\n--------------------\nFalse\nTrue\nVariable containing:\n 1\n 2\n 3\n[torch.FloatTensor of size 3x1]\n</code></pre>\n<p>I installed pytorch via <code>pip install http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp36-cp36m-linux_x86_64.whl </code> (version 0.1.12_2)</p>", "body_text": "Hi, I notice that if I use view on a variable, the variable's grad would always be None. I'm new to this library and couldn't find any doc mentions this? Is this the expected behavior?\n#!/usr/bin/env python\n\nimport torch\nfrom torch.autograd import Variable\n\na = Variable(torch.Tensor([1, 2, 3])).view(1, 3)\nb = Variable(torch.Tensor([4, 5, 6]), requires_grad=True).view(3, 1) # view applied to variable makes b.grad None\nc = a.mm(b).sum()\nc.backward()\nprint(a.requires_grad)\nprint(b.requires_grad)\nprint(b.grad)\n\nprint('-' * 20)\n\ndel a, b, c\n\na = Variable(torch.Tensor([1, 2, 3])).view(1, 3)\nb = Variable(torch.Tensor([4, 5, 6]).view(3, 1), requires_grad=True) # view applied to tensor works\nc = a.mm(b).sum()\nc.backward()\nprint(a.requires_grad)\nprint(b.requires_grad)\nprint(b.grad)\noutput:\nFalse\nTrue\nNone\n--------------------\nFalse\nTrue\nVariable containing:\n 1\n 2\n 3\n[torch.FloatTensor of size 3x1]\n\nI installed pytorch via pip install http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp36-cp36m-linux_x86_64.whl  (version 0.1.12_2)", "body": "Hi, I notice that if I use `view` on a variable, the variable's `grad` would always be `None`. I'm new to this library and couldn't find any doc mentions this? Is this the expected behavior?\r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\na = Variable(torch.Tensor([1, 2, 3])).view(1, 3)\r\nb = Variable(torch.Tensor([4, 5, 6]), requires_grad=True).view(3, 1) # view applied to variable makes b.grad None\r\nc = a.mm(b).sum()\r\nc.backward()\r\nprint(a.requires_grad)\r\nprint(b.requires_grad)\r\nprint(b.grad)\r\n\r\nprint('-' * 20)\r\n\r\ndel a, b, c\r\n\r\na = Variable(torch.Tensor([1, 2, 3])).view(1, 3)\r\nb = Variable(torch.Tensor([4, 5, 6]).view(3, 1), requires_grad=True) # view applied to tensor works\r\nc = a.mm(b).sum()\r\nc.backward()\r\nprint(a.requires_grad)\r\nprint(b.requires_grad)\r\nprint(b.grad)\r\n```\r\n\r\noutput:\r\n```\r\nFalse\r\nTrue\r\nNone\r\n--------------------\r\nFalse\r\nTrue\r\nVariable containing:\r\n 1\r\n 2\r\n 3\r\n[torch.FloatTensor of size 3x1]\r\n```\r\nI installed pytorch via `pip install http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp36-cp36m-linux_x86_64.whl ` (version 0.1.12_2)"}