{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5744", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5744/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5744/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5744/events", "html_url": "https://github.com/pytorch/pytorch/pull/5744", "id": 304892179, "node_id": "MDExOlB1bGxSZXF1ZXN0MTc0NzU2NzY3", "number": 5744, "title": "Fix bmm memory leak", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-03-13T18:51:47Z", "updated_at": "2018-03-15T14:44:36Z", "closed_at": "2018-03-15T14:44:36Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5744", "html_url": "https://github.com/pytorch/pytorch/pull/5744", "diff_url": "https://github.com/pytorch/pytorch/pull/5744.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5744.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #5611.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"303152463\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5611\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5611/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/5611\">#5611</a>.</p>\n<p><code>THCTensor_(baddbmm)</code> assumes that newContiguous will always return a new tensor (this is a bad assumption). At the end of the function, tensors are freed if <code>tensor_new != tensor_old</code>. As a result, some tensors aren't freed if they were initially contiguous and <code>newContiguous</code> is called on them.</p>\n<h3>Test Plan</h3>\n<ul>\n<li>code reading</li>\n<li>run the following (from the <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"303152463\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5611\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/5611/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/5611\">#5611</a> bug report) and assert that the memory doesn't leak anymore</li>\n</ul>\n<pre><code>import subprocess\nimport torch\nfrom torch.autograd import Variable\n\n# This is from https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\ndef get_gpu_memory_map():\n    \"\"\"Get the current gpu usage.\n\n    Returns\n    -------\n    usage: dict\n        Keys are device ids as integers.\n        Values are memory usage as integers in MB.\n    \"\"\"\n    result = subprocess.check_output(\n        [\n            'nvidia-smi', '--query-gpu=memory.used',\n            '--format=csv,nounits,noheader'\n        ], encoding='utf-8')\n    # Convert lines into a dictionary\n    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n    return gpu_memory_map\n\nl, m, n = 1, 9, 1\nw = torch.nn.Parameter(torch.Tensor(1024, 2, l, m).cuda())\nfor i in range(10000):\n    a = Variable(torch.Tensor(1024, 2, m, n).cuda())\n    torch.matmul(w, a).permute(0, 3, 1, 2).mean().backward()\n    if i % 100 == 0:\n        gpu_mem = get_gpu_memory_map()\n        print(\"GPU: {:.2f} KB\".format(gpu_mem[0]))\n</code></pre>", "body_text": "Fixes #5611.\nTHCTensor_(baddbmm) assumes that newContiguous will always return a new tensor (this is a bad assumption). At the end of the function, tensors are freed if tensor_new != tensor_old. As a result, some tensors aren't freed if they were initially contiguous and newContiguous is called on them.\nTest Plan\n\ncode reading\nrun the following (from the #5611 bug report) and assert that the memory doesn't leak anymore\n\nimport subprocess\nimport torch\nfrom torch.autograd import Variable\n\n# This is from https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\ndef get_gpu_memory_map():\n    \"\"\"Get the current gpu usage.\n\n    Returns\n    -------\n    usage: dict\n        Keys are device ids as integers.\n        Values are memory usage as integers in MB.\n    \"\"\"\n    result = subprocess.check_output(\n        [\n            'nvidia-smi', '--query-gpu=memory.used',\n            '--format=csv,nounits,noheader'\n        ], encoding='utf-8')\n    # Convert lines into a dictionary\n    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n    return gpu_memory_map\n\nl, m, n = 1, 9, 1\nw = torch.nn.Parameter(torch.Tensor(1024, 2, l, m).cuda())\nfor i in range(10000):\n    a = Variable(torch.Tensor(1024, 2, m, n).cuda())\n    torch.matmul(w, a).permute(0, 3, 1, 2).mean().backward()\n    if i % 100 == 0:\n        gpu_mem = get_gpu_memory_map()\n        print(\"GPU: {:.2f} KB\".format(gpu_mem[0]))", "body": "Fixes #5611.\r\n\r\n`THCTensor_(baddbmm)` assumes that newContiguous will always return a new tensor (this is a bad assumption). At the end of the function, tensors are freed if `tensor_new != tensor_old`. As a result, some tensors aren't freed if they were initially contiguous and `newContiguous` is called on them.\r\n\r\n### Test Plan\r\n- code reading\r\n- run the following (from the #5611 bug report) and assert that the memory doesn't leak anymore\r\n```\r\nimport subprocess\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n# This is from https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\r\ndef get_gpu_memory_map():\r\n    \"\"\"Get the current gpu usage.\r\n\r\n    Returns\r\n    -------\r\n    usage: dict\r\n        Keys are device ids as integers.\r\n        Values are memory usage as integers in MB.\r\n    \"\"\"\r\n    result = subprocess.check_output(\r\n        [\r\n            'nvidia-smi', '--query-gpu=memory.used',\r\n            '--format=csv,nounits,noheader'\r\n        ], encoding='utf-8')\r\n    # Convert lines into a dictionary\r\n    gpu_memory = [int(x) for x in result.strip().split('\\n')]\r\n    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\r\n    return gpu_memory_map\r\n\r\nl, m, n = 1, 9, 1\r\nw = torch.nn.Parameter(torch.Tensor(1024, 2, l, m).cuda())\r\nfor i in range(10000):\r\n    a = Variable(torch.Tensor(1024, 2, m, n).cuda())\r\n    torch.matmul(w, a).permute(0, 3, 1, 2).mean().backward()\r\n    if i % 100 == 0:\r\n        gpu_mem = get_gpu_memory_map()\r\n        print(\"GPU: {:.2f} KB\".format(gpu_mem[0]))\r\n```"}