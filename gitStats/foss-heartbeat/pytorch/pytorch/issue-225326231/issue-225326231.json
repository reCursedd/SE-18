{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1410", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1410/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1410/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1410/events", "html_url": "https://github.com/pytorch/pytorch/issues/1410", "id": 225326231, "node_id": "MDU6SXNzdWUyMjUzMjYyMzE=", "number": 1410, "title": "BatchNorm should use Bessel's correction consistently", "user": {"login": "Cyanogenoid", "id": 1807181, "node_id": "MDQ6VXNlcjE4MDcxODE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1807181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Cyanogenoid", "html_url": "https://github.com/Cyanogenoid", "followers_url": "https://api.github.com/users/Cyanogenoid/followers", "following_url": "https://api.github.com/users/Cyanogenoid/following{/other_user}", "gists_url": "https://api.github.com/users/Cyanogenoid/gists{/gist_id}", "starred_url": "https://api.github.com/users/Cyanogenoid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Cyanogenoid/subscriptions", "organizations_url": "https://api.github.com/users/Cyanogenoid/orgs", "repos_url": "https://api.github.com/users/Cyanogenoid/repos", "events_url": "https://api.github.com/users/Cyanogenoid/events{/privacy}", "received_events_url": "https://api.github.com/users/Cyanogenoid/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-04-30T13:11:50Z", "updated_at": "2017-04-30T13:11:50Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Currently, Bessel's correction is applied to the running variance estimator in BatchNorm on <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/BatchNormalization.cu#L196\">this line</a> and the correction is not applied at train time. While the original paper for <a href=\"https://arxiv.org/abs/1502.03167\" rel=\"nofollow\">BatchNorm</a> agrees with this, in the follow-up paper of the author, <a href=\"https://arxiv.org/abs/1702.03275\" rel=\"nofollow\">Batch Renormalization</a>, they state on page 2 when talking about BatchNorm during training:</p>\n<blockquote>\n<p>[...] the computation of the sample mean \u00b5_B and sample standard deviation \u03c3_B are part of the model architecture [...]</p>\n</blockquote>\n<p>and on page 3 state:</p>\n<blockquote>\n<p>Batch Normalization, in fact, simply sets r = 1</p>\n</blockquote>\n<p>where r is the ratio of minibatch standard deviation and estimated standard deviation, implying that during both training and testing, Bessel's correction is applied.</p>\n<p>I think the approach for BatchNorm in Batch Renormalization makes more sense than unbiasing one variance but not the other, so I believe that PyTorch should allow an argument to BatchNorm that toggles this behaviour and perhaps consider switching the default. An argument would also bring visibility towards this implementation difference in Deep Learning frameworks.</p>\n<p>Looking through some of the most popular DL libraries, I found no implementation that uses the unbiased variance during training and inference, but some that don't unbias either. Note that the former is essentially the same as the latter when the latter initializes gamma with the correction factor, so I imagine that they should behave mostly the same even without correcting gamma.<br>\nSome of the frameworks that don't unbias at all are: <a href=\"https://github.com/fchollet/keras/blob/master/keras/layers/normalization.py#L182-L184\">Keras</a>, <a href=\"https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/normalization.py#L297-L299\">Lasagne</a>, <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L696-L697\">TF-Slim</a>.<br>\nSome of course follow the original paper by unbiasing the running variance only: <a href=\"https://github.com/BVLC/caffe/blob/master/src/caffe/layers/batch_norm_layer.cpp#L143\">Caffe</a>, <a href=\"http://deeplearning.net/software/theano/library/tensor/nnet/bn.html\" rel=\"nofollow\">Theano</a>.</p>\n<p>Anecdotally, when biasing the running unbiased variances again by inverting Bessel's correction, I get statistically significant slightly better validation accuracies when using relatively small batch sizes (32) in densely connected networks.</p>", "body_text": "Currently, Bessel's correction is applied to the running variance estimator in BatchNorm on this line and the correction is not applied at train time. While the original paper for BatchNorm agrees with this, in the follow-up paper of the author, Batch Renormalization, they state on page 2 when talking about BatchNorm during training:\n\n[...] the computation of the sample mean \u00b5_B and sample standard deviation \u03c3_B are part of the model architecture [...]\n\nand on page 3 state:\n\nBatch Normalization, in fact, simply sets r = 1\n\nwhere r is the ratio of minibatch standard deviation and estimated standard deviation, implying that during both training and testing, Bessel's correction is applied.\nI think the approach for BatchNorm in Batch Renormalization makes more sense than unbiasing one variance but not the other, so I believe that PyTorch should allow an argument to BatchNorm that toggles this behaviour and perhaps consider switching the default. An argument would also bring visibility towards this implementation difference in Deep Learning frameworks.\nLooking through some of the most popular DL libraries, I found no implementation that uses the unbiased variance during training and inference, but some that don't unbias either. Note that the former is essentially the same as the latter when the latter initializes gamma with the correction factor, so I imagine that they should behave mostly the same even without correcting gamma.\nSome of the frameworks that don't unbias at all are: Keras, Lasagne, TF-Slim.\nSome of course follow the original paper by unbiasing the running variance only: Caffe, Theano.\nAnecdotally, when biasing the running unbiased variances again by inverting Bessel's correction, I get statistically significant slightly better validation accuracies when using relatively small batch sizes (32) in densely connected networks.", "body": "Currently, Bessel's correction is applied to the running variance estimator in BatchNorm on [this line](https://github.com/pytorch/pytorch/blob/master/torch/lib/THCUNN/BatchNormalization.cu#L196) and the correction is not applied at train time. While the original paper for [BatchNorm](https://arxiv.org/abs/1502.03167) agrees with this, in the follow-up paper of the author, [Batch Renormalization](https://arxiv.org/abs/1702.03275), they state on page 2 when talking about BatchNorm during training:\r\n\r\n> [...] the computation of the sample mean \u00b5_B and sample standard deviation \u03c3_B are part of the model architecture [...]\r\n\r\nand on page 3 state:\r\n\r\n> Batch Normalization, in fact, simply sets r = 1\r\n\r\nwhere r is the ratio of minibatch standard deviation and estimated standard deviation, implying that during both training and testing, Bessel's correction is applied.\r\n\r\nI think the approach for BatchNorm in Batch Renormalization makes more sense than unbiasing one variance but not the other, so I believe that PyTorch should allow an argument to BatchNorm that toggles this behaviour and perhaps consider switching the default. An argument would also bring visibility towards this implementation difference in Deep Learning frameworks.\r\n\r\nLooking through some of the most popular DL libraries, I found no implementation that uses the unbiased variance during training and inference, but some that don't unbias either. Note that the former is essentially the same as the latter when the latter initializes gamma with the correction factor, so I imagine that they should behave mostly the same even without correcting gamma.\r\nSome of the frameworks that don't unbias at all are: [Keras](https://github.com/fchollet/keras/blob/master/keras/layers/normalization.py#L182-L184), [Lasagne](https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/normalization.py#L297-L299), [TF-Slim](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L696-L697).\r\nSome of course follow the original paper by unbiasing the running variance only: [Caffe](https://github.com/BVLC/caffe/blob/master/src/caffe/layers/batch_norm_layer.cpp#L143), [Theano](http://deeplearning.net/software/theano/library/tensor/nnet/bn.html).\r\n\r\nAnecdotally, when biasing the running unbiased variances again by inverting Bessel's correction, I get statistically significant slightly better validation accuracies when using relatively small batch sizes (32) in densely connected networks."}