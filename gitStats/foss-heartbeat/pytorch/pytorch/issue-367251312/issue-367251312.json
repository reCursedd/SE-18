{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12372", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12372/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12372/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12372/events", "html_url": "https://github.com/pytorch/pytorch/issues/12372", "id": 367251312, "node_id": "MDU6SXNzdWUzNjcyNTEzMTI=", "number": 12372, "title": "Bad Inputs to GPU CrossEntropyLoss puts CUDA into a dirty state", "user": {"login": "Erotemic", "id": 3186211, "node_id": "MDQ6VXNlcjMxODYyMTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3186211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erotemic", "html_url": "https://github.com/Erotemic", "followers_url": "https://api.github.com/users/Erotemic/followers", "following_url": "https://api.github.com/users/Erotemic/following{/other_user}", "gists_url": "https://api.github.com/users/Erotemic/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erotemic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erotemic/subscriptions", "organizations_url": "https://api.github.com/users/Erotemic/orgs", "repos_url": "https://api.github.com/users/Erotemic/repos", "events_url": "https://api.github.com/users/Erotemic/events{/privacy}", "received_events_url": "https://api.github.com/users/Erotemic/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-05T15:05:49Z", "updated_at": "2018-10-05T22:16:50Z", "closed_at": "2018-10-05T22:16:50Z", "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n\n<p>On the GPU, if you pass CrossEntropyLoss a label id that is larger than the number of columns in the the network predictions, it results in an uncaught C++ error, which seems to bork the internal torch state. All further operations results in a RuntimeError. Note that the problem does not exist on the CPU side.</p>\n<p>Also, I've only tested on torch 0.4.1, its possible this was fixed in 1.0, but I haven't checked. However, I did do a quick search and AFAIK this bug has not yet been reported.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\ncriterion <span class=\"pl-k\">=</span> torch.nn.CrossEntropyLoss(<span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>none<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Setup dummy data</span>\nbsize <span class=\"pl-k\">=</span> <span class=\"pl-c1\">16</span>\nnum_classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Dummy groundtruth labels</span>\nlabels <span class=\"pl-k\">=</span> torch.randint(num_classes, (bsize,)).long()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Dummy network class probability predictions</span>\nprobs <span class=\"pl-k\">=</span> torch.rand(bsize, num_classes)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Computing the loss here works correctly</span>\nloss <span class=\"pl-k\">=</span> criterion(probs, labels)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>---------</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Expected behavior on the CPU</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>---------</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Error case, where a target class id is greater than probs.shape[0]</span>\nlabels[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> num_classes <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\nloss <span class=\"pl-k\">=</span> criterion(probs, labels)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The previous line correctly threw a Python error, but we can still recover</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> and execute more logic.</span>\nprobs <span class=\"pl-k\">+</span> <span class=\"pl-c1\">3</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>---------</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Unexpected behavior on the GPU</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>---------</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> But when we move to the GPU bad things happen</span>\nxpu <span class=\"pl-k\">=</span> torch.device(<span class=\"pl-c1\">0</span>)\nprobs <span class=\"pl-k\">=</span> probs.to(xpu)\nlabels <span class=\"pl-k\">=</span> labels.to(xpu)\n\nloss <span class=\"pl-k\">=</span> criterion(probs, labels)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> I would expect the previous line to throw a Python error, but instead we get</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> a message written to stdout:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> /pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor&lt;Dtype, 2, int, DefaultPtrTraits&gt;, THCDeviceTensor&lt;long, 1, int, DefaultPtrTraits&gt;, THCDeviceTensor&lt;Dtype, 1, int, DefaultPtrTraits&gt;, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [0,0,0] Assertion `cur_target &gt;= 0 &amp;&amp; cur_target &lt; n_classes` failed.</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> And now the device seems to be in a dirty state. Any further attempt</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> to execute code results in RuntimeErrors.</span>\nprobs <span class=\"pl-k\">+</span> <span class=\"pl-c1\">3</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMathPairwise.cu:21</span></pre></div>\n<h2>Expected behavior</h2>\n<p>The GPU version should raise: <code>RuntimeError: Target 6 out of bounds at </code></p>\n<h2>Environment</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 18.04.1 LTS<br>\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0<br>\nCMake version: version 3.12.0</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.1.85<br>\nGPU models and configuration:<br>\nGPU 0: GeForce GTX 1080 Ti<br>\nGPU 1: GeForce GTX 1080 Ti</p>\n<p>Nvidia driver version: 390.48<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip] Could not collect<br>\n[conda] Could not collect</p>", "body_text": "\ud83d\udc1b Bug\n\nOn the GPU, if you pass CrossEntropyLoss a label id that is larger than the number of columns in the the network predictions, it results in an uncaught C++ error, which seems to bork the internal torch state. All further operations results in a RuntimeError. Note that the problem does not exist on the CPU side.\nAlso, I've only tested on torch 0.4.1, its possible this was fixed in 1.0, but I haven't checked. However, I did do a quick search and AFAIK this bug has not yet been reported.\nTo Reproduce\nSteps to reproduce the behavior:\nimport torch\ncriterion = torch.nn.CrossEntropyLoss(reduction='none')\n\n# Setup dummy data\nbsize = 16\nnum_classes = 5\n# Dummy groundtruth labels\nlabels = torch.randint(num_classes, (bsize,)).long()\n# Dummy network class probability predictions\nprobs = torch.rand(bsize, num_classes)\n\n# Computing the loss here works correctly\nloss = criterion(probs, labels)\n\n#---------\n# Expected behavior on the CPU\n#---------\n# Error case, where a target class id is greater than probs.shape[0]\nlabels[0] = num_classes + 1\nloss = criterion(probs, labels)\n# The previous line correctly threw a Python error, but we can still recover\n# and execute more logic.\nprobs + 3\n\n\n#---------\n# Unexpected behavior on the GPU\n#---------\n# But when we move to the GPU bad things happen\nxpu = torch.device(0)\nprobs = probs.to(xpu)\nlabels = labels.to(xpu)\n\nloss = criterion(probs, labels)\n\n# I would expect the previous line to throw a Python error, but instead we get\n# a message written to stdout:\n#\n# /pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [0,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\n\n\n# And now the device seems to be in a dirty state. Any further attempt\n# to execute code results in RuntimeErrors.\nprobs + 3\n# RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMathPairwise.cu:21\nExpected behavior\nThe GPU version should raise: RuntimeError: Target 6 out of bounds at \nEnvironment\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 18.04.1 LTS\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\nCMake version: version 3.12.0\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration:\nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nNvidia driver version: 390.48\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\nOn the GPU, if you pass CrossEntropyLoss a label id that is larger than the number of columns in the the network predictions, it results in an uncaught C++ error, which seems to bork the internal torch state. All further operations results in a RuntimeError. Note that the problem does not exist on the CPU side. \r\n\r\nAlso, I've only tested on torch 0.4.1, its possible this was fixed in 1.0, but I haven't checked. However, I did do a quick search and AFAIK this bug has not yet been reported. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```python\r\nimport torch\r\ncriterion = torch.nn.CrossEntropyLoss(reduction='none')\r\n\r\n# Setup dummy data\r\nbsize = 16\r\nnum_classes = 5\r\n# Dummy groundtruth labels\r\nlabels = torch.randint(num_classes, (bsize,)).long()\r\n# Dummy network class probability predictions\r\nprobs = torch.rand(bsize, num_classes)\r\n\r\n# Computing the loss here works correctly\r\nloss = criterion(probs, labels)\r\n\r\n#---------\r\n# Expected behavior on the CPU\r\n#---------\r\n# Error case, where a target class id is greater than probs.shape[0]\r\nlabels[0] = num_classes + 1\r\nloss = criterion(probs, labels)\r\n# The previous line correctly threw a Python error, but we can still recover\r\n# and execute more logic.\r\nprobs + 3\r\n\r\n\r\n#---------\r\n# Unexpected behavior on the GPU\r\n#---------\r\n# But when we move to the GPU bad things happen\r\nxpu = torch.device(0)\r\nprobs = probs.to(xpu)\r\nlabels = labels.to(xpu)\r\n\r\nloss = criterion(probs, labels)\r\n\r\n# I would expect the previous line to throw a Python error, but instead we get\r\n# a message written to stdout:\r\n#\r\n# /pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:56: void ClassNLLCriterion_updateOutput_no_reduce_kernel(int, THCDeviceTensor<Dtype, 2, int, DefaultPtrTraits>, THCDeviceTensor<long, 1, int, DefaultPtrTraits>, THCDeviceTensor<Dtype, 1, int, DefaultPtrTraits>, Dtype *, int, int) [with Dtype = float]: block: [0,0,0], thread: [0,0,0] Assertion `cur_target >= 0 && cur_target < n_classes` failed.\r\n\r\n\r\n# And now the device seems to be in a dirty state. Any further attempt\r\n# to execute code results in RuntimeErrors.\r\nprobs + 3\r\n# RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMathPairwise.cu:21\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe GPU version should raise: `RuntimeError: Target 6 out of bounds at `\r\n\r\n## Environment\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 18.04.1 LTS\r\nGCC version: (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 390.48\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n"}