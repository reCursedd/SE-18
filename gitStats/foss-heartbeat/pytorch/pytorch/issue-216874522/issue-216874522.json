{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1088", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1088/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1088/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1088/events", "html_url": "https://github.com/pytorch/pytorch/issues/1088", "id": 216874522, "node_id": "MDU6SXNzdWUyMTY4NzQ1MjI=", "number": 1088, "title": "GPU slower than CPU on a simple RNN test code", "user": {"login": "aleingrosso", "id": 15464896, "node_id": "MDQ6VXNlcjE1NDY0ODk2", "avatar_url": "https://avatars0.githubusercontent.com/u/15464896?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aleingrosso", "html_url": "https://github.com/aleingrosso", "followers_url": "https://api.github.com/users/aleingrosso/followers", "following_url": "https://api.github.com/users/aleingrosso/following{/other_user}", "gists_url": "https://api.github.com/users/aleingrosso/gists{/gist_id}", "starred_url": "https://api.github.com/users/aleingrosso/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aleingrosso/subscriptions", "organizations_url": "https://api.github.com/users/aleingrosso/orgs", "repos_url": "https://api.github.com/users/aleingrosso/repos", "events_url": "https://api.github.com/users/aleingrosso/events{/privacy}", "received_events_url": "https://api.github.com/users/aleingrosso/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-24T18:48:29Z", "updated_at": "2017-10-09T17:16:17Z", "closed_at": "2017-03-24T19:03:19Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I wanted to write an RNN from scratch using the pytorch cuda capabilities and I ran some preliminary tests to compare the speed of the CPU vs GPU. The task is very simple and consists of a for loop mimicking the update of the internal state x in an RNN with recurrent weight matrix J. I'm using a Quadro K620 with cuda 8.0.</p>\n<p>When the size of x is N=1000 there seems to be a trade-off, with the GPU implementation consistently getting slower when the number of iterations increases (I ran some other tests with different sizes of the J matrix and this behaviour seems pretty systematic).</p>\n<p>This is an example of running times I get when running the enclosed script (number of iterations are 100, 1000, 10000, 100000):</p>\n<p>cpu: [0.010117292404174805, 0.058980703353881836, 0.45785975456237793, 4.512230634689331]<br>\ngpu: [0.0019445419311523438, 0.05474495887756348, 0.7503962516784668, 7.011191129684448]</p>\n<p>I'd really appreciate some help on this. Thanks in advance.</p>\n<p>The test script is the following:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch <span class=\"pl-k\">as</span> tr\n<span class=\"pl-k\">import</span> math\n<span class=\"pl-k\">import</span> time\n\n<span class=\"pl-c1\">GPUID</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\ntr.cuda.set_device(<span class=\"pl-c1\">GPUID</span>)\n\nN <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n\nJ <span class=\"pl-k\">=</span> tr.randn(N,N)\nx <span class=\"pl-k\">=</span> tr.randn(N)\nr <span class=\"pl-k\">=</span> tr.randn(N)\n\nJn <span class=\"pl-k\">=</span> J.numpy()\nxn <span class=\"pl-k\">=</span> x.numpy()\nrn <span class=\"pl-k\">=</span> r.numpy()\n\ncputimes <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> sampl <span class=\"pl-k\">in</span> (<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">100000</span>):\n    start <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(sampl):\n        rn <span class=\"pl-k\">=</span> np.tanh(xn)\n        xn <span class=\"pl-k\">=</span> Jn.dot(xn)<span class=\"pl-bu\">;</span>\n    end <span class=\"pl-k\">=</span> time.time()\n    cputimes.append(end<span class=\"pl-k\">-</span>start)\n<span class=\"pl-c1\">print</span>(cputimes)\n\nJc <span class=\"pl-k\">=</span> J.cuda()\nxc <span class=\"pl-k\">=</span> x.cuda()\nrc <span class=\"pl-k\">=</span> r.cuda()\n\ngputimes <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> sampl <span class=\"pl-k\">in</span> (<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">100000</span>):\n    start <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(sampl):\n        rc <span class=\"pl-k\">=</span> tr.tanh(xc)\n        xc <span class=\"pl-k\">=</span> Jc.mv(xc)<span class=\"pl-bu\">;</span>\n    end <span class=\"pl-k\">=</span> time.time()\n    gputimes.append(end<span class=\"pl-k\">-</span>start)\n<span class=\"pl-c1\">print</span>(gputimes)</pre></div>", "body_text": "Hi,\nI wanted to write an RNN from scratch using the pytorch cuda capabilities and I ran some preliminary tests to compare the speed of the CPU vs GPU. The task is very simple and consists of a for loop mimicking the update of the internal state x in an RNN with recurrent weight matrix J. I'm using a Quadro K620 with cuda 8.0.\nWhen the size of x is N=1000 there seems to be a trade-off, with the GPU implementation consistently getting slower when the number of iterations increases (I ran some other tests with different sizes of the J matrix and this behaviour seems pretty systematic).\nThis is an example of running times I get when running the enclosed script (number of iterations are 100, 1000, 10000, 100000):\ncpu: [0.010117292404174805, 0.058980703353881836, 0.45785975456237793, 4.512230634689331]\ngpu: [0.0019445419311523438, 0.05474495887756348, 0.7503962516784668, 7.011191129684448]\nI'd really appreciate some help on this. Thanks in advance.\nThe test script is the following:\nimport numpy as np\nimport torch as tr\nimport math\nimport time\n\nGPUID = 0\ntr.cuda.set_device(GPUID)\n\nN = 1000\n\nJ = tr.randn(N,N)\nx = tr.randn(N)\nr = tr.randn(N)\n\nJn = J.numpy()\nxn = x.numpy()\nrn = r.numpy()\n\ncputimes = []\nfor sampl in (100, 1000, 10000, 100000):\n    start = time.time()\n    for i in xrange(sampl):\n        rn = np.tanh(xn)\n        xn = Jn.dot(xn);\n    end = time.time()\n    cputimes.append(end-start)\nprint(cputimes)\n\nJc = J.cuda()\nxc = x.cuda()\nrc = r.cuda()\n\ngputimes = []\nfor sampl in (100, 1000, 10000, 100000):\n    start = time.time()\n    for i in xrange(sampl):\n        rc = tr.tanh(xc)\n        xc = Jc.mv(xc);\n    end = time.time()\n    gputimes.append(end-start)\nprint(gputimes)", "body": "Hi,\r\n\r\nI wanted to write an RNN from scratch using the pytorch cuda capabilities and I ran some preliminary tests to compare the speed of the CPU vs GPU. The task is very simple and consists of a for loop mimicking the update of the internal state x in an RNN with recurrent weight matrix J. I'm using a Quadro K620 with cuda 8.0.\r\n\r\nWhen the size of x is N=1000 there seems to be a trade-off, with the GPU implementation consistently getting slower when the number of iterations increases (I ran some other tests with different sizes of the J matrix and this behaviour seems pretty systematic).\r\n\r\nThis is an example of running times I get when running the enclosed script (number of iterations are 100, 1000, 10000, 100000):\r\n\r\ncpu: [0.010117292404174805, 0.058980703353881836, 0.45785975456237793, 4.512230634689331]\r\ngpu: [0.0019445419311523438, 0.05474495887756348, 0.7503962516784668, 7.011191129684448]\r\n\r\nI'd really appreciate some help on this. Thanks in advance.\r\n\r\nThe test script is the following:\r\n\r\n```python\r\nimport numpy as np\r\nimport torch as tr\r\nimport math\r\nimport time\r\n\r\nGPUID = 0\r\ntr.cuda.set_device(GPUID)\r\n\r\nN = 1000\r\n\r\nJ = tr.randn(N,N)\r\nx = tr.randn(N)\r\nr = tr.randn(N)\r\n\r\nJn = J.numpy()\r\nxn = x.numpy()\r\nrn = r.numpy()\r\n\r\ncputimes = []\r\nfor sampl in (100, 1000, 10000, 100000):\r\n    start = time.time()\r\n    for i in xrange(sampl):\r\n        rn = np.tanh(xn)\r\n        xn = Jn.dot(xn);\r\n    end = time.time()\r\n    cputimes.append(end-start)\r\nprint(cputimes)\r\n\r\nJc = J.cuda()\r\nxc = x.cuda()\r\nrc = r.cuda()\r\n\r\ngputimes = []\r\nfor sampl in (100, 1000, 10000, 100000):\r\n    start = time.time()\r\n    for i in xrange(sampl):\r\n        rc = tr.tanh(xc)\r\n        xc = Jc.mv(xc);\r\n    end = time.time()\r\n    gputimes.append(end-start)\r\nprint(gputimes)\r\n```"}