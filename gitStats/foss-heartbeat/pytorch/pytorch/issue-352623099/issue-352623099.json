{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10732", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10732/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10732/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10732/events", "html_url": "https://github.com/pytorch/pytorch/issues/10732", "id": 352623099, "node_id": "MDU6SXNzdWUzNTI2MjMwOTk=", "number": 10732, "title": "bceloss weight shape incorrect", "user": {"login": "nkundiushuti", "id": 1208171, "node_id": "MDQ6VXNlcjEyMDgxNzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1208171?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nkundiushuti", "html_url": "https://github.com/nkundiushuti", "followers_url": "https://api.github.com/users/nkundiushuti/followers", "following_url": "https://api.github.com/users/nkundiushuti/following{/other_user}", "gists_url": "https://api.github.com/users/nkundiushuti/gists{/gist_id}", "starred_url": "https://api.github.com/users/nkundiushuti/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nkundiushuti/subscriptions", "organizations_url": "https://api.github.com/users/nkundiushuti/orgs", "repos_url": "https://api.github.com/users/nkundiushuti/repos", "events_url": "https://api.github.com/users/nkundiushuti/events{/privacy}", "received_events_url": "https://api.github.com/users/nkundiushuti/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-21T16:41:47Z", "updated_at": "2018-08-23T15:25:59Z", "closed_at": "2018-08-21T18:34:51Z", "author_association": "NONE", "body_html": "<p>If you have a question or would like help and support, please ask at our<br>\n<a href=\"https://discuss.pytorch.org/\" rel=\"nofollow\">forums</a>.</p>\n<p>If you are submitting a feature request, please preface the title with [feature request].<br>\nIf you are submitting a bug report, please fill in the following details.</p>\n<h2>Issue description</h2>\n<p>I have an error with BCELoss when using weights with batch size 64:</p>\n<p>loss = self.criterion.forward(y_hat, y_batch)<br>\nFile \"/home/NET1/mironma/anaconda3/envs/fatml/lib/python3.6/site-packages/torch/nn/modules/loss.py\", line 486, in forward<br>\nreturn F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)<br>\nFile \"/home/NET1/mironma/anaconda3/envs/fatml/lib/python3.6/site-packages/torch/nn/functional.py\", line 1603, in binary_cross_entropy<br>\nreturn torch._C._nn.binary_cross_entropy(input, target, weight, reduction)<br>\nRuntimeError: input and weights have different number of elements: input[64 x 1] has 64 elements, while weights[64 x 64] has 4096 elements at /opt/conda/conda-bld/pytorch_1532579245307/work/aten/src/THCUNN/generic/BCECriterion.cu:14</p>\n<h2>Code example</h2>\n<pre><code>def train_epoch(self,x,y,w=None):\n    self.model.train()\n    r = np.arange(x.size(0))\n    np.random.shuffle(r)\n    r = torch.LongTensor(r).cuda()\n\n    for i in range(0, len(r), self.sbatch):\n        if i+self.sbatch&lt;=len(r): b=r[i:i+self.sbatch]\n        else: b=r[i:]\n        x_batch = torch.autograd.Variable(x[b])\n        y_batch = torch.autograd.Variable(y[b])\n        if w is not None:\n            w_batch = torch.autograd.Variable(w[b])\n        #printing w_batch.data.size() gives torch.Size([64])\n\n        self.optimizer.zero_grad()\n        # (1) Forward\n        y_hat = self.model.forward(x_batch)\n        # (2) Compute diff\n        if w is not None:\n            self.criterion.weight = w_batch\n            loss = self.criterion.forward(y_hat, y_batch)\n        else:\n            loss = self.criterion(y_hat, y_batch)\n        # (3) Compute gradients\n        loss.backward()\n        # (4) update weights\n        #torch.nn.utils.clip_grad_norm(self.model.parameters(), self.clipgrad)\n        self.optimizer.step()\n\n    return\n</code></pre>\n<h2>System Info</h2>\n<p>Please copy and paste the output from our<br>\n<a href=\"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\" rel=\"nofollow\">environment collection script</a><br>\n(or fill out the checklist below manually).</p>\n<p>You can get the script and run it with:</p>\n<pre><code>wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n</code></pre>\n<ul>\n<li>PyTorch or Caffe2:</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS:</li>\n<li>PyTorch version:</li>\n<li>Python version:</li>\n<li>CUDA/cuDNN version:</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version:</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "If you have a question or would like help and support, please ask at our\nforums.\nIf you are submitting a feature request, please preface the title with [feature request].\nIf you are submitting a bug report, please fill in the following details.\nIssue description\nI have an error with BCELoss when using weights with batch size 64:\nloss = self.criterion.forward(y_hat, y_batch)\nFile \"/home/NET1/mironma/anaconda3/envs/fatml/lib/python3.6/site-packages/torch/nn/modules/loss.py\", line 486, in forward\nreturn F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\nFile \"/home/NET1/mironma/anaconda3/envs/fatml/lib/python3.6/site-packages/torch/nn/functional.py\", line 1603, in binary_cross_entropy\nreturn torch._C._nn.binary_cross_entropy(input, target, weight, reduction)\nRuntimeError: input and weights have different number of elements: input[64 x 1] has 64 elements, while weights[64 x 64] has 4096 elements at /opt/conda/conda-bld/pytorch_1532579245307/work/aten/src/THCUNN/generic/BCECriterion.cu:14\nCode example\ndef train_epoch(self,x,y,w=None):\n    self.model.train()\n    r = np.arange(x.size(0))\n    np.random.shuffle(r)\n    r = torch.LongTensor(r).cuda()\n\n    for i in range(0, len(r), self.sbatch):\n        if i+self.sbatch<=len(r): b=r[i:i+self.sbatch]\n        else: b=r[i:]\n        x_batch = torch.autograd.Variable(x[b])\n        y_batch = torch.autograd.Variable(y[b])\n        if w is not None:\n            w_batch = torch.autograd.Variable(w[b])\n        #printing w_batch.data.size() gives torch.Size([64])\n\n        self.optimizer.zero_grad()\n        # (1) Forward\n        y_hat = self.model.forward(x_batch)\n        # (2) Compute diff\n        if w is not None:\n            self.criterion.weight = w_batch\n            loss = self.criterion.forward(y_hat, y_batch)\n        else:\n            loss = self.criterion(y_hat, y_batch)\n        # (3) Compute gradients\n        loss.backward()\n        # (4) update weights\n        #torch.nn.utils.clip_grad_norm(self.model.parameters(), self.clipgrad)\n        self.optimizer.step()\n\n    return\n\nSystem Info\nPlease copy and paste the output from our\nenvironment collection script\n(or fill out the checklist below manually).\nYou can get the script and run it with:\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n\n\nPyTorch or Caffe2:\nHow you installed PyTorch (conda, pip, source): conda\nBuild command you used (if compiling from source):\nOS:\nPyTorch version:\nPython version:\nCUDA/cuDNN version:\nGPU models and configuration:\nGCC version (if compiling from source):\nCMake version:\nVersions of any other relevant libraries:", "body": "If you have a question or would like help and support, please ask at our\r\n[forums](https://discuss.pytorch.org/).\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\nIf you are submitting a bug report, please fill in the following details.\r\n\r\n## Issue description\r\n\r\nI have an error with BCELoss when using weights with batch size 64:\r\n\r\nloss = self.criterion.forward(y_hat, y_batch)\r\n  File \"/home/NET1/mironma/anaconda3/envs/fatml/lib/python3.6/site-packages/torch/nn/modules/loss.py\", line 486, in forward\r\n    return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\r\n  File \"/home/NET1/mironma/anaconda3/envs/fatml/lib/python3.6/site-packages/torch/nn/functional.py\", line 1603, in binary_cross_entropy\r\n    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction)\r\nRuntimeError: input and weights have different number of elements: input[64 x 1] has 64 elements, while weights[64 x 64] has 4096 elements at /opt/conda/conda-bld/pytorch_1532579245307/work/aten/src/THCUNN/generic/BCECriterion.cu:14\r\n\r\n\r\n## Code example\r\n    def train_epoch(self,x,y,w=None):\r\n        self.model.train()\r\n        r = np.arange(x.size(0))\r\n        np.random.shuffle(r)\r\n        r = torch.LongTensor(r).cuda()\r\n\r\n        for i in range(0, len(r), self.sbatch):\r\n            if i+self.sbatch<=len(r): b=r[i:i+self.sbatch]\r\n            else: b=r[i:]\r\n            x_batch = torch.autograd.Variable(x[b])\r\n            y_batch = torch.autograd.Variable(y[b])\r\n            if w is not None:\r\n                w_batch = torch.autograd.Variable(w[b])\r\n            #printing w_batch.data.size() gives torch.Size([64])\r\n\r\n            self.optimizer.zero_grad()\r\n            # (1) Forward\r\n            y_hat = self.model.forward(x_batch)\r\n            # (2) Compute diff\r\n            if w is not None:\r\n                self.criterion.weight = w_batch\r\n                loss = self.criterion.forward(y_hat, y_batch)\r\n            else:\r\n                loss = self.criterion(y_hat, y_batch)\r\n            # (3) Compute gradients\r\n            loss.backward()\r\n            # (4) update weights\r\n            #torch.nn.utils.clip_grad_norm(self.model.parameters(), self.clipgrad)\r\n            self.optimizer.step()\r\n\r\n        return\r\n\r\n\r\n## System Info\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n- PyTorch or Caffe2:\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Build command you used (if compiling from source): \r\n- OS: \r\n- PyTorch version:\r\n- Python version:\r\n- CUDA/cuDNN version:\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n"}