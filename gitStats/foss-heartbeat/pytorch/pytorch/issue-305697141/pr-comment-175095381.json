{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175095381", "pull_request_review_id": 104572217, "id": 175095381, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTA5NTM4MQ==", "diff_hunk": "@@ -3,146 +3,252 @@\n #include <stdint.h>\n #include <string>\n #include <unordered_map>\n+#include <algorithm>\n \n #include \"torch/csrc/jit/generated/aten_interned_strings.h\"\n \n namespace torch { namespace jit {\n \n-// JIT symbols are synthetic operators that occur only in the JIT IR\n+// Every symbol is classified in a namespace, specifying what kind of symbol it\n+// is.  Unsigned char to ensure widening to unique_t (also an unsigned type)\n+enum class SymbolNamespace : unsigned char {\n+  onnx  = 'o',\n+  prim  = 'p',\n+  aten  = 't',\n+  // NB: ONNX and ATen attributes all live in a global namespace, as\n+  // their interpretation depends on the operator name (which is namespaced)\n+  attr  = 'a',\n+  // TODO: eliminate me\n+  scope = 's'\n+};\n+\n+namespace detail {\n+\n+  // TODO: programatically generate this\n+  inline std::string valid_namespaces_str() {\n+    return \"'onnx', 'prim', 'aten', 'attr', 'scope'\";\n+  }\n+\n+}\n+\n+// Primitive symbols are synthetic operators that occur only in the IR\n // and don't have corresponding implementations in ATen.\n //\n // TODO: We need documentation for all of these symbols.\n+//\n+// TODO: Consider moving the synthetic onnx operators to their own\n+// namespace.\n \n-#define FORALL_JIT_SYMBOLS(_) \\\n-_(Assign) \\\n-_(Constant) \\\n-_(CppOp) \\\n-_(Drop) \\\n-_(Eval) \\\n-_(Expand) \\\n-_(FusionGroup) \\\n-_(GraphExecutor) \\\n-_(If) \\\n-_(Jump) \\\n-_(JumpNZ) \\\n-_(JumpZ) \\\n-_(Load) \\\n-_(Loop) \\\n-_(Param) \\\n-_(Placeholder) \\\n-_(Print) \\\n-_(PythonOp) \\\n-_(ReplaceIfUndef) \\\n-_(Reverse) \\\n-_(Return) \\\n-_(Store) \\\n-_(Undefined) \\\n-_(__JIT_END)\n+#define FORALL_PRIM_SYMBOLS(_) \\\n+_(prim, Assign) \\\n+_(prim, Constant) \\\n+_(prim, CppOp) \\\n+_(prim, Drop) \\\n+_(prim, Eval) \\\n+_(prim, Expand) /* onnx */ \\\n+_(prim, FusionGroup) \\\n+_(prim, GraphExecutor) \\\n+_(prim, If) \\\n+_(prim, Jump) /* debug */ \\\n+_(prim, JumpNZ) /* debug */ \\\n+_(prim, JumpZ) /* debug */ \\\n+_(prim, Load) \\\n+_(prim, Loop) \\\n+_(prim, Param) \\\n+_(prim, PackPadded) /* onnx */ \\\n+_(prim, PadPacked) /* onnx */ \\\n+_(prim, Placeholder) /* debug */ \\\n+_(prim, Print) \\\n+_(prim, PythonOp) \\\n+_(prim, ReplaceIfUndef) \\\n+_(prim, Reverse) \\\n+_(prim, Return) \\\n+_(prim, Store) \\\n+_(prim, Undefined) \\\n+/* end */\n \n // Workaround for some not-yet-defined ATen symbols, see\n //  - __not__: https://github.com/pytorch/pytorch/issues/5495\n //  - ones, zeros: https://github.com/pytorch/pytorch/issues/5496\n \n #define FORALL_ATEN_EXTRA_SYMBOLS(_) \\\n-_(__not__) \\\n-_(ones) \\\n-_(zeros) \\\n-_(__ATEN_EXTRA_END)\n+_(aten, __not__) \\\n+/* end */\n+\n+#define FORALL_ATEN_SYMBOLS(_) \\\n+FORALL_ATEN_BASE_SYMBOLS(_) \\\n+FORALL_ATEN_EXTRA_SYMBOLS(_)\n \n // These symbols correspond to ONNX operators.  Their semantics\n // are defined in https://github.com/onnx/onnx/blob/master/docs/Operators.md\n // The particular version we are targeting is specified by '_onnx_opset_version'\n // in torch.onnx.symbolic\n+//\n+// In general, most ONNX operators won't get an entry here, because they\n+// are handled from the Python end.  However, you may occasionally need\n+// to intern an ONNX symbol here so that you can conveniently write an\n+// optimization on ONNX operations.\n \n #define FORALL_ONNX_SYMBOLS(_) \\\n-_(Add) \\\n-/* _(Constant) conflicts with JIT */ \\\n-_(Div) \\\n-_(GRU) \\\n-_(Gemm) \\\n-_(LSTM) \\\n-_(Mul) \\\n-_(PackPadded) \\\n-_(PadPacked) \\\n-_(Pow) \\\n-_(RNN) \\\n-_(Slice) /* used by test only */ \\\n-_(Sub) \\\n-_(Transpose) \\\n-_(__ONNX_END)\n+_(onnx, Add) \\\n+_(onnx, Constant) \\\n+_(onnx, Div) \\\n+_(onnx, GRU) \\\n+_(onnx, Gemm) \\\n+_(onnx, LSTM) \\\n+_(onnx, Mul) \\\n+_(onnx, Pow) \\\n+_(onnx, RNN) \\\n+_(onnx, Sub) \\\n+_(onnx, Transpose) \\\n+/* end */\n \n // These symbols are attribute keys.  They are shared between both ONNX and ATen\n-// operators (you disambiguate their meaning by looking at the operator itself)\n+// operators (you disambiguate their meaning by looking at the operator itself).\n+// In general, you only need to define attribute keys that are used by\n+// onnx or prim; ATen attributes are automatically generated in FORALL_ATTR_BASE_SYMBOLS.\n+\n+#define FORALL_ATTR_EXTRA_SYMBOLS(_) \\\n+_(attr, Subgraph) \\\n+_(attr, axis) \\\n+_(attr, broadcast) \\\n+_(attr, device) \\\n+_(attr, inplace) \\\n+_(attr, is_zero) \\\n+_(attr, perm) \\\n+_(attr, sizes) \\\n+_(attr, transA) \\\n+_(attr, transB) \\\n+/* end */\n \n #define FORALL_ATTR_SYMBOLS(_) \\\n-_(Subgraph) \\\n-_(alpha) \\\n-_(axis) \\\n-_(broadcast) \\\n-_(device) \\\n-/* _(dim) conflicts with ATen */ \\\n-_(end) \\\n-_(exponent) \\\n-_(inplace) \\\n-_(is_zero) \\\n-_(keepdim) \\\n-_(length) \\\n-_(other) \\\n-_(perm) \\\n-/* _(size) conflicts with ATen */ \\\n-/* _(sizes) conflicts with ATen */ \\\n-_(start) \\\n-_(step) \\\n-_(transA) \\\n-_(transB) \\\n-_(value) \\\n-_(__ATTR_END)\n+FORALL_ATTR_BASE_SYMBOLS(_) \\\n+FORALL_ATTR_EXTRA_SYMBOLS(_)\n \n #define FORALL_BUILTIN_SYMBOLS(_) \\\n-FORALL_JIT_SYMBOLS(_) \\\n-FORALL_ATEN_SYMBOLS(_) \\\n-FORALL_ATEN_EXTRA_SYMBOLS(_) \\\n-FORALL_ONNX_SYMBOLS(_) \\\n-FORALL_ATTR_SYMBOLS(_) \\\n-_(__BUILTIN_END)\n-\n-  enum BuiltinSymbol {\n-    #define DEFINE_SYMBOL(s) \\\n-      k##s,\n-    FORALL_BUILTIN_SYMBOLS(DEFINE_SYMBOL)\n-    #undef DEFINE_SYMBOL\n-    kLastSymbol, //where we start counting for new symbols\n-  };\n+  FORALL_ONNX_SYMBOLS(_) \\\n+  FORALL_ATEN_SYMBOLS(_) \\\n+  FORALL_ATTR_SYMBOLS(_) \\\n+  FORALL_PRIM_SYMBOLS(_) \\\n+  /* end */\n+\n+// Note [Symbol allocation]\n+// ~~~~~~~~~~~~~~~~~~~~~~~~\n+//\n+//  1. Symbol namespace is split up into namespaces.  The byte structure\n+//  of our symbols is TUUUUUUU, where T is the tag byte and U are the unique", "path": "torch/csrc/jit/interned_strings.h", "position": null, "original_position": 219, "commit_id": "80b8fd35ed644bbd95c4aa4f380aea8263c6208c", "original_commit_id": "59eebb6b3fa9c515d7177c23b7064cceef535985", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Oops, I can't math. That means we only have 16777216 uniques.", "created_at": "2018-03-16T13:52:16Z", "updated_at": "2018-11-23T15:40:51Z", "html_url": "https://github.com/pytorch/pytorch/pull/5820#discussion_r175095381", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5820", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175095381"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5820#discussion_r175095381"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5820"}}, "body_html": "<p>Oops, I can't math. That means we only have 16777216 uniques.</p>", "body_text": "Oops, I can't math. That means we only have 16777216 uniques.", "in_reply_to_id": 175058707}