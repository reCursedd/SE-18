{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/226405858", "pull_request_review_id": 166220977, "id": 226405858, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNjQwNTg1OA==", "diff_hunk": "@@ -0,0 +1,209 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/CPUApplyUtils.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/ExpandUtils.h\"\n+#include \"ATen/NativeFunctions.h\"\n+\n+#include \"ATen/native/LinearAlgebraUtils.h\"\n+\n+#include \"TH.h\"  // for USE_LAPACK\n+\n+#include <vector>\n+\n+// First the required LAPACK implementations are registered here.\n+// A comment above the registered LAPACK routine suggest which batched\n+// linear algebra function uses that routine\n+#ifdef USE_LAPACK\n+\n+// gesv\n+extern \"C\" void dgesv_(int* n, int* nrhs, double* a, int* lda, int *ipiv, double* b, int* ldb, int* info);\n+extern \"C\" void sgesv_(int* n, int* nrhs, float* a, int* lda, int* ipiv, float* b, int* ldb, int* info);\n+\n+// inverse\n+extern \"C\" void dgetrf_(int *m, int *n, double *a, int *lda, int *ipiv, int *info);\n+extern \"C\" void sgetrf_(int *m, int *n, float *a, int *lda, int *ipiv, int *info);\n+extern \"C\" void dgetri_(int *n, double *a, int *lda, int *ipiv, double *work, int *lwork, int *info);\n+extern \"C\" void sgetri_(int *n, float *a, int *lda, int *ipiv, float *work, int *lwork, int *info);\n+#endif\n+\n+namespace at {\n+namespace native {\n+\n+// Define the per-batch functions to be used in the main implementation of the batched\n+// linear algebra operations\n+template<class scalar_t>\n+void lapackGesv(int n, int nrhs, scalar_t* a, int lda, int* ipiv, scalar_t* b, int ldb, int* info) {\n+  AT_ERROR(\"gesv only takes float or double Tensors\");\n+}\n+\n+template<class scalar_t>\n+void lapackGetrf(int m, int n, scalar_t* a, int lda, int *ipiv, int *info) {\n+  AT_ERROR(\"getrf only takes float or double Tensors\");\n+}\n+\n+template<class scalar_t>\n+void lapackGetri(int n, scalar_t *a, int lda, int *ipiv, scalar_t *work, int lwork, int *info) {\n+  AT_ERROR(\"getri only takes float or double Tensors\");\n+}\n+\n+#ifdef USE_LAPACK\n+template<> void lapackGesv<double>(int n, int nrhs, double* a, int lda, int* ipiv, double* b, int ldb, int* info) {\n+  dgesv_(&n, &nrhs, a, &lda, ipiv, b, &ldb, info);\n+}\n+\n+template<> void lapackGesv<float>(int n, int nrhs, float* a, int lda, int* ipiv, float* b, int ldb, int* info) {\n+  sgesv_(&n, &nrhs, a, &lda, ipiv, b, &ldb, info);\n+}\n+\n+template<> void lapackGetri<double>(int n, double *a, int lda, int *ipiv, double *work, int lwork, int *info) {\n+  dgetri_(&n, a, &lda, ipiv, work, &lwork, info);\n+}\n+\n+template<> void lapackGetri<float>(int n, float *a, int lda, int *ipiv, float *work, int lwork, int *info) {\n+  sgetri_(&n, a, &lda, ipiv, work, &lwork, info);\n+}\n+\n+template<> void lapackGetrf<double>(int m, int n, double *a, int lda, int *ipiv, int *info) {\n+  dgetrf_(&m, &n, a, &lda, ipiv, info);\n+}\n+\n+template<> void lapackGetrf<float>(int m, int n, float *a, int lda, int *ipiv, int *info) {\n+  sgetrf_(&m, &n, a, &lda, ipiv, info);\n+}\n+#endif\n+\n+// Below of the definitions of the functions operating on a batch that are going to be dispatched\n+// in the main helper functions for the linear algebra operations\n+\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ gesv ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+template<typename scalar_t>\n+static void apply_gesv(Tensor& b, Tensor& A, std::vector<int64_t>& infos) {\n+#ifndef USE_LAPACK\n+  AT_ERROR(\"gesv: LAPACK library not found in compilation\");\n+#endif\n+  auto A_data = A.data<scalar_t>();\n+  auto b_data = b.data<scalar_t>();\n+  auto A_mat_stride = matrixStride(A);\n+  auto b_mat_stride = matrixStride(b);\n+\n+  auto batch_size = batchCount(A);\n+  auto n = A.size(-2);\n+  auto nrhs = b.size(-1);\n+\n+  auto ipiv = at::empty({n}, b.type().toScalarType(kInt));\n+\n+  for (int64_t i = 0; i < batch_size; i++) {\n+    int info;\n+    scalar_t* A_working_ptr = &A_data[i * A_mat_stride];\n+    scalar_t* b_working_ptr = &b_data[i * b_mat_stride];\n+    lapackGesv<scalar_t>(n, nrhs, A_working_ptr, n, ipiv.data<int>(), b_working_ptr, n, &info);\n+    infos[i] = info;\n+    if (info != 0) {\n+      return;\n+    }\n+  }\n+}\n+\n+// These utilities are specified in LinearAlgebraUtils.h\n+LINALG_HELPER_2_ARGS(gesv, self, A, cpu)\n+\n+// Supports arbitrary batch dimensions for self and A\n+std::tuple<Tensor,Tensor> gesv(const Tensor& self, const Tensor& A) {\n+  if (self.dim() <= 2 && A.dim() <= 2) {\n+    // TODO: #7102: It's not necessary to have gesv (single) bindings for both\n+    // TH and ATen. We should remove the TH gesv bindings, especially\n+    // since the lapackGesv function is already in ATen.\n+    return at::_gesv_single(self, A);\n+  }\n+\n+  gesvCheckInputs(self, A);\n+\n+  // broadcast the batch dimensions of self and A.\n+  IntList self_batch_sizes(self.sizes().data(), self.ndimension() - 2);\n+  IntList A_batch_sizes(A.sizes().data(), A.ndimension() - 2);\n+  std::vector<int64_t> expand_batch_portion = infer_size(self_batch_sizes, A_batch_sizes);\n+\n+  std::vector<int64_t> self_expand_size({expand_batch_portion});\n+  self_expand_size.insert(self_expand_size.end(), { self.size(-2), self.size(-1) });\n+\n+  std::vector<int64_t> A_expand_size({expand_batch_portion});\n+  A_expand_size.insert(A_expand_size.end(), { A.size(-2), A.size(-1) });\n+\n+  Tensor self_broadcasted  = self.expand(self_expand_size);\n+  Tensor A_broadcasted = A.expand(A_expand_size);\n+  return at::_gesv_helper(self_broadcasted, A_broadcasted);\n+}\n+\n+std::tuple<Tensor&,Tensor&> gesv_out(Tensor& solution, Tensor& lu, const Tensor& self, const Tensor& A) {\n+  AT_CHECK(self.dim() == 2 && A.dim() == 2, \n+           \"torch.gesv() with the `out` keyword does not support batching. \"\n+           \"b.dim() (\", self.dim(), \") and A.dim() (\", A.dim(), \") must both be 2.\");\n+  return at::_gesv_single_out(solution, lu, self, A);\n+}\n+\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ inverse ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+template <typename scalar_t>\n+static void apply_inverse(Tensor& self, std::vector<int64_t>& infos) {\n+#ifndef USE_LAPACK\n+  AT_ERROR(\"inverse: LAPACK library not found in compilation\");\n+#endif\n+  auto self_data = self.data<scalar_t>();\n+  auto self_matrix_stride = matrixStride(self);", "path": "aten/src/ATen/native/BatchLinearAlgebra.cpp", "position": 153, "original_position": 153, "commit_id": "8cc65045618a15a741a33ac31a59edbbc7279290", "original_commit_id": "3cb4485890fdff543d8abf3b049bbe671e6fbaed", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "Maybe add an assert that self has memory in column major form? (that is the case, right?)", "created_at": "2018-10-18T17:54:58Z", "updated_at": "2018-11-23T15:53:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/9949#discussion_r226405858", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9949", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/226405858"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9949#discussion_r226405858"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9949"}}, "body_html": "<p>Maybe add an assert that self has memory in column major form? (that is the case, right?)</p>", "body_text": "Maybe add an assert that self has memory in column major form? (that is the case, right?)"}