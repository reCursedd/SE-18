{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1817", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1817/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1817/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1817/events", "html_url": "https://github.com/pytorch/pytorch/issues/1817", "id": 236345155, "node_id": "MDU6SXNzdWUyMzYzNDUxNTU=", "number": 1817, "title": "option for \"infinity-math\"", "user": {"login": "jxwuyi", "id": 5471293, "node_id": "MDQ6VXNlcjU0NzEyOTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/5471293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jxwuyi", "html_url": "https://github.com/jxwuyi", "followers_url": "https://api.github.com/users/jxwuyi/followers", "following_url": "https://api.github.com/users/jxwuyi/following{/other_user}", "gists_url": "https://api.github.com/users/jxwuyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jxwuyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jxwuyi/subscriptions", "organizations_url": "https://api.github.com/users/jxwuyi/orgs", "repos_url": "https://api.github.com/users/jxwuyi/repos", "events_url": "https://api.github.com/users/jxwuyi/events{/privacy}", "received_events_url": "https://api.github.com/users/jxwuyi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-15T23:57:26Z", "updated_at": "2017-06-16T01:52:57Z", "closed_at": "2017-06-16T01:52:56Z", "author_association": "NONE", "body_html": "<p>I'm using Gumbel-Softmax trick, that is I need to compute:<br>\n<code>softmax(-log(-log(uniform(0, 1))))</code><br>\nIn Tenserflow,<br>\nrunning <code>tf.nn.softmax(-tf.log(-tf.log(tf.uniform(...))))</code> is fine.<br>\nBut in Pytorch,<br>\nif you run <code>F.softmax(-torch.log(-torch.log(torch.rand())))</code><br>\nYou will get \"nan\".<br>\nI think the reason is: rand() will sometimes return exactly 0; while in tensorflow exp(-inf) will produce 0 while in pytorch \"nan\" will be thrown.<br>\nAny option for pytorch to allow \"infinity-math\"???</p>", "body_text": "I'm using Gumbel-Softmax trick, that is I need to compute:\nsoftmax(-log(-log(uniform(0, 1))))\nIn Tenserflow,\nrunning tf.nn.softmax(-tf.log(-tf.log(tf.uniform(...)))) is fine.\nBut in Pytorch,\nif you run F.softmax(-torch.log(-torch.log(torch.rand())))\nYou will get \"nan\".\nI think the reason is: rand() will sometimes return exactly 0; while in tensorflow exp(-inf) will produce 0 while in pytorch \"nan\" will be thrown.\nAny option for pytorch to allow \"infinity-math\"???", "body": "I'm using Gumbel-Softmax trick, that is I need to compute:\r\n`softmax(-log(-log(uniform(0, 1))))`\r\nIn Tenserflow, \r\nrunning `tf.nn.softmax(-tf.log(-tf.log(tf.uniform(...))))` is fine.\r\nBut in Pytorch,\r\nif you run `F.softmax(-torch.log(-torch.log(torch.rand())))`\r\nYou will get \"nan\".\r\nI think the reason is: rand() will sometimes return exactly 0; while in tensorflow exp(-inf) will produce 0 while in pytorch \"nan\" will be thrown.\r\nAny option for pytorch to allow \"infinity-math\"???"}