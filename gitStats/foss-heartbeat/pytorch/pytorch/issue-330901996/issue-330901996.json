{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8304", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8304/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8304/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8304/events", "html_url": "https://github.com/pytorch/pytorch/issues/8304", "id": 330901996, "node_id": "MDU6SXNzdWUzMzA5MDE5OTY=", "number": 8304, "title": "[feature request] Efficient Jacobian calculation", "user": {"login": "phizaz", "id": 451667, "node_id": "MDQ6VXNlcjQ1MTY2Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/451667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phizaz", "html_url": "https://github.com/phizaz", "followers_url": "https://api.github.com/users/phizaz/followers", "following_url": "https://api.github.com/users/phizaz/following{/other_user}", "gists_url": "https://api.github.com/users/phizaz/gists{/gist_id}", "starred_url": "https://api.github.com/users/phizaz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phizaz/subscriptions", "organizations_url": "https://api.github.com/users/phizaz/orgs", "repos_url": "https://api.github.com/users/phizaz/repos", "events_url": "https://api.github.com/users/phizaz/events{/privacy}", "received_events_url": "https://api.github.com/users/phizaz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-06-09T16:46:30Z", "updated_at": "2018-06-12T18:59:06Z", "closed_at": "2018-06-12T18:46:08Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>A function <code>f(x): R^n -&gt; R^m</code> will have Jacobian w.r.t x as <code>[df1(x)/dx, df2(x)/dx, ... df_m(x)/dx]</code> where each <code>df_m(x)/dx</code> is an <code>R^n</code> vector.</p>\n<p>As far as I know, Pytorch autograd's library doesn't provide a \"one-shot\" solution for this calculation. Th current solution is to call <code>torch.autograd.grad</code> multiple times on different parts of the output. This could be slow since it doesn't (presumably) make use of the parallelization of GPUs.</p>\n<h2>Code example</h2>\n<p>The current solution I know is:</p>\n<pre><code>J = []\nF = f(x)\nfor i in range(len(x)):\n    J.append(torch.autograd.grad(F[i], x))\n</code></pre>", "body_text": "Issue description\nA function f(x): R^n -> R^m will have Jacobian w.r.t x as [df1(x)/dx, df2(x)/dx, ... df_m(x)/dx] where each df_m(x)/dx is an R^n vector.\nAs far as I know, Pytorch autograd's library doesn't provide a \"one-shot\" solution for this calculation. Th current solution is to call torch.autograd.grad multiple times on different parts of the output. This could be slow since it doesn't (presumably) make use of the parallelization of GPUs.\nCode example\nThe current solution I know is:\nJ = []\nF = f(x)\nfor i in range(len(x)):\n    J.append(torch.autograd.grad(F[i], x))", "body": "## Issue description\r\n\r\nA function `f(x): R^n -> R^m` will have Jacobian w.r.t x as `[df1(x)/dx, df2(x)/dx, ... df_m(x)/dx]` where each `df_m(x)/dx` is an `R^n` vector. \r\n\r\nAs far as I know, Pytorch autograd's library doesn't provide a \"one-shot\" solution for this calculation. Th current solution is to call `torch.autograd.grad` multiple times on different parts of the output. This could be slow since it doesn't (presumably) make use of the parallelization of GPUs.\r\n\r\n## Code example\r\n\r\nThe current solution I know is:\r\n\r\n```\r\nJ = []\r\nF = f(x)\r\nfor i in range(len(x)):\r\n    J.append(torch.autograd.grad(F[i], x))\r\n```"}