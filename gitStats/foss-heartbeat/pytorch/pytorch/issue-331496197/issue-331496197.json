{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8366", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8366/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8366/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8366/events", "html_url": "https://github.com/pytorch/pytorch/issues/8366", "id": 331496197, "node_id": "MDU6SXNzdWUzMzE0OTYxOTc=", "number": 8366, "title": "[feature request] add deviceTensor interface to ATen", "user": {"login": "ClementPinard", "id": 4380424, "node_id": "MDQ6VXNlcjQzODA0MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4380424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ClementPinard", "html_url": "https://github.com/ClementPinard", "followers_url": "https://api.github.com/users/ClementPinard/followers", "following_url": "https://api.github.com/users/ClementPinard/following{/other_user}", "gists_url": "https://api.github.com/users/ClementPinard/gists{/gist_id}", "starred_url": "https://api.github.com/users/ClementPinard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ClementPinard/subscriptions", "organizations_url": "https://api.github.com/users/ClementPinard/orgs", "repos_url": "https://api.github.com/users/ClementPinard/repos", "events_url": "https://api.github.com/users/ClementPinard/events{/privacy}", "received_events_url": "https://api.github.com/users/ClementPinard/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2018-06-12T08:54:13Z", "updated_at": "2018-08-10T13:03:41Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>This is a continuation from this issue : <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"331278932\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/extension-cpp/issues/12\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/extension-cpp/issues/12/hovercard\" href=\"https://github.com/pytorch/extension-cpp/issues/12\">pytorch/extension-cpp/issues/12</a></p>\n<p>This issue comes with a little proof of concept written on top of current extension-cpp by Goldsborough, thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a> for helping me figuring out what to include (for now !) : <a href=\"https://github.com/ClementPinard/extension-cpp/tree/deviceTensorExperiments\">https://github.com/ClementPinard/extension-cpp/tree/deviceTensorExperiments</a></p>\n<p>It is largely inspired from this code in pytorch internals : <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/SpatialGridSamplerBilinear.cu#L29\">https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/SpatialGridSamplerBilinear.cu#L29</a></p>\n<p>The main idea is that writing a cuda kernel for your own custom layers could be more user-friendly using something similar to <code>THCDeviceTensor</code>, which lets you access an array point by indexing it as if it was a multi dimensional array. You can see a comparison between current kernel and proposed one in the issue cited above.</p>\n<p>However, THC is and should remain an internal tool not reachable from ATen user interface, so my proof of concept is a little hacky for the moment, even more so when considering big refactoring is currently happening to ATen internals right now.</p>\n<p>The question is <em>Would it be useful (and possible) to add an official support to ATen to make indexing NDimensional arrays in cuda kernels possible ?</em></p>\n<p>Cuda code is already complicated as it is, and I feel like this could help a lot of people to write them right, and without having to give all tensor metadata information in the kernel arguments (sizes and strides of every tensor) as it is already encapsulated in deviceTensor.</p>", "body_text": "This is a continuation from this issue : pytorch/extension-cpp/issues/12\nThis issue comes with a little proof of concept written on top of current extension-cpp by Goldsborough, thanks @gchanan for helping me figuring out what to include (for now !) : https://github.com/ClementPinard/extension-cpp/tree/deviceTensorExperiments\nIt is largely inspired from this code in pytorch internals : https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/SpatialGridSamplerBilinear.cu#L29\nThe main idea is that writing a cuda kernel for your own custom layers could be more user-friendly using something similar to THCDeviceTensor, which lets you access an array point by indexing it as if it was a multi dimensional array. You can see a comparison between current kernel and proposed one in the issue cited above.\nHowever, THC is and should remain an internal tool not reachable from ATen user interface, so my proof of concept is a little hacky for the moment, even more so when considering big refactoring is currently happening to ATen internals right now.\nThe question is Would it be useful (and possible) to add an official support to ATen to make indexing NDimensional arrays in cuda kernels possible ?\nCuda code is already complicated as it is, and I feel like this could help a lot of people to write them right, and without having to give all tensor metadata information in the kernel arguments (sizes and strides of every tensor) as it is already encapsulated in deviceTensor.", "body": "This is a continuation from this issue : pytorch/extension-cpp/issues/12\r\n\r\nThis issue comes with a little proof of concept written on top of current extension-cpp by Goldsborough, thanks @gchanan for helping me figuring out what to include (for now !) : https://github.com/ClementPinard/extension-cpp/tree/deviceTensorExperiments\r\n\r\nIt is largely inspired from this code in pytorch internals : https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/SpatialGridSamplerBilinear.cu#L29\r\n\r\nThe main idea is that writing a cuda kernel for your own custom layers could be more user-friendly using something similar to `THCDeviceTensor`, which lets you access an array point by indexing it as if it was a multi dimensional array. You can see a comparison between current kernel and proposed one in the issue cited above.\r\n\r\nHowever, THC is and should remain an internal tool not reachable from ATen user interface, so my proof of concept is a little hacky for the moment, even more so when considering big refactoring is currently happening to ATen internals right now.\r\n\r\nThe question is _Would it be useful (and possible) to add an official support to ATen to make indexing NDimensional arrays in cuda kernels possible ?_\r\n\r\nCuda code is already complicated as it is, and I feel like this could help a lot of people to write them right, and without having to give all tensor metadata information in the kernel arguments (sizes and strides of every tensor) as it is already encapsulated in deviceTensor."}