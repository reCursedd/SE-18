{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4181", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4181/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4181/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4181/events", "html_url": "https://github.com/pytorch/pytorch/issues/4181", "id": 282264901, "node_id": "MDU6SXNzdWUyODIyNjQ5MDE=", "number": 4181, "title": "Fused RNN refactor plan", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 806617721, "node_id": "MDU6TGFiZWw4MDY2MTc3MjE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cudnn", "name": "cudnn", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-12-14T22:50:03Z", "updated_at": "2018-01-16T17:25:56Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>This ticket is to track our plan to refactor the fused RNN API (which makes use of the CuDNN implementation of RNNs).</p>\n<p><strong>Why is this difficult?</strong> There are a number of factors which make fused RNNs unusual, compared to most of the other differentiable operations in PyTorch</p>\n<ul>\n<li>\n<p>It requires an unusually large, structured series of weights. Most differentiable operators have a fixed number of weights, but an RNN for an entire sequence must have weights for every layer of the RNN. To make matters worse, each layer needs not one but two tensors; the weight and the bias. No other operator in PyTorch behaves like this.</p>\n</li>\n<li>\n<p>CuDNN requires these weights and inputs to be packed in a particular way.  The required packing operations are frequently reported by users as an extremely confusing aspect of PyTorch.</p>\n</li>\n<li>\n<p>Not only do the weights vary depending on the type of RNN, so do the hidden tensors. If you are an LSTM, you have both hx and cx; for other RNNs, only hx is needed.</p>\n</li>\n<li>\n<p>RNN with dropout is a stateful API, which requires dropout descriptors to be passed between invocations to handle randomness.</p>\n</li>\n</ul>\n<p><strong>What do we want to do?</strong> Here are the desired goals of the RNN refactor:</p>\n<ul>\n<li>Make CuDNN RNN available from ATen</li>\n</ul>\n<p><strong>Design ideas.</strong></p>\n<ul>\n<li>The ATen API will take two tensors, <code>hx</code> and <code>cx</code>, with <code>cx</code> being undefined tensor for non-LSTM networks.</li>\n</ul>", "body_text": "This ticket is to track our plan to refactor the fused RNN API (which makes use of the CuDNN implementation of RNNs).\nWhy is this difficult? There are a number of factors which make fused RNNs unusual, compared to most of the other differentiable operations in PyTorch\n\n\nIt requires an unusually large, structured series of weights. Most differentiable operators have a fixed number of weights, but an RNN for an entire sequence must have weights for every layer of the RNN. To make matters worse, each layer needs not one but two tensors; the weight and the bias. No other operator in PyTorch behaves like this.\n\n\nCuDNN requires these weights and inputs to be packed in a particular way.  The required packing operations are frequently reported by users as an extremely confusing aspect of PyTorch.\n\n\nNot only do the weights vary depending on the type of RNN, so do the hidden tensors. If you are an LSTM, you have both hx and cx; for other RNNs, only hx is needed.\n\n\nRNN with dropout is a stateful API, which requires dropout descriptors to be passed between invocations to handle randomness.\n\n\nWhat do we want to do? Here are the desired goals of the RNN refactor:\n\nMake CuDNN RNN available from ATen\n\nDesign ideas.\n\nThe ATen API will take two tensors, hx and cx, with cx being undefined tensor for non-LSTM networks.", "body": "This ticket is to track our plan to refactor the fused RNN API (which makes use of the CuDNN implementation of RNNs).\r\n\r\n**Why is this difficult?** There are a number of factors which make fused RNNs unusual, compared to most of the other differentiable operations in PyTorch\r\n\r\n* It requires an unusually large, structured series of weights. Most differentiable operators have a fixed number of weights, but an RNN for an entire sequence must have weights for every layer of the RNN. To make matters worse, each layer needs not one but two tensors; the weight and the bias. No other operator in PyTorch behaves like this.\r\n\r\n* CuDNN requires these weights and inputs to be packed in a particular way.  The required packing operations are frequently reported by users as an extremely confusing aspect of PyTorch.\r\n\r\n* Not only do the weights vary depending on the type of RNN, so do the hidden tensors. If you are an LSTM, you have both hx and cx; for other RNNs, only hx is needed.\r\n\r\n* RNN with dropout is a stateful API, which requires dropout descriptors to be passed between invocations to handle randomness.\r\n\r\n**What do we want to do?** Here are the desired goals of the RNN refactor:\r\n\r\n* Make CuDNN RNN available from ATen\r\n\r\n**Design ideas.**\r\n\r\n* The ATen API will take two tensors, `hx` and `cx`, with `cx` being undefined tensor for non-LSTM networks."}