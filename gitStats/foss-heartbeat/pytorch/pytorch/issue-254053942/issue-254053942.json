{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2577", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2577/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2577/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2577/events", "html_url": "https://github.com/pytorch/pytorch/issues/2577", "id": 254053942, "node_id": "MDU6SXNzdWUyNTQwNTM5NDI=", "number": 2577, "title": "gradients / values nonsenically becoming nan", "user": {"login": "dougalsutherland", "id": 36478, "node_id": "MDQ6VXNlcjM2NDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/36478?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dougalsutherland", "html_url": "https://github.com/dougalsutherland", "followers_url": "https://api.github.com/users/dougalsutherland/followers", "following_url": "https://api.github.com/users/dougalsutherland/following{/other_user}", "gists_url": "https://api.github.com/users/dougalsutherland/gists{/gist_id}", "starred_url": "https://api.github.com/users/dougalsutherland/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dougalsutherland/subscriptions", "organizations_url": "https://api.github.com/users/dougalsutherland/orgs", "repos_url": "https://api.github.com/users/dougalsutherland/repos", "events_url": "https://api.github.com/users/dougalsutherland/events{/privacy}", "received_events_url": "https://api.github.com/users/dougalsutherland/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-08-30T16:05:34Z", "updated_at": "2017-09-08T07:26:12Z", "closed_at": "2017-08-30T22:59:45Z", "author_association": "NONE", "body_html": "<p><a href=\"https://gist.github.com/dougalsutherland/83432508d796377bd7b8371fa3b52c4a\">https://gist.github.com/dougalsutherland/83432508d796377bd7b8371fa3b52c4a</a> has a test file and a data file where the gradients (and, strangely, the parameter values too) eventually become <code>nan</code> for seemingly no reason; similar behavior on GPU or not. Expected output:</p>\n<pre><code>0/0: 33028.4140625\n0/1: 29880.9160156\n0/2: 32534.9492188\n0/3: 31306.3242188\n0/4: 32576.6816406\n0/5: 34216.7734375\n0/6: 28895.5039062\n0/7: 34759.0078125\n1/0: 32968.1289062\n1/1: 30035.6972656\nTraceback (most recent call last):\n  File \"./model.py\", line 63, in &lt;module&gt;\n    raise ValueError(\"Bad values for \" + bads)\nValueError: Bad values for gen, gen_grad, latent, latent_grad\n</code></pre>\n<p>This happens when the loss is <code>torch.norm(a - b, p=1)</code>, but doesn't seem to happen with <code>torch.nn.functional.l1_loss</code>.</p>\n<p>I wasn't able to reproduce the behavior in a couple tries with random data, hence the data file too.</p>\n<p>This is still not doing exactly what I mentioned on Slack yesterday (<code>invalid pointer</code>, <code>corrupted unsorted chunks</code>, etc), but I wasn't able to reproduce that with a smaller dataset / model yet, and might be related to this problem, who knows.</p>", "body_text": "https://gist.github.com/dougalsutherland/83432508d796377bd7b8371fa3b52c4a has a test file and a data file where the gradients (and, strangely, the parameter values too) eventually become nan for seemingly no reason; similar behavior on GPU or not. Expected output:\n0/0: 33028.4140625\n0/1: 29880.9160156\n0/2: 32534.9492188\n0/3: 31306.3242188\n0/4: 32576.6816406\n0/5: 34216.7734375\n0/6: 28895.5039062\n0/7: 34759.0078125\n1/0: 32968.1289062\n1/1: 30035.6972656\nTraceback (most recent call last):\n  File \"./model.py\", line 63, in <module>\n    raise ValueError(\"Bad values for \" + bads)\nValueError: Bad values for gen, gen_grad, latent, latent_grad\n\nThis happens when the loss is torch.norm(a - b, p=1), but doesn't seem to happen with torch.nn.functional.l1_loss.\nI wasn't able to reproduce the behavior in a couple tries with random data, hence the data file too.\nThis is still not doing exactly what I mentioned on Slack yesterday (invalid pointer, corrupted unsorted chunks, etc), but I wasn't able to reproduce that with a smaller dataset / model yet, and might be related to this problem, who knows.", "body": "https://gist.github.com/dougalsutherland/83432508d796377bd7b8371fa3b52c4a has a test file and a data file where the gradients (and, strangely, the parameter values too) eventually become `nan` for seemingly no reason; similar behavior on GPU or not. Expected output:\r\n\r\n```\r\n0/0: 33028.4140625\r\n0/1: 29880.9160156\r\n0/2: 32534.9492188\r\n0/3: 31306.3242188\r\n0/4: 32576.6816406\r\n0/5: 34216.7734375\r\n0/6: 28895.5039062\r\n0/7: 34759.0078125\r\n1/0: 32968.1289062\r\n1/1: 30035.6972656\r\nTraceback (most recent call last):\r\n  File \"./model.py\", line 63, in <module>\r\n    raise ValueError(\"Bad values for \" + bads)\r\nValueError: Bad values for gen, gen_grad, latent, latent_grad\r\n```\r\n\r\nThis happens when the loss is `torch.norm(a - b, p=1)`, but doesn't seem to happen with `torch.nn.functional.l1_loss`.\r\n\r\nI wasn't able to reproduce the behavior in a couple tries with random data, hence the data file too.\r\n\r\nThis is still not doing exactly what I mentioned on Slack yesterday (`invalid pointer`, `corrupted unsorted chunks`, etc), but I wasn't able to reproduce that with a smaller dataset / model yet, and might be related to this problem, who knows."}