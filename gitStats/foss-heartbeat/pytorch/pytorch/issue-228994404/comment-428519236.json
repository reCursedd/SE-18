{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/428519236", "html_url": "https://github.com/pytorch/pytorch/issues/1570#issuecomment-428519236", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1570", "id": 428519236, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODUxOTIzNg==", "user": {"login": "f0k", "id": 629706, "node_id": "MDQ6VXNlcjYyOTcwNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/629706?v=4", "gravatar_id": "", "url": "https://api.github.com/users/f0k", "html_url": "https://github.com/f0k", "followers_url": "https://api.github.com/users/f0k/followers", "following_url": "https://api.github.com/users/f0k/following{/other_user}", "gists_url": "https://api.github.com/users/f0k/gists{/gist_id}", "starred_url": "https://api.github.com/users/f0k/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/f0k/subscriptions", "organizations_url": "https://api.github.com/users/f0k/orgs", "repos_url": "https://api.github.com/users/f0k/repos", "events_url": "https://api.github.com/users/f0k/events{/privacy}", "received_events_url": "https://api.github.com/users/f0k/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-10T10:22:02Z", "updated_at": "2018-10-10T10:22:02Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<blockquote>\n<p>Indeed, the behaviour is expected but I cannot imagine a use-case where it is what the users want. Why not raising an exception like the <code>view</code> method does?: <code>RuntimeError: input is not contiguous at...</code></p>\n</blockquote>\n</blockquote>\n<blockquote>\n<p>I think it might be used internally even when the tensor is not contiguous (sometimes it might be valid), but I'll need to make sure</p>\n</blockquote>\n<p>Just adding myself to the list of people bitten by inadvertently resizing a non-contiguous tensor... I think either a warning/exception on non-contiguity or updating the documentation to tell to use <code>x = x.view()</code> or <code>x = x.reshape()</code> instead of <code>x.resize_()</code> would be helpful.<br>\n(I was searching for <code>x.view_()</code> and thought it's called <code>x.resize_()</code> for some reason, not realizing the different semantics).<br>\nI'm fine sending a PR to update <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/c2f8f5076c5824890fc076ac05bfb00abd72b53f/torch/_tensor_docs.py#L1814-L1833\">pytorch/torch/_tensor_docs.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 1814 to 1833\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/c2f8f5076c5824890fc076ac05bfb00abd72b53f\">c2f8f50</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L1814\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1814\"></td>\n          <td id=\"LC1814\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> add_docstr_all(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>resize_<span class=\"pl-pds\">'</span></span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1815\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1815\"></td>\n          <td id=\"LC1815\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                <span class=\"pl-sr\"><span class=\"pl-k\">r</span><span class=\"pl-pds\">\"\"\"</span></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1816\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1816\"></td>\n          <td id=\"LC1816\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">resize_<span class=\"pl-c1\">(</span><span class=\"pl-k\">*</span>sizes<span class=\"pl-c1\">)</span> -&gt; Tensor</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1817\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1817\"></td>\n          <td id=\"LC1817\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\"></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1818\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1818\"></td>\n          <td id=\"LC1818\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">Resizes :attr:`self` tensor to the specified size<span class=\"pl-c1\">.</span> If the number of elements is</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1819\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1819\"></td>\n          <td id=\"LC1819\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">larger than the current storage size, then the underlying storage is resized</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1820\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1820\"></td>\n          <td id=\"LC1820\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">to fit the new number of elements<span class=\"pl-c1\">.</span> If the number of elements is smaller, the</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1821\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1821\"></td>\n          <td id=\"LC1821\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">underlying storage is not changed<span class=\"pl-c1\">.</span> Existing elements are preserved but any new</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1822\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1822\"></td>\n          <td id=\"LC1822\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">memory is uninitialized<span class=\"pl-c1\">.</span></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1823\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1823\"></td>\n          <td id=\"LC1823\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\"></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1824\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1824\"></td>\n          <td id=\"LC1824\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">Args:</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1825\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1825\"></td>\n          <td id=\"LC1825\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">    sizes <span class=\"pl-c1\">(</span>torch<span class=\"pl-c1\">.</span>Size or int<span class=\"pl-c1\">...</span><span class=\"pl-c1\">)</span>: the desired size</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1826\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1826\"></td>\n          <td id=\"LC1826\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\"></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1827\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1827\"></td>\n          <td id=\"LC1827\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">Example::</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1828\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1828\"></td>\n          <td id=\"LC1828\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\"></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1829\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1829\"></td>\n          <td id=\"LC1829\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">    &gt;&gt;&gt; x = torch<span class=\"pl-c1\">.</span>tensor<span class=\"pl-c1\">(</span>[<span class=\"pl-c1\">[1, 2</span>], [<span class=\"pl-c1\">3, 4</span>], [<span class=\"pl-c1\">5, 6</span>]]<span class=\"pl-c1\">)</span></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1830\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1830\"></td>\n          <td id=\"LC1830\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">    &gt;&gt;&gt; x<span class=\"pl-c1\">.</span>resize_<span class=\"pl-c1\">(</span>2, 2<span class=\"pl-c1\">)</span></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1831\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1831\"></td>\n          <td id=\"LC1831\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">    tensor<span class=\"pl-c1\">(</span>[<span class=\"pl-c1\">[ 1,  2</span>],</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1832\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1832\"></td>\n          <td id=\"LC1832\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">            [<span class=\"pl-c1\"> 3,  4</span>]]<span class=\"pl-c1\">)</span></span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1833\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1833\"></td>\n          <td id=\"LC1833\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\"><span class=\"pl-pds\">\"\"\"</span></span>) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n if you're interested.</p>", "body_text": "Indeed, the behaviour is expected but I cannot imagine a use-case where it is what the users want. Why not raising an exception like the view method does?: RuntimeError: input is not contiguous at...\n\n\n\nI think it might be used internally even when the tensor is not contiguous (sometimes it might be valid), but I'll need to make sure\n\nJust adding myself to the list of people bitten by inadvertently resizing a non-contiguous tensor... I think either a warning/exception on non-contiguity or updating the documentation to tell to use x = x.view() or x = x.reshape() instead of x.resize_() would be helpful.\n(I was searching for x.view_() and thought it's called x.resize_() for some reason, not realizing the different semantics).\nI'm fine sending a PR to update \n  \n    \n      pytorch/torch/_tensor_docs.py\n    \n    \n        Lines 1814 to 1833\n      in\n      c2f8f50\n    \n    \n    \n    \n\n        \n          \n           add_docstr_all('resize_', \n        \n\n        \n          \n                          r\"\"\" \n        \n\n        \n          \n           resize_(*sizes) -> Tensor \n        \n\n        \n          \n            \n        \n\n        \n          \n           Resizes :attr:`self` tensor to the specified size. If the number of elements is \n        \n\n        \n          \n           larger than the current storage size, then the underlying storage is resized \n        \n\n        \n          \n           to fit the new number of elements. If the number of elements is smaller, the \n        \n\n        \n          \n           underlying storage is not changed. Existing elements are preserved but any new \n        \n\n        \n          \n           memory is uninitialized. \n        \n\n        \n          \n            \n        \n\n        \n          \n           Args: \n        \n\n        \n          \n               sizes (torch.Size or int...): the desired size \n        \n\n        \n          \n            \n        \n\n        \n          \n           Example:: \n        \n\n        \n          \n            \n        \n\n        \n          \n               >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]]) \n        \n\n        \n          \n               >>> x.resize_(2, 2) \n        \n\n        \n          \n               tensor([[ 1,  2], \n        \n\n        \n          \n                       [ 3,  4]]) \n        \n\n        \n          \n           \"\"\") \n        \n    \n  \n\n if you're interested.", "body": ">> Indeed, the behaviour is expected but I cannot imagine a use-case where it is what the users want. Why not raising an exception like the `view` method does?: `RuntimeError: input is not contiguous at...`\r\n\r\n> I think it might be used internally even when the tensor is not contiguous (sometimes it might be valid), but I'll need to make sure\r\n\r\nJust adding myself to the list of people bitten by inadvertently resizing a non-contiguous tensor... I think either a warning/exception on non-contiguity or updating the documentation to tell to use `x = x.view()` or `x = x.reshape()` instead of `x.resize_()` would be helpful.\r\n(I was searching for `x.view_()` and thought it's called `x.resize_()` for some reason, not realizing the different semantics).\r\nI'm fine sending a PR to update https://github.com/pytorch/pytorch/blob/c2f8f5076c5824890fc076ac05bfb00abd72b53f/torch/_tensor_docs.py#L1814-L1833 if you're interested."}