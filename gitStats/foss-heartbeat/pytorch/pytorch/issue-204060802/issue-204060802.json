{"url": "https://api.github.com/repos/pytorch/pytorch/issues/642", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/642/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/642/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/642/events", "html_url": "https://github.com/pytorch/pytorch/issues/642", "id": 204060802, "node_id": "MDU6SXNzdWUyMDQwNjA4MDI=", "number": 642, "title": "Cuda memory leak with unpooling", "user": {"login": "aboulch", "id": 9435359, "node_id": "MDQ6VXNlcjk0MzUzNTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/9435359?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aboulch", "html_url": "https://github.com/aboulch", "followers_url": "https://api.github.com/users/aboulch/followers", "following_url": "https://api.github.com/users/aboulch/following{/other_user}", "gists_url": "https://api.github.com/users/aboulch/gists{/gist_id}", "starred_url": "https://api.github.com/users/aboulch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aboulch/subscriptions", "organizations_url": "https://api.github.com/users/aboulch/orgs", "repos_url": "https://api.github.com/users/aboulch/repos", "events_url": "https://api.github.com/users/aboulch/events{/privacy}", "received_events_url": "https://api.github.com/users/aboulch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 491934870, "node_id": "MDU6TGFiZWw0OTE5MzQ4NzA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/dependency%20bug", "name": "dependency bug", "color": "b60205", "default": false}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-01-30T16:35:01Z", "updated_at": "2018-08-03T06:34:50Z", "closed_at": "2017-01-31T09:59:18Z", "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>I have tested the following code :</p>\n<p><code>import numpy as np</code><br>\n<code>import torch</code><br>\n<code>import torch.nn as nn</code><br>\n<code>import torch.nn.functional as F</code><br>\n<code>import torchvision.models as models</code><br>\n<code>from torch.autograd import Variable</code><br>\n<code>import torch.optim as optim</code></p>\n<p><code>class Net(nn.Module):</code><br>\n<code>    def __init__(self):</code><br>\n<code>        super(Net, self).__init__()</code><br>\n<code>        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)</code><br>\n<code>        self.conv2 = nn.Conv2d(64, 3, kernel_size=3, padding=1)</code><br>\n<code>    def forward(self, x):</code><br>\n<code>        x = self.conv1(x)</code><br>\n<code>        x, id1 = F.max_pool2d(x,kernel_size=2, stride=2,return_indices=True)</code><br>\n<code>        x = F.max_unpool2d(x, id1, kernel_size=2, stride=2)</code><br>\n<code>        x = self.conv2(x)</code><br>\n<code>        return x</code></p>\n<p><code>model = Net()</code><br>\n<code>model.cuda()</code></p>\n<p><code>optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)</code></p>\n<p><code>model.train()</code></p>\n<p><code>batch_= np.zeros((8,3, 224, 224), dtype=float)</code><br>\n<code>data = Variable(torch.Tensor(batch_))</code><br>\n<code>data = data.cuda()</code><br>\n<code>while(True):</code><br>\n<code>    output = model(data)</code></p>\n<p>I can see the memory usage of my GPU increasing until I get a \"RuntimeError: cuda runtime error (2) : out of memory\". When I comment the unpooling I get a stable memory usage. Am I missing something ? Could it be a memory leak in the unpooling layer ?</p>\n<p>I use Ubuntu 14.04, GTX 1070, cuda 8.<br>\nI tested on python 2.7 and 3.4 and got the same result.</p>\n<p>Thank you for your help.</p>", "body_text": "Hello,\nI have tested the following code :\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.autograd import Variable\nimport torch.optim as optim\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n    def forward(self, x):\n        x = self.conv1(x)\n        x, id1 = F.max_pool2d(x,kernel_size=2, stride=2,return_indices=True)\n        x = F.max_unpool2d(x, id1, kernel_size=2, stride=2)\n        x = self.conv2(x)\n        return x\nmodel = Net()\nmodel.cuda()\noptimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\nmodel.train()\nbatch_= np.zeros((8,3, 224, 224), dtype=float)\ndata = Variable(torch.Tensor(batch_))\ndata = data.cuda()\nwhile(True):\n    output = model(data)\nI can see the memory usage of my GPU increasing until I get a \"RuntimeError: cuda runtime error (2) : out of memory\". When I comment the unpooling I get a stable memory usage. Am I missing something ? Could it be a memory leak in the unpooling layer ?\nI use Ubuntu 14.04, GTX 1070, cuda 8.\nI tested on python 2.7 and 3.4 and got the same result.\nThank you for your help.", "body": "Hello,\r\n\r\nI have tested the following code :\r\n\r\n\r\n`import numpy as np`\r\n`import torch`\r\n`import torch.nn as nn`\r\n`import torch.nn.functional as F`\r\n`import torchvision.models as models`\r\n`from torch.autograd import Variable`\r\n`import torch.optim as optim`\r\n\r\n`class Net(nn.Module):`\r\n`    def __init__(self):`\r\n`        super(Net, self).__init__()`\r\n`        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)`\r\n`        self.conv2 = nn.Conv2d(64, 3, kernel_size=3, padding=1)`\r\n`    def forward(self, x):`\r\n`        x = self.conv1(x)`\r\n`        x, id1 = F.max_pool2d(x,kernel_size=2, stride=2,return_indices=True)`\r\n`        x = F.max_unpool2d(x, id1, kernel_size=2, stride=2)`\r\n`        x = self.conv2(x)`\r\n`        return x`\r\n\r\n`model = Net()`\r\n`model.cuda()`\r\n\r\n`optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)`\r\n\r\n`model.train()`\r\n\r\n`batch_= np.zeros((8,3, 224, 224), dtype=float)`\r\n`data = Variable(torch.Tensor(batch_))`\r\n`data = data.cuda()`\r\n`while(True):`\r\n`    output = model(data)`\r\n\r\nI can see the memory usage of my GPU increasing until I get a \"RuntimeError: cuda runtime error (2) : out of memory\". When I comment the unpooling I get a stable memory usage. Am I missing something ? Could it be a memory leak in the unpooling layer ?\r\n\r\nI use Ubuntu 14.04, GTX 1070, cuda 8.\r\nI tested on python 2.7 and 3.4 and got the same result.\r\n\r\nThank you for your help."}