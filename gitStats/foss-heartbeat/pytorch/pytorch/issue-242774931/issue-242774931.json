{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2085", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2085/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2085/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2085/events", "html_url": "https://github.com/pytorch/pytorch/issues/2085", "id": 242774931, "node_id": "MDU6SXNzdWUyNDI3NzQ5MzE=", "number": 2085, "title": "Consistent Learning Rate Parameters Requirements", "user": {"login": "PetrochukM", "id": 7424737, "node_id": "MDQ6VXNlcjc0MjQ3Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7424737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PetrochukM", "html_url": "https://github.com/PetrochukM", "followers_url": "https://api.github.com/users/PetrochukM/followers", "following_url": "https://api.github.com/users/PetrochukM/following{/other_user}", "gists_url": "https://api.github.com/users/PetrochukM/gists{/gist_id}", "starred_url": "https://api.github.com/users/PetrochukM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PetrochukM/subscriptions", "organizations_url": "https://api.github.com/users/PetrochukM/orgs", "repos_url": "https://api.github.com/users/PetrochukM/repos", "events_url": "https://api.github.com/users/PetrochukM/events{/privacy}", "received_events_url": "https://api.github.com/users/PetrochukM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-07-13T17:29:40Z", "updated_at": "2017-07-13T17:34:59Z", "closed_at": "2017-07-13T17:34:59Z", "author_association": "NONE", "body_html": "<p><strong>Background:</strong><br>\nI am implementing a CLI to use a Pytorch model. If the user sets the <code>lr</code> to <code>None</code> it defaults to Pytorch's default <code>lr</code> per different optimizers. This works well except for <code>torch.optim.SGD</code> as it requires a <code>lr</code> parameter instead of defaulting to 1. This behavior is not consistent with every other <code>torch.optim.*</code> and forces me to handle a one-off case.</p>\n<p><strong>Issue:</strong><br>\n<code>torch.optim.SGD</code> unlike every other <code>torch.optim.*</code> has <code>lr</code> as a non-optional parameter.  For consistency, can Pytorch always require <code>lr</code> or never require it. <code>torch.optim.SGD</code> seems like a one-off case.</p>", "body_text": "Background:\nI am implementing a CLI to use a Pytorch model. If the user sets the lr to None it defaults to Pytorch's default lr per different optimizers. This works well except for torch.optim.SGD as it requires a lr parameter instead of defaulting to 1. This behavior is not consistent with every other torch.optim.* and forces me to handle a one-off case.\nIssue:\ntorch.optim.SGD unlike every other torch.optim.* has lr as a non-optional parameter.  For consistency, can Pytorch always require lr or never require it. torch.optim.SGD seems like a one-off case.", "body": "**Background:**\r\nI am implementing a CLI to use a Pytorch model. If the user sets the `lr` to `None` it defaults to Pytorch's default `lr` per different optimizers. This works well except for `torch.optim.SGD` as it requires a `lr` parameter instead of defaulting to 1. This behavior is not consistent with every other `torch.optim.*` and forces me to handle a one-off case. \r\n\r\n**Issue:**\r\n`torch.optim.SGD` unlike every other `torch.optim.*` has `lr` as a non-optional parameter.  For consistency, can Pytorch always require `lr` or never require it. `torch.optim.SGD` seems like a one-off case.\r\n\r\n\r\n"}