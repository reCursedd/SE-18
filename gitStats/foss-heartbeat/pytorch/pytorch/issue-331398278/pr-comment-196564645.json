{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196564645", "pull_request_review_id": 130143059, "id": 196564645, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NjU2NDY0NQ==", "diff_hunk": "@@ -410,6 +441,22 @@ auto Engine::evaluate_function(FunctionTask& task) -> void {\n       is_ready = true;\n     }\n \n+    // Switches to and sync with child's stream (if needed)\n+    #ifdef USE_CUDA\n+      // Sets child's device and stream\n+      const int child_device = output.is_cuda() ? output.get_device() : - 1;\n+      auto child_stream = next.function->input_metadata(next.input_nr).stream();\n+      AutoGPUStream child_auto_gpu_stream{child_device, child_stream};\n+\n+      // Syncs GPU->GPU parent and child\n+      if (parent_device != -1 \n+      && child_device != -1 \n+      && parent_stream != child_stream) {\n+        auto_cuda_event.record(parent_stream);\n+        auto_cuda_event.wait_on(child_stream);\n+      }      ", "path": "torch/csrc/autograd/engine.cpp", "position": null, "original_position": 116, "commit_id": "1b56a400c446aabd207a90585845ab81545bbcdd", "original_commit_id": "36598758d04d2104d5604f038ccd5a31b0377fe2", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "body": "I don't think that's quire right, and this is a really interesting point of the current engine.\r\n\r\nThe way InputBuffer is written is highly suggestive that all input tensors to a function live on the same device. After all, you call .device() on the InputBuffer. This is somewhat misleading, however, and functions can actually receive tensors from and send them to multiple devices. Using DataParallel is an easy way to observe this behavior (and an easy way to trigger the metadata out of date issue above).\r\n\r\nThe current invariant in master is that each slot in the input buffer only contains tensors from the same device, and that cross-device synchronization is the responsibility of individual functions. I don't think this is ideal, and follow-ups to this PR could improve this invariant and take responsibility for these synchronizations, but that's outside our current scope. (Edit: I expand on this a little below.)\r\n\r\nPerhaps surprisingly, the addition of streams here does not change this invariant. Metadata is per slot, so each slot now has a device and a stream. Even if a function creates outputs on multiple streams on the same device, their metadata slot will still only have one stream. In current Master we can think of this stream as always being the device's default stream. With this change that stream is maybe a different one. As long as these tensors are available before backward() is called, then, they will operate correctly, just like today. It is still the caller's responsibility to ensure synchronization across multiple streams in forward.\r\n\r\nDoes that make sense? Maybe hit me up on Slack if we need to get deeper into the details.", "created_at": "2018-06-19T20:21:27Z", "updated_at": "2018-11-23T15:45:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/8354#discussion_r196564645", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8354", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196564645"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8354#discussion_r196564645"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8354"}}, "body_html": "<p>I don't think that's quire right, and this is a really interesting point of the current engine.</p>\n<p>The way InputBuffer is written is highly suggestive that all input tensors to a function live on the same device. After all, you call .device() on the InputBuffer. This is somewhat misleading, however, and functions can actually receive tensors from and send them to multiple devices. Using DataParallel is an easy way to observe this behavior (and an easy way to trigger the metadata out of date issue above).</p>\n<p>The current invariant in master is that each slot in the input buffer only contains tensors from the same device, and that cross-device synchronization is the responsibility of individual functions. I don't think this is ideal, and follow-ups to this PR could improve this invariant and take responsibility for these synchronizations, but that's outside our current scope. (Edit: I expand on this a little below.)</p>\n<p>Perhaps surprisingly, the addition of streams here does not change this invariant. Metadata is per slot, so each slot now has a device and a stream. Even if a function creates outputs on multiple streams on the same device, their metadata slot will still only have one stream. In current Master we can think of this stream as always being the device's default stream. With this change that stream is maybe a different one. As long as these tensors are available before backward() is called, then, they will operate correctly, just like today. It is still the caller's responsibility to ensure synchronization across multiple streams in forward.</p>\n<p>Does that make sense? Maybe hit me up on Slack if we need to get deeper into the details.</p>", "body_text": "I don't think that's quire right, and this is a really interesting point of the current engine.\nThe way InputBuffer is written is highly suggestive that all input tensors to a function live on the same device. After all, you call .device() on the InputBuffer. This is somewhat misleading, however, and functions can actually receive tensors from and send them to multiple devices. Using DataParallel is an easy way to observe this behavior (and an easy way to trigger the metadata out of date issue above).\nThe current invariant in master is that each slot in the input buffer only contains tensors from the same device, and that cross-device synchronization is the responsibility of individual functions. I don't think this is ideal, and follow-ups to this PR could improve this invariant and take responsibility for these synchronizations, but that's outside our current scope. (Edit: I expand on this a little below.)\nPerhaps surprisingly, the addition of streams here does not change this invariant. Metadata is per slot, so each slot now has a device and a stream. Even if a function creates outputs on multiple streams on the same device, their metadata slot will still only have one stream. In current Master we can think of this stream as always being the device's default stream. With this change that stream is maybe a different one. As long as these tensors are available before backward() is called, then, they will operate correctly, just like today. It is still the caller's responsibility to ensure synchronization across multiple streams in forward.\nDoes that make sense? Maybe hit me up on Slack if we need to get deeper into the details.", "in_reply_to_id": 196557267}