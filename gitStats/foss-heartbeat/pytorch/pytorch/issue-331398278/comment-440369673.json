{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/440369673", "html_url": "https://github.com/pytorch/pytorch/pull/8354#issuecomment-440369673", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354", "id": 440369673, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDM2OTY3Mw==", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-20T17:51:08Z", "updated_at": "2018-11-20T17:51:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7799218\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mcarilli\">@mcarilli</a></p>\n<p>Regarding the 2 concerns:</p>\n<ol>\n<li>\n<p>This piece of synchronization needs to be done at the bucketing level. We could go for something where we maintain an event per bucket entry, record it upon getting the backward hook, and then wait on all of them before queuing the concat for the actual bucketing. The bucketing will then use whatever stream the hook was called on and c10d will synchronize with that. This is a blocker for merging this. I also think this is a rather risky change close to 1.0 and we should merge post 1.0.</p>\n</li>\n<li>\n<p>Calling <code>work.wait()</code> will synchronize completion of the asynchronous work with the currently set stream. After that, the tensor is unbucketed on the current stream. With reentrant backward we could choose to do it at the top level and not have a hook called from autograd, but rather run <code>_sync_reduction_works()</code> from an overridden <code>backward()</code> function after autograd completed. As long as autograd doesn't do any CUDA synchronization itself this should have the same effect as calling a hook from autograd, while being guaranteed you're synchronizing with the right user stream.</p>\n</li>\n</ol>\n<p>Ping <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> on the recent device guard, stream guard, and event changes.</p>", "body_text": "@mruberry @mcarilli\nRegarding the 2 concerns:\n\n\nThis piece of synchronization needs to be done at the bucketing level. We could go for something where we maintain an event per bucket entry, record it upon getting the backward hook, and then wait on all of them before queuing the concat for the actual bucketing. The bucketing will then use whatever stream the hook was called on and c10d will synchronize with that. This is a blocker for merging this. I also think this is a rather risky change close to 1.0 and we should merge post 1.0.\n\n\nCalling work.wait() will synchronize completion of the asynchronous work with the currently set stream. After that, the tensor is unbucketed on the current stream. With reentrant backward we could choose to do it at the top level and not have a hook called from autograd, but rather run _sync_reduction_works() from an overridden backward() function after autograd completed. As long as autograd doesn't do any CUDA synchronization itself this should have the same effect as calling a hook from autograd, while being guaranteed you're synchronizing with the right user stream.\n\n\nPing @ezyang on the recent device guard, stream guard, and event changes.", "body": "@mruberry @mcarilli \r\n\r\nRegarding the 2 concerns:\r\n\r\n1. This piece of synchronization needs to be done at the bucketing level. We could go for something where we maintain an event per bucket entry, record it upon getting the backward hook, and then wait on all of them before queuing the concat for the actual bucketing. The bucketing will then use whatever stream the hook was called on and c10d will synchronize with that. This is a blocker for merging this. I also think this is a rather risky change close to 1.0 and we should merge post 1.0.\r\n\r\n2. Calling `work.wait()` will synchronize completion of the asynchronous work with the currently set stream. After that, the tensor is unbucketed on the current stream. With reentrant backward we could choose to do it at the top level and not have a hook called from autograd, but rather run `_sync_reduction_works()` from an overridden `backward()` function after autograd completed. As long as autograd doesn't do any CUDA synchronization itself this should have the same effect as calling a hook from autograd, while being guaranteed you're synchronizing with the right user stream.\r\n\r\nPing @ezyang on the recent device guard, stream guard, and event changes."}