{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397173009", "html_url": "https://github.com/pytorch/pytorch/pull/8354#issuecomment-397173009", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354", "id": 397173009, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzE3MzAwOQ==", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-14T05:14:12Z", "updated_at": "2018-06-14T05:15:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I incorporated your feedback and I think this latest version is significantly more streamlined.</p>\n<p>As for timing, I have tested the mnist, word_language, and time_series examples and there does not appear to any regression from master in the timings. On the simple example I described above, the timing difference vs running without streams is what one might expect. If tensors are too small then CPU latency prevents overlap. If tensors are too large then the GPU cannot overlap them (much) even though they are on different streams. There is a goldilocks zone, however. When x is a 2^11x2^11 tensor then there is nontrivial overlap. In this case 200 reps from the model takes about 3.8s without streaming and 3.6s with streaming, an improvement of 5% end to end performance. The actual backwards pass speeds up by 9% in this case.</p>\n<p>A 9% backwards speedup is nontrivial, and I'm sure more elaborate examples could be constructed. This example is about as simple, straightforward, and not overfitting as it comes, however, without writing some new RNN modules (although I do look forward to doing that).</p>", "body_text": "I incorporated your feedback and I think this latest version is significantly more streamlined.\nAs for timing, I have tested the mnist, word_language, and time_series examples and there does not appear to any regression from master in the timings. On the simple example I described above, the timing difference vs running without streams is what one might expect. If tensors are too small then CPU latency prevents overlap. If tensors are too large then the GPU cannot overlap them (much) even though they are on different streams. There is a goldilocks zone, however. When x is a 2^11x2^11 tensor then there is nontrivial overlap. In this case 200 reps from the model takes about 3.8s without streaming and 3.6s with streaming, an improvement of 5% end to end performance. The actual backwards pass speeds up by 9% in this case.\nA 9% backwards speedup is nontrivial, and I'm sure more elaborate examples could be constructed. This example is about as simple, straightforward, and not overfitting as it comes, however, without writing some new RNN modules (although I do look forward to doing that).", "body": "I incorporated your feedback and I think this latest version is significantly more streamlined.\r\n\r\nAs for timing, I have tested the mnist, word_language, and time_series examples and there does not appear to any regression from master in the timings. On the simple example I described above, the timing difference vs running without streams is what one might expect. If tensors are too small then CPU latency prevents overlap. If tensors are too large then the GPU cannot overlap them (much) even though they are on different streams. There is a goldilocks zone, however. When x is a 2^11x2^11 tensor then there is nontrivial overlap. In this case 200 reps from the model takes about 3.8s without streaming and 3.6s with streaming, an improvement of 5% end to end performance. The actual backwards pass speeds up by 9% in this case. \r\n\r\nA 9% backwards speedup is nontrivial, and I'm sure more elaborate examples could be constructed. This example is about as simple, straightforward, and not overfitting as it comes, however, without writing some new RNN modules (although I do look forward to doing that). "}