{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8354", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354/events", "html_url": "https://github.com/pytorch/pytorch/pull/8354", "id": 331398278, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk0MTM5Mzc2", "number": 8354, "title": "Updates autograd engine to respect streams set in forward", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 28, "created_at": "2018-06-12T00:45:42Z", "updated_at": "2018-11-23T15:46:04Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8354", "html_url": "https://github.com/pytorch/pytorch/pull/8354", "diff_url": "https://github.com/pytorch/pytorch/pull/8354.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8354.patch"}, "body_html": "<p>This PR addresses issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"323420582\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7601\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7601/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7601\">#7601</a>.</p>\n<p>Currently models that use streams explicitly in forward have to do a lot of extra work to make backwards respect those streams. This PR extends the (recently added) input tracing (see TypeAndShape) to record the devices and streams of inputs. The autograd engine then uses this metadata to enact the expected stream parallelism without extra work from the user.</p>\n<p>For example, a model with forward declared like (original example courtesy of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a>):</p>\n<pre><code>def forward(self,x):\n        x0 = x.clone()\n        torch._C._cuda_setStream(self.stream1._cdata)\n        y0 = self.fc1(x0)\n        self.event1.record(stream = torch.cuda.current_stream())\n        \n        torch._C._cuda_setStream(self.stream2._cdata)\n        y1 = self.fc2(x)\n        self.event2.record(stream = torch.cuda.current_stream())\n        self.stream2.wait_event(self.event1)\n        return y0 + y1\n</code></pre>\n<p>currently will backward on a single stream. With this change the kernels will go on the streams they are assigned in forward and both forward and backward will (for appropriate sizes) run the fc1 and fc2 kernels simultaneously.</p>\n<p>The crux of this change is, as mentioned, an expansion of the TypeAndShape tracing and a relatively simple change to the autograd engine to use cuda events for stream synchronization. To make this efficient I also added a new AutoGPUAndStream class, exposed getting and setting streams on devices, and removed InputBuffer's AutoGPU (it's now redundant). While making these modifications I also fixed AutoGPU to check before setting the GPU when it's destroyed and to use THCudaCheck instead of its custom error handler. These changes mean that an often excessive cudaSetDevice() is not being called when inputs are added to a buffer.</p>\n<p>In addition to allowing users to easily set and use streams that are respected in both forward and backward, this change may encourage modules to do the same and the expanded tracing might allow further optimizations in the autograd engine. (<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, for example, now after initial enumeration we know the number of devices that will be used by a graph task, which might help provide a sense of the \"level of parallelism\" we should expect.)</p>", "body_text": "This PR addresses issue #7601.\nCurrently models that use streams explicitly in forward have to do a lot of extra work to make backwards respect those streams. This PR extends the (recently added) input tracing (see TypeAndShape) to record the devices and streams of inputs. The autograd engine then uses this metadata to enact the expected stream parallelism without extra work from the user.\nFor example, a model with forward declared like (original example courtesy of @ngimel):\ndef forward(self,x):\n        x0 = x.clone()\n        torch._C._cuda_setStream(self.stream1._cdata)\n        y0 = self.fc1(x0)\n        self.event1.record(stream = torch.cuda.current_stream())\n        \n        torch._C._cuda_setStream(self.stream2._cdata)\n        y1 = self.fc2(x)\n        self.event2.record(stream = torch.cuda.current_stream())\n        self.stream2.wait_event(self.event1)\n        return y0 + y1\n\ncurrently will backward on a single stream. With this change the kernels will go on the streams they are assigned in forward and both forward and backward will (for appropriate sizes) run the fc1 and fc2 kernels simultaneously.\nThe crux of this change is, as mentioned, an expansion of the TypeAndShape tracing and a relatively simple change to the autograd engine to use cuda events for stream synchronization. To make this efficient I also added a new AutoGPUAndStream class, exposed getting and setting streams on devices, and removed InputBuffer's AutoGPU (it's now redundant). While making these modifications I also fixed AutoGPU to check before setting the GPU when it's destroyed and to use THCudaCheck instead of its custom error handler. These changes mean that an often excessive cudaSetDevice() is not being called when inputs are added to a buffer.\nIn addition to allowing users to easily set and use streams that are respected in both forward and backward, this change may encourage modules to do the same and the expanded tracing might allow further optimizations in the autograd engine. (@apaszke, for example, now after initial enumeration we know the number of devices that will be used by a graph task, which might help provide a sense of the \"level of parallelism\" we should expect.)", "body": "This PR addresses issue #7601.\r\n\r\nCurrently models that use streams explicitly in forward have to do a lot of extra work to make backwards respect those streams. This PR extends the (recently added) input tracing (see TypeAndShape) to record the devices and streams of inputs. The autograd engine then uses this metadata to enact the expected stream parallelism without extra work from the user.\r\n\r\nFor example, a model with forward declared like (original example courtesy of @ngimel):\r\n\r\n```\r\ndef forward(self,x):\r\n        x0 = x.clone()\r\n        torch._C._cuda_setStream(self.stream1._cdata)\r\n        y0 = self.fc1(x0)\r\n        self.event1.record(stream = torch.cuda.current_stream())\r\n        \r\n        torch._C._cuda_setStream(self.stream2._cdata)\r\n        y1 = self.fc2(x)\r\n        self.event2.record(stream = torch.cuda.current_stream())\r\n        self.stream2.wait_event(self.event1)\r\n        return y0 + y1\r\n```\r\n\r\ncurrently will backward on a single stream. With this change the kernels will go on the streams they are assigned in forward and both forward and backward will (for appropriate sizes) run the fc1 and fc2 kernels simultaneously.\r\n\r\nThe crux of this change is, as mentioned, an expansion of the TypeAndShape tracing and a relatively simple change to the autograd engine to use cuda events for stream synchronization. To make this efficient I also added a new AutoGPUAndStream class, exposed getting and setting streams on devices, and removed InputBuffer's AutoGPU (it's now redundant). While making these modifications I also fixed AutoGPU to check before setting the GPU when it's destroyed and to use THCudaCheck instead of its custom error handler. These changes mean that an often excessive cudaSetDevice() is not being called when inputs are added to a buffer.\r\n\r\nIn addition to allowing users to easily set and use streams that are respected in both forward and backward, this change may encourage modules to do the same and the expanded tracing might allow further optimizations in the autograd engine. (@apaszke, for example, now after initial enumeration we know the number of devices that will be used by a graph task, which might help provide a sense of the \"level of parallelism\" we should expect.) \r\n\r\n"}