{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/398543531", "html_url": "https://github.com/pytorch/pytorch/pull/8354#issuecomment-398543531", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354", "id": 398543531, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODU0MzUzMQ==", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-19T20:58:32Z", "updated_at": "2018-06-20T01:15:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>These latest refinements sound smart and also like the appropriate way to merge with Master.</p>\n<p>The only open issue on the code itself is, I think, what invariant we're setting? Please review what I wrote and see if that fits with your thinking.</p>\n<p>I appreciate the general concern about performance, but I think the CPU latency impact of these changes is very low. Again, the tracing is analogous to that already found in python_function (see VariableInfo). With your suggested changes we're only adding a few conditionals and a couple set streams calls (which are fast and just hitting the tls) per function in backwards (we'll make the same number of get/set gpu calls as current master), and we're adding a couple of conditionals, a ++, and 128 CPU bytes in forward.</p>\n<p>As for benefits, as mentioned, even for the simple case of parallel linear layers there is a 9% backwards speedup when streaming for \"goldilocks\" sizes. What I am really looking forward, to, however, is using a stream-aware backwards to implement \"fast and flexible RNNs.\" See (longstanding) issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"206652810\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/711\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/711/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/711\">#711</a>. To get cuDNN-like speeds we're going to have to stream forward and we'll then want to stream backward (see <a href=\"https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/\" rel=\"nofollow\">Appleyard</a>). If we have to write custom code to stream that backwards pass that'll (1) be a pain and (2) limit the flexibility.</p>\n<p>There are additional independent opportunities for the tracing, too. For example, the autograd engine could be updated so that specialized code like CopyBackwards is not needed (I actually think this would be a really nice change). Currently that can't be done because a function has no notion of what device it expects inputs on, so we don't know whether we need to transfer a tensor or not. It also means that synchronization points can be identified upfront in the initial enumeration of the graph. This latter point is extremely interesting but, unfortunately, the current potential upside there is limited by support for reentrancy.</p>\n<p>Now if we're really concerned about cpu latency vs. feature value, let's talk about reentrancy. If the initial graph enumeration was actually the complete graph, then we could use this tracing knowledge to enqueue and encode all synchronization points in advance, getting rid of the current mutex structure and the ref counting done in backward(). That would likely be a significant speedup. If you like I'll commit to a PR that adds a \"allow_reentrancy\" flag to .backward() (true by default) and if it's set to false uses this alternative fast path. Callbacks, by the way, are also \"broken\" for reentrant backwards because they're not associated with any GraphTask (and there is no mechanism for them to be) and calling backward() while you backward() will clear them. Ensuring only one backward() happens at a time seems pretty natural (and we can add a torch.autograd.backward() that takes a list of tensors for multiple simultaneous backward() calls, too). Largely a separate issue, I admit, but my point is that tracing is potentially very useful at reducing the exact issue we're concerned it could exacerbate.</p>\n<p>(Disallowing reentrancy would also allow us to easily reuse the main thread in backward, btw.)</p>", "body_text": "These latest refinements sound smart and also like the appropriate way to merge with Master.\nThe only open issue on the code itself is, I think, what invariant we're setting? Please review what I wrote and see if that fits with your thinking.\nI appreciate the general concern about performance, but I think the CPU latency impact of these changes is very low. Again, the tracing is analogous to that already found in python_function (see VariableInfo). With your suggested changes we're only adding a few conditionals and a couple set streams calls (which are fast and just hitting the tls) per function in backwards (we'll make the same number of get/set gpu calls as current master), and we're adding a couple of conditionals, a ++, and 128 CPU bytes in forward.\nAs for benefits, as mentioned, even for the simple case of parallel linear layers there is a 9% backwards speedup when streaming for \"goldilocks\" sizes. What I am really looking forward, to, however, is using a stream-aware backwards to implement \"fast and flexible RNNs.\" See (longstanding) issue #711. To get cuDNN-like speeds we're going to have to stream forward and we'll then want to stream backward (see Appleyard). If we have to write custom code to stream that backwards pass that'll (1) be a pain and (2) limit the flexibility.\nThere are additional independent opportunities for the tracing, too. For example, the autograd engine could be updated so that specialized code like CopyBackwards is not needed (I actually think this would be a really nice change). Currently that can't be done because a function has no notion of what device it expects inputs on, so we don't know whether we need to transfer a tensor or not. It also means that synchronization points can be identified upfront in the initial enumeration of the graph. This latter point is extremely interesting but, unfortunately, the current potential upside there is limited by support for reentrancy.\nNow if we're really concerned about cpu latency vs. feature value, let's talk about reentrancy. If the initial graph enumeration was actually the complete graph, then we could use this tracing knowledge to enqueue and encode all synchronization points in advance, getting rid of the current mutex structure and the ref counting done in backward(). That would likely be a significant speedup. If you like I'll commit to a PR that adds a \"allow_reentrancy\" flag to .backward() (true by default) and if it's set to false uses this alternative fast path. Callbacks, by the way, are also \"broken\" for reentrant backwards because they're not associated with any GraphTask (and there is no mechanism for them to be) and calling backward() while you backward() will clear them. Ensuring only one backward() happens at a time seems pretty natural (and we can add a torch.autograd.backward() that takes a list of tensors for multiple simultaneous backward() calls, too). Largely a separate issue, I admit, but my point is that tracing is potentially very useful at reducing the exact issue we're concerned it could exacerbate.\n(Disallowing reentrancy would also allow us to easily reuse the main thread in backward, btw.)", "body": "These latest refinements sound smart and also like the appropriate way to merge with Master.\r\n\r\nThe only open issue on the code itself is, I think, what invariant we're setting? Please review what I wrote and see if that fits with your thinking. \r\n\r\nI appreciate the general concern about performance, but I think the CPU latency impact of these changes is very low. Again, the tracing is analogous to that already found in python_function (see VariableInfo). With your suggested changes we're only adding a few conditionals and a couple set streams calls (which are fast and just hitting the tls) per function in backwards (we'll make the same number of get/set gpu calls as current master), and we're adding a couple of conditionals, a ++, and 128 CPU bytes in forward.  \r\n\r\nAs for benefits, as mentioned, even for the simple case of parallel linear layers there is a 9% backwards speedup when streaming for \"goldilocks\" sizes. What I am really looking forward, to, however, is using a stream-aware backwards to implement \"fast and flexible RNNs.\" See (longstanding) issue #711. To get cuDNN-like speeds we're going to have to stream forward and we'll then want to stream backward (see [Appleyard](https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/)). If we have to write custom code to stream that backwards pass that'll (1) be a pain and (2) limit the flexibility.\r\n\r\nThere are additional independent opportunities for the tracing, too. For example, the autograd engine could be updated so that specialized code like CopyBackwards is not needed (I actually think this would be a really nice change). Currently that can't be done because a function has no notion of what device it expects inputs on, so we don't know whether we need to transfer a tensor or not. It also means that synchronization points can be identified upfront in the initial enumeration of the graph. This latter point is extremely interesting but, unfortunately, the current potential upside there is limited by support for reentrancy. \r\n\r\nNow if we're really concerned about cpu latency vs. feature value, let's talk about reentrancy. If the initial graph enumeration was actually the complete graph, then we could use this tracing knowledge to enqueue and encode all synchronization points in advance, getting rid of the current mutex structure and the ref counting done in backward(). That would likely be a significant speedup. If you like I'll commit to a PR that adds a \"allow_reentrancy\" flag to .backward() (true by default) and if it's set to false uses this alternative fast path. Callbacks, by the way, are also \"broken\" for reentrant backwards because they're not associated with any GraphTask (and there is no mechanism for them to be) and calling backward() while you backward() will clear them. Ensuring only one backward() happens at a time seems pretty natural (and we can add a torch.autograd.backward() that takes a list of tensors for multiple simultaneous backward() calls, too). Largely a separate issue, I admit, but my point is that tracing is potentially very useful at reducing the exact issue we're concerned it could exacerbate. \r\n\r\n(Disallowing reentrancy would also allow us to easily reuse the main thread in backward, btw.) "}