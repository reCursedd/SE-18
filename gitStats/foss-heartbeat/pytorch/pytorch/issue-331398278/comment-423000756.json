{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423000756", "html_url": "https://github.com/pytorch/pytorch/pull/8354#issuecomment-423000756", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354", "id": 423000756, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzAwMDc1Ng==", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T00:30:38Z", "updated_at": "2018-09-20T00:30:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This update significantly changes the PR.</p>\n<p>Since the original submission, considerable architectural work has been on PyTorch. CUDAStreams now use a stream pool, a CUDAEvent wrapper has been created and metadata tracing was extended. These changes make this PR much slimmer.</p>\n<p>This PR does the following:</p>\n<ul>\n<li>Fixes a bug in CUDAStream that would cause segfaults when a stream created on one thread was passed to another, and that second stream attempted to set the given stream as the current stream.</li>\n<li>Makes CUDAStreams hashable (to let them go into an unordered_set)</li>\n<li>Adds 2 streaming backwards tests to test_cuda.py. The first is from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> and tests the backwards compatibility of this PR (more below). The second is from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> and tests a network with two streams in forward.</li>\n<li>Updates the autograd engine to run functions on the stream used in forward, to automatically sync streaming in backwards, and to finish by synchronizing the backward operation with the default streams. This last change is for backward compatibility.</li>\n<li>Adds a helper to function.h to easily acquire a function's stream (for code clarity in engine.cpp)</li>\n<li>Removes the now superfluous device guard from InputBuffer.cpp</li>\n<li>Extends input_metadata with stream information</li>\n</ul>\n<p>One result of these changes is that if a user runs a network with multiple streams in forward(), then they do not need to synchronize before calling backward().</p>\n<p>One concern I have with streaming backwards is if overlapping communication will be adversely affected by this change. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9845\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pietern\">@pietern</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7799218\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mcarilli\">@mcarilli</a> to review, please. Is there another test we should add?</p>", "body_text": "This update significantly changes the PR.\nSince the original submission, considerable architectural work has been on PyTorch. CUDAStreams now use a stream pool, a CUDAEvent wrapper has been created and metadata tracing was extended. These changes make this PR much slimmer.\nThis PR does the following:\n\nFixes a bug in CUDAStream that would cause segfaults when a stream created on one thread was passed to another, and that second stream attempted to set the given stream as the current stream.\nMakes CUDAStreams hashable (to let them go into an unordered_set)\nAdds 2 streaming backwards tests to test_cuda.py. The first is from @colesbury and tests the backwards compatibility of this PR (more below). The second is from @ngimel and tests a network with two streams in forward.\nUpdates the autograd engine to run functions on the stream used in forward, to automatically sync streaming in backwards, and to finish by synchronizing the backward operation with the default streams. This last change is for backward compatibility.\nAdds a helper to function.h to easily acquire a function's stream (for code clarity in engine.cpp)\nRemoves the now superfluous device guard from InputBuffer.cpp\nExtends input_metadata with stream information\n\nOne result of these changes is that if a user runs a network with multiple streams in forward(), then they do not need to synchronize before calling backward().\nOne concern I have with streaming backwards is if overlapping communication will be adversely affected by this change. @pietern @teng-li @mcarilli to review, please. Is there another test we should add?", "body": "This update significantly changes the PR.\r\n\r\nSince the original submission, considerable architectural work has been on PyTorch. CUDAStreams now use a stream pool, a CUDAEvent wrapper has been created and metadata tracing was extended. These changes make this PR much slimmer.\r\n\r\nThis PR does the following:\r\n\r\n- Fixes a bug in CUDAStream that would cause segfaults when a stream created on one thread was passed to another, and that second stream attempted to set the given stream as the current stream.\r\n- Makes CUDAStreams hashable (to let them go into an unordered_set)\r\n- Adds 2 streaming backwards tests to test_cuda.py. The first is from @colesbury and tests the backwards compatibility of this PR (more below). The second is from @ngimel and tests a network with two streams in forward. \r\n- Updates the autograd engine to run functions on the stream used in forward, to automatically sync streaming in backwards, and to finish by synchronizing the backward operation with the default streams. This last change is for backward compatibility.\r\n- Adds a helper to function.h to easily acquire a function's stream (for code clarity in engine.cpp)\r\n- Removes the now superfluous device guard from InputBuffer.cpp\r\n- Extends input_metadata with stream information\r\n\r\nOne result of these changes is that if a user runs a network with multiple streams in forward(), then they do not need to synchronize before calling backward(). \r\n\r\nOne concern I have with streaming backwards is if overlapping communication will be adversely affected by this change. @pietern @teng-li @mcarilli to review, please. Is there another test we should add? \r\n"}