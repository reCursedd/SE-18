{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397005400", "html_url": "https://github.com/pytorch/pytorch/pull/8354#issuecomment-397005400", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354", "id": 397005400, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzAwNTQwMA==", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-13T16:40:10Z", "updated_at": "2018-06-13T17:44:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Great feedback. Let me try to break it down here:</p>\n<ol>\n<li>\n<p>Code complexity: let me pretty up the code per your suggestions and I think it'll be much clearer and simpler. Shouldn't be an issue afterwards.</p>\n</li>\n<li>\n<p>Recording metadata: we are adding at most 128 CPU bytes to an existing tracing structure. The CPU cost and memory overhead of this should be minimal. Fun fact, if you look at python_function.cpp there's already a VariableInfo struct that records type, device, size, and whether the variable requires grad or not, and this is a comparable expansion of the analogous struct for Functions.</p>\n</li>\n<li>\n<p>Backwards streaming design (generally). While there are always better ways to stream backwards I think this is a natural place to plugin streaming with no downside and considerable upside. This PR means that PyTorch users simply stream in forward as they wish and reap the benefits again in backward. While it may seem like we're hammering the stream every op, in practice we're actually just setting the same stream across multiple ops, but getting and setting streams is fast. The per op tracing is also helpful for future prs (I have one planned that I expect will make use of it) and I think it'll be interesting to allow engineers to experiment with multistream modules. cuDNN, for example, makes heavy use of streams, and this change will allow engineers working only in Python to exploit that feature.</p>\n</li>\n<li>\n<p>Performance. Remember that I removed an (almost always) excessive setDevice on every call of evaluateFunction. setDevice is EXPENSIVE. Everything we're doing on the CPU is very fast, in contrast.</p>\n</li>\n</ol>\n<p>All that said, I do actually have a follow-up PR to address CPU latency and simplify the autograd engine code, but I'm loathe to complicate this PR by including it (it's a large and separable piece of work) and don't want to go down the rabbit hole of discussing it too much. While I think this PR is perf neutral, my follow-up actually does give a performance improvement.</p>\n<p>Let me take your code suggestions and gather some appropriate numbers and I think things will look much improved.</p>", "body_text": "Great feedback. Let me try to break it down here:\n\n\nCode complexity: let me pretty up the code per your suggestions and I think it'll be much clearer and simpler. Shouldn't be an issue afterwards.\n\n\nRecording metadata: we are adding at most 128 CPU bytes to an existing tracing structure. The CPU cost and memory overhead of this should be minimal. Fun fact, if you look at python_function.cpp there's already a VariableInfo struct that records type, device, size, and whether the variable requires grad or not, and this is a comparable expansion of the analogous struct for Functions.\n\n\nBackwards streaming design (generally). While there are always better ways to stream backwards I think this is a natural place to plugin streaming with no downside and considerable upside. This PR means that PyTorch users simply stream in forward as they wish and reap the benefits again in backward. While it may seem like we're hammering the stream every op, in practice we're actually just setting the same stream across multiple ops, but getting and setting streams is fast. The per op tracing is also helpful for future prs (I have one planned that I expect will make use of it) and I think it'll be interesting to allow engineers to experiment with multistream modules. cuDNN, for example, makes heavy use of streams, and this change will allow engineers working only in Python to exploit that feature.\n\n\nPerformance. Remember that I removed an (almost always) excessive setDevice on every call of evaluateFunction. setDevice is EXPENSIVE. Everything we're doing on the CPU is very fast, in contrast.\n\n\nAll that said, I do actually have a follow-up PR to address CPU latency and simplify the autograd engine code, but I'm loathe to complicate this PR by including it (it's a large and separable piece of work) and don't want to go down the rabbit hole of discussing it too much. While I think this PR is perf neutral, my follow-up actually does give a performance improvement.\nLet me take your code suggestions and gather some appropriate numbers and I think things will look much improved.", "body": "Great feedback. Let me try to break it down here:\r\n\r\n1. Code complexity: let me pretty up the code per your suggestions and I think it'll be much clearer and simpler. Shouldn't be an issue afterwards.\r\n\r\n2. Recording metadata: we are adding at most 128 CPU bytes to an existing tracing structure. The CPU cost and memory overhead of this should be minimal. Fun fact, if you look at python_function.cpp there's already a VariableInfo struct that records type, device, size, and whether the variable requires grad or not, and this is a comparable expansion of the analogous struct for Functions.\r\n\r\n3. Backwards streaming design (generally). While there are always better ways to stream backwards I think this is a natural place to plugin streaming with no downside and considerable upside. This PR means that PyTorch users simply stream in forward as they wish and reap the benefits again in backward. While it may seem like we're hammering the stream every op, in practice we're actually just setting the same stream across multiple ops, but getting and setting streams is fast. The per op tracing is also helpful for future prs (I have one planned that I expect will make use of it) and I think it'll be interesting to allow engineers to experiment with multistream modules. cuDNN, for example, makes heavy use of streams, and this change will allow engineers working only in Python to exploit that feature.\r\n\r\n4. Performance. Remember that I removed an (almost always) excessive setDevice on every call of evaluateFunction. setDevice is EXPENSIVE. Everything we're doing on the CPU is very fast, in contrast. \r\n\r\nAll that said, I do actually have a follow-up PR to address CPU latency and simplify the autograd engine code, but I'm loathe to complicate this PR by including it (it's a large and separable piece of work) and don't want to go down the rabbit hole of discussing it too much. While I think this PR is perf neutral, my follow-up actually does give a performance improvement. \r\n\r\nLet me take your code suggestions and gather some appropriate numbers and I think things will look much improved."}