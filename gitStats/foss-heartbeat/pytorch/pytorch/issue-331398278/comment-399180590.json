{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/399180590", "html_url": "https://github.com/pytorch/pytorch/pull/8354#issuecomment-399180590", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354", "id": 399180590, "node_id": "MDEyOklzc3VlQ29tbWVudDM5OTE4MDU5MA==", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-21T17:22:34Z", "updated_at": "2018-06-21T17:24:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> Thanks for taking such a deep dive. I think you're proposing a smart design change but let's verify we're on the same page.</p>\n<p>First, totally agree on the testing. I'll add a few as well that we can look at.</p>\n<p>Now for design. Currently in master if you're working on your own stream and call backward() then you are responsible for syncing with the streams backward() uses (the default stream on each device).</p>\n<p>Currently in this PR the same is true, except backward() is no longer always using the default stream. In particular, if you move the default_stream.wait_stream(stream) in the failing test to after output.sum().backward() it will pass. You don't need to wait after the apply().</p>\n<p>In this way the PR is more natural as backward() reuses the streams you used in forward and you don't need to worry about synchronizing with streams you may not have thought you were using (the default streams).</p>\n<p>What may be surprising, however, is the particular relationship between a tensor created in one stream and backward(). I think what you're saying is that users expect this tensor's gradient will be available to the stream it was created in without additional synchronization? That's totally reasonable. However, it will still mean that if you create a tensor in Stream X, then forward/backward in Stream Y, you will still need to sync Y with X after backwarding if you use Y to access the gradient. (Edit: actually this will probably just work. You only need to sync Y with X if Y is not part of X's dependency chain.)</p>\n<p>I like this idea a lot because it ensures consistency of what's need to sync with. So, just to be clear, this change will not eliminate the need to sync (which exists today in an even worse way). But it will eliminate the need to sync in these tests because backward() will sync with the stream the tensor was created on naturally.</p>\n<p>Cool? I'll start working on the fix now.</p>", "body_text": "@colesbury Thanks for taking such a deep dive. I think you're proposing a smart design change but let's verify we're on the same page.\nFirst, totally agree on the testing. I'll add a few as well that we can look at.\nNow for design. Currently in master if you're working on your own stream and call backward() then you are responsible for syncing with the streams backward() uses (the default stream on each device).\nCurrently in this PR the same is true, except backward() is no longer always using the default stream. In particular, if you move the default_stream.wait_stream(stream) in the failing test to after output.sum().backward() it will pass. You don't need to wait after the apply().\nIn this way the PR is more natural as backward() reuses the streams you used in forward and you don't need to worry about synchronizing with streams you may not have thought you were using (the default streams).\nWhat may be surprising, however, is the particular relationship between a tensor created in one stream and backward(). I think what you're saying is that users expect this tensor's gradient will be available to the stream it was created in without additional synchronization? That's totally reasonable. However, it will still mean that if you create a tensor in Stream X, then forward/backward in Stream Y, you will still need to sync Y with X after backwarding if you use Y to access the gradient. (Edit: actually this will probably just work. You only need to sync Y with X if Y is not part of X's dependency chain.)\nI like this idea a lot because it ensures consistency of what's need to sync with. So, just to be clear, this change will not eliminate the need to sync (which exists today in an even worse way). But it will eliminate the need to sync in these tests because backward() will sync with the stream the tensor was created on naturally.\nCool? I'll start working on the fix now.", "body": "@colesbury Thanks for taking such a deep dive. I think you're proposing a smart design change but let's verify we're on the same page.\r\n\r\nFirst, totally agree on the testing. I'll add a few as well that we can look at.\r\n\r\nNow for design. Currently in master if you're working on your own stream and call backward() then you are responsible for syncing with the streams backward() uses (the default stream on each device). \r\n\r\nCurrently in this PR the same is true, except backward() is no longer always using the default stream. In particular, if you move the default_stream.wait_stream(stream) in the failing test to after output.sum().backward() it will pass. You don't need to wait after the apply(). \r\n\r\nIn this way the PR is more natural as backward() reuses the streams you used in forward and you don't need to worry about synchronizing with streams you may not have thought you were using (the default streams). \r\n\r\nWhat may be surprising, however, is the particular relationship between a tensor created in one stream and backward(). I think what you're saying is that users expect this tensor's gradient will be available to the stream it was created in without additional synchronization? That's totally reasonable. However, it will still mean that if you create a tensor in Stream X, then forward/backward in Stream Y, you will still need to sync Y with X after backwarding if you use Y to access the gradient. (Edit: actually this will probably just work. You only need to sync Y with X if Y is not part of X's dependency chain.)\r\n\r\nI like this idea a lot because it ensures consistency of what's need to sync with. So, just to be clear, this change will not eliminate the need to sync (which exists today in an even worse way). But it will eliminate the need to sync in these tests because backward() will sync with the stream the tensor was created on naturally. \r\n\r\nCool? I'll start working on the fix now. "}