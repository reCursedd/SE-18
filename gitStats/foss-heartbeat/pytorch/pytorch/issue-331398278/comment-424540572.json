{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424540572", "html_url": "https://github.com/pytorch/pytorch/pull/8354#issuecomment-424540572", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354", "id": 424540572, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDU0MDU3Mg==", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T00:01:05Z", "updated_at": "2018-09-26T00:01:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9845\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pietern\">@pietern</a><br>\nWhile benchmarking/exploring c10d DDP over the last few days, I realized the streaming backward logic creates two potential vulnerabilities.</p>\n<ol>\n<li>\n<p>(<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a> mentioned you were already aware of this vulnerability) After streaming backward is merged, it will no longer be certain what stream each backward hook will run on.  Therefore, I think we need to sync on the current stream every time <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L293\">we enter the backward hook</a>.  By the time we reach a hook that actually fills next_bucket and kicks off an allreduce, we will have forgotten what streams were used to populate the previous grads that are also participating in this bucket, so it will be too late to sync on their streams and ensure that they are populated.  (I don't see any explicit syncs in your code at all actually.  Maybe this is taken care of by <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L355-L356\">self.process_group.allreduce</a> under the hood?)</p>\n</li>\n<li>\n<p>(I did NOT hear Ruberry mention this vulnerability.  Maybe you've realized it already, but it's definitely worth discussing) At the very end of backward, you need to make sure that whatever stream is about to be used outside of backward (ie, the training script's default stream) is synced on the allreduce stream, so that later ops after backwards are guaranteed to see the allreduced gradients.  I assume that's taken care of by <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L364-L365\">self.reduction_works[bucket_idx].wait()</a>.  Currently, that works because within the hooks (as far as I know) the \"current stream\" is the same same stream that will be used in later ops after backwards, so synchronizing the hook's \"current stream\" on the reduction stream will ensure that later ops are safe.  However, after the streaming backwards PR is merged, the \"current stream\" within the backwards hooks will almost certainly NOT correspond to the stream that will be used for ops after backwards.  Therefore, ops after backward() will be unsafe.</p>\n</li>\n</ol>\n<p>Our tentative plan to address 2. is to have a backward epilogue that syncs on the default stream:</p>\n<pre><code>        def backward_epilogue(self):\n            torch.cuda.current_stream().wait_stream(self.reduction_stream)\n</code></pre>\n<p>then, within the parameter hooks, enqueue this as a callback which will run at the very end of backward:</p>\n<pre><code>                            if not self.callback_queued:\n                                Variable._execution_engine.queue_callback(backward_epilogue)\n                                self.callback_queued = True\n</code></pre>\n<p>Ruberry says it's reasonable to expect that this queued callback (which is neither a variable nor a function hook) will be run with the same \"current stream\" as the later ops after backward().</p>\n<p>Another possibility (I think) is to stash a reference to the \"current stream\" at the beginning of DDP's forward(), and then, at the very end of the very last function hook,</p>\n<pre><code>torch.cuda.stashed_current_stream.wait_stream(self.reduction_stream)\n</code></pre>\n<p>I think this will work as long as the ops after backward() are run in the same stream as the model's forward pass.  If this is acceptable, there's no need to use <code>Variable._execution_engine.queue_callback</code>.</p>", "body_text": "@teng-li @pietern\nWhile benchmarking/exploring c10d DDP over the last few days, I realized the streaming backward logic creates two potential vulnerabilities.\n\n\n(@mruberry mentioned you were already aware of this vulnerability) After streaming backward is merged, it will no longer be certain what stream each backward hook will run on.  Therefore, I think we need to sync on the current stream every time we enter the backward hook.  By the time we reach a hook that actually fills next_bucket and kicks off an allreduce, we will have forgotten what streams were used to populate the previous grads that are also participating in this bucket, so it will be too late to sync on their streams and ensure that they are populated.  (I don't see any explicit syncs in your code at all actually.  Maybe this is taken care of by self.process_group.allreduce under the hood?)\n\n\n(I did NOT hear Ruberry mention this vulnerability.  Maybe you've realized it already, but it's definitely worth discussing) At the very end of backward, you need to make sure that whatever stream is about to be used outside of backward (ie, the training script's default stream) is synced on the allreduce stream, so that later ops after backwards are guaranteed to see the allreduced gradients.  I assume that's taken care of by self.reduction_works[bucket_idx].wait().  Currently, that works because within the hooks (as far as I know) the \"current stream\" is the same same stream that will be used in later ops after backwards, so synchronizing the hook's \"current stream\" on the reduction stream will ensure that later ops are safe.  However, after the streaming backwards PR is merged, the \"current stream\" within the backwards hooks will almost certainly NOT correspond to the stream that will be used for ops after backwards.  Therefore, ops after backward() will be unsafe.\n\n\nOur tentative plan to address 2. is to have a backward epilogue that syncs on the default stream:\n        def backward_epilogue(self):\n            torch.cuda.current_stream().wait_stream(self.reduction_stream)\n\nthen, within the parameter hooks, enqueue this as a callback which will run at the very end of backward:\n                            if not self.callback_queued:\n                                Variable._execution_engine.queue_callback(backward_epilogue)\n                                self.callback_queued = True\n\nRuberry says it's reasonable to expect that this queued callback (which is neither a variable nor a function hook) will be run with the same \"current stream\" as the later ops after backward().\nAnother possibility (I think) is to stash a reference to the \"current stream\" at the beginning of DDP's forward(), and then, at the very end of the very last function hook,\ntorch.cuda.stashed_current_stream.wait_stream(self.reduction_stream)\n\nI think this will work as long as the ops after backward() are run in the same stream as the model's forward pass.  If this is acceptable, there's no need to use Variable._execution_engine.queue_callback.", "body": "@teng-li @pietern \r\nWhile benchmarking/exploring c10d DDP over the last few days, I realized the streaming backward logic creates two potential vulnerabilities.\r\n\r\n1.  (@mruberry mentioned you were already aware of this vulnerability) After streaming backward is merged, it will no longer be certain what stream each backward hook will run on.  Therefore, I think we need to sync on the current stream every time [we enter the backward hook](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L293).  By the time we reach a hook that actually fills next_bucket and kicks off an allreduce, we will have forgotten what streams were used to populate the previous grads that are also participating in this bucket, so it will be too late to sync on their streams and ensure that they are populated.  (I don't see any explicit syncs in your code at all actually.  Maybe this is taken care of by [self.process_group.allreduce](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L355-L356) under the hood?) \r\n\r\n2.  (I did NOT hear Ruberry mention this vulnerability.  Maybe you've realized it already, but it's definitely worth discussing) At the very end of backward, you need to make sure that whatever stream is about to be used outside of backward (ie, the training script's default stream) is synced on the allreduce stream, so that later ops after backwards are guaranteed to see the allreduced gradients.  I assume that's taken care of by [self.reduction_works[bucket_idx].wait()](https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L364-L365).  Currently, that works because within the hooks (as far as I know) the \"current stream\" is the same same stream that will be used in later ops after backwards, so synchronizing the hook's \"current stream\" on the reduction stream will ensure that later ops are safe.  However, after the streaming backwards PR is merged, the \"current stream\" within the backwards hooks will almost certainly NOT correspond to the stream that will be used for ops after backwards.  Therefore, ops after backward() will be unsafe.\r\n\r\nOur tentative plan to address 2. is to have a backward epilogue that syncs on the default stream:\r\n```\r\n        def backward_epilogue(self):\r\n            torch.cuda.current_stream().wait_stream(self.reduction_stream)\r\n```\r\nthen, within the parameter hooks, enqueue this as a callback which will run at the very end of backward:\r\n```\r\n                            if not self.callback_queued:\r\n                                Variable._execution_engine.queue_callback(backward_epilogue)\r\n                                self.callback_queued = True\r\n```\r\nRuberry says it's reasonable to expect that this queued callback (which is neither a variable nor a function hook) will be run with the same \"current stream\" as the later ops after backward().\r\n\r\nAnother possibility (I think) is to stash a reference to the \"current stream\" at the beginning of DDP's forward(), and then, at the very end of the very last function hook,\r\n```\r\ntorch.cuda.stashed_current_stream.wait_stream(self.reduction_stream)\r\n```\r\nI think this will work as long as the ops after backward() are run in the same stream as the model's forward pass.  If this is acceptable, there's no need to use `Variable._execution_engine.queue_callback`.\r\n\r\n"}