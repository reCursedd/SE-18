{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/440382024", "html_url": "https://github.com/pytorch/pytorch/pull/8354#issuecomment-440382024", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8354", "id": 440382024, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDM4MjAyNA==", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-20T18:29:21Z", "updated_at": "2018-11-20T18:29:46Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<pre><code>1. This piece of synchronization needs to be done at the bucketing level. We could go for something where we maintain an event per bucket entry, record it upon getting the backward hook, and then wait on all of them before queuing the concat for the actual bucketing.\n</code></pre>\n</blockquote>\n<p>I like the idea of recording an event every time a bucket entry is filled.  I don't think this prevents DDP from using its own owned stream though (which might help to overlap the concat, averaging, and unflattening at the end).  Every time a bucket of size N is full, you can tell DDP's owned stream to sync on the N events, then kick off its own work.</p>\n<blockquote>\n<pre><code>2. Calling `work.wait()` will synchronize completion of the asynchronous work with the currently set stream. After that, the tensor is unbucketed on the current stream. With reentrant backward we could choose to do it at the top level and not have a hook called from autograd, but rather run `_sync_reduction_works()` from an overridden `backward()` function after autograd completed. As long as autograd doesn't do any CUDA synchronization itself this should have the same effect as calling a hook from autograd, while being guaranteed you're synchronizing with the right user stream.\n</code></pre>\n</blockquote>\n<p>Apex DDP uses a <a href=\"https://github.com/NVIDIA/apex/blob/master/apex/parallel/distributed.py#L341\">queue_callback</a> for this purpose, which is not an autograd hook.  In overlapping-comms mode, the queue_callback <a href=\"https://github.com/NVIDIA/apex/blob/master/apex/parallel/distributed.py#L285\">does nothing but tell the current stream to sync with Apex's internal (owned) reduction stream</a>.  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a> says we can expect that the current stream during queue_callbacks will be the same as the surrounding user stream, so maybe a queue_callback will suffice instead of an \"overridden backward function\" (to be honest I'm not 100% sure what you mean by that).</p>", "body_text": "1. This piece of synchronization needs to be done at the bucketing level. We could go for something where we maintain an event per bucket entry, record it upon getting the backward hook, and then wait on all of them before queuing the concat for the actual bucketing.\n\n\nI like the idea of recording an event every time a bucket entry is filled.  I don't think this prevents DDP from using its own owned stream though (which might help to overlap the concat, averaging, and unflattening at the end).  Every time a bucket of size N is full, you can tell DDP's owned stream to sync on the N events, then kick off its own work.\n\n2. Calling `work.wait()` will synchronize completion of the asynchronous work with the currently set stream. After that, the tensor is unbucketed on the current stream. With reentrant backward we could choose to do it at the top level and not have a hook called from autograd, but rather run `_sync_reduction_works()` from an overridden `backward()` function after autograd completed. As long as autograd doesn't do any CUDA synchronization itself this should have the same effect as calling a hook from autograd, while being guaranteed you're synchronizing with the right user stream.\n\n\nApex DDP uses a queue_callback for this purpose, which is not an autograd hook.  In overlapping-comms mode, the queue_callback does nothing but tell the current stream to sync with Apex's internal (owned) reduction stream.  @mruberry says we can expect that the current stream during queue_callbacks will be the same as the surrounding user stream, so maybe a queue_callback will suffice instead of an \"overridden backward function\" (to be honest I'm not 100% sure what you mean by that).", "body": ">     1. This piece of synchronization needs to be done at the bucketing level. We could go for something where we maintain an event per bucket entry, record it upon getting the backward hook, and then wait on all of them before queuing the concat for the actual bucketing.\r\n\r\nI like the idea of recording an event every time a bucket entry is filled.  I don't think this prevents DDP from using its own owned stream though (which might help to overlap the concat, averaging, and unflattening at the end).  Every time a bucket of size N is full, you can tell DDP's owned stream to sync on the N events, then kick off its own work.\r\n\r\n>     2. Calling `work.wait()` will synchronize completion of the asynchronous work with the currently set stream. After that, the tensor is unbucketed on the current stream. With reentrant backward we could choose to do it at the top level and not have a hook called from autograd, but rather run `_sync_reduction_works()` from an overridden `backward()` function after autograd completed. As long as autograd doesn't do any CUDA synchronization itself this should have the same effect as calling a hook from autograd, while being guaranteed you're synchronizing with the right user stream.\r\n\r\nApex DDP uses a [queue_callback](https://github.com/NVIDIA/apex/blob/master/apex/parallel/distributed.py#L341) for this purpose, which is not an autograd hook.  In overlapping-comms mode, the queue_callback [does nothing but tell the current stream to sync with Apex's internal (owned) reduction stream](https://github.com/NVIDIA/apex/blob/master/apex/parallel/distributed.py#L285).  @mruberry says we can expect that the current stream during queue_callbacks will be the same as the surrounding user stream, so maybe a queue_callback will suffice instead of an \"overridden backward function\" (to be honest I'm not 100% sure what you mean by that)."}