{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199892536", "pull_request_review_id": 134100681, "id": 199892536, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5OTg5MjUzNg==", "diff_hunk": "@@ -1,104 +1,118 @@\n #pragma once\n \n-#include \"caffe2/utils/flat_hash_map/flat_hash_map.h\"\n-#include \"caffe2/utils/Metaprogramming.h\"\n #include \"caffe2/core/dispatch/OpSchema.h\"\n+#include \"caffe2/utils/Metaprogramming.h\"\n+#include \"caffe2/utils/flat_hash_map/flat_hash_map.h\"\n \n-#include <type_traits>\n #include <array>\n-#include <unordered_map>\n+#include <atomic>\n #include <iostream>\n #include <mutex>\n+#include <type_traits>\n+#include <unordered_map>\n \n namespace c10 {\n \n namespace details {\n-\n /// Kernel implementations in a thread-safe hash table.\n-template<class Key>\n+template <class Key>\n class ThreadsafeOperatorTable_ final {\n-public:\n-    // TODO The current implementation below does not have the correct correctness characteristics\n-    // which we need.  It's worth spelling out exactly what we need:\n-    //\n-    //  - We need LOCK FREE read access to the table (as per the performance benchmark\n-    //    at https://fb.quip.com/hvz3AGnx8MQ8\n-    //\n-    //  - We need to support writes which are possibly concurrent with reads, occurring when\n-    //    a dynamic library is loaded or unloaded.\n-    //\n-    //  - We probably can require that dynamic library loads/unloads be synchronized (so\n-    //    there are never two concurrent loads.)\n-\n-    template<class Key_>\n-    void emplace(Key_&& key, void* value) {\n-      using std::to_string;\n-      // TODO Locking\n-      //std::unique_lock<std::shared_timed_mutex> lock(mutex_);\n-\n-      auto result = map_.emplace(std::forward<Key>(key), value);\n-      if (!result.second) {\n-        std::ostringstream msg;\n-        msg << \"Tried to register conflicting kernels to the dispatcher: \" << key;\n-        throw std::logic_error(msg.str());\n-      }\n+ public:\n+  ThreadsafeOperatorTable_() {\n+    map_.store(new ska::flat_hash_map<Key, void*>());\n+  }\n+\n+  virtual ~ThreadsafeOperatorTable_() {\n+    delete map_.load();\n+    for (auto* map : maps_to_delete_) {\n+      delete map;\n     }\n+  }\n \n-    void erase(const Key& key) {\n-      // TODO Locking\n-      //std::unique_lock<std::shared_timed_mutex> lock(mutex_);\n+  template <class Key_>\n+  void emplace(Key_&& key, void* value) {", "path": "caffe2/core/dispatch/DispatchTable.h", "position": null, "original_position": 67, "commit_id": "e9d4dce6e816361a8a2eac25911ed20cc422357d", "original_commit_id": "5d249483d9da22876c58f751ef803ade31087dd5", "user": {"login": "smessmer", "id": 2373925, "node_id": "MDQ6VXNlcjIzNzM5MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/2373925?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smessmer", "html_url": "https://github.com/smessmer", "followers_url": "https://api.github.com/users/smessmer/followers", "following_url": "https://api.github.com/users/smessmer/following{/other_user}", "gists_url": "https://api.github.com/users/smessmer/gists{/gist_id}", "starred_url": "https://api.github.com/users/smessmer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smessmer/subscriptions", "organizations_url": "https://api.github.com/users/smessmer/orgs", "repos_url": "https://api.github.com/users/smessmer/repos", "events_url": "https://api.github.com/users/smessmer/events{/privacy}", "received_events_url": "https://api.github.com/users/smessmer/received_events", "type": "User", "site_admin": false}, "body": "If we avoid allocating a new hash map every time, we'd be amortized `O(n)`. It should be possible to just keep two hash maps around, which are both always identical. One is read from, writing writes to the other one, swaps them, then writes the same change to the first one.", "created_at": "2018-07-03T17:33:56Z", "updated_at": "2018-11-23T15:46:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/9126#discussion_r199892536", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9126", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199892536"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9126#discussion_r199892536"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9126"}}, "body_html": "<p>If we avoid allocating a new hash map every time, we'd be amortized <code>O(n)</code>. It should be possible to just keep two hash maps around, which are both always identical. One is read from, writing writes to the other one, swaps them, then writes the same change to the first one.</p>", "body_text": "If we avoid allocating a new hash map every time, we'd be amortized O(n). It should be possible to just keep two hash maps around, which are both always identical. One is read from, writing writes to the other one, swaps them, then writes the same change to the first one.", "in_reply_to_id": 199822779}