{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190950262", "pull_request_review_id": 123440774, "id": 190950262, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MDk1MDI2Mg==", "diff_hunk": "@@ -0,0 +1,145 @@\n+#pragma once\n+\n+#include \"ProcessGroup.hpp\"\n+#include \"Types.hpp\"\n+#include \"Utils.hpp\"\n+\n+#include <mpi.h>\n+\n+#include <condition_variable>\n+#include <deque>\n+#include <exception>\n+#include <memory>\n+#include <mutex>\n+#include <thread>\n+#include <vector>\n+\n+namespace c10d {\n+\n+// WorkEntry is the state associated with a single MPI run instance.\n+// It include the source Tensor list and destination Tensor list, as well as\n+// The actual run function that will operate either on src or dst or both.\n+struct WorkEntry {\n+  explicit WorkEntry(\n+      std::vector<at::Tensor>* src,\n+      std::vector<at::Tensor>* dst,\n+      std::function<void(std::unique_ptr<WorkEntry>&)> run)\n+      : src(src), dst(dst), run(run) {}\n+\n+  // Not copyable\n+  WorkEntry(const WorkEntry&) = delete;\n+  // Not copy assignable\n+  WorkEntry& operator=(const WorkEntry&) = delete;\n+\n+  // For input and output tensors (in-place), we will always use src\n+  std::vector<at::Tensor>* src;\n+  std::vector<at::Tensor>* dst;\n+  std::function<void(std::unique_ptr<WorkEntry>&)> run;\n+};\n+\n+// ProcessGroupMPI implements MPI bindings for c10d.\n+//\n+// All functions on this class are expected to be called in the same\n+// order across processes in the group. This is the only way that we\n+// can guarantee to match up the same calls across processes.\n+//\n+// All MPI functions provided by this class is asynchronously scheduled on a\n+// Worker thread. Therefore, ProcessGroupMPI requires the MPI implementation\n+// that is used to have a minimum thread support value of MPI_THREAD_SERIALIZED.\n+// That is, The process may be multi-threaded, and multiple threads may make\n+// MPI calls, but only one at a time: MPI calls are not made concurrently from\n+// two distinct threads (all MPI calls are serialized). However, with\n+// MPI_THREAD_SERIALIZED, ProcessGroupMPI will only support a singe process\n+// group. In other words, no more than 1 process group can be created globally.\n+//\n+// If you would like to use multiple ProcessGroupMPI, it requres your MPI\n+// implemenation to have a thread support value of MPI_THREAD_MULTIPLE, that is,\n+// multiple threads may call MPI, with no restriction.\n+//\n+// Also note that ProcessGroupMPI only supports a single Tensor operation. In\n+// other words, the size of the input Tensor vector should always be 1.\n+//\n+// CUDA tensor can be supported if the MPI used is CUDA-aware MPI, and\n+// ProcessGroupMPI will automatically detect this support.\n+class ProcessGroupMPI : public ProcessGroup {\n+ public:\n+  class WorkMPI : public ProcessGroup::Work {\n+   public:\n+    WorkMPI();\n+    virtual ~WorkMPI();\n+\n+    // Checks if request has completed. Non-blocking operation.\n+    bool isCompleted() const override;\n+\n+    // Returns if the work completed successfully\n+    // if false, the exception function can be called to get details.\n+    bool isSuccess() const override;\n+\n+    // Waits until request completes. Blocking operation\n+    // Returns false if the work completed with an exception\n+    bool wait() override;\n+\n+    // Return the exception if wait() returned false.\n+    const std::exception& exception() const override;\n+\n+   protected:\n+    void finish();\n+    void finishWithException(std::exception_ptr caughtWorkException);\n+\n+    std::mutex workMutex_;\n+    std::condition_variable workCV_;\n+    std::atomic<bool> completed_;\n+\n+    std::exception_ptr workException_;", "path": "torch/lib/c10d/ProcessGroupMPI.hpp", "position": 93, "original_position": 93, "commit_id": "3bee9f02c78e069a41c05cea172eca6cc772af55", "original_commit_id": "effc473695f7c7af7f9d56f3c5f427a17e35f760", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "body": "Nice, we should change the Gloo one to do the same. Right now the exception is captured as value.", "created_at": "2018-05-25T16:44:57Z", "updated_at": "2018-11-23T15:44:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/7783#discussion_r190950262", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7783", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190950262"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7783#discussion_r190950262"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7783"}}, "body_html": "<p>Nice, we should change the Gloo one to do the same. Right now the exception is captured as value.</p>", "body_text": "Nice, we should change the Gloo one to do the same. Right now the exception is captured as value."}