{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7783", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7783/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7783/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7783/events", "html_url": "https://github.com/pytorch/pytorch/pull/7783", "id": 325551436, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg5ODU4MTcz", "number": 7783, "title": "[c10d] MPI Process Group Implementation", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-23T05:10:39Z", "updated_at": "2018-11-23T15:44:42Z", "closed_at": "2018-05-29T21:06:49Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7783", "html_url": "https://github.com/pytorch/pytorch/pull/7783", "diff_url": "https://github.com/pytorch/pytorch/pull/7783.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7783.patch"}, "body_html": "<p>This provides a bare-minimum MPI Process Group implementation, the commit is on top of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9845\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pietern\">@pietern</a>'s Gloo Process Group PR.</p>\n<p>Included in the commit: (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/91cdd104e7bac5e6ab545f85d72bae30b778e81e/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/91cdd104e7bac5e6ab545f85d72bae30b778e81e\"><tt>91cdd10</tt></a>)</p>\n<pre><code>ProcessGroupMPI.hpp\nProcessGroupMPI.cpp\ntest/ProcessGroupMPITest.cpp\nCMakeLists.txt changes\n</code></pre>\n<p>Build: will automatically detect MPI lib and build if MPI is found</p>\n<pre><code>cmake ../ -DCMAKE_INSTALL_PREFIX=\"$PWD/../../tmp_install\"\n-- The C compiler identification is GNU 4.8.5\n-- The CXX compiler identification is GNU 4.8.5\n-- Check for working C compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/cc\n-- Check for working C compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/c++\n-- Check for working CXX compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found MPI_C: /public/apps/openmpi/2.1.1/gcc.4.8.4/lib/libmpi.so\n-- Found MPI_CXX: /public/apps/openmpi/2.1.1/gcc.4.8.4/lib/libmpi.so\n-- MPI_LIBRARIES: /public/apps/openmpi/2.1.1/gcc.4.8.4/lib/libmpi.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /private/home/tengli/pieter_pytorch/pytorch/torch/lib/c10d/build\n</code></pre>\n<p>Test:</p>\n<pre><code>mpirun --mca oob tcp -mca btl_base_warn_component_unused 0 -mca oob_tcp_if_exclude em1 -np 4 test/ProcessGroupMPITest\nTest successful\nTest successful\nTest successful\nTest successful\n</code></pre>\n<p>TODO: adding all other operators and tests.</p>", "body_text": "This provides a bare-minimum MPI Process Group implementation, the commit is on top of @pietern's Gloo Process Group PR.\nIncluded in the commit: (91cdd10)\nProcessGroupMPI.hpp\nProcessGroupMPI.cpp\ntest/ProcessGroupMPITest.cpp\nCMakeLists.txt changes\n\nBuild: will automatically detect MPI lib and build if MPI is found\ncmake ../ -DCMAKE_INSTALL_PREFIX=\"$PWD/../../tmp_install\"\n-- The C compiler identification is GNU 4.8.5\n-- The CXX compiler identification is GNU 4.8.5\n-- Check for working C compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/cc\n-- Check for working C compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/c++\n-- Check for working CXX compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found MPI_C: /public/apps/openmpi/2.1.1/gcc.4.8.4/lib/libmpi.so\n-- Found MPI_CXX: /public/apps/openmpi/2.1.1/gcc.4.8.4/lib/libmpi.so\n-- MPI_LIBRARIES: /public/apps/openmpi/2.1.1/gcc.4.8.4/lib/libmpi.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /private/home/tengli/pieter_pytorch/pytorch/torch/lib/c10d/build\n\nTest:\nmpirun --mca oob tcp -mca btl_base_warn_component_unused 0 -mca oob_tcp_if_exclude em1 -np 4 test/ProcessGroupMPITest\nTest successful\nTest successful\nTest successful\nTest successful\n\nTODO: adding all other operators and tests.", "body": "This provides a bare-minimum MPI Process Group implementation, the commit is on top of @pietern's Gloo Process Group PR.\r\n\r\nIncluded in the commit: (https://github.com/pytorch/pytorch/pull/7783/commits/91cdd104e7bac5e6ab545f85d72bae30b778e81e)\r\n```\r\nProcessGroupMPI.hpp\r\nProcessGroupMPI.cpp\r\ntest/ProcessGroupMPITest.cpp\r\nCMakeLists.txt changes\r\n```\r\n\r\nBuild: will automatically detect MPI lib and build if MPI is found\r\n```\r\ncmake ../ -DCMAKE_INSTALL_PREFIX=\"$PWD/../../tmp_install\"\r\n-- The C compiler identification is GNU 4.8.5\r\n-- The CXX compiler identification is GNU 4.8.5\r\n-- Check for working C compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/cc\r\n-- Check for working C compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/c++\r\n-- Check for working CXX compiler: /private/home/tengli/miniconda3/envs/cuda9/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found MPI_C: /public/apps/openmpi/2.1.1/gcc.4.8.4/lib/libmpi.so\r\n-- Found MPI_CXX: /public/apps/openmpi/2.1.1/gcc.4.8.4/lib/libmpi.so\r\n-- MPI_LIBRARIES: /public/apps/openmpi/2.1.1/gcc.4.8.4/lib/libmpi.so\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /private/home/tengli/pieter_pytorch/pytorch/torch/lib/c10d/build\r\n```\r\n\r\nTest: \r\n```\r\nmpirun --mca oob tcp -mca btl_base_warn_component_unused 0 -mca oob_tcp_if_exclude em1 -np 4 test/ProcessGroupMPITest\r\nTest successful\r\nTest successful\r\nTest successful\r\nTest successful\r\n```\r\nTODO: adding all other operators and tests."}