{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190949574", "pull_request_review_id": 123440774, "id": 190949574, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MDk0OTU3NA==", "diff_hunk": "@@ -0,0 +1,281 @@\n+#include \"ProcessGroupMPI.hpp\"\n+\n+#include <iostream>\n+#include <map>\n+#include \"mpi-ext.h\" // Needed for CUDA-sware check\n+\n+namespace c10d {\n+\n+#define MPI_CHECK(cmd)                                                   \\\n+  do {                                                                   \\\n+    int mpiStatus = cmd;                                                 \\\n+    if (mpiStatus != MPI_SUCCESS) {                                      \\\n+      std::string err = \"MPI error in: \" + std::string(__FILE__) + \":\" + \\\n+          std::to_string(__LINE__) +                                     \\\n+          \", with error code: \" + std::to_string(mpiStatus);             \\\n+      std::cerr << err << std::endl;                                     \\\n+      throw std::runtime_error(err);                                     \\\n+    }                                                                    \\\n+  } while (0)\n+\n+namespace {\n+\n+// Op mapping\n+std::map<ReduceOp, MPI_Op> mpiOp = {\n+    {ReduceOp::MIN, MPI_MIN},\n+    {ReduceOp::MAX, MPI_MAX},\n+    {ReduceOp::SUM, MPI_SUM},\n+    {ReduceOp::PRODUCT, MPI_PROD},\n+};\n+// Type mapping\n+std::map<at::ScalarType, MPI_Datatype> mpiDatatype = {\n+    {at::kByte, MPI_UNSIGNED_CHAR},\n+    {at::kChar, MPI_CHAR},\n+    {at::kDouble, MPI_DOUBLE},\n+    {at::kFloat, MPI_FLOAT},\n+    {at::kInt, MPI_INT},\n+    {at::kLong, MPI_LONG},\n+    {at::kShort, MPI_SHORT},\n+};\n+\n+// Checking CUDA-aware MPI support\n+bool cudaAwareMpiCheck() {\n+// Run time check\n+#if defined(MPIX_CUDA_AWARE_SUPPORT)\n+  if (MPIX_Query_cuda_support() == 1) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+#else // !defined(MPIX_CUDA_AWARE_SUPPORT)\n+  return false;\n+#endif // MPIX_CUDA_AWARE_SUPPORT\n+}\n+\n+void mpiExit() {\n+  MPI_CHECK(MPI_Finalize());\n+}\n+\n+} // namespace\n+\n+// ProcessGroupMPI::WorkMPI\n+ProcessGroupMPI::WorkMPI::WorkMPI() : completed_(false) {}\n+\n+ProcessGroupMPI::WorkMPI::~WorkMPI() {}\n+\n+bool ProcessGroupMPI::WorkMPI::isCompleted() const {\n+  return completed_;\n+}\n+\n+bool ProcessGroupMPI::WorkMPI::isSuccess() const {\n+  return !workException_;\n+}\n+\n+bool ProcessGroupMPI::WorkMPI::wait() {\n+  std::unique_lock<std::mutex> lock(workMutex_);\n+  while (!completed_) {\n+    workCV_.wait(lock);\n+  }\n+  return isSuccess();\n+}\n+\n+void ProcessGroupMPI::WorkMPI::finish() {\n+  {\n+    std::unique_lock<std::mutex> lock(workMutex_);\n+    completed_ = true;\n+  }\n+  workCV_.notify_all();\n+}\n+\n+void ProcessGroupMPI::WorkMPI::finishWithException(\n+    std::exception_ptr caughtWorkException) {\n+  {\n+    std::unique_lock<std::mutex> lock(workMutex_);\n+    completed_ = true;\n+    workException_ = caughtWorkException;\n+  }\n+  workCV_.notify_all();\n+}\n+\n+const std::exception& ProcessGroupMPI::WorkMPI::exception() const {\n+  try {\n+    std::rethrow_exception(workException_);\n+  } catch (const std::exception& e) {\n+    return e;\n+  }\n+}\n+\n+// ProcessGroupMPI\n+\n+// Static global states\n+int ProcessGroupMPI::numProcessGroups_ = 0;\n+int ProcessGroupMPI::mpiThreadSupport_ = 0;\n+bool ProcessGroupMPI::mpiInitialized_ = false;\n+std::mutex ProcessGroupMPI::pgGlobalMutex_;\n+\n+ProcessGroupMPI::ProcessGroupMPI() : ProcessGroup(-1, 0), stop_(false) {\n+  // Initialize MPI environment\n+  std::unique_lock<std::mutex> globalLock(pgGlobalMutex_);\n+  // We only want to initialize once\n+  if (!mpiInitialized_) {\n+    MPI_CHECK(MPI_Init_thread(\n+        nullptr, nullptr, MPI_THREAD_MULTIPLE, &mpiThreadSupport_));\n+    if (mpiThreadSupport_ < MPI_THREAD_SERIALIZED) {\n+      throw std::runtime_error(\n+          \"Used MPI implementation doesn't have the \"\n+          \"minimum level of threading support: \"\n+          \"MPI_THREAD_SERIALIZED. This is required by \"\n+          \"c10d package\");\n+    }\n+    if (std::atexit(mpiExit)) {\n+      throw std::runtime_error(\"Fail to register the MPI exit handler\");\n+    }\n+    mpiInitialized_ = true;\n+  }\n+\n+  // Update the world size and rank\n+  MPI_CHECK(MPI_Comm_size(MPI_COMM_WORLD, &size_));\n+  MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &rank_));\n+\n+  if (mpiThreadSupport_ != MPI_THREAD_MULTIPLE && numProcessGroups_ >= 1) {\n+    throw std::runtime_error(\n+        \"More than one process group created, \"\n+        \"this is not supported due to the used MPI \"\n+        \"implementation doesn't provide the full support \"\n+        \"of multi-threading\");\n+  }\n+  // increase the total PG count\n+  ++numProcessGroups_;\n+\n+  // Start the worker thread accepting MPI calls\n+  workerThread_ = std::thread(&ProcessGroupMPI::runLoop, this);\n+}\n+\n+ProcessGroupMPI::~ProcessGroupMPI() {\n+  destroy();\n+}\n+\n+void ProcessGroupMPI::destroy() {\n+  std::unique_lock<std::mutex> lock(pgMutex_);\n+\n+  while (!queue_.empty()) {\n+    queueConsumeCV_.wait(lock);\n+  }\n+  // Queue is empty, signal stop\n+  stop_ = true;\n+\n+  // Release lock to allow threads to terminate\n+  queueProduceCV_.notify_all();\n+\n+  lock.unlock();\n+\n+  // Join the single worker thread\n+  workerThread_.join();\n+\n+  // Decrease the number of PG created\n+  std::unique_lock<std::mutex> globalLock(pgGlobalMutex_);\n+  --numProcessGroups_;\n+}\n+\n+void ProcessGroupMPI::abort() {\n+  destroy();\n+  MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);\n+}\n+\n+void ProcessGroupMPI::runLoop() {\n+  std::unique_lock<std::mutex> lock(pgMutex_);\n+\n+  while (!stop_) {\n+    if (queue_.empty()) {\n+      queueProduceCV_.wait(lock);\n+      continue;\n+    }\n+\n+    auto workTuple = std::move(queue_.front());\n+\n+    queue_.pop_front();\n+    queueConsumeCV_.notify_one();\n+\n+    auto& workEntry = std::get<0>(workTuple);\n+    auto& work = std::get<1>(workTuple);\n+\n+    lock.unlock();\n+\n+    try {\n+      workEntry->run(workEntry);\n+      work->finish();\n+    } catch (...) {\n+      work->finishWithException(std::current_exception());\n+    }\n+\n+    lock.lock();\n+  }\n+}\n+\n+std::shared_ptr<ProcessGroup::Work> ProcessGroupMPI::enqueue(\n+    std::unique_ptr<WorkEntry> entry) {\n+  auto work = std::make_shared<WorkMPI>();\n+  std::unique_lock<std::mutex> lock(pgMutex_);\n+  queue_.push_back(std::make_tuple(std::move(entry), work));\n+  queueProduceCV_.notify_one();\n+  return work;\n+}\n+\n+void ProcessGroupMPI::checkSingleTensor(\n+    const std::vector<at::Tensor>& tensors) {\n+  if (tensors.size() != 1) {\n+    throw std::runtime_error(\n+        \"MPI process group only supports a single \"\n+        \"tensor op\");\n+  }\n+  if (!tensors[0].is_contiguous()) {\n+    throw std::runtime_error(\"input tensor has to be contiguous\");\n+  }\n+  if (tensors[0].is_cuda() && !cudaAwareMpiCheck()) {\n+    throw std::runtime_error(\n+        \"CUDA tensor detected and the MPI used doesn't \"\n+        \"have CUDA-aware MPI support\");\n+  }\n+}\n+\n+std::shared_ptr<ProcessGroup::Work> ProcessGroupMPI::broadcast(\n+    std::vector<at::Tensor>& tensors,\n+    const BroadcastOptions& opts) {\n+  checkSingleTensor(tensors);\n+  std::function<void(std::unique_ptr<WorkEntry>&)> runFunc =", "path": "torch/lib/c10d/ProcessGroupMPI.cpp", "position": 258, "original_position": 245, "commit_id": "3bee9f02c78e069a41c05cea172eca6cc772af55", "original_commit_id": "0f2296bb5ca6191632e76fda2b5c21b86de87a43", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "body": "What's special about this case?", "created_at": "2018-05-25T16:41:57Z", "updated_at": "2018-11-23T15:44:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/7783#discussion_r190949574", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7783", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190949574"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7783#discussion_r190949574"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7783"}}, "body_html": "<p>What's special about this case?</p>", "body_text": "What's special about this case?", "in_reply_to_id": 190413679}