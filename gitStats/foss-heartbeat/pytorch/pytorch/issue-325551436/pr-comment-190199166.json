{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190199166", "pull_request_review_id": 122523591, "id": 190199166, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MDE5OTE2Ng==", "diff_hunk": "@@ -0,0 +1,295 @@\n+#include \"ProcessGroupMPI.hpp\"\n+\n+#include <map>\n+#include <iostream>\n+\n+namespace c10d {\n+\n+#define MPI_CHECK(cmd) do {                                \\\n+  int mpiStatus = cmd;                                     \\\n+  if (mpiStatus != MPI_SUCCESS) {                          \\\n+    std::string err = \"MPI error in: \" +                   \\\n+                      std::string(__FILE__) + \":\" +        \\\n+                      std::to_string(__LINE__) +           \\\n+                      \", with error code: \" +              \\\n+                      std::to_string(mpiStatus);           \\\n+    std::cerr << err << std::endl;                         \\\n+    throw std::runtime_error(err);                         \\\n+  }                                                        \\\n+} while (0)\n+\n+namespace {\n+\n+// Op mapping\n+std::map<ReduceOp, MPI_Op> mpiOp = {\n+  {ReduceOp::MIN, MPI_MIN},\n+  {ReduceOp::MAX, MPI_MAX},\n+  {ReduceOp::SUM, MPI_SUM},\n+  {ReduceOp::PRODUCT, MPI_PROD},\n+};\n+// Type mapping\n+std::map<at::ScalarType, MPI_Datatype> mpiDatatype = {\n+  {at::kByte, MPI_UNSIGNED_CHAR},\n+  {at::kChar, MPI_CHAR},\n+  {at::kDouble, MPI_DOUBLE},\n+  {at::kFloat, MPI_FLOAT},\n+  {at::kInt, MPI_INT},\n+  {at::kLong, MPI_LONG},\n+  {at::kShort, MPI_SHORT},\n+};\n+\n+// Checking CUDA-aware MPI support\n+bool cudaAwareMpiCheck() {\n+#if defined(MPIX_CUDA_AWARE_SUPPORT) && MPIX_CUDA_AWARE_SUPPORT\n+// Compile time check\n+#elif defined(MPIX_CUDA_AWARE_SUPPORT) && !MPIX_CUDA_AWARE_SUPPORT\n+  return false;\n+#else\n+  return false;\n+#endif /* MPIX_CUDA_AWARE_SUPPORT */\n+\n+// Run time check\n+#if defined(MPIX_CUDA_AWARE_SUPPORT)\n+  if (MPIX_Query_cuda_support() == 1) {\n+    return true;\n+  } else {\n+    return false;\n+  }\n+#else /* !defined(MPIX_CUDA_AWARE_SUPPORT) */\n+  return false;\n+#endif /* MPIX_CUDA_AWARE_SUPPORT */\n+}\n+\n+} // namespace\n+\n+\n+// ProcessGroupMPI::WorkMPI\n+ProcessGroupMPI::WorkMPI::WorkMPI() : completed_(false) {}\n+\n+ProcessGroupMPI::WorkMPI::~WorkMPI() {}\n+\n+bool ProcessGroupMPI::WorkMPI::isCompleted() const {\n+  return completed_;\n+}\n+\n+bool ProcessGroupMPI::WorkMPI::isSuccess() const {\n+  return !ex_;\n+}\n+\n+bool ProcessGroupMPI::WorkMPI::wait() {\n+  std::unique_lock<std::mutex> lock(workMutex_);\n+  while (!completed_) {\n+    workCV_.wait(lock);\n+  }\n+  return isSuccess();\n+}\n+\n+void ProcessGroupMPI::WorkMPI::finish() {\n+  {\n+    std::unique_lock<std::mutex> lock(workMutex_);\n+    completed_ = true;\n+  }\n+  workCV_.notify_all();\n+}\n+\n+void ProcessGroupMPI::WorkMPI::finishWithException(\n+    const std::exception& ex) {\n+  {\n+    std::unique_lock<std::mutex> lock(workMutex_);\n+    completed_ = true;\n+    ex_ = std::unique_ptr<std::exception>(new std::exception(ex));\n+  }\n+  workCV_.notify_all();\n+}\n+\n+const std::exception& ProcessGroupMPI::WorkMPI::exception() const {\n+  return *ex_;\n+}\n+\n+// ProcessGroupMPI\n+\n+// Static global states\n+int ProcessGroupMPI::numProcessGroups_ = 0;\n+int ProcessGroupMPI::mpiThreadSupport_ = 0;\n+bool ProcessGroupMPI::mpiInitialized_ = false;\n+std::mutex ProcessGroupMPI::pgGlobalMutex_;\n+\n+ProcessGroupMPI::ProcessGroupMPI()\n+: ProcessGroup(-1, 0)\n+, stop_(false)\n+{\n+  initMPI();\n+  workerThread_ = std::thread(&ProcessGroupMPI::runLoop, this);\n+}\n+\n+ProcessGroupMPI::~ProcessGroupMPI() {\n+  destroy();\n+}\n+\n+void ProcessGroupMPI::initMPI() {\n+  std::unique_lock<std::mutex> globalLock(pgGlobalMutex_);\n+  // We only want to initialize once\n+  try {\n+    if (!mpiInitialized_) {\n+      MPI_CHECK(MPI_Init_thread(nullptr,\n+                                nullptr,\n+                                MPI_THREAD_MULTIPLE,\n+                                &mpiThreadSupport_));\n+      if (mpiThreadSupport_ < MPI_THREAD_SERIALIZED) {\n+        throw std::runtime_error(\"Used MPI implementation doesn't have the \"\n+                                 \"minimum level of threading support: \"\n+                                 \"MPI_THREAD_SERIALIZED. This is required by \"\n+                                 \"c10d package\");\n+      }\n+      if (mpiThreadSupport_ != MPI_THREAD_MULTIPLE) {\n+        std::cerr << \"WARNING: Used MPI implementation doesn't support \"\n+                  << \"multithreading, so distributed cannot support more \"\n+                  << \"than one process group work.\"\n+                  << std::endl;\n+      }\n+      mpiInitialized_ = true;\n+    }\n+\n+    // Update the world size and rank\n+    MPI_CHECK(MPI_Comm_size(MPI_COMM_WORLD, &size_));\n+    MPI_CHECK(MPI_Comm_rank(MPI_COMM_WORLD, &rank_));\n+\n+  } catch (...) {\n+    MPI_CHECK(MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE));", "path": "torch/lib/c10d/ProcessGroupMPI.cpp", "position": null, "original_position": 158, "commit_id": "3bee9f02c78e069a41c05cea172eca6cc772af55", "original_commit_id": "91cdd104e7bac5e6ab545f85d72bae30b778e81e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "We should not abort the whole program unconditionally using `MPI_Abort`, because it will suppress any any errors you raised from inside this function (including the one about thread level support).", "created_at": "2018-05-23T10:35:09Z", "updated_at": "2018-11-23T15:44:33Z", "html_url": "https://github.com/pytorch/pytorch/pull/7783#discussion_r190199166", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7783", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190199166"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7783#discussion_r190199166"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7783"}}, "body_html": "<p>We should not abort the whole program unconditionally using <code>MPI_Abort</code>, because it will suppress any any errors you raised from inside this function (including the one about thread level support).</p>", "body_text": "We should not abort the whole program unconditionally using MPI_Abort, because it will suppress any any errors you raised from inside this function (including the one about thread level support)."}