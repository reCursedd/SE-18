{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1441", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1441/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1441/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1441/events", "html_url": "https://github.com/pytorch/pytorch/issues/1441", "id": 225792748, "node_id": "MDU6SXNzdWUyMjU3OTI3NDg=", "number": 1441, "title": "Sparse tensors: clearly indicate/document operations which are not invariant under coalesce", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-02T19:13:45Z", "updated_at": "2017-05-05T13:49:47Z", "closed_at": "2017-05-05T13:49:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Whether or not a sparse tensor is coalesced or uncoalesced to be an implementation detail, and operations should operate identically whether or not I have called coalesce or not on a sparse tensor. If an operation DOES behave differently depending on if you've coalesced or not, that is an important detail to document, since it means the operation is mucking about with the internal representation of sparse tensors and you need to be extra careful. In other words, these operations by-in-large should be used internally in PyTorch, and not recommended for external use.</p>\n<p>The operators (which I know of) which have this property are:</p>\n<ul>\n<li>values</li>\n<li>indices</li>\n<li>sparse_mask</li>\n<li>nnz</li>\n</ul>\n<p>So, I think documenting the behavior here should be pretty uncontroversial, but I also want to make an argument that we should ALSO rename these functions (e.g., prepend an underscore) to make it visually distinctive that something is going on here. The reason I believe this is a good idea is not because people might do it wrong (though, evidenced by bugs in PyTorch today, I DO think people will get it wrong), but because you can get it wrong, <em>and think that your code is correct</em>, because all of your testing happens to be on coalesced sparse tensors where things happen to work out correctly. Then you are in for a nasty surprise if you pass an uncoalesced tensor at some point in the future.</p>\n<p>(This is a follow up to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"225751123\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1438\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1438/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1438\">#1438</a>.)</p>", "body_text": "Whether or not a sparse tensor is coalesced or uncoalesced to be an implementation detail, and operations should operate identically whether or not I have called coalesce or not on a sparse tensor. If an operation DOES behave differently depending on if you've coalesced or not, that is an important detail to document, since it means the operation is mucking about with the internal representation of sparse tensors and you need to be extra careful. In other words, these operations by-in-large should be used internally in PyTorch, and not recommended for external use.\nThe operators (which I know of) which have this property are:\n\nvalues\nindices\nsparse_mask\nnnz\n\nSo, I think documenting the behavior here should be pretty uncontroversial, but I also want to make an argument that we should ALSO rename these functions (e.g., prepend an underscore) to make it visually distinctive that something is going on here. The reason I believe this is a good idea is not because people might do it wrong (though, evidenced by bugs in PyTorch today, I DO think people will get it wrong), but because you can get it wrong, and think that your code is correct, because all of your testing happens to be on coalesced sparse tensors where things happen to work out correctly. Then you are in for a nasty surprise if you pass an uncoalesced tensor at some point in the future.\n(This is a follow up to #1438.)", "body": "Whether or not a sparse tensor is coalesced or uncoalesced to be an implementation detail, and operations should operate identically whether or not I have called coalesce or not on a sparse tensor. If an operation DOES behave differently depending on if you've coalesced or not, that is an important detail to document, since it means the operation is mucking about with the internal representation of sparse tensors and you need to be extra careful. In other words, these operations by-in-large should be used internally in PyTorch, and not recommended for external use.\r\n\r\nThe operators (which I know of) which have this property are:\r\n\r\n* values\r\n* indices\r\n* sparse_mask\r\n* nnz\r\n\r\nSo, I think documenting the behavior here should be pretty uncontroversial, but I also want to make an argument that we should ALSO rename these functions (e.g., prepend an underscore) to make it visually distinctive that something is going on here. The reason I believe this is a good idea is not because people might do it wrong (though, evidenced by bugs in PyTorch today, I DO think people will get it wrong), but because you can get it wrong, *and think that your code is correct*, because all of your testing happens to be on coalesced sparse tensors where things happen to work out correctly. Then you are in for a nasty surprise if you pass an uncoalesced tensor at some point in the future.\r\n\r\n(This is a follow up to #1438.)"}