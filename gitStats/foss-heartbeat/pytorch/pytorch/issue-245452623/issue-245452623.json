{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2203", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2203/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2203/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2203/events", "html_url": "https://github.com/pytorch/pytorch/issues/2203", "id": 245452623, "node_id": "MDU6SXNzdWUyNDU0NTI2MjM=", "number": 2203, "title": "[Feature Request] attach()", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-07-25T16:03:32Z", "updated_at": "2017-07-25T17:19:10Z", "closed_at": "2017-07-25T17:19:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Some applications of neural networks could benefit from the addition of an <code>.attach()</code> method to complement <code>.detach()</code>. Example:</p>\n<pre><code>x -&gt; f() -&gt; g() -&gt; h() -&gt; j() -&gt; y -&gt; [loss1(y, t1), loss2(y, t2), loss3(y, t3)]\n</code></pre>\n<p>Where we want to backprop <code>loss1</code> to get gradients only up to <code>h()</code>, manipulate these to create another loss based on these gradients, and then backprop these all the way (alongside other normal losses). This is essentially what is done in the \"efficient\" trust region method of <a href=\"https://arxiv.org/abs/1611.01224\" rel=\"nofollow\">Sample Efficient Actor-Critic with Experience Replay</a>. There is an example of this being achieved <a href=\"https://github.com/chainer/chainerrl/blob/master/chainerrl/agents/acer.py#L193-L200\">in Chainer</a>.</p>\n<p>The current solution would be to <code>detach()</code> after <code>g()</code>, keep the original and copy going through the network and losses, do the above on the detached copy, and then pass the gradients/new loss to the original. Achieving what Chainer does is tricky as Pytorch does not store intermediate <code>.grad</code>s, but it could perhaps be achieved with some manual work from the user to \"ready\" parts of the computation graph for this.</p>", "body_text": "Some applications of neural networks could benefit from the addition of an .attach() method to complement .detach(). Example:\nx -> f() -> g() -> h() -> j() -> y -> [loss1(y, t1), loss2(y, t2), loss3(y, t3)]\n\nWhere we want to backprop loss1 to get gradients only up to h(), manipulate these to create another loss based on these gradients, and then backprop these all the way (alongside other normal losses). This is essentially what is done in the \"efficient\" trust region method of Sample Efficient Actor-Critic with Experience Replay. There is an example of this being achieved in Chainer.\nThe current solution would be to detach() after g(), keep the original and copy going through the network and losses, do the above on the detached copy, and then pass the gradients/new loss to the original. Achieving what Chainer does is tricky as Pytorch does not store intermediate .grads, but it could perhaps be achieved with some manual work from the user to \"ready\" parts of the computation graph for this.", "body": "Some applications of neural networks could benefit from the addition of an `.attach()` method to complement `.detach()`. Example:\r\n\r\n```\r\nx -> f() -> g() -> h() -> j() -> y -> [loss1(y, t1), loss2(y, t2), loss3(y, t3)]\r\n```\r\n\r\nWhere we want to backprop `loss1` to get gradients only up to `h()`, manipulate these to create another loss based on these gradients, and then backprop these all the way (alongside other normal losses). This is essentially what is done in the \"efficient\" trust region method of [Sample Efficient Actor-Critic with Experience Replay](https://arxiv.org/abs/1611.01224). There is an example of this being achieved [in Chainer](https://github.com/chainer/chainerrl/blob/master/chainerrl/agents/acer.py#L193-L200).\r\n\r\nThe current solution would be to `detach()` after `g()`, keep the original and copy going through the network and losses, do the above on the detached copy, and then pass the gradients/new loss to the original. Achieving what Chainer does is tricky as Pytorch does not store intermediate `.grad`s, but it could perhaps be achieved with some manual work from the user to \"ready\" parts of the computation graph for this."}