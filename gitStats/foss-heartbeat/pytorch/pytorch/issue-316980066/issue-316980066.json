{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6879", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6879/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6879/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6879/events", "html_url": "https://github.com/pytorch/pytorch/issues/6879", "id": 316980066, "node_id": "MDU6SXNzdWUzMTY5ODAwNjY=", "number": 6879, "title": "[ONNX] LSTM weight tensors: wrong dimensions", "user": {"login": "stgrue", "id": 18679126, "node_id": "MDQ6VXNlcjE4Njc5MTI2", "avatar_url": "https://avatars0.githubusercontent.com/u/18679126?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stgrue", "html_url": "https://github.com/stgrue", "followers_url": "https://api.github.com/users/stgrue/followers", "following_url": "https://api.github.com/users/stgrue/following{/other_user}", "gists_url": "https://api.github.com/users/stgrue/gists{/gist_id}", "starred_url": "https://api.github.com/users/stgrue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stgrue/subscriptions", "organizations_url": "https://api.github.com/users/stgrue/orgs", "repos_url": "https://api.github.com/users/stgrue/repos", "events_url": "https://api.github.com/users/stgrue/events{/privacy}", "received_events_url": "https://api.github.com/users/stgrue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "anderspapitto", "id": 1388690, "node_id": "MDQ6VXNlcjEzODg2OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1388690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anderspapitto", "html_url": "https://github.com/anderspapitto", "followers_url": "https://api.github.com/users/anderspapitto/followers", "following_url": "https://api.github.com/users/anderspapitto/following{/other_user}", "gists_url": "https://api.github.com/users/anderspapitto/gists{/gist_id}", "starred_url": "https://api.github.com/users/anderspapitto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anderspapitto/subscriptions", "organizations_url": "https://api.github.com/users/anderspapitto/orgs", "repos_url": "https://api.github.com/users/anderspapitto/repos", "events_url": "https://api.github.com/users/anderspapitto/events{/privacy}", "received_events_url": "https://api.github.com/users/anderspapitto/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "anderspapitto", "id": 1388690, "node_id": "MDQ6VXNlcjEzODg2OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1388690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anderspapitto", "html_url": "https://github.com/anderspapitto", "followers_url": "https://api.github.com/users/anderspapitto/followers", "following_url": "https://api.github.com/users/anderspapitto/following{/other_user}", "gists_url": "https://api.github.com/users/anderspapitto/gists{/gist_id}", "starred_url": "https://api.github.com/users/anderspapitto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anderspapitto/subscriptions", "organizations_url": "https://api.github.com/users/anderspapitto/orgs", "repos_url": "https://api.github.com/users/anderspapitto/repos", "events_url": "https://api.github.com/users/anderspapitto/events{/privacy}", "received_events_url": "https://api.github.com/users/anderspapitto/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-04-23T20:56:58Z", "updated_at": "2018-05-10T15:19:18Z", "closed_at": "2018-05-10T15:19:18Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>The <a href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#inputs-3---8\">ONNX specification</a> requires the following dimensions for the weight tensors <code>W</code>, <code>R</code>, and <code>B</code> of an LSTM:</p>\n<p><code>W</code>: <code>[num_directions, 4*hidden_size, input_size]</code> (rank 3)<br>\n<code>R</code>: <code>[num_directions, 4*hidden_size, hidden_size]</code> (rank 3)<br>\n<code>B</code>: <code>[num_directions, 8*hidden_size]</code> (rank 2)</p>\n<p>However, when exporting an LSTM from PyTorch, the tensors used for the LSTM weights in the resulting model have the following dimensions:</p>\n<p><code>W</code>: <code>[num_directions*4*hidden_size, input_size]</code> (rank 2)<br>\n<code>R</code>: <code>[num_directions*4*hidden_size, hidden_size]</code> (rank 2)<br>\n<code>B</code>: <code>[num_directions*8*hidden_size]</code> (rank 1),</p>\n<p>i.e. the weights for the different directions are concatenated, instead of being stored in separate dimensions. This does not conform to the specification.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\ninputs <span class=\"pl-k\">=</span> torch.Tensor(<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>)\nlstm <span class=\"pl-k\">=</span> torch.nn.LSTM(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">bidirectional</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Input dim = hidden dim = 3</span>\nout, hidden <span class=\"pl-k\">=</span> lstm(inputs)  \n\ntorch.onnx.export(lstm, inputs, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lstm.onnx<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>According to the specification, the weight tensors of the LSTM should have the following dimensions:</p>\n<p><code>W</code>: <code>[2, 12, 3]</code> (rank 3)<br>\n<code>R</code>: <code>[2, 12, 3]</code> (rank 3)<br>\n<code>B</code>: <code>[2, 24]</code> (rank 2)</p>\n<p>However, inspecting <code>lstm.onnx</code> yields the following dimensions:</p>\n<p><code>W</code>: <code>[24, 3]</code> (rank 2)<br>\n<code>R</code>: <code>[24, 3]</code> (rank 2)<br>\n<code>B</code>: <code>[48]</code> (rank 1).</p>", "body_text": "Issue description\nThe ONNX specification requires the following dimensions for the weight tensors W, R, and B of an LSTM:\nW: [num_directions, 4*hidden_size, input_size] (rank 3)\nR: [num_directions, 4*hidden_size, hidden_size] (rank 3)\nB: [num_directions, 8*hidden_size] (rank 2)\nHowever, when exporting an LSTM from PyTorch, the tensors used for the LSTM weights in the resulting model have the following dimensions:\nW: [num_directions*4*hidden_size, input_size] (rank 2)\nR: [num_directions*4*hidden_size, hidden_size] (rank 2)\nB: [num_directions*8*hidden_size] (rank 1),\ni.e. the weights for the different directions are concatenated, instead of being stored in separate dimensions. This does not conform to the specification.\nCode example\nimport torch\n\ninputs = torch.Tensor(5,1,3)\nlstm = torch.nn.LSTM(3, 3, bidirectional=True) # Input dim = hidden dim = 3\nout, hidden = lstm(inputs)  \n\ntorch.onnx.export(lstm, inputs, \"lstm.onnx\")\nAccording to the specification, the weight tensors of the LSTM should have the following dimensions:\nW: [2, 12, 3] (rank 3)\nR: [2, 12, 3] (rank 3)\nB: [2, 24] (rank 2)\nHowever, inspecting lstm.onnx yields the following dimensions:\nW: [24, 3] (rank 2)\nR: [24, 3] (rank 2)\nB: [48] (rank 1).", "body": "## Issue description\r\n\r\nThe [ONNX specification](https://github.com/onnx/onnx/blob/master/docs/Operators.md#inputs-3---8) requires the following dimensions for the weight tensors `W`, `R`, and `B` of an LSTM:\r\n\r\n`W`: `[num_directions, 4*hidden_size, input_size]` (rank 3)\r\n`R`: `[num_directions, 4*hidden_size, hidden_size]` (rank 3)\r\n`B`: `[num_directions, 8*hidden_size]` (rank 2)\r\n\r\nHowever, when exporting an LSTM from PyTorch, the tensors used for the LSTM weights in the resulting model have the following dimensions:\r\n\r\n`W`: `[num_directions*4*hidden_size, input_size]` (rank 2)\r\n`R`: `[num_directions*4*hidden_size, hidden_size]` (rank 2)\r\n`B`: `[num_directions*8*hidden_size]` (rank 1),\r\n\r\ni.e. the weights for the different directions are concatenated, instead of being stored in separate dimensions. This does not conform to the specification.\r\n\r\n## Code example\r\n\r\n```python\r\nimport torch\r\n\r\ninputs = torch.Tensor(5,1,3)\r\nlstm = torch.nn.LSTM(3, 3, bidirectional=True) # Input dim = hidden dim = 3\r\nout, hidden = lstm(inputs)  \r\n\r\ntorch.onnx.export(lstm, inputs, \"lstm.onnx\")\r\n```\r\n\r\nAccording to the specification, the weight tensors of the LSTM should have the following dimensions:\r\n\r\n`W`: `[2, 12, 3]` (rank 3)\r\n`R`: `[2, 12, 3]` (rank 3)\r\n`B`: `[2, 24]` (rank 2)\r\n\r\nHowever, inspecting `lstm.onnx` yields the following dimensions:\r\n\r\n`W`: `[24, 3]` (rank 2)\r\n`R`: `[24, 3]` (rank 2)\r\n`B`: `[48]` (rank 1).\r\n"}