{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9249", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9249/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9249/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9249/events", "html_url": "https://github.com/pytorch/pytorch/issues/9249", "id": 339274263, "node_id": "MDU6SXNzdWUzMzkyNzQyNjM=", "number": 9249, "title": "RuntimeError: cuda runtime error (77) ", "user": {"login": "Whamp", "id": 1115485, "node_id": "MDQ6VXNlcjExMTU0ODU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1115485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Whamp", "html_url": "https://github.com/Whamp", "followers_url": "https://api.github.com/users/Whamp/followers", "following_url": "https://api.github.com/users/Whamp/following{/other_user}", "gists_url": "https://api.github.com/users/Whamp/gists{/gist_id}", "starred_url": "https://api.github.com/users/Whamp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Whamp/subscriptions", "organizations_url": "https://api.github.com/users/Whamp/orgs", "repos_url": "https://api.github.com/users/Whamp/repos", "events_url": "https://api.github.com/users/Whamp/events{/privacy}", "received_events_url": "https://api.github.com/users/Whamp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-09T00:56:43Z", "updated_at": "2018-07-15T17:24:48Z", "closed_at": "2018-07-15T17:24:48Z", "author_association": "NONE", "body_html": "<p>RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at c:\\anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\thc\\generic/THCTensorCopy.c:20</p>\n<p>I have started getting this error at different sections of my code, the training phase and also the prediction phase. The error doesn't consistently show up in the same spot but once it's been triggered only a kernel restart will fix the issue.</p>\n<p>I'm using pytorch through the fastai library that sits on top.</p>\n<p>pytorch version: 0.3.1.post2<br>\nCUDA 9.2.127<br>\nwindows 10 Version    10.0.17134 Build 17134<br>\npython 3.6.4</p>\n<p>fastai source: <a href=\"https://github.com/fastai/fastai/tree/master/fastai\">https://github.com/fastai/fastai/tree/master/fastai</a></p>\n<pre><code>source code referenced in the stack trace:\n\ndef lr_find(self, start_lr=1e-5, end_lr=10, wds=None, linear=False, **kwargs):\n    self.save('tmp')\n        layer_opt = self.get_layer_opt(start_lr, wds)\n        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\n        self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\n        self.load('tmp')\n\n\n\nbs = 256\ndata = ColumnarModelData.from_data_frame(PATH, val_idxs= val_idxs, df= X, y= y_trn, cat_flds= cat_feats, bs= bs, is_multi= False, is_reg=True,test_df= X_tst,shuffle=True)\n\nm = data.get_learner(emb_szs=emb_szs, n_cont=len(contin_feats)+len(mom_feats), emb_drop=0.2,out_sz=16, szs= [1024,1024], drops= [0.0,0.5], y_range=y_range)\n\nm.lr_find(start_lr=1e-6,end_lr=10)\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-12-e0201546ce0d&gt; in &lt;module&gt;()\n      1 #m.lr_find(start_lr=1e-6,end_lr=1e-3)\n----&gt; 2 m.lr_find(start_lr=1e-6,end_lr=10)\n\n~\\fastai\\courses\\dl1\\structured\\fastai\\learner.py in lr_find(self, start_lr, end_lr, wds, linear, **kwargs)\n    328         layer_opt = self.get_layer_opt(start_lr, wds)\n    329         self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\n--&gt; 330         self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\n    331         self.load('tmp')\n    332 \n\n~\\fastai\\courses\\dl1\\structured\\fastai\\learner.py in fit_gen(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\n    232             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\n    233             swa_model=self.swa_model if use_swa else None, swa_start=swa_start,\n--&gt; 234             swa_eval_freq=swa_eval_freq, **kwargs)\n    235 \n    236     def get_layer_groups(self): return self.models.get_layer_groups()\n\n~\\fastai\\courses\\dl1\\structured\\fastai\\model.py in fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\n    138             batch_num += 1\n    139             for cb in callbacks: cb.on_batch_begin()\n--&gt; 140             loss = model_stepper.step(V(x),V(y), epoch)\n    141             avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\n    142             debias_loss = avg_loss / (1 - avg_mom**batch_num)\n\n~\\fastai\\courses\\dl1\\structured\\fastai\\model.py in step(self, xs, y, epoch)\n     55         if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\n     56         if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n---&gt; 57         loss.backward()\n     58         if self.fp16: update_fp32_grads(self.fp32_params, self.m)\n     59         if self.loss_scale != 1:\n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    165                 Variable.\n    166         \"\"\"\n--&gt; 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    168 \n    169     def register_hook(self, hook):\n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     97 \n     98     Variable._execution_engine.run_backward(\n---&gt; 99         variables, grad_variables, retain_graph)\n    100 \n    101 \n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\function.py in apply(self, *args)\n     89 \n     90     def apply(self, *args):\n---&gt; 91         return self._forward_cls.backward(self, *args)\n     92 \n     93 \n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\function.py in wrapper(ctx, *args)\n    203         tensor_args = [arg.data if isinstance(arg, Variable) else arg\n    204                        for arg in args]\n--&gt; 205         outputs = fn(ctx, *tensor_args)\n    206         # XXX: this is only an approximation of these flags - there's no way\n    207         # to figure out if fn didn't use ctx.saved_variables and as a result\n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\_functions\\thnn\\sparse.py in backward(ctx, grad_output)\n     84                     _sorted = _indices = None\n     85 \n---&gt; 86             grad_weight = grad_output.new(ctx._weight_size).zero_()\n     87             # Doesn't support Variable grad_output\n     88             ctx._backend.LookupTable_accGradParameters(\n\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at c:\\anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\thc\\generic/THCTensorMath.cu:26\n</code></pre>", "body_text": "RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at c:\\anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\thc\\generic/THCTensorCopy.c:20\nI have started getting this error at different sections of my code, the training phase and also the prediction phase. The error doesn't consistently show up in the same spot but once it's been triggered only a kernel restart will fix the issue.\nI'm using pytorch through the fastai library that sits on top.\npytorch version: 0.3.1.post2\nCUDA 9.2.127\nwindows 10 Version    10.0.17134 Build 17134\npython 3.6.4\nfastai source: https://github.com/fastai/fastai/tree/master/fastai\nsource code referenced in the stack trace:\n\ndef lr_find(self, start_lr=1e-5, end_lr=10, wds=None, linear=False, **kwargs):\n    self.save('tmp')\n        layer_opt = self.get_layer_opt(start_lr, wds)\n        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\n        self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\n        self.load('tmp')\n\n\n\nbs = 256\ndata = ColumnarModelData.from_data_frame(PATH, val_idxs= val_idxs, df= X, y= y_trn, cat_flds= cat_feats, bs= bs, is_multi= False, is_reg=True,test_df= X_tst,shuffle=True)\n\nm = data.get_learner(emb_szs=emb_szs, n_cont=len(contin_feats)+len(mom_feats), emb_drop=0.2,out_sz=16, szs= [1024,1024], drops= [0.0,0.5], y_range=y_range)\n\nm.lr_find(start_lr=1e-6,end_lr=10)\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-12-e0201546ce0d> in <module>()\n      1 #m.lr_find(start_lr=1e-6,end_lr=1e-3)\n----> 2 m.lr_find(start_lr=1e-6,end_lr=10)\n\n~\\fastai\\courses\\dl1\\structured\\fastai\\learner.py in lr_find(self, start_lr, end_lr, wds, linear, **kwargs)\n    328         layer_opt = self.get_layer_opt(start_lr, wds)\n    329         self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\n--> 330         self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\n    331         self.load('tmp')\n    332 \n\n~\\fastai\\courses\\dl1\\structured\\fastai\\learner.py in fit_gen(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\n    232             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\n    233             swa_model=self.swa_model if use_swa else None, swa_start=swa_start,\n--> 234             swa_eval_freq=swa_eval_freq, **kwargs)\n    235 \n    236     def get_layer_groups(self): return self.models.get_layer_groups()\n\n~\\fastai\\courses\\dl1\\structured\\fastai\\model.py in fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\n    138             batch_num += 1\n    139             for cb in callbacks: cb.on_batch_begin()\n--> 140             loss = model_stepper.step(V(x),V(y), epoch)\n    141             avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\n    142             debias_loss = avg_loss / (1 - avg_mom**batch_num)\n\n~\\fastai\\courses\\dl1\\structured\\fastai\\model.py in step(self, xs, y, epoch)\n     55         if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\n     56         if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n---> 57         loss.backward()\n     58         if self.fp16: update_fp32_grads(self.fp32_params, self.m)\n     59         if self.loss_scale != 1:\n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    165                 Variable.\n    166         \"\"\"\n--> 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    168 \n    169     def register_hook(self, hook):\n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     97 \n     98     Variable._execution_engine.run_backward(\n---> 99         variables, grad_variables, retain_graph)\n    100 \n    101 \n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\function.py in apply(self, *args)\n     89 \n     90     def apply(self, *args):\n---> 91         return self._forward_cls.backward(self, *args)\n     92 \n     93 \n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\function.py in wrapper(ctx, *args)\n    203         tensor_args = [arg.data if isinstance(arg, Variable) else arg\n    204                        for arg in args]\n--> 205         outputs = fn(ctx, *tensor_args)\n    206         # XXX: this is only an approximation of these flags - there's no way\n    207         # to figure out if fn didn't use ctx.saved_variables and as a result\n\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\_functions\\thnn\\sparse.py in backward(ctx, grad_output)\n     84                     _sorted = _indices = None\n     85 \n---> 86             grad_weight = grad_output.new(ctx._weight_size).zero_()\n     87             # Doesn't support Variable grad_output\n     88             ctx._backend.LookupTable_accGradParameters(\n\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at c:\\anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\thc\\generic/THCTensorMath.cu:26", "body": "RuntimeError: cuda runtime error (77) : an illegal memory access was encountered at c:\\anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\thc\\generic/THCTensorCopy.c:20\r\n\r\nI have started getting this error at different sections of my code, the training phase and also the prediction phase. The error doesn't consistently show up in the same spot but once it's been triggered only a kernel restart will fix the issue. \r\n\r\nI'm using pytorch through the fastai library that sits on top.\r\n\r\npytorch version: 0.3.1.post2\r\nCUDA 9.2.127\r\nwindows 10 Version    10.0.17134 Build 17134\r\npython 3.6.4\r\n\r\nfastai source: https://github.com/fastai/fastai/tree/master/fastai\r\n\r\n\r\n```\r\nsource code referenced in the stack trace:\r\n\r\ndef lr_find(self, start_lr=1e-5, end_lr=10, wds=None, linear=False, **kwargs):\r\n    self.save('tmp')\r\n        layer_opt = self.get_layer_opt(start_lr, wds)\r\n        self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\r\n        self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\r\n        self.load('tmp')\r\n\r\n\r\n\r\nbs = 256\r\ndata = ColumnarModelData.from_data_frame(PATH, val_idxs= val_idxs, df= X, y= y_trn, cat_flds= cat_feats, bs= bs, is_multi= False, is_reg=True,test_df= X_tst,shuffle=True)\r\n\r\nm = data.get_learner(emb_szs=emb_szs, n_cont=len(contin_feats)+len(mom_feats), emb_drop=0.2,out_sz=16, szs= [1024,1024], drops= [0.0,0.5], y_range=y_range)\r\n\r\nm.lr_find(start_lr=1e-6,end_lr=10)\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-12-e0201546ce0d> in <module>()\r\n      1 #m.lr_find(start_lr=1e-6,end_lr=1e-3)\r\n----> 2 m.lr_find(start_lr=1e-6,end_lr=10)\r\n\r\n~\\fastai\\courses\\dl1\\structured\\fastai\\learner.py in lr_find(self, start_lr, end_lr, wds, linear, **kwargs)\r\n    328         layer_opt = self.get_layer_opt(start_lr, wds)\r\n    329         self.sched = LR_Finder(layer_opt, len(self.data.trn_dl), end_lr, linear=linear)\r\n--> 330         self.fit_gen(self.model, self.data, layer_opt, 1, **kwargs)\r\n    331         self.load('tmp')\r\n    332 \r\n\r\n~\\fastai\\courses\\dl1\\structured\\fastai\\learner.py in fit_gen(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\r\n    232             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\r\n    233             swa_model=self.swa_model if use_swa else None, swa_start=swa_start,\r\n--> 234             swa_eval_freq=swa_eval_freq, **kwargs)\r\n    235 \r\n    236     def get_layer_groups(self): return self.models.get_layer_groups()\r\n\r\n~\\fastai\\courses\\dl1\\structured\\fastai\\model.py in fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)\r\n    138             batch_num += 1\r\n    139             for cb in callbacks: cb.on_batch_begin()\r\n--> 140             loss = model_stepper.step(V(x),V(y), epoch)\r\n    141             avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\r\n    142             debias_loss = avg_loss / (1 - avg_mom**batch_num)\r\n\r\n~\\fastai\\courses\\dl1\\structured\\fastai\\model.py in step(self, xs, y, epoch)\r\n     55         if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\r\n     56         if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\r\n---> 57         loss.backward()\r\n     58         if self.fp16: update_fp32_grads(self.fp32_params, self.m)\r\n     59         if self.loss_scale != 1:\r\n\r\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\variable.py in backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    165                 Variable.\r\n    166         \"\"\"\r\n--> 167         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    168 \r\n    169     def register_hook(self, hook):\r\n\r\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\r\n     97 \r\n     98     Variable._execution_engine.run_backward(\r\n---> 99         variables, grad_variables, retain_graph)\r\n    100 \r\n    101 \r\n\r\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\function.py in apply(self, *args)\r\n     89 \r\n     90     def apply(self, *args):\r\n---> 91         return self._forward_cls.backward(self, *args)\r\n     92 \r\n     93 \r\n\r\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\autograd\\function.py in wrapper(ctx, *args)\r\n    203         tensor_args = [arg.data if isinstance(arg, Variable) else arg\r\n    204                        for arg in args]\r\n--> 205         outputs = fn(ctx, *tensor_args)\r\n    206         # XXX: this is only an approximation of these flags - there's no way\r\n    207         # to figure out if fn didn't use ctx.saved_variables and as a result\r\n\r\n~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\_functions\\thnn\\sparse.py in backward(ctx, grad_output)\r\n     84                     _sorted = _indices = None\r\n     85 \r\n---> 86             grad_weight = grad_output.new(ctx._weight_size).zero_()\r\n     87             # Doesn't support Variable grad_output\r\n     88             ctx._backend.LookupTable_accGradParameters(\r\n\r\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at c:\\anaconda2\\conda-bld\\pytorch_1519501749874\\work\\torch\\lib\\thc\\generic/THCTensorMath.cu:26\r\n```"}