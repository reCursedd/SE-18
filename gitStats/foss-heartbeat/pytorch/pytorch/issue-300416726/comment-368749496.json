{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/368749496", "html_url": "https://github.com/pytorch/pytorch/pull/5420#issuecomment-368749496", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5420", "id": 368749496, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODc0OTQ5Ng==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-27T05:10:28Z", "updated_at": "2018-02-27T05:10:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1388690\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/anderspapitto\">@anderspapitto</a> I don't think you have to discard the tracer to fix this problem.</p>\n<p>The root problem is that the current symbolic override tracing hack is implemented via Python-level autograd Function, which imposes the \"arguments to forward must be Variable\"s restriction. But there is no fundamental reason for ONNX export that this must be the case, and indeed, for the ATen exports you can see many operations which take non-Variable inputs and use those for export. And <em>changing</em> the implementation of a PyTorch autograd function (being used to actually compute backwards) is absolutely a case of tail wagging the dog. In this particular case it doesn't matter (because he net compute is the same) but in some cases it could be very important for an operation to not be hoisted out of forward/backward, because the automatically computed backwards is less efficient than a manually imported one.</p>\n<p>Now, how to fix this without discarding the tracer? Simple: we need a new API. Recall how we implement tracing in ATen (the most recent description is in <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/27505e642963bb8b8c18abdcc6dbed4d41ffa0db/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/27505e642963bb8b8c18abdcc6dbed4d41ffa0db\"><tt>27505e6</tt></a>). There's three steps:</p>\n<ol>\n<li>We run a <code>symbolic</code> on the inputs to the function we want to override</li>\n<li>Run the function</li>\n<li>Attach the trace to the returned outputs</li>\n</ol>\n<p>(You have to split it this way to handle inplace). So all we need is to be able to do this from Python, and then the symbolic override hack is a lot simpler, and we can use this to implement tracing for Python-level functions. What do you think.</p>", "body_text": "@anderspapitto I don't think you have to discard the tracer to fix this problem.\nThe root problem is that the current symbolic override tracing hack is implemented via Python-level autograd Function, which imposes the \"arguments to forward must be Variable\"s restriction. But there is no fundamental reason for ONNX export that this must be the case, and indeed, for the ATen exports you can see many operations which take non-Variable inputs and use those for export. And changing the implementation of a PyTorch autograd function (being used to actually compute backwards) is absolutely a case of tail wagging the dog. In this particular case it doesn't matter (because he net compute is the same) but in some cases it could be very important for an operation to not be hoisted out of forward/backward, because the automatically computed backwards is less efficient than a manually imported one.\nNow, how to fix this without discarding the tracer? Simple: we need a new API. Recall how we implement tracing in ATen (the most recent description is in 27505e6). There's three steps:\n\nWe run a symbolic on the inputs to the function we want to override\nRun the function\nAttach the trace to the returned outputs\n\n(You have to split it this way to handle inplace). So all we need is to be able to do this from Python, and then the symbolic override hack is a lot simpler, and we can use this to implement tracing for Python-level functions. What do you think.", "body": "@anderspapitto I don't think you have to discard the tracer to fix this problem.\r\n\r\nThe root problem is that the current symbolic override tracing hack is implemented via Python-level autograd Function, which imposes the \"arguments to forward must be Variable\"s restriction. But there is no fundamental reason for ONNX export that this must be the case, and indeed, for the ATen exports you can see many operations which take non-Variable inputs and use those for export. And *changing* the implementation of a PyTorch autograd function (being used to actually compute backwards) is absolutely a case of tail wagging the dog. In this particular case it doesn't matter (because he net compute is the same) but in some cases it could be very important for an operation to not be hoisted out of forward/backward, because the automatically computed backwards is less efficient than a manually imported one.\r\n\r\nNow, how to fix this without discarding the tracer? Simple: we need a new API. Recall how we implement tracing in ATen (the most recent description is in 27505e642963bb8b8c18abdcc6dbed4d41ffa0db). There's three steps:\r\n\r\n1. We run a `symbolic` on the inputs to the function we want to override\r\n2. Run the function\r\n3. Attach the trace to the returned outputs\r\n\r\n(You have to split it this way to handle inplace). So all we need is to be able to do this from Python, and then the symbolic override hack is a lot simpler, and we can use this to implement tracing for Python-level functions. What do you think."}