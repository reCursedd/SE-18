{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/366100285", "html_url": "https://github.com/pytorch/pytorch/issues/5164#issuecomment-366100285", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5164", "id": 366100285, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjEwMDI4NQ==", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-15T23:45:10Z", "updated_at": "2018-02-16T00:07:47Z", "author_association": "NONE", "body_html": "<p>The use-case was writing a weight loader (in my case a loader from caffemodel):<br>\n<a href=\"https://github.com/vadimkantorov/caffemodel2pytorch/blob/master/caffemodel2pytorch.py#L352\">https://github.com/vadimkantorov/caffemodel2pytorch/blob/master/caffemodel2pytorch.py#L352</a></p>\n<p>I was looking to create a Parameter that shared the device of the original parameter, the type of original parameter and requires_grad.</p>\n<p>What didn't work:</p>\n<ul>\n<li><code>self.weight.resize_as_(my_float_tensor).copy_(my_float_tensor)</code>, <code>self.weight</code> is a cuda nn.Parameter</li>\n<li><code>self.weight = self.weight.new(my_float_tensor.size()).copy_(my_float_tensor)</code></li>\n<li><code>self.weight = nn.Parameter(my_float_tensor).type_as(self.weight)</code></li>\n</ul>\n<p>The version that woked is linked above.</p>\n<p>I expected that the <code>new()</code> would respect the contract of Variable and Tensor's new, which I thought was returning an object of the original type.</p>", "body_text": "The use-case was writing a weight loader (in my case a loader from caffemodel):\nhttps://github.com/vadimkantorov/caffemodel2pytorch/blob/master/caffemodel2pytorch.py#L352\nI was looking to create a Parameter that shared the device of the original parameter, the type of original parameter and requires_grad.\nWhat didn't work:\n\nself.weight.resize_as_(my_float_tensor).copy_(my_float_tensor), self.weight is a cuda nn.Parameter\nself.weight = self.weight.new(my_float_tensor.size()).copy_(my_float_tensor)\nself.weight = nn.Parameter(my_float_tensor).type_as(self.weight)\n\nThe version that woked is linked above.\nI expected that the new() would respect the contract of Variable and Tensor's new, which I thought was returning an object of the original type.", "body": "The use-case was writing a weight loader (in my case a loader from caffemodel):\r\nhttps://github.com/vadimkantorov/caffemodel2pytorch/blob/master/caffemodel2pytorch.py#L352\r\n\r\nI was looking to create a Parameter that shared the device of the original parameter, the type of original parameter and requires_grad.\r\n\r\nWhat didn't work:\r\n* `self.weight.resize_as_(my_float_tensor).copy_(my_float_tensor)`, `self.weight` is a cuda nn.Parameter\r\n* `self.weight = self.weight.new(my_float_tensor.size()).copy_(my_float_tensor)`\r\n* `self.weight = nn.Parameter(my_float_tensor).type_as(self.weight)`\r\n\r\nThe version that woked is linked above.\r\n\r\nI expected that the `new()` would respect the contract of Variable and Tensor's new, which I thought was returning an object of the original type."}