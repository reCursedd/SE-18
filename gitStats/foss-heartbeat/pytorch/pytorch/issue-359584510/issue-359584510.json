{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11585", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11585/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11585/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11585/events", "html_url": "https://github.com/pytorch/pytorch/issues/11585", "id": 359584510, "node_id": "MDU6SXNzdWUzNTk1ODQ1MTA=", "number": 11585, "title": "pytorch gesv gives incorrect result", "user": {"login": "wthrif", "id": 25537356, "node_id": "MDQ6VXNlcjI1NTM3MzU2", "avatar_url": "https://avatars3.githubusercontent.com/u/25537356?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wthrif", "html_url": "https://github.com/wthrif", "followers_url": "https://api.github.com/users/wthrif/followers", "following_url": "https://api.github.com/users/wthrif/following{/other_user}", "gists_url": "https://api.github.com/users/wthrif/gists{/gist_id}", "starred_url": "https://api.github.com/users/wthrif/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wthrif/subscriptions", "organizations_url": "https://api.github.com/users/wthrif/orgs", "repos_url": "https://api.github.com/users/wthrif/repos", "events_url": "https://api.github.com/users/wthrif/events{/privacy}", "received_events_url": "https://api.github.com/users/wthrif/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-12T17:42:29Z", "updated_at": "2018-09-12T22:04:58Z", "closed_at": "2018-09-12T22:04:58Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I'm trying to implement <a href=\"https://stackoverflow.com/questions/29156532/python-baseline-correction-library\" rel=\"nofollow\">baseline als subtraction</a> in pytorch so that I can run it on my GPU but I am running into problems because pytorch.gesv gives a different result than scipy.linalg.spsolve (scipy is correct, pytorch is not).</p>\n<p>Using the dummy input: y = (5,5,5,5,5,10,10,5,5,5,10,10,10,5,5,5,5,5,5,5),</p>\n<p>I get (3.68010263, 4.90344214, 6.12679489, 7.35022406, 8.57384278, 9.79774074, 11.02197199, 12.2465927 , 13.47164891, 14.69711435,15.92287813, 17.14873257, 18.37456982, 19.60038184, 20.82626043,22.05215157, 23.27805103, 24.50400438, 25.73010693, 26.95625922) from scipy and</p>\n<p>(6.4938312 , 6.46912395, 6.44440175, 6.41963499, 6.39477958,6.36977727, 6.34455582, 6.31907933, 6.29334844, 6.26735058, 6.24106029, 6.21443939, 6.18748732, 6.16024137, 6.13277694,6.10515785, 6.07743658, 6.04965455, 6.02184242, 5.99402035) from pytorch.</p>\n<p>This is with just one iteration of the loop.</p>\n<p>Interestingly, if I use a shorter dummy input (5,5,5,5,5,10,10,5,5,5), I get the same answer from both. My real input is 1011 dimensional.</p>\n<h2>Code example</h2>\n<p>scipy code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">baseline_als</span>(<span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">lam</span>, <span class=\"pl-smi\">p</span>, <span class=\"pl-smi\">niter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>):\n  L <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(y)\n  D <span class=\"pl-k\">=</span> sparse.diags([<span class=\"pl-c1\">1</span>,<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>],[<span class=\"pl-c1\">0</span>,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>], <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(L,L<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>))\n  w <span class=\"pl-k\">=</span> np.ones(L)\n  <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(niter):\n    W <span class=\"pl-k\">=</span> sparse.spdiags(w, <span class=\"pl-c1\">0</span>, L, L)\n    Z <span class=\"pl-k\">=</span> W <span class=\"pl-k\">+</span> lam <span class=\"pl-k\">*</span> D.dot(D.transpose())\n    z <span class=\"pl-k\">=</span> spsolve(Z, w<span class=\"pl-k\">*</span>y)\n    w <span class=\"pl-k\">=</span> p <span class=\"pl-k\">*</span> (y <span class=\"pl-k\">&gt;</span> z) <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>p) <span class=\"pl-k\">*</span> (y <span class=\"pl-k\">&lt;</span> z)\n<span class=\"pl-k\">return</span> z</pre></div>\n<p>pytorch code (sorry it looks bad, I'm just learning pytorch):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">baseline_als_pytorch</span>(<span class=\"pl-smi\">y</span>, <span class=\"pl-smi\">lam</span>, <span class=\"pl-smi\">p</span>, <span class=\"pl-smi\">niter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>):\n    diag <span class=\"pl-k\">=</span> torch.tensor(np.repeat(<span class=\"pl-c1\">1</span>, L))\n    diag <span class=\"pl-k\">=</span> torch.diag(diag, <span class=\"pl-c1\">0</span>)\n    diag_minus_one <span class=\"pl-k\">=</span> torch.tensor(np.repeat(<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>, L <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>))\n    diag_minus_one <span class=\"pl-k\">=</span> torch.diag(diag_minus_one, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    diag_minus_two <span class=\"pl-k\">=</span> torch.tensor(np.repeat(<span class=\"pl-c1\">1</span>, L <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span>))\n    diag_minus_two <span class=\"pl-k\">=</span> torch.diag(diag_minus_two, <span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>)\n    D <span class=\"pl-k\">=</span> diag <span class=\"pl-k\">+</span> diag_minus_one <span class=\"pl-k\">+</span> diag_minus_two\n    D <span class=\"pl-k\">=</span> D[:, :L <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span>].double()\n    w <span class=\"pl-k\">=</span> torch.tensor(np.repeat(<span class=\"pl-c1\">1</span>, L)).double()\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n        W <span class=\"pl-k\">=</span> diag.double()\n        Z <span class=\"pl-k\">=</span> W <span class=\"pl-k\">+</span> lam <span class=\"pl-k\">*</span> torch.mm(D, D.permute(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>))\n        z <span class=\"pl-k\">=</span> torch.gesv(w <span class=\"pl-k\">*</span> y, Z)\n        z <span class=\"pl-k\">=</span> z[<span class=\"pl-c1\">0</span>].squeeze()\n        w <span class=\"pl-k\">=</span> p <span class=\"pl-k\">*</span> (y <span class=\"pl-k\">&gt;</span> z).double() <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> p) <span class=\"pl-k\">*</span> (y <span class=\"pl-k\">&lt;</span> z).double()\n    <span class=\"pl-k\">return</span> z</pre></div>\n<p>I've confirmed that Z, w, and y are all the same going into both scipy and pytorch and that z is different between them right after I try to solve the system of equations. It seems to be the same problem as <a href=\"https://github.com/pytorch/pytorch/issues/3558\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3558/hovercard\">here</a>, which was fixed by updating pytorch, but I think I am using the most recent version of pytorch.</p>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>OS: ubuntu 17.10</li>\n<li>PyTorch version: 0.4.1</li>\n<li>Python version: 3.6.3</li>\n<li>CUDA/cuDNN version: 9.0.176</li>\n<li>GPU models and configuration: GeForce GTX 1080 Ti/PCIe/SSE2</li>\n</ul>", "body_text": "Issue description\nI'm trying to implement baseline als subtraction in pytorch so that I can run it on my GPU but I am running into problems because pytorch.gesv gives a different result than scipy.linalg.spsolve (scipy is correct, pytorch is not).\nUsing the dummy input: y = (5,5,5,5,5,10,10,5,5,5,10,10,10,5,5,5,5,5,5,5),\nI get (3.68010263, 4.90344214, 6.12679489, 7.35022406, 8.57384278, 9.79774074, 11.02197199, 12.2465927 , 13.47164891, 14.69711435,15.92287813, 17.14873257, 18.37456982, 19.60038184, 20.82626043,22.05215157, 23.27805103, 24.50400438, 25.73010693, 26.95625922) from scipy and\n(6.4938312 , 6.46912395, 6.44440175, 6.41963499, 6.39477958,6.36977727, 6.34455582, 6.31907933, 6.29334844, 6.26735058, 6.24106029, 6.21443939, 6.18748732, 6.16024137, 6.13277694,6.10515785, 6.07743658, 6.04965455, 6.02184242, 5.99402035) from pytorch.\nThis is with just one iteration of the loop.\nInterestingly, if I use a shorter dummy input (5,5,5,5,5,10,10,5,5,5), I get the same answer from both. My real input is 1011 dimensional.\nCode example\nscipy code:\ndef baseline_als(y, lam, p, niter=10):\n  L = len(y)\n  D = sparse.diags([1,-2,1],[0,-1,-2], shape=(L,L-2))\n  w = np.ones(L)\n  for i in range(niter):\n    W = sparse.spdiags(w, 0, L, L)\n    Z = W + lam * D.dot(D.transpose())\n    z = spsolve(Z, w*y)\n    w = p * (y > z) + (1-p) * (y < z)\nreturn z\npytorch code (sorry it looks bad, I'm just learning pytorch):\ndef baseline_als_pytorch(y, lam, p, niter=10):\n    diag = torch.tensor(np.repeat(1, L))\n    diag = torch.diag(diag, 0)\n    diag_minus_one = torch.tensor(np.repeat(-2, L - 1))\n    diag_minus_one = torch.diag(diag_minus_one, -1)\n    diag_minus_two = torch.tensor(np.repeat(1, L - 2))\n    diag_minus_two = torch.diag(diag_minus_two, -2)\n    D = diag + diag_minus_one + diag_minus_two\n    D = D[:, :L - 2].double()\n    w = torch.tensor(np.repeat(1, L)).double()\n    for i in range(10):\n        W = diag.double()\n        Z = W + lam * torch.mm(D, D.permute(1, 0))\n        z = torch.gesv(w * y, Z)\n        z = z[0].squeeze()\n        w = p * (y > z).double() + (1 - p) * (y < z).double()\n    return z\nI've confirmed that Z, w, and y are all the same going into both scipy and pytorch and that z is different between them right after I try to solve the system of equations. It seems to be the same problem as here, which was fixed by updating pytorch, but I think I am using the most recent version of pytorch.\n\nPyTorch or Caffe2: PyTorch\nHow you installed PyTorch (conda, pip, source): pip\nOS: ubuntu 17.10\nPyTorch version: 0.4.1\nPython version: 3.6.3\nCUDA/cuDNN version: 9.0.176\nGPU models and configuration: GeForce GTX 1080 Ti/PCIe/SSE2", "body": "## Issue description\r\nI'm trying to implement [baseline als subtraction](https://stackoverflow.com/questions/29156532/python-baseline-correction-library) in pytorch so that I can run it on my GPU but I am running into problems because pytorch.gesv gives a different result than scipy.linalg.spsolve (scipy is correct, pytorch is not). \r\n\r\nUsing the dummy input: y = (5,5,5,5,5,10,10,5,5,5,10,10,10,5,5,5,5,5,5,5),\r\n\r\nI get (3.68010263, 4.90344214, 6.12679489, 7.35022406, 8.57384278, 9.79774074, 11.02197199, 12.2465927 , 13.47164891, 14.69711435,15.92287813, 17.14873257, 18.37456982, 19.60038184, 20.82626043,22.05215157, 23.27805103, 24.50400438, 25.73010693, 26.95625922) from scipy and\r\n\r\n(6.4938312 , 6.46912395, 6.44440175, 6.41963499, 6.39477958,6.36977727, 6.34455582, 6.31907933, 6.29334844, 6.26735058, 6.24106029, 6.21443939, 6.18748732, 6.16024137, 6.13277694,6.10515785, 6.07743658, 6.04965455, 6.02184242, 5.99402035) from pytorch.\r\n\r\nThis is with just one iteration of the loop.\r\n\r\nInterestingly, if I use a shorter dummy input (5,5,5,5,5,10,10,5,5,5), I get the same answer from both. My real input is 1011 dimensional.\r\n\r\n\r\n## Code example\r\nscipy code: \r\n```python\r\ndef baseline_als(y, lam, p, niter=10):\r\n  L = len(y)\r\n  D = sparse.diags([1,-2,1],[0,-1,-2], shape=(L,L-2))\r\n  w = np.ones(L)\r\n  for i in range(niter):\r\n    W = sparse.spdiags(w, 0, L, L)\r\n    Z = W + lam * D.dot(D.transpose())\r\n    z = spsolve(Z, w*y)\r\n    w = p * (y > z) + (1-p) * (y < z)\r\nreturn z\r\n```\r\n\r\npytorch code (sorry it looks bad, I'm just learning pytorch):\r\n```python\r\ndef baseline_als_pytorch(y, lam, p, niter=10):\r\n    diag = torch.tensor(np.repeat(1, L))\r\n    diag = torch.diag(diag, 0)\r\n    diag_minus_one = torch.tensor(np.repeat(-2, L - 1))\r\n    diag_minus_one = torch.diag(diag_minus_one, -1)\r\n    diag_minus_two = torch.tensor(np.repeat(1, L - 2))\r\n    diag_minus_two = torch.diag(diag_minus_two, -2)\r\n    D = diag + diag_minus_one + diag_minus_two\r\n    D = D[:, :L - 2].double()\r\n    w = torch.tensor(np.repeat(1, L)).double()\r\n    for i in range(10):\r\n        W = diag.double()\r\n        Z = W + lam * torch.mm(D, D.permute(1, 0))\r\n        z = torch.gesv(w * y, Z)\r\n        z = z[0].squeeze()\r\n        w = p * (y > z).double() + (1 - p) * (y < z).double()\r\n    return z\r\n```\r\nI've confirmed that Z, w, and y are all the same going into both scipy and pytorch and that z is different between them right after I try to solve the system of equations. It seems to be the same problem as [here](https://github.com/pytorch/pytorch/issues/3558), which was fixed by updating pytorch, but I think I am using the most recent version of pytorch.\r\n\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- OS: ubuntu 17.10\r\n- PyTorch version: 0.4.1\r\n- Python version: 3.6.3\r\n- CUDA/cuDNN version: 9.0.176\r\n- GPU models and configuration: GeForce GTX 1080 Ti/PCIe/SSE2"}