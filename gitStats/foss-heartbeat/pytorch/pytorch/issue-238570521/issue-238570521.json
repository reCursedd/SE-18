{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1910", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1910/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1910/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1910/events", "html_url": "https://github.com/pytorch/pytorch/issues/1910", "id": 238570521, "node_id": "MDU6SXNzdWUyMzg1NzA1MjE=", "number": 1910, "title": "torch.expand doesn't  throw an exception in the forward pass when the size arguments are passed as a list", "user": {"login": "qizhex", "id": 2403143, "node_id": "MDQ6VXNlcjI0MDMxNDM=", "avatar_url": "https://avatars1.githubusercontent.com/u/2403143?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qizhex", "html_url": "https://github.com/qizhex", "followers_url": "https://api.github.com/users/qizhex/followers", "following_url": "https://api.github.com/users/qizhex/following{/other_user}", "gists_url": "https://api.github.com/users/qizhex/gists{/gist_id}", "starred_url": "https://api.github.com/users/qizhex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qizhex/subscriptions", "organizations_url": "https://api.github.com/users/qizhex/orgs", "repos_url": "https://api.github.com/users/qizhex/repos", "events_url": "https://api.github.com/users/qizhex/events{/privacy}", "received_events_url": "https://api.github.com/users/qizhex/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-26T14:56:30Z", "updated_at": "2017-07-20T05:46:27Z", "closed_at": "2017-07-20T05:46:27Z", "author_association": "NONE", "body_html": "<p>tensor.expand should be used as t.expand(a, b, c). However, when the size arguments are passed in a list, i.e., t.expand([a, b, c]). Forward is fine but backward fails. The following code shows this issue (from a discussion <a href=\"https://discuss.pytorch.org/t/solved-backward-error-when-using-expand/4273/6\" rel=\"nofollow\">https://discuss.pytorch.org/t/solved-backward-error-when-using-expand/4273/6</a>)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">TwoLayerNet</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">D_in</span>, <span class=\"pl-smi\">H</span>, <span class=\"pl-smi\">D_out</span>, <span class=\"pl-smi\">h</span>, <span class=\"pl-smi\">w</span>):\n    <span class=\"pl-c1\">super</span>(TwoLayerNet, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.linear1 <span class=\"pl-k\">=</span> torch.nn.Linear(D_in, H)\n    <span class=\"pl-c1\">self</span>.linear2 <span class=\"pl-k\">=</span> torch.nn.Linear(H <span class=\"pl-k\">*</span> h <span class=\"pl-k\">*</span> w, D_out)\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n    h_relu <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear1(x).clamp(<span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    h_relu <span class=\"pl-k\">=</span> torch.unsqueeze(torch.unsqueeze(h_relu, <span class=\"pl-c1\">2</span>), <span class=\"pl-c1\">3</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> -&gt; N x H x 1 x 1</span>\n    h_expand <span class=\"pl-k\">=</span> h_relu.expand([<span class=\"pl-c1\">64</span>, H, h, w]).contiguous().view(<span class=\"pl-c1\">64</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> -&gt; N x H x h x w</span>\n    y_pred <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear2(h_expand) <span class=\"pl-c\"><span class=\"pl-c\">#</span> -&gt; N x D_out</span>\n    <span class=\"pl-k\">return</span> y_pred\n\n\n\nx <span class=\"pl-k\">=</span> Variable(torch.randn(N, D_in), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny <span class=\"pl-k\">=</span> Variable(torch.randn(N, D_out), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\nmodel <span class=\"pl-k\">=</span> TwoLayerNet(D_in, H, D_out, h, w)\n\ncriterion <span class=\"pl-k\">=</span> torch.nn.MSELoss(<span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\noptimizer <span class=\"pl-k\">=</span> torch.optim.SGD(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>)\n<span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">500</span>):\n  y_pred <span class=\"pl-k\">=</span> model(x)\n  loss <span class=\"pl-k\">=</span> criterion(y_pred, y)\n  <span class=\"pl-c1\">print</span>(t, loss.data[<span class=\"pl-c1\">0</span>])\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()</pre></div>\n<p>(N is batch size; D_in is input dimension;<br>\nH is hidden dimension; D_out is output dimension.<br>\nN, D_in, H, D_out, h, w = 64, 1000, 100, 10, 6, 6)</p>\n<p>The traceback is as follows:</p>\n<pre><code>Traceback (most recent call last):\nFile \"try.py\", line 31, in \nloss.backward()\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 152, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\nvariables, grad_variables, retain_graph)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/function.py\", line 91, in apply\nreturn self.forwardcls.backward(self, *args)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/pointwise.py\", line 289, in backward\nreturn grad_output * mask, None\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 802, in mul\nreturn self.mul(other)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 311, in mul\nreturn Mul.apply(self, other)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py\", line 48, in forward\nreturn a.mul(b)\nRuntimeError: inconsistent tensor size, expected r_ [1 x 100 x 6 x 6], t [1 x 100 x 6 x 6] and src [64 x 100] to have the same number of elements, but got 3600, 3600 and 6400 elements respectively at /home/qizhe/tool/pytorch/torch/lib/TH/generic/THTensorMath.c:875\n</code></pre>\n<p>The issue exists for both the bleeding-edge version and the stable version torch-0.1.12.post2-cp27. Should forward throw an exception to help debuging?</p>", "body_text": "tensor.expand should be used as t.expand(a, b, c). However, when the size arguments are passed in a list, i.e., t.expand([a, b, c]). Forward is fine but backward fails. The following code shows this issue (from a discussion https://discuss.pytorch.org/t/solved-backward-error-when-using-expand/4273/6)\nimport torch\nfrom torch.autograd import Variable\nclass TwoLayerNet(torch.nn.Module):\n  def __init__(self, D_in, H, D_out, h, w):\n    super(TwoLayerNet, self).__init__()\n    self.linear1 = torch.nn.Linear(D_in, H)\n    self.linear2 = torch.nn.Linear(H * h * w, D_out)\n  def forward(self, x):\n    h_relu = self.linear1(x).clamp(min=0)\n    h_relu = torch.unsqueeze(torch.unsqueeze(h_relu, 2), 3) # -> N x H x 1 x 1\n    h_expand = h_relu.expand([64, H, h, w]).contiguous().view(64, -1) # -> N x H x h x w\n    y_pred = self.linear2(h_expand) # -> N x D_out\n    return y_pred\n\n\n\nx = Variable(torch.randn(N, D_in), requires_grad=True)\ny = Variable(torch.randn(N, D_out), requires_grad=False)\n\nmodel = TwoLayerNet(D_in, H, D_out, h, w)\n\ncriterion = torch.nn.MSELoss(size_average=False)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\nfor t in range(500):\n  y_pred = model(x)\n  loss = criterion(y_pred, y)\n  print(t, loss.data[0])\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n(N is batch size; D_in is input dimension;\nH is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out, h, w = 64, 1000, 100, 10, 6, 6)\nThe traceback is as follows:\nTraceback (most recent call last):\nFile \"try.py\", line 31, in \nloss.backward()\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 152, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\nvariables, grad_variables, retain_graph)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/function.py\", line 91, in apply\nreturn self.forwardcls.backward(self, *args)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/pointwise.py\", line 289, in backward\nreturn grad_output * mask, None\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 802, in mul\nreturn self.mul(other)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 311, in mul\nreturn Mul.apply(self, other)\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py\", line 48, in forward\nreturn a.mul(b)\nRuntimeError: inconsistent tensor size, expected r_ [1 x 100 x 6 x 6], t [1 x 100 x 6 x 6] and src [64 x 100] to have the same number of elements, but got 3600, 3600 and 6400 elements respectively at /home/qizhe/tool/pytorch/torch/lib/TH/generic/THTensorMath.c:875\n\nThe issue exists for both the bleeding-edge version and the stable version torch-0.1.12.post2-cp27. Should forward throw an exception to help debuging?", "body": "tensor.expand should be used as t.expand(a, b, c). However, when the size arguments are passed in a list, i.e., t.expand([a, b, c]). Forward is fine but backward fails. The following code shows this issue (from a discussion https://discuss.pytorch.org/t/solved-backward-error-when-using-expand/4273/6)\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nclass TwoLayerNet(torch.nn.Module):\r\n  def __init__(self, D_in, H, D_out, h, w):\r\n    super(TwoLayerNet, self).__init__()\r\n    self.linear1 = torch.nn.Linear(D_in, H)\r\n    self.linear2 = torch.nn.Linear(H * h * w, D_out)\r\n  def forward(self, x):\r\n    h_relu = self.linear1(x).clamp(min=0)\r\n    h_relu = torch.unsqueeze(torch.unsqueeze(h_relu, 2), 3) # -> N x H x 1 x 1\r\n    h_expand = h_relu.expand([64, H, h, w]).contiguous().view(64, -1) # -> N x H x h x w\r\n    y_pred = self.linear2(h_expand) # -> N x D_out\r\n    return y_pred\r\n\r\n\r\n\r\nx = Variable(torch.randn(N, D_in), requires_grad=True)\r\ny = Variable(torch.randn(N, D_out), requires_grad=False)\r\n\r\nmodel = TwoLayerNet(D_in, H, D_out, h, w)\r\n\r\ncriterion = torch.nn.MSELoss(size_average=False)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\r\nfor t in range(500):\r\n  y_pred = model(x)\r\n  loss = criterion(y_pred, y)\r\n  print(t, loss.data[0])\r\n  optimizer.zero_grad()\r\n  loss.backward()\r\n  optimizer.step()\r\n```\r\n\r\n(N is batch size; D_in is input dimension;\r\n H is hidden dimension; D_out is output dimension.\r\nN, D_in, H, D_out, h, w = 64, 1000, 100, 10, 6, 6)\r\n\r\nThe traceback is as follows:\r\n\r\n```\r\nTraceback (most recent call last):\r\nFile \"try.py\", line 31, in \r\nloss.backward()\r\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 152, in backward\r\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\r\nvariables, grad_variables, retain_graph)\r\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/function.py\", line 91, in apply\r\nreturn self.forwardcls.backward(self, *args)\r\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/pointwise.py\", line 289, in backward\r\nreturn grad_output * mask, None\r\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 802, in mul\r\nreturn self.mul(other)\r\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 311, in mul\r\nreturn Mul.apply(self, other)\r\nFile \"/home/qizhe/tool/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py\", line 48, in forward\r\nreturn a.mul(b)\r\nRuntimeError: inconsistent tensor size, expected r_ [1 x 100 x 6 x 6], t [1 x 100 x 6 x 6] and src [64 x 100] to have the same number of elements, but got 3600, 3600 and 6400 elements respectively at /home/qizhe/tool/pytorch/torch/lib/TH/generic/THTensorMath.c:875\r\n```\r\n\r\nThe issue exists for both the bleeding-edge version and the stable version torch-0.1.12.post2-cp27. Should forward throw an exception to help debuging?"}