{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2843", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2843/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2843/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2843/events", "html_url": "https://github.com/pytorch/pytorch/issues/2843", "id": 260017113, "node_id": "MDU6SXNzdWUyNjAwMTcxMTM=", "number": 2843, "title": "register_hook-modified gradient cannot be applied to optimizer", "user": {"login": "tailintalent", "id": 11731145, "node_id": "MDQ6VXNlcjExNzMxMTQ1", "avatar_url": "https://avatars3.githubusercontent.com/u/11731145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tailintalent", "html_url": "https://github.com/tailintalent", "followers_url": "https://api.github.com/users/tailintalent/followers", "following_url": "https://api.github.com/users/tailintalent/following{/other_user}", "gists_url": "https://api.github.com/users/tailintalent/gists{/gist_id}", "starred_url": "https://api.github.com/users/tailintalent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tailintalent/subscriptions", "organizations_url": "https://api.github.com/users/tailintalent/orgs", "repos_url": "https://api.github.com/users/tailintalent/repos", "events_url": "https://api.github.com/users/tailintalent/events{/privacy}", "received_events_url": "https://api.github.com/users/tailintalent/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-09-23T15:55:56Z", "updated_at": "2017-09-26T02:33:22Z", "closed_at": "2017-09-26T02:20:01Z", "author_association": "NONE", "body_html": "<p>I want to add some Gaussian noise to the gradient before passing the gradient to the optimizer. I tried using register_hooks, as the test code below:</p>\n<pre><code>X = Variable(torch.FloatTensor([4]))\na = Variable(torch.FloatTensor([2]), requires_grad = True)\ny = a * X\ncriterion = nn.MSELoss(size_average = True)\noptimizer = torch.optim.Adam([a], lr = 1e-2)\n\nh = a.register_hook(lambda grad: grad * 10)\noptimizer.zero_grad()\nloss = criterion(y, Variable(torch.FloatTensor([7])))\nloss.backward()\noptimizer.step()\nprint(a)\nprint(a.grad)\n</code></pre>\n<p>But I find that although I can modified the gradient at will, the optimizer always updates the gradient using the original gradient, not the one after register_hook(). Is this behavior what is supposed to be? Or how can I applied the modified gradient to the optimizer?</p>\n<p>Thanks!</p>", "body_text": "I want to add some Gaussian noise to the gradient before passing the gradient to the optimizer. I tried using register_hooks, as the test code below:\nX = Variable(torch.FloatTensor([4]))\na = Variable(torch.FloatTensor([2]), requires_grad = True)\ny = a * X\ncriterion = nn.MSELoss(size_average = True)\noptimizer = torch.optim.Adam([a], lr = 1e-2)\n\nh = a.register_hook(lambda grad: grad * 10)\noptimizer.zero_grad()\nloss = criterion(y, Variable(torch.FloatTensor([7])))\nloss.backward()\noptimizer.step()\nprint(a)\nprint(a.grad)\n\nBut I find that although I can modified the gradient at will, the optimizer always updates the gradient using the original gradient, not the one after register_hook(). Is this behavior what is supposed to be? Or how can I applied the modified gradient to the optimizer?\nThanks!", "body": "I want to add some Gaussian noise to the gradient before passing the gradient to the optimizer. I tried using register_hooks, as the test code below:\r\n\r\n```\r\nX = Variable(torch.FloatTensor([4]))\r\na = Variable(torch.FloatTensor([2]), requires_grad = True)\r\ny = a * X\r\ncriterion = nn.MSELoss(size_average = True)\r\noptimizer = torch.optim.Adam([a], lr = 1e-2)\r\n\r\nh = a.register_hook(lambda grad: grad * 10)\r\noptimizer.zero_grad()\r\nloss = criterion(y, Variable(torch.FloatTensor([7])))\r\nloss.backward()\r\noptimizer.step()\r\nprint(a)\r\nprint(a.grad)\r\n```\r\nBut I find that although I can modified the gradient at will, the optimizer always updates the gradient using the original gradient, not the one after register_hook(). Is this behavior what is supposed to be? Or how can I applied the modified gradient to the optimizer?\r\n\r\nThanks!"}