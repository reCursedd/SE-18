{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/339904369", "html_url": "https://github.com/pytorch/pytorch/issues/1249#issuecomment-339904369", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1249", "id": 339904369, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTkwNDM2OQ==", "user": {"login": "IssamLaradji", "id": 3382128, "node_id": "MDQ6VXNlcjMzODIxMjg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3382128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IssamLaradji", "html_url": "https://github.com/IssamLaradji", "followers_url": "https://api.github.com/users/IssamLaradji/followers", "following_url": "https://api.github.com/users/IssamLaradji/following{/other_user}", "gists_url": "https://api.github.com/users/IssamLaradji/gists{/gist_id}", "starred_url": "https://api.github.com/users/IssamLaradji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IssamLaradji/subscriptions", "organizations_url": "https://api.github.com/users/IssamLaradji/orgs", "repos_url": "https://api.github.com/users/IssamLaradji/repos", "events_url": "https://api.github.com/users/IssamLaradji/events{/privacy}", "received_events_url": "https://api.github.com/users/IssamLaradji/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-27T08:13:38Z", "updated_at": "2017-10-27T08:56:38Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10129358\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/varghesealex90\">@varghesealex90</a> A naive quick way is to apply the dice loss on each channel with a different weight. Note that this does not tie the classes in the output layer as it treats each class independently as a binary problem. But earlier layers would try to learn features that differentiate between the classes.</p>\n<p>Here is an <strong>inefficient</strong> way of doing this,</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">dice_loss</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>):\n    smooth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>.\n    loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>.\n    <span class=\"pl-k\">for</span> c <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n_classes):\n           iflat <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>[:, c ].view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n           tflat <span class=\"pl-k\">=</span> target[:, c].view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n           intersection <span class=\"pl-k\">=</span> (iflat <span class=\"pl-k\">*</span> tflat).sum()\n           \n           w <span class=\"pl-k\">=</span> class_weights[c]\n           loss <span class=\"pl-k\">+=</span> w<span class=\"pl-k\">*</span>(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> ((<span class=\"pl-c1\">2</span>. <span class=\"pl-k\">*</span> intersection <span class=\"pl-k\">+</span> smooth) <span class=\"pl-k\">/</span>\n                             (iflat.sum() <span class=\"pl-k\">+</span> tflat.sum() <span class=\"pl-k\">+</span> smooth)))\n    <span class=\"pl-k\">return</span> loss</pre></div>\n<p>Where <code>class_weights</code> is a list containing the weight for each class and <code>input</code> and <code>target</code> are shaped as <code>(n_batches, n_classes, height, width)</code>. <code>target</code> is assumed to be one-hot encoded.</p>\n<p>With proper vectorization, you can make this run much faster.</p>", "body_text": "@varghesealex90 A naive quick way is to apply the dice loss on each channel with a different weight. Note that this does not tie the classes in the output layer as it treats each class independently as a binary problem. But earlier layers would try to learn features that differentiate between the classes.\nHere is an inefficient way of doing this,\ndef dice_loss(input, target):\n    smooth = 1.\n    loss = 0.\n    for c in range(n_classes):\n           iflat = input[:, c ].view(-1)\n           tflat = target[:, c].view(-1)\n           intersection = (iflat * tflat).sum()\n           \n           w = class_weights[c]\n           loss += w*(1 - ((2. * intersection + smooth) /\n                             (iflat.sum() + tflat.sum() + smooth)))\n    return loss\nWhere class_weights is a list containing the weight for each class and input and target are shaped as (n_batches, n_classes, height, width). target is assumed to be one-hot encoded.\nWith proper vectorization, you can make this run much faster.", "body": "@varghesealex90 A naive quick way is to apply the dice loss on each channel with a different weight. Note that this does not tie the classes in the output layer as it treats each class independently as a binary problem. But earlier layers would try to learn features that differentiate between the classes.\r\n \r\nHere is an **inefficient** way of doing this,\r\n\r\n```python\r\ndef dice_loss(input, target):\r\n    smooth = 1.\r\n    loss = 0.\r\n    for c in range(n_classes):\r\n           iflat = input[:, c ].view(-1)\r\n           tflat = target[:, c].view(-1)\r\n           intersection = (iflat * tflat).sum()\r\n           \r\n           w = class_weights[c]\r\n           loss += w*(1 - ((2. * intersection + smooth) /\r\n                             (iflat.sum() + tflat.sum() + smooth)))\r\n    return loss\r\n```\r\nWhere `class_weights` is a list containing the weight for each class and `input` and `target` are shaped as `(n_batches, n_classes, height, width)`. `target` is assumed to be one-hot encoded.\r\n\r\nWith proper vectorization, you can make this run much faster."}