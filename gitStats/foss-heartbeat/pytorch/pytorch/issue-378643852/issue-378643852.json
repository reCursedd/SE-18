{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13714", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13714/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13714/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13714/events", "html_url": "https://github.com/pytorch/pytorch/issues/13714", "id": 378643852, "node_id": "MDU6SXNzdWUzNzg2NDM4NTI=", "number": 13714, "title": "Full precision display of long tensors", "user": {"login": "mdouze", "id": 25364969, "node_id": "MDQ6VXNlcjI1MzY0OTY5", "avatar_url": "https://avatars1.githubusercontent.com/u/25364969?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mdouze", "html_url": "https://github.com/mdouze", "followers_url": "https://api.github.com/users/mdouze/followers", "following_url": "https://api.github.com/users/mdouze/following{/other_user}", "gists_url": "https://api.github.com/users/mdouze/gists{/gist_id}", "starred_url": "https://api.github.com/users/mdouze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mdouze/subscriptions", "organizations_url": "https://api.github.com/users/mdouze/orgs", "repos_url": "https://api.github.com/users/mdouze/repos", "events_url": "https://api.github.com/users/mdouze/events{/privacy}", "received_events_url": "https://api.github.com/users/mdouze/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-11-08T09:24:05Z", "updated_at": "2018-11-08T17:07:07Z", "closed_at": "2018-11-08T17:07:07Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>It would be nice if long tensors were displayed in full precision instead of scientific notation.</p>\n<h2>Motivation</h2>\n<p>This is the default way that numpy displays int tensors</p>\n<h2>Pitch</h2>\n<p>When I call</p>\n<pre><code>(train_labels != train_labels_0).sum()\n</code></pre>\n<p>I want a count, but I get <code>tensor(2.7653e+05)</code> (on Torch 0.5.0a0+61a15cb). This is particularly annoying for 1-component tensors like here. And it is even more annoying because torch's scientific display format keeps few (5) digits.</p>\n<h2>Alternatives</h2>\n<p>convert the tensor to numpy prior to display</p>", "body_text": "\ud83d\ude80 Feature\nIt would be nice if long tensors were displayed in full precision instead of scientific notation.\nMotivation\nThis is the default way that numpy displays int tensors\nPitch\nWhen I call\n(train_labels != train_labels_0).sum()\n\nI want a count, but I get tensor(2.7653e+05) (on Torch 0.5.0a0+61a15cb). This is particularly annoying for 1-component tensors like here. And it is even more annoying because torch's scientific display format keeps few (5) digits.\nAlternatives\nconvert the tensor to numpy prior to display", "body": "## \ud83d\ude80 Feature\r\n\r\nIt would be nice if long tensors were displayed in full precision instead of scientific notation. \r\n\r\n## Motivation\r\n\r\nThis is the default way that numpy displays int tensors\r\n\r\n## Pitch\r\n\r\nWhen I call \r\n```\r\n(train_labels != train_labels_0).sum()\r\n```\r\nI want a count, but I get `tensor(2.7653e+05)` (on Torch 0.5.0a0+61a15cb). This is particularly annoying for 1-component tensors like here. And it is even more annoying because torch's scientific display format keeps few (5) digits.\r\n\r\n## Alternatives\r\n\r\nconvert the tensor to numpy prior to display\r\n\r\n"}