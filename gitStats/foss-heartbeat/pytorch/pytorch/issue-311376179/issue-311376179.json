{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6278", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6278/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6278/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6278/events", "html_url": "https://github.com/pytorch/pytorch/issues/6278", "id": 311376179, "node_id": "MDU6SXNzdWUzMTEzNzYxNzk=", "number": 6278, "title": "Segmentation fault on loss.grad_fn()", "user": {"login": "pyhh", "id": 29337689, "node_id": "MDQ6VXNlcjI5MzM3Njg5", "avatar_url": "https://avatars3.githubusercontent.com/u/29337689?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pyhh", "html_url": "https://github.com/pyhh", "followers_url": "https://api.github.com/users/pyhh/followers", "following_url": "https://api.github.com/users/pyhh/following{/other_user}", "gists_url": "https://api.github.com/users/pyhh/gists{/gist_id}", "starred_url": "https://api.github.com/users/pyhh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pyhh/subscriptions", "organizations_url": "https://api.github.com/users/pyhh/orgs", "repos_url": "https://api.github.com/users/pyhh/repos", "events_url": "https://api.github.com/users/pyhh/events{/privacy}", "received_events_url": "https://api.github.com/users/pyhh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-04-04T20:18:54Z", "updated_at": "2018-04-05T14:37:49Z", "closed_at": "2018-04-05T14:37:49Z", "author_association": "NONE", "body_html": "<p>In the following code I get a segmentation fault when I evaluate <code>loss.grad_fn()</code>. Any ideas?</p>\n<p>System:</p>\n<ul>\n<li>PyTorch or Caffe2: Pytorch</li>\n<li>OS: Ubuntu 16.04 LTS</li>\n<li>PyTorch version: 0.3.1.post2</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version: 9.0</li>\n<li>GPU models and configuration: Geforce GTX 970</li>\n<li>GCC version (if compiling from source): 5.4.0</li>\n</ul>\n<p>Script:</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square you can only specify a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\nnet = Net()\nprint(net)\n\nparams = list(net.parameters())\nprint(len(params))\nprint(params[0].size())  # conv1's .weight\n\ninput = Variable(torch.randn(1, 1, 32, 32), requires_grad=True)\nout = net(input)\nprint(out)\n\nnet.zero_grad()\nout.backward(torch.randn(1, 10))\n\noutput = net(input)\ntarget = Variable(torch.arange(1, 11))  # a dummy target, for example\ntarget = target.view(1, -1)  # make it the same shape as output\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target)\nprint(loss)\n\nloss.grad_fn()\n</code></pre>\n<p>Trace:</p>\n<pre><code>Thread 1 \"python\" received signal SIGSEGV, Segmentation fault.\ntorch::autograd::generated::MseLossBackward::apply (this=0x555556819f48, grads=...)\n    at torch/csrc/autograd/generated/Functions.cpp:2316\n2316\ttorch/csrc/autograd/generated/Functions.cpp: No such file or directory.\n(gdb) where\n#0  torch::autograd::generated::MseLossBackward::apply (this=0x555556819f48, grads=...)\n    at torch/csrc/autograd/generated/Functions.cpp:2316\n#1  0x00007fffed020c0e in torch::autograd::Function::operator() (inputs=..., this=0x555556819f48)\n    at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/function.h:89\n#2  torch::autograd::(anonymous namespace)::THPCppFunction_call (self=&lt;optimised out&gt;, \n    args=&lt;optimised out&gt;, kwargs=&lt;optimised out&gt;) at torch/csrc/autograd/python_cpp_function.cpp:45\n#3  0x00005555556631bb in _PyObject_FastCallDict ()\n#4  0x00005555556f0d3e in call_function ()\n#5  0x000055555571519a in _PyEval_EvalFrameDefault ()\n#6  0x00005555556eb529 in PyEval_EvalCodeEx ()\n#7  0x00005555556ec2cc in PyEval_EvalCode ()\n#8  0x0000555555768af4 in run_mod ()\n#9  0x0000555555768ef1 in PyRun_FileExFlags ()\n#10 0x00005555557690f4 in PyRun_SimpleFileExFlags ()\n#11 0x000055555576cc28 in Py_Main ()\n#12 0x000055555563471e in main ()\n</code></pre>\n<p>If I change the loss function to <code>loss = ((output - target) ** 2.0).sum()</code>, then I get the following:</p>\n<pre><code>Thread 1 \"python\" received signal SIGSEGV, Segmentation fault.\n0x00007fffeda11285 in torch::autograd::generated::SumBackward0::apply(std::vector&lt;torch::autograd::Variable, std::allocator&lt;torch::autograd::Variable&gt; &gt; const&amp;) ()\n   from /home/user/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n(gdb) bt\n#0  0x00007fffeda11285 in torch::autograd::generated::SumBackward0::apply(std::vector&lt;torch::autograd::Variable, std::allocator&lt;torch::autograd::Variable&gt; &gt; const&amp;) ()\n   from /home/user/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#1  0x00007fffed970aa8 in torch::autograd::(anonymous namespace)::THPCppFunction_call(_object*, _object*, _object*) ()\n   from /home/user/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#2  0x00005555556631bb in _PyObject_FastCallDict ()\n#3  0x00005555556f0d3e in call_function ()\n#4  0x000055555571519a in _PyEval_EvalFrameDefault ()\n#5  0x00005555556eb529 in PyEval_EvalCodeEx ()\n#6  0x00005555556ec2cc in PyEval_EvalCode ()\n#7  0x0000555555768af4 in run_mod ()\n#8  0x0000555555768ef1 in PyRun_FileExFlags ()\n#9  0x00005555557690f4 in PyRun_SimpleFileExFlags ()\n#10 0x000055555576cc28 in Py_Main ()\n#11 0x000055555563471e in main ()\n</code></pre>", "body_text": "In the following code I get a segmentation fault when I evaluate loss.grad_fn(). Any ideas?\nSystem:\n\nPyTorch or Caffe2: Pytorch\nOS: Ubuntu 16.04 LTS\nPyTorch version: 0.3.1.post2\nHow you installed PyTorch (conda, pip, source): conda\nPython version: 3.6.4\nCUDA/cuDNN version: 9.0\nGPU models and configuration: Geforce GTX 970\nGCC version (if compiling from source): 5.4.0\n\nScript:\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        # kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square you can only specify a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\nnet = Net()\nprint(net)\n\nparams = list(net.parameters())\nprint(len(params))\nprint(params[0].size())  # conv1's .weight\n\ninput = Variable(torch.randn(1, 1, 32, 32), requires_grad=True)\nout = net(input)\nprint(out)\n\nnet.zero_grad()\nout.backward(torch.randn(1, 10))\n\noutput = net(input)\ntarget = Variable(torch.arange(1, 11))  # a dummy target, for example\ntarget = target.view(1, -1)  # make it the same shape as output\ncriterion = nn.MSELoss()\n\nloss = criterion(output, target)\nprint(loss)\n\nloss.grad_fn()\n\nTrace:\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\ntorch::autograd::generated::MseLossBackward::apply (this=0x555556819f48, grads=...)\n    at torch/csrc/autograd/generated/Functions.cpp:2316\n2316\ttorch/csrc/autograd/generated/Functions.cpp: No such file or directory.\n(gdb) where\n#0  torch::autograd::generated::MseLossBackward::apply (this=0x555556819f48, grads=...)\n    at torch/csrc/autograd/generated/Functions.cpp:2316\n#1  0x00007fffed020c0e in torch::autograd::Function::operator() (inputs=..., this=0x555556819f48)\n    at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/function.h:89\n#2  torch::autograd::(anonymous namespace)::THPCppFunction_call (self=<optimised out>, \n    args=<optimised out>, kwargs=<optimised out>) at torch/csrc/autograd/python_cpp_function.cpp:45\n#3  0x00005555556631bb in _PyObject_FastCallDict ()\n#4  0x00005555556f0d3e in call_function ()\n#5  0x000055555571519a in _PyEval_EvalFrameDefault ()\n#6  0x00005555556eb529 in PyEval_EvalCodeEx ()\n#7  0x00005555556ec2cc in PyEval_EvalCode ()\n#8  0x0000555555768af4 in run_mod ()\n#9  0x0000555555768ef1 in PyRun_FileExFlags ()\n#10 0x00005555557690f4 in PyRun_SimpleFileExFlags ()\n#11 0x000055555576cc28 in Py_Main ()\n#12 0x000055555563471e in main ()\n\nIf I change the loss function to loss = ((output - target) ** 2.0).sum(), then I get the following:\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\n0x00007fffeda11285 in torch::autograd::generated::SumBackward0::apply(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&) ()\n   from /home/user/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n(gdb) bt\n#0  0x00007fffeda11285 in torch::autograd::generated::SumBackward0::apply(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&) ()\n   from /home/user/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#1  0x00007fffed970aa8 in torch::autograd::(anonymous namespace)::THPCppFunction_call(_object*, _object*, _object*) ()\n   from /home/user/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#2  0x00005555556631bb in _PyObject_FastCallDict ()\n#3  0x00005555556f0d3e in call_function ()\n#4  0x000055555571519a in _PyEval_EvalFrameDefault ()\n#5  0x00005555556eb529 in PyEval_EvalCodeEx ()\n#6  0x00005555556ec2cc in PyEval_EvalCode ()\n#7  0x0000555555768af4 in run_mod ()\n#8  0x0000555555768ef1 in PyRun_FileExFlags ()\n#9  0x00005555557690f4 in PyRun_SimpleFileExFlags ()\n#10 0x000055555576cc28 in Py_Main ()\n#11 0x000055555563471e in main ()", "body": "In the following code I get a segmentation fault when I evaluate `loss.grad_fn()`. Any ideas?\r\n\r\nSystem:\r\n- PyTorch or Caffe2: Pytorch\r\n- OS: Ubuntu 16.04 LTS\r\n- PyTorch version: 0.3.1.post2\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: 9.0\r\n- GPU models and configuration: Geforce GTX 970\r\n- GCC version (if compiling from source): 5.4.0\r\n\r\nScript:\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass Net(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        # 1 input image channel, 6 output channels, 5x5 square convolution\r\n        # kernel\r\n        self.conv1 = nn.Conv2d(1, 6, 5)\r\n        self.conv2 = nn.Conv2d(6, 16, 5)\r\n        # an affine operation: y = Wx + b\r\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\r\n        self.fc2 = nn.Linear(120, 84)\r\n        self.fc3 = nn.Linear(84, 10)\r\n\r\n    def forward(self, x):\r\n        # Max pooling over a (2, 2) window\r\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\r\n        # If the size is a square you can only specify a single number\r\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\r\n        x = x.view(-1, self.num_flat_features(x))\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n        x = self.fc3(x)\r\n        return x\r\n\r\n    def num_flat_features(self, x):\r\n        size = x.size()[1:]  # all dimensions except the batch dimension\r\n        num_features = 1\r\n        for s in size:\r\n            num_features *= s\r\n        return num_features\r\n\r\nnet = Net()\r\nprint(net)\r\n\r\nparams = list(net.parameters())\r\nprint(len(params))\r\nprint(params[0].size())  # conv1's .weight\r\n\r\ninput = Variable(torch.randn(1, 1, 32, 32), requires_grad=True)\r\nout = net(input)\r\nprint(out)\r\n\r\nnet.zero_grad()\r\nout.backward(torch.randn(1, 10))\r\n\r\noutput = net(input)\r\ntarget = Variable(torch.arange(1, 11))  # a dummy target, for example\r\ntarget = target.view(1, -1)  # make it the same shape as output\r\ncriterion = nn.MSELoss()\r\n\r\nloss = criterion(output, target)\r\nprint(loss)\r\n\r\nloss.grad_fn()\r\n```\r\n\r\nTrace:\r\n```\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\ntorch::autograd::generated::MseLossBackward::apply (this=0x555556819f48, grads=...)\r\n    at torch/csrc/autograd/generated/Functions.cpp:2316\r\n2316\ttorch/csrc/autograd/generated/Functions.cpp: No such file or directory.\r\n(gdb) where\r\n#0  torch::autograd::generated::MseLossBackward::apply (this=0x555556819f48, grads=...)\r\n    at torch/csrc/autograd/generated/Functions.cpp:2316\r\n#1  0x00007fffed020c0e in torch::autograd::Function::operator() (inputs=..., this=0x555556819f48)\r\n    at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/function.h:89\r\n#2  torch::autograd::(anonymous namespace)::THPCppFunction_call (self=<optimised out>, \r\n    args=<optimised out>, kwargs=<optimised out>) at torch/csrc/autograd/python_cpp_function.cpp:45\r\n#3  0x00005555556631bb in _PyObject_FastCallDict ()\r\n#4  0x00005555556f0d3e in call_function ()\r\n#5  0x000055555571519a in _PyEval_EvalFrameDefault ()\r\n#6  0x00005555556eb529 in PyEval_EvalCodeEx ()\r\n#7  0x00005555556ec2cc in PyEval_EvalCode ()\r\n#8  0x0000555555768af4 in run_mod ()\r\n#9  0x0000555555768ef1 in PyRun_FileExFlags ()\r\n#10 0x00005555557690f4 in PyRun_SimpleFileExFlags ()\r\n#11 0x000055555576cc28 in Py_Main ()\r\n#12 0x000055555563471e in main ()\r\n```\r\nIf I change the loss function to `loss = ((output - target) ** 2.0).sum()`, then I get the following:\r\n```\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fffeda11285 in torch::autograd::generated::SumBackward0::apply(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&) ()\r\n   from /home/user/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\r\n(gdb) bt\r\n#0  0x00007fffeda11285 in torch::autograd::generated::SumBackward0::apply(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&) ()\r\n   from /home/user/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\r\n#1  0x00007fffed970aa8 in torch::autograd::(anonymous namespace)::THPCppFunction_call(_object*, _object*, _object*) ()\r\n   from /home/user/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\r\n#2  0x00005555556631bb in _PyObject_FastCallDict ()\r\n#3  0x00005555556f0d3e in call_function ()\r\n#4  0x000055555571519a in _PyEval_EvalFrameDefault ()\r\n#5  0x00005555556eb529 in PyEval_EvalCodeEx ()\r\n#6  0x00005555556ec2cc in PyEval_EvalCode ()\r\n#7  0x0000555555768af4 in run_mod ()\r\n#8  0x0000555555768ef1 in PyRun_FileExFlags ()\r\n#9  0x00005555557690f4 in PyRun_SimpleFileExFlags ()\r\n#10 0x000055555576cc28 in Py_Main ()\r\n#11 0x000055555563471e in main ()\r\n```\r\n\r\n"}