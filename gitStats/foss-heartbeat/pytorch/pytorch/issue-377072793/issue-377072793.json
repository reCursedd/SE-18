{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13545", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13545/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13545/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13545/events", "html_url": "https://github.com/pytorch/pytorch/issues/13545", "id": 377072793, "node_id": "MDU6SXNzdWUzNzcwNzI3OTM=", "number": 13545, "title": "KL Divergence for Independent", "user": {"login": "didriknielsen", "id": 20969012, "node_id": "MDQ6VXNlcjIwOTY5MDEy", "avatar_url": "https://avatars1.githubusercontent.com/u/20969012?v=4", "gravatar_id": "", "url": "https://api.github.com/users/didriknielsen", "html_url": "https://github.com/didriknielsen", "followers_url": "https://api.github.com/users/didriknielsen/followers", "following_url": "https://api.github.com/users/didriknielsen/following{/other_user}", "gists_url": "https://api.github.com/users/didriknielsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/didriknielsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/didriknielsen/subscriptions", "organizations_url": "https://api.github.com/users/didriknielsen/orgs", "repos_url": "https://api.github.com/users/didriknielsen/repos", "events_url": "https://api.github.com/users/didriknielsen/events{/privacy}", "received_events_url": "https://api.github.com/users/didriknielsen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 819357941, "node_id": "MDU6TGFiZWw4MTkzNTc5NDE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributions", "name": "distributions", "color": "39d651", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-03T16:57:28Z", "updated_at": "2018-11-05T18:58:19Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>It would be useful to be able to compute the KL divergence between two instances of <code>torch.distributions.Independent</code>.</p>\n<h2>Motivation</h2>\n<p>Currently, computing the KL divergence between two instances of <code>torch.distributions.Independent</code> raises a <code>NotImplementedError</code>, even if the KL divergence is well defined for the base distributions.<br>\nThe example code below illustrates the problem:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.distributions <span class=\"pl-k\">import</span> Normal, Independent, kl\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Normals with batch_shape=(3,) and event_shape=()</span>\ndist1 <span class=\"pl-k\">=</span> Normal(torch.zeros(<span class=\"pl-c1\">3</span>), torch.ones(<span class=\"pl-c1\">3</span>))\ndist2 <span class=\"pl-k\">=</span> Normal(torch.zeros(<span class=\"pl-c1\">3</span>), <span class=\"pl-c1\">5</span><span class=\"pl-k\">*</span>torch.ones(<span class=\"pl-c1\">3</span>))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Normals with batch_shape=() and event_shape=(3,)</span>\ndist1_indep <span class=\"pl-k\">=</span> Independent(dist1, <span class=\"pl-c1\">1</span>)\ndist2_indep <span class=\"pl-k\">=</span> Independent(dist2, <span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Works fine, returns KL with shape (3,), one for each Normal</span>\nkl_divergence <span class=\"pl-k\">=</span> kl.kl_divergence(dist1, dist2) \n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Raises NotImplementedError, should return KL with shape (), the sum of the KL for each Normal</span>\nkl_divergence_indep <span class=\"pl-k\">=</span> kl.kl_divergence(dist1_indep, dist2_indep)</pre></div>\n<p>It would make sense to let <code>kl.kl_divergence</code> for two instances of <code>torch.distributions.Independent</code> return the sum of the KL divergences for each of the base distributions.</p>", "body_text": "\ud83d\ude80 Feature\nIt would be useful to be able to compute the KL divergence between two instances of torch.distributions.Independent.\nMotivation\nCurrently, computing the KL divergence between two instances of torch.distributions.Independent raises a NotImplementedError, even if the KL divergence is well defined for the base distributions.\nThe example code below illustrates the problem:\nimport torch\nfrom torch.distributions import Normal, Independent, kl\n\n# Normals with batch_shape=(3,) and event_shape=()\ndist1 = Normal(torch.zeros(3), torch.ones(3))\ndist2 = Normal(torch.zeros(3), 5*torch.ones(3))\n\n# Normals with batch_shape=() and event_shape=(3,)\ndist1_indep = Independent(dist1, 1)\ndist2_indep = Independent(dist2, 1)\n\n# Works fine, returns KL with shape (3,), one for each Normal\nkl_divergence = kl.kl_divergence(dist1, dist2) \n\n# Raises NotImplementedError, should return KL with shape (), the sum of the KL for each Normal\nkl_divergence_indep = kl.kl_divergence(dist1_indep, dist2_indep)\nIt would make sense to let kl.kl_divergence for two instances of torch.distributions.Independent return the sum of the KL divergences for each of the base distributions.", "body": "## \ud83d\ude80 Feature\r\n\r\nIt would be useful to be able to compute the KL divergence between two instances of `torch.distributions.Independent`.\r\n\r\n## Motivation\r\n\r\nCurrently, computing the KL divergence between two instances of `torch.distributions.Independent` raises a `NotImplementedError`, even if the KL divergence is well defined for the base distributions.  \r\nThe example code below illustrates the problem:\r\n```python\r\nimport torch\r\nfrom torch.distributions import Normal, Independent, kl\r\n\r\n# Normals with batch_shape=(3,) and event_shape=()\r\ndist1 = Normal(torch.zeros(3), torch.ones(3))\r\ndist2 = Normal(torch.zeros(3), 5*torch.ones(3))\r\n\r\n# Normals with batch_shape=() and event_shape=(3,)\r\ndist1_indep = Independent(dist1, 1)\r\ndist2_indep = Independent(dist2, 1)\r\n\r\n# Works fine, returns KL with shape (3,), one for each Normal\r\nkl_divergence = kl.kl_divergence(dist1, dist2) \r\n\r\n# Raises NotImplementedError, should return KL with shape (), the sum of the KL for each Normal\r\nkl_divergence_indep = kl.kl_divergence(dist1_indep, dist2_indep)\r\n```\r\nIt would make sense to let `kl.kl_divergence` for two instances of `torch.distributions.Independent` return the sum of the KL divergences for each of the base distributions. "}