{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383414577", "html_url": "https://github.com/pytorch/pytorch/issues/3076#issuecomment-383414577", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3076", "id": 383414577, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzQxNDU3Nw==", "user": {"login": "fartashf", "id": 7307513, "node_id": "MDQ6VXNlcjczMDc1MTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/7307513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fartashf", "html_url": "https://github.com/fartashf", "followers_url": "https://api.github.com/users/fartashf/followers", "following_url": "https://api.github.com/users/fartashf/following{/other_user}", "gists_url": "https://api.github.com/users/fartashf/gists{/gist_id}", "starred_url": "https://api.github.com/users/fartashf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fartashf/subscriptions", "organizations_url": "https://api.github.com/users/fartashf/orgs", "repos_url": "https://api.github.com/users/fartashf/repos", "events_url": "https://api.github.com/users/fartashf/events{/privacy}", "received_events_url": "https://api.github.com/users/fartashf/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-22T21:41:20Z", "updated_at": "2018-04-22T21:41:20Z", "author_association": "NONE", "body_html": "<p>Is this issue fully resolved? I'm doing jacobian vector product on <code>vgg19</code> vs <code>vgg19_bn</code> and it is very slow on <code>vgg19_bn</code>. That is aside from a memory issue with <code>vgg19_bn</code> that forces me to use a batch_size of 8.</p>\n<p>The output I get from the code below:</p>\n<pre><code>vgg19 forward: 0.0225s\nvgg19 grad 1: 0.0212s\nvgg19 grad 2: 0.0081s\nvgg19_bn forward: 0.0249s\nvgg19_bn grad 1: 0.0259s\nvgg19_bn grad 2: 0.7063s\n</code></pre>\n<p>Here is the code I use to test:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> torchvision.models <span class=\"pl-k\">as</span> models\n<span class=\"pl-k\">import</span> time\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">dbp</span>(<span class=\"pl-smi\">model</span>):\n    retain_graph <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n    W <span class=\"pl-k\">=</span> model.parameters()\n    batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\n    x <span class=\"pl-k\">=</span> Variable(torch.randn(batch_size, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)).cuda()\n    t <span class=\"pl-k\">=</span> Variable(torch.ones(batch_size).long()).cuda()\n\n    y <span class=\"pl-k\">=</span> model(x)\n    tic <span class=\"pl-k\">=</span> time.time()\n    loss_ex <span class=\"pl-k\">=</span> F.nll_loss(y, t, <span class=\"pl-v\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)<span class=\"pl-k\">/</span>batch_size\n    loss <span class=\"pl-k\">=</span> loss_ex.sum()\n    t0 <span class=\"pl-k\">=</span> time.time()<span class=\"pl-k\">-</span>tic\n\n    tic <span class=\"pl-k\">=</span> time.time()\n    v <span class=\"pl-k\">=</span> Variable(torch.ones_like(loss_ex.data), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>).cuda()\n    grad_params <span class=\"pl-k\">=</span> torch.autograd.grad(loss_ex, W, v,\n                                      <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span>retain_graph)\n    t1 <span class=\"pl-k\">=</span> time.time()<span class=\"pl-k\">-</span>tic\n    m <span class=\"pl-k\">=</span> [torch.ones_like(g) <span class=\"pl-k\">for</span> g <span class=\"pl-k\">in</span> grad_params]\n    tic <span class=\"pl-k\">=</span> time.time()\n    jvp <span class=\"pl-k\">=</span> torch.autograd.grad(grad_params, v, m,\n                              <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span>retain_graph)[<span class=\"pl-c1\">0</span>]\n    t2 <span class=\"pl-k\">=</span> time.time()<span class=\"pl-k\">-</span>tic\n    <span class=\"pl-k\">del</span> loss, jvp\n    <span class=\"pl-k\">return</span> t0, t1, t2\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_dbp</span>(<span class=\"pl-smi\">arch</span>):\n    t0 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    t1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    t2 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    num <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    model <span class=\"pl-k\">=</span> models.<span class=\"pl-c1\">__dict__</span>[arch]()\n    model.cuda()\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num):\n        ttt <span class=\"pl-k\">=</span> dbp(model)\n        t0 <span class=\"pl-k\">+=</span> ttt[<span class=\"pl-c1\">0</span>]\n        t1 <span class=\"pl-k\">+=</span> ttt[<span class=\"pl-c1\">1</span>]\n        t2 <span class=\"pl-k\">+=</span> ttt[<span class=\"pl-c1\">2</span>]\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%s</span> forward: <span class=\"pl-c1\">%.4f</span>s<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (arch, t0<span class=\"pl-k\">/</span>num))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%s</span> grad 1: <span class=\"pl-c1\">%.4f</span>s<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (arch, t1<span class=\"pl-k\">/</span>num))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%s</span> grad 2: <span class=\"pl-c1\">%.4f</span>s<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (arch, t2<span class=\"pl-k\">/</span>num))\n\ntest_dbp(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>vgg19<span class=\"pl-pds\">'</span></span>)\ntest_dbp(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>vgg19_bn<span class=\"pl-pds\">'</span></span>)</pre></div>", "body_text": "Is this issue fully resolved? I'm doing jacobian vector product on vgg19 vs vgg19_bn and it is very slow on vgg19_bn. That is aside from a memory issue with vgg19_bn that forces me to use a batch_size of 8.\nThe output I get from the code below:\nvgg19 forward: 0.0225s\nvgg19 grad 1: 0.0212s\nvgg19 grad 2: 0.0081s\nvgg19_bn forward: 0.0249s\nvgg19_bn grad 1: 0.0259s\nvgg19_bn grad 2: 0.7063s\n\nHere is the code I use to test:\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport time\n\n\ndef dbp(model):\n    retain_graph = False\n    W = model.parameters()\n    batch_size = 8\n    x = Variable(torch.randn(batch_size, 3, 224, 224)).cuda()\n    t = Variable(torch.ones(batch_size).long()).cuda()\n\n    y = model(x)\n    tic = time.time()\n    loss_ex = F.nll_loss(y, t, reduce=False)/batch_size\n    loss = loss_ex.sum()\n    t0 = time.time()-tic\n\n    tic = time.time()\n    v = Variable(torch.ones_like(loss_ex.data), requires_grad=True).cuda()\n    grad_params = torch.autograd.grad(loss_ex, W, v,\n                                      retain_graph=retain_graph)\n    t1 = time.time()-tic\n    m = [torch.ones_like(g) for g in grad_params]\n    tic = time.time()\n    jvp = torch.autograd.grad(grad_params, v, m,\n                              retain_graph=retain_graph)[0]\n    t2 = time.time()-tic\n    del loss, jvp\n    return t0, t1, t2\n\n\ndef test_dbp(arch):\n    t0 = 0\n    t1 = 0\n    t2 = 0\n    num = 10\n    model = models.__dict__[arch]()\n    model.cuda()\n    for i in range(num):\n        ttt = dbp(model)\n        t0 += ttt[0]\n        t1 += ttt[1]\n        t2 += ttt[2]\n\n    print('%s forward: %.4fs' % (arch, t0/num))\n    print('%s grad 1: %.4fs' % (arch, t1/num))\n    print('%s grad 2: %.4fs' % (arch, t2/num))\n\ntest_dbp('vgg19')\ntest_dbp('vgg19_bn')", "body": "Is this issue fully resolved? I'm doing jacobian vector product on `vgg19` vs `vgg19_bn` and it is very slow on `vgg19_bn`. That is aside from a memory issue with `vgg19_bn` that forces me to use a batch_size of 8.\r\n\r\nThe output I get from the code below:\r\n```\r\nvgg19 forward: 0.0225s\r\nvgg19 grad 1: 0.0212s\r\nvgg19 grad 2: 0.0081s\r\nvgg19_bn forward: 0.0249s\r\nvgg19_bn grad 1: 0.0259s\r\nvgg19_bn grad 2: 0.7063s\r\n```\r\n\r\nHere is the code I use to test:\r\n\r\n```python\r\nfrom torch.autograd import Variable\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport torchvision.models as models\r\nimport time\r\n\r\n\r\ndef dbp(model):\r\n    retain_graph = False\r\n    W = model.parameters()\r\n    batch_size = 8\r\n    x = Variable(torch.randn(batch_size, 3, 224, 224)).cuda()\r\n    t = Variable(torch.ones(batch_size).long()).cuda()\r\n\r\n    y = model(x)\r\n    tic = time.time()\r\n    loss_ex = F.nll_loss(y, t, reduce=False)/batch_size\r\n    loss = loss_ex.sum()\r\n    t0 = time.time()-tic\r\n\r\n    tic = time.time()\r\n    v = Variable(torch.ones_like(loss_ex.data), requires_grad=True).cuda()\r\n    grad_params = torch.autograd.grad(loss_ex, W, v,\r\n                                      retain_graph=retain_graph)\r\n    t1 = time.time()-tic\r\n    m = [torch.ones_like(g) for g in grad_params]\r\n    tic = time.time()\r\n    jvp = torch.autograd.grad(grad_params, v, m,\r\n                              retain_graph=retain_graph)[0]\r\n    t2 = time.time()-tic\r\n    del loss, jvp\r\n    return t0, t1, t2\r\n\r\n\r\ndef test_dbp(arch):\r\n    t0 = 0\r\n    t1 = 0\r\n    t2 = 0\r\n    num = 10\r\n    model = models.__dict__[arch]()\r\n    model.cuda()\r\n    for i in range(num):\r\n        ttt = dbp(model)\r\n        t0 += ttt[0]\r\n        t1 += ttt[1]\r\n        t2 += ttt[2]\r\n\r\n    print('%s forward: %.4fs' % (arch, t0/num))\r\n    print('%s grad 1: %.4fs' % (arch, t1/num))\r\n    print('%s grad 2: %.4fs' % (arch, t2/num))\r\n\r\ntest_dbp('vgg19')\r\ntest_dbp('vgg19_bn')\r\n```"}