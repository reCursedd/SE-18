{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/335924855", "html_url": "https://github.com/pytorch/pytorch/issues/3076#issuecomment-335924855", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3076", "id": 335924855, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTkyNDg1NQ==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-11T19:40:48Z", "updated_at": "2017-10-11T19:40:48Z", "author_association": "MEMBER", "body_html": "<p>This might probably happen because double backprop in <code>BatchNorm</code> is <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/batchnorm_double_backwards.py\">entirely implemented in python</a> for the moment, and does a lot of autograd operations, while double backprop <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/convolution.cpp#L454-L632\">in <code>Conv</code> is written in C++</a> and calls into the (optimized) <code>Conv</code> operations (written using cudnn).</p>\n<p>There are plans to move some operations to C++, but I'm not sure about the time tables for double backprop for batch norm.</p>", "body_text": "This might probably happen because double backprop in BatchNorm is entirely implemented in python for the moment, and does a lot of autograd operations, while double backprop in Conv is written in C++ and calls into the (optimized) Conv operations (written using cudnn).\nThere are plans to move some operations to C++, but I'm not sure about the time tables for double backprop for batch norm.", "body": "This might probably happen because double backprop in `BatchNorm` is [entirely implemented in python](https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/batchnorm_double_backwards.py) for the moment, and does a lot of autograd operations, while double backprop [in `Conv` is written in C++](https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/convolution.cpp#L454-L632) and calls into the (optimized) `Conv` operations (written using cudnn).\r\n\r\nThere are plans to move some operations to C++, but I'm not sure about the time tables for double backprop for batch norm."}