{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3076", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3076/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3076/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3076/events", "html_url": "https://github.com/pytorch/pytorch/issues/3076", "id": 264688956, "node_id": "MDU6SXNzdWUyNjQ2ODg5NTY=", "number": 3076, "title": "BN slows down double-backprop enormously", "user": {"login": "cjsg", "id": 28411935, "node_id": "MDQ6VXNlcjI4NDExOTM1", "avatar_url": "https://avatars0.githubusercontent.com/u/28411935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cjsg", "html_url": "https://github.com/cjsg", "followers_url": "https://api.github.com/users/cjsg/followers", "following_url": "https://api.github.com/users/cjsg/following{/other_user}", "gists_url": "https://api.github.com/users/cjsg/gists{/gist_id}", "starred_url": "https://api.github.com/users/cjsg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cjsg/subscriptions", "organizations_url": "https://api.github.com/users/cjsg/orgs", "repos_url": "https://api.github.com/users/cjsg/repos", "events_url": "https://api.github.com/users/cjsg/events{/privacy}", "received_events_url": "https://api.github.com/users/cjsg/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2017-10-11T18:28:22Z", "updated_at": "2018-04-24T18:43:00Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>When using a ConvNet without batchnorm layers, the optimization with a gradient penalty takes approximately 5 times longer than without the gradient penalty. This ratio 1:5 corresponds exactly to the ratio expected by the double-backpropagation algorithm. But when the ConvNet has batchnorm layers, this ratio goes up to approximately 1:30.</p>\n<p>Note that these ratios are independent of the number of layers.<br>\nFor instance, the output of the following code with 3 conv layers (n_layers=3) gives:</p>\n<blockquote>\n<p>No BN and no lambda<br>\nepoch  0 loss 2.30 time 9<br>\nNo BN and with lambda<br>\nepoch  0 loss 2.30 time 50<br>\nWith BN and no lambda<br>\nepoch  0 loss 2.30 time 12<br>\nWith BN and with lambda<br>\nepoch  0 loss 2.30 time 305</p>\n</blockquote>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch                                                                                                                                                                                                                                                                                                                                                                \n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> torch.backends.cudnn <span class=\"pl-k\">as</span> cudnn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> time\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Identity</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>):\n        <span class=\"pl-c1\">super</span>(Identity, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>): \n        <span class=\"pl-k\">return</span> x\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Layer</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">in_planes</span>, <span class=\"pl-smi\">planes</span>, <span class=\"pl-smi\">BN</span>):\n        <span class=\"pl-c1\">super</span>(Layer, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.bn <span class=\"pl-k\">=</span> BN(planes)\n        <span class=\"pl-c1\">self</span>.conv <span class=\"pl-k\">=</span> nn.Conv2d(in_planes, planes, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        <span class=\"pl-c1\">self</span>.act <span class=\"pl-k\">=</span> nn.ReLU()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>): \n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.act(<span class=\"pl-c1\">self</span>.bn(<span class=\"pl-c1\">self</span>.conv(x)))\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">BN</span>, <span class=\"pl-smi\">n_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.in_planes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span> \n        <span class=\"pl-c1\">self</span>.layers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._make_layers(Layer,  <span class=\"pl-c1\">64</span>, n_layers, <span class=\"pl-c1\">BN</span>) \n        <span class=\"pl-c1\">self</span>.linear <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">10</span>) \n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_make_layers</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">block</span>, <span class=\"pl-smi\">planes</span>, <span class=\"pl-smi\">num_blocks</span>, <span class=\"pl-smi\">BN</span>):\n        strides <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">+</span> [<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> (num_blocks <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>)\n        layers <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> stride <span class=\"pl-k\">in</span> strides:\n            layers.append(block(<span class=\"pl-c1\">self</span>.in_planes, planes, <span class=\"pl-c1\">BN</span>))\n            <span class=\"pl-c1\">self</span>.in_planes <span class=\"pl-k\">=</span> planes\n        <span class=\"pl-k\">return</span> nn.Sequential(<span class=\"pl-k\">*</span>layers)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>): \n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.layers(x)\n        out <span class=\"pl-k\">=</span> F.avg_pool2d(out, out.size(<span class=\"pl-c1\">2</span>))\n        out <span class=\"pl-k\">=</span> out.view(out.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>) \n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.linear(out)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">do_epoch</span>(<span class=\"pl-smi\">net</span>, <span class=\"pl-smi\">criterion</span>, <span class=\"pl-smi\">optimizer</span>, <span class=\"pl-smi\">lam</span>):\n    net.train()\n    aggr_loss <span class=\"pl-k\">=</span> count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span> \n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n        inputs <span class=\"pl-k\">=</span> Variable(torch.cuda.FloatTensor(<span class=\"pl-c1\">128</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>).normal_(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        targets <span class=\"pl-k\">=</span> Variable(torch.LongTensor(<span class=\"pl-c1\">128</span>).random_(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">10</span>).cuda())\n        outputs <span class=\"pl-k\">=</span> net(inputs)\n\n        optimizer.zero_grad()\n        loss <span class=\"pl-k\">=</span> criterion(outputs, targets)\n        loss.backward(<span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span>(lam <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>)) \n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> gradient penalty</span>\n        <span class=\"pl-k\">if</span> lam <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n            gpenalty <span class=\"pl-k\">=</span> inputs.grad.view(inputs.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>).add(<span class=\"pl-c1\">1e-5</span>).norm(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>).mean()\n            (lam <span class=\"pl-k\">*</span> gpenalty).backward()\n\n        optimizer.step()\n\n        count <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        aggr_loss <span class=\"pl-k\">+=</span> loss.data[<span class=\"pl-c1\">0</span>]\n\n    <span class=\"pl-k\">return</span> aggr_loss <span class=\"pl-k\">/</span> count\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">net</span>, <span class=\"pl-smi\">lam</span>):\n    net.cuda()\n    cudnn.benchmark <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n    criterion <span class=\"pl-k\">=</span> torch.nn.CrossEntropyLoss()\n    criterion.cuda()\n\n    optimizer <span class=\"pl-k\">=</span> torch.optim.SGD(net.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">.001</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)\n\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>):\n        time_start <span class=\"pl-k\">=</span> time.time()\n\n        loss <span class=\"pl-k\">=</span> do_epoch(net, criterion, optimizer, lam)\n\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>epoch <span class=\"pl-c1\">%2d</span> loss <span class=\"pl-c1\">%.2f</span> time <span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (epoch, loss, time.time()<span class=\"pl-k\">-</span>time_start))\n\nn_layers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span> \n<span class=\"pl-c1\">BN</span> <span class=\"pl-k\">=</span> Identity\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>No BN and no lambda<span class=\"pl-pds\">'</span></span>)\nmain(Net(<span class=\"pl-c1\">BN</span>, n_layers), <span class=\"pl-c1\">0</span>.) \n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>No BN and with lambda<span class=\"pl-pds\">'</span></span>)\nmain(Net(<span class=\"pl-c1\">BN</span>, n_layers), <span class=\"pl-c1\">.001</span>)\n\n<span class=\"pl-c1\">BN</span> <span class=\"pl-k\">=</span> nn.BatchNorm2d\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>With BN and no lambda<span class=\"pl-pds\">'</span></span>)\nmain(Net(<span class=\"pl-c1\">BN</span>, n_layers), <span class=\"pl-c1\">0</span>.)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>With BN and with lambda<span class=\"pl-pds\">'</span></span>)\nmain(Net(<span class=\"pl-c1\">BN</span>, n_layers), <span class=\"pl-c1\">.001</span>)</pre></div>", "body_text": "When using a ConvNet without batchnorm layers, the optimization with a gradient penalty takes approximately 5 times longer than without the gradient penalty. This ratio 1:5 corresponds exactly to the ratio expected by the double-backpropagation algorithm. But when the ConvNet has batchnorm layers, this ratio goes up to approximately 1:30.\nNote that these ratios are independent of the number of layers.\nFor instance, the output of the following code with 3 conv layers (n_layers=3) gives:\n\nNo BN and no lambda\nepoch  0 loss 2.30 time 9\nNo BN and with lambda\nepoch  0 loss 2.30 time 50\nWith BN and no lambda\nepoch  0 loss 2.30 time 12\nWith BN and with lambda\nepoch  0 loss 2.30 time 305\n\nimport torch                                                                                                                                                                                                                                                                                                                                                                \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nimport time\n\n\nclass Identity(nn.Module):\n    def __init__(self, *args):\n        super(Identity, self).__init__()\n\n    def forward(self, x): \n        return x\n\nclass Layer(nn.Module):\n    def __init__(self, in_planes, planes, BN):\n        super(Layer, self).__init__()\n        self.bn = BN(planes)\n        self.conv = nn.Conv2d(in_planes, planes, 3, padding=1, bias=False)\n        self.act = nn.ReLU()\n\n    def forward(self, x): \n        return self.act(self.bn(self.conv(x)))\n\nclass Net(nn.Module):\n    def __init__(self, BN, n_layers=3):\n        super(Net, self).__init__()\n        self.in_planes = 3 \n        self.layers = self._make_layers(Layer,  64, n_layers, BN) \n        self.linear = nn.Linear(64, 10) \n\n    def _make_layers(self, block, planes, num_blocks, BN):\n        strides = [1] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, BN))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x): \n        out = self.layers(x)\n        out = F.avg_pool2d(out, out.size(2))\n        out = out.view(out.size(0), -1) \n        return self.linear(out)\n\ndef do_epoch(net, criterion, optimizer, lam):\n    net.train()\n    aggr_loss = count = 0 \n    for _ in range(1000):\n        inputs = Variable(torch.cuda.FloatTensor(128,3,32,32).normal_(), requires_grad=True)\n        targets = Variable(torch.LongTensor(128).random_(0, 10).cuda())\n        outputs = net(inputs)\n\n        optimizer.zero_grad()\n        loss = criterion(outputs, targets)\n        loss.backward(create_graph=(lam > 0)) \n\n        # gradient penalty\n        if lam > 0:\n            gpenalty = inputs.grad.view(inputs.size(0), -1).add(1e-5).norm(1, 1).mean()\n            (lam * gpenalty).backward()\n\n        optimizer.step()\n\n        count += 1\n        aggr_loss += loss.data[0]\n\n    return aggr_loss / count\n\ndef main(net, lam):\n    net.cuda()\n    cudnn.benchmark = True\n\n    criterion = torch.nn.CrossEntropyLoss()\n    criterion.cuda()\n\n    optimizer = torch.optim.SGD(net.parameters(), lr=.001, momentum=0.9)\n\n    for epoch in range(1):\n        time_start = time.time()\n\n        loss = do_epoch(net, criterion, optimizer, lam)\n\n        print(\"epoch %2d loss %.2f time %d\" % (epoch, loss, time.time()-time_start))\n\nn_layers = 3 \nBN = Identity\nprint('No BN and no lambda')\nmain(Net(BN, n_layers), 0.) \n\nprint('No BN and with lambda')\nmain(Net(BN, n_layers), .001)\n\nBN = nn.BatchNorm2d\nprint('With BN and no lambda')\nmain(Net(BN, n_layers), 0.)\n\nprint('With BN and with lambda')\nmain(Net(BN, n_layers), .001)", "body": "When using a ConvNet without batchnorm layers, the optimization with a gradient penalty takes approximately 5 times longer than without the gradient penalty. This ratio 1:5 corresponds exactly to the ratio expected by the double-backpropagation algorithm. But when the ConvNet has batchnorm layers, this ratio goes up to approximately 1:30.\r\n\r\nNote that these ratios are independent of the number of layers.\r\nFor instance, the output of the following code with 3 conv layers (n_layers=3) gives:\r\n\r\n> No BN and no lambda\r\n> epoch  0 loss 2.30 time 9\r\n> No BN and with lambda\r\n> epoch  0 loss 2.30 time 50\r\n> With BN and no lambda\r\n> epoch  0 loss 2.30 time 12\r\n> With BN and with lambda\r\n> epoch  0 loss 2.30 time 305\r\n\r\n\r\n```python\r\nimport torch                                                                                                                                                                                                                                                                                                                                                                \r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.backends.cudnn as cudnn\r\nfrom torch.autograd import Variable\r\nimport time\r\n\r\n\r\nclass Identity(nn.Module):\r\n    def __init__(self, *args):\r\n        super(Identity, self).__init__()\r\n\r\n    def forward(self, x): \r\n        return x\r\n\r\nclass Layer(nn.Module):\r\n    def __init__(self, in_planes, planes, BN):\r\n        super(Layer, self).__init__()\r\n        self.bn = BN(planes)\r\n        self.conv = nn.Conv2d(in_planes, planes, 3, padding=1, bias=False)\r\n        self.act = nn.ReLU()\r\n\r\n    def forward(self, x): \r\n        return self.act(self.bn(self.conv(x)))\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self, BN, n_layers=3):\r\n        super(Net, self).__init__()\r\n        self.in_planes = 3 \r\n        self.layers = self._make_layers(Layer,  64, n_layers, BN) \r\n        self.linear = nn.Linear(64, 10) \r\n\r\n    def _make_layers(self, block, planes, num_blocks, BN):\r\n        strides = [1] + [1] * (num_blocks - 1)\r\n        layers = []\r\n        for stride in strides:\r\n            layers.append(block(self.in_planes, planes, BN))\r\n            self.in_planes = planes\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x): \r\n        out = self.layers(x)\r\n        out = F.avg_pool2d(out, out.size(2))\r\n        out = out.view(out.size(0), -1) \r\n        return self.linear(out)\r\n\r\ndef do_epoch(net, criterion, optimizer, lam):\r\n    net.train()\r\n    aggr_loss = count = 0 \r\n    for _ in range(1000):\r\n        inputs = Variable(torch.cuda.FloatTensor(128,3,32,32).normal_(), requires_grad=True)\r\n        targets = Variable(torch.LongTensor(128).random_(0, 10).cuda())\r\n        outputs = net(inputs)\r\n\r\n        optimizer.zero_grad()\r\n        loss = criterion(outputs, targets)\r\n        loss.backward(create_graph=(lam > 0)) \r\n\r\n        # gradient penalty\r\n        if lam > 0:\r\n            gpenalty = inputs.grad.view(inputs.size(0), -1).add(1e-5).norm(1, 1).mean()\r\n            (lam * gpenalty).backward()\r\n\r\n        optimizer.step()\r\n\r\n        count += 1\r\n        aggr_loss += loss.data[0]\r\n\r\n    return aggr_loss / count\r\n\r\ndef main(net, lam):\r\n    net.cuda()\r\n    cudnn.benchmark = True\r\n\r\n    criterion = torch.nn.CrossEntropyLoss()\r\n    criterion.cuda()\r\n\r\n    optimizer = torch.optim.SGD(net.parameters(), lr=.001, momentum=0.9)\r\n\r\n    for epoch in range(1):\r\n        time_start = time.time()\r\n\r\n        loss = do_epoch(net, criterion, optimizer, lam)\r\n\r\n        print(\"epoch %2d loss %.2f time %d\" % (epoch, loss, time.time()-time_start))\r\n\r\nn_layers = 3 \r\nBN = Identity\r\nprint('No BN and no lambda')\r\nmain(Net(BN, n_layers), 0.) \r\n\r\nprint('No BN and with lambda')\r\nmain(Net(BN, n_layers), .001)\r\n\r\nBN = nn.BatchNorm2d\r\nprint('With BN and no lambda')\r\nmain(Net(BN, n_layers), 0.)\r\n\r\nprint('With BN and with lambda')\r\nmain(Net(BN, n_layers), .001)\r\n```"}