{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386204347", "html_url": "https://github.com/pytorch/pytorch/issues/7209#issuecomment-386204347", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7209", "id": 386204347, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjIwNDM0Nw==", "user": {"login": "vfdev-5", "id": 2459423, "node_id": "MDQ6VXNlcjI0NTk0MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2459423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vfdev-5", "html_url": "https://github.com/vfdev-5", "followers_url": "https://api.github.com/users/vfdev-5/followers", "following_url": "https://api.github.com/users/vfdev-5/following{/other_user}", "gists_url": "https://api.github.com/users/vfdev-5/gists{/gist_id}", "starred_url": "https://api.github.com/users/vfdev-5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vfdev-5/subscriptions", "organizations_url": "https://api.github.com/users/vfdev-5/orgs", "repos_url": "https://api.github.com/users/vfdev-5/repos", "events_url": "https://api.github.com/users/vfdev-5/events{/privacy}", "received_events_url": "https://api.github.com/users/vfdev-5/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-03T06:44:50Z", "updated_at": "2018-05-03T06:45:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> sorry for being unclear, let me be more explicit. Yes, I confirm that today <code>DataLoader</code> is deterministic (and for <code>num_workers &gt; 0</code>) and the following code (pytorch 0.4.0) :</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> np.arange(<span class=\"pl-c1\">30</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">transform</span>(<span class=\"pl-smi\">v</span>):    <span class=\"pl-k\">return</span> v <span class=\"pl-k\">+</span> torch.rand(<span class=\"pl-c1\">1</span>).item()\n        \n<span class=\"pl-k\">class</span> <span class=\"pl-en\">TestDataset</span>(<span class=\"pl-e\">Dataset</span>):    \n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">30</span>    \n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">index</span>):        <span class=\"pl-k\">return</span> transform(x[index]), x[index]\n    \ndataset <span class=\"pl-k\">=</span> TestDataset()\ntorch.manual_seed(<span class=\"pl-c1\">12345</span>)\nloader <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">drop_last</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> loader:\n    <span class=\"pl-c1\">print</span>(batch)</pre></div>\n<p>gives every time the same results:</p>\n<pre><code>[tensor([ 15.9327,  17.3807,  10.6407,  28.2207], dtype=torch.float64), tensor([ 15,  17,  10,  28])]\n[tensor([ 12.5148,  13.5664,  27.7817,   6.7614], dtype=torch.float64), tensor([ 12,  13,  27,   6])]\n[tensor([ 25.5011,   8.8289,  16.3753,  18.9677], dtype=torch.float64), tensor([ 25,   8,  16,  18])]\n[tensor([ 29.1541,   4.7404,  23.7058,   7.0936], dtype=torch.float64), tensor([ 29,   4,  23,   7])]\n[tensor([ 19.2820,   9.9384,  22.6470,  20.5152], dtype=torch.float64), tensor([ 19,   9,  22,  20])]\n[tensor([ 21.4232,   5.6175,   0.6057,  24.5759], dtype=torch.float64), tensor([ 21,   5,   0,  24])]\n[tensor([ 11.7328,   2.0943,   3.6006,  14.7609], dtype=torch.float64), tensor([ 11,   2,   3,  14])]\n</code></pre>\n<p>and this is good. Pay attention on <code>target</code> values: 15, 17, 10, 28 etc. these values are generated by the batch sampler (in my case by <code>RandomSampler</code> with <code>BatchSampler</code>).</p>\n<p>If I execute the previous code with <code>num_workers=0</code>, the result is:</p>\n<pre><code>[tensor([  0.6659,  20.0413,  19.5361,  15.3120], dtype=torch.float64), tensor([  0,  20,  19,  15])]\n[tensor([ 22.4288,  17.2770,  28.4377,  18.9498], dtype=torch.float64), tensor([ 22,  17,  28,  18])]\n[tensor([ 21.0193,  26.2634,  11.9249,  16.4737], dtype=torch.float64), tensor([ 21,  26,  11,  16])]\n[tensor([ 10.3961,   1.6681,  25.2888,  12.7053], dtype=torch.float64), tensor([ 10,   1,  25,  12])]\n[tensor([  5.1916,  14.3497,  29.8827,  23.5490], dtype=torch.float64), tensor([  5,  14,  29,  23])]\n[tensor([ 13.1045,   9.8004,   2.1677,  24.4677], dtype=torch.float64), tensor([ 13,   9,   2,  24])]\n[tensor([  3.4986,   8.2797,  27.7238,   4.4506], dtype=torch.float64), tensor([  3,   8,  27,   4])]\n</code></pre>\n<p>and target values: 0, 20, 19, 15 etc are the same as if I run only:</p>\n<pre><code>torch.manual_seed(12345)\nloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, drop_last=True)\nbatch_sampler_iter = iter(loader.batch_sampler)\nfor indices in batch_sampler_iter:\n    print(indices)\n</code></pre>\n<p>the output is</p>\n<pre><code>[0, 20, 19, 15]\n[22, 17, 28, 18]\n[21, 26, 11, 16]\n[10, 1, 25, 12]\n[5, 14, 29, 23]\n[13, 9, 2, 24]\n[3, 8, 27, 4]\n</code></pre>\n<p>This difference is due to the fact that <code>RandomSampler</code> uses torch random generator on <code>next()</code> call.<br>\nAnd in case of <code>num_workers&gt;0</code> torch random generator is called once to generate <code>base_seed</code> before calling <code>next()</code>.</p>\n<p><em>Why it can be important to reproduce the same indices?</em> I imagine an epoch with a large number of iterations, so that resuming crashed DataLoader state from a given iteration instead of epoch begining can be useful. So, to skip previous batches can be simply run multiple times <code>next</code> on the batch sampler.</p>\n<p>Concerning the fix I propose, if I understand correctly <code>base_seed</code> was set as random number to avoid same random state in all workers and thus producing the same random augmentation.</p>\n<blockquote>\n<p>Furthermore, your <code>base_seed = torch.LongTensor(1).random_(seed)[0]</code> will just generate same seed in every worker.</p>\n</blockquote>\n<p>Actually <code>seed</code> used to generate <code>base_seed</code> in the worker is setup as previously <code>base_seed + worker_id</code>, so they will be different for a single run and same for different epochs.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> hope being more clear now. What do you think about this ?</p>", "body_text": "@SsnL sorry for being unclear, let me be more explicit. Yes, I confirm that today DataLoader is deterministic (and for num_workers > 0) and the following code (pytorch 0.4.0) :\nx = np.arange(30)\n\ndef transform(v):    return v + torch.rand(1).item()\n        \nclass TestDataset(Dataset):    \n    def __len__(self):        return 30    \n    def __getitem__(self, index):        return transform(x[index]), x[index]\n    \ndataset = TestDataset()\ntorch.manual_seed(12345)\nloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\n\nfor batch in loader:\n    print(batch)\ngives every time the same results:\n[tensor([ 15.9327,  17.3807,  10.6407,  28.2207], dtype=torch.float64), tensor([ 15,  17,  10,  28])]\n[tensor([ 12.5148,  13.5664,  27.7817,   6.7614], dtype=torch.float64), tensor([ 12,  13,  27,   6])]\n[tensor([ 25.5011,   8.8289,  16.3753,  18.9677], dtype=torch.float64), tensor([ 25,   8,  16,  18])]\n[tensor([ 29.1541,   4.7404,  23.7058,   7.0936], dtype=torch.float64), tensor([ 29,   4,  23,   7])]\n[tensor([ 19.2820,   9.9384,  22.6470,  20.5152], dtype=torch.float64), tensor([ 19,   9,  22,  20])]\n[tensor([ 21.4232,   5.6175,   0.6057,  24.5759], dtype=torch.float64), tensor([ 21,   5,   0,  24])]\n[tensor([ 11.7328,   2.0943,   3.6006,  14.7609], dtype=torch.float64), tensor([ 11,   2,   3,  14])]\n\nand this is good. Pay attention on target values: 15, 17, 10, 28 etc. these values are generated by the batch sampler (in my case by RandomSampler with BatchSampler).\nIf I execute the previous code with num_workers=0, the result is:\n[tensor([  0.6659,  20.0413,  19.5361,  15.3120], dtype=torch.float64), tensor([  0,  20,  19,  15])]\n[tensor([ 22.4288,  17.2770,  28.4377,  18.9498], dtype=torch.float64), tensor([ 22,  17,  28,  18])]\n[tensor([ 21.0193,  26.2634,  11.9249,  16.4737], dtype=torch.float64), tensor([ 21,  26,  11,  16])]\n[tensor([ 10.3961,   1.6681,  25.2888,  12.7053], dtype=torch.float64), tensor([ 10,   1,  25,  12])]\n[tensor([  5.1916,  14.3497,  29.8827,  23.5490], dtype=torch.float64), tensor([  5,  14,  29,  23])]\n[tensor([ 13.1045,   9.8004,   2.1677,  24.4677], dtype=torch.float64), tensor([ 13,   9,   2,  24])]\n[tensor([  3.4986,   8.2797,  27.7238,   4.4506], dtype=torch.float64), tensor([  3,   8,  27,   4])]\n\nand target values: 0, 20, 19, 15 etc are the same as if I run only:\ntorch.manual_seed(12345)\nloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, drop_last=True)\nbatch_sampler_iter = iter(loader.batch_sampler)\nfor indices in batch_sampler_iter:\n    print(indices)\n\nthe output is\n[0, 20, 19, 15]\n[22, 17, 28, 18]\n[21, 26, 11, 16]\n[10, 1, 25, 12]\n[5, 14, 29, 23]\n[13, 9, 2, 24]\n[3, 8, 27, 4]\n\nThis difference is due to the fact that RandomSampler uses torch random generator on next() call.\nAnd in case of num_workers>0 torch random generator is called once to generate base_seed before calling next().\nWhy it can be important to reproduce the same indices? I imagine an epoch with a large number of iterations, so that resuming crashed DataLoader state from a given iteration instead of epoch begining can be useful. So, to skip previous batches can be simply run multiple times next on the batch sampler.\nConcerning the fix I propose, if I understand correctly base_seed was set as random number to avoid same random state in all workers and thus producing the same random augmentation.\n\nFurthermore, your base_seed = torch.LongTensor(1).random_(seed)[0] will just generate same seed in every worker.\n\nActually seed used to generate base_seed in the worker is setup as previously base_seed + worker_id, so they will be different for a single run and same for different epochs.\n@SsnL hope being more clear now. What do you think about this ?", "body": "@SsnL sorry for being unclear, let me be more explicit. Yes, I confirm that today `DataLoader` is deterministic (and for `num_workers > 0`) and the following code (pytorch 0.4.0) : \r\n```python\r\nx = np.arange(30)\r\n\r\ndef transform(v):    return v + torch.rand(1).item()\r\n        \r\nclass TestDataset(Dataset):    \r\n    def __len__(self):        return 30    \r\n    def __getitem__(self, index):        return transform(x[index]), x[index]\r\n    \r\ndataset = TestDataset()\r\ntorch.manual_seed(12345)\r\nloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, drop_last=True)\r\n\r\nfor batch in loader:\r\n    print(batch)\r\n```\r\ngives every time the same results:\r\n```\r\n[tensor([ 15.9327,  17.3807,  10.6407,  28.2207], dtype=torch.float64), tensor([ 15,  17,  10,  28])]\r\n[tensor([ 12.5148,  13.5664,  27.7817,   6.7614], dtype=torch.float64), tensor([ 12,  13,  27,   6])]\r\n[tensor([ 25.5011,   8.8289,  16.3753,  18.9677], dtype=torch.float64), tensor([ 25,   8,  16,  18])]\r\n[tensor([ 29.1541,   4.7404,  23.7058,   7.0936], dtype=torch.float64), tensor([ 29,   4,  23,   7])]\r\n[tensor([ 19.2820,   9.9384,  22.6470,  20.5152], dtype=torch.float64), tensor([ 19,   9,  22,  20])]\r\n[tensor([ 21.4232,   5.6175,   0.6057,  24.5759], dtype=torch.float64), tensor([ 21,   5,   0,  24])]\r\n[tensor([ 11.7328,   2.0943,   3.6006,  14.7609], dtype=torch.float64), tensor([ 11,   2,   3,  14])]\r\n```\r\nand this is good. Pay attention on `target` values: 15, 17, 10, 28 etc. these values are generated by the batch sampler (in my case by `RandomSampler` with `BatchSampler`).\r\n\r\nIf I execute the previous code with `num_workers=0`, the result is: \r\n```\r\n[tensor([  0.6659,  20.0413,  19.5361,  15.3120], dtype=torch.float64), tensor([  0,  20,  19,  15])]\r\n[tensor([ 22.4288,  17.2770,  28.4377,  18.9498], dtype=torch.float64), tensor([ 22,  17,  28,  18])]\r\n[tensor([ 21.0193,  26.2634,  11.9249,  16.4737], dtype=torch.float64), tensor([ 21,  26,  11,  16])]\r\n[tensor([ 10.3961,   1.6681,  25.2888,  12.7053], dtype=torch.float64), tensor([ 10,   1,  25,  12])]\r\n[tensor([  5.1916,  14.3497,  29.8827,  23.5490], dtype=torch.float64), tensor([  5,  14,  29,  23])]\r\n[tensor([ 13.1045,   9.8004,   2.1677,  24.4677], dtype=torch.float64), tensor([ 13,   9,   2,  24])]\r\n[tensor([  3.4986,   8.2797,  27.7238,   4.4506], dtype=torch.float64), tensor([  3,   8,  27,   4])]\r\n```\r\nand target values: 0, 20, 19, 15 etc are the same as if I run only:\r\n```\r\ntorch.manual_seed(12345)\r\nloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, drop_last=True)\r\nbatch_sampler_iter = iter(loader.batch_sampler)\r\nfor indices in batch_sampler_iter:\r\n    print(indices)\r\n```\r\nthe output is \r\n```\r\n[0, 20, 19, 15]\r\n[22, 17, 28, 18]\r\n[21, 26, 11, 16]\r\n[10, 1, 25, 12]\r\n[5, 14, 29, 23]\r\n[13, 9, 2, 24]\r\n[3, 8, 27, 4]\r\n```\r\nThis difference is due to the fact that `RandomSampler` uses torch random generator on `next()` call. \r\nAnd in case of `num_workers>0` torch random generator is called once to generate `base_seed` before calling `next()`. \r\n\r\n*Why it can be important to reproduce the same indices?* I imagine an epoch with a large number of iterations, so that resuming crashed DataLoader state from a given iteration instead of epoch begining can be useful. So, to skip previous batches can be simply run multiple times `next` on the batch sampler.\r\n\r\nConcerning the fix I propose, if I understand correctly `base_seed` was set as random number to avoid same random state in all workers and thus producing the same random augmentation.\r\n\r\n> Furthermore, your `base_seed = torch.LongTensor(1).random_(seed)[0]` will just generate same seed in every worker.\r\n\r\nActually `seed` used to generate `base_seed` in the worker is setup as previously `base_seed + worker_id`, so they will be different for a single run and same for different epochs.\r\n\r\n@SsnL hope being more clear now. What do you think about this ?\r\n\r\n  \r\n\r\n\r\n\r\n"}