{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7209", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7209/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7209/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7209/events", "html_url": "https://github.com/pytorch/pytorch/issues/7209", "id": 319746163, "node_id": "MDU6SXNzdWUzMTk3NDYxNjM=", "number": 7209, "title": "[Feature Request][PyTorch] Deterministic and predictable behaviour of batch sampler ", "user": {"login": "vfdev-5", "id": 2459423, "node_id": "MDQ6VXNlcjI0NTk0MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2459423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vfdev-5", "html_url": "https://github.com/vfdev-5", "followers_url": "https://api.github.com/users/vfdev-5/followers", "following_url": "https://api.github.com/users/vfdev-5/following{/other_user}", "gists_url": "https://api.github.com/users/vfdev-5/gists{/gist_id}", "starred_url": "https://api.github.com/users/vfdev-5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vfdev-5/subscriptions", "organizations_url": "https://api.github.com/users/vfdev-5/orgs", "repos_url": "https://api.github.com/users/vfdev-5/repos", "events_url": "https://api.github.com/users/vfdev-5/events{/privacy}", "received_events_url": "https://api.github.com/users/vfdev-5/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-05-02T23:32:58Z", "updated_at": "2018-05-04T11:17:45Z", "closed_at": "2018-05-04T11:17:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Today <code>DataLoader</code> works well and returns reproducible results when fixing <code>torch.manual_seed</code>.</p>\n<p>However, the problem is that the batch sampler or sampler (if it uses torch random generator) returns differents results if it used alone or in data loader (even fixing the random state) when <code>num_workers &gt; 0</code></p>\n<p>The problem is <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L261\">here</a>, when <code>base_seed</code> is randomly set, random indices generated by the first call of <code>next(sampler_iter)</code> that happens <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L296\">after</a> are not the same as they'd have been generated alone (with <code>next</code>) or by the data loader with <code>num_workers=0</code>.</p>\n<p>Can we set <code>base_seed</code> differently without using torch random generator?</p>\n<p>For example, we can use a fix <code>base_seed</code> in <code>DataLoaderIter</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>            base_seed <span class=\"pl-k\">=</span> <span class=\"pl-c1\">7056</span>\n            <span class=\"pl-c1\">self</span>.workers <span class=\"pl-k\">=</span> [\n                multiprocessing.Process(\n                    <span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>_worker_loop,\n                    <span class=\"pl-v\">args</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">self</span>.dataset, <span class=\"pl-c1\">self</span>.index_queue, <span class=\"pl-c1\">self</span>.worker_result_queue, <span class=\"pl-c1\">self</span>.collate_fn,\n                          base_seed <span class=\"pl-k\">+</span> i, <span class=\"pl-c1\">self</span>.worker_init_fn, i))\n                <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">self</span>.num_workers)]</pre></div>\n<p>and set a random seed in <code>_worker_loop</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_worker_loop</span>(<span class=\"pl-smi\">dataset</span>, <span class=\"pl-smi\">index_queue</span>, <span class=\"pl-smi\">data_queue</span>, <span class=\"pl-smi\">collate_fn</span>, <span class=\"pl-smi\">seed</span>, <span class=\"pl-smi\">init_fn</span>, <span class=\"pl-smi\">worker_id</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ...    </span>\n    base_seed <span class=\"pl-k\">=</span> torch.LongTensor(<span class=\"pl-c1\">1</span>).random_(seed)[<span class=\"pl-c1\">0</span>]\n    torch.set_num_threads(<span class=\"pl-c1\">1</span>)\n    torch.manual_seed(base_seed)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> ...</span></pre></div>\n<p>What do you think ?</p>\n<p>Context: this can be helpful if I would like to resume <code>DataLoader</code> from a given iteration (if training has crashed for example).</p>\n<p>Thanks</p>", "body_text": "Today DataLoader works well and returns reproducible results when fixing torch.manual_seed.\nHowever, the problem is that the batch sampler or sampler (if it uses torch random generator) returns differents results if it used alone or in data loader (even fixing the random state) when num_workers > 0\nThe problem is here, when base_seed is randomly set, random indices generated by the first call of next(sampler_iter) that happens after are not the same as they'd have been generated alone (with next) or by the data loader with num_workers=0.\nCan we set base_seed differently without using torch random generator?\nFor example, we can use a fix base_seed in DataLoaderIter:\n            base_seed = 7056\n            self.workers = [\n                multiprocessing.Process(\n                    target=_worker_loop,\n                    args=(self.dataset, self.index_queue, self.worker_result_queue, self.collate_fn,\n                          base_seed + i, self.worker_init_fn, i))\n                for i in range(self.num_workers)]\nand set a random seed in _worker_loop:\ndef _worker_loop(dataset, index_queue, data_queue, collate_fn, seed, init_fn, worker_id):\n    # ...    \n    base_seed = torch.LongTensor(1).random_(seed)[0]\n    torch.set_num_threads(1)\n    torch.manual_seed(base_seed)\n    # ...\nWhat do you think ?\nContext: this can be helpful if I would like to resume DataLoader from a given iteration (if training has crashed for example).\nThanks", "body": "Today `DataLoader` works well and returns reproducible results when fixing `torch.manual_seed`.\r\n\r\nHowever, the problem is that the batch sampler or sampler (if it uses torch random generator) returns differents results if it used alone or in data loader (even fixing the random state) when `num_workers > 0`\r\n\r\nThe problem is [here](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L261), when `base_seed` is randomly set, random indices generated by the first call of `next(sampler_iter)` that happens [after](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataloader.py#L296) are not the same as they'd have been generated alone (with `next`) or by the data loader with `num_workers=0`.\r\n\r\nCan we set `base_seed` differently without using torch random generator?\r\n\r\nFor example, we can use a fix `base_seed` in `DataLoaderIter`:\r\n```python\r\n            base_seed = 7056\r\n            self.workers = [\r\n                multiprocessing.Process(\r\n                    target=_worker_loop,\r\n                    args=(self.dataset, self.index_queue, self.worker_result_queue, self.collate_fn,\r\n                          base_seed + i, self.worker_init_fn, i))\r\n                for i in range(self.num_workers)]\r\n```\r\nand set a random seed in `_worker_loop`:\r\n```python\r\ndef _worker_loop(dataset, index_queue, data_queue, collate_fn, seed, init_fn, worker_id):\r\n    # ...    \r\n    base_seed = torch.LongTensor(1).random_(seed)[0]\r\n    torch.set_num_threads(1)\r\n    torch.manual_seed(base_seed)\r\n    # ...\r\n```\r\nWhat do you think ?\r\n\r\nContext: this can be helpful if I would like to resume `DataLoader` from a given iteration (if training has crashed for example).\r\n\r\nThanks\r\n"}