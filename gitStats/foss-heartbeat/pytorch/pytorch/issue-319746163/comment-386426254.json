{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386426254", "html_url": "https://github.com/pytorch/pytorch/issues/7209#issuecomment-386426254", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7209", "id": 386426254, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjQyNjI1NA==", "user": {"login": "vfdev-5", "id": 2459423, "node_id": "MDQ6VXNlcjI0NTk0MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2459423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vfdev-5", "html_url": "https://github.com/vfdev-5", "followers_url": "https://api.github.com/users/vfdev-5/followers", "following_url": "https://api.github.com/users/vfdev-5/following{/other_user}", "gists_url": "https://api.github.com/users/vfdev-5/gists{/gist_id}", "starred_url": "https://api.github.com/users/vfdev-5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vfdev-5/subscriptions", "organizations_url": "https://api.github.com/users/vfdev-5/orgs", "repos_url": "https://api.github.com/users/vfdev-5/repos", "events_url": "https://api.github.com/users/vfdev-5/events{/privacy}", "received_events_url": "https://api.github.com/users/vfdev-5/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-03T20:29:00Z", "updated_at": "2018-05-03T20:32:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> a little more precision on your suggestion. Do you mean to instantiate <code>base_seed</code> outside the <code>num_workers</code> condition, like this ?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">_DataLoaderIter</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-s\"><span class=\"pl-k\">r</span><span class=\"pl-pds\">\"\"\"</span>Iterates once over the DataLoader's dataset, as specified by the sampler<span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">loader</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> ....</span>\n        <span class=\"pl-c1\">self</span>.sample_iter <span class=\"pl-k\">=</span> <span class=\"pl-c1\">iter</span>(<span class=\"pl-c1\">self</span>.batch_sampler)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> base seed is randomly instantiated here</span>\n        base_seed <span class=\"pl-k\">=</span> torch.LongTensor(<span class=\"pl-c1\">1</span>).random_()[<span class=\"pl-c1\">0</span>]\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.num_workers <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> ....</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> removed ouside -&gt; base_seed = torch.LongTensor(1).random_()[0]</span></pre></div>\n<p>if yes, sure that this solves the problem of different batch indices for all values of <code>num_workers</code>, however indices still remain different of those generated by batch_sampler standalone:</p>\n<div class=\"highlight highlight-source-python\"><pre>torch.manual_seed(<span class=\"pl-c1\">12345</span>)\nloader <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span>any_value, <span class=\"pl-v\">drop_last</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> loader:\n    <span class=\"pl-c1\">print</span>(batch)</pre></div>\n<p>with the output:</p>\n<pre><code>[tensor([ 15.9327,  17.3807,  10.6407,  28.2207], dtype=torch.float64), tensor([ 15,  17,  10,  28])]\n[tensor([ 12.5148,  13.5664,  27.7817,   6.7614], dtype=torch.float64), tensor([ 12,  13,  27,   6])]\n[tensor([ 25.5011,   8.8289,  16.3753,  18.9677], dtype=torch.float64), tensor([ 25,   8,  16,  18])]\n[tensor([ 29.1541,   4.7404,  23.7058,   7.0936], dtype=torch.float64), tensor([ 29,   4,  23,   7])]\n[tensor([ 19.2820,   9.9384,  22.6470,  20.5152], dtype=torch.float64), tensor([ 19,   9,  22,  20])]\n[tensor([ 21.4232,   5.6175,   0.6057,  24.5759], dtype=torch.float64), tensor([ 21,   5,   0,  24])]\n[tensor([ 11.7328,   2.0943,   3.6006,  14.7609], dtype=torch.float64), tensor([ 11,   2,   3,  14])]\n</code></pre>\n<p>vs</p>\n<div class=\"highlight highlight-source-python\"><pre>torch.manual_seed(<span class=\"pl-c1\">12345</span>)\nloader <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span>any_value, <span class=\"pl-v\">drop_last</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nbatch_sampler_iter <span class=\"pl-k\">=</span> <span class=\"pl-c1\">iter</span>(loader.batch_sampler)\n<span class=\"pl-k\">for</span> indices <span class=\"pl-k\">in</span> batch_sampler_iter:\n    <span class=\"pl-c1\">print</span>(indices)</pre></div>\n<p>with the output:</p>\n<pre><code>[0, 20, 19, 15]\n[22, 17, 28, 18]\n[21, 26, 11, 16]\n[10, 1, 25, 12]\n[5, 14, 29, 23]\n[13, 9, 2, 24]\n[3, 8, 27, 4]\n</code></pre>\n<p>The problem is that when indices are \"released\" by <code>self._put_indices()</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>    base_seed <span class=\"pl-k\">=</span> torch.LongTensor(<span class=\"pl-c1\">1</span>).random_()[<span class=\"pl-c1\">0</span>]\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.num_workers <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:     \n         <span class=\"pl-c\"><span class=\"pl-c\">#</span> ....</span>\n\n         <span class=\"pl-c\"><span class=\"pl-c\">#</span> prime the prefetch loop</span>\n         <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.num_workers):\n             <span class=\"pl-c1\">self</span>._put_indices()</pre></div>\n<p>the random state is not the same anymore when set by <code>torch.manual_seed</code></p>\n<p>Another fix is to save to random state before initialization of <code>base_seed</code> and restore it just before<br>\n<code>self._put_indices()</code>.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> what do you guys think about ?</p>", "body_text": "@apaszke a little more precision on your suggestion. Do you mean to instantiate base_seed outside the num_workers condition, like this ?\nclass _DataLoaderIter(object):\n    r\"\"\"Iterates once over the DataLoader's dataset, as specified by the sampler\"\"\"\n\n    def __init__(self, loader):\n        # ....\n        self.sample_iter = iter(self.batch_sampler)\n\n        # base seed is randomly instantiated here\n        base_seed = torch.LongTensor(1).random_()[0]\n        if self.num_workers > 0:\n            # ....\n            # removed ouside -> base_seed = torch.LongTensor(1).random_()[0]\nif yes, sure that this solves the problem of different batch indices for all values of num_workers, however indices still remain different of those generated by batch_sampler standalone:\ntorch.manual_seed(12345)\nloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=any_value, drop_last=True)\nfor batch in loader:\n    print(batch)\nwith the output:\n[tensor([ 15.9327,  17.3807,  10.6407,  28.2207], dtype=torch.float64), tensor([ 15,  17,  10,  28])]\n[tensor([ 12.5148,  13.5664,  27.7817,   6.7614], dtype=torch.float64), tensor([ 12,  13,  27,   6])]\n[tensor([ 25.5011,   8.8289,  16.3753,  18.9677], dtype=torch.float64), tensor([ 25,   8,  16,  18])]\n[tensor([ 29.1541,   4.7404,  23.7058,   7.0936], dtype=torch.float64), tensor([ 29,   4,  23,   7])]\n[tensor([ 19.2820,   9.9384,  22.6470,  20.5152], dtype=torch.float64), tensor([ 19,   9,  22,  20])]\n[tensor([ 21.4232,   5.6175,   0.6057,  24.5759], dtype=torch.float64), tensor([ 21,   5,   0,  24])]\n[tensor([ 11.7328,   2.0943,   3.6006,  14.7609], dtype=torch.float64), tensor([ 11,   2,   3,  14])]\n\nvs\ntorch.manual_seed(12345)\nloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=any_value, drop_last=True)\nbatch_sampler_iter = iter(loader.batch_sampler)\nfor indices in batch_sampler_iter:\n    print(indices)\nwith the output:\n[0, 20, 19, 15]\n[22, 17, 28, 18]\n[21, 26, 11, 16]\n[10, 1, 25, 12]\n[5, 14, 29, 23]\n[13, 9, 2, 24]\n[3, 8, 27, 4]\n\nThe problem is that when indices are \"released\" by self._put_indices():\n    base_seed = torch.LongTensor(1).random_()[0]\n    if self.num_workers > 0:     \n         # ....\n\n         # prime the prefetch loop\n         for _ in range(2 * self.num_workers):\n             self._put_indices()\nthe random state is not the same anymore when set by torch.manual_seed\nAnother fix is to save to random state before initialization of base_seed and restore it just before\nself._put_indices().\n@SsnL @apaszke what do you guys think about ?", "body": "@apaszke a little more precision on your suggestion. Do you mean to instantiate `base_seed` outside the `num_workers` condition, like this ?\r\n```python\r\nclass _DataLoaderIter(object):\r\n    r\"\"\"Iterates once over the DataLoader's dataset, as specified by the sampler\"\"\"\r\n\r\n    def __init__(self, loader):\r\n        # ....\r\n        self.sample_iter = iter(self.batch_sampler)\r\n\r\n        # base seed is randomly instantiated here\r\n        base_seed = torch.LongTensor(1).random_()[0]\r\n        if self.num_workers > 0:\r\n            # ....\r\n            # removed ouside -> base_seed = torch.LongTensor(1).random_()[0]\r\n```\r\n\r\nif yes, sure that this solves the problem of different batch indices for all values of `num_workers`, however indices still remain different of those generated by batch_sampler standalone:\r\n```python\r\ntorch.manual_seed(12345)\r\nloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=any_value, drop_last=True)\r\nfor batch in loader:\r\n    print(batch)\r\n```\r\nwith the output:\r\n```\r\n[tensor([ 15.9327,  17.3807,  10.6407,  28.2207], dtype=torch.float64), tensor([ 15,  17,  10,  28])]\r\n[tensor([ 12.5148,  13.5664,  27.7817,   6.7614], dtype=torch.float64), tensor([ 12,  13,  27,   6])]\r\n[tensor([ 25.5011,   8.8289,  16.3753,  18.9677], dtype=torch.float64), tensor([ 25,   8,  16,  18])]\r\n[tensor([ 29.1541,   4.7404,  23.7058,   7.0936], dtype=torch.float64), tensor([ 29,   4,  23,   7])]\r\n[tensor([ 19.2820,   9.9384,  22.6470,  20.5152], dtype=torch.float64), tensor([ 19,   9,  22,  20])]\r\n[tensor([ 21.4232,   5.6175,   0.6057,  24.5759], dtype=torch.float64), tensor([ 21,   5,   0,  24])]\r\n[tensor([ 11.7328,   2.0943,   3.6006,  14.7609], dtype=torch.float64), tensor([ 11,   2,   3,  14])]\r\n```\r\nvs \r\n```python\r\ntorch.manual_seed(12345)\r\nloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=any_value, drop_last=True)\r\nbatch_sampler_iter = iter(loader.batch_sampler)\r\nfor indices in batch_sampler_iter:\r\n    print(indices)\r\n```\r\nwith the output:\r\n```\r\n[0, 20, 19, 15]\r\n[22, 17, 28, 18]\r\n[21, 26, 11, 16]\r\n[10, 1, 25, 12]\r\n[5, 14, 29, 23]\r\n[13, 9, 2, 24]\r\n[3, 8, 27, 4]\r\n```\r\n\r\nThe problem is that when indices are \"released\" by `self._put_indices()`:\r\n```python\r\n\r\n    base_seed = torch.LongTensor(1).random_()[0]\r\n    if self.num_workers > 0:     \r\n         # ....\r\n\r\n         # prime the prefetch loop\r\n         for _ in range(2 * self.num_workers):\r\n             self._put_indices()\r\n```\r\nthe random state is not the same anymore when set by `torch.manual_seed`\r\n \r\nAnother fix is to save to random state before initialization of `base_seed` and restore it just before \r\n`self._put_indices()`. \r\n\r\n@SsnL @apaszke what do you guys think about ?"}