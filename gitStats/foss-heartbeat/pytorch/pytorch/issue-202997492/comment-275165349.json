{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/275165349", "html_url": "https://github.com/pytorch/pytorch/issues/584#issuecomment-275165349", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/584", "id": 275165349, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTE2NTM0OQ==", "user": {"login": "napsternxg", "id": 112678, "node_id": "MDQ6VXNlcjExMjY3OA==", "avatar_url": "https://avatars0.githubusercontent.com/u/112678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/napsternxg", "html_url": "https://github.com/napsternxg", "followers_url": "https://api.github.com/users/napsternxg/followers", "following_url": "https://api.github.com/users/napsternxg/following{/other_user}", "gists_url": "https://api.github.com/users/napsternxg/gists{/gist_id}", "starred_url": "https://api.github.com/users/napsternxg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/napsternxg/subscriptions", "organizations_url": "https://api.github.com/users/napsternxg/orgs", "repos_url": "https://api.github.com/users/napsternxg/repos", "events_url": "https://api.github.com/users/napsternxg/events{/privacy}", "received_events_url": "https://api.github.com/users/napsternxg/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-25T16:57:56Z", "updated_at": "2017-01-25T17:01:22Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> This is what prompted me to start this issue:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">LinearRegression</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">output_size</span>):\n        <span class=\"pl-c1\">super</span>(LinearRegression, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.x2o <span class=\"pl-k\">=</span> nn.Linear(input_size, output_size)\n        \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">X</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.x2o(X)\n\nbatch <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">3</span>))\ntarget <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">1</span>))\n\nno_gpu_model <span class=\"pl-k\">=</span> LinearRegression(<span class=\"pl-v\">input_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">output_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\nno_gpu_model.forward(batch).size()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> OUT: torch.Size([10, 1])</span>\n\n<span class=\"pl-k\">if</span> torch.cuda.is_available():\n    gpu_model <span class=\"pl-k\">=</span> no_gpu_model.cuda()\n    <span class=\"pl-k\">try</span>:\n        <span class=\"pl-c1\">print</span>(gpu_model.forward(batch))\n    <span class=\"pl-k\">except</span> <span class=\"pl-c1\">TypeError</span> <span class=\"pl-k\">as</span> e:\n        <span class=\"pl-c1\">print</span>(e) </pre></div>\n<p>The above code prints the following error:</p>\n<pre><code>addmm_ received an invalid combination of arguments - got (int, int, torch.FloatTensor, torch.cuda.FloatTensor), but expected one of:\n * (torch.FloatTensor mat1, torch.FloatTensor mat2)\n * (torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\n * (float beta, torch.FloatTensor mat1, torch.FloatTensor mat2)\n * (float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2)\n * (float beta, torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\n * (float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\n * (float beta, float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2)\n      didn't match because some of the arguments have invalid types: (int, int, torch.FloatTensor, torch.cuda.FloatTensor)\n * (float beta, float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\n      didn't match because some of the arguments have invalid types: (int, int, torch.FloatTensor, torch.cuda.FloatTensor)\n</code></pre>\n<p>However, the following works:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> torch.cuda.is_available():\n    gpu_model <span class=\"pl-k\">=</span> no_gpu_model.cuda()\n    <span class=\"pl-k\">try</span>:\n        <span class=\"pl-c1\">print</span>(gpu_model.forward(batch.cuda()).size())\n    <span class=\"pl-k\">except</span> <span class=\"pl-c1\">TypeError</span> <span class=\"pl-k\">as</span> e:\n        <span class=\"pl-c1\">print</span>(e)   \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> OUT: torch.Size([10, 1])</span></pre></div>\n<p>It is the second case, I would want some feedback weather the variables should be send to cuda or not based on the module.</p>\n<p>The reason this functionality will be required is in the case when someone explicitly sets a module to run on cuda backend. In that case using this flag we can set all the input values also on cuda. Otherwise, there will be compatibility issues running the same code on CPU and GPU.</p>\n<p>The reason I was suggesting some explicit context manager is that in the context manager we can do a graceful fallback to CPU instead of giving this error.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I didn't know about <code>torch.set_default_tensor_type</code>. Might be useful in a python module (again I am thinking along the lines of context manager, so that one modules defaults don't spill to other code ). I think having everything of default type might not be a very good idea. But in that case, the user should have the freedom to choose, what types they want, dynamically, based on the available device.</p>", "body_text": "@colesbury This is what prompted me to start this issue:\nimport torch\nfrom torch.autograd import Variable\n\nclass LinearRegression(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LinearRegression, self).__init__()\n        self.x2o = nn.Linear(input_size, output_size)\n        \n    def forward(self, X):\n        return self.x2o(X)\n\nbatch = Variable(torch.randn(10,3))\ntarget = Variable(torch.randn(10,1))\n\nno_gpu_model = LinearRegression(input_size=3, output_size=1)\n\nno_gpu_model.forward(batch).size()\n# OUT: torch.Size([10, 1])\n\nif torch.cuda.is_available():\n    gpu_model = no_gpu_model.cuda()\n    try:\n        print(gpu_model.forward(batch))\n    except TypeError as e:\n        print(e) \nThe above code prints the following error:\naddmm_ received an invalid combination of arguments - got (int, int, torch.FloatTensor, torch.cuda.FloatTensor), but expected one of:\n * (torch.FloatTensor mat1, torch.FloatTensor mat2)\n * (torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\n * (float beta, torch.FloatTensor mat1, torch.FloatTensor mat2)\n * (float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2)\n * (float beta, torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\n * (float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\n * (float beta, float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2)\n      didn't match because some of the arguments have invalid types: (int, int, torch.FloatTensor, torch.cuda.FloatTensor)\n * (float beta, float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\n      didn't match because some of the arguments have invalid types: (int, int, torch.FloatTensor, torch.cuda.FloatTensor)\n\nHowever, the following works:\nif torch.cuda.is_available():\n    gpu_model = no_gpu_model.cuda()\n    try:\n        print(gpu_model.forward(batch.cuda()).size())\n    except TypeError as e:\n        print(e)   \n# OUT: torch.Size([10, 1])\nIt is the second case, I would want some feedback weather the variables should be send to cuda or not based on the module.\nThe reason this functionality will be required is in the case when someone explicitly sets a module to run on cuda backend. In that case using this flag we can set all the input values also on cuda. Otherwise, there will be compatibility issues running the same code on CPU and GPU.\nThe reason I was suggesting some explicit context manager is that in the context manager we can do a graceful fallback to CPU instead of giving this error.\n@apaszke I didn't know about torch.set_default_tensor_type. Might be useful in a python module (again I am thinking along the lines of context manager, so that one modules defaults don't spill to other code ). I think having everything of default type might not be a very good idea. But in that case, the user should have the freedom to choose, what types they want, dynamically, based on the available device.", "body": "@colesbury This is what prompted me to start this issue:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nclass LinearRegression(nn.Module):\r\n    def __init__(self, input_size, output_size):\r\n        super(LinearRegression, self).__init__()\r\n        self.x2o = nn.Linear(input_size, output_size)\r\n        \r\n    def forward(self, X):\r\n        return self.x2o(X)\r\n\r\nbatch = Variable(torch.randn(10,3))\r\ntarget = Variable(torch.randn(10,1))\r\n\r\nno_gpu_model = LinearRegression(input_size=3, output_size=1)\r\n\r\nno_gpu_model.forward(batch).size()\r\n# OUT: torch.Size([10, 1])\r\n\r\nif torch.cuda.is_available():\r\n    gpu_model = no_gpu_model.cuda()\r\n    try:\r\n        print(gpu_model.forward(batch))\r\n    except TypeError as e:\r\n        print(e) \r\n```\r\n\r\nThe above code prints the following error:\r\n\r\n```\r\naddmm_ received an invalid combination of arguments - got (int, int, torch.FloatTensor, torch.cuda.FloatTensor), but expected one of:\r\n * (torch.FloatTensor mat1, torch.FloatTensor mat2)\r\n * (torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\r\n * (float beta, torch.FloatTensor mat1, torch.FloatTensor mat2)\r\n * (float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2)\r\n * (float beta, torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\r\n * (float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\r\n * (float beta, float alpha, torch.FloatTensor mat1, torch.FloatTensor mat2)\r\n      didn't match because some of the arguments have invalid types: (int, int, torch.FloatTensor, torch.cuda.FloatTensor)\r\n * (float beta, float alpha, torch.SparseFloatTensor mat1, torch.FloatTensor mat2)\r\n      didn't match because some of the arguments have invalid types: (int, int, torch.FloatTensor, torch.cuda.FloatTensor)\r\n```\r\n\r\nHowever, the following works:\r\n\r\n```python\r\nif torch.cuda.is_available():\r\n    gpu_model = no_gpu_model.cuda()\r\n    try:\r\n        print(gpu_model.forward(batch.cuda()).size())\r\n    except TypeError as e:\r\n        print(e)   \r\n# OUT: torch.Size([10, 1])\r\n```\r\n\r\nIt is the second case, I would want some feedback weather the variables should be send to cuda or not based on the module. \r\n\r\nThe reason this functionality will be required is in the case when someone explicitly sets a module to run on cuda backend. In that case using this flag we can set all the input values also on cuda. Otherwise, there will be compatibility issues running the same code on CPU and GPU. \r\n\r\nThe reason I was suggesting some explicit context manager is that in the context manager we can do a graceful fallback to CPU instead of giving this error. \r\n\r\n@apaszke I didn't know about `torch.set_default_tensor_type`. Might be useful in a python module (again I am thinking along the lines of context manager, so that one modules defaults don't spill to other code ). I think having everything of default type might not be a very good idea. But in that case, the user should have the freedom to choose, what types they want, dynamically, based on the available device. "}