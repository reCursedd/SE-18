{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/177843984", "pull_request_review_id": 107793294, "id": 177843984, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3Nzg0Mzk4NA==", "diff_hunk": "@@ -0,0 +1,297 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/native/SpectralOpsUtils.h\"\n+#include \"ATen/Config.h\"\n+\n+#if !AT_MKL_ENABLED()\n+\n+namespace at { namespace native {\n+\n+Tensor _fft_mkl(const Tensor& input, int64_t signal_ndim,\n+                bool complex_input, bool complex_output,\n+                bool inverse, IntList checked_signal_sizes,\n+                bool normalized, bool onesided,\n+                IntList output_sizes) {\n+  throw std::runtime_error(\"fft: ATen not compiled with MKL support\");\n+}\n+\n+}}\n+\n+#else // AT_MKL_ENABLED\n+\n+#include \"ATen/ATen.h\"\n+#include \"ATen/Config.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/NativeFunctions.h\"\n+\n+#include <algorithm>\n+#include <vector>\n+#include <numeric>\n+#include <cmath>\n+\n+#include <mkl_dfti.h>\n+#include <ATen/mkl/Exceptions.h>\n+#include <ATen/mkl/Descriptors.h>\n+#include <ATen/mkl/Limits.h>\n+\n+#ifdef _OPENMP\n+#include <omp.h>\n+#endif\n+\n+namespace at { namespace native {\n+\n+// In real-to-complex transform, MKL FFT only fills half of the values due to\n+// conjugate symmetry. See native/SpectralUtils.h for more details.\n+// The following structs are used to fill in the other half with symmetry in\n+// case of real-to-complex transform with onesided=False flag.\n+\n+template <typename scalar_t>\n+static inline void _fft_fill_with_conjugate_symmetry_slice(Tensor& output,\n+                       int64_t signal_ndim, int64_t size_last_dim,\n+                       int64_t start_last_dim_idx, int64_t i, int64_t num) {\n+  scalar_t *data = output.data<scalar_t>();\n+\n+  // A slice means a slice of last dimension (of size size_last_dim)\n+\n+  // This function iterates through the slices to fill, i.e. to_slice_data\n+  // (basically data_slices[i:i+num]), and keeps track of the slices it reads\n+  // data from, i.e., from_slice_data, using from_slice_indices, a vector\n+  // containing the index of the from_slice_data slice.\n+\n+  // Compute the indices for the first from_slice_data\n+  std::vector<int64_t> from_slice_indices(signal_ndim);  // up to before last signal dim\n+  int64_t remainder = i;\n+  // set last signal dim values\n+  int64_t from_slice_offset = 0;\n+  for (int64_t d = signal_ndim - 1; d >= 0; d--) {\n+    int64_t dim_size = output.size(d);\n+    int64_t dim_idx = remainder % dim_size;\n+    remainder = remainder / dim_size;\n+    from_slice_indices[d] = dim_idx;\n+    if (d == 0) {\n+      from_slice_offset += dim_idx * output.stride(d);\n+    } else if (dim_idx != 0) {\n+      from_slice_offset += (dim_size - dim_idx) * output.stride(d);\n+    }\n+  }\n+\n+  // First to_slice_data and from_slice_data\n+  scalar_t *to_slice_data = data + i * size_last_dim * 2;\n+  scalar_t *from_slice_data = data + from_slice_offset;\n+\n+  while (num > 0) {\n+    // Fill to_slice_data from values in from_slice_data\n+    for (int64_t j = start_last_dim_idx; j < size_last_dim; j++) {\n+      // multiply index by 2 because of the last complex dim has size 2\n+      int64_t to_idx = j * 2;\n+      int64_t from_idx = (size_last_dim - j) * 2;\n+      to_slice_data[to_idx] = from_slice_data[from_idx];\n+      to_slice_data[to_idx + 1] = -from_slice_data[from_idx + 1];\n+    }\n+    // Compute the next to_slice_data and from_slice_data slices\n+    to_slice_data += size_last_dim * 2;\n+    for (int64_t d = signal_ndim - 1; d >= 0; d--) {\n+      // Compute the next index at this dimension using conjugate symmetry\n+      // Break out of this loop if nothing carries over\n+      from_slice_indices[d] = (from_slice_indices[d] + 1) % output.size(d);\n+      if (d > 0) {\n+        // At d > 0 nonbatch dim, to get next from_slice_data offset\n+        //   1. if this dim idx becomes 1, will need to add (size - 1) * stride\n+        //   2. otherwise, will need to subtract stride\n+        if (from_slice_indices[d] == 0) {\n+          // Substract. Carries over to previous dimension\n+          from_slice_data -= output.stride(d);\n+        } else if (from_slice_indices[d] == 1) {\n+          // Dimension index becomes 1\n+          // Doesn't carry over to previous dimension\n+          from_slice_data += (output.size(d) - 1) * output.stride(d);\n+          break;\n+        } else {\n+          // Substract. Doesn't carry over to previous dimension\n+          from_slice_data -= output.stride(d);\n+          break;\n+        }\n+      } else {\n+        // At d = 0 nonbatch dim, it means that to_slice_data ise now at a the\n+        // beginning of a data sample. It maps to itself by conjugate symmetry.\n+        from_slice_data = to_slice_data;\n+      }\n+    }\n+    num--;\n+  }\n+}\n+\n+// input should be a contiguous batched tensor of same size as full (twosided)\n+// signals, but only contains half (onesided) of the values.\n+// This function modifies inplace.\n+static inline void _fft_fill_with_conjugate_symmetry_(Tensor& input,\n+                      int64_t signal_ndim, int64_t size_last_dim,\n+                      int64_t last_dim_start_slice) {\n+  if (last_dim_start_slice >= size_last_dim) {\n+    return;\n+  }\n+\n+  int64_t num = 1;\n+  for (int64_t d = 0; d < signal_ndim; d++) {\n+    num *= input.size(d);\n+  }\n+#ifdef _OPENMP\n+  if (num > 500) {\n+    int nthreads = omp_get_num_threads();\n+    int64_t num_slices_per_thread = num / nthreads + 1;\n+    #pragma omp parallel\n+    {\n+      int tid = omp_get_thread_num();\n+      int64_t start = tid * num_slices_per_thread;\n+      AT_DISPATCH_FLOATING_TYPES(input.type(), \"_fft_fill_with_conjugate_symmetry\", [&] {\n+        _fft_fill_with_conjugate_symmetry_slice<scalar_t>(input, signal_ndim, size_last_dim,\n+            last_dim_start_slice, start, std::min(num_slices_per_thread, num - start));\n+      });\n+    }\n+    return;\n+  }\n+#endif\n+  AT_DISPATCH_FLOATING_TYPES(input.type(), \"_fft_fill_with_conjugate_symmetry\", [&] {\n+    _fft_fill_with_conjugate_symmetry_slice<scalar_t>(input, signal_ndim, size_last_dim,\n+        last_dim_start_slice, 0, num);\n+  });\n+}\n+\n+// Returns undefined tensor if the parameter range is greater than MKL_LONG.\n+Tensor _fft_mkl(const Tensor& self, int64_t signal_ndim,\n+                bool complex_input, bool complex_output,\n+                bool inverse, IntList checked_signal_sizes,\n+                bool normalized, bool onesided,\n+                IntList output_sizes) {\n+  int64_t batch = self.size(0);\n+  Tensor input = self;\n+  // real/imag dimension must be like complex type\n+  if (complex_input && input.stride(-1) != 1) {\n+    input = input.contiguous();\n+  }\n+\n+  // check if we can use MKL because MKL_LONG is 32bit on some OS, e.g. Windows\n+  // need to check input and output size and strides\n+  // be careful about complex domain, where the stride needs to be divided by 2\n+  // only need to test upper bound MKL_LONG_MAX as these values are non-negative\n+  if (sizeof(MKL_LONG) < sizeof(int64_t)) {", "path": "aten/src/ATen/native/mkl/SpectralOps.cpp", "position": 177, "original_position": 177, "commit_id": "0f0d59ae356362438b95c5bccb195be5e283c849", "original_commit_id": "0f0d59ae356362438b95c5bccb195be5e283c849", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "MKL is nicer than cuFFT. It takes in strides directly. So non-contiguous tensors are fine, as long as we can represent everything using `MKL_LONG`.", "created_at": "2018-03-28T18:17:32Z", "updated_at": "2018-11-23T15:41:14Z", "html_url": "https://github.com/pytorch/pytorch/pull/5856#discussion_r177843984", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5856", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/177843984"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5856#discussion_r177843984"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5856"}}, "body_html": "<p>MKL is nicer than cuFFT. It takes in strides directly. So non-contiguous tensors are fine, as long as we can represent everything using <code>MKL_LONG</code>.</p>", "body_text": "MKL is nicer than cuFFT. It takes in strides directly. So non-contiguous tensors are fine, as long as we can represent everything using MKL_LONG.", "in_reply_to_id": 177841621}