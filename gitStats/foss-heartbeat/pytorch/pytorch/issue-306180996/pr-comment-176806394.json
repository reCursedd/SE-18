{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/176806394", "pull_request_review_id": 106580696, "id": 176806394, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NjgwNjM5NA==", "diff_hunk": "@@ -0,0 +1,274 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/Config.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/native/SpectralOpsUtils.h\"\n+#include \"ATen/native/cuda/CuFFTUtils.h\"\n+\n+#include \"ATen/cuda/AccumulateType.cuh\"\n+#include \"ATen/cuda/CUDATensorMethods.cuh\"\n+#include \"ATen/cuda/CUDATypeConversion.cuh\"\n+\n+#include <THC/THCDeviceUtils.cuh>\n+#include <THC/THCNumerics.cuh>\n+#include <THC/THCTensorMathReduce.cuh>\n+#include <THC/THCTensorSort.cuh>\n+#include <THC/THCThrustAllocator.cuh>\n+#include <THCUNN/THCHalfAutoNumerics.cuh>\n+\n+#include <thrust/execution_policy.h>\n+#include <thrust/unique.h>\n+#include <cufft.h>\n+#include <cufftXt.h>\n+#include <cmath>\n+#include <numeric>\n+#include <iostream>\n+\n+namespace at { namespace native {\n+\n+__forceinline__\n+static bool is_pow_of_two(long long int  x) {\n+  return (x & (x - 1)) == 0;\n+}\n+\n+// counting_iterator => index to fill\n+struct cnt_to_dst_idx_functor : public thrust::unary_function<int64_t, int64_t>\n+{\n+  const int64_t last_dim_size;\n+  const int64_t last_dim_start_slice;\n+  const int64_t last_dim_to_fill_size;\n+\n+  cnt_to_dst_idx_functor(int64_t last_dim_size, int64_t last_dim_start_slice) :\n+    last_dim_size(last_dim_size), last_dim_start_slice(last_dim_start_slice),\n+    last_dim_to_fill_size(last_dim_size - last_dim_start_slice) {}\n+\n+  __host__ __device__ __forceinline__\n+  int64_t operator()(const int64_t& i) const\n+  {\n+    int64_t imag = i % 2;\n+    int64_t idx = i / 2;\n+    int64_t num_dim = idx / last_dim_to_fill_size;\n+    int64_t slice_idx = idx % last_dim_to_fill_size;\n+    return (num_dim * last_dim_size + last_dim_start_slice + slice_idx) * 2 + imag;\n+  }\n+};\n+\n+// index to fill => index to read from\n+template <typename scalar_t>\n+struct dst_idx_to_src_functor : public thrust::unary_function<int64_t, scalar_t>\n+{\n+  // output can have at most dim 5 (batch + 3 signal dim + real/imag)\n+  int64_t sizes[5], strides[5];\n+  const int64_t signal_ndim;\n+  scalar_t *data;  // device ptr\n+\n+  dst_idx_to_src_functor(const Tensor& batched_complex_signal)\n+    : signal_ndim(batched_complex_signal.dim() - 1),\n+      data(batched_complex_signal.data<scalar_t>()) {\n+    for (int64_t i = 0; i < signal_ndim; i++) {\n+      sizes[i] = batched_complex_signal.size(i);\n+      strides[i] = batched_complex_signal.stride(i);\n+    }\n+  }\n+\n+  __device__ __forceinline__\n+  scalar_t operator()(const int64_t& write_idx_with_imag) const\n+  {\n+    int64_t imag = write_idx_with_imag % 2;\n+    // all but first (batch) and last (real/imag) dims need to be reflected\n+    int64_t read_idx = 0;\n+    int64_t remainder = write_idx_with_imag - imag;\n+    int64_t dim_idx, dim_stride;\n+    for (int64_t i = 0; i < signal_ndim; i++) {\n+      dim_stride = strides[i];\n+      dim_idx = remainder / dim_stride;\n+      if (i == 0) {\n+        read_idx += dim_idx * dim_stride;\n+      } else if (dim_idx != 0) {\n+        read_idx += (sizes[i] - dim_idx) * dim_stride;\n+      }\n+      remainder = remainder % dim_stride;\n+    }\n+    if (imag) {\n+      return -data[read_idx + 1];\n+    } else {\n+      return data[read_idx];\n+    }\n+  }\n+};\n+\n+// input should be a contiguous batched tensor of full signals (i.e. twosided)\n+static inline void _fft_fill_with_hermitian_symmetry_(Tensor& input,\n+                      int64_t size_last_dim, int64_t last_dim_start_slice) {\n+  if (last_dim_start_slice >= size_last_dim) {\n+    return;\n+  }\n+\n+  // copy\n+  int64_t n = input.numel() / size_last_dim * (size_last_dim - last_dim_start_slice);\n+\n+  cudaStream_t stream = globalContext().getCurrentCUDAStream();\n+  auto allocator = THCThrustAllocator(globalContext().lazyInitCUDA());\n+  auto policy = thrust::cuda::par(allocator).on(stream);\n+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"_fft_fill_with_hermitian_symmetry_\", [&] {\n+    using cuda_scalar_t = cuda::type<scalar_t>;\n+    typedef thrust::device_ptr<cuda_scalar_t> device_ptr;\n+    typedef thrust::counting_iterator<int64_t> counter;\n+    typedef thrust::transform_iterator<cnt_to_dst_idx_functor, counter> dst_idx_iterator;\n+    typedef thrust::permutation_iterator<device_ptr, dst_idx_iterator> dst_iterator;\n+    typedef thrust::transform_iterator<dst_idx_to_src_functor<cuda_scalar_t>, dst_idx_iterator> src_iterator;\n+\n+    dst_idx_iterator dst_idxs(counter(0), cnt_to_dst_idx_functor(size_last_dim, last_dim_start_slice));\n+\n+    auto data = device_ptr(input.data<cuda_scalar_t>());\n+    dst_iterator dsts(data, dst_idxs);\n+    src_iterator srcs(dst_idxs, dst_idx_to_src_functor<cuda_scalar_t>(input));\n+    thrust::copy_n(policy, srcs, n, dsts);\n+  });\n+}\n+\n+// cufft\n+Tensor _fft_cufft(const Tensor& self, int64_t signal_ndim,\n+                  bool complex_input, bool complex_output,\n+                  bool inverse, IntList checked_signal_sizes,\n+                  bool normalized, bool onesided,\n+                  IntList output_sizes) {\n+  Tensor input = self;\n+\n+  bool is_half = input.type().scalarType() == ScalarType::Half;\n+\n+  // cuFFT requires input and output data pointers to complex type aligned\n+  // our allocated output tensor is always 256 bytes aligned so it is fine, but\n+  // we need to check input tensor to make sure that it is not unaligned, e.g.\n+  // from a slicing.\n+  auto complex_size_bytes = 2 * input.type().elementSizeInBytes();\n+  if (reinterpret_cast<std::uintptr_t>(input.data_ptr()) % complex_size_bytes != 0) {\n+    input = self.clone();\n+  }\n+\n+  // check input batch size\n+  long long int batch = input.size(0);\n+\n+  // check the input sizes and strides to see if we need to make it contiguous\n+  std::vector<long long int> signal_sizes(signal_ndim);\n+  std::vector<long long int> inembeds(signal_ndim);\n+  long long int istride, signal_size;\n+  bool need_contiguous = input.stride(signal_ndim) == 0;\n+  if (complex_input) {\n+    // real/imag dimension must be like complex type\n+    need_contiguous |= input.stride(-1) != 1;\n+  } else if (is_half) {\n+    // For half, base strides on the real part of real-to-complex and\n+    // complex-to-real transforms are not supported. Since our output is always\n+    // contiguous, only need to check real-to-complex case.\n+    need_contiguous |= input.stride(signal_ndim) != 1;\n+  }\n+  // store last tensor stride to infer inembeds array\n+  // complex input's last dim is size 2 and contiguous\n+  long long int ilast_stride = complex_input ? 2 : 1;\n+   // for each signal dim from innermost to outermost\n+  for (int64_t i = signal_ndim - 1; i >= 0; i--) {\n+    signal_size = checked_signal_sizes[i];\n+    istride = input.stride(i + 1);\n+    if (is_half && !is_pow_of_two(signal_size)) {\n+      std::ostringstream ss;\n+      ss << \"cuFFT doesn't support signals of half type with size that is not \"\n+         << \"a power of two, but gets a signal size of \" << signal_size << \" at\"\n+         << \"signal dimension \" << i;\n+      throw std::runtime_error(ss.str());\n+    }\n+    signal_sizes[i] = signal_size;\n+    if (!need_contiguous) {\n+      // set the inembeds for dim in last dimension\n+      if (ilast_stride == 0) {\n+        need_contiguous = istride != 0;\n+        if (i < signal_ndim - 1) {\n+          inembeds[i + 1] = 0;\n+        }\n+      } else {\n+        need_contiguous = istride % ilast_stride != 0;\n+        if (i < signal_ndim - 1) {\n+          inembeds[i + 1] = istride / ilast_stride;\n+        }\n+      }\n+      ilast_stride = istride;\n+    }\n+  }\n+\n+  need_contiguous |= (input.size(0) > 1 && input.stride(0) == 0);\n+  if (need_contiguous) {\n+    input = input.contiguous();\n+    auto input_sizes = input.sizes();\n+    std::copy(&input_sizes[1], &input_sizes[signal_ndim] + 1, &inembeds[0]);\n+  }\n+\n+  // set base_istride (stride at innermost dim of signal)\n+  // set idist (stride at batch dim)\n+  long long int idist = complex_input ? input.stride(0) >> 1 : input.stride(0);\n+  // Even if batch dimension is one, then idist (stride(0)) doesn't matter,\n+  // cuFFT errors if idist = 0. This is hack to make it succeed.\n+  if (idist == 0 && batch == 1) {\n+    idist = 1;\n+  }\n+  long long int base_istride = complex_input ? input.stride(signal_ndim) >> 1 : input.stride(signal_ndim);\n+\n+  // output, base_ostride, odist\n+  auto output = input.type().tensor(output_sizes);\n+  std::vector<long long int> onembeds(&output_sizes[1], &output_sizes[signal_ndim] + 1);\n+  long long int odist = complex_output ? output.stride(0) >> 1 : output.stride(0);\n+  long long int base_ostride = 1;\n+\n+  CufftHandle plan;\n+  size_t ws = 0;\n+  cudaDataType itype, otype, exec_type;\n+  if (input.type().scalarType() == ScalarType::Float) {\n+    itype = complex_input ? CUDA_C_32F : CUDA_R_32F;\n+    otype = complex_output ? CUDA_C_32F : CUDA_R_32F;\n+    exec_type = CUDA_C_32F;\n+  } else if (input.type().scalarType() == ScalarType::Double) {\n+    itype = complex_input ? CUDA_C_64F : CUDA_R_64F;\n+    otype = complex_output ? CUDA_C_64F : CUDA_R_64F;\n+    exec_type = CUDA_C_64F;\n+  } else if (input.type().scalarType() == ScalarType::Half) {\n+    itype = complex_input ? CUDA_C_16F : CUDA_R_16F;\n+    otype = complex_output ? CUDA_C_16F : CUDA_R_16F;\n+    exec_type = CUDA_C_16F;\n+  } else {\n+    std::ostringstream ss;\n+    ss << \"cuFFT doesn't support tensor of type: \"\n+       << at::toString(input.type().scalarType());\n+    throw std::runtime_error(ss.str());\n+  }\n+\n+  CUFFT_CHECK(cufftXtMakePlanMany(plan.get(), signal_ndim, &signal_sizes[0], &inembeds[0],", "path": "aten/src/ATen/native/cuda/SpectralOps.cu", "position": null, "original_position": 243, "commit_id": "0f0d59ae356362438b95c5bccb195be5e283c849", "original_commit_id": "1e3e1fdf253de18864657ebecdf9065ce2f465e1", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "We're C++11, you can use `signal_sizes.data()`", "created_at": "2018-03-23T17:19:37Z", "updated_at": "2018-11-23T15:41:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/5856#discussion_r176806394", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5856", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/176806394"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5856#discussion_r176806394"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5856"}}, "body_html": "<p>We're C++11, you can use <code>signal_sizes.data()</code></p>", "body_text": "We're C++11, you can use signal_sizes.data()"}