{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/176787932", "pull_request_review_id": 106558876, "id": 176787932, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3Njc4NzkzMg==", "diff_hunk": "@@ -5,33 +5,152 @@\n #endif\n \n #include \"ATen/ATen.h\"\n+#include \"ATen/Config.h\"\n #include \"ATen/NativeFunctions.h\"\n-#include \"ATen/WrapDimUtils.h\"\n-#include \"ATen/ExpandUtils.h\"\n+#include \"ATen/native/SpectralOpsUtils.h\"\n \n #include <algorithm>\n-#include <functional>\n-#include <numeric>\n #include <vector>\n-\n+#include <cmath>\n \n namespace at { namespace native {\n \n-// Since real-to-complex satisfy the Hermitian symmetry, i.e.,\n-// X[m, \\omega] = X[m, N - \\omega]*. We return only the first floor(N / 2) + 1\n-// values by default. This is also the assumption in libraries including cuFFT.\n-static inline int64_t infer_ft_complex_length(int64_t real_length) {\n-  return (real_length >> 1) + 1;\n+static inline Tensor _fft(const Tensor &self, const int64_t signal_ndim,\n+           const bool complex_input, const bool complex_output,\n+           const bool inverse, IntList signal_sizes, const bool normalized,\n+           const bool onesided) {\n+\n+  if (signal_ndim < 1 || signal_ndim > 3) {\n+    std::ostringstream ss;\n+    ss << \"Expected signal_ndim to be 1, 2, or 3, but got signal_ndim=\"\n+       << signal_ndim;\n+    throw std::runtime_error(ss.str());\n+  }\n+  if (!at::isFloatingType(self.type().scalarType())) {\n+    std::ostringstream ss;\n+    ss << \"Expected an input tensor of floating types, but got input \"\n+       << self.type() << self.sizes();\n+    throw std::runtime_error(ss.str());\n+  }\n+\n+  auto signal_tensor_ndim = signal_ndim + (int) complex_input;  // add complex dim\n+  if (self.dim() != signal_tensor_ndim && self.dim() != signal_tensor_ndim + 1) {\n+    std::ostringstream ss;\n+    ss << \"Given signal_ndim=\" << signal_ndim << \", expected an input tensor \"\n+       << \"of \" << signal_tensor_ndim << \"D or \" << signal_tensor_ndim + 1\n+       << \"D (batch mode)\";\n+    if (complex_input) {\n+      ss << \" (complex input adds an extra dimension)\";\n+    }\n+    ss << \", but got input \" << self.type() << self.sizes();\n+    throw std::runtime_error(ss.str());\n+  }\n+  bool is_batched = self.dim() == signal_tensor_ndim + 1;\n+\n+  Tensor input = self;\n+\n+  if (!is_batched) {\n+    input = input.unsqueeze(0);\n+  }\n+\n+  if (complex_input) {\n+    if (input.size(signal_ndim + 1) != 2) {\n+      std::ostringstream ss;\n+      ss << \"Expected an input tensor with a last dimension of size 2 \"\n+         << \"representing real + imaginary components, but got input \"\n+         << self.type() << self.sizes();\n+      throw std::runtime_error(ss.str());\n+    }\n+  }\n+\n+  // build signal_sizes and output_size\n+  if (signal_sizes.size() > 0 && (int64_t) signal_sizes.size() != signal_ndim) {\n+    std::ostringstream ss;\n+    ss << \"Expected signal_sizes to be empty (default) or of signal_ndim=\"\n+       << signal_ndim << \"D, but got signal_sizes=\" << signal_sizes;\n+    throw std::runtime_error(ss.str());\n+  }\n+  std::vector<int64_t> output_sizes(signal_ndim + 1 + (int) complex_output);\n+  output_sizes[0] = input.size(0);\n+  std::vector<int64_t> checked_signal_sizes(signal_ndim);\n+  int64_t input_size;\n+  for (int64_t i = 0; i < signal_ndim; i++) {\n+    input_size = input.size(i + 1);\n+    if (i == signal_ndim - 1 && onesided && complex_input && !complex_output) {\n+      // if last dim and complex-to-real onesided, input is only half of\n+      // signal, and we need to infer basing on signal_sizes, if given\n+      int64_t inferred_size;\n+      if (signal_sizes.size() > 0) {\n+        inferred_size = infer_ft_complex_to_real_onesided_size(input_size, signal_sizes[i]);\n+      } else {\n+        inferred_size = infer_ft_complex_to_real_onesided_size(input_size);\n+      }\n+      checked_signal_sizes[i] = inferred_size;\n+      output_sizes[i + 1] = inferred_size;\n+    } else {\n+      if (i == signal_ndim - 1 && onesided && !complex_input && complex_output) {\n+        // if last dim and real-to-complex onesided, output should be only\n+        // half of the signal, and we need to infer using input_size\n+        output_sizes[i + 1] = infer_ft_real_to_complex_onesided_size(input_size);\n+      } else {\n+        output_sizes[i + 1] = input_size;\n+      }\n+      checked_signal_sizes[i] = input_size;\n+      if (signal_sizes.size() > 0 && signal_sizes[i] != checked_signal_sizes[i]) {\n+        std::ostringstream ss;\n+        ss << \"Expected given signal_sizes=\" << signal_sizes << \" to have same \"\n+           << \"shape with input at signal dimension \" << i << \", but got \"\n+           << \"signal_sizes[\" << i << \"] = \" << signal_sizes[i] << \" and \"\n+           << \"input.size(\" << i + (int)is_batched << \") = \" << input_size;\n+        throw std::runtime_error(ss.str());\n+      }\n+    }\n+  }\n+  if (complex_output) {\n+    output_sizes[signal_ndim + 1] = 2;\n+  }\n+\n+  Tensor output = at::_fft_with_size(input, signal_ndim, complex_input,\n+                                     complex_output, inverse,\n+                                     checked_signal_sizes, normalized, onesided,\n+                                     output_sizes);\n+\n+  // un-batch if needed\n+  if (!is_batched) {\n+    return output.squeeze_(0);\n+  } else {\n+    return output;\n+  }\n }\n \n+Tensor fft(const Tensor& self, const int64_t signal_ndim, const bool normalized) {\n+  return _fft(self, signal_ndim, true, true, false, {}, normalized, false);\n+}\n+\n+Tensor ifft(const Tensor& self, const int64_t signal_ndim, const bool normalized) {\n+  return _fft(self, signal_ndim, true, true, true, {}, normalized, false);\n+}\n+\n+Tensor rfft(const Tensor& self, const int64_t signal_ndim, const bool normalized,\n+           const bool onesided) {\n+  return _fft(self, signal_ndim, false, true, false, {}, normalized, onesided);\n+}\n+\n+Tensor irfft(const Tensor& self, const int64_t signal_ndim,  IntList signal_sizes,\n+            const bool normalized, const bool onesided) {\n+  return _fft(self, signal_ndim, true, false, true, signal_sizes, normalized, onesided);\n+}", "path": "aten/src/ATen/native/SpectralOps.cpp", "position": null, "original_position": 148, "commit_id": "0f0d59ae356362438b95c5bccb195be5e283c849", "original_commit_id": "1e3e1fdf253de18864657ebecdf9065ce2f465e1", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "The given settings are the only valid ones here.", "created_at": "2018-03-23T16:18:20Z", "updated_at": "2018-11-23T15:41:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/5856#discussion_r176787932", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5856", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/176787932"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5856#discussion_r176787932"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5856"}}, "body_html": "<p>The given settings are the only valid ones here.</p>", "body_text": "The given settings are the only valid ones here.", "in_reply_to_id": 176787371}