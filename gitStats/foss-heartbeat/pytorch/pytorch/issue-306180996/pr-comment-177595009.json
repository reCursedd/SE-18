{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/177595009", "pull_request_review_id": 107494499, "id": 177595009, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NzU5NTAwOQ==", "diff_hunk": "@@ -0,0 +1,274 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/Config.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/native/SpectralOpsUtils.h\"\n+#include \"ATen/native/cuda/CuFFTUtils.h\"\n+\n+#include \"ATen/cuda/AccumulateType.cuh\"\n+#include \"ATen/cuda/CUDATensorMethods.cuh\"\n+#include \"ATen/cuda/CUDATypeConversion.cuh\"\n+\n+#include <THC/THCDeviceUtils.cuh>\n+#include <THC/THCNumerics.cuh>\n+#include <THC/THCTensorMathReduce.cuh>\n+#include <THC/THCTensorSort.cuh>\n+#include <THC/THCThrustAllocator.cuh>\n+#include <THCUNN/THCHalfAutoNumerics.cuh>\n+\n+#include <thrust/execution_policy.h>\n+#include <thrust/unique.h>\n+#include <cufft.h>\n+#include <cufftXt.h>\n+#include <cmath>\n+#include <numeric>\n+#include <iostream>\n+\n+namespace at { namespace native {\n+\n+__forceinline__\n+static bool is_pow_of_two(long long int  x) {\n+  return (x & (x - 1)) == 0;\n+}\n+\n+// counting_iterator => index to fill\n+struct cnt_to_dst_idx_functor : public thrust::unary_function<int64_t, int64_t>\n+{\n+  const int64_t last_dim_size;\n+  const int64_t last_dim_start_slice;\n+  const int64_t last_dim_to_fill_size;\n+\n+  cnt_to_dst_idx_functor(int64_t last_dim_size, int64_t last_dim_start_slice) :\n+    last_dim_size(last_dim_size), last_dim_start_slice(last_dim_start_slice),\n+    last_dim_to_fill_size(last_dim_size - last_dim_start_slice) {}\n+\n+  __host__ __device__ __forceinline__\n+  int64_t operator()(const int64_t& i) const\n+  {\n+    int64_t imag = i % 2;\n+    int64_t idx = i / 2;\n+    int64_t num_dim = idx / last_dim_to_fill_size;\n+    int64_t slice_idx = idx % last_dim_to_fill_size;\n+    return (num_dim * last_dim_size + last_dim_start_slice + slice_idx) * 2 + imag;\n+  }\n+};\n+\n+// index to fill => index to read from\n+template <typename scalar_t>\n+struct dst_idx_to_src_functor : public thrust::unary_function<int64_t, scalar_t>\n+{\n+  // output can have at most dim 5 (batch + 3 signal dim + real/imag)\n+  int64_t sizes[5], strides[5];\n+  const int64_t signal_ndim;\n+  scalar_t *data;  // device ptr\n+\n+  dst_idx_to_src_functor(const Tensor& batched_complex_signal)\n+    : signal_ndim(batched_complex_signal.dim() - 1),\n+      data(batched_complex_signal.data<scalar_t>()) {\n+    for (int64_t i = 0; i < signal_ndim; i++) {\n+      sizes[i] = batched_complex_signal.size(i);\n+      strides[i] = batched_complex_signal.stride(i);\n+    }\n+  }\n+\n+  __device__ __forceinline__\n+  scalar_t operator()(const int64_t& write_idx_with_imag) const\n+  {\n+    int64_t imag = write_idx_with_imag % 2;\n+    // all but first (batch) and last (real/imag) dims need to be reflected\n+    int64_t read_idx = 0;\n+    int64_t remainder = write_idx_with_imag - imag;\n+    int64_t dim_idx, dim_stride;\n+    for (int64_t i = 0; i < signal_ndim; i++) {\n+      dim_stride = strides[i];\n+      dim_idx = remainder / dim_stride;\n+      if (i == 0) {\n+        read_idx += dim_idx * dim_stride;\n+      } else if (dim_idx != 0) {\n+        read_idx += (sizes[i] - dim_idx) * dim_stride;\n+      }\n+      remainder = remainder % dim_stride;\n+    }\n+    if (imag) {\n+      return -data[read_idx + 1];\n+    } else {\n+      return data[read_idx];\n+    }\n+  }\n+};\n+\n+// input should be a contiguous batched tensor of full signals (i.e. twosided)\n+static inline void _fft_fill_with_hermitian_symmetry_(Tensor& input,\n+                      int64_t size_last_dim, int64_t last_dim_start_slice) {\n+  if (last_dim_start_slice >= size_last_dim) {\n+    return;\n+  }\n+\n+  // copy\n+  int64_t n = input.numel() / size_last_dim * (size_last_dim - last_dim_start_slice);\n+\n+  cudaStream_t stream = globalContext().getCurrentCUDAStream();\n+  auto allocator = THCThrustAllocator(globalContext().lazyInitCUDA());\n+  auto policy = thrust::cuda::par(allocator).on(stream);\n+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"_fft_fill_with_hermitian_symmetry_\", [&] {\n+    using cuda_scalar_t = cuda::type<scalar_t>;\n+    typedef thrust::device_ptr<cuda_scalar_t> device_ptr;\n+    typedef thrust::counting_iterator<int64_t> counter;\n+    typedef thrust::transform_iterator<cnt_to_dst_idx_functor, counter> dst_idx_iterator;\n+    typedef thrust::permutation_iterator<device_ptr, dst_idx_iterator> dst_iterator;\n+    typedef thrust::transform_iterator<dst_idx_to_src_functor<cuda_scalar_t>, dst_idx_iterator> src_iterator;\n+\n+    dst_idx_iterator dst_idxs(counter(0), cnt_to_dst_idx_functor(size_last_dim, last_dim_start_slice));\n+\n+    auto data = device_ptr(input.data<cuda_scalar_t>());\n+    dst_iterator dsts(data, dst_idxs);\n+    src_iterator srcs(dst_idxs, dst_idx_to_src_functor<cuda_scalar_t>(input));\n+    thrust::copy_n(policy, srcs, n, dsts);\n+  });\n+}\n+\n+// cufft\n+Tensor _fft_cufft(const Tensor& self, int64_t signal_ndim,\n+                  bool complex_input, bool complex_output,\n+                  bool inverse, IntList checked_signal_sizes,\n+                  bool normalized, bool onesided,\n+                  IntList output_sizes) {\n+  Tensor input = self;\n+\n+  bool is_half = input.type().scalarType() == ScalarType::Half;", "path": "aten/src/ATen/native/cuda/SpectralOps.cu", "position": 147, "original_position": 138, "commit_id": "0f0d59ae356362438b95c5bccb195be5e283c849", "original_commit_id": "1e3e1fdf253de18864657ebecdf9065ce2f465e1", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Added a test for this.", "created_at": "2018-03-27T22:47:16Z", "updated_at": "2018-11-23T15:41:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/5856#discussion_r177595009", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5856", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/177595009"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5856#discussion_r177595009"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5856"}}, "body_html": "<p>Added a test for this.</p>", "body_text": "Added a test for this.", "in_reply_to_id": 176809845}