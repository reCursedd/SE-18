{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/375743509", "html_url": "https://github.com/pytorch/pytorch/pull/5856#issuecomment-375743509", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5856", "id": 375743509, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTc0MzUwOQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-23T17:33:22Z", "updated_at": "2018-03-23T17:33:22Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Yes. One can use multiple gpu to do large FFT. But we don\u2019t do it here so\nit\u2019s fine.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Mar 23, 2018 at 13:32 Edward Z. Yang ***@***.***&gt; wrote:\n ***@***.**** commented on this pull request.\n ------------------------------\n\n In aten/src/ATen/native/cuda/SpectralOps.cu\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"306180996\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5856\" href=\"https://github.com/pytorch/pytorch/pull/5856#discussion_r176809845\">#5856 (comment)</a>&gt;:\n\n &gt; +    auto data = device_ptr(input.data&lt;cuda_scalar_t&gt;());\n +    dst_iterator dsts(data, dst_idxs);\n +    src_iterator srcs(dst_idxs, dst_idx_to_src_functor&lt;cuda_scalar_t&gt;(input));\n +    thrust::copy_n(policy, srcs, n, dsts);\n +  });\n +}\n +\n +// cufft\n +Tensor _fft_cufft(const Tensor&amp; self, int64_t signal_ndim,\n +                  bool complex_input, bool complex_output,\n +                  bool inverse, IntList checked_signal_sizes,\n +                  bool normalized, bool onesided,\n +                  IntList output_sizes) {\n +  Tensor input = self;\n +\n +  bool is_half = input.type().scalarType() == ScalarType::Half;\n\n I am reading the half restrictions and they state \"More than one GPU is\n not supported\". Do you know what this means?\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"306180996\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5856\" href=\"https://github.com/pytorch/pytorch/pull/5856#pullrequestreview-106585019\">#5856 (review)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AFaWZa1dENEDaz5aG1KealWc8f0Ythi0ks5thTGbgaJpZM4Su66h\">https://github.com/notifications/unsubscribe-auth/AFaWZa1dENEDaz5aG1KealWc8f0Ythi0ks5thTGbgaJpZM4Su66h</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Yes. One can use multiple gpu to do large FFT. But we don\u2019t do it here so\nit\u2019s fine.\n\u2026\nOn Fri, Mar 23, 2018 at 13:32 Edward Z. Yang ***@***.***> wrote:\n ***@***.**** commented on this pull request.\n ------------------------------\n\n In aten/src/ATen/native/cuda/SpectralOps.cu\n <#5856 (comment)>:\n\n > +    auto data = device_ptr(input.data<cuda_scalar_t>());\n +    dst_iterator dsts(data, dst_idxs);\n +    src_iterator srcs(dst_idxs, dst_idx_to_src_functor<cuda_scalar_t>(input));\n +    thrust::copy_n(policy, srcs, n, dsts);\n +  });\n +}\n +\n +// cufft\n +Tensor _fft_cufft(const Tensor& self, int64_t signal_ndim,\n +                  bool complex_input, bool complex_output,\n +                  bool inverse, IntList checked_signal_sizes,\n +                  bool normalized, bool onesided,\n +                  IntList output_sizes) {\n +  Tensor input = self;\n +\n +  bool is_half = input.type().scalarType() == ScalarType::Half;\n\n I am reading the half restrictions and they state \"More than one GPU is\n not supported\". Do you know what this means?\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#5856 (review)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AFaWZa1dENEDaz5aG1KealWc8f0Ythi0ks5thTGbgaJpZM4Su66h>\n .", "body": "Yes. One can use multiple gpu to do large FFT. But we don\u2019t do it here so\nit\u2019s fine.\n\nOn Fri, Mar 23, 2018 at 13:32 Edward Z. Yang <notifications@github.com>\nwrote:\n\n> *@ezyang* commented on this pull request.\n> ------------------------------\n>\n> In aten/src/ATen/native/cuda/SpectralOps.cu\n> <https://github.com/pytorch/pytorch/pull/5856#discussion_r176809845>:\n>\n> > +    auto data = device_ptr(input.data<cuda_scalar_t>());\n> +    dst_iterator dsts(data, dst_idxs);\n> +    src_iterator srcs(dst_idxs, dst_idx_to_src_functor<cuda_scalar_t>(input));\n> +    thrust::copy_n(policy, srcs, n, dsts);\n> +  });\n> +}\n> +\n> +// cufft\n> +Tensor _fft_cufft(const Tensor& self, int64_t signal_ndim,\n> +                  bool complex_input, bool complex_output,\n> +                  bool inverse, IntList checked_signal_sizes,\n> +                  bool normalized, bool onesided,\n> +                  IntList output_sizes) {\n> +  Tensor input = self;\n> +\n> +  bool is_half = input.type().scalarType() == ScalarType::Half;\n>\n> I am reading the half restrictions and they state \"More than one GPU is\n> not supported\". Do you know what this means?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/pull/5856#pullrequestreview-106585019>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFaWZa1dENEDaz5aG1KealWc8f0Ythi0ks5thTGbgaJpZM4Su66h>\n> .\n>\n"}