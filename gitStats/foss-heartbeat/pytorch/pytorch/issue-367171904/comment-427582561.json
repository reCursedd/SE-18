{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427582561", "html_url": "https://github.com/pytorch/pytorch/pull/12368#issuecomment-427582561", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12368", "id": 427582561, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzU4MjU2MQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-06T15:25:33Z", "updated_at": "2018-10-06T15:25:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So it turns out that for fp16 even when using the variance in save_var, it doesn't work. Previously, this didn't affect the forward as we kept mean and var in accscalar_t variable within the kernel. Now that separated it into two kernels for performance reasons, we see that keeping it in fp16 isn't enough.<br>\nMy conclusion is that we would would want to have save_mean/save_var as fp32 for fp16 batch norm. (This is part of what is done in CuDNN, but only for the stats that aren't handed to the user.)</p>", "body_text": "So it turns out that for fp16 even when using the variance in save_var, it doesn't work. Previously, this didn't affect the forward as we kept mean and var in accscalar_t variable within the kernel. Now that separated it into two kernels for performance reasons, we see that keeping it in fp16 isn't enough.\nMy conclusion is that we would would want to have save_mean/save_var as fp32 for fp16 batch norm. (This is part of what is done in CuDNN, but only for the stats that aren't handed to the user.)", "body": "So it turns out that for fp16 even when using the variance in save_var, it doesn't work. Previously, this didn't affect the forward as we kept mean and var in accscalar_t variable within the kernel. Now that separated it into two kernels for performance reasons, we see that keeping it in fp16 isn't enough.\r\nMy conclusion is that we would would want to have save_mean/save_var as fp32 for fp16 batch norm. (This is part of what is done in CuDNN, but only for the stats that aren't handed to the user.)\r\n"}