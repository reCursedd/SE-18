{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225315040", "pull_request_review_id": 164893828, "id": 225315040, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNTMxNTA0MA==", "diff_hunk": "@@ -0,0 +1,444 @@\n+#include <THC/THCDeviceUtils.cuh>\n+#include <THC/THCGeneral.h>\n+#include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int WARP_SIZE = 64;\n+#else\n+constexpr int WARP_SIZE = 32;\n+#endif\n+\n+// The maximum number of threads in a block\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int MAX_BLOCK_SIZE = 256;\n+#else\n+constexpr int MAX_BLOCK_SIZE = 512;\n+#endif\n+\n+// Number of threads in a block given an input size up to MAX_BLOCK_SIZE\n+static int getNumThreads(int nElem) {\n+#if defined(__HIP_PLATFORM_HCC__)\n+  int threadSizes[5] = { 16, 32, 64, 128, MAX_BLOCK_SIZE };\n+#else\n+  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };\n+#endif\n+  for (int i = 0; i != 5; ++i) {\n+    if (nElem <= threadSizes[i]) {\n+      return threadSizes[i];\n+    }\n+  }\n+  return MAX_BLOCK_SIZE;\n+}\n+\n+// Returns the index of the most significant 1 bit in `val`.\n+__device__ __forceinline__ int getMSB(int val) {\n+  return 31 - __clz(val);\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct Float2 {\n+  accscalar_t v1, v2;\n+  __device__ Float2() {}\n+  __device__ Float2(scalar_t v1, scalar_t v2) : v1(static_cast<accscalar_t>(v1)), v2(static_cast<accscalar_t>(v2)) {}\n+  __device__ Float2(int v) : v1(static_cast<accscalar_t>(v)), v2(static_cast<accscalar_t>(v)) {}\n+  __device__ Float2& operator+=(const Float2& a) {\n+    v1 += a.v1;\n+    v2 += a.v2;\n+    return *this;\n+  }\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct SumOp {\n+  __device__ SumOp(const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& t) : tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    return static_cast<accscalar_t>(tensor[batch][plane][n]);\n+  }\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& tensor;\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct VarOp {\n+  __device__ VarOp(accscalar_t m, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& t) : mean(m), tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    accscalar_t val = tensor[batch][plane][n];\n+    return (val - mean) * (val - mean);\n+  }\n+  const accscalar_t mean;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& tensor;\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct GradOp {\n+  __device__ GradOp(accscalar_t m, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& i, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& g)\n+    : mean(m), input(i), grad_output(g) {}\n+  __device__ __forceinline__ Float2<scalar_t, accscalar_t> operator()(int batch, int plane, int n) {\n+    accscalar_t g = grad_output[batch][plane][n];\n+    accscalar_t c = static_cast<accscalar_t>(input[batch][plane][n]) - mean;\n+    return Float2<scalar_t, accscalar_t>(g, g * c);\n+  }\n+  const accscalar_t mean;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& input;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& grad_output;\n+};\n+\n+// Sum across all threads within a warp\n+template <typename T>\n+static __device__ __forceinline__ T warpSum(T val) {\n+#if __CUDA_ARCH__ >= 300\n+  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n+    val += WARP_SHFL_XOR(val, 1 << i, WARP_SIZE);\n+  }\n+#else\n+  __shared__ T values[MAX_BLOCK_SIZE];\n+  values[threadIdx.x] = val;\n+  __threadfence_block();\n+  const int base = (threadIdx.x / WARP_SIZE) * WARP_SIZE;\n+  for (int i = 1; i < WARP_SIZE; i++) {\n+    val += values[base + ((i + threadIdx.x) % WARP_SIZE)];\n+  }\n+#endif\n+  return val;\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+static __device__ __forceinline__ Float2<scalar_t, accscalar_t> warpSum(Float2<scalar_t, accscalar_t> value) {\n+  value.v1 = warpSum(value.v1);\n+  value.v2 = warpSum(value.v2);\n+  return value;\n+}\n+\n+// Sum across (batch, x/y/z) applying Op() pointwise\n+template<typename scalar_t, typename Op, typename PTA>\n+__device__ scalar_t reduce(Op op, PTA tensor, int plane) {\n+  scalar_t sum = static_cast<scalar_t>(0);\n+  for (int batch = 0; batch < tensor.size(0); ++batch) {\n+    for (int x = threadIdx.x; x < tensor.size(2); x += blockDim.x) {\n+      sum += op(batch, plane, x);\n+    }\n+  }\n+\n+  // sum over NumThreads within a warp\n+  sum = warpSum(sum);\n+\n+  // 'transpose', and reduce within warp again\n+  __shared__ scalar_t shared[WARP_SIZE];\n+  __syncthreads();\n+  if (threadIdx.x % WARP_SIZE == 0) {\n+    shared[threadIdx.x / WARP_SIZE] = sum;\n+  }\n+  if (threadIdx.x >= blockDim.x / WARP_SIZE && threadIdx.x < WARP_SIZE) {\n+    // zero out the other entries in shared\n+    shared[threadIdx.x] = (scalar_t)0;\n+  }\n+  __syncthreads();\n+  if (threadIdx.x / WARP_SIZE == 0) {\n+    sum = warpSum(shared[threadIdx.x]);\n+    if (threadIdx.x == 0) {\n+      shared[0] = sum;\n+    }\n+  }\n+  __syncthreads();\n+\n+  // Everyone picks it up, should be broadcast into the whole grad_input\n+  return shared[0];\n+}\n+\n+template <typename scalar_t, typename accscalar_t, bool train>\n+__global__ void batch_norm_transform_input_kernel(\n+    const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> input,\n+    PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> output,\n+    const PackedTensorAccessor<typename std::conditional<train, accscalar_t, scalar_t>::type, 1, at::RestrictPtrTraits> mean_,\n+    const PackedTensorAccessor<typename std::conditional<train, accscalar_t, scalar_t>::type, 1, at::RestrictPtrTraits> var_or_invstd,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> weight,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> bias,\n+    accscalar_t epsilon) {\n+\n+  int plane = blockIdx.y * blockDim.y + threadIdx.y;\n+\n+  if (plane >= input.size(1)) {\n+    return;\n+  }\n+\n+  accscalar_t gamma = weight.size(0) > 0 ? static_cast<accscalar_t>(weight[plane]) : static_cast<accscalar_t>(1);\n+  accscalar_t beta = bias.size(0) > 0 ? static_cast<accscalar_t>(bias[plane]) : static_cast<accscalar_t>(0);\n+  accscalar_t mean = static_cast<accscalar_t>(mean_[plane]);\n+  accscalar_t invstd;\n+  if (train) {\n+    invstd = var_or_invstd[plane];\n+  } else {\n+    invstd = static_cast<accscalar_t>(1) / std::sqrt(static_cast<accscalar_t>(var_or_invstd[plane]) + epsilon);\n+  }\n+  for (int64_t batch = blockIdx.x; batch < input.size(0); batch += gridDim.x) {\n+    for (int64_t feature = blockIdx.z; feature < input.size(2); feature += gridDim.z) {\n+      output[batch][plane][feature] = static_cast<scalar_t>(gamma * (input[batch][plane][feature] - mean) * invstd + beta);\n+    }\n+  }\n+}\n+\n+\n+template <typename scalar_t, typename accscalar_t>\n+__global__ void batch_norm_collect_statistics_kernel(\n+    const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> input,\n+    PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> output,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> weight,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> bias,\n+    const accscalar_t epsilon,\n+    const accscalar_t momentum,\n+    PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> running_mean,\n+    PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> running_var,\n+    PackedTensorAccessor<accscalar_t, 1, at::RestrictPtrTraits> save_mean,\n+    PackedTensorAccessor<accscalar_t, 1, at::RestrictPtrTraits> save_invstd) {\n+\n+  int plane = blockIdx.x;\n+  int N = input.size(0) * input.size(2);\n+\n+  accscalar_t norm = accscalar_t(1) / N;\n+\n+  // Compute the mean and variance across (batch, x/y/z)\n+  accscalar_t mean = reduce<accscalar_t>(SumOp<scalar_t, accscalar_t>(input), input, plane) * norm;\n+  __syncthreads();\n+  accscalar_t varN = reduce<accscalar_t>(VarOp<scalar_t, accscalar_t>(mean, input), input, plane);\n+\n+  // Save the mean, variance, and moving averages\n+  if (threadIdx.x == 0) {\n+    accscalar_t invstd = 0;\n+    if (varN != static_cast<accscalar_t>(0) || epsilon != static_cast<accscalar_t>(0)) {\n+      invstd = static_cast<accscalar_t>(1) / std::sqrt(varN * norm + epsilon);\n+    }\n+    save_mean[plane] = mean;\n+    save_invstd[plane] = invstd;\n+    if (running_mean.data() != NULL) {\n+      running_mean[plane] = static_cast<scalar_t>((1 - momentum) * running_mean[plane] + momentum * mean);\n+    }\n+    if (running_var.data() != NULL) {\n+      accscalar_t unbiasedVar = varN / (N - 1);\n+      running_var[plane] = static_cast<scalar_t>((1 - momentum) * running_var[plane] + momentum * unbiasedVar);\n+    }\n+  }\n+\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+__global__ void batch_norm_backward_kernel(\n+    const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> input,", "path": "aten/src/ATen/native/cuda/Normalization.cu", "position": null, "original_position": 231, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "5f3896fd66cfbde095029109f8065f26e87bf107", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I don't understand. What's the argument for **not** making them `__restrict__`? Weren't they previously `__restrict__`?", "created_at": "2018-10-15T21:00:12Z", "updated_at": "2018-11-23T15:53:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r225315040", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225315040"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r225315040"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>I don't understand. What's the argument for <strong>not</strong> making them <code>__restrict__</code>? Weren't they previously <code>__restrict__</code>?</p>", "body_text": "I don't understand. What's the argument for not making them __restrict__? Weren't they previously __restrict__?", "in_reply_to_id": 225012547}