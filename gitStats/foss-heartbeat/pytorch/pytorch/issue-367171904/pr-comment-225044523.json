{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225044523", "pull_request_review_id": 164557185, "id": 225044523, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNTA0NDUyMw==", "diff_hunk": "@@ -0,0 +1,444 @@\n+#include <THC/THCDeviceUtils.cuh>\n+#include <THC/THCGeneral.h>\n+#include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int WARP_SIZE = 64;\n+#else\n+constexpr int WARP_SIZE = 32;\n+#endif\n+\n+// The maximum number of threads in a block\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int MAX_BLOCK_SIZE = 256;\n+#else\n+constexpr int MAX_BLOCK_SIZE = 512;\n+#endif\n+\n+// Number of threads in a block given an input size up to MAX_BLOCK_SIZE\n+static int getNumThreads(int nElem) {\n+#if defined(__HIP_PLATFORM_HCC__)\n+  int threadSizes[5] = { 16, 32, 64, 128, MAX_BLOCK_SIZE };\n+#else\n+  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };\n+#endif\n+  for (int i = 0; i != 5; ++i) {\n+    if (nElem <= threadSizes[i]) {\n+      return threadSizes[i];\n+    }\n+  }\n+  return MAX_BLOCK_SIZE;\n+}\n+\n+// Returns the index of the most significant 1 bit in `val`.\n+__device__ __forceinline__ int getMSB(int val) {\n+  return 31 - __clz(val);\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct Float2 {\n+  accscalar_t v1, v2;\n+  __device__ Float2() {}\n+  __device__ Float2(scalar_t v1, scalar_t v2) : v1(static_cast<accscalar_t>(v1)), v2(static_cast<accscalar_t>(v2)) {}\n+  __device__ Float2(int v) : v1(static_cast<accscalar_t>(v)), v2(static_cast<accscalar_t>(v)) {}\n+  __device__ Float2& operator+=(const Float2& a) {\n+    v1 += a.v1;\n+    v2 += a.v2;\n+    return *this;\n+  }\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct SumOp {\n+  __device__ SumOp(const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& t) : tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    return static_cast<accscalar_t>(tensor[batch][plane][n]);\n+  }\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& tensor;\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct VarOp {\n+  __device__ VarOp(accscalar_t m, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& t) : mean(m), tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    accscalar_t val = tensor[batch][plane][n];\n+    return (val - mean) * (val - mean);\n+  }\n+  const accscalar_t mean;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& tensor;\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct GradOp {\n+  __device__ GradOp(accscalar_t m, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& i, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& g)\n+    : mean(m), input(i), grad_output(g) {}\n+  __device__ __forceinline__ Float2<scalar_t, accscalar_t> operator()(int batch, int plane, int n) {\n+    accscalar_t g = grad_output[batch][plane][n];\n+    accscalar_t c = static_cast<accscalar_t>(input[batch][plane][n]) - mean;\n+    return Float2<scalar_t, accscalar_t>(g, g * c);\n+  }\n+  const accscalar_t mean;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& input;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& grad_output;\n+};\n+\n+// Sum across all threads within a warp\n+template <typename T>\n+static __device__ __forceinline__ T warpSum(T val) {\n+#if __CUDA_ARCH__ >= 300\n+  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n+    val += WARP_SHFL_XOR(val, 1 << i, WARP_SIZE);\n+  }\n+#else\n+  __shared__ T values[MAX_BLOCK_SIZE];\n+  values[threadIdx.x] = val;\n+  __threadfence_block();\n+  const int base = (threadIdx.x / WARP_SIZE) * WARP_SIZE;\n+  for (int i = 1; i < WARP_SIZE; i++) {\n+    val += values[base + ((i + threadIdx.x) % WARP_SIZE)];\n+  }\n+#endif\n+  return val;\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+static __device__ __forceinline__ Float2<scalar_t, accscalar_t> warpSum(Float2<scalar_t, accscalar_t> value) {\n+  value.v1 = warpSum(value.v1);\n+  value.v2 = warpSum(value.v2);\n+  return value;\n+}\n+\n+// Sum across (batch, x/y/z) applying Op() pointwise\n+template<typename scalar_t, typename Op, typename PTA>\n+__device__ scalar_t reduce(Op op, PTA tensor, int plane) {\n+  scalar_t sum = static_cast<scalar_t>(0);\n+  for (int batch = 0; batch < tensor.size(0); ++batch) {\n+    for (int x = threadIdx.x; x < tensor.size(2); x += blockDim.x) {\n+      sum += op(batch, plane, x);\n+    }\n+  }\n+\n+  // sum over NumThreads within a warp\n+  sum = warpSum(sum);", "path": "aten/src/ATen/native/cuda/Normalization.cu", "position": 151, "original_position": 129, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "5f3896fd66cfbde095029109f8065f26e87bf107", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "This is unchanged logic from THCUNN, it took me a very close look to understand, too.\r\n\r\nIt's a two step warp sum to reduce from \"one result per thread\" to \"<=32 results\" to 1 result.\r\nWhat the code does is that it has a first warpSum that reduces from whatever number of threads (<= 1024=32*32) to <= 32 intermediate sums. Then the \"warp leader\" will put that into shared memory. Then there is another round of reduction with warpSum  where the intermediate sums are reduced.\r\nThe alternative pattern is a reduction where you reduce down to <=32 first an then have a single round of reduction. (Which is done e.g. for the WeightNorm code.)\r\nFrom what I've learned from @mcarilli and @jjsjann123, I think that it is a pattern that may offer some performance advantage, but not that much. In fact, my first stab at having a better backward uses the single reduction pattern.\r\n", "created_at": "2018-10-15T05:55:48Z", "updated_at": "2018-11-23T15:53:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r225044523", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225044523"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r225044523"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>This is unchanged logic from THCUNN, it took me a very close look to understand, too.</p>\n<p>It's a two step warp sum to reduce from \"one result per thread\" to \"&lt;=32 results\" to 1 result.<br>\nWhat the code does is that it has a first warpSum that reduces from whatever number of threads (&lt;= 1024=32*32) to &lt;= 32 intermediate sums. Then the \"warp leader\" will put that into shared memory. Then there is another round of reduction with warpSum  where the intermediate sums are reduced.<br>\nThe alternative pattern is a reduction where you reduce down to &lt;=32 first an then have a single round of reduction. (Which is done e.g. for the WeightNorm code.)<br>\nFrom what I've learned from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7799218\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mcarilli\">@mcarilli</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3709243\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jjsjann123\">@jjsjann123</a>, I think that it is a pattern that may offer some performance advantage, but not that much. In fact, my first stab at having a better backward uses the single reduction pattern.</p>", "body_text": "This is unchanged logic from THCUNN, it took me a very close look to understand, too.\nIt's a two step warp sum to reduce from \"one result per thread\" to \"<=32 results\" to 1 result.\nWhat the code does is that it has a first warpSum that reduces from whatever number of threads (<= 1024=32*32) to <= 32 intermediate sums. Then the \"warp leader\" will put that into shared memory. Then there is another round of reduction with warpSum  where the intermediate sums are reduced.\nThe alternative pattern is a reduction where you reduce down to <=32 first an then have a single round of reduction. (Which is done e.g. for the WeightNorm code.)\nFrom what I've learned from @mcarilli and @jjsjann123, I think that it is a pattern that may offer some performance advantage, but not that much. In fact, my first stab at having a better backward uses the single reduction pattern.", "in_reply_to_id": 225012253}