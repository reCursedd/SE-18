{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225012253", "pull_request_review_id": 164518277, "id": 225012253, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNTAxMjI1Mw==", "diff_hunk": "@@ -0,0 +1,444 @@\n+#include <THC/THCDeviceUtils.cuh>\n+#include <THC/THCGeneral.h>\n+#include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int WARP_SIZE = 64;\n+#else\n+constexpr int WARP_SIZE = 32;\n+#endif\n+\n+// The maximum number of threads in a block\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int MAX_BLOCK_SIZE = 256;\n+#else\n+constexpr int MAX_BLOCK_SIZE = 512;\n+#endif\n+\n+// Number of threads in a block given an input size up to MAX_BLOCK_SIZE\n+static int getNumThreads(int nElem) {\n+#if defined(__HIP_PLATFORM_HCC__)\n+  int threadSizes[5] = { 16, 32, 64, 128, MAX_BLOCK_SIZE };\n+#else\n+  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };\n+#endif\n+  for (int i = 0; i != 5; ++i) {\n+    if (nElem <= threadSizes[i]) {\n+      return threadSizes[i];\n+    }\n+  }\n+  return MAX_BLOCK_SIZE;\n+}\n+\n+// Returns the index of the most significant 1 bit in `val`.\n+__device__ __forceinline__ int getMSB(int val) {\n+  return 31 - __clz(val);\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct Float2 {\n+  accscalar_t v1, v2;\n+  __device__ Float2() {}\n+  __device__ Float2(scalar_t v1, scalar_t v2) : v1(static_cast<accscalar_t>(v1)), v2(static_cast<accscalar_t>(v2)) {}\n+  __device__ Float2(int v) : v1(static_cast<accscalar_t>(v)), v2(static_cast<accscalar_t>(v)) {}\n+  __device__ Float2& operator+=(const Float2& a) {\n+    v1 += a.v1;\n+    v2 += a.v2;\n+    return *this;\n+  }\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct SumOp {\n+  __device__ SumOp(const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& t) : tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    return static_cast<accscalar_t>(tensor[batch][plane][n]);\n+  }\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& tensor;\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct VarOp {\n+  __device__ VarOp(accscalar_t m, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& t) : mean(m), tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    accscalar_t val = tensor[batch][plane][n];\n+    return (val - mean) * (val - mean);\n+  }\n+  const accscalar_t mean;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& tensor;\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct GradOp {\n+  __device__ GradOp(accscalar_t m, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& i, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& g)\n+    : mean(m), input(i), grad_output(g) {}\n+  __device__ __forceinline__ Float2<scalar_t, accscalar_t> operator()(int batch, int plane, int n) {\n+    accscalar_t g = grad_output[batch][plane][n];\n+    accscalar_t c = static_cast<accscalar_t>(input[batch][plane][n]) - mean;\n+    return Float2<scalar_t, accscalar_t>(g, g * c);\n+  }\n+  const accscalar_t mean;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& input;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& grad_output;\n+};\n+\n+// Sum across all threads within a warp\n+template <typename T>\n+static __device__ __forceinline__ T warpSum(T val) {\n+#if __CUDA_ARCH__ >= 300\n+  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n+    val += WARP_SHFL_XOR(val, 1 << i, WARP_SIZE);\n+  }\n+#else\n+  __shared__ T values[MAX_BLOCK_SIZE];\n+  values[threadIdx.x] = val;\n+  __threadfence_block();\n+  const int base = (threadIdx.x / WARP_SIZE) * WARP_SIZE;\n+  for (int i = 1; i < WARP_SIZE; i++) {\n+    val += values[base + ((i + threadIdx.x) % WARP_SIZE)];\n+  }\n+#endif\n+  return val;\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+static __device__ __forceinline__ Float2<scalar_t, accscalar_t> warpSum(Float2<scalar_t, accscalar_t> value) {\n+  value.v1 = warpSum(value.v1);\n+  value.v2 = warpSum(value.v2);\n+  return value;\n+}\n+\n+// Sum across (batch, x/y/z) applying Op() pointwise\n+template<typename scalar_t, typename Op, typename PTA>\n+__device__ scalar_t reduce(Op op, PTA tensor, int plane) {\n+  scalar_t sum = static_cast<scalar_t>(0);\n+  for (int batch = 0; batch < tensor.size(0); ++batch) {\n+    for (int x = threadIdx.x; x < tensor.size(2); x += blockDim.x) {\n+      sum += op(batch, plane, x);\n+    }\n+  }\n+\n+  // sum over NumThreads within a warp\n+  sum = warpSum(sum);", "path": "aten/src/ATen/native/cuda/Normalization.cu", "position": 151, "original_position": 129, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "5f3896fd66cfbde095029109f8065f26e87bf107", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "I'm a little bit lost here. I thought `warpSum` already sums all threads in a warp. Why do we need reduce again below?", "created_at": "2018-10-14T22:42:39Z", "updated_at": "2018-11-23T15:52:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r225012253", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225012253"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r225012253"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>I'm a little bit lost here. I thought <code>warpSum</code> already sums all threads in a warp. Why do we need reduce again below?</p>", "body_text": "I'm a little bit lost here. I thought warpSum already sums all threads in a warp. Why do we need reduce again below?"}