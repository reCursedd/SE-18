{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/431815475", "html_url": "https://github.com/pytorch/pytorch/pull/12368#issuecomment-431815475", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12368", "id": 431815475, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTgxNTQ3NQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-22T12:06:31Z", "updated_at": "2018-10-22T12:06:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I still don't know what makes the native batch norm administratively slower than THCUNNs. (There might be int vs. int64 for indexing, inlining of TensorAccessor things? There isn't really anything algorithmic in the evaluation forward...) :(<br>\nBut the optimizations of the block/grid layout and Welford seem to compensate that, even if it would be preferable to have the benefits of both.</p>\n<p>So I've heard that realistic channel feature combinations might be (64, 12544), (64, 3136), (256, 3136), (128, 3136), (128, 784), (512, 784),  (256, 784), (256, 196), (1024, 196), (512, 196), (512, 49), (2048, 49) and put that through my benchmark with several batch sizes.</p>\n<p>From the things that take &gt;1msec, these are the worst:</p>\n<table>\n<thead>\n<tr>\n<th>bs</th>\n<th>channels</th>\n<th>features</th>\n<th>train</th>\n<th>cudnn</th>\n<th>native</th>\n<th>slowness</th>\n<th>reference</th>\n<th>slowness</th>\n<th>evaluation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>128</td>\n<td>2048</td>\n<td>49</td>\n<td>e-fb</td>\n<td>1,4454</td>\n<td>1,746</td>\n<td>1,21</td>\n<td>1,5894</td>\n<td>1,1</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>1024</td>\n<td>512</td>\n<td>49</td>\n<td>e-f</td>\n<td>0,8066</td>\n<td>1,0233</td>\n<td>1,27</td>\n<td>0,9302</td>\n<td>1,1</td>\n<td>bad</td>\n</tr>\n<tr>\n<td>64</td>\n<td>2048</td>\n<td>49</td>\n<td>t-fb</td>\n<td>0,5744</td>\n<td>1,1305</td>\n<td>1,97</td>\n<td>1,0425</td>\n<td>1,08</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>64</td>\n<td>1024</td>\n<td>196</td>\n<td>e-fb</td>\n<td>1,2717</td>\n<td>1,2731</td>\n<td>1</td>\n<td>1,1907</td>\n<td>1,07</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>32</td>\n<td>512</td>\n<td>784</td>\n<td>e-fb</td>\n<td>1,1432</td>\n<td>1,1382</td>\n<td>1</td>\n<td>1,0851</td>\n<td>1,05</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>64</td>\n<td>512</td>\n<td>784</td>\n<td>e-fb</td>\n<td>2,2656</td>\n<td>2,2567</td>\n<td>1</td>\n<td>2,1661</td>\n<td>1,04</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>128</td>\n<td>1024</td>\n<td>196</td>\n<td>e-fb</td>\n<td>2,5212</td>\n<td>2,5353</td>\n<td>1,01</td>\n<td>2,4556</td>\n<td>1,03</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>16</td>\n<td>256</td>\n<td>3136</td>\n<td>e-fb</td>\n<td>1,1148</td>\n<td>1,1143</td>\n<td>1</td>\n<td>1,0993</td>\n<td>1,01</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>32</td>\n<td>256</td>\n<td>3136</td>\n<td>e-fb</td>\n<td>2,2072</td>\n<td>2,2078</td>\n<td>1</td>\n<td>2,177</td>\n<td>1,01</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>64</td>\n<td>256</td>\n<td>3136</td>\n<td>e-fb</td>\n<td>4,4469</td>\n<td>4,4497</td>\n<td>1</td>\n<td>4,4212</td>\n<td>1,01</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>64</td>\n<td>256</td>\n<td>784</td>\n<td>e-fb</td>\n<td>1,174</td>\n<td>1,1753</td>\n<td>1</td>\n<td>1,1632</td>\n<td>1,01</td>\n<td>\u00a0</td>\n</tr>\n<tr>\n<td>128</td>\n<td>512</td>\n<td>196</td>\n<td>e-fb</td>\n<td>1,2994</td>\n<td>1,308</td>\n<td>1,01</td>\n<td>1,2941</td>\n<td>1,01</td>\n<td>\u00a0</td>\n</tr>\n</tbody>\n</table>\n<p>Relative performance of fast kernels will be worse, e.g. bs=8,c=2048,f=49 takes 0.24msec vs. 0.066msec before.<br>\nAnd these are the best:</p>\n<table>\n<thead>\n<tr>\n<th>bs</th>\n<th>channels</th>\n<th>features</th>\n<th>train</th>\n<th>cudnn</th>\n<th>native</th>\n<th>slowness</th>\n<th>reference</th>\n<th>slowness</th>\n<th>evaluation</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>64</td>\n<td>128</td>\n<td>3136</td>\n<td>t-f</td>\n<td>0,7592</td>\n<td>1,0865</td>\n<td>1,43</td>\n<td>1,5893</td>\n<td>0,68</td>\n<td>good</td>\n</tr>\n<tr>\n<td>256</td>\n<td>1024</td>\n<td>196</td>\n<td>t-f</td>\n<td>1,6057</td>\n<td>2,2576</td>\n<td>1,41</td>\n<td>3,3175</td>\n<td>0,68</td>\n<td>good</td>\n</tr>\n<tr>\n<td>256</td>\n<td>2048</td>\n<td>49</td>\n<td>t-f</td>\n<td>0,8236</td>\n<td>1,587</td>\n<td>1,93</td>\n<td>2,369</td>\n<td>0,67</td>\n<td>good</td>\n</tr>\n<tr>\n<td>256</td>\n<td>512</td>\n<td>196</td>\n<td>t-f</td>\n<td>0,8657</td>\n<td>1,1503</td>\n<td>1,33</td>\n<td>1,7539</td>\n<td>0,66</td>\n<td>good</td>\n</tr>\n<tr>\n<td>128</td>\n<td>128</td>\n<td>3136</td>\n<td>t-f</td>\n<td>1,5487</td>\n<td>2,1475</td>\n<td>1,39</td>\n<td>3,2846</td>\n<td>0,65</td>\n<td>good</td>\n</tr>\n<tr>\n<td>256</td>\n<td>256</td>\n<td>784</td>\n<td>t-f</td>\n<td>1,5348</td>\n<td>2,1127</td>\n<td>1,38</td>\n<td>3,2883</td>\n<td>0,64</td>\n<td>good</td>\n</tr>\n<tr>\n<td>256</td>\n<td>128</td>\n<td>784</td>\n<td>t-f</td>\n<td>0,8075</td>\n<td>1,169</td>\n<td>1,45</td>\n<td>1,8861</td>\n<td>0,62</td>\n<td>good</td>\n</tr>\n<tr>\n<td>1024</td>\n<td>512</td>\n<td>49</td>\n<td>t-f</td>\n<td>1,3702</td>\n<td>1,4977</td>\n<td>1,09</td>\n<td>2,46</td>\n<td>0,61</td>\n<td>good</td>\n</tr>\n<tr>\n<td>1024</td>\n<td>256</td>\n<td>196</td>\n<td>e-f</td>\n<td>1,2353</td>\n<td>1,2558</td>\n<td>1,02</td>\n<td>2,4357</td>\n<td>0,52</td>\n<td>good</td>\n</tr>\n<tr>\n<td>1024</td>\n<td>256</td>\n<td>196</td>\n<td>t-f</td>\n<td>1,6817</td>\n<td>2,3312</td>\n<td>1,39</td>\n<td>4,7348</td>\n<td>0,49</td>\n<td>good</td>\n</tr>\n</tbody>\n</table>\n<p>My other benchmark shows a smiliar thing.</p>", "body_text": "I still don't know what makes the native batch norm administratively slower than THCUNNs. (There might be int vs. int64 for indexing, inlining of TensorAccessor things? There isn't really anything algorithmic in the evaluation forward...) :(\nBut the optimizations of the block/grid layout and Welford seem to compensate that, even if it would be preferable to have the benefits of both.\nSo I've heard that realistic channel feature combinations might be (64, 12544), (64, 3136), (256, 3136), (128, 3136), (128, 784), (512, 784),  (256, 784), (256, 196), (1024, 196), (512, 196), (512, 49), (2048, 49) and put that through my benchmark with several batch sizes.\nFrom the things that take >1msec, these are the worst:\n\n\n\nbs\nchannels\nfeatures\ntrain\ncudnn\nnative\nslowness\nreference\nslowness\nevaluation\n\n\n\n\n128\n2048\n49\ne-fb\n1,4454\n1,746\n1,21\n1,5894\n1,1\n\u00a0\n\n\n1024\n512\n49\ne-f\n0,8066\n1,0233\n1,27\n0,9302\n1,1\nbad\n\n\n64\n2048\n49\nt-fb\n0,5744\n1,1305\n1,97\n1,0425\n1,08\n\u00a0\n\n\n64\n1024\n196\ne-fb\n1,2717\n1,2731\n1\n1,1907\n1,07\n\u00a0\n\n\n32\n512\n784\ne-fb\n1,1432\n1,1382\n1\n1,0851\n1,05\n\u00a0\n\n\n64\n512\n784\ne-fb\n2,2656\n2,2567\n1\n2,1661\n1,04\n\u00a0\n\n\n128\n1024\n196\ne-fb\n2,5212\n2,5353\n1,01\n2,4556\n1,03\n\u00a0\n\n\n16\n256\n3136\ne-fb\n1,1148\n1,1143\n1\n1,0993\n1,01\n\u00a0\n\n\n32\n256\n3136\ne-fb\n2,2072\n2,2078\n1\n2,177\n1,01\n\u00a0\n\n\n64\n256\n3136\ne-fb\n4,4469\n4,4497\n1\n4,4212\n1,01\n\u00a0\n\n\n64\n256\n784\ne-fb\n1,174\n1,1753\n1\n1,1632\n1,01\n\u00a0\n\n\n128\n512\n196\ne-fb\n1,2994\n1,308\n1,01\n1,2941\n1,01\n\u00a0\n\n\n\nRelative performance of fast kernels will be worse, e.g. bs=8,c=2048,f=49 takes 0.24msec vs. 0.066msec before.\nAnd these are the best:\n\n\n\nbs\nchannels\nfeatures\ntrain\ncudnn\nnative\nslowness\nreference\nslowness\nevaluation\n\n\n\n\n64\n128\n3136\nt-f\n0,7592\n1,0865\n1,43\n1,5893\n0,68\ngood\n\n\n256\n1024\n196\nt-f\n1,6057\n2,2576\n1,41\n3,3175\n0,68\ngood\n\n\n256\n2048\n49\nt-f\n0,8236\n1,587\n1,93\n2,369\n0,67\ngood\n\n\n256\n512\n196\nt-f\n0,8657\n1,1503\n1,33\n1,7539\n0,66\ngood\n\n\n128\n128\n3136\nt-f\n1,5487\n2,1475\n1,39\n3,2846\n0,65\ngood\n\n\n256\n256\n784\nt-f\n1,5348\n2,1127\n1,38\n3,2883\n0,64\ngood\n\n\n256\n128\n784\nt-f\n0,8075\n1,169\n1,45\n1,8861\n0,62\ngood\n\n\n1024\n512\n49\nt-f\n1,3702\n1,4977\n1,09\n2,46\n0,61\ngood\n\n\n1024\n256\n196\ne-f\n1,2353\n1,2558\n1,02\n2,4357\n0,52\ngood\n\n\n1024\n256\n196\nt-f\n1,6817\n2,3312\n1,39\n4,7348\n0,49\ngood\n\n\n\nMy other benchmark shows a smiliar thing.", "body": "I still don't know what makes the native batch norm administratively slower than THCUNNs. (There might be int vs. int64 for indexing, inlining of TensorAccessor things? There isn't really anything algorithmic in the evaluation forward...) :(\r\nBut the optimizations of the block/grid layout and Welford seem to compensate that, even if it would be preferable to have the benefits of both.\r\n\r\nSo I've heard that realistic channel feature combinations might be (64, 12544), (64, 3136), (256, 3136), (128, 3136), (128, 784), (512, 784),  (256, 784), (256, 196), (1024, 196), (512, 196), (512, 49), (2048, 49) and put that through my benchmark with several batch sizes.\r\n\r\nFrom the things that take >1msec, these are the worst:\r\n\r\nbs | channels | features | train | cudnn | native | slowness | reference | slowness | evaluation\r\n-- | -- | -- | -- | -- | -- | -- | -- | -- | --\r\n128 | 2048 | 49 | e-fb | 1,4454 | 1,746 | 1,21 | 1,5894 | 1,1 | \u00a0\r\n1024 | 512 | 49 | e-f | 0,8066 | 1,0233 | 1,27 | 0,9302 | 1,1 | bad\r\n64 | 2048 | 49 | t-fb | 0,5744 | 1,1305 | 1,97 | 1,0425 | 1,08 | \u00a0\r\n64 | 1024 | 196 | e-fb | 1,2717 | 1,2731 | 1 | 1,1907 | 1,07 | \u00a0\r\n32 | 512 | 784 | e-fb | 1,1432 | 1,1382 | 1 | 1,0851 | 1,05 | \u00a0\r\n64 | 512 | 784 | e-fb | 2,2656 | 2,2567 | 1 | 2,1661 | 1,04 | \u00a0\r\n128 | 1024 | 196 | e-fb | 2,5212 | 2,5353 | 1,01 | 2,4556 | 1,03 | \u00a0\r\n16 | 256 | 3136 | e-fb | 1,1148 | 1,1143 | 1 | 1,0993 | 1,01 | \u00a0\r\n32 | 256 | 3136 | e-fb | 2,2072 | 2,2078 | 1 | 2,177 | 1,01 | \u00a0\r\n64 | 256 | 3136 | e-fb | 4,4469 | 4,4497 | 1 | 4,4212 | 1,01 | \u00a0\r\n64 | 256 | 784 | e-fb | 1,174 | 1,1753 | 1 | 1,1632 | 1,01 | \u00a0\r\n128 | 512 | 196 | e-fb | 1,2994 | 1,308 | 1,01 | 1,2941 | 1,01 | \u00a0\r\n\r\nRelative performance of fast kernels will be worse, e.g. bs=8,c=2048,f=49 takes 0.24msec vs. 0.066msec before.\r\nAnd these are the best:\r\n\r\nbs | channels | features | train | cudnn | native | slowness | reference | slowness | evaluation\r\n-- | -- | -- | -- | -- | -- | -- | -- | -- | --\r\n64 | 128 | 3136 | t-f | 0,7592 | 1,0865 | 1,43 | 1,5893 | 0,68 | good\r\n256 | 1024 | 196 | t-f | 1,6057 | 2,2576 | 1,41 | 3,3175 | 0,68 | good\r\n256 | 2048 | 49 | t-f | 0,8236 | 1,587 | 1,93 | 2,369 | 0,67 | good\r\n256 | 512 | 196 | t-f | 0,8657 | 1,1503 | 1,33 | 1,7539 | 0,66 | good\r\n128 | 128 | 3136 | t-f | 1,5487 | 2,1475 | 1,39 | 3,2846 | 0,65 | good\r\n256 | 256 | 784 | t-f | 1,5348 | 2,1127 | 1,38 | 3,2883 | 0,64 | good\r\n256 | 128 | 784 | t-f | 0,8075 | 1,169 | 1,45 | 1,8861 | 0,62 | good\r\n1024 | 512 | 49 | t-f | 1,3702 | 1,4977 | 1,09 | 2,46 | 0,61 | good\r\n1024 | 256 | 196 | e-f | 1,2353 | 1,2558 | 1,02 | 2,4357 | 0,52 | good\r\n1024 | 256 | 196 | t-f | 1,6817 | 2,3312 | 1,39 | 4,7348 | 0,49 | good\r\n\r\nMy other benchmark shows a smiliar thing.\r\n"}