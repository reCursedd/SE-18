{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227139479", "pull_request_review_id": 167111026, "id": 227139479, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzEzOTQ3OQ==", "diff_hunk": "@@ -432,10 +466,15 @@ std::tuple<Tensor, Tensor, Tensor> batch_norm_backward_cuda_template(const Tenso\n \n   using accscalar_t = at::acc_type<scalar_t, true>;\n   Tensor grad_input_;\n+  Tensor grad_input_cont;\n   Tensor grad_weight_;\n   Tensor grad_bias_;\n+  auto input_cont = input_.reshape({input_.size(0), input_.size(1), -1});", "path": "aten/src/ATen/native/cuda/Normalization.cu", "position": null, "original_position": 360, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "89e7cdb9019632cd5e109b8cb1eac5a475a8290d", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "I deliberately left it out, because I figured that I'd avoid the copy and the kernels don't need contiguity (except in the dimensions that get merged).\r\nRegarding efficiency, I think it depends on whether it's possible to have a more efficient access pattern when making things contiguous than on non-contiguous data. In the end I figured that I might as well be going with non-contiguous data.\r\nBut I should have renamed the Tensors and will do now.", "created_at": "2018-10-22T21:09:38Z", "updated_at": "2018-11-23T15:53:23Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r227139479", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227139479"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r227139479"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>I deliberately left it out, because I figured that I'd avoid the copy and the kernels don't need contiguity (except in the dimensions that get merged).<br>\nRegarding efficiency, I think it depends on whether it's possible to have a more efficient access pattern when making things contiguous than on non-contiguous data. In the end I figured that I might as well be going with non-contiguous data.<br>\nBut I should have renamed the Tensors and will do now.</p>", "body_text": "I deliberately left it out, because I figured that I'd avoid the copy and the kernels don't need contiguity (except in the dimensions that get merged).\nRegarding efficiency, I think it depends on whether it's possible to have a more efficient access pattern when making things contiguous than on non-contiguous data. In the end I figured that I might as well be going with non-contiguous data.\nBut I should have renamed the Tensors and will do now.", "in_reply_to_id": 227110117}