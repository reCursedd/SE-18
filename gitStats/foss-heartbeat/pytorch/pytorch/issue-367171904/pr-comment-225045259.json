{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225045259", "pull_request_review_id": 164558075, "id": 225045259, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNTA0NTI1OQ==", "diff_hunk": "@@ -0,0 +1,444 @@\n+#include <THC/THCDeviceUtils.cuh>\n+#include <THC/THCGeneral.h>\n+#include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int WARP_SIZE = 64;\n+#else\n+constexpr int WARP_SIZE = 32;\n+#endif\n+\n+// The maximum number of threads in a block\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int MAX_BLOCK_SIZE = 256;\n+#else\n+constexpr int MAX_BLOCK_SIZE = 512;\n+#endif\n+\n+// Number of threads in a block given an input size up to MAX_BLOCK_SIZE\n+static int getNumThreads(int nElem) {\n+#if defined(__HIP_PLATFORM_HCC__)\n+  int threadSizes[5] = { 16, 32, 64, 128, MAX_BLOCK_SIZE };\n+#else\n+  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };\n+#endif\n+  for (int i = 0; i != 5; ++i) {\n+    if (nElem <= threadSizes[i]) {\n+      return threadSizes[i];\n+    }\n+  }\n+  return MAX_BLOCK_SIZE;\n+}\n+\n+// Returns the index of the most significant 1 bit in `val`.\n+__device__ __forceinline__ int getMSB(int val) {\n+  return 31 - __clz(val);\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct Float2 {\n+  accscalar_t v1, v2;\n+  __device__ Float2() {}\n+  __device__ Float2(scalar_t v1, scalar_t v2) : v1(static_cast<accscalar_t>(v1)), v2(static_cast<accscalar_t>(v2)) {}\n+  __device__ Float2(int v) : v1(static_cast<accscalar_t>(v)), v2(static_cast<accscalar_t>(v)) {}\n+  __device__ Float2& operator+=(const Float2& a) {\n+    v1 += a.v1;\n+    v2 += a.v2;\n+    return *this;\n+  }\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct SumOp {\n+  __device__ SumOp(const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& t) : tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    return static_cast<accscalar_t>(tensor[batch][plane][n]);\n+  }\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& tensor;\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct VarOp {\n+  __device__ VarOp(accscalar_t m, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& t) : mean(m), tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    accscalar_t val = tensor[batch][plane][n];\n+    return (val - mean) * (val - mean);\n+  }\n+  const accscalar_t mean;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& tensor;\n+};\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct GradOp {\n+  __device__ GradOp(accscalar_t m, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& i, const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& g)\n+    : mean(m), input(i), grad_output(g) {}\n+  __device__ __forceinline__ Float2<scalar_t, accscalar_t> operator()(int batch, int plane, int n) {\n+    accscalar_t g = grad_output[batch][plane][n];\n+    accscalar_t c = static_cast<accscalar_t>(input[batch][plane][n]) - mean;\n+    return Float2<scalar_t, accscalar_t>(g, g * c);\n+  }\n+  const accscalar_t mean;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& input;\n+  const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>& grad_output;\n+};\n+\n+// Sum across all threads within a warp\n+template <typename T>\n+static __device__ __forceinline__ T warpSum(T val) {\n+#if __CUDA_ARCH__ >= 300\n+  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n+    val += WARP_SHFL_XOR(val, 1 << i, WARP_SIZE);\n+  }\n+#else\n+  __shared__ T values[MAX_BLOCK_SIZE];\n+  values[threadIdx.x] = val;\n+  __threadfence_block();\n+  const int base = (threadIdx.x / WARP_SIZE) * WARP_SIZE;\n+  for (int i = 1; i < WARP_SIZE; i++) {\n+    val += values[base + ((i + threadIdx.x) % WARP_SIZE)];\n+  }\n+#endif\n+  return val;\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+static __device__ __forceinline__ Float2<scalar_t, accscalar_t> warpSum(Float2<scalar_t, accscalar_t> value) {\n+  value.v1 = warpSum(value.v1);\n+  value.v2 = warpSum(value.v2);\n+  return value;\n+}\n+\n+// Sum across (batch, x/y/z) applying Op() pointwise\n+template<typename scalar_t, typename Op, typename PTA>\n+__device__ scalar_t reduce(Op op, PTA tensor, int plane) {\n+  scalar_t sum = static_cast<scalar_t>(0);\n+  for (int batch = 0; batch < tensor.size(0); ++batch) {\n+    for (int x = threadIdx.x; x < tensor.size(2); x += blockDim.x) {\n+      sum += op(batch, plane, x);\n+    }\n+  }\n+\n+  // sum over NumThreads within a warp\n+  sum = warpSum(sum);\n+\n+  // 'transpose', and reduce within warp again\n+  __shared__ scalar_t shared[WARP_SIZE];\n+  __syncthreads();\n+  if (threadIdx.x % WARP_SIZE == 0) {\n+    shared[threadIdx.x / WARP_SIZE] = sum;\n+  }\n+  if (threadIdx.x >= blockDim.x / WARP_SIZE && threadIdx.x < WARP_SIZE) {\n+    // zero out the other entries in shared\n+    shared[threadIdx.x] = (scalar_t)0;\n+  }\n+  __syncthreads();\n+  if (threadIdx.x / WARP_SIZE == 0) {\n+    sum = warpSum(shared[threadIdx.x]);\n+    if (threadIdx.x == 0) {\n+      shared[0] = sum;\n+    }\n+  }\n+  __syncthreads();\n+\n+  // Everyone picks it up, should be broadcast into the whole grad_input\n+  return shared[0];\n+}\n+\n+template <typename scalar_t, typename accscalar_t, bool train>\n+__global__ void batch_norm_transform_input_kernel(\n+    const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> input,\n+    PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> output,\n+    const PackedTensorAccessor<typename std::conditional<train, accscalar_t, scalar_t>::type, 1, at::RestrictPtrTraits> mean_,\n+    const PackedTensorAccessor<typename std::conditional<train, accscalar_t, scalar_t>::type, 1, at::RestrictPtrTraits> var_or_invstd,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> weight,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> bias,\n+    accscalar_t epsilon) {\n+\n+  int plane = blockIdx.y * blockDim.y + threadIdx.y;\n+\n+  if (plane >= input.size(1)) {\n+    return;\n+  }\n+\n+  accscalar_t gamma = weight.size(0) > 0 ? static_cast<accscalar_t>(weight[plane]) : static_cast<accscalar_t>(1);\n+  accscalar_t beta = bias.size(0) > 0 ? static_cast<accscalar_t>(bias[plane]) : static_cast<accscalar_t>(0);\n+  accscalar_t mean = static_cast<accscalar_t>(mean_[plane]);\n+  accscalar_t invstd;\n+  if (train) {\n+    invstd = var_or_invstd[plane];\n+  } else {\n+    invstd = static_cast<accscalar_t>(1) / std::sqrt(static_cast<accscalar_t>(var_or_invstd[plane]) + epsilon);\n+  }\n+  for (int64_t batch = blockIdx.x; batch < input.size(0); batch += gridDim.x) {\n+    for (int64_t feature = blockIdx.z; feature < input.size(2); feature += gridDim.z) {\n+      output[batch][plane][feature] = static_cast<scalar_t>(gamma * (input[batch][plane][feature] - mean) * invstd + beta);\n+    }\n+  }\n+}\n+\n+\n+template <typename scalar_t, typename accscalar_t>\n+__global__ void batch_norm_collect_statistics_kernel(\n+    const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> input,\n+    PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> output,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> weight,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> bias,\n+    const accscalar_t epsilon,\n+    const accscalar_t momentum,\n+    PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> running_mean,\n+    PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> running_var,\n+    PackedTensorAccessor<accscalar_t, 1, at::RestrictPtrTraits> save_mean,\n+    PackedTensorAccessor<accscalar_t, 1, at::RestrictPtrTraits> save_invstd) {\n+\n+  int plane = blockIdx.x;\n+  int N = input.size(0) * input.size(2);\n+\n+  accscalar_t norm = accscalar_t(1) / N;\n+\n+  // Compute the mean and variance across (batch, x/y/z)\n+  accscalar_t mean = reduce<accscalar_t>(SumOp<scalar_t, accscalar_t>(input), input, plane) * norm;\n+  __syncthreads();\n+  accscalar_t varN = reduce<accscalar_t>(VarOp<scalar_t, accscalar_t>(mean, input), input, plane);\n+\n+  // Save the mean, variance, and moving averages\n+  if (threadIdx.x == 0) {\n+    accscalar_t invstd = 0;\n+    if (varN != static_cast<accscalar_t>(0) || epsilon != static_cast<accscalar_t>(0)) {\n+      invstd = static_cast<accscalar_t>(1) / std::sqrt(varN * norm + epsilon);\n+    }\n+    save_mean[plane] = mean;\n+    save_invstd[plane] = invstd;\n+    if (running_mean.data() != NULL) {\n+      running_mean[plane] = static_cast<scalar_t>((1 - momentum) * running_mean[plane] + momentum * mean);\n+    }\n+    if (running_var.data() != NULL) {\n+      accscalar_t unbiasedVar = varN / (N - 1);\n+      running_var[plane] = static_cast<scalar_t>((1 - momentum) * running_var[plane] + momentum * unbiasedVar);\n+    }\n+  }\n+\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+__global__ void batch_norm_backward_kernel(\n+    const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> input,\n+    const PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> grad_output,\n+    PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits> grad_input,\n+    PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> grad_weight,\n+    PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> grad_bias,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> weight,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> running_mean,\n+    const PackedTensorAccessor<scalar_t, 1, at::RestrictPtrTraits> running_var,\n+    const PackedTensorAccessor<accscalar_t, 1, at::RestrictPtrTraits> save_mean,\n+    const PackedTensorAccessor<accscalar_t, 1, at::RestrictPtrTraits> save_invstd,\n+    bool train,\n+    accscalar_t epsilon) {\n+\n+  int plane = blockIdx.x;\n+  int N = grad_output.size(0) * grad_output.size(2);\n+\n+  accscalar_t mean, invstd;\n+  if (train) {\n+    mean = save_mean[plane];\n+    invstd = save_invstd[plane];\n+  } else {\n+    mean = static_cast<accscalar_t>(running_mean[plane]);\n+    invstd = static_cast<accscalar_t>(1) / std::sqrt(static_cast<accscalar_t>(running_var[plane]) + epsilon);\n+  }\n+\n+  accscalar_t weight_val = weight.size(0) > 0 ? static_cast<accscalar_t>(weight[plane]) : accscalar_t(1);\n+  accscalar_t norm = accscalar_t(1) / N;\n+\n+  // Compute two values across (batch, x/y/z) in one pass:\n+  // 1. Sum(grad_output)\n+  // 2. DotProduct(input - mean, grad_output)\n+  GradOp<scalar_t, accscalar_t> g(mean, input, grad_output);\n+  Float2<scalar_t, accscalar_t> res = reduce<Float2<scalar_t, accscalar_t>, GradOp<scalar_t, accscalar_t>, PackedTensorAccessor<scalar_t, 3, at::RestrictPtrTraits>>(g, grad_output, plane);\n+  accscalar_t grad_output_sum = res.v1;\n+  accscalar_t dot_p = res.v2;\n+\n+  accscalar_t grad_mean = grad_output_sum * norm;\n+  accscalar_t proj_scale = dot_p * norm * invstd * invstd;\n+  accscalar_t grad_scale = invstd * weight_val;\n+\n+  if (grad_input.data() != NULL) {\n+    for (int batch = 0; batch < grad_output.size(0); ++batch) {\n+      for (int x = threadIdx.x; x < grad_output.size(2); x += blockDim.x) {\n+        scalar_t go = grad_output[batch][plane][x];\n+        if (train) {\n+          scalar_t inp = input[batch][plane][x];\n+          accscalar_t proj = (inp - mean) * proj_scale;\n+          grad_input[batch][plane][x] = static_cast<scalar_t>((go - proj - grad_mean) * grad_scale);\n+        } else {\n+          grad_input[batch][plane][x] = static_cast<scalar_t>(go * grad_scale);\n+        }\n+      }\n+    }\n+  }\n+\n+  if (grad_weight.size(0) > 0) {\n+    if (threadIdx.x == 0) {\n+      grad_weight[plane] = static_cast<scalar_t>(dot_p * invstd);\n+    }\n+  }\n+\n+  if (grad_bias.size(0) > 0) {\n+    if (threadIdx.x == 0) {\n+      grad_bias[plane] = static_cast<scalar_t>(grad_output_sum);\n+    }\n+  }\n+}\n+\n+// TensorAccessor in which the last dimensions are collapsed or expanded as needed\n+template <typename scalar_t, int64_t dim>\n+static PackedTensorAccessor<scalar_t, dim, at::RestrictPtrTraits> reshaped_packed_accessor(const Tensor& t) {\n+  // undefined...\n+  if (! t.defined()) {\n+    const std::vector<int64_t> zeros(dim);\n+    return PackedTensorAccessor<scalar_t, dim, at::RestrictPtrTraits>(nullptr, zeros.data(), zeros.data());\n+  }\n+  int64_t in_dim = t.dim();\n+  if (in_dim == dim) {\n+    return t.packed_accessor<scalar_t, dim, at::RestrictPtrTraits>();\n+  }\n+\n+  AT_CHECK(in_dim < dim || t.is_contiguous(), \"need contiguous or <= 3d tensor\");\n+  std::vector<int64_t> sizes(dim);\n+  std::vector<int64_t> strides(dim);\n+  for (int i = 0; i < in_dim || i < dim; ++i) {\n+    if (i < dim && i < in_dim) {\n+      sizes[i] = t.size(i);\n+      strides[i] = t.stride(i);\n+    } else if (i < dim) {\n+      sizes[i] = 1;\n+      strides[i] = 0;\n+    } else {\n+      sizes[dim - 1] *= t.size(i);\n+      strides[dim -1] = 1;\n+    }\n+  }\n+  // evil trick to get adjusted 2d tensors to have large dimension last\n+  if (dim == 3 && sizes[0] > sizes[2]) {\n+    std::swap(sizes[0], sizes[2]);\n+    std::swap(strides[0], strides[2]);\n+  }\n+  return PackedTensorAccessor<scalar_t, dim, at::RestrictPtrTraits>(t.data<scalar_t>(), sizes.data(), strides.data());\n+}\n+\n+template<typename scalar_t>\n+std::tuple<Tensor, Tensor, Tensor> batch_norm_cuda_template(const Tensor& input_, const Tensor& weight_, const Tensor& bias_,\n+\t\t\t\t\t\t\t    const Tensor& running_mean_, const Tensor& running_var_,\n+\t\t\t\t\t\t\t    bool train, double momentum, double epsilon) {\n+\n+  using accscalar_t = at::acc_type<scalar_t, true>;\n+  Tensor output_= at::empty_like(input_);\n+  int64_t n_input = input_.size(1);\n+  Tensor save_mean_;\n+  Tensor save_invstd_;\n+  auto input_options = input_.options();\n+  if (input_options.dtype() == ScalarType::Half) {\n+    input_options.dtype(ScalarType::Float);\n+  }\n+  if (train) {\n+    save_mean_ = at::empty({n_input}, input_options);\n+    save_invstd_ = at::empty({n_input}, input_options);\n+  } else {\n+    save_mean_ = at::empty({0}, input_options);\n+    save_invstd_ = at::empty({0}, input_options);\n+  }\n+  auto input = reshaped_packed_accessor<scalar_t, 3>(input_);\n+  auto output = reshaped_packed_accessor<scalar_t, 3>(output_);\n+  auto weight = reshaped_packed_accessor<scalar_t, 1>(weight_);\n+  auto bias = reshaped_packed_accessor<scalar_t, 1>(bias_);\n+  auto running_mean = reshaped_packed_accessor<scalar_t, 1>(running_mean_);\n+  auto running_var = reshaped_packed_accessor<scalar_t, 1>(running_var_);\n+  auto save_mean = reshaped_packed_accessor<accscalar_t, 1>(save_mean_);\n+  auto save_invstd = reshaped_packed_accessor<accscalar_t, 1>(save_invstd_);\n+  auto stream = at::cuda::getCurrentCUDAStream();\n+\n+  constexpr int max_blocks_per_input = 60000;\n+  int feature_blocks = std::min<int>(input.size(2), max_blocks_per_input);\n+  int batch_blocks   = std::min<int>(input.size(0), max_blocks_per_input / feature_blocks);\n+  dim3 blocks(batch_blocks, (input.size(1)+127)/128, feature_blocks);\n+  dim3 threads(1, 128);\n+  if (!train) {\n+    batch_norm_transform_input_kernel<scalar_t, accscalar_t, false> <<<blocks, threads, 0, stream>>>\n+      (input, output, running_mean, running_var, weight, bias, epsilon);\n+  } else {\n+    dim3 blocks_red(input.size(1));\n+    dim3 threads_red(getNumThreads(input.size(2)));\n+    batch_norm_collect_statistics_kernel<scalar_t, accscalar_t> <<<blocks_red, threads_red, 0, stream>>>", "path": "aten/src/ATen/native/cuda/Normalization.cu", "position": null, "original_position": 377, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "5f3896fd66cfbde095029109f8065f26e87bf107", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "Right.", "created_at": "2018-10-15T06:00:49Z", "updated_at": "2018-11-23T15:53:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r225045259", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/225045259"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r225045259"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>Right.</p>", "body_text": "Right.", "in_reply_to_id": 225012596}