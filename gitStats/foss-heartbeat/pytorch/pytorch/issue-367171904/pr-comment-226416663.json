{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/226416663", "pull_request_review_id": 166232926, "id": 226416663, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNjQxNjY2Mw==", "diff_hunk": "@@ -387,21 +393,23 @@ std::tuple<Tensor, Tensor, Tensor> batch_norm_cuda_template(const Tensor& input_\n     save_mean_ = at::empty({0}, input_options);\n     save_invstd_ = at::empty({0}, input_options);\n   }\n-  auto input = reshaped_packed_accessor<scalar_t, 3>(input_);\n-  auto output = reshaped_packed_accessor<scalar_t, 3>(output_);\n-  auto weight = reshaped_packed_accessor<scalar_t, 1>(weight_);\n-  auto bias = reshaped_packed_accessor<scalar_t, 1>(bias_);\n-  auto running_mean = reshaped_packed_accessor<scalar_t, 1>(running_mean_);\n-  auto running_var = reshaped_packed_accessor<scalar_t, 1>(running_var_);\n-  auto save_mean = reshaped_packed_accessor<accscalar_t, 1>(save_mean_);\n-  auto save_invstd = reshaped_packed_accessor<accscalar_t, 1>(save_invstd_);\n+  auto input = reshaped_packed_accessor<scalar_t, 3>(input_, true);\n+  auto output = reshaped_packed_accessor<scalar_t, 3>(output_, true);\n+  auto weight = reshaped_packed_accessor<scalar_t, 1>(weight_, true);\n+  auto bias = reshaped_packed_accessor<scalar_t, 1>(bias_, true);\n+  auto running_mean = reshaped_packed_accessor<scalar_t, 1>(running_mean_, true);\n+  auto running_var = reshaped_packed_accessor<scalar_t, 1>(running_var_, true);\n+  auto save_mean = reshaped_packed_accessor<accscalar_t, 1>(save_mean_, true);\n+  auto save_invstd = reshaped_packed_accessor<accscalar_t, 1>(save_invstd_, true);\n   auto stream = at::cuda::getCurrentCUDAStream();\n \n-  constexpr int max_blocks_per_input = 60000;\n-  int feature_blocks = std::min<int>(input.size(2), max_blocks_per_input);\n-  int batch_blocks   = std::min<int>(input.size(0), max_blocks_per_input / feature_blocks);\n-  dim3 blocks(batch_blocks, (input.size(1)+127)/128, feature_blocks);\n-  dim3 threads(1, 128);\n+  constexpr int max_blocks = 60000;\n+  int feature_threads = std::min(getNumThreads(input.size(2)), 128);", "path": "aten/src/ATen/native/cuda/Normalization.cu", "position": null, "original_position": 80, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "7d368ec1f9900bc141553f9e389cd0dc92acf142", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "I guess to better phrase the question: why does setting them this way benefit perf?", "created_at": "2018-10-18T18:26:29Z", "updated_at": "2018-11-23T15:53:13Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r226416663", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/226416663"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r226416663"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>I guess to better phrase the question: why does setting them this way benefit perf?</p>", "body_text": "I guess to better phrase the question: why does setting them this way benefit perf?", "in_reply_to_id": 226416447}