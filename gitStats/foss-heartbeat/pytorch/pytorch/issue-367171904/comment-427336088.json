{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427336088", "html_url": "https://github.com/pytorch/pytorch/pull/12368#issuecomment-427336088", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12368", "id": 427336088, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzMzNjA4OA==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-05T11:37:24Z", "updated_at": "2018-10-05T11:37:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It's not the prettiest code, but I used this to arrive at the conclusion that for large numbers of features, it's still slow:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> timeit\n<span class=\"pl-k\">import</span> gc\n<span class=\"pl-k\">import</span> numpy\n\nbatch_sizes <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">16</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">128</span>]\nchannels    <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">256</span>,<span class=\"pl-c1\">1024</span>]\nfeatures    <span class=\"pl-k\">=</span> [(<span class=\"pl-c1\">16</span>,),\n               (<span class=\"pl-c1\">32</span>,),\n               (<span class=\"pl-c1\">64</span>,),\n               (<span class=\"pl-c1\">128</span>,),\n               (<span class=\"pl-c1\">256</span>,),\n               (<span class=\"pl-c1\">1024</span>,),\n               (<span class=\"pl-c1\">10240</span>,),\n               (<span class=\"pl-c1\">102400</span>,),\n               (<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>),\n               (<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">64</span>),\n               (<span class=\"pl-c1\">128</span>,<span class=\"pl-c1\">128</span>),\n               (<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>),\n               (<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">64</span>)]\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">run_bn</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">running_mean</span>, <span class=\"pl-smi\">running_var</span>, <span class=\"pl-smi\">weight</span>, <span class=\"pl-smi\">bias</span>, <span class=\"pl-smi\">training</span>, <span class=\"pl-smi\">backward</span>):\n    out <span class=\"pl-k\">=</span> torch.nn.functional.batch_norm(<span class=\"pl-c1\">input</span>, running_mean, running_var, weight, bias, training, <span class=\"pl-c1\">0.1</span>, <span class=\"pl-c1\">1e-5</span>)\n    <span class=\"pl-k\">if</span> backward:\n        grads <span class=\"pl-k\">=</span> torch.autograd.grad(out, [<span class=\"pl-c1\">input</span>, weight, bias], torch.ones_like(out))\n    torch.cuda.synchronize()\n\n<span class=\"pl-k\">for</span> bs <span class=\"pl-k\">in</span> batch_sizes:\n    <span class=\"pl-k\">for</span> c <span class=\"pl-k\">in</span> channels:\n        <span class=\"pl-k\">for</span> f <span class=\"pl-k\">in</span> features:\n            shape <span class=\"pl-k\">=</span> (bs, c)<span class=\"pl-k\">+</span>f\n            size <span class=\"pl-k\">=</span> numpy.prod(shape)\n            <span class=\"pl-k\">if</span> size <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">100_000_000</span>:\n                running_mean <span class=\"pl-k\">=</span> torch.randn(c, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>)\n                running_var <span class=\"pl-k\">=</span> torch.randn(c, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>).exp()\n                weight <span class=\"pl-k\">=</span> torch.randn(c, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                bias <span class=\"pl-k\">=</span> torch.randn(c, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.randn(shape, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n                gc.collect()\n                <span class=\"pl-k\">for</span> training <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">True</span>, <span class=\"pl-c1\">False</span>]:\n                    run_bn(<span class=\"pl-c1\">input</span>, running_mean, running_var, weight, bias, training, training)\n                    torch.cuda.synchronize()\n                    res1 <span class=\"pl-k\">=</span> timeit.timeit(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>run_bn(input, running_mean, running_var, weight, bias, training, training)<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">globals</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">globals</span>())\n                    <span class=\"pl-k\">with</span> torch.backends.cudnn.flags(<span class=\"pl-v\">enabled</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n                      res2 <span class=\"pl-k\">=</span> timeit.timeit(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>run_bn(input, running_mean, running_var, weight, bias, training, training)<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">globals</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">globals</span>())\n                    <span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">{}</span> | <span class=\"pl-c1\">{}</span> | <span class=\"pl-c1\">{}</span> | <span class=\"pl-c1\">{}</span> | <span class=\"pl-c1\">{<span class=\"pl-k\">:.2f</span>}</span> | <span class=\"pl-c1\">{<span class=\"pl-k\">:.2f</span>}</span> | <span class=\"pl-c1\">{<span class=\"pl-k\">:.1f</span>}</span><span class=\"pl-pds\">'</span></span>.format(bs, c, f, training, res1, res2, res1<span class=\"pl-k\">/</span>res2))</pre></div>", "body_text": "It's not the prettiest code, but I used this to arrive at the conclusion that for large numbers of features, it's still slow:\nimport torch\nimport timeit\nimport gc\nimport numpy\n\nbatch_sizes = [8,16,32,64,128]\nchannels    = [2,32,256,1024]\nfeatures    = [(16,),\n               (32,),\n               (64,),\n               (128,),\n               (256,),\n               (1024,),\n               (10240,),\n               (102400,),\n               (32,32),\n               (64,64),\n               (128,128),\n               (32,32,32),\n               (64,64,64)]\n\ndef run_bn(input, running_mean, running_var, weight, bias, training, backward):\n    out = torch.nn.functional.batch_norm(input, running_mean, running_var, weight, bias, training, 0.1, 1e-5)\n    if backward:\n        grads = torch.autograd.grad(out, [input, weight, bias], torch.ones_like(out))\n    torch.cuda.synchronize()\n\nfor bs in batch_sizes:\n    for c in channels:\n        for f in features:\n            shape = (bs, c)+f\n            size = numpy.prod(shape)\n            if size < 100_000_000:\n                running_mean = torch.randn(c, device='cuda')\n                running_var = torch.randn(c, device='cuda').exp()\n                weight = torch.randn(c, device='cuda', requires_grad=True)\n                bias = torch.randn(c, device='cuda', requires_grad=True)\n                input = torch.randn(shape, device='cuda', requires_grad=True)\n                gc.collect()\n                for training in [True, False]:\n                    run_bn(input, running_mean, running_var, weight, bias, training, training)\n                    torch.cuda.synchronize()\n                    res1 = timeit.timeit('run_bn(input, running_mean, running_var, weight, bias, training, training)', number=100, globals=globals())\n                    with torch.backends.cudnn.flags(enabled=False):\n                      res2 = timeit.timeit('run_bn(input, running_mean, running_var, weight, bias, training, training)', number=100, globals=globals())\n                    print ('{} | {} | {} | {} | {:.2f} | {:.2f} | {:.1f}'.format(bs, c, f, training, res1, res2, res1/res2))", "body": "It's not the prettiest code, but I used this to arrive at the conclusion that for large numbers of features, it's still slow:\r\n```python\r\nimport torch\r\nimport timeit\r\nimport gc\r\nimport numpy\r\n\r\nbatch_sizes = [8,16,32,64,128]\r\nchannels    = [2,32,256,1024]\r\nfeatures    = [(16,),\r\n               (32,),\r\n               (64,),\r\n               (128,),\r\n               (256,),\r\n               (1024,),\r\n               (10240,),\r\n               (102400,),\r\n               (32,32),\r\n               (64,64),\r\n               (128,128),\r\n               (32,32,32),\r\n               (64,64,64)]\r\n\r\ndef run_bn(input, running_mean, running_var, weight, bias, training, backward):\r\n    out = torch.nn.functional.batch_norm(input, running_mean, running_var, weight, bias, training, 0.1, 1e-5)\r\n    if backward:\r\n        grads = torch.autograd.grad(out, [input, weight, bias], torch.ones_like(out))\r\n    torch.cuda.synchronize()\r\n\r\nfor bs in batch_sizes:\r\n    for c in channels:\r\n        for f in features:\r\n            shape = (bs, c)+f\r\n            size = numpy.prod(shape)\r\n            if size < 100_000_000:\r\n                running_mean = torch.randn(c, device='cuda')\r\n                running_var = torch.randn(c, device='cuda').exp()\r\n                weight = torch.randn(c, device='cuda', requires_grad=True)\r\n                bias = torch.randn(c, device='cuda', requires_grad=True)\r\n                input = torch.randn(shape, device='cuda', requires_grad=True)\r\n                gc.collect()\r\n                for training in [True, False]:\r\n                    run_bn(input, running_mean, running_var, weight, bias, training, training)\r\n                    torch.cuda.synchronize()\r\n                    res1 = timeit.timeit('run_bn(input, running_mean, running_var, weight, bias, training, training)', number=100, globals=globals())\r\n                    with torch.backends.cudnn.flags(enabled=False):\r\n                      res2 = timeit.timeit('run_bn(input, running_mean, running_var, weight, bias, training, training)', number=100, globals=globals())\r\n                    print ('{} | {} | {} | {} | {:.2f} | {:.2f} | {:.1f}'.format(bs, c, f, training, res1, res2, res1/res2))\r\n```"}