{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223107931", "pull_request_review_id": 162162755, "id": 223107931, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzEwNzkzMQ==", "diff_hunk": "@@ -23,6 +25,198 @@ namespace {\n   }\n }\n \n+// TensorAccessor when it is defined to work around undefined...\n+template <typename scalar_t>\n+static TensorAccessor<scalar_t, 1> conditional_accessor_1d(const Tensor& t) {\n+  if (! t.defined()) {\n+    return TensorAccessor<scalar_t, 1>(nullptr, nullptr, nullptr);\n+  }\n+  return t.accessor<scalar_t, 1>();\n+}\n+\n+\n+template<typename scalar_t>\n+std::tuple<Tensor,Tensor,Tensor> batch_norm_cpu_template(const Tensor& input, const Tensor& weight, const Tensor& bias,\n+\t\t\t       const Tensor& running_mean, const Tensor& running_var, bool train, double momentum, double eps) {\n+\n+  using accscalar_t = at::acc_type<scalar_t, false>;\n+  Tensor output = at::native::empty_like(input);\n+\n+  int64_t n_input = input.size(1);\n+  int64_t f;\n+  int64_t n = input.numel() / n_input;\n+\n+  Tensor save_mean;\n+  Tensor save_var;\n+  const int64_t zero = 0;\n+  if (train) {\n+    save_mean = at::native::empty({n_input}, input.options());\n+    save_var = at::native::empty({n_input}, input.options());\n+  }\n+  auto save_mean_a = conditional_accessor_1d<scalar_t>(save_mean);\n+  auto save_var_a = conditional_accessor_1d<scalar_t>(save_var);\n+\n+  auto running_mean_a = conditional_accessor_1d<scalar_t>(running_mean);\n+  auto running_var_a = conditional_accessor_1d<scalar_t>(running_var);\n+\n+  #pragma omp parallel for", "path": "aten/src/ATen/native/Normalization.cpp", "position": null, "original_position": 46, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "607a1cc696baad976fd4fe4908edb0bf7eb16fee", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "Indeed. I fall for that. I'll use ATen's parallel_for instead.\r\n\r\nI think one should also see whether some of the computation is better done using TensorIterators (currently I have very much kept the logic from TH(CU)NN except for splitting the cuda forward kernel).\r\n", "created_at": "2018-10-05T18:52:11Z", "updated_at": "2018-11-23T15:52:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r223107931", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223107931"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r223107931"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>Indeed. I fall for that. I'll use ATen's parallel_for instead.</p>\n<p>I think one should also see whether some of the computation is better done using TensorIterators (currently I have very much kept the logic from TH(CU)NN except for splitting the cuda forward kernel).</p>", "body_text": "Indeed. I fall for that. I'll use ATen's parallel_for instead.\nI think one should also see whether some of the computation is better done using TensorIterators (currently I have very much kept the logic from TH(CU)NN except for splitting the cuda forward kernel).", "in_reply_to_id": 223099152}