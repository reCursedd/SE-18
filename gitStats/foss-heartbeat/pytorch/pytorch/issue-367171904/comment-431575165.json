{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/431575165", "html_url": "https://github.com/pytorch/pytorch/pull/12368#issuecomment-431575165", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12368", "id": 431575165, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTU3NTE2NQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-20T12:17:07Z", "updated_at": "2018-10-20T12:17:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So I scrapped a lot of what I had done because it was getting on the wrong path and re-thought some bits:</p>\n<ul>\n<li>In order to not reason too much about things and then changing performance characteristics, I think it is best to \"welfordise\" (compute mean and var in one go) the forward first, and indeed I have been very happy with that. (I'll push this after benchmarking.)</li>\n<li>I have also thought about <code>__restrict__</code> a bit more: The kernels follow a pattern of \"do all computation first\" and then \"write outputs at the end\". To me this would suggest that we can apply restrict without worries - the output, save_mean, save_var are our own anyway, but the running_var/mean are only written to at the end of the collect_stats kernel. I seem to see 5-10% speedup.</li>\n<li>For the input transform (eval / 2nd part in training) there is a balance to be struck between occupancy and memory. We want those for loops in the transform_input-kernel to loop some because it saves us reading the mean/var weight/bias all too often. This will again go have some heuristics for how many threads to use, but I think we'll be much happier when this is done.</li>\n<li>I replaced the funny TensorAccessor manipulation by a reshape and optional transpose.</li>\n<li>For the transpose (swap batch, feature), it seems that calling continuous is good but we have to balance the cost of copying with the advantage from swapping. There might be more room for experimentation, but for now I have some heuristic.</li>\n</ul>", "body_text": "So I scrapped a lot of what I had done because it was getting on the wrong path and re-thought some bits:\n\nIn order to not reason too much about things and then changing performance characteristics, I think it is best to \"welfordise\" (compute mean and var in one go) the forward first, and indeed I have been very happy with that. (I'll push this after benchmarking.)\nI have also thought about __restrict__ a bit more: The kernels follow a pattern of \"do all computation first\" and then \"write outputs at the end\". To me this would suggest that we can apply restrict without worries - the output, save_mean, save_var are our own anyway, but the running_var/mean are only written to at the end of the collect_stats kernel. I seem to see 5-10% speedup.\nFor the input transform (eval / 2nd part in training) there is a balance to be struck between occupancy and memory. We want those for loops in the transform_input-kernel to loop some because it saves us reading the mean/var weight/bias all too often. This will again go have some heuristics for how many threads to use, but I think we'll be much happier when this is done.\nI replaced the funny TensorAccessor manipulation by a reshape and optional transpose.\nFor the transpose (swap batch, feature), it seems that calling continuous is good but we have to balance the cost of copying with the advantage from swapping. There might be more room for experimentation, but for now I have some heuristic.", "body": "So I scrapped a lot of what I had done because it was getting on the wrong path and re-thought some bits:\r\n- In order to not reason too much about things and then changing performance characteristics, I think it is best to \"welfordise\" (compute mean and var in one go) the forward first, and indeed I have been very happy with that. (I'll push this after benchmarking.)\r\n- I have also thought about `__restrict__` a bit more: The kernels follow a pattern of \"do all computation first\" and then \"write outputs at the end\". To me this would suggest that we can apply restrict without worries - the output, save_mean, save_var are our own anyway, but the running_var/mean are only written to at the end of the collect_stats kernel. I seem to see 5-10% speedup.\r\n- For the input transform (eval / 2nd part in training) there is a balance to be struck between occupancy and memory. We want those for loops in the transform_input-kernel to loop some because it saves us reading the mean/var weight/bias all too often. This will again go have some heuristics for how many threads to use, but I think we'll be much happier when this is done.\r\n- I replaced the funny TensorAccessor manipulation by a reshape and optional transpose.\r\n- For the transpose (swap batch, feature), it seems that calling continuous is good but we have to balance the cost of copying with the advantage from swapping. There might be more room for experimentation, but for now I have some heuristic.\r\n"}