{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227892003", "pull_request_review_id": 168034553, "id": 227892003, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzg5MjAwMw==", "diff_hunk": "@@ -0,0 +1,536 @@\n+#include <THC/THCDeviceUtils.cuh>\n+#include <THC/THCGeneral.h>\n+#include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int WARP_SIZE = 64;\n+\n+// take these out when ROCm implements std:: math functions\n+#include <math.h>\n+template <typename scalar_t>\n+static __forceinline__ __device__ scalar_t device_sqrt(scalar_t val);\n+\n+template <>\n+__forceinline__ __device__ float device_sqrt(float val) {\n+  return ::sqrtf(val);\n+}\n+\n+template <>\n+__forceinline__ __device__ double device_sqrt(double val) {\n+  return ::sqrt(val);\n+}\n+\n+#else\n+constexpr int WARP_SIZE = 32;\n+\n+template<typename scalar_t>\n+__forceinline__ __device__ double device_sqrt(scalar_t val) {\n+  return std::sqrt(val);\n+}\n+#endif\n+\n+// The maximum number of threads in a block\n+#if defined(__HIP_PLATFORM_HCC__)\n+constexpr int MAX_BLOCK_SIZE = 256;\n+#else\n+constexpr int MAX_BLOCK_SIZE = 512;\n+#endif\n+\n+// Number of threads in a block given an input size up to MAX_BLOCK_SIZE\n+static int getNumThreads(int nElem) {\n+#if defined(__HIP_PLATFORM_HCC__)\n+  int threadSizes[5] = { 16, 32, 64, 128, MAX_BLOCK_SIZE };\n+#else\n+  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };\n+#endif\n+  for (int i = 0; i != 5; ++i) {\n+    if (nElem <= threadSizes[i]) {\n+      return threadSizes[i];\n+    }\n+  }\n+  return MAX_BLOCK_SIZE;\n+}\n+\n+// Returns the index of the most significant 1 bit in `val`.\n+__device__ __forceinline__ int getMSB(int val) {\n+  return 31 - __clz(val);\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+struct Float2 {\n+  accscalar_t v1, v2;\n+  __device__ Float2() {}\n+  __device__ Float2(scalar_t v1, scalar_t v2) : v1(static_cast<accscalar_t>(v1)), v2(static_cast<accscalar_t>(v2)) {}\n+  __device__ Float2(int v) : v1(static_cast<accscalar_t>(v)), v2(static_cast<accscalar_t>(v)) {}\n+  __device__ Float2& operator+=(const Float2& a) {\n+    v1 += a.v1;\n+    v2 += a.v2;\n+    return *this;\n+  }\n+};\n+\n+template <typename scalar_t, typename accscalar_t, typename PTA>\n+struct SumOp {\n+  __device__ SumOp(const PTA& t) : tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    return static_cast<accscalar_t>(tensor[batch][plane][n]);\n+  }\n+  const PTA& tensor;\n+};\n+\n+  template <typename scalar_t, typename accscalar_t, typename PTA>\n+struct VarOp {\n+  __device__ VarOp(accscalar_t m, const PTA& t) : mean(m), tensor(t) {}\n+  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n+    accscalar_t val = tensor[batch][plane][n];\n+    return (val - mean) * (val - mean);\n+  }\n+  const accscalar_t mean;\n+  const PTA& tensor;\n+};\n+\n+template <typename scalar_t, typename accscalar_t, typename PTA>\n+struct GradOp {\n+  __device__ GradOp(accscalar_t m, const PTA& i, const PTA& g)\n+    : mean(m), input(i), grad_output(g) {}\n+  __device__ __forceinline__ Float2<scalar_t, accscalar_t> operator()(int batch, int plane, int n) {\n+    accscalar_t g = grad_output[batch][plane][n];\n+    accscalar_t c = static_cast<accscalar_t>(input[batch][plane][n]) - mean;\n+    return Float2<scalar_t, accscalar_t>(g, g * c);\n+  }\n+  const accscalar_t mean;\n+  const PTA& input;\n+  const PTA& grad_output;\n+};\n+\n+// Sum across all threads within a warp\n+template <typename T>\n+static __device__ __forceinline__ T warpSum(T val) {\n+  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n+    val += WARP_SHFL_XOR(val, 1 << i, WARP_SIZE);\n+  }\n+  return val;\n+}\n+\n+template <typename scalar_t, typename accscalar_t>\n+static __device__ __forceinline__ Float2<scalar_t, accscalar_t> warpSum(Float2<scalar_t, accscalar_t> value) {\n+  value.v1 = warpSum(value.v1);\n+  value.v2 = warpSum(value.v2);\n+  return value;\n+}\n+\n+// Sum across (batch, x/y/z) applying Op() pointwise\n+// this works by first having each thread sum it's part\n+// of the data. Then there is a double-shuffeling reduction.\n+// First each warp (of WARP_SIZE threads) uses warpSum to reduce its\n+// data to the \"warp leader\", who writes its value into shared memory.\n+// Then a single warp reads the remaining (at most WARP_SIZE) items\n+// and reduces them using another warpSum.\n+// The implicit assumption is that there are no more\n+// than WARP_SIZE**2 threads.\n+template<typename scalar_t, typename Op, typename PTA>\n+__device__ scalar_t reduce(Op op, PTA tensor, int plane) {\n+  // first the reductions each thread does separately\n+  scalar_t sum = static_cast<scalar_t>(0);\n+  for (int batch = threadIdx.y; batch < tensor.size(0); batch += blockDim.y) {\n+    for (int x = threadIdx.x; x < tensor.size(2); x += blockDim.x) {\n+      sum += op(batch, plane, x);\n+    }\n+  }\n+\n+  // first warpSum to get one value per thread to\n+  // one value per warp\n+  sum = warpSum(sum);\n+\n+  // this writes each warps  item into shared memory\n+  // there are at most WARP_SIZE items left because\n+  // there are at most WARP_SIZE**2 threads at the beginning\n+  __shared__ scalar_t shared[WARP_SIZE];\n+  __syncthreads();\n+  int tid = threadIdx.x + threadIdx.y * blockDim.x;\n+  if (tid % WARP_SIZE == 0) {\n+    shared[tid / WARP_SIZE] = sum;\n+  }\n+  if (tid >= blockDim.x * blockDim.y / WARP_SIZE && tid < WARP_SIZE) {\n+    // zero out the other entries in shared\n+    shared[tid] = (scalar_t)0;\n+  }\n+  __syncthreads();\n+  // now have a second warpSum to reduce the intermediate values\n+  // from shared memory to a single number. The very first\n+  // thread writes it to shared memory.\n+\n+  if (tid / WARP_SIZE == 0) {\n+    sum = warpSum(shared[tid]);\n+    if (tid == 0) {\n+      shared[0] = sum;\n+    }\n+  }\n+  __syncthreads();\n+\n+  // Everyone picks it up, should be broadcast into the whole grad_input\n+  return shared[0];\n+}\n+\n+template <typename scalar_t, typename accscalar_t, bool train, typename index_t>\n+__global__ void batch_norm_transform_input_kernel(\n+    const PackedTensorAccessor<scalar_t, 3, RestrictPtrTraits, index_t> input,\n+    PackedTensorAccessor<scalar_t, 3, RestrictPtrTraits, index_t> output,\n+    const PackedTensorAccessor<typename std::conditional<train, accscalar_t, scalar_t>::type, 1, RestrictPtrTraits, index_t> mean_,\n+    const PackedTensorAccessor<typename std::conditional<train, accscalar_t, scalar_t>::type, 1, RestrictPtrTraits, index_t> var_or_invstd,\n+    const PackedTensorAccessor<scalar_t, 1, RestrictPtrTraits, index_t> weight,\n+    const PackedTensorAccessor<scalar_t, 1, RestrictPtrTraits, index_t> bias,\n+    accscalar_t epsilon) {\n+\n+  index_t plane = blockIdx.x;\n+\n+  if (plane >= input.size(1)) {\n+    return;\n+  }\n+\n+  accscalar_t gamma = weight.size(0) > 0 ? static_cast<accscalar_t>(weight[plane]) : static_cast<accscalar_t>(1);\n+  accscalar_t beta = bias.size(0) > 0 ? static_cast<accscalar_t>(bias[plane]) : static_cast<accscalar_t>(0);\n+  accscalar_t mean = static_cast<accscalar_t>(mean_[plane]);\n+  accscalar_t invstd;\n+  if (train) {\n+    invstd = var_or_invstd[plane];\n+  } else {\n+    invstd = static_cast<accscalar_t>(1) / device_sqrt(static_cast<accscalar_t>(var_or_invstd[plane]) + epsilon);\n+  }\n+\n+  index_t bs = input.size(0);\n+  index_t fs = input.size(2);\n+\n+  index_t bstep  = blockDim.y * gridDim.y;\n+  for (index_t batch = threadIdx.y + blockIdx.y * blockDim.y; batch < bs; batch += bstep) {\n+    auto o = output[batch][plane];\n+    auto i = input[batch][plane];\n+    for (index_t feature = threadIdx.x; feature < fs; feature += blockDim.x) {\n+      o[feature] = static_cast<scalar_t>(gamma * (i[feature] - mean) * invstd + beta);\n+    }\n+  }\n+}\n+\n+\n+template <typename scalar_t, typename accscalar_t, typename index_t>\n+__global__ void batch_norm_collect_statistics_kernel(\n+    const PackedTensorAccessor<scalar_t, 3, RestrictPtrTraits, index_t> input,\n+    const accscalar_t epsilon,\n+    const accscalar_t momentum,\n+    PackedTensorAccessor<scalar_t, 1, RestrictPtrTraits, index_t> running_mean,\n+    PackedTensorAccessor<scalar_t, 1, RestrictPtrTraits, index_t> running_var,\n+    PackedTensorAccessor<accscalar_t, 1, RestrictPtrTraits, index_t> save_mean,\n+    PackedTensorAccessor<accscalar_t, 1, RestrictPtrTraits, index_t> save_invstd) {\n+\n+  __shared__ int shared_n[2 * 2 * WARP_SIZE + WARP_SIZE];\n+\n+  int plane = blockIdx.x;\n+  int N = input.size(0) * input.size(2);\n+  int tid = threadIdx.x + threadIdx.y * blockDim.x;\n+\n+  // Compute the mean and variance across (batch, x/y/z)\n+  // this uses the Welford (in the for loop)/parallel algorithm (to sum across the block)\n+  // https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_Online_algorithm\n+  // and the parallel algorithm on the same page.\n+  // We use two shuffles to reduce across the entire block.\n+  // https://devblogs.nvidia.com/faster-parallel-reductions-kepler/ has a description.\n+  accscalar_t* shared_avg_var = (accscalar_t*) &shared_n[WARP_SIZE];\n+\n+  // first the reductions each thread does separately\n+  accscalar_t avg = 0;\n+  accscalar_t var_n = 0;\n+  int n = 0;\n+  for (int batch = threadIdx.y; batch < input.size(0); batch += blockDim.y) {\n+    for (int x = threadIdx.x; x < input.size(2); x += blockDim.x) {\n+      accscalar_t v = input[batch][plane][x];\n+      accscalar_t d1 = v - avg;\n+      n++;\n+      avg += d1 / n;\n+      var_n += d1 * (v - avg);\n+    }\n+  }\n+\n+  // first warpSum to get one value per thread to\n+  // one value per warp\n+  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n+    accscalar_t o_avg = WARP_SHFL_XOR(avg, 1 << i, WARP_SIZE);\n+    int o_n = WARP_SHFL_XOR(n, 1 << i, WARP_SIZE);\n+    if (n + o_n > 0) {\n+      var_n += WARP_SHFL_XOR(var_n, 1 << i, WARP_SIZE) + ((avg - o_avg) * (avg - o_avg) * n * o_n) / (n + o_n);\n+      avg = (n * avg + o_n * o_avg)/(n+o_n);\n+      n += o_n;\n+    }\n+  }\n+\n+  // this writes each warps  item into shared memory\n+  // there are at most WARP_SIZE items left because\n+  // there are at most WARP_SIZE**2 threads at the beginning  \n+  __syncthreads();\n+  if (tid % WARP_SIZE == 0) {\n+    shared_n[tid / WARP_SIZE] = n;\n+    shared_avg_var[tid / WARP_SIZE * 2] = avg;\n+    shared_avg_var[tid / WARP_SIZE * 2 + 1] = var_n;\n+  }\n+  __syncthreads();\n+  // now have a second warpSum to reduce the intermediate values\n+  // from shared memory to a single number. The very first\n+  // thread writes it to shared memory.\n+\n+  if (tid < WARP_SIZE) {\n+    n = (tid < blockDim.x * blockDim.y / WARP_SIZE ? shared_n[tid] : 0);\n+    avg = (tid < blockDim.x * blockDim.y  / WARP_SIZE ? shared_avg_var[2 * tid] : 0);\n+    var_n = (tid < blockDim.x * blockDim.y  / WARP_SIZE ? shared_avg_var[2 * tid + 1] : 0);\n+  }\n+  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n+    accscalar_t o_avg = WARP_SHFL_XOR(avg, 1 << i, WARP_SIZE);\n+    int o_n = WARP_SHFL_XOR(n, 1 << i, WARP_SIZE);\n+    if (n + o_n > 0) {\n+      var_n += WARP_SHFL_XOR(var_n, 1 << i, WARP_SIZE) + ((avg - o_avg) * (avg - o_avg) * n * o_n) / (n + o_n);\n+      avg = (n * avg + o_n * o_avg)/(n+o_n);\n+      n += o_n;\n+    }\n+  }\n+\n+  // Save the mean, variance, and moving averages\n+  if (tid == 0) {\n+    accscalar_t invstd = 0;\n+    if (var_n != static_cast<accscalar_t>(0) || epsilon != static_cast<accscalar_t>(0)) {\n+      invstd = static_cast<accscalar_t>(1) / device_sqrt(var_n / N + epsilon);\n+    }\n+    save_mean[plane] = avg;\n+    save_invstd[plane] = invstd;\n+    if (running_mean.data() != NULL) {\n+      running_mean[plane] = static_cast<scalar_t>((1 - momentum) * running_mean[plane] + momentum * avg);\n+    }\n+    if (running_var.data() != NULL) {\n+      accscalar_t unbiasedVar = var_n / (N - 1);\n+      running_var[plane] = static_cast<scalar_t>((1 - momentum) * running_var[plane] + momentum * unbiasedVar);\n+    }\n+  }\n+\n+}\n+\n+template <typename scalar_t, typename accscalar_t, typename index_t>\n+__global__ void batch_norm_backward_kernel(\n+    const PackedTensorAccessor<scalar_t, 3, DefaultPtrTraits, index_t> input,\n+    const PackedTensorAccessor<scalar_t, 3, DefaultPtrTraits, index_t> grad_output,\n+    PackedTensorAccessor<scalar_t, 3, DefaultPtrTraits, index_t> grad_input,\n+    PackedTensorAccessor<scalar_t, 1, DefaultPtrTraits, index_t> grad_weight,\n+    PackedTensorAccessor<scalar_t, 1, DefaultPtrTraits, index_t> grad_bias,\n+    const PackedTensorAccessor<scalar_t, 1, DefaultPtrTraits, index_t> weight,\n+    const PackedTensorAccessor<scalar_t, 1, DefaultPtrTraits, index_t> running_mean,\n+    const PackedTensorAccessor<scalar_t, 1, DefaultPtrTraits, index_t> running_var,\n+    const PackedTensorAccessor<accscalar_t, 1, DefaultPtrTraits, index_t> save_mean,\n+    const PackedTensorAccessor<accscalar_t, 1, DefaultPtrTraits, index_t> save_invstd,\n+    bool train,\n+    accscalar_t epsilon) {\n+\n+  index_t plane = blockIdx.x;\n+  index_t N = grad_output.size(0) * grad_output.size(2);\n+\n+  accscalar_t mean, invstd;\n+  if (train) {\n+    mean = save_mean[plane];\n+    invstd = save_invstd[plane];\n+  } else {\n+    mean = static_cast<accscalar_t>(running_mean[plane]);\n+    invstd = static_cast<accscalar_t>(1) / device_sqrt(static_cast<accscalar_t>(running_var[plane]) + epsilon);\n+  }\n+\n+  accscalar_t weight_val = weight.size(0) > 0 ? static_cast<accscalar_t>(weight[plane]) : accscalar_t(1);\n+  accscalar_t norm = accscalar_t(1) / N;\n+\n+  // Compute two values across (batch, x/y/z) in one pass:\n+  // 1. Sum(grad_output)\n+  // 2. DotProduct(input - mean, grad_output)\n+  GradOp<scalar_t, accscalar_t, PackedTensorAccessor<scalar_t, 3, DefaultPtrTraits, index_t>> g(mean, input, grad_output);\n+  Float2<scalar_t, accscalar_t> res = reduce<Float2<scalar_t, accscalar_t>, GradOp<scalar_t, accscalar_t,\n+\t\t\t\t\t\t\t\t\t\t   PackedTensorAccessor<scalar_t, 3, DefaultPtrTraits, index_t>>>(g, grad_output, plane);\n+  accscalar_t grad_output_sum = res.v1;\n+  accscalar_t dot_p = res.v2;\n+\n+  accscalar_t grad_mean = grad_output_sum * norm;\n+  accscalar_t proj_scale = dot_p * norm * invstd * invstd;\n+  accscalar_t grad_scale = invstd * weight_val;\n+\n+  if (grad_input.data() != NULL) {\n+    for (int batch = threadIdx.y; batch < grad_output.size(0); batch += blockDim.y) {\n+      for (int x = threadIdx.x; x < grad_output.size(2); x += blockDim.x) {\n+        scalar_t go = grad_output[batch][plane][x];\n+        if (train) {\n+          scalar_t inp = input[batch][plane][x];\n+          accscalar_t proj = (inp - mean) * proj_scale;\n+          grad_input[batch][plane][x] = static_cast<scalar_t>((go - proj - grad_mean) * grad_scale);\n+        } else {\n+          grad_input[batch][plane][x] = static_cast<scalar_t>(go * grad_scale);\n+        }\n+      }\n+    }\n+  }\n+\n+  if (grad_weight.size(0) > 0) {\n+    if (threadIdx.x == 0) {\n+      grad_weight[plane] = static_cast<scalar_t>(dot_p * invstd);\n+    }\n+  }\n+\n+  if (grad_bias.size(0) > 0) {\n+    if (threadIdx.x == 0) {\n+      grad_bias[plane] = static_cast<scalar_t>(grad_output_sum);\n+    }\n+  }\n+}\n+\n+template <typename scalar_t, int64_t dim, template <typename U> class PtrTraits = DefaultPtrTraits, typename index_t = int64_t>\n+static PackedTensorAccessor<scalar_t, dim, PtrTraits, index_t> packed_accessor_or_dummy(const Tensor& t) {\n+  if (! t.defined()) {\n+    const std::vector<index_t> zeros(dim);\n+    return PackedTensorAccessor<scalar_t, dim, PtrTraits, index_t>(nullptr, zeros.data(), zeros.data());\n+  }\n+  return t.packed_accessor<scalar_t, dim, PtrTraits, index_t>();\n+}\n+\n+template<typename scalar_t, typename index_t>\n+std::tuple<Tensor, Tensor, Tensor> batch_norm_cuda_template(const Tensor& input_, const Tensor& weight_, const Tensor& bias_,\n+\t\t\t\t\t\t\t    const Tensor& running_mean_, const Tensor& running_var_,\n+\t\t\t\t\t\t\t    bool train, double momentum, double epsilon) {\n+\n+  using accscalar_t = at::acc_type<scalar_t, true>;\n+  int64_t n_input = input_.size(1);\n+  Tensor save_mean_;\n+  Tensor save_invstd_;\n+  auto input_reshaped = input_.reshape({input_.size(0), input_.size(1), -1}); // internally we merge the feature dimensions\n+  auto output_reshaped = at::empty_like(input_reshaped);\n+\n+  auto bs = input_reshaped.size(0);\n+  auto features = input_reshaped.size(2);\n+  auto input = input_reshaped.packed_accessor<scalar_t, 3, RestrictPtrTraits, index_t>();\n+  auto input_options = input_.options();\n+  if (input_options.dtype() == ScalarType::Half) {\n+    input_options = input_options.dtype(ScalarType::Float);\n+  }\n+  if (train) {\n+    save_mean_ = at::empty({n_input}, input_options);\n+    save_invstd_ = at::empty({n_input}, input_options);\n+  } else {\n+    save_mean_ = at::empty({0}, input_options);\n+    save_invstd_ = at::empty({0}, input_options);\n+  }\n+  auto output = output_reshaped.packed_accessor<scalar_t, 3, RestrictPtrTraits, index_t>();\n+  auto weight = packed_accessor_or_dummy<scalar_t, 1, RestrictPtrTraits, index_t>(weight_);\n+  auto bias = packed_accessor_or_dummy<scalar_t, 1, RestrictPtrTraits, index_t>(bias_);\n+  auto running_mean = packed_accessor_or_dummy<scalar_t, 1, RestrictPtrTraits, index_t>(running_mean_);\n+  auto running_var = packed_accessor_or_dummy<scalar_t, 1, RestrictPtrTraits, index_t>(running_var_);\n+  auto save_mean = save_mean_.packed_accessor<accscalar_t, 1, RestrictPtrTraits, index_t>();\n+  auto save_invstd = save_invstd_.packed_accessor<accscalar_t, 1, RestrictPtrTraits, index_t>();\n+  auto stream = at::cuda::getCurrentCUDAStream();\n+\n+  // The input_transform kernel is pointwise, but we need to balance reading parameters (save_var/mean,\n+  // weight/bias) - which we only do once and have a for loop afterwards - with having many threads and blocks\n+  // and good occupancy. Quiet likely, we could go with even more blocks than 1024.\n+  // The various planes are independent, so we use blocks for them.\n+  int tf = std::max<int>(getNumThreads(input.size(2)/4),\n+\t\t\t std::min<int>(getNumThreads(input.size(2)), 64));\n+  int tb = std::max<int>(64/tf, 1);\n+  dim3 blocks_trans(input.size(1), std::max<int>(1, std::min<int>((256*1024)/input.size(1),\n+\t\t\t\t\t\t\t\t  (input.size(0)+tb-1)/tb)));\n+  dim3 threads_trans(tf, tb);\n+  if (!train) {\n+    batch_norm_transform_input_kernel<scalar_t, accscalar_t, false, index_t> <<<blocks_trans, threads_trans, 0, stream>>>\n+      (input, output, running_mean, running_var, weight, bias, epsilon);\n+  } else {\n+    // for the reduction, we cannot use blocks for the batch dim, but if we have few threads in\n+    // the feature dimension, we'll use some threads for blocks\n+    dim3 blocks(input.size(1));\n+    tf = getNumThreads(input.size(2));\n+    dim3 threads(tf, std::max<int>(1, MAX_BLOCK_SIZE/tf));\n+    batch_norm_collect_statistics_kernel<scalar_t, accscalar_t, index_t> <<<blocks, threads, 0, stream>>>\n+      (input, epsilon, momentum, running_mean, running_var, save_mean, save_invstd);\n+    batch_norm_transform_input_kernel<scalar_t, accscalar_t, true, index_t> <<<blocks_trans, threads_trans, 0, stream>>>\n+      (input, output, save_mean, save_invstd, weight, bias, epsilon);\n+  }\n+  THCudaCheck(cudaGetLastError());\n+  return std::make_tuple(output_reshaped.view(input_.sizes()), save_mean_, save_invstd_);\n+}\n+\n+template<typename scalar_t, typename index_t>\n+std::tuple<Tensor, Tensor, Tensor> batch_norm_backward_cuda_template(const Tensor& grad_out_, const Tensor& input_, const Tensor& weight_,\n+\t\t\t\t\t\t\t\t     const Tensor& running_mean_, const Tensor& running_var_, const Tensor& save_mean_, const Tensor& save_invstd_,\n+\t\t\t\t\t\t\t\t     bool train, double epsilon, std::array<bool,3> grad_input_mask) {\n+\n+  using accscalar_t = at::acc_type<scalar_t, true>;\n+  Tensor grad_input_;\n+  Tensor grad_input_reshaped;\n+  Tensor grad_weight_;\n+  Tensor grad_bias_;\n+  auto input_reshaped = input_.reshape({input_.size(0), input_.size(1), -1});\n+  auto grad_output_reshaped = grad_out_.reshape(input_reshaped.sizes());\n+\n+  if (grad_input_mask[0]) {\n+    grad_input_ = at::empty_like(input_);\n+    grad_input_reshaped = grad_input_.view(input_reshaped.sizes());\n+  }\n+  if (grad_input_mask[1]) {\n+    grad_weight_ = at::empty_like(weight_);\n+  }\n+  if (grad_input_mask[2]) {\n+    grad_bias_ = at::empty_like(weight_);\n+  }\n+\n+  auto input = input_reshaped.packed_accessor<scalar_t, 3, DefaultPtrTraits, index_t>();\n+  auto grad_output = grad_output_reshaped.packed_accessor<scalar_t, 3, DefaultPtrTraits, index_t>();\n+  auto grad_input = packed_accessor_or_dummy<scalar_t, 3, DefaultPtrTraits, index_t>(grad_input_reshaped);\n+  auto weight = packed_accessor_or_dummy<scalar_t, 1, DefaultPtrTraits, index_t>(weight_);\n+  auto grad_weight = packed_accessor_or_dummy<scalar_t, 1, DefaultPtrTraits, index_t>(grad_weight_);\n+  auto grad_bias = packed_accessor_or_dummy<scalar_t, 1, DefaultPtrTraits, index_t>(grad_bias_);\n+  auto running_mean = packed_accessor_or_dummy<scalar_t, 1, DefaultPtrTraits, index_t>(running_mean_);\n+  auto running_var = packed_accessor_or_dummy<scalar_t, 1, DefaultPtrTraits, index_t>(running_var_);\n+  auto save_mean = packed_accessor_or_dummy<accscalar_t, 1, DefaultPtrTraits, index_t>(save_mean_);\n+  auto save_invstd = packed_accessor_or_dummy<accscalar_t, 1, DefaultPtrTraits, index_t>(save_invstd_);\n+\n+  auto stream = at::cuda::getCurrentCUDAStream();\n+  dim3 blocks(input.size(1));\n+  int tf = getNumThreads(input.size(2));\n+  dim3 threads(tf, std::max<int>(1, MAX_BLOCK_SIZE/tf));\n+\n+  batch_norm_backward_kernel<scalar_t,  accscalar_t, index_t> <<<blocks, threads, 0, stream>>>\n+    (input, grad_output, grad_input, grad_weight, grad_bias, weight, running_mean, running_var,\n+     save_mean, save_invstd, train, epsilon);\n+  THCudaCheck(cudaGetLastError());\n+\n+  return std::make_tuple(grad_input_, grad_weight_, grad_bias_);\n+}\n+\n+} // anonymous namespace\n+\n+std::tuple<Tensor, Tensor, Tensor> batch_norm_cuda(const Tensor& self, const Tensor& weight, const Tensor& bias,\n+\t\t\t\t\t\t   const Tensor& running_mean, const Tensor& running_var, bool train, double momentum, double epsilon) {\n+  return AT_DISPATCH_FLOATING_TYPES_AND_HALF(self.type(), \"batch_norm\", [&] {\n+      if (cuda::detail::canUse32BitIndexMath(self)) {\n+\treturn batch_norm_cuda_template<scalar_t, int32_t>(self, weight, bias, running_mean, running_var, train, momentum, epsilon);", "path": "aten/src/ATen/native/cuda/Normalization.cu", "position": null, "original_position": 518, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "a7b0d1c4500265f5e7c3b6e83f021358f38e063c", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "mis-indented.", "created_at": "2018-10-24T17:44:19Z", "updated_at": "2018-11-23T15:53:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r227892003", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227892003"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r227892003"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>mis-indented.</p>", "body_text": "mis-indented."}