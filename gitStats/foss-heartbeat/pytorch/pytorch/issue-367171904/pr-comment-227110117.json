{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227110117", "pull_request_review_id": 167075203, "id": 227110117, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNzExMDExNw==", "diff_hunk": "@@ -432,10 +466,15 @@ std::tuple<Tensor, Tensor, Tensor> batch_norm_backward_cuda_template(const Tenso\n \n   using accscalar_t = at::acc_type<scalar_t, true>;\n   Tensor grad_input_;\n+  Tensor grad_input_cont;\n   Tensor grad_weight_;\n   Tensor grad_bias_;\n+  auto input_cont = input_.reshape({input_.size(0), input_.size(1), -1});", "path": "aten/src/ATen/native/cuda/Normalization.cu", "position": null, "original_position": 360, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "89e7cdb9019632cd5e109b8cb1eac5a475a8290d", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "you probably want to `.contiguous()` this before `reshape`. `reshape` doesn't guarantee contiguity.\r\n\r\nhowever, I am a bit puzzled here. in theory, shouldn't `run kernel with noncontig data` always be at least as fast as `make contiguous + run kernel with contig data`?", "created_at": "2018-10-22T19:39:55Z", "updated_at": "2018-11-23T15:53:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r227110117", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/227110117"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r227110117"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>you probably want to <code>.contiguous()</code> this before <code>reshape</code>. <code>reshape</code> doesn't guarantee contiguity.</p>\n<p>however, I am a bit puzzled here. in theory, shouldn't <code>run kernel with noncontig data</code> always be at least as fast as <code>make contiguous + run kernel with contig data</code>?</p>", "body_text": "you probably want to .contiguous() this before reshape. reshape doesn't guarantee contiguity.\nhowever, I am a bit puzzled here. in theory, shouldn't run kernel with noncontig data always be at least as fast as make contiguous + run kernel with contig data?"}