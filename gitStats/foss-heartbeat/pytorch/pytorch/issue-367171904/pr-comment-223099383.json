{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223099383", "pull_request_review_id": 162152471, "id": 223099383, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzA5OTM4Mw==", "diff_hunk": "@@ -23,6 +25,198 @@ namespace {\n   }\n }\n \n+// TensorAccessor when it is defined to work around undefined...\n+template <typename scalar_t>\n+static TensorAccessor<scalar_t, 1> conditional_accessor_1d(const Tensor& t) {\n+  if (! t.defined()) {\n+    return TensorAccessor<scalar_t, 1>(nullptr, nullptr, nullptr);\n+  }\n+  return t.accessor<scalar_t, 1>();\n+}\n+\n+\n+template<typename scalar_t>\n+std::tuple<Tensor,Tensor,Tensor> batch_norm_cpu_template(const Tensor& input, const Tensor& weight, const Tensor& bias,\n+\t\t\t       const Tensor& running_mean, const Tensor& running_var, bool train, double momentum, double eps) {\n+\n+  using accscalar_t = at::acc_type<scalar_t, false>;\n+  Tensor output = at::native::empty_like(input);\n+\n+  int64_t n_input = input.size(1);\n+  int64_t f;\n+  int64_t n = input.numel() / n_input;\n+\n+  Tensor save_mean;\n+  Tensor save_var;\n+  const int64_t zero = 0;\n+  if (train) {\n+    save_mean = at::native::empty({n_input}, input.options());\n+    save_var = at::native::empty({n_input}, input.options());\n+  }\n+  auto save_mean_a = conditional_accessor_1d<scalar_t>(save_mean);\n+  auto save_var_a = conditional_accessor_1d<scalar_t>(save_var);\n+\n+  auto running_mean_a = conditional_accessor_1d<scalar_t>(running_mean);\n+  auto running_var_a = conditional_accessor_1d<scalar_t>(running_var);\n+\n+  #pragma omp parallel for\n+  for (f = 0; f < n_input; ++f) {\n+    Tensor in = input.select(1, f);\n+    Tensor out = output.select(1, f);\n+\n+    scalar_t mean, invstd;\n+\n+    if (train) {\n+      // compute mean per input\n+      accscalar_t sum = in.sum()._local_scalar().to<accscalar_t>(); // accumulation dtype ?\n+\n+      mean = (scalar_t) sum / n;\n+      save_mean_a[f] = mean;\n+\n+      // compute variance per input\n+      sum = 0;\n+      CPU_tensor_apply1<scalar_t>(in, [&] (const scalar_t& i) {\n+\t  sum += (i - mean) * (i - mean);\n+\t});\n+\n+      if (sum == 0 && eps == 0.0) {\n+        invstd = 0;\n+      } else {\n+        invstd = (scalar_t) (1 / std::sqrt(sum/n + eps));\n+      }\n+      save_var_a[f] = sum/n;\n+\n+      // update running averages\n+      if (running_mean.defined()) {\n+\trunning_mean_a[f] = momentum * mean + (1 - momentum) * running_mean_a[f];\n+      }\n+      if (running_var.defined()) {\n+        accscalar_t unbiased_var = sum / (n - 1);\n+\trunning_var_a[f] = momentum * unbiased_var + (1 - momentum) * running_var_a[f];\n+      }\n+    } else {\n+      mean = running_mean_a[f];\n+      invstd = 1 / std::sqrt(running_var_a[f] + eps);\n+    }\n+\n+    // compute output\n+    scalar_t w = weight.defined() ? weight.data<scalar_t>()[f * weight.stride(0)] : 1;\n+    scalar_t b = bias.defined() ? bias.data<scalar_t>()[f * bias.stride(0)] : 0;\n+\n+    CPU_tensor_apply2<scalar_t,scalar_t>(out, in, [&](scalar_t& o, const scalar_t& i) {\n+\to = ((i - mean) * invstd) * w + b;\n+      });\n+  }\n+  return std::make_tuple(output, save_mean, save_var);\n+}\n+\n+\n+template<typename scalar_t>\n+std::tuple<Tensor, Tensor, Tensor> batch_norm_backward_cpu_template(const Tensor& grad_out_, const Tensor& input, const Tensor& weight,\n+\t\t\t\t\t\t\t\t    const Tensor& running_mean, const Tensor& running_var, const Tensor& save_mean, const Tensor& save_var,\n+\t\t\t\t\t\t\t\t    bool train, double eps, std::array<bool,3> grad_input_mask) {\n+\n+  using accscalar_t = at::acc_type<scalar_t, false>;\n+\n+  Tensor grad_input;\n+  Tensor grad_weight;\n+  Tensor grad_bias;\n+  if (grad_input_mask[0]) {\n+    grad_input = at::native::empty_like(input);\n+  }\n+  if (grad_input_mask[1]) {\n+    grad_weight = at::native::empty_like(weight);\n+  }\n+  if (grad_input_mask[2]) {\n+    grad_bias = at::native::empty_like(weight);\n+  }\n+\n+  auto weight_a = conditional_accessor_1d<scalar_t>(weight);\n+  auto grad_weight_a = conditional_accessor_1d<scalar_t>(grad_weight);\n+  auto grad_bias_a = conditional_accessor_1d<scalar_t>(grad_bias);\n+\n+  int64_t n_input = input.size(1);\n+  int64_t n = input.numel() / n_input;\n+\n+  auto save_mean_a = conditional_accessor_1d<scalar_t>(save_mean);\n+  auto save_var_a = conditional_accessor_1d<scalar_t>(save_var);\n+\n+  auto running_mean_a = conditional_accessor_1d<scalar_t>(running_mean);\n+  auto running_var_a = conditional_accessor_1d<scalar_t>(running_var);\n+\n+  int64_t f;\n+  #pragma omp parallel for", "path": "aten/src/ATen/native/Normalization.cpp", "position": null, "original_position": 132, "commit_id": "0d7aac942b890f4b345f3511c14ab41fede97741", "original_commit_id": "607a1cc696baad976fd4fe4908edb0bf7eb16fee", "user": {"login": "vishwakftw", "id": 23639302, "node_id": "MDQ6VXNlcjIzNjM5MzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/23639302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishwakftw", "html_url": "https://github.com/vishwakftw", "followers_url": "https://api.github.com/users/vishwakftw/followers", "following_url": "https://api.github.com/users/vishwakftw/following{/other_user}", "gists_url": "https://api.github.com/users/vishwakftw/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishwakftw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishwakftw/subscriptions", "organizations_url": "https://api.github.com/users/vishwakftw/orgs", "repos_url": "https://api.github.com/users/vishwakftw/repos", "events_url": "https://api.github.com/users/vishwakftw/events{/privacy}", "received_events_url": "https://api.github.com/users/vishwakftw/received_events", "type": "User", "site_admin": false}, "body": "Likewise.", "created_at": "2018-10-05T18:23:46Z", "updated_at": "2018-11-23T15:52:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/12368#discussion_r223099383", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12368", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223099383"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12368#discussion_r223099383"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12368"}}, "body_html": "<p>Likewise.</p>", "body_text": "Likewise."}