{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4917", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4917/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4917/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4917/events", "html_url": "https://github.com/pytorch/pytorch/issues/4917", "id": 292566229, "node_id": "MDU6SXNzdWUyOTI1NjYyMjk=", "number": 4917, "title": "Adding numpy log to torch Variable", "user": {"login": "16lawrencel", "id": 3189051, "node_id": "MDQ6VXNlcjMxODkwNTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/3189051?v=4", "gravatar_id": "", "url": "https://api.github.com/users/16lawrencel", "html_url": "https://github.com/16lawrencel", "followers_url": "https://api.github.com/users/16lawrencel/followers", "following_url": "https://api.github.com/users/16lawrencel/following{/other_user}", "gists_url": "https://api.github.com/users/16lawrencel/gists{/gist_id}", "starred_url": "https://api.github.com/users/16lawrencel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/16lawrencel/subscriptions", "organizations_url": "https://api.github.com/users/16lawrencel/orgs", "repos_url": "https://api.github.com/users/16lawrencel/repos", "events_url": "https://api.github.com/users/16lawrencel/events{/privacy}", "received_events_url": "https://api.github.com/users/16lawrencel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-29T21:25:47Z", "updated_at": "2018-01-29T21:28:32Z", "closed_at": "2018-01-29T21:28:25Z", "author_association": "NONE", "body_html": "<p>For example,</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\nimport numpy as np\n\na = Variable(torch.Tensor([1, 2, 3]))\nb = np.log(3)\nprint(b + a)\n</code></pre>\n<p>Outputs</p>\n<pre><code>[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\n 2.0986\n[torch.FloatTensor of size 1]\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\n 3.0986\n[torch.FloatTensor of size 1]\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\n 4.0986\n[torch.FloatTensor of size 1]\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\n</code></pre>\n<p>for me.<br>\nI don't know too much about pytorch, but I'd be surprised if this was intended behavior.<br>\nInterestingly enough, changing</p>\n<pre><code>print(b + a)\n</code></pre>\n<p>to</p>\n<pre><code>print(a + b)\n</code></pre>\n<p>fixes the problem.</p>", "body_text": "For example,\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\n\na = Variable(torch.Tensor([1, 2, 3]))\nb = np.log(3)\nprint(b + a)\n\nOutputs\n[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\n 2.0986\n[torch.FloatTensor of size 1]\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\n 3.0986\n[torch.FloatTensor of size 1]\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\n 4.0986\n[torch.FloatTensor of size 1]\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\n\nfor me.\nI don't know too much about pytorch, but I'd be surprised if this was intended behavior.\nInterestingly enough, changing\nprint(b + a)\n\nto\nprint(a + b)\n\nfixes the problem.", "body": "For example, \r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\n\r\na = Variable(torch.Tensor([1, 2, 3]))\r\nb = np.log(3)\r\nprint(b + a)\r\n```\r\nOutputs \r\n```\r\n[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\r\n 2.0986\r\n[torch.FloatTensor of size 1]\r\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\r\n 3.0986\r\n[torch.FloatTensor of size 1]\r\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[Variable containing:\r\n 4.0986\r\n[torch.FloatTensor of size 1]\r\n]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\r\n```\r\nfor me.\r\nI don't know too much about pytorch, but I'd be surprised if this was intended behavior.\r\nInterestingly enough, changing \r\n```\r\nprint(b + a)\r\n```\r\nto\r\n```\r\nprint(a + b)\r\n```\r\nfixes the problem."}