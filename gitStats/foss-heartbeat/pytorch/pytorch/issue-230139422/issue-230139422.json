{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1600", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1600/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1600/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1600/events", "html_url": "https://github.com/pytorch/pytorch/issues/1600", "id": 230139422, "node_id": "MDU6SXNzdWUyMzAxMzk0MjI=", "number": 1600, "title": "Segmentation fault when using backward()", "user": {"login": "psmaragdis", "id": 4094887, "node_id": "MDQ6VXNlcjQwOTQ4ODc=", "avatar_url": "https://avatars2.githubusercontent.com/u/4094887?v=4", "gravatar_id": "", "url": "https://api.github.com/users/psmaragdis", "html_url": "https://github.com/psmaragdis", "followers_url": "https://api.github.com/users/psmaragdis/followers", "following_url": "https://api.github.com/users/psmaragdis/following{/other_user}", "gists_url": "https://api.github.com/users/psmaragdis/gists{/gist_id}", "starred_url": "https://api.github.com/users/psmaragdis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/psmaragdis/subscriptions", "organizations_url": "https://api.github.com/users/psmaragdis/orgs", "repos_url": "https://api.github.com/users/psmaragdis/repos", "events_url": "https://api.github.com/users/psmaragdis/events{/privacy}", "received_events_url": "https://api.github.com/users/psmaragdis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/pytorch/pytorch/milestones/2", "html_url": "https://github.com/pytorch/pytorch/milestone/2", "labels_url": "https://api.github.com/repos/pytorch/pytorch/milestones/2/labels", "id": 2536200, "node_id": "MDk6TWlsZXN0b25lMjUzNjIwMA==", "number": 2, "title": "v0.2", "description": "", "creator": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "open_issues": 2, "closed_issues": 34, "state": "closed", "created_at": "2017-05-22T18:19:28Z", "updated_at": "2018-08-06T21:16:06Z", "due_on": "2017-06-04T07:00:00Z", "closed_at": "2018-08-06T21:16:06Z"}, "comments": 1, "created_at": "2017-05-20T08:31:11Z", "updated_at": "2017-05-25T00:46:34Z", "closed_at": "2017-05-25T00:46:34Z", "author_association": "NONE", "body_html": "<p>I have a network defined as: <code>conv1d</code> -&gt; <code>linear</code> -&gt; <code>conv_transpose1d</code>. In this particular case, I want to keep the two conv layer weights fixed, and only adapt the middle layer weights.</p>\n<p>In the code below I have three ways of defining the conv layers parameters, one with buffers, one with parameters that have <code>requires_grad=False</code>, and one with regular parameters (which doesn't keep the weights fixed, but I'm using it as a reference). If I use the first two definitions (commented out in the code below), when I call <code>loss.backward()</code> it results in a kernel crash. If I use the third definition, it works fine albeit without maintaining fixed weights.</p>\n<p>In order to apply the linear layer I reshape the <code>conv1d</code> output and <code>conv_transpose1d</code> inputs, and I believe the crash stems from somewhere there (on a side note: Is this the right way to apply a linear layer on a time sequence?)</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n# Define the network's loss function\ndef myloss( out, targ):\n    return torch.mean( torch.pow( out-targ, 2.))\n\n# Define the network\nclass mynn( nn.Module):\n    def __init__( self):\n        super( mynn, self).__init__()\n\n        # Make fixed conv layer weights\n\n#         # This weights definition crashes\n#         self.register_buffer( 'l1', Variable( torch.randn( 8, 1, 8)))\n#         self.register_buffer( 'l2', Variable( torch.randn( 8, 1, 8)))\n\n#         # This one crashes too\n#         self.l1 = torch.nn.Parameter( torch.randn( 8, 1, 8), requires_grad=False)\n#         self.l2 = torch.nn.Parameter( torch.randn( 8, 1, 8), requires_grad=False)\n\n        # This one works, but isn't fixed-weight as desired\n        self.l1 = torch.nn.Parameter( torch.randn( 8, 1, 8))\n        self.l2 = torch.nn.Parameter( torch.randn( 8, 1, 8))\n\n        # Add a learnable layer just to have a parameter\n        self.l = nn.Linear( 8, 8)\n\n    def forward( self, x):\n        bt = x.size(0)\n        \n        # Makes a (bt x dims x time) output\n        x = F.conv1d( x, self.l1)\n\n        # Permute to (bt x time x dims) and view as (bt*time x dims) to go through an nn.Linear\n        x = x.permute( 0, 2, 1).contiguous().view( -1, x.size(1))\n        x = self.l( x)\n        \n        # Go back to (bt x dims x time) to pass through a conv layer\n        x = x.view( bt, -1, x.size(1)).permute( 0, 2, 1)\n        x = F.conv_transpose1d( x, self.l2)\n        return x\n\n# Make the network\nnet = mynn()\n\n# Do a forward pass\nx = Variable( torch.randn( 2, 1, 16))\nz = net.forward( x)\nloss = myloss( z, x)\n\n# Make it crash\nloss.backward()\n</code></pre>", "body_text": "I have a network defined as: conv1d -> linear -> conv_transpose1d. In this particular case, I want to keep the two conv layer weights fixed, and only adapt the middle layer weights.\nIn the code below I have three ways of defining the conv layers parameters, one with buffers, one with parameters that have requires_grad=False, and one with regular parameters (which doesn't keep the weights fixed, but I'm using it as a reference). If I use the first two definitions (commented out in the code below), when I call loss.backward() it results in a kernel crash. If I use the third definition, it works fine albeit without maintaining fixed weights.\nIn order to apply the linear layer I reshape the conv1d output and conv_transpose1d inputs, and I believe the crash stems from somewhere there (on a side note: Is this the right way to apply a linear layer on a time sequence?)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n# Define the network's loss function\ndef myloss( out, targ):\n    return torch.mean( torch.pow( out-targ, 2.))\n\n# Define the network\nclass mynn( nn.Module):\n    def __init__( self):\n        super( mynn, self).__init__()\n\n        # Make fixed conv layer weights\n\n#         # This weights definition crashes\n#         self.register_buffer( 'l1', Variable( torch.randn( 8, 1, 8)))\n#         self.register_buffer( 'l2', Variable( torch.randn( 8, 1, 8)))\n\n#         # This one crashes too\n#         self.l1 = torch.nn.Parameter( torch.randn( 8, 1, 8), requires_grad=False)\n#         self.l2 = torch.nn.Parameter( torch.randn( 8, 1, 8), requires_grad=False)\n\n        # This one works, but isn't fixed-weight as desired\n        self.l1 = torch.nn.Parameter( torch.randn( 8, 1, 8))\n        self.l2 = torch.nn.Parameter( torch.randn( 8, 1, 8))\n\n        # Add a learnable layer just to have a parameter\n        self.l = nn.Linear( 8, 8)\n\n    def forward( self, x):\n        bt = x.size(0)\n        \n        # Makes a (bt x dims x time) output\n        x = F.conv1d( x, self.l1)\n\n        # Permute to (bt x time x dims) and view as (bt*time x dims) to go through an nn.Linear\n        x = x.permute( 0, 2, 1).contiguous().view( -1, x.size(1))\n        x = self.l( x)\n        \n        # Go back to (bt x dims x time) to pass through a conv layer\n        x = x.view( bt, -1, x.size(1)).permute( 0, 2, 1)\n        x = F.conv_transpose1d( x, self.l2)\n        return x\n\n# Make the network\nnet = mynn()\n\n# Do a forward pass\nx = Variable( torch.randn( 2, 1, 16))\nz = net.forward( x)\nloss = myloss( z, x)\n\n# Make it crash\nloss.backward()", "body": "I have a network defined as: ```conv1d``` -> ```linear``` -> ```conv_transpose1d```. In this particular case, I want to keep the two conv layer weights fixed, and only adapt the middle layer weights. \r\n\r\nIn the code below I have three ways of defining the conv layers parameters, one with buffers, one with parameters that have ```requires_grad=False```, and one with regular parameters (which doesn't keep the weights fixed, but I'm using it as a reference). If I use the first two definitions (commented out in the code below), when I call ```loss.backward()``` it results in a kernel crash. If I use the third definition, it works fine albeit without maintaining fixed weights.\r\n\r\nIn order to apply the linear layer I reshape the ```conv1d``` output and ```conv_transpose1d``` inputs, and I believe the crash stems from somewhere there (on a side note: Is this the right way to apply a linear layer on a time sequence?)\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\n# Define the network's loss function\r\ndef myloss( out, targ):\r\n    return torch.mean( torch.pow( out-targ, 2.))\r\n\r\n# Define the network\r\nclass mynn( nn.Module):\r\n    def __init__( self):\r\n        super( mynn, self).__init__()\r\n\r\n        # Make fixed conv layer weights\r\n\r\n#         # This weights definition crashes\r\n#         self.register_buffer( 'l1', Variable( torch.randn( 8, 1, 8)))\r\n#         self.register_buffer( 'l2', Variable( torch.randn( 8, 1, 8)))\r\n\r\n#         # This one crashes too\r\n#         self.l1 = torch.nn.Parameter( torch.randn( 8, 1, 8), requires_grad=False)\r\n#         self.l2 = torch.nn.Parameter( torch.randn( 8, 1, 8), requires_grad=False)\r\n\r\n        # This one works, but isn't fixed-weight as desired\r\n        self.l1 = torch.nn.Parameter( torch.randn( 8, 1, 8))\r\n        self.l2 = torch.nn.Parameter( torch.randn( 8, 1, 8))\r\n\r\n        # Add a learnable layer just to have a parameter\r\n        self.l = nn.Linear( 8, 8)\r\n\r\n    def forward( self, x):\r\n        bt = x.size(0)\r\n        \r\n        # Makes a (bt x dims x time) output\r\n        x = F.conv1d( x, self.l1)\r\n\r\n        # Permute to (bt x time x dims) and view as (bt*time x dims) to go through an nn.Linear\r\n        x = x.permute( 0, 2, 1).contiguous().view( -1, x.size(1))\r\n        x = self.l( x)\r\n        \r\n        # Go back to (bt x dims x time) to pass through a conv layer\r\n        x = x.view( bt, -1, x.size(1)).permute( 0, 2, 1)\r\n        x = F.conv_transpose1d( x, self.l2)\r\n        return x\r\n\r\n# Make the network\r\nnet = mynn()\r\n\r\n# Do a forward pass\r\nx = Variable( torch.randn( 2, 1, 16))\r\nz = net.forward( x)\r\nloss = myloss( z, x)\r\n\r\n# Make it crash\r\nloss.backward()\r\n```\r\n"}