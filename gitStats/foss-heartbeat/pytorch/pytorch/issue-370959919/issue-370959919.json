{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12756", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12756/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12756/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12756/events", "html_url": "https://github.com/pytorch/pytorch/issues/12756", "id": 370959919, "node_id": "MDU6SXNzdWUzNzA5NTk5MTk=", "number": 12756, "title": "Autograd isn\u2019t functioning when networks\u2019s parameters are taken from other networks", "user": {"login": "JoyChopra1298", "id": 27682820, "node_id": "MDQ6VXNlcjI3NjgyODIw", "avatar_url": "https://avatars2.githubusercontent.com/u/27682820?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JoyChopra1298", "html_url": "https://github.com/JoyChopra1298", "followers_url": "https://api.github.com/users/JoyChopra1298/followers", "following_url": "https://api.github.com/users/JoyChopra1298/following{/other_user}", "gists_url": "https://api.github.com/users/JoyChopra1298/gists{/gist_id}", "starred_url": "https://api.github.com/users/JoyChopra1298/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JoyChopra1298/subscriptions", "organizations_url": "https://api.github.com/users/JoyChopra1298/orgs", "repos_url": "https://api.github.com/users/JoyChopra1298/repos", "events_url": "https://api.github.com/users/JoyChopra1298/events{/privacy}", "received_events_url": "https://api.github.com/users/JoyChopra1298/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-10-17T08:39:48Z", "updated_at": "2018-10-17T11:15:13Z", "closed_at": "2018-10-17T09:32:03Z", "author_association": "NONE", "body_html": "<p>I have 3 networks with same architecture - A, B &amp; C<br>\nThe weights of C are set as convex combination of weights of A &amp; B as shown below</p>\n<pre><code>for a_param, b_param, c_param in zip(a.parameters(), b.parameters(),\n                                               c.parameters()):\n    c_param.data = weight * a_param + (1 - weight) * b_param\n</code></pre>\n<p>On doing forward pass through C and using backward as shown below, gradients for A &amp; B aren\u2019t being calculated.</p>\n<pre><code>y = c.forward(x)\noptimiser.zero_grad()\ny.backward()\noptimiser.step()\n</code></pre>\n<p>Can anyone tell which step is blocking the gradient flow?</p>\n<p>EDIT -<br>\nSeems like parameters() is the problem; directly accessing the _modules and _parameters allows gradient calculation for A &amp; B</p>\n<pre><code>c._modules['linear_layers'][0]._parameters['weight'] = weight * a._modules['linear_layers'][0]._parameters['weight'] + (1-weight) * b._modules['linear_layers'][0]._parameters['weight']\n\n</code></pre>", "body_text": "I have 3 networks with same architecture - A, B & C\nThe weights of C are set as convex combination of weights of A & B as shown below\nfor a_param, b_param, c_param in zip(a.parameters(), b.parameters(),\n                                               c.parameters()):\n    c_param.data = weight * a_param + (1 - weight) * b_param\n\nOn doing forward pass through C and using backward as shown below, gradients for A & B aren\u2019t being calculated.\ny = c.forward(x)\noptimiser.zero_grad()\ny.backward()\noptimiser.step()\n\nCan anyone tell which step is blocking the gradient flow?\nEDIT -\nSeems like parameters() is the problem; directly accessing the _modules and _parameters allows gradient calculation for A & B\nc._modules['linear_layers'][0]._parameters['weight'] = weight * a._modules['linear_layers'][0]._parameters['weight'] + (1-weight) * b._modules['linear_layers'][0]._parameters['weight']", "body": "I have 3 networks with same architecture - A, B & C\r\nThe weights of C are set as convex combination of weights of A & B as shown below\r\n\r\n```\r\nfor a_param, b_param, c_param in zip(a.parameters(), b.parameters(),\r\n                                               c.parameters()):\r\n    c_param.data = weight * a_param + (1 - weight) * b_param\r\n```\r\nOn doing forward pass through C and using backward as shown below, gradients for A & B aren\u2019t being calculated.\r\n```\r\ny = c.forward(x)\r\noptimiser.zero_grad()\r\ny.backward()\r\noptimiser.step()\r\n```\r\nCan anyone tell which step is blocking the gradient flow?\r\n\r\nEDIT -\r\nSeems like parameters() is the problem; directly accessing the _modules and _parameters allows gradient calculation for A & B\r\n```\r\nc._modules['linear_layers'][0]._parameters['weight'] = weight * a._modules['linear_layers'][0]._parameters['weight'] + (1-weight) * b._modules['linear_layers'][0]._parameters['weight']\r\n\r\n```"}