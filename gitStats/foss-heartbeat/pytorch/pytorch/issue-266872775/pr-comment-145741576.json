{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145741576", "pull_request_review_id": 70586616, "id": 145741576, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTc0MTU3Ng==", "diff_hunk": "@@ -104,6 +107,8 @@ def _accumulate(iterable, fn=lambda x, y: x + y):\n def _flatten_tensors(tensors):\n     \"\"\"Flatten tensors into a single contiguous 1D buffer\"\"\"\n     if len(tensors) == 1:", "path": "torch/_utils.py", "position": null, "original_position": 15, "commit_id": "2dc9a550198bb9f07e51d3ad5d996aa2ee8f60f9", "original_commit_id": "e323933c5f24a6c53ed68f474c436f84fa37d257", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "This is so because `reduce_add` adds each chunk from `_take_tensor` together, which is not applicable to sparse tensors. So I didn't to the case when `len(tensors) != 1`. But upon thinking about it, I do think that it should be supported. It will be at least useful for `broadcast_coalesced`. I'll update this.", "created_at": "2017-10-19T15:45:59Z", "updated_at": "2018-11-23T15:35:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/3178#discussion_r145741576", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3178", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145741576"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3178#discussion_r145741576"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3178"}}, "body_html": "<p>This is so because <code>reduce_add</code> adds each chunk from <code>_take_tensor</code> together, which is not applicable to sparse tensors. So I didn't to the case when <code>len(tensors) != 1</code>. But upon thinking about it, I do think that it should be supported. It will be at least useful for <code>broadcast_coalesced</code>. I'll update this.</p>", "body_text": "This is so because reduce_add adds each chunk from _take_tensor together, which is not applicable to sparse tensors. So I didn't to the case when len(tensors) != 1. But upon thinking about it, I do think that it should be supported. It will be at least useful for broadcast_coalesced. I'll update this.", "in_reply_to_id": 145732384}