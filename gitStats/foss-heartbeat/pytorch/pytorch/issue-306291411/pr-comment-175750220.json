{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175750220", "pull_request_review_id": 105329078, "id": 175750220, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTc1MDIyMA==", "diff_hunk": "@@ -780,6 +780,20 @@ def test_remainder(self):\n         long_res1 = long_m1.clone()\n         long_res1.remainder_(long_qs.unsqueeze(0).expand_as(long_res1))\n \n+    def test_cross_type_binary_operators(self):\n+        x = torch.randn(2, 2, dtype=torch.float)\n+        y = torch.tensor(2., dtype=torch.double)\n+\n+        operators = [\n+            operator.add, operator.sub, operator.mul, operator.truediv,\n+            operator.pow, operator.mod, operator.lt, operator.le, operator.gt,\n+            operator.ge, operator.eq, operator.ne\n+        ]\n+\n+        for arg1, arg2, arg_same_type in [(x, y, y.float()), (y, x, x.double())]:", "path": "test/test_torch.py", "position": 14, "original_position": 14, "commit_id": "5163c7781bcf0893776eb97f7c851040228c101c", "original_commit_id": "24b7c7dfbedfeb2d8cf3e57fc6945c15c9479a55", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "Sorry for not being clear; I meant all the case statements that are in unifyTypes; at a minimum you are missing:\r\n```\r\nif (isFloatingType(scalar_tensor.type().scalarType()) && isIntegralType(nonscalar_tensor.type().scalarType())) {\r\n```\r\nthis might be the only one; I didn't study it that closely.", "created_at": "2018-03-20T12:42:42Z", "updated_at": "2018-11-23T15:40:58Z", "html_url": "https://github.com/pytorch/pytorch/pull/5864#discussion_r175750220", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5864", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175750220"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5864#discussion_r175750220"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5864"}}, "body_html": "<p>Sorry for not being clear; I meant all the case statements that are in unifyTypes; at a minimum you are missing:</p>\n<pre><code>if (isFloatingType(scalar_tensor.type().scalarType()) &amp;&amp; isIntegralType(nonscalar_tensor.type().scalarType())) {\n</code></pre>\n<p>this might be the only one; I didn't study it that closely.</p>", "body_text": "Sorry for not being clear; I meant all the case statements that are in unifyTypes; at a minimum you are missing:\nif (isFloatingType(scalar_tensor.type().scalarType()) && isIntegralType(nonscalar_tensor.type().scalarType())) {\n\nthis might be the only one; I didn't study it that closely.", "in_reply_to_id": 175519434}