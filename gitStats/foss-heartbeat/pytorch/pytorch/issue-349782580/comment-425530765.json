{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/425530765", "html_url": "https://github.com/pytorch/pytorch/issues/10446#issuecomment-425530765", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10446", "id": 425530765, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTUzMDc2NQ==", "user": {"login": "thedch", "id": 13357734, "node_id": "MDQ6VXNlcjEzMzU3NzM0", "avatar_url": "https://avatars3.githubusercontent.com/u/13357734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thedch", "html_url": "https://github.com/thedch", "followers_url": "https://api.github.com/users/thedch/followers", "following_url": "https://api.github.com/users/thedch/following{/other_user}", "gists_url": "https://api.github.com/users/thedch/gists{/gist_id}", "starred_url": "https://api.github.com/users/thedch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thedch/subscriptions", "organizations_url": "https://api.github.com/users/thedch/orgs", "repos_url": "https://api.github.com/users/thedch/repos", "events_url": "https://api.github.com/users/thedch/events{/privacy}", "received_events_url": "https://api.github.com/users/thedch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-28T18:46:31Z", "updated_at": "2018-09-28T18:46:31Z", "author_association": "NONE", "body_html": "<p>Here's a code snippet that might shed some light on the issue --</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> onnx\n<span class=\"pl-k\">from</span> onnx <span class=\"pl-k\">import</span> shape_inference, utils\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">SimpleNet</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> F.interpolate(x, (<span class=\"pl-c1\">100</span>,<span class=\"pl-c1\">100</span>), <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bilinear<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">align_corners</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> x = F.relu(x)</span>\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv1(x)\n        <span class=\"pl-k\">return</span> x\n\nfilename <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>basic.onnx<span class=\"pl-pds\">'</span></span>\nmodel <span class=\"pl-k\">=</span> SimpleNet()\ninputs <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>)\ntorch.onnx.export(model, inputs, filename)\n\ngraph <span class=\"pl-k\">=</span> onnx.load(filename)\ngraph <span class=\"pl-k\">=</span> shape_inference.infer_shapes(graph)\n<span class=\"pl-c1\">print</span>(graph.graph)</pre></div>\n<p>At the bottom of the output is a list of tensors in the model:</p>\n<pre><code>input {\n  name: \"0\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 1\n        }\n        dim {\n          dim_value: 3\n        }\n        dim {\n          dim_value: 224\n        }\n        dim {\n          dim_value: 224\n        }\n      }\n    }\n  }\n}\ninput {\n  name: \"1\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 10\n        }\n        dim {\n          dim_value: 3\n        }\n        dim {\n          dim_value: 1\n        }\n        dim {\n          dim_value: 1\n        }\n      }\n    }\n  }\n}\ninput {\n  name: \"2\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 10\n        }\n      }\n    }\n  }\n}\noutput {\n  name: \"4\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 1\n        }\n        dim {\n          dim_value: 10\n        }\n        dim {\n          dim_value: 100\n        }\n        dim {\n          dim_value: 100\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<p>Notice there is no tensor <code>3</code>, which is the output of the interpolate op:</p>\n<pre><code>node {\n  input: \"0\"\n  output: \"3\"\n  op_type: \"Upsample\"\n...\n</code></pre>\n<p>and the input of the conv op:</p>\n<pre><code>node {\n  input: \"3\"\n  input: \"1\"\n  input: \"2\"\n  output: \"4\"\n  op_type: \"Conv\"\n...\n</code></pre>\n<p>However, if I uncomment the <code>F.relu</code> and comment the <code>F.interpolate</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> x = F.interpolate(x, (100,100), mode='bilinear', align_corners=False)</span>\n        x <span class=\"pl-k\">=</span> F.relu(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv1(x)\n        <span class=\"pl-k\">return</span> x</pre></div>\n<p>and rerun the code, the very bottom of the output now has a <code>value_info</code> section, which contains the needed tensor <code>3</code>.</p>\n<pre><code>value_info {\n  name: \"3\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 1\n        }\n        dim {\n          dim_value: 3\n        }\n        dim {\n          dim_value: 224\n        }\n        dim {\n          dim_value: 224\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<p>which lines up with the relu node (notice output = 3):</p>\n<pre><code>node {\n  input: \"0\"\n  output: \"3\"\n  op_type: \"Relu\"\n</code></pre>\n<p>and the conv node (notice input = 3):</p>\n<pre><code>node {\n  input: \"3\"\n  input: \"1\"\n  input: \"2\"\n  output: \"4\"\n  op_type: \"Conv\"\n</code></pre>\n<p>So it seems like the F.interpolate operation is not properly being saved in the trace export? Let me know if I am missing a command or something that handles this behavior.</p>", "body_text": "Here's a code snippet that might shed some light on the issue --\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport onnx\nfrom onnx import shape_inference, utils\n\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=1)\n\n    def forward(self, x):\n        x = F.interpolate(x, (100,100), mode='bilinear', align_corners=False)\n        # x = F.relu(x)\n        x = self.conv1(x)\n        return x\n\nfilename = 'basic.onnx'\nmodel = SimpleNet()\ninputs = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(model, inputs, filename)\n\ngraph = onnx.load(filename)\ngraph = shape_inference.infer_shapes(graph)\nprint(graph.graph)\nAt the bottom of the output is a list of tensors in the model:\ninput {\n  name: \"0\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 1\n        }\n        dim {\n          dim_value: 3\n        }\n        dim {\n          dim_value: 224\n        }\n        dim {\n          dim_value: 224\n        }\n      }\n    }\n  }\n}\ninput {\n  name: \"1\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 10\n        }\n        dim {\n          dim_value: 3\n        }\n        dim {\n          dim_value: 1\n        }\n        dim {\n          dim_value: 1\n        }\n      }\n    }\n  }\n}\ninput {\n  name: \"2\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 10\n        }\n      }\n    }\n  }\n}\noutput {\n  name: \"4\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 1\n        }\n        dim {\n          dim_value: 10\n        }\n        dim {\n          dim_value: 100\n        }\n        dim {\n          dim_value: 100\n        }\n      }\n    }\n  }\n}\n\nNotice there is no tensor 3, which is the output of the interpolate op:\nnode {\n  input: \"0\"\n  output: \"3\"\n  op_type: \"Upsample\"\n...\n\nand the input of the conv op:\nnode {\n  input: \"3\"\n  input: \"1\"\n  input: \"2\"\n  output: \"4\"\n  op_type: \"Conv\"\n...\n\nHowever, if I uncomment the F.relu and comment the F.interpolate:\n    def forward(self, x):\n        # x = F.interpolate(x, (100,100), mode='bilinear', align_corners=False)\n        x = F.relu(x)\n        x = self.conv1(x)\n        return x\nand rerun the code, the very bottom of the output now has a value_info section, which contains the needed tensor 3.\nvalue_info {\n  name: \"3\"\n  type {\n    tensor_type {\n      elem_type: FLOAT\n      shape {\n        dim {\n          dim_value: 1\n        }\n        dim {\n          dim_value: 3\n        }\n        dim {\n          dim_value: 224\n        }\n        dim {\n          dim_value: 224\n        }\n      }\n    }\n  }\n}\n\nwhich lines up with the relu node (notice output = 3):\nnode {\n  input: \"0\"\n  output: \"3\"\n  op_type: \"Relu\"\n\nand the conv node (notice input = 3):\nnode {\n  input: \"3\"\n  input: \"1\"\n  input: \"2\"\n  output: \"4\"\n  op_type: \"Conv\"\n\nSo it seems like the F.interpolate operation is not properly being saved in the trace export? Let me know if I am missing a command or something that handles this behavior.", "body": "Here's a code snippet that might shed some light on the issue -- \r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport onnx\r\nfrom onnx import shape_inference, utils\r\n\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=1)\r\n\r\n    def forward(self, x):\r\n        x = F.interpolate(x, (100,100), mode='bilinear', align_corners=False)\r\n        # x = F.relu(x)\r\n        x = self.conv1(x)\r\n        return x\r\n\r\nfilename = 'basic.onnx'\r\nmodel = SimpleNet()\r\ninputs = torch.randn(1, 3, 224, 224)\r\ntorch.onnx.export(model, inputs, filename)\r\n\r\ngraph = onnx.load(filename)\r\ngraph = shape_inference.infer_shapes(graph)\r\nprint(graph.graph)\r\n```\r\n\r\nAt the bottom of the output is a list of tensors in the model:\r\n\r\n```\r\ninput {\r\n  name: \"0\"\r\n  type {\r\n    tensor_type {\r\n      elem_type: FLOAT\r\n      shape {\r\n        dim {\r\n          dim_value: 1\r\n        }\r\n        dim {\r\n          dim_value: 3\r\n        }\r\n        dim {\r\n          dim_value: 224\r\n        }\r\n        dim {\r\n          dim_value: 224\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\ninput {\r\n  name: \"1\"\r\n  type {\r\n    tensor_type {\r\n      elem_type: FLOAT\r\n      shape {\r\n        dim {\r\n          dim_value: 10\r\n        }\r\n        dim {\r\n          dim_value: 3\r\n        }\r\n        dim {\r\n          dim_value: 1\r\n        }\r\n        dim {\r\n          dim_value: 1\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\ninput {\r\n  name: \"2\"\r\n  type {\r\n    tensor_type {\r\n      elem_type: FLOAT\r\n      shape {\r\n        dim {\r\n          dim_value: 10\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\noutput {\r\n  name: \"4\"\r\n  type {\r\n    tensor_type {\r\n      elem_type: FLOAT\r\n      shape {\r\n        dim {\r\n          dim_value: 1\r\n        }\r\n        dim {\r\n          dim_value: 10\r\n        }\r\n        dim {\r\n          dim_value: 100\r\n        }\r\n        dim {\r\n          dim_value: 100\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nNotice there is no tensor `3`, which is the output of the interpolate op:\r\n\r\n```\r\nnode {\r\n  input: \"0\"\r\n  output: \"3\"\r\n  op_type: \"Upsample\"\r\n...\r\n```\r\n\r\nand the input of the conv op:\r\n\r\n```\r\nnode {\r\n  input: \"3\"\r\n  input: \"1\"\r\n  input: \"2\"\r\n  output: \"4\"\r\n  op_type: \"Conv\"\r\n...\r\n```\r\n\r\nHowever, if I uncomment the `F.relu` and comment the `F.interpolate`: \r\n\r\n```python\r\n    def forward(self, x):\r\n        # x = F.interpolate(x, (100,100), mode='bilinear', align_corners=False)\r\n        x = F.relu(x)\r\n        x = self.conv1(x)\r\n        return x\r\n```\r\n\r\nand rerun the code, the very bottom of the output now has a `value_info` section, which contains the needed tensor `3`. \r\n\r\n```\r\nvalue_info {\r\n  name: \"3\"\r\n  type {\r\n    tensor_type {\r\n      elem_type: FLOAT\r\n      shape {\r\n        dim {\r\n          dim_value: 1\r\n        }\r\n        dim {\r\n          dim_value: 3\r\n        }\r\n        dim {\r\n          dim_value: 224\r\n        }\r\n        dim {\r\n          dim_value: 224\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nwhich lines up with the relu node (notice output = 3): \r\n\r\n```\r\nnode {\r\n  input: \"0\"\r\n  output: \"3\"\r\n  op_type: \"Relu\"\r\n```\r\n\r\nand the conv node (notice input = 3):\r\n\r\n```\r\nnode {\r\n  input: \"3\"\r\n  input: \"1\"\r\n  input: \"2\"\r\n  output: \"4\"\r\n  op_type: \"Conv\"\r\n```\r\n\r\nSo it seems like the F.interpolate operation is not properly being saved in the trace export? Let me know if I am missing a command or something that handles this behavior. "}