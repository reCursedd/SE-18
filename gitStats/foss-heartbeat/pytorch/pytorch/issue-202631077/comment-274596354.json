{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/274596354", "html_url": "https://github.com/pytorch/pytorch/issues/560#issuecomment-274596354", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/560", "id": 274596354, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDU5NjM1NA==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-23T19:47:12Z", "updated_at": "2017-01-23T19:47:27Z", "author_association": "MEMBER", "body_html": "<p>You're looking at the gradient of an intermediate computation which are not stored. To fix this, move the <code>.cuda()</code> inside the Variable()</p>\n<div class=\"highlight highlight-source-python\"><pre>xx <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>).cuda(), <span class=\"pl-v\">requires_grad</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\nyy <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span><span class=\"pl-k\">*</span>xx\nzz <span class=\"pl-k\">=</span> yy<span class=\"pl-k\">**</span><span class=\"pl-c1\">2</span>\nzz.backward()\n<span class=\"pl-c1\">print</span>(xx.grad)</pre></div>\n<p>What you originally wrote, does the <code>.cuda()</code> call <strong>on</strong> the Variable which produces a new Variable. It's equivalent to:</p>\n<div class=\"highlight highlight-source-python\"><pre>xx <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> will have .grad stored</span>\nxx_cuda <span class=\"pl-k\">=</span> xx.cuda()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> will not have .grad stored</span>\nyy <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span><span class=\"pl-k\">*</span>xx_cuda\nzz <span class=\"pl-k\">=</span> yy<span class=\"pl-k\">**</span><span class=\"pl-c1\">2</span>\nzz.backward()\n<span class=\"pl-c1\">print</span>(xx_cuda.grad)</pre></div>", "body_text": "You're looking at the gradient of an intermediate computation which are not stored. To fix this, move the .cuda() inside the Variable()\nxx = Variable(torch.randn(1,1).cuda(), requires_grad = True)\nyy = 3*xx\nzz = yy**2\nzz.backward()\nprint(xx.grad)\nWhat you originally wrote, does the .cuda() call on the Variable which produces a new Variable. It's equivalent to:\nxx = Variable(torch.randn(1,1), requires_grad = True)  # will have .grad stored\nxx_cuda = xx.cuda()  # will not have .grad stored\nyy = 3*xx_cuda\nzz = yy**2\nzz.backward()\nprint(xx_cuda.grad)", "body": "You're looking at the gradient of an intermediate computation which are not stored. To fix this, move the `.cuda()` inside the Variable()\r\n\r\n```python\r\nxx = Variable(torch.randn(1,1).cuda(), requires_grad = True)\r\nyy = 3*xx\r\nzz = yy**2\r\nzz.backward()\r\nprint(xx.grad)\r\n```\r\n\r\nWhat you originally wrote, does the `.cuda()` call **on** the Variable which produces a new Variable. It's equivalent to:\r\n\r\n\r\n```python\r\nxx = Variable(torch.randn(1,1), requires_grad = True)  # will have .grad stored\r\nxx_cuda = xx.cuda()  # will not have .grad stored\r\nyy = 3*xx_cuda\r\nzz = yy**2\r\nzz.backward()\r\nprint(xx_cuda.grad)\r\n```"}