{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5330", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5330/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5330/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5330/events", "html_url": "https://github.com/pytorch/pytorch/issues/5330", "id": 298973294, "node_id": "MDU6SXNzdWUyOTg5NzMyOTQ=", "number": 5330, "title": "Incorrect Traced Graph for Softmax", "user": {"login": "chughtapan", "id": 3305567, "node_id": "MDQ6VXNlcjMzMDU1Njc=", "avatar_url": "https://avatars3.githubusercontent.com/u/3305567?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chughtapan", "html_url": "https://github.com/chughtapan", "followers_url": "https://api.github.com/users/chughtapan/followers", "following_url": "https://api.github.com/users/chughtapan/following{/other_user}", "gists_url": "https://api.github.com/users/chughtapan/gists{/gist_id}", "starred_url": "https://api.github.com/users/chughtapan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chughtapan/subscriptions", "organizations_url": "https://api.github.com/users/chughtapan/orgs", "repos_url": "https://api.github.com/users/chughtapan/repos", "events_url": "https://api.github.com/users/chughtapan/events{/privacy}", "received_events_url": "https://api.github.com/users/chughtapan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-21T13:39:15Z", "updated_at": "2018-02-21T14:53:07Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,<br>\nConsider the following example -</p>\n<pre><code>class TestSoftMax(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(TestSoftMax, self).__init__()\n        self.W = nn.Linear(input_size, hidden_size)\n\n    def forward(self, input):\n        return F.softmax(self.W(input))\n</code></pre>\n<p>On tracing, the graph generated is</p>\n<pre><code>graph(%0 : Float(8, 1024)\n      %1 : Float(1024, 1024)\n      %2 : Float(1024)\n      -------- stage 1 --------\n      %7 : Float(8, 1024)) {\n  %3 : Float(1024!, 1024!) = t(%1)\n  %4 : Float(8!, 1024) = expand[size=[8, 1024]](%2)\n  %5 : Float(8, 1024) = addmm[beta={1}, alpha={1}](%4, %0, %3)\n  %6 : Float(8, 1024) = softmax[dim=1](%5)\n  ---------------- stage 1 ----------------\n  %8 : Float(8, 1024) = Constant[value=&lt;Tensor&gt;]()\n  %9 : Float(8, 1024) = softmax_backward[dim=1](%7, %5, %8)\n  %10 : Float(1024!, 8!) = t(%9)\n  %11 : Float(1024, 1024) = mm(%10, %0)\n  %13 : Float(1024) = sum[dim=0, keepdim=0](%9)\n  return (%6, %11, %13);\n}\n</code></pre>\n<p>The correct inputs for <code>%9</code>  should be <code>%7,%5,%6</code></p>\n<p>Thanks</p>", "body_text": "Hi,\nConsider the following example -\nclass TestSoftMax(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(TestSoftMax, self).__init__()\n        self.W = nn.Linear(input_size, hidden_size)\n\n    def forward(self, input):\n        return F.softmax(self.W(input))\n\nOn tracing, the graph generated is\ngraph(%0 : Float(8, 1024)\n      %1 : Float(1024, 1024)\n      %2 : Float(1024)\n      -------- stage 1 --------\n      %7 : Float(8, 1024)) {\n  %3 : Float(1024!, 1024!) = t(%1)\n  %4 : Float(8!, 1024) = expand[size=[8, 1024]](%2)\n  %5 : Float(8, 1024) = addmm[beta={1}, alpha={1}](%4, %0, %3)\n  %6 : Float(8, 1024) = softmax[dim=1](%5)\n  ---------------- stage 1 ----------------\n  %8 : Float(8, 1024) = Constant[value=<Tensor>]()\n  %9 : Float(8, 1024) = softmax_backward[dim=1](%7, %5, %8)\n  %10 : Float(1024!, 8!) = t(%9)\n  %11 : Float(1024, 1024) = mm(%10, %0)\n  %13 : Float(1024) = sum[dim=0, keepdim=0](%9)\n  return (%6, %11, %13);\n}\n\nThe correct inputs for %9  should be %7,%5,%6\nThanks", "body": "Hi,\r\nConsider the following example - \r\n```\r\nclass TestSoftMax(nn.Module):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(TestSoftMax, self).__init__()\r\n        self.W = nn.Linear(input_size, hidden_size)\r\n\r\n    def forward(self, input):\r\n        return F.softmax(self.W(input))\r\n```\r\nOn tracing, the graph generated is \r\n\r\n```\r\ngraph(%0 : Float(8, 1024)\r\n      %1 : Float(1024, 1024)\r\n      %2 : Float(1024)\r\n      -------- stage 1 --------\r\n      %7 : Float(8, 1024)) {\r\n  %3 : Float(1024!, 1024!) = t(%1)\r\n  %4 : Float(8!, 1024) = expand[size=[8, 1024]](%2)\r\n  %5 : Float(8, 1024) = addmm[beta={1}, alpha={1}](%4, %0, %3)\r\n  %6 : Float(8, 1024) = softmax[dim=1](%5)\r\n  ---------------- stage 1 ----------------\r\n  %8 : Float(8, 1024) = Constant[value=<Tensor>]()\r\n  %9 : Float(8, 1024) = softmax_backward[dim=1](%7, %5, %8)\r\n  %10 : Float(1024!, 8!) = t(%9)\r\n  %11 : Float(1024, 1024) = mm(%10, %0)\r\n  %13 : Float(1024) = sum[dim=0, keepdim=0](%9)\r\n  return (%6, %11, %13);\r\n}\r\n```\r\nThe correct inputs for `%9`  should be `%7,%5,%6`\r\n\r\nThanks"}