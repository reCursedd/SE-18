{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5515", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5515/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5515/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5515/events", "html_url": "https://github.com/pytorch/pytorch/issues/5515", "id": 301650998, "node_id": "MDU6SXNzdWUzMDE2NTA5OTg=", "number": 5515, "title": "[feature request] PreComputation of a layer's output ", "user": {"login": "50417", "id": 9717825, "node_id": "MDQ6VXNlcjk3MTc4MjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/9717825?v=4", "gravatar_id": "", "url": "https://api.github.com/users/50417", "html_url": "https://github.com/50417", "followers_url": "https://api.github.com/users/50417/followers", "following_url": "https://api.github.com/users/50417/following{/other_user}", "gists_url": "https://api.github.com/users/50417/gists{/gist_id}", "starred_url": "https://api.github.com/users/50417/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/50417/subscriptions", "organizations_url": "https://api.github.com/users/50417/orgs", "repos_url": "https://api.github.com/users/50417/repos", "events_url": "https://api.github.com/users/50417/events{/privacy}", "received_events_url": "https://api.github.com/users/50417/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-03-02T04:37:56Z", "updated_at": "2018-03-02T13:02:15Z", "closed_at": "2018-03-02T08:02:07Z", "author_association": "NONE", "body_html": "<p>Just like Tensor.detach_() function prevents from backward propagation and weight update of the layer. A function that would precompute the layer's output when it has been detached making the neural net smaller for testing would be beneficial for research work even if it will blow up memory when tensor are huge.</p>", "body_text": "Just like Tensor.detach_() function prevents from backward propagation and weight update of the layer. A function that would precompute the layer's output when it has been detached making the neural net smaller for testing would be beneficial for research work even if it will blow up memory when tensor are huge.", "body": "Just like Tensor.detach_() function prevents from backward propagation and weight update of the layer. A function that would precompute the layer's output when it has been detached making the neural net smaller for testing would be beneficial for research work even if it will blow up memory when tensor are huge. "}