{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/369824144", "html_url": "https://github.com/pytorch/pytorch/issues/5515#issuecomment-369824144", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5515", "id": 369824144, "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTgyNDE0NA==", "user": {"login": "50417", "id": 9717825, "node_id": "MDQ6VXNlcjk3MTc4MjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/9717825?v=4", "gravatar_id": "", "url": "https://api.github.com/users/50417", "html_url": "https://github.com/50417", "followers_url": "https://api.github.com/users/50417/followers", "following_url": "https://api.github.com/users/50417/following{/other_user}", "gists_url": "https://api.github.com/users/50417/gists{/gist_id}", "starred_url": "https://api.github.com/users/50417/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/50417/subscriptions", "organizations_url": "https://api.github.com/users/50417/orgs", "repos_url": "https://api.github.com/users/50417/repos", "events_url": "https://api.github.com/users/50417/events{/privacy}", "received_events_url": "https://api.github.com/users/50417/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-02T05:25:51Z", "updated_at": "2018-03-02T12:59:13Z", "author_association": "NONE", "body_html": "<p>Lets say I am training a CNN model</p>\n<p>While training the model, if early layer of the net converges, i detach it from backprop saving computation. Since any input through that detached layer will essentially produce same output no matter the number of future epochs, its redundant.  I am requesting for a feature such that such layer's output is precomputed and fed directly to the consecutive layers rather than computing each time.</p>", "body_text": "Lets say I am training a CNN model\nWhile training the model, if early layer of the net converges, i detach it from backprop saving computation. Since any input through that detached layer will essentially produce same output no matter the number of future epochs, its redundant.  I am requesting for a feature such that such layer's output is precomputed and fed directly to the consecutive layers rather than computing each time.", "body": "Lets say I am training a CNN model \r\n\r\nWhile training the model, if early layer of the net converges, i detach it from backprop saving computation. Since any input through that detached layer will essentially produce same output no matter the number of future epochs, its redundant.  I am requesting for a feature such that such layer's output is precomputed and fed directly to the consecutive layers rather than computing each time. "}