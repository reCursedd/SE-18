{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/349739618", "html_url": "https://github.com/pytorch/pytorch/issues/3898#issuecomment-349739618", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3898", "id": 349739618, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTczOTYxOA==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-06T18:58:05Z", "updated_at": "2018-01-05T15:42:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm taking the following approach to make sparse tensors work with dataloader: (I'm using this comment as a task tracker. Tasks that come earlier in the list may depend on later tasks):</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Make sparse tensors with with multiprocessing</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Add the ability to batch tensors in dataloader's default collate function</li>\n</ul>\n<p>Batching sparse tensors can be done by implementing a sparse torch.stack, which can be implemented with the following:</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Implement <code>stack</code> for sparse tensors\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Implement <code>unsqueeze</code> for sparse tensors</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Implement <code>cat</code> for sparse tensors</li>\n</ul>\n</li>\n</ul>\n<p>Because <code>unsqueeze</code> is an aten native function for dense tensors, I'd like to write an aten native function version of it for sparse tensors. That can be done by adding better support for sparse tensors to aten.</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Implement <code>_indices</code> and <code>_values</code> for sparse tensors in aten by exposing THS's functions</li>\n</ul>\n<p>Haven't decided where to implement cat / stack, will figure that out at some point. They're currently not aten native functions for dense tensors so I might put them in THS.</p>", "body_text": "I'm taking the following approach to make sparse tensors work with dataloader: (I'm using this comment as a task tracker. Tasks that come earlier in the list may depend on later tasks):\n\n Make sparse tensors with with multiprocessing\n Add the ability to batch tensors in dataloader's default collate function\n\nBatching sparse tensors can be done by implementing a sparse torch.stack, which can be implemented with the following:\n\n Implement stack for sparse tensors\n\n Implement unsqueeze for sparse tensors\n Implement cat for sparse tensors\n\n\n\nBecause unsqueeze is an aten native function for dense tensors, I'd like to write an aten native function version of it for sparse tensors. That can be done by adding better support for sparse tensors to aten.\n\n Implement _indices and _values for sparse tensors in aten by exposing THS's functions\n\nHaven't decided where to implement cat / stack, will figure that out at some point. They're currently not aten native functions for dense tensors so I might put them in THS.", "body": "I'm taking the following approach to make sparse tensors work with dataloader: (I'm using this comment as a task tracker. Tasks that come earlier in the list may depend on later tasks):\r\n\r\n- [ ] Make sparse tensors with with multiprocessing\r\n- [ ] Add the ability to batch tensors in dataloader's default collate function\r\n\r\nBatching sparse tensors can be done by implementing a sparse torch.stack, which can be implemented with the following:\r\n- [ ] Implement `stack` for sparse tensors\r\n  - [ ] Implement `unsqueeze` for sparse tensors\r\n  - [ ] Implement `cat` for sparse tensors\r\n\r\nBecause `unsqueeze` is an aten native function for dense tensors, I'd like to write an aten native function version of it for sparse tensors. That can be done by adding better support for sparse tensors to aten.\r\n- [x] Implement `_indices` and `_values` for sparse tensors in aten by exposing THS's functions\r\n\r\nHaven't decided where to implement cat / stack, will figure that out at some point. They're currently not aten native functions for dense tensors so I might put them in THS."}