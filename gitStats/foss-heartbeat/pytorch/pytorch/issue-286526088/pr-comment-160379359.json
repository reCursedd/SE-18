{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/160379359", "pull_request_review_id": 87482979, "id": 160379359, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MDM3OTM1OQ==", "diff_hunk": "@@ -425,6 +425,167 @@ def tmp(self):\n \n \n class TestCuda(TestCase):\n+\n+    @staticmethod\n+    def _test_memory_stats_generator(self, device=None, N=35):\n+        if device is None:\n+            device = torch.cuda.current_device()\n+\n+        m0 = torch.cuda.memory_allocated(device)\n+        last_m_arr = [torch.cuda.memory_allocated(device)]\n+        max_m_arr = [torch.cuda.max_memory_allocated(device)]\n+        last_c_arr = [torch.cuda.memory_cached(device)]\n+        max_c_arr = [torch.cuda.max_memory_cached(device)]\n+\n+        def alloc(*size):\n+            with torch.cuda.device(device):\n+                # NOTE: do **not** use methods that will have additional\n+                #       overhead, e.g., inplace random sampling methods.\n+                return torch.cuda.FloatTensor(*size)\n+\n+        def assert_change(comp=1, empty_cache=False):\n+            # comp > 0: increased\n+            # comp = 0: equal\n+            # comp < 0: decreased\n+            new_m = torch.cuda.memory_allocated(device)\n+            new_max_m = torch.cuda.max_memory_allocated(device)\n+            if comp > 0:\n+                self.assertGreater(new_m, last_m_arr[0])\n+            elif comp < 0:\n+                self.assertLess(new_m, last_m_arr[0])\n+            else:\n+                self.assertEqual(new_m, last_m_arr[0])\n+            self.assertLessEqual(new_m, new_max_m)\n+            self.assertGreaterEqual(new_max_m, max_m_arr[0])\n+            last_m_arr[0] = new_m\n+            max_m_arr[0] = new_max_m\n+\n+            new_c = torch.cuda.memory_cached(device)\n+            new_max_c = torch.cuda.max_memory_cached(device)\n+            # emptying cache may happen (due to allocation or empty_cache), so\n+            # we can't assert new_c >= last_c\n+            self.assertLessEqual(new_c, new_max_c)\n+            self.assertGreaterEqual(new_max_c, max_c_arr[0])\n+            last_c_arr[0] = new_c\n+            max_c_arr[0] = new_max_c\n+\n+            if empty_cache:\n+                torch.cuda.empty_cache()\n+                new_c = torch.cuda.memory_cached(device)\n+                new_max_c = torch.cuda.max_memory_cached(device)\n+                self.assertLessEqual(new_c, last_c_arr[0])\n+                self.assertLessEqual(new_c, new_max_c)\n+                self.assertEqual(new_max_c, max_c_arr[0])\n+                last_c_arr[0] = new_c\n+\n+        assert_change(0)\n+        assert_change(0)\n+        yield\n+\n+        tensors1 = [alloc(1), alloc(10, 20), alloc(200, 300, 2000)]\n+        m1 = torch.cuda.memory_allocated(device)\n+        assert_change(1)\n+        yield\n+\n+        tensors2 = []\n+\n+        for i in range(1, int(N / 2) + 1):\n+            # small ones\n+            tensors2.append(alloc(i, i * 4))\n+            assert_change(1)\n+            yield\n+\n+        for i in range(5, int(N / 2) + 5):\n+            # large ones\n+            tensors2.append(alloc(i, i * 7, i * 9, i * 11))\n+            assert_change(1)\n+            yield\n+\n+        tensors2.append(alloc(0, 0, 0))\n+        assert_change(0)\n+        yield\n+\n+        permute = []\n+        for i in torch.randperm(len(tensors2)):\n+            permute.append(tensors2[i])\n+            assert_change(0)\n+            yield\n+\n+        del tensors2\n+        assert_change(0)\n+        yield\n+        tensors2 = permute\n+        assert_change(0)\n+        yield\n+        del permute\n+        assert_change(0)\n+        yield\n+\n+        for i in range(int(N / 2)):\n+            x = tensors2[i].numel()\n+            del tensors2[i]\n+            assert_change(-x)  # in case that tensors2[i] is empty\n+            yield\n+\n+        for i in range(2, int(2 * N / 3) + 2):\n+            tensors2.append(alloc(i, i * 3, i * 8))\n+            assert_change(1)\n+            yield\n+\n+        del tensors2\n+        assert_change(-1)\n+        assert_change(0)\n+        self.assertEqual(torch.cuda.memory_allocated(device), m1)\n+        yield True\n+\n+        del tensors1\n+        assert_change(-1)\n+        self.assertEqual(torch.cuda.memory_allocated(device), m0)\n+\n+        assert_change(0, empty_cache=True)", "path": "test/test_cuda.py", "position": null, "original_position": 121, "commit_id": "0e87fbde69b3efa2925109ac1ba224daf338f7b7", "original_commit_id": "052261986ed4336bc96cf64392165224dee313e4", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Why do you empty cache here? Just to test it?", "created_at": "2018-01-09T11:16:09Z", "updated_at": "2018-11-23T15:37:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/4511#discussion_r160379359", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4511", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/160379359"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4511#discussion_r160379359"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4511"}}, "body_html": "<p>Why do you empty cache here? Just to test it?</p>", "body_text": "Why do you empty cache here? Just to test it?"}