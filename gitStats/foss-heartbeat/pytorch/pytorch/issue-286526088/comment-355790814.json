{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355790814", "html_url": "https://github.com/pytorch/pytorch/pull/4511#issuecomment-355790814", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4511", "id": 355790814, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTc5MDgxNA==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-07T00:59:18Z", "updated_at": "2018-01-07T00:59:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I noticed that you added stats for the \"residency\" (actual amount of live data), but not memory that was actually allocated from the GPU. The latter is also a pretty useful stat to collect, since (1) it is the thing that end users will actually see when the query nvidia-smi, and (2) it is the thing that will determine if you are actually out of memory or not is the actual memory allocated. Fragmentation could mean we are wasting memory in the caching allocator, that doesn't show up in the residency computation. (Though, tensor allocations tend to be big, so this isn't a big deal in practice...)</p>\n<p>There's tons of other stats we could add to the allocator but I guess no one needed them, so we ain't gonna add 'em :)</p>", "body_text": "I noticed that you added stats for the \"residency\" (actual amount of live data), but not memory that was actually allocated from the GPU. The latter is also a pretty useful stat to collect, since (1) it is the thing that end users will actually see when the query nvidia-smi, and (2) it is the thing that will determine if you are actually out of memory or not is the actual memory allocated. Fragmentation could mean we are wasting memory in the caching allocator, that doesn't show up in the residency computation. (Though, tensor allocations tend to be big, so this isn't a big deal in practice...)\nThere's tons of other stats we could add to the allocator but I guess no one needed them, so we ain't gonna add 'em :)", "body": "I noticed that you added stats for the \"residency\" (actual amount of live data), but not memory that was actually allocated from the GPU. The latter is also a pretty useful stat to collect, since (1) it is the thing that end users will actually see when the query nvidia-smi, and (2) it is the thing that will determine if you are actually out of memory or not is the actual memory allocated. Fragmentation could mean we are wasting memory in the caching allocator, that doesn't show up in the residency computation. (Though, tensor allocations tend to be big, so this isn't a big deal in practice...)\r\n\r\nThere's tons of other stats we could add to the allocator but I guess no one needed them, so we ain't gonna add 'em :)"}