{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/160300699", "pull_request_review_id": 87392884, "id": 160300699, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MDMwMDY5OQ==", "diff_hunk": "@@ -48,8 +48,12 @@ Memory management\n PyTorch use a caching memory allocator to speed up memory allocations. This\n allows fast memory deallocation without device synchronizations. However, the\n unused memory managed by the allocator will still show as if used in\n-`nvidia-smi`. Calling :meth:`~torch.cuda.empty_cache` can release all unused\n-cached memory from PyTorch so that those can be used by other GPU applications.\n+`nvidia-smi`. You can use :meth:`~torch.cuda.memory_allocated` and\n+:meth:`~torch.cuda.max_memory_allocated` to monitor memory occupied by\n+tensors, and use :meth:`~torch.cuda.memory_cached` and\n+:meth:`~torch.cuda.max_memory_cachted` to monitor memory managed by the caching", "path": "docs/source/notes/cuda.rst", "position": null, "original_position": 9, "commit_id": "0e87fbde69b3efa2925109ac1ba224daf338f7b7", "original_commit_id": "28958c4ca005360710a04d590eb7c4acfd9a5118", "user": {"login": "jlquinn", "id": 826841, "node_id": "MDQ6VXNlcjgyNjg0MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/826841?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlquinn", "html_url": "https://github.com/jlquinn", "followers_url": "https://api.github.com/users/jlquinn/followers", "following_url": "https://api.github.com/users/jlquinn/following{/other_user}", "gists_url": "https://api.github.com/users/jlquinn/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlquinn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlquinn/subscriptions", "organizations_url": "https://api.github.com/users/jlquinn/orgs", "repos_url": "https://api.github.com/users/jlquinn/repos", "events_url": "https://api.github.com/users/jlquinn/events{/privacy}", "received_events_url": "https://api.github.com/users/jlquinn/received_events", "type": "User", "site_admin": false}, "body": "misspelling - should be torch.cuda.max_memory_cached", "created_at": "2018-01-09T01:33:17Z", "updated_at": "2018-11-23T15:37:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/4511#discussion_r160300699", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4511", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/160300699"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4511#discussion_r160300699"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4511"}}, "body_html": "<p>misspelling - should be torch.cuda.max_memory_cached</p>", "body_text": "misspelling - should be torch.cuda.max_memory_cached"}