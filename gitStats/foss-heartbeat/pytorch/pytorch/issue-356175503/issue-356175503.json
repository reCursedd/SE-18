{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11170", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11170/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11170/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11170/events", "html_url": "https://github.com/pytorch/pytorch/issues/11170", "id": 356175503, "node_id": "MDU6SXNzdWUzNTYxNzU1MDM=", "number": 11170, "title": "[pytorch0.4.1]Segmentation fault occurred when using NCCL as backend for distribution training.", "user": {"login": "DonYum", "id": 1471864, "node_id": "MDQ6VXNlcjE0NzE4NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1471864?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DonYum", "html_url": "https://github.com/DonYum", "followers_url": "https://api.github.com/users/DonYum/followers", "following_url": "https://api.github.com/users/DonYum/following{/other_user}", "gists_url": "https://api.github.com/users/DonYum/gists{/gist_id}", "starred_url": "https://api.github.com/users/DonYum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DonYum/subscriptions", "organizations_url": "https://api.github.com/users/DonYum/orgs", "repos_url": "https://api.github.com/users/DonYum/repos", "events_url": "https://api.github.com/users/DonYum/events{/privacy}", "received_events_url": "https://api.github.com/users/DonYum/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-09-01T05:58:25Z", "updated_at": "2018-11-15T20:12:03Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>When I use pytorch to train on distributed cluster, I select NCCL as the backend. However, I got a annoying Segmentation fault as below.</p>\n<h2>Code example</h2>\n<h3>About running env</h3>\n<p>code: <a href=\"https://github.com/pytorch/examples/tree/master/imagenet\">https://github.com/pytorch/examples/tree/master/imagenet</a></p>\n<p>Modifications:</p>\n<div class=\"highlight highlight-source-diff\"><pre><span class=\"pl-c1\">diff --git a/imagenet/main.py b/imagenet/main.py</span>\nindex 7a60cd2..4d6c736 100644\n<span class=\"pl-md\">--- a/imagenet/main.py</span>\n<span class=\"pl-mi1\">+++ b/imagenet/main.py</span>\n<span class=\"pl-mdr\">@@ -51,12 +51,13 @@</span> parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n                     help='evaluate model on validation set')\n parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n                     help='use pre-trained model')\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>parser.add_argument('--world-size', default=1, type=int,</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>parser.add_argument('--world-size', default=2, type=int,</span>\n                     help='number of distributed processes')\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>parser.add_argument('--dist-url', default='tcp://192.168.10.50:2345', type=str,</span>\n                     help='url used to set up distributed training')\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>parser.add_argument('--dist-backend', default='gloo', type=str,</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>parser.add_argument('--dist-backend', default='nccl', type=str,</span>\n                     help='distributed backend')\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>parser.add_argument('--dist-rank', default=0, type=int, help='rank of distributed processes')</span>\n parser.add_argument('--seed', default=None, type=int,\n                     help='seed for initializing training. ')\n parser.add_argument('--gpu', default=None, type=int,\n<span class=\"pl-mdr\">@@ -87,7 +88,7 @@</span> def main():\n \n     if args.distributed:\n         dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>                                world_size=args.world_size)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                                world_size=args.world_size, rank=args.dist_rank)</span>\n \n     # create model\n     if args.pretrained:</pre></div>\n<p>CMD:</p>\n<p><code>python main.py /data/rjz/data/imagenet/raw --epochs 1 -b 512 -j 24 --arch resnet50 --dist-rank</code> <em><code>[0/1]</code></em></p>\n<h3>Error Info &amp; My endeavor</h3>\n<p>And then I got a segment Fault:</p>\n<pre><code>......\nEpoch: [0][1170/1252]   Time 1.270 (1.278)      Data 0.001 (0.066)      Loss 4.9393 (5.8856)    Prec@1 9.375 (3.959)    Prec@5 23.242 (11.597)\nEpoch: [0][1180/1252]   Time 1.229 (1.278)      Data 0.001 (0.065)      Loss 4.8963 (5.8769)    Prec@1 8.984 (4.017)    Prec@5 23.633 (11.723)\nEpoch: [0][1190/1252]   Time 1.225 (1.277)      Data 0.000 (0.065)      Loss 4.7729 (5.8676)    Prec@1 13.086 (4.078)   Prec@5 29.883 (11.857)\nEpoch: [0][1200/1252]   Time 1.221 (1.277)      Data 0.000 (0.065)      Loss 4.7830 (5.8589)    Prec@1 8.398 (4.130)    Prec@5 26.172 (11.978)\n[717623.493819] python[18028]: segfault at 7fff8cea7fe8 ip 00007f415cc749df sp 00007fff8cea7ff0 error 6 in _C.cpython-36m-x86_64-linux-gnu.so[7f415c8d7000+40de000]\n</code></pre>\n<p><strong>I ran it sevral times, and it always crashed on <code>Epoch: [0][1200/1252]</code>.</strong></p>\n<p>So I do sth. below to try to handle it:</p>\n<ul>\n<li>\n<p>Using <code>gloo</code> to replace <code>NCCL</code>, everything is OK.</p>\n</li>\n<li>\n<p>I open NCCL debug info, there is no more info print. It's not NCCL's business, I thick.</p>\n</li>\n<li>\n<p>Using <code>gdb -ex r --args [cmd]</code> to get more debug info. I got:</p>\n<p><strong>rank 0</strong></p>\n<pre><code>Thread 1 \"python\" received signal SIGSEGV, Segmentation fault.\nlookdict_unicode (mp=0x7ffff62b76c0, key=0x7ffff3e60068, hash=-2694725939860369776, value_addr=0x7fffff7ff070, hashpos=0x0) at Objects/dictobject.c:804\n804     Objects/dictobject.c: No such file or directory.\n(gdb) bt\n#0  lookdict_unicode (mp=0x7ffff62b76c0, key=0x7ffff3e60068, hash=-2694725939860369776, value_addr=0x7fffff7ff070, hashpos=0x0) at Objects/dictobject.c:804\n#1  0x00007ffff7959316 in PyDict_GetItem (op=op@entry=0x7ffff62b76c0, key=key@entry=0x7ffff3e60068) at Objects/dictobject.c:1439\n#2  0x00007ffff796ae5c in _PyObject_GenericGetAttrWithDict (obj=obj@entry=0x7ffff4b786d8, name=name@entry=0x7ffff3e60068, dict=0x7ffff62b76c0, dict@entry=0x0) at Objects/object.c:1089\n#3  0x00007ffff796aff7 in PyObject_GenericGetAttr (obj=obj@entry=0x7ffff4b786d8, name=name@entry=0x7ffff3e60068) at Objects/object.c:1121\n#4  0x00007ffff79684f5 in module_getattro (m=0x7ffff4b786d8, name=0x7ffff3e60068) at Objects/moduleobject.c:663\n#5  0x00007ffff7a0487c in _PyEval_EvalFrameDefault (f=&lt;optimized out&gt;, throwflag=&lt;optimized out&gt;) at Python/ceval.c:2859\n#6  0x00007ffff7a002d0 in _PyFunction_FastCall (co=&lt;optimized out&gt;, args=&lt;optimized out&gt;, nargs=1, globals=&lt;optimized out&gt;) at Python/ceval.c:4906\n#7  0x00007ffff7a099e6 in _PyFunction_FastCallDict (func=func@entry=0x7fffe926e1e0, args=args@entry=0x7fffff7ff420, nargs=1, kwargs=kwargs@entry=0x0) at Python/ceval.c:5008\n#8  0x00007ffff790e9be in _PyObject_FastCallDict (func=func@entry=0x7fffe926e1e0, args=args@entry=0x7fffff7ff420, nargs=nargs@entry=1, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2310\n#9  0x00007ffff790eaae in _PyObject_Call_Prepend (func=0x7fffe926e1e0, obj=0x7ffd800c22b0, args=0x7ffff7e09048, kwargs=0x0) at Objects/abstract.c:2373\n#10 0x00007ffff790e8cb in _PyObject_FastCallDict (func=0x7ffdd2fd1cc8, args=&lt;optimized out&gt;, nargs=&lt;optimized out&gt;, kwargs=0x0) at Objects/abstract.c:2331\n#11 0x00007ffff79874d3 in slot_tp_finalize (self=0x7ffd800c22b0) at Objects/typeobject.c:6450\n#12 0x00007ffff7969d94 in PyObject_CallFinalizer (self=self@entry=0x7ffd800c22b0) at Objects/object.c:297\n#13 0x00007ffff7969dc9 in PyObject_CallFinalizerFromDealloc (self=self@entry=0x7ffd800c22b0) at Objects/object.c:314\n#14 0x00007ffff797b9b4 in subtype_dealloc (self=0x7ffd800c22b0) at Objects/typeobject.c:1151\n#15 0x00007fff88345f62 in THPPointer&lt;_object&gt;::operator= (new_ptr=0x0, this=0x7ffd98059dc8) at /pytorch/torch/csrc/utils/object_ptr.h:17\n#16 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059dc0, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:26\n#17 0x00007fff88346084 in torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059dc0, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:27\n#18 std::default_delete&lt;THFinalizer&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x7ffd98059dc0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#19 std::unique_ptr&lt;THFinalizer, std::default_delete&lt;THFinalizer&gt; &gt;::~unique_ptr (this=0x7ffd98059df0, __in_chrg=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#20 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059de0, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:22\n#21 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059de0, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:27\n#22 std::default_delete&lt;THFinalizer&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x7ffd98059de0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#23 std::unique_ptr&lt;THFinalizer, std::default_delete&lt;THFinalizer&gt; &gt;::~unique_ptr (this=0x7ffd98059df0, __in_chrg=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#24 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059e00, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:22\n#25 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059e00, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:27\n#26 std::default_delete&lt;THFinalizer&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x7ffd98059e00) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#27 std::unique_ptr&lt;THFinalizer, std::default_delete&lt;THFinalizer&gt; &gt;::~unique_ptr (this=0x7ffd98059df0, __in_chrg=&lt;optimized out&gt;)\n</code></pre>\n<p><strong>rank 1</strong></p>\n<pre><code>Thread 1 \"python\" received signal SIGSEGV, Segmentation fault.\n0x0000555555673175 in lookdict_unicode ()\n(gdb) bt\n#0  0x0000555555673175 in lookdict_unicode ()\n#1  0x0000555555655700 in PyDict_GetItem ()\n#2  0x000055555565fe21 in _PyObject_GenericGetAttrWithDict ()\n#3  0x00005555556ad360 in module_getattro ()\n#4  0x000055555571449a in _PyEval_EvalFrameDefault ()\n#5  0x00005555556ea51b in _PyFunction_FastCallDict ()\n#6  0x0000555555662b8f in _PyObject_FastCallDict ()\n#7  0x0000555555667773 in _PyObject_Call_Prepend ()\n#8  0x00005555556629ab in _PyObject_FastCallDict ()\n#9  0x00005555556bfb32 in slot_tp_finalize ()\n#10 0x00005555556f0b85 in PyObject_CallFinalizer ()\n#11 0x00005555556f0c7b in PyObject_CallFinalizerFromDealloc ()\n#12 0x00005555556f131c in subtype_dealloc ()\n#13 0x00007fff88aa6f62 in THPPointer&lt;_object&gt;::operator= (new_ptr=0x0, this=0x7ffee80698c8) at /pytorch/torch/csrc/utils/object_ptr.h:17\n#14 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698c0, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:26\n#15 0x00007fff88aa7084 in torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698c0, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:27\n#16 std::default_delete&lt;THFinalizer&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x7ffee80698c0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#17 std::unique_ptr&lt;THFinalizer, std::default_delete&lt;THFinalizer&gt; &gt;::~unique_ptr (this=0x7ffee80698f0, __in_chrg=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#18 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698e0, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:22\n#19 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698e0, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:27\n#20 std::default_delete&lt;THFinalizer&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x7ffee80698e0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#21 std::unique_ptr&lt;THFinalizer, std::default_delete&lt;THFinalizer&gt; &gt;::~unique_ptr (this=0x7ffee80698f0, __in_chrg=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#22 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069900, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:22\n#23 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069900, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:27\n#24 std::default_delete&lt;THFinalizer&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x7ffee8069900) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#25 std::unique_ptr&lt;THFinalizer, std::default_delete&lt;THFinalizer&gt; &gt;::~unique_ptr (this=0x7ffee80698f0, __in_chrg=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#26 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069920, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:22\n#27 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069920, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:27\n#28 std::default_delete&lt;THFinalizer&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x7ffee8069920) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#29 std::unique_ptr&lt;THFinalizer, std::default_delete&lt;THFinalizer&gt; &gt;::~unique_ptr (this=0x7ffee80698f0, __in_chrg=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#30 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069940, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:22\n#31 0x00007fff88aa7084 in torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069940, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:27\n#32 std::default_delete&lt;THFinalizer&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x7ffee8069940) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#33 std::unique_ptr&lt;THFinalizer, std::default_delete&lt;THFinalizer&gt; &gt;::~unique_ptr (this=0x7ffee8069970, __in_chrg=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#34 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069960, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:22\n#35 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069960, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:27\n#36 std::default_delete&lt;THFinalizer&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x7ffee8069960) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#37 std::unique_ptr&lt;THFinalizer, std::default_delete&lt;THFinalizer&gt; &gt;::~unique_ptr (this=0x7ffee8069970, __in_chrg=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#38 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069980, __in_chrg=&lt;optimized out&gt;) at /pytorch/torch/csrc/finalizer.h:22\n.......\n</code></pre>\n<p>It crashed in lookdict_unicode()<a class=\"user-mention\" data-hovercard-type=\"organization\" data-hovercard-url=\"/orgs/python/hovercard\" href=\"https://github.com/python\">@python</a>, so I google it. Then I found some clue from <a href=\"https://discuss.pytorch.org/t/segmentation-fault/23489\" rel=\"nofollow\">https://discuss.pytorch.org/t/segmentation-fault/23489</a>.</p>\n<p>Maybe sth. wrong with sampler of Dataloader, so I try to install torchvision0.2.0 instead 0.2.1</p>\n</li>\n<li>\n<p>Using torchvision0.2.0 instead 0.2.1</p>\n<p>Fail.</p>\n</li>\n<li>\n<p>Using torchvision0.4.0 instead 0.4.1</p>\n<p>It's OK.</p>\n</li>\n</ul>\n<h3>My conclusion</h3>\n<p>I think it may be a little flaw of pytorch 0.4.1. And please pay attention about it.</p>\n<p>Thanks for your attention.</p>\n<h2>System Info</h2>\n<pre><code>Collecting environment information...\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: Ubuntu 16.04.3 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: \nGPU 0: GeForce GTX TITAN X\nGPU 1: GeForce GTX TITAN X\nGPU 2: GeForce GTX TITAN X\nGPU 3: GeForce GTX TITAN X\n\nNvidia driver version: 375.26\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] torch                     0.4.1                     &lt;pip&gt;\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n</code></pre>", "body_text": "Issue description\nWhen I use pytorch to train on distributed cluster, I select NCCL as the backend. However, I got a annoying Segmentation fault as below.\nCode example\nAbout running env\ncode: https://github.com/pytorch/examples/tree/master/imagenet\nModifications:\ndiff --git a/imagenet/main.py b/imagenet/main.py\nindex 7a60cd2..4d6c736 100644\n--- a/imagenet/main.py\n+++ b/imagenet/main.py\n@@ -51,12 +51,13 @@ parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n                     help='evaluate model on validation set')\n parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n                     help='use pre-trained model')\n-parser.add_argument('--world-size', default=1, type=int,\n+parser.add_argument('--world-size', default=2, type=int,\n                     help='number of distributed processes')\n-parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,\n+parser.add_argument('--dist-url', default='tcp://192.168.10.50:2345', type=str,\n                     help='url used to set up distributed training')\n-parser.add_argument('--dist-backend', default='gloo', type=str,\n+parser.add_argument('--dist-backend', default='nccl', type=str,\n                     help='distributed backend')\n+parser.add_argument('--dist-rank', default=0, type=int, help='rank of distributed processes')\n parser.add_argument('--seed', default=None, type=int,\n                     help='seed for initializing training. ')\n parser.add_argument('--gpu', default=None, type=int,\n@@ -87,7 +88,7 @@ def main():\n \n     if args.distributed:\n         dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n-                                world_size=args.world_size)\n+                                world_size=args.world_size, rank=args.dist_rank)\n \n     # create model\n     if args.pretrained:\nCMD:\npython main.py /data/rjz/data/imagenet/raw --epochs 1 -b 512 -j 24 --arch resnet50 --dist-rank [0/1]\nError Info & My endeavor\nAnd then I got a segment Fault:\n......\nEpoch: [0][1170/1252]   Time 1.270 (1.278)      Data 0.001 (0.066)      Loss 4.9393 (5.8856)    Prec@1 9.375 (3.959)    Prec@5 23.242 (11.597)\nEpoch: [0][1180/1252]   Time 1.229 (1.278)      Data 0.001 (0.065)      Loss 4.8963 (5.8769)    Prec@1 8.984 (4.017)    Prec@5 23.633 (11.723)\nEpoch: [0][1190/1252]   Time 1.225 (1.277)      Data 0.000 (0.065)      Loss 4.7729 (5.8676)    Prec@1 13.086 (4.078)   Prec@5 29.883 (11.857)\nEpoch: [0][1200/1252]   Time 1.221 (1.277)      Data 0.000 (0.065)      Loss 4.7830 (5.8589)    Prec@1 8.398 (4.130)    Prec@5 26.172 (11.978)\n[717623.493819] python[18028]: segfault at 7fff8cea7fe8 ip 00007f415cc749df sp 00007fff8cea7ff0 error 6 in _C.cpython-36m-x86_64-linux-gnu.so[7f415c8d7000+40de000]\n\nI ran it sevral times, and it always crashed on Epoch: [0][1200/1252].\nSo I do sth. below to try to handle it:\n\n\nUsing gloo to replace NCCL, everything is OK.\n\n\nI open NCCL debug info, there is no more info print. It's not NCCL's business, I thick.\n\n\nUsing gdb -ex r --args [cmd] to get more debug info. I got:\nrank 0\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\nlookdict_unicode (mp=0x7ffff62b76c0, key=0x7ffff3e60068, hash=-2694725939860369776, value_addr=0x7fffff7ff070, hashpos=0x0) at Objects/dictobject.c:804\n804     Objects/dictobject.c: No such file or directory.\n(gdb) bt\n#0  lookdict_unicode (mp=0x7ffff62b76c0, key=0x7ffff3e60068, hash=-2694725939860369776, value_addr=0x7fffff7ff070, hashpos=0x0) at Objects/dictobject.c:804\n#1  0x00007ffff7959316 in PyDict_GetItem (op=op@entry=0x7ffff62b76c0, key=key@entry=0x7ffff3e60068) at Objects/dictobject.c:1439\n#2  0x00007ffff796ae5c in _PyObject_GenericGetAttrWithDict (obj=obj@entry=0x7ffff4b786d8, name=name@entry=0x7ffff3e60068, dict=0x7ffff62b76c0, dict@entry=0x0) at Objects/object.c:1089\n#3  0x00007ffff796aff7 in PyObject_GenericGetAttr (obj=obj@entry=0x7ffff4b786d8, name=name@entry=0x7ffff3e60068) at Objects/object.c:1121\n#4  0x00007ffff79684f5 in module_getattro (m=0x7ffff4b786d8, name=0x7ffff3e60068) at Objects/moduleobject.c:663\n#5  0x00007ffff7a0487c in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2859\n#6  0x00007ffff7a002d0 in _PyFunction_FastCall (co=<optimized out>, args=<optimized out>, nargs=1, globals=<optimized out>) at Python/ceval.c:4906\n#7  0x00007ffff7a099e6 in _PyFunction_FastCallDict (func=func@entry=0x7fffe926e1e0, args=args@entry=0x7fffff7ff420, nargs=1, kwargs=kwargs@entry=0x0) at Python/ceval.c:5008\n#8  0x00007ffff790e9be in _PyObject_FastCallDict (func=func@entry=0x7fffe926e1e0, args=args@entry=0x7fffff7ff420, nargs=nargs@entry=1, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2310\n#9  0x00007ffff790eaae in _PyObject_Call_Prepend (func=0x7fffe926e1e0, obj=0x7ffd800c22b0, args=0x7ffff7e09048, kwargs=0x0) at Objects/abstract.c:2373\n#10 0x00007ffff790e8cb in _PyObject_FastCallDict (func=0x7ffdd2fd1cc8, args=<optimized out>, nargs=<optimized out>, kwargs=0x0) at Objects/abstract.c:2331\n#11 0x00007ffff79874d3 in slot_tp_finalize (self=0x7ffd800c22b0) at Objects/typeobject.c:6450\n#12 0x00007ffff7969d94 in PyObject_CallFinalizer (self=self@entry=0x7ffd800c22b0) at Objects/object.c:297\n#13 0x00007ffff7969dc9 in PyObject_CallFinalizerFromDealloc (self=self@entry=0x7ffd800c22b0) at Objects/object.c:314\n#14 0x00007ffff797b9b4 in subtype_dealloc (self=0x7ffd800c22b0) at Objects/typeobject.c:1151\n#15 0x00007fff88345f62 in THPPointer<_object>::operator= (new_ptr=0x0, this=0x7ffd98059dc8) at /pytorch/torch/csrc/utils/object_ptr.h:17\n#16 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059dc0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:26\n#17 0x00007fff88346084 in torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059dc0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\n#18 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffd98059dc0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#19 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffd98059df0, __in_chrg=<optimized out>)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#20 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059de0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\n#21 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059de0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\n#22 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffd98059de0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#23 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffd98059df0, __in_chrg=<optimized out>)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#24 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059e00, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\n#25 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059e00, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\n#26 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffd98059e00) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#27 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffd98059df0, __in_chrg=<optimized out>)\n\nrank 1\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\n0x0000555555673175 in lookdict_unicode ()\n(gdb) bt\n#0  0x0000555555673175 in lookdict_unicode ()\n#1  0x0000555555655700 in PyDict_GetItem ()\n#2  0x000055555565fe21 in _PyObject_GenericGetAttrWithDict ()\n#3  0x00005555556ad360 in module_getattro ()\n#4  0x000055555571449a in _PyEval_EvalFrameDefault ()\n#5  0x00005555556ea51b in _PyFunction_FastCallDict ()\n#6  0x0000555555662b8f in _PyObject_FastCallDict ()\n#7  0x0000555555667773 in _PyObject_Call_Prepend ()\n#8  0x00005555556629ab in _PyObject_FastCallDict ()\n#9  0x00005555556bfb32 in slot_tp_finalize ()\n#10 0x00005555556f0b85 in PyObject_CallFinalizer ()\n#11 0x00005555556f0c7b in PyObject_CallFinalizerFromDealloc ()\n#12 0x00005555556f131c in subtype_dealloc ()\n#13 0x00007fff88aa6f62 in THPPointer<_object>::operator= (new_ptr=0x0, this=0x7ffee80698c8) at /pytorch/torch/csrc/utils/object_ptr.h:17\n#14 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698c0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:26\n#15 0x00007fff88aa7084 in torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698c0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\n#16 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee80698c0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#17 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee80698f0, __in_chrg=<optimized out>)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#18 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698e0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\n#19 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698e0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\n#20 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee80698e0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#21 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee80698f0, __in_chrg=<optimized out>)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#22 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069900, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\n#23 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069900, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\n#24 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee8069900) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#25 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee80698f0, __in_chrg=<optimized out>)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#26 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069920, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\n#27 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069920, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\n#28 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee8069920) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#29 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee80698f0, __in_chrg=<optimized out>)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#30 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069940, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\n#31 0x00007fff88aa7084 in torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069940, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\n#32 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee8069940) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#33 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee8069970, __in_chrg=<optimized out>)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#34 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069960, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\n#35 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069960, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\n#36 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee8069960) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\n#37 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee8069970, __in_chrg=<optimized out>)\n    at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\n#38 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069980, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\n.......\n\nIt crashed in lookdict_unicode()@python, so I google it. Then I found some clue from https://discuss.pytorch.org/t/segmentation-fault/23489.\nMaybe sth. wrong with sampler of Dataloader, so I try to install torchvision0.2.0 instead 0.2.1\n\n\nUsing torchvision0.2.0 instead 0.2.1\nFail.\n\n\nUsing torchvision0.4.0 instead 0.4.1\nIt's OK.\n\n\nMy conclusion\nI think it may be a little flaw of pytorch 0.4.1. And please pay attention about it.\nThanks for your attention.\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: Ubuntu 16.04.3 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: \nGPU 0: GeForce GTX TITAN X\nGPU 1: GeForce GTX TITAN X\nGPU 2: GeForce GTX TITAN X\nGPU 3: GeForce GTX TITAN X\n\nNvidia driver version: 375.26\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] torch                     0.4.1                     <pip>\n[conda] torchvision               0.2.1                     <pip>", "body": "## Issue description\r\n\r\nWhen I use pytorch to train on distributed cluster, I select NCCL as the backend. However, I got a annoying Segmentation fault as below.\r\n\r\n## Code example\r\n\r\n### About running env\r\n\r\ncode: <https://github.com/pytorch/examples/tree/master/imagenet>\r\n\r\nModifications:\r\n\r\n```diff\r\ndiff --git a/imagenet/main.py b/imagenet/main.py\r\nindex 7a60cd2..4d6c736 100644\r\n--- a/imagenet/main.py\r\n+++ b/imagenet/main.py\r\n@@ -51,12 +51,13 @@ parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\r\n                     help='evaluate model on validation set')\r\n parser.add_argument('--pretrained', dest='pretrained', action='store_true',\r\n                     help='use pre-trained model')\r\n-parser.add_argument('--world-size', default=1, type=int,\r\n+parser.add_argument('--world-size', default=2, type=int,\r\n                     help='number of distributed processes')\r\n-parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,\r\n+parser.add_argument('--dist-url', default='tcp://192.168.10.50:2345', type=str,\r\n                     help='url used to set up distributed training')\r\n-parser.add_argument('--dist-backend', default='gloo', type=str,\r\n+parser.add_argument('--dist-backend', default='nccl', type=str,\r\n                     help='distributed backend')\r\n+parser.add_argument('--dist-rank', default=0, type=int, help='rank of distributed processes')\r\n parser.add_argument('--seed', default=None, type=int,\r\n                     help='seed for initializing training. ')\r\n parser.add_argument('--gpu', default=None, type=int,\r\n@@ -87,7 +88,7 @@ def main():\r\n \r\n     if args.distributed:\r\n         dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\r\n-                                world_size=args.world_size)\r\n+                                world_size=args.world_size, rank=args.dist_rank)\r\n \r\n     # create model\r\n     if args.pretrained:\r\n```\r\n\r\nCMD:\r\n\r\n`python main.py /data/rjz/data/imagenet/raw --epochs 1 -b 512 -j 24 --arch resnet50 --dist-rank` *`[0/1]`*\r\n\r\n### Error Info & My endeavor\r\n\r\nAnd then I got a segment Fault:\r\n\r\n```\r\n......\r\nEpoch: [0][1170/1252]   Time 1.270 (1.278)      Data 0.001 (0.066)      Loss 4.9393 (5.8856)    Prec@1 9.375 (3.959)    Prec@5 23.242 (11.597)\r\nEpoch: [0][1180/1252]   Time 1.229 (1.278)      Data 0.001 (0.065)      Loss 4.8963 (5.8769)    Prec@1 8.984 (4.017)    Prec@5 23.633 (11.723)\r\nEpoch: [0][1190/1252]   Time 1.225 (1.277)      Data 0.000 (0.065)      Loss 4.7729 (5.8676)    Prec@1 13.086 (4.078)   Prec@5 29.883 (11.857)\r\nEpoch: [0][1200/1252]   Time 1.221 (1.277)      Data 0.000 (0.065)      Loss 4.7830 (5.8589)    Prec@1 8.398 (4.130)    Prec@5 26.172 (11.978)\r\n[717623.493819] python[18028]: segfault at 7fff8cea7fe8 ip 00007f415cc749df sp 00007fff8cea7ff0 error 6 in _C.cpython-36m-x86_64-linux-gnu.so[7f415c8d7000+40de000]\r\n```\r\n\r\n**I ran it sevral times, and it always crashed on `Epoch: [0][1200/1252]`.**\r\n\r\nSo I do sth. below to try to handle it:\r\n\r\n- Using `gloo` to replace `NCCL`, everything is OK.\r\n- I open NCCL debug info, there is no more info print. It's not NCCL's business, I thick.\r\n- Using `gdb -ex r --args [cmd]` to get more debug info. I got:\r\n\r\n    **rank 0**\r\n\r\n    ```\r\n    Thread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n    lookdict_unicode (mp=0x7ffff62b76c0, key=0x7ffff3e60068, hash=-2694725939860369776, value_addr=0x7fffff7ff070, hashpos=0x0) at Objects/dictobject.c:804\r\n    804     Objects/dictobject.c: No such file or directory.\r\n    (gdb) bt\r\n    #0  lookdict_unicode (mp=0x7ffff62b76c0, key=0x7ffff3e60068, hash=-2694725939860369776, value_addr=0x7fffff7ff070, hashpos=0x0) at Objects/dictobject.c:804\r\n    #1  0x00007ffff7959316 in PyDict_GetItem (op=op@entry=0x7ffff62b76c0, key=key@entry=0x7ffff3e60068) at Objects/dictobject.c:1439\r\n    #2  0x00007ffff796ae5c in _PyObject_GenericGetAttrWithDict (obj=obj@entry=0x7ffff4b786d8, name=name@entry=0x7ffff3e60068, dict=0x7ffff62b76c0, dict@entry=0x0) at Objects/object.c:1089\r\n    #3  0x00007ffff796aff7 in PyObject_GenericGetAttr (obj=obj@entry=0x7ffff4b786d8, name=name@entry=0x7ffff3e60068) at Objects/object.c:1121\r\n    #4  0x00007ffff79684f5 in module_getattro (m=0x7ffff4b786d8, name=0x7ffff3e60068) at Objects/moduleobject.c:663\r\n    #5  0x00007ffff7a0487c in _PyEval_EvalFrameDefault (f=<optimized out>, throwflag=<optimized out>) at Python/ceval.c:2859\r\n    #6  0x00007ffff7a002d0 in _PyFunction_FastCall (co=<optimized out>, args=<optimized out>, nargs=1, globals=<optimized out>) at Python/ceval.c:4906\r\n    #7  0x00007ffff7a099e6 in _PyFunction_FastCallDict (func=func@entry=0x7fffe926e1e0, args=args@entry=0x7fffff7ff420, nargs=1, kwargs=kwargs@entry=0x0) at Python/ceval.c:5008\r\n    #8  0x00007ffff790e9be in _PyObject_FastCallDict (func=func@entry=0x7fffe926e1e0, args=args@entry=0x7fffff7ff420, nargs=nargs@entry=1, kwargs=kwargs@entry=0x0) at Objects/abstract.c:2310\r\n    #9  0x00007ffff790eaae in _PyObject_Call_Prepend (func=0x7fffe926e1e0, obj=0x7ffd800c22b0, args=0x7ffff7e09048, kwargs=0x0) at Objects/abstract.c:2373\r\n    #10 0x00007ffff790e8cb in _PyObject_FastCallDict (func=0x7ffdd2fd1cc8, args=<optimized out>, nargs=<optimized out>, kwargs=0x0) at Objects/abstract.c:2331\r\n    #11 0x00007ffff79874d3 in slot_tp_finalize (self=0x7ffd800c22b0) at Objects/typeobject.c:6450\r\n    #12 0x00007ffff7969d94 in PyObject_CallFinalizer (self=self@entry=0x7ffd800c22b0) at Objects/object.c:297\r\n    #13 0x00007ffff7969dc9 in PyObject_CallFinalizerFromDealloc (self=self@entry=0x7ffd800c22b0) at Objects/object.c:314\r\n    #14 0x00007ffff797b9b4 in subtype_dealloc (self=0x7ffd800c22b0) at Objects/typeobject.c:1151\r\n    #15 0x00007fff88345f62 in THPPointer<_object>::operator= (new_ptr=0x0, this=0x7ffd98059dc8) at /pytorch/torch/csrc/utils/object_ptr.h:17\r\n    #16 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059dc0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:26\r\n    #17 0x00007fff88346084 in torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059dc0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\r\n    #18 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffd98059dc0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\r\n    #19 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffd98059df0, __in_chrg=<optimized out>)\r\n        at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\r\n    #20 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059de0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\r\n    #21 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059de0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\r\n    #22 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffd98059de0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\r\n    #23 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffd98059df0, __in_chrg=<optimized out>)\r\n        at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\r\n    #24 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059e00, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\r\n    #25 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffd98059e00, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\r\n    #26 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffd98059e00) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\r\n    #27 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffd98059df0, __in_chrg=<optimized out>)\r\n    ```\r\n\r\n    **rank 1**\r\n\r\n    ```\r\n    Thread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n    0x0000555555673175 in lookdict_unicode ()\r\n    (gdb) bt\r\n    #0  0x0000555555673175 in lookdict_unicode ()\r\n    #1  0x0000555555655700 in PyDict_GetItem ()\r\n    #2  0x000055555565fe21 in _PyObject_GenericGetAttrWithDict ()\r\n    #3  0x00005555556ad360 in module_getattro ()\r\n    #4  0x000055555571449a in _PyEval_EvalFrameDefault ()\r\n    #5  0x00005555556ea51b in _PyFunction_FastCallDict ()\r\n    #6  0x0000555555662b8f in _PyObject_FastCallDict ()\r\n    #7  0x0000555555667773 in _PyObject_Call_Prepend ()\r\n    #8  0x00005555556629ab in _PyObject_FastCallDict ()\r\n    #9  0x00005555556bfb32 in slot_tp_finalize ()\r\n    #10 0x00005555556f0b85 in PyObject_CallFinalizer ()\r\n    #11 0x00005555556f0c7b in PyObject_CallFinalizerFromDealloc ()\r\n    #12 0x00005555556f131c in subtype_dealloc ()\r\n    #13 0x00007fff88aa6f62 in THPPointer<_object>::operator= (new_ptr=0x0, this=0x7ffee80698c8) at /pytorch/torch/csrc/utils/object_ptr.h:17\r\n    #14 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698c0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:26\r\n    #15 0x00007fff88aa7084 in torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698c0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\r\n    #16 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee80698c0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\r\n    #17 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee80698f0, __in_chrg=<optimized out>)\r\n        at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\r\n    #18 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698e0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\r\n    #19 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee80698e0, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\r\n    #20 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee80698e0) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\r\n    #21 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee80698f0, __in_chrg=<optimized out>)\r\n        at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\r\n    #22 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069900, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\r\n    #23 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069900, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\r\n    #24 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee8069900) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\r\n    #25 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee80698f0, __in_chrg=<optimized out>)\r\n        at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\r\n    #26 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069920, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\r\n    #27 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069920, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\r\n    #28 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee8069920) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\r\n    #29 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee80698f0, __in_chrg=<optimized out>)\r\n        at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\r\n    #30 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069940, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\r\n    #31 0x00007fff88aa7084 in torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069940, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\r\n    #32 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee8069940) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\r\n    #33 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee8069970, __in_chrg=<optimized out>)\r\n        at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\r\n    #34 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069960, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\r\n    #35 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069960, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:27\r\n    #36 std::default_delete<THFinalizer>::operator() (this=<optimized out>, __ptr=0x7ffee8069960) at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:76\r\n    #37 std::unique_ptr<THFinalizer, std::default_delete<THFinalizer> >::~unique_ptr (this=0x7ffee8069970, __in_chrg=<optimized out>)\r\n        at /opt/rh/devtoolset-3/root/usr/include/c++/4.9.2/bits/unique_ptr.h:236\r\n    #38 torch::PyObjectFinalizer::~PyObjectFinalizer (this=0x7ffee8069980, __in_chrg=<optimized out>) at /pytorch/torch/csrc/finalizer.h:22\r\n    .......\r\n    ```\r\n\r\n    It crashed in lookdict_unicode()@python, so I google it. Then I found some clue from <https://discuss.pytorch.org/t/segmentation-fault/23489>. \r\n\r\n    Maybe sth. wrong with sampler of Dataloader, so I try to install torchvision0.2.0 instead 0.2.1\r\n\r\n- Using torchvision0.2.0 instead 0.2.1\r\n\r\n    Fail.\r\n\r\n- Using torchvision0.4.0 instead 0.4.1\r\n\r\n    It's OK.\r\n\r\n### My conclusion\r\n\r\nI think it may be a little flaw of pytorch 0.4.1. And please pay attention about it. \r\n\r\nThanks for your attention.\r\n\r\n## System Info\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN X\r\nGPU 2: GeForce GTX TITAN X\r\nGPU 3: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 375.26\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] torch                     0.4.1                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n"}