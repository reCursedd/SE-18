{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/431570125", "html_url": "https://github.com/pytorch/pytorch/issues/12887#issuecomment-431570125", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12887", "id": 431570125, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTU3MDEyNQ==", "user": {"login": "glample", "id": 8885556, "node_id": "MDQ6VXNlcjg4ODU1NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8885556?v=4", "gravatar_id": "", "url": "https://api.github.com/users/glample", "html_url": "https://github.com/glample", "followers_url": "https://api.github.com/users/glample/followers", "following_url": "https://api.github.com/users/glample/following{/other_user}", "gists_url": "https://api.github.com/users/glample/gists{/gist_id}", "starred_url": "https://api.github.com/users/glample/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/glample/subscriptions", "organizations_url": "https://api.github.com/users/glample/orgs", "repos_url": "https://api.github.com/users/glample/repos", "events_url": "https://api.github.com/users/glample/events{/privacy}", "received_events_url": "https://api.github.com/users/glample/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-20T10:55:05Z", "updated_at": "2018-10-20T11:16:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4191866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/HKervadec\">@HKervadec</a> even though <code>a</code> is a <code>int64</code> tensor, <code>a == a</code> becomes a <code>uint8</code> tensor, so my title is correct. That being said, there is indeed no need to use a <code>LongTensor</code> for <code>a</code> as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=39071503\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alexcaveny\">@alexcaveny</a> pointed out. I updated original post with a more memory efficient way to reproduce the bug.</p>\n<p>To give a bit of context, I typically store a dataset of concatenated sentences in a single vector <code>a</code>, then I compute the number of unknown words by doing: <code>(a == UNK_IDX).sum()</code>. I currently do: <code>(a.numpy() == UNK_IDX).sum()</code> to avoid the issue, but the error shouldn't be there in PyTorch.</p>", "body_text": "@HKervadec even though a is a int64 tensor, a == a becomes a uint8 tensor, so my title is correct. That being said, there is indeed no need to use a LongTensor for a as @alexcaveny pointed out. I updated original post with a more memory efficient way to reproduce the bug.\nTo give a bit of context, I typically store a dataset of concatenated sentences in a single vector a, then I compute the number of unknown words by doing: (a == UNK_IDX).sum(). I currently do: (a.numpy() == UNK_IDX).sum() to avoid the issue, but the error shouldn't be there in PyTorch.", "body": "@HKervadec even though `a` is a `int64` tensor, `a == a` becomes a `uint8` tensor, so my title is correct. That being said, there is indeed no need to use a `LongTensor` for `a` as @alexcaveny pointed out. I updated original post with a more memory efficient way to reproduce the bug.\r\n\r\nTo give a bit of context, I typically store a dataset of concatenated sentences in a single vector `a`, then I compute the number of unknown words by doing: `(a == UNK_IDX).sum()`. I currently do: `(a.numpy() == UNK_IDX).sum()` to avoid the issue, but the error shouldn't be there in PyTorch."}