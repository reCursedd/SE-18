{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9163", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9163/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9163/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9163/events", "html_url": "https://github.com/pytorch/pytorch/issues/9163", "id": 338167457, "node_id": "MDU6SXNzdWUzMzgxNjc0NTc=", "number": 9163, "title": "DataParallel model stucks with CUDA_LAUNCH_BLOCKING=1 sometime", "user": {"login": "acgtyrant", "id": 3921062, "node_id": "MDQ6VXNlcjM5MjEwNjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3921062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acgtyrant", "html_url": "https://github.com/acgtyrant", "followers_url": "https://api.github.com/users/acgtyrant/followers", "following_url": "https://api.github.com/users/acgtyrant/following{/other_user}", "gists_url": "https://api.github.com/users/acgtyrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/acgtyrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acgtyrant/subscriptions", "organizations_url": "https://api.github.com/users/acgtyrant/orgs", "repos_url": "https://api.github.com/users/acgtyrant/repos", "events_url": "https://api.github.com/users/acgtyrant/events{/privacy}", "received_events_url": "https://api.github.com/users/acgtyrant/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-07-04T07:54:22Z", "updated_at": "2018-11-05T17:52:55Z", "closed_at": "2018-07-04T13:09:46Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>This is the context of the <code>segment.py</code>:</p>\n<pre><code>#!/usr/bin/env python\n\nimport torch\nfrom torch import nn\nimport torch.utils.data\n\n\nclass DRN(nn.Module):\n    def __init__(self):\n        super(DRN, self).__init__()\n        self.a = nn.Conv2d(3, 16, kernel_size=7)\n\n    def forward(self, x):\n        print('before DRN forward')\n        return x\n\n\nif __name__ == '__main__':\n    model = torch.nn.DataParallel(DRN()).cuda().train()\n    input_ = torch.rand(2).cuda()\n    print('before input')\n    model(input_)\n    print('end')\n</code></pre>\n<p>If I run <code>CUDA_VISIBLE_DEVICES=0,1 ./segment.py</code>, it will outputs</p>\n<p>before input<br>\nbefore DRN forward<br>\nbefore DRN forward<br>\nend</p>\n<p>However, if I run <code>CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0,1  ./segment.py</code>, it will print <code>before input</code> only and stucks like below:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3921062/42263854-c97953ae-7fa1-11e8-9b9f-e5976388252f.png\"><img src=\"https://user-images.githubusercontent.com/3921062/42263854-c97953ae-7fa1-11e8-9b9f-e5976388252f.png\" alt=\"2018-07-04-154901_1916x1058_scrot\" style=\"max-width:100%;\"></a></p>\n<p>It very strange that if I change <code>rand(2)</code> to <code>rand(1)</code> or change <code>kernel__size=7</code> to <code>kernel_size=2</code>, it does not stuck again. So I describe this bug occurs \"sometime\".</p>\n<p>I reproduce this bug in an machine which has two GTX 1080 too.</p>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.3 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.5<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.61<br>\nGPU models and configuration:<br>\nGPU 0: GeForce GTX 1080 Ti<br>\nGPU 1: GeForce GTX 1080 Ti<br>\nGPU 2: GeForce GTX 1080 Ti<br>\nGPU 3: GeForce GTX 1080 Ti</p>\n<p>Nvidia driver version: 384.98<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21<br>\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.14.2)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchvision (0.2.0)<br>\n[conda] Could not collect</p>", "body_text": "Issue description\nThis is the context of the segment.py:\n#!/usr/bin/env python\n\nimport torch\nfrom torch import nn\nimport torch.utils.data\n\n\nclass DRN(nn.Module):\n    def __init__(self):\n        super(DRN, self).__init__()\n        self.a = nn.Conv2d(3, 16, kernel_size=7)\n\n    def forward(self, x):\n        print('before DRN forward')\n        return x\n\n\nif __name__ == '__main__':\n    model = torch.nn.DataParallel(DRN()).cuda().train()\n    input_ = torch.rand(2).cuda()\n    print('before input')\n    model(input_)\n    print('end')\n\nIf I run CUDA_VISIBLE_DEVICES=0,1 ./segment.py, it will outputs\nbefore input\nbefore DRN forward\nbefore DRN forward\nend\nHowever, if I run CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0,1  ./segment.py, it will print before input only and stucks like below:\n\nIt very strange that if I change rand(2) to rand(1) or change kernel__size=7 to kernel_size=2, it does not stuck again. So I describe this bug occurs \"sometime\".\nI reproduce this bug in an machine which has two GTX 1080 too.\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.3 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration:\nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\nGPU 3: GeForce GTX 1080 Ti\nNvidia driver version: 384.98\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\nVersions of relevant libraries:\n[pip3] numpy (1.14.2)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.0)\n[conda] Could not collect", "body": "## Issue description\r\n\r\nThis is the context of the `segment.py`:\r\n\r\n    #!/usr/bin/env python\r\n\r\n    import torch\r\n    from torch import nn\r\n    import torch.utils.data\r\n\r\n\r\n    class DRN(nn.Module):\r\n        def __init__(self):\r\n            super(DRN, self).__init__()\r\n            self.a = nn.Conv2d(3, 16, kernel_size=7)\r\n\r\n        def forward(self, x):\r\n            print('before DRN forward')\r\n            return x\r\n\r\n\r\n    if __name__ == '__main__':\r\n        model = torch.nn.DataParallel(DRN()).cuda().train()\r\n        input_ = torch.rand(2).cuda()\r\n        print('before input')\r\n        model(input_)\r\n        print('end')\r\n\r\nIf I run `CUDA_VISIBLE_DEVICES=0,1 ./segment.py`, it will outputs\r\n\r\nbefore input\r\nbefore DRN forward\r\nbefore DRN forward\r\nend\r\n\r\nHowever, if I run `CUDA_LAUNCH_BLOCKING=1 CUDA_VISIBLE_DEVICES=0,1  ./segment.py`, it will print `before input` only and stucks like below:\r\n\r\n![2018-07-04-154901_1916x1058_scrot](https://user-images.githubusercontent.com/3921062/42263854-c97953ae-7fa1-11e8-9b9f-e5976388252f.png)\r\n\r\nIt very strange that if I change `rand(2)` to `rand(1)` or change `kernel__size=7` to `kernel_size=2`, it does not stuck again. So I describe this bug occurs \"sometime\".\r\n\r\nI reproduce this bug in an machine which has two GTX 1080 too.\r\n\r\n## System Info\r\n\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 384.98\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.0)\r\n[conda] Could not collect"}