{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7457", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7457/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7457/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7457/events", "html_url": "https://github.com/pytorch/pytorch/issues/7457", "id": 321850822, "node_id": "MDU6SXNzdWUzMjE4NTA4MjI=", "number": 7457, "title": "[feature request] torch.nn.DataParallel should work nicely both for cpu and gpu devices", "user": {"login": "jyzhang-bjtu", "id": 10786236, "node_id": "MDQ6VXNlcjEwNzg2MjM2", "avatar_url": "https://avatars2.githubusercontent.com/u/10786236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jyzhang-bjtu", "html_url": "https://github.com/jyzhang-bjtu", "followers_url": "https://api.github.com/users/jyzhang-bjtu/followers", "following_url": "https://api.github.com/users/jyzhang-bjtu/following{/other_user}", "gists_url": "https://api.github.com/users/jyzhang-bjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jyzhang-bjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jyzhang-bjtu/subscriptions", "organizations_url": "https://api.github.com/users/jyzhang-bjtu/orgs", "repos_url": "https://api.github.com/users/jyzhang-bjtu/repos", "events_url": "https://api.github.com/users/jyzhang-bjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/jyzhang-bjtu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-05-10T08:26:17Z", "updated_at": "2018-05-16T23:48:28Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>If the working machine has a GPU card, it seems that there is no way for torch.nn.DataParallel to work on the CPU.<br>\nThis is due to the following lines in the torch.nn.DataParallel.</p>\n<pre><code>        if not torch.cuda.is_available():\n            self.module = module\n            self.device_ids = []\n            return\n</code></pre>\n<p>Thus, we should provide an option that if device_ids = None then torch.nn.DataParallel works on the CPU.</p>\n<p>if devices_ids = [], then all available GPUs should be used.</p>\n<p>or implement the to() method for torch.nn.DataParallel.</p>\n<p>So we can freely run the code on GPU or CPU if torch.nn.DataParallel is used.</p>", "body_text": "If the working machine has a GPU card, it seems that there is no way for torch.nn.DataParallel to work on the CPU.\nThis is due to the following lines in the torch.nn.DataParallel.\n        if not torch.cuda.is_available():\n            self.module = module\n            self.device_ids = []\n            return\n\nThus, we should provide an option that if device_ids = None then torch.nn.DataParallel works on the CPU.\nif devices_ids = [], then all available GPUs should be used.\nor implement the to() method for torch.nn.DataParallel.\nSo we can freely run the code on GPU or CPU if torch.nn.DataParallel is used.", "body": "If the working machine has a GPU card, it seems that there is no way for torch.nn.DataParallel to work on the CPU.  \r\nThis is due to the following lines in the torch.nn.DataParallel. \r\n```\r\n        if not torch.cuda.is_available():\r\n            self.module = module\r\n            self.device_ids = []\r\n            return\r\n```\r\n\r\n\r\nThus, we should provide an option that if device_ids = None then torch.nn.DataParallel works on the CPU.\r\n\r\nif devices_ids = [], then all available GPUs should be used.\r\n\r\nor implement the to() method for torch.nn.DataParallel.\r\n\r\nSo we can freely run the code on GPU or CPU if torch.nn.DataParallel is used."}