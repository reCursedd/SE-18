{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2032", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2032/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2032/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2032/events", "html_url": "https://github.com/pytorch/pytorch/issues/2032", "id": 241592245, "node_id": "MDU6SXNzdWUyNDE1OTIyNDU=", "number": 2032, "title": "Possible bug in RNN module", "user": {"login": "sudongqi", "id": 16803510, "node_id": "MDQ6VXNlcjE2ODAzNTEw", "avatar_url": "https://avatars1.githubusercontent.com/u/16803510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sudongqi", "html_url": "https://github.com/sudongqi", "followers_url": "https://api.github.com/users/sudongqi/followers", "following_url": "https://api.github.com/users/sudongqi/following{/other_user}", "gists_url": "https://api.github.com/users/sudongqi/gists{/gist_id}", "starred_url": "https://api.github.com/users/sudongqi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sudongqi/subscriptions", "organizations_url": "https://api.github.com/users/sudongqi/orgs", "repos_url": "https://api.github.com/users/sudongqi/repos", "events_url": "https://api.github.com/users/sudongqi/events{/privacy}", "received_events_url": "https://api.github.com/users/sudongqi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-07-10T03:47:29Z", "updated_at": "2017-07-11T15:27:42Z", "closed_at": "2017-07-11T14:14:44Z", "author_association": "NONE", "body_html": "<p>...................................................................................................</p>\n<p>Sorry, I have to reopen this because I think this is a bug within the RNN module and I could't get help from discuss.pytorch</p>\n<p>I run a similar experiment with one linear layer neural network and the output is correct. So a reasonable explanation would be the RNN module keep different set of parameters (for optimization reason?) and didn't mix up at the end of the batch training. The following code will duplicate this problem.</p>\n<p>........................................................................................................</p>\n<p>I encounter this problem when I am trying to implement seq2seq to familiarize with this new framework. This issue seems related to parameters sharing in mini-batch. I setup a dummy training set with only one mini-batch. This mini-batch has 3 data entries in it. All with the same input, and different outputs:</p>\n<p>training_data = [<br>\n[[[4,5,1,7,8],[4,5,1,7,8],[4,5,1,7,8]], (input 1)<br>\n[[0],[0],[0]], (input 2)<br>\n[[1],[3],[5]]] (target)<br>\n]</p>\n<p>In theory, the model will never learn this data because the contradiction. However, the loss reach near 0 after only a few hundred epochs. But if I split 3 data entries into 3 mini-batch, the model will not learn the data set which should be the correct result.</p>\n<p>So the model must be keeping different set of parameter for each position in mini-batch? And the parameters are not updated to be the same after each mini-batch forward-backward? Can someone tell me if I misunderstood something?</p>\n<pre><code>import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\n\ntraining_data = [\n\t[[[4,5,1,7,8],[4,5,1,7,8],[4,5,1,7,8]], [[0],[0],[0]], [[1],[3],[5]]]\n\t]\n\n\ndef prepare_sequence(all_seq):\n\treturn autograd.Variable(torch.LongTensor(all_seq)).cuda()\n\nclass Seq2seqLSTM(nn.Module):\n\n\tdef __init__(self, vocab_size, target_size, embed_dim, hidden_dim, num_layers):\n\n\t\tsuper(Seq2seqLSTM, self).__init__()\n\t\tself.hidden_dim = hidden_dim\n\t\tself.embed_dim = embed_dim\n\t\tself.vocab_size = vocab_size\n\t\tself.target_size = target_size\n\t\tself.num_layers = num_layers\n\n\t\tself.word_embeddings = nn.Embedding(vocab_size, embed_dim)\n\t\tself.encoder = nn.LSTM(embed_dim, hidden_dim, num_layers)\n\t\tself.decoder = nn.LSTM(embed_dim, hidden_dim, num_layers)\n\n\t\tself.curr_hidden = None\n\t\tself.hidden2tag = nn.Linear(hidden_dim, target_size)\n\n\tdef init_hidden(self, batch_size):\n\t\treturn (autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda(),\n\t\t\tautograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda())\n\n\tdef forward(self, enc_seq, dec_seq):\n\n\t\tbatch_size = enc_seq.size()[0]\n\t\tself.curr_hidden = self.init_hidden(batch_size)\n\n\t\tenc_embeds = self.word_embeddings(enc_seq)\n\t\tdec_embeds = self.word_embeddings(dec_seq)\n\n\t\tenc_out, self.curr_hidden = self.encoder(\n\t\t\tenc_embeds.view(-1, batch_size, self.embed_dim), self.curr_hidden)\n\t\tdec_out, self.curr_hidden = self.decoder(\n\t\t\tdec_embeds.view(-1, batch_size, self.embed_dim), self.curr_hidden)\n\n\t\ttag_space = self.hidden2tag(dec_out.view(batch_size * len(dec_out), -1))\n\t\ttag_scores = F.log_softmax(tag_space)\n\t\treturn tag_scores\n\nEMBED_DIM = 10\nHIDDEN_DIM = 10\nVOCAB_SIZE = 10\nTARGET_SIZE = 10\nNUM_LAYERS = 2\n\nmodel = Seq2seqLSTM(VOCAB_SIZE, TARGET_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS).cuda()\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())\nprint model\n\nfor epoch in range(1000):\n\n\tone_batch = map(lambda x:prepare_sequence(x), training_data[epoch%len(training_data)])\n\tenc_inp = one_batch[0]\n\tdec_inp = one_batch[1]\n\ttarget = one_batch[2]\n\n\tmodel.zero_grad()\n\ttag_scores = model(enc_inp, dec_inp)\n\n\tloss = loss_function(tag_scores, target.view(-1))\n\tloss.backward()\n\toptimizer.step()\n\n\tprint loss\n</code></pre>", "body_text": "...................................................................................................\nSorry, I have to reopen this because I think this is a bug within the RNN module and I could't get help from discuss.pytorch\nI run a similar experiment with one linear layer neural network and the output is correct. So a reasonable explanation would be the RNN module keep different set of parameters (for optimization reason?) and didn't mix up at the end of the batch training. The following code will duplicate this problem.\n........................................................................................................\nI encounter this problem when I am trying to implement seq2seq to familiarize with this new framework. This issue seems related to parameters sharing in mini-batch. I setup a dummy training set with only one mini-batch. This mini-batch has 3 data entries in it. All with the same input, and different outputs:\ntraining_data = [\n[[[4,5,1,7,8],[4,5,1,7,8],[4,5,1,7,8]], (input 1)\n[[0],[0],[0]], (input 2)\n[[1],[3],[5]]] (target)\n]\nIn theory, the model will never learn this data because the contradiction. However, the loss reach near 0 after only a few hundred epochs. But if I split 3 data entries into 3 mini-batch, the model will not learn the data set which should be the correct result.\nSo the model must be keeping different set of parameter for each position in mini-batch? And the parameters are not updated to be the same after each mini-batch forward-backward? Can someone tell me if I misunderstood something?\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\n\ntraining_data = [\n\t[[[4,5,1,7,8],[4,5,1,7,8],[4,5,1,7,8]], [[0],[0],[0]], [[1],[3],[5]]]\n\t]\n\n\ndef prepare_sequence(all_seq):\n\treturn autograd.Variable(torch.LongTensor(all_seq)).cuda()\n\nclass Seq2seqLSTM(nn.Module):\n\n\tdef __init__(self, vocab_size, target_size, embed_dim, hidden_dim, num_layers):\n\n\t\tsuper(Seq2seqLSTM, self).__init__()\n\t\tself.hidden_dim = hidden_dim\n\t\tself.embed_dim = embed_dim\n\t\tself.vocab_size = vocab_size\n\t\tself.target_size = target_size\n\t\tself.num_layers = num_layers\n\n\t\tself.word_embeddings = nn.Embedding(vocab_size, embed_dim)\n\t\tself.encoder = nn.LSTM(embed_dim, hidden_dim, num_layers)\n\t\tself.decoder = nn.LSTM(embed_dim, hidden_dim, num_layers)\n\n\t\tself.curr_hidden = None\n\t\tself.hidden2tag = nn.Linear(hidden_dim, target_size)\n\n\tdef init_hidden(self, batch_size):\n\t\treturn (autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda(),\n\t\t\tautograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda())\n\n\tdef forward(self, enc_seq, dec_seq):\n\n\t\tbatch_size = enc_seq.size()[0]\n\t\tself.curr_hidden = self.init_hidden(batch_size)\n\n\t\tenc_embeds = self.word_embeddings(enc_seq)\n\t\tdec_embeds = self.word_embeddings(dec_seq)\n\n\t\tenc_out, self.curr_hidden = self.encoder(\n\t\t\tenc_embeds.view(-1, batch_size, self.embed_dim), self.curr_hidden)\n\t\tdec_out, self.curr_hidden = self.decoder(\n\t\t\tdec_embeds.view(-1, batch_size, self.embed_dim), self.curr_hidden)\n\n\t\ttag_space = self.hidden2tag(dec_out.view(batch_size * len(dec_out), -1))\n\t\ttag_scores = F.log_softmax(tag_space)\n\t\treturn tag_scores\n\nEMBED_DIM = 10\nHIDDEN_DIM = 10\nVOCAB_SIZE = 10\nTARGET_SIZE = 10\nNUM_LAYERS = 2\n\nmodel = Seq2seqLSTM(VOCAB_SIZE, TARGET_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS).cuda()\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())\nprint model\n\nfor epoch in range(1000):\n\n\tone_batch = map(lambda x:prepare_sequence(x), training_data[epoch%len(training_data)])\n\tenc_inp = one_batch[0]\n\tdec_inp = one_batch[1]\n\ttarget = one_batch[2]\n\n\tmodel.zero_grad()\n\ttag_scores = model(enc_inp, dec_inp)\n\n\tloss = loss_function(tag_scores, target.view(-1))\n\tloss.backward()\n\toptimizer.step()\n\n\tprint loss", "body": "\r\n...................................................................................................\r\n\r\nSorry, I have to reopen this because I think this is a bug within the RNN module and I could't get help from discuss.pytorch\r\n\r\nI run a similar experiment with one linear layer neural network and the output is correct. So a reasonable explanation would be the RNN module keep different set of parameters (for optimization reason?) and didn't mix up at the end of the batch training. The following code will duplicate this problem.\r\n\r\n........................................................................................................\r\n\r\nI encounter this problem when I am trying to implement seq2seq to familiarize with this new framework. This issue seems related to parameters sharing in mini-batch. I setup a dummy training set with only one mini-batch. This mini-batch has 3 data entries in it. All with the same input, and different outputs:\r\n\r\ntraining_data = [\r\n[[[4,5,1,7,8],[4,5,1,7,8],[4,5,1,7,8]], (input 1)\r\n[[0],[0],[0]], (input 2)\r\n[[1],[3],[5]]] (target)\r\n]\r\n\r\nIn theory, the model will never learn this data because the contradiction. However, the loss reach near 0 after only a few hundred epochs. But if I split 3 data entries into 3 mini-batch, the model will not learn the data set which should be the correct result.\r\n\r\nSo the model must be keeping different set of parameter for each position in mini-batch? And the parameters are not updated to be the same after each mini-batch forward-backward? Can someone tell me if I misunderstood something?\r\n\r\n```\r\nimport torch\r\nimport torch.autograd as autograd\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\n\r\n\r\n\r\ntraining_data = [\r\n\t[[[4,5,1,7,8],[4,5,1,7,8],[4,5,1,7,8]], [[0],[0],[0]], [[1],[3],[5]]]\r\n\t]\r\n\r\n\r\ndef prepare_sequence(all_seq):\r\n\treturn autograd.Variable(torch.LongTensor(all_seq)).cuda()\r\n\r\nclass Seq2seqLSTM(nn.Module):\r\n\r\n\tdef __init__(self, vocab_size, target_size, embed_dim, hidden_dim, num_layers):\r\n\r\n\t\tsuper(Seq2seqLSTM, self).__init__()\r\n\t\tself.hidden_dim = hidden_dim\r\n\t\tself.embed_dim = embed_dim\r\n\t\tself.vocab_size = vocab_size\r\n\t\tself.target_size = target_size\r\n\t\tself.num_layers = num_layers\r\n\r\n\t\tself.word_embeddings = nn.Embedding(vocab_size, embed_dim)\r\n\t\tself.encoder = nn.LSTM(embed_dim, hidden_dim, num_layers)\r\n\t\tself.decoder = nn.LSTM(embed_dim, hidden_dim, num_layers)\r\n\r\n\t\tself.curr_hidden = None\r\n\t\tself.hidden2tag = nn.Linear(hidden_dim, target_size)\r\n\r\n\tdef init_hidden(self, batch_size):\r\n\t\treturn (autograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda(),\r\n\t\t\tautograd.Variable(torch.zeros(self.num_layers, batch_size, self.hidden_dim)).cuda())\r\n\r\n\tdef forward(self, enc_seq, dec_seq):\r\n\r\n\t\tbatch_size = enc_seq.size()[0]\r\n\t\tself.curr_hidden = self.init_hidden(batch_size)\r\n\r\n\t\tenc_embeds = self.word_embeddings(enc_seq)\r\n\t\tdec_embeds = self.word_embeddings(dec_seq)\r\n\r\n\t\tenc_out, self.curr_hidden = self.encoder(\r\n\t\t\tenc_embeds.view(-1, batch_size, self.embed_dim), self.curr_hidden)\r\n\t\tdec_out, self.curr_hidden = self.decoder(\r\n\t\t\tdec_embeds.view(-1, batch_size, self.embed_dim), self.curr_hidden)\r\n\r\n\t\ttag_space = self.hidden2tag(dec_out.view(batch_size * len(dec_out), -1))\r\n\t\ttag_scores = F.log_softmax(tag_space)\r\n\t\treturn tag_scores\r\n\r\nEMBED_DIM = 10\r\nHIDDEN_DIM = 10\r\nVOCAB_SIZE = 10\r\nTARGET_SIZE = 10\r\nNUM_LAYERS = 2\r\n\r\nmodel = Seq2seqLSTM(VOCAB_SIZE, TARGET_SIZE, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS).cuda()\r\nloss_function = nn.NLLLoss()\r\noptimizer = optim.Adam(model.parameters())\r\nprint model\r\n\r\nfor epoch in range(1000):\r\n\r\n\tone_batch = map(lambda x:prepare_sequence(x), training_data[epoch%len(training_data)])\r\n\tenc_inp = one_batch[0]\r\n\tdec_inp = one_batch[1]\r\n\ttarget = one_batch[2]\r\n\r\n\tmodel.zero_grad()\r\n\ttag_scores = model(enc_inp, dec_inp)\r\n\r\n\tloss = loss_function(tag_scores, target.view(-1))\r\n\tloss.backward()\r\n\toptimizer.step()\r\n\r\n\tprint loss\r\n```"}