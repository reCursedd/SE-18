{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11791", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11791/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11791/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11791/events", "html_url": "https://github.com/pytorch/pytorch/issues/11791", "id": 361169683, "node_id": "MDU6SXNzdWUzNjExNjk2ODM=", "number": 11791, "title": "[caffe2]Can i build caffe2 library only for cpu inference purpose and reduce the binary size?", "user": {"login": "vegetablemao", "id": 1546097, "node_id": "MDQ6VXNlcjE1NDYwOTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1546097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vegetablemao", "html_url": "https://github.com/vegetablemao", "followers_url": "https://api.github.com/users/vegetablemao/followers", "following_url": "https://api.github.com/users/vegetablemao/following{/other_user}", "gists_url": "https://api.github.com/users/vegetablemao/gists{/gist_id}", "starred_url": "https://api.github.com/users/vegetablemao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vegetablemao/subscriptions", "organizations_url": "https://api.github.com/users/vegetablemao/orgs", "repos_url": "https://api.github.com/users/vegetablemao/repos", "events_url": "https://api.github.com/users/vegetablemao/events{/privacy}", "received_events_url": "https://api.github.com/users/vegetablemao/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-18T07:28:40Z", "updated_at": "2018-11-10T15:17:24Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I want to deploy caffe2 on mobile device(including Android &amp; IOS), but i found that the binary size is <strong>too large</strong> for my app. I only use a little operators on my net(convolution and forward calculation), how can I build caffe2 just including the inference operators i wanted to <strong>reduce the size of library</strong>?<br>\nIt's an urgent task for me, if anyone can provide help, i will be appreciate.</p>", "body_text": "I want to deploy caffe2 on mobile device(including Android & IOS), but i found that the binary size is too large for my app. I only use a little operators on my net(convolution and forward calculation), how can I build caffe2 just including the inference operators i wanted to reduce the size of library?\nIt's an urgent task for me, if anyone can provide help, i will be appreciate.", "body": "  I want to deploy caffe2 on mobile device(including Android & IOS), but i found that the binary size is **too large** for my app. I only use a little operators on my net(convolution and forward calculation), how can I build caffe2 just including the inference operators i wanted to **reduce the size of library**?\r\n  It's an urgent task for me, if anyone can provide help, i will be appreciate."}