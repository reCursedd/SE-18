{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/336224083", "html_url": "https://github.com/pytorch/pytorch/issues/3081#issuecomment-336224083", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3081", "id": 336224083, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjIyNDA4Mw==", "user": {"login": "esube", "id": 7385894, "node_id": "MDQ6VXNlcjczODU4OTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/7385894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/esube", "html_url": "https://github.com/esube", "followers_url": "https://api.github.com/users/esube/followers", "following_url": "https://api.github.com/users/esube/following{/other_user}", "gists_url": "https://api.github.com/users/esube/gists{/gist_id}", "starred_url": "https://api.github.com/users/esube/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/esube/subscriptions", "organizations_url": "https://api.github.com/users/esube/orgs", "repos_url": "https://api.github.com/users/esube/repos", "events_url": "https://api.github.com/users/esube/events{/privacy}", "received_events_url": "https://api.github.com/users/esube/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-12T18:23:57Z", "updated_at": "2017-10-12T20:54:24Z", "author_association": "NONE", "body_html": "<p>The code base is large to share. But, I just noticed that a medium network (ResNet34 base network with various modifications) produced the same segmentation fault although the GPUs (2 Titan X) didn't run out of memory. The strange thing is: switching to ResNet50 (used as feature extractor) instead of ResNet34 dies earlier. However, ResNet18 trains and finishes without a hitch. All the three networks use same code base, same dataset, same DataParallel (2 GPUs). It appears that some exceptions are not being handled.</p>\n<p>The machine has an Intel Xeon CPU and Titan X GPUs. It could be related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"264936650\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3089\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3089/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3089\">#3089</a> so I am changing the out of memory error title on this one. For me, pytorch + cuda 8 + cudnn v5.1 was working without this segfault ever. I switch to cuda 9 + cudnn v7 this week and checked pytorch and rebuild it and this started happening.</p>\n<p>[Update]</p>\n<ul>\n<li>\n<p>Also, I noticed (on htop) that quite a lot more threads are spawned in the new setup (cuda 9 + cudnn 7 + latest pytorch) than my previous setup.</p>\n</li>\n<li>\n<p>I tried to run all the three networks on single GPU without DataParallel and to my surprise, all the three are running just fine without the segfault. My data for each minibatch is large and the input size varies randomly in each minibatch using adaptive pooling. Although, the GPU memory comes closer to full capacity on some minibatches, it runs fine on signle GPU that used to throw cuda memory error on two GPUs!</p>\n</li>\n</ul>\n<p>So, at this point, it appears that the segfault is related to DataParallel and I think, it is not related to out of memory or whether the platform was ppc64 (the case of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"264936650\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3089\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3089/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3089\">#3089</a> ) or intel (my case). So, we can merge these two issues as one as the problem is similar.</p>", "body_text": "The code base is large to share. But, I just noticed that a medium network (ResNet34 base network with various modifications) produced the same segmentation fault although the GPUs (2 Titan X) didn't run out of memory. The strange thing is: switching to ResNet50 (used as feature extractor) instead of ResNet34 dies earlier. However, ResNet18 trains and finishes without a hitch. All the three networks use same code base, same dataset, same DataParallel (2 GPUs). It appears that some exceptions are not being handled.\nThe machine has an Intel Xeon CPU and Titan X GPUs. It could be related to #3089 so I am changing the out of memory error title on this one. For me, pytorch + cuda 8 + cudnn v5.1 was working without this segfault ever. I switch to cuda 9 + cudnn v7 this week and checked pytorch and rebuild it and this started happening.\n[Update]\n\n\nAlso, I noticed (on htop) that quite a lot more threads are spawned in the new setup (cuda 9 + cudnn 7 + latest pytorch) than my previous setup.\n\n\nI tried to run all the three networks on single GPU without DataParallel and to my surprise, all the three are running just fine without the segfault. My data for each minibatch is large and the input size varies randomly in each minibatch using adaptive pooling. Although, the GPU memory comes closer to full capacity on some minibatches, it runs fine on signle GPU that used to throw cuda memory error on two GPUs!\n\n\nSo, at this point, it appears that the segfault is related to DataParallel and I think, it is not related to out of memory or whether the platform was ppc64 (the case of #3089 ) or intel (my case). So, we can merge these two issues as one as the problem is similar.", "body": "The code base is large to share. But, I just noticed that a medium network (ResNet34 base network with various modifications) produced the same segmentation fault although the GPUs (2 Titan X) didn't run out of memory. The strange thing is: switching to ResNet50 (used as feature extractor) instead of ResNet34 dies earlier. However, ResNet18 trains and finishes without a hitch. All the three networks use same code base, same dataset, same DataParallel (2 GPUs). It appears that some exceptions are not being handled. \r\n\r\nThe machine has an Intel Xeon CPU and Titan X GPUs. It could be related to #3089 so I am changing the out of memory error title on this one. For me, pytorch + cuda 8 + cudnn v5.1 was working without this segfault ever. I switch to cuda 9 + cudnn v7 this week and checked pytorch and rebuild it and this started happening.\r\n\r\n[Update]\r\n- Also, I noticed (on htop) that quite a lot more threads are spawned in the new setup (cuda 9 + cudnn 7 + latest pytorch) than my previous setup.\r\n\r\n- I tried to run all the three networks on single GPU without DataParallel and to my surprise, all the three are running just fine without the segfault. My data for each minibatch is large and the input size varies randomly in each minibatch using adaptive pooling. Although, the GPU memory comes closer to full capacity on some minibatches, it runs fine on signle GPU that used to throw cuda memory error on two GPUs!\r\n\r\nSo, at this point, it appears that the segfault is related to DataParallel and I think, it is not related to out of memory or whether the platform was ppc64 (the case of #3089 ) or intel (my case). So, we can merge these two issues as one as the problem is similar."}