{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234827414", "pull_request_review_id": 176553377, "id": 234827414, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNDgyNzQxNA==", "diff_hunk": "@@ -90,35 +90,21 @@ class SpatialBNOp : public Operator<Context> {\n           alpha_data,\n           beta_data);\n     } else {\n-      auto* saved_mean = Output(SAVED_MEAN);\n-      auto* saved_rstd = Output(SAVED_INV_STD);\n-      if (num_batches_ == 1) {\n-        saved_mean->Resize(C);\n-        saved_rstd->Resize(C);\n-      } else {\n-        const auto& batch_mean_sum = Input(BATCH_MEAN_SUM);\n-        const auto& batch_var_sum = Input(BATCH_VAR_SUM);\n-        if (saved_mean != &batch_mean_sum) {\n-          saved_mean->Resize(C);\n-        }\n-        if (saved_rstd != &batch_var_sum) {\n-          saved_rstd->Resize(C);\n-        }\n-      }\n+      auto* saved_mean = Output(SAVED_MEAN, {C}, at::dtype<T>());\n+      auto* saved_rstd = Output(SAVED_INV_STD, {C}, at::dtype<T>());\n       T* saved_mean_data = saved_mean->template mutable_data<T>();\n       T* saved_rstd_data = saved_rstd->template mutable_data<T>();\n-      auto* running_mean = Output(RUNNING_MEAN);\n-      auto* running_var = Output(RUNNING_VAR);\n-      if (running_mean->numel() != C) {\n-        running_mean->Resize(C);\n-        math::Set<T, Context>(", "path": "caffe2/operators/spatial_batch_norm_op.h", "position": 27, "original_position": 27, "commit_id": "44a39b74ed6b9315a66f440e45176b3ec7767ab9", "original_commit_id": "cb3dc3e5064f10a0e9e0f3929908d372b3eb929c", "user": {"login": "dzhulgakov", "id": 17890620, "node_id": "MDQ6VXNlcjE3ODkwNjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/17890620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzhulgakov", "html_url": "https://github.com/dzhulgakov", "followers_url": "https://api.github.com/users/dzhulgakov/followers", "following_url": "https://api.github.com/users/dzhulgakov/following{/other_user}", "gists_url": "https://api.github.com/users/dzhulgakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzhulgakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzhulgakov/subscriptions", "organizations_url": "https://api.github.com/users/dzhulgakov/orgs", "repos_url": "https://api.github.com/users/dzhulgakov/repos", "events_url": "https://api.github.com/users/dzhulgakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dzhulgakov/received_events", "type": "User", "site_admin": false}, "body": "So ideally, we shouldn't rely on magic on-the-fly initialization as it can change the size of parameter/buffer. Thus ideally we'd add code here that enforces the size of the output to be correct. Note, that your current refactoring doesn't do that - if the output has the wrong size, we'd silently resize it and not initialize it at all. You can add an explicit ENFORCE on shape for the corresponding Input (as you already rely it being the same blob).\r\n\r\nHowever, I'm a bit worried that there might be existing code relying on this behavior somewhere. The main wrapper (https://github.com/pytorch/pytorch/blob/master/caffe2/python/helpers/normalization.py#L129) does handle things correctly, but maybe there are other places.\r\n\r\nTo be safe - we can bring back the comparison / initialization code and add LOG_EVERY_MS() log to inform users that it's a broken behavior. You can check the size of input first as it's guaranteed that the output is the same.", "created_at": "2018-11-20T00:03:26Z", "updated_at": "2018-11-23T15:55:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/13856#discussion_r234827414", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13856", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234827414"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13856#discussion_r234827414"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13856"}}, "body_html": "<p>So ideally, we shouldn't rely on magic on-the-fly initialization as it can change the size of parameter/buffer. Thus ideally we'd add code here that enforces the size of the output to be correct. Note, that your current refactoring doesn't do that - if the output has the wrong size, we'd silently resize it and not initialize it at all. You can add an explicit ENFORCE on shape for the corresponding Input (as you already rely it being the same blob).</p>\n<p>However, I'm a bit worried that there might be existing code relying on this behavior somewhere. The main wrapper (<a href=\"https://github.com/pytorch/pytorch/blob/master/caffe2/python/helpers/normalization.py#L129\">https://github.com/pytorch/pytorch/blob/master/caffe2/python/helpers/normalization.py#L129</a>) does handle things correctly, but maybe there are other places.</p>\n<p>To be safe - we can bring back the comparison / initialization code and add LOG_EVERY_MS() log to inform users that it's a broken behavior. You can check the size of input first as it's guaranteed that the output is the same.</p>", "body_text": "So ideally, we shouldn't rely on magic on-the-fly initialization as it can change the size of parameter/buffer. Thus ideally we'd add code here that enforces the size of the output to be correct. Note, that your current refactoring doesn't do that - if the output has the wrong size, we'd silently resize it and not initialize it at all. You can add an explicit ENFORCE on shape for the corresponding Input (as you already rely it being the same blob).\nHowever, I'm a bit worried that there might be existing code relying on this behavior somewhere. The main wrapper (https://github.com/pytorch/pytorch/blob/master/caffe2/python/helpers/normalization.py#L129) does handle things correctly, but maybe there are other places.\nTo be safe - we can bring back the comparison / initialization code and add LOG_EVERY_MS() log to inform users that it's a broken behavior. You can check the size of input first as it's guaranteed that the output is the same.", "in_reply_to_id": 232865374}