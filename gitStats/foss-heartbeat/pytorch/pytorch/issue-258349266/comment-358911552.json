{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358911552", "html_url": "https://github.com/pytorch/pytorch/pull/2764#issuecomment-358911552", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2764", "id": 358911552, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODkxMTU1Mg==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-19T09:26:46Z", "updated_at": "2018-01-19T09:26:46Z", "author_association": "MEMBER", "body_html": "<p>About point 2, don't worry about it. The previous behaviour was not the expected anyway, I was just documenting it here. No need to look at it further.</p>\n<p>About point 1, you are right, the problem seems to happen when the output and the input overlap storage and have zero stride.</p>\n<p>You know what, I have just found out that our CUDA kernels in case 1 have the right behaviour (outputs 1 instead of 10000), which means that the CPU and the CUDA code do not return the same result already in this situation.</p>\n<p>Maybe then in this case it might not be worth it trying to keep compatibility with the old (and somewhat wrong) CPU behaviour, and we could maybe just merge this in as is. What do you think? cc. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>", "body_text": "About point 2, don't worry about it. The previous behaviour was not the expected anyway, I was just documenting it here. No need to look at it further.\nAbout point 1, you are right, the problem seems to happen when the output and the input overlap storage and have zero stride.\nYou know what, I have just found out that our CUDA kernels in case 1 have the right behaviour (outputs 1 instead of 10000), which means that the CPU and the CUDA code do not return the same result already in this situation.\nMaybe then in this case it might not be worth it trying to keep compatibility with the old (and somewhat wrong) CPU behaviour, and we could maybe just merge this in as is. What do you think? cc. @soumith", "body": "About point 2, don't worry about it. The previous behaviour was not the expected anyway, I was just documenting it here. No need to look at it further.\r\n\r\nAbout point 1, you are right, the problem seems to happen when the output and the input overlap storage and have zero stride.\r\n\r\nYou know what, I have just found out that our CUDA kernels in case 1 have the right behaviour (outputs 1 instead of 10000), which means that the CPU and the CUDA code do not return the same result already in this situation.\r\n\r\nMaybe then in this case it might not be worth it trying to keep compatibility with the old (and somewhat wrong) CPU behaviour, and we could maybe just merge this in as is. What do you think? cc. @soumith "}