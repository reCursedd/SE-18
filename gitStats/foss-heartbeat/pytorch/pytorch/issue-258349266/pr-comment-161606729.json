{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/161606729", "pull_request_review_id": 88898808, "id": 161606729, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MTYwNjcyOQ==", "diff_hunk": "@@ -235,4 +245,238 @@\n #define TH_TENSOR_APPLY(TYPE, TENSOR, CODE) \\\n   TH_TENSOR_APPLY_D(TYPE, TENSOR, -1, CODE)\n \n+\n+#ifdef _OPENMP\n+\n+#ifndef _WIN32\n+#define PRAGMA(P) _Pragma(#P)\n+#else\n+#define PRAGMA(P) __pragma(P)\n+#endif\n+\n+#define TH_OMP_OVERHEAD_THRESHOLD_OMP 5000\n+#include <omp.h>\n+#include <x86intrin.h>\n+\n+#define __TH_TENSOR_APPLYX_CAL_OFFSET(TENSOR) \\\n+  int64_t *TENSOR##_counter_tmp = (int64_t*)THAlloc(sizeof(int64_t) * TENSOR##_dim);\\\n+  ptrdiff_t TENSOR##_offset = 0;                                                                \\\n+  ptrdiff_t TENSOR##_quot = line_index_offset;                                                           \\\n+  for (TENSOR##_i = TENSOR##_dim-1; TENSOR##_i>=0; --TENSOR##_i) {                              \\\n+    TENSOR##_counter_tmp[TENSOR##_i] = TENSOR##_quot%TENSOR##_sizes[TENSOR##_i];                         \\\n+    TENSOR##_quot /= TENSOR##_sizes[TENSOR##_i];                                                         \\\n+    TENSOR##_offset += TENSOR##_counter_tmp[TENSOR##_i] * TENSOR##_strides[TENSOR##_i];         \\\n+  }\n+\n+#define __TH_TENSOR_APPLYX_UPDATE_COUNTERS_OMP(TENSOR) \\\n+  if(TENSOR##_i == TENSOR##_size && TENSOR##_dim > 1){ \\\n+    int TENSOR##_carry_coord = 1; \\\n+    TENSOR##_data -= TENSOR##_size * TENSOR##_stride; \\\n+    TENSOR##_start = 0;                               \\\n+    for(TENSOR##_i = TENSOR##_dim - 2; (TENSOR##_i >= 0) && (TENSOR##_carry_coord); TENSOR##_i--){ \\\n+      TENSOR##_counter_tmp[TENSOR##_i]++; \\\n+      TENSOR##_data += TENSOR##_strides[TENSOR##_i]; \\\n+      if(TENSOR##_counter_tmp[TENSOR##_i] == TENSOR##_sizes[TENSOR##_i]){ \\\n+        TENSOR##_data -= TENSOR##_sizes[TENSOR##_i] * TENSOR##_strides[TENSOR##_i]; \\\n+        TENSOR##_counter_tmp[TENSOR##_i] = 0; \\\n+      } else { \\\n+        TENSOR##_carry_coord = 0; \\\n+      } \\\n+    } \\\n+  } else { \\\n+    TENSOR##_start = TENSOR##_i;                               \\\n+  }\n+\n+\n+#define TH_TENSOR_APPLY_REDUCTION_OMP(TYPE, TENSOR, OPERATION, CODE) \\\n+{\\\n+  TYPE *rp = TENSOR->storage->data+TENSOR->storageOffset;                   \\\n+  int TENSOR##Contg = THTensor_(isContiguous)(TENSOR);                      \\\n+  ptrdiff_t TENSOR##Size = THTensor_(nElement)(TENSOR);                     \\\n+  ptrdiff_t iter = 0;                                                         \\\n+  if(TENSOR##Contg){                                                         \\\n+    TYPE *TENSOR##_data = NULL;         \\\n+    PRAGMA( omp parallel for if (TENSOR##Size > TH_OMP_OVERHEAD_THRESHOLD_OMP) private(TENSOR##_data,  iter) reduction(OPERATION) ) \\\n+    for (iter = 0; iter < TENSOR##Size; iter++) { \\\n+      TENSOR##_data = rp+iter;                    \\\n+      CODE                                         \\\n+    }                                              \\\n+  } else {                                         \\\n+    int TH_TENSOR_APPLY_hasFinished = 0;           \\\n+    int64_t TH_TENSOR_dim_index = 0;               \\\n+    __TH_TENSOR_APPLYX_PREAMBLE(TYPE, TENSOR, -1, 1);\\\n+    PRAGMA(omp parallel if (TENSOR##Size > TH_OMP_OVERHEAD_THRESHOLD_OMP) firstprivate(TENSOR##_sizes, TENSOR##_strides, TENSOR##_dim, TENSOR##_stride, TENSOR##_size, TENSOR##_i) reduction(OPERATION))\\\n+    {\\\n+      size_t num_threads = omp_get_num_threads();\\\n+      size_t tid = omp_get_thread_num();\\\n+      ptrdiff_t line_index_offset = tid * (TENSOR##Size/num_threads);\\\n+      ptrdiff_t line_index_end = (tid == num_threads - 1) ? TENSOR##Size:(line_index_offset + TENSOR##Size/num_threads);\\\n+      ptrdiff_t line_seg_len = line_index_end - line_index_offset;\\\n+      __TH_TENSOR_APPLYX_CAL_OFFSET(TENSOR);\\\n+      TYPE *TENSOR##_data = rp + TENSOR##_offset;\\\n+      ptrdiff_t count = 0;\\\n+      ptrdiff_t TENSOR##_start = TENSOR##_counter_tmp[TENSOR##_dim - 1];\\\n+      while(count < line_seg_len){\\\n+        for(TENSOR##_i=TENSOR##_start; (count < line_seg_len)&&(TENSOR##_i < TENSOR##_size); ++TENSOR##_i, ++count){\\\n+          CODE\\\n+          TENSOR##_data += TENSOR##_stride;\\\n+        }\\\n+        if(count < line_seg_len){\\\n+          __TH_TENSOR_APPLYX_UPDATE_COUNTERS_OMP(TENSOR);\\\n+        }\\\n+      }\\\n+      if(TENSOR##_counter_tmp != NULL) \\\n+        THFree(TENSOR##_counter_tmp); \\\n+    }\\\n+    if(TENSOR##_counter != NULL)\\\n+      THFree(TENSOR##_counter);\\\n+  }\\\n+}\n+#define TH_TENSOR_APPLY2_OMP(SIZE, CONTIG1, CONTIG2, TYPE1, TENSOR1, TYPE2, TENSOR2, CODE) \\\n+{                                                                                              \\\n+  /* for advanced searching index*/                                                            \\\n+  TYPE1 *rp = TENSOR1->storage->data+TENSOR1->storageOffset;                                    \\\n+  TYPE2 *tp = TENSOR2->storage->data+TENSOR2->storageOffset;                                    \\\n+  if( CONTIG1 && CONTIG2 ){                                                                    \\\n+    ptrdiff_t iter = 0;                                                                        \\\n+    if(tp != rp) {                                                                             \\\n+      PRAGMA( omp parallel for if (SIZE > TH_OMP_OVERHEAD_THRESHOLD_OMP) firstprivate(rp, tp)) \\\n+      PRAGMA(ivdep) \\\n+      for (iter = 0; iter < SIZE; iter++) {                             \\\n+        TYPE2 *TENSOR2##_data = tp+iter;                                \\\n+        TYPE1 *TENSOR1##_data = rp+iter;                                \\\n+        CODE                                                            \\\n+      }\\\n+    } else {\\\n+      PRAGMA( omp parallel for if (SIZE > TH_OMP_OVERHEAD_THRESHOLD_OMP) firstprivate(rp, tp) )  \\\n+      PRAGMA(simd) \\\n+      for (iter = 0; iter < SIZE; iter++) {\\\n+        TYPE2* TENSOR2##_data = tp+iter;\\\n+        TYPE1* TENSOR1##_data = rp+iter;\\\n+        CODE                                \\\n+      }\\\n+    }\\\n+  } else {              \\\n+    int TH_TENSOR_APPLY_hasFinished = 0; \\\n+    int64_t TH_TENSOR_dim_index = 0; \\\n+    __TH_TENSOR_APPLYX_PREAMBLE(TYPE2, TENSOR2, -1, 1) \\\n+    __TH_TENSOR_APPLYX_PREAMBLE(TYPE1, TENSOR1, -1, 1) \\\n+    PRAGMA(omp parallel if (SIZE > TH_OMP_OVERHEAD_THRESHOLD_OMP) firstprivate(TENSOR2##_sizes, TENSOR2##_strides, TENSOR2##_dim, TENSOR2##_stride, TENSOR2##_size, TENSOR2##_i, TENSOR1##_sizes, TENSOR1##_strides, TENSOR1##_dim, TENSOR1##_stride, TENSOR1##_size, TENSOR1##_i)) \\\n+    {                                                                                                    \\\n+      size_t num_threads = omp_get_num_threads();                                                        \\\n+      size_t tid = omp_get_thread_num();                                                                 \\\n+      ptrdiff_t line_index_offset = tid * (SIZE/num_threads);                                            \\\n+      ptrdiff_t line_index_end = (tid == num_threads - 1) ? SIZE:(line_index_offset + SIZE/num_threads); \\\n+      ptrdiff_t line_seg_len = line_index_end - line_index_offset;                                       \\\n+      /*divided segment maybe does not cover the size of the last contiguous dimension*/                 \\\n+      /*calculate each coord of the preamble tensor*/                                                    \\\n+      __TH_TENSOR_APPLYX_CAL_OFFSET(TENSOR2);                                                            \\\n+      __TH_TENSOR_APPLYX_CAL_OFFSET(TENSOR1);                                                            \\\n+      TYPE2 *TENSOR2##_data = tp + TENSOR2##_offset;                                                     \\\n+      TYPE1 *TENSOR1##_data = rp + TENSOR1##_offset;                                                     \\\n+      ptrdiff_t count = 0;                                                                               \\\n+      ptrdiff_t TENSOR2##_start =  TENSOR2##_counter_tmp[TENSOR2##_dim-1];                               \\\n+      ptrdiff_t TENSOR1##_start =  TENSOR1##_counter_tmp[TENSOR1##_dim-1];                               \\\n+      while (count < line_seg_len) {                                                                     \\\n+        for(TENSOR2##_i=TENSOR2##_start, TENSOR1##_i = TENSOR1##_start; ((count < line_seg_len) && (TENSOR2##_i < TENSOR2##_size) && (TENSOR1##_i < TENSOR1##_size)); ++TENSOR2##_i, ++TENSOR1##_i, ++count){ \\\n+          CODE                                                                                               \\\n+          TENSOR2##_data += TENSOR2##_stride;                                                                \\\n+          TENSOR1##_data += TENSOR1##_stride;                                                                \\\n+        }                                                                                                    \\\n+        if (count < line_seg_len){                                                                           \\", "path": "aten/src/TH/THTensorApply.h", "position": null, "original_position": 159, "commit_id": "1ccad046aab11cd78eab647a18fa3713bb9cfd6f", "original_commit_id": "7dcd48c85d05ca5989967a09fe76118bde6f1db9", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "A comment here mentioning that the iteration index usually decreases once reaches this part might be helpful as well.", "created_at": "2018-01-15T20:10:02Z", "updated_at": "2018-11-23T15:38:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/2764#discussion_r161606729", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2764", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/161606729"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2764#discussion_r161606729"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2764"}}, "body_html": "<p>A comment here mentioning that the iteration index usually decreases once reaches this part might be helpful as well.</p>", "body_text": "A comment here mentioning that the iteration index usually decreases once reaches this part might be helpful as well."}