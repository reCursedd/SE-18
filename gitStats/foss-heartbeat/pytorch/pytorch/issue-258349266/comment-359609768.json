{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/359609768", "html_url": "https://github.com/pytorch/pytorch/pull/2764#issuecomment-359609768", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2764", "id": 359609768, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTYwOTc2OA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-22T23:28:44Z", "updated_at": "2018-01-22T23:28:44Z", "author_association": "MEMBER", "body_html": "<p>Ok, I had a closer look into the reason why numpy was showing incredibly good performances using a single thread compared to our multi-threaded implementation, and I think I have found the reason.</p>\n<p>Indeed, contrary to pytorch, the result of numpy operations on non-contiguous arrays might return non-contiguous arrays.<br>\nFor example</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">300</span>, <span class=\"pl-c1\">300</span>, <span class=\"pl-c1\">1000</span>).permute(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>)\nan <span class=\"pl-k\">=</span> a.numpy()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> to check</span>\n<span class=\"pl-c1\">print</span>(a.stride())\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (1000L, 300000L, 1L)</span>\n<span class=\"pl-c1\">print</span>(an.strides)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (4000, 1200000, 4)</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> now let's perform some operations</span>\n<span class=\"pl-c1\">print</span>((a <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>).stride())\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> gives (300000L, 1000L, 1L), a contiguous tensor</span>\n<span class=\"pl-c1\">print</span>((an <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>).strides)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> gives (4000, 1200000, 4), the same as an</span></pre></div>\n<p>For contiguous tensors, in my small tests the performance of pytorch was on par with numpy in the single threaded case (as both seem to leverage SIMD instructions), including for operations like <code>log</code> and <code>exp</code>.</p>\n<p>Since the beginning the contract in pytorch (and lua torch) was that (almost) all operations return a contiguous tensor, even if the inputs are non-contiguous. This doesn't seem to be the case, and lead to simple benchmarks leaning towards numpy being much faster.</p>\n<p>The question is, in real pipelines with lots of operations, is there a value in keeping the original strides of the tensor \u00e0 la numpy, in order to get better runtimes? Does it change the result in a significant manner?</p>", "body_text": "Ok, I had a closer look into the reason why numpy was showing incredibly good performances using a single thread compared to our multi-threaded implementation, and I think I have found the reason.\nIndeed, contrary to pytorch, the result of numpy operations on non-contiguous arrays might return non-contiguous arrays.\nFor example\na = torch.rand(300, 300, 1000).permute(1, 0, 2)\nan = a.numpy()\n\n# to check\nprint(a.stride())\n# (1000L, 300000L, 1L)\nprint(an.strides)\n# (4000, 1200000, 4)\n\n# now let's perform some operations\nprint((a * 2).stride())\n# gives (300000L, 1000L, 1L), a contiguous tensor\nprint((an * 2).strides)\n# gives (4000, 1200000, 4), the same as an\nFor contiguous tensors, in my small tests the performance of pytorch was on par with numpy in the single threaded case (as both seem to leverage SIMD instructions), including for operations like log and exp.\nSince the beginning the contract in pytorch (and lua torch) was that (almost) all operations return a contiguous tensor, even if the inputs are non-contiguous. This doesn't seem to be the case, and lead to simple benchmarks leaning towards numpy being much faster.\nThe question is, in real pipelines with lots of operations, is there a value in keeping the original strides of the tensor \u00e0 la numpy, in order to get better runtimes? Does it change the result in a significant manner?", "body": "Ok, I had a closer look into the reason why numpy was showing incredibly good performances using a single thread compared to our multi-threaded implementation, and I think I have found the reason.\r\n\r\nIndeed, contrary to pytorch, the result of numpy operations on non-contiguous arrays might return non-contiguous arrays.\r\nFor example\r\n```python\r\na = torch.rand(300, 300, 1000).permute(1, 0, 2)\r\nan = a.numpy()\r\n\r\n# to check\r\nprint(a.stride())\r\n# (1000L, 300000L, 1L)\r\nprint(an.strides)\r\n# (4000, 1200000, 4)\r\n\r\n# now let's perform some operations\r\nprint((a * 2).stride())\r\n# gives (300000L, 1000L, 1L), a contiguous tensor\r\nprint((an * 2).strides)\r\n# gives (4000, 1200000, 4), the same as an\r\n```\r\n\r\nFor contiguous tensors, in my small tests the performance of pytorch was on par with numpy in the single threaded case (as both seem to leverage SIMD instructions), including for operations like `log` and `exp`.\r\n\r\nSince the beginning the contract in pytorch (and lua torch) was that (almost) all operations return a contiguous tensor, even if the inputs are non-contiguous. This doesn't seem to be the case, and lead to simple benchmarks leaning towards numpy being much faster.\r\n\r\nThe question is, in real pipelines with lots of operations, is there a value in keeping the original strides of the tensor \u00e0 la numpy, in order to get better runtimes? Does it change the result in a significant manner? "}