{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358811159", "html_url": "https://github.com/pytorch/pytorch/pull/2764#issuecomment-358811159", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2764", "id": 358811159, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODgxMTE1OQ==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-18T23:00:46Z", "updated_at": "2018-01-18T23:00:46Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20226293\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/MlWoo\">@MlWoo</a><br>\nOnce again thanks for this awesome PR!</p>\n<p>I had another look at your PR, I compiled your code locally and I think this is <em>almost</em> good to be merged.</p>\n<p>Here are my remarks</p>\n<p><strong>1 -</strong> I believe there is a small problem with concurrent writes in some edge cases.<br>\nMore precisely, when the <strong>output</strong> tensor has zero strides, the output result does not agree with the previous implementation.</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>).expand(<span class=\"pl-c1\">10000</span>)\na <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">print</span>(a[<span class=\"pl-c1\">0</span>]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> should print 10000, but it doesn't</span></pre></div>\n<p><strong>2 -</strong> This one is not important, but another edge-case (which didn't give the right answer before) is this one <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"211558366\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/906\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/906/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/906\">#906</a>. The current code produces a different output than before, even though the answer before was not the right one either (but I'm documenting here for completeness).</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.range(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10000</span>)\na[<span class=\"pl-c1\">1</span>:] <span class=\"pl-k\">=</span> a[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n<span class=\"pl-c1\">print</span>(a[:<span class=\"pl-c1\">10</span>])</pre></div>\n<p>gives</p>\n<pre><code> 1\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 8\n[torch.FloatTensor of size 10]\n</code></pre>\n<p>while</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> old codepath</span>\nb <span class=\"pl-k\">=</span> torch.range(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">11</span>)\nb[<span class=\"pl-c1\">1</span>:] <span class=\"pl-k\">=</span> b[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n<span class=\"pl-c1\">print</span>(b[:<span class=\"pl-c1\">10</span>])</pre></div>\n<p>gives</p>\n<pre><code> 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n[torch.FloatTensor of size 10]\n</code></pre>\n<p>Apart from those tiny edge-cases, all the other cases I tested look good.</p>\n<p>I believe there is nothing we can do about point 2, and I don't think it deserves much attention (it didn't give the expected answer before anyway :-) ).</p>\n<p>But I believe it would be good to fix point 1.<br>\nI think the necessary fix here is to disable OMP when the output tensor has zero stride (because concurrent writes are not OK, but concurrent reads are OK to the best of my knowledge).<br>\nYou might want to recover your previous <code>hasZeroStride</code> function, but apply it only on the output tensor.</p>\n<p>One annoyance is that the <code>TH_TENSOR_APPLY_XXX</code> macros don't enforce any order in the arguments (so the output tensor could be either <code>TENSOR1</code> or <code>TENSOR2</code>, depending on how the user implemented it in his calling function). Thus, we might need to have a per-function check that disables the OMP path if the output tensor is not contiguous, but this doesn't look great<br>\nAnother option, which was what you had initially done, is to disable OMP whenever <em>any</em> of the tensors has zero stride. But concurrent reads work OK and we would lose some performance in those cases.</p>\n<p>What do you think? Would you mind applying this fix?</p>\n<p>Thanks!</p>", "body_text": "Hi @MlWoo\nOnce again thanks for this awesome PR!\nI had another look at your PR, I compiled your code locally and I think this is almost good to be merged.\nHere are my remarks\n1 - I believe there is a small problem with concurrent writes in some edge cases.\nMore precisely, when the output tensor has zero strides, the output result does not agree with the previous implementation.\na = torch.zeros(1).expand(10000)\na += 1\nprint(a[0]) # should print 10000, but it doesn't\n2 - This one is not important, but another edge-case (which didn't give the right answer before) is this one #906. The current code produces a different output than before, even though the answer before was not the right one either (but I'm documenting here for completeness).\na = torch.range(1, 10000)\na[1:] = a[:-1]\nprint(a[:10])\ngives\n 1\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 8\n[torch.FloatTensor of size 10]\n\nwhile\n# old codepath\nb = torch.range(1, 11)\nb[1:] = b[:-1]\nprint(b[:10])\ngives\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n 1\n[torch.FloatTensor of size 10]\n\nApart from those tiny edge-cases, all the other cases I tested look good.\nI believe there is nothing we can do about point 2, and I don't think it deserves much attention (it didn't give the expected answer before anyway :-) ).\nBut I believe it would be good to fix point 1.\nI think the necessary fix here is to disable OMP when the output tensor has zero stride (because concurrent writes are not OK, but concurrent reads are OK to the best of my knowledge).\nYou might want to recover your previous hasZeroStride function, but apply it only on the output tensor.\nOne annoyance is that the TH_TENSOR_APPLY_XXX macros don't enforce any order in the arguments (so the output tensor could be either TENSOR1 or TENSOR2, depending on how the user implemented it in his calling function). Thus, we might need to have a per-function check that disables the OMP path if the output tensor is not contiguous, but this doesn't look great\nAnother option, which was what you had initially done, is to disable OMP whenever any of the tensors has zero stride. But concurrent reads work OK and we would lose some performance in those cases.\nWhat do you think? Would you mind applying this fix?\nThanks!", "body": "Hi @MlWoo \r\nOnce again thanks for this awesome PR!\r\n\r\nI had another look at your PR, I compiled your code locally and I think this is *almost* good to be merged.\r\n\r\nHere are my remarks\r\n\r\n**1 -** I believe there is a small problem with concurrent writes in some edge cases.\r\nMore precisely, when the **output** tensor has zero strides, the output result does not agree with the previous implementation.\r\n```python\r\na = torch.zeros(1).expand(10000)\r\na += 1\r\nprint(a[0]) # should print 10000, but it doesn't\r\n```\r\n\r\n**2 -** This one is not important, but another edge-case (which didn't give the right answer before) is this one https://github.com/pytorch/pytorch/issues/906. The current code produces a different output than before, even though the answer before was not the right one either (but I'm documenting here for completeness).\r\n\r\n```python\r\na = torch.range(1, 10000)\r\na[1:] = a[:-1]\r\nprint(a[:10])\r\n```\r\ngives\r\n```\r\n 1\r\n 1\r\n 2\r\n 3\r\n 4\r\n 5\r\n 6\r\n 7\r\n 8\r\n 8\r\n[torch.FloatTensor of size 10]\r\n```\r\n\r\nwhile\r\n```python\r\n# old codepath\r\nb = torch.range(1, 11)\r\nb[1:] = b[:-1]\r\nprint(b[:10])\r\n```\r\ngives\r\n```\r\n 1\r\n 1\r\n 1\r\n 1\r\n 1\r\n 1\r\n 1\r\n 1\r\n 1\r\n 1\r\n[torch.FloatTensor of size 10]\r\n```\r\n\r\nApart from those tiny edge-cases, all the other cases I tested look good.\r\n\r\nI believe there is nothing we can do about point 2, and I don't think it deserves much attention (it didn't give the expected answer before anyway :-) ).\r\n\r\nBut I believe it would be good to fix point 1.\r\nI think the necessary fix here is to disable OMP when the output tensor has zero stride (because concurrent writes are not OK, but concurrent reads are OK to the best of my knowledge).\r\nYou might want to recover your previous `hasZeroStride` function, but apply it only on the output tensor.\r\n\r\nOne annoyance is that the `TH_TENSOR_APPLY_XXX` macros don't enforce any order in the arguments (so the output tensor could be either `TENSOR1` or `TENSOR2`, depending on how the user implemented it in his calling function). Thus, we might need to have a per-function check that disables the OMP path if the output tensor is not contiguous, but this doesn't look great\r\nAnother option, which was what you had initially done, is to disable OMP whenever *any* of the tensors has zero stride. But concurrent reads work OK and we would lose some performance in those cases.\r\n\r\nWhat do you think? Would you mind applying this fix?\r\n\r\nThanks!"}