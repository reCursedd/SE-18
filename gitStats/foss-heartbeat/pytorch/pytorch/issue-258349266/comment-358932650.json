{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358932650", "html_url": "https://github.com/pytorch/pytorch/pull/2764#issuecomment-358932650", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2764", "id": 358932650, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODkzMjY1MA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-19T10:53:04Z", "updated_at": "2018-01-19T10:53:04Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <code>THTensor_(sum)</code> won't be affected by this PR I believe. And the behavior in the GPU is currently not compatible with CPU I believe, so I don't think we should rely on it.</p>\n<p>On a somewhat unrelated topic, yesterday I did some quick benchmark tests comparing against numpy, and although numpy didn't seem to use multiple threads on the CPU, the runtime performances were still very competitive to our threaded implementation provided by this PR.<br>\nI'll try to have a look to see how they managed to get such performances. Maybe they perform some dimension reordering during the computations to maximize cache locality?</p>", "body_text": "@apaszke THTensor_(sum) won't be affected by this PR I believe. And the behavior in the GPU is currently not compatible with CPU I believe, so I don't think we should rely on it.\nOn a somewhat unrelated topic, yesterday I did some quick benchmark tests comparing against numpy, and although numpy didn't seem to use multiple threads on the CPU, the runtime performances were still very competitive to our threaded implementation provided by this PR.\nI'll try to have a look to see how they managed to get such performances. Maybe they perform some dimension reordering during the computations to maximize cache locality?", "body": "@apaszke `THTensor_(sum)` won't be affected by this PR I believe. And the behavior in the GPU is currently not compatible with CPU I believe, so I don't think we should rely on it.\r\n\r\nOn a somewhat unrelated topic, yesterday I did some quick benchmark tests comparing against numpy, and although numpy didn't seem to use multiple threads on the CPU, the runtime performances were still very competitive to our threaded implementation provided by this PR.\r\nI'll try to have a look to see how they managed to get such performances. Maybe they perform some dimension reordering during the computations to maximize cache locality?"}