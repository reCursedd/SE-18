{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/358843156", "html_url": "https://github.com/pytorch/pytorch/pull/2764#issuecomment-358843156", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2764", "id": 358843156, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODg0MzE1Ng==", "user": {"login": "MlWoo", "id": 20226293, "node_id": "MDQ6VXNlcjIwMjI2Mjkz", "avatar_url": "https://avatars2.githubusercontent.com/u/20226293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MlWoo", "html_url": "https://github.com/MlWoo", "followers_url": "https://api.github.com/users/MlWoo/followers", "following_url": "https://api.github.com/users/MlWoo/following{/other_user}", "gists_url": "https://api.github.com/users/MlWoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/MlWoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MlWoo/subscriptions", "organizations_url": "https://api.github.com/users/MlWoo/orgs", "repos_url": "https://api.github.com/users/MlWoo/repos", "events_url": "https://api.github.com/users/MlWoo/events{/privacy}", "received_events_url": "https://api.github.com/users/MlWoo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-19T01:57:52Z", "updated_at": "2018-01-19T05:29:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a>  Thanks a lot. In terms of point 1, the expected output should be 1. I think it is a typo. The error happens in the edge case only when the dst and src share memory.  In that situation, concurrent write will lead to the problem.  I found the result is right when dst and src do not share memory.</p>\n<pre><code>&gt;&gt;&gt; b = torch.zeros(1).expand(100000)\n&gt;&gt;&gt; c =  b + 1\n&gt;&gt;&gt; c\n\n 1\n 1\n 1\n\u22ee\n 1\n 1\n 1\n[torch.FloatTensor of size 100000]\n</code></pre>\n<p>In another circumstances, dst do not share memory with src but has zero stride. But the result is OK because torch allocates a new  completed memory to dst.</p>\n<pre><code>&gt;&gt;&gt; b = torch.zeros(1).expand(100000)\n&gt;&gt;&gt; a = torch.zeros(1).expand(100000)\n&gt;&gt;&gt; b =  a + 1\n&gt;&gt;&gt; b\n\n 1\n 1\n 1\n\u22ee\n 1\n 1\n 1\n[torch.FloatTensor of size 100000]\n\n&gt;&gt;&gt; b.is_contiguous()\nTrue\n\n</code></pre>\n<p>So I think we can check both the memory address and <code>hasZeroStride</code> porperty to avoid using the macro.  It will be easier to handle with the tricky occasion.  What do you think?  I will fix it ASAP.</p>\n<p>There will be another sequential modification to improve performance in the next commit. It will pass the threshold  parameters to the macro according to complexity of operations.  Is it OK?</p>\n<p>I have glanced at point 2, the result is totally dependent on the implementation in C code. In other words, the read and write sequence will determine the output. If PyTorch has clear definition on the indexing operation later, I will follow it. I have no time to invetigate point 2 furtherly for the moment<br>\nbecause I have to enable ICC compilation in pytorch.</p>", "body_text": "@fmassa  Thanks a lot. In terms of point 1, the expected output should be 1. I think it is a typo. The error happens in the edge case only when the dst and src share memory.  In that situation, concurrent write will lead to the problem.  I found the result is right when dst and src do not share memory.\n>>> b = torch.zeros(1).expand(100000)\n>>> c =  b + 1\n>>> c\n\n 1\n 1\n 1\n\u22ee\n 1\n 1\n 1\n[torch.FloatTensor of size 100000]\n\nIn another circumstances, dst do not share memory with src but has zero stride. But the result is OK because torch allocates a new  completed memory to dst.\n>>> b = torch.zeros(1).expand(100000)\n>>> a = torch.zeros(1).expand(100000)\n>>> b =  a + 1\n>>> b\n\n 1\n 1\n 1\n\u22ee\n 1\n 1\n 1\n[torch.FloatTensor of size 100000]\n\n>>> b.is_contiguous()\nTrue\n\n\nSo I think we can check both the memory address and hasZeroStride porperty to avoid using the macro.  It will be easier to handle with the tricky occasion.  What do you think?  I will fix it ASAP.\nThere will be another sequential modification to improve performance in the next commit. It will pass the threshold  parameters to the macro according to complexity of operations.  Is it OK?\nI have glanced at point 2, the result is totally dependent on the implementation in C code. In other words, the read and write sequence will determine the output. If PyTorch has clear definition on the indexing operation later, I will follow it. I have no time to invetigate point 2 furtherly for the moment\nbecause I have to enable ICC compilation in pytorch.", "body": "@fmassa  Thanks a lot. In terms of point 1, the expected output should be 1. I think it is a typo. The error happens in the edge case only when the dst and src share memory.  In that situation, concurrent write will lead to the problem.  I found the result is right when dst and src do not share memory. \r\n```\r\n>>> b = torch.zeros(1).expand(100000)\r\n>>> c =  b + 1\r\n>>> c\r\n\r\n 1\r\n 1\r\n 1\r\n\u22ee\r\n 1\r\n 1\r\n 1\r\n[torch.FloatTensor of size 100000]\r\n```\r\nIn another circumstances, dst do not share memory with src but has zero stride. But the result is OK because torch allocates a new  completed memory to dst.\r\n```\r\n>>> b = torch.zeros(1).expand(100000)\r\n>>> a = torch.zeros(1).expand(100000)\r\n>>> b =  a + 1\r\n>>> b\r\n\r\n 1\r\n 1\r\n 1\r\n\u22ee\r\n 1\r\n 1\r\n 1\r\n[torch.FloatTensor of size 100000]\r\n\r\n>>> b.is_contiguous()\r\nTrue\r\n\r\n```\r\n So I think we can check both the memory address and `hasZeroStride` porperty to avoid using the macro.  It will be easier to handle with the tricky occasion.  What do you think?  I will fix it ASAP.\r\n\r\nThere will be another sequential modification to improve performance in the next commit. It will pass the threshold  parameters to the macro according to complexity of operations.  Is it OK?\r\n\r\nI have glanced at point 2, the result is totally dependent on the implementation in C code. In other words, the read and write sequence will determine the output. If PyTorch has clear definition on the indexing operation later, I will follow it. I have no time to invetigate point 2 furtherly for the moment \r\n because I have to enable ICC compilation in pytorch. "}