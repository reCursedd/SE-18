{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/389938119", "html_url": "https://github.com/pytorch/pytorch/issues/7620#issuecomment-389938119", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7620", "id": 389938119, "node_id": "MDEyOklzc3VlQ29tbWVudDM4OTkzODExOQ==", "user": {"login": "emilmelnikov", "id": 1649961, "node_id": "MDQ6VXNlcjE2NDk5NjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1649961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emilmelnikov", "html_url": "https://github.com/emilmelnikov", "followers_url": "https://api.github.com/users/emilmelnikov/followers", "following_url": "https://api.github.com/users/emilmelnikov/following{/other_user}", "gists_url": "https://api.github.com/users/emilmelnikov/gists{/gist_id}", "starred_url": "https://api.github.com/users/emilmelnikov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emilmelnikov/subscriptions", "organizations_url": "https://api.github.com/users/emilmelnikov/orgs", "repos_url": "https://api.github.com/users/emilmelnikov/repos", "events_url": "https://api.github.com/users/emilmelnikov/events{/privacy}", "received_events_url": "https://api.github.com/users/emilmelnikov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-17T17:02:00Z", "updated_at": "2018-05-17T17:02:00Z", "author_association": "NONE", "body_html": "<p>Yes, it's assigned in the <code>__init__</code> now:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">in_features</span>, <span class=\"pl-smi\">out_features</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-c1\">super</span>(Linear, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.in_features <span class=\"pl-k\">=</span> in_features\n    <span class=\"pl-c1\">self</span>.out_features <span class=\"pl-k\">=</span> out_features\n    <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">=</span> Parameter(torch.Tensor(out_features, in_features))\n    <span class=\"pl-k\">if</span> bias:\n        <span class=\"pl-c1\">self</span>.bias <span class=\"pl-k\">=</span> Parameter(torch.Tensor(out_features))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c1\">self</span>.register_parameter(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">None</span>)\n    <span class=\"pl-c1\">self</span>.reset_parameters()</pre></div>\n<p>The original proposal was to extract initialization of <code>self.weight</code> and <code>self.bias</code> into separate method.<br>\nSomething like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">in_features</span>, <span class=\"pl-smi\">out_features</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-c1\">super</span>(Linear, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.in_features <span class=\"pl-k\">=</span> in_features\n    <span class=\"pl-c1\">self</span>.out_features <span class=\"pl-k\">=</span> out_features\n    <span class=\"pl-c1\">self</span>.use_bias <span class=\"pl-k\">=</span> bias\n    <span class=\"pl-c1\">self</span>.refresh_parameters()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">refresh_parameters</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">del</span> <span class=\"pl-c1\">self</span>._parameters[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight<span class=\"pl-pds\">'</span></span>]\n    <span class=\"pl-k\">del</span> <span class=\"pl-c1\">self</span>._parameters[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>]\n    <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">=</span> Parameter(torch.Tensor(<span class=\"pl-c1\">self</span>.out_features, <span class=\"pl-c1\">self</span>.in_features))\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.use_bias:\n        <span class=\"pl-c1\">self</span>.bias <span class=\"pl-k\">=</span> Parameter(torch.Tensor(<span class=\"pl-c1\">self</span>.out_features))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c1\">self</span>.register_parameter(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">None</span>)\n    <span class=\"pl-c1\">self</span>.reset_parameters()</pre></div>\n<p>Alternatively, create a special cloning method with optional parameters, like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">in_features</span>, <span class=\"pl-smi\">out_features</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-c1\">super</span>(Linear, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.in_features <span class=\"pl-k\">=</span> in_features\n    <span class=\"pl-c1\">self</span>.out_features <span class=\"pl-k\">=</span> out_features\n    <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">=</span> Parameter(torch.Tensor(out_features, in_features))\n    <span class=\"pl-k\">if</span> bias:\n        <span class=\"pl-c1\">self</span>.bias <span class=\"pl-k\">=</span> Parameter(torch.Tensor(out_features))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c1\">self</span>.register_parameter(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">None</span>)\n    <span class=\"pl-c1\">self</span>.reset_parameters()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">copy</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n    args <span class=\"pl-k\">=</span> <span class=\"pl-c1\">dict</span>(\n        <span class=\"pl-v\">in_features</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.in_features,\n        <span class=\"pl-v\">out_features</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.out_features,\n        <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.use_bias,\n    )\n    args.update(kwargs)\n    <span class=\"pl-k\">return</span> Linear(<span class=\"pl-k\">**</span>args)</pre></div>", "body_text": "Yes, it's assigned in the __init__ now:\ndef __init__(self, in_features, out_features, bias=True):\n    super(Linear, self).__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n    self.weight = Parameter(torch.Tensor(out_features, in_features))\n    if bias:\n        self.bias = Parameter(torch.Tensor(out_features))\n    else:\n        self.register_parameter('bias', None)\n    self.reset_parameters()\nThe original proposal was to extract initialization of self.weight and self.bias into separate method.\nSomething like this:\ndef __init__(self, in_features, out_features, bias=True):\n    super(Linear, self).__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n    self.use_bias = bias\n    self.refresh_parameters()\n\ndef refresh_parameters(self):\n    del self._parameters['weight']\n    del self._parameters['bias']\n    self.weight = Parameter(torch.Tensor(self.out_features, self.in_features))\n    if self.use_bias:\n        self.bias = Parameter(torch.Tensor(self.out_features))\n    else:\n        self.register_parameter('bias', None)\n    self.reset_parameters()\nAlternatively, create a special cloning method with optional parameters, like this:\ndef __init__(self, in_features, out_features, bias=True):\n    super(Linear, self).__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n    self.weight = Parameter(torch.Tensor(out_features, in_features))\n    if bias:\n        self.bias = Parameter(torch.Tensor(out_features))\n    else:\n        self.register_parameter('bias', None)\n    self.reset_parameters()\n\ndef copy(self, **kwargs):\n    args = dict(\n        in_features=self.in_features,\n        out_features=self.out_features,\n        bias=self.use_bias,\n    )\n    args.update(kwargs)\n    return Linear(**args)", "body": "Yes, it's assigned in the `__init__` now:\r\n```python\r\ndef __init__(self, in_features, out_features, bias=True):\r\n    super(Linear, self).__init__()\r\n    self.in_features = in_features\r\n    self.out_features = out_features\r\n    self.weight = Parameter(torch.Tensor(out_features, in_features))\r\n    if bias:\r\n        self.bias = Parameter(torch.Tensor(out_features))\r\n    else:\r\n        self.register_parameter('bias', None)\r\n    self.reset_parameters()\r\n```\r\n\r\nThe original proposal was to extract initialization of `self.weight` and `self.bias` into separate method.\r\nSomething like this:\r\n```python\r\ndef __init__(self, in_features, out_features, bias=True):\r\n    super(Linear, self).__init__()\r\n    self.in_features = in_features\r\n    self.out_features = out_features\r\n    self.use_bias = bias\r\n    self.refresh_parameters()\r\n\r\ndef refresh_parameters(self):\r\n    del self._parameters['weight']\r\n    del self._parameters['bias']\r\n    self.weight = Parameter(torch.Tensor(self.out_features, self.in_features))\r\n    if self.use_bias:\r\n        self.bias = Parameter(torch.Tensor(self.out_features))\r\n    else:\r\n        self.register_parameter('bias', None)\r\n    self.reset_parameters()\r\n```\r\n\r\nAlternatively, create a special cloning method with optional parameters, like this:\r\n\r\n```python\r\ndef __init__(self, in_features, out_features, bias=True):\r\n    super(Linear, self).__init__()\r\n    self.in_features = in_features\r\n    self.out_features = out_features\r\n    self.weight = Parameter(torch.Tensor(out_features, in_features))\r\n    if bias:\r\n        self.bias = Parameter(torch.Tensor(out_features))\r\n    else:\r\n        self.register_parameter('bias', None)\r\n    self.reset_parameters()\r\n\r\ndef copy(self, **kwargs):\r\n    args = dict(\r\n        in_features=self.in_features,\r\n        out_features=self.out_features,\r\n        bias=self.use_bias,\r\n    )\r\n    args.update(kwargs)\r\n    return Linear(**args)\r\n```"}