{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164541574", "pull_request_review_id": 92329471, "id": 164541574, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDU0MTU3NA==", "diff_hunk": "@@ -434,6 +447,7 @@ void THTensor_(getRNGState)(THGenerator *_generator, THTensor *self)\n   THArgCheck(THTensor_(isContiguous)(self), 1, \"RNG state needs to be contiguous\");\n   rng_state = (THGenerator *)THTensor_(data)(self);\n   THGenerator_copy(rng_state, _generator);\n+  rng_state->mutex = NULL;  // mutex should not be part of the generator state", "path": "aten/src/TH/generic/THTensorRandom.cpp", "position": null, "original_position": 116, "commit_id": "4028540d217200a9ded3ea020ae8fc8cf40dde2f", "original_commit_id": "2b648a6cbd09f92f3002e0bf413955bdb250157e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Oh god, what this function is even doing (not referring to your change, just a general observation). It's UB at best. We should change it to pack all the data manually, without doing this hack with dumping raw memory into a tensor (this will dump padding too!!). BTW `setRNGState` will copy the bytes of `mutex` from here into the `mutex `field of `_generator`, yielding an invalid state (generator without any mutex) and probably leading to a segfault.", "created_at": "2018-01-29T19:41:20Z", "updated_at": "2018-11-23T15:38:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/4041#discussion_r164541574", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4041", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164541574"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4041#discussion_r164541574"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4041"}}, "body_html": "<p>Oh god, what this function is even doing (not referring to your change, just a general observation). It's UB at best. We should change it to pack all the data manually, without doing this hack with dumping raw memory into a tensor (this will dump padding too!!). BTW <code>setRNGState</code> will copy the bytes of <code>mutex</code> from here into the <code>mutex </code>field of <code>_generator</code>, yielding an invalid state (generator without any mutex) and probably leading to a segfault.</p>", "body_text": "Oh god, what this function is even doing (not referring to your change, just a general observation). It's UB at best. We should change it to pack all the data manually, without doing this hack with dumping raw memory into a tensor (this will dump padding too!!). BTW setRNGState will copy the bytes of mutex from here into the mutex field of _generator, yielding an invalid state (generator without any mutex) and probably leading to a segfault."}