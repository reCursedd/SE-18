{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3077", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3077/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3077/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3077/events", "html_url": "https://github.com/pytorch/pytorch/issues/3077", "id": 264705092, "node_id": "MDU6SXNzdWUyNjQ3MDUwOTI=", "number": 3077, "title": "VariableVersion::join_with has hidden invariant / mark_shared_storage order matters (and is unchecked)", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-10-11T19:21:56Z", "updated_at": "2018-02-20T05:43:49Z", "closed_at": "2018-02-20T05:43:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I was working on the PyTorch paper, and while trying to explain <code>join_with</code>, I realized that there are problems.</p>\n<p>From its name, you'd expect VariableVersion::join_with to be a symmetric operation; in fact, there is even a comment (added by me) claiming this is the case. However, a quick look at the implementation makes it clear that it is NOT symmetric:</p>\n<pre><code>  void join_with(VariableVersion &amp;other) {\n    if (this == &amp;other) {\n      return;\n    }\n    cleanup();\n    version_block = other.version_block;\n    version_block[1]++;\n    version_block[2]++;\n  }\n</code></pre>\n<p>There are three call sites of <code>join_with</code>, and one of them accepts a user provided pair of variables to join counters (triggered by <code>mark_shared_storage</code>). From this, I was able to reverse engineer an example where we silently do the wrong thing:</p>\n<pre><code>import torch\n\nx = torch.autograd.Variable(torch.Tensor([[0.1, 0.2], [0.3, 0.4]]), requires_grad=True)\n\nclass SomeFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        y = x.view(4,1)\n        ctx.mark_shared_storage((y, x))\n        #ctx.mark_shared_storage((x, y))\n        return y\n        \n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.contiguous().view(2,2)\n\nb = x.tanh()\na = SomeFn.apply(b)\ndel b\na.add_(2)\n#a = a.add(2)\na.sum().backward()\nprint(x.grad)\n</code></pre>\n<p>This program does not fail, despite the fact that the <code>add_</code> operation is clobbering a view of <code>b</code>, which is needed to perform <code>tanh</code> backward computation.  You can see the correct output by commenting <code>a.add_(2)</code> and uncommenting <code>a = a.add(2)</code>; you can trigger an error by switching the order of x and y in the <code>mark_shared_storage</code> call (see the commented out segment.) The problem is that if you swap the order of variables in <code>mark_shared_storage</code>, we end up discarding the \"old\" version counter associated with b (which, when incremented, would trigger our sanity checking on backwards) and using the version counter for a instead (which isn't hooked up to anything; it was just generated.)</p>\n<p>It's easy enough to add the necessary check to <code>mark_shared_storage</code> processing, but it seems like a rename of <code>join_with</code> is also in order; it is more of a \"set storage to\" operation, and this new name makes clear the asymmetry. Arguably, we should be forced to specify the version counter when <em>creating</em> a Variable in the first place, but this is somewhat inconvenient to do.</p>\n<p>Related: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"263517867\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3006\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3006/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3006\">#3006</a>.</p>", "body_text": "I was working on the PyTorch paper, and while trying to explain join_with, I realized that there are problems.\nFrom its name, you'd expect VariableVersion::join_with to be a symmetric operation; in fact, there is even a comment (added by me) claiming this is the case. However, a quick look at the implementation makes it clear that it is NOT symmetric:\n  void join_with(VariableVersion &other) {\n    if (this == &other) {\n      return;\n    }\n    cleanup();\n    version_block = other.version_block;\n    version_block[1]++;\n    version_block[2]++;\n  }\n\nThere are three call sites of join_with, and one of them accepts a user provided pair of variables to join counters (triggered by mark_shared_storage). From this, I was able to reverse engineer an example where we silently do the wrong thing:\nimport torch\n\nx = torch.autograd.Variable(torch.Tensor([[0.1, 0.2], [0.3, 0.4]]), requires_grad=True)\n\nclass SomeFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        y = x.view(4,1)\n        ctx.mark_shared_storage((y, x))\n        #ctx.mark_shared_storage((x, y))\n        return y\n        \n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output.contiguous().view(2,2)\n\nb = x.tanh()\na = SomeFn.apply(b)\ndel b\na.add_(2)\n#a = a.add(2)\na.sum().backward()\nprint(x.grad)\n\nThis program does not fail, despite the fact that the add_ operation is clobbering a view of b, which is needed to perform tanh backward computation.  You can see the correct output by commenting a.add_(2) and uncommenting a = a.add(2); you can trigger an error by switching the order of x and y in the mark_shared_storage call (see the commented out segment.) The problem is that if you swap the order of variables in mark_shared_storage, we end up discarding the \"old\" version counter associated with b (which, when incremented, would trigger our sanity checking on backwards) and using the version counter for a instead (which isn't hooked up to anything; it was just generated.)\nIt's easy enough to add the necessary check to mark_shared_storage processing, but it seems like a rename of join_with is also in order; it is more of a \"set storage to\" operation, and this new name makes clear the asymmetry. Arguably, we should be forced to specify the version counter when creating a Variable in the first place, but this is somewhat inconvenient to do.\nRelated: #3006.", "body": "I was working on the PyTorch paper, and while trying to explain `join_with`, I realized that there are problems.\r\n\r\nFrom its name, you'd expect VariableVersion::join_with to be a symmetric operation; in fact, there is even a comment (added by me) claiming this is the case. However, a quick look at the implementation makes it clear that it is NOT symmetric:\r\n\r\n```\r\n  void join_with(VariableVersion &other) {\r\n    if (this == &other) {\r\n      return;\r\n    }\r\n    cleanup();\r\n    version_block = other.version_block;\r\n    version_block[1]++;\r\n    version_block[2]++;\r\n  }\r\n```\r\n\r\nThere are three call sites of `join_with`, and one of them accepts a user provided pair of variables to join counters (triggered by `mark_shared_storage`). From this, I was able to reverse engineer an example where we silently do the wrong thing:\r\n\r\n```\r\nimport torch\r\n\r\nx = torch.autograd.Variable(torch.Tensor([[0.1, 0.2], [0.3, 0.4]]), requires_grad=True)\r\n\r\nclass SomeFn(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        y = x.view(4,1)\r\n        ctx.mark_shared_storage((y, x))\r\n        #ctx.mark_shared_storage((x, y))\r\n        return y\r\n        \r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        return grad_output.contiguous().view(2,2)\r\n\r\nb = x.tanh()\r\na = SomeFn.apply(b)\r\ndel b\r\na.add_(2)\r\n#a = a.add(2)\r\na.sum().backward()\r\nprint(x.grad)\r\n```\r\n\r\nThis program does not fail, despite the fact that the `add_` operation is clobbering a view of `b`, which is needed to perform `tanh` backward computation.  You can see the correct output by commenting `a.add_(2)` and uncommenting `a = a.add(2)`; you can trigger an error by switching the order of x and y in the `mark_shared_storage` call (see the commented out segment.) The problem is that if you swap the order of variables in `mark_shared_storage`, we end up discarding the \"old\" version counter associated with b (which, when incremented, would trigger our sanity checking on backwards) and using the version counter for a instead (which isn't hooked up to anything; it was just generated.)\r\n\r\nIt's easy enough to add the necessary check to `mark_shared_storage` processing, but it seems like a rename of `join_with` is also in order; it is more of a \"set storage to\" operation, and this new name makes clear the asymmetry. Arguably, we should be forced to specify the version counter when *creating* a Variable in the first place, but this is somewhat inconvenient to do.\r\n\r\nRelated: #3006."}