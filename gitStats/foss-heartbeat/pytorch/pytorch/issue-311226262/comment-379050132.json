{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379050132", "html_url": "https://github.com/pytorch/pytorch/issues/6269#issuecomment-379050132", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6269", "id": 379050132, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTA1MDEzMg==", "user": {"login": "bkovalev", "id": 22419555, "node_id": "MDQ6VXNlcjIyNDE5NTU1", "avatar_url": "https://avatars0.githubusercontent.com/u/22419555?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bkovalev", "html_url": "https://github.com/bkovalev", "followers_url": "https://api.github.com/users/bkovalev/followers", "following_url": "https://api.github.com/users/bkovalev/following{/other_user}", "gists_url": "https://api.github.com/users/bkovalev/gists{/gist_id}", "starred_url": "https://api.github.com/users/bkovalev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bkovalev/subscriptions", "organizations_url": "https://api.github.com/users/bkovalev/orgs", "repos_url": "https://api.github.com/users/bkovalev/repos", "events_url": "https://api.github.com/users/bkovalev/events{/privacy}", "received_events_url": "https://api.github.com/users/bkovalev/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-05T19:28:29Z", "updated_at": "2018-04-05T19:28:29Z", "author_association": "NONE", "body_html": "<p>When I execute multi GPU per host by</p>\n<p>python /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 4 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 0 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3</p>\n<p>python /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 4 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 1 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3</p>\n<p>I have another issue.</p>\n<p>On first node i see</p>\n<p>INFO:resnet50_trainer:Running on GPUs: [0, 1, 2, 3]<br>\nINFO:resnet50_trainer:Using epoch size: 9984<br>\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]<br>\nINFO:data_parallel_model:Create input and model training operators<br>\nINFO:data_parallel_model:Model for GPU : 0<br>\nINFO:data_parallel_model:Model for GPU : 1<br>\nINFO:data_parallel_model:Model for GPU : 2<br>\nINFO:data_parallel_model:Model for GPU : 3<br>\nINFO:data_parallel_model:Adding gradient operators<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Add initial parameter sync<br>\nE0405 22:24:37.457337 17253 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3<br>\nE0405 22:24:37.457785 17253 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"<br>\nE0405 22:24:37.457815 17253 operator.cc:465] Returning empty results.<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0757119655609 secs<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0735418796539 secs<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0704200267792 secs<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0703098773956 secs<br>\nINFO:resnet50_trainer:----- Create test net ----<br>\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]<br>\nINFO:data_parallel_model:Create input and model training operators<br>\nINFO:data_parallel_model:Model for GPU : 0<br>\nINFO:data_parallel_model:Model for GPU : 1<br>\nINFO:data_parallel_model:Model for GPU : 2<br>\nINFO:data_parallel_model:Model for GPU : 3<br>\nINFO:data_parallel_model:Parameter update function not defined --&gt; only forward<br>\nINFO:resnet50_trainer:Starting epoch 0/2</p>\n<p>On second node:<br>\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.<br>\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.<br>\nE0405 22:24:39.795949 24032 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.<br>\nE0405 22:24:39.796234 24032 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.<br>\nE0405 22:24:39.796247 24032 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.<br>\nINFO:resnet50_trainer:Running on GPUs: [0, 1, 2, 3]<br>\nINFO:resnet50_trainer:Using epoch size: 9984<br>\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]<br>\nINFO:data_parallel_model:Create input and model training operators<br>\nINFO:data_parallel_model:Model for GPU : 0<br>\nINFO:data_parallel_model:Model for GPU : 1<br>\nINFO:data_parallel_model:Model for GPU : 2<br>\nINFO:data_parallel_model:Model for GPU : 3<br>\nINFO:data_parallel_model:Adding gradient operators<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Add initial parameter sync<br>\nE0405 22:24:40.959513 24032 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail &gt;= dkernel. 2 vs 3<br>\nE0405 22:24:40.960314 24032 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"<br>\nE0405 22:24:40.960363 24032 operator.cc:465] Returning empty results.<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0762090682983 secs<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.111880064011 secs<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0926520824432 secs<br>\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory<br>\nINFO:memonger:Memonger memory optimization took 0.0926530361176 secs<br>\nINFO:resnet50_trainer:----- Create test net ----<br>\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]<br>\nINFO:data_parallel_model:Create input and model training operators<br>\nINFO:data_parallel_model:Model for GPU : 0<br>\nINFO:data_parallel_model:Model for GPU : 1<br>\nINFO:data_parallel_model:Model for GPU : 2<br>\nINFO:data_parallel_model:Model for GPU : 3<br>\nINFO:data_parallel_model:Parameter update function not defined --&gt; only forward<br>\nINFO:resnet50_trainer:Starting epoch 0/2<br>\nE0405 22:24:57.784853 24154 net_dag.cc:188] Exception from operator chain starting at 'last_out_L1000_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_0_cw\" input: \"gpu_0/last_out_L1000_b_grad\" input: \"gpu_1/last_out_L1000_b_grad\" input: \"gpu_2/last_out_L1000_b_grad\" input: \"gpu_3/last_out_L1000_b_grad\" output: \"gpu_0/last_out_L1000_b_grad\" output: \"gpu_1/last_out_L1000_b_grad\" output: \"gpu_2/last_out_L1000_b_grad\" output: \"gpu_3/last_out_L1000_b_grad\" name: \"last_out_L1000_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_last_out_L1000_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"<br>\nE0405 22:24:57.864006 24142 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_spatbn_3_s_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_3_cw\" input: \"gpu_0/comp_15_spatbn_3_s_grad\" input: \"gpu_1/comp_15_spatbn_3_s_grad\" input: \"gpu_2/comp_15_spatbn_3_s_grad\" input: \"gpu_3/comp_15_spatbn_3_s_grad\" output: \"gpu_0/comp_15_spatbn_3_s_grad\" output: \"gpu_1/comp_15_spatbn_3_s_grad\" output: \"gpu_2/comp_15_spatbn_3_s_grad\" output: \"gpu_3/comp_15_spatbn_3_s_grad\" name: \"comp_15_spatbn_3_s_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_s_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"<br>\nE0405 22:24:57.869789 24153 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_spatbn_3_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" input: \"gpu_2/comp_15_spatbn_3_b_grad\" input: \"gpu_3/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_2/comp_15_spatbn_3_b_grad\" output: \"gpu_3/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"<br>\nOriginal python traceback for operator 1752 in network <code>resnet50</code> in exception above (most recent call last):<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 434, in Train<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 271, in Parallelize<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1149, in _AllReduceBlobs<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1301, in _AllReduceBlobsDistributed<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1292, in allreduce<br>\nTraceback (most recent call last):<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <br>\nmain()<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main<br>\nTrain(args)<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train<br>\nexplog<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 175, in RunEpoch<br>\nworkspace.RunNet(train_model.net.Proto().name)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 215, in RunNet<br>\nStringifyNetName(name), num_iter, allow_fail,<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 177, in CallWithExceptionIntercept<br>\nreturn func(*args, **kwargs)<br>\nRuntimeError: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:<br>\ninput: \"allreduce_0_cw\" input: \"gpu_0/last_out_L1000_b_grad\" input: \"gpu_1/last_out_L1000_b_grad\" input: \"gpu_2/last_out_L1000_b_grad\" input: \"gpu_3/last_out_L1000_b_grad\" output: \"gpu_0/last_out_L1000_b_grad\" output: \"gpu_1/last_out_L1000_b_grad\" output: \"gpu_2/last_out_L1000_b_grad\" output: \"gpu_3/last_out_L1000_b_grad\" name: \"last_out_L1000_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_last_out_L1000_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"</p>", "body_text": "When I execute multi GPU per host by\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 4 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 0 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 4 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 1 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\nI have another issue.\nOn first node i see\nINFO:resnet50_trainer:Running on GPUs: [0, 1, 2, 3]\nINFO:resnet50_trainer:Using epoch size: 9984\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Model for GPU : 2\nINFO:data_parallel_model:Model for GPU : 3\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nE0405 22:24:37.457337 17253 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\nE0405 22:24:37.457785 17253 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0405 22:24:37.457815 17253 operator.cc:465] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0757119655609 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0735418796539 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0704200267792 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0703098773956 secs\nINFO:resnet50_trainer:----- Create test net ----\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Model for GPU : 2\nINFO:data_parallel_model:Model for GPU : 3\nINFO:data_parallel_model:Parameter update function not defined --> only forward\nINFO:resnet50_trainer:Starting epoch 0/2\nOn second node:\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\nE0405 22:24:39.795949 24032 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0405 22:24:39.796234 24032 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nE0405 22:24:39.796247 24032 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\nINFO:resnet50_trainer:Running on GPUs: [0, 1, 2, 3]\nINFO:resnet50_trainer:Using epoch size: 9984\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Model for GPU : 2\nINFO:data_parallel_model:Model for GPU : 3\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nE0405 22:24:40.959513 24032 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3\nE0405 22:24:40.960314 24032 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\nE0405 22:24:40.960363 24032 operator.cc:465] Returning empty results.\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0762090682983 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.111880064011 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0926520824432 secs\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\nINFO:memonger:Memonger memory optimization took 0.0926530361176 secs\nINFO:resnet50_trainer:----- Create test net ----\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]\nINFO:data_parallel_model:Create input and model training operators\nINFO:data_parallel_model:Model for GPU : 0\nINFO:data_parallel_model:Model for GPU : 1\nINFO:data_parallel_model:Model for GPU : 2\nINFO:data_parallel_model:Model for GPU : 3\nINFO:data_parallel_model:Parameter update function not defined --> only forward\nINFO:resnet50_trainer:Starting epoch 0/2\nE0405 22:24:57.784853 24154 net_dag.cc:188] Exception from operator chain starting at 'last_out_L1000_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_0_cw\" input: \"gpu_0/last_out_L1000_b_grad\" input: \"gpu_1/last_out_L1000_b_grad\" input: \"gpu_2/last_out_L1000_b_grad\" input: \"gpu_3/last_out_L1000_b_grad\" output: \"gpu_0/last_out_L1000_b_grad\" output: \"gpu_1/last_out_L1000_b_grad\" output: \"gpu_2/last_out_L1000_b_grad\" output: \"gpu_3/last_out_L1000_b_grad\" name: \"last_out_L1000_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_last_out_L1000_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nE0405 22:24:57.864006 24142 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_spatbn_3_s_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_3_cw\" input: \"gpu_0/comp_15_spatbn_3_s_grad\" input: \"gpu_1/comp_15_spatbn_3_s_grad\" input: \"gpu_2/comp_15_spatbn_3_s_grad\" input: \"gpu_3/comp_15_spatbn_3_s_grad\" output: \"gpu_0/comp_15_spatbn_3_s_grad\" output: \"gpu_1/comp_15_spatbn_3_s_grad\" output: \"gpu_2/comp_15_spatbn_3_s_grad\" output: \"gpu_3/comp_15_spatbn_3_s_grad\" name: \"comp_15_spatbn_3_s_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_s_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nE0405 22:24:57.869789 24153 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_spatbn_3_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" input: \"gpu_2/comp_15_spatbn_3_b_grad\" input: \"gpu_3/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_2/comp_15_spatbn_3_b_grad\" output: \"gpu_3/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\nOriginal python traceback for operator 1752 in network resnet50 in exception above (most recent call last):\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in \nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 434, in Train\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 271, in Parallelize\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1149, in _AllReduceBlobs\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1301, in _AllReduceBlobsDistributed\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1292, in allreduce\nTraceback (most recent call last):\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in \nmain()\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\nTrain(args)\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train\nexplog\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 175, in RunEpoch\nworkspace.RunNet(train_model.net.Proto().name)\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 215, in RunNet\nStringifyNetName(name), num_iter, allow_fail,\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 177, in CallWithExceptionIntercept\nreturn func(*args, **kwargs)\nRuntimeError: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator:\ninput: \"allreduce_0_cw\" input: \"gpu_0/last_out_L1000_b_grad\" input: \"gpu_1/last_out_L1000_b_grad\" input: \"gpu_2/last_out_L1000_b_grad\" input: \"gpu_3/last_out_L1000_b_grad\" output: \"gpu_0/last_out_L1000_b_grad\" output: \"gpu_1/last_out_L1000_b_grad\" output: \"gpu_2/last_out_L1000_b_grad\" output: \"gpu_3/last_out_L1000_b_grad\" name: \"last_out_L1000_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_last_out_L1000_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"", "body": "When I execute multi GPU per host by\r\n\r\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 4 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 0 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\n\r\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 4 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 1 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\n\r\nI have another issue.\r\n\r\nOn first node i see\r\n\r\nINFO:resnet50_trainer:Running on GPUs: [0, 1, 2, 3]\r\nINFO:resnet50_trainer:Using epoch size: 9984\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Model for GPU : 2\r\nINFO:data_parallel_model:Model for GPU : 3\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nE0405 22:24:37.457337 17253 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3 \r\nE0405 22:24:37.457785 17253 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0405 22:24:37.457815 17253 operator.cc:465] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0757119655609 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0735418796539 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0704200267792 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0703098773956 secs\r\nINFO:resnet50_trainer:----- Create test net ----\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Model for GPU : 2\r\nINFO:data_parallel_model:Model for GPU : 3\r\nINFO:data_parallel_model:Parameter update function not defined --> only forward\r\nINFO:resnet50_trainer:Starting epoch 0/2\r\n\r\nOn second node:\r\nIgnoring @/caffe2/caffe2/contrib/nccl:nccl_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/contrib/gloo:gloo_ops_gpu as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:file_store_handler_ops as it is not a valid file.\r\nIgnoring @/caffe2/caffe2/distributed:redis_store_handler_ops as it is not a valid file.\r\nE0405 22:24:39.795949 24032 init_intrinsics_check.cc:43] CPU feature avx is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0405 22:24:39.796234 24032 init_intrinsics_check.cc:43] CPU feature avx2 is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nE0405 22:24:39.796247 24032 init_intrinsics_check.cc:43] CPU feature fma is present on your machine, but the Caffe2 binary is not compiled with it. It means you may not get the full speed of your CPU.\r\nINFO:resnet50_trainer:Running on GPUs: [0, 1, 2, 3]\r\nINFO:resnet50_trainer:Using epoch size: 9984\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Model for GPU : 2\r\nINFO:data_parallel_model:Model for GPU : 3\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nE0405 22:24:40.959513 24032 operator.cc:463] Shape inference error: [enforce fail at conv_pool_op_base.h:601] in_size + *pad_head + *pad_tail >= dkernel. 2 vs 3 \r\nE0405 22:24:40.960314 24032 operator.cc:464] Operator: input: \"gpu_0/conv1_spatbn_relu\" output: \"gpu_0/pool1\" name: \"\" type: \"MaxPool\" arg { name: \"order\" s: \"NCHW\" } arg { name: \"kernel\" i: 3 } arg { name: \"stride\" i: 2 } arg { name: \"ws_nbytes_limit\" i: 67108864 } arg { name: \"cudnn_exhaustive_search\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"CUDNN\"\r\nE0405 22:24:40.960363 24032 operator.cc:465] Returning empty results.\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0762090682983 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.111880064011 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0926520824432 secs\r\nWARNING:memonger:NOTE: Executing memonger to optimize gradient memory\r\nINFO:memonger:Memonger memory optimization took 0.0926530361176 secs\r\nINFO:resnet50_trainer:----- Create test net ----\r\nINFO:data_parallel_model:Parallelizing model for devices: [0, 1, 2, 3]\r\nINFO:data_parallel_model:Create input and model training operators\r\nINFO:data_parallel_model:Model for GPU : 0\r\nINFO:data_parallel_model:Model for GPU : 1\r\nINFO:data_parallel_model:Model for GPU : 2\r\nINFO:data_parallel_model:Model for GPU : 3\r\nINFO:data_parallel_model:Parameter update function not defined --> only forward\r\nINFO:resnet50_trainer:Starting epoch 0/2\r\nE0405 22:24:57.784853 24154 net_dag.cc:188] Exception from operator chain starting at 'last_out_L1000_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_0_cw\" input: \"gpu_0/last_out_L1000_b_grad\" input: \"gpu_1/last_out_L1000_b_grad\" input: \"gpu_2/last_out_L1000_b_grad\" input: \"gpu_3/last_out_L1000_b_grad\" output: \"gpu_0/last_out_L1000_b_grad\" output: \"gpu_1/last_out_L1000_b_grad\" output: \"gpu_2/last_out_L1000_b_grad\" output: \"gpu_3/last_out_L1000_b_grad\" name: \"last_out_L1000_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_last_out_L1000_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\nE0405 22:24:57.864006 24142 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_spatbn_3_s_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_3_cw\" input: \"gpu_0/comp_15_spatbn_3_s_grad\" input: \"gpu_1/comp_15_spatbn_3_s_grad\" input: \"gpu_2/comp_15_spatbn_3_s_grad\" input: \"gpu_3/comp_15_spatbn_3_s_grad\" output: \"gpu_0/comp_15_spatbn_3_s_grad\" output: \"gpu_1/comp_15_spatbn_3_s_grad\" output: \"gpu_2/comp_15_spatbn_3_s_grad\" output: \"gpu_3/comp_15_spatbn_3_s_grad\" name: \"comp_15_spatbn_3_s_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_s_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\nE0405 22:24:57.869789 24153 net_dag.cc:188] Secondary exception from operator chain starting at 'comp_15_spatbn_3_b_grad' (type 'Allreduce'): caffe2::EnforceNotMet: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_2_cw\" input: \"gpu_0/comp_15_spatbn_3_b_grad\" input: \"gpu_1/comp_15_spatbn_3_b_grad\" input: \"gpu_2/comp_15_spatbn_3_b_grad\" input: \"gpu_3/comp_15_spatbn_3_b_grad\" output: \"gpu_0/comp_15_spatbn_3_b_grad\" output: \"gpu_1/comp_15_spatbn_3_b_grad\" output: \"gpu_2/comp_15_spatbn_3_b_grad\" output: \"gpu_3/comp_15_spatbn_3_b_grad\" name: \"comp_15_spatbn_3_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_comp_15_spatbn_3_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\nOriginal python traceback for operator 1752 in network `resnet50` in exception above (most recent call last):\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <module>\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 434, in Train\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 271, in Parallelize\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1149, in _AllReduceBlobs\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1301, in _AllReduceBlobsDistributed\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/data_parallel_model.py\", line 1292, in allreduce\r\nTraceback (most recent call last):\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <module>\r\n    main()\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\r\n    Train(args)\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train\r\n    explog\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 175, in RunEpoch\r\n    workspace.RunNet(train_model.net.Proto().name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 215, in RunNet\r\n    StringifyNetName(name), num_iter, allow_fail,\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 177, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [enforce fail at context_gpu.h:155] . Encountered CUDA error: invalid resource handle Error from operator: \r\ninput: \"allreduce_0_cw\" input: \"gpu_0/last_out_L1000_b_grad\" input: \"gpu_1/last_out_L1000_b_grad\" input: \"gpu_2/last_out_L1000_b_grad\" input: \"gpu_3/last_out_L1000_b_grad\" output: \"gpu_0/last_out_L1000_b_grad\" output: \"gpu_1/last_out_L1000_b_grad\" output: \"gpu_2/last_out_L1000_b_grad\" output: \"gpu_3/last_out_L1000_b_grad\" name: \"last_out_L1000_b_grad\" type: \"Allreduce\" arg { name: \"status_blob\" s: \"allreduce_last_out_L1000_b_grad_status\" } arg { name: \"gpu_direct\" i: 1 } device_option { device_type: 1 cuda_gpu_id: 0 } engine: \"GLOO\"\r\n\r\n\r\n\r\n"}