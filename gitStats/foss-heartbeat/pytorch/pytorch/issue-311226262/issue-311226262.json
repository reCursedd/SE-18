{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6269", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6269/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6269/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6269/events", "html_url": "https://github.com/pytorch/pytorch/issues/6269", "id": 311226262, "node_id": "MDU6SXNzdWUzMTEyMjYyNjI=", "number": 6269, "title": "[caffe2] Run resnet50_trainer.py error between 2 machines using GLOO/Redis", "user": {"login": "bkovalev", "id": 22419555, "node_id": "MDQ6VXNlcjIyNDE5NTU1", "avatar_url": "https://avatars0.githubusercontent.com/u/22419555?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bkovalev", "html_url": "https://github.com/bkovalev", "followers_url": "https://api.github.com/users/bkovalev/followers", "following_url": "https://api.github.com/users/bkovalev/following{/other_user}", "gists_url": "https://api.github.com/users/bkovalev/gists{/gist_id}", "starred_url": "https://api.github.com/users/bkovalev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bkovalev/subscriptions", "organizations_url": "https://api.github.com/users/bkovalev/orgs", "repos_url": "https://api.github.com/users/bkovalev/repos", "events_url": "https://api.github.com/users/bkovalev/events{/privacy}", "received_events_url": "https://api.github.com/users/bkovalev/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-04-04T13:19:00Z", "updated_at": "2018-04-09T17:08:33Z", "closed_at": "2018-04-09T17:08:33Z", "author_association": "NONE", "body_html": "<ul>\n<li>\n<p>Caffe2</p>\n</li>\n<li>\n<p>OS: Ubuntu 16.04<br>\n-- Python version: 2.7</p>\n</li>\n<li>\n<p>CUDA/cuDNN version: 9.1 , 7.0.5</p>\n</li>\n<li>\n<p>GPU models and configuration: P100</p>\n</li>\n<li>\n<p>GCC version (if compiling from source):5.4.0</p>\n</li>\n<li>\n<p>Mellanox OFEDversion: 4.2.1</p>\n</li>\n<li>\n<p>Build command you used (if compiling from source):</p>\n<p>Eigen with CUDA9: fatal error\uff1amath_functions.hpp\uff1aNo such file or directory.</p>\n</li>\n</ul>\n<p>Was fixed in eigen at <a href=\"https://bitbucket.org/eigen/eigen/commits/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58?at=default\" rel=\"nofollow\">https://bitbucket.org/eigen/eigen/commits/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58?at=default</a></p>\n<pre><code>$ mkdir build\n$ cd build\n$ cmake .. -DCUDA_ARCH_NAME=Manual -DCUDA_ARCH_BIN=\"60 61\" -DCUDA_ARCH_PTX=\"61\" -DUSE_NNPACK=OFF -DUSE_ROCKSDB=OFF -DUSE_GLOO=ON -DUSE_REDIS=ON -DUSE_IBVERBS=ON -DUSE_MPI=OFF\n$ make -j\"$(nproc)\" install\n$ ldconfig\n$ make clean\n</code></pre>\n<p>I try to run resnet50_trainer.py on one node by:<br>\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 64 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 4</p>\n<p>and have result: ~ 430 imgs/sec</p>\n<p>When I try to run resnet50_trainer.py between 2 machines using GLOO/Redis, but I get the following error messages.<br>\nHow to fix the error and run correctly using mpirun?</p>\n<p>My commands:<br>\nOn each node was ran the sample command:<br>\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 1 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 0 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3</p>\n<p>python /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 1 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 1 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3</p>\n<p>Errors:<br>\nINFO:resnet50_trainer:Finished iteration 155/156 of epoch 0 (207.29 images/sec)<br>\nINFO:resnet50_trainer:Training loss: 0.000181300914846, accuracy: 1.0<br>\nINFO:resnet50_trainer:Finished iteration 156/156 of epoch 0 (204.85 images/sec)<br>\nINFO:resnet50_trainer:Training loss: 0.00019393475668, accuracy: 1.0<br>\nE0404 16:13:32.042721 39912 prefetch_op.h:110] Prefetching error std::bad_alloc<br>\nE0404 16:13:32.042976 39911 prefetch_op.h:83] Prefetching failed.<br>\nE0404 16:13:32.043117 39911 net_dag.cc:231] Operator chain failed starting at: input: \"test_reader\" output: \"gpu_0/data\" output: \"gpu_0/label\" name: \"\" type: \"ImageInput\" arg { name: \"std\" f: 128 } arg { name: \"scale\" i: 256 } arg { name: \"use_gpu_transform\" i: 1 } arg { name: \"cudnn_exhaustive_search\" i: 1 } arg { name: \"crop\" i: 256 } arg { name: \"is_test\" i: 1 } arg { name: \"use_cudnn\" i: 1 } arg { name: \"use_caffe_datum\" i: 1 } arg { name: \"mirror\" i: 1 } arg { name: \"output_type\" s: \"float16\" } arg { name: \"batch_size\" i: 32 } arg { name: \"mean\" f: 128 } device_option { device_type: 1 cuda_gpu_id: 0 }<br>\nE0404 16:13:32.043376 39646 net.h:54] Failed to execute async run<br>\nOriginal python traceback for operator 1886221678 in network <code>resnet50_test</code> in exception above (most recent call last):<br>\nTraceback (most recent call last):<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <br>\nmain()<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main<br>\nTrain(args)<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train<br>\nexplog<br>\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 201, in RunEpoch<br>\nworkspace.RunNet(test_model.net.Proto().name)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 215, in RunNet<br>\nStringifyNetName(name), num_iter, allow_fail,<br>\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 177, in CallWithExceptionIntercept<br>\nreturn func(*args, **kwargs)<br>\nRuntimeError: [enforce fail at pybind_state.cc:1001] success. Error running net resnet50_test</p>\n<p>O second node:<br>\nINFO:resnet50_trainer:Training loss: 6.98969364166, accuracy: 0.0<br>\nINFO:resnet50_trainer:Finished iteration 156/156 of epoch 0 (205.14 images/sec)<br>\nINFO:resnet50_trainer:Training loss: 6.99336242676, accuracy: 0.0<br>\nINFO:resnet50_trainer:Starting epoch 1/2<br>\nterminate called after throwing an instance of 'gloo::EnforceNotMet'<br>\nwhat():  [enforce fail at /root/caffe2/third_party/gloo/gloo/transport/ibverbs/pair.cc:417] wc-&gt;status == IBV_WC_SUCCESS. 12 vs 0. Send for slot 2: transport retry counter exceeded<br>\n*** Aborted at 1522847623 (unix time) try \"date -d @1522847623\" if you are using GNU date ***<br>\nPC: @     0x7f84fc58e428 gsignal<br>\n*** SIGABRT (@0x9a14) received by PID 39444 (TID 0x7f83ebf8d700) from PID 39444; stack trace: ***<br>\n@     0x7f84fc58e4b0 (unknown)<br>\n@     0x7f84fc58e428 gsignal<br>\n@     0x7f84fc59002a abort<br>\n@     0x7f848079584d __gnu_cxx::__verbose_terminate_handler()<br>\n@     0x7f84807936b6 (unknown)<br>\n@     0x7f8480793701 std::terminate()<br>\n@     0x7f84807bed38 (unknown)<br>\n@     0x7f84fc92a6ba start_thread<br>\n@     0x7f84fc66041d clone<br>\n@                0x0 (unknown)<br>\nAborted (core dumped)</p>", "body_text": "Caffe2\n\n\nOS: Ubuntu 16.04\n-- Python version: 2.7\n\n\nCUDA/cuDNN version: 9.1 , 7.0.5\n\n\nGPU models and configuration: P100\n\n\nGCC version (if compiling from source):5.4.0\n\n\nMellanox OFEDversion: 4.2.1\n\n\nBuild command you used (if compiling from source):\nEigen with CUDA9: fatal error\uff1amath_functions.hpp\uff1aNo such file or directory.\n\n\nWas fixed in eigen at https://bitbucket.org/eigen/eigen/commits/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58?at=default\n$ mkdir build\n$ cd build\n$ cmake .. -DCUDA_ARCH_NAME=Manual -DCUDA_ARCH_BIN=\"60 61\" -DCUDA_ARCH_PTX=\"61\" -DUSE_NNPACK=OFF -DUSE_ROCKSDB=OFF -DUSE_GLOO=ON -DUSE_REDIS=ON -DUSE_IBVERBS=ON -DUSE_MPI=OFF\n$ make -j\"$(nproc)\" install\n$ ldconfig\n$ make clean\n\nI try to run resnet50_trainer.py on one node by:\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 64 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 4\nand have result: ~ 430 imgs/sec\nWhen I try to run resnet50_trainer.py between 2 machines using GLOO/Redis, but I get the following error messages.\nHow to fix the error and run correctly using mpirun?\nMy commands:\nOn each node was ran the sample command:\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 1 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 0 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 1 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 1 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\nErrors:\nINFO:resnet50_trainer:Finished iteration 155/156 of epoch 0 (207.29 images/sec)\nINFO:resnet50_trainer:Training loss: 0.000181300914846, accuracy: 1.0\nINFO:resnet50_trainer:Finished iteration 156/156 of epoch 0 (204.85 images/sec)\nINFO:resnet50_trainer:Training loss: 0.00019393475668, accuracy: 1.0\nE0404 16:13:32.042721 39912 prefetch_op.h:110] Prefetching error std::bad_alloc\nE0404 16:13:32.042976 39911 prefetch_op.h:83] Prefetching failed.\nE0404 16:13:32.043117 39911 net_dag.cc:231] Operator chain failed starting at: input: \"test_reader\" output: \"gpu_0/data\" output: \"gpu_0/label\" name: \"\" type: \"ImageInput\" arg { name: \"std\" f: 128 } arg { name: \"scale\" i: 256 } arg { name: \"use_gpu_transform\" i: 1 } arg { name: \"cudnn_exhaustive_search\" i: 1 } arg { name: \"crop\" i: 256 } arg { name: \"is_test\" i: 1 } arg { name: \"use_cudnn\" i: 1 } arg { name: \"use_caffe_datum\" i: 1 } arg { name: \"mirror\" i: 1 } arg { name: \"output_type\" s: \"float16\" } arg { name: \"batch_size\" i: 32 } arg { name: \"mean\" f: 128 } device_option { device_type: 1 cuda_gpu_id: 0 }\nE0404 16:13:32.043376 39646 net.h:54] Failed to execute async run\nOriginal python traceback for operator 1886221678 in network resnet50_test in exception above (most recent call last):\nTraceback (most recent call last):\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in \nmain()\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\nTrain(args)\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train\nexplog\nFile \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 201, in RunEpoch\nworkspace.RunNet(test_model.net.Proto().name)\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 215, in RunNet\nStringifyNetName(name), num_iter, allow_fail,\nFile \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 177, in CallWithExceptionIntercept\nreturn func(*args, **kwargs)\nRuntimeError: [enforce fail at pybind_state.cc:1001] success. Error running net resnet50_test\nO second node:\nINFO:resnet50_trainer:Training loss: 6.98969364166, accuracy: 0.0\nINFO:resnet50_trainer:Finished iteration 156/156 of epoch 0 (205.14 images/sec)\nINFO:resnet50_trainer:Training loss: 6.99336242676, accuracy: 0.0\nINFO:resnet50_trainer:Starting epoch 1/2\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\nwhat():  [enforce fail at /root/caffe2/third_party/gloo/gloo/transport/ibverbs/pair.cc:417] wc->status == IBV_WC_SUCCESS. 12 vs 0. Send for slot 2: transport retry counter exceeded\n*** Aborted at 1522847623 (unix time) try \"date -d @1522847623\" if you are using GNU date ***\nPC: @     0x7f84fc58e428 gsignal\n*** SIGABRT (@0x9a14) received by PID 39444 (TID 0x7f83ebf8d700) from PID 39444; stack trace: ***\n@     0x7f84fc58e4b0 (unknown)\n@     0x7f84fc58e428 gsignal\n@     0x7f84fc59002a abort\n@     0x7f848079584d __gnu_cxx::__verbose_terminate_handler()\n@     0x7f84807936b6 (unknown)\n@     0x7f8480793701 std::terminate()\n@     0x7f84807bed38 (unknown)\n@     0x7f84fc92a6ba start_thread\n@     0x7f84fc66041d clone\n@                0x0 (unknown)\nAborted (core dumped)", "body": "- Caffe2\r\n- OS: Ubuntu 16.04\r\n-- Python version: 2.7\r\n- CUDA/cuDNN version: 9.1 , 7.0.5\r\n- GPU models and configuration: P100\r\n- GCC version (if compiling from source):5.4.0\r\n- Mellanox OFEDversion: 4.2.1\r\n- Build command you used (if compiling from source):\r\n\r\n  Eigen with CUDA9: fatal error\uff1amath_functions.hpp\uff1aNo such file or directory.\r\n\r\nWas fixed in eigen at https://bitbucket.org/eigen/eigen/commits/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58?at=default\r\n\r\n    $ mkdir build\r\n    $ cd build\r\n    $ cmake .. -DCUDA_ARCH_NAME=Manual -DCUDA_ARCH_BIN=\"60 61\" -DCUDA_ARCH_PTX=\"61\" -DUSE_NNPACK=OFF -DUSE_ROCKSDB=OFF -DUSE_GLOO=ON -DUSE_REDIS=ON -DUSE_IBVERBS=ON -DUSE_MPI=OFF\r\n    $ make -j\"$(nproc)\" install\r\n    $ ldconfig\r\n    $ make clean\r\n\r\nI try to run resnet50_trainer.py on one node by: \r\n python /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 64 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 4\r\n\r\nand have result: ~ 430 imgs/sec\r\n\r\nWhen I try to run resnet50_trainer.py between 2 machines using GLOO/Redis, but I get the following error messages. \r\nHow to fix the error and run correctly using mpirun?\r\n\r\nMy commands:\r\nOn each node was ran the sample command:\r\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 1 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 0 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\n\r\npython /root/caffe2/caffe2/python/examples/resnet50_trainer.py --train_data /data/ilsvrc12_train_lmdb/ --test_data /data/ilsvrc12_val_lmdb --batch_size 32 --run_id 1 --epoch_size 10000 --num_epochs 2 --image_size 256 --num_gpus 1 --redis_host 10.143.119.44 --redis_port 5555 --num_shards 2 --shard_id 1 --dtype float16 --float16_compute --distributed_transport ibverbs --distributed_interfaces mlx5_3\r\n\r\nErrors:\r\nINFO:resnet50_trainer:Finished iteration 155/156 of epoch 0 (207.29 images/sec)\r\nINFO:resnet50_trainer:Training loss: 0.000181300914846, accuracy: 1.0\r\nINFO:resnet50_trainer:Finished iteration 156/156 of epoch 0 (204.85 images/sec)\r\nINFO:resnet50_trainer:Training loss: 0.00019393475668, accuracy: 1.0\r\nE0404 16:13:32.042721 39912 prefetch_op.h:110] Prefetching error std::bad_alloc\r\nE0404 16:13:32.042976 39911 prefetch_op.h:83] Prefetching failed.\r\nE0404 16:13:32.043117 39911 net_dag.cc:231] Operator chain failed starting at: input: \"test_reader\" output: \"gpu_0/data\" output: \"gpu_0/label\" name: \"\" type: \"ImageInput\" arg { name: \"std\" f: 128 } arg { name: \"scale\" i: 256 } arg { name: \"use_gpu_transform\" i: 1 } arg { name: \"cudnn_exhaustive_search\" i: 1 } arg { name: \"crop\" i: 256 } arg { name: \"is_test\" i: 1 } arg { name: \"use_cudnn\" i: 1 } arg { name: \"use_caffe_datum\" i: 1 } arg { name: \"mirror\" i: 1 } arg { name: \"output_type\" s: \"float16\" } arg { name: \"batch_size\" i: 32 } arg { name: \"mean\" f: 128 } device_option { device_type: 1 cuda_gpu_id: 0 }\r\nE0404 16:13:32.043376 39646 net.h:54] Failed to execute async run\r\nOriginal python traceback for operator 1886221678 in network `resnet50_test` in exception above (most recent call last):\r\nTraceback (most recent call last):\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 615, in <module>\r\n    main()\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 611, in main\r\n    Train(args)\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 529, in Train\r\n    explog\r\n  File \"/root/caffe2/caffe2/python/examples/resnet50_trainer.py\", line 201, in RunEpoch\r\n    workspace.RunNet(test_model.net.Proto().name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 215, in RunNet\r\n    StringifyNetName(name), num_iter, allow_fail,\r\n  File \"/usr/local/lib/python2.7/dist-packages/caffe2/python/workspace.py\", line 177, in CallWithExceptionIntercept\r\n    return func(*args, **kwargs)\r\nRuntimeError: [enforce fail at pybind_state.cc:1001] success. Error running net resnet50_test \r\n\r\n\r\n\r\nO second node:\r\nINFO:resnet50_trainer:Training loss: 6.98969364166, accuracy: 0.0\r\nINFO:resnet50_trainer:Finished iteration 156/156 of epoch 0 (205.14 images/sec)\r\nINFO:resnet50_trainer:Training loss: 6.99336242676, accuracy: 0.0\r\nINFO:resnet50_trainer:Starting epoch 1/2\r\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\r\n  what():  [enforce fail at /root/caffe2/third_party/gloo/gloo/transport/ibverbs/pair.cc:417] wc->status == IBV_WC_SUCCESS. 12 vs 0. Send for slot 2: transport retry counter exceeded\r\n*** Aborted at 1522847623 (unix time) try \"date -d @1522847623\" if you are using GNU date ***\r\nPC: @     0x7f84fc58e428 gsignal\r\n*** SIGABRT (@0x9a14) received by PID 39444 (TID 0x7f83ebf8d700) from PID 39444; stack trace: ***\r\n    @     0x7f84fc58e4b0 (unknown)\r\n    @     0x7f84fc58e428 gsignal\r\n    @     0x7f84fc59002a abort\r\n    @     0x7f848079584d __gnu_cxx::__verbose_terminate_handler()\r\n    @     0x7f84807936b6 (unknown)\r\n    @     0x7f8480793701 std::terminate()\r\n    @     0x7f84807bed38 (unknown)\r\n    @     0x7f84fc92a6ba start_thread\r\n    @     0x7f84fc66041d clone\r\n    @                0x0 (unknown)\r\nAborted (core dumped)\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}