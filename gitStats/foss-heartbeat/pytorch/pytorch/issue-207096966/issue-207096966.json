{"url": "https://api.github.com/repos/pytorch/pytorch/issues/727", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/727/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/727/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/727/events", "html_url": "https://github.com/pytorch/pytorch/issues/727", "id": 207096966, "node_id": "MDU6SXNzdWUyMDcwOTY5NjY=", "number": 727, "title": "F.relu(inplace) followed by F.dropout(inplace) breaks backward pass", "user": {"login": "andreasveit", "id": 9089561, "node_id": "MDQ6VXNlcjkwODk1NjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/9089561?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andreasveit", "html_url": "https://github.com/andreasveit", "followers_url": "https://api.github.com/users/andreasveit/followers", "following_url": "https://api.github.com/users/andreasveit/following{/other_user}", "gists_url": "https://api.github.com/users/andreasveit/gists{/gist_id}", "starred_url": "https://api.github.com/users/andreasveit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andreasveit/subscriptions", "organizations_url": "https://api.github.com/users/andreasveit/orgs", "repos_url": "https://api.github.com/users/andreasveit/repos", "events_url": "https://api.github.com/users/andreasveit/events{/privacy}", "received_events_url": "https://api.github.com/users/andreasveit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-02-13T00:22:51Z", "updated_at": "2017-02-13T01:01:56Z", "closed_at": "2017-02-13T01:01:56Z", "author_association": "NONE", "body_html": "<p>The following snipped demonstrates the problem:</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn.functional as F\n\nx = Variable(torch.from_numpy(np.random.randn(4, 6, 10,10)).float(), requires_grad=True)\nx = F.relu(x, inplace=True)\nx = F.dropout(x,p=0.1, inplace=True)\nresult = torch.sum(x)\nresult.backward()\n</code></pre>", "body_text": "The following snipped demonstrates the problem:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn.functional as F\n\nx = Variable(torch.from_numpy(np.random.randn(4, 6, 10,10)).float(), requires_grad=True)\nx = F.relu(x, inplace=True)\nx = F.dropout(x,p=0.1, inplace=True)\nresult = torch.sum(x)\nresult.backward()", "body": "The following snipped demonstrates the problem:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport torch.nn.functional as F\r\n\r\nx = Variable(torch.from_numpy(np.random.randn(4, 6, 10,10)).float(), requires_grad=True)\r\nx = F.relu(x, inplace=True)\r\nx = F.dropout(x,p=0.1, inplace=True)\r\nresult = torch.sum(x)\r\nresult.backward()\r\n```"}