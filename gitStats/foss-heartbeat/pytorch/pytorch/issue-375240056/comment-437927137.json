{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/437927137", "html_url": "https://github.com/pytorch/pytorch/issues/13273#issuecomment-437927137", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13273", "id": 437927137, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzkyNzEzNw==", "user": {"login": "phi-go", "id": 28248770, "node_id": "MDQ6VXNlcjI4MjQ4Nzcw", "avatar_url": "https://avatars0.githubusercontent.com/u/28248770?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phi-go", "html_url": "https://github.com/phi-go", "followers_url": "https://api.github.com/users/phi-go/followers", "following_url": "https://api.github.com/users/phi-go/following{/other_user}", "gists_url": "https://api.github.com/users/phi-go/gists{/gist_id}", "starred_url": "https://api.github.com/users/phi-go/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phi-go/subscriptions", "organizations_url": "https://api.github.com/users/phi-go/orgs", "repos_url": "https://api.github.com/users/phi-go/repos", "events_url": "https://api.github.com/users/phi-go/events{/privacy}", "received_events_url": "https://api.github.com/users/phi-go/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-12T15:41:43Z", "updated_at": "2018-11-12T15:41:43Z", "author_association": "NONE", "body_html": "<p>Hi, I'm facing this problem as I want to train a model that depending on the input might not use some parameters/layers. Maybe the following sample code will help. As expected without the changes in the pull request this code fails.</p>\n<pre><code>TypeError: _queue_reduction(): incompatible function arguments. The following argument types are supported:\n    1. (process_group: torch.distributed.ProcessGroup, grads_batch: List[List[at::Tensor]], devices: List[int]) -&gt; Tuple[torch.distributed.Work, at::Tensor]\n\nInvoked with: &lt;torch.distributed.ProcessGroupNCCL object at 0x7fa6dfd804c8&gt;, [[None, None, tensor([[-6.3432e+11, -2.8616e+12, -1.3041e+12],\n        [-3.8208e+12, -1.7236e+13, -7.8549e+12],\n        [-3.4752e+12, -1.5678e+13, -7.1446e+12],\n        [-2.8048e+12, -1.2653e+13, -5.7663e+12]], device='cuda:0'), tensor([ -387039.5000, -2331274.5000, -2120455.7500, -1711401.0000],\n       device='cuda:0')]], [0]\n</code></pre>\n<p>However even with them applied:</p>\n<pre><code>terminate called after throwing an instance of 'gloo::EnforceNotMet'\n  what():  [enforce fail at /opt/conda/conda-bld/pytorch-nightly_1541790704598/work/third_party/gloo/gloo/transport/tcp/pair.cc:456] op.preamble.length &lt;= op.nbytes. 20 vs 16\n</code></pre>\n<p>Sample code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">from</span> torch.multiprocessing <span class=\"pl-k\">import</span> Process\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n\n<span class=\"pl-k\">from</span> torch.nn.parallel <span class=\"pl-k\">import</span> DistributedDataParallel\n<span class=\"pl-k\">from</span> torch.utils.data.distributed <span class=\"pl-k\">import</span> DistributedSampler\n\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CustomDataset</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">utils</span>.<span class=\"pl-e\">data</span>.<span class=\"pl-e\">Dataset</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">data_list</span>):\n        <span class=\"pl-c1\">self</span>.data_list <span class=\"pl-k\">=</span> data_list\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.data_list)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">item</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.data_list[item]\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.some_used_lin <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>)\n        <span class=\"pl-c1\">self</span>.lin <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.squeeze()\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">3</span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.lin(<span class=\"pl-c1\">input</span>)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">return</span> torch.cat((<span class=\"pl-c1\">self</span>.lin(<span class=\"pl-c1\">input</span>[:<span class=\"pl-c1\">3</span>]), <span class=\"pl-c1\">self</span>.some_used_lin(<span class=\"pl-c1\">input</span>[<span class=\"pl-c1\">3</span>].unsqueeze(<span class=\"pl-c1\">0</span>))))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_dataset</span>():\n    data_list <span class=\"pl-k\">=</span> [{<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>: torch.FloatTensor(<span class=\"pl-c1\">3</span> <span class=\"pl-k\">+</span> (i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">2</span>)).random_(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>: torch.FloatTensor(<span class=\"pl-c1\">4</span> <span class=\"pl-k\">+</span> (i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">2</span>)<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>).random_()}\n                 <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>)]\n    <span class=\"pl-k\">return</span> CustomDataset(data_list)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>, <span class=\"pl-smi\">device_name</span>):\n    dataset <span class=\"pl-k\">=</span> get_dataset()\n    distributed_sampler <span class=\"pl-k\">=</span> DistributedSampler(dataset, size, rank)\n    train_loader <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">pin_memory</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>distributed_sampler)\n    device <span class=\"pl-k\">=</span> torch.device(device_name)\n    model <span class=\"pl-k\">=</span> Model()\n    model.to(device)\n    model <span class=\"pl-k\">=</span> DistributedDataParallel(model, <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span>[device])\n    optimizer <span class=\"pl-k\">=</span> torch.optim.Adam(model.parameters())\n\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):\n        model.train()\n        <span class=\"pl-k\">for</span> batch_num, batch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rank<span class=\"pl-pds\">\"</span></span>, rank, batch[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>].shape)\n            optimizer.zero_grad()\n            pred <span class=\"pl-k\">=</span> model(batch[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>].to(device))\n            exp <span class=\"pl-k\">=</span> batch[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>].to(device).squeeze()\n            loss <span class=\"pl-k\">=</span> F.binary_cross_entropy_with_logits(exp, pred)\n            loss.backward()\n            <span class=\"pl-k\">del</span> loss\n\n            optimizer.step()\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">init_processes</span>(<span class=\"pl-smi\">rank</span>, <span class=\"pl-smi\">size</span>, <span class=\"pl-smi\">fn</span>, <span class=\"pl-smi\">ddp_args</span>, <span class=\"pl-smi\">device</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> Initialize the distributed environment. <span class=\"pl-pds\">\"\"\"</span></span>\n    os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MASTER_ADDR<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> ddp_args[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>master_addr<span class=\"pl-pds\">'</span></span>]\n    os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MASTER_PORT<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> ddp_args[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>master_port<span class=\"pl-pds\">'</span></span>]\n    os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>WORLD_SIZE<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">str</span>(size)\n    os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>RANK<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">str</span>(rank)\n    dist.init_process_group(ddp_args[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>backend<span class=\"pl-pds\">'</span></span>], <span class=\"pl-v\">rank</span><span class=\"pl-k\">=</span>rank, <span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span>size)\n    fn(rank, size, device)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>done<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    ddp_args <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>master_addr<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>127.0.0.1<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>master_port<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>29500<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>backend<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gloo<span class=\"pl-pds\">\"</span></span>}\n    devices <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda:0<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda:0<span class=\"pl-pds\">\"</span></span>]\n    size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(devices)\n    processes <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> rank, device <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(devices):\n        p <span class=\"pl-k\">=</span> Process(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>init_processes, <span class=\"pl-v\">args</span><span class=\"pl-k\">=</span>(rank, size, train, ddp_args, device))\n        p.start()\n        processes.append(p)\n\n    <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">any</span>(p.exitcode <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> processes):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> all worker processes have finished</span>\n        time.sleep(<span class=\"pl-c1\">0.2</span>)\n\n    <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> processes:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>join<span class=\"pl-pds\">\"</span></span>)\n        p.join()\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>join done<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>all done<span class=\"pl-pds\">\"</span></span>)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    main()</pre></div>\n<p>Env:</p>\n<pre><code>PyTorch version: 1.0.0.dev20181109\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\n\nOS: Arch Linux\nGCC version: (GCC) 8.2.1 20180831\nCMake version: version 3.12.4\n\nPython version: 3.7\nIs CUDA available: Yes\nCUDA runtime version: 10.0.130\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\nNvidia driver version: 410.66\n</code></pre>", "body_text": "Hi, I'm facing this problem as I want to train a model that depending on the input might not use some parameters/layers. Maybe the following sample code will help. As expected without the changes in the pull request this code fails.\nTypeError: _queue_reduction(): incompatible function arguments. The following argument types are supported:\n    1. (process_group: torch.distributed.ProcessGroup, grads_batch: List[List[at::Tensor]], devices: List[int]) -> Tuple[torch.distributed.Work, at::Tensor]\n\nInvoked with: <torch.distributed.ProcessGroupNCCL object at 0x7fa6dfd804c8>, [[None, None, tensor([[-6.3432e+11, -2.8616e+12, -1.3041e+12],\n        [-3.8208e+12, -1.7236e+13, -7.8549e+12],\n        [-3.4752e+12, -1.5678e+13, -7.1446e+12],\n        [-2.8048e+12, -1.2653e+13, -5.7663e+12]], device='cuda:0'), tensor([ -387039.5000, -2331274.5000, -2120455.7500, -1711401.0000],\n       device='cuda:0')]], [0]\n\nHowever even with them applied:\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\n  what():  [enforce fail at /opt/conda/conda-bld/pytorch-nightly_1541790704598/work/third_party/gloo/gloo/transport/tcp/pair.cc:456] op.preamble.length <= op.nbytes. 20 vs 16\n\nSample code:\nimport os\nimport time\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn.functional as F\nfrom torch.multiprocessing import Process\nfrom torch.utils.data import DataLoader\n\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.utils.data.distributed import DistributedSampler\n\n\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, data_list):\n        self.data_list = data_list\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, item):\n        return self.data_list[item]\n\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.some_used_lin = torch.nn.Linear(1, 2)\n        self.lin = torch.nn.Linear(3, 4)\n\n    def forward(self, input):\n        input = input.squeeze()\n        if input.size(0) == 3:\n            return self.lin(input)\n        else:\n            return torch.cat((self.lin(input[:3]), self.some_used_lin(input[3].unsqueeze(0))))\n\n\ndef get_dataset():\n    data_list = [{\"x\": torch.FloatTensor(3 + (i % 2)).random_(), \"y\": torch.FloatTensor(4 + (i % 2)*2).random_()}\n                 for i in range(10)]\n    return CustomDataset(data_list)\n\n\ndef train(rank, size, device_name):\n    dataset = get_dataset()\n    distributed_sampler = DistributedSampler(dataset, size, rank)\n    train_loader = DataLoader(dataset, pin_memory=True, sampler=distributed_sampler)\n    device = torch.device(device_name)\n    model = Model()\n    model.to(device)\n    model = DistributedDataParallel(model, device_ids=[device])\n    optimizer = torch.optim.Adam(model.parameters())\n\n    for epoch in range(2):\n        model.train()\n        for batch_num, batch in enumerate(train_loader):\n            print(\"rank\", rank, batch['x'].shape)\n            optimizer.zero_grad()\n            pred = model(batch['x'].to(device))\n            exp = batch['y'].to(device).squeeze()\n            loss = F.binary_cross_entropy_with_logits(exp, pred)\n            loss.backward()\n            del loss\n\n            optimizer.step()\n\n\ndef init_processes(rank, size, fn, ddp_args, device):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = ddp_args['master_addr']\n    os.environ['MASTER_PORT'] = ddp_args['master_port']\n    os.environ['WORLD_SIZE'] = str(size)\n    os.environ['RANK'] = str(rank)\n    dist.init_process_group(ddp_args['backend'], rank=rank, world_size=size)\n    fn(rank, size, device)\n    print(\"done\")\n\n\ndef main():\n    ddp_args = {\"master_addr\": \"127.0.0.1\", \"master_port\": \"29500\", \"backend\": \"gloo\"}\n    devices = [\"cuda:0\", \"cuda:0\"]\n    size = len(devices)\n    processes = []\n    for rank, device in enumerate(devices):\n        p = Process(target=init_processes, args=(rank, size, train, ddp_args, device))\n        p.start()\n        processes.append(p)\n\n    while not any(p.exitcode is not None for p in processes):  # all worker processes have finished\n        time.sleep(0.2)\n\n    for p in processes:\n        print(\"join\")\n        p.join()\n        print(\"join done\")\n    print(\"all done\")\n\n\nif __name__ == \"__main__\":\n    main()\nEnv:\nPyTorch version: 1.0.0.dev20181109\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\n\nOS: Arch Linux\nGCC version: (GCC) 8.2.1 20180831\nCMake version: version 3.12.4\n\nPython version: 3.7\nIs CUDA available: Yes\nCUDA runtime version: 10.0.130\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\nNvidia driver version: 410.66", "body": "Hi, I'm facing this problem as I want to train a model that depending on the input might not use some parameters/layers. Maybe the following sample code will help. As expected without the changes in the pull request this code fails.\r\n\r\n```\r\nTypeError: _queue_reduction(): incompatible function arguments. The following argument types are supported:\r\n    1. (process_group: torch.distributed.ProcessGroup, grads_batch: List[List[at::Tensor]], devices: List[int]) -> Tuple[torch.distributed.Work, at::Tensor]\r\n\r\nInvoked with: <torch.distributed.ProcessGroupNCCL object at 0x7fa6dfd804c8>, [[None, None, tensor([[-6.3432e+11, -2.8616e+12, -1.3041e+12],\r\n        [-3.8208e+12, -1.7236e+13, -7.8549e+12],\r\n        [-3.4752e+12, -1.5678e+13, -7.1446e+12],\r\n        [-2.8048e+12, -1.2653e+13, -5.7663e+12]], device='cuda:0'), tensor([ -387039.5000, -2331274.5000, -2120455.7500, -1711401.0000],\r\n       device='cuda:0')]], [0]\r\n```\r\n\r\nHowever even with them applied:\r\n\r\n```\r\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\r\n  what():  [enforce fail at /opt/conda/conda-bld/pytorch-nightly_1541790704598/work/third_party/gloo/gloo/transport/tcp/pair.cc:456] op.preamble.length <= op.nbytes. 20 vs 16\r\n```\r\n\r\nSample code:\r\n\r\n```python\r\nimport os\r\nimport time\r\n\r\nimport torch\r\nimport torch.distributed as dist\r\nimport torch.nn.functional as F\r\nfrom torch.multiprocessing import Process\r\nfrom torch.utils.data import DataLoader\r\n\r\nfrom torch.nn.parallel import DistributedDataParallel\r\nfrom torch.utils.data.distributed import DistributedSampler\r\n\r\n\r\n\r\nclass CustomDataset(torch.utils.data.Dataset):\r\n    def __init__(self, data_list):\r\n        self.data_list = data_list\r\n\r\n    def __len__(self):\r\n        return len(self.data_list)\r\n\r\n    def __getitem__(self, item):\r\n        return self.data_list[item]\r\n\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.some_used_lin = torch.nn.Linear(1, 2)\r\n        self.lin = torch.nn.Linear(3, 4)\r\n\r\n    def forward(self, input):\r\n        input = input.squeeze()\r\n        if input.size(0) == 3:\r\n            return self.lin(input)\r\n        else:\r\n            return torch.cat((self.lin(input[:3]), self.some_used_lin(input[3].unsqueeze(0))))\r\n\r\n\r\ndef get_dataset():\r\n    data_list = [{\"x\": torch.FloatTensor(3 + (i % 2)).random_(), \"y\": torch.FloatTensor(4 + (i % 2)*2).random_()}\r\n                 for i in range(10)]\r\n    return CustomDataset(data_list)\r\n\r\n\r\ndef train(rank, size, device_name):\r\n    dataset = get_dataset()\r\n    distributed_sampler = DistributedSampler(dataset, size, rank)\r\n    train_loader = DataLoader(dataset, pin_memory=True, sampler=distributed_sampler)\r\n    device = torch.device(device_name)\r\n    model = Model()\r\n    model.to(device)\r\n    model = DistributedDataParallel(model, device_ids=[device])\r\n    optimizer = torch.optim.Adam(model.parameters())\r\n\r\n    for epoch in range(2):\r\n        model.train()\r\n        for batch_num, batch in enumerate(train_loader):\r\n            print(\"rank\", rank, batch['x'].shape)\r\n            optimizer.zero_grad()\r\n            pred = model(batch['x'].to(device))\r\n            exp = batch['y'].to(device).squeeze()\r\n            loss = F.binary_cross_entropy_with_logits(exp, pred)\r\n            loss.backward()\r\n            del loss\r\n\r\n            optimizer.step()\r\n\r\n\r\ndef init_processes(rank, size, fn, ddp_args, device):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    os.environ['MASTER_ADDR'] = ddp_args['master_addr']\r\n    os.environ['MASTER_PORT'] = ddp_args['master_port']\r\n    os.environ['WORLD_SIZE'] = str(size)\r\n    os.environ['RANK'] = str(rank)\r\n    dist.init_process_group(ddp_args['backend'], rank=rank, world_size=size)\r\n    fn(rank, size, device)\r\n    print(\"done\")\r\n\r\n\r\ndef main():\r\n    ddp_args = {\"master_addr\": \"127.0.0.1\", \"master_port\": \"29500\", \"backend\": \"gloo\"}\r\n    devices = [\"cuda:0\", \"cuda:0\"]\r\n    size = len(devices)\r\n    processes = []\r\n    for rank, device in enumerate(devices):\r\n        p = Process(target=init_processes, args=(rank, size, train, ddp_args, device))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    while not any(p.exitcode is not None for p in processes):  # all worker processes have finished\r\n        time.sleep(0.2)\r\n\r\n    for p in processes:\r\n        print(\"join\")\r\n        p.join()\r\n        print(\"join done\")\r\n    print(\"all done\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nEnv:\r\n\r\n```\r\nPyTorch version: 1.0.0.dev20181109\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Arch Linux\r\nGCC version: (GCC) 8.2.1 20180831\r\nCMake version: version 3.12.4\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 1080 Ti\r\nNvidia driver version: 410.66\r\n```"}