{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2875", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2875/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2875/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2875/events", "html_url": "https://github.com/pytorch/pytorch/issues/2875", "id": 260856483, "node_id": "MDU6SXNzdWUyNjA4NTY0ODM=", "number": 2875, "title": "Using DistributedDataParallel with modules having no buffers cause errors", "user": {"login": "kamo-naoyuki", "id": 19261024, "node_id": "MDQ6VXNlcjE5MjYxMDI0", "avatar_url": "https://avatars0.githubusercontent.com/u/19261024?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kamo-naoyuki", "html_url": "https://github.com/kamo-naoyuki", "followers_url": "https://api.github.com/users/kamo-naoyuki/followers", "following_url": "https://api.github.com/users/kamo-naoyuki/following{/other_user}", "gists_url": "https://api.github.com/users/kamo-naoyuki/gists{/gist_id}", "starred_url": "https://api.github.com/users/kamo-naoyuki/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kamo-naoyuki/subscriptions", "organizations_url": "https://api.github.com/users/kamo-naoyuki/orgs", "repos_url": "https://api.github.com/users/kamo-naoyuki/repos", "events_url": "https://api.github.com/users/kamo-naoyuki/events{/privacy}", "received_events_url": "https://api.github.com/users/kamo-naoyuki/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 679955625, "node_id": "MDU6TGFiZWw2Nzk5NTU2MjU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/crash", "name": "crash", "color": "d93f0b", "default": false}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-27T06:04:23Z", "updated_at": "2017-09-28T03:49:11Z", "closed_at": "2017-09-28T03:49:11Z", "author_association": "NONE", "body_html": "<p>I'm trying distributed training with pytorch, and I encountered the following  error.</p>\n<pre><code>  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 259, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 154, in forward\n    self._sync_params()\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 183, in _sync_params\n    flat_buffers = _flatten_tensors(buffers)\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/_utils.py\", line 111, in _flatten_tensors\n    flat = tensors[0].new(size)\nIndexError: list index out of range\n</code></pre>\n<p>Reproduce code:</p>\n<pre><code># main.py\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nimport torch.distributed as dist\nimport torch.utils.data.distributed\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\ndef main():\n    BACKEND = os.environ['BACKEND']\n    WORLD_SIZE = os.environ['WORLD_SIZE']\n    INIT_METHOD = os.getenv('INIT_METHOD', 'env://')\n\n    dist.init_process_group(\n            init_method=INIT_METHOD,\n            backend=BACKEND, world_size=int(WORLD_SIZE))\n    train_dataset = datasets.MNIST(root='./torch', train=True, download=True,\n                                   transform=transforms.Compose([\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,),\n                                                            (0.3081,))]))\n    train_sampler =\\\n        torch.utils.data.distributed.DistributedSampler(train_dataset)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, sampler=train_sampler,\n        batch_size=64, shuffle=False, num_workers=0, pin_memory=False)\n\n    model = Net()\n    model.cuda()\n    model = torch.nn.parallel.DistributedDataParallel(model)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    train_sampler.set_epoch(0)\n    model.train()\n    for data, target in train_loader:\n        data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n\nif __name__ == '__main__':\n    import argparse\n    p = argparse.ArgumentParser()\n    main()\n</code></pre>\n<p>Launching shell script:</p>\n<pre><code>#!/bin/sh\n\ndistributed_set_up() {\n  export TEMP_DIR=\"$(mktemp -d)\"\n  rm -rf \"$TEMP_DIR/\"*\n  mkdir \"$TEMP_DIR/barrier\"\n  mkdir \"$TEMP_DIR/test_dir\"\n}\n\ndistributed_tear_down() {\n  rm -rf \"$TEMP_DIR\"\n}\n\n\ndistributed_set_up\nls $TEMP_DIR\nbackend=gloo\nworld_size=2\nfor i in seq 0 $(expr $world_size - 1);do\n    MASTER_ADDR='29500' MASTER_PORT='127.0.0.1' RANK=$i BACKEND=$backend WORLD_SIZE=$world_size INIT_METHOD='file://'$TEMP_DIR'/shared_init_file' python main.py &amp;\ndone\nwait\ndistributed_tear_down\n</code></pre>\n<p>Perhaps you are not taking account modules not having any buffers, so I modified <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L182-L192\">https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L182-L192</a> like this and it works,</p>\n<pre><code>        buffers = list(self.module._all_buffers())                                                                                                                                                  \n        if len(buffers) &gt; 0:                                                                                                                                                                        \n            flat_buffers = _flatten_tensors(buffers)                                                                                                                                                \n            dist.broadcast(flat_buffers, 0)                                                                                                                                                         \n            for buf, synced in zip(buffers, _unflatten_tensors(flat_buffers, buffers)):                                                                                                             \n                buf.copy_(synced)                                                                                                                                                                   \n                                                                                                                                                                                                    \n            # intra-node buffer sync                                                                                                                                                                \n            result = broadcast_coalesced(buffers, self.device_ids, self.broadcast_bucket_size)                                                                                                      \n            for tensors, module in zip(result[1:], self._module_copies[1:]):                                                                                                                        \n                for tensor, buf in zip(tensors, module._all_buffers()):                                                                                                                             \n                    buf.set_(tensor)      \n</code></pre>\n<p>My correction is not right?</p>\n<p>After this, maybe this is another problem, but i got the following message at process shutdown. The training seems to success, so I think this is not a critical problem.</p>\n<pre><code>terminate called after throwing an instance of 'gloo::EnforceNotMet'\n  what():  [enforce fail at /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\n  what():  [enforce fail at /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down\n./run.sh: line 22: 19545 Aborted                 (core dumped) MASTER_ADDR='29500' MASTER_PORT='127.0.0.1' RANK=$i BACKEND=$backend WORLD_SIZE=$world_size INIT_METHOD='file://'$TEMP_DIR'/shared_init_file' python main.py\n./run.sh: line 22: 19546 Aborted                 (core dumped) MASTER_ADDR='29500' MASTER_PORT='127.0.0.1'  RANK=$i BACKEND=$backend WORLD_SIZE=$world_size INIT_METHOD='file://'$TEMP_DIR'/shared_init_file' python main.py\n</code></pre>\n<p>My environment:</p>\n<pre><code>OS: CentOS7\ngcc:  4.8.5 20150623 (Red Hat 4.8.5-11) (GCC) \nPython3.6\nPytorch: de757805fcdbcbb831d51827d72e73e55a49a106 (built by setup.py)\nGPU: GeForce GTX 1080\nCUDA8.0\nCUDNN6.0\nWithout MPI\n</code></pre>", "body_text": "I'm trying distributed training with pytorch, and I encountered the following  error.\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 259, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 154, in forward\n    self._sync_params()\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 183, in _sync_params\n    flat_buffers = _flatten_tensors(buffers)\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/_utils.py\", line 111, in _flatten_tensors\n    flat = tensors[0].new(size)\nIndexError: list index out of range\n\nReproduce code:\n# main.py\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nimport torch.distributed as dist\nimport torch.utils.data.distributed\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\ndef main():\n    BACKEND = os.environ['BACKEND']\n    WORLD_SIZE = os.environ['WORLD_SIZE']\n    INIT_METHOD = os.getenv('INIT_METHOD', 'env://')\n\n    dist.init_process_group(\n            init_method=INIT_METHOD,\n            backend=BACKEND, world_size=int(WORLD_SIZE))\n    train_dataset = datasets.MNIST(root='./torch', train=True, download=True,\n                                   transform=transforms.Compose([\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,),\n                                                            (0.3081,))]))\n    train_sampler =\\\n        torch.utils.data.distributed.DistributedSampler(train_dataset)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, sampler=train_sampler,\n        batch_size=64, shuffle=False, num_workers=0, pin_memory=False)\n\n    model = Net()\n    model.cuda()\n    model = torch.nn.parallel.DistributedDataParallel(model)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    train_sampler.set_epoch(0)\n    model.train()\n    for data, target in train_loader:\n        data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n\nif __name__ == '__main__':\n    import argparse\n    p = argparse.ArgumentParser()\n    main()\n\nLaunching shell script:\n#!/bin/sh\n\ndistributed_set_up() {\n  export TEMP_DIR=\"$(mktemp -d)\"\n  rm -rf \"$TEMP_DIR/\"*\n  mkdir \"$TEMP_DIR/barrier\"\n  mkdir \"$TEMP_DIR/test_dir\"\n}\n\ndistributed_tear_down() {\n  rm -rf \"$TEMP_DIR\"\n}\n\n\ndistributed_set_up\nls $TEMP_DIR\nbackend=gloo\nworld_size=2\nfor i in seq 0 $(expr $world_size - 1);do\n    MASTER_ADDR='29500' MASTER_PORT='127.0.0.1' RANK=$i BACKEND=$backend WORLD_SIZE=$world_size INIT_METHOD='file://'$TEMP_DIR'/shared_init_file' python main.py &\ndone\nwait\ndistributed_tear_down\n\nPerhaps you are not taking account modules not having any buffers, so I modified https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L182-L192 like this and it works,\n        buffers = list(self.module._all_buffers())                                                                                                                                                  \n        if len(buffers) > 0:                                                                                                                                                                        \n            flat_buffers = _flatten_tensors(buffers)                                                                                                                                                \n            dist.broadcast(flat_buffers, 0)                                                                                                                                                         \n            for buf, synced in zip(buffers, _unflatten_tensors(flat_buffers, buffers)):                                                                                                             \n                buf.copy_(synced)                                                                                                                                                                   \n                                                                                                                                                                                                    \n            # intra-node buffer sync                                                                                                                                                                \n            result = broadcast_coalesced(buffers, self.device_ids, self.broadcast_bucket_size)                                                                                                      \n            for tensors, module in zip(result[1:], self._module_copies[1:]):                                                                                                                        \n                for tensor, buf in zip(tensors, module._all_buffers()):                                                                                                                             \n                    buf.set_(tensor)      \n\nMy correction is not right?\nAfter this, maybe this is another problem, but i got the following message at process shutdown. The training seems to success, so I think this is not a critical problem.\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\n  what():  [enforce fail at /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\n  what():  [enforce fail at /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down\n./run.sh: line 22: 19545 Aborted                 (core dumped) MASTER_ADDR='29500' MASTER_PORT='127.0.0.1' RANK=$i BACKEND=$backend WORLD_SIZE=$world_size INIT_METHOD='file://'$TEMP_DIR'/shared_init_file' python main.py\n./run.sh: line 22: 19546 Aborted                 (core dumped) MASTER_ADDR='29500' MASTER_PORT='127.0.0.1'  RANK=$i BACKEND=$backend WORLD_SIZE=$world_size INIT_METHOD='file://'$TEMP_DIR'/shared_init_file' python main.py\n\nMy environment:\nOS: CentOS7\ngcc:  4.8.5 20150623 (Red Hat 4.8.5-11) (GCC) \nPython3.6\nPytorch: de757805fcdbcbb831d51827d72e73e55a49a106 (built by setup.py)\nGPU: GeForce GTX 1080\nCUDA8.0\nCUDNN6.0\nWithout MPI", "body": "I'm trying distributed training with pytorch, and I encountered the following  error.\r\n\r\n```\r\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 259, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 154, in forward\r\n    self._sync_params()\r\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 183, in _sync_params\r\n    flat_buffers = _flatten_tensors(buffers)\r\n  File \"/home.local/kamo/.anyenv/envs/pyenv/versions/torch2/lib/python3.6/site-packages/torch/_utils.py\", line 111, in _flatten_tensors\r\n    flat = tensors[0].new(size)\r\nIndexError: list index out of range\r\n```\r\n\r\nReproduce code:\r\n```\r\n# main.py\r\nimport os\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\nfrom torch.autograd import Variable\r\nimport torch.distributed as dist\r\nimport torch.utils.data.distributed\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.conv2_drop = nn.Dropout2d()\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n        x = x.view(-1, 320)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x)\r\n\r\ndef main():\r\n    BACKEND = os.environ['BACKEND']\r\n    WORLD_SIZE = os.environ['WORLD_SIZE']\r\n    INIT_METHOD = os.getenv('INIT_METHOD', 'env://')\r\n\r\n    dist.init_process_group(\r\n            init_method=INIT_METHOD,\r\n            backend=BACKEND, world_size=int(WORLD_SIZE))\r\n    train_dataset = datasets.MNIST(root='./torch', train=True, download=True,\r\n                                   transform=transforms.Compose([\r\n                                       transforms.ToTensor(),\r\n                                       transforms.Normalize((0.1307,),\r\n                                                            (0.3081,))]))\r\n    train_sampler =\\\r\n        torch.utils.data.distributed.DistributedSampler(train_dataset)\r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset, sampler=train_sampler,\r\n        batch_size=64, shuffle=False, num_workers=0, pin_memory=False)\r\n\r\n    model = Net()\r\n    model.cuda()\r\n    model = torch.nn.parallel.DistributedDataParallel(model)\r\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\r\n\r\n    train_sampler.set_epoch(0)\r\n    model.train()\r\n    for data, target in train_loader:\r\n        data, target = data.cuda(), target.cuda()\r\n        data, target = Variable(data), Variable(target)\r\n        optimizer.zero_grad()\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\nif __name__ == '__main__':\r\n    import argparse\r\n    p = argparse.ArgumentParser()\r\n    main()\r\n```\r\n\r\nLaunching shell script:\r\n```\r\n#!/bin/sh\r\n\r\ndistributed_set_up() {\r\n  export TEMP_DIR=\"$(mktemp -d)\"\r\n  rm -rf \"$TEMP_DIR/\"*\r\n  mkdir \"$TEMP_DIR/barrier\"\r\n  mkdir \"$TEMP_DIR/test_dir\"\r\n}\r\n\r\ndistributed_tear_down() {\r\n  rm -rf \"$TEMP_DIR\"\r\n}\r\n\r\n\r\ndistributed_set_up\r\nls $TEMP_DIR\r\nbackend=gloo\r\nworld_size=2\r\nfor i in seq 0 $(expr $world_size - 1);do\r\n    MASTER_ADDR='29500' MASTER_PORT='127.0.0.1' RANK=$i BACKEND=$backend WORLD_SIZE=$world_size INIT_METHOD='file://'$TEMP_DIR'/shared_init_file' python main.py &\r\ndone\r\nwait\r\ndistributed_tear_down\r\n```\r\n\r\nPerhaps you are not taking account modules not having any buffers, so I modified https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L182-L192 like this and it works,\r\n\r\n```\r\n        buffers = list(self.module._all_buffers())                                                                                                                                                  \r\n        if len(buffers) > 0:                                                                                                                                                                        \r\n            flat_buffers = _flatten_tensors(buffers)                                                                                                                                                \r\n            dist.broadcast(flat_buffers, 0)                                                                                                                                                         \r\n            for buf, synced in zip(buffers, _unflatten_tensors(flat_buffers, buffers)):                                                                                                             \r\n                buf.copy_(synced)                                                                                                                                                                   \r\n                                                                                                                                                                                                    \r\n            # intra-node buffer sync                                                                                                                                                                \r\n            result = broadcast_coalesced(buffers, self.device_ids, self.broadcast_bucket_size)                                                                                                      \r\n            for tensors, module in zip(result[1:], self._module_copies[1:]):                                                                                                                        \r\n                for tensor, buf in zip(tensors, module._all_buffers()):                                                                                                                             \r\n                    buf.set_(tensor)      \r\n```\r\nMy correction is not right?\r\n\r\nAfter this, maybe this is another problem, but i got the following message at process shutdown. The training seems to success, so I think this is not a critical problem.\r\n\r\n```\r\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\r\n  what():  [enforce fail at /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down\r\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\r\n  what():  [enforce fail at /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home.local/kamo/work/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down\r\n./run.sh: line 22: 19545 Aborted                 (core dumped) MASTER_ADDR='29500' MASTER_PORT='127.0.0.1' RANK=$i BACKEND=$backend WORLD_SIZE=$world_size INIT_METHOD='file://'$TEMP_DIR'/shared_init_file' python main.py\r\n./run.sh: line 22: 19546 Aborted                 (core dumped) MASTER_ADDR='29500' MASTER_PORT='127.0.0.1'  RANK=$i BACKEND=$backend WORLD_SIZE=$world_size INIT_METHOD='file://'$TEMP_DIR'/shared_init_file' python main.py\r\n```\r\n\r\nMy environment:\r\n\r\n```\r\nOS: CentOS7\r\ngcc:  4.8.5 20150623 (Red Hat 4.8.5-11) (GCC) \r\nPython3.6\r\nPytorch: de757805fcdbcbb831d51827d72e73e55a49a106 (built by setup.py)\r\nGPU: GeForce GTX 1080\r\nCUDA8.0\r\nCUDNN6.0\r\nWithout MPI\r\n```\r\n\r\n"}