{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/132451231", "pull_request_review_id": 55532503, "id": 132451231, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMjQ1MTIzMQ==", "diff_hunk": "@@ -71,7 +71,10 @@ def safe_zeros_backward(inp, dim):\n             exclusive_normal = exclusive_normal_nocp.cumprod(dim)\n \n             def reverse_dim(var, dim):\n-                return var.index_select(dim, Variable(torch.arange(var.size(dim) - 1, -1, -1)).long())\n+                index = Variable(torch.arange(var.size(dim) - 1, -1, -1)).long()\n+                if var.is_cuda:\n+                    index = index.cuda()", "path": "torch/autograd/_functions/reduce.py", "position": null, "original_position": 7, "commit_id": "47fee489814fe5cc79e7dc22d4b92b06a3b04a1d", "original_commit_id": "2e8062825820eb9e353067b1dcf5d1ee5aa1d9a9", "user": {"login": "Stonesjtu", "id": 4556044, "node_id": "MDQ6VXNlcjQ1NTYwNDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4556044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Stonesjtu", "html_url": "https://github.com/Stonesjtu", "followers_url": "https://api.github.com/users/Stonesjtu/followers", "following_url": "https://api.github.com/users/Stonesjtu/following{/other_user}", "gists_url": "https://api.github.com/users/Stonesjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Stonesjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Stonesjtu/subscriptions", "organizations_url": "https://api.github.com/users/Stonesjtu/orgs", "repos_url": "https://api.github.com/users/Stonesjtu/repos", "events_url": "https://api.github.com/users/Stonesjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/Stonesjtu/received_events", "type": "User", "site_admin": false}, "body": "Exactly what I'm looking for, it seems more compact. \r\nBut this recipe still doesn't work if the current device is different from the device you want to send the `index` to.\r\n\r\nYou can try this:\r\n```python\r\nvar = Variable(torch.rand(5,5).cuda(1))\r\n# ensure the current deviece\r\n# torch.cuda.set_device(0)\r\ntorch.arange(var.size(dim) -1, -1, -1, out=var.data.new().long()) # Error raised\r\n```", "created_at": "2017-08-10T13:25:39Z", "updated_at": "2018-11-23T15:34:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/2353#discussion_r132451231", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2353", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/132451231"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2353#discussion_r132451231"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2353"}}, "body_html": "<p>Exactly what I'm looking for, it seems more compact.<br>\nBut this recipe still doesn't work if the current device is different from the device you want to send the <code>index</code> to.</p>\n<p>You can try this:</p>\n<div class=\"highlight highlight-source-python\"><pre>var <span class=\"pl-k\">=</span> Variable(torch.rand(<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">5</span>).cuda(<span class=\"pl-c1\">1</span>))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ensure the current deviece</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> torch.cuda.set_device(0)</span>\ntorch.arange(var.size(dim) <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">out</span><span class=\"pl-k\">=</span>var.data.new().long()) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Error raised</span></pre></div>", "body_text": "Exactly what I'm looking for, it seems more compact.\nBut this recipe still doesn't work if the current device is different from the device you want to send the index to.\nYou can try this:\nvar = Variable(torch.rand(5,5).cuda(1))\n# ensure the current deviece\n# torch.cuda.set_device(0)\ntorch.arange(var.size(dim) -1, -1, -1, out=var.data.new().long()) # Error raised", "in_reply_to_id": 132419500}