{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1059", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1059/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1059/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1059/events", "html_url": "https://github.com/pytorch/pytorch/issues/1059", "id": 216041401, "node_id": "MDU6SXNzdWUyMTYwNDE0MDE=", "number": 1059, "title": "RuntimeError: CUDNN_STATUS_NOT_SUPPORTED when batch_size is larger than 1024", "user": {"login": "chenyuntc", "id": 9301117, "node_id": "MDQ6VXNlcjkzMDExMTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/9301117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenyuntc", "html_url": "https://github.com/chenyuntc", "followers_url": "https://api.github.com/users/chenyuntc/followers", "following_url": "https://api.github.com/users/chenyuntc/following{/other_user}", "gists_url": "https://api.github.com/users/chenyuntc/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenyuntc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenyuntc/subscriptions", "organizations_url": "https://api.github.com/users/chenyuntc/orgs", "repos_url": "https://api.github.com/users/chenyuntc/repos", "events_url": "https://api.github.com/users/chenyuntc/events{/privacy}", "received_events_url": "https://api.github.com/users/chenyuntc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-22T12:12:30Z", "updated_at": "2017-03-22T12:18:04Z", "closed_at": "2017-03-22T12:18:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>when the batch size is 1024, my model work fine, but when I change it to 1025, it throw error</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">RuntimeError</span>: <span class=\"pl-c1\">CUDNN_STATUS_NOT_SUPPORTED</span></pre></div>\n<p>this is how I load data:</p>\n<div class=\"highlight highlight-source-python\"><pre>val_dataloader<span class=\"pl-k\">=</span>t.utils.data.DataLoader(val_dataset,<span class=\"pl-c1\">1025</span>,<span class=\"pl-c1\">True</span>,<span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">8</span>,\\\n        <span class=\"pl-v\">collate_fn</span><span class=\"pl-k\">=</span>my_collate)</pre></div>\n<p>the full trace is here:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">4</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">91b240474ca9</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> forward(<span class=\"pl-c1\">self</span>, <span class=\"pl-c1\">input</span>)\n     <span class=\"pl-c1\">21</span>         <span class=\"pl-c1\">self</span>.model.add_module(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv5<span class=\"pl-pds\">'</span></span>,nn.Conv2d(opt.ndf<span class=\"pl-k\">*</span><span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))\n     <span class=\"pl-c1\">22</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,<span class=\"pl-smi\">input</span>):\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">23</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.model(<span class=\"pl-c1\">input</span>)\n     <span class=\"pl-c1\">24</span>         gpuids<span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>\n     <span class=\"pl-c1\">25</span>         <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.ngpu:\n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>modules<span class=\"pl-k\">/</span>module.pyc <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-c1\">self</span>, <span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">208</span> \n    <span class=\"pl-c1\">209</span>     <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">input</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">210</span>         result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.forward(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">211</span>         <span class=\"pl-k\">for</span> hook <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>._forward_hooks.values():\n    <span class=\"pl-c1\">212</span>             hook_result <span class=\"pl-k\">=</span> hook(<span class=\"pl-c1\">self</span>, <span class=\"pl-c1\">input</span>, result)\n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>modules<span class=\"pl-k\">/</span>container.pyc <span class=\"pl-k\">in</span> forward(<span class=\"pl-c1\">self</span>, <span class=\"pl-c1\">input</span>)\n     <span class=\"pl-c1\">61</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n     <span class=\"pl-c1\">62</span>         <span class=\"pl-k\">for</span> module <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>._modules.values():\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">63</span>             <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> module(<span class=\"pl-c1\">input</span>)\n     <span class=\"pl-c1\">64</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">input</span>\n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>modules<span class=\"pl-k\">/</span>module.pyc <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-c1\">self</span>, <span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">208</span> \n    <span class=\"pl-c1\">209</span>     <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">input</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">210</span>         result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.forward(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">211</span>         <span class=\"pl-k\">for</span> hook <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>._forward_hooks.values():\n    <span class=\"pl-c1\">212</span>             hook_result <span class=\"pl-k\">=</span> hook(<span class=\"pl-c1\">self</span>, <span class=\"pl-c1\">input</span>, result)\n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>modules<span class=\"pl-k\">/</span>batchnorm.pyc <span class=\"pl-k\">in</span> forward(<span class=\"pl-c1\">self</span>, <span class=\"pl-c1\">input</span>)\n     <span class=\"pl-c1\">41</span>         <span class=\"pl-k\">return</span> F.batch_norm(\n     <span class=\"pl-c1\">42</span>             <span class=\"pl-c1\">input</span>, <span class=\"pl-c1\">self</span>.running_mean, <span class=\"pl-c1\">self</span>.running_var, <span class=\"pl-c1\">self</span>.weight, <span class=\"pl-c1\">self</span>.bias,\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">43</span>             <span class=\"pl-c1\">self</span>.training, <span class=\"pl-c1\">self</span>.momentum, <span class=\"pl-c1\">self</span>.eps)\n     <span class=\"pl-c1\">44</span> \n     <span class=\"pl-c1\">45</span>     <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__repr__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>functional.pyc <span class=\"pl-k\">in</span> batch_norm(<span class=\"pl-c1\">input</span>, running_mean, running_var, weight, bias, training, momentum, eps)\n    <span class=\"pl-c1\">483</span>     state <span class=\"pl-k\">=</span> _functions.batchnorm.BatchNorm(\n    <span class=\"pl-c1\">484</span>         running_mean, running_var, training, momentum, eps)\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">485</span>     <span class=\"pl-k\">return</span> weight <span class=\"pl-k\">and</span> state(<span class=\"pl-c1\">input</span>, weight, bias) <span class=\"pl-k\">or</span> state(<span class=\"pl-c1\">input</span>)\n    <span class=\"pl-c1\">486</span> \n    <span class=\"pl-c1\">487</span> \n\n<span class=\"pl-k\">/</span>usr<span class=\"pl-k\">/</span>local<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python2.7<span class=\"pl-k\">/</span>dist<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>_functions<span class=\"pl-k\">/</span>batchnorm.pyc <span class=\"pl-k\">in</span> forward(<span class=\"pl-c1\">self</span>, <span class=\"pl-c1\">input</span>, weight, bias)\n     <span class=\"pl-c1\">34</span>                 <span class=\"pl-c1\">input</span>, output, weight, bias,\n     <span class=\"pl-c1\">35</span>                 <span class=\"pl-c1\">self</span>.running_mean, <span class=\"pl-c1\">self</span>.running_var, <span class=\"pl-c1\">self</span>._save_mean,\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">36</span>                 <span class=\"pl-c1\">self</span>._save_std, <span class=\"pl-c1\">self</span>.training, <span class=\"pl-c1\">self</span>.momentum, <span class=\"pl-c1\">self</span>.eps)\n     <span class=\"pl-c1\">37</span>         <span class=\"pl-k\">else</span>:\n     <span class=\"pl-c1\">38</span>             backend <span class=\"pl-k\">=</span> type2backend[<span class=\"pl-c1\">type</span>(<span class=\"pl-c1\">input</span>)]\n\n<span class=\"pl-c1\">RuntimeError</span>: <span class=\"pl-c1\">CUDNN_STATUS_NOT_SUPPORTED</span>\n</pre></div>", "body_text": "when the batch size is 1024, my model work fine, but when I change it to 1025, it throw error\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED\nthis is how I load data:\nval_dataloader=t.utils.data.DataLoader(val_dataset,1025,True,num_workers=8,\\\n        collate_fn=my_collate)\nthe full trace is here:\n<ipython-input-4-91b240474ca9> in forward(self, input)\n     21         self.model.add_module('conv5',nn.Conv2d(opt.ndf*8,1,4,1,0,bias=False))\n     22     def forward(self,input):\n---> 23         return self.model(input)\n     24         gpuids=None\n     25         if self.ngpu:\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    208 \n    209     def __call__(self, *input, **kwargs):\n--> 210         result = self.forward(*input, **kwargs)\n    211         for hook in self._forward_hooks.values():\n    212             hook_result = hook(self, input, result)\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.pyc in forward(self, input)\n     61     def forward(self, input):\n     62         for module in self._modules.values():\n---> 63             input = module(input)\n     64         return input\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\n    208 \n    209     def __call__(self, *input, **kwargs):\n--> 210         result = self.forward(*input, **kwargs)\n    211         for hook in self._forward_hooks.values():\n    212             hook_result = hook(self, input, result)\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/batchnorm.pyc in forward(self, input)\n     41         return F.batch_norm(\n     42             input, self.running_mean, self.running_var, self.weight, self.bias,\n---> 43             self.training, self.momentum, self.eps)\n     44 \n     45     def __repr__(self):\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/functional.pyc in batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)\n    483     state = _functions.batchnorm.BatchNorm(\n    484         running_mean, running_var, training, momentum, eps)\n--> 485     return weight and state(input, weight, bias) or state(input)\n    486 \n    487 \n\n/usr/local/lib/python2.7/dist-packages/torch/nn/_functions/batchnorm.pyc in forward(self, input, weight, bias)\n     34                 input, output, weight, bias,\n     35                 self.running_mean, self.running_var, self._save_mean,\n---> 36                 self._save_std, self.training, self.momentum, self.eps)\n     37         else:\n     38             backend = type2backend[type(input)]\n\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED", "body": "when the batch size is 1024, my model work fine, but when I change it to 1025, it throw error \r\n```python\r\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED\r\n```\r\nthis is how I load data:\r\n```python\r\nval_dataloader=t.utils.data.DataLoader(val_dataset,1025,True,num_workers=8,\\\r\n        collate_fn=my_collate)\r\n```\r\nthe full trace is here:\r\n```python\r\n<ipython-input-4-91b240474ca9> in forward(self, input)\r\n     21         self.model.add_module('conv5',nn.Conv2d(opt.ndf*8,1,4,1,0,bias=False))\r\n     22     def forward(self,input):\r\n---> 23         return self.model(input)\r\n     24         gpuids=None\r\n     25         if self.ngpu:\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\r\n    208 \r\n    209     def __call__(self, *input, **kwargs):\r\n--> 210         result = self.forward(*input, **kwargs)\r\n    211         for hook in self._forward_hooks.values():\r\n    212             hook_result = hook(self, input, result)\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/container.pyc in forward(self, input)\r\n     61     def forward(self, input):\r\n     62         for module in self._modules.values():\r\n---> 63             input = module(input)\r\n     64         return input\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in __call__(self, *input, **kwargs)\r\n    208 \r\n    209     def __call__(self, *input, **kwargs):\r\n--> 210         result = self.forward(*input, **kwargs)\r\n    211         for hook in self._forward_hooks.values():\r\n    212             hook_result = hook(self, input, result)\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/batchnorm.pyc in forward(self, input)\r\n     41         return F.batch_norm(\r\n     42             input, self.running_mean, self.running_var, self.weight, self.bias,\r\n---> 43             self.training, self.momentum, self.eps)\r\n     44 \r\n     45     def __repr__(self):\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/functional.pyc in batch_norm(input, running_mean, running_var, weight, bias, training, momentum, eps)\r\n    483     state = _functions.batchnorm.BatchNorm(\r\n    484         running_mean, running_var, training, momentum, eps)\r\n--> 485     return weight and state(input, weight, bias) or state(input)\r\n    486 \r\n    487 \r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/_functions/batchnorm.pyc in forward(self, input, weight, bias)\r\n     34                 input, output, weight, bias,\r\n     35                 self.running_mean, self.running_var, self._save_mean,\r\n---> 36                 self._save_std, self.training, self.momentum, self.eps)\r\n     37         else:\r\n     38             backend = type2backend[type(input)]\r\n\r\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED\r\n\r\n```"}