{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/303606067", "html_url": "https://github.com/pytorch/pytorch/issues/1623#issuecomment-303606067", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1623", "id": 303606067, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzYwNjA2Nw==", "user": {"login": "gongbudaizhe", "id": 5178646, "node_id": "MDQ6VXNlcjUxNzg2NDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/5178646?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gongbudaizhe", "html_url": "https://github.com/gongbudaizhe", "followers_url": "https://api.github.com/users/gongbudaizhe/followers", "following_url": "https://api.github.com/users/gongbudaizhe/following{/other_user}", "gists_url": "https://api.github.com/users/gongbudaizhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/gongbudaizhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gongbudaizhe/subscriptions", "organizations_url": "https://api.github.com/users/gongbudaizhe/orgs", "repos_url": "https://api.github.com/users/gongbudaizhe/repos", "events_url": "https://api.github.com/users/gongbudaizhe/events{/privacy}", "received_events_url": "https://api.github.com/users/gongbudaizhe/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-24T03:13:09Z", "updated_at": "2017-05-24T03:13:09Z", "author_association": "NONE", "body_html": "<p>After some experimentation, I have pin pointed where exactly the error comes from. It seems batch norm in cudnn 5.1.10 doesn't support eps as small as 1e-6.</p>\n<p>A small test case would be:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">TestModel</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-c1\">super</span>(TestModel, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n    <span class=\"pl-c1\">self</span>.bn <span class=\"pl-k\">=</span> nn.BatchNorm2d(<span class=\"pl-c1\">96</span>, <span class=\"pl-v\">eps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-6</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Bug here</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> self.bn = nn.BatchNorm2d(96, eps=1e-5) # No Bug version</span>\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n    out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.bn(x)\n    <span class=\"pl-k\">return</span> out\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n  feat <span class=\"pl-k\">=</span> Variable(torch.rand((<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">96</span>, <span class=\"pl-c1\">59</span>, <span class=\"pl-c1\">59</span>))).cuda()\n  model <span class=\"pl-k\">=</span> TestModel().cuda()\n  out <span class=\"pl-k\">=</span> model(feat)</pre></div>", "body_text": "After some experimentation, I have pin pointed where exactly the error comes from. It seems batch norm in cudnn 5.1.10 doesn't support eps as small as 1e-6.\nA small test case would be:\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nclass TestModel(nn.Module):\n  def __init__(self):\n    super(TestModel, self).__init__()\n    self.bn = nn.BatchNorm2d(96, eps=1e-6) # Bug here\n    # self.bn = nn.BatchNorm2d(96, eps=1e-5) # No Bug version\n\n  def forward(self, x):\n    out = self.bn(x)\n    return out\n\nif __name__ == '__main__':\n  feat = Variable(torch.rand((8, 96, 59, 59))).cuda()\n  model = TestModel().cuda()\n  out = model(feat)", "body": "After some experimentation, I have pin pointed where exactly the error comes from. It seems batch norm in cudnn 5.1.10 doesn't support eps as small as 1e-6. \r\n\r\nA small test case would be:\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\nclass TestModel(nn.Module):\r\n  def __init__(self):\r\n    super(TestModel, self).__init__()\r\n    self.bn = nn.BatchNorm2d(96, eps=1e-6) # Bug here\r\n    # self.bn = nn.BatchNorm2d(96, eps=1e-5) # No Bug version\r\n\r\n  def forward(self, x):\r\n    out = self.bn(x)\r\n    return out\r\n\r\nif __name__ == '__main__':\r\n  feat = Variable(torch.rand((8, 96, 59, 59))).cuda()\r\n  model = TestModel().cuda()\r\n  out = model(feat)\r\n```"}