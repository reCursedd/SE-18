{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6535", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6535/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6535/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6535/events", "html_url": "https://github.com/pytorch/pytorch/issues/6535", "id": 313554400, "node_id": "MDU6SXNzdWUzMTM1NTQ0MDA=", "number": 6535, "title": "[PyTorch] autograd.grad silently sets allow_unused=True", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-04-12T02:30:58Z", "updated_at": "2018-04-12T19:31:24Z", "closed_at": "2018-04-12T19:31:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Why do we deprecate <code>allow_unused</code>? Even if we deprecate it, why do we <strong>silently</strong> make it True? This will break sooo many code without ever throwing a warning/error (if you use <code>allow_unused=False</code> which is the <strong>default</strong>). The worst thing is that on the doc signature it even says <code>torch.autograd.grad(..., allow_unsued=False)</code>. This change in behavior is nowhere to be found on the doc. Isn't this a serious regression?</p>\n<p>Sorry for the rant. But I just wasted a day's computation on this, without knowing that all my <code>autograd.grad</code> operations are silently returning <code>None</code> on 0.4.</p>\n<p>Repro:<br>\nv0.3.1:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.autograd.Variable(torch.randn(<span class=\"pl-c1\">3</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> y <span class=\"pl-k\">=</span> torch.autograd.Variable(torch.randn(<span class=\"pl-c1\">3</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.autograd.grad(y.sum(), x)\n<span class=\"pl-c1\">RuntimeError</span>: One of the differentiated Variables appears to <span class=\"pl-k\">not</span> have been used <span class=\"pl-k\">in</span> the graph\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> z<span class=\"pl-k\">=</span> x.sum()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.autograd.grad(y.sum(), x)\n<span class=\"pl-c1\">RuntimeError</span>: differentiated <span class=\"pl-c1\">input</span> <span class=\"pl-k\">is</span> unreachable</pre></div>\n<p>(also I think these two should state the same error message.)</p>\n<p>master:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> y <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.autograd.grad(y.sum(), x)\n(<span class=\"pl-c1\">None</span>,)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> z <span class=\"pl-k\">=</span> x.sum()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.autograd.grad(y.sum(), x)\n(<span class=\"pl-c1\">None</span>,)</pre></div>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> for the PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"289042703\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4690\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4690/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4690\">#4690</a></p>", "body_text": "Why do we deprecate allow_unused? Even if we deprecate it, why do we silently make it True? This will break sooo many code without ever throwing a warning/error (if you use allow_unused=False which is the default). The worst thing is that on the doc signature it even says torch.autograd.grad(..., allow_unsued=False). This change in behavior is nowhere to be found on the doc. Isn't this a serious regression?\nSorry for the rant. But I just wasted a day's computation on this, without knowing that all my autograd.grad operations are silently returning None on 0.4.\nRepro:\nv0.3.1:\n>>> import torch\n>>> x = torch.autograd.Variable(torch.randn(3), requires_grad=True)\n>>> y = torch.autograd.Variable(torch.randn(3), requires_grad=True)\n>>> torch.autograd.grad(y.sum(), x)\nRuntimeError: One of the differentiated Variables appears to not have been used in the graph\n>>> z= x.sum()\n>>> torch.autograd.grad(y.sum(), x)\nRuntimeError: differentiated input is unreachable\n(also I think these two should state the same error message.)\nmaster:\n>>> import torch\n>>> x = torch.randn(3, requires_grad=True)\n>>> y = torch.randn(3, requires_grad=True)\n>>> torch.autograd.grad(y.sum(), x)\n(None,)\n>>> z = x.sum()\n>>> torch.autograd.grad(y.sum(), x)\n(None,)\ncc @apaszke for the PR #4690", "body": "Why do we deprecate `allow_unused`? Even if we deprecate it, why do we **silently** make it True? This will break sooo many code without ever throwing a warning/error (if you use `allow_unused=False` which is the **default**). The worst thing is that on the doc signature it even says `torch.autograd.grad(..., allow_unsued=False)`. This change in behavior is nowhere to be found on the doc. Isn't this a serious regression?\r\n\r\nSorry for the rant. But I just wasted a day's computation on this, without knowing that all my `autograd.grad` operations are silently returning `None` on 0.4.\r\n\r\nRepro:\r\nv0.3.1:\r\n```python\r\n>>> import torch\r\n>>> x = torch.autograd.Variable(torch.randn(3), requires_grad=True)\r\n>>> y = torch.autograd.Variable(torch.randn(3), requires_grad=True)\r\n>>> torch.autograd.grad(y.sum(), x)\r\nRuntimeError: One of the differentiated Variables appears to not have been used in the graph\r\n>>> z= x.sum()\r\n>>> torch.autograd.grad(y.sum(), x)\r\nRuntimeError: differentiated input is unreachable\r\n```\r\n(also I think these two should state the same error message.)\r\n\r\nmaster:\r\n```python\r\n>>> import torch\r\n>>> x = torch.randn(3, requires_grad=True)\r\n>>> y = torch.randn(3, requires_grad=True)\r\n>>> torch.autograd.grad(y.sum(), x)\r\n(None,)\r\n>>> z = x.sum()\r\n>>> torch.autograd.grad(y.sum(), x)\r\n(None,)\r\n```\r\n\r\ncc @apaszke for the PR #4690 "}