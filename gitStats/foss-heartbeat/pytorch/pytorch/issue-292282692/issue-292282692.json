{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4903", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4903/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4903/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4903/events", "html_url": "https://github.com/pytorch/pytorch/issues/4903", "id": 292282692, "node_id": "MDU6SXNzdWUyOTIyODI2OTI=", "number": 4903, "title": "`with torch.cuda.device(gpu)` creates context on the wrong GPU", "user": {"login": "shubho", "id": 1183609, "node_id": "MDQ6VXNlcjExODM2MDk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1183609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shubho", "html_url": "https://github.com/shubho", "followers_url": "https://api.github.com/users/shubho/followers", "following_url": "https://api.github.com/users/shubho/following{/other_user}", "gists_url": "https://api.github.com/users/shubho/gists{/gist_id}", "starred_url": "https://api.github.com/users/shubho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shubho/subscriptions", "organizations_url": "https://api.github.com/users/shubho/orgs", "repos_url": "https://api.github.com/users/shubho/repos", "events_url": "https://api.github.com/users/shubho/events{/privacy}", "received_events_url": "https://api.github.com/users/shubho/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-01-29T04:39:53Z", "updated_at": "2018-02-15T19:22:18Z", "closed_at": "2018-02-15T19:22:18Z", "author_association": "NONE", "body_html": "<ul>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/e58a53af6fd7b1f161865ca85a4f726914baca36/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/e58a53af6fd7b1f161865ca85a4f726914baca36\"><tt>e58a53a</tt></a></li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Python version:  3.6.4</li>\n<li>CUDA/cuDNN version: 9.0 / 7.0.3</li>\n<li>GPU models and configuration: V100</li>\n<li>GCC version (if compiling from source): 5.4</li>\n</ul>\n<p>There seems to have been a regression from PyTorch 0.30 in master. You would need multi GPU machine to show this problem. Please have two windows open</p>\n<p>Window 1<br>\nRun <code>nvidia-smi -l 5</code></p>\n<p>Window 2<br>\nRun the following python script by stepping through <code>pdb</code></p>\n<pre><code>import torch\nimport pdb\n\ngpu = 1\npdb.set_trace()\n\nwith torch.cuda.device(gpu):\n    pdb.set_trace()\n</code></pre>\n<p>Here is what happens</p>\n<ol>\n<li>Hit the first <code>set_trace</code> - no context created on any GPUs</li>\n<li>Hit the second <code>set_trace</code> - context is created on GPU 0 (instead of GPU1)</li>\n</ol>\n<p>This is due to how initialization happens inside <code>torch/cuda/__init__.py</code></p>\n<ol>\n<li>First hit is <code>__enter__</code> for <code>class device</code> which calls <code>_lazy_init()</code></li>\n<li>Since this is the first call <code>_lazy_init()</code> will actually do something. Problem starts at <code>torch._C._cuda_init()</code></li>\n<li>This calls goes to <code>THCPModule_initExtension</code> in <code>torch/csrc/cuda/Module.cpp</code></li>\n<li>This will call <code>THCPModule_initCuda</code> which in turn calls <code>state = at::globalContext().lazyInitCUDA();</code>.</li>\n<li>Inside ATen contexts will eventually get created on GPU 0 since it assumes that <code>cudaSetDevice</code> has already been called</li>\n</ol>\n<p>In fact any function that calls <code>_lazy_init()</code> in <code>torch.cuda</code> will create a context on GPU 0 the first time it is called because of this, irrespective what the user asked for. This is not harmful in a single GPU setting, but on a multi GPU setting as is the case with our cluster, different users get different GPUs, and everybody will have these contexts on GPU 0, no matter what. And contexts take up a fair bit of memory.</p>\n<p>I think the right thing to do is to have a <code>_lazy_init(gpuId)</code> which in turns sends down into <code>THCPModule_initExtension</code> to do the right thing.</p>\n<p>Quite interestingly <code>set_device()</code> does not call <code>_lazy_init()</code> which also looks like a bug to me since <code>_lazy_init()</code> seems to do a lot of initialization.</p>\n<p>This works just fine in PyTorch 0.30 btw where <code>with torch.cuda.device(gpu)</code> will only create contexts on the GPU you ask for.</p>\n<p>I am happy to try to fix this but I wanted to get a broader context on why things are this way.</p>", "body_text": "OS: Ubuntu 16.04\nPyTorch version: e58a53a\nHow you installed PyTorch (conda, pip, source): source\nPython version:  3.6.4\nCUDA/cuDNN version: 9.0 / 7.0.3\nGPU models and configuration: V100\nGCC version (if compiling from source): 5.4\n\nThere seems to have been a regression from PyTorch 0.30 in master. You would need multi GPU machine to show this problem. Please have two windows open\nWindow 1\nRun nvidia-smi -l 5\nWindow 2\nRun the following python script by stepping through pdb\nimport torch\nimport pdb\n\ngpu = 1\npdb.set_trace()\n\nwith torch.cuda.device(gpu):\n    pdb.set_trace()\n\nHere is what happens\n\nHit the first set_trace - no context created on any GPUs\nHit the second set_trace - context is created on GPU 0 (instead of GPU1)\n\nThis is due to how initialization happens inside torch/cuda/__init__.py\n\nFirst hit is __enter__ for class device which calls _lazy_init()\nSince this is the first call _lazy_init() will actually do something. Problem starts at torch._C._cuda_init()\nThis calls goes to THCPModule_initExtension in torch/csrc/cuda/Module.cpp\nThis will call THCPModule_initCuda which in turn calls state = at::globalContext().lazyInitCUDA();.\nInside ATen contexts will eventually get created on GPU 0 since it assumes that cudaSetDevice has already been called\n\nIn fact any function that calls _lazy_init() in torch.cuda will create a context on GPU 0 the first time it is called because of this, irrespective what the user asked for. This is not harmful in a single GPU setting, but on a multi GPU setting as is the case with our cluster, different users get different GPUs, and everybody will have these contexts on GPU 0, no matter what. And contexts take up a fair bit of memory.\nI think the right thing to do is to have a _lazy_init(gpuId) which in turns sends down into THCPModule_initExtension to do the right thing.\nQuite interestingly set_device() does not call _lazy_init() which also looks like a bug to me since _lazy_init() seems to do a lot of initialization.\nThis works just fine in PyTorch 0.30 btw where with torch.cuda.device(gpu) will only create contexts on the GPU you ask for.\nI am happy to try to fix this but I wanted to get a broader context on why things are this way.", "body": "- OS: Ubuntu 16.04\r\n- PyTorch version: e58a53a\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version:  3.6.4\r\n- CUDA/cuDNN version: 9.0 / 7.0.3\r\n- GPU models and configuration: V100\r\n- GCC version (if compiling from source): 5.4\r\n\r\nThere seems to have been a regression from PyTorch 0.30 in master. You would need multi GPU machine to show this problem. Please have two windows open\r\n\r\nWindow 1\r\nRun `nvidia-smi -l 5`\r\n\r\nWindow 2\r\nRun the following python script by stepping through `pdb`\r\n\r\n```\r\nimport torch\r\nimport pdb\r\n\r\ngpu = 1\r\npdb.set_trace()\r\n\r\nwith torch.cuda.device(gpu):\r\n    pdb.set_trace()\r\n```\r\nHere is what happens\r\n1. Hit the first `set_trace` - no context created on any GPUs\r\n2. Hit the second `set_trace` - context is created on GPU 0 (instead of GPU1)\r\n\r\nThis is due to how initialization happens inside `torch/cuda/__init__.py`\r\n\r\n1. First hit is `__enter__` for `class device` which calls `_lazy_init()`\r\n2. Since this is the first call `_lazy_init()` will actually do something. Problem starts at `torch._C._cuda_init()`\r\n3. This calls goes to `THCPModule_initExtension` in `torch/csrc/cuda/Module.cpp`\r\n4. This will call `THCPModule_initCuda` which in turn calls `state = at::globalContext().lazyInitCUDA();`. \r\n5. Inside ATen contexts will eventually get created on GPU 0 since it assumes that `cudaSetDevice` has already been called\r\n\r\nIn fact any function that calls `_lazy_init()` in `torch.cuda` will create a context on GPU 0 the first time it is called because of this, irrespective what the user asked for. This is not harmful in a single GPU setting, but on a multi GPU setting as is the case with our cluster, different users get different GPUs, and everybody will have these contexts on GPU 0, no matter what. And contexts take up a fair bit of memory.\r\n\r\nI think the right thing to do is to have a `_lazy_init(gpuId)` which in turns sends down into `THCPModule_initExtension` to do the right thing.\r\n\r\nQuite interestingly `set_device()` does not call `_lazy_init()` which also looks like a bug to me since `_lazy_init()` seems to do a lot of initialization.\r\n\r\nThis works just fine in PyTorch 0.30 btw where `with torch.cuda.device(gpu)` will only create contexts on the GPU you ask for.\r\n\r\nI am happy to try to fix this but I wanted to get a broader context on why things are this way.\r\n\r\n\r\n"}