{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/303123155", "html_url": "https://github.com/pytorch/pytorch/issues/1605#issuecomment-303123155", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1605", "id": 303123155, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzEyMzE1NQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-22T14:50:47Z", "updated_at": "2017-05-22T14:57:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4273204\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zym1010\">@zym1010</a> I think it is clearer to think about the meaning of multiple parameters to <code>backward</code> as introducing an on-the-fly loss function operating on the parameters which has a gradient equal the arguments you pass. So, your example is equivalent to this:</p>\n<pre><code>loss = f(y, y, y)\ntorch.autograd.backward(loss)\n# where f' = [torch.randn(10), torch.randn(10), torch.randn(10)]\n</code></pre>\n<p>Now it should be pretty clear what the intended semantics of your example above should be.</p>", "body_text": "@zym1010 I think it is clearer to think about the meaning of multiple parameters to backward as introducing an on-the-fly loss function operating on the parameters which has a gradient equal the arguments you pass. So, your example is equivalent to this:\nloss = f(y, y, y)\ntorch.autograd.backward(loss)\n# where f' = [torch.randn(10), torch.randn(10), torch.randn(10)]\n\nNow it should be pretty clear what the intended semantics of your example above should be.", "body": "@zym1010 I think it is clearer to think about the meaning of multiple parameters to `backward` as introducing an on-the-fly loss function operating on the parameters which has a gradient equal the arguments you pass. So, your example is equivalent to this:\r\n\r\n```\r\nloss = f(y, y, y)\r\ntorch.autograd.backward(loss)\r\n# where f' = [torch.randn(10), torch.randn(10), torch.randn(10)]\r\n```\r\n\r\nNow it should be pretty clear what the intended semantics of your example above should be."}