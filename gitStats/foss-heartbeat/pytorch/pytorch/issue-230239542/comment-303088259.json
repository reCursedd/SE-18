{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/303088259", "html_url": "https://github.com/pytorch/pytorch/issues/1605#issuecomment-303088259", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1605", "id": 303088259, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzA4ODI1OQ==", "user": {"login": "zym1010", "id": 4273204, "node_id": "MDQ6VXNlcjQyNzMyMDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4273204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zym1010", "html_url": "https://github.com/zym1010", "followers_url": "https://api.github.com/users/zym1010/followers", "following_url": "https://api.github.com/users/zym1010/following{/other_user}", "gists_url": "https://api.github.com/users/zym1010/gists{/gist_id}", "starred_url": "https://api.github.com/users/zym1010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zym1010/subscriptions", "organizations_url": "https://api.github.com/users/zym1010/orgs", "repos_url": "https://api.github.com/users/zym1010/repos", "events_url": "https://api.github.com/users/zym1010/events{/privacy}", "received_events_url": "https://api.github.com/users/zym1010/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-22T12:39:49Z", "updated_at": "2017-05-22T12:39:49Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>Maybe we want the semantics of PyTorch is such that, if we set the grad of some intermediate variable, it's understood that we have some additional auxiliary output that is identity function of that variable?</p>\n</blockquote>\n<p>I think this would also introduce some messy problems. Consider the meaning of</p>\n<pre><code>torch.autograd.backward([y, y, y], [torch.randn(10), torch.randn(10), torch.randn(10)])\n</code></pre>\n<p>I think the best approach is to explicitly declare that we want to create some identity function.</p>", "body_text": "Maybe we want the semantics of PyTorch is such that, if we set the grad of some intermediate variable, it's understood that we have some additional auxiliary output that is identity function of that variable?\n\nI think this would also introduce some messy problems. Consider the meaning of\ntorch.autograd.backward([y, y, y], [torch.randn(10), torch.randn(10), torch.randn(10)])\n\nI think the best approach is to explicitly declare that we want to create some identity function.", "body": "> Maybe we want the semantics of PyTorch is such that, if we set the grad of some intermediate variable, it's understood that we have some additional auxiliary output that is identity function of that variable?\r\n\r\nI think this would also introduce some messy problems. Consider the meaning of\r\n\r\n```\r\ntorch.autograd.backward([y, y, y], [torch.randn(10), torch.randn(10), torch.randn(10)])\r\n```\r\n\r\nI think the best approach is to explicitly declare that we want to create some identity function."}