{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/303081994", "html_url": "https://github.com/pytorch/pytorch/issues/1605#issuecomment-303081994", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1605", "id": 303081994, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzA4MTk5NA==", "user": {"login": "zym1010", "id": 4273204, "node_id": "MDQ6VXNlcjQyNzMyMDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4273204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zym1010", "html_url": "https://github.com/zym1010", "followers_url": "https://api.github.com/users/zym1010/followers", "following_url": "https://api.github.com/users/zym1010/following{/other_user}", "gists_url": "https://api.github.com/users/zym1010/gists{/gist_id}", "starred_url": "https://api.github.com/users/zym1010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zym1010/subscriptions", "organizations_url": "https://api.github.com/users/zym1010/orgs", "repos_url": "https://api.github.com/users/zym1010/repos", "events_url": "https://api.github.com/users/zym1010/events{/privacy}", "received_events_url": "https://api.github.com/users/zym1010/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-22T12:09:40Z", "updated_at": "2017-05-22T12:09:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I see what you mean. Currently I can overcome the issue like this.</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\n\nx = Variable(torch.randn(10), requires_grad=True)\ny = x ** 2\nz = y ** 3\nk = y ** 1\n\ntorch.autograd.backward([k, z], [torch.randn(10), torch.randn(10)])\n</code></pre>\n<p>When I change <code>k = y**1</code> to <code>k = y</code>, then it breaks. But in any case, I think setting the grad of <code>y</code> and setting the grad of a (maybe trivial) function of <code>y</code> are totally different. Maybe we want the semantics of PyTorch is such that, if we set the grad of some intermediate variable, it's understood that we have some additional auxiliary output that is identity function of that variable?</p>", "body_text": "@apaszke I see what you mean. Currently I can overcome the issue like this.\nimport torch\nfrom torch.autograd import Variable\n\nx = Variable(torch.randn(10), requires_grad=True)\ny = x ** 2\nz = y ** 3\nk = y ** 1\n\ntorch.autograd.backward([k, z], [torch.randn(10), torch.randn(10)])\n\nWhen I change k = y**1 to k = y, then it breaks. But in any case, I think setting the grad of y and setting the grad of a (maybe trivial) function of y are totally different. Maybe we want the semantics of PyTorch is such that, if we set the grad of some intermediate variable, it's understood that we have some additional auxiliary output that is identity function of that variable?", "body": "@apaszke I see what you mean. Currently I can overcome the issue like this.\r\n\r\n~~~\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nx = Variable(torch.randn(10), requires_grad=True)\r\ny = x ** 2\r\nz = y ** 3\r\nk = y ** 1\r\n\r\ntorch.autograd.backward([k, z], [torch.randn(10), torch.randn(10)])\r\n~~~\r\n\r\nWhen I change `k = y**1` to `k = y`, then it breaks. But in any case, I think setting the grad of `y` and setting the grad of a (maybe trivial) function of `y` are totally different. Maybe we want the semantics of PyTorch is such that, if we set the grad of some intermediate variable, it's understood that we have some additional auxiliary output that is identity function of that variable?\r\n"}