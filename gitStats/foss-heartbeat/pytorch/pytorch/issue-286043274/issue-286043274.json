{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4477", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4477/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4477/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4477/events", "html_url": "https://github.com/pytorch/pytorch/issues/4477", "id": 286043274, "node_id": "MDU6SXNzdWUyODYwNDMyNzQ=", "number": 4477, "title": "Concrete native function hazard", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-04T16:37:58Z", "updated_at": "2018-01-23T20:04:54Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Consider the following native function:</p>\n<pre><code>Tensor f(Tensor x) {\n  return CUDA(kByte).tensor()\n}\n</code></pre>\n<p>This is a valid native function, but it is not a \"transparently differentiable\" one. It turns out that if you forget to correctly define an entry for <code>f</code> in <code>tools/autograd/derivatives.yaml</code>, this function will segfault on return, because Variable wrapping will assume that everything returned by <code>f</code> is a Variable (which clearly is not the case.) The segfault is a bit obtuse: it looks like this:</p>\n<pre><code>#0  0x00007fffc2303733 in __dynamic_cast ()\n   from /home/ezyang/local/pytorch-env/lib/libstdc++.so.6\n#1  0x00007fffe37fdf30 in THPVariable_NewWithVar (type=0x12a6228, var=...)\n    at torch/csrc/autograd/python_variable.cpp:38\n#2  0x00007fffe37fe0cb in THPVariable_Wrap (var=...)\n    at torch/csrc/autograd/python_variable.cpp:62\n#3  0x00007fffe3804508 in torch::autograd::utils::wrap (tensor=...)\n    at /data/users/ezyang/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:16\n#4  0x00007fffe3a66075 in torch::autograd::utils::wrap (tensors=...)\n    at /data/users/ezyang/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:42\n#5  0x00007fffe3a0d828 in torch::autograd::THPVariable__cudnn_rnn (self=0x0, \n    args=0x7fffb76cf210, kwargs=0x0)\n</code></pre>\n<p>Are we OK with adding an <em>explicit</em> check that a <code>Tensor</code> is indeed a <code>Variable</code> before wrapping it? That would allow us to raise an exception, rather than segfault the <code>dynamic_cast</code>.</p>", "body_text": "Consider the following native function:\nTensor f(Tensor x) {\n  return CUDA(kByte).tensor()\n}\n\nThis is a valid native function, but it is not a \"transparently differentiable\" one. It turns out that if you forget to correctly define an entry for f in tools/autograd/derivatives.yaml, this function will segfault on return, because Variable wrapping will assume that everything returned by f is a Variable (which clearly is not the case.) The segfault is a bit obtuse: it looks like this:\n#0  0x00007fffc2303733 in __dynamic_cast ()\n   from /home/ezyang/local/pytorch-env/lib/libstdc++.so.6\n#1  0x00007fffe37fdf30 in THPVariable_NewWithVar (type=0x12a6228, var=...)\n    at torch/csrc/autograd/python_variable.cpp:38\n#2  0x00007fffe37fe0cb in THPVariable_Wrap (var=...)\n    at torch/csrc/autograd/python_variable.cpp:62\n#3  0x00007fffe3804508 in torch::autograd::utils::wrap (tensor=...)\n    at /data/users/ezyang/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:16\n#4  0x00007fffe3a66075 in torch::autograd::utils::wrap (tensors=...)\n    at /data/users/ezyang/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:42\n#5  0x00007fffe3a0d828 in torch::autograd::THPVariable__cudnn_rnn (self=0x0, \n    args=0x7fffb76cf210, kwargs=0x0)\n\nAre we OK with adding an explicit check that a Tensor is indeed a Variable before wrapping it? That would allow us to raise an exception, rather than segfault the dynamic_cast.", "body": "Consider the following native function:\r\n\r\n```\r\nTensor f(Tensor x) {\r\n  return CUDA(kByte).tensor()\r\n}\r\n```\r\n\r\nThis is a valid native function, but it is not a \"transparently differentiable\" one. It turns out that if you forget to correctly define an entry for `f` in `tools/autograd/derivatives.yaml`, this function will segfault on return, because Variable wrapping will assume that everything returned by `f` is a Variable (which clearly is not the case.) The segfault is a bit obtuse: it looks like this:\r\n\r\n```\r\n#0  0x00007fffc2303733 in __dynamic_cast ()\r\n   from /home/ezyang/local/pytorch-env/lib/libstdc++.so.6\r\n#1  0x00007fffe37fdf30 in THPVariable_NewWithVar (type=0x12a6228, var=...)\r\n    at torch/csrc/autograd/python_variable.cpp:38\r\n#2  0x00007fffe37fe0cb in THPVariable_Wrap (var=...)\r\n    at torch/csrc/autograd/python_variable.cpp:62\r\n#3  0x00007fffe3804508 in torch::autograd::utils::wrap (tensor=...)\r\n    at /data/users/ezyang/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:16\r\n#4  0x00007fffe3a66075 in torch::autograd::utils::wrap (tensors=...)\r\n    at /data/users/ezyang/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:42\r\n#5  0x00007fffe3a0d828 in torch::autograd::THPVariable__cudnn_rnn (self=0x0, \r\n    args=0x7fffb76cf210, kwargs=0x0)\r\n```\r\n\r\nAre we OK with adding an *explicit* check that a `Tensor` is indeed a `Variable` before wrapping it? That would allow us to raise an exception, rather than segfault the `dynamic_cast`."}