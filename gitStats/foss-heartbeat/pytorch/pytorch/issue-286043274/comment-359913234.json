{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/359913234", "html_url": "https://github.com/pytorch/pytorch/issues/4477#issuecomment-359913234", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4477", "id": 359913234, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTkxMzIzNA==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-23T20:04:54Z", "updated_at": "2018-01-23T20:04:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I wasted another hour on this yesterday. The segfault this time was:</p>\n<pre><code>Thread 1 \"python\" received signal SIGSEGV, Segmentation fault.\n0x00007fffe366921d in THPVariable_Wrap (var=..., allow_scalar=false) at torch/csrc/auto\ngrad/python_variable.cpp:58\n58          Py_INCREF(obj);\n(gdb) bt\n#0  0x00007fffe366921d in THPVariable_Wrap (var=..., allow_scalar=false) at torch/csrc/\nautograd/python_variable.cpp:58\n#1  0x00007fffe366f6bf in torch::autograd::utils::wrap (tensor=...) at /data/users/ezya\nng/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:16\n#2  0x00007fffe396d72a in torch::autograd::utils::wrap (tensors=...) at /data/users/ezy\nang/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:42\n#3  0x00007fffe38f030b in torch::autograd::THPVariable__cudnn_rnn (self=0x0, args=0x7ff\nfb5561508, kwargs=0x0) at torch/csrc/autograd/generated/python_torch_functions.cpp:348\n#4  0x00007ffff7996302 in _PyCFunction_FastCallDict (func_obj=0x7fffe79cfea0, args=0x5a\nd5df00, nargs=&lt;optimized out&gt;, kwargs=0x0) at Objects/methodobject.c:231\n</code></pre>\n<p>This is even worse than it was before.</p>\n<p>Also, the correct way to allocate <code>CUDA(kByte).tensor()</code> is to instead do <code>input.type().toScalarType(kByte).tensor()</code>.</p>", "body_text": "I wasted another hour on this yesterday. The segfault this time was:\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\n0x00007fffe366921d in THPVariable_Wrap (var=..., allow_scalar=false) at torch/csrc/auto\ngrad/python_variable.cpp:58\n58          Py_INCREF(obj);\n(gdb) bt\n#0  0x00007fffe366921d in THPVariable_Wrap (var=..., allow_scalar=false) at torch/csrc/\nautograd/python_variable.cpp:58\n#1  0x00007fffe366f6bf in torch::autograd::utils::wrap (tensor=...) at /data/users/ezya\nng/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:16\n#2  0x00007fffe396d72a in torch::autograd::utils::wrap (tensors=...) at /data/users/ezy\nang/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:42\n#3  0x00007fffe38f030b in torch::autograd::THPVariable__cudnn_rnn (self=0x0, args=0x7ff\nfb5561508, kwargs=0x0) at torch/csrc/autograd/generated/python_torch_functions.cpp:348\n#4  0x00007ffff7996302 in _PyCFunction_FastCallDict (func_obj=0x7fffe79cfea0, args=0x5a\nd5df00, nargs=<optimized out>, kwargs=0x0) at Objects/methodobject.c:231\n\nThis is even worse than it was before.\nAlso, the correct way to allocate CUDA(kByte).tensor() is to instead do input.type().toScalarType(kByte).tensor().", "body": "I wasted another hour on this yesterday. The segfault this time was:\r\n\r\n```\r\nThread 1 \"python\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fffe366921d in THPVariable_Wrap (var=..., allow_scalar=false) at torch/csrc/auto\r\ngrad/python_variable.cpp:58\r\n58          Py_INCREF(obj);\r\n(gdb) bt\r\n#0  0x00007fffe366921d in THPVariable_Wrap (var=..., allow_scalar=false) at torch/csrc/\r\nautograd/python_variable.cpp:58\r\n#1  0x00007fffe366f6bf in torch::autograd::utils::wrap (tensor=...) at /data/users/ezya\r\nng/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:16\r\n#2  0x00007fffe396d72a in torch::autograd::utils::wrap (tensors=...) at /data/users/ezy\r\nang/pytorch/torch/csrc/autograd/utils/wrap_outputs.h:42\r\n#3  0x00007fffe38f030b in torch::autograd::THPVariable__cudnn_rnn (self=0x0, args=0x7ff\r\nfb5561508, kwargs=0x0) at torch/csrc/autograd/generated/python_torch_functions.cpp:348\r\n#4  0x00007ffff7996302 in _PyCFunction_FastCallDict (func_obj=0x7fffe79cfea0, args=0x5a\r\nd5df00, nargs=<optimized out>, kwargs=0x0) at Objects/methodobject.c:231\r\n```\r\n\r\nThis is even worse than it was before.\r\n\r\nAlso, the correct way to allocate `CUDA(kByte).tensor()` is to instead do `input.type().toScalarType(kByte).tensor()`."}