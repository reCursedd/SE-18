{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4241", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4241/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4241/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4241/events", "html_url": "https://github.com/pytorch/pytorch/issues/4241", "id": 283039896, "node_id": "MDU6SXNzdWUyODMwMzk4OTY=", "number": 4241, "title": "single-gpu works but multi-gpu hangs", "user": {"login": "proliu618", "id": 27710190, "node_id": "MDQ6VXNlcjI3NzEwMTkw", "avatar_url": "https://avatars2.githubusercontent.com/u/27710190?v=4", "gravatar_id": "", "url": "https://api.github.com/users/proliu618", "html_url": "https://github.com/proliu618", "followers_url": "https://api.github.com/users/proliu618/followers", "following_url": "https://api.github.com/users/proliu618/following{/other_user}", "gists_url": "https://api.github.com/users/proliu618/gists{/gist_id}", "starred_url": "https://api.github.com/users/proliu618/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/proliu618/subscriptions", "organizations_url": "https://api.github.com/users/proliu618/orgs", "repos_url": "https://api.github.com/users/proliu618/repos", "events_url": "https://api.github.com/users/proliu618/events{/privacy}", "received_events_url": "https://api.github.com/users/proliu618/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-18T22:19:08Z", "updated_at": "2017-12-18T22:44:34Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>My machine has 4 titanXP, CUDA9.0, CuDNN 7.5, Ubuntu 16.04</p>\n<p>When I run an image classification task with single GPU, it runs just fine. But it hangs at the line<br>\nmodel = nn.DataParallel(model) when I try to run with 2 or more GPUs.</p>\n<p>I ran p2pBandwidthLatencyTest and got the following report:<br>\nP2P (Peer-to-Peer) GPU Bandwidth Latency Test]<br>\nDevice: 0, TITAN Xp, pciBusID: 3b, pciDeviceID: 0, pciDomainID:0<br>\nDevice: 1, TITAN Xp, pciBusID: 5e, pciDeviceID: 0, pciDomainID:0<br>\nDevice: 2, TITAN Xp, pciBusID: af, pciDeviceID: 0, pciDomainID:0<br>\nDevice: 3, TITAN Xp, pciBusID: d8, pciDeviceID: 0, pciDomainID:0<br>\nDevice=0 CAN Access Peer Device=1<br>\nDevice=0 CAN Access Peer Device=2<br>\nDevice=0 CAN Access Peer Device=3<br>\nDevice=1 CAN Access Peer Device=0<br>\nDevice=1 CAN Access Peer Device=2<br>\nDevice=1 CAN Access Peer Device=3<br>\nDevice=2 CAN Access Peer Device=0<br>\nDevice=2 CAN Access Peer Device=1<br>\nDevice=2 CAN Access Peer Device=3<br>\nDevice=3 CAN Access Peer Device=0<br>\nDevice=3 CAN Access Peer Device=1<br>\nDevice=3 CAN Access Peer Device=2</p>\n<p>***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.<br>\nSo you can see lesser Bandwidth (GB/s) in those cases.</p>\n<p>P2P Connectivity Matrix<br>\nD\\D     0     1     2     3<br>\n0\t     1     1     1     1<br>\n1\t     1     1     1     1<br>\n2\t     1     1     1     1<br>\n3\t     1     1     1     1<br>\nUnidirectional P2P=Disabled Bandwidth Matrix (GB/s)<br>\nD\\D     0      1      2      3<br>\n0 423.18  11.44  11.37  11.36<br>\n1  11.42 426.77  11.41  11.36<br>\n2  11.38  11.38 425.52  11.34<br>\n3  11.36  11.40  11.31 427.77<br>\nUnidirectional P2P=Enabled Bandwidth Matrix (GB/s)<br>\nD\\D     0      1      2      3<br>\n0 426.11   7.59   8.38   6.35<br>\n1  10.19 426.39   8.36   9.08<br>\n2   8.39   9.08 426.03  10.16<br>\n3   8.39   9.09  10.14 427.85<br>\nBidirectional P2P=Disabled Bandwidth Matrix (GB/s)<br>\nD\\D     0      1      2      3<br>\n0 426.50  19.49  18.98  19.07<br>\n1  19.87 430.72  20.02  19.93<br>\n2  17.91  19.95 429.49  18.79<br>\n3  19.98  19.92  18.81 429.73<br>\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)<br>\nD\\D     0      1      2      3<br>\n0 423.94  15.74  14.40  16.00<br>\n1  20.26 429.07  16.00  17.32<br>\n2  16.07  16.02 429.69  20.23<br>\n3  16.01  17.32  20.26 428.26<br>\nP2P=Disabled Latency Matrix (us)<br>\nD\\D     0      1      2      3<br>\n0   9.90  17.02  14.31  14.31<br>\n1  14.02   5.55  14.59  14.53<br>\n2  14.50  14.48   5.82  14.31<br>\n3  14.85  14.40  14.57   5.80<br>\nP2P=Enabled Latency Matrix (us)<br>\nD\\D     0      1      2      3<br>\n0  11.51  11.45   8.36   8.82<br>\n1   8.86   5.54   9.07   8.38<br>\n2   8.76   9.11   5.76   9.12<br>\n3   8.80   8.80   9.08   5.78</p>\n<p>which seemed fine to me.</p>\n<p>Can anyone please help me out? Thanks!</p>", "body_text": "My machine has 4 titanXP, CUDA9.0, CuDNN 7.5, Ubuntu 16.04\nWhen I run an image classification task with single GPU, it runs just fine. But it hangs at the line\nmodel = nn.DataParallel(model) when I try to run with 2 or more GPUs.\nI ran p2pBandwidthLatencyTest and got the following report:\nP2P (Peer-to-Peer) GPU Bandwidth Latency Test]\nDevice: 0, TITAN Xp, pciBusID: 3b, pciDeviceID: 0, pciDomainID:0\nDevice: 1, TITAN Xp, pciBusID: 5e, pciDeviceID: 0, pciDomainID:0\nDevice: 2, TITAN Xp, pciBusID: af, pciDeviceID: 0, pciDomainID:0\nDevice: 3, TITAN Xp, pciBusID: d8, pciDeviceID: 0, pciDomainID:0\nDevice=0 CAN Access Peer Device=1\nDevice=0 CAN Access Peer Device=2\nDevice=0 CAN Access Peer Device=3\nDevice=1 CAN Access Peer Device=0\nDevice=1 CAN Access Peer Device=2\nDevice=1 CAN Access Peer Device=3\nDevice=2 CAN Access Peer Device=0\nDevice=2 CAN Access Peer Device=1\nDevice=2 CAN Access Peer Device=3\nDevice=3 CAN Access Peer Device=0\nDevice=3 CAN Access Peer Device=1\nDevice=3 CAN Access Peer Device=2\n***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\nSo you can see lesser Bandwidth (GB/s) in those cases.\nP2P Connectivity Matrix\nD\\D     0     1     2     3\n0\t     1     1     1     1\n1\t     1     1     1     1\n2\t     1     1     1     1\n3\t     1     1     1     1\nUnidirectional P2P=Disabled Bandwidth Matrix (GB/s)\nD\\D     0      1      2      3\n0 423.18  11.44  11.37  11.36\n1  11.42 426.77  11.41  11.36\n2  11.38  11.38 425.52  11.34\n3  11.36  11.40  11.31 427.77\nUnidirectional P2P=Enabled Bandwidth Matrix (GB/s)\nD\\D     0      1      2      3\n0 426.11   7.59   8.38   6.35\n1  10.19 426.39   8.36   9.08\n2   8.39   9.08 426.03  10.16\n3   8.39   9.09  10.14 427.85\nBidirectional P2P=Disabled Bandwidth Matrix (GB/s)\nD\\D     0      1      2      3\n0 426.50  19.49  18.98  19.07\n1  19.87 430.72  20.02  19.93\n2  17.91  19.95 429.49  18.79\n3  19.98  19.92  18.81 429.73\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\nD\\D     0      1      2      3\n0 423.94  15.74  14.40  16.00\n1  20.26 429.07  16.00  17.32\n2  16.07  16.02 429.69  20.23\n3  16.01  17.32  20.26 428.26\nP2P=Disabled Latency Matrix (us)\nD\\D     0      1      2      3\n0   9.90  17.02  14.31  14.31\n1  14.02   5.55  14.59  14.53\n2  14.50  14.48   5.82  14.31\n3  14.85  14.40  14.57   5.80\nP2P=Enabled Latency Matrix (us)\nD\\D     0      1      2      3\n0  11.51  11.45   8.36   8.82\n1   8.86   5.54   9.07   8.38\n2   8.76   9.11   5.76   9.12\n3   8.80   8.80   9.08   5.78\nwhich seemed fine to me.\nCan anyone please help me out? Thanks!", "body": "My machine has 4 titanXP, CUDA9.0, CuDNN 7.5, Ubuntu 16.04\r\n\r\nWhen I run an image classification task with single GPU, it runs just fine. But it hangs at the line\r\nmodel = nn.DataParallel(model) when I try to run with 2 or more GPUs. \r\n\r\nI ran p2pBandwidthLatencyTest and got the following report:\r\nP2P (Peer-to-Peer) GPU Bandwidth Latency Test]\r\nDevice: 0, TITAN Xp, pciBusID: 3b, pciDeviceID: 0, pciDomainID:0\r\nDevice: 1, TITAN Xp, pciBusID: 5e, pciDeviceID: 0, pciDomainID:0\r\nDevice: 2, TITAN Xp, pciBusID: af, pciDeviceID: 0, pciDomainID:0\r\nDevice: 3, TITAN Xp, pciBusID: d8, pciDeviceID: 0, pciDomainID:0\r\nDevice=0 CAN Access Peer Device=1\r\nDevice=0 CAN Access Peer Device=2\r\nDevice=0 CAN Access Peer Device=3\r\nDevice=1 CAN Access Peer Device=0\r\nDevice=1 CAN Access Peer Device=2\r\nDevice=1 CAN Access Peer Device=3\r\nDevice=2 CAN Access Peer Device=0\r\nDevice=2 CAN Access Peer Device=1\r\nDevice=2 CAN Access Peer Device=3\r\nDevice=3 CAN Access Peer Device=0\r\nDevice=3 CAN Access Peer Device=1\r\nDevice=3 CAN Access Peer Device=2\r\n\r\n***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\r\nSo you can see lesser Bandwidth (GB/s) in those cases.\r\n\r\nP2P Connectivity Matrix\r\n     D\\D     0     1     2     3\r\n     0\t     1     1     1     1\r\n     1\t     1     1     1     1\r\n     2\t     1     1     1     1\r\n     3\t     1     1     1     1\r\nUnidirectional P2P=Disabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3 \r\n     0 423.18  11.44  11.37  11.36 \r\n     1  11.42 426.77  11.41  11.36 \r\n     2  11.38  11.38 425.52  11.34 \r\n     3  11.36  11.40  11.31 427.77 \r\nUnidirectional P2P=Enabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3 \r\n     0 426.11   7.59   8.38   6.35 \r\n     1  10.19 426.39   8.36   9.08 \r\n     2   8.39   9.08 426.03  10.16 \r\n     3   8.39   9.09  10.14 427.85 \r\nBidirectional P2P=Disabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3 \r\n     0 426.50  19.49  18.98  19.07 \r\n     1  19.87 430.72  20.02  19.93 \r\n     2  17.91  19.95 429.49  18.79 \r\n     3  19.98  19.92  18.81 429.73 \r\nBidirectional P2P=Enabled Bandwidth Matrix (GB/s)\r\n   D\\D     0      1      2      3 \r\n     0 423.94  15.74  14.40  16.00 \r\n     1  20.26 429.07  16.00  17.32 \r\n     2  16.07  16.02 429.69  20.23 \r\n     3  16.01  17.32  20.26 428.26 \r\nP2P=Disabled Latency Matrix (us)\r\n   D\\D     0      1      2      3 \r\n     0   9.90  17.02  14.31  14.31 \r\n     1  14.02   5.55  14.59  14.53 \r\n     2  14.50  14.48   5.82  14.31 \r\n     3  14.85  14.40  14.57   5.80 \r\nP2P=Enabled Latency Matrix (us)\r\n   D\\D     0      1      2      3 \r\n     0  11.51  11.45   8.36   8.82 \r\n     1   8.86   5.54   9.07   8.38 \r\n     2   8.76   9.11   5.76   9.12 \r\n     3   8.80   8.80   9.08   5.78 \r\n\r\nwhich seemed fine to me. \r\n\r\nCan anyone please help me out? Thanks!"}