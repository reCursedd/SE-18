{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430288454", "html_url": "https://github.com/pytorch/pytorch/issues/10009#issuecomment-430288454", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10009", "id": 430288454, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDI4ODQ1NA==", "user": {"login": "mjschoeberl", "id": 36539326, "node_id": "MDQ6VXNlcjM2NTM5MzI2", "avatar_url": "https://avatars3.githubusercontent.com/u/36539326?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mjschoeberl", "html_url": "https://github.com/mjschoeberl", "followers_url": "https://api.github.com/users/mjschoeberl/followers", "following_url": "https://api.github.com/users/mjschoeberl/following{/other_user}", "gists_url": "https://api.github.com/users/mjschoeberl/gists{/gist_id}", "starred_url": "https://api.github.com/users/mjschoeberl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mjschoeberl/subscriptions", "organizations_url": "https://api.github.com/users/mjschoeberl/orgs", "repos_url": "https://api.github.com/users/mjschoeberl/repos", "events_url": "https://api.github.com/users/mjschoeberl/events{/privacy}", "received_events_url": "https://api.github.com/users/mjschoeberl/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-16T15:43:08Z", "updated_at": "2018-10-16T15:43:08Z", "author_association": "NONE", "body_html": "<p>Unfortunately the reduction of the mse_loss is still performed despite setting reduction='none' if both, input and target, requires_grad is True.</p>\n<pre><code>input = torch.randn(10,2, requires_grad =True)\ntarget = torch.torch.ones(10,2,requires_grad=True)\nloss = torch.nn.functional.mse_loss(input, target, reduction='none')\nprint loss.shape\n</code></pre>\n<p>In case only the input requires a gradient and not the target, the output shape of the loss is correct.</p>\n<p>Has anyone observed the same phenomenon? Or am I abusing the function by having target and input requiring a gradient?<br>\nApologizes in case my comment is obsolete. Thank you for your help.</p>\n<p>Python: 2.7<br>\nPyTorch: 0.4.1 (CPU) Updated on 10/14/2018</p>", "body_text": "Unfortunately the reduction of the mse_loss is still performed despite setting reduction='none' if both, input and target, requires_grad is True.\ninput = torch.randn(10,2, requires_grad =True)\ntarget = torch.torch.ones(10,2,requires_grad=True)\nloss = torch.nn.functional.mse_loss(input, target, reduction='none')\nprint loss.shape\n\nIn case only the input requires a gradient and not the target, the output shape of the loss is correct.\nHas anyone observed the same phenomenon? Or am I abusing the function by having target and input requiring a gradient?\nApologizes in case my comment is obsolete. Thank you for your help.\nPython: 2.7\nPyTorch: 0.4.1 (CPU) Updated on 10/14/2018", "body": "Unfortunately the reduction of the mse_loss is still performed despite setting reduction='none' if both, input and target, requires_grad is True.\r\n\r\n    input = torch.randn(10,2, requires_grad =True)\r\n    target = torch.torch.ones(10,2,requires_grad=True)\r\n    loss = torch.nn.functional.mse_loss(input, target, reduction='none')\r\n    print loss.shape\r\n\r\nIn case only the input requires a gradient and not the target, the output shape of the loss is correct.\r\n\r\nHas anyone observed the same phenomenon? Or am I abusing the function by having target and input requiring a gradient? \r\nApologizes in case my comment is obsolete. Thank you for your help.\r\n\r\nPython: 2.7\r\nPyTorch: 0.4.1 (CPU) Updated on 10/14/2018"}