{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/394910031", "html_url": "https://github.com/pytorch/pytorch/issues/8153#issuecomment-394910031", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8153", "id": 394910031, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDkxMDAzMQ==", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-06T01:24:06Z", "updated_at": "2018-06-06T01:24:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Well, I'm just running against small tiny toy problems. I guess the thing to do would be to slot it eg into opennmt-py, <a href=\"https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/Models.py#L22\">https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/Models.py#L22</a> . I don't have time to do that myself.  My GRUv3 naive implementation looks like:</p>\n<pre><code>import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\ndef Hadamard(one, two):\n    if one.size() != two.size():\n        raise Exception('size mismatch %s vs %s' % (str(list(one.size())), str(list(two.size()))))\n    res = one * two\n    assert res.numel() == one.numel()\n    return res\n\n\nclass MyGRUv3Cell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.input_size = input_size\n        self.embedding_size = hidden_size\n\n        self.fc_x1 = nn.Linear(self.input_size, self.embedding_size)\n        self.fc_x2 = nn.Linear(self.input_size, self.embedding_size)\n        self.fc_x3 = nn.Linear(self.input_size, self.embedding_size)\n        self.fc_h1 = nn.Linear(self.embedding_size, self.embedding_size)\n        self.fc_h2 = nn.Linear(self.embedding_size, self.embedding_size)\n\n        self.fc_rh = nn.Linear(self.embedding_size, self.embedding_size)\n\n    def forward(self, x, state):\n        batch_size = x.size()[0]\n\n        r = F.sigmoid(self.fc_x1(x) + self.fc_h1(state))\n        z = F.sigmoid(self.fc_x2(x) + self.fc_h2(state))\n        htilde = F.tanh(\n            self.fc_x3(x) + self.fc_rh(Hadamard(r, state)))\n        hdot = Hadamard(z, state) + Hadamard(1 - z, htilde)\n        return hdot\n</code></pre>\n<p>Note that whilst it's tempting to fuse the various <code>Linear</code> layers, you'd need to then handle the per-gate biases somehow, which ... I'm too lazy to figure out how to do for now :P</p>", "body_text": "Well, I'm just running against small tiny toy problems. I guess the thing to do would be to slot it eg into opennmt-py, https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/Models.py#L22 . I don't have time to do that myself.  My GRUv3 naive implementation looks like:\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\ndef Hadamard(one, two):\n    if one.size() != two.size():\n        raise Exception('size mismatch %s vs %s' % (str(list(one.size())), str(list(two.size()))))\n    res = one * two\n    assert res.numel() == one.numel()\n    return res\n\n\nclass MyGRUv3Cell(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.input_size = input_size\n        self.embedding_size = hidden_size\n\n        self.fc_x1 = nn.Linear(self.input_size, self.embedding_size)\n        self.fc_x2 = nn.Linear(self.input_size, self.embedding_size)\n        self.fc_x3 = nn.Linear(self.input_size, self.embedding_size)\n        self.fc_h1 = nn.Linear(self.embedding_size, self.embedding_size)\n        self.fc_h2 = nn.Linear(self.embedding_size, self.embedding_size)\n\n        self.fc_rh = nn.Linear(self.embedding_size, self.embedding_size)\n\n    def forward(self, x, state):\n        batch_size = x.size()[0]\n\n        r = F.sigmoid(self.fc_x1(x) + self.fc_h1(state))\n        z = F.sigmoid(self.fc_x2(x) + self.fc_h2(state))\n        htilde = F.tanh(\n            self.fc_x3(x) + self.fc_rh(Hadamard(r, state)))\n        hdot = Hadamard(z, state) + Hadamard(1 - z, htilde)\n        return hdot\n\nNote that whilst it's tempting to fuse the various Linear layers, you'd need to then handle the per-gate biases somehow, which ... I'm too lazy to figure out how to do for now :P", "body": "Well, I'm just running against small tiny toy problems. I guess the thing to do would be to slot it eg into opennmt-py, https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/Models.py#L22 . I don't have time to do that myself.  My GRUv3 naive implementation looks like:\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\nimport torch.nn.functional as F\r\n\r\n\r\ndef Hadamard(one, two):\r\n    if one.size() != two.size():\r\n        raise Exception('size mismatch %s vs %s' % (str(list(one.size())), str(list(two.size()))))\r\n    res = one * two\r\n    assert res.numel() == one.numel()\r\n    return res\r\n\r\n\r\nclass MyGRUv3Cell(nn.Module):\r\n    def __init__(self, input_size, hidden_size):\r\n        super().__init__()\r\n        self.input_size = input_size\r\n        self.embedding_size = hidden_size\r\n\r\n        self.fc_x1 = nn.Linear(self.input_size, self.embedding_size)\r\n        self.fc_x2 = nn.Linear(self.input_size, self.embedding_size)\r\n        self.fc_x3 = nn.Linear(self.input_size, self.embedding_size)\r\n        self.fc_h1 = nn.Linear(self.embedding_size, self.embedding_size)\r\n        self.fc_h2 = nn.Linear(self.embedding_size, self.embedding_size)\r\n\r\n        self.fc_rh = nn.Linear(self.embedding_size, self.embedding_size)\r\n\r\n    def forward(self, x, state):\r\n        batch_size = x.size()[0]\r\n\r\n        r = F.sigmoid(self.fc_x1(x) + self.fc_h1(state))\r\n        z = F.sigmoid(self.fc_x2(x) + self.fc_h2(state))\r\n        htilde = F.tanh(\r\n            self.fc_x3(x) + self.fc_rh(Hadamard(r, state)))\r\n        hdot = Hadamard(z, state) + Hadamard(1 - z, htilde)\r\n        return hdot\r\n```\r\n\r\nNote that whilst it's tempting to fuse the various `Linear` layers, you'd need to then handle the per-gate biases somehow, which ... I'm too lazy to figure out how to do for now :P"}