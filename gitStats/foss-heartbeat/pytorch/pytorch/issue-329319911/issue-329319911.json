{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8153", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8153/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8153/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8153/events", "html_url": "https://github.com/pytorch/pytorch/issues/8153", "id": 329319911, "node_id": "MDU6SXNzdWUzMjkzMTk5MTE=", "number": 8153, "title": "GRU is implementation of GRU v1 draft rather than final GRU paper algo", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-06-05T06:38:02Z", "updated_at": "2018-09-02T07:30:07Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>GRU is implementation of v1 <a href=\"https://arxiv.org/pdf/1406.1078v1.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1406.1078v1.pdf</a> rather than final v3 <a href=\"https://arxiv.org/pdf/1406.1078.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1406.1078.pdf</a></p>\n<p>Concretely, in v1, the reset gate is applied to the result of applying a linear layer to the incoming state, equation 8:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/123560/40958883-108947ac-68c5-11e8-88b4-70ecda354f69.png\"><img width=\"287\" alt=\"screen shot 2018-06-05 at 1 33 21 pm\" src=\"https://user-images.githubusercontent.com/123560/40958883-108947ac-68c5-11e8-88b4-70ecda354f69.png\" style=\"max-width:100%;\"></a></p>\n<p>... wheras the final v3 first applies the reset gate, and then passes through a linear layer:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/123560/40958904-2e6eb018-68c5-11e8-9ae3-01fed5319651.png\"><img width=\"301\" alt=\"screen shot 2018-06-05 at 1 34 19 pm\" src=\"https://user-images.githubusercontent.com/123560/40958904-2e6eb018-68c5-11e8-9ae3-01fed5319651.png\" style=\"max-width:100%;\"></a></p>\n<p>A few minutes informal testing gave me better results on final v3 than draft v1. I imagine that pytorch is using v1 partly for execution speed, and partly because that's what cudnn uses:</p>\n<p><a href=\"https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNMode_t\" rel=\"nofollow\">https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNMode_t</a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/123560/40958992-77e050e4-68c5-11e8-9813-ff9e2582cd34.png\"><img width=\"376\" alt=\"screen shot 2018-06-05 at 1 36 22 pm\" src=\"https://user-images.githubusercontent.com/123560/40958992-77e050e4-68c5-11e8-9813-ff9e2582cd34.png\" style=\"max-width:100%;\"></a></p>\n<p>I'm not sure if that's a good reason though. I think it might be good to at least document that this approximation is being used?</p>", "body_text": "GRU is implementation of v1 https://arxiv.org/pdf/1406.1078v1.pdf rather than final v3 https://arxiv.org/pdf/1406.1078.pdf\nConcretely, in v1, the reset gate is applied to the result of applying a linear layer to the incoming state, equation 8:\n\n... wheras the final v3 first applies the reset gate, and then passes through a linear layer:\n\nA few minutes informal testing gave me better results on final v3 than draft v1. I imagine that pytorch is using v1 partly for execution speed, and partly because that's what cudnn uses:\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNMode_t\n\nI'm not sure if that's a good reason though. I think it might be good to at least document that this approximation is being used?", "body": "GRU is implementation of v1 https://arxiv.org/pdf/1406.1078v1.pdf rather than final v3 https://arxiv.org/pdf/1406.1078.pdf\r\n\r\nConcretely, in v1, the reset gate is applied to the result of applying a linear layer to the incoming state, equation 8:\r\n\r\n<img width=\"287\" alt=\"screen shot 2018-06-05 at 1 33 21 pm\" src=\"https://user-images.githubusercontent.com/123560/40958883-108947ac-68c5-11e8-88b4-70ecda354f69.png\">\r\n\r\n... wheras the final v3 first applies the reset gate, and then passes through a linear layer:\r\n\r\n<img width=\"301\" alt=\"screen shot 2018-06-05 at 1 34 19 pm\" src=\"https://user-images.githubusercontent.com/123560/40958904-2e6eb018-68c5-11e8-9ae3-01fed5319651.png\">\r\n\r\nA few minutes informal testing gave me better results on final v3 than draft v1. I imagine that pytorch is using v1 partly for execution speed, and partly because that's what cudnn uses:\r\n\r\nhttps://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#cudnnRNNMode_t\r\n\r\n<img width=\"376\" alt=\"screen shot 2018-06-05 at 1 36 22 pm\" src=\"https://user-images.githubusercontent.com/123560/40958992-77e050e4-68c5-11e8-9813-ff9e2582cd34.png\">\r\n\r\nI'm not sure if that's a good reason though. I think it might be good to at least document that this approximation is being used?\r\n"}