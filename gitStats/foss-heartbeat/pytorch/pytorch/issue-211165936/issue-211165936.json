{"url": "https://api.github.com/repos/pytorch/pytorch/issues/889", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/889/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/889/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/889/events", "html_url": "https://github.com/pytorch/pytorch/issues/889", "id": 211165936, "node_id": "MDU6SXNzdWUyMTExNjU5MzY=", "number": 889, "title": "`bmm` 5x slower on Maxwell (for some sizes)", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-03-01T18:20:44Z", "updated_at": "2017-04-05T20:49:37Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Here's a semi-minimal repro script:</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\nimport time\n\nmr = Variable(torch.cuda.FloatTensor(8192, 9, 4), requires_grad=True)\nlu = Variable(torch.cuda.FloatTensor(8192, 4, 1), requires_grad=True)\ns = time.time()\nfor i in range(300):\n    u = torch.bmm(mr, lu)\n    u.sum().backward()\nprint(time.time() - s)\n</code></pre>\n<p>Run this with <code>CUDA_LAUNCH_BLOCKING=1</code>, it takes 0.6s on Kepler and 3.0s on Maxwell. If you run in nvprof you'll see that Kepler routes <code>cublasSgemmBatched</code> to these faster kernels:</p>\n<pre><code>==3865486== Profiling result:\nTime(%)      Time     Calls       Avg       Min       Max  Name\n 53.35%  76.054ms       600  126.76us  122.82us  130.88us  void fermiPlusSgemmLDS64_batched&lt;bool=0, bool=0, int=4, int=4, int=4, int=3, int=3, bool=1, bool=0&gt;(float**, float**, float**, float*, float const *, float const *, int, int, int, int, int, int, __int64, __int64, __int64, float const *, float const *, float, float, int)\n 26.38%  37.610ms       300  125.37us  124.93us  127.26us  void fermiPlusSgemmLDS64_batched&lt;bool=1, bool=0, int=4, int=4, int=4, int=3, int=3, bool=1, bool=0&gt;(float**, float**, float**, float*, float const *, float const *, int, int, int, int, int, int, __int64, __int64, __int64, float const *, float const *, float, float, int)\n</code></pre>\n<p>whereas Maxwell uses these slow Maxwell-specific kernels:</p>\n<pre><code>==1234091== Profiling result:\nTime(%)      Time     Calls       Avg       Min       Max  Name\n 67.67%  1.66077s       600  2.7679ms  2.5584ms  3.4175ms  maxwell_sgemmBatched_128x128_raggedMn_nn\n 31.57%  774.71ms       300  2.5824ms  2.5525ms  3.1564ms  maxwell_sgemmBatched_128x128_raggedMn_tn\n</code></pre>\n<p>Not sure how to proceed. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>", "body_text": "Here's a semi-minimal repro script:\nimport torch\nfrom torch.autograd import Variable\nimport time\n\nmr = Variable(torch.cuda.FloatTensor(8192, 9, 4), requires_grad=True)\nlu = Variable(torch.cuda.FloatTensor(8192, 4, 1), requires_grad=True)\ns = time.time()\nfor i in range(300):\n    u = torch.bmm(mr, lu)\n    u.sum().backward()\nprint(time.time() - s)\n\nRun this with CUDA_LAUNCH_BLOCKING=1, it takes 0.6s on Kepler and 3.0s on Maxwell. If you run in nvprof you'll see that Kepler routes cublasSgemmBatched to these faster kernels:\n==3865486== Profiling result:\nTime(%)      Time     Calls       Avg       Min       Max  Name\n 53.35%  76.054ms       600  126.76us  122.82us  130.88us  void fermiPlusSgemmLDS64_batched<bool=0, bool=0, int=4, int=4, int=4, int=3, int=3, bool=1, bool=0>(float**, float**, float**, float*, float const *, float const *, int, int, int, int, int, int, __int64, __int64, __int64, float const *, float const *, float, float, int)\n 26.38%  37.610ms       300  125.37us  124.93us  127.26us  void fermiPlusSgemmLDS64_batched<bool=1, bool=0, int=4, int=4, int=4, int=3, int=3, bool=1, bool=0>(float**, float**, float**, float*, float const *, float const *, int, int, int, int, int, int, __int64, __int64, __int64, float const *, float const *, float, float, int)\n\nwhereas Maxwell uses these slow Maxwell-specific kernels:\n==1234091== Profiling result:\nTime(%)      Time     Calls       Avg       Min       Max  Name\n 67.67%  1.66077s       600  2.7679ms  2.5584ms  3.4175ms  maxwell_sgemmBatched_128x128_raggedMn_nn\n 31.57%  774.71ms       300  2.5824ms  2.5525ms  3.1564ms  maxwell_sgemmBatched_128x128_raggedMn_tn\n\nNot sure how to proceed. @colesbury @soumith", "body": "Here's a semi-minimal repro script:\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport time\r\n\r\nmr = Variable(torch.cuda.FloatTensor(8192, 9, 4), requires_grad=True)\r\nlu = Variable(torch.cuda.FloatTensor(8192, 4, 1), requires_grad=True)\r\ns = time.time()\r\nfor i in range(300):\r\n    u = torch.bmm(mr, lu)\r\n    u.sum().backward()\r\nprint(time.time() - s)\r\n```\r\n\r\nRun this with `CUDA_LAUNCH_BLOCKING=1`, it takes 0.6s on Kepler and 3.0s on Maxwell. If you run in nvprof you'll see that Kepler routes `cublasSgemmBatched` to these faster kernels:\r\n\r\n```\r\n==3865486== Profiling result:\r\nTime(%)      Time     Calls       Avg       Min       Max  Name\r\n 53.35%  76.054ms       600  126.76us  122.82us  130.88us  void fermiPlusSgemmLDS64_batched<bool=0, bool=0, int=4, int=4, int=4, int=3, int=3, bool=1, bool=0>(float**, float**, float**, float*, float const *, float const *, int, int, int, int, int, int, __int64, __int64, __int64, float const *, float const *, float, float, int)\r\n 26.38%  37.610ms       300  125.37us  124.93us  127.26us  void fermiPlusSgemmLDS64_batched<bool=1, bool=0, int=4, int=4, int=4, int=3, int=3, bool=1, bool=0>(float**, float**, float**, float*, float const *, float const *, int, int, int, int, int, int, __int64, __int64, __int64, float const *, float const *, float, float, int)\r\n```\r\n\r\nwhereas Maxwell uses these slow Maxwell-specific kernels:\r\n\r\n```\r\n==1234091== Profiling result:\r\nTime(%)      Time     Calls       Avg       Min       Max  Name\r\n 67.67%  1.66077s       600  2.7679ms  2.5584ms  3.4175ms  maxwell_sgemmBatched_128x128_raggedMn_nn\r\n 31.57%  774.71ms       300  2.5824ms  2.5525ms  3.1564ms  maxwell_sgemmBatched_128x128_raggedMn_tn\r\n```\r\n\r\nNot sure how to proceed. @colesbury @soumith "}