{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/284031790", "html_url": "https://github.com/pytorch/pytorch/issues/889#issuecomment-284031790", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/889", "id": 284031790, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NDAzMTc5MA==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-03T18:26:50Z", "updated_at": "2017-03-03T18:27:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Yes, the <code>cmul</code> version is much faster on Maxwell. (only for my specific case because I'm really doing a batched mv).</p>\n<pre><code>            bmm  cmul\nKepler      0.7  0.7\nMaxwell     2.7  0.3\n</code></pre>\n<p>Here's the benchmarking code if interested:</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\nimport time\n\nbs = 8192\nA = Variable(torch.rand(bs, 9, 4).cuda(), requires_grad=True)\nB = Variable(torch.rand(bs, 4, 1).cuda(), requires_grad=True)\n\ns = time.time()\nfor i in range(300):\n    u = torch.bmm(A, B)\n    u.sum().backward()\ntorch.cuda.synchronize()\nprint('bmm', bs, time.time() - s, u.data.norm())\n\ns = time.time()\nfor i in range(300):\n    Bt = B.transpose(1, 2).expand_as(A)\n    u = (A * Bt).sum(2)\n    u.sum().backward()\ntorch.cuda.synchronize()\nprint('cmul', bs, time.time() - s, u.data.norm())\n</code></pre>", "body_text": "Yes, the cmul version is much faster on Maxwell. (only for my specific case because I'm really doing a batched mv).\n            bmm  cmul\nKepler      0.7  0.7\nMaxwell     2.7  0.3\n\nHere's the benchmarking code if interested:\nimport torch\nfrom torch.autograd import Variable\nimport time\n\nbs = 8192\nA = Variable(torch.rand(bs, 9, 4).cuda(), requires_grad=True)\nB = Variable(torch.rand(bs, 4, 1).cuda(), requires_grad=True)\n\ns = time.time()\nfor i in range(300):\n    u = torch.bmm(A, B)\n    u.sum().backward()\ntorch.cuda.synchronize()\nprint('bmm', bs, time.time() - s, u.data.norm())\n\ns = time.time()\nfor i in range(300):\n    Bt = B.transpose(1, 2).expand_as(A)\n    u = (A * Bt).sum(2)\n    u.sum().backward()\ntorch.cuda.synchronize()\nprint('cmul', bs, time.time() - s, u.data.norm())", "body": "Yes, the `cmul` version is much faster on Maxwell. (only for my specific case because I'm really doing a batched mv).\r\n\r\n```\r\n            bmm  cmul\r\nKepler      0.7  0.7\r\nMaxwell     2.7  0.3\r\n```\r\n\r\nHere's the benchmarking code if interested:\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport time\r\n\r\nbs = 8192\r\nA = Variable(torch.rand(bs, 9, 4).cuda(), requires_grad=True)\r\nB = Variable(torch.rand(bs, 4, 1).cuda(), requires_grad=True)\r\n\r\ns = time.time()\r\nfor i in range(300):\r\n    u = torch.bmm(A, B)\r\n    u.sum().backward()\r\ntorch.cuda.synchronize()\r\nprint('bmm', bs, time.time() - s, u.data.norm())\r\n\r\ns = time.time()\r\nfor i in range(300):\r\n    Bt = B.transpose(1, 2).expand_as(A)\r\n    u = (A * Bt).sum(2)\r\n    u.sum().backward()\r\ntorch.cuda.synchronize()\r\nprint('cmul', bs, time.time() - s, u.data.norm())\r\n```"}