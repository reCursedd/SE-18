{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109082225", "pull_request_review_id": 30160684, "id": 109082225, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTA4MjIyNQ==", "diff_hunk": "@@ -0,0 +1,66 @@\n+import torch\n+from torch.autograd import Function\n+\n+\n+class Bilinear(Function):\n+\n+    def forward(self, input1, input2, weight, bias=None):\n+        self.save_for_backward(input1, input2, weight, bias)\n+\n+        output = input1.new()\n+\n+        self.buff2 = input1.new()\n+        self.buff2.resize_as_(input2)\n+\n+        # compute output scores:\n+        output.resize_(input1.size(0), weight.size(0))\n+        for k in range(weight.size(0)):\n+            torch.mm(input1, weight[k], out=self.buff2)\n+            self.buff2.mul_(input2)\n+            torch.sum(self.buff2, 1, out=output.narrow(1, k, 1))\n+\n+        if bias is not None:\n+            output.add_(bias.view(1, bias.nelement()).expand_as(output))\n+\n+        return output\n+\n+    def backward(self, grad_output):\n+        input1, input2, weight, bias = self.saved_tensors\n+        grad_input1 = grad_input2 = grad_weight = grad_bias = None\n+        scale = 1\n+\n+        self.buff1 = input1.new()\n+        self.buff1.resize_as_(input1)\n+\n+        if self.needs_input_grad[0]:\n+            grad_input1 = torch.mm(input2, weight[0].t())\n+            grad_input1.mul_(grad_output.narrow(1, 0, 1).expand(grad_input1.size(0),\n+                                                                grad_input1.size(1)))\n+            grad_input2 = torch.mm(input1, weight[0])\n+            grad_input2.mul_(grad_output.narrow(1, 0, 1).expand(grad_input2.size(0),\n+                                                                grad_input2.size(1)))\n+\n+            for k in range(1, weight.size(0)):\n+                torch.mm(input2, weight[k].t(), out=self.buff1)\n+                self.buff1.mul_(grad_output.narrow(1, k, 1).expand(grad_input1.size(0),\n+                                                                   grad_input1.size(1)))\n+                grad_input1.add_(self.buff1)\n+\n+                torch.mm(input1, weight[k], out=self.buff2)\n+                self.buff2.mul_(grad_output.narrow(1, k, 1).expand(grad_input2.size(0),\n+                                                                   grad_input2.size(1)))\n+                grad_input2.add_(self.buff2)\n+\n+        if self.needs_input_grad[1]:\n+            # accumulate parameter gradients:\n+            for k in range(weight.size(0)):\n+                torch.mul(input1, grad_output.narrow(1, k, 1).expand_as(input1), out=self.buff1)\n+            grad_weight = torch.mm(self.buff1.t(), input2)\n+\n+        if bias is not None and self.needs_input_grad[2]:\n+            grad_bias = torch.add(grad_output.sum(0), scale)", "path": "torch/nn/_functions/bilinear.py", "position": null, "original_position": 61, "commit_id": "736a4dd03f271dda9929b38228b51789e29e75e2", "original_commit_id": "2f40dec201019b0ae3651e609b91232225bc065e", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "by removing `scale`, this can simply become\r\n```python\r\ngrad_bias = grad_output.sum(0)\r\n```", "created_at": "2017-03-31T03:31:44Z", "updated_at": "2018-11-23T15:32:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/1146#discussion_r109082225", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1146", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109082225"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1146#discussion_r109082225"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1146"}}, "body_html": "<p>by removing <code>scale</code>, this can simply become</p>\n<div class=\"highlight highlight-source-python\"><pre>grad_bias <span class=\"pl-k\">=</span> grad_output.sum(<span class=\"pl-c1\">0</span>)</pre></div>", "body_text": "by removing scale, this can simply become\ngrad_bias = grad_output.sum(0)"}