{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1925", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1925/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1925/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1925/events", "html_url": "https://github.com/pytorch/pytorch/issues/1925", "id": 239035842, "node_id": "MDU6SXNzdWUyMzkwMzU4NDI=", "number": 1925, "title": "Type torch.cuda.LongTensor doesn't implement stateless method arange", "user": {"login": "1zb", "id": 4254703, "node_id": "MDQ6VXNlcjQyNTQ3MDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4254703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/1zb", "html_url": "https://github.com/1zb", "followers_url": "https://api.github.com/users/1zb/followers", "following_url": "https://api.github.com/users/1zb/following{/other_user}", "gists_url": "https://api.github.com/users/1zb/gists{/gist_id}", "starred_url": "https://api.github.com/users/1zb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/1zb/subscriptions", "organizations_url": "https://api.github.com/users/1zb/orgs", "repos_url": "https://api.github.com/users/1zb/repos", "events_url": "https://api.github.com/users/1zb/events{/privacy}", "received_events_url": "https://api.github.com/users/1zb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-28T02:29:23Z", "updated_at": "2017-07-19T19:49:39Z", "closed_at": "2017-07-19T19:49:39Z", "author_association": "NONE", "body_html": "<p>Minimal working example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span># define input</span>\nbatch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nchannel <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nheight, width <span class=\"pl-k\">=</span> <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">16</span>\nimages <span class=\"pl-k\">=</span> Variable(torch.randn(batch, channel, height, width)).cuda()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input shape: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(images.size())) <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1 x 1 x 16 x 16</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span># im2col</span>\nkernel_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\npatches <span class=\"pl-k\">=</span> images.unfold(<span class=\"pl-c1\">2</span>, kernel_size, <span class=\"pl-c1\">1</span>).unfold(<span class=\"pl-c1\">3</span>, kernel_size, <span class=\"pl-c1\">1</span>).contiguous().view(batch, channel <span class=\"pl-k\">*</span> kernel_size <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>, height <span class=\"pl-k\">-</span> kernel_size <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, width <span class=\"pl-k\">-</span> kernel_size <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>patches shape: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(patches.size())) <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1 x 9 x 14 x 14</span>\n\nweight <span class=\"pl-k\">=</span> Variable(torch.randn(batch, channel <span class=\"pl-k\">*</span> kernel_size <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>, height <span class=\"pl-k\">-</span> kernel_size <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, width <span class=\"pl-k\">-</span> kernel_size <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>).cuda()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weight shape: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(weight.size())) <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1 x 9 x 14 x 14</span>\n\nlatent <span class=\"pl-k\">=</span> torch.mul(patches, weight).sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>).unsqueeze(<span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>latent shape: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(latent.size())) <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1 x 1 x 14 x 14</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> score = latent.sum()</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> score.backward()</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>## if we uncomment two lines above and stopped here, the error won't show up</span>\n\noutput <span class=\"pl-k\">=</span> latent.unfold(<span class=\"pl-c1\">2</span>, kernel_size, <span class=\"pl-c1\">1</span>).unfold(<span class=\"pl-c1\">3</span>, kernel_size, <span class=\"pl-c1\">1</span>).contiguous().view(batch, kernel_size <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>, height <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> kernel_size <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>, width <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> kernel_size <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output shape: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(output.size())) <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1 x 9 x 12 x 12</span>\n\nscore <span class=\"pl-k\">=</span> output.sum()\nscore.backward()\n</pre></div>\n<p>Error message:</p>\n<pre><code>  File \"/home/biao/miniconda2/envs/pth/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\", line 756, in backward\n    torch.arange(0, ctx.input_numel, out=idx)\nTypeError: Type torch.cuda.LongTensor doesn't implement stateless method arange\n</code></pre>", "body_text": "Minimal working example:\nimport torch\nfrom torch.autograd import Variable\n\n## define input\nbatch = 1\nchannel = 1\nheight, width = 16, 16\nimages = Variable(torch.randn(batch, channel, height, width)).cuda()\nprint(\"input shape: {}\".format(images.size())) # 1 x 1 x 16 x 16\n\n## im2col\nkernel_size = 3\npatches = images.unfold(2, kernel_size, 1).unfold(3, kernel_size, 1).contiguous().view(batch, channel * kernel_size ** 2, height - kernel_size + 1, width - kernel_size + 1)\nprint(\"patches shape: {}\".format(patches.size())) # 1 x 9 x 14 x 14\n\nweight = Variable(torch.randn(batch, channel * kernel_size ** 2, height - kernel_size + 1, width - kernel_size + 1), requires_grad=True).cuda()\nprint(\"weight shape: {}\".format(weight.size())) # 1 x 9 x 14 x 14\n\nlatent = torch.mul(patches, weight).sum(dim=1).unsqueeze(1)\nprint(\"latent shape: {}\".format(latent.size())) # 1 x 1 x 14 x 14\n\n# score = latent.sum()\n# score.backward()\n### if we uncomment two lines above and stopped here, the error won't show up\n\noutput = latent.unfold(2, kernel_size, 1).unfold(3, kernel_size, 1).contiguous().view(batch, kernel_size ** 2, height - 2 * kernel_size + 2, width - 2 * kernel_size + 2)\nprint(\"output shape: {}\".format(output.size())) # 1 x 9 x 12 x 12\n\nscore = output.sum()\nscore.backward()\n\nError message:\n  File \"/home/biao/miniconda2/envs/pth/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\", line 756, in backward\n    torch.arange(0, ctx.input_numel, out=idx)\nTypeError: Type torch.cuda.LongTensor doesn't implement stateless method arange", "body": "Minimal working example:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n## define input\r\nbatch = 1\r\nchannel = 1\r\nheight, width = 16, 16\r\nimages = Variable(torch.randn(batch, channel, height, width)).cuda()\r\nprint(\"input shape: {}\".format(images.size())) # 1 x 1 x 16 x 16\r\n\r\n## im2col\r\nkernel_size = 3\r\npatches = images.unfold(2, kernel_size, 1).unfold(3, kernel_size, 1).contiguous().view(batch, channel * kernel_size ** 2, height - kernel_size + 1, width - kernel_size + 1)\r\nprint(\"patches shape: {}\".format(patches.size())) # 1 x 9 x 14 x 14\r\n\r\nweight = Variable(torch.randn(batch, channel * kernel_size ** 2, height - kernel_size + 1, width - kernel_size + 1), requires_grad=True).cuda()\r\nprint(\"weight shape: {}\".format(weight.size())) # 1 x 9 x 14 x 14\r\n\r\nlatent = torch.mul(patches, weight).sum(dim=1).unsqueeze(1)\r\nprint(\"latent shape: {}\".format(latent.size())) # 1 x 1 x 14 x 14\r\n\r\n# score = latent.sum()\r\n# score.backward()\r\n### if we uncomment two lines above and stopped here, the error won't show up\r\n\r\noutput = latent.unfold(2, kernel_size, 1).unfold(3, kernel_size, 1).contiguous().view(batch, kernel_size ** 2, height - 2 * kernel_size + 2, width - 2 * kernel_size + 2)\r\nprint(\"output shape: {}\".format(output.size())) # 1 x 9 x 12 x 12\r\n\r\nscore = output.sum()\r\nscore.backward()\r\n\r\n```\r\n\r\nError message:\r\n```\r\n  File \"/home/biao/miniconda2/envs/pth/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\", line 756, in backward\r\n    torch.arange(0, ctx.input_numel, out=idx)\r\nTypeError: Type torch.cuda.LongTensor doesn't implement stateless method arange\r\n```"}