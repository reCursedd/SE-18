{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/124427000", "pull_request_review_id": 46713898, "id": 124427000, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNDQyNzAwMA==", "diff_hunk": "@@ -568,107 +625,295 @@ static PyObject* THPTensor_(checkAdvancedIndexing)(THPTensor *self, PyObject *ar\n   Py_RETURN_FALSE;\n }\n \n-static bool THPTensor_(_convertToTensorIndexers)(PyObject *index, std::vector<THLongTensor*>& broadcasted) {\n-  // At the top-level, index must be a sequence. PySequence_Fast returns a new\n-  // reference\n-  PyObject *fast = PySequence_Fast(index, NULL);\n-  Py_ssize_t seqCount = PySequence_Fast_GET_SIZE(fast);\n-  std::vector<THPLongTensor*> indexers;\n-\n-  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n-    PyObject *temp = PyObject_CallFunctionObjArgs(THPLongTensorClass, PySequence_Fast_GET_ITEM(fast, i), NULL);\n-    THPLongTensor *indexer = (THPLongTensor *) temp;\n-    if (!indexer) {\n-      PyErr_Format(PyExc_IndexError,\n-          \"When performing advanced indexing the indexing objects must be LongTensors or convertible to such\");\n-      return false;\n+static bool THPTensor_(_convertToTensorIndexers)(\n+    PyObject *index,\n+    THTensorPtr& indexed,\n+    Py_ssize_t& sequenceLength,\n+    std::unordered_map<Py_ssize_t, THLongTensorPtr>& broadcasted) {\n+\n+  // At the top-level, each indexing element must be one of 3 things:\n+  //\n+  // 1. A LongTensor\n+  // 2. A sequence that can be converted into a LongTensor\n+  // 3. A empty slice object (i.e. ':')\n+  //\n+  // This function loops through all of the indexing elements. If we encounter\n+  // a LongTensor, we record the dimension at which it occurs. If we encounter\n+  // another sequence type, we attempt to convert it to a LongTensor, and record\n+  // its position.\n+  //\n+  // Next, once we have all of the indexing Tensors, we attempt to broadcast them.\n+  // If they can be broadcasted, we store each of the broadcasted Tensors in the\n+  // output map, with the dimension of the original tensor as the key.\n+\n+  // Indexes all indexing Tensors (pre-broadcast) by which dimension they occurred.\n+  // Because we rely upon the THPLongTensor constructor to handle sequence -> tensor\n+  // conversions, we store THPTensors rather than THTensors. We use an ordered map\n+  // to maintain the order of Tensors via dimension. Because this is limited to\n+  // ndim(Tensor), it should always be small + fast.\n+\n+  std::vector<Py_ssize_t> indexingDims;\n+  std::vector<THPLongTensor*>indexers;\n+\n+  // The top-level indexer should be a sequence, per the check above\n+  THPObjectPtr fast(PySequence_Fast(index, NULL));\n+  sequenceLength = PySequence_Fast_GET_SIZE(fast.get());\n+\n+  for (Py_ssize_t i = 0; i < sequenceLength; ++i) {\n+    PyObject *item = PySequence_Fast_GET_ITEM(fast.get(), i);\n+    if (!PySlice_Check(item)) {\n+      // Returns NULL upon conversion failure\n+      THPLongTensor *indexer = (THPLongTensor *)PyObject_CallFunctionObjArgs(\n+          THPLongTensorClass, PySequence_Fast_GET_ITEM(fast.get(), i), NULL);\n+      if (!indexer) {\n+        PyErr_Format(PyExc_IndexError,\n+            \"When performing advanced indexing the indexing objects must be LongTensors or \"\n+            \"convertible to LongTensors\");\n+\n+        // Clean up Indexers\n+        for (auto& idx : indexers) {\n+          THLongTensor_free(idx->cdata);\n+          Py_DECREF(idx);\n+        }\n+        return false;\n+      }\n+      indexingDims.push_back(i);\n+      indexers.push_back(indexer);\n     }\n-    indexers.push_back(indexer);\n   }\n-  Py_DECREF(fast);\n \n-  THLongTensor **maybeBroadcasted = (THLongTensor **)THAlloc(seqCount * sizeof(THLongTensor*));\n-  THLongTensor **candidates = (THLongTensor **)THAlloc(seqCount * sizeof(THLongTensor*));\n+  // Next, we need to verify that the Tensors are broadcastable. Keep these\n+  // as raw pointer vectors\n+  std::vector<THLongTensor*> maybeBroadcasted;\n+  std::vector<THLongTensor*> candidates;\n \n-  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n-    maybeBroadcasted[i] = THLongTensor_new();\n-    candidates[i] = THLongTensor_newWithTensor(indexers[i]->cdata);\n+  // Extract the underlying Tensors for use in the expansion API call\n+  for (const auto& indexer : indexers) {\n+    maybeBroadcasted.emplace_back(THLongTensor_new());\n+    // borrow the underlying Tensor from the indexer map\n+    candidates.emplace_back(indexer->cdata);\n   }\n \n   // Broadcast/Expand indexing Tensors as necessary\n-  bool broadcastSuccess = true;\n   try {\n-    THLongTensor_expandNd(maybeBroadcasted, candidates, seqCount);\n-    // Place Broadcasted Tensors into output vector, implicitly transferring\n-    // ownership\n-    for (Py_ssize_t i = 0; i < seqCount; ++i) {\n-      broadcasted.push_back(maybeBroadcasted[i]);\n+    THLongTensor_expandNd(maybeBroadcasted.data(), candidates.data(), maybeBroadcasted.size());\n+\n+    // Broadcast succeeded, place Broadcasted Tensors into output map by the index at\n+    // which they occurred, transferring ownership to that map object\n+    for (unsigned int i = 0; i < indexingDims.size(); ++i) {\n+      THLongTensorPtr owned(maybeBroadcasted[i]);\n+      broadcasted[indexingDims[i]] = std::move(owned);\n+    }\n+\n+    // Next, before doing any further work, we want to verify that all the indices\n+    // are in bounds at each advanced index dimension\n+\n+    ptrdiff_t nElement = THLongTensor_nElement(broadcasted.begin()->second.get());\n+    THLongStoragePtr viewer(THLongStorage_newWithSize(1));\n+    THLongStorage_set(viewer.get(), 0, nElement);\n+    for (auto& dimBroadcast : broadcasted) {\n+      Py_ssize_t dim = dimBroadcast.first;\n+      long sizeAtDim = THTensor_(size)(LIBRARY_STATE indexed, dim);\n+\n+      // Need to make contiguous to view as 1D :/\n+      THLongTensorPtr contig(THLongTensor_newContiguous(dimBroadcast.second.get()));\n+\n+      // View as 1D + get1D makes me sad :(\n+      THLongTensorPtr flat(THLongTensor_newView(contig.get(), viewer));\n+      for (ptrdiff_t i = 0; i < THLongTensor_nElement(flat.get()); ++i) {\n+        long indexAtDim = THLongTensor_get1d(flat.get(), i);\n+        if (indexAtDim >= sizeAtDim) {\n+          PyErr_Format(PyExc_IndexError, \"index %lld from broadcast indexer is out of range \"\n+              \"for dimension %lld (of size %lld)\",\n+              (long long)indexAtDim, (long long)dim, (long long)sizeAtDim);\n+\n+          // Clean up Indexers\n+          for (auto& idx : indexers) {\n+            THLongTensor_free(idx->cdata);\n+            Py_DECREF(idx);\n+          }\n+\n+          return false;\n+        }\n+      }\n     }\n   } catch (std::exception& e) {\n-    // Broadcasted failed, cleanup and set error\n-    for (int i = 0; i < seqCount; ++i) {\n-      THLongTensor_free(maybeBroadcasted[i]);\n+    // Broadcasted failed, cleanup and return error. I'm not sure if there is a better\n+    // way to do this where we don't have to manually clean up the memory\n+    for (const auto& tensor : maybeBroadcasted) {\n+      THLongTensor_free(tensor);\n     }\n-    broadcastSuccess = false;\n     PyErr_Format(PyExc_IndexError, \"The advanced indexing objects could not be broadcast\");\n+\n+    // Clean up Indexers\n+    for (auto& idx : indexers) {\n+      THLongTensor_free(idx->cdata);\n+      Py_DECREF(idx);\n+    }\n+    return false;\n   }\n \n-  // No matter what, need to cleanup the candidates\n-  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n-    THLongTensor_free(candidates[i]);\n+  // Clean up Indexers\n+  for (auto& idx : indexers) {\n+    THLongTensor_free(idx->cdata);\n+    Py_DECREF(idx);\n   }\n-  THFree(candidates);\n-  THFree(maybeBroadcasted);\n+  return true;\n+}\n+\n+static inline long THPTensor_(_indexToOffset)(\n+    THTensorPtr& indexed,\n+    std::unordered_map<Py_ssize_t, THLongTensorPtr>& broadcasted,\n+    ptrdiff_t index)\n+{\n+  // We need to translate an \"index\" into a linear offset within the Tensor indexed.\n+  // We will perform the normal mod/divide loop, except in the case of an advance indexed\n+  // dimension, we need to take special care to utilize the size and subset of indices\n+  // specified by the Tensor at the advanced indexed dimension. We hereafter refer to\n+  // this as the \"broadcast\" dimension, although in the case of a single indexer, the\n+  // broadcast op is pretty much a no-op.\n+  //\n+  // For example, suppose we have a three-dimensional Tensor x of shape (5, 10, 15),\n+  // and our indexing operation is x[:, (2, 4, 5), :].\n+  //\n+  // For Linear Index 32:\n+  //\n+  // dim = 2 (size = 15): 32 % 15 = 2; 32 / 15 = 2\n+  // dim = 1 (size = 3): 2 % 3 = 2; 2 / 3 = 0\n+  // dim = 0 (size = 5): 0 % 5 = 0; end\n+  //\n+  // So we have selected the index (0, 2, 2). Now for the strides calculation. For the\n+  // non-broadcast dimensions, we simply do the index * the stride. But for the broadcast\n+  // dimension we need to get the corresponding subset index (i.e., pick from (2, 4, 5))\n+  // and use that before multiplying by the stride at that dimension.\n+  //\n+  // (assumes that x is contiguous)\n+  //\n+  // dim = 2 (stride = 1): 2 * stride = 2, offset = 2\n+  // dim = 1 (stride = 15): (broadcast[2] = 5) * stride = 75, offset = 77\n+  // dim = 0 (stride = 75): 0 * stride = 0, offset = 77\n+  //\n+  // So we can see how this works.\n+  //\n+  // The other complication occurs when we have more than one advanced indexer. Consider\n+  // the case:\n+  //\n+  // x = torch.Tensor(3, 4, 6, 3)\n+  // x.stride = (72, 18, 3, 1)\n+  // x[:, [0, 1], [2, 3], :]\n+  //\n+  // Because the advanced indexers are broadcast and iterated as one, we need to apply\n+  // the same index in each of the advanced indexing dimensions. When we reach an advanced\n+  // indexing element, we look to see if the next dimension we will consider is also part\n+  // of the advanced indexing. If it is, we maintain the index:\n+  //\n+  // For Linear Index 16:\n+  //\n+  // dim = 3 (size = 3): 16 % 3 = 1; 16 / 3 = 5\n+  // dim = 2 (size = 2): 5 % 2 = 1; Do Not Update Index\n+  // dim = 1 (size = 2): 5 % 2 = 1; 5 / 2 = 2\n+  // dim = 0 (size = 3): 2 % 3 = 2; end\n+  //\n+  // Then for the offsets:\n+  //\n+  // dim = 3 (stride = 1): 1 * stride = 1, offset: 1\n+  // dim = 2 (stride = 3): [2, 3][1] = 3 * stride = 9, offset = 10\n+  // dim = 1 (stride = 18): [0, 1][1] = 1 * stride = 18, offset = 28\n+  // dim = 0 (stride = 72): 2 * stride = 144, offset = 172\n+  //\n+  // Special care needs to be taken to handle advanced indexers at the beginning, end.\n+\n+  long offset = 0;\n+  for (long i = THTensor_(nDimension)(LIBRARY_STATE indexed) - 1; i >= 0; --i) {\n+    // Get size at dimension i, its the size of the indexed Tensor at that dimension if its\n+    // not an advanced indexing dimension, otherwise its the size of the broadcast Tensor\n+    ptrdiff_t sizeAtDim, indexAtDim, nextIndex;\n+    long strideAtDim = THTensor_(stride)(LIBRARY_STATE indexed, i);\n+\n+    auto broadcast = broadcasted.find(i);\n+    if (broadcast != broadcasted.end()) {\n+      sizeAtDim = THLongTensor_nElement(broadcast->second.get());\n+      indexAtDim = THLongTensor_get1d(broadcast->second.get(), index % sizeAtDim);\n+\n+      if (i > 0 && broadcasted.find(i - 1) != broadcasted.end()) {\n+        nextIndex = index;\n+      } else {\n+        nextIndex = index / sizeAtDim;\n+      }\n+    } else {\n+      sizeAtDim = THTensor_(size)(LIBRARY_STATE indexed, i);\n+      indexAtDim = index % sizeAtDim;\n+      nextIndex = index / sizeAtDim;\n+    }\n \n-  return broadcastSuccess;\n+    offset += indexAtDim * strideAtDim;\n+    index = nextIndex;\n+  }\n+\n+  // size at dim is a bad name, because its really the number of elements in the\n+  // broadcast tensor, rather than the size of the indexed Tensor at that dim\n+\n+  return offset;\n }\n \n // Caller takes ownership of the returned IndexTensor\n static THIndexTensor* THPTensor_(_calculateLinearIndices)(\n-    THTensorPtr& indexed, std::vector<THLongTensor *>& broadcasted) {\n-\n-  // Get the number of indices to generate - this will be equal to the number\n-  // of elements in each broadcasted Tensor\n-  ptrdiff_t indexingElements = THLongTensor_nElement(broadcasted.at(0));\n+    THTensorPtr& indexed,\n+    Py_ssize_t sequenceLength,\n+    std::unordered_map<Py_ssize_t, THLongTensorPtr>& broadcasted) {\n+\n+  // Get the number of indices to generate - this is the product of the size at each dimension,\n+  // that is not part of the advanced indexing, multiplied by the nElement of one of the broadcast\n+  // Tensors. For example:\n+  //\n+  // x = torch.Tensor(10)\n+  // x[[0, 2, 4], ] --> no dims not part of indexing, size = 3\n+  //\n+  // x = torch.Tensor(5, 5)\n+  // x[[0, 3, 3], [1]] --> no dims not part of indexing, size = 3\n+  // x[:, [2, 3]] --> dim_0 not part of indexing, size = 5\n+  //              --> multiply by nElement of broadcast Tensor, nElement = 2\n+  //              --> total_size = 10\n+  //\n+  // x = torch.Tensor(5, 5, 5)\n+  // x[[0, 1], :, :] --> dim_1, dim_2 not part of indexing, size = 5 * 5 = 25\n+  //                 --> multiply by nElement of broadcast Tensor, nElement = 2\n+  //                 --> total_size = 50\n+\n+  // TODO: should this be 1? what if there are no things to index? ????\n+  ptrdiff_t indexingElements = THLongTensor_nElement(broadcasted.begin()->second.get());\n+  for (Py_ssize_t i = 0; i < THTensor_(nDimension)(LIBRARY_STATE indexed.get()); ++i) {\n+    indexingElements *= broadcasted.find(i) != broadcasted.end() ?\n+      1 : THTensor_(size)(LIBRARY_STATE indexed.get(), i);\n+  }\n   THLongTensor *linearIndices = THLongTensor_newWithSize1d(indexingElements);\n+\n+  // The broadcasted advanced indexing tensor might not be one-dimensional, but we are\n+  // generating a vector of indices, so we need to view the indexer as 1D prior to getting\n+  // the value for the particular dimension.\n+  std::unordered_map<Py_ssize_t, THLongTensorPtr> flattenedBroadcasters;\n   THLongStorage *indexerSize = THLongStorage_newWithSize(1);\n-  THLongStorage_set(indexerSize, 0, indexingElements);\n \n-  for (ptrdiff_t i = 0; i < indexingElements; ++i) {\n-    long linearIdx = THTensor_(storageOffset)(LIBRARY_STATE indexed);\n-    for (int j = broadcasted.size() - 1; j >= 0; --j) {\n-      // Given a series of broadcasted Tensors, we take the jth tensor from the sequence (representing\n-      // the indices at dim j) and grab the ith value (used in generating the ith output value)\n-      THLongTensor *indexer = THLongTensor_newContiguous(broadcasted.at(j));\n-\n-      // The indexing tensor might not be one-dimensional, but we are generating a vector of\n-      // indices, so we need to view the indexer as 1D prior to getting the value for the\n-      // particular dimension\n-      THLongTensor *oned = THLongTensor_newView(indexer, indexerSize);\n-\n-      // Actually laod the value, and verify it is not out-of-bounds\n-      long indexAtDim = THLongTensor_get1d(oned, i);\n-      long dimSize = THTensor_(size)(LIBRARY_STATE indexed, j);\n-      if (indexAtDim >= dimSize) {\n-        PyErr_Format(PyExc_IndexError, \"index %lld from broadcast indexer is out of range \"\n-            \"for dimension %lld (of size %lld)\",\n-            (long long)indexAtDim, (long long)j, (long long)dimSize);\n-\n-        THLongTensor_free(oned);\n-        THLongTensor_free(indexer);\n-        THLongStorage_free(indexerSize);\n-        THLongTensor_free(linearIndices);\n-        return NULL;\n-      }\n+  // All broadcast Tensors have the same number of elements\n+  THLongStorage_set(indexerSize,\n+                    0,\n+                    THLongTensor_nElement(broadcasted.begin()->second.get()));\n \n-      linearIdx += THTensor_(stride)(LIBRARY_STATE indexed, j) * THLongTensor_get1d(oned, i);\n-      THLongTensor_free(oned);\n-      THLongTensor_free(indexer);\n-    }\n-    THLongTensor_set1d(linearIndices, i, linearIdx);\n+  for (auto& broadcast : broadcasted) {\n+    THLongTensor *contig = THLongTensor_newContiguous(broadcast.second.get());\n+    THLongTensorPtr flat(THLongTensor_newView(contig, indexerSize));\n+    flattenedBroadcasters[broadcast.first] = std::move(flat);\n+    THLongTensor_free(contig);\n   }\n   THLongStorage_free(indexerSize);\n \n+  long baseOffset = THTensor_(storageOffset)(LIBRARY_STATE indexed);\n+  for (ptrdiff_t i = 0; i < indexingElements; ++i) {\n+    long linearIdx = THPTensor_(_indexToOffset)(\n+        indexed, flattenedBroadcasters, i);\n+    THLongTensor_set1d(linearIndices, i, baseOffset + linearIdx);", "path": "torch/csrc/generic/Tensor.cpp", "position": null, "original_position": 463, "commit_id": "05c83e7c696e55706dafd8316d586b7c1d294c98", "original_commit_id": "ead543fa91ca223db80aea345c54f3de8a5d5ecb", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "body": "you can use `THLongTensor_fastSet1d` which will be much faster, because it's inlined and doesn't do bounds-checks.\r\nSimilarly, wherever you used set1d, use THTensor_fastSet1d", "created_at": "2017-06-28T00:10:55Z", "updated_at": "2018-11-23T15:34:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/1890#discussion_r124427000", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1890", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/124427000"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1890#discussion_r124427000"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1890"}}, "body_html": "<p>you can use <code>THLongTensor_fastSet1d</code> which will be much faster, because it's inlined and doesn't do bounds-checks.<br>\nSimilarly, wherever you used set1d, use THTensor_fastSet1d</p>", "body_text": "you can use THLongTensor_fastSet1d which will be much faster, because it's inlined and doesn't do bounds-checks.\nSimilarly, wherever you used set1d, use THTensor_fastSet1d"}