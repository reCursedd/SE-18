{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/124369580", "pull_request_review_id": 46651843, "id": 124369580, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNDM2OTU4MA==", "diff_hunk": "@@ -568,107 +625,270 @@ static PyObject* THPTensor_(checkAdvancedIndexing)(THPTensor *self, PyObject *ar\n   Py_RETURN_FALSE;\n }\n \n-static bool THPTensor_(_convertToTensorIndexers)(PyObject *index, std::vector<THLongTensor*>& broadcasted) {\n-  // At the top-level, index must be a sequence. PySequence_Fast returns a new\n-  // reference\n-  PyObject *fast = PySequence_Fast(index, NULL);\n-  Py_ssize_t seqCount = PySequence_Fast_GET_SIZE(fast);\n-  std::vector<THPLongTensor*> indexers;\n-\n-  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n-    PyObject *temp = PyObject_CallFunctionObjArgs(THPLongTensorClass, PySequence_Fast_GET_ITEM(fast, i), NULL);\n-    THPLongTensor *indexer = (THPLongTensor *) temp;\n-    if (!indexer) {\n-      PyErr_Format(PyExc_IndexError,\n-          \"When performing advanced indexing the indexing objects must be LongTensors or convertible to such\");\n-      return false;\n+static bool THPTensor_(_convertToTensorIndexers)(\n+    PyObject *index,\n+    THTensorPtr& indexed,\n+    Py_ssize_t& sequenceLength,\n+    std::unordered_map<Py_ssize_t, THLongTensorPtr>& broadcasted) {\n+\n+  // At the top-level, each indexing element must be one of 3 things:\n+  //\n+  // 1. A LongTensor\n+  // 2. A sequence that can be converted into a LongTensor\n+  // 3. A empty slice object (i.e. ':')\n+  //\n+  // This function loops through all of the indexing elements. If we encounter\n+  // a LongTensor, we record the dimension at which it occurs. If we encounter\n+  // another sequence type, we attempt to convert it to a LongTensor, and record\n+  // its position.\n+  //\n+  // Next, once we have all of the indexing Tensors, we attempt to broadcast them.\n+  // If they can be broadcasted, we store each of the broadcasted Tensors in the\n+  // output map, with the dimension of the original tensor as the key.\n+\n+  // Indexes all indexing Tensors (pre-broadcast) by which dimension they occurred.\n+  // Because we rely upon the THPLongTensor constructor to handle sequence -> tensor\n+  // conversions, we store THPTensors rather than THTensors. We use an ordered map\n+  // to maintain the order of Tensors via dimension. Because this is limited to\n+  // ndim(Tensor), it should always be small + fast.\n+\n+  std::vector<Py_ssize_t> indexingDims;\n+  std::vector<THPLongTensor*>indexers;\n+\n+  // The top-level indexer should be a sequence, per the check above\n+  THPObjectPtr fast(PySequence_Fast(index, NULL));\n+  sequenceLength = PySequence_Fast_GET_SIZE(fast.get());\n+\n+  for (Py_ssize_t i = 0; i < sequenceLength; ++i) {\n+    PyObject *item = PySequence_Fast_GET_ITEM(fast.get(), i);\n+    if (!PySlice_Check(item)) {\n+      // Returns NULL upon conversion failure\n+      THPLongTensor *indexer = (THPLongTensor *)PyObject_CallFunctionObjArgs(\n+          THPLongTensorClass, PySequence_Fast_GET_ITEM(fast.get(), i), NULL);\n+      if (!indexer) {\n+        PyErr_Format(PyExc_IndexError,\n+            \"When performing advanced indexing the indexing objects must be LongTensors or \"\n+            \"convertible to LongTensors\");\n+        return false;\n+      }\n+      indexingDims.push_back(i);\n+      indexers.push_back(indexer);\n     }\n-    indexers.push_back(indexer);\n   }\n-  Py_DECREF(fast);\n \n-  THLongTensor **maybeBroadcasted = (THLongTensor **)THAlloc(seqCount * sizeof(THLongTensor*));\n-  THLongTensor **candidates = (THLongTensor **)THAlloc(seqCount * sizeof(THLongTensor*));\n+  // Next, we need to verify that the Tensors are broadcastable. Keep these\n+  // as raw pointer vectors\n+  std::vector<THLongTensor*> maybeBroadcasted;\n+  std::vector<THLongTensor*> candidates;\n \n-  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n-    maybeBroadcasted[i] = THLongTensor_new();\n-    candidates[i] = THLongTensor_newWithTensor(indexers[i]->cdata);\n+  // Extract the underlying Tensors for use in the expansion API call\n+  for (const auto& indexer : indexers) {\n+    maybeBroadcasted.emplace_back(THLongTensor_new());\n+    // borrow the underlying Tensor from the indexer map\n+    candidates.emplace_back(indexer->cdata);\n   }\n \n   // Broadcast/Expand indexing Tensors as necessary\n-  bool broadcastSuccess = true;\n   try {\n-    THLongTensor_expandNd(maybeBroadcasted, candidates, seqCount);\n-    // Place Broadcasted Tensors into output vector, implicitly transferring\n-    // ownership\n-    for (Py_ssize_t i = 0; i < seqCount; ++i) {\n-      broadcasted.push_back(maybeBroadcasted[i]);\n+    THLongTensor_expandNd(maybeBroadcasted.data(), candidates.data(), maybeBroadcasted.size());\n+\n+    // Broadcast succeeded, place Broadcasted Tensors into output map by the index at\n+    // which they occurred, transferring ownership to that map object\n+    for (unsigned int i = 0; i < indexingDims.size(); ++i) {\n+      THLongTensorPtr owned(maybeBroadcasted[i]);\n+      broadcasted[indexingDims[i]] = std::move(owned);\n+    }\n+\n+    // Next, before doing any further work, we want to verify that all the indices\n+    // are in bounds at each advanced index dimension\n+\n+    ptrdiff_t nElement = THLongTensor_nElement(broadcasted.begin()->second.get());\n+    THLongStoragePtr viewer(THLongStorage_newWithSize(1));\n+    THLongStorage_set(viewer.get(), 0, nElement);\n+    for (auto& dimBroadcast : broadcasted) {\n+      Py_ssize_t dim = dimBroadcast.first;\n+      long sizeAtDim = THTensor_(size)(LIBRARY_STATE indexed, dim);\n+\n+      // Need to make contiguous to view as 1D :/\n+      THLongTensorPtr contig(THLongTensor_newContiguous(dimBroadcast.second.get()));\n+\n+      // View as 1D + get1D makes me sad :(\n+      THLongTensorPtr flat(THLongTensor_newView(contig.get(), viewer));\n+      for (ptrdiff_t i = 0; i < THLongTensor_nElement(flat.get()); ++i) {\n+        long indexAtDim = THLongTensor_get1d(flat.get(), i);\n+        if (indexAtDim >= sizeAtDim) {\n+          PyErr_Format(PyExc_IndexError, \"index %lld from broadcast indexer is out of range \"\n+              \"for dimension %lld (of size %lld)\",\n+              (long long)indexAtDim, (long long)dim, (long long)sizeAtDim);\n+          return false;\n+        }\n+      }\n     }\n   } catch (std::exception& e) {\n-    // Broadcasted failed, cleanup and set error\n-    for (int i = 0; i < seqCount; ++i) {\n-      THLongTensor_free(maybeBroadcasted[i]);\n+    // Broadcasted failed, cleanup and return error. I'm not sure if there is a better\n+    // way to do this where we don't have to manually clean up the memory\n+    for (const auto& tensor : maybeBroadcasted) {\n+      THLongTensor_free(tensor);\n     }\n-    broadcastSuccess = false;\n     PyErr_Format(PyExc_IndexError, \"The advanced indexing objects could not be broadcast\");\n+    return false;\n   }\n \n-  // No matter what, need to cleanup the candidates\n-  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n-    THLongTensor_free(candidates[i]);\n+  // TODO: what do we do with candidates? Are the GC'ed, or do we need to do something with them?", "path": "torch/csrc/generic/Tensor.cpp", "position": null, "original_position": 246, "commit_id": "05c83e7c696e55706dafd8316d586b7c1d294c98", "original_commit_id": "95c97b4675842cd550030ee740e0c1393f58937f", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "body": "what are candidates?", "created_at": "2017-06-27T19:14:55Z", "updated_at": "2018-11-23T15:33:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/1890#discussion_r124369580", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1890", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/124369580"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1890#discussion_r124369580"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1890"}}, "body_html": "<p>what are candidates?</p>", "body_text": "what are candidates?"}