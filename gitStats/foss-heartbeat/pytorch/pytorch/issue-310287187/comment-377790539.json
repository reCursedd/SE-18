{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377790539", "html_url": "https://github.com/pytorch/pytorch/issues/6164#issuecomment-377790539", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6164", "id": 377790539, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Nzc5MDUzOQ==", "user": {"login": "kirk86", "id": 2902390, "node_id": "MDQ6VXNlcjI5MDIzOTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2902390?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kirk86", "html_url": "https://github.com/kirk86", "followers_url": "https://api.github.com/users/kirk86/followers", "following_url": "https://api.github.com/users/kirk86/following{/other_user}", "gists_url": "https://api.github.com/users/kirk86/gists{/gist_id}", "starred_url": "https://api.github.com/users/kirk86/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kirk86/subscriptions", "organizations_url": "https://api.github.com/users/kirk86/orgs", "repos_url": "https://api.github.com/users/kirk86/repos", "events_url": "https://api.github.com/users/kirk86/events{/privacy}", "received_events_url": "https://api.github.com/users/kirk86/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-01T14:27:25Z", "updated_at": "2018-04-01T14:27:25Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4556044\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Stonesjtu\">@Stonesjtu</a></p>\n<blockquote>\n<p>I don't quite understand the need for pre-processing data on GPU actually.</p>\n</blockquote>\n<p>Say you have many gpus, instead of doing the pre-processing on cpu you could leverage the gpus that are sitting there doing nothing.  That would speed up things a lot and during training the actual gpus that are responsible for the computation won't have to wait the cpu to finish to load data into gpu memory.  Things can happen in parallel especially if your pre-processing is computationally expensive.</p>", "body_text": "@Stonesjtu\n\nI don't quite understand the need for pre-processing data on GPU actually.\n\nSay you have many gpus, instead of doing the pre-processing on cpu you could leverage the gpus that are sitting there doing nothing.  That would speed up things a lot and during training the actual gpus that are responsible for the computation won't have to wait the cpu to finish to load data into gpu memory.  Things can happen in parallel especially if your pre-processing is computationally expensive.", "body": "@Stonesjtu \r\n\r\n> I don't quite understand the need for pre-processing data on GPU actually.\r\n\r\nSay you have many gpus, instead of doing the pre-processing on cpu you could leverage the gpus that are sitting there doing nothing.  That would speed up things a lot and during training the actual gpus that are responsible for the computation won't have to wait the cpu to finish to load data into gpu memory.  Things can happen in parallel especially if your pre-processing is computationally expensive."}