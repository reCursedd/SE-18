{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6164", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6164/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6164/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6164/events", "html_url": "https://github.com/pytorch/pytorch/issues/6164", "id": 310287187, "node_id": "MDU6SXNzdWUzMTAyODcxODc=", "number": 6164, "title": "feature requests and future roadmap", "user": {"login": "kirk86", "id": 2902390, "node_id": "MDQ6VXNlcjI5MDIzOTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2902390?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kirk86", "html_url": "https://github.com/kirk86", "followers_url": "https://api.github.com/users/kirk86/followers", "following_url": "https://api.github.com/users/kirk86/following{/other_user}", "gists_url": "https://api.github.com/users/kirk86/gists{/gist_id}", "starred_url": "https://api.github.com/users/kirk86/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kirk86/subscriptions", "organizations_url": "https://api.github.com/users/kirk86/orgs", "repos_url": "https://api.github.com/users/kirk86/repos", "events_url": "https://api.github.com/users/kirk86/events{/privacy}", "received_events_url": "https://api.github.com/users/kirk86/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-04-01T02:17:36Z", "updated_at": "2018-04-04T13:17:38Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>PyTorch Feature Requests and Future RoadMap</h2>\n<p>Hello everyone and thank your for this great project. I would like to point to some things that I find important for the future growth of pytorch.</p>\n<ol>\n<li>Easy name conventions throughout the library. I believe that some things should be renamed so that it resonates and are easier to remember. This will also lower the entry bar for new people exposed to pytorch.</li>\n</ol>\n<p>Example:</p>\n<pre><code>import torch\nimport torchvision\nimport torchvision.models as models\nmodel = models.densenet201()\n\nto iterate over the layers I'll have to do the following\n\nfor idx, m in enumerate(model.named_modules):\n</code></pre>\n<p>The <code>named_modules</code> or <code>named_children</code> doesn't mean anything to me and as soon as I use them my brain has already forgotten about it. A more sane naming good be <code>model.layers</code> that seems to be a universal convention pretty much since other major libraries are using it.</p>\n<p>I would also love to see something like <code>model.layers.add(x)</code>,  <code>model.layers.pop(x)</code>, <code>model.layers.get(x)</code>, <code>model.save()</code>, <code>model.load()</code>, etc.</p>\n<ol start=\"2\">\n<li>Model dissection.<br>\nMore often we find ourselves in a scenario where we would like to get the output of a complex model at different levels.</li>\n</ol>\n<p>Example:</p>\n<pre><code>let's say we have the model = models.densenet201()\nto get the output at different levels we would have to do something like this\nmodel1 = nn.Sequential(*list(model.children())[:4])\nmodel2 = nn.Sequential(*list(model.children())[:7])\n\nWhich feels more of a hack to me TBH.\n\nI would much  rather prefer to do the following:\n\nmodel1 = model.layers.layer_name or model.layers[4].output\n</code></pre>\n<ol start=\"3\">\n<li>\n<p>Dataset preprocessing and transformation pipeline switch between cpu/gpu<br>\nMany times we find ourselves the need to do the data preprocessing on the gpu especially in the scenario where there is an abundance. But the choice at the moment is inexistent. There should be a flag for the user to easily switch between cpu and gpu upon request in order to chose where the data preprocessing should take place.</p>\n</li>\n<li>\n<p>Addition of negative indexing in Tensors.</p>\n</li>\n<li>\n<p>Integration and applicability with already pre-existing tools.<br>\nSome tools that already pre-exist and are nice to have an out of the box support and integration with them. For instance tensorboard is great but at the moment we need to rely on third party developed plugins for this to work. Instead it would be better if pytorch had the ability to dump logs that can be consumed by tensorboard directly.</p>\n</li>\n<li>\n<p>Serving models is completely inexistent at the moment of writing.</p>\n</li>\n</ol>\n<p>For anyone else please feel free to add anything else you might think is important for future directions.<br>\nBasically I just watched the whole tf dev summit and the one advantage that pytorch might had against tf was the dynamic models but now that's gonna with eager mode. And since folks at tf have faster release cycles they can fail and iterate faster on concepts and ideas. A lot of people where nagging about the language choices which where horrific but since then they have iterated over a lot and brought it up to a sane level <code>tf.layers</code>.</p>\n<p>These things make me wonder about the future directions and where pytorch is heading?</p>\n<p>Thank you!</p>", "body_text": "PyTorch Feature Requests and Future RoadMap\nHello everyone and thank your for this great project. I would like to point to some things that I find important for the future growth of pytorch.\n\nEasy name conventions throughout the library. I believe that some things should be renamed so that it resonates and are easier to remember. This will also lower the entry bar for new people exposed to pytorch.\n\nExample:\nimport torch\nimport torchvision\nimport torchvision.models as models\nmodel = models.densenet201()\n\nto iterate over the layers I'll have to do the following\n\nfor idx, m in enumerate(model.named_modules):\n\nThe named_modules or named_children doesn't mean anything to me and as soon as I use them my brain has already forgotten about it. A more sane naming good be model.layers that seems to be a universal convention pretty much since other major libraries are using it.\nI would also love to see something like model.layers.add(x),  model.layers.pop(x), model.layers.get(x), model.save(), model.load(), etc.\n\nModel dissection.\nMore often we find ourselves in a scenario where we would like to get the output of a complex model at different levels.\n\nExample:\nlet's say we have the model = models.densenet201()\nto get the output at different levels we would have to do something like this\nmodel1 = nn.Sequential(*list(model.children())[:4])\nmodel2 = nn.Sequential(*list(model.children())[:7])\n\nWhich feels more of a hack to me TBH.\n\nI would much  rather prefer to do the following:\n\nmodel1 = model.layers.layer_name or model.layers[4].output\n\n\n\nDataset preprocessing and transformation pipeline switch between cpu/gpu\nMany times we find ourselves the need to do the data preprocessing on the gpu especially in the scenario where there is an abundance. But the choice at the moment is inexistent. There should be a flag for the user to easily switch between cpu and gpu upon request in order to chose where the data preprocessing should take place.\n\n\nAddition of negative indexing in Tensors.\n\n\nIntegration and applicability with already pre-existing tools.\nSome tools that already pre-exist and are nice to have an out of the box support and integration with them. For instance tensorboard is great but at the moment we need to rely on third party developed plugins for this to work. Instead it would be better if pytorch had the ability to dump logs that can be consumed by tensorboard directly.\n\n\nServing models is completely inexistent at the moment of writing.\n\n\nFor anyone else please feel free to add anything else you might think is important for future directions.\nBasically I just watched the whole tf dev summit and the one advantage that pytorch might had against tf was the dynamic models but now that's gonna with eager mode. And since folks at tf have faster release cycles they can fail and iterate faster on concepts and ideas. A lot of people where nagging about the language choices which where horrific but since then they have iterated over a lot and brought it up to a sane level tf.layers.\nThese things make me wonder about the future directions and where pytorch is heading?\nThank you!", "body": "PyTorch Feature Requests and Future RoadMap\r\n--------------------------------\r\nHello everyone and thank your for this great project. I would like to point to some things that I find important for the future growth of pytorch. \r\n\r\n1. Easy name conventions throughout the library. I believe that some things should be renamed so that it resonates and are easier to remember. This will also lower the entry bar for new people exposed to pytorch.\r\n\r\nExample:\r\n```\r\nimport torch\r\nimport torchvision\r\nimport torchvision.models as models\r\nmodel = models.densenet201()\r\n\r\nto iterate over the layers I'll have to do the following\r\n\r\nfor idx, m in enumerate(model.named_modules):\r\n```\r\n\r\nThe `named_modules` or `named_children` doesn't mean anything to me and as soon as I use them my brain has already forgotten about it. A more sane naming good be `model.layers` that seems to be a universal convention pretty much since other major libraries are using it.\r\n\r\nI would also love to see something like `model.layers.add(x)`,  `model.layers.pop(x)`, `model.layers.get(x)`, `model.save()`, `model.load()`, etc.\r\n\r\n2. Model dissection. \r\nMore often we find ourselves in a scenario where we would like to get the output of a complex model at different levels.\r\n\r\nExample:\r\n```\r\nlet's say we have the model = models.densenet201()\r\nto get the output at different levels we would have to do something like this\r\nmodel1 = nn.Sequential(*list(model.children())[:4])\r\nmodel2 = nn.Sequential(*list(model.children())[:7])\r\n\r\nWhich feels more of a hack to me TBH.\r\n\r\nI would much  rather prefer to do the following:\r\n\r\nmodel1 = model.layers.layer_name or model.layers[4].output\r\n```\r\n\r\n3. Dataset preprocessing and transformation pipeline switch between cpu/gpu\r\nMany times we find ourselves the need to do the data preprocessing on the gpu especially in the scenario where there is an abundance. But the choice at the moment is inexistent. There should be a flag for the user to easily switch between cpu and gpu upon request in order to chose where the data preprocessing should take place.\r\n\r\n4. Addition of negative indexing in Tensors.\r\n\r\n5. Integration and applicability with already pre-existing tools.\r\nSome tools that already pre-exist and are nice to have an out of the box support and integration with them. For instance tensorboard is great but at the moment we need to rely on third party developed plugins for this to work. Instead it would be better if pytorch had the ability to dump logs that can be consumed by tensorboard directly. \r\n\r\n5. Serving models is completely inexistent at the moment of writing.\r\n\r\nFor anyone else please feel free to add anything else you might think is important for future directions.\r\nBasically I just watched the whole tf dev summit and the one advantage that pytorch might had against tf was the dynamic models but now that's gonna with eager mode. And since folks at tf have faster release cycles they can fail and iterate faster on concepts and ideas. A lot of people where nagging about the language choices which where horrific but since then they have iterated over a lot and brought it up to a sane level `tf.layers`.\r\n\r\nThese things make me wonder about the future directions and where pytorch is heading?\r\n\r\n\r\nThank you!\r\n"}