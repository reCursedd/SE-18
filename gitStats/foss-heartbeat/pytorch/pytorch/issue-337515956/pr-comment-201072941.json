{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201072941", "pull_request_review_id": 135503235, "id": 201072941, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTA3Mjk0MQ==", "diff_hunk": "@@ -0,0 +1,137 @@\n+#include \"ATen/native/LinearAlgebraUtils.h\"\n+#include \"ATen/cudnn/Exceptions.h\" // for ATEN_CUDA_CHECK\n+\n+#include <cublas_v2.h>\n+\n+namespace at {\n+namespace native {\n+\n+template<class scalar_t>\n+int cublasGetri(cublasHandle_t handle, int n, scalar_t **a, int lda, int *pivots, scalar_t **b, int m, int *info, int batch_size) {\n+  AT_ERROR(\"cuBLAS getri is only implemented for double and float\");\n+}\n+\n+template<class scalar_t>\n+int cublasGetrf(cublasHandle_t handle, int n, scalar_t **a, int lda, int *pivots, int *info, int batch_size) {\n+  AT_ERROR(\"cuBLAS getrf is only implemented for double and float\");\n+}\n+\n+template<> int cublasGetri<float>(\n+cublasHandle_t handle, int n, float **a, int lda, int *pivots, float **b, int m, int *info, int batch_size) {\n+    return cublasSgetriBatched(handle, n, (const float**)a, lda, pivots, b, m, info, batch_size);\n+}\n+\n+template<> int cublasGetri<double>(\n+cublasHandle_t handle, int n, double **a, int lda, int *pivots, double **b, int m, int *info, int batch_size) {\n+    return cublasDgetriBatched(handle, n, (const double**)a, lda, pivots, b, m, info, batch_size);\n+}\n+\n+template<> int cublasGetrf<float>(\n+cublasHandle_t handle, int n, float **a, int lda, int *pivots, int *info, int batch_size) {\n+    return cublasSgetrfBatched(handle, n, a, lda, pivots, info, batch_size);\n+}\n+\n+template<> int cublasGetrf<double>(\n+cublasHandle_t handle, int n, double **a, int lda, int *pivots, int *info, int batch_size) {\n+    return cublasDgetrfBatched(handle, n, a, lda, pivots, info, batch_size);\n+}\n+\n+// Float and double have different sizes, so this needs to be templated\n+template<class scalar_t>\n+__global__ void createInverseBuffers(scalar_t **buffer_a, scalar_t *data_a, scalar_t **buffer_b, scalar_t *data_b, int n, int batch_size)\n+{\n+    const int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n+    if(idx < batch_size)\n+    {\n+      buffer_a[idx] = data_a + n*n*idx;\n+      buffer_b[idx] = data_b + n*n*idx;\n+    }\n+}\n+\n+// Tensor inverse\n+Tensor inverse_cuda(const Tensor& self) {\n+ \n+    if(self.dim() < 2)\n+    {\n+        AT_ERROR(\"Inverse: expected a tensor with 2 or more dimensions, got \", self.dim());\n+    }\n+\n+    cublasHandle_t handle = globalContext().getCurrentCublasHandle();\n+    Tensor input_contiguous = self.contiguous();\n+\n+    int n = self.size(-1);\n+    int m = self.size(-2);\n+    int lda = n;\n+    if(n != m)\n+    {\n+        AT_ERROR(\"Inverse: expected square matrices\");\n+    }\n+    Tensor input_reshaped = input_contiguous.view({-1, m, n});\n+    Tensor input_colmajor = cloneBatchedColumnMajor(input_reshaped);\n+    int batch_size = input_colmajor.size(0);\n+    Tensor result = at::empty_like(input_colmajor);\n+\n+    AT_DISPATCH_FLOATING_TYPES(input_reshaped.type(), \"inverse_cuda\", [&] {\n+        auto input_data = input_colmajor.data<scalar_t>();\n+        auto output_data = result.data<scalar_t>();\n+        bool errorOccurred = false;\n+        int *info_cpu = new int[batch_size];\n+\n+        scalar_t **input_gpu;\n+        scalar_t **output_gpu;\n+        scalar_t **input_ptrs = new scalar_t*[batch_size];\n+        scalar_t **output_ptrs = new scalar_t*[batch_size];\n+        AT_CUDA_CHECK(cudaMalloc(&input_gpu, batch_size*sizeof(scalar_t*)));", "path": "aten/src/ATen/native/cuda/Inverse.cu", "position": null, "original_position": 84, "commit_id": "cb3f11c7e7fb420e5b38eb7128d0753765389a1b", "original_commit_id": "3af65921d75b4a6436dc253f745e52f9528a6b2c", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "could you please replace the instances of `cudaMalloc` with `THCudaMalloc`, and `cudaFree` with `THCudaFree`? @ezyang , I don't think we expose those in ATen yet, right?", "created_at": "2018-07-09T16:50:15Z", "updated_at": "2018-11-23T15:46:58Z", "html_url": "https://github.com/pytorch/pytorch/pull/9102#discussion_r201072941", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9102", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201072941"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9102#discussion_r201072941"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9102"}}, "body_html": "<p>could you please replace the instances of <code>cudaMalloc</code> with <code>THCudaMalloc</code>, and <code>cudaFree</code> with <code>THCudaFree</code>? <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> , I don't think we expose those in ATen yet, right?</p>", "body_text": "could you please replace the instances of cudaMalloc with THCudaMalloc, and cudaFree with THCudaFree? @ezyang , I don't think we expose those in ATen yet, right?"}