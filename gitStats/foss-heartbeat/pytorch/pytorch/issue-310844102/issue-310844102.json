{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6228", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6228/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6228/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6228/events", "html_url": "https://github.com/pytorch/pytorch/issues/6228", "id": 310844102, "node_id": "MDU6SXNzdWUzMTA4NDQxMDI=", "number": 6228, "title": "GPU/CPU model synchronization", "user": {"login": "E1eMenta", "id": 32593811, "node_id": "MDQ6VXNlcjMyNTkzODEx", "avatar_url": "https://avatars3.githubusercontent.com/u/32593811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/E1eMenta", "html_url": "https://github.com/E1eMenta", "followers_url": "https://api.github.com/users/E1eMenta/followers", "following_url": "https://api.github.com/users/E1eMenta/following{/other_user}", "gists_url": "https://api.github.com/users/E1eMenta/gists{/gist_id}", "starred_url": "https://api.github.com/users/E1eMenta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/E1eMenta/subscriptions", "organizations_url": "https://api.github.com/users/E1eMenta/orgs", "repos_url": "https://api.github.com/users/E1eMenta/repos", "events_url": "https://api.github.com/users/E1eMenta/events{/privacy}", "received_events_url": "https://api.github.com/users/E1eMenta/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-03T13:47:22Z", "updated_at": "2018-04-03T21:21:18Z", "closed_at": "2018-04-03T21:21:18Z", "author_association": "NONE", "body_html": "<ul>\n<li>PyTorch 0.3.1:</li>\n<li>Ubuntu 16.04.3 LTS:</li>\n<li>PyTorch installed via pip</li>\n<li>Anaconda Python 3.5.3:</li>\n<li>CUDA 9.0/cuDNN 7.0 version:</li>\n<li>gtx 1080ti:</li>\n</ul>\n<p>I'm developing asynchronous multi-gpu. For this task I forward/backward gpu model, copy gradients to shared model, optimize, and copy weights back. Code diverges on gpu and converges on cpu. What the reason?</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net,self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\ntrain_loader = torch.utils.data.DataLoader(\n                datasets.MNIST('data', train=True, download=True,\n                               transform=transforms.Compose([\n                                   transforms.ToTensor(),\n                                   transforms.Normalize((0.1307,), (0.3081,))\n                               ])),\n                batch_size=512, shuffle=True, num_workers=1)\n\nmodel = Net().cuda()\nshared_model = Net()\n\noptimizer = torch.optim.SGD(shared_model.parameters(), lr=0.01, momentum=0.5)\n\nfor param_group in optimizer.param_groups:\n    params = param_group['params']\n\n# Grads from None to good value\nweights_mean = [torch.mean(param) for param in params]\nweights_sum = sum(weights_mean)\nweights_sum.backward()\n\nmodel.train()\nepoch = 0\nloss_sum = 0\n\nwhile True:\n    for batch_idx, (data, target) in enumerate(train_loader):\n        model.load_state_dict(shared_model.state_dict())\n\n        data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n\n        optimizer.zero_grad()\n\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n\n        for param, shared_parameter in zip(model.parameters(), shared_model.parameters()):\n                shared_parameter.grad.data = param.grad.data.cpu()\n\n        loss_sum += loss\n        optimizer.step()\n\n        loss_sum += loss\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} Loss: {:.6f}'.format(\n                epoch, loss.data[0] / 10))\n            loss_sum = 0\n    epoch += 1\n</code></pre>", "body_text": "PyTorch 0.3.1:\nUbuntu 16.04.3 LTS:\nPyTorch installed via pip\nAnaconda Python 3.5.3:\nCUDA 9.0/cuDNN 7.0 version:\ngtx 1080ti:\n\nI'm developing asynchronous multi-gpu. For this task I forward/backward gpu model, copy gradients to shared model, optimize, and copy weights back. Code diverges on gpu and converges on cpu. What the reason?\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net,self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\ntrain_loader = torch.utils.data.DataLoader(\n                datasets.MNIST('data', train=True, download=True,\n                               transform=transforms.Compose([\n                                   transforms.ToTensor(),\n                                   transforms.Normalize((0.1307,), (0.3081,))\n                               ])),\n                batch_size=512, shuffle=True, num_workers=1)\n\nmodel = Net().cuda()\nshared_model = Net()\n\noptimizer = torch.optim.SGD(shared_model.parameters(), lr=0.01, momentum=0.5)\n\nfor param_group in optimizer.param_groups:\n    params = param_group['params']\n\n# Grads from None to good value\nweights_mean = [torch.mean(param) for param in params]\nweights_sum = sum(weights_mean)\nweights_sum.backward()\n\nmodel.train()\nepoch = 0\nloss_sum = 0\n\nwhile True:\n    for batch_idx, (data, target) in enumerate(train_loader):\n        model.load_state_dict(shared_model.state_dict())\n\n        data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n\n        optimizer.zero_grad()\n\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n\n        for param, shared_parameter in zip(model.parameters(), shared_model.parameters()):\n                shared_parameter.grad.data = param.grad.data.cpu()\n\n        loss_sum += loss\n        optimizer.step()\n\n        loss_sum += loss\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} Loss: {:.6f}'.format(\n                epoch, loss.data[0] / 10))\n            loss_sum = 0\n    epoch += 1", "body": "- PyTorch 0.3.1:\r\n- Ubuntu 16.04.3 LTS:\r\n- PyTorch installed via pip\r\n- Anaconda Python 3.5.3:\r\n- CUDA 9.0/cuDNN 7.0 version:\r\n- gtx 1080ti:\r\n\r\nI'm developing asynchronous multi-gpu. For this task I forward/backward gpu model, copy gradients to shared model, optimize, and copy weights back. Code diverges on gpu and converges on cpu. What the reason?\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torchvision import datasets, transforms\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net,self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.conv2_drop = nn.Dropout2d()\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n        x = x.view(-1, 320)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x, dim=1)\r\n\r\ntrain_loader = torch.utils.data.DataLoader(\r\n                datasets.MNIST('data', train=True, download=True,\r\n                               transform=transforms.Compose([\r\n                                   transforms.ToTensor(),\r\n                                   transforms.Normalize((0.1307,), (0.3081,))\r\n                               ])),\r\n                batch_size=512, shuffle=True, num_workers=1)\r\n\r\nmodel = Net().cuda()\r\nshared_model = Net()\r\n\r\noptimizer = torch.optim.SGD(shared_model.parameters(), lr=0.01, momentum=0.5)\r\n\r\nfor param_group in optimizer.param_groups:\r\n    params = param_group['params']\r\n\r\n# Grads from None to good value\r\nweights_mean = [torch.mean(param) for param in params]\r\nweights_sum = sum(weights_mean)\r\nweights_sum.backward()\r\n\r\nmodel.train()\r\nepoch = 0\r\nloss_sum = 0\r\n\r\nwhile True:\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        model.load_state_dict(shared_model.state_dict())\r\n\r\n        data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\r\n\r\n        optimizer.zero_grad()\r\n\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n\r\n        for param, shared_parameter in zip(model.parameters(), shared_model.parameters()):\r\n                shared_parameter.grad.data = param.grad.data.cpu()\r\n\r\n        loss_sum += loss\r\n        optimizer.step()\r\n\r\n        loss_sum += loss\r\n        optimizer.step()\r\n        if batch_idx % 10 == 0:\r\n            print('Train Epoch: {} Loss: {:.6f}'.format(\r\n                epoch, loss.data[0] / 10))\r\n            loss_sum = 0\r\n    epoch += 1\r\n```\r\n\r\n"}