{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2640", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2640/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2640/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2640/events", "html_url": "https://github.com/pytorch/pytorch/issues/2640", "id": 255505428, "node_id": "MDU6SXNzdWUyNTU1MDU0Mjg=", "number": 2640, "title": "\"torch.utils.data.DataLoader\" and \".data.cpu().numpy().copy()\" goes wrong with no cuda environment.", "user": {"login": "hiram94", "id": 20653922, "node_id": "MDQ6VXNlcjIwNjUzOTIy", "avatar_url": "https://avatars0.githubusercontent.com/u/20653922?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hiram94", "html_url": "https://github.com/hiram94", "followers_url": "https://api.github.com/users/hiram94/followers", "following_url": "https://api.github.com/users/hiram94/following{/other_user}", "gists_url": "https://api.github.com/users/hiram94/gists{/gist_id}", "starred_url": "https://api.github.com/users/hiram94/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hiram94/subscriptions", "organizations_url": "https://api.github.com/users/hiram94/orgs", "repos_url": "https://api.github.com/users/hiram94/repos", "events_url": "https://api.github.com/users/hiram94/events{/privacy}", "received_events_url": "https://api.github.com/users/hiram94/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-06T07:23:45Z", "updated_at": "2017-09-07T03:26:41Z", "closed_at": "2017-09-07T00:49:16Z", "author_association": "NONE", "body_html": "<p>I am trying to modify a model code, which works well with one or multiple GPUs, so as to run with CPU-only devices. I have encountered two problems:(The one marked [<strong>solved</strong>] has been solved thanks to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1201055\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/chenzhekl\">@chenzhekl</a> 's help )<br>\n2)[<strong>unsolved</strong>] when I run the model on CPU, it has something wrong with .data.cpu().numpy().copy()</p>\n<pre><code>Computing results...\n     ---the 0th images, captions, lengths, ids--from data_loader---\n\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n          ...             \u22f1             ...          \n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n[torch.FloatTensor of size 13x4096]\n\ncaptions shape[0] 13  shape[1] 18\n\n\nColumns 0 to 12 \n    1  1240  1857  2704  1240  2813  1701  2453  2191  2704  2109  1742  1240\n    1  1240  1694  1742  1240   788  1196   895  1292     3     3  3015  1060\n    1   933   889  2920  1742   933  2516  1742  1448  2783  1240  1211   130\n    1  1240  1857  2704  1240  2813   403  2427   657   397  2600   985   130\n    1  1240   991  2704  2696  1694   788  1302  1366  1742   933  1196     2\n    1   442  1694  1738   538  2071   933  2640  1742  1240  1196   130     2\n    1   933   442   991  2704  2696  1694  1738  1742   933  1196   130     2\n    1  1240  1694   895  1742  1240  1196  1734  1240   583   130     2     0\n    1  1240  1857  2704  1240  2813  1742   823  2191  2109   130     2     0\n    1  2050   991   889    64   966  1742   933  2516   130     2     0     0\n    1  2050   991   889  3001  1742   933  2516   130     2     0     0     0\n    1   933   889   519   657   933  2516   130     2     0     0     0     0\n    1  2050   889  2069  1742   933  2516   130     2     0     0     0     0\n\nColumns 13 to 17 \n  171  2783  2259   130     2\n  544   130     2     0     0\n    2     0     0     0     0\n    2     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n[torch.LongTensor of size 13x18]\n\nlen(lengths) 13\n[18, 16, 14, 14, 13, 13, 13, 12, 12, 11, 10, 9, 9]\nlen(ids) 13\n(12, 6, 0, 11, 5, 8, 9, 7, 10, 2, 3, 1, 4)\n\nimg_embs is None\nlen(data_loader.dataset)=13\nTraceback (most recent call last):\n  File \"test.py\", line 373, in &lt;module&gt;\n    main(opt.resume,  opt.data_path, opt.split)\n  File \"test.py\", line 198, in main\n    img_embs, cap_embs = encode_data(model, data_loader)\n  File \"test.py\", line 126, in encode_data\n    img_embs[ids] = img_emb.data.cpu().numpy().copy()\nIndexError: too many indices for array\n</code></pre>\n<p>The only output difference  between on GPU and on CPU is that on CPU ids is a tuple ( )  but on GPU it is a list [ ].<br>\nHere is the output on GPU starting from the difference: (I just loaded 13 captions to reduce the run time)</p>\n<pre><code>len(ids) 13\n[12, 6, 0, 11, 5, 8, 9, 7, 10, 2, 3, 1, 4]\n\nimg_embs is None\nlen(data_loader.dataset)=13\n          ---img_emb[0]---\nVariable containing:\n 1.8998e-02\n 6.6110e-03\n 8.6337e-02\n     \u22ee     \n 1.9800e-02\n 2.4219e-02\n 1.4796e-02\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\n\n          ---img_embs[0]---\n[ 0.01899767  0.00661102  0.08633662 ...,  0.0198002   0.02421924\n  0.01479562]\n          ---cap_emb[0]---\nVariable containing:\n 5.6984e-03\n-4.3049e-02\n 3.3034e-02\n     \u22ee     \n 3.2743e-02\n-5.2397e-02\n 2.9017e-03\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\n\n          ---cap_embs[ids[0]]---\n[ 0.00569838 -0.04304908  0.03303399 ...,  0.03274297 -0.05239733\n  0.00290167]\n          ---img_emb[5]---\nVariable containing:\n 1.8998e-02\n 6.6110e-03\n 8.6337e-02\n     \u22ee     \n 1.9800e-02\n 2.4219e-02\n 1.4796e-02\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\n\n          ---img_embs[ids[5]]---\n[ 0.01899767  0.00661102  0.08633662 ...,  0.0198002   0.02421924\n  0.01479562]\n          ---cap_emb[6]---\nVariable containing:\n 0.0277\n 0.0205\n 0.0128\n   \u22ee   \n 0.0187\n-0.0072\n-0.0342\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\n\n          ---cap_embs[ids[6]]---\n[ 0.02767145  0.02054488  0.01282509 ...,  0.0187309  -0.00720693\n -0.03416499]\nTest: [0/1]\tLe 8.2995 (8.2995)\tTime 0.239 (0.000)\t\n0\nimg_embs shape[0] 13  shape[1] 1024\ncap_embs shape[0] 13  shape[1] 1024\nImages: 2, Captions: 13\n</code></pre>\n<p>This is the code associated with \".data.cpu().numpy().copy()\":</p>\n<div class=\"highlight highlight-source-shell\"><pre>def encode_data(model, data_loader, log_step=10, logging=print):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span>Encode all images and captions loadable by <span class=\"pl-s\"><span class=\"pl-pds\">`</span>data_loader<span class=\"pl-pds\">`</span></span></span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>\n    batch_time = <span class=\"pl-en\">AverageMeter</span>()\n    val_logger = <span class=\"pl-en\">LogCollector</span>()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> switch to evaluate mode</span>\n    <span class=\"pl-en\">model.val_start</span>()\n\n    end = <span class=\"pl-en\">time.time</span>()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> numpy array to keep all the embeddings</span>\n    img_embs = None\n    cap_embs = None\n    <span class=\"pl-k\">for</span> <span class=\"pl-smi\">i, (images, captions, lengths, ids)</span> <span class=\"pl-k\">in</span> enumerate(data_loader):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> make sure val logger is used</span>\n        model.logger = val_logger\n\n        if(i==0 or i <span class=\"pl-k\">&gt;</span> 38):\n                print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>     ---the {0}th images, captions, lengths, ids--from data_loader---<span class=\"pl-pds\">'</span></span>.format(i))\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>print('images shape[0] {0}  shape[1] {1} shape[2] {2} shape[3] {3}'.format(images.shape[0], images.shape[1], images.shape[2], images.shape[3]))</span>\n                print(images)\n                print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>captions shape[0] {0}  shape[1] {1}<span class=\"pl-pds\">'</span></span>.format(captions.shape[0], captions.shape[1]))\n                print(captions)\n                print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>len(lengths) {0}<span class=\"pl-pds\">'</span></span>.format(len(lengths)))\n                print(lengths)\n                print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>len(ids) {0}<span class=\"pl-pds\">'</span></span>.format(len(ids)))\n                print(ids)\n                print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute the embeddings</span>\n        img_emb, cap_emb = model.forward_emb(images, captions, lengths,\n                                             volatile=True)\n        if(i <span class=\"pl-k\">&gt;</span> 38):\n                print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>     ---the {0}th img_emb, cap_emb = model.forward_emb(images, captions, lengths, volatile=True)---<span class=\"pl-pds\">'</span></span>.format(i))\n                print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>img_emb size(0) {0}  size(1) {1}<span class=\"pl-pds\">'</span></span>.format(img_emb.size(0), img_emb.size(1)))\n                print(img_emb)\n                print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cap_emb size(0) {0}  size(1) {1}<span class=\"pl-pds\">'</span></span>.format(cap_emb.size(0), cap_emb.size(1)))\n                print(cap_emb)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> initialize the numpy arrays given the size of the embeddings</span>\n        <span class=\"pl-k\">if</span> img_embs is None:\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>img_embs is None<span class=\"pl-pds\">'</span></span>)\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>len(data_loader.dataset)={0}<span class=\"pl-pds\">'</span></span>.format(len(data_loader.dataset)))\n            img_embs = np.zeros<span class=\"pl-s\"><span class=\"pl-pds\">((</span>len(data_loader.dataset)<span class=\"pl-k\">,</span> img_emb.size(<span class=\"pl-c1\">1</span><span class=\"pl-pds\">))</span></span>)\n            cap_embs = np.zeros<span class=\"pl-s\"><span class=\"pl-pds\">((</span>len(data_loader.dataset)<span class=\"pl-k\">,</span> cap_emb.size(<span class=\"pl-c1\">1</span><span class=\"pl-pds\">))</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> preserve the embeddings by copying from gpu and converting to numpy</span>\n        img_embs[ids] = <span class=\"pl-en\">img_emb.data.cpu().numpy().copy</span>()\n        cap_embs[ids] = <span class=\"pl-en\">cap_emb.data.cpu().numpy().copy</span>()\n        if(i==0 or i <span class=\"pl-k\">&gt;</span> 38):\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>          ---img_emb[0]---<span class=\"pl-pds\">'</span></span>)\n            print(img_emb[0])\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>          ---img_embs[0]---<span class=\"pl-pds\">'</span></span>)\n            print(img_embs[0])\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>          ---cap_emb[0]---<span class=\"pl-pds\">'</span></span>)\n            print(cap_emb[0])\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>          ---cap_embs[ids[0]]---<span class=\"pl-pds\">'</span></span>)\n            print(cap_embs[ids[0]])\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>          ---img_emb[5]---<span class=\"pl-pds\">'</span></span>)\n            print(img_emb[5])\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>          ---img_embs[ids[5]]---<span class=\"pl-pds\">'</span></span>)\n            print(img_embs[ids[5]])\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>          ---cap_emb[6]---<span class=\"pl-pds\">'</span></span>)\n            print(cap_emb[6])\n            print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>          ---cap_embs[ids[6]]---<span class=\"pl-pds\">'</span></span>)\n            print(cap_embs[ids[6]])\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> measure accuracy and record loss</span>\n        model.forward_loss(img_emb, cap_emb)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> measure elapsed time</span>\n        <span class=\"pl-en\">batch_time.update(time.time</span>() - end)\n        end <span class=\"pl-k\">=</span> <span class=\"pl-en\">time.time</span>()\n\n        <span class=\"pl-k\">if</span> i % log_step == 0:\n            logging(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Test: [{0}/{1}]\\t<span class=\"pl-pds\">'</span></span>\n                    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>{e_log}\\t<span class=\"pl-pds\">'</span></span>\n                    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t<span class=\"pl-pds\">'</span></span>\n                    .format(\n                        i, len(data_loader), batch_time=batch_time,\n                        e_log=str(model.logger)))\n        del images, captions\n\n    print (i)\n    <span class=\"pl-k\">return</span> img_embs, cap_embs</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>def main(model_path, data_path=None, split=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dev<span class=\"pl-pds\">'</span></span>, fold5=False):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-s\">    Evaluate a trained model on either dev or test. If <span class=\"pl-s\"><span class=\"pl-pds\">`</span>fold5=True<span class=\"pl-pds\">`</span></span>, 5 fold</span>\n<span class=\"pl-s\">    cross-validation is done (only for MSCOCO). Otherwise, the full data is</span>\n<span class=\"pl-s\">    used for evaluation.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> load model and options</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-en\">torch.cuda.is_available</span>():\n        checkpoint = torch.load(model_path)\n    else:\n        checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n    opt = checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>opt<span class=\"pl-pds\">'</span></span>]\n    <span class=\"pl-k\">if</span> data_path is not None:\n        opt.data_path = data_path\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> load vocabulary used by the model</span>\n    with open(os.path.join(opt.vocab_path,\n                           <span class=\"pl-s\"><span class=\"pl-pds\">'</span>%s_vocab.pkl<span class=\"pl-pds\">'</span></span> % opt.data_name), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>rb<span class=\"pl-pds\">'</span></span>) as f:\n        vocab = pickle.load(f)\n    opt.vocab_size = len(vocab)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> construct model</span>\n    model = VSE(opt)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> load model state</span>\n    model.load_state_dict(checkpoint[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>model<span class=\"pl-pds\">'</span></span>])\n\n    print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loading dataset<span class=\"pl-pds\">'</span></span>)\n    data_loader = get_test_loader(split, opt.data_name, vocab, opt.crop_size,\n                                  opt.batch_size, opt.workers, opt)\n\n    print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Computing results...<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>description= raw_input(\"Please describe the picture you want:\")</span>\n    img_embs, cap_embs = encode_data(model, data_loader)\n    print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>img_embs shape[0] {0}  shape[1] {1}<span class=\"pl-pds\">'</span></span>.format(img_embs.shape[0], img_embs.shape[1]))<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n    print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cap_embs shape[0] {0}  shape[1] {1}<span class=\"pl-pds\">'</span></span>.format(cap_embs.shape[0], cap_embs.shape[1]))<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n    print(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Images: %d, Captions: %d<span class=\"pl-pds\">'</span></span> %\n          (img_embs.shape[0] / 5, cap_embs.shape[0]))\n</pre></div>\n<p>1)[<strong>solved</strong>] I have entered export NO_CUDA=1 before running Python train.py. I also moved all \".cuda()\" or \"DataParallel()\" among the code. It still goes wrong.</p>\n<pre><code>=&gt; using pre-trained model 'resnet152'\nTraceback (most recent call last):\n  File \"train.py\", line 244, in &lt;module&gt;\n    main()\n  File \"train.py\", line 116, in main\n    train(opt, train_loader, model, epoch, val_loader)\n  File \"train.py\", line 143, in train\n    for i, train_data in enumerate(train_loader):\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 201, in __next__\n    return self._process_next_batch(batch)\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 221, in _process_next_batch\n    raise batch.exc_type(batch.exc_msg)\nAssertionError: Traceback (most recent call last):\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 62, in _pin_memory_loop\n    batch = pin_memory_batch(batch)\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 123, in pin_memory_batch\n    return [pin_memory_batch(sample) for sample in batch]\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 117, in pin_memory_batch\n    return batch.pin_memory()\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/tensor.py\", line 82, in pin_memory\n    return type(self)().set_(storage.pin_memory()).view_as(self)\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/storage.py\", line 83, in pin_memory\n    allocator = torch.cuda._host_allocator()\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 220, in _host_allocator\n    _lazy_init()\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 84, in _lazy_init\n    _check_driver()\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 58, in _check_driver\n    http://www.nvidia.com/Download/index.aspx\"\"\")\nAssertionError: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx\n</code></pre>\n<p>The functions associated with \"train_loader\" are as follows:</p>\n<div class=\"highlight highlight-source-shell\"><pre>train_loader = data.get_loaders(\n        opt.data_name, vocab, opt.crop_size, opt.batch_size, opt.workers, opt)</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>def get_loaders(data_name, vocab, crop_size, batch_size, workers, opt):\n    dpath = os.path.join(opt.data_path, data_name)\n    train_loader = get_precomp_loader(dpath, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>, vocab, opt,\n                                          batch_size, True, workers)\n    <span class=\"pl-k\">return</span> train_loader</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>def get_precomp_loader(data_path, data_split, vocab, opt, batch_size=100,\n                       shuffle=True, num_workers=2):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span>Returns torch.utils.data.DataLoader for custom coco dataset.<span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>\n    dset = PrecompDataset(data_path, data_split, vocab)\n\n    data_loader = torch.utils.data.DataLoader(dataset=dset,\n                                              batch_size=batch_size,\n                                              shuffle=shuffle,\n                                              pin_memory=True,\n                                              collate_fn=collate_fn)\n    <span class=\"pl-k\">return</span> data_loader</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>def collate_fn(data):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span>Build mini-batch tensors from a list of (image, caption) tuples.</span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        data: list of (image, caption) tuple.</span>\n<span class=\"pl-s\">            - image: torch tensor of shape (3, 256, 256).</span>\n<span class=\"pl-s\">            - caption: torch tensor of shape (?); variable length.</span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        images: torch tensor of shape (batch_size, 3, 256, 256).</span>\n<span class=\"pl-s\">        targets: torch tensor of shape (batch_size, padded_length).</span>\n<span class=\"pl-s\">        lengths: list; valid length for each padded caption.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Sort a data list by caption length</span>\n    data.sort(key=lambda x: len(x[1]), reverse=True)\n    images, captions, ids, img_ids = zip(<span class=\"pl-k\">*</span>data)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Merge images (convert tuple of 3D tensor to 4D tensor)</span>\n    images = torch.stack(images, 0)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Merget captions (convert tuple of 1D tensor to 2D tensor)</span>\n    lengths = [len(cap) <span class=\"pl-k\">for</span> <span class=\"pl-smi\">cap</span> <span class=\"pl-k\">in</span> captions]\n    targets = torch.zeros(len(captions), <span class=\"pl-en\">max(lengths)).long</span>()\n    <span class=\"pl-k\">for</span> <span class=\"pl-smi\">i, cap</span> <span class=\"pl-k\">in</span> enumerate(captions):\n        end = lengths[i]\n        targets[i, :end] = cap[:end]\n\n    <span class=\"pl-k\">return</span> images, targets, lengths, ids</pre></div>\n<div class=\"highlight highlight-source-shell\"><pre>class PrecompDataset(data.Dataset):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-s\">    Load precomputed captions and image features</span>\n<span class=\"pl-s\">    Possible options: f8k, f30k, coco, 10crop</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>\n\n    def __init__(self, data_path, data_split, vocab):\n        self.vocab = vocab\n        loc = data_path + <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Captions</span>\n        self.captions = []\n        with open(loc+<span class=\"pl-s\"><span class=\"pl-pds\">'</span>%s_caps.txt<span class=\"pl-pds\">'</span></span> % data_split, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>rb<span class=\"pl-pds\">'</span></span>) as f:\n            <span class=\"pl-k\">for</span> <span class=\"pl-smi\">line</span> <span class=\"pl-k\">in</span> f:\n                <span class=\"pl-en\">self.captions.append(line.decode('utf-8').strip</span>())\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Image features</span>\n        self.images = np.load(loc+<span class=\"pl-s\"><span class=\"pl-pds\">'</span>%s_ims.npy<span class=\"pl-pds\">'</span></span> % data_split)\n        self.length = len(self.captions)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> rkiros data has redundancy in images, we divide by 5, 10crop doesn't</span>\n        <span class=\"pl-k\">if</span> self.images.shape[0] <span class=\"pl-k\">!</span>= self.length:\n            self.im_div = 5\n        else:\n            self.im_div = 1\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> the development set for coco is large and so validation would be slow</span>\n        <span class=\"pl-k\">if</span> data_split == <span class=\"pl-s\"><span class=\"pl-pds\">'</span>dev<span class=\"pl-pds\">'</span></span>:\n            self.length = 5000\n\n    def __getitem__(self, index):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> handle the image redundancy</span>\n        img_id = index/self.im_div\n        image = torch.Tensor(self.images[img_id])\n        caption = self.captions[index]\n        vocab = self.vocab\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Convert caption (string) to word ids.</span>\n        <span class=\"pl-k\">if</span> check_contain_chinese(caption):\n            result = corenlp_parser.api_call(caption, {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>annotators<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tokenize,ssplit<span class=\"pl-pds\">'</span></span>})\n            tokens = [token[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>originalText<span class=\"pl-pds\">'</span></span>] or token[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>word<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">for</span> <span class=\"pl-smi\">sentence</span> <span class=\"pl-k\">in</span> result[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sentences<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">for</span> <span class=\"pl-smi\">token</span> <span class=\"pl-k\">in</span> sentence[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tokens<span class=\"pl-pds\">'</span></span>]]\n        else:\n            tokens = <span class=\"pl-en\">nltk.tokenize.word_tokenize(str(caption).lower</span>().decode(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>utf-8<span class=\"pl-pds\">'</span></span>))\n        caption = []\n        caption.append(vocab(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;start&gt;<span class=\"pl-pds\">'</span></span>))\n        caption.extend([vocab(token) <span class=\"pl-k\">for</span> <span class=\"pl-smi\">token</span> <span class=\"pl-k\">in</span> tokens])\n        caption.append(vocab(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>&lt;end&gt;<span class=\"pl-pds\">'</span></span>))\n        target = torch.Tensor(caption)\n        <span class=\"pl-k\">return</span> image, target, index, img_id\n\n    def __len__(self):\n        <span class=\"pl-k\">return</span> self.length</pre></div>", "body_text": "I am trying to modify a model code, which works well with one or multiple GPUs, so as to run with CPU-only devices. I have encountered two problems:(The one marked [solved] has been solved thanks to @chenzhekl 's help )\n2)[unsolved] when I run the model on CPU, it has something wrong with .data.cpu().numpy().copy()\nComputing results...\n     ---the 0th images, captions, lengths, ids--from data_loader---\n\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n          ...             \u22f1             ...          \n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\n[torch.FloatTensor of size 13x4096]\n\ncaptions shape[0] 13  shape[1] 18\n\n\nColumns 0 to 12 \n    1  1240  1857  2704  1240  2813  1701  2453  2191  2704  2109  1742  1240\n    1  1240  1694  1742  1240   788  1196   895  1292     3     3  3015  1060\n    1   933   889  2920  1742   933  2516  1742  1448  2783  1240  1211   130\n    1  1240  1857  2704  1240  2813   403  2427   657   397  2600   985   130\n    1  1240   991  2704  2696  1694   788  1302  1366  1742   933  1196     2\n    1   442  1694  1738   538  2071   933  2640  1742  1240  1196   130     2\n    1   933   442   991  2704  2696  1694  1738  1742   933  1196   130     2\n    1  1240  1694   895  1742  1240  1196  1734  1240   583   130     2     0\n    1  1240  1857  2704  1240  2813  1742   823  2191  2109   130     2     0\n    1  2050   991   889    64   966  1742   933  2516   130     2     0     0\n    1  2050   991   889  3001  1742   933  2516   130     2     0     0     0\n    1   933   889   519   657   933  2516   130     2     0     0     0     0\n    1  2050   889  2069  1742   933  2516   130     2     0     0     0     0\n\nColumns 13 to 17 \n  171  2783  2259   130     2\n  544   130     2     0     0\n    2     0     0     0     0\n    2     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n    0     0     0     0     0\n[torch.LongTensor of size 13x18]\n\nlen(lengths) 13\n[18, 16, 14, 14, 13, 13, 13, 12, 12, 11, 10, 9, 9]\nlen(ids) 13\n(12, 6, 0, 11, 5, 8, 9, 7, 10, 2, 3, 1, 4)\n\nimg_embs is None\nlen(data_loader.dataset)=13\nTraceback (most recent call last):\n  File \"test.py\", line 373, in <module>\n    main(opt.resume,  opt.data_path, opt.split)\n  File \"test.py\", line 198, in main\n    img_embs, cap_embs = encode_data(model, data_loader)\n  File \"test.py\", line 126, in encode_data\n    img_embs[ids] = img_emb.data.cpu().numpy().copy()\nIndexError: too many indices for array\n\nThe only output difference  between on GPU and on CPU is that on CPU ids is a tuple ( )  but on GPU it is a list [ ].\nHere is the output on GPU starting from the difference: (I just loaded 13 captions to reduce the run time)\nlen(ids) 13\n[12, 6, 0, 11, 5, 8, 9, 7, 10, 2, 3, 1, 4]\n\nimg_embs is None\nlen(data_loader.dataset)=13\n          ---img_emb[0]---\nVariable containing:\n 1.8998e-02\n 6.6110e-03\n 8.6337e-02\n     \u22ee     \n 1.9800e-02\n 2.4219e-02\n 1.4796e-02\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\n\n          ---img_embs[0]---\n[ 0.01899767  0.00661102  0.08633662 ...,  0.0198002   0.02421924\n  0.01479562]\n          ---cap_emb[0]---\nVariable containing:\n 5.6984e-03\n-4.3049e-02\n 3.3034e-02\n     \u22ee     \n 3.2743e-02\n-5.2397e-02\n 2.9017e-03\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\n\n          ---cap_embs[ids[0]]---\n[ 0.00569838 -0.04304908  0.03303399 ...,  0.03274297 -0.05239733\n  0.00290167]\n          ---img_emb[5]---\nVariable containing:\n 1.8998e-02\n 6.6110e-03\n 8.6337e-02\n     \u22ee     \n 1.9800e-02\n 2.4219e-02\n 1.4796e-02\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\n\n          ---img_embs[ids[5]]---\n[ 0.01899767  0.00661102  0.08633662 ...,  0.0198002   0.02421924\n  0.01479562]\n          ---cap_emb[6]---\nVariable containing:\n 0.0277\n 0.0205\n 0.0128\n   \u22ee   \n 0.0187\n-0.0072\n-0.0342\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\n\n          ---cap_embs[ids[6]]---\n[ 0.02767145  0.02054488  0.01282509 ...,  0.0187309  -0.00720693\n -0.03416499]\nTest: [0/1]\tLe 8.2995 (8.2995)\tTime 0.239 (0.000)\t\n0\nimg_embs shape[0] 13  shape[1] 1024\ncap_embs shape[0] 13  shape[1] 1024\nImages: 2, Captions: 13\n\nThis is the code associated with \".data.cpu().numpy().copy()\":\ndef encode_data(model, data_loader, log_step=10, logging=print):\n    \"\"\"Encode all images and captions loadable by `data_loader`\n    \"\"\"\n    batch_time = AverageMeter()\n    val_logger = LogCollector()\n\n    # switch to evaluate mode\n    model.val_start()\n\n    end = time.time()\n\n    # numpy array to keep all the embeddings\n    img_embs = None\n    cap_embs = None\n    for i, (images, captions, lengths, ids) in enumerate(data_loader):\n        # make sure val logger is used\n        model.logger = val_logger\n\n        if(i==0 or i > 38):\n                print('     ---the {0}th images, captions, lengths, ids--from data_loader---'.format(i))\n                #print('images shape[0] {0}  shape[1] {1} shape[2] {2} shape[3] {3}'.format(images.shape[0], images.shape[1], images.shape[2], images.shape[3]))\n                print(images)\n                print('captions shape[0] {0}  shape[1] {1}'.format(captions.shape[0], captions.shape[1]))\n                print(captions)\n                print('len(lengths) {0}'.format(len(lengths)))\n                print(lengths)\n                print('len(ids) {0}'.format(len(ids)))\n                print(ids)\n                print('')\n        # compute the embeddings\n        img_emb, cap_emb = model.forward_emb(images, captions, lengths,\n                                             volatile=True)\n        if(i > 38):\n                print('     ---the {0}th img_emb, cap_emb = model.forward_emb(images, captions, lengths, volatile=True)---'.format(i))\n                print('img_emb size(0) {0}  size(1) {1}'.format(img_emb.size(0), img_emb.size(1)))\n                print(img_emb)\n                print('cap_emb size(0) {0}  size(1) {1}'.format(cap_emb.size(0), cap_emb.size(1)))\n                print(cap_emb)\n\n        # initialize the numpy arrays given the size of the embeddings\n        if img_embs is None:\n            print('img_embs is None')\n            print('len(data_loader.dataset)={0}'.format(len(data_loader.dataset)))\n            img_embs = np.zeros((len(data_loader.dataset), img_emb.size(1)))\n            cap_embs = np.zeros((len(data_loader.dataset), cap_emb.size(1)))\n\n        # preserve the embeddings by copying from gpu and converting to numpy\n        img_embs[ids] = img_emb.data.cpu().numpy().copy()\n        cap_embs[ids] = cap_emb.data.cpu().numpy().copy()\n        if(i==0 or i > 38):\n            print('          ---img_emb[0]---')\n            print(img_emb[0])\n            print('          ---img_embs[0]---')\n            print(img_embs[0])\n            print('          ---cap_emb[0]---')\n            print(cap_emb[0])\n            print('          ---cap_embs[ids[0]]---')\n            print(cap_embs[ids[0]])\n            print('          ---img_emb[5]---')\n            print(img_emb[5])\n            print('          ---img_embs[ids[5]]---')\n            print(img_embs[ids[5]])\n            print('          ---cap_emb[6]---')\n            print(cap_emb[6])\n            print('          ---cap_embs[ids[6]]---')\n            print(cap_embs[ids[6]])\n        # measure accuracy and record loss\n        model.forward_loss(img_emb, cap_emb)\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % log_step == 0:\n            logging('Test: [{0}/{1}]\\t'\n                    '{e_log}\\t'\n                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                    .format(\n                        i, len(data_loader), batch_time=batch_time,\n                        e_log=str(model.logger)))\n        del images, captions\n\n    print (i)\n    return img_embs, cap_embs\ndef main(model_path, data_path=None, split='dev', fold5=False):\n    \"\"\"\n    Evaluate a trained model on either dev or test. If `fold5=True`, 5 fold\n    cross-validation is done (only for MSCOCO). Otherwise, the full data is\n    used for evaluation.\n    \"\"\"\n    # load model and options\n    if torch.cuda.is_available():\n        checkpoint = torch.load(model_path)\n    else:\n        checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n    opt = checkpoint['opt']\n    if data_path is not None:\n        opt.data_path = data_path\n\n    # load vocabulary used by the model\n    with open(os.path.join(opt.vocab_path,\n                           '%s_vocab.pkl' % opt.data_name), 'rb') as f:\n        vocab = pickle.load(f)\n    opt.vocab_size = len(vocab)\n\n    # construct model\n    model = VSE(opt)\n\n    # load model state\n    model.load_state_dict(checkpoint['model'])\n\n    print('Loading dataset')\n    data_loader = get_test_loader(split, opt.data_name, vocab, opt.crop_size,\n                                  opt.batch_size, opt.workers, opt)\n\n    print('Computing results...')\n    #description= raw_input(\"Please describe the picture you want:\")\n    img_embs, cap_embs = encode_data(model, data_loader)\n    print('img_embs shape[0] {0}  shape[1] {1}'.format(img_embs.shape[0], img_embs.shape[1]))#\n    print('cap_embs shape[0] {0}  shape[1] {1}'.format(cap_embs.shape[0], cap_embs.shape[1]))#\n    print('Images: %d, Captions: %d' %\n          (img_embs.shape[0] / 5, cap_embs.shape[0]))\n\n1)[solved] I have entered export NO_CUDA=1 before running Python train.py. I also moved all \".cuda()\" or \"DataParallel()\" among the code. It still goes wrong.\n=> using pre-trained model 'resnet152'\nTraceback (most recent call last):\n  File \"train.py\", line 244, in <module>\n    main()\n  File \"train.py\", line 116, in main\n    train(opt, train_loader, model, epoch, val_loader)\n  File \"train.py\", line 143, in train\n    for i, train_data in enumerate(train_loader):\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 201, in __next__\n    return self._process_next_batch(batch)\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 221, in _process_next_batch\n    raise batch.exc_type(batch.exc_msg)\nAssertionError: Traceback (most recent call last):\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 62, in _pin_memory_loop\n    batch = pin_memory_batch(batch)\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 123, in pin_memory_batch\n    return [pin_memory_batch(sample) for sample in batch]\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 117, in pin_memory_batch\n    return batch.pin_memory()\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/tensor.py\", line 82, in pin_memory\n    return type(self)().set_(storage.pin_memory()).view_as(self)\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/storage.py\", line 83, in pin_memory\n    allocator = torch.cuda._host_allocator()\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 220, in _host_allocator\n    _lazy_init()\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 84, in _lazy_init\n    _check_driver()\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 58, in _check_driver\n    http://www.nvidia.com/Download/index.aspx\"\"\")\nAssertionError: \nFound no NVIDIA driver on your system. Please check that you\nhave an NVIDIA GPU and installed a driver from\nhttp://www.nvidia.com/Download/index.aspx\n\nThe functions associated with \"train_loader\" are as follows:\ntrain_loader = data.get_loaders(\n        opt.data_name, vocab, opt.crop_size, opt.batch_size, opt.workers, opt)\ndef get_loaders(data_name, vocab, crop_size, batch_size, workers, opt):\n    dpath = os.path.join(opt.data_path, data_name)\n    train_loader = get_precomp_loader(dpath, 'train', vocab, opt,\n                                          batch_size, True, workers)\n    return train_loader\ndef get_precomp_loader(data_path, data_split, vocab, opt, batch_size=100,\n                       shuffle=True, num_workers=2):\n    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n    dset = PrecompDataset(data_path, data_split, vocab)\n\n    data_loader = torch.utils.data.DataLoader(dataset=dset,\n                                              batch_size=batch_size,\n                                              shuffle=shuffle,\n                                              pin_memory=True,\n                                              collate_fn=collate_fn)\n    return data_loader\ndef collate_fn(data):\n    \"\"\"Build mini-batch tensors from a list of (image, caption) tuples.\n    Args:\n        data: list of (image, caption) tuple.\n            - image: torch tensor of shape (3, 256, 256).\n            - caption: torch tensor of shape (?); variable length.\n    Returns:\n        images: torch tensor of shape (batch_size, 3, 256, 256).\n        targets: torch tensor of shape (batch_size, padded_length).\n        lengths: list; valid length for each padded caption.\n    \"\"\"\n    # Sort a data list by caption length\n    data.sort(key=lambda x: len(x[1]), reverse=True)\n    images, captions, ids, img_ids = zip(*data)\n\n    # Merge images (convert tuple of 3D tensor to 4D tensor)\n    images = torch.stack(images, 0)\n\n    # Merget captions (convert tuple of 1D tensor to 2D tensor)\n    lengths = [len(cap) for cap in captions]\n    targets = torch.zeros(len(captions), max(lengths)).long()\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        targets[i, :end] = cap[:end]\n\n    return images, targets, lengths, ids\nclass PrecompDataset(data.Dataset):\n    \"\"\"\n    Load precomputed captions and image features\n    Possible options: f8k, f30k, coco, 10crop\n    \"\"\"\n\n    def __init__(self, data_path, data_split, vocab):\n        self.vocab = vocab\n        loc = data_path + '/'\n\n        # Captions\n        self.captions = []\n        with open(loc+'%s_caps.txt' % data_split, 'rb') as f:\n            for line in f:\n                self.captions.append(line.decode('utf-8').strip())\n\n        # Image features\n        self.images = np.load(loc+'%s_ims.npy' % data_split)\n        self.length = len(self.captions)\n        # rkiros data has redundancy in images, we divide by 5, 10crop doesn't\n        if self.images.shape[0] != self.length:\n            self.im_div = 5\n        else:\n            self.im_div = 1\n        # the development set for coco is large and so validation would be slow\n        if data_split == 'dev':\n            self.length = 5000\n\n    def __getitem__(self, index):\n        # handle the image redundancy\n        img_id = index/self.im_div\n        image = torch.Tensor(self.images[img_id])\n        caption = self.captions[index]\n        vocab = self.vocab\n\n        # Convert caption (string) to word ids.\n        if check_contain_chinese(caption):\n            result = corenlp_parser.api_call(caption, {'annotators': 'tokenize,ssplit'})\n            tokens = [token['originalText'] or token['word'] for sentence in result['sentences'] for token in sentence['tokens']]\n        else:\n            tokens = nltk.tokenize.word_tokenize(str(caption).lower().decode('utf-8'))\n        caption = []\n        caption.append(vocab('<start>'))\n        caption.extend([vocab(token) for token in tokens])\n        caption.append(vocab('<end>'))\n        target = torch.Tensor(caption)\n        return image, target, index, img_id\n\n    def __len__(self):\n        return self.length", "body": "I am trying to modify a model code, which works well with one or multiple GPUs, so as to run with CPU-only devices. I have encountered two problems:(The one marked [**solved**] has been solved thanks to @chenzhekl 's help )\r\n2)[**unsolved**] when I run the model on CPU, it has something wrong with .data.cpu().numpy().copy()\r\n```\r\nComputing results...\r\n     ---the 0th images, captions, lengths, ids--from data_loader---\r\n\r\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\r\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\r\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\r\n          ...             \u22f1             ...          \r\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\r\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\r\n 0.0000  0.0000  0.0000  ...   0.0000  0.0570  0.0000\r\n[torch.FloatTensor of size 13x4096]\r\n\r\ncaptions shape[0] 13  shape[1] 18\r\n\r\n\r\nColumns 0 to 12 \r\n    1  1240  1857  2704  1240  2813  1701  2453  2191  2704  2109  1742  1240\r\n    1  1240  1694  1742  1240   788  1196   895  1292     3     3  3015  1060\r\n    1   933   889  2920  1742   933  2516  1742  1448  2783  1240  1211   130\r\n    1  1240  1857  2704  1240  2813   403  2427   657   397  2600   985   130\r\n    1  1240   991  2704  2696  1694   788  1302  1366  1742   933  1196     2\r\n    1   442  1694  1738   538  2071   933  2640  1742  1240  1196   130     2\r\n    1   933   442   991  2704  2696  1694  1738  1742   933  1196   130     2\r\n    1  1240  1694   895  1742  1240  1196  1734  1240   583   130     2     0\r\n    1  1240  1857  2704  1240  2813  1742   823  2191  2109   130     2     0\r\n    1  2050   991   889    64   966  1742   933  2516   130     2     0     0\r\n    1  2050   991   889  3001  1742   933  2516   130     2     0     0     0\r\n    1   933   889   519   657   933  2516   130     2     0     0     0     0\r\n    1  2050   889  2069  1742   933  2516   130     2     0     0     0     0\r\n\r\nColumns 13 to 17 \r\n  171  2783  2259   130     2\r\n  544   130     2     0     0\r\n    2     0     0     0     0\r\n    2     0     0     0     0\r\n    0     0     0     0     0\r\n    0     0     0     0     0\r\n    0     0     0     0     0\r\n    0     0     0     0     0\r\n    0     0     0     0     0\r\n    0     0     0     0     0\r\n    0     0     0     0     0\r\n    0     0     0     0     0\r\n    0     0     0     0     0\r\n[torch.LongTensor of size 13x18]\r\n\r\nlen(lengths) 13\r\n[18, 16, 14, 14, 13, 13, 13, 12, 12, 11, 10, 9, 9]\r\nlen(ids) 13\r\n(12, 6, 0, 11, 5, 8, 9, 7, 10, 2, 3, 1, 4)\r\n\r\nimg_embs is None\r\nlen(data_loader.dataset)=13\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 373, in <module>\r\n    main(opt.resume,  opt.data_path, opt.split)\r\n  File \"test.py\", line 198, in main\r\n    img_embs, cap_embs = encode_data(model, data_loader)\r\n  File \"test.py\", line 126, in encode_data\r\n    img_embs[ids] = img_emb.data.cpu().numpy().copy()\r\nIndexError: too many indices for array\r\n```\r\nThe only output difference  between on GPU and on CPU is that on CPU ids is a tuple ( )  but on GPU it is a list [ ].\r\nHere is the output on GPU starting from the difference: (I just loaded 13 captions to reduce the run time)\r\n```\r\nlen(ids) 13\r\n[12, 6, 0, 11, 5, 8, 9, 7, 10, 2, 3, 1, 4]\r\n\r\nimg_embs is None\r\nlen(data_loader.dataset)=13\r\n          ---img_emb[0]---\r\nVariable containing:\r\n 1.8998e-02\r\n 6.6110e-03\r\n 8.6337e-02\r\n     \u22ee     \r\n 1.9800e-02\r\n 2.4219e-02\r\n 1.4796e-02\r\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\r\n\r\n          ---img_embs[0]---\r\n[ 0.01899767  0.00661102  0.08633662 ...,  0.0198002   0.02421924\r\n  0.01479562]\r\n          ---cap_emb[0]---\r\nVariable containing:\r\n 5.6984e-03\r\n-4.3049e-02\r\n 3.3034e-02\r\n     \u22ee     \r\n 3.2743e-02\r\n-5.2397e-02\r\n 2.9017e-03\r\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\r\n\r\n          ---cap_embs[ids[0]]---\r\n[ 0.00569838 -0.04304908  0.03303399 ...,  0.03274297 -0.05239733\r\n  0.00290167]\r\n          ---img_emb[5]---\r\nVariable containing:\r\n 1.8998e-02\r\n 6.6110e-03\r\n 8.6337e-02\r\n     \u22ee     \r\n 1.9800e-02\r\n 2.4219e-02\r\n 1.4796e-02\r\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\r\n\r\n          ---img_embs[ids[5]]---\r\n[ 0.01899767  0.00661102  0.08633662 ...,  0.0198002   0.02421924\r\n  0.01479562]\r\n          ---cap_emb[6]---\r\nVariable containing:\r\n 0.0277\r\n 0.0205\r\n 0.0128\r\n   \u22ee   \r\n 0.0187\r\n-0.0072\r\n-0.0342\r\n[torch.cuda.FloatTensor of size 1024 (GPU 0)]\r\n\r\n          ---cap_embs[ids[6]]---\r\n[ 0.02767145  0.02054488  0.01282509 ...,  0.0187309  -0.00720693\r\n -0.03416499]\r\nTest: [0/1]\tLe 8.2995 (8.2995)\tTime 0.239 (0.000)\t\r\n0\r\nimg_embs shape[0] 13  shape[1] 1024\r\ncap_embs shape[0] 13  shape[1] 1024\r\nImages: 2, Captions: 13\r\n```\r\nThis is the code associated with \".data.cpu().numpy().copy()\":\r\n```bash\r\ndef encode_data(model, data_loader, log_step=10, logging=print):\r\n    \"\"\"Encode all images and captions loadable by `data_loader`\r\n    \"\"\"\r\n    batch_time = AverageMeter()\r\n    val_logger = LogCollector()\r\n\r\n    # switch to evaluate mode\r\n    model.val_start()\r\n\r\n    end = time.time()\r\n\r\n    # numpy array to keep all the embeddings\r\n    img_embs = None\r\n    cap_embs = None\r\n    for i, (images, captions, lengths, ids) in enumerate(data_loader):\r\n        # make sure val logger is used\r\n        model.logger = val_logger\r\n\r\n        if(i==0 or i > 38):\r\n                print('     ---the {0}th images, captions, lengths, ids--from data_loader---'.format(i))\r\n                #print('images shape[0] {0}  shape[1] {1} shape[2] {2} shape[3] {3}'.format(images.shape[0], images.shape[1], images.shape[2], images.shape[3]))\r\n                print(images)\r\n                print('captions shape[0] {0}  shape[1] {1}'.format(captions.shape[0], captions.shape[1]))\r\n                print(captions)\r\n                print('len(lengths) {0}'.format(len(lengths)))\r\n                print(lengths)\r\n                print('len(ids) {0}'.format(len(ids)))\r\n                print(ids)\r\n                print('')\r\n        # compute the embeddings\r\n        img_emb, cap_emb = model.forward_emb(images, captions, lengths,\r\n                                             volatile=True)\r\n        if(i > 38):\r\n                print('     ---the {0}th img_emb, cap_emb = model.forward_emb(images, captions, lengths, volatile=True)---'.format(i))\r\n                print('img_emb size(0) {0}  size(1) {1}'.format(img_emb.size(0), img_emb.size(1)))\r\n                print(img_emb)\r\n                print('cap_emb size(0) {0}  size(1) {1}'.format(cap_emb.size(0), cap_emb.size(1)))\r\n                print(cap_emb)\r\n\r\n        # initialize the numpy arrays given the size of the embeddings\r\n        if img_embs is None:\r\n            print('img_embs is None')\r\n            print('len(data_loader.dataset)={0}'.format(len(data_loader.dataset)))\r\n            img_embs = np.zeros((len(data_loader.dataset), img_emb.size(1)))\r\n            cap_embs = np.zeros((len(data_loader.dataset), cap_emb.size(1)))\r\n\r\n        # preserve the embeddings by copying from gpu and converting to numpy\r\n        img_embs[ids] = img_emb.data.cpu().numpy().copy()\r\n        cap_embs[ids] = cap_emb.data.cpu().numpy().copy()\r\n        if(i==0 or i > 38):\r\n            print('          ---img_emb[0]---')\r\n            print(img_emb[0])\r\n            print('          ---img_embs[0]---')\r\n            print(img_embs[0])\r\n            print('          ---cap_emb[0]---')\r\n            print(cap_emb[0])\r\n            print('          ---cap_embs[ids[0]]---')\r\n            print(cap_embs[ids[0]])\r\n            print('          ---img_emb[5]---')\r\n            print(img_emb[5])\r\n            print('          ---img_embs[ids[5]]---')\r\n            print(img_embs[ids[5]])\r\n            print('          ---cap_emb[6]---')\r\n            print(cap_emb[6])\r\n            print('          ---cap_embs[ids[6]]---')\r\n            print(cap_embs[ids[6]])\r\n        # measure accuracy and record loss\r\n        model.forward_loss(img_emb, cap_emb)\r\n\r\n        # measure elapsed time\r\n        batch_time.update(time.time() - end)\r\n        end = time.time()\r\n\r\n        if i % log_step == 0:\r\n            logging('Test: [{0}/{1}]\\t'\r\n                    '{e_log}\\t'\r\n                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\r\n                    .format(\r\n                        i, len(data_loader), batch_time=batch_time,\r\n                        e_log=str(model.logger)))\r\n        del images, captions\r\n\r\n    print (i)\r\n    return img_embs, cap_embs\r\n```\r\n```bash\r\ndef main(model_path, data_path=None, split='dev', fold5=False):\r\n    \"\"\"\r\n    Evaluate a trained model on either dev or test. If `fold5=True`, 5 fold\r\n    cross-validation is done (only for MSCOCO). Otherwise, the full data is\r\n    used for evaluation.\r\n    \"\"\"\r\n    # load model and options\r\n    if torch.cuda.is_available():\r\n        checkpoint = torch.load(model_path)\r\n    else:\r\n        checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\r\n    opt = checkpoint['opt']\r\n    if data_path is not None:\r\n        opt.data_path = data_path\r\n\r\n    # load vocabulary used by the model\r\n    with open(os.path.join(opt.vocab_path,\r\n                           '%s_vocab.pkl' % opt.data_name), 'rb') as f:\r\n        vocab = pickle.load(f)\r\n    opt.vocab_size = len(vocab)\r\n\r\n    # construct model\r\n    model = VSE(opt)\r\n\r\n    # load model state\r\n    model.load_state_dict(checkpoint['model'])\r\n\r\n    print('Loading dataset')\r\n    data_loader = get_test_loader(split, opt.data_name, vocab, opt.crop_size,\r\n                                  opt.batch_size, opt.workers, opt)\r\n\r\n    print('Computing results...')\r\n    #description= raw_input(\"Please describe the picture you want:\")\r\n    img_embs, cap_embs = encode_data(model, data_loader)\r\n    print('img_embs shape[0] {0}  shape[1] {1}'.format(img_embs.shape[0], img_embs.shape[1]))#\r\n    print('cap_embs shape[0] {0}  shape[1] {1}'.format(cap_embs.shape[0], cap_embs.shape[1]))#\r\n    print('Images: %d, Captions: %d' %\r\n          (img_embs.shape[0] / 5, cap_embs.shape[0]))\r\n\r\n```\r\n\r\n1)[**solved**] I have entered export NO_CUDA=1 before running Python train.py. I also moved all \".cuda()\" or \"DataParallel()\" among the code. It still goes wrong. \r\n```\r\n=> using pre-trained model 'resnet152'\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 244, in <module>\r\n    main()\r\n  File \"train.py\", line 116, in main\r\n    train(opt, train_loader, model, epoch, val_loader)\r\n  File \"train.py\", line 143, in train\r\n    for i, train_data in enumerate(train_loader):\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 201, in __next__\r\n    return self._process_next_batch(batch)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 221, in _process_next_batch\r\n    raise batch.exc_type(batch.exc_msg)\r\nAssertionError: Traceback (most recent call last):\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 62, in _pin_memory_loop\r\n    batch = pin_memory_batch(batch)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 123, in pin_memory_batch\r\n    return [pin_memory_batch(sample) for sample in batch]\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 117, in pin_memory_batch\r\n    return batch.pin_memory()\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/tensor.py\", line 82, in pin_memory\r\n    return type(self)().set_(storage.pin_memory()).view_as(self)\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/storage.py\", line 83, in pin_memory\r\n    allocator = torch.cuda._host_allocator()\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 220, in _host_allocator\r\n    _lazy_init()\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 84, in _lazy_init\r\n    _check_driver()\r\n  File \"/root/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py\", line 58, in _check_driver\r\n    http://www.nvidia.com/Download/index.aspx\"\"\")\r\nAssertionError: \r\nFound no NVIDIA driver on your system. Please check that you\r\nhave an NVIDIA GPU and installed a driver from\r\nhttp://www.nvidia.com/Download/index.aspx\r\n```\r\nThe functions associated with \"train_loader\" are as follows:\r\n```bash\r\ntrain_loader = data.get_loaders(\r\n        opt.data_name, vocab, opt.crop_size, opt.batch_size, opt.workers, opt)\r\n```\r\n```bash\r\ndef get_loaders(data_name, vocab, crop_size, batch_size, workers, opt):\r\n    dpath = os.path.join(opt.data_path, data_name)\r\n    train_loader = get_precomp_loader(dpath, 'train', vocab, opt,\r\n                                          batch_size, True, workers)\r\n    return train_loader\r\n```\r\n```bash\r\ndef get_precomp_loader(data_path, data_split, vocab, opt, batch_size=100,\r\n                       shuffle=True, num_workers=2):\r\n    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\r\n    dset = PrecompDataset(data_path, data_split, vocab)\r\n\r\n    data_loader = torch.utils.data.DataLoader(dataset=dset,\r\n                                              batch_size=batch_size,\r\n                                              shuffle=shuffle,\r\n                                              pin_memory=True,\r\n                                              collate_fn=collate_fn)\r\n    return data_loader\r\n```\r\n```bash\r\ndef collate_fn(data):\r\n    \"\"\"Build mini-batch tensors from a list of (image, caption) tuples.\r\n    Args:\r\n        data: list of (image, caption) tuple.\r\n            - image: torch tensor of shape (3, 256, 256).\r\n            - caption: torch tensor of shape (?); variable length.\r\n    Returns:\r\n        images: torch tensor of shape (batch_size, 3, 256, 256).\r\n        targets: torch tensor of shape (batch_size, padded_length).\r\n        lengths: list; valid length for each padded caption.\r\n    \"\"\"\r\n    # Sort a data list by caption length\r\n    data.sort(key=lambda x: len(x[1]), reverse=True)\r\n    images, captions, ids, img_ids = zip(*data)\r\n\r\n    # Merge images (convert tuple of 3D tensor to 4D tensor)\r\n    images = torch.stack(images, 0)\r\n\r\n    # Merget captions (convert tuple of 1D tensor to 2D tensor)\r\n    lengths = [len(cap) for cap in captions]\r\n    targets = torch.zeros(len(captions), max(lengths)).long()\r\n    for i, cap in enumerate(captions):\r\n        end = lengths[i]\r\n        targets[i, :end] = cap[:end]\r\n\r\n    return images, targets, lengths, ids\r\n```\r\n```bash\r\nclass PrecompDataset(data.Dataset):\r\n    \"\"\"\r\n    Load precomputed captions and image features\r\n    Possible options: f8k, f30k, coco, 10crop\r\n    \"\"\"\r\n\r\n    def __init__(self, data_path, data_split, vocab):\r\n        self.vocab = vocab\r\n        loc = data_path + '/'\r\n\r\n        # Captions\r\n        self.captions = []\r\n        with open(loc+'%s_caps.txt' % data_split, 'rb') as f:\r\n            for line in f:\r\n                self.captions.append(line.decode('utf-8').strip())\r\n\r\n        # Image features\r\n        self.images = np.load(loc+'%s_ims.npy' % data_split)\r\n        self.length = len(self.captions)\r\n        # rkiros data has redundancy in images, we divide by 5, 10crop doesn't\r\n        if self.images.shape[0] != self.length:\r\n            self.im_div = 5\r\n        else:\r\n            self.im_div = 1\r\n        # the development set for coco is large and so validation would be slow\r\n        if data_split == 'dev':\r\n            self.length = 5000\r\n\r\n    def __getitem__(self, index):\r\n        # handle the image redundancy\r\n        img_id = index/self.im_div\r\n        image = torch.Tensor(self.images[img_id])\r\n        caption = self.captions[index]\r\n        vocab = self.vocab\r\n\r\n        # Convert caption (string) to word ids.\r\n        if check_contain_chinese(caption):\r\n            result = corenlp_parser.api_call(caption, {'annotators': 'tokenize,ssplit'})\r\n            tokens = [token['originalText'] or token['word'] for sentence in result['sentences'] for token in sentence['tokens']]\r\n        else:\r\n            tokens = nltk.tokenize.word_tokenize(str(caption).lower().decode('utf-8'))\r\n        caption = []\r\n        caption.append(vocab('<start>'))\r\n        caption.extend([vocab(token) for token in tokens])\r\n        caption.append(vocab('<end>'))\r\n        target = torch.Tensor(caption)\r\n        return image, target, index, img_id\r\n\r\n    def __len__(self):\r\n        return self.length\r\n```"}