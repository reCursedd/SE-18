{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8937", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8937/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8937/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8937/events", "html_url": "https://github.com/pytorch/pytorch/issues/8937", "id": 336231688, "node_id": "MDU6SXNzdWUzMzYyMzE2ODg=", "number": 8937, "title": "[caffe2] Is there any method to implement learning rate scheduler?", "user": {"login": "BIGBALLON", "id": 7837172, "node_id": "MDQ6VXNlcjc4MzcxNzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/7837172?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BIGBALLON", "html_url": "https://github.com/BIGBALLON", "followers_url": "https://api.github.com/users/BIGBALLON/followers", "following_url": "https://api.github.com/users/BIGBALLON/following{/other_user}", "gists_url": "https://api.github.com/users/BIGBALLON/gists{/gist_id}", "starred_url": "https://api.github.com/users/BIGBALLON/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BIGBALLON/subscriptions", "organizations_url": "https://api.github.com/users/BIGBALLON/orgs", "repos_url": "https://api.github.com/users/BIGBALLON/repos", "events_url": "https://api.github.com/users/BIGBALLON/events{/privacy}", "received_events_url": "https://api.github.com/users/BIGBALLON/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-27T13:31:51Z", "updated_at": "2018-07-02T18:11:21Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hello, everyone.</p>\n<p>since tensorflow or pytorch can adjust learning rate easily by using a scheduler function.</p>\n<div class=\"highlight highlight-source-python\"><pre>epochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">200</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">step_decay_scheduler</span>(<span class=\"pl-smi\">epoch</span>):\n    <span class=\"pl-k\">if</span> epoch <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">81</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">0.1</span>\n    <span class=\"pl-k\">if</span> epoch <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">122</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">0.01</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">0.001</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">step_decay_scheduler2</span>(<span class=\"pl-smi\">epoch</span>):\n    <span class=\"pl-k\">if</span> epoch <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">100</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">0.1</span>\n    <span class=\"pl-k\">if</span> epoch <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">150</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">0.01</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">0.001</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">cos_scheduler</span>(<span class=\"pl-smi\">epoch</span>):\n    <span class=\"pl-k\">return</span> (start_lr<span class=\"pl-k\">+</span>end_lr)<span class=\"pl-k\">/</span><span class=\"pl-c1\">2</span>.<span class=\"pl-k\">+</span>(start_lr<span class=\"pl-k\">-</span>end_lr)<span class=\"pl-k\">/</span><span class=\"pl-c1\">2</span>.<span class=\"pl-k\">*</span>math.cos(math.pi<span class=\"pl-k\">/</span><span class=\"pl-c1\">2.0</span><span class=\"pl-k\">*</span>(epoch<span class=\"pl-k\">/</span>(epochs<span class=\"pl-k\">/</span><span class=\"pl-c1\">2.0</span>)))</pre></div>\n<p>Is there anyway to use a different learning rate scheduler(e.g. cos scheduler)???</p>", "body_text": "Hello, everyone.\nsince tensorflow or pytorch can adjust learning rate easily by using a scheduler function.\nepochs = 200\ndef step_decay_scheduler(epoch):\n    if epoch < 81:\n        return 0.1\n    if epoch < 122:\n        return 0.01\n    return 0.001\n\ndef step_decay_scheduler2(epoch):\n    if epoch < 100:\n        return 0.1\n    if epoch < 150:\n        return 0.01\n    return 0.001\n\ndef cos_scheduler(epoch):\n    return (start_lr+end_lr)/2.+(start_lr-end_lr)/2.*math.cos(math.pi/2.0*(epoch/(epochs/2.0)))\nIs there anyway to use a different learning rate scheduler(e.g. cos scheduler)???", "body": "Hello, everyone. \r\n\r\nsince tensorflow or pytorch can adjust learning rate easily by using a scheduler function.\r\n\r\n```python\r\nepochs = 200\r\ndef step_decay_scheduler(epoch):\r\n    if epoch < 81:\r\n        return 0.1\r\n    if epoch < 122:\r\n        return 0.01\r\n    return 0.001\r\n\r\ndef step_decay_scheduler2(epoch):\r\n    if epoch < 100:\r\n        return 0.1\r\n    if epoch < 150:\r\n        return 0.01\r\n    return 0.001\r\n\r\ndef cos_scheduler(epoch):\r\n    return (start_lr+end_lr)/2.+(start_lr-end_lr)/2.*math.cos(math.pi/2.0*(epoch/(epochs/2.0)))\r\n```\r\n\r\nIs there anyway to use a different learning rate scheduler(e.g. cos scheduler)???\r\n\r\n "}