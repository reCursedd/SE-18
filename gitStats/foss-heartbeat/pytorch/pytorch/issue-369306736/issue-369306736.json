{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12580", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12580/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12580/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12580/events", "html_url": "https://github.com/pytorch/pytorch/issues/12580", "id": 369306736, "node_id": "MDU6SXNzdWUzNjkzMDY3MzY=", "number": 12580, "title": "[JIT] torch.arange is basically untraceable", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-11T21:11:18Z", "updated_at": "2018-10-12T09:48:02Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<pre><code>import torch\n\ndef foo(x):\n    dim = x.size(1)\n    print('tracer state', torch._C._get_tracing_state())\n    return torch.arange(0, dim)\n\ntraced = torch.jit.trace(foo, (torch.rand(3, 4),), check_trace=False)\nprint(traced.graph)\n</code></pre>\n<pre><code>tracer state graph(%0 : Float(3, 4)) {\n  %1 : int = prim::Constant[value=1]()\n  %2 : int = aten::size(%0, %1)\n  %3 : Long() = prim::NumToTensor(%2)\n  return ();\n}\n\ngraph(%0 : Float(3, 4)) {\n  %4 : int = prim::Constant[value=0]()\n  %5 : int = prim::Constant[value=4]()\n  %6 : int = prim::Constant[value=1]()\n  %7 : int = prim::Constant[value=4]()\n  %8 : int = prim::Constant[value=0]()\n  %9 : int[] = prim::Constant[value=[0, -1]]()\n  %10 : Long(4) = aten::arange(%4, %5, %6, %7, %8, %9)\n  return (%10);\n}\n</code></pre>\n<p>We are hard-coding values into the graph. <code>arange</code> takes scalars, while the values we're getting are torch.Tensor by the time we get to arange calls. Are we not recording Tensor-&gt;Scalar conversion in tracing?</p>", "body_text": "import torch\n\ndef foo(x):\n    dim = x.size(1)\n    print('tracer state', torch._C._get_tracing_state())\n    return torch.arange(0, dim)\n\ntraced = torch.jit.trace(foo, (torch.rand(3, 4),), check_trace=False)\nprint(traced.graph)\n\ntracer state graph(%0 : Float(3, 4)) {\n  %1 : int = prim::Constant[value=1]()\n  %2 : int = aten::size(%0, %1)\n  %3 : Long() = prim::NumToTensor(%2)\n  return ();\n}\n\ngraph(%0 : Float(3, 4)) {\n  %4 : int = prim::Constant[value=0]()\n  %5 : int = prim::Constant[value=4]()\n  %6 : int = prim::Constant[value=1]()\n  %7 : int = prim::Constant[value=4]()\n  %8 : int = prim::Constant[value=0]()\n  %9 : int[] = prim::Constant[value=[0, -1]]()\n  %10 : Long(4) = aten::arange(%4, %5, %6, %7, %8, %9)\n  return (%10);\n}\n\nWe are hard-coding values into the graph. arange takes scalars, while the values we're getting are torch.Tensor by the time we get to arange calls. Are we not recording Tensor->Scalar conversion in tracing?", "body": "```\r\nimport torch\r\n\r\ndef foo(x):\r\n    dim = x.size(1)\r\n    print('tracer state', torch._C._get_tracing_state())\r\n    return torch.arange(0, dim)\r\n\r\ntraced = torch.jit.trace(foo, (torch.rand(3, 4),), check_trace=False)\r\nprint(traced.graph)\r\n```\r\n\r\n```\r\ntracer state graph(%0 : Float(3, 4)) {\r\n  %1 : int = prim::Constant[value=1]()\r\n  %2 : int = aten::size(%0, %1)\r\n  %3 : Long() = prim::NumToTensor(%2)\r\n  return ();\r\n}\r\n\r\ngraph(%0 : Float(3, 4)) {\r\n  %4 : int = prim::Constant[value=0]()\r\n  %5 : int = prim::Constant[value=4]()\r\n  %6 : int = prim::Constant[value=1]()\r\n  %7 : int = prim::Constant[value=4]()\r\n  %8 : int = prim::Constant[value=0]()\r\n  %9 : int[] = prim::Constant[value=[0, -1]]()\r\n  %10 : Long(4) = aten::arange(%4, %5, %6, %7, %8, %9)\r\n  return (%10);\r\n}\r\n```\r\n\r\nWe are hard-coding values into the graph. `arange` takes scalars, while the values we're getting are torch.Tensor by the time we get to arange calls. Are we not recording Tensor->Scalar conversion in tracing?"}