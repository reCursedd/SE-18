{"url": "https://api.github.com/repos/pytorch/pytorch/issues/565", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/565/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/565/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/565/events", "html_url": "https://github.com/pytorch/pytorch/issues/565", "id": 202651207, "node_id": "MDU6SXNzdWUyMDI2NTEyMDc=", "number": 565, "title": "Optional constructor param to set embedding weights", "user": {"login": "dpressel", "id": 247881, "node_id": "MDQ6VXNlcjI0Nzg4MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/247881?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dpressel", "html_url": "https://github.com/dpressel", "followers_url": "https://api.github.com/users/dpressel/followers", "following_url": "https://api.github.com/users/dpressel/following{/other_user}", "gists_url": "https://api.github.com/users/dpressel/gists{/gist_id}", "starred_url": "https://api.github.com/users/dpressel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dpressel/subscriptions", "organizations_url": "https://api.github.com/users/dpressel/orgs", "repos_url": "https://api.github.com/users/dpressel/repos", "events_url": "https://api.github.com/users/dpressel/events{/privacy}", "received_events_url": "https://api.github.com/users/dpressel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-01-23T21:08:38Z", "updated_at": "2017-10-12T18:46:25Z", "closed_at": "2017-01-23T22:30:18Z", "author_association": "NONE", "body_html": "<p>It would be nice if it was possible to pre-initialize the embedding weights with an existing Tensor.  For example, this is possible in Keras, using the optional \"weights\" parameter (which defaults to None)</p>\n<p>Looking at sparse.py in nn/modules It looks like this would be fairly straightforward to add this (though I assume one would want to validate the input):</p>\n<pre><code>    def __init__(self, num_embeddings, embedding_dim, padding_idx=None,\n                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\n                 sparse=False, weight=None):\n        super(Embedding, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        \n        self.weight = Parameter(weight or torch.Tensor(num_embeddings, embedding_dim))\n        self.sparse = sparse\n\n        self.reset_parameters()\n</code></pre>", "body_text": "It would be nice if it was possible to pre-initialize the embedding weights with an existing Tensor.  For example, this is possible in Keras, using the optional \"weights\" parameter (which defaults to None)\nLooking at sparse.py in nn/modules It looks like this would be fairly straightforward to add this (though I assume one would want to validate the input):\n    def __init__(self, num_embeddings, embedding_dim, padding_idx=None,\n                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\n                 sparse=False, weight=None):\n        super(Embedding, self).__init__()\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        \n        self.weight = Parameter(weight or torch.Tensor(num_embeddings, embedding_dim))\n        self.sparse = sparse\n\n        self.reset_parameters()", "body": "It would be nice if it was possible to pre-initialize the embedding weights with an existing Tensor.  For example, this is possible in Keras, using the optional \"weights\" parameter (which defaults to None) \r\n\r\nLooking at sparse.py in nn/modules It looks like this would be fairly straightforward to add this (though I assume one would want to validate the input):\r\n\r\n```\r\n    def __init__(self, num_embeddings, embedding_dim, padding_idx=None,\r\n                 max_norm=None, norm_type=2, scale_grad_by_freq=False,\r\n                 sparse=False, weight=None):\r\n        super(Embedding, self).__init__()\r\n        self.num_embeddings = num_embeddings\r\n        self.embedding_dim = embedding_dim\r\n        self.padding_idx = padding_idx\r\n        self.max_norm = max_norm\r\n        self.norm_type = norm_type\r\n        self.scale_grad_by_freq = scale_grad_by_freq\r\n        \r\n        self.weight = Parameter(weight or torch.Tensor(num_embeddings, embedding_dim))\r\n        self.sparse = sparse\r\n\r\n        self.reset_parameters()\r\n```"}