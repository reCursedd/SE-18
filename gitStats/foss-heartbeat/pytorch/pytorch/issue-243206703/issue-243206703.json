{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2115", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2115/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2115/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2115/events", "html_url": "https://github.com/pytorch/pytorch/issues/2115", "id": 243206703, "node_id": "MDU6SXNzdWUyNDMyMDY3MDM=", "number": 2115, "title": "Support different weights in each batch in the loss function like BCEWithLogitsLoss ?", "user": {"login": "Liu0329", "id": 8122099, "node_id": "MDQ6VXNlcjgxMjIwOTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/8122099?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Liu0329", "html_url": "https://github.com/Liu0329", "followers_url": "https://api.github.com/users/Liu0329/followers", "following_url": "https://api.github.com/users/Liu0329/following{/other_user}", "gists_url": "https://api.github.com/users/Liu0329/gists{/gist_id}", "starred_url": "https://api.github.com/users/Liu0329/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Liu0329/subscriptions", "organizations_url": "https://api.github.com/users/Liu0329/orgs", "repos_url": "https://api.github.com/users/Liu0329/repos", "events_url": "https://api.github.com/users/Liu0329/events{/privacy}", "received_events_url": "https://api.github.com/users/Liu0329/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-07-15T23:28:10Z", "updated_at": "2017-07-15T23:28:10Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hello, my need is to set different weights for each instance in a training batch when using BCEWithLogitsLoss. But I found it only supports the same weight for all instances in a batch. Is there anyway to deal with it or should we add the support ? Below is my test code</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass TwoLayerNet(torch.nn.Module):\n  def __init__(self, D_in, H, D_out):\n    super(TwoLayerNet, self).__init__()\n    self.linear1 = nn.Linear(D_in, H)\n    self.linear2 = nn.Linear(H, D_out)\n\n  def forward(self, x):\n    h_relu = self.linear1(x).clamp(min=0)\n    y_pred = self.linear2(h_relu)\n    return y_pred\n\nN, D_in, H, D_out = 64, 1000, 100, 10\n\nx = Variable(torch.randn(N, D_in), requires_grad=True)\ny = Variable(torch.randn(N, D_out), requires_grad=False)\n\nmodel = TwoLayerNet(D_in, H, D_out)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\nfor t in range(10):\n  y_pred = model(x)\n\n  #mask = torch.ones(y_pred.size()) # I wish I could use it like this\n  mask = torch.ones(D_out)\n  mask[t] = 0\n\n  criterion = nn.BCEWithLogitsLoss(size_average=True, weight=mask)\n  loss = criterion(y_pred, y)\n  print(t, loss.data[0])\n\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n</code></pre>", "body_text": "Hello, my need is to set different weights for each instance in a training batch when using BCEWithLogitsLoss. But I found it only supports the same weight for all instances in a batch. Is there anyway to deal with it or should we add the support ? Below is my test code\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass TwoLayerNet(torch.nn.Module):\n  def __init__(self, D_in, H, D_out):\n    super(TwoLayerNet, self).__init__()\n    self.linear1 = nn.Linear(D_in, H)\n    self.linear2 = nn.Linear(H, D_out)\n\n  def forward(self, x):\n    h_relu = self.linear1(x).clamp(min=0)\n    y_pred = self.linear2(h_relu)\n    return y_pred\n\nN, D_in, H, D_out = 64, 1000, 100, 10\n\nx = Variable(torch.randn(N, D_in), requires_grad=True)\ny = Variable(torch.randn(N, D_out), requires_grad=False)\n\nmodel = TwoLayerNet(D_in, H, D_out)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n\nfor t in range(10):\n  y_pred = model(x)\n\n  #mask = torch.ones(y_pred.size()) # I wish I could use it like this\n  mask = torch.ones(D_out)\n  mask[t] = 0\n\n  criterion = nn.BCEWithLogitsLoss(size_average=True, weight=mask)\n  loss = criterion(y_pred, y)\n  print(t, loss.data[0])\n\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()", "body": "Hello, my need is to set different weights for each instance in a training batch when using BCEWithLogitsLoss. But I found it only supports the same weight for all instances in a batch. Is there anyway to deal with it or should we add the support ? Below is my test code\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nclass TwoLayerNet(torch.nn.Module):\r\n  def __init__(self, D_in, H, D_out):\r\n    super(TwoLayerNet, self).__init__()\r\n    self.linear1 = nn.Linear(D_in, H)\r\n    self.linear2 = nn.Linear(H, D_out)\r\n\r\n  def forward(self, x):\r\n    h_relu = self.linear1(x).clamp(min=0)\r\n    y_pred = self.linear2(h_relu)\r\n    return y_pred\r\n\r\nN, D_in, H, D_out = 64, 1000, 100, 10\r\n\r\nx = Variable(torch.randn(N, D_in), requires_grad=True)\r\ny = Variable(torch.randn(N, D_out), requires_grad=False)\r\n\r\nmodel = TwoLayerNet(D_in, H, D_out)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\r\n\r\nfor t in range(10):\r\n  y_pred = model(x)\r\n\r\n  #mask = torch.ones(y_pred.size()) # I wish I could use it like this\r\n  mask = torch.ones(D_out)\r\n  mask[t] = 0\r\n\r\n  criterion = nn.BCEWithLogitsLoss(size_average=True, weight=mask)\r\n  loss = criterion(y_pred, y)\r\n  print(t, loss.data[0])\r\n\r\n  optimizer.zero_grad()\r\n  loss.backward()\r\n  optimizer.step()\r\n```"}