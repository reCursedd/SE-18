{"url": "https://api.github.com/repos/pytorch/pytorch/issues/913", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/913/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/913/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/913/events", "html_url": "https://github.com/pytorch/pytorch/issues/913", "id": 211774684, "node_id": "MDU6SXNzdWUyMTE3NzQ2ODQ=", "number": 913, "title": "pointwise ops autograd produces grad with wrong size", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-03T19:05:22Z", "updated_at": "2017-03-03T19:11:20Z", "closed_at": "2017-03-03T19:11:20Z", "author_association": "CONTRIBUTOR", "body_html": "<pre><code>import torch\nfrom torch.autograd import Variable\n\nv1 = Variable(torch.Tensor(5,2,1), requires_grad=True)\nv2 = Variable(torch.Tensor(5,2), requires_grad=True)\n(v1+v2).sum().backward()\nprint(v2.data.size(), v2.grad.size())\n\n&gt;&gt; ((5L, 2L), (5L, 2L, 1L))\n</code></pre>\n<p>If <code>v2</code> was the result of say, a linear layer, then the backwards pass will fail.</p>", "body_text": "import torch\nfrom torch.autograd import Variable\n\nv1 = Variable(torch.Tensor(5,2,1), requires_grad=True)\nv2 = Variable(torch.Tensor(5,2), requires_grad=True)\n(v1+v2).sum().backward()\nprint(v2.data.size(), v2.grad.size())\n\n>> ((5L, 2L), (5L, 2L, 1L))\n\nIf v2 was the result of say, a linear layer, then the backwards pass will fail.", "body": "```\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nv1 = Variable(torch.Tensor(5,2,1), requires_grad=True)\r\nv2 = Variable(torch.Tensor(5,2), requires_grad=True)\r\n(v1+v2).sum().backward()\r\nprint(v2.data.size(), v2.grad.size())\r\n\r\n>> ((5L, 2L), (5L, 2L, 1L))\r\n```\r\n\r\nIf `v2` was the result of say, a linear layer, then the backwards pass will fail."}