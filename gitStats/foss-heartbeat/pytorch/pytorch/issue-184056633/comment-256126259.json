{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/256126259", "html_url": "https://github.com/pytorch/pytorch/issues/143#issuecomment-256126259", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/143", "id": 256126259, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NjEyNjI1OQ==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-25T18:26:00Z", "updated_at": "2016-10-25T18:26:00Z", "author_association": "MEMBER", "body_html": "<p>Yeah, it would be nice if you could assign everything you can access.<br>\nHere's another idea: what if we introduce a new type<br>\n\"torch.autograd.Parameter\" which extends Varaiable but is always a leaf<br>\n(never constructed by a autograd op). It would serve to distinguish model<br>\nparameters from stateful Variables like RNN hidden state. Weights and biases<br>\nwould be Parameters</p>\n<p>On Sunday, October 23, 2016, Adam Paszke <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p>So I actually don't think it's a very good idea. I don't like it that<br>\nsometimes we allow this, sometimes not. Right now we're having this nice<br>\nnotation of module.child.child2.weight, so it would be nice if you could<br>\n(re)assign everything on arbitrary nesting level. On the other hand I agree<br>\nit's nice to keep such temporary state in containers, even though it makes<br>\nthings a more messy (e.g. with serialisation or forgetting to call<br>\n.reset()).</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"184056633\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/143\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/143/hovercard?comment_id=255604086&amp;comment_type=issue_comment\" href=\"https://github.com/pytorch/pytorch/issues/143#issuecomment-255604086\">#143 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/AAoB-ouCfOC0l_v8gMyGlvoGNcAMSIJsks5q26NHgaJpZM4KbYVm\">https://github.com/notifications/unsubscribe-auth/AAoB-ouCfOC0l_v8gMyGlvoGNcAMSIJsks5q26NHgaJpZM4KbYVm</a><br>\n.</p>\n</blockquote>", "body_text": "Yeah, it would be nice if you could assign everything you can access.\nHere's another idea: what if we introduce a new type\n\"torch.autograd.Parameter\" which extends Varaiable but is always a leaf\n(never constructed by a autograd op). It would serve to distinguish model\nparameters from stateful Variables like RNN hidden state. Weights and biases\nwould be Parameters\nOn Sunday, October 23, 2016, Adam Paszke notifications@github.com wrote:\n\nSo I actually don't think it's a very good idea. I don't like it that\nsometimes we allow this, sometimes not. Right now we're having this nice\nnotation of module.child.child2.weight, so it would be nice if you could\n(re)assign everything on arbitrary nesting level. On the other hand I agree\nit's nice to keep such temporary state in containers, even though it makes\nthings a more messy (e.g. with serialisation or forgetting to call\n.reset()).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n#143 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAoB-ouCfOC0l_v8gMyGlvoGNcAMSIJsks5q26NHgaJpZM4KbYVm\n.", "body": "Yeah, it would be nice if you could assign everything you can access.\nHere's another idea: what if we introduce a new type\n\"torch.autograd.Parameter\" which extends Varaiable but is always a leaf\n(never constructed by a autograd op). It would serve to distinguish model\nparameters from stateful Variables like RNN hidden state. Weights and biases\nwould be Parameters\n\nOn Sunday, October 23, 2016, Adam Paszke notifications@github.com wrote:\n\n> So I actually don't think it's a very good idea. I don't like it that\n> sometimes we allow this, sometimes not. Right now we're having this nice\n> notation of module.child.child2.weight, so it would be nice if you could\n> (re)assign everything on arbitrary nesting level. On the other hand I agree\n> it's nice to keep such temporary state in containers, even though it makes\n> things a more messy (e.g. with serialisation or forgetting to call\n> .reset()).\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/pytorch/pytorch/issues/143#issuecomment-255604086,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAoB-ouCfOC0l_v8gMyGlvoGNcAMSIJsks5q26NHgaJpZM4KbYVm\n> .\n"}