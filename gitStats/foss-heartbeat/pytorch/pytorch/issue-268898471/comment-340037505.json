{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340037505", "html_url": "https://github.com/pytorch/pytorch/issues/3313#issuecomment-340037505", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3313", "id": 340037505, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDAzNzUwNQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-27T17:40:42Z", "updated_at": "2017-10-27T17:41:04Z", "author_association": "MEMBER", "body_html": "<p>A few comments:</p>\n<ol>\n<li>Having stale state is terrible. It's not only <code>compute_next_functions</code> that will have to be updated. We'll have to go over a lot of code and make sure we never actually access state until we know it's up to date. I can see a lot of easy to miss bugs created because of this. However, I can see that this might be the only option to have reasonable perf in this setting...</li>\n<li>The cost of backward for in-place operations on views might become unbearable. People will think that their code will be blazing fast because they're only working in-place, but this will mean a ton of <code>Unstride</code>s in the backward, and each has to allocate full-sized tensors and zero them. That's the problem that makes backward of <code>pack_padded_sequence</code> slow. It has a lot of <code>narrow</code>s inside, and just writing out backward manually would make it a loooot faster. In general I think it's possible to overcome this problem, but this requires structuring the backward graph a bit differently (it would have to be a single atomic autograd op, that does slicing, performs another call, and copies the result back into the original thing. add clones where necessary)</li>\n<li>You're talking about modifications to views. What about modifications to base Variable? Does it just make all the views stale and that's it?</li>\n<li>I don't understand the part about non-contiguous base Variables. If you were to create a new non-contiguous tensor, you would still need to allocate a large contiguous chunk of memory for it, but the <code>copy_</code> would only initialize the referenced parts, leaving garbage between them. Not sure if it would ever be used again. If yes, it's an obvious bug, otherwise it might be possible to avoid allocating this huge chunk and compress the operation</li>\n</ol>", "body_text": "A few comments:\n\nHaving stale state is terrible. It's not only compute_next_functions that will have to be updated. We'll have to go over a lot of code and make sure we never actually access state until we know it's up to date. I can see a lot of easy to miss bugs created because of this. However, I can see that this might be the only option to have reasonable perf in this setting...\nThe cost of backward for in-place operations on views might become unbearable. People will think that their code will be blazing fast because they're only working in-place, but this will mean a ton of Unstrides in the backward, and each has to allocate full-sized tensors and zero them. That's the problem that makes backward of pack_padded_sequence slow. It has a lot of narrows inside, and just writing out backward manually would make it a loooot faster. In general I think it's possible to overcome this problem, but this requires structuring the backward graph a bit differently (it would have to be a single atomic autograd op, that does slicing, performs another call, and copies the result back into the original thing. add clones where necessary)\nYou're talking about modifications to views. What about modifications to base Variable? Does it just make all the views stale and that's it?\nI don't understand the part about non-contiguous base Variables. If you were to create a new non-contiguous tensor, you would still need to allocate a large contiguous chunk of memory for it, but the copy_ would only initialize the referenced parts, leaving garbage between them. Not sure if it would ever be used again. If yes, it's an obvious bug, otherwise it might be possible to avoid allocating this huge chunk and compress the operation", "body": "A few comments:\r\n1. Having stale state is terrible. It's not only `compute_next_functions` that will have to be updated. We'll have to go over a lot of code and make sure we never actually access state until we know it's up to date. I can see a lot of easy to miss bugs created because of this. However, I can see that this might be the only option to have reasonable perf in this setting...\r\n1. The cost of backward for in-place operations on views might become unbearable. People will think that their code will be blazing fast because they're only working in-place, but this will mean a ton of `Unstride`s in the backward, and each has to allocate full-sized tensors and zero them. That's the problem that makes backward of `pack_padded_sequence` slow. It has a lot of `narrow`s inside, and just writing out backward manually would make it a loooot faster. In general I think it's possible to overcome this problem, but this requires structuring the backward graph a bit differently (it would have to be a single atomic autograd op, that does slicing, performs another call, and copies the result back into the original thing. add clones where necessary)\r\n1. You're talking about modifications to views. What about modifications to base Variable? Does it just make all the views stale and that's it?\r\n1. I don't understand the part about non-contiguous base Variables. If you were to create a new non-contiguous tensor, you would still need to allocate a large contiguous chunk of memory for it, but the `copy_` would only initialize the referenced parts, leaving garbage between them. Not sure if it would ever be used again. If yes, it's an obvious bug, otherwise it might be possible to avoid allocating this huge chunk and compress the operation"}