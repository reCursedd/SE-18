{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340047840", "html_url": "https://github.com/pytorch/pytorch/issues/3313#issuecomment-340047840", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3313", "id": 340047840, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDA0Nzg0MA==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-27T18:22:26Z", "updated_at": "2017-10-27T18:22:26Z", "author_association": "MEMBER", "body_html": "<ol>\n<li></li>\n</ol>\n<p>One way to think about this is that views don't store <code>grad_fn</code>. When they're used in a computation, we dynamically create a view <code>Function</code> that does the inverse view based on the sizes and strides of the base and the view. But for performance reasons, it's convenient to cache the <code>grad_fn</code> along with a saved version like in SavedVariable to avoid unnecessarily creating duplicate functions when a view is used multiple times.</p>\n<p>There's a single accessor <code>Variable.grad_fn()</code> that everything goes through. I'm going to make VariableImpl.grad_fn private or at least rename it to <code>_grad_fn</code>.</p>\n<ol start=\"2\">\n<li></li>\n</ol>\n<p>I don't think this is specific to in-place operations. Narrowing operations are free in the forward-pass, but expensive in the backward pass because you have to pad the gradient with zeros.</p>\n<p>In-place ops on views have the cost of the basic backwards op (e.g. MulBackward) plus the view backwards (e.g. NarrowBackward).</p>\n<p>(If the base is not contiguous, the cost might be ~2x that)</p>\n<p>It'd be good to address this in general, perhaps with sparse gradients.</p>\n<ol start=\"3\">\n<li></li>\n</ol>\n<p>Yes, it makes the <code>grad_fn</code> on the views stale implicitly by increment the shared version counter. To be clear, the graph of Function nodes is still immutable, only Variable.grad_fn can change. And the base Variable does <strong>not</strong> have a reference to its views. The reference is one-way from views to base.</p>\n<ol start=\"4\">\n<li></li>\n</ol>\n<p>This is a bit tricky to explain. The viewing operation is completely described by the sizes, strides, and relative storageOffset of the base and view variables. We want to perform an equivalent viewing operation on the incoming <code>grad</code> as the one that created the view. (Note: grad is the same size as base, but not necessarily the same strides).</p>\n<p>If the <code>base</code> and <code>grad</code> are both contiguous, then we apply the view.sizes(), view.strides(), and relative storage offset to the grad. (This is basically free).</p>\n<p>If it's not the case, then we can do the <code>restride</code> op from above. You're right that if <code>base</code> has \"holes\" then it'll create more memory then necessary. These holes will never be accessed, because the view is guaranteed to only contain a subset (inclusive) of the elements in base.</p>\n<p>There's an optimization which avoids creating the wholes in most cases: divide the strides by the GCD.</p>", "body_text": "One way to think about this is that views don't store grad_fn. When they're used in a computation, we dynamically create a view Function that does the inverse view based on the sizes and strides of the base and the view. But for performance reasons, it's convenient to cache the grad_fn along with a saved version like in SavedVariable to avoid unnecessarily creating duplicate functions when a view is used multiple times.\nThere's a single accessor Variable.grad_fn() that everything goes through. I'm going to make VariableImpl.grad_fn private or at least rename it to _grad_fn.\n\n\n\nI don't think this is specific to in-place operations. Narrowing operations are free in the forward-pass, but expensive in the backward pass because you have to pad the gradient with zeros.\nIn-place ops on views have the cost of the basic backwards op (e.g. MulBackward) plus the view backwards (e.g. NarrowBackward).\n(If the base is not contiguous, the cost might be ~2x that)\nIt'd be good to address this in general, perhaps with sparse gradients.\n\n\n\nYes, it makes the grad_fn on the views stale implicitly by increment the shared version counter. To be clear, the graph of Function nodes is still immutable, only Variable.grad_fn can change. And the base Variable does not have a reference to its views. The reference is one-way from views to base.\n\n\n\nThis is a bit tricky to explain. The viewing operation is completely described by the sizes, strides, and relative storageOffset of the base and view variables. We want to perform an equivalent viewing operation on the incoming grad as the one that created the view. (Note: grad is the same size as base, but not necessarily the same strides).\nIf the base and grad are both contiguous, then we apply the view.sizes(), view.strides(), and relative storage offset to the grad. (This is basically free).\nIf it's not the case, then we can do the restride op from above. You're right that if base has \"holes\" then it'll create more memory then necessary. These holes will never be accessed, because the view is guaranteed to only contain a subset (inclusive) of the elements in base.\nThere's an optimization which avoids creating the wholes in most cases: divide the strides by the GCD.", "body": "1. \r\n\r\nOne way to think about this is that views don't store `grad_fn`. When they're used in a computation, we dynamically create a view `Function` that does the inverse view based on the sizes and strides of the base and the view. But for performance reasons, it's convenient to cache the `grad_fn` along with a saved version like in SavedVariable to avoid unnecessarily creating duplicate functions when a view is used multiple times.\r\n\r\nThere's a single accessor `Variable.grad_fn()` that everything goes through. I'm going to make VariableImpl.grad_fn private or at least rename it to `_grad_fn`.\r\n\r\n2. \r\n\r\nI don't think this is specific to in-place operations. Narrowing operations are free in the forward-pass, but expensive in the backward pass because you have to pad the gradient with zeros.\r\n\r\nIn-place ops on views have the cost of the basic backwards op (e.g. MulBackward) plus the view backwards (e.g. NarrowBackward).\r\n\r\n(If the base is not contiguous, the cost might be ~2x that)\r\n\r\nIt'd be good to address this in general, perhaps with sparse gradients.\r\n\r\n3. \r\n\r\nYes, it makes the `grad_fn` on the views stale implicitly by increment the shared version counter. To be clear, the graph of Function nodes is still immutable, only Variable.grad_fn can change. And the base Variable does **not** have a reference to its views. The reference is one-way from views to base.\r\n\r\n4. \r\n\r\nThis is a bit tricky to explain. The viewing operation is completely described by the sizes, strides, and relative storageOffset of the base and view variables. We want to perform an equivalent viewing operation on the incoming `grad` as the one that created the view. (Note: grad is the same size as base, but not necessarily the same strides).\r\n\r\nIf the `base` and `grad` are both contiguous, then we apply the view.sizes(), view.strides(), and relative storage offset to the grad. (This is basically free).\r\n\r\nIf it's not the case, then we can do the `restride` op from above. You're right that if `base` has \"holes\" then it'll create more memory then necessary. These holes will never be accessed, because the view is guaranteed to only contain a subset (inclusive) of the elements in base.\r\n\r\nThere's an optimization which avoids creating the wholes in most cases: divide the strides by the GCD."}