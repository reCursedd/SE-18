{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3313", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3313/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3313/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3313/events", "html_url": "https://github.com/pytorch/pytorch/issues/3313", "id": 268898471, "node_id": "MDU6SXNzdWUyNjg4OTg0NzE=", "number": 3313, "title": "Support in-place operations on Variable views", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-10-26T20:21:37Z", "updated_at": "2017-11-06T23:20:05Z", "closed_at": "2017-11-06T23:20:05Z", "author_association": "MEMBER", "body_html": "<p>Currently, we don't allow in-place operations on views if there are any other live Variables sharing the same data. We should allow in-place operations and compute the gradient correctly. There are a few changes necessary to support this.</p>\n<p>Viewing operations include <code>view</code>, <code>narrow</code>, <code>select</code>, <code>expand</code>, <code>[un]squeeze</code>, and <code>unfold</code>. By \"a view\" I mean a Variable that is a view on another Variable.</p>\n<ol>\n<li>Add a field \"base\" to Variable. Every view has a pointer to a single base Variable. (The base is never a view)</li>\n<li>In-place operations on views change the <code>grad_fn</code> of the base, not of the view.</li>\n<li>The <code>grad_fn</code> on a view may become stale. So views also store an <code>expected_version</code></li>\n</ol>\n<h3>In-place operations on views</h3>\n<p>Every in-place operation on a view inserts three <code>Functions</code> to <code>base.grad_fn</code>. For example:</p>\n<pre><code>x = base[2]\nx.mul_(y)\n</code></pre>\n<p>becomes:</p>\n<pre><code>Stride(MulBackward(Unstride(&lt;old base.grad_fn&gt;), &lt;y.grad_fn&gt;))\n</code></pre>\n<p>Assume, for now, that <code>base</code> is contiguous. The Stride op takes the <code>grad_output</code>, makes it contiguous, and applies the <code>stride</code>, <code>sizes</code>, and <code>storageOffset</code> of the view (<code>x</code>).  This performs the identical viewing operation on <code>grad_output</code> that was performed on <code>base</code> to get <code>x</code>.  For the above example, this is equivalent to:</p>\n<pre><code>grad_input = grad_output[2]\n</code></pre>\n<p>Unstride performs the reverse operation. It creates a <code>grad_input</code> with the same shape as <code>base</code>, applies the striding of the view <code>x</code> to <code>grad_input</code>, and copies <code>grad_output</code> into that view. For the above example, this is equivalent to:</p>\n<pre><code>grad_input = zeros_like(base)\ngrad_input[2] = grad_output\n</code></pre>\n<p><code>Stride</code> and <code>Unstride</code> are both implemented in terms of a differentiable restride operation that generalizes all of our viewing operations:</p>\n<pre><code>Tensor restride(Tensor self, IntList sizes, IntList strides, int offset);\n</code></pre>\n<h3>Stale grad_fn on views</h3>\n<p>An in-place operation on a view makes the <code>grad_fn</code> of all other views stale. The <code>compute_next_functions</code> needs to be updated to handle views: it should re-create the <code>Function</code> for a view if it's stale before returning it.</p>\n<h3>Non-contiguous base Variables</h3>\n<p>If base is not contiguous, then we also need the strides/sizes of the base in <code>restride</code>:</p>\n<pre><code>Tensor restride(Tensor self, IntList sizes, IntList strides, IntList base_sizes, IntList base_strides, int offset) {\n  Tensor copy = self.type().tensor(base_sizes, base_strides);\n  copy.copy_(self);\n  return restride(copy, sizes, strides, offset);\n}\n</code></pre>", "body_text": "Currently, we don't allow in-place operations on views if there are any other live Variables sharing the same data. We should allow in-place operations and compute the gradient correctly. There are a few changes necessary to support this.\nViewing operations include view, narrow, select, expand, [un]squeeze, and unfold. By \"a view\" I mean a Variable that is a view on another Variable.\n\nAdd a field \"base\" to Variable. Every view has a pointer to a single base Variable. (The base is never a view)\nIn-place operations on views change the grad_fn of the base, not of the view.\nThe grad_fn on a view may become stale. So views also store an expected_version\n\nIn-place operations on views\nEvery in-place operation on a view inserts three Functions to base.grad_fn. For example:\nx = base[2]\nx.mul_(y)\n\nbecomes:\nStride(MulBackward(Unstride(<old base.grad_fn>), <y.grad_fn>))\n\nAssume, for now, that base is contiguous. The Stride op takes the grad_output, makes it contiguous, and applies the stride, sizes, and storageOffset of the view (x).  This performs the identical viewing operation on grad_output that was performed on base to get x.  For the above example, this is equivalent to:\ngrad_input = grad_output[2]\n\nUnstride performs the reverse operation. It creates a grad_input with the same shape as base, applies the striding of the view x to grad_input, and copies grad_output into that view. For the above example, this is equivalent to:\ngrad_input = zeros_like(base)\ngrad_input[2] = grad_output\n\nStride and Unstride are both implemented in terms of a differentiable restride operation that generalizes all of our viewing operations:\nTensor restride(Tensor self, IntList sizes, IntList strides, int offset);\n\nStale grad_fn on views\nAn in-place operation on a view makes the grad_fn of all other views stale. The compute_next_functions needs to be updated to handle views: it should re-create the Function for a view if it's stale before returning it.\nNon-contiguous base Variables\nIf base is not contiguous, then we also need the strides/sizes of the base in restride:\nTensor restride(Tensor self, IntList sizes, IntList strides, IntList base_sizes, IntList base_strides, int offset) {\n  Tensor copy = self.type().tensor(base_sizes, base_strides);\n  copy.copy_(self);\n  return restride(copy, sizes, strides, offset);\n}", "body": "Currently, we don't allow in-place operations on views if there are any other live Variables sharing the same data. We should allow in-place operations and compute the gradient correctly. There are a few changes necessary to support this.\r\n\r\nViewing operations include `view`, `narrow`, `select`, `expand`, `[un]squeeze`, and `unfold`. By \"a view\" I mean a Variable that is a view on another Variable.\r\n\r\n1.  Add a field \"base\" to Variable. Every view has a pointer to a single base Variable. (The base is never a view)\r\n2. In-place operations on views change the `grad_fn` of the base, not of the view.\r\n3. The `grad_fn` on a view may become stale. So views also store an `expected_version`\r\n\r\n### In-place operations on views\r\n\r\nEvery in-place operation on a view inserts three `Functions` to `base.grad_fn`. For example:\r\n\r\n```\r\nx = base[2]\r\nx.mul_(y)\r\n```\r\n\r\nbecomes:\r\n\r\n```\r\nStride(MulBackward(Unstride(<old base.grad_fn>), <y.grad_fn>))\r\n```\r\n\r\nAssume, for now, that `base` is contiguous. The Stride op takes the `grad_output`, makes it contiguous, and applies the `stride`, `sizes`, and `storageOffset` of the view (`x`).  This performs the identical viewing operation on `grad_output` that was performed on `base` to get `x`.  For the above example, this is equivalent to:\r\n\r\n```\r\ngrad_input = grad_output[2]\r\n```\r\n\r\nUnstride performs the reverse operation. It creates a `grad_input` with the same shape as `base`, applies the striding of the view `x` to `grad_input`, and copies `grad_output` into that view. For the above example, this is equivalent to:\r\n\r\n```\r\ngrad_input = zeros_like(base)\r\ngrad_input[2] = grad_output\r\n```\r\n\r\n`Stride` and `Unstride` are both implemented in terms of a differentiable restride operation that generalizes all of our viewing operations:\r\n\r\n```\r\nTensor restride(Tensor self, IntList sizes, IntList strides, int offset);\r\n```\r\n\r\n### Stale grad_fn on views\r\n\r\nAn in-place operation on a view makes the `grad_fn` of all other views stale. The `compute_next_functions` needs to be updated to handle views: it should re-create the `Function` for a view if it's stale before returning it.\r\n\r\n### Non-contiguous base Variables\r\n\r\nIf base is not contiguous, then we also need the strides/sizes of the base in `restride`:\r\n\r\n```\r\nTensor restride(Tensor self, IntList sizes, IntList strides, IntList base_sizes, IntList base_strides, int offset) {\r\n  Tensor copy = self.type().tensor(base_sizes, base_strides);\r\n  copy.copy_(self);\r\n  return restride(copy, sizes, strides, offset);\r\n}\r\n```"}