{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2659", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2659/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2659/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2659/events", "html_url": "https://github.com/pytorch/pytorch/issues/2659", "id": 255937517, "node_id": "MDU6SXNzdWUyNTU5Mzc1MTc=", "number": 2659, "title": "CUDNN_STATUS_NOT_SUPPORTED when using higher order grads ", "user": {"login": "dzimmerer", "id": 13333807, "node_id": "MDQ6VXNlcjEzMzMzODA3", "avatar_url": "https://avatars0.githubusercontent.com/u/13333807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzimmerer", "html_url": "https://github.com/dzimmerer", "followers_url": "https://api.github.com/users/dzimmerer/followers", "following_url": "https://api.github.com/users/dzimmerer/following{/other_user}", "gists_url": "https://api.github.com/users/dzimmerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzimmerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzimmerer/subscriptions", "organizations_url": "https://api.github.com/users/dzimmerer/orgs", "repos_url": "https://api.github.com/users/dzimmerer/repos", "events_url": "https://api.github.com/users/dzimmerer/events{/privacy}", "received_events_url": "https://api.github.com/users/dzimmerer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 806617721, "node_id": "MDU6TGFiZWw4MDY2MTc3MjE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cudnn", "name": "cudnn", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-07T13:30:30Z", "updated_at": "2018-01-16T17:28:45Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I've been getting the error</p>\n<p><code>RuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input. </code></p>\n<p>when running the higher order grad script with my own model when I try to call grad_norm.backward()</p>\n<p>Here is my higher order grad script as well as my model:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torchvision.models <span class=\"pl-k\">import</span> resnet18\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">import</span> models.model\n\nmodel <span class=\"pl-k\">=</span> models.model.Encoder(<span class=\"pl-v\">image_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>), <span class=\"pl-v\">h_size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">512</span>], <span class=\"pl-v\">z_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\nmodel <span class=\"pl-k\">=</span> model.cuda()\n\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> dummy inputs for the example</span>\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">128</span>,<span class=\"pl-c1\">128</span>).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ntarget <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">2</span>).cuda())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> as usual</span>\noutput <span class=\"pl-k\">=</span> model(<span class=\"pl-c1\">input</span>)\nloss <span class=\"pl-k\">=</span> torch.nn.functional.mse_loss(output, target)\n\ngrad_params <span class=\"pl-k\">=</span> torch.autograd.grad(loss, model.parameters(), <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> torch.autograd.grad does not accumuate the gradients into the .grad attributes</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> It instead returns the gradients as Variable tuples.</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> now compute the 2-norm of the grad_params</span>\ngrad_norm <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n<span class=\"pl-k\">for</span> grad <span class=\"pl-k\">in</span> grad_params:\n    grad_norm <span class=\"pl-k\">+=</span> grad.pow(<span class=\"pl-c1\">2</span>).sum()\ngrad_norm <span class=\"pl-k\">=</span> grad_norm.sqrt()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> take the gradients wrt grad_norm. backward() will accumulate</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the gradients into the .grad attributes</span>\ngrad_norm.backward()\n</pre></div>\n<p>with the model:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">non_lin</span>(<span class=\"pl-smi\">nn_module</span>, <span class=\"pl-smi\">normalization</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weight<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-smi\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SELU<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-smi\">feat_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Non Lienar activation unit<span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">if</span> normalization <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch<span class=\"pl-pds\">\"</span></span>:\n        <span class=\"pl-k\">assert</span> feat_size <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>\n        nn_module.append(nn.BatchNorm2d(feat_size))\n    <span class=\"pl-k\">elif</span> normalization <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weight<span class=\"pl-pds\">\"</span></span>:\n        nn_module[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>] <span class=\"pl-k\">=</span> nn.utils.weight_norm(nn_module[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n    <span class=\"pl-k\">elif</span> normalization <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>instance<span class=\"pl-pds\">\"</span></span>:\n        <span class=\"pl-k\">assert</span> feat_size <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>\n        nn_module.append(nn.InstanceNorm2d(feat_size))\n\n    <span class=\"pl-k\">if</span> activation <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>LReLU<span class=\"pl-pds\">\"</span></span>:\n        nn_module.append(nn.LeakyReLU(<span class=\"pl-c1\">0.2</span>))\n    <span class=\"pl-k\">elif</span> activation <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ELU<span class=\"pl-pds\">\"</span></span>:\n        nn_module.append(nn.ELU())\n    <span class=\"pl-k\">elif</span> activation <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ReLU<span class=\"pl-pds\">\"</span></span>:\n        nn_module.append(nn.ReLU())\n    <span class=\"pl-k\">elif</span> activation <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SELU<span class=\"pl-pds\">\"</span></span>:\n        nn_module.append(nn.SELU())\n    <span class=\"pl-k\">else</span>:\n        warnings.warn(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Will not use any non linear activation function<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">RuntimeWarning</span>)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Encoder</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">image_size</span>, <span class=\"pl-smi\">z_dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>, <span class=\"pl-smi\">h_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>)):\n        <span class=\"pl-c1\">super</span>(Encoder, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        n_channels <span class=\"pl-k\">=</span> image_size[<span class=\"pl-c1\">0</span>]\n        img_size_new <span class=\"pl-k\">=</span> np.array([image_size[<span class=\"pl-c1\">1</span>], image_size[<span class=\"pl-c1\">2</span>]])\n\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(h_size, <span class=\"pl-c1\">list</span>) <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(h_size, <span class=\"pl-c1\">tuple</span>):\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">AttributeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>h_size has to be either a list or tuple or an int<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">len</span>(h_size) <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">3</span>:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">AttributeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>h_size has to contain at least three elements<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-k\">else</span>:\n            h_size_bot <span class=\"pl-k\">=</span> h_size[<span class=\"pl-c1\">0</span>]\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>## Start block</span>\n        start_block <span class=\"pl-k\">=</span> []\n\n        start_block.append(nn.Conv2d(n_channels, h_size_bot, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))\n        non_lin(start_block, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>LReLU<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">normalization</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">feat_size</span><span class=\"pl-k\">=</span>h_size_bot)\n\n        <span class=\"pl-c1\">self</span>.start <span class=\"pl-k\">=</span> nn.Sequential(<span class=\"pl-k\">*</span>start_block)\n        img_size_new <span class=\"pl-k\">=</span> img_size_new <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>## Middle block (Done until we reach ? x 4 x 4)</span>\n        <span class=\"pl-c1\">self</span>.middle_blocks <span class=\"pl-k\">=</span> []\n\n        <span class=\"pl-k\">for</span> h_size_top <span class=\"pl-k\">in</span> h_size[<span class=\"pl-c1\">1</span>:]:\n            middle_block <span class=\"pl-k\">=</span> []\n            middle_block.append(nn.Conv2d(h_size_bot, h_size_top, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))\n            non_lin(middle_block, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>LReLU<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">normalization</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">feat_size</span><span class=\"pl-k\">=</span>h_size_top)\n\n            middle <span class=\"pl-k\">=</span> nn.Sequential(<span class=\"pl-k\">*</span>middle_block)\n            <span class=\"pl-c1\">self</span>.middle_blocks.append(middle)\n            <span class=\"pl-c1\">self</span>.add_module(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>middle<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(h_size_top), middle)\n\n            h_size_bot <span class=\"pl-k\">=</span> h_size_top\n            img_size_new <span class=\"pl-k\">=</span> img_size_new <span class=\"pl-k\">//</span> <span class=\"pl-c1\">2</span>\n\n            <span class=\"pl-k\">if</span> np.min(img_size_new) <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">2</span>:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">AttributeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>h_size to long, one image dimension has already perished<span class=\"pl-pds\">\"</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>## End block</span>\n        end_block <span class=\"pl-k\">=</span> []\n\n        end_block.append(\n            nn.Conv2d(h_size_bot, z_dim, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span>img_size_new.tolist(), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))\n\n        <span class=\"pl-c1\">self</span>.end <span class=\"pl-k\">=</span> nn.Sequential(<span class=\"pl-k\">*</span>end_block)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inp</span>):\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.start(inp)\n        <span class=\"pl-k\">for</span> middle <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.middle_blocks:\n            output <span class=\"pl-k\">=</span> middle(output)\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.end(output)\n        <span class=\"pl-k\">return</span> output\n\n</pre></div>", "body_text": "Hi,\nI've been getting the error\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input. \nwhen running the higher order grad script with my own model when I try to call grad_norm.backward()\nHere is my higher order grad script as well as my model:\nimport torch\nfrom torchvision.models import resnet18\nfrom torch.autograd import Variable\n\nimport models.model\n\nmodel = models.model.Encoder(image_size=(3, 128, 128), h_size=[64, 128, 256, 512], z_dim=1)\nmodel = model.cuda()\n\n\n\n# dummy inputs for the example\ninput = Variable(torch.randn(2,3,128,128).cuda(), requires_grad=True)\ntarget = Variable(torch.zeros(2).cuda())\n\n# as usual\noutput = model(input)\nloss = torch.nn.functional.mse_loss(output, target)\n\ngrad_params = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n# torch.autograd.grad does not accumuate the gradients into the .grad attributes\n# It instead returns the gradients as Variable tuples.\n\n# now compute the 2-norm of the grad_params\ngrad_norm = 0\nfor grad in grad_params:\n    grad_norm += grad.pow(2).sum()\ngrad_norm = grad_norm.sqrt()\n\n# take the gradients wrt grad_norm. backward() will accumulate\n# the gradients into the .grad attributes\ngrad_norm.backward()\n\nwith the model:\ndef non_lin(nn_module, normalization=\"weight\", activation=\"SELU\", feat_size=None):\n    \"\"\"Non Lienar activation unit\"\"\"\n\n    if normalization == \"batch\":\n        assert feat_size is not None\n        nn_module.append(nn.BatchNorm2d(feat_size))\n    elif normalization == \"weight\":\n        nn_module[-1] = nn.utils.weight_norm(nn_module[-1])\n    elif normalization == \"instance\":\n        assert feat_size is not None\n        nn_module.append(nn.InstanceNorm2d(feat_size))\n\n    if activation == \"LReLU\":\n        nn_module.append(nn.LeakyReLU(0.2))\n    elif activation == \"ELU\":\n        nn_module.append(nn.ELU())\n    elif activation == \"ReLU\":\n        nn_module.append(nn.ReLU())\n    elif activation == \"SELU\":\n        nn_module.append(nn.SELU())\n    else:\n        warnings.warn(\"Will not use any non linear activation function\", RuntimeWarning)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, image_size, z_dim=256, h_size=(64, 128, 256)):\n        super(Encoder, self).__init__()\n\n        n_channels = image_size[0]\n        img_size_new = np.array([image_size[1], image_size[2]])\n\n        if not isinstance(h_size, list) and not isinstance(h_size, tuple):\n            raise AttributeError(\"h_size has to be either a list or tuple or an int\")\n        elif len(h_size) < 3:\n            raise AttributeError(\"h_size has to contain at least three elements\")\n        else:\n            h_size_bot = h_size[0]\n\n        ### Start block\n        start_block = []\n\n        start_block.append(nn.Conv2d(n_channels, h_size_bot, kernel_size=4, stride=2, padding=1, bias=False))\n        non_lin(start_block, activation=\"LReLU\", normalization=\"batch\", feat_size=h_size_bot)\n\n        self.start = nn.Sequential(*start_block)\n        img_size_new = img_size_new // 2\n\n        ### Middle block (Done until we reach ? x 4 x 4)\n        self.middle_blocks = []\n\n        for h_size_top in h_size[1:]:\n            middle_block = []\n            middle_block.append(nn.Conv2d(h_size_bot, h_size_top, kernel_size=4, stride=2, padding=1, bias=False))\n            non_lin(middle_block, activation=\"LReLU\", normalization=\"batch\", feat_size=h_size_top)\n\n            middle = nn.Sequential(*middle_block)\n            self.middle_blocks.append(middle)\n            self.add_module(\"middle\" + str(h_size_top), middle)\n\n            h_size_bot = h_size_top\n            img_size_new = img_size_new // 2\n\n            if np.min(img_size_new) < 2:\n                raise AttributeError(\"h_size to long, one image dimension has already perished\")\n\n        ### End block\n        end_block = []\n\n        end_block.append(\n            nn.Conv2d(h_size_bot, z_dim, kernel_size=img_size_new.tolist(), stride=1, padding=0, bias=False))\n\n        self.end = nn.Sequential(*end_block)\n\n    def forward(self, inp):\n        output = self.start(inp)\n        for middle in self.middle_blocks:\n            output = middle(output)\n        output = self.end(output)\n        return output", "body": "Hi,\r\n\r\nI've been getting the error \r\n\r\n`RuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\r\n`\r\n\r\nwhen running the higher order grad script with my own model when I try to call grad_norm.backward()\r\n\r\nHere is my higher order grad script as well as my model: \r\n\r\n```python\r\nimport torch\r\nfrom torchvision.models import resnet18\r\nfrom torch.autograd import Variable\r\n\r\nimport models.model\r\n\r\nmodel = models.model.Encoder(image_size=(3, 128, 128), h_size=[64, 128, 256, 512], z_dim=1)\r\nmodel = model.cuda()\r\n\r\n\r\n\r\n# dummy inputs for the example\r\ninput = Variable(torch.randn(2,3,128,128).cuda(), requires_grad=True)\r\ntarget = Variable(torch.zeros(2).cuda())\r\n\r\n# as usual\r\noutput = model(input)\r\nloss = torch.nn.functional.mse_loss(output, target)\r\n\r\ngrad_params = torch.autograd.grad(loss, model.parameters(), create_graph=True)\r\n# torch.autograd.grad does not accumuate the gradients into the .grad attributes\r\n# It instead returns the gradients as Variable tuples.\r\n\r\n# now compute the 2-norm of the grad_params\r\ngrad_norm = 0\r\nfor grad in grad_params:\r\n    grad_norm += grad.pow(2).sum()\r\ngrad_norm = grad_norm.sqrt()\r\n\r\n# take the gradients wrt grad_norm. backward() will accumulate\r\n# the gradients into the .grad attributes\r\ngrad_norm.backward()\r\n\r\n```\r\nwith the model:\r\n\r\n```python \r\ndef non_lin(nn_module, normalization=\"weight\", activation=\"SELU\", feat_size=None):\r\n    \"\"\"Non Lienar activation unit\"\"\"\r\n\r\n    if normalization == \"batch\":\r\n        assert feat_size is not None\r\n        nn_module.append(nn.BatchNorm2d(feat_size))\r\n    elif normalization == \"weight\":\r\n        nn_module[-1] = nn.utils.weight_norm(nn_module[-1])\r\n    elif normalization == \"instance\":\r\n        assert feat_size is not None\r\n        nn_module.append(nn.InstanceNorm2d(feat_size))\r\n\r\n    if activation == \"LReLU\":\r\n        nn_module.append(nn.LeakyReLU(0.2))\r\n    elif activation == \"ELU\":\r\n        nn_module.append(nn.ELU())\r\n    elif activation == \"ReLU\":\r\n        nn_module.append(nn.ReLU())\r\n    elif activation == \"SELU\":\r\n        nn_module.append(nn.SELU())\r\n    else:\r\n        warnings.warn(\"Will not use any non linear activation function\", RuntimeWarning)\r\n\r\n\r\nclass Encoder(nn.Module):\r\n    def __init__(self, image_size, z_dim=256, h_size=(64, 128, 256)):\r\n        super(Encoder, self).__init__()\r\n\r\n        n_channels = image_size[0]\r\n        img_size_new = np.array([image_size[1], image_size[2]])\r\n\r\n        if not isinstance(h_size, list) and not isinstance(h_size, tuple):\r\n            raise AttributeError(\"h_size has to be either a list or tuple or an int\")\r\n        elif len(h_size) < 3:\r\n            raise AttributeError(\"h_size has to contain at least three elements\")\r\n        else:\r\n            h_size_bot = h_size[0]\r\n\r\n        ### Start block\r\n        start_block = []\r\n\r\n        start_block.append(nn.Conv2d(n_channels, h_size_bot, kernel_size=4, stride=2, padding=1, bias=False))\r\n        non_lin(start_block, activation=\"LReLU\", normalization=\"batch\", feat_size=h_size_bot)\r\n\r\n        self.start = nn.Sequential(*start_block)\r\n        img_size_new = img_size_new // 2\r\n\r\n        ### Middle block (Done until we reach ? x 4 x 4)\r\n        self.middle_blocks = []\r\n\r\n        for h_size_top in h_size[1:]:\r\n            middle_block = []\r\n            middle_block.append(nn.Conv2d(h_size_bot, h_size_top, kernel_size=4, stride=2, padding=1, bias=False))\r\n            non_lin(middle_block, activation=\"LReLU\", normalization=\"batch\", feat_size=h_size_top)\r\n\r\n            middle = nn.Sequential(*middle_block)\r\n            self.middle_blocks.append(middle)\r\n            self.add_module(\"middle\" + str(h_size_top), middle)\r\n\r\n            h_size_bot = h_size_top\r\n            img_size_new = img_size_new // 2\r\n\r\n            if np.min(img_size_new) < 2:\r\n                raise AttributeError(\"h_size to long, one image dimension has already perished\")\r\n\r\n        ### End block\r\n        end_block = []\r\n\r\n        end_block.append(\r\n            nn.Conv2d(h_size_bot, z_dim, kernel_size=img_size_new.tolist(), stride=1, padding=0, bias=False))\r\n\r\n        self.end = nn.Sequential(*end_block)\r\n\r\n    def forward(self, inp):\r\n        output = self.start(inp)\r\n        for middle in self.middle_blocks:\r\n            output = middle(output)\r\n        output = self.end(output)\r\n        return output\r\n\r\n\r\n```\r\n\r\n\r\n\r\n"}