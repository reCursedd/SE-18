{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/322521486", "html_url": "https://github.com/pytorch/pytorch/issues/1272#issuecomment-322521486", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1272", "id": 322521486, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjUyMTQ4Ng==", "user": {"login": "jcjohnson", "id": 2718714, "node_id": "MDQ6VXNlcjI3MTg3MTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/2718714?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcjohnson", "html_url": "https://github.com/jcjohnson", "followers_url": "https://api.github.com/users/jcjohnson/followers", "following_url": "https://api.github.com/users/jcjohnson/following{/other_user}", "gists_url": "https://api.github.com/users/jcjohnson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcjohnson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcjohnson/subscriptions", "organizations_url": "https://api.github.com/users/jcjohnson/orgs", "repos_url": "https://api.github.com/users/jcjohnson/repos", "events_url": "https://api.github.com/users/jcjohnson/events{/privacy}", "received_events_url": "https://api.github.com/users/jcjohnson/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-15T16:44:21Z", "updated_at": "2017-08-15T16:57:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> I'm (embarrassingly) not very adept with docker - can you even do this with Docker? If I've understood correctly, with vanilla Docker you must reinstall the NVIDIA driver inside the container and make sure it matches the version of the driver on the host; with nvidia-docker the driver from the host is used inside the container. Either way you need to install the NVIDIA driver on the host before starting the Docker container, so you cannot fully encapsulate the state of the system in a single Dockerfile.</p>\n<p>However here are a few minimal shell scripts to reproduce the problem:</p>\n<p><a href=\"https://gist.github.com/jcjohnson/4976067b503d8d1ebafb5eb6f38f9aae\">https://gist.github.com/jcjohnson/4976067b503d8d1ebafb5eb6f38f9aae</a></p>\n<p>I can reproduce on Google Compute Engine, starting from the public Ubuntu 16.04 image (ubuntu-1604-xenial-v20170811) and creating a machine with 4 vCPU, 16GB of RAM, 64 GB of standard persistent disk, and a single Tesla K80 GPU.</p>\n<p>After creating the instance, run either install_driver_375.sh or install_driver_384.sh to install one of the drivers; this will also reboot the machine.</p>\n<p>After the machine comes back up, the script run.sh will install PyTorch 0.2 (which ships with its own CUDA and cuDNN) using pip and run the script leak.py which reproduces the problem.</p>\n<p>Using 375.66 drivers, CPU memory usage climbs from 1275 MB to 1574 MB over 10k forward / backward iterations, but with 384.66 drivers CPU memory usage stays at 1285 MB.</p>", "body_text": "@soumith I'm (embarrassingly) not very adept with docker - can you even do this with Docker? If I've understood correctly, with vanilla Docker you must reinstall the NVIDIA driver inside the container and make sure it matches the version of the driver on the host; with nvidia-docker the driver from the host is used inside the container. Either way you need to install the NVIDIA driver on the host before starting the Docker container, so you cannot fully encapsulate the state of the system in a single Dockerfile.\nHowever here are a few minimal shell scripts to reproduce the problem:\nhttps://gist.github.com/jcjohnson/4976067b503d8d1ebafb5eb6f38f9aae\nI can reproduce on Google Compute Engine, starting from the public Ubuntu 16.04 image (ubuntu-1604-xenial-v20170811) and creating a machine with 4 vCPU, 16GB of RAM, 64 GB of standard persistent disk, and a single Tesla K80 GPU.\nAfter creating the instance, run either install_driver_375.sh or install_driver_384.sh to install one of the drivers; this will also reboot the machine.\nAfter the machine comes back up, the script run.sh will install PyTorch 0.2 (which ships with its own CUDA and cuDNN) using pip and run the script leak.py which reproduces the problem.\nUsing 375.66 drivers, CPU memory usage climbs from 1275 MB to 1574 MB over 10k forward / backward iterations, but with 384.66 drivers CPU memory usage stays at 1285 MB.", "body": "@soumith I'm (embarrassingly) not very adept with docker - can you even do this with Docker? If I've understood correctly, with vanilla Docker you must reinstall the NVIDIA driver inside the container and make sure it matches the version of the driver on the host; with nvidia-docker the driver from the host is used inside the container. Either way you need to install the NVIDIA driver on the host before starting the Docker container, so you cannot fully encapsulate the state of the system in a single Dockerfile.\r\n\r\nHowever here are a few minimal shell scripts to reproduce the problem:\r\n\r\nhttps://gist.github.com/jcjohnson/4976067b503d8d1ebafb5eb6f38f9aae\r\n\r\nI can reproduce on Google Compute Engine, starting from the public Ubuntu 16.04 image (ubuntu-1604-xenial-v20170811) and creating a machine with 4 vCPU, 16GB of RAM, 64 GB of standard persistent disk, and a single Tesla K80 GPU.\r\n\r\nAfter creating the instance, run either install_driver_375.sh or install_driver_384.sh to install one of the drivers; this will also reboot the machine.\r\n\r\nAfter the machine comes back up, the script run.sh will install PyTorch 0.2 (which ships with its own CUDA and cuDNN) using pip and run the script leak.py which reproduces the problem.\r\n\r\nUsing 375.66 drivers, CPU memory usage climbs from 1275 MB to 1574 MB over 10k forward / backward iterations, but with 384.66 drivers CPU memory usage stays at 1285 MB."}