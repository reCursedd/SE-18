{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9171", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9171/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9171/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9171/events", "html_url": "https://github.com/pytorch/pytorch/issues/9171", "id": 338373840, "node_id": "MDU6SXNzdWUzMzgzNzM4NDA=", "number": 9171, "title": "Mismatch in behaviour of WeightedRandomSampler and other samplers", "user": {"login": "rsnk96", "id": 10851575, "node_id": "MDQ6VXNlcjEwODUxNTc1", "avatar_url": "https://avatars1.githubusercontent.com/u/10851575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsnk96", "html_url": "https://github.com/rsnk96", "followers_url": "https://api.github.com/users/rsnk96/followers", "following_url": "https://api.github.com/users/rsnk96/following{/other_user}", "gists_url": "https://api.github.com/users/rsnk96/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsnk96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsnk96/subscriptions", "organizations_url": "https://api.github.com/users/rsnk96/orgs", "repos_url": "https://api.github.com/users/rsnk96/repos", "events_url": "https://api.github.com/users/rsnk96/events{/privacy}", "received_events_url": "https://api.github.com/users/rsnk96/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-04T20:55:46Z", "updated_at": "2018-07-10T14:39:25Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>All the other samplers for <code>torch.utils.data.DataLoader()</code> are designed to be used such that you can iterate an epoch (one epoch being the number of listed indices/data-points) over them by simply having them in a <code>for</code> loop. However, looping through a <code>DataLoader</code> with the sampler as <code>WeightedRandomSampler</code> simply occurs <code>num_samples</code> times. The <code>num_samples</code> argument is misleading, as one might imagine they need only one sample at a time, and/or they plan to update the weights after every sample.</p>\n<p>It would be more consistent with other samplers if  the <code>WeightedRandomSampler</code> can be iterated over as many times as the length of the <code>weights</code> parameter (similar to how many times the <code>SubsetRandomSampler</code> iterates) and the <code>num_samples</code> argument is made to default to <code>1</code></p>\n<h2>Code example</h2>\n<p>Note: The <code>enumerate()</code> function is simply used to count the number of times the loop can procede</p>\n<p>Code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torchvision <span class=\"pl-k\">import</span> datasets, transforms\n\n\nmnist_test_dataset <span class=\"pl-k\">=</span> datasets.MNIST(\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>data<span class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n    <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose([transforms.ToTensor()]),\n)\ntrain_loader_no_sampler <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_test_dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>\n)\ntrain_loader_random <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_test_dataset,\n    <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>torch.utils.data.sampler.RandomSampler(mnist_test_dataset),\n)\ntrain_loader_subsetrandom <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_test_dataset,\n    <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>torch.utils.data.sampler.SubsetRandomSampler(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>)),\n)\ntrain_loader_weightedrandom <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(\n    mnist_test_dataset,\n    <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>torch.utils.data.sampler.WeightedRandomSampler(\n        <span class=\"pl-v\">weights</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">10000</span>, <span class=\"pl-v\">num_samples</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>\n    ),\n)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader_no_sampler):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>No sampler: <span class=\"pl-pds\">\"</span></span>, iter_no)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader_random):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Random sampler: <span class=\"pl-pds\">\"</span></span>, iter_no)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader_subsetrandom):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Subset Random sampler: <span class=\"pl-pds\">\"</span></span>, iter_no)\n\n<span class=\"pl-k\">for</span> iter_no, (data, label) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader_weightedrandom):\n    <span class=\"pl-k\">continue</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Weighted Random sampler: <span class=\"pl-pds\">\"</span></span>, iter_no)\n</pre></div>\n<p>Output: (MNIST test dataset contains 10000 samples)</p>\n<pre><code>No sampler:  9999\nRandom sampler:  9999\nSubset Random sampler:  9999\nWeighted Random sampler:  0\n</code></pre>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.11.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.2.88<br>\nGPU models and configuration: GPU 0: GeForce 840M<br>\nNvidia driver version: 396.24.02<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.1.4<br>\n/usr/local/cuda-9.0/lib64/libcudnn_static.a<br>\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.1.4<br>\n/usr/local/cuda-9.2/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.5)<br>\n[pip] numpydoc (0.8.0)<br>\n[pip] torch (0.4.0)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] magma-cuda80              2.3.0                         1    soumith<br>\n[conda] torch                     0.4.0                     <br>\n[conda] torchvision               0.2.1                     </p>", "body_text": "Issue description\nAll the other samplers for torch.utils.data.DataLoader() are designed to be used such that you can iterate an epoch (one epoch being the number of listed indices/data-points) over them by simply having them in a for loop. However, looping through a DataLoader with the sampler as WeightedRandomSampler simply occurs num_samples times. The num_samples argument is misleading, as one might imagine they need only one sample at a time, and/or they plan to update the weights after every sample.\nIt would be more consistent with other samplers if  the WeightedRandomSampler can be iterated over as many times as the length of the weights parameter (similar to how many times the SubsetRandomSampler iterates) and the num_samples argument is made to default to 1\nCode example\nNote: The enumerate() function is simply used to count the number of times the loop can procede\nCode:\nimport torch\nfrom torchvision import datasets, transforms\n\n\nmnist_test_dataset = datasets.MNIST(\n    \"data\",\n    train=False,\n    download=True,\n    transform=transforms.Compose([transforms.ToTensor()]),\n)\ntrain_loader_no_sampler = torch.utils.data.DataLoader(\n    mnist_test_dataset, batch_size=1, shuffle=True\n)\ntrain_loader_random = torch.utils.data.DataLoader(\n    mnist_test_dataset,\n    sampler=torch.utils.data.sampler.RandomSampler(mnist_test_dataset),\n)\ntrain_loader_subsetrandom = torch.utils.data.DataLoader(\n    mnist_test_dataset,\n    sampler=torch.utils.data.sampler.SubsetRandomSampler(range(10000)),\n)\ntrain_loader_weightedrandom = torch.utils.data.DataLoader(\n    mnist_test_dataset,\n    sampler=torch.utils.data.sampler.WeightedRandomSampler(\n        weights=[1] * 10000, num_samples=1\n    ),\n)\n\nfor iter_no, (data, label) in enumerate(train_loader_no_sampler):\n    continue\nprint(\"No sampler: \", iter_no)\n\nfor iter_no, (data, label) in enumerate(train_loader_random):\n    continue\nprint(\"Random sampler: \", iter_no)\n\nfor iter_no, (data, label) in enumerate(train_loader_subsetrandom):\n    continue\nprint(\"Subset Random sampler: \", iter_no)\n\nfor iter_no, (data, label) in enumerate(train_loader_weightedrandom):\n    continue\nprint(\"Weighted Random sampler: \", iter_no)\n\nOutput: (MNIST test dataset contains 10000 samples)\nNo sampler:  9999\nRandom sampler:  9999\nSubset Random sampler:  9999\nWeighted Random sampler:  0\n\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.11.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.2.88\nGPU models and configuration: GPU 0: GeForce 840M\nNvidia driver version: 396.24.02\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.1.4\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.1.4\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip] numpy (1.14.5)\n[pip] numpydoc (0.8.0)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.1)\n[conda] magma-cuda80              2.3.0                         1    soumith\n[conda] torch                     0.4.0                     \n[conda] torchvision               0.2.1", "body": "## Issue description\r\n\r\nAll the other samplers for `torch.utils.data.DataLoader()` are designed to be used such that you can iterate an epoch (one epoch being the number of listed indices/data-points) over them by simply having them in a `for` loop. However, looping through a `DataLoader` with the sampler as `WeightedRandomSampler` simply occurs `num_samples` times. The `num_samples` argument is misleading, as one might imagine they need only one sample at a time, and/or they plan to update the weights after every sample.\r\n\r\nIt would be more consistent with other samplers if  the `WeightedRandomSampler` can be iterated over as many times as the length of the `weights` parameter (similar to how many times the `SubsetRandomSampler` iterates) and the `num_samples` argument is made to default to `1`\r\n\r\n\r\n## Code example\r\n\r\nNote: The `enumerate()` function is simply used to count the number of times the loop can procede\r\n\r\nCode:\r\n```python\r\nimport torch\r\nfrom torchvision import datasets, transforms\r\n\r\n\r\nmnist_test_dataset = datasets.MNIST(\r\n    \"data\",\r\n    train=False,\r\n    download=True,\r\n    transform=transforms.Compose([transforms.ToTensor()]),\r\n)\r\ntrain_loader_no_sampler = torch.utils.data.DataLoader(\r\n    mnist_test_dataset, batch_size=1, shuffle=True\r\n)\r\ntrain_loader_random = torch.utils.data.DataLoader(\r\n    mnist_test_dataset,\r\n    sampler=torch.utils.data.sampler.RandomSampler(mnist_test_dataset),\r\n)\r\ntrain_loader_subsetrandom = torch.utils.data.DataLoader(\r\n    mnist_test_dataset,\r\n    sampler=torch.utils.data.sampler.SubsetRandomSampler(range(10000)),\r\n)\r\ntrain_loader_weightedrandom = torch.utils.data.DataLoader(\r\n    mnist_test_dataset,\r\n    sampler=torch.utils.data.sampler.WeightedRandomSampler(\r\n        weights=[1] * 10000, num_samples=1\r\n    ),\r\n)\r\n\r\nfor iter_no, (data, label) in enumerate(train_loader_no_sampler):\r\n    continue\r\nprint(\"No sampler: \", iter_no)\r\n\r\nfor iter_no, (data, label) in enumerate(train_loader_random):\r\n    continue\r\nprint(\"Random sampler: \", iter_no)\r\n\r\nfor iter_no, (data, label) in enumerate(train_loader_subsetrandom):\r\n    continue\r\nprint(\"Subset Random sampler: \", iter_no)\r\n\r\nfor iter_no, (data, label) in enumerate(train_loader_weightedrandom):\r\n    continue\r\nprint(\"Weighted Random sampler: \", iter_no)\r\n\r\n```\r\n\r\nOutput: (MNIST test dataset contains 10000 samples)\r\n```\r\nNo sampler:  9999\r\nRandom sampler:  9999\r\nSubset Random sampler:  9999\r\nWeighted Random sampler:  0\r\n```\r\n\r\n## System Info\r\n\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.88\r\nGPU models and configuration: GPU 0: GeForce 840M\r\nNvidia driver version: 396.24.02\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.0/lib64/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.5)\r\n[pip] numpydoc (0.8.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] magma-cuda80              2.3.0                         1    soumith\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n"}