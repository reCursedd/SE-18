{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103171993", "pull_request_review_id": 23949315, "id": 103171993, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMzE3MTk5Mw==", "diff_hunk": "@@ -1,6 +1,39 @@\n-\n+import torch\n+from .module import Module\n # TODO: Cosine\n-# TODO: CosineDistance - make sure lua's CosineDistance isn't actually cosine similarity\n+\n+\n+class CosineDistance(Module):\n+    \"\"\"\n+    Computes cosine similarity between two vectors x1,x2:\n+\n+          x1*x2\n+        ---------\n+        |x1|*|x2|\n+\n+    \"\"\"\n+\n+    def __init__(self):\n+        super(CosineDistance, self).__init__()\n+        self.eps = 1e-12\n+\n+    def forward(self, x):\n+        assert len(x) == 2, \"Input needs to be two vectors\"\n+        x1,x2 = x\n+        if x1.dim() == 1 or x2.dim() == 1:\n+            x1 = x1.view(1, -1)\n+            x2 = x2.view(1, -1)\n+        #\n+        #\t x1 * x2\t w12\n+        #\t--------- = -----\n+        #\t|x1|*|x2|\tw1*w2\n+        #\n+        # Nominator\n+        w12 = torch.sum(x1 * x2, 1)", "path": "torch/nn/modules/distance.py", "position": null, "original_position": 34, "commit_id": "f7499beb196e80645cb6b19e7daf7b1760174379", "original_commit_id": "1fa331e80a16dfd7464f7ff374e21d0927eb97d8", "user": {"login": "RicherMans", "id": 3957278, "node_id": "MDQ6VXNlcjM5NTcyNzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3957278?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RicherMans", "html_url": "https://github.com/RicherMans", "followers_url": "https://api.github.com/users/RicherMans/followers", "following_url": "https://api.github.com/users/RicherMans/following{/other_user}", "gists_url": "https://api.github.com/users/RicherMans/gists{/gist_id}", "starred_url": "https://api.github.com/users/RicherMans/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RicherMans/subscriptions", "organizations_url": "https://api.github.com/users/RicherMans/orgs", "repos_url": "https://api.github.com/users/RicherMans/repos", "events_url": "https://api.github.com/users/RicherMans/events{/privacy}", "received_events_url": "https://api.github.com/users/RicherMans/received_events", "type": "User", "site_admin": false}, "body": "I am just a bit unclear here, in this case, I assume that the input might also be of size `batch x dim`, therefore a `torch.dot` would return a single scalar, removing the batch dimension completely.\r\nOr did I misunderstand something?", "created_at": "2017-02-27T10:07:25Z", "updated_at": "2018-11-23T15:32:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/860#discussion_r103171993", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/860", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103171993"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/860#discussion_r103171993"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/860"}}, "body_html": "<p>I am just a bit unclear here, in this case, I assume that the input might also be of size <code>batch x dim</code>, therefore a <code>torch.dot</code> would return a single scalar, removing the batch dimension completely.<br>\nOr did I misunderstand something?</p>", "body_text": "I am just a bit unclear here, in this case, I assume that the input might also be of size batch x dim, therefore a torch.dot would return a single scalar, removing the batch dimension completely.\nOr did I misunderstand something?", "in_reply_to_id": 103166037}