{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427329721", "html_url": "https://github.com/pytorch/pytorch/pull/12342#issuecomment-427329721", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12342", "id": 427329721, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzMyOTcyMQ==", "user": {"login": "aidancully", "id": 503301, "node_id": "MDQ6VXNlcjUwMzMwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/503301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aidancully", "html_url": "https://github.com/aidancully", "followers_url": "https://api.github.com/users/aidancully/followers", "following_url": "https://api.github.com/users/aidancully/following{/other_user}", "gists_url": "https://api.github.com/users/aidancully/gists{/gist_id}", "starred_url": "https://api.github.com/users/aidancully/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aidancully/subscriptions", "organizations_url": "https://api.github.com/users/aidancully/orgs", "repos_url": "https://api.github.com/users/aidancully/repos", "events_url": "https://api.github.com/users/aidancully/events{/privacy}", "received_events_url": "https://api.github.com/users/aidancully/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-05T11:13:29Z", "updated_at": "2018-10-05T11:13:29Z", "author_association": "NONE", "body_html": "<p>I couldn't figure out how to easily get an automated test for this, if you've got any pointers I'm all ears. It's tricky to target a general test to this, because the problem depends on hardware configuration, and I didn't see any way of intercepting the cuda calls in the existing tests.</p>\n<p>The problem occurred for us with 4 (virtualized) GPUs in a single host (no rendezvous), none of which had p2p access, so we went through <a href=\"https://github.com/pytorch/pytorch/blob/1a72e72a808facc1698bc686f2855c32b288e96c/caffe2/python/data_parallel_model.py#L1070-L1073\">this code path</a>:</p>\n<pre><code>    elif len(devices) == 4:\n        sumN(0, 1)\n        sumN(2, 3)\n        sumN(0, 2)\n</code></pre>\n<p>Because it was a single host, the DeviceScope guard had been set <a href=\"https://github.com/pytorch/pytorch/blob/1a72e72a808facc1698bc686f2855c32b288e96c/caffe2/python/data_parallel_model.py#L1410-L1429\">here</a>:</p>\n<pre><code>    master_device_opt = core.DeviceOption(model._device_type, devices[0])\n...\n    for blob_name in blob_names:\n...\n        if _IsGPUBlob(model, blob_name):\n            with core.DeviceScope(master_device_opt):\n                if not isinstance(blobs_group[0], core.GradientSlice):\n                    _AllReduce(\n                        devices, model, net, blob_name, use_nccl, last_out\n                    )\n</code></pre>\n<p>to device 0, so that the operations in this call:</p>\n<pre><code>        sumN(2, 3)\n</code></pre>\n<p>would have been trying to copy from device 3 to device 0, and then sum from device 0 into device 2, which is the bug we hit.</p>", "body_text": "I couldn't figure out how to easily get an automated test for this, if you've got any pointers I'm all ears. It's tricky to target a general test to this, because the problem depends on hardware configuration, and I didn't see any way of intercepting the cuda calls in the existing tests.\nThe problem occurred for us with 4 (virtualized) GPUs in a single host (no rendezvous), none of which had p2p access, so we went through this code path:\n    elif len(devices) == 4:\n        sumN(0, 1)\n        sumN(2, 3)\n        sumN(0, 2)\n\nBecause it was a single host, the DeviceScope guard had been set here:\n    master_device_opt = core.DeviceOption(model._device_type, devices[0])\n...\n    for blob_name in blob_names:\n...\n        if _IsGPUBlob(model, blob_name):\n            with core.DeviceScope(master_device_opt):\n                if not isinstance(blobs_group[0], core.GradientSlice):\n                    _AllReduce(\n                        devices, model, net, blob_name, use_nccl, last_out\n                    )\n\nto device 0, so that the operations in this call:\n        sumN(2, 3)\n\nwould have been trying to copy from device 3 to device 0, and then sum from device 0 into device 2, which is the bug we hit.", "body": "I couldn't figure out how to easily get an automated test for this, if you've got any pointers I'm all ears. It's tricky to target a general test to this, because the problem depends on hardware configuration, and I didn't see any way of intercepting the cuda calls in the existing tests.\r\n\r\nThe problem occurred for us with 4 (virtualized) GPUs in a single host (no rendezvous), none of which had p2p access, so we went through [this code path](https://github.com/pytorch/pytorch/blob/1a72e72a808facc1698bc686f2855c32b288e96c/caffe2/python/data_parallel_model.py#L1070-L1073):\r\n```\r\n    elif len(devices) == 4:\r\n        sumN(0, 1)\r\n        sumN(2, 3)\r\n        sumN(0, 2)\r\n```\r\nBecause it was a single host, the DeviceScope guard had been set [here](https://github.com/pytorch/pytorch/blob/1a72e72a808facc1698bc686f2855c32b288e96c/caffe2/python/data_parallel_model.py#L1410-L1429):\r\n```\r\n    master_device_opt = core.DeviceOption(model._device_type, devices[0])\r\n...\r\n    for blob_name in blob_names:\r\n...\r\n        if _IsGPUBlob(model, blob_name):\r\n            with core.DeviceScope(master_device_opt):\r\n                if not isinstance(blobs_group[0], core.GradientSlice):\r\n                    _AllReduce(\r\n                        devices, model, net, blob_name, use_nccl, last_out\r\n                    )\r\n```\r\nto device 0, so that the operations in this call:\r\n```\r\n        sumN(2, 3)\r\n```\r\nwould have been trying to copy from device 3 to device 0, and then sum from device 0 into device 2, which is the bug we hit."}