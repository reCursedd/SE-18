{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/327957892", "html_url": "https://github.com/pytorch/pytorch/issues/2637#issuecomment-327957892", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2637", "id": 327957892, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzk1Nzg5Mg==", "user": {"login": "braingineer", "id": 1455742, "node_id": "MDQ6VXNlcjE0NTU3NDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1455742?v=4", "gravatar_id": "", "url": "https://api.github.com/users/braingineer", "html_url": "https://github.com/braingineer", "followers_url": "https://api.github.com/users/braingineer/followers", "following_url": "https://api.github.com/users/braingineer/following{/other_user}", "gists_url": "https://api.github.com/users/braingineer/gists{/gist_id}", "starred_url": "https://api.github.com/users/braingineer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/braingineer/subscriptions", "organizations_url": "https://api.github.com/users/braingineer/orgs", "repos_url": "https://api.github.com/users/braingineer/repos", "events_url": "https://api.github.com/users/braingineer/events{/privacy}", "received_events_url": "https://api.github.com/users/braingineer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-07T23:40:38Z", "updated_at": "2017-09-07T23:40:38Z", "author_association": "NONE", "body_html": "<p>hacky, but can do some 1d padding w/</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">pad1d</span>(<span class=\"pl-smi\">tensor</span>, <span class=\"pl-smi\">pad</span>, <span class=\"pl-smi\">permute_dims</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor should be in shape (batch, time, feat)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> pad should be in shape (left, right)</span>\n    <span class=\"pl-k\">if</span> permute_dims:\n        tensor <span class=\"pl-k\">=</span> tensor.permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>).contiguous() <span class=\"pl-c\"><span class=\"pl-c\">#</span> get features on first dim since we are padding time</span>\n    <span class=\"pl-k\">else</span>:\n        tenosr <span class=\"pl-k\">=</span> tensor.contiguous()\n    original_size <span class=\"pl-k\">=</span> tensor.size() <span class=\"pl-c\"><span class=\"pl-c\">#</span> (batch, feat, time)</span>\n    final_new_size <span class=\"pl-k\">=</span> (original_size[<span class=\"pl-c1\">0</span>], original_size[<span class=\"pl-c1\">1</span>], original_size[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">+</span> pad[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">+</span> pad[<span class=\"pl-c1\">1</span>])\n    temp_new_size <span class=\"pl-k\">=</span> original_size[:<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span>,) <span class=\"pl-k\">+</span> original_size[<span class=\"pl-c1\">2</span>:]\n    <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">len</span>(temp_new_size) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">4</span>\n    tensor <span class=\"pl-k\">=</span> tensor.view(<span class=\"pl-k\">*</span>temp_new_size)\n    pad <span class=\"pl-k\">=</span> pad <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>)\n    tensor <span class=\"pl-k\">=</span> F.pad(tensor, pad)\n    tensor <span class=\"pl-k\">=</span> tensor.view(<span class=\"pl-k\">*</span>final_new_size)\n    <span class=\"pl-k\">if</span> permute_dims:\n        tensor <span class=\"pl-k\">=</span> tensor.permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">return</span> tensor</pre></div>\n<p>can even do asymmetric padding</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(x)\n<span class=\"pl-c1\">print</span>(pad1d(x, (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>)))</pre></div>\n<pre><code>(0 ,.,.) = \n  0.7670\n -0.9891\n[torch.FloatTensor of size 1x2x1]\n\nVariable containing:\n(0 ,.,.) = \n  0.0000\n  0.0000\n  0.7670\n -0.9891\n  0.0000\n[torch.FloatTensor of size 1x5x1]\n</code></pre>", "body_text": "hacky, but can do some 1d padding w/\nimport torch\nimport torch.nn.functional as F\n\ndef pad1d(tensor, pad, permute_dims=True):\n    # tensor should be in shape (batch, time, feat)\n    # pad should be in shape (left, right)\n    if permute_dims:\n        tensor = tensor.permute(0, 2, 1).contiguous() # get features on first dim since we are padding time\n    else:\n        tenosr = tensor.contiguous()\n    original_size = tensor.size() # (batch, feat, time)\n    final_new_size = (original_size[0], original_size[1], original_size[2] + pad[0] + pad[1])\n    temp_new_size = original_size[:2] + (1,) + original_size[2:]\n    assert len(temp_new_size) == 4\n    tensor = tensor.view(*temp_new_size)\n    pad = pad + (0, 0)\n    tensor = F.pad(tensor, pad)\n    tensor = tensor.view(*final_new_size)\n    if permute_dims:\n        tensor = tensor.permute(0, 2, 1)\n    return tensor\ncan even do asymmetric padding\nx = torch.randn(1, 2, 1)\nprint(x)\nprint(pad1d(x, (2, 1)))\n(0 ,.,.) = \n  0.7670\n -0.9891\n[torch.FloatTensor of size 1x2x1]\n\nVariable containing:\n(0 ,.,.) = \n  0.0000\n  0.0000\n  0.7670\n -0.9891\n  0.0000\n[torch.FloatTensor of size 1x5x1]", "body": "hacky, but can do some 1d padding w/ \r\n\r\n```python\r\nimport torch\r\nimport torch.nn.functional as F\r\n\r\ndef pad1d(tensor, pad, permute_dims=True):\r\n    # tensor should be in shape (batch, time, feat)\r\n    # pad should be in shape (left, right)\r\n    if permute_dims:\r\n        tensor = tensor.permute(0, 2, 1).contiguous() # get features on first dim since we are padding time\r\n    else:\r\n        tenosr = tensor.contiguous()\r\n    original_size = tensor.size() # (batch, feat, time)\r\n    final_new_size = (original_size[0], original_size[1], original_size[2] + pad[0] + pad[1])\r\n    temp_new_size = original_size[:2] + (1,) + original_size[2:]\r\n    assert len(temp_new_size) == 4\r\n    tensor = tensor.view(*temp_new_size)\r\n    pad = pad + (0, 0)\r\n    tensor = F.pad(tensor, pad)\r\n    tensor = tensor.view(*final_new_size)\r\n    if permute_dims:\r\n        tensor = tensor.permute(0, 2, 1)\r\n    return tensor\r\n```\r\n\r\ncan even do asymmetric padding\r\n\r\n```python\r\nx = torch.randn(1, 2, 1)\r\nprint(x)\r\nprint(pad1d(x, (2, 1)))\r\n```\r\n\r\n```\r\n(0 ,.,.) = \r\n  0.7670\r\n -0.9891\r\n[torch.FloatTensor of size 1x2x1]\r\n\r\nVariable containing:\r\n(0 ,.,.) = \r\n  0.0000\r\n  0.0000\r\n  0.7670\r\n -0.9891\r\n  0.0000\r\n[torch.FloatTensor of size 1x5x1]\r\n```"}