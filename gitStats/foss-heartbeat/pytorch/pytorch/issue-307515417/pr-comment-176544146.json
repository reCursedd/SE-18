{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/176544146", "pull_request_review_id": 106267562, "id": 176544146, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NjU0NDE0Ng==", "diff_hunk": "@@ -354,19 +355,34 @@ class AffineTransform(Transform):\n \n     def __init__(self, loc, scale, event_dim=0, cache_size=0):\n         super(AffineTransform, self).__init__(cache_size=cache_size)\n-        self.loc, self.scale = broadcast_all(loc, scale)\n+        self.loc = loc\n+        self.scale = scale", "path": "torch/distributions/transforms.py", "position": null, "original_position": 27, "commit_id": "5940a3017e6892f01328dc02c6afb73352e387ba", "original_commit_id": "3a25db73c8fa4cf5fe53705bb392c711dd95b980", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "I agree that device placement and precision is a constant source of bugs. Here's what I'm concerned with in this PR:\r\n\r\nWe'd like to have simple constraints like `constraints.positive`, which is equivalent to `constraints.greater_than(0)`. Then to transform a tensor from `constraints.real` to `constraints.greater_than(0)`, you can apply a simple `ExpTransform()`. This works with tensors on all devices and of all precisions (half/float/double).\r\n\r\nThe situation is trickier for `constraints.greater_than(1)`. Now we need to transform via `ComposeTransform([ExpTransform(), AffineTransform(1, 1)])`. Notice all the `1`s still work across all devices and all precisions. If we were to promote these `1`s to tensors, we would need to know precision and device placement.\r\n\r\nIf we want to eagerly promote numbers to floats, I think we would need to pass around a prototype tensor throughout the entire constraints and transforms libraries, significantly changing the interface. Early promotion would prevent static attributes like `.support`, and would prevent static registration in functions like `transform_to()` and `biject_to()` (or would require registering factory methods or some other way to defer the decision of precision and device).\r\n\r\nI'd like to learn more ways of writing device-and-precision-agnostic code, and I'd appreciate any pointers or tips. My current go-to method is `numbers.Number` as in this PR.", "created_at": "2018-03-22T19:23:37Z", "updated_at": "2018-11-23T15:41:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/5931#discussion_r176544146", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5931", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/176544146"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5931#discussion_r176544146"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5931"}}, "body_html": "<p>I agree that device placement and precision is a constant source of bugs. Here's what I'm concerned with in this PR:</p>\n<p>We'd like to have simple constraints like <code>constraints.positive</code>, which is equivalent to <code>constraints.greater_than(0)</code>. Then to transform a tensor from <code>constraints.real</code> to <code>constraints.greater_than(0)</code>, you can apply a simple <code>ExpTransform()</code>. This works with tensors on all devices and of all precisions (half/float/double).</p>\n<p>The situation is trickier for <code>constraints.greater_than(1)</code>. Now we need to transform via <code>ComposeTransform([ExpTransform(), AffineTransform(1, 1)])</code>. Notice all the <code>1</code>s still work across all devices and all precisions. If we were to promote these <code>1</code>s to tensors, we would need to know precision and device placement.</p>\n<p>If we want to eagerly promote numbers to floats, I think we would need to pass around a prototype tensor throughout the entire constraints and transforms libraries, significantly changing the interface. Early promotion would prevent static attributes like <code>.support</code>, and would prevent static registration in functions like <code>transform_to()</code> and <code>biject_to()</code> (or would require registering factory methods or some other way to defer the decision of precision and device).</p>\n<p>I'd like to learn more ways of writing device-and-precision-agnostic code, and I'd appreciate any pointers or tips. My current go-to method is <code>numbers.Number</code> as in this PR.</p>", "body_text": "I agree that device placement and precision is a constant source of bugs. Here's what I'm concerned with in this PR:\nWe'd like to have simple constraints like constraints.positive, which is equivalent to constraints.greater_than(0). Then to transform a tensor from constraints.real to constraints.greater_than(0), you can apply a simple ExpTransform(). This works with tensors on all devices and of all precisions (half/float/double).\nThe situation is trickier for constraints.greater_than(1). Now we need to transform via ComposeTransform([ExpTransform(), AffineTransform(1, 1)]). Notice all the 1s still work across all devices and all precisions. If we were to promote these 1s to tensors, we would need to know precision and device placement.\nIf we want to eagerly promote numbers to floats, I think we would need to pass around a prototype tensor throughout the entire constraints and transforms libraries, significantly changing the interface. Early promotion would prevent static attributes like .support, and would prevent static registration in functions like transform_to() and biject_to() (or would require registering factory methods or some other way to defer the decision of precision and device).\nI'd like to learn more ways of writing device-and-precision-agnostic code, and I'd appreciate any pointers or tips. My current go-to method is numbers.Number as in this PR.", "in_reply_to_id": 176361909}