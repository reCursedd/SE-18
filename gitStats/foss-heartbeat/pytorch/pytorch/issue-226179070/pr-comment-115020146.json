{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/115020146", "pull_request_review_id": 36532963, "id": 115020146, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNTAyMDE0Ng==", "diff_hunk": "@@ -744,14 +744,17 @@ def triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-6, s\n     return loss\n \n \n-def normalize(input, p, dim, eps=1e-12):\n-    r\"\"\"Performs l_p normalization of inputs over specified dimension.\n+def normalize(input, p=2, dim=-1, eps=1e-12):\n+    r\"\"\"Performs :math:`L_p` normalization of inputs over specified dimension.\n \n     Does:\n \n     .. math::\n         v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}\n \n-    for each subtensor v over dimension dim of input.\n+    for each subtensor v over dimension dim of input. Each subtensor is flattened into a vector,\n+    i.e. :math:`\\lVert v \\rVert_p` is not a matrix norm.\n+\n+    With default arguments normalizes over the last dimension with Euclidean norm.", "path": "torch/nn/functional.py", "position": null, "original_position": 18, "commit_id": "403a49eea097a3329ba2f00f3d70583961cad6d2", "original_commit_id": "a809e1adfa42a8e4f0622e6a4a9507ba2d688cbe", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I'm not saying there are no cases where a different dim is needed, I'm only thinking what is more common. I think that in most applications last dim will be time or width and it seems weird to normalize over that.", "created_at": "2017-05-05T15:11:53Z", "updated_at": "2018-11-23T15:33:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/1467#discussion_r115020146", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1467", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/115020146"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1467#discussion_r115020146"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1467"}}, "body_html": "<p>I'm not saying there are no cases where a different dim is needed, I'm only thinking what is more common. I think that in most applications last dim will be time or width and it seems weird to normalize over that.</p>", "body_text": "I'm not saying there are no cases where a different dim is needed, I'm only thinking what is more common. I think that in most applications last dim will be time or width and it seems weird to normalize over that.", "in_reply_to_id": 115013199}