{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419905155", "html_url": "https://github.com/pytorch/pytorch/issues/9708#issuecomment-419905155", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9708", "id": 419905155, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTkwNTE1NQ==", "user": {"login": "DanieleBaccega", "id": 15123534, "node_id": "MDQ6VXNlcjE1MTIzNTM0", "avatar_url": "https://avatars3.githubusercontent.com/u/15123534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DanieleBaccega", "html_url": "https://github.com/DanieleBaccega", "followers_url": "https://api.github.com/users/DanieleBaccega/followers", "following_url": "https://api.github.com/users/DanieleBaccega/following{/other_user}", "gists_url": "https://api.github.com/users/DanieleBaccega/gists{/gist_id}", "starred_url": "https://api.github.com/users/DanieleBaccega/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DanieleBaccega/subscriptions", "organizations_url": "https://api.github.com/users/DanieleBaccega/orgs", "repos_url": "https://api.github.com/users/DanieleBaccega/repos", "events_url": "https://api.github.com/users/DanieleBaccega/events{/privacy}", "received_events_url": "https://api.github.com/users/DanieleBaccega/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-10T13:06:14Z", "updated_at": "2018-09-10T13:06:41Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7659413\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Tron-x\">@Tron-x</a>. I met the same error when I run resnet50_trainer with mpirun on two node with two gpus:</p>\n<p>Command:<br>\nmpirun.openmpi -machinefile addr.conf -npernode 1 -x PATH -x LD_LIBRARY_PATH -x LIBRARY_PATH python resnet50_trainer.py --train_data cifar10v2/cifar10_train_minidb --db_type minidb --num_gpus 2 --num_shards 2 --num_epochs 1 --epoch_size 50000 --base_learning_rate 0.1 --batch_size 256 --distributed_interfaces ib0 --test_data cifar10v2/cifar10_test_minidb --run_id 30 --num_labels 10 --image_size 32 --num_channels 3</p>\n<h2>Error:<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nINFO:data_parallel_model:Adding gradient operators<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Add initial parameter sync<br>\nINFO:data_parallel_model:Creating barrier net<br>\nTraceback (most recent call last):<br>\nFile \"resnet50_trainer.py\", line 606, in <br>\nmain()<br>\nFile \"resnet50_trainer.py\", line 602, in main<br>\nTrain(args)<br>\nFile \"resnet50_trainer.py\", line 441, in Train<br>\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)<br>\nFile \"/opt/conda/lib/python2.7/site-packages/caffe2/python/data_parallel_model.py\", line 1735, in OptimizeGradientMemory<br>\ninput_shapes_all_devices,<br>\nFile \"/opt/conda/lib/python2.7/site-packages/caffe2/python/workspace.py\", line 261, in InferShapesAndTypes<br>\nnet_protos, blob_dimensions<br>\n<strong>RuntimeError: [enforce fail at operator.cc:517] . Invalid shape inference for operator Broadcast Expected 2 outputs, but got 1</strong><br>\nINFO:data_parallel_model:Add initial parameter sync</h2>\n<h2>Primary job  terminated normally, but 1 process returned<br>\na non-zero exit code.. Per user-direction, the job has been aborted.</h2>\n<p>Have you find a solution? Thank you!</p>", "body_text": "Hi @Tron-x. I met the same error when I run resnet50_trainer with mpirun on two node with two gpus:\nCommand:\nmpirun.openmpi -machinefile addr.conf -npernode 1 -x PATH -x LD_LIBRARY_PATH -x LIBRARY_PATH python resnet50_trainer.py --train_data cifar10v2/cifar10_train_minidb --db_type minidb --num_gpus 2 --num_shards 2 --num_epochs 1 --epoch_size 50000 --base_learning_rate 0.1 --batch_size 256 --distributed_interfaces ib0 --test_data cifar10v2/cifar10_test_minidb --run_id 30 --num_labels 10 --image_size 32 --num_channels 3\nError:\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Creating barrier net\nTraceback (most recent call last):\nFile \"resnet50_trainer.py\", line 606, in \nmain()\nFile \"resnet50_trainer.py\", line 602, in main\nTrain(args)\nFile \"resnet50_trainer.py\", line 441, in Train\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\nFile \"/opt/conda/lib/python2.7/site-packages/caffe2/python/data_parallel_model.py\", line 1735, in OptimizeGradientMemory\ninput_shapes_all_devices,\nFile \"/opt/conda/lib/python2.7/site-packages/caffe2/python/workspace.py\", line 261, in InferShapesAndTypes\nnet_protos, blob_dimensions\nRuntimeError: [enforce fail at operator.cc:517] . Invalid shape inference for operator Broadcast Expected 2 outputs, but got 1\nINFO:data_parallel_model:Add initial parameter sync\nPrimary job  terminated normally, but 1 process returned\na non-zero exit code.. Per user-direction, the job has been aborted.\nHave you find a solution? Thank you!", "body": "Hi @Tron-x. I met the same error when I run resnet50_trainer with mpirun on two node with two gpus:\r\n\r\nCommand:\r\nmpirun.openmpi -machinefile addr.conf -npernode 1 -x PATH -x LD_LIBRARY_PATH -x LIBRARY_PATH python resnet50_trainer.py --train_data cifar10v2/cifar10_train_minidb --db_type minidb --num_gpus 2 --num_shards 2 --num_epochs 1 --epoch_size 50000 --base_learning_rate 0.1 --batch_size 256 --distributed_interfaces ib0 --test_data cifar10v2/cifar10_test_minidb --run_id 30 --num_labels 10 --image_size 32 --num_channels 3\r\n\r\nError:\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nINFO:data_parallel_model:Creating barrier net\r\nTraceback (most recent call last):\r\n  File \"resnet50_trainer.py\", line 606, in <module>\r\n    main()\r\n  File \"resnet50_trainer.py\", line 602, in main\r\n    Train(args)\r\n  File \"resnet50_trainer.py\", line 441, in Train\r\n    data_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\r\n  File \"/opt/conda/lib/python2.7/site-packages/caffe2/python/data_parallel_model.py\", line 1735, in OptimizeGradientMemory\r\n    input_shapes_all_devices,\r\n  File \"/opt/conda/lib/python2.7/site-packages/caffe2/python/workspace.py\", line 261, in InferShapesAndTypes\r\n    net_protos, blob_dimensions\r\n**RuntimeError: [enforce fail at operator.cc:517] . Invalid shape inference for operator Broadcast Expected 2 outputs, but got 1**\r\nINFO:data_parallel_model:Add initial parameter sync\r\n-------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code.. Per user-direction, the job has been aborted.\r\n-------------------------------------------------------\r\n\r\nHave you find a solution? Thank you!"}