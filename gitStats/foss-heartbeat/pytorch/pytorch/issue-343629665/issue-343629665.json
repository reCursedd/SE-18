{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9708", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9708/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9708/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9708/events", "html_url": "https://github.com/pytorch/pytorch/issues/9708", "id": 343629665, "node_id": "MDU6SXNzdWUzNDM2Mjk2NjU=", "number": 9708, "title": "[caffe2]mpirun multi-node multi GPU in Distributed mode ,run resnet50_trainer.py get RuntimeError", "user": {"login": "Tron-x", "id": 7659413, "node_id": "MDQ6VXNlcjc2NTk0MTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7659413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tron-x", "html_url": "https://github.com/Tron-x", "followers_url": "https://api.github.com/users/Tron-x/followers", "following_url": "https://api.github.com/users/Tron-x/following{/other_user}", "gists_url": "https://api.github.com/users/Tron-x/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tron-x/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tron-x/subscriptions", "organizations_url": "https://api.github.com/users/Tron-x/orgs", "repos_url": "https://api.github.com/users/Tron-x/repos", "events_url": "https://api.github.com/users/Tron-x/events{/privacy}", "received_events_url": "https://api.github.com/users/Tron-x/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-07-23T13:19:10Z", "updated_at": "2018-09-10T13:06:41Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>run with MPI<br>\nMPI is for coordinating machine rendezvous through MPI<br>\nso is it possible just using MPI as rendezvous but not redis or nfs?<br>\ni do not use redis and nfs<br>\nas my caffe2 is included in the container singularity \uff0c<br>\ni run the under command:<br>\nmpirun -np 4 singularity exec /public/DL_Data/cnic_ai_20180316.img python resnet50_trainer.py --train_data  ~/ILSVRC/ilsvrc12_train_lmdb --num_gpus 8  --batch_size 256 --num_epochs 90 --num_shards 4 --shard_id 1 --run_id 50002<br>\ngot runtime erro :</p>\n<h2>INFO:data_parallel_model:Adding gradient operators<br>\nINFO:data_parallel_model:Adding gradient operators<br>\nINFO:data_parallel_model:Adding gradient operators<br>\nINFO:data_parallel_model:Adding gradient operators<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Post-iteration operators for updating params<br>\nINFO:data_parallel_model:Calling optimizer builder function<br>\nINFO:data_parallel_model:Add initial parameter sync<br>\nINFO:data_parallel_model:Add initial parameter sync<br>\nINFO:data_parallel_model:Add initial parameter sync<br>\nINFO:data_parallel_model:Add initial parameter sync<br>\nTraceback (most recent call last):<br>\nFile \"resnet50_trainer.py\", line 602, in <br>\nmain()<br>\nFile \"resnet50_trainer.py\", line 598, in main<br>\nTrain(args)<br>\nFile \"resnet50_trainer.py\", line 436, in Train<br>\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory<br>\ninput_shapes_all_devices,<br>\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes<br>\nnet_protos, blob_dimensions<br>\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1<br>\nTraceback (most recent call last):<br>\nFile \"resnet50_trainer.py\", line 602, in <br>\nmain()<br>\nFile \"resnet50_trainer.py\", line 598, in main<br>\nTrain(args)<br>\nFile \"resnet50_trainer.py\", line 436, in Train<br>\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory<br>\ninput_shapes_all_devices,<br>\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes<br>\nnet_protos, blob_dimensions<br>\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1<br>\nTraceback (most recent call last):<br>\nFile \"resnet50_trainer.py\", line 602, in <br>\nmain()<br>\nFile \"resnet50_trainer.py\", line 598, in main<br>\nTrain(args)<br>\nFile \"resnet50_trainer.py\", line 436, in Train<br>\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory<br>\ninput_shapes_all_devices,<br>\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes<br>\nnet_protos, blob_dimensions<br>\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1<br>\nTraceback (most recent call last):<br>\nFile \"resnet50_trainer.py\", line 602, in <br>\nmain()<br>\nFile \"resnet50_trainer.py\", line 598, in main<br>\nTrain(args)<br>\nFile \"resnet50_trainer.py\", line 436, in Train<br>\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)<br>\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory<br>\ninput_shapes_all_devices,<br>\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes<br>\nnet_protos, blob_dimensions<br>\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1</h2>\n<h2>Primary job  terminated normally, but 1 process returned<br>\na non-zero exit code. Per user-direction, the job has been aborted.</h2>\n<hr>\n<p>mpirun detected that one or more processes exited with non-zero status, thus causing<br>\nthe job to be terminated. The first process to do so was:</p>\n<h2>Process name: [[43321,1],1]<br>\nExit code:    1</h2>\n<h2>if run with single node 8 gpus ,# success !</h2>\n<p>i have 4 nodes ,8 gpus per node<br>\nso how to run distributed caffe2 with mpi, whether redis or nfs is necessary? my machine does not install redis and nfs\u3002</p>", "body_text": "run with MPI\nMPI is for coordinating machine rendezvous through MPI\nso is it possible just using MPI as rendezvous but not redis or nfs?\ni do not use redis and nfs\nas my caffe2 is included in the container singularity \uff0c\ni run the under command:\nmpirun -np 4 singularity exec /public/DL_Data/cnic_ai_20180316.img python resnet50_trainer.py --train_data  ~/ILSVRC/ilsvrc12_train_lmdb --num_gpus 8  --batch_size 256 --num_epochs 90 --num_shards 4 --shard_id 1 --run_id 50002\ngot runtime erro :\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Adding gradient operators\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Post-iteration operators for updating params\nINFO:data_parallel_model:Calling optimizer builder function\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Add initial parameter sync\nINFO:data_parallel_model:Add initial parameter sync\nTraceback (most recent call last):\nFile \"resnet50_trainer.py\", line 602, in \nmain()\nFile \"resnet50_trainer.py\", line 598, in main\nTrain(args)\nFile \"resnet50_trainer.py\", line 436, in Train\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory\ninput_shapes_all_devices,\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes\nnet_protos, blob_dimensions\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1\nTraceback (most recent call last):\nFile \"resnet50_trainer.py\", line 602, in \nmain()\nFile \"resnet50_trainer.py\", line 598, in main\nTrain(args)\nFile \"resnet50_trainer.py\", line 436, in Train\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory\ninput_shapes_all_devices,\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes\nnet_protos, blob_dimensions\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1\nTraceback (most recent call last):\nFile \"resnet50_trainer.py\", line 602, in \nmain()\nFile \"resnet50_trainer.py\", line 598, in main\nTrain(args)\nFile \"resnet50_trainer.py\", line 436, in Train\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory\ninput_shapes_all_devices,\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes\nnet_protos, blob_dimensions\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1\nTraceback (most recent call last):\nFile \"resnet50_trainer.py\", line 602, in \nmain()\nFile \"resnet50_trainer.py\", line 598, in main\nTrain(args)\nFile \"resnet50_trainer.py\", line 436, in Train\ndata_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory\ninput_shapes_all_devices,\nFile \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes\nnet_protos, blob_dimensions\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1\nPrimary job  terminated normally, but 1 process returned\na non-zero exit code. Per user-direction, the job has been aborted.\n\nmpirun detected that one or more processes exited with non-zero status, thus causing\nthe job to be terminated. The first process to do so was:\nProcess name: [[43321,1],1]\nExit code:    1\nif run with single node 8 gpus ,# success !\ni have 4 nodes ,8 gpus per node\nso how to run distributed caffe2 with mpi, whether redis or nfs is necessary? my machine does not install redis and nfs\u3002", "body": "run with MPI\r\nMPI is for coordinating machine rendezvous through MPI\r\nso is it possible just using MPI as rendezvous but not redis or nfs?\r\ni do not use redis and nfs \r\nas my caffe2 is included in the container singularity \uff0c\r\ni run the under command:\r\nmpirun -np 4 singularity exec /public/DL_Data/cnic_ai_20180316.img python resnet50_trainer.py --train_data  ~/ILSVRC/ilsvrc12_train_lmdb --num_gpus 8  --batch_size 256 --num_epochs 90 --num_shards 4 --shard_id 1 --run_id 50002 \r\ngot runtime erro :\r\n\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Adding gradient operators\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nINFO:data_parallel_model:Add gradient all-reduces for SyncSGD\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nWARNING:data_parallel_model:Distributed broadcast of computed params is not implemented yet\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Post-iteration operators for updating params\r\nINFO:data_parallel_model:Calling optimizer builder function\r\nINFO:data_parallel_model:Add initial parameter sync\r\nINFO:data_parallel_model:Add initial parameter sync\r\nINFO:data_parallel_model:Add initial parameter sync\r\nINFO:data_parallel_model:Add initial parameter sync\r\nTraceback (most recent call last):\r\n  File \"resnet50_trainer.py\", line 602, in <module>\r\n    main()\r\n  File \"resnet50_trainer.py\", line 598, in main\r\n    Train(args)\r\n  File \"resnet50_trainer.py\", line 436, in Train\r\n    data_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory\r\n    input_shapes_all_devices,\r\n  File \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes\r\n    net_protos, blob_dimensions\r\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1 \r\nTraceback (most recent call last):\r\n  File \"resnet50_trainer.py\", line 602, in <module>\r\n    main()\r\n  File \"resnet50_trainer.py\", line 598, in main\r\n    Train(args)\r\n  File \"resnet50_trainer.py\", line 436, in Train\r\n    data_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory\r\n    input_shapes_all_devices,\r\n  File \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes\r\n    net_protos, blob_dimensions\r\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1 \r\nTraceback (most recent call last):\r\n  File \"resnet50_trainer.py\", line 602, in <module>\r\n    main()\r\n  File \"resnet50_trainer.py\", line 598, in main\r\n    Train(args)\r\n  File \"resnet50_trainer.py\", line 436, in Train\r\n    data_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory\r\n    input_shapes_all_devices,\r\n  File \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes\r\n    net_protos, blob_dimensions\r\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1 \r\nTraceback (most recent call last):\r\n  File \"resnet50_trainer.py\", line 602, in <module>\r\n    main()\r\n  File \"resnet50_trainer.py\", line 598, in main\r\n    Train(args)\r\n  File \"resnet50_trainer.py\", line 436, in Train\r\n    data_parallel_model.OptimizeGradientMemory(train_model, {}, set(), False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/caffe2/python/data_parallel_model.py\", line 1688, in OptimizeGradientMemory\r\n    input_shapes_all_devices,\r\n  File \"/usr/local/lib/python3.6/dist-packages/caffe2/python/workspace.py\", line 257, in InferShapesAndTypes\r\n    net_protos, blob_dimensions\r\nRuntimeError: [enforce fail at operator.cc:492] . Invalid shape inference for operator Broadcast Expected 8 outputs, but got 1 \r\n-------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n-------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun detected that one or more processes exited with non-zero status, thus causing\r\nthe job to be terminated. The first process to do so was:\r\n\r\n  Process name: [[43321,1],1]\r\n  Exit code:    1\r\n-----------------------------------------------\r\nif run with single node 8 gpus ,# success !\r\n-----------------------------------------------\r\ni have 4 nodes ,8 gpus per node\r\nso how to run distributed caffe2 with mpi, whether redis or nfs is necessary? my machine does not install redis and nfs\u3002\r\n\r\n\r\n\r\n\r\n\r\n"}