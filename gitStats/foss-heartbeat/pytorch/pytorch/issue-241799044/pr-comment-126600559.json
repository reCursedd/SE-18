{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/126600559", "pull_request_review_id": 49097636, "id": 126600559, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNjYwMDU1OQ==", "diff_hunk": "@@ -907,23 +906,45 @@ static THIndexTensor* THPTensor_(_calculateLinearIndices)(\n   }\n   THLongStorage_free(indexerSize);\n \n+#ifdef THC_GENERIC_FILE\n+  // Call GPU kernel for index calculation\n+  THCudaLongTensor *cudaIndices =\n+    THCudaLongTensor_newWithSize1d(LIBRARY_STATE indexingElements);\n+  long baseOffset = THTensor_(storageOffset)(LIBRARY_STATE indexed);\n+\n+  // Need to pass broadcast Tensors to API, pass NULL ptr for all empty\n+  // (i.e. not-advanced indexed) dims\n+  THCudaLongTensor *indexers[THTensor_(nDimension)(LIBRARY_STATE indexed.get())];\n+\n+  for (int i = 0; i < THTensor_(nDimension)(LIBRARY_STATE indexed.get()); ++i) {\n+    bool adv = flattenedBroadcasters.find(i) != flattenedBroadcasters.end();\n+    if (adv) {\n+      THCudaLongTensor *bcIndices = THCudaLongTensor_newWithSize1d(\n+        LIBRARY_STATE THLongTensor_nElement(flattenedBroadcasters[i].get()));\n+      THCudaLongTensor_copyAsyncCPU(LIBRARY_STATE bcIndices, flattenedBroadcasters[i].get());\n+      indexers[i] = bcIndices;\n+    } else {\n+      indexers[i] = NULL;\n+    }\n+  }\n+\n+  THTensor_(calculateAdvancedIndexingOffsets)(LIBRARY_STATE cudaIndices, indexed, baseOffset, indexers);\n+\n+  // Free the indexers\n+  for (int i = 0; i < THTensor_(nDimension)(LIBRARY_STATE indexed.get()); ++i) {\n+    if (indexers[i] != NULL) {\n+      THCudaLongTensor_free(LIBRARY_STATE indexers[i]);", "path": "torch/csrc/generic/Tensor.cpp", "position": null, "original_position": 39, "commit_id": "5197f58602341b8c02036283337f502913271a88", "original_commit_id": "25db5f04a3ccf5d966c7f90fa10020255713b31e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I guess it would be better to allocate a single chunk of memory and use views onto it for each dim", "created_at": "2017-07-11T05:46:19Z", "updated_at": "2018-11-23T15:34:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/2039#discussion_r126600559", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2039", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/126600559"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2039#discussion_r126600559"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2039"}}, "body_html": "<p>I guess it would be better to allocate a single chunk of memory and use views onto it for each dim</p>", "body_text": "I guess it would be better to allocate a single chunk of memory and use views onto it for each dim"}