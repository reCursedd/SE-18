{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2244", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2244/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2244/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2244/events", "html_url": "https://github.com/pytorch/pytorch/issues/2244", "id": 246548171, "node_id": "MDU6SXNzdWUyNDY1NDgxNzE=", "number": 2244, "title": "Bus error (core dumped) model share memory ", "user": {"login": "acrosson", "id": 4795661, "node_id": "MDQ6VXNlcjQ3OTU2NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4795661?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acrosson", "html_url": "https://github.com/acrosson", "followers_url": "https://api.github.com/users/acrosson/followers", "following_url": "https://api.github.com/users/acrosson/following{/other_user}", "gists_url": "https://api.github.com/users/acrosson/gists{/gist_id}", "starred_url": "https://api.github.com/users/acrosson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acrosson/subscriptions", "organizations_url": "https://api.github.com/users/acrosson/orgs", "repos_url": "https://api.github.com/users/acrosson/repos", "events_url": "https://api.github.com/users/acrosson/events{/privacy}", "received_events_url": "https://api.github.com/users/acrosson/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-07-29T22:08:07Z", "updated_at": "2018-09-13T19:08:45Z", "closed_at": "2017-07-30T07:52:00Z", "author_association": "NONE", "body_html": "<p>I'm getting a Bus error (core dumped) when using the share_memory method on a model.</p>\n<p>OS : Ubuntu 16.04<br>\nIt's happening in python 2.7 and 3.5, conda environment and hard install. I'm using the latest version from <a href=\"http://pytorch.org/\" rel=\"nofollow\">http://pytorch.org/</a>. I've also tried installing from source, same issue.</p>\n<p>I tried doing a basic test using this code:</p>\n<pre><code>import torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(2563*50, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nn = Net()\nn.share_memory()\nprint('okay')\n</code></pre>\n<p>If the input size is small it works fine, but anything greater than some threshold throws the Bus error. If I don't call share_memory() it works fine.</p>\n<p>I ran trace, here are the last few lines of the output.</p>\n<pre><code>module.py(391):             if module is not None and module not in memo:                                                                                                                                 [567/1904]\nmodule.py(392):                 memo.add(module)\nmodule.py(393):                 yield name, module\nmodule.py(378):             yield module\nmodule.py(118):             module._apply(fn)\n --- modulename: module, funcname: _apply\nmodule.py(117):         for module in self.children():\n --- modulename: module, funcname: children\nmodule.py(377):         for name, module in self.named_children():\n --- modulename: module, funcname: named_children\nmodule.py(389):         memo = set()\nmodule.py(390):         for name, module in self._modules.items():\nmodule.py(120):         for param in self._parameters.values():\nmodule.py(121):             if param is not None:\nmodule.py(124):                 param.data = fn(param.data)\n --- modulename: module, funcname: &lt;lambda&gt;\nmodule.py(468):         return self._apply(lambda t: t.share_memory_())\n --- modulename: tensor, funcname: share_memory_\ntensor.py(86):         self.storage().share_memory_()\n --- modulename: storage, funcname: share_memory_\nstorage.py(95):         from torch.multiprocessing import get_sharing_strategy\n --- modulename: _bootstrap, funcname: _handle_fromlist\n&lt;frozen importlib._bootstrap&gt;(1006): &lt;frozen importlib._bootstrap&gt;(1007): &lt;frozen importlib._bootstrap&gt;(1012): &lt;frozen importlib._bootstrap&gt;(1013): &lt;frozen importlib._bootstrap&gt;(1012): &lt;frozen importlib._bootstra\np&gt;(1025): storage.py(96):         if self.is_cuda:\nstorage.py(98):         elif get_sharing_strategy() == 'file_system':\n --- modulename: __init__, funcname: get_sharing_strategy\n__init__.py(59):     return _sharing_strategy\nstorage.py(101):             self._share_fd_()\nBus error (core dumped)\n</code></pre>\n<p>I tried running gdb, but it wont give me a full trace.</p>\n<p>I've tried creating a symbolic link to the libgomp.so.1 as I suspect it's a similar issue, but still the same error.</p>\n<p>Any suggestions? This is running inside a docker container btw.</p>", "body_text": "I'm getting a Bus error (core dumped) when using the share_memory method on a model.\nOS : Ubuntu 16.04\nIt's happening in python 2.7 and 3.5, conda environment and hard install. I'm using the latest version from http://pytorch.org/. I've also tried installing from source, same issue.\nI tried doing a basic test using this code:\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(2563*50, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nn = Net()\nn.share_memory()\nprint('okay')\n\nIf the input size is small it works fine, but anything greater than some threshold throws the Bus error. If I don't call share_memory() it works fine.\nI ran trace, here are the last few lines of the output.\nmodule.py(391):             if module is not None and module not in memo:                                                                                                                                 [567/1904]\nmodule.py(392):                 memo.add(module)\nmodule.py(393):                 yield name, module\nmodule.py(378):             yield module\nmodule.py(118):             module._apply(fn)\n --- modulename: module, funcname: _apply\nmodule.py(117):         for module in self.children():\n --- modulename: module, funcname: children\nmodule.py(377):         for name, module in self.named_children():\n --- modulename: module, funcname: named_children\nmodule.py(389):         memo = set()\nmodule.py(390):         for name, module in self._modules.items():\nmodule.py(120):         for param in self._parameters.values():\nmodule.py(121):             if param is not None:\nmodule.py(124):                 param.data = fn(param.data)\n --- modulename: module, funcname: <lambda>\nmodule.py(468):         return self._apply(lambda t: t.share_memory_())\n --- modulename: tensor, funcname: share_memory_\ntensor.py(86):         self.storage().share_memory_()\n --- modulename: storage, funcname: share_memory_\nstorage.py(95):         from torch.multiprocessing import get_sharing_strategy\n --- modulename: _bootstrap, funcname: _handle_fromlist\n<frozen importlib._bootstrap>(1006): <frozen importlib._bootstrap>(1007): <frozen importlib._bootstrap>(1012): <frozen importlib._bootstrap>(1013): <frozen importlib._bootstrap>(1012): <frozen importlib._bootstra\np>(1025): storage.py(96):         if self.is_cuda:\nstorage.py(98):         elif get_sharing_strategy() == 'file_system':\n --- modulename: __init__, funcname: get_sharing_strategy\n__init__.py(59):     return _sharing_strategy\nstorage.py(101):             self._share_fd_()\nBus error (core dumped)\n\nI tried running gdb, but it wont give me a full trace.\nI've tried creating a symbolic link to the libgomp.so.1 as I suspect it's a similar issue, but still the same error.\nAny suggestions? This is running inside a docker container btw.", "body": "I'm getting a Bus error (core dumped) when using the share_memory method on a model. \r\n\r\nOS : Ubuntu 16.04\r\nIt's happening in python 2.7 and 3.5, conda environment and hard install. I'm using the latest version from http://pytorch.org/. I've also tried installing from source, same issue. \r\n\r\nI tried doing a basic test using this code:\r\n```\r\nimport torch.nn as nn\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(2563*50, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.conv2_drop = nn.Dropout2d()\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n        x = x.view(-1, 320)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x)\r\n\r\nn = Net()\r\nn.share_memory()\r\nprint('okay')\r\n```\r\nIf the input size is small it works fine, but anything greater than some threshold throws the Bus error. If I don't call share_memory() it works fine.\r\n\r\nI ran trace, here are the last few lines of the output.\r\n```\r\nmodule.py(391):             if module is not None and module not in memo:                                                                                                                                 [567/1904]\r\nmodule.py(392):                 memo.add(module)\r\nmodule.py(393):                 yield name, module\r\nmodule.py(378):             yield module\r\nmodule.py(118):             module._apply(fn)\r\n --- modulename: module, funcname: _apply\r\nmodule.py(117):         for module in self.children():\r\n --- modulename: module, funcname: children\r\nmodule.py(377):         for name, module in self.named_children():\r\n --- modulename: module, funcname: named_children\r\nmodule.py(389):         memo = set()\r\nmodule.py(390):         for name, module in self._modules.items():\r\nmodule.py(120):         for param in self._parameters.values():\r\nmodule.py(121):             if param is not None:\r\nmodule.py(124):                 param.data = fn(param.data)\r\n --- modulename: module, funcname: <lambda>\r\nmodule.py(468):         return self._apply(lambda t: t.share_memory_())\r\n --- modulename: tensor, funcname: share_memory_\r\ntensor.py(86):         self.storage().share_memory_()\r\n --- modulename: storage, funcname: share_memory_\r\nstorage.py(95):         from torch.multiprocessing import get_sharing_strategy\r\n --- modulename: _bootstrap, funcname: _handle_fromlist\r\n<frozen importlib._bootstrap>(1006): <frozen importlib._bootstrap>(1007): <frozen importlib._bootstrap>(1012): <frozen importlib._bootstrap>(1013): <frozen importlib._bootstrap>(1012): <frozen importlib._bootstra\r\np>(1025): storage.py(96):         if self.is_cuda:\r\nstorage.py(98):         elif get_sharing_strategy() == 'file_system':\r\n --- modulename: __init__, funcname: get_sharing_strategy\r\n__init__.py(59):     return _sharing_strategy\r\nstorage.py(101):             self._share_fd_()\r\nBus error (core dumped)\r\n```\r\nI tried running gdb, but it wont give me a full trace.\r\n\r\nI've tried creating a symbolic link to the libgomp.so.1 as I suspect it's a similar issue, but still the same error.\r\n\r\nAny suggestions? This is running inside a docker container btw.\r\n"}