{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12331", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12331/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12331/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12331/events", "html_url": "https://github.com/pytorch/pytorch/issues/12331", "id": 366890524, "node_id": "MDU6SXNzdWUzNjY4OTA1MjQ=", "number": 12331, "title": "Feedback about PyTorch register_backward_hook", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-10-04T17:20:51Z", "updated_at": "2018-10-13T18:04:11Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Maxim Naumov <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=36135179\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mnaumovfb\">@mnaumovfb</a> sends in the following feedback:</p>\n<hr>\n<p>I thought I would share some feedback from my recent work with register_backward_hook function. I will describe some some inconsistencies in it that may be worth addressing in the future.</p>\n<h2>Background</h2>\n<p>PyTorch supports different hook functions, including register_hook, register_forward_hook and register_backward_hook. The former is applied to a tensor variable, while the latter two are applied to a layer module. I will discuss the latest form here. It has the following signature <a href=\"https://pytorch.org/docs/stable/nn.html\" rel=\"nofollow\">https://pytorch.org/docs/stable/nn.html</a> <code>func(layer, grad_input, grad_output)</code></p>\n<h2>Details</h2>\n<p>It is extremely important to understand the meaning of gradients to register_backwards_hook. Let a layer be defined as</p>\n<pre><code> output z  (grad_output)\n     ____|____\n    |__layer__|\n         |\n input x  (grad_input)\n</code></pre>\n<p>with overall loss E, error gradient dE/dz^(k) and weight gradient dE/dw^(k).<br>\nFirst, let us assume the simplest case: a layer with no bias. Then,</p>\n<pre><code>grad_output= [dE/dz^(k)]\ngrad_input = [dE/dz^(k-1), dE/dw^(k)]\n</code></pre>\n<h2>Inconsistencies</h2>\n<p>It seems that there are some inconsistencies in how gradients for different layers are handled by this function.</p>\n<ol>\n<li>Shape\n<ul>\n<li>in convolution layers the weight gradient has the same shape as the weights</li>\n<li>in fully connected layers the weight gradient is transpose of the weights</li>\n</ul>\n</li>\n<li>Bias\n<ul>\n<li>in convolution layers bias gradient are appended: grad_input = [dE/dz^(k-1), dE/dw^(k), dE/db^(k)]</li>\n<li>in fully connected layers bias gradient are prepended: grad_input = [dE/db^(k), dE/dz^(k-1), dE/dw^(k)]</li>\n</ul>\n</li>\n<li>Batch size &gt; 1\n<ul>\n<li>in convolution layers bias gradient corresponds to the gradient over the entire batch: grad_input = [dE/dz^(k-1), dE/dw^(k), dE/db^(k)]</li>\n<li>in fully connected layers bias gradient corresponds to the gradient per data point j=1,...,r in the batch (therefore it needs to be added to get the gradient over the entire batch): grad_input = [[dE/db^(k,1),...,dE/db^(k,r)], dE/dz^(k-1), dE/dw^(k)]</li>\n</ul>\n</li>\n</ol>\n<p>These discrepancies can make handling of different layers, bias and batch sizes quite cumbersome in the code. It would help if they were done more consistently in the future.</p>", "body_text": "Maxim Naumov @mnaumovfb sends in the following feedback:\n\nI thought I would share some feedback from my recent work with register_backward_hook function. I will describe some some inconsistencies in it that may be worth addressing in the future.\nBackground\nPyTorch supports different hook functions, including register_hook, register_forward_hook and register_backward_hook. The former is applied to a tensor variable, while the latter two are applied to a layer module. I will discuss the latest form here. It has the following signature https://pytorch.org/docs/stable/nn.html func(layer, grad_input, grad_output)\nDetails\nIt is extremely important to understand the meaning of gradients to register_backwards_hook. Let a layer be defined as\n output z  (grad_output)\n     ____|____\n    |__layer__|\n         |\n input x  (grad_input)\n\nwith overall loss E, error gradient dE/dz^(k) and weight gradient dE/dw^(k).\nFirst, let us assume the simplest case: a layer with no bias. Then,\ngrad_output= [dE/dz^(k)]\ngrad_input = [dE/dz^(k-1), dE/dw^(k)]\n\nInconsistencies\nIt seems that there are some inconsistencies in how gradients for different layers are handled by this function.\n\nShape\n\nin convolution layers the weight gradient has the same shape as the weights\nin fully connected layers the weight gradient is transpose of the weights\n\n\nBias\n\nin convolution layers bias gradient are appended: grad_input = [dE/dz^(k-1), dE/dw^(k), dE/db^(k)]\nin fully connected layers bias gradient are prepended: grad_input = [dE/db^(k), dE/dz^(k-1), dE/dw^(k)]\n\n\nBatch size > 1\n\nin convolution layers bias gradient corresponds to the gradient over the entire batch: grad_input = [dE/dz^(k-1), dE/dw^(k), dE/db^(k)]\nin fully connected layers bias gradient corresponds to the gradient per data point j=1,...,r in the batch (therefore it needs to be added to get the gradient over the entire batch): grad_input = [[dE/db^(k,1),...,dE/db^(k,r)], dE/dz^(k-1), dE/dw^(k)]\n\n\n\nThese discrepancies can make handling of different layers, bias and batch sizes quite cumbersome in the code. It would help if they were done more consistently in the future.", "body": "Maxim Naumov @mnaumovfb sends in the following feedback:\r\n\r\n----\r\n\r\nI thought I would share some feedback from my recent work with register_backward_hook function. I will describe some some inconsistencies in it that may be worth addressing in the future.\r\n\r\n## Background\r\n\r\nPyTorch supports different hook functions, including register_hook, register_forward_hook and register_backward_hook. The former is applied to a tensor variable, while the latter two are applied to a layer module. I will discuss the latest form here. It has the following signature https://pytorch.org/docs/stable/nn.html `func(layer, grad_input, grad_output)`\r\n\r\n## Details\r\n\r\nIt is extremely important to understand the meaning of gradients to register_backwards_hook. Let a layer be defined as\r\n\r\n```\r\n output z  (grad_output)\r\n     ____|____\r\n    |__layer__|\r\n         |\r\n input x  (grad_input)\r\n```\r\n\r\nwith overall loss E, error gradient dE/dz^(k) and weight gradient dE/dw^(k).\r\nFirst, let us assume the simplest case: a layer with no bias. Then, \r\n\r\n```\r\ngrad_output= [dE/dz^(k)]\r\ngrad_input = [dE/dz^(k-1), dE/dw^(k)]\r\n```\r\n\r\n## Inconsistencies\r\n\r\nIt seems that there are some inconsistencies in how gradients for different layers are handled by this function.\r\n\r\n1.  Shape\r\n    * in convolution layers the weight gradient has the same shape as the weights\r\n    * in fully connected layers the weight gradient is transpose of the weights\r\n2. Bias\r\n    * in convolution layers bias gradient are appended: grad_input = [dE/dz^(k-1), dE/dw^(k), dE/db^(k)]\r\n    * in fully connected layers bias gradient are prepended: grad_input = [dE/db^(k), dE/dz^(k-1), dE/dw^(k)]\r\n3. Batch size > 1\r\n    * in convolution layers bias gradient corresponds to the gradient over the entire batch: grad_input = [dE/dz^(k-1), dE/dw^(k), dE/db^(k)]\r\n    * in fully connected layers bias gradient corresponds to the gradient per data point j=1,...,r in the batch (therefore it needs to be added to get the gradient over the entire batch): grad_input = [[dE/db^(k,1),...,dE/db^(k,r)], dE/dz^(k-1), dE/dw^(k)]\r\n\r\nThese discrepancies can make handling of different layers, bias and batch sizes quite cumbersome in the code. It would help if they were done more consistently in the future."}