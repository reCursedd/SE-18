{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427302413", "html_url": "https://github.com/pytorch/pytorch/issues/12331#issuecomment-427302413", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12331", "id": 427302413, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzMwMjQxMw==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-05T09:28:49Z", "updated_at": "2018-10-05T09:28:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm not entirely sure if anyone fixed it, but if not, I think that as per <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"203417987\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/598\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/598/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/598\">#598</a> , module backward hooks are more broken than it looks, as they just hook into the last operation's output.<br>\nThe inconsistencies are the result of the modules being investigated consisting of \"some simple input transformation\" (like transpose, so you immediately recognize the values) and a single final autograd.</p>\n<p>The main \"fix\" I can imagine would be to wrap the module in a single autograd function (similar to checkpointing but with enabling autograd during the first forward), so we can use the backward hook on that. As in the checkpointing case, there may still be limitations of how well it works, but I must admit that I cannot offer insight here without experimentation.</p>", "body_text": "I'm not entirely sure if anyone fixed it, but if not, I think that as per #598 , module backward hooks are more broken than it looks, as they just hook into the last operation's output.\nThe inconsistencies are the result of the modules being investigated consisting of \"some simple input transformation\" (like transpose, so you immediately recognize the values) and a single final autograd.\nThe main \"fix\" I can imagine would be to wrap the module in a single autograd function (similar to checkpointing but with enabling autograd during the first forward), so we can use the backward hook on that. As in the checkpointing case, there may still be limitations of how well it works, but I must admit that I cannot offer insight here without experimentation.", "body": "I'm not entirely sure if anyone fixed it, but if not, I think that as per #598 , module backward hooks are more broken than it looks, as they just hook into the last operation's output.\r\nThe inconsistencies are the result of the modules being investigated consisting of \"some simple input transformation\" (like transpose, so you immediately recognize the values) and a single final autograd.\r\n\r\nThe main \"fix\" I can imagine would be to wrap the module in a single autograd function (similar to checkpointing but with enabling autograd during the first forward), so we can use the backward hook on that. As in the checkpointing case, there may still be limitations of how well it works, but I must admit that I cannot offer insight here without experimentation."}