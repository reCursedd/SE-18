{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2850", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2850/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2850/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2850/events", "html_url": "https://github.com/pytorch/pytorch/issues/2850", "id": 260137103, "node_id": "MDU6SXNzdWUyNjAxMzcxMDM=", "number": 2850, "title": "Learning rate schedulers are suppose to be applied at different times in training", "user": {"login": "ddkang", "id": 1894961, "node_id": "MDQ6VXNlcjE4OTQ5NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/1894961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ddkang", "html_url": "https://github.com/ddkang", "followers_url": "https://api.github.com/users/ddkang/followers", "following_url": "https://api.github.com/users/ddkang/following{/other_user}", "gists_url": "https://api.github.com/users/ddkang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ddkang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ddkang/subscriptions", "organizations_url": "https://api.github.com/users/ddkang/orgs", "repos_url": "https://api.github.com/users/ddkang/repos", "events_url": "https://api.github.com/users/ddkang/events{/privacy}", "received_events_url": "https://api.github.com/users/ddkang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-09-25T02:15:06Z", "updated_at": "2017-10-03T08:51:59Z", "closed_at": "2017-10-03T08:51:59Z", "author_association": "NONE", "body_html": "<p>This is a strange design choice.</p>\n<p>From <a href=\"http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate\" rel=\"nofollow\">http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate</a></p>\n<p>StepLR:</p>\n<pre><code>&gt;&gt;&gt; scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n&gt;&gt;&gt; for epoch in range(100):\n&gt;&gt;&gt;     **scheduler.step()**\n&gt;&gt;&gt;     train(...)\n&gt;&gt;&gt;     validate(...)\n</code></pre>\n<p>ReduceLROnPlateau:</p>\n<pre><code>&gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n&gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, 'min')\n&gt;&gt;&gt; for epoch in range(10):\n&gt;&gt;&gt;     train(...)\n&gt;&gt;&gt;     val_loss = validate(...)\n&gt;&gt;&gt;     # Note that step should be called after validate()\n&gt;&gt;&gt;     **scheduler.step(val_loss)**\n</code></pre>", "body_text": "This is a strange design choice.\nFrom http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate\nStepLR:\n>>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n>>> for epoch in range(100):\n>>>     **scheduler.step()**\n>>>     train(...)\n>>>     validate(...)\n\nReduceLROnPlateau:\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n>>> for epoch in range(10):\n>>>     train(...)\n>>>     val_loss = validate(...)\n>>>     # Note that step should be called after validate()\n>>>     **scheduler.step(val_loss)**", "body": "This is a strange design choice.\r\n\r\nFrom http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate\r\n\r\nStepLR:\r\n```\r\n>>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\r\n>>> for epoch in range(100):\r\n>>>     **scheduler.step()**\r\n>>>     train(...)\r\n>>>     validate(...)\r\n```\r\n\r\nReduceLROnPlateau:\r\n```\r\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\n>>> scheduler = ReduceLROnPlateau(optimizer, 'min')\r\n>>> for epoch in range(10):\r\n>>>     train(...)\r\n>>>     val_loss = validate(...)\r\n>>>     # Note that step should be called after validate()\r\n>>>     **scheduler.step(val_loss)**\r\n```"}