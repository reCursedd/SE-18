{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5313", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5313/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5313/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5313/events", "html_url": "https://github.com/pytorch/pytorch/pull/5313", "id": 298709271, "node_id": "MDExOlB1bGxSZXF1ZXN0MTcwMjc5MTI1", "number": 5313, "title": "converted cudaMalloc to cudaMallocManaged", "user": {"login": "ReyhaneAskari", "id": 11895018, "node_id": "MDQ6VXNlcjExODk1MDE4", "avatar_url": "https://avatars0.githubusercontent.com/u/11895018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ReyhaneAskari", "html_url": "https://github.com/ReyhaneAskari", "followers_url": "https://api.github.com/users/ReyhaneAskari/followers", "following_url": "https://api.github.com/users/ReyhaneAskari/following{/other_user}", "gists_url": "https://api.github.com/users/ReyhaneAskari/gists{/gist_id}", "starred_url": "https://api.github.com/users/ReyhaneAskari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ReyhaneAskari/subscriptions", "organizations_url": "https://api.github.com/users/ReyhaneAskari/orgs", "repos_url": "https://api.github.com/users/ReyhaneAskari/repos", "events_url": "https://api.github.com/users/ReyhaneAskari/events{/privacy}", "received_events_url": "https://api.github.com/users/ReyhaneAskari/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2018-02-20T18:57:47Z", "updated_at": "2018-04-03T12:45:41Z", "closed_at": "2018-03-23T21:23:30Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5313", "html_url": "https://github.com/pytorch/pytorch/pull/5313", "diff_url": "https://github.com/pytorch/pytorch/pull/5313.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5313.patch"}, "body_html": "<p>This PR enables CPU ram as a swap for GPU ram. I have simply changed  <code>cudaMalloc </code> with <code>cudaMallocManaged</code>.  I tested it with a simple gemm on a pascal GPU with 2G ram with the following code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n\nmy_list <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n    a <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">4096</span>, <span class=\"pl-c1\">4096</span>).astype(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>)\n    a <span class=\"pl-k\">=</span> torch.Tensor(a).cuda()\n    torch.mm(a, a)\n    my_list.append(a)</pre></div>\n<p>It breaks with an <code> N = 30</code> when the GPU ram is filled with the current pytorch implementation. But, with the new changes, when the GPU ram is at max, it stays at max and uses the cpu ram and it doesn't break even with <code>N = 100</code>.<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=180987\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nouiz\">@nouiz</a></p>", "body_text": "This PR enables CPU ram as a swap for GPU ram. I have simply changed  cudaMalloc  with cudaMallocManaged.  I tested it with a simple gemm on a pascal GPU with 2G ram with the following code:\nimport numpy as np\nimport torch\n\nmy_list = []\nfor i in range(N):\n    a = np.random.rand(4096, 4096).astype('float32')\n    a = torch.Tensor(a).cuda()\n    torch.mm(a, a)\n    my_list.append(a)\nIt breaks with an  N = 30 when the GPU ram is filled with the current pytorch implementation. But, with the new changes, when the GPU ram is at max, it stays at max and uses the cpu ram and it doesn't break even with N = 100.\n@nouiz", "body": "This PR enables CPU ram as a swap for GPU ram. I have simply changed  `cudaMalloc ` with `cudaMallocManaged`.  I tested it with a simple gemm on a pascal GPU with 2G ram with the following code:\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\n\r\nmy_list = []\r\nfor i in range(N):\r\n    a = np.random.rand(4096, 4096).astype('float32')\r\n    a = torch.Tensor(a).cuda()\r\n    torch.mm(a, a)\r\n    my_list.append(a)\r\n```\r\nIt breaks with an ` N = 30` when the GPU ram is filled with the current pytorch implementation. But, with the new changes, when the GPU ram is at max, it stays at max and uses the cpu ram and it doesn't break even with `N = 100`.\r\n@nouiz "}