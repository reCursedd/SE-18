{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367523347", "html_url": "https://github.com/pytorch/pytorch/pull/5313#issuecomment-367523347", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5313", "id": 367523347, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzUyMzM0Nw==", "user": {"login": "lamblin", "id": 178249, "node_id": "MDQ6VXNlcjE3ODI0OQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/178249?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lamblin", "html_url": "https://github.com/lamblin", "followers_url": "https://api.github.com/users/lamblin/followers", "following_url": "https://api.github.com/users/lamblin/following{/other_user}", "gists_url": "https://api.github.com/users/lamblin/gists{/gist_id}", "starred_url": "https://api.github.com/users/lamblin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lamblin/subscriptions", "organizations_url": "https://api.github.com/users/lamblin/orgs", "repos_url": "https://api.github.com/users/lamblin/repos", "events_url": "https://api.github.com/users/lamblin/events{/privacy}", "received_events_url": "https://api.github.com/users/lamblin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-22T00:16:21Z", "updated_at": "2018-02-22T00:16:21Z", "author_association": "NONE", "body_html": "<p>I agree we should have a couple use cases and benchmarks (which is why we are putting it out there), but here is why I think it has some potential:</p>\n<ul>\n<li>checkpointing and recomputing intermediate values for the backward pass seem to be useful in practice (for instance <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"287612353\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4594\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4594/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4594\">#4594</a>, or in <a href=\"https://mxnet.incubator.apache.org/versions/master/faq/env_var.html#memonger\" rel=\"nofollow\">MXNet</a> or <a href=\"https://github.com/openai/gradient-checkpointing\">TensorFlow</a>) to train deeper models or longer sequences on smaller GPUs,</li>\n<li>paging out and transferring back those results could be quicker than recomputing them (especially on NVLink systems)</li>\n<li>of course, this all depends on how cuda manages that memory. Maybe explicitly identifying the variables that should be managed would be necessary (like it is for checkpointing currently), or maybe in practice recomputing is faster than transferring.</li>\n</ul>", "body_text": "I agree we should have a couple use cases and benchmarks (which is why we are putting it out there), but here is why I think it has some potential:\n\ncheckpointing and recomputing intermediate values for the backward pass seem to be useful in practice (for instance #4594, or in MXNet or TensorFlow) to train deeper models or longer sequences on smaller GPUs,\npaging out and transferring back those results could be quicker than recomputing them (especially on NVLink systems)\nof course, this all depends on how cuda manages that memory. Maybe explicitly identifying the variables that should be managed would be necessary (like it is for checkpointing currently), or maybe in practice recomputing is faster than transferring.", "body": "I agree we should have a couple use cases and benchmarks (which is why we are putting it out there), but here is why I think it has some potential:\r\n- checkpointing and recomputing intermediate values for the backward pass seem to be useful in practice (for instance #4594, or in [MXNet](https://mxnet.incubator.apache.org/versions/master/faq/env_var.html#memonger) or [TensorFlow](https://github.com/openai/gradient-checkpointing)) to train deeper models or longer sequences on smaller GPUs,\r\n- paging out and transferring back those results could be quicker than recomputing them (especially on NVLink systems)\r\n- of course, this all depends on how cuda manages that memory. Maybe explicitly identifying the variables that should be managed would be necessary (like it is for checkpointing currently), or maybe in practice recomputing is faster than transferring."}