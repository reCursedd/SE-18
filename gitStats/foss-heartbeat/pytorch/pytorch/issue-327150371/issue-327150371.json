{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7904", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7904/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7904/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7904/events", "html_url": "https://github.com/pytorch/pytorch/issues/7904", "id": 327150371, "node_id": "MDU6SXNzdWUzMjcxNTAzNzE=", "number": 7904, "title": "TracedModules don't support parameter sharing between modules", "user": {"login": "deepbrain", "id": 10003025, "node_id": "MDQ6VXNlcjEwMDAzMDI1", "avatar_url": "https://avatars3.githubusercontent.com/u/10003025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepbrain", "html_url": "https://github.com/deepbrain", "followers_url": "https://api.github.com/users/deepbrain/followers", "following_url": "https://api.github.com/users/deepbrain/following{/other_user}", "gists_url": "https://api.github.com/users/deepbrain/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepbrain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepbrain/subscriptions", "organizations_url": "https://api.github.com/users/deepbrain/orgs", "repos_url": "https://api.github.com/users/deepbrain/repos", "events_url": "https://api.github.com/users/deepbrain/events{/privacy}", "received_events_url": "https://api.github.com/users/deepbrain/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-05-29T02:23:17Z", "updated_at": "2018-10-24T07:45:11Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I have a multihead module with 2 heads that share parameters during training. I want to use the JIT compiler to increase the performance only during the inference when only 1 head is used. When I create a jit compiled module, JIT code looks at all of the parameters, even those that will never be used in the 2nd head and raises the exception above. Instead of the exception, please create a warning here:</p>\n<p>File \"anaconda2/lib/python2.7/site-packages/torch/jit/<strong>init</strong>.py\", line 643, in check_unique<br>\nraise ValueError(\"TracedModules don't support parameter sharing between modules\")</p>", "body_text": "I have a multihead module with 2 heads that share parameters during training. I want to use the JIT compiler to increase the performance only during the inference when only 1 head is used. When I create a jit compiled module, JIT code looks at all of the parameters, even those that will never be used in the 2nd head and raises the exception above. Instead of the exception, please create a warning here:\nFile \"anaconda2/lib/python2.7/site-packages/torch/jit/init.py\", line 643, in check_unique\nraise ValueError(\"TracedModules don't support parameter sharing between modules\")", "body": "I have a multihead module with 2 heads that share parameters during training. I want to use the JIT compiler to increase the performance only during the inference when only 1 head is used. When I create a jit compiled module, JIT code looks at all of the parameters, even those that will never be used in the 2nd head and raises the exception above. Instead of the exception, please create a warning here:\r\n\r\n File \"anaconda2/lib/python2.7/site-packages/torch/jit/__init__.py\", line 643, in check_unique\r\n    raise ValueError(\"TracedModules don't support parameter sharing between modules\")\r\n"}