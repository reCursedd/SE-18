{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/337044683", "html_url": "https://github.com/pytorch/pytorch/issues/2787#issuecomment-337044683", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2787", "id": 337044683, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzA0NDY4Mw==", "user": {"login": "stsievert", "id": 1320475, "node_id": "MDQ6VXNlcjEzMjA0NzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1320475?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stsievert", "html_url": "https://github.com/stsievert", "followers_url": "https://api.github.com/users/stsievert/followers", "following_url": "https://api.github.com/users/stsievert/following{/other_user}", "gists_url": "https://api.github.com/users/stsievert/gists{/gist_id}", "starred_url": "https://api.github.com/users/stsievert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stsievert/subscriptions", "organizations_url": "https://api.github.com/users/stsievert/orgs", "repos_url": "https://api.github.com/users/stsievert/repos", "events_url": "https://api.github.com/users/stsievert/events{/privacy}", "received_events_url": "https://api.github.com/users/stsievert/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-16T21:11:23Z", "updated_at": "2017-10-16T21:33:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I want to resolve this issue for one of my classes final project. Can one of you look over <a href=\"https://www.dropbox.com/sh/7uryl07t2tkkhu9/AABgZcBgJhlPXkbXSV0_gFkga?dl=0\" rel=\"nofollow\">my proposal</a>? I'd appreciate comments on issues I might face or where to start developing.</p>\n<p>Here's the proposal text:</p>\n<blockquote>\n<h4>Motivation</h4>\n<p>Modern machine learning applications with deep learning takes a long<br>\ntime, on the order of days. This is primarily in deep neural networks,<br>\nwhich have millions of parameters to tune and require thousands or<br>\nmillions of training data.</p>\n<p>Training these deep neural networks requires communicating gradients,<br>\nwhich are as large as the models. With any high performance computation,<br>\nmemory bandwidth is a primary concern in modern computer architectures<br>\nas shown in Figure\u00a0[fig:memory-bandwidth]. This figures does not<br>\nincrease linearly as expected, indicating different caches are used as<br>\nthe model gets larger.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"src/memory-bandwidth.png\"><img src=\"src/memory-bandwidth.png\" alt=\"The time to compute the gradient and perform an optimization step as the neural network depth increases. This experiment was performed on an Amazon AWS g2.8xlarge machine with 8 GPUs.[]{data-label=&quot;fig:memory-bandwidth&quot;}\" style=\"max-width:100%;\"></a>{width=\"70.00000%\"}</p>\n<p>Reducing the model gradient size will reduce the cache use, meaning that<br>\nlarger models can be trained more quickly. Previous work has quantized<br>\nthe gradients to address precisely the memory bandwidth issue<br>\n[@alistarh2016qsgd; @wen2017terngrad]. However, both have coding schemes<br>\nwith Tensorflow and CNTK that are not easy to modify or use. CNTK<br>\nrequires building from source with a special compilation flag<br>\n[@cntk1bit] and Tensorflow requires use of a third party package<br>\n[@terngradsource] used in [@wen2017terngrad].</p>\n<p>We propose developing an coding API for PyTorch to code the the<br>\ngradients, and potentially reduce the bandwidth required to communicate<br>\nthe gradients. The work done in this project will be easily used, and<br>\nwill be used in later research. PyTorch is similar to both Tensorflow<br>\nand CNTK, and the proposed API will be relatively straightforward to<br>\nuse.</p>\n<h4>User facing API</h4>\n<p>PyTorch is accessible via the popular high-level language Python.<br>\nPyTorch makes calls to C++ to perform the core computations, meaning the<br>\nbelow API is feasible and desirable in terms of memory performance and<br>\nspeed.</p>\n<p>The simple example of the API we propose would be</p>\n<p>If this API is successfully created it will immediately be used for<br>\nlarge-scale machine learning research.</p>\n<h4>Rationale for new API function</h4>\n<p><code>param.register_hook</code> is a backwards hook and is called every time the<br>\ngradient for <code>param</code> is computed. However, <code>param.register_hook</code> does<br>\nperform certain checks to insure <code>param.size()</code> does not change. This<br>\nfunction can not be used when it\u2019s desired to compress the gradient and<br>\nremove the current gradient from memory.</p>\n<p>This project will also involve creating a forward hook, likely<br>\n<code>param.register_forward_hook</code>.</p>\n</blockquote>", "body_text": "I want to resolve this issue for one of my classes final project. Can one of you look over my proposal? I'd appreciate comments on issues I might face or where to start developing.\nHere's the proposal text:\n\nMotivation\nModern machine learning applications with deep learning takes a long\ntime, on the order of days. This is primarily in deep neural networks,\nwhich have millions of parameters to tune and require thousands or\nmillions of training data.\nTraining these deep neural networks requires communicating gradients,\nwhich are as large as the models. With any high performance computation,\nmemory bandwidth is a primary concern in modern computer architectures\nas shown in Figure\u00a0[fig:memory-bandwidth]. This figures does not\nincrease linearly as expected, indicating different caches are used as\nthe model gets larger.\n{width=\"70.00000%\"}\nReducing the model gradient size will reduce the cache use, meaning that\nlarger models can be trained more quickly. Previous work has quantized\nthe gradients to address precisely the memory bandwidth issue\n[@alistarh2016qsgd; @wen2017terngrad]. However, both have coding schemes\nwith Tensorflow and CNTK that are not easy to modify or use. CNTK\nrequires building from source with a special compilation flag\n[@cntk1bit] and Tensorflow requires use of a third party package\n[@terngradsource] used in [@wen2017terngrad].\nWe propose developing an coding API for PyTorch to code the the\ngradients, and potentially reduce the bandwidth required to communicate\nthe gradients. The work done in this project will be easily used, and\nwill be used in later research. PyTorch is similar to both Tensorflow\nand CNTK, and the proposed API will be relatively straightforward to\nuse.\nUser facing API\nPyTorch is accessible via the popular high-level language Python.\nPyTorch makes calls to C++ to perform the core computations, meaning the\nbelow API is feasible and desirable in terms of memory performance and\nspeed.\nThe simple example of the API we propose would be\nIf this API is successfully created it will immediately be used for\nlarge-scale machine learning research.\nRationale for new API function\nparam.register_hook is a backwards hook and is called every time the\ngradient for param is computed. However, param.register_hook does\nperform certain checks to insure param.size() does not change. This\nfunction can not be used when it\u2019s desired to compress the gradient and\nremove the current gradient from memory.\nThis project will also involve creating a forward hook, likely\nparam.register_forward_hook.", "body": "I want to resolve this issue for one of my classes final project. Can one of you look over [my proposal][1]? I'd appreciate comments on issues I might face or where to start developing.\r\n\r\nHere's the proposal text:\r\n\r\n> #### Motivation\r\n>\r\n> Modern machine learning applications with deep learning takes a long\r\n> time, on the order of days. This is primarily in deep neural networks,\r\n> which have millions of parameters to tune and require thousands or\r\n> millions of training data.\r\n>\r\n> Training these deep neural networks requires communicating gradients,\r\n> which are as large as the models. With any high performance computation,\r\n> memory bandwidth is a primary concern in modern computer architectures\r\n> as shown in Figure\u00a0\\[fig:memory-bandwidth\\]. This figures does not\r\n> increase linearly as expected, indicating different caches are used as\r\n> the model gets larger.\r\n>\r\n> ![The time to compute the gradient and perform an optimization step as\r\n> the neural network depth increases. This experiment was performed on an\r\n> Amazon AWS g2.8xlarge machine with 8\r\n> GPUs.[]{data-label=\"fig:memory-bandwidth\"}](src/memory-bandwidth.png){width=\"70.00000%\"}\r\n>\r\n> Reducing the model gradient size will reduce the cache use, meaning that\r\n> larger models can be trained more quickly. Previous work has quantized\r\n> the gradients to address precisely the memory bandwidth issue\r\n> [@alistarh2016qsgd; @wen2017terngrad]. However, both have coding schemes\r\n> with Tensorflow and CNTK that are not easy to modify or use. CNTK\r\n> requires building from source with a special compilation flag\r\n> [@cntk1bit] and Tensorflow requires use of a third party package\r\n> [@terngradsource] used in [@wen2017terngrad].\r\n>\r\n> We propose developing an coding API for PyTorch to code the the\r\n> gradients, and potentially reduce the bandwidth required to communicate\r\n> the gradients. The work done in this project will be easily used, and\r\n> will be used in later research. PyTorch is similar to both Tensorflow\r\n> and CNTK, and the proposed API will be relatively straightforward to\r\n> use.\r\n>\r\n> #### User facing API\r\n>\r\n> PyTorch is accessible via the popular high-level language Python.\r\n> PyTorch makes calls to C++ to perform the core computations, meaning the\r\n> below API is feasible and desirable in terms of memory performance and\r\n> speed.\r\n>\r\n> The simple example of the API we propose would be\r\n>\r\n> If this API is successfully created it will immediately be used for\r\n> large-scale machine learning research.\r\n>\r\n> #### Rationale for new API function\r\n>\r\n> `param.register_hook` is a backwards hook and is called every time the\r\n> gradient for `param` is computed. However, `param.register_hook` does\r\n> perform certain checks to insure `param.size()` does not change. This\r\n> function can not be used when it\u2019s desired to compress the gradient and\r\n> remove the current gradient from memory.\r\n>\r\n> This project will also involve creating a forward hook, likely\r\n> `param.register_forward_hook`.\r\n\r\n\r\n[1]:https://www.dropbox.com/sh/7uryl07t2tkkhu9/AABgZcBgJhlPXkbXSV0_gFkga?dl=0"}