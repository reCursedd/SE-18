{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/332634354", "html_url": "https://github.com/pytorch/pytorch/issues/2787#issuecomment-332634354", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2787", "id": 332634354, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjYzNDM1NA==", "user": {"login": "stsievert", "id": 1320475, "node_id": "MDQ6VXNlcjEzMjA0NzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1320475?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stsievert", "html_url": "https://github.com/stsievert", "followers_url": "https://api.github.com/users/stsievert/followers", "following_url": "https://api.github.com/users/stsievert/following{/other_user}", "gists_url": "https://api.github.com/users/stsievert/gists{/gist_id}", "starred_url": "https://api.github.com/users/stsievert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stsievert/subscriptions", "organizations_url": "https://api.github.com/users/stsievert/orgs", "repos_url": "https://api.github.com/users/stsievert/repos", "events_url": "https://api.github.com/users/stsievert/events{/privacy}", "received_events_url": "https://api.github.com/users/stsievert/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-27T19:45:00Z", "updated_at": "2017-09-28T16:22:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Sorry for the explanation.</p>\n<p>What I'm trying to do is similar to the <a href=\"https://arxiv.org/abs/1610.02132\" rel=\"nofollow\">quantized SGD</a> or <a href=\"http://ai2-s2-pdfs.s3.amazonaws.com/3439/a127e45fb763881f03ef3ec735a1db0e0ccc.pdf\" rel=\"nofollow\">1bit SGD</a>. It approximates the gradient by reducing the gradient precision (e.g., by reducing precision from 64 bit float to 8 bits). My use case would rather want to store a dict, or at least a tensor of a different size.</p>\n<p>This is a coding scheme that can both encode and decode the gradient. I'd like the <code>encode</code> function to be called every time the gradient (and then the gradient value will not be needed), and <code>decode</code> to be called every time the gradient is used. Something like</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">encode</span>(<span class=\"pl-smi\">var</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>In encode<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>grad<span class=\"pl-pds\">'</span></span>: var.grad, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>other<span class=\"pl-pds\">'</span></span>: stuff}\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">decode</span>(<span class=\"pl-smi\">encode_output</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>In decode<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> encode_output[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>grad<span class=\"pl-pds\">'</span></span>]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> in setup</span>\n<span class=\"pl-k\">for</span> param <span class=\"pl-k\">in</span> model.named_parameters():\n    param.encoding(encode)\n    param.decoding(decode)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> later in training</span>\nloss.backwards() \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> \"In encode\" (prints at least once)</span>\noptimizer.step() \n<span class=\"pl-c\"><span class=\"pl-c\">#</span> \"In decode\" (prints at least once)</span></pre></div>\n<p>This is the simple case where encoding/decoding does nothing, but it can grow more complex. It's assumed that <code>decode(encode(x)) \\approx x</code>, but this means that <code>encode</code> can return anything.</p>\n<p>Does this explanation make sense?</p>", "body_text": "Sorry for the explanation.\nWhat I'm trying to do is similar to the quantized SGD or 1bit SGD. It approximates the gradient by reducing the gradient precision (e.g., by reducing precision from 64 bit float to 8 bits). My use case would rather want to store a dict, or at least a tensor of a different size.\nThis is a coding scheme that can both encode and decode the gradient. I'd like the encode function to be called every time the gradient (and then the gradient value will not be needed), and decode to be called every time the gradient is used. Something like\ndef encode(var):\n    print('In encode')\n    return {'grad': var.grad, 'other': stuff}\ndef decode(encode_output):\n    print('In decode')\n    return encode_output['grad']\n\n# in setup\nfor param in model.named_parameters():\n    param.encoding(encode)\n    param.decoding(decode)\n\n# later in training\nloss.backwards() \n# \"In encode\" (prints at least once)\noptimizer.step() \n# \"In decode\" (prints at least once)\nThis is the simple case where encoding/decoding does nothing, but it can grow more complex. It's assumed that decode(encode(x)) \\approx x, but this means that encode can return anything.\nDoes this explanation make sense?", "body": "Sorry for the explanation.\r\n\r\nWhat I'm trying to do is similar to the [quantized SGD][1] or [1bit SGD][2]. It approximates the gradient by reducing the gradient precision (e.g., by reducing precision from 64 bit float to 8 bits). My use case would rather want to store a dict, or at least a tensor of a different size.\r\n\r\nThis is a coding scheme that can both encode and decode the gradient. I'd like the `encode` function to be called every time the gradient (and then the gradient value will not be needed), and `decode` to be called every time the gradient is used. Something like\r\n\r\n``` python\r\ndef encode(var):\r\n    print('In encode')\r\n    return {'grad': var.grad, 'other': stuff}\r\ndef decode(encode_output):\r\n    print('In decode')\r\n    return encode_output['grad']\r\n\r\n# in setup\r\nfor param in model.named_parameters():\r\n    param.encoding(encode)\r\n    param.decoding(decode)\r\n\r\n# later in training\r\nloss.backwards() \r\n# \"In encode\" (prints at least once)\r\noptimizer.step() \r\n# \"In decode\" (prints at least once)\r\n```\r\n\r\nThis is the simple case where encoding/decoding does nothing, but it can grow more complex. It's assumed that `decode(encode(x)) \\approx x`, but this means that `encode` can return anything.\r\n\r\nDoes this explanation make sense?\r\n\r\n[2]:http://ai2-s2-pdfs.s3.amazonaws.com/3439/a127e45fb763881f03ef3ec735a1db0e0ccc.pdf\r\n[1]:https://arxiv.org/abs/1610.02132"}