{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/337194077", "html_url": "https://github.com/pytorch/pytorch/issues/2787#issuecomment-337194077", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2787", "id": 337194077, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzE5NDA3Nw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-17T10:52:28Z", "updated_at": "2017-10-17T10:52:28Z", "author_association": "MEMBER", "body_html": "<p>I still can't see how would we implement this. It would require us to constantly encode and decode the gradients, because autograd functions are implemented in such a way that they can only work with floating point gradients contained in Variables. I'm not entirely sure what's your use case? Do you want to quantize the intermediate gradients (so sort of have a hook that is called for each intermediate grad)?</p>", "body_text": "I still can't see how would we implement this. It would require us to constantly encode and decode the gradients, because autograd functions are implemented in such a way that they can only work with floating point gradients contained in Variables. I'm not entirely sure what's your use case? Do you want to quantize the intermediate gradients (so sort of have a hook that is called for each intermediate grad)?", "body": "I still can't see how would we implement this. It would require us to constantly encode and decode the gradients, because autograd functions are implemented in such a way that they can only work with floating point gradients contained in Variables. I'm not entirely sure what's your use case? Do you want to quantize the intermediate gradients (so sort of have a hook that is called for each intermediate grad)?"}