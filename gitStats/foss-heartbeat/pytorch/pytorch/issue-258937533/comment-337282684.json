{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/337282684", "html_url": "https://github.com/pytorch/pytorch/issues/2787#issuecomment-337282684", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2787", "id": 337282684, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNzI4MjY4NA==", "user": {"login": "stsievert", "id": 1320475, "node_id": "MDQ6VXNlcjEzMjA0NzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1320475?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stsievert", "html_url": "https://github.com/stsievert", "followers_url": "https://api.github.com/users/stsievert/followers", "following_url": "https://api.github.com/users/stsievert/following{/other_user}", "gists_url": "https://api.github.com/users/stsievert/gists{/gist_id}", "starred_url": "https://api.github.com/users/stsievert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stsievert/subscriptions", "organizations_url": "https://api.github.com/users/stsievert/orgs", "repos_url": "https://api.github.com/users/stsievert/repos", "events_url": "https://api.github.com/users/stsievert/events{/privacy}", "received_events_url": "https://api.github.com/users/stsievert/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-17T16:09:29Z", "updated_at": "2017-10-17T16:09:29Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>I'm not entirely sure what's your use case?</p>\n</blockquote>\n<p>My use case mirrors the one in <a href=\"https://arxiv.org/abs/1610.02132\" rel=\"nofollow\">the QSGD paper</a>. That is, I would like to encode/decode gradients to preserve bandwidth on the CPU\u2013GPU connection to avoid communication bottlenecks.</p>\n<p>Bandwidth is limited is limited between GPU\u2013GPU and CPU\u2013GPU, leading to communication bottlenecks. I'd like to perform something like below to avoid this:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1320475/31675449-4b24c6c6-b32a-11e7-99bb-c756c653370a.png\"><img width=\"879\" alt=\"screen shot 2017-10-17 at 10 59 32 am\" src=\"https://user-images.githubusercontent.com/1320475/31675449-4b24c6c6-b32a-11e7-99bb-c756c653370a.png\" style=\"max-width:100%;\"></a></p>\n<p>All blocks exist in PyTorch right now except the encode/decode blocks, and I don't want to touch them. For my use case device1 and device2 are on the same machine (and it looks like they're both GPUs in the QSGD paper).</p>\n<blockquote>\n<p>Do you want to quantize the intermediate gradients (so sort of have a hook that is called for each intermediate grad)?</p>\n</blockquote>\n<p>No, I do not want to quantize each intermediate gradient. I want a forward hook that is only called after computation of the gradient is finished.</p>", "body_text": "I'm not entirely sure what's your use case?\n\nMy use case mirrors the one in the QSGD paper. That is, I would like to encode/decode gradients to preserve bandwidth on the CPU\u2013GPU connection to avoid communication bottlenecks.\nBandwidth is limited is limited between GPU\u2013GPU and CPU\u2013GPU, leading to communication bottlenecks. I'd like to perform something like below to avoid this:\n\nAll blocks exist in PyTorch right now except the encode/decode blocks, and I don't want to touch them. For my use case device1 and device2 are on the same machine (and it looks like they're both GPUs in the QSGD paper).\n\nDo you want to quantize the intermediate gradients (so sort of have a hook that is called for each intermediate grad)?\n\nNo, I do not want to quantize each intermediate gradient. I want a forward hook that is only called after computation of the gradient is finished.", "body": ">  I'm not entirely sure what's your use case?\r\n\r\nMy use case mirrors the one in [the QSGD paper][qsgd]. That is, I would like to encode/decode gradients to preserve bandwidth on the CPU\u2013GPU connection to avoid communication bottlenecks.\r\n\r\nBandwidth is limited is limited between GPU\u2013GPU and CPU\u2013GPU, leading to communication bottlenecks. I'd like to perform something like below to avoid this:\r\n\r\n<img width=\"879\" alt=\"screen shot 2017-10-17 at 10 59 32 am\" src=\"https://user-images.githubusercontent.com/1320475/31675449-4b24c6c6-b32a-11e7-99bb-c756c653370a.png\">\r\n\r\nAll blocks exist in PyTorch right now except the encode/decode blocks, and I don't want to touch them. For my use case device1 and device2 are on the same machine (and it looks like they're both GPUs in the QSGD paper).\r\n\r\n> Do you want to quantize the intermediate gradients (so sort of have a hook that is called for each intermediate grad)?\r\n\r\nNo, I do not want to quantize each intermediate gradient. I want a forward hook that is only called after computation of the gradient is finished.\r\n\r\n[qsgd]:https://arxiv.org/abs/1610.02132"}