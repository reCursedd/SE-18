{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/360340855", "html_url": "https://github.com/pytorch/pytorch/issues/4808#issuecomment-360340855", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4808", "id": 360340855, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDM0MDg1NQ==", "user": {"login": "nouiz", "id": 180987, "node_id": "MDQ6VXNlcjE4MDk4Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/180987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nouiz", "html_url": "https://github.com/nouiz", "followers_url": "https://api.github.com/users/nouiz/followers", "following_url": "https://api.github.com/users/nouiz/following{/other_user}", "gists_url": "https://api.github.com/users/nouiz/gists{/gist_id}", "starred_url": "https://api.github.com/users/nouiz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nouiz/subscriptions", "organizations_url": "https://api.github.com/users/nouiz/orgs", "repos_url": "https://api.github.com/users/nouiz/repos", "events_url": "https://api.github.com/users/nouiz/events{/privacy}", "received_events_url": "https://api.github.com/users/nouiz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-25T02:26:48Z", "updated_at": "2018-01-25T02:26:48Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Where someone here able to reproduce the speed up from float16 with any\nconvolution model? If so, is the code available somewhere?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Jan 24, 2018 at 2:20 PM Reyhane Askari ***@***.***&gt; wrote:\n Thanks <a class=\"user-mention\" href=\"https://github.com/csarofeen\">@csarofeen</a> &lt;<a href=\"https://github.com/csarofeen\">https://github.com/csarofeen</a>&gt; . I wasn't careful with\n the GPU synchronization. I know the parameterization of convolution is\n weird because I wanted to have my matrices the exact same size as I had in\n my Fully connected network (which was basically the gemm) that I had seen\n the speedup with float16.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"290954063\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4808\" href=\"https://github.com/pytorch/pytorch/issues/4808#issuecomment-360243852\">#4808 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AALC-9NDFJJszJi-5Vw33pcKlGEYda4Jks5tN4KEgaJpZM4RqJCY\">https://github.com/notifications/unsubscribe-auth/AALC-9NDFJJszJi-5Vw33pcKlGEYda4Jks5tN4KEgaJpZM4RqJCY</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Where someone here able to reproduce the speed up from float16 with any\nconvolution model? If so, is the code available somewhere?\n\u2026\nOn Wed, Jan 24, 2018 at 2:20 PM Reyhane Askari ***@***.***> wrote:\n Thanks @csarofeen <https://github.com/csarofeen> . I wasn't careful with\n the GPU synchronization. I know the parameterization of convolution is\n weird because I wanted to have my matrices the exact same size as I had in\n my Fully connected network (which was basically the gemm) that I had seen\n the speedup with float16.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#4808 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AALC-9NDFJJszJi-5Vw33pcKlGEYda4Jks5tN4KEgaJpZM4RqJCY>\n .", "body": "Where someone here able to reproduce the speed up from float16 with any\nconvolution model? If so, is the code available somewhere?\n\nOn Wed, Jan 24, 2018 at 2:20 PM Reyhane Askari <notifications@github.com>\nwrote:\n\n> Thanks @csarofeen <https://github.com/csarofeen> . I wasn't careful with\n> the GPU synchronization. I know the parameterization of convolution is\n> weird because I wanted to have my matrices the exact same size as I had in\n> my Fully connected network (which was basically the gemm) that I had seen\n> the speedup with float16.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/4808#issuecomment-360243852>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AALC-9NDFJJszJi-5Vw33pcKlGEYda4Jks5tN4KEgaJpZM4RqJCY>\n> .\n>\n"}