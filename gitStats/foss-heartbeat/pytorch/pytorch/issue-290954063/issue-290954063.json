{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4808", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4808/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4808/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4808/events", "html_url": "https://github.com/pytorch/pytorch/issues/4808", "id": 290954063, "node_id": "MDU6SXNzdWUyOTA5NTQwNjM=", "number": 4808, "title": "Convolution with float 16 has no speedup vs float32", "user": {"login": "ReyhaneAskari", "id": 11895018, "node_id": "MDQ6VXNlcjExODk1MDE4", "avatar_url": "https://avatars0.githubusercontent.com/u/11895018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ReyhaneAskari", "html_url": "https://github.com/ReyhaneAskari", "followers_url": "https://api.github.com/users/ReyhaneAskari/followers", "following_url": "https://api.github.com/users/ReyhaneAskari/following{/other_user}", "gists_url": "https://api.github.com/users/ReyhaneAskari/gists{/gist_id}", "starred_url": "https://api.github.com/users/ReyhaneAskari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ReyhaneAskari/subscriptions", "organizations_url": "https://api.github.com/users/ReyhaneAskari/orgs", "repos_url": "https://api.github.com/users/ReyhaneAskari/repos", "events_url": "https://api.github.com/users/ReyhaneAskari/events{/privacy}", "received_events_url": "https://api.github.com/users/ReyhaneAskari/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-01-23T18:56:45Z", "updated_at": "2018-01-25T20:27:43Z", "closed_at": "2018-01-25T20:27:43Z", "author_association": "NONE", "body_html": "<p>I have tried to use mixed float precision on pytorch. First I followed <a href=\"https://docs.nvidia.com/deeplearning/sdk/pdf/Training-Mixed-Precision-User-Guide.pdf\" rel=\"nofollow\">this guide </a> to train pytorch's <a href=\"https://github.com/pytorch/examples/tree/master/word_language_model\">word_language_model</a>.  And here is the results on Titan V, with cuda 9 and cudnn 7:</p>\n<ul>\n<li><strong>mixed float</strong>: 164m38.965s</li>\n<li><strong>float32</strong> : 247m46.186s</li>\n</ul>\n<p>So it's less than 2X speed up but we might have a bottle neck somewhere else.</p>\n<p>I then tried to time a <code>gemm</code> with <a href=\"https://github.com/ReyhaneAskari/pytorch_experiments/blob/32317adb8986f32d3ed4c31f751aef9a036fb26b/torch_float16.py\">this code</a>. It's a simple fully connected network with no non-linearity. And this time I actually did see the speed up:</p>\n<ul>\n<li><strong>mixed float</strong>: 100 sec + 38958 microsec</li>\n<li><strong>float32</strong> : 710 sec + 302950 microsec</li>\n</ul>\n<p>It's almost 7X faster. So I am sure that Tensor Core works.</p>\n<p>I know the speedup also depends on the size of the matrix so I tried to time a simple convolution but I made sure that the size of the matrices are the exact same sizes as my FC case. So <a href=\"https://github.com/ReyhaneAskari/pytorch_experiments/blob/master/fp_32_16.py\">here is the code for convolution</a> and this was the result:</p>\n<ul>\n<li><strong>mixed float</strong>: 1.0285618305206299s</li>\n<li><strong>float32</strong> :  \u200b1.0206332206726074s</li>\n</ul>\n<p>So with convolution there is no speedup. So I would like to know why the convolution does not have the speedup. I'm willing to work on this if you could give me some tips. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=180987\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nouiz\">@nouiz</a> is also supervising me.</p>", "body_text": "I have tried to use mixed float precision on pytorch. First I followed this guide  to train pytorch's word_language_model.  And here is the results on Titan V, with cuda 9 and cudnn 7:\n\nmixed float: 164m38.965s\nfloat32 : 247m46.186s\n\nSo it's less than 2X speed up but we might have a bottle neck somewhere else.\nI then tried to time a gemm with this code. It's a simple fully connected network with no non-linearity. And this time I actually did see the speed up:\n\nmixed float: 100 sec + 38958 microsec\nfloat32 : 710 sec + 302950 microsec\n\nIt's almost 7X faster. So I am sure that Tensor Core works.\nI know the speedup also depends on the size of the matrix so I tried to time a simple convolution but I made sure that the size of the matrices are the exact same sizes as my FC case. So here is the code for convolution and this was the result:\n\nmixed float: 1.0285618305206299s\nfloat32 :  \u200b1.0206332206726074s\n\nSo with convolution there is no speedup. So I would like to know why the convolution does not have the speedup. I'm willing to work on this if you could give me some tips. @nouiz is also supervising me.", "body": "I have tried to use mixed float precision on pytorch. First I followed [this guide ](https://docs.nvidia.com/deeplearning/sdk/pdf/Training-Mixed-Precision-User-Guide.pdf) to train pytorch's [word_language_model](https://github.com/pytorch/examples/tree/master/word_language_model).  And here is the results on Titan V, with cuda 9 and cudnn 7: \r\n\r\n- **mixed float**: 164m38.965s\r\n- **float32** : 247m46.186s\r\n\r\nSo it's less than 2X speed up but we might have a bottle neck somewhere else. \r\n\r\nI then tried to time a `gemm` with [this code](https://github.com/ReyhaneAskari/pytorch_experiments/blob/32317adb8986f32d3ed4c31f751aef9a036fb26b/torch_float16.py). It's a simple fully connected network with no non-linearity. And this time I actually did see the speed up:\r\n- **mixed float**: 100 sec + 38958 microsec\r\n- **float32** : 710 sec + 302950 microsec\r\n\r\nIt's almost 7X faster. So I am sure that Tensor Core works. \r\n\r\nI know the speedup also depends on the size of the matrix so I tried to time a simple convolution but I made sure that the size of the matrices are the exact same sizes as my FC case. So [here is the code for convolution](https://github.com/ReyhaneAskari/pytorch_experiments/blob/master/fp_32_16.py) and this was the result:\r\n- **mixed float**: 1.0285618305206299s\r\n- **float32** :  \u200b1.0206332206726074s\r\n\r\nSo with convolution there is no speedup. So I would like to know why the convolution does not have the speedup. I'm willing to work on this if you could give me some tips. @nouiz is also supervising me. "}