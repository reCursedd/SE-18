{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8281", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8281/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8281/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8281/events", "html_url": "https://github.com/pytorch/pytorch/issues/8281", "id": 330593061, "node_id": "MDU6SXNzdWUzMzA1OTMwNjE=", "number": 8281, "title": "Does torch.Tensor work well with share memory list?", "user": {"login": "kunkunpang", "id": 20398426, "node_id": "MDQ6VXNlcjIwMzk4NDI2", "avatar_url": "https://avatars2.githubusercontent.com/u/20398426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kunkunpang", "html_url": "https://github.com/kunkunpang", "followers_url": "https://api.github.com/users/kunkunpang/followers", "following_url": "https://api.github.com/users/kunkunpang/following{/other_user}", "gists_url": "https://api.github.com/users/kunkunpang/gists{/gist_id}", "starred_url": "https://api.github.com/users/kunkunpang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kunkunpang/subscriptions", "organizations_url": "https://api.github.com/users/kunkunpang/orgs", "repos_url": "https://api.github.com/users/kunkunpang/repos", "events_url": "https://api.github.com/users/kunkunpang/events{/privacy}", "received_events_url": "https://api.github.com/users/kunkunpang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-08T09:50:33Z", "updated_at": "2018-06-08T10:53:10Z", "closed_at": "2018-06-08T10:53:10Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I was trying to share a list with large number of elements using multiprocess. However this is causing an error:</p>\n<p>'received 0 item with ancdata'.</p>\n<p>Why does this work using a numpy array but fails with pytorch array?</p>\n<h2>Code example</h2>\n<p>We can use the code below to reproduce the problem.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> multiprocessing <span class=\"pl-k\">as</span> mp\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">reproduce_bug</span>():\n    a<span class=\"pl-k\">=</span>[]              \n    manager<span class=\"pl-k\">=</span>mp.Manager()\n    shm_list <span class=\"pl-k\">=</span> manager.list([])\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):    \n        a.append(torch.rand(<span class=\"pl-c1\">100</span>))\n        shm_list.append(torch.rand(<span class=\"pl-c1\">100</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> where it cause the error</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pytorch!<span class=\"pl-pds\">'</span></span>)\n    \n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Using numpy everything is fine</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bugfree</span>():\n    a<span class=\"pl-k\">=</span>[]\n    manager<span class=\"pl-k\">=</span>mp.Manager()\n    shm_list <span class=\"pl-k\">=</span> manager.list([])\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):\n        a.append(np.random.rand(<span class=\"pl-c1\">100</span>))\n        shm_list.append(np.random.rand(<span class=\"pl-c1\">100</span>))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>numpy!<span class=\"pl-pds\">'</span></span>)\n\nbugfree()\nreproduce_bug()</pre></div>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: pytorch</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS:ubuntu</li>\n<li>PyTorch version: 0.4.0</li>\n<li>Python version: 3.6.4/3.6.3</li>\n<li>CUDA/cuDNN version:8.0/6.0</li>\n<li>GPU models and configuration: NA</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version: NA</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "Issue description\nI was trying to share a list with large number of elements using multiprocess. However this is causing an error:\n'received 0 item with ancdata'.\nWhy does this work using a numpy array but fails with pytorch array?\nCode example\nWe can use the code below to reproduce the problem.\nimport multiprocessing as mp\nimport torch\nimport numpy as np\n\ndef reproduce_bug():\n    a=[]              \n    manager=mp.Manager()\n    shm_list = manager.list([])\n    for i in range(10000):    \n        a.append(torch.rand(100))\n        shm_list.append(torch.rand(100)) # where it cause the error\n    print('pytorch!')\n    \n\n# Using numpy everything is fine\ndef bugfree():\n    a=[]\n    manager=mp.Manager()\n    shm_list = manager.list([])\n    for i in range(10000):\n        a.append(np.random.rand(100))\n        shm_list.append(np.random.rand(100))\n    print('numpy!')\n\nbugfree()\nreproduce_bug()\nSystem Info\n\nPyTorch or Caffe2: pytorch\nHow you installed PyTorch (conda, pip, source): conda\nBuild command you used (if compiling from source):\nOS:ubuntu\nPyTorch version: 0.4.0\nPython version: 3.6.4/3.6.3\nCUDA/cuDNN version:8.0/6.0\nGPU models and configuration: NA\nGCC version (if compiling from source):\nCMake version: NA\nVersions of any other relevant libraries:", "body": "## Issue description\r\n\r\nI was trying to share a list with large number of elements using multiprocess. However this is causing an error: \r\n\r\n'received 0 item with ancdata'. \r\n\r\nWhy does this work using a numpy array but fails with pytorch array?\r\n\r\n## Code example\r\n\r\nWe can use the code below to reproduce the problem.\r\n\r\n```python\r\nimport multiprocessing as mp\r\nimport torch\r\nimport numpy as np\r\n\r\ndef reproduce_bug():\r\n    a=[]              \r\n    manager=mp.Manager()\r\n    shm_list = manager.list([])\r\n    for i in range(10000):    \r\n        a.append(torch.rand(100))\r\n        shm_list.append(torch.rand(100)) # where it cause the error\r\n    print('pytorch!')\r\n    \r\n\r\n# Using numpy everything is fine\r\ndef bugfree():\r\n    a=[]\r\n    manager=mp.Manager()\r\n    shm_list = manager.list([])\r\n    for i in range(10000):\r\n        a.append(np.random.rand(100))\r\n        shm_list.append(np.random.rand(100))\r\n    print('numpy!')\r\n\r\nbugfree()\r\nreproduce_bug()\r\n```\r\n\r\n## System Info\r\n- PyTorch or Caffe2: pytorch\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Build command you used (if compiling from source):\r\n- OS:ubuntu\r\n- PyTorch version: 0.4.0\r\n- Python version: 3.6.4/3.6.3\r\n- CUDA/cuDNN version:8.0/6.0\r\n- GPU models and configuration: NA\r\n- GCC version (if compiling from source): \r\n- CMake version: NA\r\n- Versions of any other relevant libraries:\r\n"}