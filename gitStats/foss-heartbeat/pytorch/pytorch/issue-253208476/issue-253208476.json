{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2548", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2548/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2548/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2548/events", "html_url": "https://github.com/pytorch/pytorch/issues/2548", "id": 253208476, "node_id": "MDU6SXNzdWUyNTMyMDg0NzY=", "number": 2548, "title": "MultiStepLR for multi gamma", "user": {"login": "robotcator", "id": 5803439, "node_id": "MDQ6VXNlcjU4MDM0Mzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/5803439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robotcator", "html_url": "https://github.com/robotcator", "followers_url": "https://api.github.com/users/robotcator/followers", "following_url": "https://api.github.com/users/robotcator/following{/other_user}", "gists_url": "https://api.github.com/users/robotcator/gists{/gist_id}", "starred_url": "https://api.github.com/users/robotcator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robotcator/subscriptions", "organizations_url": "https://api.github.com/users/robotcator/orgs", "repos_url": "https://api.github.com/users/robotcator/repos", "events_url": "https://api.github.com/users/robotcator/events{/privacy}", "received_events_url": "https://api.github.com/users/robotcator/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-28T02:01:58Z", "updated_at": "2017-08-30T00:37:18Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi, sometimes I may confront with the situation that schedule the learning rate with multi <code>gamma</code>. For the current MultiStepLR, the optimizer uses lr for all groups.<br>\nFor example:</p>\n<pre><code>&gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30\n&gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 80\n&gt;&gt;&gt; # lr = 0.0005   if epoch &gt;= 80\n</code></pre>\n<p>But we may have a list <code>gammas</code>, such as milestones=[30, 80], gamma=[0.1, 0.2]</p>\n<pre><code>&gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30\n&gt;&gt;&gt; # lr = 0.05*0.1    if 30 &lt;= epoch &lt; 80\n&gt;&gt;&gt; # lr = 0.05*0.1*0.2   if epoch &gt;= 80\n</code></pre>\n<p>This is my code for this feature:</p>\n<pre><code>from torch.optim.lr_scheduler import _LRScheduler\nclass myMultiStepLR(_LRScheduler):\n    def __init__(self, optimizer, milestones, gamma, last_epoch=-1):\n        if not list(milestones) == sorted(milestones):\n            raise ValueError('Milestones should be a list of'\n                             ' increasing integers. Got {}', milestones)\n        self.milestones = milestones\n        self.gamma = gamma\n        super(myMultiStepLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch in self.milestones:\n            index = self.milestones.index(self.last_epoch)\n            _gamma = self.gamma[index]\n        else:\n            _gamma = 1.0\n        return [base_lr * _gamma for base_lr in self.base_lrs]\n</code></pre>", "body_text": "Hi, sometimes I may confront with the situation that schedule the learning rate with multi gamma. For the current MultiStepLR, the optimizer uses lr for all groups.\nFor example:\n>>> # lr = 0.05     if epoch < 30\n>>> # lr = 0.005    if 30 <= epoch < 80\n>>> # lr = 0.0005   if epoch >= 80\n\nBut we may have a list gammas, such as milestones=[30, 80], gamma=[0.1, 0.2]\n>>> # lr = 0.05     if epoch < 30\n>>> # lr = 0.05*0.1    if 30 <= epoch < 80\n>>> # lr = 0.05*0.1*0.2   if epoch >= 80\n\nThis is my code for this feature:\nfrom torch.optim.lr_scheduler import _LRScheduler\nclass myMultiStepLR(_LRScheduler):\n    def __init__(self, optimizer, milestones, gamma, last_epoch=-1):\n        if not list(milestones) == sorted(milestones):\n            raise ValueError('Milestones should be a list of'\n                             ' increasing integers. Got {}', milestones)\n        self.milestones = milestones\n        self.gamma = gamma\n        super(myMultiStepLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        if self.last_epoch in self.milestones:\n            index = self.milestones.index(self.last_epoch)\n            _gamma = self.gamma[index]\n        else:\n            _gamma = 1.0\n        return [base_lr * _gamma for base_lr in self.base_lrs]", "body": "Hi, sometimes I may confront with the situation that schedule the learning rate with multi `gamma`. For the current MultiStepLR, the optimizer uses lr for all groups.\r\nFor example:\r\n```\r\n>>> # lr = 0.05     if epoch < 30\r\n>>> # lr = 0.005    if 30 <= epoch < 80\r\n>>> # lr = 0.0005   if epoch >= 80\r\n```\r\nBut we may have a list `gammas`, such as milestones=[30, 80], gamma=[0.1, 0.2]\r\n```\r\n>>> # lr = 0.05     if epoch < 30\r\n>>> # lr = 0.05*0.1    if 30 <= epoch < 80\r\n>>> # lr = 0.05*0.1*0.2   if epoch >= 80\r\n```\r\nThis is my code for this feature:\r\n```\r\nfrom torch.optim.lr_scheduler import _LRScheduler\r\nclass myMultiStepLR(_LRScheduler):\r\n    def __init__(self, optimizer, milestones, gamma, last_epoch=-1):\r\n        if not list(milestones) == sorted(milestones):\r\n            raise ValueError('Milestones should be a list of'\r\n                             ' increasing integers. Got {}', milestones)\r\n        self.milestones = milestones\r\n        self.gamma = gamma\r\n        super(myMultiStepLR, self).__init__(optimizer, last_epoch)\r\n\r\n    def get_lr(self):\r\n        if self.last_epoch in self.milestones:\r\n            index = self.milestones.index(self.last_epoch)\r\n            _gamma = self.gamma[index]\r\n        else:\r\n            _gamma = 1.0\r\n        return [base_lr * _gamma for base_lr in self.base_lrs]\r\n```"}