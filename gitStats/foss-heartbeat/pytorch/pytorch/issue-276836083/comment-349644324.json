{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/349644324", "html_url": "https://github.com/pytorch/pytorch/issues/3883#issuecomment-349644324", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3883", "id": 349644324, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTY0NDMyNA==", "user": {"login": "ducksoup", "id": 1279573, "node_id": "MDQ6VXNlcjEyNzk1NzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1279573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ducksoup", "html_url": "https://github.com/ducksoup", "followers_url": "https://api.github.com/users/ducksoup/followers", "following_url": "https://api.github.com/users/ducksoup/following{/other_user}", "gists_url": "https://api.github.com/users/ducksoup/gists{/gist_id}", "starred_url": "https://api.github.com/users/ducksoup/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ducksoup/subscriptions", "organizations_url": "https://api.github.com/users/ducksoup/orgs", "repos_url": "https://api.github.com/users/ducksoup/repos", "events_url": "https://api.github.com/users/ducksoup/events{/privacy}", "received_events_url": "https://api.github.com/users/ducksoup/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-06T13:47:59Z", "updated_at": "2017-12-06T13:47:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> We've also been struggling with this issues for a long time. Unfortunately, finding which part of our code specifically is triggering the assert (which used to be a segfault before <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"271032403\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3466\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3466/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3466\">#3466</a>) has proven extremely hard and, as of now, unsuccessful. In fact, we never submitted a bug report as we couldn't create a self-contained piece of code to trigger it.</p>\n<p>I'll try to summarize all the information we've gathered:</p>\n<ul>\n<li>We know for a fact that the issue appeared sometime after this commit: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/cd9b27231b51633e76e28b6a34002ab83b0660fc/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/cd9b27231b51633e76e28b6a34002ab83b0660fc\"><tt>cd9b272</tt></a>. All our code works perfectly fine on <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/cd9b27231b51633e76e28b6a34002ab83b0660fc/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/cd9b27231b51633e76e28b6a34002ab83b0660fc\"><tt>cd9b272</tt></a> or v0.2.</li>\n<li>The assert is always triggered during the backward pass, but the exact moment when it happens seems to be completely random: when training over Imagenet we've seen this after 50 iterations as well as after 5000. This makes it super-hard to understand whether a particular piece of code is triggering the assert or not, as you can never be sure how many iterations are enough to exclude that the assert will ever trigger!</li>\n<li>The issue seems to be somehow related to the use of multiple GPUs (via <code>nn.DataParallel</code>): we've never observed it while training with a single GPU.</li>\n<li>The issue seems also to be related to the use of user-defined <code>autograd.Function</code> classes: we've never seen it when training networks composed of \"standard\" Pytorch functions / modules only...</li>\n<li>... however, simply running forward / backward passes with our custom functions doesn't seem to be enough to trigger the assertion. In our experience, working with \"complex\" computational graphs increases the likelihood of encountering this issue.</li>\n</ul>\n<p>Recently we've been able to re-create the issue using <a href=\"https://github.com/pytorch/examples/tree/master/imagenet\">https://github.com/pytorch/examples/tree/master/imagenet</a> and a modified version of <a href=\"https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\">https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py</a>, where we replace some of the layers with our custom-made ones. Unfortunately I can't share the code for now (our paper describing the new layers is still not out), but I'll come back to you as soon as I can!</p>\n<p>Some additional information: all our experiments are on Ubuntu 16.04 using CUDA 8, CUDNN 7 and TITAN X and Xp GPUs.</p>", "body_text": "@soumith We've also been struggling with this issues for a long time. Unfortunately, finding which part of our code specifically is triggering the assert (which used to be a segfault before #3466) has proven extremely hard and, as of now, unsuccessful. In fact, we never submitted a bug report as we couldn't create a self-contained piece of code to trigger it.\nI'll try to summarize all the information we've gathered:\n\nWe know for a fact that the issue appeared sometime after this commit: cd9b272. All our code works perfectly fine on cd9b272 or v0.2.\nThe assert is always triggered during the backward pass, but the exact moment when it happens seems to be completely random: when training over Imagenet we've seen this after 50 iterations as well as after 5000. This makes it super-hard to understand whether a particular piece of code is triggering the assert or not, as you can never be sure how many iterations are enough to exclude that the assert will ever trigger!\nThe issue seems to be somehow related to the use of multiple GPUs (via nn.DataParallel): we've never observed it while training with a single GPU.\nThe issue seems also to be related to the use of user-defined autograd.Function classes: we've never seen it when training networks composed of \"standard\" Pytorch functions / modules only...\n... however, simply running forward / backward passes with our custom functions doesn't seem to be enough to trigger the assertion. In our experience, working with \"complex\" computational graphs increases the likelihood of encountering this issue.\n\nRecently we've been able to re-create the issue using https://github.com/pytorch/examples/tree/master/imagenet and a modified version of https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py, where we replace some of the layers with our custom-made ones. Unfortunately I can't share the code for now (our paper describing the new layers is still not out), but I'll come back to you as soon as I can!\nSome additional information: all our experiments are on Ubuntu 16.04 using CUDA 8, CUDNN 7 and TITAN X and Xp GPUs.", "body": "@soumith We've also been struggling with this issues for a long time. Unfortunately, finding which part of our code specifically is triggering the assert (which used to be a segfault before #3466) has proven extremely hard and, as of now, unsuccessful. In fact, we never submitted a bug report as we couldn't create a self-contained piece of code to trigger it.\r\n\r\nI'll try to summarize all the information we've gathered:\r\n\r\n- We know for a fact that the issue appeared sometime after this commit: cd9b27231b51633e76e28b6a34002ab83b0660fc. All our code works perfectly fine on cd9b27231b51633e76e28b6a34002ab83b0660fc or v0.2.\r\n- The assert is always triggered during the backward pass, but the exact moment when it happens seems to be completely random: when training over Imagenet we've seen this after 50 iterations as well as after 5000. This makes it super-hard to understand whether a particular piece of code is triggering the assert or not, as you can never be sure how many iterations are enough to exclude that the assert will ever trigger!\r\n- The issue seems to be somehow related to the use of multiple GPUs (via `nn.DataParallel`): we've never observed it while training with a single GPU.\r\n- The issue seems also to be related to the use of user-defined `autograd.Function` classes: we've never seen it when training networks composed of \"standard\" Pytorch functions / modules only...\r\n- ... however, simply running forward / backward passes with our custom functions doesn't seem to be enough to trigger the assertion. In our experience, working with \"complex\" computational graphs increases the likelihood of encountering this issue.\r\n\r\nRecently we've been able to re-create the issue using https://github.com/pytorch/examples/tree/master/imagenet and a modified version of https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py, where we replace some of the layers with our custom-made ones. Unfortunately I can't share the code for now (our paper describing the new layers is still not out), but I'll come back to you as soon as I can!\r\n\r\nSome additional information: all our experiments are on Ubuntu 16.04 using CUDA 8, CUDNN 7 and TITAN X and Xp GPUs."}