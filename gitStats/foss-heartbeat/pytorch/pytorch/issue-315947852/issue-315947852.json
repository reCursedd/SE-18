{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6770", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6770/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6770/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6770/events", "html_url": "https://github.com/pytorch/pytorch/pull/6770", "id": 315947852, "node_id": "MDExOlB1bGxSZXF1ZXN0MTgyODI5MDcx", "number": 6770, "title": "Fix performance regression on simple cases of indexing", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-19T16:18:21Z", "updated_at": "2018-11-23T15:42:53Z", "closed_at": "2018-04-20T07:05:07Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6770", "html_url": "https://github.com/pytorch/pytorch/pull/6770", "diff_url": "https://github.com/pytorch/pytorch/pull/6770.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6770.patch"}, "body_html": "<p>This PR fixes some of the performance regression on Tensors going from 0.3 to 0.4.</p>\n<p>It dispatches the operations to the old kernels that we have, whenever possible.<br>\nThis means that we use <code>index_select</code> / <code>masked_select</code> / <code>index_fill_</code> or <code>masked_fill_</code> when the arguments for the indexing allow it.</p>\n<p>Ideally, we want to put this code in ATen, but at first sight it seemed simpler to change it here.</p>\n<p>Here are the results from a quick micro benchmark:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> master</span>\n\na <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)\n<span class=\"pl-k\">%</span>timeit a[[<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span> ,<span class=\"pl-c1\">4</span>]]\n<span class=\"pl-c1\">196</span> \u00b5s \u00b1 <span class=\"pl-c1\">42.8</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\n<span class=\"pl-k\">%</span>timeit a[[<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span> ,<span class=\"pl-c1\">4</span>]] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n<span class=\"pl-c1\">409</span> \u00b5s \u00b1 <span class=\"pl-c1\">203</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\n<span class=\"pl-k\">%</span>timeit a[a <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0.5</span>]\n<span class=\"pl-c1\">44.7</span> ms \u00b1 <span class=\"pl-c1\">5.76</span> ms per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">10</span> loops each)\n\n<span class=\"pl-k\">%</span>timeit a[a <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0.5</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">50.5</span> ms \u00b1 <span class=\"pl-c1\">9.23</span> ms per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">10</span> loops each)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> this branch</span>\n\na <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)\n<span class=\"pl-k\">%</span>timeit a[[<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span> ,<span class=\"pl-c1\">4</span>]]\n<span class=\"pl-c1\">17.5</span> \u00b5s \u00b1 <span class=\"pl-c1\">2.19</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">10000</span> loops each)\n\n<span class=\"pl-k\">%</span>timeit a[[<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span> ,<span class=\"pl-c1\">4</span>]] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n<span class=\"pl-c1\">13.6</span> \u00b5s \u00b1 <span class=\"pl-c1\">1.6</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">10000</span> loops each)\n\n<span class=\"pl-k\">%</span>timeit a[a <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0.5</span>]\n<span class=\"pl-c1\">7.54</span> ms \u00b1 <span class=\"pl-c1\">1.34</span> ms per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100</span> loops each)\n\n<span class=\"pl-k\">%</span>timeit a[a <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0.5</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">5.81</span> ms \u00b1 <span class=\"pl-c1\">1.64</span> ms per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100</span> loops each)</pre></div>\n<p>We see roughly 5-10x speedup on those simple cases.</p>", "body_text": "This PR fixes some of the performance regression on Tensors going from 0.3 to 0.4.\nIt dispatches the operations to the old kernels that we have, whenever possible.\nThis means that we use index_select / masked_select / index_fill_ or masked_fill_ when the arguments for the indexing allow it.\nIdeally, we want to put this code in ATen, but at first sight it seemed simpler to change it here.\nHere are the results from a quick micro benchmark:\n# master\n\na = torch.rand(100, 100, 100)\n%timeit a[[2, 1, 0 ,4]]\n196 \u00b5s \u00b1 42.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n%timeit a[[2, 1, 0 ,4]] = 5\n409 \u00b5s \u00b1 203 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n%timeit a[a > 0.5]\n44.7 ms \u00b1 5.76 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n%timeit a[a > 0.5] = 1\n50.5 ms \u00b1 9.23 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n\n# this branch\n\na = torch.rand(100, 100, 100)\n%timeit a[[2, 1, 0 ,4]]\n17.5 \u00b5s \u00b1 2.19 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n%timeit a[[2, 1, 0 ,4]] = 5\n13.6 \u00b5s \u00b1 1.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n%timeit a[a > 0.5]\n7.54 ms \u00b1 1.34 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%timeit a[a > 0.5] = 1\n5.81 ms \u00b1 1.64 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\nWe see roughly 5-10x speedup on those simple cases.", "body": "This PR fixes some of the performance regression on Tensors going from 0.3 to 0.4.\r\n\r\nIt dispatches the operations to the old kernels that we have, whenever possible.\r\nThis means that we use `index_select` / `masked_select` / `index_fill_` or `masked_fill_` when the arguments for the indexing allow it.\r\n\r\nIdeally, we want to put this code in ATen, but at first sight it seemed simpler to change it here.\r\n\r\nHere are the results from a quick micro benchmark:\r\n\r\n```python\r\n# master\r\n\r\na = torch.rand(100, 100, 100)\r\n%timeit a[[2, 1, 0 ,4]]\r\n196 \u00b5s \u00b1 42.8 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n%timeit a[[2, 1, 0 ,4]] = 5\r\n409 \u00b5s \u00b1 203 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n%timeit a[a > 0.5]\r\n44.7 ms \u00b1 5.76 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n%timeit a[a > 0.5] = 1\r\n50.5 ms \u00b1 9.23 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\r\n\r\n# this branch\r\n\r\na = torch.rand(100, 100, 100)\r\n%timeit a[[2, 1, 0 ,4]]\r\n17.5 \u00b5s \u00b1 2.19 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n%timeit a[[2, 1, 0 ,4]] = 5\r\n13.6 \u00b5s \u00b1 1.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n%timeit a[a > 0.5]\r\n7.54 ms \u00b1 1.34 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\n%timeit a[a > 0.5] = 1\r\n5.81 ms \u00b1 1.64 ms per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nWe see roughly 5-10x speedup on those simple cases.\r\n"}