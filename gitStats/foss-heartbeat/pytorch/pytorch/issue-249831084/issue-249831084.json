{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2398", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2398/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2398/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2398/events", "html_url": "https://github.com/pytorch/pytorch/issues/2398", "id": 249831084, "node_id": "MDU6SXNzdWUyNDk4MzEwODQ=", "number": 2398, "title": "synchronize() didn't wait for topk()", "user": {"login": "synxlin", "id": 16437040, "node_id": "MDQ6VXNlcjE2NDM3MDQw", "avatar_url": "https://avatars0.githubusercontent.com/u/16437040?v=4", "gravatar_id": "", "url": "https://api.github.com/users/synxlin", "html_url": "https://github.com/synxlin", "followers_url": "https://api.github.com/users/synxlin/followers", "following_url": "https://api.github.com/users/synxlin/following{/other_user}", "gists_url": "https://api.github.com/users/synxlin/gists{/gist_id}", "starred_url": "https://api.github.com/users/synxlin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/synxlin/subscriptions", "organizations_url": "https://api.github.com/users/synxlin/orgs", "repos_url": "https://api.github.com/users/synxlin/repos", "events_url": "https://api.github.com/users/synxlin/events{/privacy}", "received_events_url": "https://api.github.com/users/synxlin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-08-12T19:35:57Z", "updated_at": "2017-10-03T09:45:13Z", "closed_at": "2017-10-03T09:45:13Z", "author_association": "NONE", "body_html": "<p>Hi~<br>\nI start several threads with different cuda streams to execute topk() on different tensors created in default stream. code snippet inside thread is,</p>\n<div class=\"highlight highlight-source-python\"><pre>y <span class=\"pl-k\">=</span> x[::stride]\nvalues, _ <span class=\"pl-k\">=</span> torch.topk(y, k, <span class=\"pl-c1\">0</span>, <span class=\"pl-v\">largest</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">sorted</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\ni <span class=\"pl-k\">=</span> torch.ge(x, values.min()).nonzero()</pre></div>\n<p>However, it frequently gives me a random number returned by values.min() and follows by <code>no dimension tensor i</code>, even if I add <code>torch.cuda.current_stream().synchronize()</code>.<br>\nI have to fix code as follows,</p>\n<div class=\"highlight highlight-source-python\"><pre>stream <span class=\"pl-k\">=</span> torch.cuda.current_stream()\ny <span class=\"pl-k\">=</span> x[::stride]\nvalues, _ <span class=\"pl-k\">=</span> torch.topk(y, k, <span class=\"pl-c1\">0</span>, <span class=\"pl-v\">largest</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">sorted</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nnum_i, loop_count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>\n<span class=\"pl-k\">while</span> num_i <span class=\"pl-k\">&lt;=</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">or</span> num_i <span class=\"pl-k\">&gt;=</span> x.numel():\n    stream.synchronize()\n    loop_count <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-k\">if</span> loop_count <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        values, _ <span class=\"pl-k\">=</span> torch.topk(y, k, <span class=\"pl-c1\">0</span>, <span class=\"pl-v\">largest</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">sorted</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        num_i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">continue</span>\n    i <span class=\"pl-k\">=</span> torch.ge(x, values.min()).nonzero()\n    num_i <span class=\"pl-k\">=</span> i.numel()</pre></div>\n<p>It could work for a while but then whole process freezes  -- seems to be a deadlock somewhere.<br>\nI want to reproduce this problem with clear code snippet, but I cannot reproduce the problem with simple codes.<br>\nCan anyone here help me please?</p>", "body_text": "Hi~\nI start several threads with different cuda streams to execute topk() on different tensors created in default stream. code snippet inside thread is,\ny = x[::stride]\nvalues, _ = torch.topk(y, k, 0, largest=True, sorted=False)\ni = torch.ge(x, values.min()).nonzero()\nHowever, it frequently gives me a random number returned by values.min() and follows by no dimension tensor i, even if I add torch.cuda.current_stream().synchronize().\nI have to fix code as follows,\nstream = torch.cuda.current_stream()\ny = x[::stride]\nvalues, _ = torch.topk(y, k, 0, largest=True, sorted=False)\nnum_i, loop_count = 0, 0\nwhile num_i <= 0 or num_i >= x.numel():\n    stream.synchronize()\n    loop_count += 1\n    if loop_count % 10 == 0:\n        values, _ = torch.topk(y, k, 0, largest=True, sorted=False)\n        num_i = 0\n        continue\n    i = torch.ge(x, values.min()).nonzero()\n    num_i = i.numel()\nIt could work for a while but then whole process freezes  -- seems to be a deadlock somewhere.\nI want to reproduce this problem with clear code snippet, but I cannot reproduce the problem with simple codes.\nCan anyone here help me please?", "body": "Hi~\r\nI start several threads with different cuda streams to execute topk() on different tensors created in default stream. code snippet inside thread is,\r\n```python\r\ny = x[::stride]\r\nvalues, _ = torch.topk(y, k, 0, largest=True, sorted=False)\r\ni = torch.ge(x, values.min()).nonzero()\r\n```\r\n\r\nHowever, it frequently gives me a random number returned by values.min() and follows by `no dimension tensor i`, even if I add `torch.cuda.current_stream().synchronize()`.\r\nI have to fix code as follows,\r\n```python\r\nstream = torch.cuda.current_stream()\r\ny = x[::stride]\r\nvalues, _ = torch.topk(y, k, 0, largest=True, sorted=False)\r\nnum_i, loop_count = 0, 0\r\nwhile num_i <= 0 or num_i >= x.numel():\r\n    stream.synchronize()\r\n    loop_count += 1\r\n    if loop_count % 10 == 0:\r\n        values, _ = torch.topk(y, k, 0, largest=True, sorted=False)\r\n        num_i = 0\r\n        continue\r\n    i = torch.ge(x, values.min()).nonzero()\r\n    num_i = i.numel()\r\n```\r\nIt could work for a while but then whole process freezes  -- seems to be a deadlock somewhere.\r\nI want to reproduce this problem with clear code snippet, but I cannot reproduce the problem with simple codes.\r\nCan anyone here help me please?"}