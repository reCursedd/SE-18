{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/277767767", "html_url": "https://github.com/pytorch/pytorch/issues/678#issuecomment-277767767", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/678", "id": 277767767, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Nzc2Nzc2Nw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-06T18:21:53Z", "updated_at": "2017-02-06T18:21:53Z", "author_association": "MEMBER", "body_html": "<p>That's true. Adding lgamma should be easy and maybe we can do that soon, but I don't think we're going to invest much effort into porting digamma for CUDA right now. It might be possible to adapt some existing implementation (e.g. from <a href=\"https://github.com/boostorg/math/blob/242e9d669198415d165bb0213f730c00985bee9a/include/boost/math/special_functions/digamma.hpp\">Boost</a>), but it might be quite time consuming.</p>\n<p>Still, if any of you knows CUDA and wants to give it a try, then we can help you with some code pointers and reviews.</p>", "body_text": "That's true. Adding lgamma should be easy and maybe we can do that soon, but I don't think we're going to invest much effort into porting digamma for CUDA right now. It might be possible to adapt some existing implementation (e.g. from Boost), but it might be quite time consuming.\nStill, if any of you knows CUDA and wants to give it a try, then we can help you with some code pointers and reviews.", "body": "That's true. Adding lgamma should be easy and maybe we can do that soon, but I don't think we're going to invest much effort into porting digamma for CUDA right now. It might be possible to adapt some existing implementation (e.g. from [Boost](https://github.com/boostorg/math/blob/242e9d669198415d165bb0213f730c00985bee9a/include/boost/math/special_functions/digamma.hpp)), but it might be quite time consuming.\r\n\r\nStill, if any of you knows CUDA and wants to give it a try, then we can help you with some code pointers and reviews."}