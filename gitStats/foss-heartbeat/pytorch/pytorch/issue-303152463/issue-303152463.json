{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5611", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5611/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5611/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5611/events", "html_url": "https://github.com/pytorch/pytorch/issues/5611", "id": 303152463, "node_id": "MDU6SXNzdWUzMDMxNTI0NjM=", "number": 5611, "title": "Memory leak using matmul() and permute() on GPU", "user": {"login": "dmarnerides", "id": 7605917, "node_id": "MDQ6VXNlcjc2MDU5MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/7605917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dmarnerides", "html_url": "https://github.com/dmarnerides", "followers_url": "https://api.github.com/users/dmarnerides/followers", "following_url": "https://api.github.com/users/dmarnerides/following{/other_user}", "gists_url": "https://api.github.com/users/dmarnerides/gists{/gist_id}", "starred_url": "https://api.github.com/users/dmarnerides/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dmarnerides/subscriptions", "organizations_url": "https://api.github.com/users/dmarnerides/orgs", "repos_url": "https://api.github.com/users/dmarnerides/repos", "events_url": "https://api.github.com/users/dmarnerides/events{/privacy}", "received_events_url": "https://api.github.com/users/dmarnerides/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-07T15:51:42Z", "updated_at": "2018-03-15T14:44:36Z", "closed_at": "2018-03-15T14:44:36Z", "author_association": "NONE", "body_html": "<p>I've come accross a weird memory leak when using matmul() and permute() on GPU tensors:</p>\n<div class=\"highlight highlight-source-python\"><pre>l, m, n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">1</span>\nw <span class=\"pl-k\">=</span> torch.nn.Parameter(torch.Tensor(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">2</span>, l, m).cuda())\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):\n    a <span class=\"pl-k\">=</span> Variable(torch.Tensor(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">2</span>, m, n).cuda())\n    torch.matmul(w, a).permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>).mean().backward()\n</pre></div>\n<p>This only happens when both l and n are set to 1 and only on the GPU. Other dimensionalities work ok, and other permutations too. I couldn't figure out if this is due to something from matmul or from permute.</p>\n<hr>\n<ul>\n<li>OS: Ubuntu 16.04.4</li>\n<li>PyTorch version: '0.4.0a0+abd8501'</li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version: 7</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source): 5.4.0</li>\n</ul>\n<hr>\n<p>Here is the same code with imports and GPU memory monitoring</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> subprocess\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This is from https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_gpu_memory_map</span>():\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Get the current gpu usage.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns</span>\n<span class=\"pl-s\">    -------</span>\n<span class=\"pl-s\">    usage: dict</span>\n<span class=\"pl-s\">        Keys are device ids as integers.</span>\n<span class=\"pl-s\">        Values are memory usage as integers in MB.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    result <span class=\"pl-k\">=</span> subprocess.check_output(\n        [\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>nvidia-smi<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--query-gpu=memory.used<span class=\"pl-pds\">'</span></span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>--format=csv,nounits,noheader<span class=\"pl-pds\">'</span></span>\n        ], <span class=\"pl-v\">encoding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>utf-8<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Convert lines into a dictionary</span>\n    gpu_memory <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">int</span>(x) <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> result.strip().split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)]\n    gpu_memory_map <span class=\"pl-k\">=</span> <span class=\"pl-c1\">dict</span>(<span class=\"pl-c1\">zip</span>(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(gpu_memory)), gpu_memory))\n    <span class=\"pl-k\">return</span> gpu_memory_map\n\nl, m, n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">1</span>\nw <span class=\"pl-k\">=</span> torch.nn.Parameter(torch.Tensor(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">2</span>, l, m).cuda())\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):\n    a <span class=\"pl-k\">=</span> Variable(torch.Tensor(<span class=\"pl-c1\">1024</span>, <span class=\"pl-c1\">2</span>, m, n).cuda())\n    torch.matmul(w, a).permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>).mean().backward()\n    <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">100</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        gpu_mem <span class=\"pl-k\">=</span> get_gpu_memory_map()\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>GPU: <span class=\"pl-c1\">{<span class=\"pl-k\">:.2f</span>}</span> KB<span class=\"pl-pds\">\"</span></span>.format(gpu_mem[<span class=\"pl-c1\">0</span>]))\n</pre></div>", "body_text": "I've come accross a weird memory leak when using matmul() and permute() on GPU tensors:\nl, m, n = 1, 9, 1\nw = torch.nn.Parameter(torch.Tensor(1024, 2, l, m).cuda())\nfor i in range(10000):\n    a = Variable(torch.Tensor(1024, 2, m, n).cuda())\n    torch.matmul(w, a).permute(0, 3, 1, 2).mean().backward()\n\nThis only happens when both l and n are set to 1 and only on the GPU. Other dimensionalities work ok, and other permutations too. I couldn't figure out if this is due to something from matmul or from permute.\n\n\nOS: Ubuntu 16.04.4\nPyTorch version: '0.4.0a0+abd8501'\nHow you installed PyTorch (conda, pip, source): source\nPython version: 3.6.4\nCUDA/cuDNN version: 7\nGPU models and configuration:\nGCC version (if compiling from source): 5.4.0\n\n\nHere is the same code with imports and GPU memory monitoring\nimport subprocess\nimport torch\nfrom torch.autograd import Variable\n\n# This is from https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\ndef get_gpu_memory_map():\n    \"\"\"Get the current gpu usage.\n\n    Returns\n    -------\n    usage: dict\n        Keys are device ids as integers.\n        Values are memory usage as integers in MB.\n    \"\"\"\n    result = subprocess.check_output(\n        [\n            'nvidia-smi', '--query-gpu=memory.used',\n            '--format=csv,nounits,noheader'\n        ], encoding='utf-8')\n    # Convert lines into a dictionary\n    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n    return gpu_memory_map\n\nl, m, n = 1, 9, 1\nw = torch.nn.Parameter(torch.Tensor(1024, 2, l, m).cuda())\nfor i in range(10000):\n    a = Variable(torch.Tensor(1024, 2, m, n).cuda())\n    torch.matmul(w, a).permute(0, 3, 1, 2).mean().backward()\n    if i % 100 == 0:\n        gpu_mem = get_gpu_memory_map()\n        print(\"GPU: {:.2f} KB\".format(gpu_mem[0]))", "body": "I've come accross a weird memory leak when using matmul() and permute() on GPU tensors:\r\n\r\n```python\r\nl, m, n = 1, 9, 1\r\nw = torch.nn.Parameter(torch.Tensor(1024, 2, l, m).cuda())\r\nfor i in range(10000):\r\n    a = Variable(torch.Tensor(1024, 2, m, n).cuda())\r\n    torch.matmul(w, a).permute(0, 3, 1, 2).mean().backward()\r\n\r\n```\r\n\r\nThis only happens when both l and n are set to 1 and only on the GPU. Other dimensionalities work ok, and other permutations too. I couldn't figure out if this is due to something from matmul or from permute.\r\n\r\n---\r\n\r\n- OS: Ubuntu 16.04.4\r\n- PyTorch version: '0.4.0a0+abd8501'\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: 7\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source): 5.4.0\r\n\r\n--- \r\n\r\nHere is the same code with imports and GPU memory monitoring\r\n\r\n```python\r\nimport subprocess\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n# This is from https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\r\ndef get_gpu_memory_map():\r\n    \"\"\"Get the current gpu usage.\r\n\r\n    Returns\r\n    -------\r\n    usage: dict\r\n        Keys are device ids as integers.\r\n        Values are memory usage as integers in MB.\r\n    \"\"\"\r\n    result = subprocess.check_output(\r\n        [\r\n            'nvidia-smi', '--query-gpu=memory.used',\r\n            '--format=csv,nounits,noheader'\r\n        ], encoding='utf-8')\r\n    # Convert lines into a dictionary\r\n    gpu_memory = [int(x) for x in result.strip().split('\\n')]\r\n    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\r\n    return gpu_memory_map\r\n\r\nl, m, n = 1, 9, 1\r\nw = torch.nn.Parameter(torch.Tensor(1024, 2, l, m).cuda())\r\nfor i in range(10000):\r\n    a = Variable(torch.Tensor(1024, 2, m, n).cuda())\r\n    torch.matmul(w, a).permute(0, 3, 1, 2).mean().backward()\r\n    if i % 100 == 0:\r\n        gpu_mem = get_gpu_memory_map()\r\n        print(\"GPU: {:.2f} KB\".format(gpu_mem[0]))\r\n\r\n```"}