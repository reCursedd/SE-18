{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/372700940", "html_url": "https://github.com/pytorch/pytorch/issues/5611#issuecomment-372700940", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5611", "id": 372700940, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjcwMDk0MA==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-13T15:14:14Z", "updated_at": "2018-03-13T15:14:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Am looking into it. Leaving this here for my reference:</p>\n<pre><code>l, m, n = 1, 9, 1\nnbatch = 2900 # as nbatch gets larger, memory leaks get faster\nniter = 100000\nw = torch.nn.Parameter(torch.ones(nbatch, l, m).cuda())\nfor i in range(niter):\n    a = torch.ones(nbatch, m, n).cuda()\n    torch.bmm(w, a).permute(2, 1, 0).mean().backward()  ## all three of these, bmm, permute, and mean appear to be necessary\n    if i % 100 == 0:\n        gpu_mem = get_gpu_memory_map()\n        print(\"GPU: {:.2f} KB\".format(gpu_mem[0]))\n</code></pre>", "body_text": "Am looking into it. Leaving this here for my reference:\nl, m, n = 1, 9, 1\nnbatch = 2900 # as nbatch gets larger, memory leaks get faster\nniter = 100000\nw = torch.nn.Parameter(torch.ones(nbatch, l, m).cuda())\nfor i in range(niter):\n    a = torch.ones(nbatch, m, n).cuda()\n    torch.bmm(w, a).permute(2, 1, 0).mean().backward()  ## all three of these, bmm, permute, and mean appear to be necessary\n    if i % 100 == 0:\n        gpu_mem = get_gpu_memory_map()\n        print(\"GPU: {:.2f} KB\".format(gpu_mem[0]))", "body": "Am looking into it. Leaving this here for my reference:\r\n```\r\nl, m, n = 1, 9, 1\r\nnbatch = 2900 # as nbatch gets larger, memory leaks get faster\r\nniter = 100000\r\nw = torch.nn.Parameter(torch.ones(nbatch, l, m).cuda())\r\nfor i in range(niter):\r\n    a = torch.ones(nbatch, m, n).cuda()\r\n    torch.bmm(w, a).permute(2, 1, 0).mean().backward()  ## all three of these, bmm, permute, and mean appear to be necessary\r\n    if i % 100 == 0:\r\n        gpu_mem = get_gpu_memory_map()\r\n        print(\"GPU: {:.2f} KB\".format(gpu_mem[0]))\r\n```"}