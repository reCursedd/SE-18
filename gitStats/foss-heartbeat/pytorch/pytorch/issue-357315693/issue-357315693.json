{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11278", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11278/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11278/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11278/events", "html_url": "https://github.com/pytorch/pytorch/issues/11278", "id": 357315693, "node_id": "MDU6SXNzdWUzNTczMTU2OTM=", "number": 11278, "title": "How can I make training deterministic/reproducible?", "user": {"login": "themightyoarfish", "id": 11613312, "node_id": "MDQ6VXNlcjExNjEzMzEy", "avatar_url": "https://avatars0.githubusercontent.com/u/11613312?v=4", "gravatar_id": "", "url": "https://api.github.com/users/themightyoarfish", "html_url": "https://github.com/themightyoarfish", "followers_url": "https://api.github.com/users/themightyoarfish/followers", "following_url": "https://api.github.com/users/themightyoarfish/following{/other_user}", "gists_url": "https://api.github.com/users/themightyoarfish/gists{/gist_id}", "starred_url": "https://api.github.com/users/themightyoarfish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/themightyoarfish/subscriptions", "organizations_url": "https://api.github.com/users/themightyoarfish/orgs", "repos_url": "https://api.github.com/users/themightyoarfish/repos", "events_url": "https://api.github.com/users/themightyoarfish/events{/privacy}", "received_events_url": "https://api.github.com/users/themightyoarfish/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-09-05T16:49:45Z", "updated_at": "2018-09-05T20:28:11Z", "closed_at": "2018-09-05T20:28:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I seem to be having a hard time to get consistent results over several runs of the same experiment, despite seeding everything I can think of. <strong>Can this be related to the indeterministic ordering of CUDA reduction operations?</strong> On the CPU, this problem doesn't exist.</p>\n<p>I'm seeding these things:</p>\n<pre><code>def seed_everything(seed=1234):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n</code></pre>\n<p>Here is some training code to reproduce the problem. As can be seen, the weight matrix examined here is slightly different between runs. During training it seems these deviations add up and lead to wildly different results in my experiments.</p>\n<pre><code>import torch\nimport random, torch, os, numpy as np\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss\n\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, input_shape, num_classes=10):\n        super(Model, self).__init__()\n\n        # if channel dim not present, add 1\n        if len(input_shape) == 2:\n            input_shape.append(1)\n        H, W, C = input_shape\n\n        self.features = torch.nn.Sequential(\n            torch.nn.Conv2d(C, 64, kernel_size=5, stride=2, padding=1),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n            torch.nn.Conv2d(64, 192, kernel_size=3, padding=2),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n            torch.nn.Conv2d(192, 192, kernel_size=3, padding=1),\n            torch.nn.ReLU(inplace=True),\n        )\n        self.H_out =  H // (2 * 2 * 2)\n        self.W_out =  W // (2 * 2 * 2)\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Dropout(),\n            torch.nn.Linear(192 * self.H_out * self.W_out, 2048),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Dropout(),\n            torch.nn.Linear(2048, 2048),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Linear(2048, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), 192 * self.H_out * self.W_out)\n        x = self.classifier(x)\n        return x\n\n\ndef main():\n    seed_everything()\n    batch_size = 1024\n    dataset = CIFAR10('/home/share/data',\n                      download=True,\n                      train=True,\n                      transform=ToTensor()\n                      )\n    model = Model((32, 32, 3)).cuda()\n    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.3)\n    for epoch in range(3):\n        print(f'epoch {epoch}')\n        print(f'Kernel weight sum: {model.features[0].weight.sum()}')\n        data_iter = iter(dataloader)\n        while True:\n            try:\n                X, Y         = next(data_iter)\n                data, labels = X.cuda(async=True), Y.cuda(async=True)\n                optimizer.zero_grad()\n                output       = model(data)\n                loss         = CrossEntropyLoss()(output, labels)\n                loss.backward()\n                optimizer.step()\n            except StopIteration:\n                break\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n<p>Example output for me:</p>\n<pre><code>(torch) [rasmus@justins-puter:.../software/ai_ikkuna]$ python mwe.py\nFiles already downloaded and verified\nepoch 0\nKernel weight sum: -0.7570829391479492\nepoch 1\nKernel weight sum: 1.4230268001556396\nepoch 2\nKernel weight sum: 3.2366812229156494\n(torch) [rasmus@justins-puter:.../software/ai_ikkuna]$ python mwe.py\nFiles already downloaded and verified\nepoch 0\nKernel weight sum: -0.7570829391479492\nepoch 1\nKernel weight sum: 1.4367918968200684\nepoch 2\nKernel weight sum: 3.198667526245117\n</code></pre>", "body_text": "I seem to be having a hard time to get consistent results over several runs of the same experiment, despite seeding everything I can think of. Can this be related to the indeterministic ordering of CUDA reduction operations? On the CPU, this problem doesn't exist.\nI'm seeding these things:\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nHere is some training code to reproduce the problem. As can be seen, the weight matrix examined here is slightly different between runs. During training it seems these deviations add up and lead to wildly different results in my experiments.\nimport torch\nimport random, torch, os, numpy as np\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss\n\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, input_shape, num_classes=10):\n        super(Model, self).__init__()\n\n        # if channel dim not present, add 1\n        if len(input_shape) == 2:\n            input_shape.append(1)\n        H, W, C = input_shape\n\n        self.features = torch.nn.Sequential(\n            torch.nn.Conv2d(C, 64, kernel_size=5, stride=2, padding=1),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n            torch.nn.Conv2d(64, 192, kernel_size=3, padding=2),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=3, stride=2),\n            torch.nn.Conv2d(192, 192, kernel_size=3, padding=1),\n            torch.nn.ReLU(inplace=True),\n        )\n        self.H_out =  H // (2 * 2 * 2)\n        self.W_out =  W // (2 * 2 * 2)\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Dropout(),\n            torch.nn.Linear(192 * self.H_out * self.W_out, 2048),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Dropout(),\n            torch.nn.Linear(2048, 2048),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Linear(2048, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), 192 * self.H_out * self.W_out)\n        x = self.classifier(x)\n        return x\n\n\ndef main():\n    seed_everything()\n    batch_size = 1024\n    dataset = CIFAR10('/home/share/data',\n                      download=True,\n                      train=True,\n                      transform=ToTensor()\n                      )\n    model = Model((32, 32, 3)).cuda()\n    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.3)\n    for epoch in range(3):\n        print(f'epoch {epoch}')\n        print(f'Kernel weight sum: {model.features[0].weight.sum()}')\n        data_iter = iter(dataloader)\n        while True:\n            try:\n                X, Y         = next(data_iter)\n                data, labels = X.cuda(async=True), Y.cuda(async=True)\n                optimizer.zero_grad()\n                output       = model(data)\n                loss         = CrossEntropyLoss()(output, labels)\n                loss.backward()\n                optimizer.step()\n            except StopIteration:\n                break\n\nif __name__ == '__main__':\n    main()\n\nExample output for me:\n(torch) [rasmus@justins-puter:.../software/ai_ikkuna]$ python mwe.py\nFiles already downloaded and verified\nepoch 0\nKernel weight sum: -0.7570829391479492\nepoch 1\nKernel weight sum: 1.4230268001556396\nepoch 2\nKernel weight sum: 3.2366812229156494\n(torch) [rasmus@justins-puter:.../software/ai_ikkuna]$ python mwe.py\nFiles already downloaded and verified\nepoch 0\nKernel weight sum: -0.7570829391479492\nepoch 1\nKernel weight sum: 1.4367918968200684\nepoch 2\nKernel weight sum: 3.198667526245117", "body": "I seem to be having a hard time to get consistent results over several runs of the same experiment, despite seeding everything I can think of. **Can this be related to the indeterministic ordering of CUDA reduction operations?** On the CPU, this problem doesn't exist.\r\n\r\nI'm seeding these things:\r\n```\r\ndef seed_everything(seed=1234):\r\n    random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n    np.random.seed(seed)\r\n    os.environ['PYTHONHASHSEED'] = str(seed)\r\n```\r\n\r\nHere is some training code to reproduce the problem. As can be seen, the weight matrix examined here is slightly different between runs. During training it seems these deviations add up and lead to wildly different results in my experiments.\r\n\r\n```\r\nimport torch\r\nimport random, torch, os, numpy as np\r\nfrom torchvision.datasets import CIFAR10\r\nfrom torchvision.transforms import ToTensor\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.nn import CrossEntropyLoss\r\n\r\n\r\ndef seed_everything(seed=1234):\r\n    random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n    np.random.seed(seed)\r\n    os.environ['PYTHONHASHSEED'] = str(seed)\r\n\r\n\r\nclass Model(torch.nn.Module):\r\n    def __init__(self, input_shape, num_classes=10):\r\n        super(Model, self).__init__()\r\n\r\n        # if channel dim not present, add 1\r\n        if len(input_shape) == 2:\r\n            input_shape.append(1)\r\n        H, W, C = input_shape\r\n\r\n        self.features = torch.nn.Sequential(\r\n            torch.nn.Conv2d(C, 64, kernel_size=5, stride=2, padding=1),\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.MaxPool2d(kernel_size=3, stride=2),\r\n            torch.nn.Conv2d(64, 192, kernel_size=3, padding=2),\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.MaxPool2d(kernel_size=3, stride=2),\r\n            torch.nn.Conv2d(192, 192, kernel_size=3, padding=1),\r\n            torch.nn.ReLU(inplace=True),\r\n        )\r\n        self.H_out =  H // (2 * 2 * 2)\r\n        self.W_out =  W // (2 * 2 * 2)\r\n        self.classifier = torch.nn.Sequential(\r\n            torch.nn.Dropout(),\r\n            torch.nn.Linear(192 * self.H_out * self.W_out, 2048),\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.Dropout(),\r\n            torch.nn.Linear(2048, 2048),\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.Linear(2048, num_classes),\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = self.features(x)\r\n        x = x.view(x.size(0), 192 * self.H_out * self.W_out)\r\n        x = self.classifier(x)\r\n        return x\r\n\r\n\r\ndef main():\r\n    seed_everything()\r\n    batch_size = 1024\r\n    dataset = CIFAR10('/home/share/data',\r\n                      download=True,\r\n                      train=True,\r\n                      transform=ToTensor()\r\n                      )\r\n    model = Model((32, 32, 3)).cuda()\r\n    dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.3)\r\n    for epoch in range(3):\r\n        print(f'epoch {epoch}')\r\n        print(f'Kernel weight sum: {model.features[0].weight.sum()}')\r\n        data_iter = iter(dataloader)\r\n        while True:\r\n            try:\r\n                X, Y         = next(data_iter)\r\n                data, labels = X.cuda(async=True), Y.cuda(async=True)\r\n                optimizer.zero_grad()\r\n                output       = model(data)\r\n                loss         = CrossEntropyLoss()(output, labels)\r\n                loss.backward()\r\n                optimizer.step()\r\n            except StopIteration:\r\n                break\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\nExample output for me:\r\n```\r\n(torch) [rasmus@justins-puter:.../software/ai_ikkuna]$ python mwe.py\r\nFiles already downloaded and verified\r\nepoch 0\r\nKernel weight sum: -0.7570829391479492\r\nepoch 1\r\nKernel weight sum: 1.4230268001556396\r\nepoch 2\r\nKernel weight sum: 3.2366812229156494\r\n(torch) [rasmus@justins-puter:.../software/ai_ikkuna]$ python mwe.py\r\nFiles already downloaded and verified\r\nepoch 0\r\nKernel weight sum: -0.7570829391479492\r\nepoch 1\r\nKernel weight sum: 1.4367918968200684\r\nepoch 2\r\nKernel weight sum: 3.198667526245117\r\n```\r\n\r\n"}