{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/330053275", "html_url": "https://github.com/pytorch/pytorch/issues/2718#issuecomment-330053275", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2718", "id": 330053275, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDA1MzI3NQ==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-17T14:37:21Z", "updated_at": "2017-09-17T14:37:21Z", "author_association": "MEMBER", "body_html": "<p>took some time to get to this, but once i took a look it was a simple user-error.</p>\n<p>So, <code>generated_latent</code> is done outside the for-loop.<br>\nAnd in the for-loop, there's this line:</p>\n<pre><code>generated = generated_latent[indices, :]\n</code></pre>\n<p>So now <code>generated</code> is connected to <code>generated_latent</code>, which means, gradients coming from <code>generated</code> will be passed back through to <code>generated_latent</code>.</p>\n<p>However, once you do <code>loss.backward()</code> the graph's temporary buffers are freed by default, which means <code>generated_latent</code> cannot backpropagate gradients correctly.</p>\n<p>If your intention is not backprop back through <code>generated_latent</code> for the discriminators for-loop, then you can simply do:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> rewrap Variable to not back-propagate through generated_latent</span>\ngenerated <span class=\"pl-k\">=</span> Variable(generated_latent[indices, :].data)</pre></div>\n<p>If your intention is to backprop back through <code>generated_latent</code>, then you can simply do:</p>\n<div class=\"highlight highlight-source-python\"><pre>loss.backward(<span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>", "body_text": "took some time to get to this, but once i took a look it was a simple user-error.\nSo, generated_latent is done outside the for-loop.\nAnd in the for-loop, there's this line:\ngenerated = generated_latent[indices, :]\n\nSo now generated is connected to generated_latent, which means, gradients coming from generated will be passed back through to generated_latent.\nHowever, once you do loss.backward() the graph's temporary buffers are freed by default, which means generated_latent cannot backpropagate gradients correctly.\nIf your intention is not backprop back through generated_latent for the discriminators for-loop, then you can simply do:\n# rewrap Variable to not back-propagate through generated_latent\ngenerated = Variable(generated_latent[indices, :].data)\nIf your intention is to backprop back through generated_latent, then you can simply do:\nloss.backward(create_graph=True)", "body": "took some time to get to this, but once i took a look it was a simple user-error.\r\n\r\nSo, `generated_latent` is done outside the for-loop.\r\nAnd in the for-loop, there's this line:\r\n```\r\ngenerated = generated_latent[indices, :]\r\n```\r\n\r\nSo now `generated` is connected to `generated_latent`, which means, gradients coming from `generated` will be passed back through to `generated_latent`.\r\n\r\nHowever, once you do `loss.backward()` the graph's temporary buffers are freed by default, which means `generated_latent` cannot backpropagate gradients correctly.\r\n\r\nIf your intention is not backprop back through `generated_latent` for the discriminators for-loop, then you can simply do:\r\n\r\n```python\r\n# rewrap Variable to not back-propagate through generated_latent\r\ngenerated = Variable(generated_latent[indices, :].data)\r\n```\r\n\r\nIf your intention is to backprop back through `generated_latent`, then you can simply do:\r\n\r\n```python\r\nloss.backward(create_graph=True)\r\n```"}