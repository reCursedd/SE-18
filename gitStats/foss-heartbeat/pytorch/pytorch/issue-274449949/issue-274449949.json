{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3735", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3735/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3735/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3735/events", "html_url": "https://github.com/pytorch/pytorch/issues/3735", "id": 274449949, "node_id": "MDU6SXNzdWUyNzQ0NDk5NDk=", "number": 3735, "title": "DataPrallel uses only one device", "user": {"login": "MaratZakirov", "id": 24436072, "node_id": "MDQ6VXNlcjI0NDM2MDcy", "avatar_url": "https://avatars1.githubusercontent.com/u/24436072?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MaratZakirov", "html_url": "https://github.com/MaratZakirov", "followers_url": "https://api.github.com/users/MaratZakirov/followers", "following_url": "https://api.github.com/users/MaratZakirov/following{/other_user}", "gists_url": "https://api.github.com/users/MaratZakirov/gists{/gist_id}", "starred_url": "https://api.github.com/users/MaratZakirov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MaratZakirov/subscriptions", "organizations_url": "https://api.github.com/users/MaratZakirov/orgs", "repos_url": "https://api.github.com/users/MaratZakirov/repos", "events_url": "https://api.github.com/users/MaratZakirov/events{/privacy}", "received_events_url": "https://api.github.com/users/MaratZakirov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-16T09:37:05Z", "updated_at": "2017-11-16T15:15:57Z", "closed_at": "2017-11-16T15:15:46Z", "author_association": "NONE", "body_html": "<p>Hello pytorches!</p>\n<p>I am new to pytorch so this issue could be just stupid mistake.<br>\nRecently I am trying to build language model on pytorch (plese see code below). I am using DataParallel but by some reason it uses only one device but practically I see that then I call model it actually splits the input.  nvidia-smi shows that only one device is actually used. Why is that so?</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom pandas import DataFrame as df\nimport numpy\nimport re\n\nDIR = './'\n\nS       = '_\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044c\u044b\u044a\u044d\u044e\u044f '\nchar2id = dict([(c, i) for i, c in enumerate(S)])\nid2char = dict([(i, c) for i, c in enumerate(S)])\n\n# Hyper Parameters\nNUMDEVICE     = 3\n\nhidden_size   = 512\nbatch_size    = NUMDEVICE * 32\nlinesize      = 128\nnum_epochs    = 5\nnum_samples   = 1000  # number of words to be sampled\nlearning_rate = 0.002\n\n# RNN Based Language Model\nclass RNNLM(nn.Module):\n    def __init__(self, charnum, hidden_size):\n        super(RNNLM, self).__init__()\n        self.embed  = nn.Embedding(charnum, charnum)\n        self.lstm1  = nn.LSTM(charnum, hidden_size, 1, batch_first=True)\n        self.linear = nn.Linear(hidden_size, charnum)\n        self.init_weights()\n\n    def init_weights(self):\n        self.linear.bias.data.fill_(0)\n        self.linear.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, x, h, t):\n        x      = self.embed(x)\n        out, h = self.lstm1(x, h)\n        out = out.contiguous().view(out.size(0) * out.size(1), out.size(2))\n        out = self.linear(out)\n        return out, h, t\n\nclass LangDataset(Dataset):\n    def __init__(self, csv_file, char2id):\n        data = df.from_csv(csv_file, sep='\\t')['FullTranscriptionText'].values\n        self.text = []\n        for i in range(len(data)):\n            line = ''\n            for word in re.findall('[' + S[1:-1] + ']+', data[i].lower()):\n                if len(line) + 1 + len(word) &gt;= linesize:\n                    self.text.append(line)\n                    line = ''\n                if len(word) + 1 &gt; linesize:\n                    continue\n                line += word + ' '\n            if len(line) &gt; 0:\n                self.text.append(line[:-1])\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        data = numpy.zeros(linesize)\n        for k, c in enumerate(self.text[idx]):\n            data[k] = char2id[c]\n        return (data[:-1], data[1:])\n\ndataset = LangDataset(DIR + 'audio_manifest.tsv', char2id)\n\nmodel = RNNLM(len(S), hidden_size)\nmodel = nn.DataParallel(model)\nmodel = model.cuda()\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss().cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Truncated Backpropagation\ndef detach(states):\n    return [state.detach() for state in states]\n\n# Training\nfor epoch in range(num_epochs):\n    # Initial hidden and memory states\n    states = (Variable(torch.zeros(1, int(batch_size / NUMDEVICE), hidden_size)).cuda(),\n              Variable(torch.zeros(1, int(batch_size / NUMDEVICE), hidden_size)).cuda())\n\n    for i, batch in enumerate(DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4, drop_last=True)):\n        # Get batch inputs and targets\n        inputs  = Variable(batch[0]).cuda()\n        targets = Variable(batch[1]).cuda()#.contiguous()).cuda()\n\n        # Forward + Backward + Optimize\n        model.zero_grad()\n        states = detach(states)\n\n        outputs, states, targets = model(inputs.long(), states, targets)\n\n        # Cast values\n        targets = targets.view(targets.size(0) * targets.size(1))\n        targets = targets.long()\n\n        loss = criterion(outputs, targets)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n        optimizer.step()\n\n        if i % 100 == 0:\n            print('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n                  (epoch + 1, num_epochs, i, len(dataset) / batch_size, loss.data[0], np.exp(loss.data[0])))\n</code></pre>", "body_text": "Hello pytorches!\nI am new to pytorch so this issue could be just stupid mistake.\nRecently I am trying to build language model on pytorch (plese see code below). I am using DataParallel but by some reason it uses only one device but practically I see that then I call model it actually splits the input.  nvidia-smi shows that only one device is actually used. Why is that so?\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom pandas import DataFrame as df\nimport numpy\nimport re\n\nDIR = './'\n\nS       = '_\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044c\u044b\u044a\u044d\u044e\u044f '\nchar2id = dict([(c, i) for i, c in enumerate(S)])\nid2char = dict([(i, c) for i, c in enumerate(S)])\n\n# Hyper Parameters\nNUMDEVICE     = 3\n\nhidden_size   = 512\nbatch_size    = NUMDEVICE * 32\nlinesize      = 128\nnum_epochs    = 5\nnum_samples   = 1000  # number of words to be sampled\nlearning_rate = 0.002\n\n# RNN Based Language Model\nclass RNNLM(nn.Module):\n    def __init__(self, charnum, hidden_size):\n        super(RNNLM, self).__init__()\n        self.embed  = nn.Embedding(charnum, charnum)\n        self.lstm1  = nn.LSTM(charnum, hidden_size, 1, batch_first=True)\n        self.linear = nn.Linear(hidden_size, charnum)\n        self.init_weights()\n\n    def init_weights(self):\n        self.linear.bias.data.fill_(0)\n        self.linear.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, x, h, t):\n        x      = self.embed(x)\n        out, h = self.lstm1(x, h)\n        out = out.contiguous().view(out.size(0) * out.size(1), out.size(2))\n        out = self.linear(out)\n        return out, h, t\n\nclass LangDataset(Dataset):\n    def __init__(self, csv_file, char2id):\n        data = df.from_csv(csv_file, sep='\\t')['FullTranscriptionText'].values\n        self.text = []\n        for i in range(len(data)):\n            line = ''\n            for word in re.findall('[' + S[1:-1] + ']+', data[i].lower()):\n                if len(line) + 1 + len(word) >= linesize:\n                    self.text.append(line)\n                    line = ''\n                if len(word) + 1 > linesize:\n                    continue\n                line += word + ' '\n            if len(line) > 0:\n                self.text.append(line[:-1])\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, idx):\n        data = numpy.zeros(linesize)\n        for k, c in enumerate(self.text[idx]):\n            data[k] = char2id[c]\n        return (data[:-1], data[1:])\n\ndataset = LangDataset(DIR + 'audio_manifest.tsv', char2id)\n\nmodel = RNNLM(len(S), hidden_size)\nmodel = nn.DataParallel(model)\nmodel = model.cuda()\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss().cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Truncated Backpropagation\ndef detach(states):\n    return [state.detach() for state in states]\n\n# Training\nfor epoch in range(num_epochs):\n    # Initial hidden and memory states\n    states = (Variable(torch.zeros(1, int(batch_size / NUMDEVICE), hidden_size)).cuda(),\n              Variable(torch.zeros(1, int(batch_size / NUMDEVICE), hidden_size)).cuda())\n\n    for i, batch in enumerate(DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4, drop_last=True)):\n        # Get batch inputs and targets\n        inputs  = Variable(batch[0]).cuda()\n        targets = Variable(batch[1]).cuda()#.contiguous()).cuda()\n\n        # Forward + Backward + Optimize\n        model.zero_grad()\n        states = detach(states)\n\n        outputs, states, targets = model(inputs.long(), states, targets)\n\n        # Cast values\n        targets = targets.view(targets.size(0) * targets.size(1))\n        targets = targets.long()\n\n        loss = criterion(outputs, targets)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n        optimizer.step()\n\n        if i % 100 == 0:\n            print('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n                  (epoch + 1, num_epochs, i, len(dataset) / batch_size, loss.data[0], np.exp(loss.data[0])))", "body": "Hello pytorches!\r\n\r\nI am new to pytorch so this issue could be just stupid mistake. \r\nRecently I am trying to build language model on pytorch (plese see code below). I am using DataParallel but by some reason it uses only one device but practically I see that then I call model it actually splits the input.  nvidia-smi shows that only one device is actually used. Why is that so?\r\n\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\nfrom torch.utils.data import Dataset, DataLoader\r\nfrom pandas import DataFrame as df\r\nimport numpy\r\nimport re\r\n\r\nDIR = './'\r\n\r\nS       = '_\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044c\u044b\u044a\u044d\u044e\u044f '\r\nchar2id = dict([(c, i) for i, c in enumerate(S)])\r\nid2char = dict([(i, c) for i, c in enumerate(S)])\r\n\r\n# Hyper Parameters\r\nNUMDEVICE     = 3\r\n\r\nhidden_size   = 512\r\nbatch_size    = NUMDEVICE * 32\r\nlinesize      = 128\r\nnum_epochs    = 5\r\nnum_samples   = 1000  # number of words to be sampled\r\nlearning_rate = 0.002\r\n\r\n# RNN Based Language Model\r\nclass RNNLM(nn.Module):\r\n    def __init__(self, charnum, hidden_size):\r\n        super(RNNLM, self).__init__()\r\n        self.embed  = nn.Embedding(charnum, charnum)\r\n        self.lstm1  = nn.LSTM(charnum, hidden_size, 1, batch_first=True)\r\n        self.linear = nn.Linear(hidden_size, charnum)\r\n        self.init_weights()\r\n\r\n    def init_weights(self):\r\n        self.linear.bias.data.fill_(0)\r\n        self.linear.weight.data.uniform_(-0.1, 0.1)\r\n\r\n    def forward(self, x, h, t):\r\n        x      = self.embed(x)\r\n        out, h = self.lstm1(x, h)\r\n        out = out.contiguous().view(out.size(0) * out.size(1), out.size(2))\r\n        out = self.linear(out)\r\n        return out, h, t\r\n\r\nclass LangDataset(Dataset):\r\n    def __init__(self, csv_file, char2id):\r\n        data = df.from_csv(csv_file, sep='\\t')['FullTranscriptionText'].values\r\n        self.text = []\r\n        for i in range(len(data)):\r\n            line = ''\r\n            for word in re.findall('[' + S[1:-1] + ']+', data[i].lower()):\r\n                if len(line) + 1 + len(word) >= linesize:\r\n                    self.text.append(line)\r\n                    line = ''\r\n                if len(word) + 1 > linesize:\r\n                    continue\r\n                line += word + ' '\r\n            if len(line) > 0:\r\n                self.text.append(line[:-1])\r\n\r\n    def __len__(self):\r\n        return len(self.text)\r\n\r\n    def __getitem__(self, idx):\r\n        data = numpy.zeros(linesize)\r\n        for k, c in enumerate(self.text[idx]):\r\n            data[k] = char2id[c]\r\n        return (data[:-1], data[1:])\r\n\r\ndataset = LangDataset(DIR + 'audio_manifest.tsv', char2id)\r\n\r\nmodel = RNNLM(len(S), hidden_size)\r\nmodel = nn.DataParallel(model)\r\nmodel = model.cuda()\r\n\r\n# Loss and Optimizer\r\ncriterion = nn.CrossEntropyLoss().cuda()\r\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n\r\n# Truncated Backpropagation\r\ndef detach(states):\r\n    return [state.detach() for state in states]\r\n\r\n# Training\r\nfor epoch in range(num_epochs):\r\n    # Initial hidden and memory states\r\n    states = (Variable(torch.zeros(1, int(batch_size / NUMDEVICE), hidden_size)).cuda(),\r\n              Variable(torch.zeros(1, int(batch_size / NUMDEVICE), hidden_size)).cuda())\r\n\r\n    for i, batch in enumerate(DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4, drop_last=True)):\r\n        # Get batch inputs and targets\r\n        inputs  = Variable(batch[0]).cuda()\r\n        targets = Variable(batch[1]).cuda()#.contiguous()).cuda()\r\n\r\n        # Forward + Backward + Optimize\r\n        model.zero_grad()\r\n        states = detach(states)\r\n\r\n        outputs, states, targets = model(inputs.long(), states, targets)\r\n\r\n        # Cast values\r\n        targets = targets.view(targets.size(0) * targets.size(1))\r\n        targets = targets.long()\r\n\r\n        loss = criterion(outputs, targets)\r\n        loss.backward()\r\n        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\r\n        optimizer.step()\r\n\r\n        if i % 100 == 0:\r\n            print('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\r\n                  (epoch + 1, num_epochs, i, len(dataset) / batch_size, loss.data[0], np.exp(loss.data[0])))\r\n```\r\n\r\n"}