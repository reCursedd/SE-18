{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7380", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7380/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7380/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7380/events", "html_url": "https://github.com/pytorch/pytorch/issues/7380", "id": 321289557, "node_id": "MDU6SXNzdWUzMjEyODk1NTc=", "number": 7380, "title": "Delete attribute (delattr) doesn't work as expected?", "user": {"login": "brando90", "id": 1855278, "node_id": "MDQ6VXNlcjE4NTUyNzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1855278?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brando90", "html_url": "https://github.com/brando90", "followers_url": "https://api.github.com/users/brando90/followers", "following_url": "https://api.github.com/users/brando90/following{/other_user}", "gists_url": "https://api.github.com/users/brando90/gists{/gist_id}", "starred_url": "https://api.github.com/users/brando90/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brando90/subscriptions", "organizations_url": "https://api.github.com/users/brando90/orgs", "repos_url": "https://api.github.com/users/brando90/repos", "events_url": "https://api.github.com/users/brando90/events{/privacy}", "received_events_url": "https://api.github.com/users/brando90/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-05-08T18:07:41Z", "updated_at": "2018-05-08T18:31:18Z", "closed_at": "2018-05-08T18:23:04Z", "author_association": "NONE", "body_html": "<p>Context I've been trying to solve:</p>\n<ul>\n<li><a href=\"https://discuss.pytorch.org/t/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-original-trainable-param-trainable/17388\" rel=\"nofollow\">https://discuss.pytorch.org/t/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-original-trainable-param-trainable/17388</a></li>\n<li><a href=\"https://stackoverflow.com/questions/50144597/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-or\" rel=\"nofollow\">https://stackoverflow.com/questions/50144597/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-or</a></li>\n</ul>\n<h2>Issue description</h2>\n<p>It seems to me that the issue is that <code>delattr</code> isn't working as it should. Perhaps I'm wrong but I wanted to report it cuz it seemed like a bug to me. So if I am looping through the parameter dict I try to delete a specific parameter/layer from my net I get the following error:</p>\n<pre><code>AttributeError: conv1.weight\n</code></pre>\n<p>when I do:</p>\n<pre><code>delattr(net_place_holder, name)\n</code></pre>\n<h2>Code example</h2>\n<p>I provided a 100% self contained (device agnostic code):</p>\n<pre><code>import torch\nfrom torch import nn\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom collections import OrderedDict\n\nimport copy\n\ndef dont_train(net):\n    '''\n    set training parameters to false.\n    '''\n    for param in net.parameters():\n        param.requires_grad = False\n    return net\n\ndef get_cifar10():\n    transform = transforms.Compose(\n        [transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,shuffle=True, num_workers=2)\n    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n    return trainloader,classes\n\ndef combine_nets(net_train,net_no_train,net_place_holder):\n    '''\n        Combine nets in a way train net is trainable\n    '''\n    params_train = net_no_train.named_parameters()\n    dict_params_place_holder = dict( net_place_holder.named_parameters() )\n    dict_params_no_train = dict(net_train.named_parameters())\n    for name,param_train in params_train:\n        if name in dict_params_place_holder:\n            param_no_train = dict_params_no_train[name]\n            delattr(net_place_holder, name)\n            W_new = param_train + param_no_train # notice addition is just chosen for the sake of an example\n            setattr(net_place_holder, name, W_new)\n    return net_place_holder\n\ndef combining_nets_lead_to_error():\n    '''\n    Intention is to only train the net with trainable params.\n    Placeholde rnet is a dummy net, it doesn't actually do anything except hold the combination of params and its the\n    net that does the forward pass on the data.\n    '''\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    ''' create three musketeers '''\n    net_train = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ])).to(device)\n    net_no_train = copy.deepcopy(net_train).to(device)\n    net_place_holder = copy.deepcopy(net_train).to(device)\n    ''' prepare train, hyperparams '''\n    trainloader,classes = get_cifar10()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net_train.parameters(), lr=0.001, momentum=0.9)\n    ''' train '''\n    net_train.train()\n    net_no_train.eval()\n    net_place_holder.eval()\n    for epoch in range(2):  # loop over the dataset multiple times\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(trainloader, 0):\n            optimizer.zero_grad() # zero the parameter gradients\n            inputs, labels = inputs.to(device), labels.to(device)\n            # combine nets\n            net_place_holder = combine_nets(net_train,net_no_train,net_place_holder)\n            #\n            outputs = net_place_holder(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            # print statistics\n            running_loss += loss.item()\n            if i % 2000 == 1999:  # print every 2000 mini-batches\n                print('[%d, %5d] loss: %.3f' %\n                      (epoch + 1, i + 1, running_loss / 2000))\n                running_loss = 0.0\n    ''' DONE '''\n    print('Done \\a')\n\nif __name__ == '__main__':\n    combining_nets_lead_to_error()\n</code></pre>\n<h2>System Info</h2>\n<pre><code>Collecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: Could not collect\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: 8.0.44\nGPU models and configuration: Could not collect\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] numpy (1.14.2)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.1)\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\n[conda] torch                     0.4.0                     &lt;pip&gt;\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n[conda] torchvision               0.2.1                    py36_1    pytorch\n</code></pre>", "body_text": "Context I've been trying to solve:\n\nhttps://discuss.pytorch.org/t/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-original-trainable-param-trainable/17388\nhttps://stackoverflow.com/questions/50144597/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-or\n\nIssue description\nIt seems to me that the issue is that delattr isn't working as it should. Perhaps I'm wrong but I wanted to report it cuz it seemed like a bug to me. So if I am looping through the parameter dict I try to delete a specific parameter/layer from my net I get the following error:\nAttributeError: conv1.weight\n\nwhen I do:\ndelattr(net_place_holder, name)\n\nCode example\nI provided a 100% self contained (device agnostic code):\nimport torch\nfrom torch import nn\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom collections import OrderedDict\n\nimport copy\n\ndef dont_train(net):\n    '''\n    set training parameters to false.\n    '''\n    for param in net.parameters():\n        param.requires_grad = False\n    return net\n\ndef get_cifar10():\n    transform = transforms.Compose(\n        [transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,shuffle=True, num_workers=2)\n    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n    return trainloader,classes\n\ndef combine_nets(net_train,net_no_train,net_place_holder):\n    '''\n        Combine nets in a way train net is trainable\n    '''\n    params_train = net_no_train.named_parameters()\n    dict_params_place_holder = dict( net_place_holder.named_parameters() )\n    dict_params_no_train = dict(net_train.named_parameters())\n    for name,param_train in params_train:\n        if name in dict_params_place_holder:\n            param_no_train = dict_params_no_train[name]\n            delattr(net_place_holder, name)\n            W_new = param_train + param_no_train # notice addition is just chosen for the sake of an example\n            setattr(net_place_holder, name, W_new)\n    return net_place_holder\n\ndef combining_nets_lead_to_error():\n    '''\n    Intention is to only train the net with trainable params.\n    Placeholde rnet is a dummy net, it doesn't actually do anything except hold the combination of params and its the\n    net that does the forward pass on the data.\n    '''\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    ''' create three musketeers '''\n    net_train = nn.Sequential(OrderedDict([\n          ('conv1', nn.Conv2d(1,20,5)),\n          ('relu1', nn.ReLU()),\n          ('conv2', nn.Conv2d(20,64,5)),\n          ('relu2', nn.ReLU())\n        ])).to(device)\n    net_no_train = copy.deepcopy(net_train).to(device)\n    net_place_holder = copy.deepcopy(net_train).to(device)\n    ''' prepare train, hyperparams '''\n    trainloader,classes = get_cifar10()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net_train.parameters(), lr=0.001, momentum=0.9)\n    ''' train '''\n    net_train.train()\n    net_no_train.eval()\n    net_place_holder.eval()\n    for epoch in range(2):  # loop over the dataset multiple times\n        running_loss = 0.0\n        for i, (inputs, labels) in enumerate(trainloader, 0):\n            optimizer.zero_grad() # zero the parameter gradients\n            inputs, labels = inputs.to(device), labels.to(device)\n            # combine nets\n            net_place_holder = combine_nets(net_train,net_no_train,net_place_holder)\n            #\n            outputs = net_place_holder(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            # print statistics\n            running_loss += loss.item()\n            if i % 2000 == 1999:  # print every 2000 mini-batches\n                print('[%d, %5d] loss: %.3f' %\n                      (epoch + 1, i + 1, running_loss / 2000))\n                running_loss = 0.0\n    ''' DONE '''\n    print('Done \\a')\n\nif __name__ == '__main__':\n    combining_nets_lead_to_error()\n\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: Could not collect\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: 8.0.44\nGPU models and configuration: Could not collect\nNvidia driver version: Could not collect\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] numpy (1.14.2)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.1)\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\n[conda] torch                     0.4.0                     <pip>\n[conda] torchvision               0.2.1                     <pip>\n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "Context I've been trying to solve:\r\n\r\n- https://discuss.pytorch.org/t/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-original-trainable-param-trainable/17388\r\n- https://stackoverflow.com/questions/50144597/if-we-combine-one-trainable-parameters-with-a-non-trainable-parameter-is-the-or\r\n\r\n## Issue description\r\n\r\nIt seems to me that the issue is that `delattr` isn't working as it should. Perhaps I'm wrong but I wanted to report it cuz it seemed like a bug to me. So if I am looping through the parameter dict I try to delete a specific parameter/layer from my net I get the following error:\r\n\r\n    AttributeError: conv1.weight\r\n\r\nwhen I do:\r\n\r\n    delattr(net_place_holder, name)\r\n\r\n## Code example\r\n\r\nI provided a 100% self contained (device agnostic code):\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\nimport torch.optim as optim\r\n\r\nimport torchvision\r\nimport torchvision.transforms as transforms\r\n\r\nfrom collections import OrderedDict\r\n\r\nimport copy\r\n\r\ndef dont_train(net):\r\n    '''\r\n    set training parameters to false.\r\n    '''\r\n    for param in net.parameters():\r\n        param.requires_grad = False\r\n    return net\r\n\r\ndef get_cifar10():\r\n    transform = transforms.Compose(\r\n        [transforms.ToTensor(),\r\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\r\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,shuffle=True, num_workers=2)\r\n    classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\r\n    return trainloader,classes\r\n\r\ndef combine_nets(net_train,net_no_train,net_place_holder):\r\n    '''\r\n        Combine nets in a way train net is trainable\r\n    '''\r\n    params_train = net_no_train.named_parameters()\r\n    dict_params_place_holder = dict( net_place_holder.named_parameters() )\r\n    dict_params_no_train = dict(net_train.named_parameters())\r\n    for name,param_train in params_train:\r\n        if name in dict_params_place_holder:\r\n            param_no_train = dict_params_no_train[name]\r\n            delattr(net_place_holder, name)\r\n            W_new = param_train + param_no_train # notice addition is just chosen for the sake of an example\r\n            setattr(net_place_holder, name, W_new)\r\n    return net_place_holder\r\n\r\ndef combining_nets_lead_to_error():\r\n    '''\r\n    Intention is to only train the net with trainable params.\r\n    Placeholde rnet is a dummy net, it doesn't actually do anything except hold the combination of params and its the\r\n    net that does the forward pass on the data.\r\n    '''\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n    ''' create three musketeers '''\r\n    net_train = nn.Sequential(OrderedDict([\r\n          ('conv1', nn.Conv2d(1,20,5)),\r\n          ('relu1', nn.ReLU()),\r\n          ('conv2', nn.Conv2d(20,64,5)),\r\n          ('relu2', nn.ReLU())\r\n        ])).to(device)\r\n    net_no_train = copy.deepcopy(net_train).to(device)\r\n    net_place_holder = copy.deepcopy(net_train).to(device)\r\n    ''' prepare train, hyperparams '''\r\n    trainloader,classes = get_cifar10()\r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = optim.SGD(net_train.parameters(), lr=0.001, momentum=0.9)\r\n    ''' train '''\r\n    net_train.train()\r\n    net_no_train.eval()\r\n    net_place_holder.eval()\r\n    for epoch in range(2):  # loop over the dataset multiple times\r\n        running_loss = 0.0\r\n        for i, (inputs, labels) in enumerate(trainloader, 0):\r\n            optimizer.zero_grad() # zero the parameter gradients\r\n            inputs, labels = inputs.to(device), labels.to(device)\r\n            # combine nets\r\n            net_place_holder = combine_nets(net_train,net_no_train,net_place_holder)\r\n            #\r\n            outputs = net_place_holder(inputs)\r\n            loss = criterion(outputs, labels)\r\n            loss.backward()\r\n            optimizer.step()\r\n            # print statistics\r\n            running_loss += loss.item()\r\n            if i % 2000 == 1999:  # print every 2000 mini-batches\r\n                print('[%d, %5d] loss: %.3f' %\r\n                      (epoch + 1, i + 1, running_loss / 2000))\r\n                running_loss = 0.0\r\n    ''' DONE '''\r\n    print('Done \\a')\r\n\r\nif __name__ == '__main__':\r\n    combining_nets_lead_to_error()\r\n```\r\n\r\n## System Info\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: 8.0.44\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.2)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n```"}