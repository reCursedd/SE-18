{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430313523", "html_url": "https://github.com/pytorch/pytorch/issues/12202#issuecomment-430313523", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12202", "id": 430313523, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDMxMzUyMw==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-16T16:53:57Z", "updated_at": "2018-10-16T16:53:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is expected. A tensor without a .grad field will have independent .grad fields after being sent to different processes.</p>\n<p>However, a tensor with an existing .grad field will have its .grad field shared among different processes. You should do something like</p>\n<pre><code># before sending tensor to a different process\ntensor.grad = torch.zeros_like(tensor)\n# send tensor to a different process\n</code></pre>\n<p>and then the gradients will all be connected.</p>", "body_text": "This is expected. A tensor without a .grad field will have independent .grad fields after being sent to different processes.\nHowever, a tensor with an existing .grad field will have its .grad field shared among different processes. You should do something like\n# before sending tensor to a different process\ntensor.grad = torch.zeros_like(tensor)\n# send tensor to a different process\n\nand then the gradients will all be connected.", "body": "This is expected. A tensor without a .grad field will have independent .grad fields after being sent to different processes.\r\n\r\nHowever, a tensor with an existing .grad field will have its .grad field shared among different processes. You should do something like\r\n```\r\n# before sending tensor to a different process\r\ntensor.grad = torch.zeros_like(tensor)\r\n# send tensor to a different process\r\n```\r\nand then the gradients will all be connected."}