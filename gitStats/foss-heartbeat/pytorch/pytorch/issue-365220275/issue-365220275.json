{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12202", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12202/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12202/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12202/events", "html_url": "https://github.com/pytorch/pytorch/issues/12202", "id": 365220275, "node_id": "MDU6SXNzdWUzNjUyMjAyNzU=", "number": 12202, "title": "Gradient disconnected after Multiprocessing pool(starmap)", "user": {"login": "ahyunSeo", "id": 22852018, "node_id": "MDQ6VXNlcjIyODUyMDE4", "avatar_url": "https://avatars0.githubusercontent.com/u/22852018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahyunSeo", "html_url": "https://github.com/ahyunSeo", "followers_url": "https://api.github.com/users/ahyunSeo/followers", "following_url": "https://api.github.com/users/ahyunSeo/following{/other_user}", "gists_url": "https://api.github.com/users/ahyunSeo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahyunSeo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahyunSeo/subscriptions", "organizations_url": "https://api.github.com/users/ahyunSeo/orgs", "repos_url": "https://api.github.com/users/ahyunSeo/repos", "events_url": "https://api.github.com/users/ahyunSeo/events{/privacy}", "received_events_url": "https://api.github.com/users/ahyunSeo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-30T12:17:39Z", "updated_at": "2018-10-16T16:53:57Z", "closed_at": "2018-10-16T16:53:57Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"question\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/2753.png\">\u2753</g-emoji> Questions and Help</h2>\n<p>Hello,<br>\nI'm using pytorch 0.4.1 with python 3.6, Anaconda, Ubuntu 16.04, Titan XP.<br>\nI have a problem with multiprocessing.<br>\n<strong>Basically I have to do sample-specific \"non-batchable\" operations within the batch,<br>\nso to make it faster I tried to use multiprocessing.</strong><br>\nThe operation consists of computing losses in sequential manner,<br>\nbut each of the samples have different length of the sequences.<br>\nAnyway,<br>\nI used pool and starmap.<br>\nAs an argument of the starmap I tried zip of torch.chunk and list of label_path.<br>\n<strong>I checked that until the torch chunk stage, the gradient works fine,<br>\nbut after it gets into the sub-process which is get_reward_loss, the gradient is disconnected.</strong></p>\n<blockquote>\n<ol>\n<li>How can I connect it?</li>\n<li>It it is impossible, is there any other way to do for-loop within the batch efficiently? (I know about Data-parallel)</li>\n</ol>\n</blockquote>\n<p>Here's the simplified snippet of my code.</p>\n<pre><code>def get_reward_loss(feat, path):\n    print(feat.requires_grad)   # Already False\n    # Do some operations to make input_feat\n    # Using path, get label and do some processing to get target\n    loss = criterion(input_feat, target)\n    return loss\n\ntrain_loader = DataLoader(MNISTToy(data_path, data_transform), batch_size=1, num_workers=1)\n\noptimizer = optim.Adam(net.parameters(), lr=0.01)\nt0 = datetime.now()\n\nif __name__ == '__main__':\n    set_start_method('spawn', force=True)\n    pool = Pool(processes=4)\n    for idx, (input, label_path) in enumerate(train_loader):\n        input = input.to(device)\n        optimizer.zero_grad()\n        output = net(input)\n        batch_loss = pool.starmap(get_reward_loss, zip(torch.chunk(output, batch_size, dim=0), label_path))\n        loss = # sum of batch_loss(this is list)\n        loss.backward()\n        optimizer.step()\n\n</code></pre>", "body_text": "\u2753 Questions and Help\nHello,\nI'm using pytorch 0.4.1 with python 3.6, Anaconda, Ubuntu 16.04, Titan XP.\nI have a problem with multiprocessing.\nBasically I have to do sample-specific \"non-batchable\" operations within the batch,\nso to make it faster I tried to use multiprocessing.\nThe operation consists of computing losses in sequential manner,\nbut each of the samples have different length of the sequences.\nAnyway,\nI used pool and starmap.\nAs an argument of the starmap I tried zip of torch.chunk and list of label_path.\nI checked that until the torch chunk stage, the gradient works fine,\nbut after it gets into the sub-process which is get_reward_loss, the gradient is disconnected.\n\n\nHow can I connect it?\nIt it is impossible, is there any other way to do for-loop within the batch efficiently? (I know about Data-parallel)\n\n\nHere's the simplified snippet of my code.\ndef get_reward_loss(feat, path):\n    print(feat.requires_grad)   # Already False\n    # Do some operations to make input_feat\n    # Using path, get label and do some processing to get target\n    loss = criterion(input_feat, target)\n    return loss\n\ntrain_loader = DataLoader(MNISTToy(data_path, data_transform), batch_size=1, num_workers=1)\n\noptimizer = optim.Adam(net.parameters(), lr=0.01)\nt0 = datetime.now()\n\nif __name__ == '__main__':\n    set_start_method('spawn', force=True)\n    pool = Pool(processes=4)\n    for idx, (input, label_path) in enumerate(train_loader):\n        input = input.to(device)\n        optimizer.zero_grad()\n        output = net(input)\n        batch_loss = pool.starmap(get_reward_loss, zip(torch.chunk(output, batch_size, dim=0), label_path))\n        loss = # sum of batch_loss(this is list)\n        loss.backward()\n        optimizer.step()", "body": "## \u2753 Questions and Help\r\n\r\nHello,\r\nI'm using pytorch 0.4.1 with python 3.6, Anaconda, Ubuntu 16.04, Titan XP.\r\nI have a problem with multiprocessing.\r\n**Basically I have to do sample-specific \"non-batchable\" operations within the batch,\r\nso to make it faster I tried to use multiprocessing.**\r\nThe operation consists of computing losses in sequential manner,\r\nbut each of the samples have different length of the sequences.\r\nAnyway,\r\nI used pool and starmap.\r\nAs an argument of the starmap I tried zip of torch.chunk and list of label_path.\r\n**I checked that until the torch chunk stage, the gradient works fine, \r\nbut after it gets into the sub-process which is get_reward_loss, the gradient is disconnected.**\r\n\r\n> 1. How can I connect it?\r\n> 2. It it is impossible, is there any other way to do for-loop within the batch efficiently? (I know about Data-parallel)\r\n\r\nHere's the simplified snippet of my code.\r\n\r\n```\r\ndef get_reward_loss(feat, path):\r\n    print(feat.requires_grad)   # Already False\r\n    # Do some operations to make input_feat\r\n    # Using path, get label and do some processing to get target\r\n    loss = criterion(input_feat, target)\r\n    return loss\r\n\r\ntrain_loader = DataLoader(MNISTToy(data_path, data_transform), batch_size=1, num_workers=1)\r\n\r\noptimizer = optim.Adam(net.parameters(), lr=0.01)\r\nt0 = datetime.now()\r\n\r\nif __name__ == '__main__':\r\n    set_start_method('spawn', force=True)\r\n    pool = Pool(processes=4)\r\n    for idx, (input, label_path) in enumerate(train_loader):\r\n        input = input.to(device)\r\n        optimizer.zero_grad()\r\n        output = net(input)\r\n        batch_loss = pool.starmap(get_reward_loss, zip(torch.chunk(output, batch_size, dim=0), label_path))\r\n        loss = # sum of batch_loss(this is list)\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n```"}