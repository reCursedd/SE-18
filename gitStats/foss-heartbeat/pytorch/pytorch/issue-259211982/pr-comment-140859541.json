{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140859541", "pull_request_review_id": 64956713, "id": 140859541, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDg1OTU0MQ==", "diff_hunk": "@@ -84,18 +286,79 @@ def create_autograd_functions(top_env, declarations):\n     These contain the auto-generated subclasses of torch::autograd::Function\n     for each every differentiable torch function.\n     \"\"\"\n-    # function_definitions = top_env['autograd_function_definitions']\n-    # function_declarations = top_env['autograd_function_declarations']\n+    function_definitions = top_env['autograd_function_definitions']\n+    function_declarations = top_env['autograd_function_declarations']\n+    py_function_initializers = top_env['py_autograd_function_initializers']\n+\n+    def group_by_name(options):\n+        res = {}\n+        for option in options:\n+            name = option['name']\n+            if name not in res:\n+                res[name] = []\n+            res[name].append(option)\n+        return res\n+\n+    for name, options in group_by_name(declarations).items():\n+        for i, option in enumerate(options):\n+            name = option['name']\n+            option['op'] = name[0].upper() + name[1:] + 'Backward'\n+            if len(options) > 1:\n+                option['op'] += str(i)\n+\n+    def process_function(op):\n+        if op['fallthrough']:\n+            return\n+\n+        saved_variables = []\n+        for arg in op['saved']:\n+            name = arg['name']\n+            if arg['type'] == 'Tensor':\n+                saved_variables.append('SavedVariable {}_;'.format(name))\n+            elif arg['type'] == 'IntList':\n+                saved_variables.append('std::vector<int64_t> {};'.format(name))\n+            else:\n+                saved_variables.append('{} {};'.format(arg['type'], name))\n+        op['saved_variables'] = saved_variables\n \n-    def process_function(option):\n-        # TODO: generate autograd::Function classes for each function specified\n-        # in derivatives.yaml\n-        pass\n+        body = []\n+        body.append('auto& grad = inputs[0];')\n+\n+        def unpack_args(derivative):\n+            unpack = []\n+            for arg in op['saved']:\n+                if arg['type'] == 'Tensor':\n+                    name = arg['name']\n+                    unpack.append('auto {} = {}_.unpack();'.format(name, name))\n+            return unpack\n+\n+        i = 0\n+        for arg in op['python_arguments']:\n+            derivative = arg.get('derivative')\n+            if derivative is None:\n+                continue\n+            body.append(DERIVATIVE.substitute({\n+                'unpack_save_variables': unpack_args(derivative),\n+                'i': i,\n+                'derivative': derivative,\n+            }))\n+            i += 1\n+\n+        op['body'] = body\n+        function_declarations.append(FUNCTION_DECLARATION.substitute(op))\n+        function_definitions.append(FUNCTION_DEFINITION.substitute(op))\n+        py_function_initializers.append(PY_FUNCTION_DEFINITION.substitute(op))\n \n     for option in declarations:\n         process_function(option)\n \n \n+def is_implemented(option):\n+    return (option['return_type'] in FALLTHROUGH_RETURN_TYPES or\n+            option['name'] in FALLTHROUGH_FUNCTIONS or\n+            option.get('derivative') is not None)", "path": "tools/autograd/gen_variable_type.py", "position": 338, "original_position": 341, "commit_id": "e471756dd09f27c1ac6fff4e977d1faeb4b030cf", "original_commit_id": "a5e60c1254517fffaebc9f5432faf08fb533ae4e", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Going to get rid of option['fallthrough'] for now.\r\n\r\nIf the method falls through to the base tensor type (e.g. `is_contiguous`) then we implement it in VariableType.cpp. If it has a derivative formula, we also implement the VariableType function.\r\n\r\nOtherwise, we stick a placeholder `throw std::runtime_error` in VariableType.cpp so that as you scroll through the generated file you can see which functions still need to be implemented.", "created_at": "2017-09-25T18:29:11Z", "updated_at": "2018-11-23T15:34:48Z", "html_url": "https://github.com/pytorch/pytorch/pull/2805#discussion_r140859541", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2805", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140859541"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2805#discussion_r140859541"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2805"}}, "body_html": "<p>Going to get rid of option['fallthrough'] for now.</p>\n<p>If the method falls through to the base tensor type (e.g. <code>is_contiguous</code>) then we implement it in VariableType.cpp. If it has a derivative formula, we also implement the VariableType function.</p>\n<p>Otherwise, we stick a placeholder <code>throw std::runtime_error</code> in VariableType.cpp so that as you scroll through the generated file you can see which functions still need to be implemented.</p>", "body_text": "Going to get rid of option['fallthrough'] for now.\nIf the method falls through to the base tensor type (e.g. is_contiguous) then we implement it in VariableType.cpp. If it has a derivative formula, we also implement the VariableType function.\nOtherwise, we stick a placeholder throw std::runtime_error in VariableType.cpp so that as you scroll through the generated file you can see which functions still need to be implemented.", "in_reply_to_id": 140109681}