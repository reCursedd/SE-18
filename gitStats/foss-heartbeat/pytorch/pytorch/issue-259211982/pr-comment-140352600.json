{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140352600", "pull_request_review_id": 64415147, "id": 140352600, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDM1MjYwMA==", "diff_hunk": "@@ -1,5 +1,355 @@\n-# TODO: define derivatives e.g.\n-# - name: addbmm(Scalar beta=1, Tensor self, Scalar alpha=1, Tensor mat1, Tensor mat2)\n-#   self: grad * alpha\n-#   mat1: grad.bmm(mat2.transpose(1, 2)) * beta\n-#   mat2: mat1.transpose(1, 2).bmm(grad) * beta\n+# Defines derivative formulas and Python signatures of methods on Variable\n+\n+- name: __and__\n+- name: __iand__\n+- name: __ilshift__\n+- name: __ior__\n+- name: __irshift__\n+- name: __ixor__\n+- name: __lshift__\n+- name: __or__\n+- name: __rshift__\n+- name: __xor__\n+\n+- name: abs(Tensor self)\n+  self: grad * self.sign()\n+\n+- name: acos(Tensor self)\n+  self: grad * -((-self * self + 1).sqrt().reciprocal())\n+\n+- name: add(Tensor self, Scalar other, *, Scalar alpha=1)\n+  self: grad\n+\n+- name: add(Tensor self, Tensor other, *, Scalar alpha=1)\n+  aten: add(self, alpha, other)\n+  self: grad\n+  other: grad * alpha\n+\n+- name: addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1)\n+  aten: addbmm(beta, self, alpha, batch1, batch2)\n+  self: grad * alpha", "path": "tools/autograd/derivatives.yaml", "position": 35, "original_position": 35, "commit_id": "e471756dd09f27c1ac6fff4e977d1faeb4b030cf", "original_commit_id": "a5e60c1254517fffaebc9f5432faf08fb533ae4e", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "We can always use `grad` for single output things just for clarity, and `gradN` in case of multiple outputs (kind of like we automatically number backward functions). On the other hand I'm not sure if it will ever be needed - in most cases the additional outputs are things like indices, that are non-differentiable, and they will be leaves once we stop recording graphs for things that don't require grad.", "created_at": "2017-09-21T20:37:35Z", "updated_at": "2018-11-23T15:34:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/2805#discussion_r140352600", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2805", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140352600"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2805#discussion_r140352600"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2805"}}, "body_html": "<p>We can always use <code>grad</code> for single output things just for clarity, and <code>gradN</code> in case of multiple outputs (kind of like we automatically number backward functions). On the other hand I'm not sure if it will ever be needed - in most cases the additional outputs are things like indices, that are non-differentiable, and they will be leaves once we stop recording graphs for things that don't require grad.</p>", "body_text": "We can always use grad for single output things just for clarity, and gradN in case of multiple outputs (kind of like we automatically number backward functions). On the other hand I'm not sure if it will ever be needed - in most cases the additional outputs are things like indices, that are non-differentiable, and they will be leaves once we stop recording graphs for things that don't require grad.", "in_reply_to_id": 140314552}