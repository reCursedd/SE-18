{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140837635", "pull_request_review_id": 64956713, "id": 140837635, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDgzNzYzNQ==", "diff_hunk": "@@ -120,47 +399,128 @@ def unpack_args(option):\n         option['unpacked_args'] = unpacked_args\n         return body\n \n+    def emit_body(option):\n+        if not is_implemented(option):\n+            return METHOD_DEFINITION_NYI.substitute(option)\n+\n+        body = []\n+        body += unpack_args(option)\n+        if option['return_type'] in FALLTHROUGH_RETURN_TYPES or option.get('fallthrough'):\n+            body.extend(METHOD_DEFINITION_FALLTHROUGH.substitute(option).split('\\n'))\n+            return body\n+        elif option['derivative'] is None:\n+            assert option['name'] in FALLTHROUGH_FUNCTIONS\n+            body.extend(METHOD_DEFINITION_FALLTHROUGH_VARIABLE.substitute(option).split('\\n'))\n+            return body\n+\n+        if option['inplace']:\n+            body.extend(METHOD_DEFINITION_INPLACE.substitute(option).split('\\n'))\n+        else:\n+            body.extend(METHOD_DEFINITION_DERIVATIVE.substitute(option).split('\\n'))\n+        return body\n+\n     def process_function(option):\n         option['formals'] = [arg['type'] + ' ' + arg['name']\n                              for arg in option['arguments']]\n         option['args'] = [arg['name'] for arg in option['arguments']]\n         option['api_name'] = option['name']\n         return_type = format_return_type(option['returns'])\n         option['return_type'] = return_type\n+        if option.get('derivative') is not None:\n+            derivative = option['derivative']\n+            option['op'] = derivative['op']\n+            option['save_variables'] = save_variables(option, derivative)\n+            option['tensor_args'] = [arg['name'] for arg in option['arguments']\n+                                     if arg['dynamic_type'] == 'Tensor']\n+        if return_type == 'Scalar':\n+            option['return_value'] = 'Scalar(output)'\n+        else:\n+            option['return_value'] = 'Tensor(std::move(output))'\n+\n         option['type_definition_body'] = emit_body(option)\n \n         type_declarations.append(METHOD_DECLARATION.substitute(option))\n-        type_definitions.append(METHOD_DEFINITION.substitute(option))\n-\n-    def emit_body(option):\n-        body = []\n-        body += unpack_args(option)\n-\n-        if option['return_type'] in FALLTHROUGH_RETURN_TYPES:\n-            body.extend(METHOD_DEFINITION_FALLTHROUGH.substitute(option).split('\\n'))\n-            return body\n-\n-        return METHOD_DEFINITION_NYI.substitute(option)\n+        if option['name'] != 'resize_':\n+            type_definitions.append(METHOD_DEFINITION.substitute(option))\n \n     for function in aten_declarations:\n         process_function(function)\n \n \n-def create_python_bindings(top_env, aten_decls, derivatives):\n+def create_python_bindings(top_env, python_functions):\n     \"\"\"python_variable_methods.cpp\n \n     Generates Python bindings to Variable methods\n     \"\"\"\n+    py_methods = top_env['py_methods']\n+    py_method_defs = top_env['py_method_defs']\n+    py_method_dispatch = top_env['py_method_dispatch']\n \n-    # py_methods = top_env['py_methods']\n-    # py_method_defs = top_env['py_method_defs']\n-\n-    def process_option(option):\n-        # TODO: generate Python bindings\n-        pass\n+    unpack_methods = {\n+        'int64_t': 'toInt64',\n+        'bool': 'toBool'\n+    }\n \n-    for option in aten_decls:\n-        process_option(option)\n+    def args_without_self(args):\n+        return [arg for arg in args if arg['name'] != 'self']\n+\n+    def emit_dispatch(i, option):\n+        env = {}\n+\n+        args = []\n+        formal_args = ['Tensor & self']\n+        python_params = args_without_self(option['python_arguments'])\n+        for arg_idx, arg in enumerate(python_params):\n+            unpack = unpack_methods.get(arg['type'], arg['type'].lower())\n+            args.append('r.{}({})'.format(unpack, arg_idx))\n+            dispatch_type = arg['type']\n+            dispatch_type = dispatch_type.replace('Tensor', 'const Tensor &')\n+            formal_args.append('{} {}'.format(dispatch_type, arg['name']))\n+\n+        env['i'] = i\n+        env['dispatch_args'] = [arg for arg in option['call_args'] if arg != 'self']\n+        env['args_with_self'] = ['self_'] + args\n+        env['AutoNoGIL'] = 'AutoNoGIL no_gil;'\n+        env['AutoGPU'] = 'AutoGPU auto_gpu(self);'\n+        env['formal_args'] = formal_args\n+        env['cond'] = 'if' if i == 0 else '} else if'\n+        env = nested_dict(env, option)\n+        py_method_dispatch.append(PY_VARIABLE_DISPATCH.substitute(env))\n+        return PY_VARIABLE_CASE.substitute(env)", "path": "tools/autograd/gen_variable_type.py", "position": 490, "original_position": 481, "commit_id": "e471756dd09f27c1ac6fff4e977d1faeb4b030cf", "original_commit_id": "a5e60c1254517fffaebc9f5432faf08fb533ae4e", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "We talked about this a bit in-person. To summarize: the dispatch functions release the GIL and set the current device. It's awkward to do this directly in the bindings because the calls to e.g. `r.tensor(0)` require the GIL.\r\n\r\n[py_variable_methods.cpp](https://gist.github.com/colesbury/779e9c4f05d5bc3160a72af66f196f73)\r\n[py_variable_methods_dispatch.h](https://gist.github.com/colesbury/803c348ede74fa4190ec7bf52a119a6e)", "created_at": "2017-09-25T17:08:36Z", "updated_at": "2018-11-23T15:34:47Z", "html_url": "https://github.com/pytorch/pytorch/pull/2805#discussion_r140837635", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2805", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/140837635"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2805#discussion_r140837635"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2805"}}, "body_html": "<p>We talked about this a bit in-person. To summarize: the dispatch functions release the GIL and set the current device. It's awkward to do this directly in the bindings because the calls to e.g. <code>r.tensor(0)</code> require the GIL.</p>\n<p><a href=\"https://gist.github.com/colesbury/779e9c4f05d5bc3160a72af66f196f73\">py_variable_methods.cpp</a><br>\n<a href=\"https://gist.github.com/colesbury/803c348ede74fa4190ec7bf52a119a6e\">py_variable_methods_dispatch.h</a></p>", "body_text": "We talked about this a bit in-person. To summarize: the dispatch functions release the GIL and set the current device. It's awkward to do this directly in the bindings because the calls to e.g. r.tensor(0) require the GIL.\npy_variable_methods.cpp\npy_variable_methods_dispatch.h", "in_reply_to_id": 140289699}