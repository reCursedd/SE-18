{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/369731906", "html_url": "https://github.com/pytorch/pytorch/pull/5503#issuecomment-369731906", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5503", "id": 369731906, "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTczMTkwNg==", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-01T21:10:40Z", "updated_at": "2018-03-01T21:23:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Look great, had mostly nits.<br>\nFor generic types, the way you'll want to do this is move all your logic into a separate template function (e.g. in an anonymous namespace above):</p>\n<pre><code>template&lt;typename scalar_t&gt;\nvoid unique_op(at::Tensor ...) {\n// use scalar_t instead of int64_t here\n}\n</code></pre>\n<p>then in the current <code>unique</code> function, something like:</p>\n<pre><code>AT_DISPATCH_ALL_TYPES(self.type(), \"unique\", [&amp;] {\n  unique_op&lt;scalar_&gt;t(tensors...);\n});\n</code></pre>\n<p>See <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/EmbeddingBag.cu#L269\">here</a> for an example.</p>\n<p>EDIT: This is a better example for CPU: <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorCompare.cpp\">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorCompare.cpp</a></p>", "body_text": "Look great, had mostly nits.\nFor generic types, the way you'll want to do this is move all your logic into a separate template function (e.g. in an anonymous namespace above):\ntemplate<typename scalar_t>\nvoid unique_op(at::Tensor ...) {\n// use scalar_t instead of int64_t here\n}\n\nthen in the current unique function, something like:\nAT_DISPATCH_ALL_TYPES(self.type(), \"unique\", [&] {\n  unique_op<scalar_>t(tensors...);\n});\n\nSee here for an example.\nEDIT: This is a better example for CPU: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorCompare.cpp", "body": "Look great, had mostly nits.\r\nFor generic types, the way you'll want to do this is move all your logic into a separate template function (e.g. in an anonymous namespace above):\r\n```\r\ntemplate<typename scalar_t>\r\nvoid unique_op(at::Tensor ...) {\r\n// use scalar_t instead of int64_t here\r\n}\r\n```\r\n\r\nthen in the current `unique` function, something like:\r\n```\r\nAT_DISPATCH_ALL_TYPES(self.type(), \"unique\", [&] {\r\n  unique_op<scalar_>t(tensors...);\r\n});\r\n```\r\n\r\nSee [here](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/EmbeddingBag.cu#L269) for an example. \r\n\r\nEDIT: This is a better example for CPU: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorCompare.cpp"}