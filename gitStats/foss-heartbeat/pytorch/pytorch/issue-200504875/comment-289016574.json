{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/289016574", "html_url": "https://github.com/pytorch/pytorch/issues/440#issuecomment-289016574", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/440", "id": 289016574, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTAxNjU3NA==", "user": {"login": "chenyuntc", "id": 9301117, "node_id": "MDQ6VXNlcjkzMDExMTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/9301117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenyuntc", "html_url": "https://github.com/chenyuntc", "followers_url": "https://api.github.com/users/chenyuntc/followers", "following_url": "https://api.github.com/users/chenyuntc/following{/other_user}", "gists_url": "https://api.github.com/users/chenyuntc/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenyuntc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenyuntc/subscriptions", "organizations_url": "https://api.github.com/users/chenyuntc/orgs", "repos_url": "https://api.github.com/users/chenyuntc/repos", "events_url": "https://api.github.com/users/chenyuntc/events{/privacy}", "received_events_url": "https://api.github.com/users/chenyuntc/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-24T13:01:56Z", "updated_at": "2017-03-24T13:01:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I try to implement <code>Var</code> like this, I don't know whether it's more memory-efficient, but it's a little troublesome to handle <code>dim = None</code> in <code>Tensor</code></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Var</span>(<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">unbiased</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">self</span>.dim <span class=\"pl-k\">=</span> dim\n        <span class=\"pl-c1\">self</span>.unbiased <span class=\"pl-k\">=</span> unbiased\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.dim <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            mean <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.new(<span class=\"pl-c1\">1</span>).fill_(<span class=\"pl-c1\">input</span>.mean()).view(<span class=\"pl-k\">*</span>(<span class=\"pl-c1\">1</span> <span class=\"pl-k\">for</span> s <span class=\"pl-k\">in</span> <span class=\"pl-c1\">input</span>.size()))\n        <span class=\"pl-k\">else</span>:\n            mean <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.mean(<span class=\"pl-c1\">self</span>.dim)\n        mean_expanded <span class=\"pl-k\">=</span> mean.expand_as(<span class=\"pl-c1\">input</span>)\n        zero_centered <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.sub(mean_expanded)\n        <span class=\"pl-c1\">self</span>.zero_centered <span class=\"pl-k\">=</span> zero_centered\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.dim <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            var <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.new(<span class=\"pl-c1\">1</span>).fill_(zero_centered.mul(zero_centered).sum())\n        <span class=\"pl-k\">else</span>:\n            var <span class=\"pl-k\">=</span> zero_centered.mul(zero_centered).sum(<span class=\"pl-c1\">self</span>.dim)\n        numel <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.numel() <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.dim <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">self</span>.dim)\n        <span class=\"pl-c1\">self</span>.n <span class=\"pl-k\">=</span> numel <span class=\"pl-k\">-</span> <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">self</span>.unbiased)\n        <span class=\"pl-k\">return</span> var.div(<span class=\"pl-c1\">self</span>.n)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> import pdb; pdb.set_trace()</span>\n        <span class=\"pl-k\">return</span> grad_output.expand_as(<span class=\"pl-c1\">self</span>.zero_centered) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.zero_centered <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">2.0</span> <span class=\"pl-k\">/</span> <span class=\"pl-c1\">self</span>.n)</pre></div>", "body_text": "I try to implement Var like this, I don't know whether it's more memory-efficient, but it's a little troublesome to handle dim = None in Tensor\nclass Var(Function):\n    def __init__(self, dim=None, unbiased=True):\n        self.dim = dim\n        self.unbiased = unbiased\n\n    def forward(self, input):\n        if self.dim is None:\n            mean = input.new(1).fill_(input.mean()).view(*(1 for s in input.size()))\n        else:\n            mean = input.mean(self.dim)\n        mean_expanded = mean.expand_as(input)\n        zero_centered = input.sub(mean_expanded)\n        self.zero_centered = zero_centered\n        if self.dim is None:\n            var = input.new(1).fill_(zero_centered.mul(zero_centered).sum())\n        else:\n            var = zero_centered.mul(zero_centered).sum(self.dim)\n        numel = input.numel() if self.dim is None else input.size(self.dim)\n        self.n = numel - int(self.unbiased)\n        return var.div(self.n)\n\n    def backward(self, grad_output):\n        # import pdb; pdb.set_trace()\n        return grad_output.expand_as(self.zero_centered) * self.zero_centered * (2.0 / self.n)", "body": "I try to implement `Var` like this, I don't know whether it's more memory-efficient, but it's a little troublesome to handle `dim = None` in `Tensor`\r\n```python\r\nclass Var(Function):\r\n    def __init__(self, dim=None, unbiased=True):\r\n        self.dim = dim\r\n        self.unbiased = unbiased\r\n\r\n    def forward(self, input):\r\n        if self.dim is None:\r\n            mean = input.new(1).fill_(input.mean()).view(*(1 for s in input.size()))\r\n        else:\r\n            mean = input.mean(self.dim)\r\n        mean_expanded = mean.expand_as(input)\r\n        zero_centered = input.sub(mean_expanded)\r\n        self.zero_centered = zero_centered\r\n        if self.dim is None:\r\n            var = input.new(1).fill_(zero_centered.mul(zero_centered).sum())\r\n        else:\r\n            var = zero_centered.mul(zero_centered).sum(self.dim)\r\n        numel = input.numel() if self.dim is None else input.size(self.dim)\r\n        self.n = numel - int(self.unbiased)\r\n        return var.div(self.n)\r\n\r\n    def backward(self, grad_output):\r\n        # import pdb; pdb.set_trace()\r\n        return grad_output.expand_as(self.zero_centered) * self.zero_centered * (2.0 / self.n)\r\n```\r\n"}