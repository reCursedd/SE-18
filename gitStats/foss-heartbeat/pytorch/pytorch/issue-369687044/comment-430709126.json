{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430709126", "html_url": "https://github.com/pytorch/pytorch/issues/12614#issuecomment-430709126", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12614", "id": 430709126, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDcwOTEyNg==", "user": {"login": "asford", "id": 282792, "node_id": "MDQ6VXNlcjI4Mjc5Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/282792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asford", "html_url": "https://github.com/asford", "followers_url": "https://api.github.com/users/asford/followers", "following_url": "https://api.github.com/users/asford/following{/other_user}", "gists_url": "https://api.github.com/users/asford/gists{/gist_id}", "starred_url": "https://api.github.com/users/asford/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asford/subscriptions", "organizations_url": "https://api.github.com/users/asford/orgs", "repos_url": "https://api.github.com/users/asford/repos", "events_url": "https://api.github.com/users/asford/events{/privacy}", "received_events_url": "https://api.github.com/users/asford/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T17:03:57Z", "updated_at": "2018-10-17T17:03:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This appears to be an issue with how numba is picking up the cuda installation on the image. I've been able to resolve it by explicitly providing the <a href=\"https://numba.pydata.org/numba-doc/dev/user/installing.html#installing-using-pip-on-x86-x86-64-platforms\" rel=\"nofollow\">path to the NVVM library via numba-specific env vars</a>.</p>\n<pre><code>+fordas@fela:/scratch/USERS/fordas/workspace$ nvidia-docker run -it soumith/manylinux-cuda80 /bin/bash\n[root@f0ac885cb355 /]# export PATH=/opt/python/cp27-cp27mu/bin:$PATH\n[root@f0ac885cb355 /]# pip install numba\nCollecting numba\n    [...]\nInstalling collected packages: enum34, llvmlite, funcsigs, six, singledispatch, numpy, numba\nSuccessfully installed enum34-1.1.6 funcsigs-1.0.2 llvmlite-0.25.0 numba-0.40.1 numpy-1.15.2 singledispatch-3.4.0.3 six-1.11.0\n</code></pre>\n<pre><code>[root@f0ac885cb355 /]# export NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so\n[root@f0ac885cb355 /]# python -c 'import numba.cuda; numba.cuda.detect(); print \"numba.cuda.is_available: %s\" % numba.cuda.is_available()'\nFound 2 CUDA devices\nid 0    Tesla V100-PCIE-32GB                              [SUPPORTED]\n                      compute capability: 7.0\n                           pci device id: 0\n                              pci bus id: 94\nid 1    Tesla V100-PCIE-32GB                              [SUPPORTED]\n                      compute capability: 7.0\n                           pci device id: 0\n                              pci bus id: 134\nSummary:\n        2/2 devices are supported\nnumba.cuda.is_available: True\n</code></pre>\n<p>I'm not familiar with your nightly infrastructure, but does it run within a conda environment? If so it <em>may</em> be simpler to just install <a href=\"https://numba.pydata.org/numba-doc/dev/user/installing.html#installing-using-conda-on-x86-x86-64-power-platforms\" rel=\"nofollow\"><code>numba</code> &amp; <code>cudatoolkit</code> via conda</a>.</p>", "body_text": "This appears to be an issue with how numba is picking up the cuda installation on the image. I've been able to resolve it by explicitly providing the path to the NVVM library via numba-specific env vars.\n+fordas@fela:/scratch/USERS/fordas/workspace$ nvidia-docker run -it soumith/manylinux-cuda80 /bin/bash\n[root@f0ac885cb355 /]# export PATH=/opt/python/cp27-cp27mu/bin:$PATH\n[root@f0ac885cb355 /]# pip install numba\nCollecting numba\n    [...]\nInstalling collected packages: enum34, llvmlite, funcsigs, six, singledispatch, numpy, numba\nSuccessfully installed enum34-1.1.6 funcsigs-1.0.2 llvmlite-0.25.0 numba-0.40.1 numpy-1.15.2 singledispatch-3.4.0.3 six-1.11.0\n\n[root@f0ac885cb355 /]# export NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so\n[root@f0ac885cb355 /]# python -c 'import numba.cuda; numba.cuda.detect(); print \"numba.cuda.is_available: %s\" % numba.cuda.is_available()'\nFound 2 CUDA devices\nid 0    Tesla V100-PCIE-32GB                              [SUPPORTED]\n                      compute capability: 7.0\n                           pci device id: 0\n                              pci bus id: 94\nid 1    Tesla V100-PCIE-32GB                              [SUPPORTED]\n                      compute capability: 7.0\n                           pci device id: 0\n                              pci bus id: 134\nSummary:\n        2/2 devices are supported\nnumba.cuda.is_available: True\n\nI'm not familiar with your nightly infrastructure, but does it run within a conda environment? If so it may be simpler to just install numba & cudatoolkit via conda.", "body": "This appears to be an issue with how numba is picking up the cuda installation on the image. I've been able to resolve it by explicitly providing the [path to the NVVM library via numba-specific env vars](https://numba.pydata.org/numba-doc/dev/user/installing.html#installing-using-pip-on-x86-x86-64-platforms).\r\n\r\n```\r\n+fordas@fela:/scratch/USERS/fordas/workspace$ nvidia-docker run -it soumith/manylinux-cuda80 /bin/bash\r\n[root@f0ac885cb355 /]# export PATH=/opt/python/cp27-cp27mu/bin:$PATH\r\n[root@f0ac885cb355 /]# pip install numba\r\nCollecting numba\r\n    [...]\r\nInstalling collected packages: enum34, llvmlite, funcsigs, six, singledispatch, numpy, numba\r\nSuccessfully installed enum34-1.1.6 funcsigs-1.0.2 llvmlite-0.25.0 numba-0.40.1 numpy-1.15.2 singledispatch-3.4.0.3 six-1.11.0\r\n```\r\n```\r\n[root@f0ac885cb355 /]# export NUMBAPRO_NVVM=/usr/local/cuda/nvvm/lib64/libnvvm.so\r\n[root@f0ac885cb355 /]# python -c 'import numba.cuda; numba.cuda.detect(); print \"numba.cuda.is_available: %s\" % numba.cuda.is_available()'\r\nFound 2 CUDA devices\r\nid 0    Tesla V100-PCIE-32GB                              [SUPPORTED]\r\n                      compute capability: 7.0\r\n                           pci device id: 0\r\n                              pci bus id: 94\r\nid 1    Tesla V100-PCIE-32GB                              [SUPPORTED]\r\n                      compute capability: 7.0\r\n                           pci device id: 0\r\n                              pci bus id: 134\r\nSummary:\r\n        2/2 devices are supported\r\nnumba.cuda.is_available: True\r\n```\r\n\r\nI'm not familiar with your nightly infrastructure, but does it run within a conda environment? If so it _may_ be simpler to just install [`numba` & `cudatoolkit` via conda](https://numba.pydata.org/numba-doc/dev/user/installing.html#installing-using-conda-on-x86-x86-64-power-platforms)."}