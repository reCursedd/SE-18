{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/384606170", "html_url": "https://github.com/pytorch/pytorch/pull/1583#issuecomment-384606170", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1583", "id": 384606170, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDYwNjE3MA==", "user": {"login": "bwesen", "id": 3290190, "node_id": "MDQ6VXNlcjMyOTAxOTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/3290190?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bwesen", "html_url": "https://github.com/bwesen", "followers_url": "https://api.github.com/users/bwesen/followers", "following_url": "https://api.github.com/users/bwesen/following{/other_user}", "gists_url": "https://api.github.com/users/bwesen/gists{/gist_id}", "starred_url": "https://api.github.com/users/bwesen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bwesen/subscriptions", "organizations_url": "https://api.github.com/users/bwesen/orgs", "repos_url": "https://api.github.com/users/bwesen/repos", "events_url": "https://api.github.com/users/bwesen/events{/privacy}", "received_events_url": "https://api.github.com/users/bwesen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-26T11:27:39Z", "updated_at": "2018-04-26T11:27:39Z", "author_association": "NONE", "body_html": "<p>I've tested this as well a bit more, and while functionally the unfold solution works fine, it requires gigantonormous intermediary tensors as unfold has to expand the input tensor into a form that can be directly element-wise multiplied with the weights, and if the input tensor is batched, the intermediaries often don't fit in the GPU even if you have 10 GB of VRAM.</p>\n<p>For example, I tried a relatively small 128x128 input with a 13x13 kernel, this requires a weight tensor of 128x128x13x13 which is around 10 MBytes. But as soon as you start batching (which is required for speed), even for a very small batch of 32 this is 0.3 GB, and the unfold solution requires more than one such intermediary.</p>\n<p>I'm not saying the proposed solution shouldn't be merged as it's much better than nothing, but it would be super awesome if there could be a more native solution that doesn't require the scratchpad RAM. The operation itself is local so this is theoretically possible at least. I'd be happy to contribute it but I'm a rookie at pyTorch and there seems to be so many different interacting backends that I'm not sure where to actually put such an implementation.</p>\n<p>By the way, this is not in cuDNN, but it would be interesting to know if Nvidia has an implementation in the works.</p>", "body_text": "I've tested this as well a bit more, and while functionally the unfold solution works fine, it requires gigantonormous intermediary tensors as unfold has to expand the input tensor into a form that can be directly element-wise multiplied with the weights, and if the input tensor is batched, the intermediaries often don't fit in the GPU even if you have 10 GB of VRAM.\nFor example, I tried a relatively small 128x128 input with a 13x13 kernel, this requires a weight tensor of 128x128x13x13 which is around 10 MBytes. But as soon as you start batching (which is required for speed), even for a very small batch of 32 this is 0.3 GB, and the unfold solution requires more than one such intermediary.\nI'm not saying the proposed solution shouldn't be merged as it's much better than nothing, but it would be super awesome if there could be a more native solution that doesn't require the scratchpad RAM. The operation itself is local so this is theoretically possible at least. I'd be happy to contribute it but I'm a rookie at pyTorch and there seems to be so many different interacting backends that I'm not sure where to actually put such an implementation.\nBy the way, this is not in cuDNN, but it would be interesting to know if Nvidia has an implementation in the works.", "body": "I've tested this as well a bit more, and while functionally the unfold solution works fine, it requires gigantonormous intermediary tensors as unfold has to expand the input tensor into a form that can be directly element-wise multiplied with the weights, and if the input tensor is batched, the intermediaries often don't fit in the GPU even if you have 10 GB of VRAM.\r\n\r\nFor example, I tried a relatively small 128x128 input with a 13x13 kernel, this requires a weight tensor of 128x128x13x13 which is around 10 MBytes. But as soon as you start batching (which is required for speed), even for a very small batch of 32 this is 0.3 GB, and the unfold solution requires more than one such intermediary.\r\n\r\nI'm not saying the proposed solution shouldn't be merged as it's much better than nothing, but it would be super awesome if there could be a more native solution that doesn't require the scratchpad RAM. The operation itself is local so this is theoretically possible at least. I'd be happy to contribute it but I'm a rookie at pyTorch and there seems to be so many different interacting backends that I'm not sure where to actually put such an implementation.\r\n\r\nBy the way, this is not in cuDNN, but it would be interesting to know if Nvidia has an implementation in the works."}