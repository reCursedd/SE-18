{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2330", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2330/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2330/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2330/events", "html_url": "https://github.com/pytorch/pytorch/issues/2330", "id": 248608068, "node_id": "MDU6SXNzdWUyNDg2MDgwNjg=", "number": 2330, "title": "[BatchNorm1d] Question on the gradient of bias", "user": {"login": "cdluminate", "id": 5723047, "node_id": "MDQ6VXNlcjU3MjMwNDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5723047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cdluminate", "html_url": "https://github.com/cdluminate", "followers_url": "https://api.github.com/users/cdluminate/followers", "following_url": "https://api.github.com/users/cdluminate/following{/other_user}", "gists_url": "https://api.github.com/users/cdluminate/gists{/gist_id}", "starred_url": "https://api.github.com/users/cdluminate/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cdluminate/subscriptions", "organizations_url": "https://api.github.com/users/cdluminate/orgs", "repos_url": "https://api.github.com/users/cdluminate/repos", "events_url": "https://api.github.com/users/cdluminate/events{/privacy}", "received_events_url": "https://api.github.com/users/cdluminate/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-08T05:55:17Z", "updated_at": "2017-08-08T06:09:19Z", "closed_at": "2017-08-08T06:09:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>According to <a href=\"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\" rel=\"nofollow\">https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html</a><br>\n, the gradient of the bias in a BatchNorm1d layer should be <code>gradOutput.sum(0)</code> given that the size of gradOutput is <code>(N,D)</code>.</p>\n<pre><code>#step9\ndbeta = np.sum(dout, axis=0)\n</code></pre>\n<p>However, when I'm verifying this, pytorch seems to use <code>input.sum(0)</code> as the bias gradient.</p>\n<p>Well ... if we formulate the affine part as <code>y = vx + b</code>, then <code>\u2202y/\u2202b = x</code> ??</p>\n<p>I searched the code and the only part that checked BatchNorm1d layer just uses one without affine transformation. <code>test_autograd.py</code></p>\n<div class=\"highlight highlight-source-python\"><pre> <span class=\"pl-c1\">307</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">test_hooks_cpp</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):                                                   \n <span class=\"pl-c1\">308</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> Tests hooks for autograd function implemented in C++                  </span>\n <span class=\"pl-c1\">309</span>         bn <span class=\"pl-k\">=</span> torch.nn.BatchNorm1d(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">affine</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)                              \n <span class=\"pl-c1\">310</span>         bn.eval()                                                               \n <span class=\"pl-c1\">311</span>                                                                                 \n <span class=\"pl-c1\">312</span>         counter <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">0</span>]                                                           \n <span class=\"pl-c1\">313</span>                                                                                 \n <span class=\"pl-c1\">314</span>         <span class=\"pl-k\">def</span> <span class=\"pl-en\">bw_hook</span>(<span class=\"pl-smi\">grad</span>):                                                      \n <span class=\"pl-c1\">315</span>             counter[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>                                                     \n <span class=\"pl-c1\">316</span>             <span class=\"pl-k\">return</span> grad <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>                                                     \n <span class=\"pl-c1\">317</span>                                                                                 \n <span class=\"pl-c1\">318</span>         x <span class=\"pl-k\">=</span> Variable(torch.ones(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)                      \n <span class=\"pl-c1\">319</span>         z <span class=\"pl-k\">=</span> bn(x)                                                               \n <span class=\"pl-c1\">320</span>         z.register_hook(bw_hook)                                                \n <span class=\"pl-c1\">321</span>         z.sum().backward()                                                      \n <span class=\"pl-c1\">322</span>                                                                                 \n <span class=\"pl-c1\">323</span>         <span class=\"pl-c1\">self</span>.assertEqual(counter[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">1</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bw_hook not called<span class=\"pl-pds\">'</span></span>)                   \n <span class=\"pl-c1\">324</span>         <span class=\"pl-c1\">self</span>.assertEqual(x.grad.data, torch.ones(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>)                     </pre></div>\n<p>Code to reproduce:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch <span class=\"pl-k\">as</span> th\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\nx <span class=\"pl-k\">=</span> Variable(th.rand(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">5</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nl <span class=\"pl-k\">=</span> th.nn.BatchNorm1d(<span class=\"pl-c1\">5</span>)\ny <span class=\"pl-k\">=</span> l(x)\n\ngy <span class=\"pl-k\">=</span> Variable(th.ones(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">5</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nl.zero_grad()\ny.backward(x)\n\nl.bias.grad\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">Variable containing:</span>\n<span class=\"pl-s\"> 1.6464</span>\n<span class=\"pl-s\"> 1.4370</span>\n<span class=\"pl-s\"> 1.6897</span>\n<span class=\"pl-s\"> 1.9258</span>\n<span class=\"pl-s\"> 1.6882</span>\n<span class=\"pl-s\">[torch.FloatTensor of size 5]</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> -- the expected gradient of bias</span>\ngy.sum(<span class=\"pl-c1\">0</span>)\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">Variable containing:</span>\n<span class=\"pl-s\"> 3</span>\n<span class=\"pl-s\"> 3</span>\n<span class=\"pl-s\"> 3</span>\n<span class=\"pl-s\"> 3</span>\n<span class=\"pl-s\"> 3</span>\n<span class=\"pl-s\">[torch.FloatTensor of size 5]</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n\nx.sum(<span class=\"pl-c1\">0</span>)\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">Variable containing:</span>\n<span class=\"pl-s\"> 1.6464</span>\n<span class=\"pl-s\"> 1.4370</span>\n<span class=\"pl-s\"> 1.6897</span>\n<span class=\"pl-s\"> 1.9258</span>\n<span class=\"pl-s\"> 1.6882</span>\n<span class=\"pl-s\">[torch.FloatTensor of size 5]</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span></pre></div>\n<p>Is there something wrong?</p>\n<pre><code>In [31]: th.__version__\nOut[31]: '0.2.0_1'\n</code></pre>", "body_text": "According to https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n, the gradient of the bias in a BatchNorm1d layer should be gradOutput.sum(0) given that the size of gradOutput is (N,D).\n#step9\ndbeta = np.sum(dout, axis=0)\n\nHowever, when I'm verifying this, pytorch seems to use input.sum(0) as the bias gradient.\nWell ... if we formulate the affine part as y = vx + b, then \u2202y/\u2202b = x ??\nI searched the code and the only part that checked BatchNorm1d layer just uses one without affine transformation. test_autograd.py\n 307     def test_hooks_cpp(self):                                                   \n 308         # Tests hooks for autograd function implemented in C++                  \n 309         bn = torch.nn.BatchNorm1d(5, affine=False)                              \n 310         bn.eval()                                                               \n 311                                                                                 \n 312         counter = [0]                                                           \n 313                                                                                 \n 314         def bw_hook(grad):                                                      \n 315             counter[0] += 1                                                     \n 316             return grad * 2                                                     \n 317                                                                                 \n 318         x = Variable(torch.ones(5, 5), requires_grad=True)                      \n 319         z = bn(x)                                                               \n 320         z.register_hook(bw_hook)                                                \n 321         z.sum().backward()                                                      \n 322                                                                                 \n 323         self.assertEqual(counter[0], 1, 'bw_hook not called')                   \n 324         self.assertEqual(x.grad.data, torch.ones(5, 5) * 2)                     \nCode to reproduce:\nimport torch as th\nfrom torch.autograd import Variable\nx = Variable(th.rand(3,5), requires_grad=True)\nl = th.nn.BatchNorm1d(5)\ny = l(x)\n\ngy = Variable(th.ones(3,5), requires_grad=False)\nl.zero_grad()\ny.backward(x)\n\nl.bias.grad\n'''\nVariable containing:\n 1.6464\n 1.4370\n 1.6897\n 1.9258\n 1.6882\n[torch.FloatTensor of size 5]\n'''\n\n# -- the expected gradient of bias\ngy.sum(0)\n'''\nVariable containing:\n 3\n 3\n 3\n 3\n 3\n[torch.FloatTensor of size 5]\n'''\n\nx.sum(0)\n'''\nVariable containing:\n 1.6464\n 1.4370\n 1.6897\n 1.9258\n 1.6882\n[torch.FloatTensor of size 5]\n'''\nIs there something wrong?\nIn [31]: th.__version__\nOut[31]: '0.2.0_1'", "body": "According to https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\r\n, the gradient of the bias in a BatchNorm1d layer should be `gradOutput.sum(0)` given that the size of gradOutput is `(N,D)`.\r\n\r\n```\r\n#step9\r\ndbeta = np.sum(dout, axis=0)\r\n```\r\n\r\nHowever, when I'm verifying this, pytorch seems to use `input.sum(0)` as the bias gradient.\r\n\r\nWell ... if we formulate the affine part as `y = vx + b`, then `\u2202y/\u2202b = x` ??\r\n\r\nI searched the code and the only part that checked BatchNorm1d layer just uses one without affine transformation. `test_autograd.py`\r\n```python\r\n 307     def test_hooks_cpp(self):                                                   \r\n 308         # Tests hooks for autograd function implemented in C++                  \r\n 309         bn = torch.nn.BatchNorm1d(5, affine=False)                              \r\n 310         bn.eval()                                                               \r\n 311                                                                                 \r\n 312         counter = [0]                                                           \r\n 313                                                                                 \r\n 314         def bw_hook(grad):                                                      \r\n 315             counter[0] += 1                                                     \r\n 316             return grad * 2                                                     \r\n 317                                                                                 \r\n 318         x = Variable(torch.ones(5, 5), requires_grad=True)                      \r\n 319         z = bn(x)                                                               \r\n 320         z.register_hook(bw_hook)                                                \r\n 321         z.sum().backward()                                                      \r\n 322                                                                                 \r\n 323         self.assertEqual(counter[0], 1, 'bw_hook not called')                   \r\n 324         self.assertEqual(x.grad.data, torch.ones(5, 5) * 2)                     \r\n```\r\n\r\nCode to reproduce:\r\n```python\r\nimport torch as th\r\nfrom torch.autograd import Variable\r\nx = Variable(th.rand(3,5), requires_grad=True)\r\nl = th.nn.BatchNorm1d(5)\r\ny = l(x)\r\n\r\ngy = Variable(th.ones(3,5), requires_grad=False)\r\nl.zero_grad()\r\ny.backward(x)\r\n\r\nl.bias.grad\r\n'''\r\nVariable containing:\r\n 1.6464\r\n 1.4370\r\n 1.6897\r\n 1.9258\r\n 1.6882\r\n[torch.FloatTensor of size 5]\r\n'''\r\n\r\n# -- the expected gradient of bias\r\ngy.sum(0)\r\n'''\r\nVariable containing:\r\n 3\r\n 3\r\n 3\r\n 3\r\n 3\r\n[torch.FloatTensor of size 5]\r\n'''\r\n\r\nx.sum(0)\r\n'''\r\nVariable containing:\r\n 1.6464\r\n 1.4370\r\n 1.6897\r\n 1.9258\r\n 1.6882\r\n[torch.FloatTensor of size 5]\r\n'''\r\n```\r\n\r\nIs there something wrong?\r\n\r\n```\r\nIn [31]: th.__version__\r\nOut[31]: '0.2.0_1'\r\n```"}