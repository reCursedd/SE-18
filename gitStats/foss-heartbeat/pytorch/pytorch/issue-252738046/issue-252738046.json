{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2533", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2533/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2533/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2533/events", "html_url": "https://github.com/pytorch/pytorch/issues/2533", "id": 252738046, "node_id": "MDU6SXNzdWUyNTI3MzgwNDY=", "number": 2533, "title": "Distributed issues", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-08-24T21:34:42Z", "updated_at": "2018-09-26T20:42:50Z", "closed_at": "2018-09-26T20:42:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For correct distributed operation it is required that gradients are computed for all the parameters that were used to create buckets. The warning does not quite reflect that (\"This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later\"). Even if no parameters are removed, but by mistake a model contained a parameter which is not used in forward, or forward uses different subsets of submodules for the different iterations (dynamic graphs?), or some parameters don't require gradients (PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"250718760\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2464\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/2464/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/2464\">#2464</a> formally allowed that, but results would be wrong), the reductions won't be properly triggered every iteration.</p>", "body_text": "For correct distributed operation it is required that gradients are computed for all the parameters that were used to create buckets. The warning does not quite reflect that (\"This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later\"). Even if no parameters are removed, but by mistake a model contained a parameter which is not used in forward, or forward uses different subsets of submodules for the different iterations (dynamic graphs?), or some parameters don't require gradients (PR #2464 formally allowed that, but results would be wrong), the reductions won't be properly triggered every iteration.", "body": "For correct distributed operation it is required that gradients are computed for all the parameters that were used to create buckets. The warning does not quite reflect that (\"This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later\"). Even if no parameters are removed, but by mistake a model contained a parameter which is not used in forward, or forward uses different subsets of submodules for the different iterations (dynamic graphs?), or some parameters don't require gradients (PR #2464 formally allowed that, but results would be wrong), the reductions won't be properly triggered every iteration. "}