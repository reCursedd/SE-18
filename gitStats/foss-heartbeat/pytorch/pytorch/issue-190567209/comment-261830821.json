{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/261830821", "html_url": "https://github.com/pytorch/pytorch/issues/241#issuecomment-261830821", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/241", "id": 261830821, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTgzMDgyMQ==", "user": {"login": "thatguymike", "id": 3503919, "node_id": "MDQ6VXNlcjM1MDM5MTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3503919?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thatguymike", "html_url": "https://github.com/thatguymike", "followers_url": "https://api.github.com/users/thatguymike/followers", "following_url": "https://api.github.com/users/thatguymike/following{/other_user}", "gists_url": "https://api.github.com/users/thatguymike/gists{/gist_id}", "starred_url": "https://api.github.com/users/thatguymike/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thatguymike/subscriptions", "organizations_url": "https://api.github.com/users/thatguymike/orgs", "repos_url": "https://api.github.com/users/thatguymike/repos", "events_url": "https://api.github.com/users/thatguymike/events{/privacy}", "received_events_url": "https://api.github.com/users/thatguymike/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-21T02:55:20Z", "updated_at": "2016-11-21T02:55:20Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1183609\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shubho\">@shubho</a> - I agree with respect to the scheduler, but I also don't see a large issue.  However, we want to take some care in being reliant on MPI and checkpoint support.  I like MPI because it gives you a good default support base, but can have interesting performance issues.  We tend to go right to verbs and sockets because performance can be much better.</p>\n<p>What we have found works the best currently is fast on node reduction and then reduction across nodes.  However, in practice this means not reducing across PCIe root complexes and treating each root complex that GPUs and hopefully a NIC are attached to as a node.  But if you have more complex system configs, for example more IB cards, it gets more complex.  Or if you have a custom interconnect...  ;-)</p>\n<p>Scaling you can achieve will matter a whole lot on the speed of each \"node\" and the model, but I would expect speech models specifcally to scale pretty well, especially because you can increase the global batch pretty aggressively and maintain convergence. However, that can make checkpoint and recovery on a different sized total machine more problematic if that machine can't fit the global batch of the original.  All the hyperparams need to change (generally) to adjust.</p>\n<p>But, I like the general direction and idea.  I think it's really figuring out an abstraction that allows the transport layer to be changed easily (even if by extension) and support hierarchical setups.</p>\n<p>More complex will be model and hybrid paralel designs.</p>\n<p>However, the most complicated thing we tend to run into is the input IO pipeline and synchronization across a filesystem.  It can make for some interesting interactions, especially if your IO is on the same NICs you are using for reduction...</p>", "body_text": "@shubho - I agree with respect to the scheduler, but I also don't see a large issue.  However, we want to take some care in being reliant on MPI and checkpoint support.  I like MPI because it gives you a good default support base, but can have interesting performance issues.  We tend to go right to verbs and sockets because performance can be much better.\nWhat we have found works the best currently is fast on node reduction and then reduction across nodes.  However, in practice this means not reducing across PCIe root complexes and treating each root complex that GPUs and hopefully a NIC are attached to as a node.  But if you have more complex system configs, for example more IB cards, it gets more complex.  Or if you have a custom interconnect...  ;-)\nScaling you can achieve will matter a whole lot on the speed of each \"node\" and the model, but I would expect speech models specifcally to scale pretty well, especially because you can increase the global batch pretty aggressively and maintain convergence. However, that can make checkpoint and recovery on a different sized total machine more problematic if that machine can't fit the global batch of the original.  All the hyperparams need to change (generally) to adjust.\nBut, I like the general direction and idea.  I think it's really figuring out an abstraction that allows the transport layer to be changed easily (even if by extension) and support hierarchical setups.\nMore complex will be model and hybrid paralel designs.\nHowever, the most complicated thing we tend to run into is the input IO pipeline and synchronization across a filesystem.  It can make for some interesting interactions, especially if your IO is on the same NICs you are using for reduction...", "body": "@shubho - I agree with respect to the scheduler, but I also don't see a large issue.  However, we want to take some care in being reliant on MPI and checkpoint support.  I like MPI because it gives you a good default support base, but can have interesting performance issues.  We tend to go right to verbs and sockets because performance can be much better.\r\n\r\nWhat we have found works the best currently is fast on node reduction and then reduction across nodes.  However, in practice this means not reducing across PCIe root complexes and treating each root complex that GPUs and hopefully a NIC are attached to as a node.  But if you have more complex system configs, for example more IB cards, it gets more complex.  Or if you have a custom interconnect...  ;-)\r\n\r\nScaling you can achieve will matter a whole lot on the speed of each \"node\" and the model, but I would expect speech models specifcally to scale pretty well, especially because you can increase the global batch pretty aggressively and maintain convergence. However, that can make checkpoint and recovery on a different sized total machine more problematic if that machine can't fit the global batch of the original.  All the hyperparams need to change (generally) to adjust.\r\n\r\nBut, I like the general direction and idea.  I think it's really figuring out an abstraction that allows the transport layer to be changed easily (even if by extension) and support hierarchical setups.\r\n\r\nMore complex will be model and hybrid paralel designs.\r\n\r\nHowever, the most complicated thing we tend to run into is the input IO pipeline and synchronization across a filesystem.  It can make for some interesting interactions, especially if your IO is on the same NICs you are using for reduction..."}