{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/261823672", "html_url": "https://github.com/pytorch/pytorch/issues/241#issuecomment-261823672", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/241", "id": 261823672, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTgyMzY3Mg==", "user": {"login": "shubho", "id": 1183609, "node_id": "MDQ6VXNlcjExODM2MDk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1183609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shubho", "html_url": "https://github.com/shubho", "followers_url": "https://api.github.com/users/shubho/followers", "following_url": "https://api.github.com/users/shubho/following{/other_user}", "gists_url": "https://api.github.com/users/shubho/gists{/gist_id}", "starred_url": "https://api.github.com/users/shubho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shubho/subscriptions", "organizations_url": "https://api.github.com/users/shubho/orgs", "repos_url": "https://api.github.com/users/shubho/repos", "events_url": "https://api.github.com/users/shubho/events{/privacy}", "received_events_url": "https://api.github.com/users/shubho/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-21T01:41:30Z", "updated_at": "2016-11-21T05:04:05Z", "author_association": "NONE", "body_html": "<p>Shubho here from SVAIL @ Baidu</p>\n<p>One long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using <code>salloc</code> with <code>pytorch_exec</code> should be fairly easy.</p>\n<p>The framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.</p>\n<p>Possibly <code>torch.distributed</code> is more full featured than this... I haven't started looking at the code yet.. but will soon...</p>\n<p>Happy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code</p>", "body_text": "Shubho here from SVAIL @ Baidu\nOne long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using salloc with pytorch_exec should be fairly easy.\nThe framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.\nPossibly torch.distributed is more full featured than this... I haven't started looking at the code yet.. but will soon...\nHappy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code", "body": "Shubho here from SVAIL @ Baidu\r\n\r\nOne long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using `salloc` with `pytorch_exec` should be fairly easy.\r\n\r\nThe framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.\r\n\r\nPossibly `torch.distributed` is more full featured than this... I haven't started looking at the code yet.. but will soon...\r\n\r\nHappy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code"}