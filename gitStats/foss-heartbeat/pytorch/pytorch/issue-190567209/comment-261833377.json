{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/261833377", "html_url": "https://github.com/pytorch/pytorch/issues/241#issuecomment-261833377", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/241", "id": 261833377, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTgzMzM3Nw==", "user": {"login": "shubho", "id": 1183609, "node_id": "MDQ6VXNlcjExODM2MDk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1183609?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shubho", "html_url": "https://github.com/shubho", "followers_url": "https://api.github.com/users/shubho/followers", "following_url": "https://api.github.com/users/shubho/following{/other_user}", "gists_url": "https://api.github.com/users/shubho/gists{/gist_id}", "starred_url": "https://api.github.com/users/shubho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shubho/subscriptions", "organizations_url": "https://api.github.com/users/shubho/orgs", "repos_url": "https://api.github.com/users/shubho/repos", "events_url": "https://api.github.com/users/shubho/events{/privacy}", "received_events_url": "https://api.github.com/users/shubho/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-21T03:21:13Z", "updated_at": "2016-11-21T03:21:13Z", "author_association": "NONE", "body_html": "<p>We have thought about doing more topology aware reduction like you suggested - but then backed off because our collective turned out fast enough the ring-reduce algorithm is really good for large matrices. The ring-reduce conceptually treats every neighboring link as the same - though sometimes they will span QPI bus and sometimes go to the IB switch. Thankfully ignoring this difference hasn't bitten us... yet.. :) In some sense I am just using MPI as a thin layer over all the different transports with just calls to Send and Recv along with their non-blocking versions. In some sense I want a OpenMPI-lite that only has Send, Recv, ISend, IRecv and SendRecv - the rest of the collectives are best written by hand and then you can even play around with sending in reduced precision etc.</p>\n<p>Actually speech is not that easy to scale - anything above global batch 1024 hasn't so far worked. So we rarely train beyond 64 GPUs though we can easily scale to 256 GPUs with weak linear scaling with worse convergence. So far hyperparameter or model search hasn't yielded something amenable above 1024 :(</p>\n<p>Yes input I/O is the 800 lb gorilla in the room for training that nobody seems to talk about. Designing networked read-only filesystems that can saturate the GPUs is an unsolved problem. We have solved through various hacks - they are not pretty and not very scalable and has been a source of a lot of frustration. And our checkpoint writing can also stress the filesystem. I think this is a broad research topic - training data input and checkpoint data output - but we should worry about this later.</p>", "body_text": "We have thought about doing more topology aware reduction like you suggested - but then backed off because our collective turned out fast enough the ring-reduce algorithm is really good for large matrices. The ring-reduce conceptually treats every neighboring link as the same - though sometimes they will span QPI bus and sometimes go to the IB switch. Thankfully ignoring this difference hasn't bitten us... yet.. :) In some sense I am just using MPI as a thin layer over all the different transports with just calls to Send and Recv along with their non-blocking versions. In some sense I want a OpenMPI-lite that only has Send, Recv, ISend, IRecv and SendRecv - the rest of the collectives are best written by hand and then you can even play around with sending in reduced precision etc.\nActually speech is not that easy to scale - anything above global batch 1024 hasn't so far worked. So we rarely train beyond 64 GPUs though we can easily scale to 256 GPUs with weak linear scaling with worse convergence. So far hyperparameter or model search hasn't yielded something amenable above 1024 :(\nYes input I/O is the 800 lb gorilla in the room for training that nobody seems to talk about. Designing networked read-only filesystems that can saturate the GPUs is an unsolved problem. We have solved through various hacks - they are not pretty and not very scalable and has been a source of a lot of frustration. And our checkpoint writing can also stress the filesystem. I think this is a broad research topic - training data input and checkpoint data output - but we should worry about this later.", "body": "We have thought about doing more topology aware reduction like you suggested - but then backed off because our collective turned out fast enough the ring-reduce algorithm is really good for large matrices. The ring-reduce conceptually treats every neighboring link as the same - though sometimes they will span QPI bus and sometimes go to the IB switch. Thankfully ignoring this difference hasn't bitten us... yet.. :) In some sense I am just using MPI as a thin layer over all the different transports with just calls to Send and Recv along with their non-blocking versions. In some sense I want a OpenMPI-lite that only has Send, Recv, ISend, IRecv and SendRecv - the rest of the collectives are best written by hand and then you can even play around with sending in reduced precision etc.\r\n\r\nActually speech is not that easy to scale - anything above global batch 1024 hasn't so far worked. So we rarely train beyond 64 GPUs though we can easily scale to 256 GPUs with weak linear scaling with worse convergence. So far hyperparameter or model search hasn't yielded something amenable above 1024 :(\r\n\r\nYes input I/O is the 800 lb gorilla in the room for training that nobody seems to talk about. Designing networked read-only filesystems that can saturate the GPUs is an unsolved problem. We have solved through various hacks - they are not pretty and not very scalable and has been a source of a lot of frustration. And our checkpoint writing can also stress the filesystem. I think this is a broad research topic - training data input and checkpoint data output - but we should worry about this later."}