{"url": "https://api.github.com/repos/pytorch/pytorch/issues/241", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/241/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/241/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/241/events", "html_url": "https://github.com/pytorch/pytorch/issues/241", "id": 190567209, "node_id": "MDU6SXNzdWUxOTA1NjcyMDk=", "number": 241, "title": "PyTorch goes distributed", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 586699385, "node_id": "MDU6TGFiZWw1ODY2OTkzODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/24hr+", "name": "24hr+", "color": "d4a5d9", "default": false}, {"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2016-11-20T16:18:38Z", "updated_at": "2018-11-14T14:38:55Z", "closed_at": "2017-08-25T14:57:41Z", "author_association": "MEMBER", "body_html": "<p>Together with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7978161\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/0mp\">@0mp</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3855799\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/VirrageS\">@VirrageS</a> andy <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9921596\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jytug\">@jytug</a> we're developing a <code>torch.distributed</code> package for PyTorch. All work is done <a href=\"https://github.com/apaszke/pytorch-dist/tree/thd\">in a fork</a> on a <code>thd</code> branch (we didn't want to make a lot of unnecessary noise in the main repo). We're creating this issue, so we can gather feedback on our API designs from all you guys.</p>\n<p>We plan to make the package have two modes. The user has to choose one of them as part of the initialisation.</p>\n<h3>Process group mode</h3>\n<p>This is very similar to the API defined in MPI. We assume all processes are equal, assign them ranks and later on, allow them to use a well known set of communication collectives like <code>reduce</code>, <code>broadcast</code>, <code>allReduce</code>, <code>gather</code>, <code>scatter</code>, etc.</p>\n<p><strong>Example:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch.distributed\ntorch.distributed.init_process_group(<span class=\"pl-v\">backend</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>tcp<span class=\"pl-pds\">'</span></span>)\nmy_rank <span class=\"pl-k\">=</span> torch.distributed.get_rank()\nnum_processes <span class=\"pl-k\">=</span> torch.distributed.get_num_processes()\n\n<span class=\"pl-c1\">...</span>\n\n<span class=\"pl-k\">if</span> my_rank <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n    torch.distributed.send(tensor, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">else</span>:\n    tensor <span class=\"pl-k\">=</span> torch.distributed.recv(<span class=\"pl-c1\">0</span>)\n\n<span class=\"pl-c1\">...</span>\n\nresult <span class=\"pl-k\">=</span> torch.distributed.all_reduce(tensor)</pre></div>\n<h3>Master-worker mode</h3>\n<p>This would provide a very similar API to the <code>torch.cuda</code> package. At the beginning of your script you would have to call <code>torch.distributed.init_master_worker(backend='mpi')</code></p>\n<p>Operation execution is asynchronous w.r.t. to the master process, we'll implement a CUDA-like concurrency model (streams + events). Until then, the only sync points are copies between master and workers.</p>\n<p><strong>Example:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch.distributed\ntorch.distributed.init_master_worker(<span class=\"pl-v\">backend</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>tcp<span class=\"pl-pds\">'</span></span>)\n\nx <span class=\"pl-k\">=</span> torch.distributed.FloatTensor(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">20</span>).fill_(<span class=\"pl-c1\">4</span>)\ny <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">20</span>).dist_send()\nz <span class=\"pl-k\">=</span> x <span class=\"pl-k\">+</span> y\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> z.get_node(), z.get_device() == 0, -1 (i.e. CPU)</span>\ncuda_x <span class=\"pl-k\">=</span> x.cuda()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> cuda_x.get_node(), cuda_x.get_device() == 0, 0</span>\n<span class=\"pl-k\">with</span> torch.distributed.node(<span class=\"pl-c1\">1</span>):\n    a <span class=\"pl-k\">=</span> torch.distributed.FloatTensor(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> a.get_node(), a.get_device() == 1, 1</span>\n    cuda_y <span class=\"pl-k\">=</span> y.cuda()\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> cuda_y.get_node(), cuda_y.get_device() == 0, 0</span>\n    q <span class=\"pl-k\">=</span> cuda_x <span class=\"pl-k\">+</span> cuda_y\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> q.get_node(), q.get_device() == 0, 0</span></pre></div>\n<h3>How to launch the jobs</h3>\n<p>We'll provide a <code>pytorch_exec</code> utility that will spawn the process groups in a similar fashion that <code>mpiexec</code> does.</p>\n<h3>Decoupling data backends from other logic</h3>\n<p>You might have noticed that both <code>init_process_group</code> and <code>init_master_worker</code> accept a <code>backend</code> argument. We're aware that the best strategy for sending the data might be different for every user, and it will be crucial to pick a good one to limit communication overhead. This was the reason why we decided to introduce a <code>DataChannel</code> interface, so users will be able to pick from one of the provided implementations (initially MPI and raw TCP sockets, later RDMA etc.), or add custom ones, so they can easily achieve the lowest overhead possible in their setup.</p>\n<p><strong>Please let us know what you think! Thanks!</strong></p>", "body_text": "Together with @0mp, @VirrageS andy @jytug we're developing a torch.distributed package for PyTorch. All work is done in a fork on a thd branch (we didn't want to make a lot of unnecessary noise in the main repo). We're creating this issue, so we can gather feedback on our API designs from all you guys.\nWe plan to make the package have two modes. The user has to choose one of them as part of the initialisation.\nProcess group mode\nThis is very similar to the API defined in MPI. We assume all processes are equal, assign them ranks and later on, allow them to use a well known set of communication collectives like reduce, broadcast, allReduce, gather, scatter, etc.\nExample:\nimport torch.distributed\ntorch.distributed.init_process_group(backend='tcp')\nmy_rank = torch.distributed.get_rank()\nnum_processes = torch.distributed.get_num_processes()\n\n...\n\nif my_rank == 0:\n    torch.distributed.send(tensor, 1)\nelse:\n    tensor = torch.distributed.recv(0)\n\n...\n\nresult = torch.distributed.all_reduce(tensor)\nMaster-worker mode\nThis would provide a very similar API to the torch.cuda package. At the beginning of your script you would have to call torch.distributed.init_master_worker(backend='mpi')\nOperation execution is asynchronous w.r.t. to the master process, we'll implement a CUDA-like concurrency model (streams + events). Until then, the only sync points are copies between master and workers.\nExample:\nimport torch.distributed\ntorch.distributed.init_master_worker(backend='tcp')\n\nx = torch.distributed.FloatTensor(20, 20).fill_(4)\ny = torch.randn(20, 20).dist_send()\nz = x + y\n# z.get_node(), z.get_device() == 0, -1 (i.e. CPU)\ncuda_x = x.cuda()\n# cuda_x.get_node(), cuda_x.get_device() == 0, 0\nwith torch.distributed.node(1):\n    a = torch.distributed.FloatTensor(10, device=1)\n    # a.get_node(), a.get_device() == 1, 1\n    cuda_y = y.cuda()\n    # cuda_y.get_node(), cuda_y.get_device() == 0, 0\n    q = cuda_x + cuda_y\n    # q.get_node(), q.get_device() == 0, 0\nHow to launch the jobs\nWe'll provide a pytorch_exec utility that will spawn the process groups in a similar fashion that mpiexec does.\nDecoupling data backends from other logic\nYou might have noticed that both init_process_group and init_master_worker accept a backend argument. We're aware that the best strategy for sending the data might be different for every user, and it will be crucial to pick a good one to limit communication overhead. This was the reason why we decided to introduce a DataChannel interface, so users will be able to pick from one of the provided implementations (initially MPI and raw TCP sockets, later RDMA etc.), or add custom ones, so they can easily achieve the lowest overhead possible in their setup.\nPlease let us know what you think! Thanks!", "body": "Together with @0mp, @VirrageS andy @jytug we're developing a `torch.distributed` package for PyTorch. All work is done [in a fork](https://github.com/apaszke/pytorch-dist/tree/thd) on a `thd` branch (we didn't want to make a lot of unnecessary noise in the main repo). We're creating this issue, so we can gather feedback on our API designs from all you guys.\r\n\r\nWe plan to make the package have two modes. The user has to choose one of them as part of the initialisation.\r\n\r\n### Process group mode\r\n\r\nThis is very similar to the API defined in MPI. We assume all processes are equal, assign them ranks and later on, allow them to use a well known set of communication collectives like `reduce`, `broadcast`, `allReduce`, `gather`, `scatter`, etc.\r\n\r\n**Example:**\r\n\r\n```python\r\nimport torch.distributed\r\ntorch.distributed.init_process_group(backend='tcp')\r\nmy_rank = torch.distributed.get_rank()\r\nnum_processes = torch.distributed.get_num_processes()\r\n\r\n...\r\n\r\nif my_rank == 0:\r\n    torch.distributed.send(tensor, 1)\r\nelse:\r\n    tensor = torch.distributed.recv(0)\r\n\r\n...\r\n\r\nresult = torch.distributed.all_reduce(tensor)\r\n```\r\n\r\n### Master-worker mode\r\n\r\nThis would provide a very similar API to the `torch.cuda` package. At the beginning of your script you would have to call `torch.distributed.init_master_worker(backend='mpi')`\r\n\r\nOperation execution is asynchronous w.r.t. to the master process, we'll implement a CUDA-like concurrency model (streams + events). Until then, the only sync points are copies between master and workers.\r\n\r\n**Example:**\r\n\r\n```python\r\nimport torch.distributed\r\ntorch.distributed.init_master_worker(backend='tcp')\r\n\r\nx = torch.distributed.FloatTensor(20, 20).fill_(4)\r\ny = torch.randn(20, 20).dist_send()\r\nz = x + y\r\n# z.get_node(), z.get_device() == 0, -1 (i.e. CPU)\r\ncuda_x = x.cuda()\r\n# cuda_x.get_node(), cuda_x.get_device() == 0, 0\r\nwith torch.distributed.node(1):\r\n    a = torch.distributed.FloatTensor(10, device=1)\r\n    # a.get_node(), a.get_device() == 1, 1\r\n    cuda_y = y.cuda()\r\n    # cuda_y.get_node(), cuda_y.get_device() == 0, 0\r\n    q = cuda_x + cuda_y\r\n    # q.get_node(), q.get_device() == 0, 0\r\n```\r\n\r\n### How to launch the jobs\r\n\r\nWe'll provide a `pytorch_exec` utility that will spawn the process groups in a similar fashion that `mpiexec` does.\r\n\r\n### Decoupling data backends from other logic\r\n\r\nYou might have noticed that both `init_process_group` and `init_master_worker` accept a `backend` argument. We're aware that the best strategy for sending the data might be different for every user, and it will be crucial to pick a good one to limit communication overhead. This was the reason why we decided to introduce a `DataChannel` interface, so users will be able to pick from one of the provided implementations (initially MPI and raw TCP sockets, later RDMA etc.), or add custom ones, so they can easily achieve the lowest overhead possible in their setup.\r\n\r\n**Please let us know what you think! Thanks!**"}