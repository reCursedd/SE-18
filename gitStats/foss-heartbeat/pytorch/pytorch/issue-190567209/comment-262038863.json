{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/262038863", "html_url": "https://github.com/pytorch/pytorch/issues/241#issuecomment-262038863", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/241", "id": 262038863, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MjAzODg2Mw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-21T19:20:30Z", "updated_at": "2016-11-21T19:20:30Z", "author_association": "MEMBER", "body_html": "<p>Actually, we\u2019ve abstracted the code used for transmitting the data into a <a href=\"https://github.com/apaszke/pytorch-dist/blob/thd/torch/lib/THD/base/DataChannel.hpp\">DataChannel interface</a>. Right now we have two implementations - TCP and MPI. But afterwards, we can probably do something like what either / both of you have converged to - which is to use NCCL at a PCI-e root level, and MPI / IBVerbs at a Node level.</p>\n<p>However, the whole point is that we want you to be able to customize it even further by writing custom backends. We tried to make it so that the changes to THD code would be minimal. Later, your own implementations can be selected at startup at the Python level by passing its identifier to an init method (and the rest of the python code remains completely unchanged). This can even be maintained as a separate internal library.</p>\n<p>At the initial stages, we are focusing on correctness, so we're implementing a simple-stupid-working implementation - with simply calling MPI or writing non-performant TCP routines. Once we cross the unit tests stage, and write all Python bindings, we'll aggressively focus on perf as well.</p>\n<p>Also, RPC calls in master-worker mode always use ZMQ for communication, so it won\u2019t mess up any logic in your custom DataChannels. You can be sure they\u2019ll only be used for tensor data. Since all messages will be very very small (10-100B) we decided that it\u2019s not necessary to make the command channel interchangeable.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1183609\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shubho\">@shubho</a> - it seems that the approach you used is compatible with the process group mode. It\u2019s pretty much the same as using raw MPI - all workers are equal and there\u2019s no master. We\u2019re adding master-worker mode just because it might be simpler for some people to use an API they know from our CUDA packages. It\u2019s pretty much the same principle - you can make your code automatically distributed by only calling <code>.dist_send()</code>. No need to rearrange stuff and interleave it with MPI collectives. It\u2019s probably not going to keep 100 workers busy, but as long as you keep it &lt;20 it should be ok (workers can use multiple GPUs).</p>\n<p>About SLURM, if you use the MPI backend, you wouldn't even need to use pytorch_exec, you can simply use mpiexec.</p>\n<p>If any of you were browsing the code and would like to ask any questions feel free to reach out to me on slack or by email, and I can give you a tour or explain what\u2019s the plan at that moment.</p>", "body_text": "Actually, we\u2019ve abstracted the code used for transmitting the data into a DataChannel interface. Right now we have two implementations - TCP and MPI. But afterwards, we can probably do something like what either / both of you have converged to - which is to use NCCL at a PCI-e root level, and MPI / IBVerbs at a Node level.\nHowever, the whole point is that we want you to be able to customize it even further by writing custom backends. We tried to make it so that the changes to THD code would be minimal. Later, your own implementations can be selected at startup at the Python level by passing its identifier to an init method (and the rest of the python code remains completely unchanged). This can even be maintained as a separate internal library.\nAt the initial stages, we are focusing on correctness, so we're implementing a simple-stupid-working implementation - with simply calling MPI or writing non-performant TCP routines. Once we cross the unit tests stage, and write all Python bindings, we'll aggressively focus on perf as well.\nAlso, RPC calls in master-worker mode always use ZMQ for communication, so it won\u2019t mess up any logic in your custom DataChannels. You can be sure they\u2019ll only be used for tensor data. Since all messages will be very very small (10-100B) we decided that it\u2019s not necessary to make the command channel interchangeable.\n@shubho - it seems that the approach you used is compatible with the process group mode. It\u2019s pretty much the same as using raw MPI - all workers are equal and there\u2019s no master. We\u2019re adding master-worker mode just because it might be simpler for some people to use an API they know from our CUDA packages. It\u2019s pretty much the same principle - you can make your code automatically distributed by only calling .dist_send(). No need to rearrange stuff and interleave it with MPI collectives. It\u2019s probably not going to keep 100 workers busy, but as long as you keep it <20 it should be ok (workers can use multiple GPUs).\nAbout SLURM, if you use the MPI backend, you wouldn't even need to use pytorch_exec, you can simply use mpiexec.\nIf any of you were browsing the code and would like to ask any questions feel free to reach out to me on slack or by email, and I can give you a tour or explain what\u2019s the plan at that moment.", "body": "Actually, we\u2019ve abstracted the code used for transmitting the data into a [DataChannel interface](https://github.com/apaszke/pytorch-dist/blob/thd/torch/lib/THD/base/DataChannel.hpp). Right now we have two implementations - TCP and MPI. But afterwards, we can probably do something like what either / both of you have converged to - which is to use NCCL at a PCI-e root level, and MPI / IBVerbs at a Node level.\r\n\r\nHowever, the whole point is that we want you to be able to customize it even further by writing custom backends. We tried to make it so that the changes to THD code would be minimal. Later, your own implementations can be selected at startup at the Python level by passing its identifier to an init method (and the rest of the python code remains completely unchanged). This can even be maintained as a separate internal library.\r\n\r\nAt the initial stages, we are focusing on correctness, so we're implementing a simple-stupid-working implementation - with simply calling MPI or writing non-performant TCP routines. Once we cross the unit tests stage, and write all Python bindings, we'll aggressively focus on perf as well.\r\n\r\nAlso, RPC calls in master-worker mode always use ZMQ for communication, so it won\u2019t mess up any logic in your custom DataChannels. You can be sure they\u2019ll only be used for tensor data. Since all messages will be very very small (10-100B) we decided that it\u2019s not necessary to make the command channel interchangeable.\r\n\r\n@shubho - it seems that the approach you used is compatible with the process group mode. It\u2019s pretty much the same as using raw MPI - all workers are equal and there\u2019s no master. We\u2019re adding master-worker mode just because it might be simpler for some people to use an API they know from our CUDA packages. It\u2019s pretty much the same principle - you can make your code automatically distributed by only calling `.dist_send()`. No need to rearrange stuff and interleave it with MPI collectives. It\u2019s probably not going to keep 100 workers busy, but as long as you keep it <20 it should be ok (workers can use multiple GPUs).\r\n\r\nAbout SLURM, if you use the MPI backend, you wouldn't even need to use pytorch_exec, you can simply use mpiexec.\r\n\r\nIf any of you were browsing the code and would like to ask any questions feel free to reach out to me on slack or by email, and I can give you a tour or explain what\u2019s the plan at that moment."}