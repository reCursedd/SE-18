{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175876089", "pull_request_review_id": 105080028, "id": 175876089, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTg3NjA4OQ==", "diff_hunk": "@@ -0,0 +1,207 @@\n+import torch\n+\n+from . import Sequential, ModuleList, Linear\n+from .module import Module\n+from ..functional import log_softmax, cross_entropy\n+\n+\n+class AdaptiveLogSoftmax(Module):\n+    r\"\"\"Efficient softmax approximation as described in\n+    `Efficient softmax approximation for GPUs`_ by Edouard Grave, Armand Joulin,\n+     Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.\n+\n+    Adaptive softmax is an approximate strategy for training models with large\n+    output spaces. It is most effective when the label distribution is highly\n+    imbalanced, for example in natural language modelling, where the word\n+    frequency distribution approximately follows the `Zipf's law`_.\n+\n+    Adaptive softmax partitions the labels into several clusters, according to\n+    their frequency. These clusters may contain different number of targets\n+    each.\n+    Additionally, clusters containig less frequent labels assign lower\n+    dimensional embeddings to those labels, which speeds up the computation.\n+    For each minibatch, only clusters for which at least one target is\n+    present are used.\n+\n+    The idea is that the cluster that the clusters that are accessed often\n+    (like the first one, containing most frequent labels), should also be cheap\n+    to compute -- that is, contain a small number of assigned targets.\n+\n+    We highly recommend taking a look at the original paper for more details.\n+\n+    ``cutoffs`` should be a Sequence of integers. It controls number of clusters\n+    and the partitioning of targets into clusters. For example setting\n+    ``cutoffs = [10, 100, 1000]`` means that first `10` targets will be assigned\n+    to the 'head' of the adaptive softmax, targets `11, 12, ..., 100` will be\n+    assigned to the first cluster, and targets `101, 102, ..., 1000` will be\n+    assigned to the second cluster, while targets\n+    `1001, 1002, ..., n_classes - 1` will be assigned to the last, third cluster\n+\n+    ``div_value`` is used to compute the size of each additional cluster,\n+    which is given as :math:`\\lfloor \\frac{in\\_features}{div\\_value^i} \\rfloor`,\n+    where :math:`i` is the cluster index (with clusters for less frequent words\n+    having larger indices, and indices starting at :math:`1`).\n+\n+    .. warning::\n+        Targets passed as inputs to this module should be sorted accoridng to\n+        their frequency. This means that the most frequent target should be\n+        represented by the index `0`, and the least frequent\n+        target should be represented by the index `n_classes - 1`.\n+\n+    .. note::\n+        To compute log-probabilities for all classes, the `predict_log_proba`\n+        method can be used.\n+\n+    Args:\n+        in_features (int): Number of features in the input tensor\n+        n_classes (int): Number of classes in the dataset.\n+        cutoffs (Sequence): Cutoffs used to assign targets to their buckets.\n+        div_value (float, optional): value used as an exponent to compute sizes\n+        of the clusters. Default: 2.0\n+\n+    Returns:\n+        A Variable of size ``N``, containing computed target log probabilities\n+        for each example\n+\n+    Shape:\n+        - Input: :math:`(N, in\\_features)`\n+        - Target: :math:`(N)` where each value is `0 <= targets[i] <= C - 1`\n+        - Output: :math:`(N)`\n+\n+    .. _Efficient softmax approximation for GPUs:\n+        https://arxiv.org/abs/1609.04309\n+\n+    .. _Zipf's law:\n+        https://en.wikipedia.org/wiki/Zipf%27s_law\n+    \"\"\"\n+\n+    def __init__(self, in_features, n_classes, cutoffs, div_value=2.,\n+                 return_logprob=False):\n+        super(AdaptiveLogSoftmax, self).__init__()\n+\n+        cutoffs = list(cutoffs)\n+\n+        if (cutoffs != sorted(cutoffs)):\n+            raise ValueError('Cutoffs should be a list of unique, positive in')\n+\n+        if (cutoffs != sorted(cutoffs)) \\\n+                or (max(cutoffs) >= (n_classes - 1)) \\\n+                or (len(set(cutoffs)) != len(cutoffs)):\n+\n+            raise ValueError(\"Cutoffs should be a sequence of unique, positive \"\n+                             \"integers sorted in an increasing order, where \"\n+                             \"each value is between 1 and n_classes-1\")\n+\n+        self.return_logprob = return_logprob\n+        self.in_features = in_features\n+        self.n_classes = n_classes\n+        self.cutoffs = cutoffs + [n_classes]\n+        self.div_value = div_value\n+\n+        self.shortlist_size = self.cutoffs[0]\n+        self.n_clusters = len(self.cutoffs) - 1\n+        self.head_size = self.shortlist_size + self.n_clusters\n+\n+        self.head = Linear(self.in_features, self.head_size)\n+        self.tail = ModuleList()\n+\n+        for i in range(self.n_clusters):\n+\n+            hsz = int(self.in_features // (self.div_value ** (i + 1)))\n+            osz = self.cutoffs[i + 1] - self.cutoffs[i]\n+\n+            projection = Sequential(\n+                Linear(self.in_features, hsz, bias=False),\n+                Linear(hsz, osz)\n+            )\n+\n+            self.tail.append(projection)\n+\n+    def reset_parameters(self):\n+        self.head.reset_parameters()\n+        for i2h, h2o in self.tail:\n+            i2h.reset_parameters()\n+            h2o.reset_parameters()\n+\n+    def forward(self, input, target):\n+        if input.size(0) != target.size(0):\n+            raise RuntimeError('Input and target should have the same size '\n+                               'in the batch dimension.')\n+\n+        used_rows = 0\n+        batch_size = target.size(0)\n+        out_size = batch_size if (self.return_logprob) else 1\n+\n+        output = input.new(out_size).zero_()\n+        gather_inds = target.new(batch_size).zero_()\n+\n+        cutoff_values = [0] + self.cutoffs\n+        for i in range(len(cutoff_values) - 1):\n+\n+            low_idx = cutoff_values[i]\n+            high_idx = cutoff_values[i + 1]\n+\n+            target_mask = (target >= low_idx) & (target < high_idx)\n+            row_indices = target_mask.nonzero().squeeze()\n+\n+            if row_indices.numel() == 0:\n+                continue\n+\n+            relative_target = target[target_mask] - cutoff_values[i]\n+\n+            if i == 0:\n+                gather_inds.index_copy_(0, row_indices, relative_target)\n+\n+            else:\n+                input_subset = input.index_select(0, row_indices)\n+                cluster_output = self.tail[i - 1](input_subset)\n+                cluster_index = self.shortlist_size + i - 1\n+\n+                gather_inds.index_fill_(0, row_indices, cluster_index)\n+\n+                if self.return_logprob:\n+                    cluster_logprob = log_softmax(cluster_output, dim=1)\n+                    local_logprob = cluster_logprob.gather(1, relative_target.unsqueeze(1))\n+                    output.index_copy_(0, row_indices, local_logprob.squeeze(1))\n+\n+                else:\n+                    output += cross_entropy(cluster_output, relative_target, size_average=False)\n+\n+            used_rows += row_indices.numel()\n+\n+        if used_rows != batch_size:\n+            raise RuntimeError(\"Target values should be in [0, {}], \"\n+                               \"but values in range [{}, {}] \"\n+                               \"were found. \".format(self.n_classes - 1,\n+                                                     int(target.min()),\n+                                                     int(target.max())))\n+\n+        head_output = self.head(input)\n+\n+        if self.return_logprob:\n+            head_logprob = log_softmax(head_output, dim=1)\n+            output += head_logprob.gather(1, gather_inds.unsqueeze(1)).squeeze()\n+\n+        else:\n+            output += cross_entropy(head_output, gather_inds, size_average=False)\n+            output /= batch_size\n+\n+        return output\n+\n+    def get_log_proba(self, input):", "path": "torch/nn/modules/adaptive.py", "position": null, "original_position": 191, "commit_id": "7f89e1506a4e2df824118d3bc6e1e31f4073fc70", "original_commit_id": "5cf5cdce51b9a44f0ead48f09643719e9fa6d5de", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "This should read `log_prob` or `log_probs` to be consistent with other places in PyTorch that use log probability (e.g. distributions).", "created_at": "2018-03-20T18:25:39Z", "updated_at": "2018-11-23T15:40:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/5287#discussion_r175876089", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5287", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175876089"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5287#discussion_r175876089"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5287"}}, "body_html": "<p>This should read <code>log_prob</code> or <code>log_probs</code> to be consistent with other places in PyTorch that use log probability (e.g. distributions).</p>", "body_text": "This should read log_prob or log_probs to be consistent with other places in PyTorch that use log probability (e.g. distributions)."}