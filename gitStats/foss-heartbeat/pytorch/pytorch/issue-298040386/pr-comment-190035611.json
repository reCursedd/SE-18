{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190035611", "pull_request_review_id": 122331588, "id": 190035611, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MDAzNTYxMQ==", "diff_hunk": "@@ -4927,6 +4923,89 @@ def test_grad_conv3d_input(self):\n     def test_grad_conv3d_weight(self):\n         self.run_grad_conv_test(F.conv3d, F.grad.conv3d_weight, 3, 'weight')\n \n+    def test_adaptive_log_softmax(self):\n+        # args validation\n+        with self.assertRaises(ValueError):\n+            _ = nn.AdaptiveLogSoftmaxWithLoss(16, 20, [5, 15, 15], div_value=2.)\n+\n+        with self.assertRaises(ValueError):\n+            _ = nn.AdaptiveLogSoftmaxWithLoss(16, 20, [5, 15, 10], div_value=2.)\n+\n+        with self.assertRaises(ValueError):\n+            _ = nn.AdaptiveLogSoftmaxWithLoss(16, 20, [5, 10, 25], div_value=2.)\n+\n+        # input shapes\n+        with self.assertRaisesRegex(RuntimeError, \"Input and target should have the same size\"):\n+            asfm = nn.AdaptiveLogSoftmaxWithLoss(16, 20, [5, 10, 15], div_value=2.)\n+            x = torch.randn(2, 16)\n+            y = torch.tensor([0, 5, 10])\n+            asfm(x, y)\n+\n+        # out-of-bound targets\n+        with self.assertRaisesRegex(RuntimeError, \"Target values should be in\"):\n+            asfm = nn.AdaptiveLogSoftmaxWithLoss(16, 20, [5, 10, 15], div_value=2.)\n+            x = torch.randn(2, 16)\n+            y = torch.tensor([0, 20])\n+            asfm(x, y)\n+\n+        # cluster sizes\n+        asfm = nn.AdaptiveLogSoftmaxWithLoss(16, 20, [5, 10, 15], div_value=2.)\n+        x = torch.randn(2, 16)\n+        y = torch.tensor([0, 17])\n+\n+        self.assertEqual(asfm.head.weight.size(), (5 + 3, 16))   # 5 targets in head, 3 clusters, dimensionality 16\n+        self.assertEqual(asfm.tail[0][1].weight.size(), (5, 8))  # 5 targets in this cluster, dimensionality 8\n+        self.assertEqual(asfm.tail[1][1].weight.size(), (5, 4))\n+        self.assertEqual(asfm.tail[2][1].weight.size(), (5, 2))\n+\n+        self.assertEqual(asfm(x, y).output.size(), (2, ))\n+\n+        # log_probs actually returns log_proba\n+        asfm = nn.AdaptiveLogSoftmaxWithLoss(8, 4, [2], div_value=2.)\n+        x = torch.randn(4, 8)\n+        logprob_out = asfm.log_prob(x)\n+\n+        self.assertEqual(torch.exp(logprob_out).data.sum(1), torch.ones(4))\n+\n+        # forward returns the same thing as log_probs\n+        for v in [0, 1, 2, 3]:\n+            y = torch.full((4,), v, dtype=torch.long)\n+            out, loss = asfm(x, y)\n+\n+            self.assertEqual(out, logprob_out.gather(1, y.unsqueeze(1)).squeeze())\n+            self.assertEqual(loss, F.nll_loss(logprob_out, y))\n+\n+        # predict\n+        x = torch.randn(64, 8).abs_()\n+\n+        # argmax in shortlist\n+        asfm = nn.AdaptiveLogSoftmaxWithLoss(8, 10, [4, 8], div_value=2.)\n+        asfm.head.weight.data[:asfm.shortlist_size, :] += 100.", "path": "test/test_nn.py", "position": null, "original_position": 80, "commit_id": "7f89e1506a4e2df824118d3bc6e1e31f4073fc70", "original_commit_id": "572e9ffb1207b1720e485694f3890235033eb1a5", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "nit: can you zero_ the other part instead? It will be more robust.", "created_at": "2018-05-22T20:06:54Z", "updated_at": "2018-11-23T15:44:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/5287#discussion_r190035611", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5287", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/190035611"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5287#discussion_r190035611"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5287"}}, "body_html": "<p>nit: can you zero_ the other part instead? It will be more robust.</p>", "body_text": "nit: can you zero_ the other part instead? It will be more robust."}