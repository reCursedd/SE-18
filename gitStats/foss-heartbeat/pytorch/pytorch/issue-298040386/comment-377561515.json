{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377561515", "html_url": "https://github.com/pytorch/pytorch/pull/5287#issuecomment-377561515", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5287", "id": 377561515, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzU2MTUxNQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T16:20:48Z", "updated_at": "2018-03-30T16:20:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> and I had a discussion on this. Given the awkward nature of the module, here is the best approach we can think of. Let me know what you think, and please tell us any suggestions you have.</p>\n<ol>\n<li>Keep the class fused with name <code>AdaptiveSoftmax</code>.</li>\n<li>It's forward should be <code>def forward(self, input /* B x input_feature_before_linear_transform */, target /* B */)</code> (exactly as what you have now)</li>\n<li>Have three modes: return normalized probabilities (default), return log probability, and return loss (with support for <code>size_average</code> and <code>reduce</code> options). E.g., one way to support this is to let the mode be set using constructor and helper methods on the module.</li>\n<li>It should be listed under <code>Non-linear activations</code> in the docs.</li>\n</ol>", "body_text": "@ezyang and I had a discussion on this. Given the awkward nature of the module, here is the best approach we can think of. Let me know what you think, and please tell us any suggestions you have.\n\nKeep the class fused with name AdaptiveSoftmax.\nIt's forward should be def forward(self, input /* B x input_feature_before_linear_transform */, target /* B */) (exactly as what you have now)\nHave three modes: return normalized probabilities (default), return log probability, and return loss (with support for size_average and reduce options). E.g., one way to support this is to let the mode be set using constructor and helper methods on the module.\nIt should be listed under Non-linear activations in the docs.", "body": "@ezyang and I had a discussion on this. Given the awkward nature of the module, here is the best approach we can think of. Let me know what you think, and please tell us any suggestions you have.\r\n\r\n1. Keep the class fused with name `AdaptiveSoftmax`.\r\n2. It's forward should be `def forward(self, input /* B x input_feature_before_linear_transform */, target /* B */)` (exactly as what you have now)\r\n3. Have three modes: return normalized probabilities (default), return log probability, and return loss (with support for `size_average` and `reduce` options). E.g., one way to support this is to let the mode be set using constructor and helper methods on the module.\r\n4. It should be listed under `Non-linear activations` in the docs."}