{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189422156", "pull_request_review_id": 121605123, "id": 189422156, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTQyMjE1Ng==", "diff_hunk": "@@ -0,0 +1,229 @@\n+from collections import namedtuple\n+\n+import torch\n+\n+from . import Sequential, ModuleList, Linear\n+from .module import Module\n+from ..functional import log_softmax\n+\n+\n+_ASMoutput = namedtuple('ASMoutput', ['output', 'loss'])\n+\n+\n+class AdaptiveLogSoftmaxWithLoss(Module):\n+    r\"\"\"Efficient softmax approximation as described in\n+    `Efficient softmax approximation for GPUs`_ by Edouard Grave, Armand Joulin,\n+    Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou.\n+\n+    Adaptive softmax is an approximate strategy for training models with large\n+    output spaces. It is most effective when the label distribution is highly\n+    imbalanced, for example in natural language modelling, where the word\n+    frequency distribution approximately follows the `Zipf's law`_.\n+\n+    Adaptive softmax partitions the labels into several clusters, according to\n+    their frequency. These clusters may contain different number of targets\n+    each.\n+    Additionally, clusters containig less frequent labels assign lower\n+    dimensional embeddings to those labels, which speeds up the computation.\n+    For each minibatch, only clusters for which at least one target is\n+    present are evaluated.\n+\n+    The idea is that the clusters which are accessed frequently\n+    (like the first one, containing most frequent labels), should also be cheap\n+    to compute -- that is, contain a small number of assigned labels.\n+\n+    We highly recommend taking a look at the original paper for more details.\n+\n+    * :attr:`cutoffs` should be an ordered Sequence of integers sorted\n+      in the increasing order.\n+      It controls number of clusters and the partitioning of targets into\n+      clusters. For example setting ``cutoffs = [10, 100, 1000]``\n+      means that first `10` targets will be assigned\n+      to the 'head' of the adaptive softmax, targets `11, 12, ..., 100` will be\n+      assigned to the first cluster, and targets `101, 102, ..., 1000` will be\n+      assigned to the second cluster, while targets\n+      `1001, 1002, ..., n_classes - 1` will be assigned\n+      to the last, third cluster\n+\n+    * :attr:`div_value` is used to compute the size of each additional cluster,\n+      which is given as\n+      :math:`\\left\\lfloor\\frac{in\\_features}{div\\_value^{idx}}\\right\\rfloor`,\n+      where :math:`idx` is the cluster index (with clusters\n+      for less frequent words having larger indices,\n+      and indices starting from :math:`1`).\n+\n+    .. warning::\n+        Labels passed as inputs to this module should be sorted accoridng to\n+        their frequency. This means that the most frequent label should be\n+        represented by the index `0`, and the least frequent\n+        label should be represented by the index `n_classes - 1`.\n+\n+    .. note::\n+        This module returns a ``NamedTuple`` with ``output``\n+        and ``loss`` fields. See further documentation for details.\n+\n+    .. note::\n+        To compute log-probabilities for all classes, the ``log_prob``\n+        method can be used.\n+\n+    Args:\n+        in_features (int): Number of features in the input tensor\n+        n_classes (int): Number of classes in the dataset.\n+        cutoffs (Sequence): Cutoffs used to assign targets to their buckets.\n+        div_value (float, optional): value used as an exponent to compute sizes\n+            of the clusters. Default: 4.0\n+\n+    Returns:\n+        ``NamedTuple`` with ``output`` and ``loss`` fields:\n+            * **output** is a Tensor of size ``N`` containing computed target\n+              log probabilities for each example\n+            * **loss** is a Scalar representing the computed negative\n+              log likelihood loss\n+\n+    Shape:\n+        - input: :math:`(N, in\\_features)`\n+        - target: :math:`(N)` where each value satisfies :math:`0 <= target[i] <= n\\_classes`\n+        - output: :math:`(N)`\n+        - loss: ``Scalar``\n+\n+\n+    .. _Efficient softmax approximation for GPUs:\n+        https://arxiv.org/abs/1609.04309\n+\n+    .. _Zipf's law:\n+        https://en.wikipedia.org/wiki/Zipf%27s_law\n+    \"\"\"\n+\n+    def __init__(self, in_features, n_classes, cutoffs, div_value=4.):\n+        super(AdaptiveLogSoftmaxWithLoss, self).__init__()\n+\n+        cutoffs = list(cutoffs)\n+\n+        if (cutoffs != sorted(cutoffs)) \\\n+                or (min(cutoffs) <= 0) \\\n+                or (max(cutoffs) >= (n_classes - 1)) \\\n+                or (len(set(cutoffs)) != len(cutoffs)) \\\n+                or any([int(c) != c for c in cutoffs]):\n+\n+            raise ValueError(\"cutoffs should be a sequence of unique, positive \"\n+                             \"integers sorted in an increasing order, where \"\n+                             \"each value is between 1 and n_classes-1\")\n+\n+        self.in_features = in_features\n+        self.n_classes = n_classes\n+        self.cutoffs = cutoffs + [n_classes]\n+        self.div_value = div_value\n+\n+        self.shortlist_size = self.cutoffs[0]\n+        self.n_clusters = len(self.cutoffs) - 1\n+        self.head_size = self.shortlist_size + self.n_clusters\n+\n+        self.head = Linear(self.in_features, self.head_size)\n+        self.tail = ModuleList()\n+\n+        for i in range(self.n_clusters):\n+\n+            hsz = int(self.in_features // (self.div_value ** (i + 1)))\n+            osz = self.cutoffs[i + 1] - self.cutoffs[i]\n+\n+            projection = Sequential(\n+                Linear(self.in_features, hsz, bias=False),\n+                Linear(hsz, osz, bias=False)\n+            )\n+\n+            self.tail.append(projection)\n+\n+    def reset_parameters(self):\n+        self.head.reset_parameters()\n+        for i2h, h2o in self.tail:\n+            i2h.reset_parameters()\n+            h2o.reset_parameters()\n+\n+    def forward(self, input, target):\n+        if input.size(0) != target.size(0):\n+            raise RuntimeError('Input and target should have the same size '\n+                               'in the batch dimension.')\n+\n+        used_rows = 0\n+        batch_size = target.size(0)\n+\n+        output = input.new_zeros(batch_size)\n+        gather_inds = target.new_zeros(batch_size)\n+\n+        cutoff_values = [0] + self.cutoffs\n+        for i in range(len(cutoff_values) - 1):\n+\n+            low_idx = cutoff_values[i]\n+            high_idx = cutoff_values[i + 1]\n+\n+            target_mask = (target >= low_idx) & (target < high_idx)\n+            row_indices = target_mask.nonzero().squeeze()\n+\n+            if row_indices.numel() == 0:\n+                continue\n+\n+            if i == 0:\n+                gather_inds.index_copy_(0, row_indices, target[target_mask])\n+\n+            else:\n+                relative_target = target[target_mask] - cutoff_values[i]\n+                input_subset = input.index_select(0, row_indices)\n+\n+                cluster_output = self.tail[i - 1](input_subset)\n+                cluster_index = self.shortlist_size + i - 1\n+\n+                gather_inds.index_fill_(0, row_indices, cluster_index)\n+\n+                cluster_logprob = log_softmax(cluster_output, dim=1)\n+                local_logprob = cluster_logprob.gather(1, relative_target.unsqueeze(1))\n+                output.index_copy_(0, row_indices, local_logprob.squeeze(1))\n+\n+            used_rows += row_indices.numel()\n+\n+        if used_rows != batch_size:\n+            raise RuntimeError(\"Target values should be in [0, {}], \"\n+                               \"but values in range [{}, {}] \"\n+                               \"were found. \".format(self.n_classes - 1,\n+                                                     int(target.min()),\n+                                                     int(target.max())))\n+\n+        head_output = self.head(input)\n+        head_logprob = log_softmax(head_output, dim=1)\n+        output += head_logprob.gather(1, gather_inds.unsqueeze(1)).squeeze()\n+        loss = (-output).mean()\n+\n+        return _ASMoutput(output, loss)\n+\n+    def log_prob(self, input):\n+        \"\"\" Computes log probabilities for all :math:`n\\_classes`\n+\n+        Args:\n+            input (Tensor): a minibatch of examples\n+\n+        Returns:\n+            log-probabilities of for each class :math:`c`\n+            in range :math:`0 <= c <= n\\_classes`, where :math:`n\\_classes` is a\n+            parameter passed to ``AdaptiveLogSoftmaxWithLoss`` constructor.\n+\n+        Shape:\n+            - Input: :math:`(N, in\\_features)`\n+            - Output: :math:`(N, n\\_classes)`\n+\n+        \"\"\"\n+\n+        with torch.no_grad():\n+            out = input.new_zeros((input.size(0), self.n_classes))", "path": "torch/nn/modules/adaptive.py", "position": null, "original_position": 215, "commit_id": "7f89e1506a4e2df824118d3bc6e1e31f4073fc70", "original_commit_id": "c7d98d7d6fd60f3aed57372c1189418f2b3b3993", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "nit: `new_empty`", "created_at": "2018-05-19T02:13:32Z", "updated_at": "2018-11-23T15:44:18Z", "html_url": "https://github.com/pytorch/pytorch/pull/5287#discussion_r189422156", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5287", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189422156"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5287#discussion_r189422156"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5287"}}, "body_html": "<p>nit: <code>new_empty</code></p>", "body_text": "nit: new_empty"}