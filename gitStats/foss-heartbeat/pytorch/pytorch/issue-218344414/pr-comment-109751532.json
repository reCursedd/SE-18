{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109751532", "pull_request_review_id": 30671730, "id": 109751532, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTc1MTUzMg==", "diff_hunk": "@@ -24,38 +41,424 @@ void THCTensor_(spaddcdiv)(THCState *state, THCTensor *r_, THCTensor *t, real va\n }\n \n void THCSTensor_(spaddmm)(THCState *state, THCTensor *r_, real beta, THCTensor *t, real alpha, THCSTensor *sparse, THCTensor *dense) {\n-  THError(\"WARNING: Sparse Cuda Tensor op spaddmm is not implemented\");\n-  // TODO This is just a cusparse call (gemm?)\n+#if defined(THCS_REAL_IS_FLOAT) || defined(THCS_REAL_IS_DOUBLE)\n+  THCAssertSameGPU(THCSTensor_(checkGPU)(state, 1, 4, sparse, r_, t, dense));\n+  THCudaIntTensor *csr;\n+  THCIndexTensor *indices;\n+  THCTensor *values, *r__, *dense_;\n+\n+  THArgCheck(sparse->nDimensionI == 2, 2,\n+      \"matrices expected, got %dD tensor\", sparse->nDimensionI);\n+  THArgCheck(sparse->nDimensionV == 0, 2,\n+      \"scalar values expected, got %dD values\", sparse->nDimensionV);\n+  THArgCheck(dense->nDimension == 2, 2,\n+      \"matrices expected, got %dD tensor\", dense->nDimension);\n+\n+  long m = THCSTensor_(size)(state, sparse, 0);\n+  long k = THCSTensor_(size)(state, sparse, 1);\n+  long n = THCTensor_(size)(state, dense, 1);\n+\n+  THArgCheck(THCTensor_(size)(state, t, 0) == m, 1,\n+      \"Expected dim 0 size %d, got %d\", m, THCTensor_(size)(state, t, 0));\n+  THArgCheck(THCTensor_(size)(state, t, 1) == n, 1,\n+      \"Expected dim 1 size %d, got %d\", n, THCTensor_(size)(state, t, 1));\n+  THArgCheck(THCTensor_(size)(state, dense, 0) == k, 3,\n+      \"Expected dim 0 size %d, got %d\", k, THCTensor_(size)(state, dense, 0));\n+\n+  THCSTensor_(contiguous)(state, sparse);\n+\n+  long nnz = THCSTensor_(nnz)(state, sparse);\n+  indices = THCSTensor_(indices)(state, sparse);\n+  values = THCSTensor_(values)(state, sparse);\n+\n+  THCIndexTensor *rowIndices = THCIndexTensor_(new)(state);\n+  THCIndexTensor *colIndices = THCIndexTensor_(new)(state);\n+  THCIndexTensor_(select)(state, rowIndices, indices, 0, 0);\n+  THCIndexTensor_(select)(state, colIndices, indices, 0, 1);\n+  csr = THCSTensor_(toCSR)(state, rowIndices, m, nnz);\n+  THCudaIntTensor *colIndicesInt = THCudaIntTensor_newWithSize1d(state, colIndices->size[0]);\n+  THCudaIntTensor_copyCudaLong(state, colIndicesInt, colIndices);\n+\n+  char transpose_dense;\n+\n+  if(t != r_)\n+  {\n+    THCTensor_(resizeAs)(state, r_, t);\n+    THCTensor_(copy)(state, r_, t);\n+  }\n+\n+  /* r_ */\n+  if(r_->stride[0] == 1 && r_->stride[1] != 0) {\n+    r__ = r_;\n+  } else {\n+    THCTensor *transp_r_ = THCTensor_(newTranspose)(state, r_, 0, 1);\n+    r__ = THCTensor_(newClone)(state, transp_r_);\n+    THCTensor_(free)(state, transp_r_);\n+    THCTensor_(transpose)(state, r__, NULL, 0, 1);\n+  }\n+\n+  /* dense */\n+  if(dense->stride[0] == 1 && dense->stride[1] != 0) {\n+    transpose_dense = 'n';\n+    dense_ = dense;\n+  } else if(dense->stride[1] == 1 && dense->stride[0] != 0) {\n+    transpose_dense = 't';\n+    dense_ = dense;\n+  } else {\n+    transpose_dense = 't';\n+    dense_ = THCTensor_(newContiguous)(state, dense);\n+  }\n+#if defined(THCS_REAL_IS_FLOAT)\n+  THCudaSparse_Scsrmm2(\n+#elif defined(THCS_REAL_IS_DOUBLE)\n+  THCudaSparse_Dcsrmm2(\n+#endif\n+    state,\n+    'n',\n+    transpose_dense,\n+    m,\n+    n,\n+    k,\n+    nnz,\n+    alpha,\n+    THCTensor_(data)(state, values),\n+    THCudaIntTensor_data(state, csr),\n+    THCudaIntTensor_data(state, colIndicesInt),\n+    THCTensor_(data)(state, dense_),\n+    (transpose_dense == 'n' ? dense_->stride[1] : dense_->stride[0]),\n+    beta,\n+    THCTensor_(data)(state, r__),\n+    r__->stride[1]);\n+\n+  /* free intermediate variables */\n+  if(dense_ != dense) {\n+    THCTensor_(free)(state, dense_);", "path": "torch/lib/THCS/generic/THCSTensorMath.cu", "position": null, "original_position": 135, "commit_id": "f4105bcbb3dc4ebc23f20112949a0de79e24d978", "original_commit_id": "179b2dafe865e7cbdd79b18f58143e8ca5d4782d", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "body": "This isn't safe, because `THCTensor_(newContiguous)` may or may not have made a copy. (Although if you fix the bug above with contiguous, I guess it will work).\r\n\r\nSafer would be to call `retain` in the first two branches above, and then call `free` here unconditionally.\r\n\r\nSame for `r_` below.", "created_at": "2017-04-04T19:07:54Z", "updated_at": "2018-11-23T15:33:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109751532", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1147", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109751532"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109751532"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1147"}}, "body_html": "<p>This isn't safe, because <code>THCTensor_(newContiguous)</code> may or may not have made a copy. (Although if you fix the bug above with contiguous, I guess it will work).</p>\n<p>Safer would be to call <code>retain</code> in the first two branches above, and then call <code>free</code> here unconditionally.</p>\n<p>Same for <code>r_</code> below.</p>", "body_text": "This isn't safe, because THCTensor_(newContiguous) may or may not have made a copy. (Although if you fix the bug above with contiguous, I guess it will work).\nSafer would be to call retain in the first two branches above, and then call free here unconditionally.\nSame for r_ below."}