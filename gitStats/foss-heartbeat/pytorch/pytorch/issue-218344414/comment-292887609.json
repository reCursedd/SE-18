{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/292887609", "html_url": "https://github.com/pytorch/pytorch/pull/1147#issuecomment-292887609", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1147", "id": 292887609, "node_id": "MDEyOklzc3VlQ29tbWVudDI5Mjg4NzYwOQ==", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-10T08:48:51Z", "updated_at": "2017-04-10T09:22:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a></p>\n<blockquote>\n<p>In dense libs, we never make the input contiguous for the user, because it could break sharing + it makes for a much less magical API</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> also noticed it is surprising that <code>contiguous</code> operates in-place. We could just rename it to <code>contiguous_</code>. We might also be able to enforce that operations never make the input contiguous automatically (but either make a contiguous copy or throw an exception so that the user calls <code>contiguous_</code> explicitly).</p>\n<blockquote>\n<p>I'm also worried that someone will be adding non-linear ops, and they will forget that non-contiguous means not only that the indices aren't ordered, but they're duplicated too</p>\n</blockquote>\n<p>I believe making indices unique is an O(NlogN) operation anyway, so if we're doing that we might as well make the tensor contiguous. However I think we'd be wasting a lot of computation if we were forcing all sparse tensors to always be contiguous. For example it means you would have to spend O(NlogN) time for every single sparse gradient aggregation, rather than doing a single O(NlogN) operation when applying the gradient update.</p>\n<blockquote>\n<p>Is it true that right now we don't do any contiguous() calls on the tensors that are using for gradients in Embedding?</p>\n</blockquote>\n<p>In the case of Adagrad, which is non-linear, we have to call <code>contiguous</code> on sparse gradients. Right now only SGD and Adagrad are supported with sparse gradients (which is why <code>sparse=True</code> is not the default for <code>Embedding</code>). We were discussing with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a> about the best way to support other optimizers (it requires adjusting the maths a bit to support momentum without deviating too much from the dense formulation).</p>", "body_text": "@apaszke\n\nIn dense libs, we never make the input contiguous for the user, because it could break sharing + it makes for a much less magical API\n\n@fmassa also noticed it is surprising that contiguous operates in-place. We could just rename it to contiguous_. We might also be able to enforce that operations never make the input contiguous automatically (but either make a contiguous copy or throw an exception so that the user calls contiguous_ explicitly).\n\nI'm also worried that someone will be adding non-linear ops, and they will forget that non-contiguous means not only that the indices aren't ordered, but they're duplicated too\n\nI believe making indices unique is an O(NlogN) operation anyway, so if we're doing that we might as well make the tensor contiguous. However I think we'd be wasting a lot of computation if we were forcing all sparse tensors to always be contiguous. For example it means you would have to spend O(NlogN) time for every single sparse gradient aggregation, rather than doing a single O(NlogN) operation when applying the gradient update.\n\nIs it true that right now we don't do any contiguous() calls on the tensors that are using for gradients in Embedding?\n\nIn the case of Adagrad, which is non-linear, we have to call contiguous on sparse gradients. Right now only SGD and Adagrad are supported with sparse gradients (which is why sparse=True is not the default for Embedding). We were discussing with @adamlerer about the best way to support other optimizers (it requires adjusting the maths a bit to support momentum without deviating too much from the dense formulation).", "body": "@apaszke \r\n\r\n> In dense libs, we never make the input contiguous for the user, because it could break sharing + it makes for a much less magical API\r\n\r\n@fmassa also noticed it is surprising that `contiguous` operates in-place. We could just rename it to `contiguous_`. We might also be able to enforce that operations never make the input contiguous automatically (but either make a contiguous copy or throw an exception so that the user calls `contiguous_` explicitly).\r\n\r\n> I'm also worried that someone will be adding non-linear ops, and they will forget that non-contiguous means not only that the indices aren't ordered, but they're duplicated too\r\n\r\nI believe making indices unique is an O(NlogN) operation anyway, so if we're doing that we might as well make the tensor contiguous. However I think we'd be wasting a lot of computation if we were forcing all sparse tensors to always be contiguous. For example it means you would have to spend O(NlogN) time for every single sparse gradient aggregation, rather than doing a single O(NlogN) operation when applying the gradient update.\r\n\r\n> Is it true that right now we don't do any contiguous() calls on the tensors that are using for gradients in Embedding?\r\n\r\nIn the case of Adagrad, which is non-linear, we have to call `contiguous` on sparse gradients. Right now only SGD and Adagrad are supported with sparse gradients (which is why `sparse=True` is not the default for `Embedding`). We were discussing with @adamlerer about the best way to support other optimizers (it requires adjusting the maths a bit to support momentum without deviating too much from the dense formulation)."}