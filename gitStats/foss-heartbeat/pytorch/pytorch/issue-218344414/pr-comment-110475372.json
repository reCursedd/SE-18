{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110475372", "pull_request_review_id": 31644119, "id": 110475372, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMDQ3NTM3Mg==", "diff_hunk": "@@ -24,38 +41,424 @@ void THCTensor_(spaddcdiv)(THCState *state, THCTensor *r_, THCTensor *t, real va\n }\n \n void THCSTensor_(spaddmm)(THCState *state, THCTensor *r_, real beta, THCTensor *t, real alpha, THCSTensor *sparse, THCTensor *dense) {\n-  THError(\"WARNING: Sparse Cuda Tensor op spaddmm is not implemented\");\n-  // TODO This is just a cusparse call (gemm?)\n+#if defined(THCS_REAL_IS_FLOAT) || defined(THCS_REAL_IS_DOUBLE)\n+  THCAssertSameGPU(THCSTensor_(checkGPU)(state, 1, 4, sparse, r_, t, dense));\n+  THCudaIntTensor *csr;\n+  THCIndexTensor *indices;\n+  THCTensor *values, *r__, *dense_;\n+\n+  THArgCheck(sparse->nDimensionI == 2, 2,\n+      \"matrices expected, got %dD tensor\", sparse->nDimensionI);\n+  THArgCheck(sparse->nDimensionV == 0, 2,\n+      \"scalar values expected, got %dD values\", sparse->nDimensionV);\n+  THArgCheck(dense->nDimension == 2, 2,\n+      \"matrices expected, got %dD tensor\", dense->nDimension);\n+\n+  long m = THCSTensor_(size)(state, sparse, 0);\n+  long k = THCSTensor_(size)(state, sparse, 1);\n+  long n = THCTensor_(size)(state, dense, 1);\n+\n+  THArgCheck(THCTensor_(size)(state, t, 0) == m, 1,\n+      \"Expected dim 0 size %d, got %d\", m, THCTensor_(size)(state, t, 0));\n+  THArgCheck(THCTensor_(size)(state, t, 1) == n, 1,\n+      \"Expected dim 1 size %d, got %d\", n, THCTensor_(size)(state, t, 1));\n+  THArgCheck(THCTensor_(size)(state, dense, 0) == k, 3,\n+      \"Expected dim 0 size %d, got %d\", k, THCTensor_(size)(state, dense, 0));\n+\n+  THCSTensor_(contiguous)(state, sparse);\n+\n+  long nnz = THCSTensor_(nnz)(state, sparse);\n+  indices = THCSTensor_(indices)(state, sparse);\n+  values = THCSTensor_(values)(state, sparse);\n+\n+  THCIndexTensor *rowIndices = THCIndexTensor_(new)(state);\n+  THCIndexTensor *colIndices = THCIndexTensor_(new)(state);\n+  THCIndexTensor_(select)(state, rowIndices, indices, 0, 0);\n+  THCIndexTensor_(select)(state, colIndices, indices, 0, 1);\n+  csr = THCSTensor_(toCSR)(state, rowIndices, m, nnz);\n+  THCudaIntTensor *colIndicesInt = THCudaIntTensor_newWithSize1d(state, colIndices->size[0]);\n+  THCudaIntTensor_copyCudaLong(state, colIndicesInt, colIndices);\n+\n+  char transpose_dense;\n+\n+  if(t != r_)\n+  {\n+    THCTensor_(resizeAs)(state, r_, t);\n+    THCTensor_(copy)(state, r_, t);\n+  }\n+\n+  /* r_ */\n+  if(r_->stride[0] == 1 && r_->stride[1] != 0) {", "path": "torch/lib/THCS/generic/THCSTensorMath.cu", "position": null, "original_position": 91, "commit_id": "f4105bcbb3dc4ebc23f20112949a0de79e24d978", "original_commit_id": "179b2dafe865e7cbdd79b18f58143e8ca5d4782d", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "body": "Yes, those operations expect column-major order. That is one of the two big issues I had with cuSPARSE, the other being that `long` indices are not supported (only `int`).\r\n\r\nYou're right I should explicitly check against `r_->size[0]`, and same below (I took inspiration from our wrappers around cuBLAS, but maybe in those wrappers the tensors are known to be contiguous modulo transposition)", "created_at": "2017-04-07T20:23:51Z", "updated_at": "2018-11-23T15:33:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/1147#discussion_r110475372", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1147", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110475372"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1147#discussion_r110475372"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1147"}}, "body_html": "<p>Yes, those operations expect column-major order. That is one of the two big issues I had with cuSPARSE, the other being that <code>long</code> indices are not supported (only <code>int</code>).</p>\n<p>You're right I should explicitly check against <code>r_-&gt;size[0]</code>, and same below (I took inspiration from our wrappers around cuBLAS, but maybe in those wrappers the tensors are known to be contiguous modulo transposition)</p>", "body_text": "Yes, those operations expect column-major order. That is one of the two big issues I had with cuSPARSE, the other being that long indices are not supported (only int).\nYou're right I should explicitly check against r_->size[0], and same below (I took inspiration from our wrappers around cuBLAS, but maybe in those wrappers the tensors are known to be contiguous modulo transposition)", "in_reply_to_id": 109750982}