{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109405245", "pull_request_review_id": 30446003, "id": 109405245, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTQwNTI0NQ==", "diff_hunk": "@@ -24,38 +41,425 @@ void THCTensor_(spaddcdiv)(THCState *state, THCTensor *r_, THCTensor *t, real va\n }\n \n void THCSTensor_(spaddmm)(THCState *state, THCTensor *r_, real beta, THCTensor *t, real alpha, THCSTensor *sparse, THCTensor *dense) {\n-  THError(\"WARNING: Sparse Cuda Tensor op spaddmm is not implemented\");\n-  // TODO This is just a cusparse call (gemm?)\n+#if defined(THCS_REAL_IS_FLOAT) || defined(THCS_REAL_IS_DOUBLE)\n+  THCAssertSameGPU(THCSTensor_(checkGPU)(state, 1, 4, sparse, r_, t, dense));\n+  THCudaIntTensor *csr;\n+  THCIndexTensor *indices;\n+  THCTensor *values, *r__, *dense_;\n+\n+  THArgCheck(sparse->nDimensionI == 2, 2,\n+      \"matrices expected, got %dD tensor\", sparse->nDimensionI);\n+  THArgCheck(sparse->nDimensionV == 0, 2,\n+      \"scalar values expected, got %dD values\", sparse->nDimensionV);\n+  THArgCheck(dense->nDimension == 2, 2,\n+      \"matrices expected, got %dD tensor\", dense->nDimension);\n+\n+  long m = THCSTensor_(size)(state, sparse, 0);\n+  long k = THCSTensor_(size)(state, sparse, 1);\n+  long n = THCTensor_(size)(state, dense, 1);\n+\n+  THArgCheck(THCTensor_(size)(state, t, 0) == m, 1,\n+      \"Expected dim 0 size %d, got %d\", m, THCTensor_(size)(state, t, 0));\n+  THArgCheck(THCTensor_(size)(state, t, 1) == n, 1,\n+      \"Expected dim 1 size %d, got %d\", n, THCTensor_(size)(state, t, 1));\n+  THArgCheck(THCTensor_(size)(state, dense, 0) == k, 3,\n+      \"Expected dim 0 size %d, got %d\", k, THCTensor_(size)(state, dense, 0));\n+\n+  THCSTensor_(contiguous)(state, sparse);\n+\n+  long nnz     = THCSTensor_(nnz)(state, sparse);\n+  indices = THCSTensor_(indices)(state, sparse);\n+  values  = THCSTensor_(values)(state, sparse);\n+\n+  THCIndexTensor *rowIndices = THCIndexTensor_(new)(state);\n+  THCIndexTensor *colIndices = THCIndexTensor_(new)(state);\n+  THCIndexTensor_(select)(state, rowIndices, indices, 0, 0);\n+  THCIndexTensor_(select)(state, colIndices, indices, 0, 1);\n+  csr = THCSTensor_(toCSR)(state, rowIndices, m, nnz);\n+  THCudaIntTensor *colIndicesInt = THCudaIntTensor_newWithSize1d(state, colIndices->size[0]);\n+  THCudaIntTensor_copyCudaLong(state, colIndicesInt, colIndices);\n+\n+\n+  char transpose_dense;\n+\n+  if(t != r_)\n+  {\n+    THCTensor_(resizeAs)(state, r_, t);\n+    THCTensor_(copy)(state, r_, t);\n+  }\n+\n+  /* r_ */\n+  if(r_->stride[0] == 1 && r_->stride[1] != 0) {\n+    r__ = r_;\n+  } else {\n+    THCTensor *transp_r_ = THCTensor_(newTranspose)(state, r_, 0, 1);\n+    r__ = THCTensor_(newClone)(state, transp_r_);\n+    THCTensor_(free)(state, transp_r_);\n+    THCTensor_(transpose)(state, r__, NULL, 0, 1);\n+  }\n+\n+  /* dense */\n+  if(dense->stride[0] == 1 && dense->stride[1] != 0) {\n+    transpose_dense = 'n';\n+    dense_ = dense;\n+  } else if(dense->stride[1] == 1 && dense->stride[0] != 0) {\n+    transpose_dense = 't';\n+    dense_ = dense;\n+  } else {\n+    transpose_dense = 't';\n+    dense_ = THCTensor_(newContiguous)(state, dense);\n+  }\n+#if defined(THCS_REAL_IS_FLOAT)\n+  THCudaSparse_Scsrmm2(\n+#elif defined(THCS_REAL_IS_DOUBLE)\n+  THCudaSparse_Dcsrmm2(\n+#endif\n+    state,\n+    'n',\n+    transpose_dense,\n+    m,\n+    n,\n+    k,\n+    nnz,\n+    alpha,\n+    THCTensor_(data)(state, values),\n+    THCudaIntTensor_data(state, csr),\n+    THCudaIntTensor_data(state, colIndicesInt),\n+    THCTensor_(data)(state, dense_),\n+    (transpose_dense == 'n' ? dense_->stride[1] : dense_->stride[0]),\n+    beta,\n+    THCTensor_(data)(state, r__),\n+    r__->stride[1]);\n+\n+  /* free intermediate variables */\n+  if(dense_ != dense) {\n+    THCTensor_(free)(state, dense_);\n+  }\n+\n+  if(r__ != r_) {\n+    THCTensor_(freeCopyTo)(state, r__, r_);\n+  }\n+\n+  THCudaIntTensor_free(state, colIndicesInt);\n+  THCudaIntTensor_free(state, csr);\n+  THCIndexTensor_(free)(state, indices);\n+  THCIndexTensor_(free)(state, rowIndices);\n+  THCIndexTensor_(free)(state, colIndices);\n+  THCTensor_(free)(state, values);\n+#else\n+  THError(\"unimplemented data type\");\n+#endif\n }\n \n void THCSTensor_(sspaddmm)(THCState *state, THCSTensor *r_, real beta, THCSTensor *t, real alpha, THCSTensor *sparse, THCTensor *dense) {\n   THError(\"WARNING: Sparse Cuda Tensor op sspaddmm is not implemented\");\n   // TODO Write some kernels\n }\n \n+void THCSTensor_(hspmm)(THCState *state, THCSTensor *r_, real alpha, THCSTensor *sparse, THCTensor *dense) {\n+#if CUDA_VERSION >= 7000\n+  THCThrustAllocator thrustAlloc(state);\n+#define THRUST_EXEC(fn, ...) fn(thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)), ##__VA_ARGS__)\n+#else\n+#define THRUST_EXEC(fn, ...) fn(##__VA_ARGS__)\n+#endif\n+\n+  THCAssertSameGPU(THCSTensor_(checkGPU)(state, 2, 3, r_, sparse, dense));\n+\n+  THArgCheck(sparse->nDimensionI == 2, 3,\n+      \"matrices expected, got %dD tensor\", sparse->nDimensionI);\n+  THArgCheck(sparse->nDimensionV == 0, 3,\n+      \"scalar values expected, got %dD values\", sparse->nDimensionV);\n+  THArgCheck(dense->nDimension == 2, 4,\n+      \"matrices expected, got %dD tensor\", dense->nDimension);\n+\n+  long m = THCSTensor_(size)(state, sparse, 0);\n+  long k = THCSTensor_(size)(state, sparse, 1);\n+  long n = THCTensor_(size)(state, dense, 1);\n+\n+  THArgCheck(THCTensor_(size)(state, dense, 0) == k, 4,\n+      \"Expected dim 0 size %d, got %d\", k, THCTensor_(size)(state, dense, 0));\n+  long size[2] = {m, n};\n+  THCSTensor_(rawResize)(state, r_, 1, 1, size);\n+\n+  THCSTensor_(contiguous)(state, sparse);\n+\n+  long nnz = THCSTensor_(nnz)(state, sparse);\n+  THCIndexTensor *indices = THCIndexTensor_(newWithSize2d)(state, 1, nnz);\n+  // create values in column-major format to avoid copying in spaddmm\n+  THCTensor *values = THCTensor_(newWithSize2d)(state, n, nnz);\n+  THCTensor_(transpose)(state, values, NULL, 0, 1);\n+\n+  THCSTensor *newSparse = THCSTensor_(newClone)(state, sparse);\n+  THCIndexTensor *spIndices = THCSTensor_(indices)(state, newSparse);\n+  THCIndexTensor *dstIndices = THCIndexTensor_(new)(state);\n+  THCIndexTensor_(select)(state, dstIndices, spIndices, 0, 0);\n+  // Save destination indices to output hybrid tensor\n+  THCIndexTensor_(copy)(state, indices, dstIndices);\n+  // Replace destination indices with 0, 1, 2, 3, ... and compute output values\n+  // tensor with sparse * dense multiplication\n+  thrust::device_ptr<integer> indicesIter(THCIndexTensor_(data)(state, dstIndices));\n+  THRUST_EXEC(thrust::sequence, indicesIter, indicesIter + nnz);\n+  newSparse->size[0] = nnz;\n+  THCSTensor_(spaddmm)(state, values, ScalarConvert<int, real>::to(0), values, alpha, newSparse, dense);\n+  THCSTensor_(move)(state, r_, indices, values);\n+\n+  THCSTensor_(free)(state, newSparse);\n+  THCIndexTensor_(free)(state, spIndices);\n+  THCIndexTensor_(free)(state, dstIndices);\n+\n+#undef THRUST_EXEC\n+}\n+\n void THCSTensor_(spcadd)(THCState *state, THCTensor *r_, THCTensor *dense, real value, THCSTensor *sparse) {\n-  THError(\"WARNING: Sparse Cuda Tensor op spcadd is not implemented\");\n-  // TODO pretty sure this is also just a cusparse call (axpyi)\n+  THCAssertSameGPU(THCSTensor_(checkGPU)(state, 1, 3, sparse, r_, dense));\n+  THCTensor_(resizeAs)(state, r_, dense);\n+  THCSTensor_(contiguous)(state, sparse);\n+\n+  THCIndexTensor *indices = THCSTensor_(indices)(state, sparse);\n+  THCTensor *values = THCSTensor_(values)(state, sparse);\n+  long nDim = THCTensor_(nDimension)(state, dense);\n+  long nDimI = THCSTensor_(nDimensionI)(state, sparse);\n+\n+  if (r_ != dense) {\n+    THCTensor_(resizeAs)(state, r_, dense);\n+    THCTensor_(copy)(state, r_, dense);\n+  } else {\n+    if(!THCTensor_(isContiguous)(state, r_)) {\n+      THCTensor* r_contiguous = THCTensor_(newContiguous)(state, r_);\n+      THCTensor_(copy)(state, r_, r_contiguous);\n+      THCTensor_(free)(state, r_contiguous);\n+    }\n+  }\n+\n+  // TODO more benchmarking\n+  const dim3 block = getApplyBlock();\n+  dim3 grid;\n+  if (sparse->nDimensionV == 0) {\n+    THArgCheck(getApplyGrid(state, sparse->nnz, grid), 1, CUTORCH_DIM_WARNING);\n+\n+    THCSTensor_spcKernelScalar<TensorCAddOp<real>, unsigned long, real>\n+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n+        TensorCAddOp<real>(value),\n+        V_INFO(r_), I_INFO(indices), V_INFO(values),\n+        (unsigned long)sparse->nnz);\n+  } else {\n+    THArgCheck(getApplyGrid(state, sparse->nnz * block.x, grid), 1, CUTORCH_DIM_WARNING);\n+\n+    THCSTensor_spcKernel<TensorCAddOp<real>, unsigned long, real>\n+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n+        TensorCAddOp<real>(value),\n+        V_INFO(r_), I_INFO(indices), V_INFO(values),\n+        (unsigned long)sparse->nnz);\n+  }\n+  THCudaCheck(cudaGetLastError());\n+\n+  THCIndexTensor_(free)(state, indices);\n+  THCTensor_(free)(state, values);\n }\n \n void THCSTensor_(mul)(THCState *state, THCSTensor *r_, THCSTensor *t, real value) {\n-  THError(\"WARNING: Sparse Cuda Tensor op mul is not implemented\");\n+  if (r_ == t) {\n+    THCTensor *r_values_ = THCSTensor_(values)(state, r_);\n+    THCTensor_(mul)(state, r_values_, r_values_, value);\n+    THCTensor_(free)(state, r_values_);\n+  } else {\n+    THCSTensor_(resizeAs)(state, r_, t);\n+\n+    THCIndexTensor *r_indices_ = THCSTensor_(indices)(state, r_);\n+    THCTensor *r_values_ = THCSTensor_(values)(state, r_);\n+    THCIndexTensor *t_indices_ = THCSTensor_(indices)(state, t);\n+    THCTensor *t_values_ = THCSTensor_(values)(state, t);\n+\n+    THCIndexTensor_(resizeAs)(state, r_indices_, t_indices_);\n+    THCIndexTensor_(copy)(state, r_indices_, t_indices_);\n+    THCTensor_(mul)(state, r_values_, t_values_, value);\n+    r_->nnz = t->nnz;\n+    r_->contiguous = t->contiguous;\n+\n+    THCIndexTensor_(free)(state, r_indices_);\n+    THCTensor_(free)(state, r_values_);\n+    THCIndexTensor_(free)(state, t_indices_);\n+    THCTensor_(free)(state, t_values_);\n+  }\n }\n \n void THCSTensor_(div)(THCState *state, THCSTensor *r_, THCSTensor *t, real value) {\n-  THError(\"WARNING: Sparse Cuda Tensor op div is not implemented\");\n+  if (r_ == t) {\n+    THCTensor *r_values_ = THCSTensor_(values)(state, r_);\n+    THCTensor_(div)(state, r_values_, r_values_, value);\n+    THCTensor_(free)(state, r_values_);\n+  } else {\n+    THCSTensor_(resizeAs)(state, r_, t);\n+\n+    THCIndexTensor *r_indices_ = THCSTensor_(indices)(state, r_);\n+    THCTensor *r_values_ = THCSTensor_(values)(state, r_);\n+    THCIndexTensor *t_indices_ = THCSTensor_(indices)(state, t);\n+    THCTensor *t_values_ = THCSTensor_(values)(state, t);\n+\n+    THCIndexTensor_(resizeAs)(state, r_indices_, t_indices_);\n+    THCIndexTensor_(copy)(state, r_indices_, t_indices_);\n+    THCTensor_(div)(state, r_values_, t_values_, value);\n+    r_->nnz = t->nnz;\n+    r_->contiguous = t->contiguous;\n+\n+    THCIndexTensor_(free)(state, r_indices_);\n+    THCTensor_(free)(state, r_values_);\n+    THCIndexTensor_(free)(state, t_indices_);\n+    THCTensor_(free)(state, t_values_);\n+  }\n }\n \n void THCSTensor_(cadd)(THCState *state, THCSTensor *r_, THCSTensor *t, real value, THCSTensor *src) {\n-  THError(\"WARNING: Sparse Cuda Tensor op cadd is not implemented\");\n+  THCAssertSameGPU(THCSTensor_(checkGPU)(state, 3, 3, r_, t, src));\n+  if(!THCSTensor_(isSameSizeAs)(state, t, src)) {\n+    THError(\"cadd operands have incompatible sizes or dimension types\");\n+  }\n+  THCSTensor_(contiguous)(state, t);\n+  THCSTensor_(contiguous)(state, src);\n+\n+  if (src->nnz == 0) {\n+    THCSTensor_(copy)(state, r_, t);\n+    return;\n+  }\n+  if (t->nnz == 0) {\n+    THCSTensor_(mul)(state, r_, src, value);\n+    return;\n+  }\n+\n+  // We deliberately choose to simply concat the indices and values tensors\n+  // rather than merging them. This removes the need to synchronously fetch nnz\n+  // at the end of the operation, at the cost of having a non-contiguous result.\n+  // This trade-off is preferable for the common use-case of gradient accumulation.\n+  // TODO have two distinct functions? The other option is commented out below", "path": "torch/lib/THCS/generic/THCSTensorMath.cu", "position": 338, "original_position": 339, "commit_id": "f4105bcbb3dc4ebc23f20112949a0de79e24d978", "original_commit_id": "8e4660ac5145c4a48e35620f8810fa8863cb7b50", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "body": "for THS/THCS, contiguous = sorted indices + no duplicate indices. Here we explicitly return a non-contiguous tensor, which means that there might be duplicate indices. When calling the `contiguous` function, indices will be sorted and deduplicated.", "created_at": "2017-04-03T12:35:34Z", "updated_at": "2018-11-23T15:32:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109405245", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1147", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109405245"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109405245"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1147"}}, "body_html": "<p>for THS/THCS, contiguous = sorted indices + no duplicate indices. Here we explicitly return a non-contiguous tensor, which means that there might be duplicate indices. When calling the <code>contiguous</code> function, indices will be sorted and deduplicated.</p>", "body_text": "for THS/THCS, contiguous = sorted indices + no duplicate indices. Here we explicitly return a non-contiguous tensor, which means that there might be duplicate indices. When calling the contiguous function, indices will be sorted and deduplicated.", "in_reply_to_id": 109176112}