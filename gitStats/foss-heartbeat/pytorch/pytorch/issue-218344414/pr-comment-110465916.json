{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110465916", "pull_request_review_id": 31644119, "id": 110465916, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMDQ2NTkxNg==", "diff_hunk": "@@ -1,22 +1,238 @@\n #include \"THCSTensor.h\"\n #include \"THCApply.cuh\"\n+#include \"THCTensorMathPointwise.cuh\"\n #include \"stdio.h\"\n \n-template <typename IndexType, typename Real>\n-__global__ void THCSTensor_toDenseKernel(\n-    TensorInfo<Real, IndexType> other,\n-    TensorInfo<long, IndexType> indices,\n+template <typename IndexType, typename Real, typename Op>\n+__device__ void applyOp2(\n+    Op op, IndexType blockSize,\n+    TensorInfo<Real, IndexType> values1, IndexType idx1,\n+    TensorInfo<Real, IndexType> values2, IndexType idx2) {\n+  for (IndexType k = blockIdx.x * blockDim.x + threadIdx.x;\n+       k < blockSize;\n+       k += gridDim.x * blockDim.x) {\n+    op(values1.data + idx1 * blockSize + k, values2.data + idx2 * blockSize + k);\n+  }\n+}\n+\n+template <typename IndexType, typename Real, typename Op>\n+__device__ void applyOp3(\n+    Op op, IndexType blockSize,\n+    TensorInfo<Real, IndexType> values1, IndexType idx1,\n+    TensorInfo<Real, IndexType> values2, IndexType idx2,\n+    TensorInfo<Real, IndexType> values3, IndexType idx3) {\n+  for (IndexType k = blockIdx.x * blockDim.x + threadIdx.x;\n+       k < blockSize;\n+       k += gridDim.x * blockDim.x) {\n+    op(values1.data + idx1 * blockSize + k,\n+       values2.data + idx2 * blockSize + k,\n+       values3.data + idx3 * blockSize + k);\n+  }\n+}\n+\n+template <typename Op, typename IndexType, typename Real>\n+__global__ void THCSTensor_sparseElementwiseKernel(\n+    Op op,\n+    TensorInfo<Real, IndexType> dense,\n+    TensorInfo<integer, IndexType> indices,\n     TensorInfo<Real, IndexType> values,\n     const IndexType nnz) {\n+  IndexType indskip = indices.strides[0];\n+  IndexType valueSize = values.strides[0];\n+  for (IndexType linearId = blockIdx.x;\n+       linearId < nnz;\n+       linearId += gridDim.x) {\n+    IndexType index = 0;\n+    for (IndexType d = 0; d < indices.sizes[0]; d++) {\n+      index = dense.sizes[d] * index + indices.data[d * indskip + linearId];\n+    }\n+    Real *dst = dense.data + index * valueSize;\n+    Real *src = values.data + linearId * valueSize;\n+    for (IndexType linearId2 = threadIdx.x; linearId2 < valueSize; linearId2 += blockDim.x) {\n+      op(dst + linearId2, src + linearId2);\n+    }\n+  }\n+}\n+\n+template <typename Op, typename IndexType, typename Real>\n+__global__ void THCSTensor_sparseElementwiseKernelScalar(\n+    Op op,\n+    TensorInfo<Real, IndexType> dense,\n+    TensorInfo<integer, IndexType> indices,\n+    TensorInfo<Real, IndexType> values,\n+    const IndexType nnz) {\n+  IndexType indskip = indices.strides[0];\n   for (IndexType linearId = blockIdx.x * blockDim.x + threadIdx.x;\n-      linearId < nnz;\n-      linearId += gridDim.x * blockDim.x) {\n+       linearId < nnz;\n+       linearId += gridDim.x * blockDim.x) {\n     IndexType index = 0;\n-    IndexType indskip = indices.strides[0];\n-    for (IndexType d = 0; d < indices.sizes[0]; d++)\n-      index = other.sizes[d] * index + indices.data[d * indskip + linearId];\n-    other.data[index] = other.data[index] + values.data[linearId];\n+    for (IndexType d = 0; d < indices.sizes[0]; d++) {\n+      index = dense.sizes[d] * index + indices.data[d * indskip + linearId];\n+    }\n+    op(dense.data + index, values.data + linearId);\n+  }\n+}\n+\n+template <typename OpBoth, typename OpLeft, typename OpRight, typename IndexType, typename Real>", "path": "torch/lib/THCS/THCSTensor.cu", "position": 86, "original_position": 86, "commit_id": "f4105bcbb3dc4ebc23f20112949a0de79e24d978", "original_commit_id": "179b2dafe865e7cbdd79b18f58143e8ca5d4782d", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "body": "Ah, actually the parallelization is in `applyOp2` and `applyOp3`. It is only useful in the case of non-scalar values though :) I agree we should do better in the scalar case (and do more benchmarking/optimization in general).\r\n\r\nThe grid size should already be 1 for scalar values, I'll make sure the block size is not more than the value size so we don't waste resources for scalar/small values.", "created_at": "2017-04-07T19:35:32Z", "updated_at": "2018-11-23T15:33:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/1147#discussion_r110465916", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1147", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110465916"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1147#discussion_r110465916"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1147"}}, "body_html": "<p>Ah, actually the parallelization is in <code>applyOp2</code> and <code>applyOp3</code>. It is only useful in the case of non-scalar values though :) I agree we should do better in the scalar case (and do more benchmarking/optimization in general).</p>\n<p>The grid size should already be 1 for scalar values, I'll make sure the block size is not more than the value size so we don't waste resources for scalar/small values.</p>", "body_text": "Ah, actually the parallelization is in applyOp2 and applyOp3. It is only useful in the case of non-scalar values though :) I agree we should do better in the scalar case (and do more benchmarking/optimization in general).\nThe grid size should already be 1 for scalar values, I'll make sure the block size is not more than the value size so we don't waste resources for scalar/small values.", "in_reply_to_id": 109771093}