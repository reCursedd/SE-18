{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109771093", "pull_request_review_id": 30671730, "id": 109771093, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTc3MTA5Mw==", "diff_hunk": "@@ -1,22 +1,238 @@\n #include \"THCSTensor.h\"\n #include \"THCApply.cuh\"\n+#include \"THCTensorMathPointwise.cuh\"\n #include \"stdio.h\"\n \n-template <typename IndexType, typename Real>\n-__global__ void THCSTensor_toDenseKernel(\n-    TensorInfo<Real, IndexType> other,\n-    TensorInfo<long, IndexType> indices,\n+template <typename IndexType, typename Real, typename Op>\n+__device__ void applyOp2(\n+    Op op, IndexType blockSize,\n+    TensorInfo<Real, IndexType> values1, IndexType idx1,\n+    TensorInfo<Real, IndexType> values2, IndexType idx2) {\n+  for (IndexType k = blockIdx.x * blockDim.x + threadIdx.x;\n+       k < blockSize;\n+       k += gridDim.x * blockDim.x) {\n+    op(values1.data + idx1 * blockSize + k, values2.data + idx2 * blockSize + k);\n+  }\n+}\n+\n+template <typename IndexType, typename Real, typename Op>\n+__device__ void applyOp3(\n+    Op op, IndexType blockSize,\n+    TensorInfo<Real, IndexType> values1, IndexType idx1,\n+    TensorInfo<Real, IndexType> values2, IndexType idx2,\n+    TensorInfo<Real, IndexType> values3, IndexType idx3) {\n+  for (IndexType k = blockIdx.x * blockDim.x + threadIdx.x;\n+       k < blockSize;\n+       k += gridDim.x * blockDim.x) {\n+    op(values1.data + idx1 * blockSize + k,\n+       values2.data + idx2 * blockSize + k,\n+       values3.data + idx3 * blockSize + k);\n+  }\n+}\n+\n+template <typename Op, typename IndexType, typename Real>\n+__global__ void THCSTensor_sparseElementwiseKernel(\n+    Op op,\n+    TensorInfo<Real, IndexType> dense,\n+    TensorInfo<integer, IndexType> indices,\n     TensorInfo<Real, IndexType> values,\n     const IndexType nnz) {\n+  IndexType indskip = indices.strides[0];\n+  IndexType valueSize = values.strides[0];\n+  for (IndexType linearId = blockIdx.x;\n+       linearId < nnz;\n+       linearId += gridDim.x) {\n+    IndexType index = 0;\n+    for (IndexType d = 0; d < indices.sizes[0]; d++) {\n+      index = dense.sizes[d] * index + indices.data[d * indskip + linearId];\n+    }\n+    Real *dst = dense.data + index * valueSize;\n+    Real *src = values.data + linearId * valueSize;\n+    for (IndexType linearId2 = threadIdx.x; linearId2 < valueSize; linearId2 += blockDim.x) {\n+      op(dst + linearId2, src + linearId2);\n+    }\n+  }\n+}\n+\n+template <typename Op, typename IndexType, typename Real>\n+__global__ void THCSTensor_sparseElementwiseKernelScalar(\n+    Op op,\n+    TensorInfo<Real, IndexType> dense,\n+    TensorInfo<integer, IndexType> indices,\n+    TensorInfo<Real, IndexType> values,\n+    const IndexType nnz) {\n+  IndexType indskip = indices.strides[0];\n   for (IndexType linearId = blockIdx.x * blockDim.x + threadIdx.x;\n-      linearId < nnz;\n-      linearId += gridDim.x * blockDim.x) {\n+       linearId < nnz;\n+       linearId += gridDim.x * blockDim.x) {\n     IndexType index = 0;\n-    IndexType indskip = indices.strides[0];\n-    for (IndexType d = 0; d < indices.sizes[0]; d++)\n-      index = other.sizes[d] * index + indices.data[d * indskip + linearId];\n-    other.data[index] = other.data[index] + values.data[linearId];\n+    for (IndexType d = 0; d < indices.sizes[0]; d++) {\n+      index = dense.sizes[d] * index + indices.data[d * indskip + linearId];\n+    }\n+    op(dense.data + index, values.data + linearId);\n+  }\n+}\n+\n+template <typename OpBoth, typename OpLeft, typename OpRight, typename IndexType, typename Real>", "path": "torch/lib/THCS/THCSTensor.cu", "position": 86, "original_position": 86, "commit_id": "f4105bcbb3dc4ebc23f20112949a0de79e24d978", "original_commit_id": "179b2dafe865e7cbdd79b18f58143e8ca5d4782d", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "body": "For all the union/intersection kernels it looks like you're running the entire computation in each thread, is that correct?\r\n\r\nObviously it would be good to parallelize these, but will take a bit of work. Modern GPU has the answers :) In the meantime, if you're only going to run with one thread, can we at least set the block/grid to run on only one thread? That may be faster and may let other things run concurrently.", "created_at": "2017-04-04T20:38:21Z", "updated_at": "2018-11-23T15:33:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109771093", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1147", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109771093"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109771093"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1147"}}, "body_html": "<p>For all the union/intersection kernels it looks like you're running the entire computation in each thread, is that correct?</p>\n<p>Obviously it would be good to parallelize these, but will take a bit of work. Modern GPU has the answers :) In the meantime, if you're only going to run with one thread, can we at least set the block/grid to run on only one thread? That may be faster and may let other things run concurrently.</p>", "body_text": "For all the union/intersection kernels it looks like you're running the entire computation in each thread, is that correct?\nObviously it would be good to parallelize these, but will take a bit of work. Modern GPU has the answers :) In the meantime, if you're only going to run with one thread, can we at least set the block/grid to run on only one thread? That may be faster and may let other things run concurrently."}