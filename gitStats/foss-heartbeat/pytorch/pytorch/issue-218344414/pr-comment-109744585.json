{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109744585", "pull_request_review_id": 30671730, "id": 109744585, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTc0NDU4NQ==", "diff_hunk": "@@ -12,70 +28,210 @@ THCTensor *THCSTensor_(toDense)(THCState *state, THCSTensor *self) {\n   // set up the new tensor\n   storage = THCSTensor_(newSizeOf)(state, self);\n   other = THCTensor_(newWithSize)(state, storage, NULL);\n+  THLongStorage_free(storage);\n   THCTensor_(zero)(state, other);\n \n-  // Some necessary dimensions and sizes\n   const ptrdiff_t nnz = THCSTensor_(nnz)(state, self);\n+  if (nnz == 0) {\n+    return other;\n+  }\n+\n+  // TODO more benchmarking\n   const dim3 block = getApplyBlock();\n   dim3 grid;\n-  THArgCheck(getApplyGrid(state, nnz, grid), 1, CUTORCH_DIM_WARNING);\n+  if (self->nDimensionV == 0) {\n+    THArgCheck(getApplyGrid(state, nnz, grid), 1, CUTORCH_DIM_WARNING);\n \n-  TensorInfo<real, unsigned long> otherInfo =\n-    getTensorInfo<THCTensor, unsigned long>(state, other);\n-  TensorInfo<long, unsigned long> indicesInfo =\n-    getTensorInfo<THCudaLongTensor, unsigned long>(state, self->indices);\n-  TensorInfo<real, unsigned long> valuesInfo =\n-    getTensorInfo<THCTensor, unsigned long>(state, self->values);\n+    THCSTensor_sparseElementwiseKernelScalar<TensorAddOp<real>, unsigned long, real>\n+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n+          TensorAddOp<real>(),\n+          V_INFO(other), I_INFO(self->indices), V_INFO(self->values),\n+          (unsigned long)(nnz));\n+  } else {\n+    THArgCheck(getApplyGrid(state, nnz * block.x, grid), 1, CUTORCH_DIM_WARNING);\n \n-  THCSTensor_toDenseKernel<unsigned long, real>\n-    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n-        otherInfo, indicesInfo, valuesInfo, (unsigned long)(nnz));\n+    THCSTensor_sparseElementwiseKernel<TensorAddOp<real>, unsigned long, real>\n+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n+          TensorAddOp<real>(),\n+          V_INFO(other), I_INFO(self->indices), V_INFO(self->values),\n+          (unsigned long)(nnz));\n+  }\n \n   THCudaCheck(cudaGetLastError());\n-  THLongStorage_free(storage);\n   return other;\n-  */\n-  THError(\"WARNING: Sparse Cuda Tensor op toDense is not implemented\");\n-  return NULL;\n }\n \n void THCSTensor_(reorder)(THCState *state, THCSTensor *self) {\n-  THError(\"WARNING: Sparse Cuda Tensor op reorder is not implemented\");\n-}\n+  if (self->nnz < 2) return;\n+#if CUDA_VERSION >= 7000\n+  THCThrustAllocator thrustAlloc(state);\n+#define THRUST_EXEC(fn, ...) fn(thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)), ##__VA_ARGS__)\n+#else\n+#define THRUST_EXEC(fn, ...) fn(##__VA_ARGS__)\n+#endif\n+\n+  // For indices, a simple sort + unique suffices\n+  // For values, we reduce the problem to a sparse x dense matrix multiplication D2 = S x D1, such that:\n+  // * D1 represents the input values, D2 represents the output values\n+  // * D1 and D2 are views over the values where:\n+  //    * the first dimension represents the nnz index (same as in the values tensor)\n+  //    * the second dimension represents the \"flattened\" values (so they can be treated as blocks of scalars even if they are N-dimensional)\n+  // * S maps values in D1 to their position in D2\n+  //   Multiple values in D1 can map to the same position in D2 if there are duplicate indices\n+  //   Values mapping to the same position are added together (which is what matrix multiplication does)\n+  //\n+  // When constructing S, we must make sure that it is contiguous (otherwise this function will call itself when doing the multiplication)\n+  // To achieve this, we define the indices tensor of S as follows:\n+  // * the second row contains the permutation corresponding to a stable sort of the original indices\n+  // * the first row \"maps\" those indices to their final location after deduplication\n+  //\n+  // The construction of the second row ensures that the first row is sorted\n+  // Because the sorting used for the second row is stable, groups of values mapped to the same position correspond to increasing subsequences of the permutation\n+  // So the indices tensor of S is guaranteed to be sorted\n+\n+  // Initialize tensors\n+  THCIndexTensor *indices = THCSTensor_(indices)(state, self);\n+  THCTensor *values = THCSTensor_(values)(state, self);\n+  THCudaLongTensor *sIndices = THCudaLongTensor_newWithSize2d(state, 2, self->nnz);\n+  THCTensor *sValues = THCTensor_(newWithSize1d)(state, self->nnz);\n+  THCTensor_(fill)(state, sValues, ScalarConvert<int, real>::to(1));\n+  THCudaLongTensor *mapping = THCudaLongTensor_new(state);\n+  THCudaLongTensor *permutation = THCudaLongTensor_new(state);\n+  THCudaLongTensor_select(state, mapping, sIndices, 0, 0);\n+  THCudaLongTensor_select(state, permutation, sIndices, 0, 1);\n+  THCudaLongTensor *uniquePositions = THCudaLongTensor_newWithSize1d(state, self->nnz);\n+\n+  // convert N-dimensional indices to scalar indices\n+  THCIndexTensor *indicesScalar = THCIndexTensor_(newWithSize1d)(state, self->nnz);\n+  THCIndexTensor *indicesSlice = THCIndexTensor_(new)(state);\n+  THCIndexTensor_(zero)(state, indicesScalar);\n+  integer factor = 1;\n+  for (int i = self->nDimensionI - 1; i >= 0; i--) {\n+    THCIndexTensor_(select)(state, indicesSlice, indices, 0, i);\n+    THCIndexTensor_(cadd)(state, indicesScalar, indicesScalar, factor, indicesSlice);\n+    factor *= self->size[i];\n+  }\n+  THCIndexTensor_(free)(state, indicesSlice);\n+\n+  // stable sort indices and remember the permutation\n+  thrust::device_ptr<long> permutationIter(THCudaLongTensor_data(state, permutation));\n+  THRUST_EXEC(thrust::sequence, permutationIter, permutationIter + self->nnz);\n+  thrust::device_ptr<integer> indicesIter(THCIndexTensor_(data)(state, indicesScalar));\n+  THRUST_EXEC(thrust::stable_sort_by_key, indicesIter, indicesIter + self->nnz, permutationIter);\n+  // Note: the code below is much faster and seems to work, but the sort is not stable.\n+  // It could be that csrmm2 works even when column indices are not sorted within rows\n+  // THCIndexTensor *indicesScalarClone = THCIndexTensor_(newClone)(state, indicesScalar);\n+  // THCIndexTensor_(sort)(state, indicesScalar, permutation, indicesScalarClone, 0, 0);", "path": "torch/lib/THCS/generic/THCSTensor.cu", "position": 147, "original_position": 140, "commit_id": "f4105bcbb3dc4ebc23f20112949a0de79e24d978", "original_commit_id": "179b2dafe865e7cbdd79b18f58143e8ca5d4782d", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "body": "It would be wonderful to get a `stable` option to `THCTensor_(sort)`; I've also wanted it at times (for reasons like this). I wonder how much work that is? @wickedfoo do you have any idea?", "created_at": "2017-04-04T18:38:24Z", "updated_at": "2018-11-23T15:33:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109744585", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1147", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109744585"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109744585"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1147"}}, "body_html": "<p>It would be wonderful to get a <code>stable</code> option to <code>THCTensor_(sort)</code>; I've also wanted it at times (for reasons like this). I wonder how much work that is? <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1911637\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wickedfoo\">@wickedfoo</a> do you have any idea?</p>", "body_text": "It would be wonderful to get a stable option to THCTensor_(sort); I've also wanted it at times (for reasons like this). I wonder how much work that is? @wickedfoo do you have any idea?"}