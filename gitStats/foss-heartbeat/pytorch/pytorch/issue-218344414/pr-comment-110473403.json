{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110473403", "pull_request_review_id": 31644119, "id": 110473403, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMDQ3MzQwMw==", "diff_hunk": "@@ -12,70 +28,210 @@ THCTensor *THCSTensor_(toDense)(THCState *state, THCSTensor *self) {\n   // set up the new tensor\n   storage = THCSTensor_(newSizeOf)(state, self);\n   other = THCTensor_(newWithSize)(state, storage, NULL);\n+  THLongStorage_free(storage);\n   THCTensor_(zero)(state, other);\n \n-  // Some necessary dimensions and sizes\n   const ptrdiff_t nnz = THCSTensor_(nnz)(state, self);\n+  if (nnz == 0) {\n+    return other;\n+  }\n+\n+  // TODO more benchmarking\n   const dim3 block = getApplyBlock();\n   dim3 grid;\n-  THArgCheck(getApplyGrid(state, nnz, grid), 1, CUTORCH_DIM_WARNING);\n+  if (self->nDimensionV == 0) {\n+    THArgCheck(getApplyGrid(state, nnz, grid), 1, CUTORCH_DIM_WARNING);\n \n-  TensorInfo<real, unsigned long> otherInfo =\n-    getTensorInfo<THCTensor, unsigned long>(state, other);\n-  TensorInfo<long, unsigned long> indicesInfo =\n-    getTensorInfo<THCudaLongTensor, unsigned long>(state, self->indices);\n-  TensorInfo<real, unsigned long> valuesInfo =\n-    getTensorInfo<THCTensor, unsigned long>(state, self->values);\n+    THCSTensor_sparseElementwiseKernelScalar<TensorAddOp<real>, unsigned long, real>\n+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n+          TensorAddOp<real>(),\n+          V_INFO(other), I_INFO(self->indices), V_INFO(self->values),\n+          (unsigned long)(nnz));\n+  } else {\n+    THArgCheck(getApplyGrid(state, nnz * block.x, grid), 1, CUTORCH_DIM_WARNING);\n \n-  THCSTensor_toDenseKernel<unsigned long, real>\n-    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n-        otherInfo, indicesInfo, valuesInfo, (unsigned long)(nnz));\n+    THCSTensor_sparseElementwiseKernel<TensorAddOp<real>, unsigned long, real>\n+      <<<grid, block, 0, THCState_getCurrentStream(state)>>>(\n+          TensorAddOp<real>(),\n+          V_INFO(other), I_INFO(self->indices), V_INFO(self->values),\n+          (unsigned long)(nnz));\n+  }\n \n   THCudaCheck(cudaGetLastError());\n-  THLongStorage_free(storage);\n   return other;\n-  */\n-  THError(\"WARNING: Sparse Cuda Tensor op toDense is not implemented\");\n-  return NULL;\n }\n \n void THCSTensor_(reorder)(THCState *state, THCSTensor *self) {\n-  THError(\"WARNING: Sparse Cuda Tensor op reorder is not implemented\");\n-}\n+  if (self->nnz < 2) return;\n+#if CUDA_VERSION >= 7000\n+  THCThrustAllocator thrustAlloc(state);\n+#define THRUST_EXEC(fn, ...) fn(thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)), ##__VA_ARGS__)\n+#else\n+#define THRUST_EXEC(fn, ...) fn(##__VA_ARGS__)\n+#endif\n+\n+  // For indices, a simple sort + unique suffices\n+  // For values, we reduce the problem to a sparse x dense matrix multiplication D2 = S x D1, such that:\n+  // * D1 represents the input values, D2 represents the output values\n+  // * D1 and D2 are views over the values where:\n+  //    * the first dimension represents the nnz index (same as in the values tensor)\n+  //    * the second dimension represents the \"flattened\" values (so they can be treated as blocks of scalars even if they are N-dimensional)\n+  // * S maps values in D1 to their position in D2\n+  //   Multiple values in D1 can map to the same position in D2 if there are duplicate indices\n+  //   Values mapping to the same position are added together (which is what matrix multiplication does)\n+  //\n+  // When constructing S, we must make sure that it is contiguous (otherwise this function will call itself when doing the multiplication)\n+  // To achieve this, we define the indices tensor of S as follows:\n+  // * the second row contains the permutation corresponding to a stable sort of the original indices\n+  // * the first row \"maps\" those indices to their final location after deduplication\n+  //\n+  // The construction of the second row ensures that the first row is sorted\n+  // Because the sorting used for the second row is stable, groups of values mapped to the same position correspond to increasing subsequences of the permutation\n+  // So the indices tensor of S is guaranteed to be sorted\n+\n+  // Initialize tensors\n+  THCIndexTensor *indices = THCSTensor_(indices)(state, self);\n+  THCTensor *values = THCSTensor_(values)(state, self);\n+  THCudaLongTensor *sIndices = THCudaLongTensor_newWithSize2d(state, 2, self->nnz);\n+  THCTensor *sValues = THCTensor_(newWithSize1d)(state, self->nnz);\n+  THCTensor_(fill)(state, sValues, ScalarConvert<int, real>::to(1));\n+  THCudaLongTensor *mapping = THCudaLongTensor_new(state);\n+  THCudaLongTensor *permutation = THCudaLongTensor_new(state);\n+  THCudaLongTensor_select(state, mapping, sIndices, 0, 0);\n+  THCudaLongTensor_select(state, permutation, sIndices, 0, 1);\n+  THCudaLongTensor *uniquePositions = THCudaLongTensor_newWithSize1d(state, self->nnz);\n+\n+  // convert N-dimensional indices to scalar indices\n+  THCIndexTensor *indicesScalar = THCIndexTensor_(newWithSize1d)(state, self->nnz);\n+  THCIndexTensor *indicesSlice = THCIndexTensor_(new)(state);\n+  THCIndexTensor_(zero)(state, indicesScalar);\n+  integer factor = 1;\n+  for (int i = self->nDimensionI - 1; i >= 0; i--) {\n+    THCIndexTensor_(select)(state, indicesSlice, indices, 0, i);\n+    THCIndexTensor_(cadd)(state, indicesScalar, indicesScalar, factor, indicesSlice);\n+    factor *= self->size[i];\n+  }\n+  THCIndexTensor_(free)(state, indicesSlice);\n+\n+  // stable sort indices and remember the permutation\n+  thrust::device_ptr<long> permutationIter(THCudaLongTensor_data(state, permutation));\n+  THRUST_EXEC(thrust::sequence, permutationIter, permutationIter + self->nnz);\n+  thrust::device_ptr<integer> indicesIter(THCIndexTensor_(data)(state, indicesScalar));\n+  THRUST_EXEC(thrust::stable_sort_by_key, indicesIter, indicesIter + self->nnz, permutationIter);\n+  // Note: the code below is much faster and seems to work, but the sort is not stable.\n+  // It could be that csrmm2 works even when column indices are not sorted within rows\n+  // THCIndexTensor *indicesScalarClone = THCIndexTensor_(newClone)(state, indicesScalar);\n+  // THCIndexTensor_(sort)(state, indicesScalar, permutation, indicesScalarClone, 0, 0);\n+  // THCIndexTensor_(free)(state, indicesScalarClone);\n+\n+  // compute a list of unique indices, along with their position in the original index tensor (using the saved permutation)\n+  thrust::device_ptr<long> uniquePositionsIter(THCudaLongTensor_data(state, uniquePositions));\n+  thrust::device_vector<integer> uniqueIndicesBuffer(self->nnz); // not used, can we optimize?\n+  thrust::pair<thrust::device_vector<integer>::iterator, thrust::device_ptr<long> > newEnd =\n+    THRUST_EXEC(thrust::unique_by_key_copy, indicesIter, indicesIter + self->nnz, permutationIter, uniqueIndicesBuffer.begin(), uniquePositionsIter);\n+  long newNnz = newEnd.second - uniquePositionsIter;\n+  THCudaLongTensor_resize1d(state, uniquePositions, newNnz);\n \n-void THCSTensor_(contiguous)(THCState *state, THCSTensor *self) {\n-  if (self->contiguous) return;\n-  THCSTensor_(reorder)(state, self);\n-  self->contiguous = 1;\n+  // compute the mapping of sorted indices to their final location after deduplication\n+  THCudaLongTensor_set1d(state, mapping, 0, 0);\n+  thrust::device_ptr<long> mappingIter(THCudaLongTensor_data(state, mapping));\n+  thrust::not_equal_to<integer> op;\n+  THRUST_EXEC(thrust::transform, indicesIter, indicesIter + self->nnz - 1, indicesIter + 1, mappingIter + 1, op);\n+  THRUST_EXEC(thrust::inclusive_scan, mappingIter, mappingIter + self->nnz, mappingIter);\n+\n+  // build S\n+  THCSTensor *S = THCSTensor_(newWithSize2d)(state, newNnz, self->nnz);\n+  THCSTensor_(_move)(state, S, sIndices, sValues);\n+  S->contiguous = 1;\n+\n+  // build output indices tensor by doing an indexSelect over the sorted list of unique indices\n+  THCIndexTensor *newIndices = THCIndexTensor_(new)(state);\n+  THCIndexTensor_(indexSelect)(state, newIndices, indices, 1, uniquePositions);\n+  THCIndexTensor_(free)(state, indices);\n+  THCIndexTensor_(free)(state, self->indices);\n+  self->indices = newIndices;\n+\n+  // create output values tensor\n+  THLongStorage *newValuesSizes = THCTensor_(newSizeOf)(state, values);\n+  THLongStorage_set(newValuesSizes, 0, newNnz);\n+  THCTensor *newValues = THCTensor_(newWithSize)(state, newValuesSizes, NULL);\n+  THLongStorage_free(newValuesSizes);\n+\n+  // create D1: view over input values tensor\n+  THCTensor *valuesView;\n+  if (THCTensor_(nDimension)(state, values) != 2) {\n+    THLongStorage *valuesViewSizes = THLongStorage_newWithSize(2);\n+    THLongStorage_set(valuesViewSizes, 0, self->nnz);\n+    THLongStorage_set(valuesViewSizes, 1, -1);\n+    valuesView = THCTensor_(newView)(state, values, valuesViewSizes);\n+    THLongStorage_free(valuesViewSizes);\n+  } else {\n+    valuesView = values;\n+  }\n+\n+  // create D2: view over output values tensor\n+  THCTensor *newValuesView;\n+  if (THCTensor_(nDimension)(state, newValues) != 2) {\n+    THLongStorage *newValuesViewSizes = THLongStorage_newWithSize(2);\n+    THLongStorage_set(newValuesViewSizes, 0, newNnz);\n+    THLongStorage_set(newValuesViewSizes, 1, -1);\n+    newValuesView = THCTensor_(newView)(state, newValues, newValuesViewSizes);\n+    THLongStorage_free(newValuesViewSizes);\n+  } else {\n+    newValuesView = newValues;\n+  }\n+\n+  // build output values tensor by computing D2 = S x D1\n+  THCSTensor_(spaddmm)(state, newValuesView, ScalarConvert<int, real>::to(0), newValuesView, ScalarConvert<int, real>::to(1), S, valuesView);\n+  THCTensor_(free)(state, values);\n+  THCTensor_(free)(state, self->values);\n+  self->values = newValues;\n+\n+  self->nnz = newNnz;\n+\n+  THCudaLongTensor_free(state, permutation);\n+  THCudaLongTensor_free(state, mapping);\n+  THCudaLongTensor_free(state, uniquePositions);\n+  THCIndexTensor_(free)(state, indicesScalar);\n+  THCSTensor_(free)(state, S);\n+  if (valuesView != values) {\n+    THCTensor_(free)(state, valuesView);\n+  }\n+  if (newValuesView != newValues) {\n+    THCTensor_(free)(state, newValuesView);\n+  }\n+\n+#undef THRUST_EXEC\n }\n \n // In place transpose\n void THCSTensor_(transpose)(THCState *state, THCSTensor *self, int d1, int d2) {\n-  /* TODO\n-  THCudaLongTensor *indices = THCSTensor_(indices)(state, self);\n-  long i;\n-  for (i = 0; i < THCSTensor_(nnz)(state, self); i++) {\n-    long tmp = THCTensor_fastGet2d(indices, d1, i);\n-    THCTensor_fastSet2d(indices, d1, i,\n-        THCTensor_fastGet2d(indices, d2, i));\n-    THCTensor_fastSet2d(indices, d2, i, tmp);\n-  }\n-  i = self->size[d1];\n+  long nDimI = THCSTensor_(nDimensionI)(state, self);\n+  long nDimV = THCSTensor_(nDimensionV)(state, self);\n+  THArgCheck(d1 < nDimI && d2 < nDimI, 1, \"Transposed dimensions should be sparse. Got nDimI: %ld, d1: %ld, d2: %ld\", nDimI, d1, d2);\n+  THCIndexTensor *indices = THCSTensor_(indices)(state, self);\n+  long nnz = THCSTensor_(nnz)(state, self);\n+  THCIndexTensor *buffer = THCIndexTensor_(newWithSize1d)(state, nnz);\n+  THCIndexTensor *slice1 = THCIndexTensor_(new)(state);\n+  THCIndexTensor *slice2 = THCIndexTensor_(new)(state);\n+  THCIndexTensor_(select)(state, slice1, indices, 0, d1);", "path": "torch/lib/THCS/generic/THCSTensor.cu", "position": null, "original_position": 247, "commit_id": "f4105bcbb3dc4ebc23f20112949a0de79e24d978", "original_commit_id": "179b2dafe865e7cbdd79b18f58143e8ca5d4782d", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "body": "\ud83d\udc4d ", "created_at": "2017-04-07T20:12:19Z", "updated_at": "2018-11-23T15:33:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/1147#discussion_r110473403", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1147", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110473403"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1147#discussion_r110473403"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1147"}}, "body_html": "<p><g-emoji class=\"g-emoji\" alias=\"+1\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f44d.png\">\ud83d\udc4d</g-emoji></p>", "body_text": "\ud83d\udc4d", "in_reply_to_id": 109746507}