{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109167927", "pull_request_review_id": 30253008, "id": 109167927, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwOTE2NzkyNw==", "diff_hunk": "@@ -5,227 +5,249 @@\n import random\n import unittest\n from common import TestCase, run_tests\n+from common_nn import TEST_CUDA\n from numbers import Number\n \n-SparseTensor = sparse.DoubleTensor\n+# triplet := (index type, value type, sparse type)\n+cpu_triplet = (\n+    torch.LongTensor,\n+    torch.DoubleTensor,\n+    torch.sparse.DoubleTensor)\n+type_triplets = [cpu_triplet]\n+if TEST_CUDA:\n+    cuda_triplet = (\n+        torch.cuda.LongTensor,\n+        torch.cuda.DoubleTensor,\n+        torch.cuda.sparse.DoubleTensor)\n+    type_triplets.append(cuda_triplet)\n \n \n class TestSparse(TestCase):\n \n     @staticmethod\n-    def _gen_sparse(d, nnz, with_size):\n+    def _gen_sparse(d, nnz, with_size, is_cuda=False):  # FIXME remove default is_cuda value to ensure coverage\n         if isinstance(with_size, Number):\n             v = torch.randn(nnz)\n             i = (torch.rand(d, nnz) * with_size).type(torch.LongTensor)\n-            x = SparseTensor(i, v)\n+            x = torch.sparse.DoubleTensor(i, v)\n         else:\n             v_size = [nnz] + list(with_size[d:])\n             v = torch.randn(*v_size)\n             i = torch.rand(d, nnz) * \\\n                 torch.Tensor(with_size[:d]).repeat(nnz, 1).transpose(0, 1)\n             i = i.type(torch.LongTensor)\n-            x = SparseTensor(i, v, torch.Size(with_size))\n+            x = torch.sparse.DoubleTensor(i, v, torch.Size(with_size))\n \n-        return x, i, v\n+        if is_cuda:\n+            return x.cuda(), i.cuda(), v.cuda()\n+        else:\n+            return x, i, v\n \n     def test_basic(self):\n-        x, i, v = self._gen_sparse(3, 10, 100)\n-\n-        self.assertEqual(i, x.indices())\n-        self.assertEqual(v, x.values())\n-\n-        x, i, v = self._gen_sparse(3, 10, [100, 100, 100])\n-        self.assertEqual(i, x.indices())\n-        self.assertEqual(v, x.values())\n-        self.assertEqual(x.ndimension(), 3)\n-        self.assertEqual(x.nnz(), 10)\n-        for i in range(3):\n-            self.assertEqual(x.size(i), 100)\n-\n-        # Make sure we can access empty indices / values\n-        x = SparseTensor()\n-        self.assertEqual(x.indices().numel(), 0)\n-        self.assertEqual(x.values().numel(), 0)\n+        for is_cuda in [False, True] if TEST_CUDA else [False]:", "path": "test/test_sparse.py", "position": null, "original_position": 65, "commit_id": "f4105bcbb3dc4ebc23f20112949a0de79e24d978", "original_commit_id": "8e4660ac5145c4a48e35620f8810fa8863cb7b50", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "can you separate CUDA and non-CUDA tests like we do in all other tests? Put code for this method in `_test_basic` and make it accept `is_cuda` argument. Then add `test_basic` and `test_basic_cuda`, and decorate the latter one with `@unittest.skipIf`. It's better because you can actually see that some tests were skipped, so they might be incomplete.\r\n\r\nSame applies to all `type_triplets` tests. Make the helper accept `type_triplet` arg and give it `cpu_type_triplet` and `gpu_type_triplet` in the corresponding test methods.", "created_at": "2017-03-31T14:09:06Z", "updated_at": "2018-11-23T15:32:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109167927", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1147", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/109167927"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1147#discussion_r109167927"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1147"}}, "body_html": "<p>can you separate CUDA and non-CUDA tests like we do in all other tests? Put code for this method in <code>_test_basic</code> and make it accept <code>is_cuda</code> argument. Then add <code>test_basic</code> and <code>test_basic_cuda</code>, and decorate the latter one with <code>@unittest.skipIf</code>. It's better because you can actually see that some tests were skipped, so they might be incomplete.</p>\n<p>Same applies to all <code>type_triplets</code> tests. Make the helper accept <code>type_triplet</code> arg and give it <code>cpu_type_triplet</code> and <code>gpu_type_triplet</code> in the corresponding test methods.</p>", "body_text": "can you separate CUDA and non-CUDA tests like we do in all other tests? Put code for this method in _test_basic and make it accept is_cuda argument. Then add test_basic and test_basic_cuda, and decorate the latter one with @unittest.skipIf. It's better because you can actually see that some tests were skipped, so they might be incomplete.\nSame applies to all type_triplets tests. Make the helper accept type_triplet arg and give it cpu_type_triplet and gpu_type_triplet in the corresponding test methods."}