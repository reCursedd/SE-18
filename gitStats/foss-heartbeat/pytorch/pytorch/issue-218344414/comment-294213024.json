{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/294213024", "html_url": "https://github.com/pytorch/pytorch/pull/1147#issuecomment-294213024", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1147", "id": 294213024, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDIxMzAyNA==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-14T18:50:01Z", "updated_at": "2017-04-14T18:50:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> re: contiguous, lets think about exactly what we mean by \"weight sharing\". I think you mean something like</p>\n<pre><code>A = torch.sparse.Tensor(...)\nB = t.t() # A and B are shared views; you don't want to silently break this sharing\nC = A[:10] # also a shared view\n</code></pre>\n<p>Except that this already doesn't work for sparse tensors! There's no way (at least currently) to have a view on a sparse tensor. So I don't think this matters. I'd like to think of sparse tensor reordering  (which we <em>really</em> should give a new name than <code>contiguous</code>) as an internal optimization detail; so logically, <code>reorder(A) === A</code> (cf. C++ <code>mutable</code> keyword). This is not the case for dense tensors where <code>contiguous</code> is externally visible. Let me know if you can think of a counterexample.<br>\nRe: possibility of user error with contiguous and non-contiguous tensors, yes you're right. It's exactly the reason why non-contiguous tensors lead to so many errors in TH proper. No getting around that - if you're writing THCS functions, you need to be aware of contiguous vs non-contiguous.</p>\n<hr>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2560662\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinraison\">@martinraison</a> yes I agree we should have a deterministic variant. But notice that this extends to indexAdd as well. Currently LookupTable is deterministic, indexAdd is non-deterministic. So I think we should have a deterministic and non-deterministic variant of indexAdd, and LookupTable / spcAdd should just delegate to that.</p>\n<p>I benchmarked autograd overhead like this, with small batch size like 10. It takes ~700us / batch, and CPU sits at 100%. <code>perf top</code> shows it's spending most of it's time in python functions as well as libTHC routines that look like they're in the memory allocator. Let me know if you have any suggestions for better profiling.</p>\n<pre><code>...\nfor i in range(N):\n    emb.zero_grad()\n    out = emb(vbatch)\n    out.backward(out.data)\n    optimizer.step()\n</code></pre>", "body_text": "@apaszke re: contiguous, lets think about exactly what we mean by \"weight sharing\". I think you mean something like\nA = torch.sparse.Tensor(...)\nB = t.t() # A and B are shared views; you don't want to silently break this sharing\nC = A[:10] # also a shared view\n\nExcept that this already doesn't work for sparse tensors! There's no way (at least currently) to have a view on a sparse tensor. So I don't think this matters. I'd like to think of sparse tensor reordering  (which we really should give a new name than contiguous) as an internal optimization detail; so logically, reorder(A) === A (cf. C++ mutable keyword). This is not the case for dense tensors where contiguous is externally visible. Let me know if you can think of a counterexample.\nRe: possibility of user error with contiguous and non-contiguous tensors, yes you're right. It's exactly the reason why non-contiguous tensors lead to so many errors in TH proper. No getting around that - if you're writing THCS functions, you need to be aware of contiguous vs non-contiguous.\n\n@martinraison yes I agree we should have a deterministic variant. But notice that this extends to indexAdd as well. Currently LookupTable is deterministic, indexAdd is non-deterministic. So I think we should have a deterministic and non-deterministic variant of indexAdd, and LookupTable / spcAdd should just delegate to that.\nI benchmarked autograd overhead like this, with small batch size like 10. It takes ~700us / batch, and CPU sits at 100%. perf top shows it's spending most of it's time in python functions as well as libTHC routines that look like they're in the memory allocator. Let me know if you have any suggestions for better profiling.\n...\nfor i in range(N):\n    emb.zero_grad()\n    out = emb(vbatch)\n    out.backward(out.data)\n    optimizer.step()", "body": "@apaszke re: contiguous, lets think about exactly what we mean by \"weight sharing\". I think you mean something like\r\n```\r\nA = torch.sparse.Tensor(...)\r\nB = t.t() # A and B are shared views; you don't want to silently break this sharing\r\nC = A[:10] # also a shared view\r\n```\r\nExcept that this already doesn't work for sparse tensors! There's no way (at least currently) to have a view on a sparse tensor. So I don't think this matters. I'd like to think of sparse tensor reordering  (which we *really* should give a new name than `contiguous`) as an internal optimization detail; so logically, `reorder(A) === A` (cf. C++ `mutable` keyword). This is not the case for dense tensors where `contiguous` is externally visible. Let me know if you can think of a counterexample.\r\nRe: possibility of user error with contiguous and non-contiguous tensors, yes you're right. It's exactly the reason why non-contiguous tensors lead to so many errors in TH proper. No getting around that - if you're writing THCS functions, you need to be aware of contiguous vs non-contiguous.\r\n\r\n----------------------------------\r\n\r\n@martinraison yes I agree we should have a deterministic variant. But notice that this extends to indexAdd as well. Currently LookupTable is deterministic, indexAdd is non-deterministic. So I think we should have a deterministic and non-deterministic variant of indexAdd, and LookupTable / spcAdd should just delegate to that.\r\n\r\nI benchmarked autograd overhead like this, with small batch size like 10. It takes ~700us / batch, and CPU sits at 100%. `perf top` shows it's spending most of it's time in python functions as well as libTHC routines that look like they're in the memory allocator. Let me know if you have any suggestions for better profiling.\r\n\r\n```\r\n...\r\nfor i in range(N):\r\n    emb.zero_grad()\r\n    out = emb(vbatch)\r\n    out.backward(out.data)\r\n    optimizer.step()\r\n```"}