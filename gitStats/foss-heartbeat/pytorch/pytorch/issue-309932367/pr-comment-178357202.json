{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178357202", "pull_request_review_id": 108410966, "id": 178357202, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3ODM1NzIwMg==", "diff_hunk": "@@ -52,63 +52,60 @@ bool isBroadcasting(Node* node) {\n   return broadcasting.count(node->kind());\n }\n \n-// First iterate over the 'from' tensor sizes. Ignore all leading and trailing\n-// dimensions that are simply one, since they can be trivially broadcasted.\n-// When iterating over the dimension sizes (with reduced 'from' tensor),\n-// starting at the trailing dimension, the dimension sizes must either be equal,\n-// or one of them does not exist. If a broadcast candidate is not found at the\n-// trailing dimension, search at the leading dimension. If one is found here,\n-// return the `axis` argument to be emitted to ONNX on the broadcasting operator\n+// This boils down to\n+//\n+//     Treating leading and trailing `1`s in `from` as wildcards, is\n+//     `from` a substring of `to`, and if so starting at which index?\n+//\n+// To answer it we do a naive, intentionally n-squared substring\n+// search, tweaked to have the correct wildcard behavior.\n //\n // Note that this is NOT equivalent to numpy broadcasting semantics, and do\n // not represent the generalized broadcasting that Pytorch implements.\n // Rather, this is Caffe2-style broadcasting.\n //\n // Return value is 1) Whether this expand is fusable, 2) the `axis` argument we\n-// should emit to ONNX. Coming from a Pytorch frontend, this should either not\n-// be emitted (if we're broadcasting trailing dimensions) or it should be\n-// emitted as `0` (leading dimensions.)\n+// should emit to ONNX.\n std::tuple<bool, at::optional<size_t>> fusibleExpandTo(at::IntList from, at::IntList to) {\n-  if (from.size() > to.size()) {\n-    return std::make_tuple(false, at::nullopt);\n-  }\n-  ssize_t from_dim_start = 0, from_dim_end = from.size() - 1;\n-  while (from_dim_start < (ssize_t) from.size() && from[from_dim_start] == 1) {\n-    from_dim_start++;\n-  }\n-  while (from_dim_end > from_dim_start && from[from_dim_end] == 1) {\n-    from_dim_end--;\n+  int leading_ones = 0;\n+  int trailing_ones = 0;\n+  for (int i = 0; i < from.size(); i++) {\n+    if (from[i] == 1) {\n+      leading_ones++;\n+    } else {\n+      break;\n+    }\n   }\n-\n-  ssize_t f = from_dim_end;\n-  ssize_t t = to.size() - 1;\n-  bool trailing_expand = true;\n-  for (; f >= from_dim_start && t >= 0; --f, --t) {\n-    if (from[f] != to[t]) {\n-      trailing_expand = false;\n+  // In case `from` is all `1`s, arbitrarily consider them to be\n+  // leading and not trailing.\n+  for (int i = from.size() - 1; i >= leading_ones; i--) {\n+    if (from[i] == 1) {\n+      trailing_ones++;\n+    } else {\n       break;\n     }\n   }\n \n-  // In the case that the 'to' tensor has leading ones in the same place that\n-  // the 'from' tensor does, f will be less than from_dim_start rather than\n-  // strictly equal. E.x.: to := [5, 1, 768] and from := [1, 1, 768]\n-  if (trailing_expand && f <= from_dim_start) {\n-    return std::make_tuple(true, at::nullopt);\n-  }\n+  std::vector<std::tuple<bool, at::optional<size_t>>> possible_results;\n \n-  f = from_dim_start;\n-  t = 0;\n-  bool leading_expand = true;\n-  for (; f <= from_dim_end && t < static_cast<ssize_t>(to.size()); ++f, ++t) {\n-    if (from[f] != to[t]) {\n-      leading_expand = false;\n-      break;\n+  for (int i = 0; i < to.size() - from.size(); i++) {\n+    bool is_substring = true;\n+    for (int j = leading_ones; j < to.size() - trailing_ones; j++) {\n+      if (to[i + j] != from[i]) {\n+        is_substring = false;\n+        break;\n+      }\n+    }\n+    if (is_substring) {\n+      possible_results.push_back(std::make_tuple(true, i));\n     }\n   }\n \n-  if (leading_expand && f >= from_dim_end) {\n-    return std::make_tuple(true, 0);\n+  if (possible_results.size() != 1) {\n+    std::cerr << \"Warning: found multiple ways to broadcast. choosing one arbitrarily\" << std::endl;", "path": "torch/csrc/jit/passes/onnx/peephole.cpp", "position": 95, "original_position": 95, "commit_id": "1960b5fd36a8e2093ac60789aac85ab6b8f7b56e", "original_commit_id": "2046b50db4b24add79ecf655b1a83cbb62e4cd1d", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "We don't have infra for this at the moment, but ideally a warning message like this would go to a callback, that would put the warning through Python's `warnings` mechanism, so that end users can suppress it. If you blat it out to stderr, there's no way to suppress this warning.", "created_at": "2018-03-30T19:20:43Z", "updated_at": "2018-11-23T15:41:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/6120#discussion_r178357202", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6120", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178357202"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6120#discussion_r178357202"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6120"}}, "body_html": "<p>We don't have infra for this at the moment, but ideally a warning message like this would go to a callback, that would put the warning through Python's <code>warnings</code> mechanism, so that end users can suppress it. If you blat it out to stderr, there's no way to suppress this warning.</p>", "body_text": "We don't have infra for this at the moment, but ideally a warning message like this would go to a callback, that would put the warning through Python's warnings mechanism, so that end users can suppress it. If you blat it out to stderr, there's no way to suppress this warning."}