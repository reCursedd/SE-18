{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9515", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9515/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9515/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9515/events", "html_url": "https://github.com/pytorch/pytorch/issues/9515", "id": 342120125, "node_id": "MDU6SXNzdWUzNDIxMjAxMjU=", "number": 9515, "title": "Proposal: type promotion logic (torch.result_type)", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tugrulates", "id": 5593188, "node_id": "MDQ6VXNlcjU1OTMxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5593188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tugrulates", "html_url": "https://github.com/tugrulates", "followers_url": "https://api.github.com/users/tugrulates/followers", "following_url": "https://api.github.com/users/tugrulates/following{/other_user}", "gists_url": "https://api.github.com/users/tugrulates/gists{/gist_id}", "starred_url": "https://api.github.com/users/tugrulates/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tugrulates/subscriptions", "organizations_url": "https://api.github.com/users/tugrulates/orgs", "repos_url": "https://api.github.com/users/tugrulates/repos", "events_url": "https://api.github.com/users/tugrulates/events{/privacy}", "received_events_url": "https://api.github.com/users/tugrulates/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tugrulates", "id": 5593188, "node_id": "MDQ6VXNlcjU1OTMxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5593188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tugrulates", "html_url": "https://github.com/tugrulates", "followers_url": "https://api.github.com/users/tugrulates/followers", "following_url": "https://api.github.com/users/tugrulates/following{/other_user}", "gists_url": "https://api.github.com/users/tugrulates/gists{/gist_id}", "starred_url": "https://api.github.com/users/tugrulates/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tugrulates/subscriptions", "organizations_url": "https://api.github.com/users/tugrulates/orgs", "repos_url": "https://api.github.com/users/tugrulates/repos", "events_url": "https://api.github.com/users/tugrulates/events{/privacy}", "received_events_url": "https://api.github.com/users/tugrulates/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-07-17T23:07:05Z", "updated_at": "2018-11-21T23:03:20Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>This proposes an algorithm for computing the result type of a mixed-type operation like <code>torch.add</code>. The function will be exposed to Python via <code>torch.result_type(*tensors)</code>. This proposal covers the default result type calculation; some operators may override the default behavior.</p>\n<p>The similar NumPy function is <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.result_type.html\" rel=\"nofollow\"><code>numpy.result_type</code></a>.</p>\n<p><strong>Wrapped numbers</strong></p>\n<p>A Tensor is a considered a \"wrapped number\" if it is auto-wrapped from a C++ or Python number type. Integer types are wrapped as 0-dim int64 tensors and floating-point types are wrapped as 0-dim double tensors. All wrapped numbers are 0-dim tensors, but not all 0-dim tensors are wrapped numbers. In general, wrapped numbers behave like normal 0-dim tensors, except they are handled specially in the <code>torch.result_type</code> calculation.</p>\n<p>For example, in <code>tensor + 5</code> and <code>torch.add(tensor, 5)</code>, <code>5</code> gets wrapped as a 0-dim <code>torch.int64</code> (a \"wrapped number\").  However, <code>torch.add(tensor, torch.tensor(5))</code> does not have a wrapped number because <code>torch.tensor(5)</code> is an explicit construction. Wrapped number status does not propagate to returned tensors. Returned tensors are never considered wrapped numbers.</p>\n<p><strong>Result type calculation</strong></p>\n<p>Each operand has a category (integer or floating-point) and a priority:</p>\n<ol>\n<li>Tensors of dimension 1 or larger</li>\n<li>Tensors of dimension 0 that are not wrapped numbers</li>\n<li>Wrapped numbers</li>\n</ol>\n<p>By default, only the highest priority operands participate in the type promotion logic. Lower priority operands participate if their category (e.g. floating-point) is of higher rank than any higher priority operands (e.g. integers).</p>\n<p>In pseudo-code the result-type calculation is:</p>\n<pre><code>def result_type(*args):\n  return promote_types(infer_scalar_type(arg) for arg in args if participates(arg, args))\n\ndef infer_scalar_type(arg):\n  if is_wrapped_number(arg):\n    return torch.get_default_dtype() if is_floating_point(arg) else torch.int64\n  else:\n    return arg.dtype\n\ndef participates(arg, args):\n  if priority(arg) &gt;= max(priority(other) for other in args):\n    return True\n  if category(arg) &gt; max(category(other) for other in args if priority(other) &gt; priority(arg)):\n   return True\n  return False\n\ndef priority(arg):\n  if arg.dim() &gt; 0: return 3\n  elif not is_wrapped_number(arg): return 2\n  else: return 1\n\ndef category(arg):\n  if is_floating_point(arg): return 2\n  else: return 1\n</code></pre>\n<p>Examples (assuming default float32 tensor dtype):</p>\n<pre><code>randn(3, dtype=float32) * 5 -&gt; float32\ntensor([0, 0, 1], dtype=uint8) + 1 -&gt; uint8\ntensor([0, 0, 1], dtype=uint8) + 1000 -&gt; uint8  # NOTE: integer overflow\ntensor([0, 0, 1], dtype=uint8) + 5.5 -&gt; float32 (default tensor dtype)\ntensor([0, 0, 1], dtype=uint8) + tensor(5.5, dtype=double) -&gt; double\n\nrandn(3, dtype=float32) + tensor(5.5, dtype=double) -&gt; float32\ntensor(5.5, dtype=float16) + 2.2 -&gt; float16\ntensor(5.5, dtype=float16) + 100000 -&gt; float16 # NOTE: inf\ntensor(5.5, dtype=float16) + tensor(100000.0) -&gt; float32 (default tensor dtype)\n</code></pre>\n<p>Appendix:</p>\n<p><strong>Why don't we use NumPy's behavior?</strong></p>\n<p>NumPy's result_type logic has two undesirable behaviors. The first is that requires examining the actual value of scalars (and 0-dim arrays). This would require a host-device synchronization for 0-dim CUDA tensors. The second is that it often up-promotes 0-dim arrays to float64 or int64. For example:</p>\n<pre><code>type(np.array(4.0, dtype=np.float32) + 1) -&gt; np.float64\ntype(np.array(0, dtype=np.uint8) + 1) -&gt; np.int64\n</code></pre>", "body_text": "This proposes an algorithm for computing the result type of a mixed-type operation like torch.add. The function will be exposed to Python via torch.result_type(*tensors). This proposal covers the default result type calculation; some operators may override the default behavior.\nThe similar NumPy function is numpy.result_type.\nWrapped numbers\nA Tensor is a considered a \"wrapped number\" if it is auto-wrapped from a C++ or Python number type. Integer types are wrapped as 0-dim int64 tensors and floating-point types are wrapped as 0-dim double tensors. All wrapped numbers are 0-dim tensors, but not all 0-dim tensors are wrapped numbers. In general, wrapped numbers behave like normal 0-dim tensors, except they are handled specially in the torch.result_type calculation.\nFor example, in tensor + 5 and torch.add(tensor, 5), 5 gets wrapped as a 0-dim torch.int64 (a \"wrapped number\").  However, torch.add(tensor, torch.tensor(5)) does not have a wrapped number because torch.tensor(5) is an explicit construction. Wrapped number status does not propagate to returned tensors. Returned tensors are never considered wrapped numbers.\nResult type calculation\nEach operand has a category (integer or floating-point) and a priority:\n\nTensors of dimension 1 or larger\nTensors of dimension 0 that are not wrapped numbers\nWrapped numbers\n\nBy default, only the highest priority operands participate in the type promotion logic. Lower priority operands participate if their category (e.g. floating-point) is of higher rank than any higher priority operands (e.g. integers).\nIn pseudo-code the result-type calculation is:\ndef result_type(*args):\n  return promote_types(infer_scalar_type(arg) for arg in args if participates(arg, args))\n\ndef infer_scalar_type(arg):\n  if is_wrapped_number(arg):\n    return torch.get_default_dtype() if is_floating_point(arg) else torch.int64\n  else:\n    return arg.dtype\n\ndef participates(arg, args):\n  if priority(arg) >= max(priority(other) for other in args):\n    return True\n  if category(arg) > max(category(other) for other in args if priority(other) > priority(arg)):\n   return True\n  return False\n\ndef priority(arg):\n  if arg.dim() > 0: return 3\n  elif not is_wrapped_number(arg): return 2\n  else: return 1\n\ndef category(arg):\n  if is_floating_point(arg): return 2\n  else: return 1\n\nExamples (assuming default float32 tensor dtype):\nrandn(3, dtype=float32) * 5 -> float32\ntensor([0, 0, 1], dtype=uint8) + 1 -> uint8\ntensor([0, 0, 1], dtype=uint8) + 1000 -> uint8  # NOTE: integer overflow\ntensor([0, 0, 1], dtype=uint8) + 5.5 -> float32 (default tensor dtype)\ntensor([0, 0, 1], dtype=uint8) + tensor(5.5, dtype=double) -> double\n\nrandn(3, dtype=float32) + tensor(5.5, dtype=double) -> float32\ntensor(5.5, dtype=float16) + 2.2 -> float16\ntensor(5.5, dtype=float16) + 100000 -> float16 # NOTE: inf\ntensor(5.5, dtype=float16) + tensor(100000.0) -> float32 (default tensor dtype)\n\nAppendix:\nWhy don't we use NumPy's behavior?\nNumPy's result_type logic has two undesirable behaviors. The first is that requires examining the actual value of scalars (and 0-dim arrays). This would require a host-device synchronization for 0-dim CUDA tensors. The second is that it often up-promotes 0-dim arrays to float64 or int64. For example:\ntype(np.array(4.0, dtype=np.float32) + 1) -> np.float64\ntype(np.array(0, dtype=np.uint8) + 1) -> np.int64", "body": "This proposes an algorithm for computing the result type of a mixed-type operation like `torch.add`. The function will be exposed to Python via `torch.result_type(*tensors)`. This proposal covers the default result type calculation; some operators may override the default behavior.\r\n\r\nThe similar NumPy function is [`numpy.result_type`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.result_type.html).\r\n\r\n**Wrapped numbers**\r\n\r\nA Tensor is a considered a \"wrapped number\" if it is auto-wrapped from a C++ or Python number type. Integer types are wrapped as 0-dim int64 tensors and floating-point types are wrapped as 0-dim double tensors. All wrapped numbers are 0-dim tensors, but not all 0-dim tensors are wrapped numbers. In general, wrapped numbers behave like normal 0-dim tensors, except they are handled specially in the `torch.result_type` calculation.\r\n\r\nFor example, in `tensor + 5` and `torch.add(tensor, 5)`, `5` gets wrapped as a 0-dim `torch.int64` (a \"wrapped number\").  However, `torch.add(tensor, torch.tensor(5))` does not have a wrapped number because `torch.tensor(5)` is an explicit construction. Wrapped number status does not propagate to returned tensors. Returned tensors are never considered wrapped numbers.\r\n\r\n**Result type calculation**\r\n\r\nEach operand has a category (integer or floating-point) and a priority:\r\n\r\n1) Tensors of dimension 1 or larger\r\n2) Tensors of dimension 0 that are not wrapped numbers\r\n3) Wrapped numbers\r\n\r\nBy default, only the highest priority operands participate in the type promotion logic. Lower priority operands participate if their category (e.g. floating-point) is of higher rank than any higher priority operands (e.g. integers).\r\n\r\nIn pseudo-code the result-type calculation is:\r\n\r\n```\r\ndef result_type(*args):\r\n  return promote_types(infer_scalar_type(arg) for arg in args if participates(arg, args))\r\n\r\ndef infer_scalar_type(arg):\r\n  if is_wrapped_number(arg):\r\n    return torch.get_default_dtype() if is_floating_point(arg) else torch.int64\r\n  else:\r\n    return arg.dtype\r\n\r\ndef participates(arg, args):\r\n  if priority(arg) >= max(priority(other) for other in args):\r\n    return True\r\n  if category(arg) > max(category(other) for other in args if priority(other) > priority(arg)):\r\n   return True\r\n  return False\r\n\r\ndef priority(arg):\r\n  if arg.dim() > 0: return 3\r\n  elif not is_wrapped_number(arg): return 2\r\n  else: return 1\r\n\r\ndef category(arg):\r\n  if is_floating_point(arg): return 2\r\n  else: return 1\r\n```\r\n\r\nExamples (assuming default float32 tensor dtype):\r\n\r\n```\r\nrandn(3, dtype=float32) * 5 -> float32\r\ntensor([0, 0, 1], dtype=uint8) + 1 -> uint8\r\ntensor([0, 0, 1], dtype=uint8) + 1000 -> uint8  # NOTE: integer overflow\r\ntensor([0, 0, 1], dtype=uint8) + 5.5 -> float32 (default tensor dtype)\r\ntensor([0, 0, 1], dtype=uint8) + tensor(5.5, dtype=double) -> double\r\n\r\nrandn(3, dtype=float32) + tensor(5.5, dtype=double) -> float32\r\ntensor(5.5, dtype=float16) + 2.2 -> float16\r\ntensor(5.5, dtype=float16) + 100000 -> float16 # NOTE: inf\r\ntensor(5.5, dtype=float16) + tensor(100000.0) -> float32 (default tensor dtype)\r\n```\r\n\r\nAppendix:\r\n\r\n**Why don't we use NumPy's behavior?**\r\n\r\nNumPy's result_type logic has two undesirable behaviors. The first is that requires examining the actual value of scalars (and 0-dim arrays). This would require a host-device synchronization for 0-dim CUDA tensors. The second is that it often up-promotes 0-dim arrays to float64 or int64. For example:\r\n\r\n```\r\ntype(np.array(4.0, dtype=np.float32) + 1) -> np.float64\r\ntype(np.array(0, dtype=np.uint8) + 1) -> np.int64\r\n```"}