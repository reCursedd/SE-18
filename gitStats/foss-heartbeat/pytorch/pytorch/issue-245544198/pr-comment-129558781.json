{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/129558781", "pull_request_review_id": 52340949, "id": 129558781, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyOTU1ODc4MQ==", "diff_hunk": "@@ -0,0 +1,97 @@\n+from torch.autograd.variable import Variable\n+from functools import reduce\n+from operator import mul\n+\n+\n+def sum_exclude_dim1(to_sum, keepdim=True):\n+    to_sum = to_sum.sum(dim=0, keepdim=True)\n+    dim = 2\n+    for dim in range(2, to_sum.dim()):\n+        to_sum = to_sum.sum(dim=dim, keepdim=True)\n+    return to_sum\n+\n+\n+# because gamma/ggG/ggB are 1-dimensional and represent dim==1, we can't\n+# do a straight expansion because it won't follow the broadcasting rules.\n+def expand_as_dim1(src, target):\n+    src_expanded = src\n+    while len(src_expanded.size()) < len(target.size()) - 1:\n+        src_expanded = src_expanded.unsqueeze(1)\n+    return src_expanded.expand_as(target)\n+\n+\n+def batchnorm_double_backwards_fn(input, gamma, ggI, ggG, ggB, gO, eps):\n+    affine = gamma is not None\n+    if affine:\n+        gamma_expanded = expand_as_dim1(gamma, input)\n+\n+        if ggG is not None:\n+            ggG_expanded = expand_as_dim1(ggG, input)\n+\n+        if ggB is not None:\n+            ggB_expanded = expand_as_dim1(ggB, input)\n+    else:\n+        gamma_expanded = 1\n+\n+    # define some terms we will reuse\n+    M = reduce(mul, input.size()[0:1] + input.size()[2:])\n+    mu = sum_exclude_dim1(input).div_(M)\n+    input_sub_mu = input - mu\n+    sigma2_eps = sum_exclude_dim1(input_sub_mu.pow(2)).div_(M).add_(eps)\n+    sigma2_eps_neg_1_2 = (sigma2_eps).pow(-1. / 2)\n+    sigma2_eps_neg_3_2 = (sigma2_eps).pow(-3. / 2)\n+\n+    # calculate gI\n+    input_mu_sigma2_neg_3_2 = (input_sub_mu * sigma2_eps_neg_3_2)\n+    gOinmu_sum = sum_exclude_dim1(gO * input_sub_mu)\n+    gO_sum = sum_exclude_dim1(gO)\n+\n+    # start with contribution of input term\n+    gI = None\n+    if ggI is not None:\n+        ggI_sum = sum_exclude_dim1(ggI)\n+        ggIinmu_sum = sum_exclude_dim1(ggI * input_sub_mu)\n+        all_sub = ((ggI_sum * gO_sum).div_(M)).sub_(sum_exclude_dim1(gO * ggI)).add_(\n+                   ((sigma2_eps).pow(-1) * gOinmu_sum * ggIinmu_sum).mul_(3. / M))\n+        gI_0t = (input_mu_sigma2_neg_3_2 * all_sub).div_(M)\n+        gI_1t = (ggIinmu_sum * sigma2_eps_neg_3_2).div_(M) * (gO_sum.div(M) - gO)\n+        gI_2t = (gOinmu_sum * sigma2_eps_neg_3_2).div_(M) * (ggI_sum.div(M) - ggI)\n+        gI = gamma_expanded * (gI_0t.add_(gI_1t).add_(gI_2t))\n+\n+    # add contribution of gamma term to gI\n+    if affine and ggG is not None:\n+        t0 = gO * sigma2_eps_neg_1_2\n+        t1 = (sigma2_eps_neg_1_2 * gO_sum).div_(-M)\n+        t2 = (input_mu_sigma2_neg_3_2 * sum_exclude_dim1(gO * input_sub_mu)).div_(-M)\n+        gI_G_term = ggG_expanded * (t0.add_(t1).add_(t2))\n+        gI = gI.add_(gI_G_term) if gI is not None else gI_G_term\n+\n+    # this is the first backward's grad_input\n+    def first_back_grad_input(gO, gamma):\n+        h0 = (gamma / (sigma2_eps).sqrt()).div_(M)\n+        h1 = (M * gO).sub_(sum_exclude_dim1(gO)).sub_(input_sub_mu.div(sigma2_eps) * sum_exclude_dim1(gO * input_sub_mu))\n+        return h0 * h1\n+\n+    # calculate gG\n+    gG = None\n+    if affine and ggI is not None:\n+        # gG is just the first backwards with the gamma term removed (then shaped properly)\n+        gG = ggI * first_back_grad_input(gO, 1)", "path": "torch/nn/_functions/thnn/batchnorm_double_backwards.py", "position": 80, "original_position": 79, "commit_id": "cff79cabfa0a8d84e68faf41249a15db38810e77", "original_commit_id": "91a20adebf9e4a6f486952b071450eb61b941dcd", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "Not sure how much you want to emphasize in the inplace part, but I think you could make this inplace as well as the return statement from `first_back_grad_input`.", "created_at": "2017-07-26T12:37:32Z", "updated_at": "2018-11-23T15:34:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/2207#discussion_r129558781", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2207", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/129558781"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2207#discussion_r129558781"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2207"}}, "body_html": "<p>Not sure how much you want to emphasize in the inplace part, but I think you could make this inplace as well as the return statement from <code>first_back_grad_input</code>.</p>", "body_text": "Not sure how much you want to emphasize in the inplace part, but I think you could make this inplace as well as the return statement from first_back_grad_input."}