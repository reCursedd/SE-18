{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/129557687", "pull_request_review_id": 52340949, "id": 129557687, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyOTU1NzY4Nw==", "diff_hunk": "@@ -0,0 +1,97 @@\n+from torch.autograd.variable import Variable\n+from functools import reduce\n+from operator import mul\n+\n+\n+def sum_exclude_dim1(to_sum, keepdim=True):\n+    to_sum = to_sum.sum(dim=0, keepdim=True)\n+    dim = 2\n+    for dim in range(2, to_sum.dim()):\n+        to_sum = to_sum.sum(dim=dim, keepdim=True)\n+    return to_sum\n+\n+\n+# because gamma/ggG/ggB are 1-dimensional and represent dim==1, we can't\n+# do a straight expansion because it won't follow the broadcasting rules.\n+def expand_as_dim1(src, target):\n+    src_expanded = src\n+    while len(src_expanded.size()) < len(target.size()) - 1:\n+        src_expanded = src_expanded.unsqueeze(1)\n+    return src_expanded.expand_as(target)\n+\n+\n+def batchnorm_double_backwards_fn(input, gamma, ggI, ggG, ggB, gO, eps):\n+    affine = gamma is not None\n+    if affine:\n+        gamma_expanded = expand_as_dim1(gamma, input)\n+\n+        if ggG is not None:\n+            ggG_expanded = expand_as_dim1(ggG, input)\n+\n+        if ggB is not None:\n+            ggB_expanded = expand_as_dim1(ggB, input)\n+    else:\n+        gamma_expanded = 1\n+\n+    # define some terms we will reuse\n+    M = reduce(mul, input.size()[0:1] + input.size()[2:])\n+    mu = sum_exclude_dim1(input).div_(M)\n+    input_sub_mu = input - mu", "path": "torch/nn/_functions/thnn/batchnorm_double_backwards.py", "position": 39, "original_position": 39, "commit_id": "cff79cabfa0a8d84e68faf41249a15db38810e77", "original_commit_id": "91a20adebf9e4a6f486952b071450eb61b941dcd", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "body": "Can this operation actually be made inplace in `mu`?", "created_at": "2017-07-26T12:32:01Z", "updated_at": "2018-11-23T15:34:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/2207#discussion_r129557687", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2207", "author_association": "COLLABORATOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/129557687"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2207#discussion_r129557687"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2207"}}, "body_html": "<p>Can this operation actually be made inplace in <code>mu</code>?</p>", "body_text": "Can this operation actually be made inplace in mu?"}