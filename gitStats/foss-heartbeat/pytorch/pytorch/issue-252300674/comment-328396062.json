{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/328396062", "html_url": "https://github.com/pytorch/pytorch/issues/2518#issuecomment-328396062", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2518", "id": 328396062, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODM5NjA2Mg==", "user": {"login": "negrinho", "id": 9198933, "node_id": "MDQ6VXNlcjkxOTg5MzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/9198933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/negrinho", "html_url": "https://github.com/negrinho", "followers_url": "https://api.github.com/users/negrinho/followers", "following_url": "https://api.github.com/users/negrinho/following{/other_user}", "gists_url": "https://api.github.com/users/negrinho/gists{/gist_id}", "starred_url": "https://api.github.com/users/negrinho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/negrinho/subscriptions", "organizations_url": "https://api.github.com/users/negrinho/orgs", "repos_url": "https://api.github.com/users/negrinho/repos", "events_url": "https://api.github.com/users/negrinho/events{/privacy}", "received_events_url": "https://api.github.com/users/negrinho/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-11T01:57:58Z", "updated_at": "2017-09-11T01:58:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8059750\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/honnibal\">@honnibal</a>, we are talking about 10x slower when compared to DyNet for the same model. This is prohibitively slow. There are cases in research where batching is difficult or you would rather bother with it later. In these cases, it is important to be able to get a working model that runs reasonably fast. This is is especially important in dynamic frameworks such as PyTorch, as flexibility is one its main selling points. A factor of ten is the difference between getting results in an hour or getting results in ten hours. This definitely makes a difference in research projects where you want to iterate fast.</p>\n<p>There is always this idea that we are able to spend most of the time in fast C/C++ code, but I wonder if it really happens that frequently. I got uniformly slow Python code on more than one occasion. I'm definitely looking forward for the coming performance fix.</p>", "body_text": "@honnibal, we are talking about 10x slower when compared to DyNet for the same model. This is prohibitively slow. There are cases in research where batching is difficult or you would rather bother with it later. In these cases, it is important to be able to get a working model that runs reasonably fast. This is is especially important in dynamic frameworks such as PyTorch, as flexibility is one its main selling points. A factor of ten is the difference between getting results in an hour or getting results in ten hours. This definitely makes a difference in research projects where you want to iterate fast.\nThere is always this idea that we are able to spend most of the time in fast C/C++ code, but I wonder if it really happens that frequently. I got uniformly slow Python code on more than one occasion. I'm definitely looking forward for the coming performance fix.", "body": "@honnibal, we are talking about 10x slower when compared to DyNet for the same model. This is prohibitively slow. There are cases in research where batching is difficult or you would rather bother with it later. In these cases, it is important to be able to get a working model that runs reasonably fast. This is is especially important in dynamic frameworks such as PyTorch, as flexibility is one its main selling points. A factor of ten is the difference between getting results in an hour or getting results in ten hours. This definitely makes a difference in research projects where you want to iterate fast. \r\n\r\nThere is always this idea that we are able to spend most of the time in fast C/C++ code, but I wonder if it really happens that frequently. I got uniformly slow Python code on more than one occasion. I'm definitely looking forward for the coming performance fix."}