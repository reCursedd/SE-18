{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/328406279", "html_url": "https://github.com/pytorch/pytorch/issues/2518#issuecomment-328406279", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2518", "id": 328406279, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODQwNjI3OQ==", "user": {"login": "negrinho", "id": 9198933, "node_id": "MDQ6VXNlcjkxOTg5MzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/9198933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/negrinho", "html_url": "https://github.com/negrinho", "followers_url": "https://api.github.com/users/negrinho/followers", "following_url": "https://api.github.com/users/negrinho/following{/other_user}", "gists_url": "https://api.github.com/users/negrinho/gists{/gist_id}", "starred_url": "https://api.github.com/users/negrinho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/negrinho/subscriptions", "organizations_url": "https://api.github.com/users/negrinho/orgs", "repos_url": "https://api.github.com/users/negrinho/repos", "events_url": "https://api.github.com/users/negrinho/events{/privacy}", "received_events_url": "https://api.github.com/users/negrinho/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-11T03:27:53Z", "updated_at": "2017-09-11T03:34:26Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8059750\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/honnibal\">@honnibal</a>, the overhead seems to be in the variable and backward computations, which are important parts of a training loop. If you do not batch, you end up getting a lot of these operations that perhaps spend most of their time on overhead. The way I read what <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> said about workloads was not as much about the time they take, but rather about the amount of work that they do. If you do not batch, the effective computation that you are doing per call may be tiny, which leads to overhead becoming an important issue.</p>\n<p>I believe these problem do come up in real implementations. The benchmark is a very minimal and straightforward example of something simple that is very inefficient. Right now, the only way of amortizing overhead seems to be doing everything using batches, which for some problems may involve a lot of tedious bookkeeping. For research, you often prefer quick and simple solutions at first.</p>", "body_text": "@honnibal, the overhead seems to be in the variable and backward computations, which are important parts of a training loop. If you do not batch, you end up getting a lot of these operations that perhaps spend most of their time on overhead. The way I read what @soumith said about workloads was not as much about the time they take, but rather about the amount of work that they do. If you do not batch, the effective computation that you are doing per call may be tiny, which leads to overhead becoming an important issue.\nI believe these problem do come up in real implementations. The benchmark is a very minimal and straightforward example of something simple that is very inefficient. Right now, the only way of amortizing overhead seems to be doing everything using batches, which for some problems may involve a lot of tedious bookkeeping. For research, you often prefer quick and simple solutions at first.", "body": "@honnibal, the overhead seems to be in the variable and backward computations, which are important parts of a training loop. If you do not batch, you end up getting a lot of these operations that perhaps spend most of their time on overhead. The way I read what @soumith said about workloads was not as much about the time they take, but rather about the amount of work that they do. If you do not batch, the effective computation that you are doing per call may be tiny, which leads to overhead becoming an important issue. \r\n\r\nI believe these problem do come up in real implementations. The benchmark is a very minimal and straightforward example of something simple that is very inefficient. Right now, the only way of amortizing overhead seems to be doing everything using batches, which for some problems may involve a lot of tedious bookkeeping. For research, you often prefer quick and simple solutions at first.  "}