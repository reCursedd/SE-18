{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/328389763", "html_url": "https://github.com/pytorch/pytorch/issues/2518#issuecomment-328389763", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2518", "id": 328389763, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODM4OTc2Mw==", "user": {"login": "honnibal", "id": 8059750, "node_id": "MDQ6VXNlcjgwNTk3NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/8059750?v=4", "gravatar_id": "", "url": "https://api.github.com/users/honnibal", "html_url": "https://github.com/honnibal", "followers_url": "https://api.github.com/users/honnibal/followers", "following_url": "https://api.github.com/users/honnibal/following{/other_user}", "gists_url": "https://api.github.com/users/honnibal/gists{/gist_id}", "starred_url": "https://api.github.com/users/honnibal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/honnibal/subscriptions", "organizations_url": "https://api.github.com/users/honnibal/orgs", "repos_url": "https://api.github.com/users/honnibal/repos", "events_url": "https://api.github.com/users/honnibal/events{/privacy}", "received_events_url": "https://api.github.com/users/honnibal/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-11T01:02:30Z", "updated_at": "2017-09-11T01:02:30Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Why didn't we catch it before-hand?</p>\n</blockquote>\n<blockquote>\n<p>It might sound funny, but:</p>\n</blockquote>\n<blockquote>\n<p>we didn't realize folks are running scripts with really small workloads (no one told us)<br>\nthe smallest workload we run before every release for performance regressions is the small &gt; word_language_model, but it was not a small enough workload to catch these performance regressions.</p>\n</blockquote>\n<p>I'm a little worried there's a misunderstanding here! Nobody is <em>really</em> running tasks with such small workloads --- the data from that Twitter thread was just a tiny sample, to ease discussion right?</p>\n<p>I think there are tasks where the overhead becomes relevant, but not because the total job size is so small. For instance I expect a model that made a prediction on each character would be sort of slow in PyTorch?</p>", "body_text": "Why didn't we catch it before-hand?\n\n\nIt might sound funny, but:\n\n\nwe didn't realize folks are running scripts with really small workloads (no one told us)\nthe smallest workload we run before every release for performance regressions is the small > word_language_model, but it was not a small enough workload to catch these performance regressions.\n\nI'm a little worried there's a misunderstanding here! Nobody is really running tasks with such small workloads --- the data from that Twitter thread was just a tiny sample, to ease discussion right?\nI think there are tasks where the overhead becomes relevant, but not because the total job size is so small. For instance I expect a model that made a prediction on each character would be sort of slow in PyTorch?", "body": "> Why didn't we catch it before-hand?\r\n\r\n> It might sound funny, but:\r\n\r\n> we didn't realize folks are running scripts with really small workloads (no one told us)\r\n> the smallest workload we run before every release for performance regressions is the small > word_language_model, but it was not a small enough workload to catch these performance regressions.\r\n\r\nI'm a little worried there's a misunderstanding here! Nobody is *really* running tasks with such small workloads --- the data from that Twitter thread was just a tiny sample, to ease discussion right?\r\n\r\nI think there are tasks where the overhead becomes relevant, but not because the total job size is so small. For instance I expect a model that made a prediction on each character would be sort of slow in PyTorch? "}