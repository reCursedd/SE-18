{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/327835296", "html_url": "https://github.com/pytorch/pytorch/issues/2518#issuecomment-327835296", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2518", "id": 327835296, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzgzNTI5Ng==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-07T15:28:40Z", "updated_at": "2017-10-15T02:00:05Z", "author_association": "MEMBER", "body_html": "<p>Here's a full report on what's going on, what we are doing to fix this. We expect to finish the work in about 1.5 to 2 weeks and get pytorch to be within ballpark of DyNet numbers, work has been going on at full swing by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>\n<h4>Reasons for slowdown</h4>\n<ol>\n<li>our autograd overhead is close to 10 microseconds per Python Function invocation.</li>\n<li>with the introduction of higher-order gradients, this got worse in <code>0.2.0</code> than in <code>0.1.12</code></li>\n<li>minor CPU optimizations at places.</li>\n</ol>\n<p>Turns out, Eigen is not a reason for the slowdown. We patched in Eigen into pytorch, and MKL into DyNet to verify this. Sorry about the initial false-positive.</p>\n<p>Slightly more detail on (1), most of our autograd functions are defined with their full logic in python. For example, see the <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L39-L66\">addmm</a> function. We did that for convenience. However, this has a real perf penalty, especially on functions that are run very frequently, and which have a lot of argument parsing logic.</p>\n<h4>Why didn't we catch it before-hand?</h4>\n<p>It might sound funny, but:</p>\n<ul>\n<li>we didn't realize folks are running scripts with really small workloads (no one told us)</li>\n<li>the smallest workload we run before every release for performance regressions is the small <a href=\"https://github.com/pytorch/examples/tree/master/word_language_model\">word_language_model</a>, but it was not a small enough workload to catch these performance regressions.</li>\n</ul>\n<h4>How are we fixing it?</h4>\n<ul>\n<li>We are doing work that's been long pending. All fundamental functions are being moved to our auto-generated <a href=\"https://github.com/zdevito/aten\">ATen</a> library, which is getting autograd support. Stuff like <code>torch.matmul</code>, <code>torch.sum</code>, etc. will be moved to ATen</li>\n</ul>\n<p>The autograd overhead after doing this work will go from 10 microseconds to 1 microsecond or less.</p>\n<p>We have evidence (<a href=\"https://github.com/gchanan/pytorch/wiki/Mini-Sequence-Labeler-PyTorch-vs-Dynet\">see gchanan's wiki below</a>) that this will pessimistically get us to 2x of DyNet's speed on this workload, and optimistically, match DyNet's speed. I expect us to be within 1.5x of DyNet, while supporting all the existing features we have such as higher order derivatives. Once we reach 1.5x, we'll further investigate to close the gap to 1.0x.</p>\n<p>This work is independent of our <a href=\"https://github.com/pytorch/pytorch/pull/2565\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/2565/hovercard\">just-in-time compiler</a> that we are working on to accelerate small workloads even further, by adding operator fusion and other optimizations.</p>\n<h4>Catching such regressions in the future</h4>\n<ul>\n<li>We are setting up regression tests with smaller workloads (including this one).</li>\n<li>We will be working on deeply integrated profilers to easily identify bottlenecks in the future</li>\n<li>We will invite powerusers to throw their workloads at us and have dedicated engineering to investigate them routinely (more details on this later).</li>\n</ul>\n<h4>A more detailed, fuller report</h4>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a> is maintaining a detailed wiki, with per-operator timing, etc. over here: <a href=\"https://github.com/gchanan/pytorch/wiki/Mini-Sequeence-Labeler-PyTorch-vs-Dynet\">https://github.com/gchanan/pytorch/wiki/Mini-Sequeence-Labeler-PyTorch-vs-Dynet</a><br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=398875\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/neubig\">@neubig</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1334513\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hal3\">@hal3</a> might be interested in this.</p>\n<p>For <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1334513\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hal3\">@hal3</a> if you have some time to help us: <a href=\"https://github.com/gchanan/pytorch/wiki/Mini-Sequence-Labeler-PyTorch-vs-Dynet#larger-model\">we couldn't reproduce the speed issues you had with the larger model</a>. with your permission, I'll start an email chain with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a> so that he can reproduce the larger model issue as well, and investigate this further.</p>", "body_text": "Here's a full report on what's going on, what we are doing to fix this. We expect to finish the work in about 1.5 to 2 weeks and get pytorch to be within ballpark of DyNet numbers, work has been going on at full swing by @gchanan and @colesbury\nReasons for slowdown\n\nour autograd overhead is close to 10 microseconds per Python Function invocation.\nwith the introduction of higher-order gradients, this got worse in 0.2.0 than in 0.1.12\nminor CPU optimizations at places.\n\nTurns out, Eigen is not a reason for the slowdown. We patched in Eigen into pytorch, and MKL into DyNet to verify this. Sorry about the initial false-positive.\nSlightly more detail on (1), most of our autograd functions are defined with their full logic in python. For example, see the addmm function. We did that for convenience. However, this has a real perf penalty, especially on functions that are run very frequently, and which have a lot of argument parsing logic.\nWhy didn't we catch it before-hand?\nIt might sound funny, but:\n\nwe didn't realize folks are running scripts with really small workloads (no one told us)\nthe smallest workload we run before every release for performance regressions is the small word_language_model, but it was not a small enough workload to catch these performance regressions.\n\nHow are we fixing it?\n\nWe are doing work that's been long pending. All fundamental functions are being moved to our auto-generated ATen library, which is getting autograd support. Stuff like torch.matmul, torch.sum, etc. will be moved to ATen\n\nThe autograd overhead after doing this work will go from 10 microseconds to 1 microsecond or less.\nWe have evidence (see gchanan's wiki below) that this will pessimistically get us to 2x of DyNet's speed on this workload, and optimistically, match DyNet's speed. I expect us to be within 1.5x of DyNet, while supporting all the existing features we have such as higher order derivatives. Once we reach 1.5x, we'll further investigate to close the gap to 1.0x.\nThis work is independent of our just-in-time compiler that we are working on to accelerate small workloads even further, by adding operator fusion and other optimizations.\nCatching such regressions in the future\n\nWe are setting up regression tests with smaller workloads (including this one).\nWe will be working on deeply integrated profilers to easily identify bottlenecks in the future\nWe will invite powerusers to throw their workloads at us and have dedicated engineering to investigate them routinely (more details on this later).\n\nA more detailed, fuller report\n@gchanan is maintaining a detailed wiki, with per-operator timing, etc. over here: https://github.com/gchanan/pytorch/wiki/Mini-Sequeence-Labeler-PyTorch-vs-Dynet\n@neubig and @hal3 might be interested in this.\nFor @hal3 if you have some time to help us: we couldn't reproduce the speed issues you had with the larger model. with your permission, I'll start an email chain with @gchanan so that he can reproduce the larger model issue as well, and investigate this further.", "body": "Here's a full report on what's going on, what we are doing to fix this. We expect to finish the work in about 1.5 to 2 weeks and get pytorch to be within ballpark of DyNet numbers, work has been going on at full swing by @gchanan and @colesbury \r\n\r\n#### Reasons for slowdown\r\n\r\n1. our autograd overhead is close to 10 microseconds per Python Function invocation.\r\n2. with the introduction of higher-order gradients, this got worse in `0.2.0` than in `0.1.12`\r\n3. minor CPU optimizations at places.\r\n\r\nTurns out, Eigen is not a reason for the slowdown. We patched in Eigen into pytorch, and MKL into DyNet to verify this. Sorry about the initial false-positive.\r\n\r\nSlightly more detail on (1), most of our autograd functions are defined with their full logic in python. For example, see the [addmm](https://github.com/pytorch/pytorch/blob/master/torch/autograd/_functions/blas.py#L39-L66) function. We did that for convenience. However, this has a real perf penalty, especially on functions that are run very frequently, and which have a lot of argument parsing logic.\r\n\r\n#### Why didn't we catch it before-hand?\r\n\r\nIt might sound funny, but:\r\n- we didn't realize folks are running scripts with really small workloads (no one told us)\r\n- the smallest workload we run before every release for performance regressions is the small [word_language_model](https://github.com/pytorch/examples/tree/master/word_language_model), but it was not a small enough workload to catch these performance regressions.\r\n\r\n#### How are we fixing it?\r\n\r\n- We are doing work that's been long pending. All fundamental functions are being moved to our auto-generated [ATen](https://github.com/zdevito/aten) library, which is getting autograd support. Stuff like `torch.matmul`, `torch.sum`, etc. will be moved to ATen\r\n\r\nThe autograd overhead after doing this work will go from 10 microseconds to 1 microsecond or less.\r\n\r\nWe have evidence ([see gchanan's wiki below](https://github.com/gchanan/pytorch/wiki/Mini-Sequence-Labeler-PyTorch-vs-Dynet)) that this will pessimistically get us to 2x of DyNet's speed on this workload, and optimistically, match DyNet's speed. I expect us to be within 1.5x of DyNet, while supporting all the existing features we have such as higher order derivatives. Once we reach 1.5x, we'll further investigate to close the gap to 1.0x. \r\n\r\nThis work is independent of our [just-in-time compiler](https://github.com/pytorch/pytorch/pull/2565) that we are working on to accelerate small workloads even further, by adding operator fusion and other optimizations.\r\n\r\n#### Catching such regressions in the future\r\n\r\n- We are setting up regression tests with smaller workloads (including this one).\r\n- We will be working on deeply integrated profilers to easily identify bottlenecks in the future\r\n- We will invite powerusers to throw their workloads at us and have dedicated engineering to investigate them routinely (more details on this later).\r\n\r\n#### A more detailed, fuller report\r\n\r\n@gchanan is maintaining a detailed wiki, with per-operator timing, etc. over here: https://github.com/gchanan/pytorch/wiki/Mini-Sequeence-Labeler-PyTorch-vs-Dynet\r\n@neubig and @hal3 might be interested in this.\r\n\r\nFor @hal3 if you have some time to help us: [we couldn't reproduce the speed issues you had with the larger model](https://github.com/gchanan/pytorch/wiki/Mini-Sequence-Labeler-PyTorch-vs-Dynet#larger-model). with your permission, I'll start an email chain with @gchanan so that he can reproduce the larger model issue as well, and investigate this further.\r\n"}