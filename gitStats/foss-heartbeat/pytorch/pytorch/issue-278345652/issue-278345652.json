{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3959", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3959/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3959/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3959/events", "html_url": "https://github.com/pytorch/pytorch/issues/3959", "id": 278345652, "node_id": "MDU6SXNzdWUyNzgzNDU2NTI=", "number": 3959, "title": "torch.topk Memory Leakage", "user": {"login": "todpole3", "id": 4227871, "node_id": "MDQ6VXNlcjQyMjc4NzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4227871?v=4", "gravatar_id": "", "url": "https://api.github.com/users/todpole3", "html_url": "https://github.com/todpole3", "followers_url": "https://api.github.com/users/todpole3/followers", "following_url": "https://api.github.com/users/todpole3/following{/other_user}", "gists_url": "https://api.github.com/users/todpole3/gists{/gist_id}", "starred_url": "https://api.github.com/users/todpole3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/todpole3/subscriptions", "organizations_url": "https://api.github.com/users/todpole3/orgs", "repos_url": "https://api.github.com/users/todpole3/repos", "events_url": "https://api.github.com/users/todpole3/events{/privacy}", "received_events_url": "https://api.github.com/users/todpole3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2017-12-01T03:21:24Z", "updated_at": "2018-07-19T19:59:50Z", "closed_at": "2018-07-19T19:59:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>#I implemented a beam search and has run into a memory leakage problem.</p>\n<pre><code>def top_k_action(log_action_dist, action_space):\n        \"\"\"\n        Get top k action for batches\n            - k = beam_size if the beam size is smaller than or equal to the beam action space size\n            - k = beam_action_space_size otherwise\n        :param log_action_dist: [batch_size*beam_size, action_space_size]\n        :param action_space (r_space, e_space):\n            r_space: [batch_size*beam_size, action_space_size]\n            e_space: [batch_size*beam_size, action_space_size]\n        :return:\n            (next_r, next_e), action_prob, action_offset: [batch_size*k]\n        \"\"\"\n        action_space_size = action_space[0].size()[1]\n        # =&gt; [batch_size, k'*action_space_size]]\n        log_action_dist = log_action_dist.view(batch_size, -1)\n        beam_action_space_size = log_action_dist.size()[1]\n        assert(beam_action_space_size % action_space_size == 0)\n        last_k = beam_action_space_size / action_space_size\n        k = beam_size if beam_size &lt; beam_action_space_size else beam_action_space_size\n        # [batch_size, k]\n        action_prob, action_ind = torch.topk(log_action_dist, k)\n        # DEBUGGING -- numpy topk operator\n        # log_action_dist_cpu = log_action_dist.data.cpu().numpy()\n        # action_prob = src.ops.var_cuda(torch.FloatTensor(np.sort(log_action_dist_cpu, axis=1)[:, -k:]))\n        # action_ind = src.ops.int_var_cuda(torch.LongTensor(np.argsort(log_action_dist_cpu, axis=1)[:, -k:]))\n        # compute parent offset\n        # [batch_size, 1]\n        action_batch_offset = (Variable(torch.arange(batch_size), volatile=True) * last_k)\\\n            .unsqueeze(1).type(torch.LongTensor).cuda()\n        # [batch_size, k]\n        action_beam_offset = action_ind / action_space_size\n        # [batch_size, k] =&gt; [batch_size*k]\n        action_offset = (action_batch_offset + action_beam_offset).view(-1)\n        # compute action indices\n        # [batch_size*k', action_space_size] =&gt; [batch_size*k, action_space_size]\n        r_space = action_space[0][action_offset]\n        e_space = action_space[1][action_offset]\n        # [batch_size*k, 1]\n        action_space_ind = (action_ind % action_space_size).view(-1, 1).detach()\n        next_r = src.ops.batch_lookup(r_space, action_space_ind)\n        next_e = src.ops.batch_lookup(e_space, action_space_ind)\n        # [batch_size, k] =&gt; [batch_size*k] \n        action_prob = action_prob.view(-1)\n        return (next_r, next_e), action_prob, action_offset\n</code></pre>\n<p>When I run beam search, the memory continuously increases as it searches over more examples and eventually it hits the following out of memory error during the foward pass:</p>\n<pre><code>THCudaCheck FAIL file=/tmp/pip-a7h6fthz-build/aten/src/THC/generic/THCStorage.cu line=58 error=2 : out of memory\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/Projects/dtt/src/experiments.py\", line 159, in &lt;module&gt;\n    main()\n  File \"/home/Projects/dtt/src/experiments.py\", line 152, in main\n    inference(pn, kg, rlf)\n  File \"/home/Projects/dtt/src/experiments.py\", line 48, in inference\n    rlf.inference(pn, kg, test_data, with_label=False, verbose=True)\n  File \"/home/Projects/dtt/src/rl.py\", line 157, in inference\n    pn, e1, r, e2, kg, num_steps=self.num_rollout_steps, beam_size=beam_size)\n  File \"/home/Projects/dtt/src/search.py\", line 80, in beam_search\n    action_dist, _ = pn.policy_nn(e, q, e_s, None, kg, use_dynamic_batching=False)\n  File \"/home/Projects/dtt/src/policy_network.py\", line 151, in policy_nn\n    A = self.get_action_embedding(KG, (r_space, e_space))\n  File \"/home/Projects/dtt/src/policy_network.py\", line 277, in get_action_embedding\n    action_embedding = torch.cat([relation_embedding, entity_embedding], dim=-1)\nRuntimeError: cuda runtime error (2) : out of memory at /tmp/pip-a7h6fthz-build/aten/src/THC/generic/THCStorage.cu:58\n</code></pre>\n<p>However, the program runs error free if I commented out the line using the <code>topk</code> operator and replace the results of that line using random tensors.</p>", "body_text": "#I implemented a beam search and has run into a memory leakage problem.\ndef top_k_action(log_action_dist, action_space):\n        \"\"\"\n        Get top k action for batches\n            - k = beam_size if the beam size is smaller than or equal to the beam action space size\n            - k = beam_action_space_size otherwise\n        :param log_action_dist: [batch_size*beam_size, action_space_size]\n        :param action_space (r_space, e_space):\n            r_space: [batch_size*beam_size, action_space_size]\n            e_space: [batch_size*beam_size, action_space_size]\n        :return:\n            (next_r, next_e), action_prob, action_offset: [batch_size*k]\n        \"\"\"\n        action_space_size = action_space[0].size()[1]\n        # => [batch_size, k'*action_space_size]]\n        log_action_dist = log_action_dist.view(batch_size, -1)\n        beam_action_space_size = log_action_dist.size()[1]\n        assert(beam_action_space_size % action_space_size == 0)\n        last_k = beam_action_space_size / action_space_size\n        k = beam_size if beam_size < beam_action_space_size else beam_action_space_size\n        # [batch_size, k]\n        action_prob, action_ind = torch.topk(log_action_dist, k)\n        # DEBUGGING -- numpy topk operator\n        # log_action_dist_cpu = log_action_dist.data.cpu().numpy()\n        # action_prob = src.ops.var_cuda(torch.FloatTensor(np.sort(log_action_dist_cpu, axis=1)[:, -k:]))\n        # action_ind = src.ops.int_var_cuda(torch.LongTensor(np.argsort(log_action_dist_cpu, axis=1)[:, -k:]))\n        # compute parent offset\n        # [batch_size, 1]\n        action_batch_offset = (Variable(torch.arange(batch_size), volatile=True) * last_k)\\\n            .unsqueeze(1).type(torch.LongTensor).cuda()\n        # [batch_size, k]\n        action_beam_offset = action_ind / action_space_size\n        # [batch_size, k] => [batch_size*k]\n        action_offset = (action_batch_offset + action_beam_offset).view(-1)\n        # compute action indices\n        # [batch_size*k', action_space_size] => [batch_size*k, action_space_size]\n        r_space = action_space[0][action_offset]\n        e_space = action_space[1][action_offset]\n        # [batch_size*k, 1]\n        action_space_ind = (action_ind % action_space_size).view(-1, 1).detach()\n        next_r = src.ops.batch_lookup(r_space, action_space_ind)\n        next_e = src.ops.batch_lookup(e_space, action_space_ind)\n        # [batch_size, k] => [batch_size*k] \n        action_prob = action_prob.view(-1)\n        return (next_r, next_e), action_prob, action_offset\n\nWhen I run beam search, the memory continuously increases as it searches over more examples and eventually it hits the following out of memory error during the foward pass:\nTHCudaCheck FAIL file=/tmp/pip-a7h6fthz-build/aten/src/THC/generic/THCStorage.cu line=58 error=2 : out of memory\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/Projects/dtt/src/experiments.py\", line 159, in <module>\n    main()\n  File \"/home/Projects/dtt/src/experiments.py\", line 152, in main\n    inference(pn, kg, rlf)\n  File \"/home/Projects/dtt/src/experiments.py\", line 48, in inference\n    rlf.inference(pn, kg, test_data, with_label=False, verbose=True)\n  File \"/home/Projects/dtt/src/rl.py\", line 157, in inference\n    pn, e1, r, e2, kg, num_steps=self.num_rollout_steps, beam_size=beam_size)\n  File \"/home/Projects/dtt/src/search.py\", line 80, in beam_search\n    action_dist, _ = pn.policy_nn(e, q, e_s, None, kg, use_dynamic_batching=False)\n  File \"/home/Projects/dtt/src/policy_network.py\", line 151, in policy_nn\n    A = self.get_action_embedding(KG, (r_space, e_space))\n  File \"/home/Projects/dtt/src/policy_network.py\", line 277, in get_action_embedding\n    action_embedding = torch.cat([relation_embedding, entity_embedding], dim=-1)\nRuntimeError: cuda runtime error (2) : out of memory at /tmp/pip-a7h6fthz-build/aten/src/THC/generic/THCStorage.cu:58\n\nHowever, the program runs error free if I commented out the line using the topk operator and replace the results of that line using random tensors.", "body": "#I implemented a beam search and has run into a memory leakage problem.\r\n\r\n```\r\ndef top_k_action(log_action_dist, action_space):\r\n        \"\"\"\r\n        Get top k action for batches\r\n            - k = beam_size if the beam size is smaller than or equal to the beam action space size\r\n            - k = beam_action_space_size otherwise\r\n        :param log_action_dist: [batch_size*beam_size, action_space_size]\r\n        :param action_space (r_space, e_space):\r\n            r_space: [batch_size*beam_size, action_space_size]\r\n            e_space: [batch_size*beam_size, action_space_size]\r\n        :return:\r\n            (next_r, next_e), action_prob, action_offset: [batch_size*k]\r\n        \"\"\"\r\n        action_space_size = action_space[0].size()[1]\r\n        # => [batch_size, k'*action_space_size]]\r\n        log_action_dist = log_action_dist.view(batch_size, -1)\r\n        beam_action_space_size = log_action_dist.size()[1]\r\n        assert(beam_action_space_size % action_space_size == 0)\r\n        last_k = beam_action_space_size / action_space_size\r\n        k = beam_size if beam_size < beam_action_space_size else beam_action_space_size\r\n        # [batch_size, k]\r\n        action_prob, action_ind = torch.topk(log_action_dist, k)\r\n        # DEBUGGING -- numpy topk operator\r\n        # log_action_dist_cpu = log_action_dist.data.cpu().numpy()\r\n        # action_prob = src.ops.var_cuda(torch.FloatTensor(np.sort(log_action_dist_cpu, axis=1)[:, -k:]))\r\n        # action_ind = src.ops.int_var_cuda(torch.LongTensor(np.argsort(log_action_dist_cpu, axis=1)[:, -k:]))\r\n        # compute parent offset\r\n        # [batch_size, 1]\r\n        action_batch_offset = (Variable(torch.arange(batch_size), volatile=True) * last_k)\\\r\n            .unsqueeze(1).type(torch.LongTensor).cuda()\r\n        # [batch_size, k]\r\n        action_beam_offset = action_ind / action_space_size\r\n        # [batch_size, k] => [batch_size*k]\r\n        action_offset = (action_batch_offset + action_beam_offset).view(-1)\r\n        # compute action indices\r\n        # [batch_size*k', action_space_size] => [batch_size*k, action_space_size]\r\n        r_space = action_space[0][action_offset]\r\n        e_space = action_space[1][action_offset]\r\n        # [batch_size*k, 1]\r\n        action_space_ind = (action_ind % action_space_size).view(-1, 1).detach()\r\n        next_r = src.ops.batch_lookup(r_space, action_space_ind)\r\n        next_e = src.ops.batch_lookup(e_space, action_space_ind)\r\n        # [batch_size, k] => [batch_size*k] \r\n        action_prob = action_prob.view(-1)\r\n        return (next_r, next_e), action_prob, action_offset\r\n```\r\n\r\nWhen I run beam search, the memory continuously increases as it searches over more examples and eventually it hits the following out of memory error during the foward pass:\r\n```\r\nTHCudaCheck FAIL file=/tmp/pip-a7h6fthz-build/aten/src/THC/generic/THCStorage.cu line=58 error=2 : out of memory\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/Projects/dtt/src/experiments.py\", line 159, in <module>\r\n    main()\r\n  File \"/home/Projects/dtt/src/experiments.py\", line 152, in main\r\n    inference(pn, kg, rlf)\r\n  File \"/home/Projects/dtt/src/experiments.py\", line 48, in inference\r\n    rlf.inference(pn, kg, test_data, with_label=False, verbose=True)\r\n  File \"/home/Projects/dtt/src/rl.py\", line 157, in inference\r\n    pn, e1, r, e2, kg, num_steps=self.num_rollout_steps, beam_size=beam_size)\r\n  File \"/home/Projects/dtt/src/search.py\", line 80, in beam_search\r\n    action_dist, _ = pn.policy_nn(e, q, e_s, None, kg, use_dynamic_batching=False)\r\n  File \"/home/Projects/dtt/src/policy_network.py\", line 151, in policy_nn\r\n    A = self.get_action_embedding(KG, (r_space, e_space))\r\n  File \"/home/Projects/dtt/src/policy_network.py\", line 277, in get_action_embedding\r\n    action_embedding = torch.cat([relation_embedding, entity_embedding], dim=-1)\r\nRuntimeError: cuda runtime error (2) : out of memory at /tmp/pip-a7h6fthz-build/aten/src/THC/generic/THCStorage.cu:58\r\n```\r\n\r\nHowever, the program runs error free if I commented out the line using the `topk` operator and replace the results of that line using random tensors."}