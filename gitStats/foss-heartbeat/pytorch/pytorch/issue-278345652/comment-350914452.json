{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/350914452", "html_url": "https://github.com/pytorch/pytorch/issues/3959#issuecomment-350914452", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3959", "id": 350914452, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDkxNDQ1Mg==", "user": {"login": "todpole3", "id": 4227871, "node_id": "MDQ6VXNlcjQyMjc4NzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/4227871?v=4", "gravatar_id": "", "url": "https://api.github.com/users/todpole3", "html_url": "https://github.com/todpole3", "followers_url": "https://api.github.com/users/todpole3/followers", "following_url": "https://api.github.com/users/todpole3/following{/other_user}", "gists_url": "https://api.github.com/users/todpole3/gists{/gist_id}", "starred_url": "https://api.github.com/users/todpole3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/todpole3/subscriptions", "organizations_url": "https://api.github.com/users/todpole3/orgs", "repos_url": "https://api.github.com/users/todpole3/repos", "events_url": "https://api.github.com/users/todpole3/events{/privacy}", "received_events_url": "https://api.github.com/users/todpole3/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-12T01:24:44Z", "updated_at": "2017-12-12T21:12:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p><code>action_space</code> is a 2D variable.</p>\n<p>We experimented with two additional cases which might confirm that the memory leakage happens in the gradient computation.</p>\n<ol>\n<li>If replaced with a numpy implementation of topk, the memory won't continuously increase and decoding could run through.</li>\n</ol>\n<pre><code># DEBUGGING -- numpy topk operator\n# log_action_dist_cpu = log_action_dist.data.cpu().numpy()\n# action_prob = src.ops.var_cuda(torch.FloatTensor(np.sort(log_action_dist_cpu, axis=1)[:, -k:]))\n# action_ind = src.ops.int_var_cuda(torch.LongTensor(np.argsort(log_action_dist_cpu, axis=1)[:, -k:]))\n</code></pre>\n<ol start=\"2\">\n<li>If I set <code>volatile=True</code> for all input variables, the memory won't continuously increase.</li>\n</ol>", "body_text": "action_space is a 2D variable.\nWe experimented with two additional cases which might confirm that the memory leakage happens in the gradient computation.\n\nIf replaced with a numpy implementation of topk, the memory won't continuously increase and decoding could run through.\n\n# DEBUGGING -- numpy topk operator\n# log_action_dist_cpu = log_action_dist.data.cpu().numpy()\n# action_prob = src.ops.var_cuda(torch.FloatTensor(np.sort(log_action_dist_cpu, axis=1)[:, -k:]))\n# action_ind = src.ops.int_var_cuda(torch.LongTensor(np.argsort(log_action_dist_cpu, axis=1)[:, -k:]))\n\n\nIf I set volatile=True for all input variables, the memory won't continuously increase.", "body": "`action_space` is a 2D variable. \r\n\r\nWe experimented with two additional cases which might confirm that the memory leakage happens in the gradient computation.\r\n\r\n1. If replaced with a numpy implementation of topk, the memory won't continuously increase and decoding could run through.\r\n```\r\n# DEBUGGING -- numpy topk operator\r\n# log_action_dist_cpu = log_action_dist.data.cpu().numpy()\r\n# action_prob = src.ops.var_cuda(torch.FloatTensor(np.sort(log_action_dist_cpu, axis=1)[:, -k:]))\r\n# action_ind = src.ops.int_var_cuda(torch.LongTensor(np.argsort(log_action_dist_cpu, axis=1)[:, -k:]))\r\n```\r\n2. If I set `volatile=True` for all input variables, the memory won't continuously increase."}