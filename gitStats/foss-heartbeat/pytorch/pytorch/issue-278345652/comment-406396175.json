{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/406396175", "html_url": "https://github.com/pytorch/pytorch/issues/3959#issuecomment-406396175", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3959", "id": 406396175, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjM5NjE3NQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-19T19:59:50Z", "updated_at": "2018-07-19T19:59:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4227871\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/todpole3\">@todpole3</a> , I think what you saw isn't a memory leak. If it wasn't run with <code>volatile=True</code>, then some necessary variables are saved for backward computation. Then as you run more iterations, it uses more memory. This explanation is also consistent with your observation that neither numpy topk nor torch topk with <code>volatile=True</code> increases memory used.</p>\n<p>I'm closing this for now. Feel free to reopen if you think the problem lies elsewhere. Alternatively, if you can have a repro script, we can take a look. Thanks!</p>", "body_text": "Hi @todpole3 , I think what you saw isn't a memory leak. If it wasn't run with volatile=True, then some necessary variables are saved for backward computation. Then as you run more iterations, it uses more memory. This explanation is also consistent with your observation that neither numpy topk nor torch topk with volatile=True increases memory used.\nI'm closing this for now. Feel free to reopen if you think the problem lies elsewhere. Alternatively, if you can have a repro script, we can take a look. Thanks!", "body": "Hi @todpole3 , I think what you saw isn't a memory leak. If it wasn't run with `volatile=True`, then some necessary variables are saved for backward computation. Then as you run more iterations, it uses more memory. This explanation is also consistent with your observation that neither numpy topk nor torch topk with `volatile=True` increases memory used.\r\n\r\nI'm closing this for now. Feel free to reopen if you think the problem lies elsewhere. Alternatively, if you can have a repro script, we can take a look. Thanks!"}