{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373314572", "html_url": "https://github.com/pytorch/pytorch/pull/5795#issuecomment-373314572", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5795", "id": 373314572, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzMxNDU3Mg==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-15T09:32:42Z", "updated_at": "2018-03-15T09:32:42Z", "author_association": "MEMBER", "body_html": "<p>This is awesome!</p>\n<p>I have the impression that the scope of this feature is to be used on logical comparators and tensor constructors, but do we plan at some point to support mixing different types for other operations (addition, multiplication, etc)?</p>\n<p>While I think it would be nice to such automatic tensor castings, I think it might be very easy for the user to shoot himself on the foot because casting <code>float32</code> + <code>int32</code> gives <code>float64</code>, so situations like</p>\n<div class=\"highlight highlight-source-python\"><pre>mytensor <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">5</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> a float tensor</span>\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> mytensor <span class=\"pl-k\">+</span> torch.tensor(<span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> double!</span>\noutput <span class=\"pl-k\">=</span> my_big_net(<span class=\"pl-c1\">input</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> slow!</span></pre></div>\n<p>return a double tensor now, and all of a sudden we have a huge slowdown and memory increase.</p>\n<p>Do we want to support mixing tensor types, as numpy does? If yes, do we keep the same casting semantics as numpy?</p>", "body_text": "This is awesome!\nI have the impression that the scope of this feature is to be used on logical comparators and tensor constructors, but do we plan at some point to support mixing different types for other operations (addition, multiplication, etc)?\nWhile I think it would be nice to such automatic tensor castings, I think it might be very easy for the user to shoot himself on the foot because casting float32 + int32 gives float64, so situations like\nmytensor = torch.rand(5)  # a float tensor\ninput = mytensor + torch.tensor(1)  # double!\noutput = my_big_net(input)  # slow!\nreturn a double tensor now, and all of a sudden we have a huge slowdown and memory increase.\nDo we want to support mixing tensor types, as numpy does? If yes, do we keep the same casting semantics as numpy?", "body": "This is awesome!\r\n\r\nI have the impression that the scope of this feature is to be used on logical comparators and tensor constructors, but do we plan at some point to support mixing different types for other operations (addition, multiplication, etc)?\r\n\r\nWhile I think it would be nice to such automatic tensor castings, I think it might be very easy for the user to shoot himself on the foot because casting `float32` + `int32` gives `float64`, so situations like\r\n```python\r\nmytensor = torch.rand(5)  # a float tensor\r\ninput = mytensor + torch.tensor(1)  # double!\r\noutput = my_big_net(input)  # slow!\r\n```\r\nreturn a double tensor now, and all of a sudden we have a huge slowdown and memory increase.\r\n\r\nDo we want to support mixing tensor types, as numpy does? If yes, do we keep the same casting semantics as numpy?"}