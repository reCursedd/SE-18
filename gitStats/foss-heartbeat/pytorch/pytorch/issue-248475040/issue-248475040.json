{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2321", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2321/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2321/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2321/events", "html_url": "https://github.com/pytorch/pytorch/issues/2321", "id": 248475040, "node_id": "MDU6SXNzdWUyNDg0NzUwNDA=", "number": 2321, "title": "Single GPU per Node for Distributed Training is not working", "user": {"login": "ArEsKay3", "id": 6547143, "node_id": "MDQ6VXNlcjY1NDcxNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6547143?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ArEsKay3", "html_url": "https://github.com/ArEsKay3", "followers_url": "https://api.github.com/users/ArEsKay3/followers", "following_url": "https://api.github.com/users/ArEsKay3/following{/other_user}", "gists_url": "https://api.github.com/users/ArEsKay3/gists{/gist_id}", "starred_url": "https://api.github.com/users/ArEsKay3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ArEsKay3/subscriptions", "organizations_url": "https://api.github.com/users/ArEsKay3/orgs", "repos_url": "https://api.github.com/users/ArEsKay3/repos", "events_url": "https://api.github.com/users/ArEsKay3/events{/privacy}", "received_events_url": "https://api.github.com/users/ArEsKay3/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-08-07T17:13:21Z", "updated_at": "2017-08-24T22:27:04Z", "closed_at": "2017-08-24T22:27:04Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am trying to use the imagenet example to run distributed training across compute nodes. However, when each node only has one GPU attached I'm seeing problems running the example. The failure I see is related to the module expecting CPU Tensors, but getting CUDA Tensors. This problem goes away if I use 2 GPUs per node. My guess is that it has something to do with the fact that when there is only one device per node the code skips all parameter syncing?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">inputs</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.device_ids) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.module(<span class=\"pl-k\">*</span>inputs, <span class=\"pl-k\">**</span>kwargs)\n        inputs, kwargs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.scatter(inputs, kwargs, <span class=\"pl-c1\">self</span>.device_ids)\n        <span class=\"pl-c1\">self</span>._sync_params()\n        outputs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.parallel_apply(<span class=\"pl-c1\">self</span>._module_copies, inputs, kwargs)\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.gather(outputs, <span class=\"pl-c1\">self</span>.output_device) </pre></div>", "body_text": "I am trying to use the imagenet example to run distributed training across compute nodes. However, when each node only has one GPU attached I'm seeing problems running the example. The failure I see is related to the module expecting CPU Tensors, but getting CUDA Tensors. This problem goes away if I use 2 GPUs per node. My guess is that it has something to do with the fact that when there is only one device per node the code skips all parameter syncing?\ndef forward(self, *inputs, **kwargs):\n        if len(self.device_ids) == 1:\n            return self.module(*inputs, **kwargs)\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        self._sync_params()\n        outputs = self.parallel_apply(self._module_copies, inputs, kwargs)\n        return self.gather(outputs, self.output_device)", "body": "I am trying to use the imagenet example to run distributed training across compute nodes. However, when each node only has one GPU attached I'm seeing problems running the example. The failure I see is related to the module expecting CPU Tensors, but getting CUDA Tensors. This problem goes away if I use 2 GPUs per node. My guess is that it has something to do with the fact that when there is only one device per node the code skips all parameter syncing?\r\n\r\n```python    \r\ndef forward(self, *inputs, **kwargs):\r\n        if len(self.device_ids) == 1:\r\n            return self.module(*inputs, **kwargs)\r\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\r\n        self._sync_params()\r\n        outputs = self.parallel_apply(self._module_copies, inputs, kwargs)\r\n        return self.gather(outputs, self.output_device) \r\n```"}