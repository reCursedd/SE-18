{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183162923", "pull_request_review_id": 114117783, "id": 183162923, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MzE2MjkyMw==", "diff_hunk": "@@ -0,0 +1,84 @@\n+import torch\n+\n+\n+def aggmo(opfunc, x, config, state=None):\n+    \"\"\"A plain implementation of AggMo\n+\n+    ARGS:\n+\n+    - `opfunc` : a function that takes a single input (X), the point\n+                of a evaluation, and returns f(X) and df/dX\n+    - `x`      : the initial point\n+    - `config` : a table with configuration parameters for the optimizer\n+    - `config['learningRate']`      : learning rate\n+    - `config['learningRateDecay']` : learning rate decay\n+    - `config['weightDecay']`       : weight decay\n+    - `config['weightDecays']`      : vector of individual weight decays\n+    - `config['momentum']`          : momentum\n+    - `config['learningRates']`     : vector of individual learning rates\n+    - `state`  : a table describing the state of the optimizer; after each\n+                call the state is modified\n+    - `state['evalCounter']`        : evaluation counter (optional: 0, by default)\n+\n+    RETURN:\n+    - `x`     : the new x vector\n+    - `f(x)`  : the function, evaluated before the update\n+\n+    (Clement Farabet, 2012)\n+    \"\"\"\n+    # (0) get/update state\n+    state = state if state is not None else config\n+    lr = config.get('learningRate', 1e-3)\n+    lrd = config.get('learningRateDecay', 0)\n+    wd = config.get('weightDecay', 0)\n+    mom = config.get('momentum', [0.0, 0.9, 0.99])\n+    lrs = config.get('learningRates', None)\n+    wds = config.get('weightDecays', None)\n+    if 'evalCounter' not in state:\n+        state['evalCounter'] = 0\n+    if wd != 0 and wds is not None:\n+        raise ValueError(\"Only one of wd and wds can be specified\")\n+\n+    # (1) evaluate f(x) and df/dx\n+    fx, dfdx = opfunc(x)\n+\n+    # (2) weight decay with single or individual parameters\n+    if wd != 0:\n+        dfdx.add_(wd, x)\n+    elif wds is not None:\n+        if not state['decayParameters']:\n+            state['decayParameters'] = torch.Tensor().type_as(x).resize_as_(dfdx)\n+\n+        state['decayParameters'].copy_(wds).mul_(x)\n+        dfdx.add_(state['decayParameters'])\n+\n+    # (3) apply momentum\n+    dx = torch.zeros_like(dfdx)\n+    if len(mom) != 0:\n+        if 'dfdx' not in state:\n+            state['dfdx'] = {}\n+            for b in mom:\n+                state['dfdx'][b] = torch.zeros_like(dfdx)\n+        for b in mom:\n+            state['dfdx'][b].mul_(b).add_(dfdx)", "path": "torch/legacy/optim/aggmo.py", "position": 63, "original_position": 63, "commit_id": "7889e2d6fe1f4974aa5a9bef832a2ee69acdac23", "original_commit_id": "7889e2d6fe1f4974aa5a9bef832a2ee69acdac23", "user": {"login": "ishaqadenali", "id": 31038639, "node_id": "MDQ6VXNlcjMxMDM4NjM5", "avatar_url": "https://avatars1.githubusercontent.com/u/31038639?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ishaqadenali", "html_url": "https://github.com/ishaqadenali", "followers_url": "https://api.github.com/users/ishaqadenali/followers", "following_url": "https://api.github.com/users/ishaqadenali/following{/other_user}", "gists_url": "https://api.github.com/users/ishaqadenali/gists{/gist_id}", "starred_url": "https://api.github.com/users/ishaqadenali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ishaqadenali/subscriptions", "organizations_url": "https://api.github.com/users/ishaqadenali/orgs", "repos_url": "https://api.github.com/users/ishaqadenali/repos", "events_url": "https://api.github.com/users/ishaqadenali/events{/privacy}", "received_events_url": "https://api.github.com/users/ishaqadenali/received_events", "type": "User", "site_admin": false}, "body": "Hi @AtheMathmo, Firstly, great paper, it was super fun to read! Somehow I ended up skimming through this code... Wondering why you don't add -dfdx and then omit the negative sign on the clr variable below.\r\n\r\nFrom what I see you're doing:\r\nV_t = beta * V_t-1 + grad(f(x_t-1))\r\nx_t = x+t-1 - gamma_t * V_t", "created_at": "2018-04-20T20:35:20Z", "updated_at": "2018-11-23T15:42:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/6270#discussion_r183162923", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6270", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183162923"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6270#discussion_r183162923"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6270"}}, "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10150986\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/AtheMathmo\">@AtheMathmo</a>, Firstly, great paper, it was super fun to read! Somehow I ended up skimming through this code... Wondering why you don't add -dfdx and then omit the negative sign on the clr variable below.</p>\n<p>From what I see you're doing:<br>\nV_t = beta * V_t-1 + grad(f(x_t-1))<br>\nx_t = x+t-1 - gamma_t * V_t</p>", "body_text": "Hi @AtheMathmo, Firstly, great paper, it was super fun to read! Somehow I ended up skimming through this code... Wondering why you don't add -dfdx and then omit the negative sign on the clr variable below.\nFrom what I see you're doing:\nV_t = beta * V_t-1 + grad(f(x_t-1))\nx_t = x+t-1 - gamma_t * V_t"}