{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183191747", "pull_request_review_id": 114153622, "id": 183191747, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MzE5MTc0Nw==", "diff_hunk": "@@ -0,0 +1,84 @@\n+import torch\n+\n+\n+def aggmo(opfunc, x, config, state=None):\n+    \"\"\"A plain implementation of AggMo\n+\n+    ARGS:\n+\n+    - `opfunc` : a function that takes a single input (X), the point\n+                of a evaluation, and returns f(X) and df/dX\n+    - `x`      : the initial point\n+    - `config` : a table with configuration parameters for the optimizer\n+    - `config['learningRate']`      : learning rate\n+    - `config['learningRateDecay']` : learning rate decay\n+    - `config['weightDecay']`       : weight decay\n+    - `config['weightDecays']`      : vector of individual weight decays\n+    - `config['momentum']`          : momentum\n+    - `config['learningRates']`     : vector of individual learning rates\n+    - `state`  : a table describing the state of the optimizer; after each\n+                call the state is modified\n+    - `state['evalCounter']`        : evaluation counter (optional: 0, by default)\n+\n+    RETURN:\n+    - `x`     : the new x vector\n+    - `f(x)`  : the function, evaluated before the update\n+\n+    (Clement Farabet, 2012)\n+    \"\"\"\n+    # (0) get/update state\n+    state = state if state is not None else config\n+    lr = config.get('learningRate', 1e-3)\n+    lrd = config.get('learningRateDecay', 0)\n+    wd = config.get('weightDecay', 0)\n+    mom = config.get('momentum', [0.0, 0.9, 0.99])\n+    lrs = config.get('learningRates', None)\n+    wds = config.get('weightDecays', None)\n+    if 'evalCounter' not in state:\n+        state['evalCounter'] = 0\n+    if wd != 0 and wds is not None:\n+        raise ValueError(\"Only one of wd and wds can be specified\")\n+\n+    # (1) evaluate f(x) and df/dx\n+    fx, dfdx = opfunc(x)\n+\n+    # (2) weight decay with single or individual parameters\n+    if wd != 0:\n+        dfdx.add_(wd, x)\n+    elif wds is not None:\n+        if not state['decayParameters']:\n+            state['decayParameters'] = torch.Tensor().type_as(x).resize_as_(dfdx)\n+\n+        state['decayParameters'].copy_(wds).mul_(x)\n+        dfdx.add_(state['decayParameters'])\n+\n+    # (3) apply momentum\n+    dx = torch.zeros_like(dfdx)\n+    if len(mom) != 0:\n+        if 'dfdx' not in state:\n+            state['dfdx'] = {}\n+            for b in mom:\n+                state['dfdx'][b] = torch.zeros_like(dfdx)\n+        for b in mom:\n+            state['dfdx'][b].mul_(b).add_(dfdx)", "path": "torch/legacy/optim/aggmo.py", "position": 63, "original_position": 63, "commit_id": "7889e2d6fe1f4974aa5a9bef832a2ee69acdac23", "original_commit_id": "7889e2d6fe1f4974aa5a9bef832a2ee69acdac23", "user": {"login": "AtheMathmo", "id": 10150986, "node_id": "MDQ6VXNlcjEwMTUwOTg2", "avatar_url": "https://avatars0.githubusercontent.com/u/10150986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AtheMathmo", "html_url": "https://github.com/AtheMathmo", "followers_url": "https://api.github.com/users/AtheMathmo/followers", "following_url": "https://api.github.com/users/AtheMathmo/following{/other_user}", "gists_url": "https://api.github.com/users/AtheMathmo/gists{/gist_id}", "starred_url": "https://api.github.com/users/AtheMathmo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AtheMathmo/subscriptions", "organizations_url": "https://api.github.com/users/AtheMathmo/orgs", "repos_url": "https://api.github.com/users/AtheMathmo/repos", "events_url": "https://api.github.com/users/AtheMathmo/events{/privacy}", "received_events_url": "https://api.github.com/users/AtheMathmo/received_events", "type": "User", "site_admin": false}, "body": "I did this only to be consistent with the code here: https://github.com/pytorch/pytorch/blob/master/torch/legacy/optim/sgd.py#L62-L84\r\n\r\nI'm not sure if there is any deeper meaning to this!", "created_at": "2018-04-21T00:03:06Z", "updated_at": "2018-11-23T15:42:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/6270#discussion_r183191747", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6270", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183191747"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6270#discussion_r183191747"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6270"}}, "body_html": "<p>I did this only to be consistent with the code here: <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/legacy/optim/sgd.py#L62-L84\">https://github.com/pytorch/pytorch/blob/master/torch/legacy/optim/sgd.py#L62-L84</a></p>\n<p>I'm not sure if there is any deeper meaning to this!</p>", "body_text": "I did this only to be consistent with the code here: https://github.com/pytorch/pytorch/blob/master/torch/legacy/optim/sgd.py#L62-L84\nI'm not sure if there is any deeper meaning to this!", "in_reply_to_id": 183162923}