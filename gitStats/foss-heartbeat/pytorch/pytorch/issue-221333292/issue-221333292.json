{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1244", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1244/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1244/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1244/events", "html_url": "https://github.com/pytorch/pytorch/issues/1244", "id": 221333292, "node_id": "MDU6SXNzdWUyMjEzMzMyOTI=", "number": 1244, "title": "Better way to update LR etc in optim.Optimizer", "user": {"login": "chsasank", "id": 9305875, "node_id": "MDQ6VXNlcjkzMDU4NzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9305875?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chsasank", "html_url": "https://github.com/chsasank", "followers_url": "https://api.github.com/users/chsasank/followers", "following_url": "https://api.github.com/users/chsasank/following{/other_user}", "gists_url": "https://api.github.com/users/chsasank/gists{/gist_id}", "starred_url": "https://api.github.com/users/chsasank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chsasank/subscriptions", "organizations_url": "https://api.github.com/users/chsasank/orgs", "repos_url": "https://api.github.com/users/chsasank/repos", "events_url": "https://api.github.com/users/chsasank/events{/privacy}", "received_events_url": "https://api.github.com/users/chsasank/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-04-12T17:22:15Z", "updated_at": "2017-05-27T18:02:06Z", "closed_at": "2017-05-27T18:02:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Currently to change LR of <code>optimizer</code>, we have to do this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> param_group <span class=\"pl-k\">in</span> optimizer.param_groups:\n        param_group[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>lr<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> lr</pre></div>\n<p>See <a href=\"https://discuss.pytorch.org/t/adaptive-learning-rate/320\" rel=\"nofollow\">here</a> for more discussion</p>\n<p>Although this works, this looks as if the user is messing with the internals of <code>optimizer</code>. Therefore I propose to add a member function to <code>optim.Optimizer</code> something like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">update_params</span>(<span class=\"pl-smi\">new_params</span>):\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(new_params, <span class=\"pl-c1\">dict</span>):\n        <span class=\"pl-k\">for</span> param_group <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.param_groups:\n            param_group.update(new_params)\n    <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">isinstance</span>(new_params, <span class=\"pl-c1\">list</span>):\n        <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">len</span>(new_params) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.param_groups)\n        <span class=\"pl-k\">for</span> param_group, new_pram <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(<span class=\"pl-c1\">self</span>.param_groups, new_params):\n            param_group.update(new_pram)\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">TypeError</span></pre></div>\n<p>Sasank.</p>", "body_text": "Currently to change LR of optimizer, we have to do this:\nfor param_group in optimizer.param_groups:\n        param_group['lr'] = lr\nSee here for more discussion\nAlthough this works, this looks as if the user is messing with the internals of optimizer. Therefore I propose to add a member function to optim.Optimizer something like this:\ndef update_params(new_params):\n    if isinstance(new_params, dict):\n        for param_group in self.param_groups:\n            param_group.update(new_params)\n    elif isinstance(new_params, list):\n        assert len(new_params) == len(self.param_groups)\n        for param_group, new_pram in zip(self.param_groups, new_params):\n            param_group.update(new_pram)\n    else:\n        raise TypeError\nSasank.", "body": "Currently to change LR of `optimizer`, we have to do this:\r\n\r\n```python\r\nfor param_group in optimizer.param_groups:\r\n        param_group['lr'] = lr\r\n```\r\n\r\nSee [here](https://discuss.pytorch.org/t/adaptive-learning-rate/320) for more discussion\r\n\r\nAlthough this works, this looks as if the user is messing with the internals of `optimizer`. Therefore I propose to add a member function to `optim.Optimizer` something like this:\r\n\r\n```python\r\ndef update_params(new_params):\r\n    if isinstance(new_params, dict):\r\n        for param_group in self.param_groups:\r\n            param_group.update(new_params)\r\n    elif isinstance(new_params, list):\r\n        assert len(new_params) == len(self.param_groups)\r\n        for param_group, new_pram in zip(self.param_groups, new_params):\r\n            param_group.update(new_pram)\r\n    else:\r\n        raise TypeError\r\n```\r\n\r\nSasank."}