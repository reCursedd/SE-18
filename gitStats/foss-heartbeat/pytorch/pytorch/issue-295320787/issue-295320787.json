{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5122", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5122/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5122/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5122/events", "html_url": "https://github.com/pytorch/pytorch/pull/5122", "id": 295320787, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY3ODMwMjQ0", "number": 5122, "title": "Add fp16 testcases in test_cuda", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-07T22:44:12Z", "updated_at": "2018-11-23T15:39:46Z", "closed_at": "2018-02-21T13:35:29Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5122", "html_url": "https://github.com/pytorch/pytorch/pull/5122", "diff_url": "https://github.com/pytorch/pytorch/pull/5122.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5122.patch"}, "body_html": "<ul>\n<li>is_float() is checking on type instead of tensor. It always returned False before.</li>\n<li>Add fp16 for all basic math operations in tests array, except for pow-3.</li>\n<li>Use torch.FloatTensor as cpu baseline for torch.cuda.HalfTensor and relax the precision for some testcases.</li>\n<li>Some input method requires operation not supported by torch.HalfTensor, workaround by convert it to float() and back to half().</li>\n</ul>", "body_text": "is_float() is checking on type instead of tensor. It always returned False before.\nAdd fp16 for all basic math operations in tests array, except for pow-3.\nUse torch.FloatTensor as cpu baseline for torch.cuda.HalfTensor and relax the precision for some testcases.\nSome input method requires operation not supported by torch.HalfTensor, workaround by convert it to float() and back to half().", "body": "* is_float() is checking on type instead of tensor. It always returned False before.\r\n* Add fp16 for all basic math operations in tests array, except for pow-3.\r\n* Use torch.FloatTensor as cpu baseline for torch.cuda.HalfTensor and relax the precision for some testcases.\r\n* Some input method requires operation not supported by torch.HalfTensor, workaround by convert it to float() and back to half().  "}