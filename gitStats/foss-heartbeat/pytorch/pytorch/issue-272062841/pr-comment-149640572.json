{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149640572", "pull_request_review_id": 75062847, "id": 149640572, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0OTY0MDU3Mg==", "diff_hunk": "@@ -40,18 +89,35 @@ struct GraphFuser {\n   bool isCuda(Node * node) {\n     return node->type()->expect<TensorType>()->device() != -1;\n   }\n-  // TODO: the fusion compiler needs to know how to handle 'alpha'\n-  // and other attributes in code generation for us to be able to fuse them\n-  // then it is safe to remove the !hasSpecialAlpha check\n-  bool hasSpecialAlpha(Node * node) {\n-    if(!node->hasAttribute(kalpha))\n+  // TODO: the fusion compiler has a lot of float-specific codegen\n+  // so for now we only consider nodes that operate on floating point numbers\n+  bool hasFloatType(Node * node) {\n+    if(!node->hasType()) {\n+      return false;\n+    }\n+    if(auto tt = node->type()->cast<TensorType>()) {\n+      return tt->scalarType() != at::kFloat;\n+    } else {\n       return false;\n-    return at::Scalar(node->t(kalpha)).toDouble() != 1;\n+    }\n+  }\n+  bool hasFloatIO(Node * node) {\n+    for(auto & o : node->outputs()) {\n+      if(!hasFloatType(o)) {\n+        return false;\n+      }\n+    }\n+    for(auto & o : node->inputs()) {", "path": "torch/csrc/jit/passes/graph_fuser.cpp", "position": 97, "original_position": 97, "commit_id": "6c888bc4ce1d0bc4d47cc9fb154a881c00f0002e", "original_commit_id": "16f6853f2e8f61afe3d11d0697436878767cde0c", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "This makes it seem like this predicate works on multiple return nodes (not tested) but isCuda was not updated appropriately.", "created_at": "2017-11-08T11:18:20Z", "updated_at": "2018-11-23T15:36:10Z", "html_url": "https://github.com/pytorch/pytorch/pull/3559#discussion_r149640572", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3559", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149640572"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3559#discussion_r149640572"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3559"}}, "body_html": "<p>This makes it seem like this predicate works on multiple return nodes (not tested) but isCuda was not updated appropriately.</p>", "body_text": "This makes it seem like this predicate works on multiple return nodes (not tested) but isCuda was not updated appropriately."}