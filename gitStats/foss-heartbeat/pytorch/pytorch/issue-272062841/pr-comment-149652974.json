{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149652974", "pull_request_review_id": 75078665, "id": 149652974, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0OTY1Mjk3NA==", "diff_hunk": "@@ -162,6 +209,31 @@ const char * scalarTypeName(at::ScalarType type) {\n   }\n }\n \n+std::string encodeRHS(Node * n) {\n+  TemplateEnv env;\n+  size_t i = 0;\n+  for(auto in : n->inputs()) {\n+    env.s(std::to_string(i++),nodeName(in));\n+  }\n+  // ops like div have a / b or a / 2 with the constant having the attribute other\n+  // so we add other as an input if it is present\n+  // 'pow' is the same but uses exponent as the attribute, so we handle that here as well\n+  if(n->hasAttribute(kother) || n->hasAttribute(kexponent)) {\n+    env.s(std::to_string(i), scalarValue(n->t(kother)));\n+  }", "path": "torch/csrc/jit/fusion_compiler.cpp", "position": 106, "original_position": 106, "commit_id": "6c888bc4ce1d0bc4d47cc9fb154a881c00f0002e", "original_commit_id": "16f6853f2e8f61afe3d11d0697436878767cde0c", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Well, we currently lose the names of the inputs when we recordTrace (the tracing code doesn't save it), but if we somehow preserved it, you could explicitly name the parameters with self/other and then the special case here wouldn't be necessary, because the tensor input name and the attribute name coincide. But since we don't have that info... this seems reasonably clear to me.", "created_at": "2017-11-08T12:22:33Z", "updated_at": "2018-11-23T15:36:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/3559#discussion_r149652974", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3559", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/149652974"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3559#discussion_r149652974"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3559"}}, "body_html": "<p>Well, we currently lose the names of the inputs when we recordTrace (the tracing code doesn't save it), but if we somehow preserved it, you could explicitly name the parameters with self/other and then the special case here wouldn't be necessary, because the tensor input name and the attribute name coincide. But since we don't have that info... this seems reasonably clear to me.</p>", "body_text": "Well, we currently lose the names of the inputs when we recordTrace (the tracing code doesn't save it), but if we somehow preserved it, you could explicitly name the parameters with self/other and then the special case here wouldn't be necessary, because the tensor input name and the attribute name coincide. But since we don't have that info... this seems reasonably clear to me.", "in_reply_to_id": 149646388}