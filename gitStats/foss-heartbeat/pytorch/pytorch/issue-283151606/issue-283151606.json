{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4246", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4246/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4246/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4246/events", "html_url": "https://github.com/pytorch/pytorch/issues/4246", "id": 283151606, "node_id": "MDU6SXNzdWUyODMxNTE2MDY=", "number": 4246, "title": "pytorch distrubuted example", "user": {"login": "gmyofustc", "id": 7859276, "node_id": "MDQ6VXNlcjc4NTkyNzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7859276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gmyofustc", "html_url": "https://github.com/gmyofustc", "followers_url": "https://api.github.com/users/gmyofustc/followers", "following_url": "https://api.github.com/users/gmyofustc/following{/other_user}", "gists_url": "https://api.github.com/users/gmyofustc/gists{/gist_id}", "starred_url": "https://api.github.com/users/gmyofustc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gmyofustc/subscriptions", "organizations_url": "https://api.github.com/users/gmyofustc/orgs", "repos_url": "https://api.github.com/users/gmyofustc/repos", "events_url": "https://api.github.com/users/gmyofustc/events{/privacy}", "received_events_url": "https://api.github.com/users/gmyofustc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-19T09:08:51Z", "updated_at": "2017-12-20T13:48:50Z", "closed_at": "2017-12-20T13:48:50Z", "author_association": "NONE", "body_html": "<p>Hi, everyone<br>\nI'm use pytorch0.3, and i want to use pytorch.distributed module for training models on multi-nodes. I follow the tutorial in <a href=\"http://pytorch.org/docs/master/distributed.html\" rel=\"nofollow\">here</a>, this really helps me understanding the distributed training using pytorch.<br>\nHowever, this tutorial does not tell me how can i init my program across multi-nodes, and to the best of my knowledge, pytorch does support distributed training on multi-nodes, and i believe the configuration will be very simple, i'll be glad to any help, thanks.</p>", "body_text": "Hi, everyone\nI'm use pytorch0.3, and i want to use pytorch.distributed module for training models on multi-nodes. I follow the tutorial in here, this really helps me understanding the distributed training using pytorch.\nHowever, this tutorial does not tell me how can i init my program across multi-nodes, and to the best of my knowledge, pytorch does support distributed training on multi-nodes, and i believe the configuration will be very simple, i'll be glad to any help, thanks.", "body": "Hi, everyone\r\n     I'm use pytorch0.3, and i want to use pytorch.distributed module for training models on multi-nodes. I follow the tutorial in [here](http://pytorch.org/docs/master/distributed.html), this really helps me understanding the distributed training using pytorch.\r\n    However, this tutorial does not tell me how can i init my program across multi-nodes, and to the best of my knowledge, pytorch does support distributed training on multi-nodes, and i believe the configuration will be very simple, i'll be glad to any help, thanks."}