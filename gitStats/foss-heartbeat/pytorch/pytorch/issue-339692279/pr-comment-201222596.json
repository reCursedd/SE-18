{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201222596", "pull_request_review_id": 135680326, "id": 201222596, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTIyMjU5Ng==", "diff_hunk": "@@ -0,0 +1,128 @@\n+#include <ATen/ATen.h>\n+#include <ATen/Error.h>\n+\n+#include <algorithm>\n+#include <cmath>\n+#include <tuple>\n+\n+namespace at {\n+namespace native {\n+namespace {\n+struct Fan {\n+  explicit Fan(Tensor& self) {\n+    const auto dimensions = self.ndimension();\n+    AT_CHECK(\n+        dimensions >= 2,\n+        \"Fan in and fan out can not be computed for self with less than 2 dimensions\");\n+\n+    if (dimensions == 2) {\n+      in = self.size(1);\n+      out = self.size(0);\n+    } else {\n+      in = self.size(1) * self[0][0].numel();\n+      out = self.size(0) * self[0][0].numel();\n+    }\n+  }\n+\n+  int64_t in;\n+  int64_t out;\n+};\n+} // namespace\n+\n+Tensor& dirac_(Tensor& self) {\n+  AT_CHECK(\n+      self.ndimension() >= 3 && self.ndimension() <= 5,\n+      \"Only tensors with 3, 4, or 5 dimensions are supported\");\n+\n+  const auto sizes = self.sizes();\n+  const auto min_dim = std::min(sizes[0], sizes[1]);\n+\n+  self.zero_();\n+  for (int64_t d = 0; d < min_dim; ++d) {\n+    switch (self.ndimension()) {\n+      case 3: // Temporal convolution\n+        self[d][d][sizes[2] / 2] = 1;\n+        break;\n+      case 4: // Spatial convolution\n+        self[d][d][sizes[2] / 2][sizes[3] / 2] = 1;\n+        break;\n+      case 5: // Volumetric convolution\n+        self[d][d][sizes[2] / 2][sizes[3] / 2][sizes[4] / 2] = 1;\n+        break;\n+    }\n+  }\n+\n+  return self;\n+}\n+\n+Tensor& eye_(Tensor& self) {\n+  AT_CHECK(\n+      self.ndimension() == 2, \"Only tensors with 2 dimensions are supported\");\n+  return eye_out(self, self.size(0), self.size(1));\n+}\n+\n+Tensor& orthogonal_(Tensor& self, double gain) {\n+  AT_CHECK(\n+      self.ndimension() >= 2,\n+      \"Only tensors with 2 or more dimensions are supported\");\n+\n+  const auto rows = self.size(0);\n+  const auto columns = self.size(1);\n+  auto flattened = at::randn({rows, columns});\n+\n+  if (rows < columns) {\n+    flattened.t_();\n+  }\n+\n+  // Compute the qr factorization\n+  Tensor q, r;\n+  std::tie(q, r) = at::qr(flattened);\n+  // Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n+  auto d = at::diag(r, 0);\n+  auto ph = d.sign();\n+  q *= ph;\n+\n+  if (rows < columns) {\n+    q.t_();\n+  }\n+\n+  self.view_as(q).copy_(q);\n+  self.mul_(gain);\n+\n+  return self;\n+}\n+\n+Tensor& sparse_(Tensor& self, double sparsity, double std) {\n+  AT_CHECK(\n+      self.ndimension() == 2, \"Only tensors with 2 dimensions are supported\");\n+\n+  const auto rows = self.size(0);\n+  const auto columns = self.size(1);\n+  const int64_t num_zeros = std::ceil(sparsity * rows);\n+  self.normal_(0, std);\n+  for (size_t column = 0; column < columns; ++column) {\n+    auto row_indices = at::randperm(rows, self.options().dtype(kLong));\n+    auto zero_indices =\n+        row_indices.slice(/*dim=*/0, /*start=*/0, /*end=*/num_zeros);\n+    self.index_select(/*dim=*/0, zero_indices).select(/*dim=*/1, column) = 0;", "path": "aten/src/ATen/native/Init.cpp", "position": null, "original_position": 107, "commit_id": "a78f62fde100bc21543e22781f98c16db06a7a69", "original_commit_id": "8531dafc8379002a2cae8028cdca4b26f7b7edcd", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "I think you want to use `index` or `index_put` for advanced indexing", "created_at": "2018-07-10T05:44:06Z", "updated_at": "2018-11-23T15:47:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/9295#discussion_r201222596", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9295", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201222596"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9295#discussion_r201222596"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9295"}}, "body_html": "<p>I think you want to use <code>index</code> or <code>index_put</code> for advanced indexing</p>", "body_text": "I think you want to use index or index_put for advanced indexing"}