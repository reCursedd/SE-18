{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2003", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2003/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2003/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2003/events", "html_url": "https://github.com/pytorch/pytorch/issues/2003", "id": 241235871, "node_id": "MDU6SXNzdWUyNDEyMzU4NzE=", "number": 2003, "title": "Handling of no/zero gradients in cpp Function", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-07-07T11:25:39Z", "updated_at": "2017-07-25T02:40:52Z", "closed_at": "2017-07-25T02:40:52Z", "author_association": "COLLABORATOR", "body_html": "<p>The problem here is what should be the returned value of a cpp <code>Function</code> when the output was ind\u00e9pendant of the inputs or no gradient was backpropagated for this specific output (it was unused later in the graph).</p>\n<p>For python <code>Function</code>, the wrapper takes care, when no gradient exist for an output to create a full dense tensor filled with <code>0</code>s of the size of the output. The same happens when a python <code>Function</code> returns <code>None</code> for a given parameter, the next <code>Function</code> will actually see a dense tensor of <code>0</code>s.</p>\n<p>In cpp, such wrapper does not exist, so if an output was not used, the corresponding element in the input <code>variable_list</code> will be and empty <code>shared_ptr</code>. And the cpp <code>Function</code> should handle empty <code>shared_ptr</code> as if it was a dense Tensor of <code>0</code>s (possibly skipping a lot of computations as in <code>ConvBackwardBackward</code>).<br>\nRight now, some <code>Function</code> do handle this case properly, but not all of them (leading for example to <a href=\"https://discuss.pytorch.org/t/error-using-autograd-grad-with-2-directly-following-conv2d-layers/4661\" rel=\"nofollow\">this</a> issue in the forum).</p>\n<p>The question is: are we ok with the above design saying that any input which is an empty <code>shared_ptr</code> is equivalent to a tensor of all <code>0</code>s ? And all cpp <code>Function</code>s should handle empty <code>shared_ptr</code> as input ?</p>", "body_text": "The problem here is what should be the returned value of a cpp Function when the output was ind\u00e9pendant of the inputs or no gradient was backpropagated for this specific output (it was unused later in the graph).\nFor python Function, the wrapper takes care, when no gradient exist for an output to create a full dense tensor filled with 0s of the size of the output. The same happens when a python Function returns None for a given parameter, the next Function will actually see a dense tensor of 0s.\nIn cpp, such wrapper does not exist, so if an output was not used, the corresponding element in the input variable_list will be and empty shared_ptr. And the cpp Function should handle empty shared_ptr as if it was a dense Tensor of 0s (possibly skipping a lot of computations as in ConvBackwardBackward).\nRight now, some Function do handle this case properly, but not all of them (leading for example to this issue in the forum).\nThe question is: are we ok with the above design saying that any input which is an empty shared_ptr is equivalent to a tensor of all 0s ? And all cpp Functions should handle empty shared_ptr as input ?", "body": "The problem here is what should be the returned value of a cpp `Function` when the output was ind\u00e9pendant of the inputs or no gradient was backpropagated for this specific output (it was unused later in the graph).\r\n\r\nFor python `Function`, the wrapper takes care, when no gradient exist for an output to create a full dense tensor filled with `0`s of the size of the output. The same happens when a python `Function` returns `None` for a given parameter, the next `Function` will actually see a dense tensor of `0`s.\r\n\r\nIn cpp, such wrapper does not exist, so if an output was not used, the corresponding element in the input `variable_list` will be and empty `shared_ptr`. And the cpp `Function` should handle empty `shared_ptr` as if it was a dense Tensor of `0`s (possibly skipping a lot of computations as in `ConvBackwardBackward`).\r\nRight now, some `Function` do handle this case properly, but not all of them (leading for example to [this](https://discuss.pytorch.org/t/error-using-autograd-grad-with-2-directly-following-conv2d-layers/4661) issue in the forum).\r\n\r\nThe question is: are we ok with the above design saying that any input which is an empty `shared_ptr` is equivalent to a tensor of all `0`s ? And all cpp `Function`s should handle empty `shared_ptr` as input ?"}