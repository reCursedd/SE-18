{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/313739605", "html_url": "https://github.com/pytorch/pytorch/issues/2003#issuecomment-313739605", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2003", "id": 313739605, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzczOTYwNQ==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T17:08:49Z", "updated_at": "2017-07-07T17:08:49Z", "author_association": "COLLABORATOR", "body_html": "<p>Ok,</p>\n<p>One question I would have is for simple functions that are not implemented in pure autograd like <a href=\"https://github.com/pytorch/pytorch/blob/ebdec9a837074a303fd5ffb6f319cd593955becc/torch/csrc/autograd/functions/tensor.cpp#L14-L24\">this</a> one. what should we do?<br>\nShould we adapt the <code>wrap_outputs</code> function such that if a tensor does not exist, it places an empty <code>shared_ptr</code> in the <code>variable_list</code> ?<br>\nIf this goes back to python side, this will be properly translated to a <code>None</code> so that the python will work as expected.</p>", "body_text": "Ok,\nOne question I would have is for simple functions that are not implemented in pure autograd like this one. what should we do?\nShould we adapt the wrap_outputs function such that if a tensor does not exist, it places an empty shared_ptr in the variable_list ?\nIf this goes back to python side, this will be properly translated to a None so that the python will work as expected.", "body": "Ok,\r\n\r\nOne question I would have is for simple functions that are not implemented in pure autograd like [this](https://github.com/pytorch/pytorch/blob/ebdec9a837074a303fd5ffb6f319cd593955becc/torch/csrc/autograd/functions/tensor.cpp#L14-L24) one. what should we do?\r\nShould we adapt the `wrap_outputs` function such that if a tensor does not exist, it places an empty `shared_ptr` in the `variable_list` ?\r\nIf this goes back to python side, this will be properly translated to a `None` so that the python will work as expected."}