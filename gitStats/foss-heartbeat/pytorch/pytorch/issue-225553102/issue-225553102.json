{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1427", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1427/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1427/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1427/events", "html_url": "https://github.com/pytorch/pytorch/issues/1427", "id": 225553102, "node_id": "MDU6SXNzdWUyMjU1NTMxMDI=", "number": 1427, "title": "th.norm and th.renorm `dim` arg is switched?", "user": {"login": "ncullen93", "id": 13004360, "node_id": "MDQ6VXNlcjEzMDA0MzYw", "avatar_url": "https://avatars0.githubusercontent.com/u/13004360?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ncullen93", "html_url": "https://github.com/ncullen93", "followers_url": "https://api.github.com/users/ncullen93/followers", "following_url": "https://api.github.com/users/ncullen93/following{/other_user}", "gists_url": "https://api.github.com/users/ncullen93/gists{/gist_id}", "starred_url": "https://api.github.com/users/ncullen93/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ncullen93/subscriptions", "organizations_url": "https://api.github.com/users/ncullen93/orgs", "repos_url": "https://api.github.com/users/ncullen93/repos", "events_url": "https://api.github.com/users/ncullen93/events{/privacy}", "received_events_url": "https://api.github.com/users/ncullen93/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-01T23:26:54Z", "updated_at": "2017-05-07T18:36:16Z", "closed_at": "2017-05-07T18:36:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It seems that to renormalize a tensor along the 1st axis, you have to use <code>th.renorm</code> along the 0th axis. Is this expected behavior?</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> th.randn(<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">10</span>)\n<span class=\"pl-c1\">print</span>(th.norm(x,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.8096</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.1135</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2.9162</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2.0999</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.5506</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>[torch.FloatTensor of size 5x1]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> doesnt work</span>\n<span class=\"pl-c1\">print</span>(th.norm(th.renorm(x,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>),<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.7102</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.0592</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2.7382</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2.0993</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.5030</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>[torch.FloatTensor of size 5x1]</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> does work</span>\n<span class=\"pl-c1\">print</span>(th.norm(th.renorm(x,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">3</span>),<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.0000</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.0000</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2.9162</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2.0999</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 3.0000</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>[torch.FloatTensor of size 5x1]</span></pre></div>", "body_text": "It seems that to renormalize a tensor along the 1st axis, you have to use th.renorm along the 0th axis. Is this expected behavior?\nx = th.randn(5,10)\nprint(th.norm(x,2,1))\n\n# 3.8096\n# 3.1135\n# 2.9162\n# 2.0999\n# 3.5506\n#[torch.FloatTensor of size 5x1]\n\n# doesnt work\nprint(th.norm(th.renorm(x,2,1,3),2,1))\n# 3.7102\n# 3.0592\n# 2.7382\n# 2.0993\n# 3.5030\n#[torch.FloatTensor of size 5x1]\n\n# does work\nprint(th.norm(th.renorm(x,2,0,3),2,1))\n# 3.0000\n# 3.0000\n# 2.9162\n# 2.0999\n# 3.0000\n#[torch.FloatTensor of size 5x1]", "body": "It seems that to renormalize a tensor along the 1st axis, you have to use `th.renorm` along the 0th axis. Is this expected behavior?\r\n\r\n```python\r\nx = th.randn(5,10)\r\nprint(th.norm(x,2,1))\r\n\r\n# 3.8096\r\n# 3.1135\r\n# 2.9162\r\n# 2.0999\r\n# 3.5506\r\n#[torch.FloatTensor of size 5x1]\r\n\r\n# doesnt work\r\nprint(th.norm(th.renorm(x,2,1,3),2,1))\r\n# 3.7102\r\n# 3.0592\r\n# 2.7382\r\n# 2.0993\r\n# 3.5030\r\n#[torch.FloatTensor of size 5x1]\r\n\r\n# does work\r\nprint(th.norm(th.renorm(x,2,0,3),2,1))\r\n# 3.0000\r\n# 3.0000\r\n# 2.9162\r\n# 2.0999\r\n# 3.0000\r\n#[torch.FloatTensor of size 5x1]\r\n```"}