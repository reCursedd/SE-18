{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157915561", "pull_request_review_id": 84638404, "id": 157915561, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzkxNTU2MQ==", "diff_hunk": "@@ -959,6 +979,25 @@ def fn(x, y):\n         self.assertTrue(\"hits: 100\" in info_str)\n         self.assertTrue(\"stage 1\" in info_str)\n \n+    def test_inplace_copy(self):\n+        x = Variable(torch.randn(4, 4), requires_grad=True)\n+        def f(x):\n+            out = Variable(torch.zeros(x.size()))\n+            out.copy_(x)\n+            return out", "path": "test/test_jit.py", "position": null, "original_position": 43, "commit_id": "bfede08f9abf8f151e4fc64a7dfacf77c23dda69", "original_commit_id": "af2b4fafbcea61e342d85c29e0f8159009ce7516", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Oh yeah, this PR was between me and Zach so I forgot to post some text.\r\n\r\nSo basically, the problem is that the implementation of backwards for view functions does exactly this idiom: it first creates a zero tensor, and then writes in the gradient inplace into the portion appropriate for the original view. So all of these functions are not traceable when the tracer can't handle this.\r\n\r\nOTOH, I feel that the tracer *should* be able to handle this idiom, esp now that inplace updates on views are supported. Those won't be traced correctly at all today.", "created_at": "2017-12-20T01:00:26Z", "updated_at": "2018-11-23T15:37:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/4249#discussion_r157915561", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4249", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157915561"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4249#discussion_r157915561"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4249"}}, "body_html": "<p>Oh yeah, this PR was between me and Zach so I forgot to post some text.</p>\n<p>So basically, the problem is that the implementation of backwards for view functions does exactly this idiom: it first creates a zero tensor, and then writes in the gradient inplace into the portion appropriate for the original view. So all of these functions are not traceable when the tracer can't handle this.</p>\n<p>OTOH, I feel that the tracer <em>should</em> be able to handle this idiom, esp now that inplace updates on views are supported. Those won't be traced correctly at all today.</p>", "body_text": "Oh yeah, this PR was between me and Zach so I forgot to post some text.\nSo basically, the problem is that the implementation of backwards for view functions does exactly this idiom: it first creates a zero tensor, and then writes in the gradient inplace into the portion appropriate for the original view. So all of these functions are not traceable when the tracer can't handle this.\nOTOH, I feel that the tracer should be able to handle this idiom, esp now that inplace updates on views are supported. Those won't be traced correctly at all today.", "in_reply_to_id": 157810493}