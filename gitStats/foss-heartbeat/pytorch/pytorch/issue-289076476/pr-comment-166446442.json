{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166446442", "pull_request_review_id": 94509610, "id": 166446442, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjQ0NjQ0Mg==", "diff_hunk": "@@ -167,6 +172,66 @@ void fuseTransposeIntoGemm(std::shared_ptr<Graph>& graph) {\n   }\n }\n \n+void pushPackingPastRnn(std::shared_ptr<Graph>& graph) {\n+  for (auto it = graph->nodes().begin(); it != graph->nodes().end(); ++it) {\n+    auto* n = *it;\n+\n+    if (n->kind() != kPackPadded) {\n+      continue;\n+    }\n+    if (n->outputs()[0]->uses().size() != 1) {\n+      // For now, only handle the case where there is one consumer.\n+      continue;\n+    }\n+    Node * rnn = n->outputs()[0]->uses()[0].user;\n+    if (!isRNN(rnn)) {\n+      continue;\n+    }\n+\n+    // remove PackPadded from in front of the RNN\n+    n->outputs()[0]->replaceFirstUseWith(n->inputs()[0]);", "path": "torch/csrc/jit/passes/onnx/peephole.cpp", "position": null, "original_position": 33, "commit_id": "c9c0a5924fdbb682f43d1e499db132b5babdba72", "original_commit_id": "a617fe07a346a29e64ce35e7e75f1ffc4acf47f7", "user": {"login": "anderspapitto", "id": 1388690, "node_id": "MDQ6VXNlcjEzODg2OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1388690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anderspapitto", "html_url": "https://github.com/anderspapitto", "followers_url": "https://api.github.com/users/anderspapitto/followers", "following_url": "https://api.github.com/users/anderspapitto/following{/other_user}", "gists_url": "https://api.github.com/users/anderspapitto/gists{/gist_id}", "starred_url": "https://api.github.com/users/anderspapitto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anderspapitto/subscriptions", "organizations_url": "https://api.github.com/users/anderspapitto/orgs", "repos_url": "https://api.github.com/users/anderspapitto/repos", "events_url": "https://api.github.com/users/anderspapitto/events{/privacy}", "received_events_url": "https://api.github.com/users/anderspapitto/received_events", "type": "User", "site_admin": false}, "body": "(also answering the next comment here)\r\n\r\nthere is only one use of `n->outputs()[0]` (enforced by line 198), so yes I will switch this to replaceAllUsesWith.\r\n\r\nthere are actually multiple consumers of `n->outputs()[1]`, and this is fine. The same blob feeds into each level of the RNN as the sequence_lens input (see the code `sequence_lens = ...` in symbolic.py).\r\n\r\nNote that pytorch RNNs do a weird thing - at each level, they both consume and return the lengths/batch_sizes (as the second element of the PackedSequence tuple), even though the returned lengths must be the same as the input lengths. ONNX RNNs don't do that weird thing, they only consume the lengths. Therefore I can't mimic the pytorch weirdness exactly, and instead must share the lengths blob between layers", "created_at": "2018-02-06T21:23:43Z", "updated_at": "2018-11-23T15:39:14Z", "html_url": "https://github.com/pytorch/pytorch/pull/4695#discussion_r166446442", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4695", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166446442"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4695#discussion_r166446442"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4695"}}, "body_html": "<p>(also answering the next comment here)</p>\n<p>there is only one use of <code>n-&gt;outputs()[0]</code> (enforced by line 198), so yes I will switch this to replaceAllUsesWith.</p>\n<p>there are actually multiple consumers of <code>n-&gt;outputs()[1]</code>, and this is fine. The same blob feeds into each level of the RNN as the sequence_lens input (see the code <code>sequence_lens = ...</code> in symbolic.py).</p>\n<p>Note that pytorch RNNs do a weird thing - at each level, they both consume and return the lengths/batch_sizes (as the second element of the PackedSequence tuple), even though the returned lengths must be the same as the input lengths. ONNX RNNs don't do that weird thing, they only consume the lengths. Therefore I can't mimic the pytorch weirdness exactly, and instead must share the lengths blob between layers</p>", "body_text": "(also answering the next comment here)\nthere is only one use of n->outputs()[0] (enforced by line 198), so yes I will switch this to replaceAllUsesWith.\nthere are actually multiple consumers of n->outputs()[1], and this is fine. The same blob feeds into each level of the RNN as the sequence_lens input (see the code sequence_lens = ... in symbolic.py).\nNote that pytorch RNNs do a weird thing - at each level, they both consume and return the lengths/batch_sizes (as the second element of the PackedSequence tuple), even though the returned lengths must be the same as the input lengths. ONNX RNNs don't do that weird thing, they only consume the lengths. Therefore I can't mimic the pytorch weirdness exactly, and instead must share the lengths blob between layers", "in_reply_to_id": 166161193}