{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4695", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4695/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4695/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4695/events", "html_url": "https://github.com/pytorch/pytorch/pull/4695", "id": 289076476, "node_id": "MDExOlB1bGxSZXF1ZXN0MTYzMzAyNjA3", "number": 4695, "title": "Handle sequence lengths correctly when exporting RNNs to ONNX", "user": {"login": "anderspapitto", "id": 1388690, "node_id": "MDQ6VXNlcjEzODg2OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1388690?v=4", "gravatar_id": "", "url": "https://api.github.com/users/anderspapitto", "html_url": "https://github.com/anderspapitto", "followers_url": "https://api.github.com/users/anderspapitto/followers", "following_url": "https://api.github.com/users/anderspapitto/following{/other_user}", "gists_url": "https://api.github.com/users/anderspapitto/gists{/gist_id}", "starred_url": "https://api.github.com/users/anderspapitto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/anderspapitto/subscriptions", "organizations_url": "https://api.github.com/users/anderspapitto/orgs", "repos_url": "https://api.github.com/users/anderspapitto/repos", "events_url": "https://api.github.com/users/anderspapitto/events{/privacy}", "received_events_url": "https://api.github.com/users/anderspapitto/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2018-01-16T22:22:13Z", "updated_at": "2018-11-23T15:39:16Z", "closed_at": "2018-02-07T02:40:28Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4695", "html_url": "https://github.com/pytorch/pytorch/pull/4695", "diff_url": "https://github.com/pytorch/pytorch/pull/4695.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4695.patch"}, "body_html": "<p>PackedSequence: store batch_sizes as tensor rather<br>\nthan converting to a list of python integers. This maintains<br>\nthe invariant that module's inputs/outputs are collections of<br>\nVariables.</p>\n<p>In particular, this causes the JIT to no longer choke when flattening<br>\nand unflattening arguments.</p>\n<hr>\n<ul>\n<li>\n<p>when uniform sequence lengths are provided, correctly omit the<br>\nargument when constructing the ONNX graph, so as to not fix the<br>\ngraph to the batch size.</p>\n</li>\n<li>\n<p>handle PackedSequences by floating them through the graph and<br>\neliminating them in an optimization pass. ONNX does not have packed<br>\nsequences, but operates on a representation equivalent to<br>\nPaddedSequence, so we hide the representation-switching from ONNX</p>\n</li>\n<li>\n<p>as a preliminary step towards handling PackedSequences, not directly<br>\ntied to ONNX export, change batch_sizes from being an argument to<br>\nthe RNN operators into being an argument to the forward() function<br>\nof those RNN operators. This more closely models the reality that<br>\nbatch_sizes are effectively part of the input sequences.</p>\n</li>\n</ul>", "body_text": "PackedSequence: store batch_sizes as tensor rather\nthan converting to a list of python integers. This maintains\nthe invariant that module's inputs/outputs are collections of\nVariables.\nIn particular, this causes the JIT to no longer choke when flattening\nand unflattening arguments.\n\n\n\nwhen uniform sequence lengths are provided, correctly omit the\nargument when constructing the ONNX graph, so as to not fix the\ngraph to the batch size.\n\n\nhandle PackedSequences by floating them through the graph and\neliminating them in an optimization pass. ONNX does not have packed\nsequences, but operates on a representation equivalent to\nPaddedSequence, so we hide the representation-switching from ONNX\n\n\nas a preliminary step towards handling PackedSequences, not directly\ntied to ONNX export, change batch_sizes from being an argument to\nthe RNN operators into being an argument to the forward() function\nof those RNN operators. This more closely models the reality that\nbatch_sizes are effectively part of the input sequences.", "body": "PackedSequence: store batch_sizes as tensor rather \r\nthan converting to a list of python integers. This maintains\r\nthe invariant that module's inputs/outputs are collections of\r\nVariables.\r\n\r\nIn particular, this causes the JIT to no longer choke when flattening\r\nand unflattening arguments.\r\n\r\n--------------\r\n\r\n- when uniform sequence lengths are provided, correctly omit the\r\n  argument when constructing the ONNX graph, so as to not fix the\r\n  graph to the batch size.\r\n\r\n- handle PackedSequences by floating them through the graph and\r\n  eliminating them in an optimization pass. ONNX does not have packed\r\n  sequences, but operates on a representation equivalent to\r\n  PaddedSequence, so we hide the representation-switching from ONNX\r\n\r\n- as a preliminary step towards handling PackedSequences, not directly\r\n  tied to ONNX export, change batch_sizes from being an argument to\r\n  the RNN operators into being an argument to the forward() function\r\n  of those RNN operators. This more closely models the reality that\r\n  batch_sizes are effectively part of the input sequences."}