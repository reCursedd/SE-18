{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/374973013", "html_url": "https://github.com/pytorch/pytorch/issues/3641#issuecomment-374973013", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3641", "id": 374973013, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NDk3MzAxMw==", "user": {"login": "flauted", "id": 29395172, "node_id": "MDQ6VXNlcjI5Mzk1MTcy", "avatar_url": "https://avatars2.githubusercontent.com/u/29395172?v=4", "gravatar_id": "", "url": "https://api.github.com/users/flauted", "html_url": "https://github.com/flauted", "followers_url": "https://api.github.com/users/flauted/followers", "following_url": "https://api.github.com/users/flauted/following{/other_user}", "gists_url": "https://api.github.com/users/flauted/gists{/gist_id}", "starred_url": "https://api.github.com/users/flauted/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/flauted/subscriptions", "organizations_url": "https://api.github.com/users/flauted/orgs", "repos_url": "https://api.github.com/users/flauted/repos", "events_url": "https://api.github.com/users/flauted/events{/privacy}", "received_events_url": "https://api.github.com/users/flauted/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-21T15:12:35Z", "updated_at": "2018-03-21T15:12:35Z", "author_association": "NONE", "body_html": "<p>I encountered this error message when using multiple multi-gpu machines. It did not occur when using a single multi-gpu machine - albeit with DataParallel, not DistributedDataParallel - nor when using multiple single-gpu machines (also didn't occur when using multi-gpu machines with <code>CUDA_VISIBLE_DEVICES=1</code> on both). I didn't try combinations of single- and multi- gpu as the OP here felt that caused his problem. PyTorch version 0.3.0.post4.</p>\n<p>I solved the problem by deleting a <code>nn.Linear</code> that I assigned to an attribute during the <code>__init__</code> of a custom <code>nn.Module</code> but never used during <code>forward</code>.</p>\n<p>IMO this case deserves a better error message or should be documented with DistributedDataParallel.</p>\n<p>Thanks!</p>", "body_text": "I encountered this error message when using multiple multi-gpu machines. It did not occur when using a single multi-gpu machine - albeit with DataParallel, not DistributedDataParallel - nor when using multiple single-gpu machines (also didn't occur when using multi-gpu machines with CUDA_VISIBLE_DEVICES=1 on both). I didn't try combinations of single- and multi- gpu as the OP here felt that caused his problem. PyTorch version 0.3.0.post4.\nI solved the problem by deleting a nn.Linear that I assigned to an attribute during the __init__ of a custom nn.Module but never used during forward.\nIMO this case deserves a better error message or should be documented with DistributedDataParallel.\nThanks!", "body": "I encountered this error message when using multiple multi-gpu machines. It did not occur when using a single multi-gpu machine - albeit with DataParallel, not DistributedDataParallel - nor when using multiple single-gpu machines (also didn't occur when using multi-gpu machines with ``CUDA_VISIBLE_DEVICES=1`` on both). I didn't try combinations of single- and multi- gpu as the OP here felt that caused his problem. PyTorch version 0.3.0.post4.\r\n\r\nI solved the problem by deleting a ``nn.Linear`` that I assigned to an attribute during the ``__init__`` of a custom ``nn.Module`` but never used during ``forward``.\r\n\r\nIMO this case deserves a better error message or should be documented with DistributedDataParallel.\r\n\r\nThanks!"}