{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3641", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3641/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3641/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3641/events", "html_url": "https://github.com/pytorch/pytorch/issues/3641", "id": 273137420, "node_id": "MDU6SXNzdWUyNzMxMzc0MjA=", "number": 3641, "title": "RuntimeError when using DistributedDataParallel", "user": {"login": "ray342659093", "id": 30654395, "node_id": "MDQ6VXNlcjMwNjU0Mzk1", "avatar_url": "https://avatars3.githubusercontent.com/u/30654395?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ray342659093", "html_url": "https://github.com/ray342659093", "followers_url": "https://api.github.com/users/ray342659093/followers", "following_url": "https://api.github.com/users/ray342659093/following{/other_user}", "gists_url": "https://api.github.com/users/ray342659093/gists{/gist_id}", "starred_url": "https://api.github.com/users/ray342659093/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ray342659093/subscriptions", "organizations_url": "https://api.github.com/users/ray342659093/orgs", "repos_url": "https://api.github.com/users/ray342659093/repos", "events_url": "https://api.github.com/users/ray342659093/events{/privacy}", "received_events_url": "https://api.github.com/users/ray342659093/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-11-11T10:57:16Z", "updated_at": "2018-06-25T18:06:05Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I tried to use DistributedDataParallel to do distributed training across different machines. However, I got error message below</p>\n<pre><code>Traceback (most recent call last):\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"/home/rusu5516/.local/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 323, in _reduction_thread_fn\n    _process_batch()  # just to have a clear scope\n  File \"/home/rusu5516/.local/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 307, in _process_batch\n    nccl.reduce(dev_coalesced, root=0, streams=nccl_streams)\n  File \"/home/rusu5516/.local/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 45, in reduce\n    torch._C._nccl_reduce(inputs, outputs, streams, root, op)\nRuntimeError: all inputs must have the same number of elements\n</code></pre>\n<p>I do not know where these inputs come from and why they do not have the same number of elements. Anyone has clues for this?</p>", "body_text": "I tried to use DistributedDataParallel to do distributed training across different machines. However, I got error message below\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"/home/rusu5516/.local/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 323, in _reduction_thread_fn\n    _process_batch()  # just to have a clear scope\n  File \"/home/rusu5516/.local/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 307, in _process_batch\n    nccl.reduce(dev_coalesced, root=0, streams=nccl_streams)\n  File \"/home/rusu5516/.local/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 45, in reduce\n    torch._C._nccl_reduce(inputs, outputs, streams, root, op)\nRuntimeError: all inputs must have the same number of elements\n\nI do not know where these inputs come from and why they do not have the same number of elements. Anyone has clues for this?", "body": "I tried to use DistributedDataParallel to do distributed training across different machines. However, I got error message below\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File \"/home/rusu5516/.local/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 323, in _reduction_thread_fn\r\n    _process_batch()  # just to have a clear scope\r\n  File \"/home/rusu5516/.local/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 307, in _process_batch\r\n    nccl.reduce(dev_coalesced, root=0, streams=nccl_streams)\r\n  File \"/home/rusu5516/.local/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 45, in reduce\r\n    torch._C._nccl_reduce(inputs, outputs, streams, root, op)\r\nRuntimeError: all inputs must have the same number of elements\r\n```\r\nI do not know where these inputs come from and why they do not have the same number of elements. Anyone has clues for this?\r\n"}