{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2210", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2210/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2210/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2210/events", "html_url": "https://github.com/pytorch/pytorch/issues/2210", "id": 245597879, "node_id": "MDU6SXNzdWUyNDU1OTc4Nzk=", "number": 2210, "title": "\"LogSoftmax can only be differentiated once\"", "user": {"login": "hongyi-zhang", "id": 3592602, "node_id": "MDQ6VXNlcjM1OTI2MDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3592602?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hongyi-zhang", "html_url": "https://github.com/hongyi-zhang", "followers_url": "https://api.github.com/users/hongyi-zhang/followers", "following_url": "https://api.github.com/users/hongyi-zhang/following{/other_user}", "gists_url": "https://api.github.com/users/hongyi-zhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/hongyi-zhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hongyi-zhang/subscriptions", "organizations_url": "https://api.github.com/users/hongyi-zhang/orgs", "repos_url": "https://api.github.com/users/hongyi-zhang/repos", "events_url": "https://api.github.com/users/hongyi-zhang/events{/privacy}", "received_events_url": "https://api.github.com/users/hongyi-zhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-07-26T03:44:25Z", "updated_at": "2017-08-01T18:24:20Z", "closed_at": "2017-08-01T18:22:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Could you also add double_backward support for LogSoftmax? PyTorch had no problem twice differentiating the old softmax before the recent commits of the \"new style autograd functions\".</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nx = Variable(torch.rand(10, 2), requires_grad=True)\ny = Variable(torch.ones(10).type(torch.LongTensor))\ncriterion = nn.CrossEntropyLoss()\nl = criterion(x, y)\ng = torch.autograd.grad(l, x, create_graph=True)\nL = l + torch.sum(g[0] * x)\nL.backward()\n</code></pre>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-2-dbea6d8d5842&gt; in &lt;module&gt;()\n      9 g = torch.autograd.grad(l, x, create_graph=True)\n     10 L = l + torch.sum(g[0] * x)\n---&gt; 11 L.backward()\n\n/anaconda/lib/python2.7/site-packages/torch/autograd/variable.pyc in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    153                 Variable.\n    154         \"\"\"\n--&gt; 155         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    156 \n    157     def register_hook(self, hook):\n\n/anaconda/lib/python2.7/site-packages/torch/autograd/__init__.pyc in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     96 \n     97     Variable._execution_engine.run_backward(\n---&gt; 98         variables, grad_variables, retain_graph)\n     99 \n    100 \n\n/anaconda/lib/python2.7/site-packages/torch/autograd/function.pyc in apply(self, *args)\n     89 \n     90     def apply(self, *args):\n---&gt; 91         return self._forward_cls.backward(self, *args)\n     92 \n     93 \n\n/anaconda/lib/python2.7/site-packages/torch/nn/_functions/thnn/auto.pyc in backward_cls_backward(ctx, *grad_params)\n    238     @staticmethod\n    239     def backward_cls_backward(ctx, *grad_params):\n--&gt; 240         return double_backwards_fn(ctx, *grad_params)\n    241 \n    242     base_class = Function if not is_inplace else InplaceFunction\n\n/anaconda/lib/python2.7/site-packages/torch/nn/_functions/thnn/auto.pyc in default_double_backwards_fn(ctx, *grad_params)\n    319             def make_default_double_backwards_fn(class_name):\n    320                 def default_double_backwards_fn(ctx, *grad_params):\n--&gt; 321                     raise ValueError(class_name + \" can only be differentiated once.\")\n    322                 return default_double_backwards_fn\n    323             double_backwards_fn = make_default_double_backwards_fn(class_name)\n\nValueError: LogSoftmax can only be differentiated once.\n</code></pre>\n<p>PS: many thanks for the new commits on double backwards of MaxPool2d etc.!</p>", "body_text": "Could you also add double_backward support for LogSoftmax? PyTorch had no problem twice differentiating the old softmax before the recent commits of the \"new style autograd functions\".\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nx = Variable(torch.rand(10, 2), requires_grad=True)\ny = Variable(torch.ones(10).type(torch.LongTensor))\ncriterion = nn.CrossEntropyLoss()\nl = criterion(x, y)\ng = torch.autograd.grad(l, x, create_graph=True)\nL = l + torch.sum(g[0] * x)\nL.backward()\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-2-dbea6d8d5842> in <module>()\n      9 g = torch.autograd.grad(l, x, create_graph=True)\n     10 L = l + torch.sum(g[0] * x)\n---> 11 L.backward()\n\n/anaconda/lib/python2.7/site-packages/torch/autograd/variable.pyc in backward(self, gradient, retain_graph, create_graph, retain_variables)\n    153                 Variable.\n    154         \"\"\"\n--> 155         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n    156 \n    157     def register_hook(self, hook):\n\n/anaconda/lib/python2.7/site-packages/torch/autograd/__init__.pyc in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\n     96 \n     97     Variable._execution_engine.run_backward(\n---> 98         variables, grad_variables, retain_graph)\n     99 \n    100 \n\n/anaconda/lib/python2.7/site-packages/torch/autograd/function.pyc in apply(self, *args)\n     89 \n     90     def apply(self, *args):\n---> 91         return self._forward_cls.backward(self, *args)\n     92 \n     93 \n\n/anaconda/lib/python2.7/site-packages/torch/nn/_functions/thnn/auto.pyc in backward_cls_backward(ctx, *grad_params)\n    238     @staticmethod\n    239     def backward_cls_backward(ctx, *grad_params):\n--> 240         return double_backwards_fn(ctx, *grad_params)\n    241 \n    242     base_class = Function if not is_inplace else InplaceFunction\n\n/anaconda/lib/python2.7/site-packages/torch/nn/_functions/thnn/auto.pyc in default_double_backwards_fn(ctx, *grad_params)\n    319             def make_default_double_backwards_fn(class_name):\n    320                 def default_double_backwards_fn(ctx, *grad_params):\n--> 321                     raise ValueError(class_name + \" can only be differentiated once.\")\n    322                 return default_double_backwards_fn\n    323             double_backwards_fn = make_default_double_backwards_fn(class_name)\n\nValueError: LogSoftmax can only be differentiated once.\n\nPS: many thanks for the new commits on double backwards of MaxPool2d etc.!", "body": "Could you also add double_backward support for LogSoftmax? PyTorch had no problem twice differentiating the old softmax before the recent commits of the \"new style autograd functions\".\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nx = Variable(torch.rand(10, 2), requires_grad=True)\r\ny = Variable(torch.ones(10).type(torch.LongTensor))\r\ncriterion = nn.CrossEntropyLoss()\r\nl = criterion(x, y)\r\ng = torch.autograd.grad(l, x, create_graph=True)\r\nL = l + torch.sum(g[0] * x)\r\nL.backward()\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-dbea6d8d5842> in <module>()\r\n      9 g = torch.autograd.grad(l, x, create_graph=True)\r\n     10 L = l + torch.sum(g[0] * x)\r\n---> 11 L.backward()\r\n\r\n/anaconda/lib/python2.7/site-packages/torch/autograd/variable.pyc in backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    153                 Variable.\r\n    154         \"\"\"\r\n--> 155         torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n    156 \r\n    157     def register_hook(self, hook):\r\n\r\n/anaconda/lib/python2.7/site-packages/torch/autograd/__init__.pyc in backward(variables, grad_variables, retain_graph, create_graph, retain_variables)\r\n     96 \r\n     97     Variable._execution_engine.run_backward(\r\n---> 98         variables, grad_variables, retain_graph)\r\n     99 \r\n    100 \r\n\r\n/anaconda/lib/python2.7/site-packages/torch/autograd/function.pyc in apply(self, *args)\r\n     89 \r\n     90     def apply(self, *args):\r\n---> 91         return self._forward_cls.backward(self, *args)\r\n     92 \r\n     93 \r\n\r\n/anaconda/lib/python2.7/site-packages/torch/nn/_functions/thnn/auto.pyc in backward_cls_backward(ctx, *grad_params)\r\n    238     @staticmethod\r\n    239     def backward_cls_backward(ctx, *grad_params):\r\n--> 240         return double_backwards_fn(ctx, *grad_params)\r\n    241 \r\n    242     base_class = Function if not is_inplace else InplaceFunction\r\n\r\n/anaconda/lib/python2.7/site-packages/torch/nn/_functions/thnn/auto.pyc in default_double_backwards_fn(ctx, *grad_params)\r\n    319             def make_default_double_backwards_fn(class_name):\r\n    320                 def default_double_backwards_fn(ctx, *grad_params):\r\n--> 321                     raise ValueError(class_name + \" can only be differentiated once.\")\r\n    322                 return default_double_backwards_fn\r\n    323             double_backwards_fn = make_default_double_backwards_fn(class_name)\r\n\r\nValueError: LogSoftmax can only be differentiated once.\r\n```\r\n\r\nPS: many thanks for the new commits on double backwards of MaxPool2d etc.!"}