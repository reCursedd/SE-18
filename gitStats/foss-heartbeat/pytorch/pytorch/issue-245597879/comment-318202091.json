{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/318202091", "html_url": "https://github.com/pytorch/pytorch/issues/2210#issuecomment-318202091", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2210", "id": 318202091, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODIwMjA5MQ==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-26T22:32:09Z", "updated_at": "2017-07-26T22:32:09Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>PyTorch had no problem twice differentiating the old softmax before the recent commits of the \"new style autograd functions\".</p>\n</blockquote>\n<p>I think what you mean is you are using the feature of DoubleBackward and LogSoftMax is not yet ready. Just wanted to clarify language for future context.</p>", "body_text": "PyTorch had no problem twice differentiating the old softmax before the recent commits of the \"new style autograd functions\".\n\nI think what you mean is you are using the feature of DoubleBackward and LogSoftMax is not yet ready. Just wanted to clarify language for future context.", "body": "> PyTorch had no problem twice differentiating the old softmax before the recent commits of the \"new style autograd functions\".\r\n\r\nI think what you mean is you are using the feature of DoubleBackward and LogSoftMax is not yet ready. Just wanted to clarify language for future context."}