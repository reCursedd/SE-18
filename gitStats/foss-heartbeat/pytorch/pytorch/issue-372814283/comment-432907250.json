{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/432907250", "html_url": "https://github.com/pytorch/pytorch/pull/12979#issuecomment-432907250", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12979", "id": 432907250, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjkwNzI1MA==", "user": {"login": "parth1595", "id": 21214965, "node_id": "MDQ6VXNlcjIxMjE0OTY1", "avatar_url": "https://avatars1.githubusercontent.com/u/21214965?v=4", "gravatar_id": "", "url": "https://api.github.com/users/parth1595", "html_url": "https://github.com/parth1595", "followers_url": "https://api.github.com/users/parth1595/followers", "following_url": "https://api.github.com/users/parth1595/following{/other_user}", "gists_url": "https://api.github.com/users/parth1595/gists{/gist_id}", "starred_url": "https://api.github.com/users/parth1595/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/parth1595/subscriptions", "organizations_url": "https://api.github.com/users/parth1595/orgs", "repos_url": "https://api.github.com/users/parth1595/repos", "events_url": "https://api.github.com/users/parth1595/events{/privacy}", "received_events_url": "https://api.github.com/users/parth1595/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-25T04:04:39Z", "updated_at": "2018-10-25T04:04:39Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>One interesting nuance here is that any input data is moved to the front of external_inputs, which some functionality in the codebase actually leverages (see predictor.cc for positional inputs). We will want to preserve this. Instead, could you merge the two lists and remove all duplicates in the process?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Now we make input/output_blobs line up with what Predictor expects.</span>\nnew_external_inputs <span class=\"pl-k\">=</span> input_blobs\n<span class=\"pl-k\">for</span> external_input <span class=\"pl-k\">in</span> proto.external_input:\n  <span class=\"pl-k\">if</span> external_input <span class=\"pl-k\">not</span> <span class=\"pl-k\">in</span> new_external_inputs:\n    new_external_inputs.append(external_input)\npredict_net.external_input.extend(new_external_inputs)\t</pre></div>\n</blockquote>\n<p>Sure <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4842908\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bwasti\">@bwasti</a> I will make the change and update the following patch.<br>\nThanks for the suggestion.</p>", "body_text": "One interesting nuance here is that any input data is moved to the front of external_inputs, which some functionality in the codebase actually leverages (see predictor.cc for positional inputs). We will want to preserve this. Instead, could you merge the two lists and remove all duplicates in the process?\n# Now we make input/output_blobs line up with what Predictor expects.\nnew_external_inputs = input_blobs\nfor external_input in proto.external_input:\n  if external_input not in new_external_inputs:\n    new_external_inputs.append(external_input)\npredict_net.external_input.extend(new_external_inputs)\t\n\nSure @bwasti I will make the change and update the following patch.\nThanks for the suggestion.", "body": "> One interesting nuance here is that any input data is moved to the front of external_inputs, which some functionality in the codebase actually leverages (see predictor.cc for positional inputs). We will want to preserve this. Instead, could you merge the two lists and remove all duplicates in the process?\r\n> \r\n> ```python\r\n> # Now we make input/output_blobs line up with what Predictor expects.\r\n> new_external_inputs = input_blobs\r\n> for external_input in proto.external_input:\r\n>   if external_input not in new_external_inputs:\r\n>     new_external_inputs.append(external_input)\r\n> predict_net.external_input.extend(new_external_inputs)\t\r\n> ```\r\n\r\nSure @bwasti I will make the change and update the following patch. \r\nThanks for the suggestion."}