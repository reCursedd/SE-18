{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1542", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1542/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1542/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1542/events", "html_url": "https://github.com/pytorch/pytorch/pull/1542", "id": 228165305, "node_id": "MDExOlB1bGxSZXF1ZXN0MTIwMjMzNjIx", "number": 1542, "title": "Cuda reduce in a consistent direction", "user": {"login": "bunelr", "id": 3354626, "node_id": "MDQ6VXNlcjMzNTQ2MjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3354626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bunelr", "html_url": "https://github.com/bunelr", "followers_url": "https://api.github.com/users/bunelr/followers", "following_url": "https://api.github.com/users/bunelr/following{/other_user}", "gists_url": "https://api.github.com/users/bunelr/gists{/gist_id}", "starred_url": "https://api.github.com/users/bunelr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bunelr/subscriptions", "organizations_url": "https://api.github.com/users/bunelr/orgs", "repos_url": "https://api.github.com/users/bunelr/repos", "events_url": "https://api.github.com/users/bunelr/events{/privacy}", "received_events_url": "https://api.github.com/users/bunelr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-12T01:35:42Z", "updated_at": "2017-05-15T17:26:40Z", "closed_at": "2017-05-15T17:26:40Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1542", "html_url": "https://github.com/pytorch/pytorch/pull/1542", "diff_url": "https://github.com/pytorch/pytorch/pull/1542.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1542.patch"}, "body_html": "<p>The bug was discovered by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15345596\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/danielamassiceti\">@danielamassiceti</a>.</p>\n<p>Repro script:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nx <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">1</span>).cuda()\n<span class=\"pl-c1\">print</span>(torch.max(x, <span class=\"pl-c1\">0</span>))\n\ny <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(torch.max(y,<span class=\"pl-c1\">0</span>))\n\n\nx <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">10</span>).cuda()\n<span class=\"pl-c1\">print</span>(torch.max(x, <span class=\"pl-c1\">0</span>))\n\ny <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">10</span>)\n<span class=\"pl-c1\">print</span>(torch.max(y,<span class=\"pl-c1\">0</span>))</pre></div>\n<p>With the current master, the resulting index that are going to be obtained as max are for each print:<br>\n-&gt; 9, 0, 0, 0</p>\n<p>So the cuda implementation has a problem.<br>\nIf you look <a href=\"https://github.com/pytorch/pytorch/blob/6fab62173e842bbf550de1c68cfae507ca35b800/torch/lib/THC/THCTensorMathReduce.cuh#L553\">here</a> or <a href=\"https://github.com/pytorch/pytorch/blob/6fab62173e842bbf550de1c68cfae507ca35b800/torch/lib/THC/THCTensorMathReduce.cuh#L553\">here</a>, you can see that the binaryOp function is called with the new value (later in the tensor) as first argument and the running max as second argument.</p>\n<p>The checks made in the comparison function default to the first argument in case of equality, so that would be here defaulting to the new value. This is different to what is implemented on the CPU (see for example <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L1515\">here</a> )</p>\n<p>This leads to the fix inside the struct. However, if we just apply these fixes, the resulting index from running the repro case becomes:<br>\n-&gt; 0, 0, 7, 0</p>\n<p>which is still wrong.</p>\n<p>The reason for this is that when reducing over the results of the threads in the kernel, the arguments are passed as early in the array as first argument, late in the array as second argument.</p>\n<p>This leads to the fix in kernelTransformReduceInnermostDimIndex.</p>\n<p>An alternative fix would be to  keep everything that I've changed as it was and swap the order of the arguments <a href=\"https://github.com/pytorch/pytorch/blob/6fab62173e842bbf550de1c68cfae507ca35b800/torch/lib/THC/THCTensorMathReduce.cuh#L472\">here</a> and <a href=\"https://github.com/pytorch/pytorch/blob/6fab62173e842bbf550de1c68cfae507ca35b800/torch/lib/THC/THCTensorMathReduce.cuh#L553\">here</a>. This would probably be cleaner.</p>\n<p>Let me know what you think.Should I add a test for this?</p>\n<p><em>EDIT</em>: I actually went ahead and  did the alternate fix. I find it cleaner.</p>", "body_text": "The bug was discovered by @danielamassiceti.\nRepro script:\nimport torch\nx = torch.zeros(10,1).cuda()\nprint(torch.max(x, 0))\n\ny = torch.zeros(10,1)\nprint(torch.max(y,0))\n\n\nx = torch.zeros(10).cuda()\nprint(torch.max(x, 0))\n\ny = torch.zeros(10)\nprint(torch.max(y,0))\nWith the current master, the resulting index that are going to be obtained as max are for each print:\n-> 9, 0, 0, 0\nSo the cuda implementation has a problem.\nIf you look here or here, you can see that the binaryOp function is called with the new value (later in the tensor) as first argument and the running max as second argument.\nThe checks made in the comparison function default to the first argument in case of equality, so that would be here defaulting to the new value. This is different to what is implemented on the CPU (see for example here )\nThis leads to the fix inside the struct. However, if we just apply these fixes, the resulting index from running the repro case becomes:\n-> 0, 0, 7, 0\nwhich is still wrong.\nThe reason for this is that when reducing over the results of the threads in the kernel, the arguments are passed as early in the array as first argument, late in the array as second argument.\nThis leads to the fix in kernelTransformReduceInnermostDimIndex.\nAn alternative fix would be to  keep everything that I've changed as it was and swap the order of the arguments here and here. This would probably be cleaner.\nLet me know what you think.Should I add a test for this?\nEDIT: I actually went ahead and  did the alternate fix. I find it cleaner.", "body": "The bug was discovered by @danielamassiceti.\r\n\r\nRepro script:\r\n\r\n```python\r\nimport torch\r\nx = torch.zeros(10,1).cuda()\r\nprint(torch.max(x, 0))\r\n\r\ny = torch.zeros(10,1)\r\nprint(torch.max(y,0))\r\n\r\n\r\nx = torch.zeros(10).cuda()\r\nprint(torch.max(x, 0))\r\n\r\ny = torch.zeros(10)\r\nprint(torch.max(y,0))\r\n```\r\n\r\nWith the current master, the resulting index that are going to be obtained as max are for each print:\r\n-> 9, 0, 0, 0\r\n\r\nSo the cuda implementation has a problem.\r\nIf you look [here](https://github.com/pytorch/pytorch/blob/6fab62173e842bbf550de1c68cfae507ca35b800/torch/lib/THC/THCTensorMathReduce.cuh#L553) or [here](https://github.com/pytorch/pytorch/blob/6fab62173e842bbf550de1c68cfae507ca35b800/torch/lib/THC/THCTensorMathReduce.cuh#L553), you can see that the binaryOp function is called with the new value (later in the tensor) as first argument and the running max as second argument.\r\n\r\nThe checks made in the comparison function default to the first argument in case of equality, so that would be here defaulting to the new value. This is different to what is implemented on the CPU (see for example [here](https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/generic/THTensorMath.c#L1515) )\r\n\r\nThis leads to the fix inside the struct. However, if we just apply these fixes, the resulting index from running the repro case becomes:\r\n-> 0, 0, 7, 0\r\n\r\nwhich is still wrong.\r\n\r\nThe reason for this is that when reducing over the results of the threads in the kernel, the arguments are passed as early in the array as first argument, late in the array as second argument.\r\n\r\nThis leads to the fix in kernelTransformReduceInnermostDimIndex.\r\n\r\n\r\n\r\n\r\n\r\n\r\nAn alternative fix would be to  keep everything that I've changed as it was and swap the order of the arguments [here](https://github.com/pytorch/pytorch/blob/6fab62173e842bbf550de1c68cfae507ca35b800/torch/lib/THC/THCTensorMathReduce.cuh#L472) and [here](https://github.com/pytorch/pytorch/blob/6fab62173e842bbf550de1c68cfae507ca35b800/torch/lib/THC/THCTensorMathReduce.cuh#L553). This would probably be cleaner.\r\n\r\n\r\nLet me know what you think.Should I add a test for this?\r\n\r\n*EDIT*: I actually went ahead and  did the alternate fix. I find it cleaner."}