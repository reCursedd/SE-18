{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7698", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7698/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7698/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7698/events", "html_url": "https://github.com/pytorch/pytorch/issues/7698", "id": 324597432, "node_id": "MDU6SXNzdWUzMjQ1OTc0MzI=", "number": 7698, "title": "[feature request] Calling autograd.grad inside autograd.Function", "user": {"login": "rtqichen", "id": 3375899, "node_id": "MDQ6VXNlcjMzNzU4OTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3375899?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rtqichen", "html_url": "https://github.com/rtqichen", "followers_url": "https://api.github.com/users/rtqichen/followers", "following_url": "https://api.github.com/users/rtqichen/following{/other_user}", "gists_url": "https://api.github.com/users/rtqichen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rtqichen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rtqichen/subscriptions", "organizations_url": "https://api.github.com/users/rtqichen/orgs", "repos_url": "https://api.github.com/users/rtqichen/repos", "events_url": "https://api.github.com/users/rtqichen/events{/privacy}", "received_events_url": "https://api.github.com/users/rtqichen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-05-19T03:35:04Z", "updated_at": "2018-05-19T22:03:47Z", "closed_at": "2018-05-19T17:53:59Z", "author_association": "NONE", "body_html": "<p>I'm trying to implement something where I'd like to be able to compute the gradients of existing autograd functions inside a new autograd.Function, but it seems that even with the new 0.4 changes this cannot be done yet. It'd be great if this could be supported.</p>\n<p>Here is a trivial new Function that just wraps around an existing one:</p>\n<pre><code>import torch\nfrom torch.autograd import Function\n\n\nclass TanhWrapper(Function):\n\n    @staticmethod\n    def forward(ctx, inputs):\n        ctx.save_for_backward(inputs)\n        return torch.tanh(inputs)\n\n    @staticmethod\n    def backward(ctx, grad_outputs):\n        inputs, = ctx.saved_tensors\n        inputs.requires_grad = True\n        print(inputs.requires_grad)\n        outputs = torch.tanh(inputs)\n        print(outputs.requires_grad)\n        return torch.autograd.grad(outputs, inputs, grad_outputs, retain_graph=False)[0]\n\n\nif __name__ == '__main__':\n    x = torch.randn(2, 2).requires_grad_(True)\n\n    y = TanhWrapper.apply(x)\n    y.sum().backward()\n    print(x.grad)\n</code></pre>\n<p>This prints out</p>\n<pre><code>True\nFalse\n...\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n</code></pre>", "body_text": "I'm trying to implement something where I'd like to be able to compute the gradients of existing autograd functions inside a new autograd.Function, but it seems that even with the new 0.4 changes this cannot be done yet. It'd be great if this could be supported.\nHere is a trivial new Function that just wraps around an existing one:\nimport torch\nfrom torch.autograd import Function\n\n\nclass TanhWrapper(Function):\n\n    @staticmethod\n    def forward(ctx, inputs):\n        ctx.save_for_backward(inputs)\n        return torch.tanh(inputs)\n\n    @staticmethod\n    def backward(ctx, grad_outputs):\n        inputs, = ctx.saved_tensors\n        inputs.requires_grad = True\n        print(inputs.requires_grad)\n        outputs = torch.tanh(inputs)\n        print(outputs.requires_grad)\n        return torch.autograd.grad(outputs, inputs, grad_outputs, retain_graph=False)[0]\n\n\nif __name__ == '__main__':\n    x = torch.randn(2, 2).requires_grad_(True)\n\n    y = TanhWrapper.apply(x)\n    y.sum().backward()\n    print(x.grad)\n\nThis prints out\nTrue\nFalse\n...\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn", "body": "I'm trying to implement something where I'd like to be able to compute the gradients of existing autograd functions inside a new autograd.Function, but it seems that even with the new 0.4 changes this cannot be done yet. It'd be great if this could be supported.\r\n\r\nHere is a trivial new Function that just wraps around an existing one:\r\n```\r\nimport torch\r\nfrom torch.autograd import Function\r\n\r\n\r\nclass TanhWrapper(Function):\r\n\r\n    @staticmethod\r\n    def forward(ctx, inputs):\r\n        ctx.save_for_backward(inputs)\r\n        return torch.tanh(inputs)\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_outputs):\r\n        inputs, = ctx.saved_tensors\r\n        inputs.requires_grad = True\r\n        print(inputs.requires_grad)\r\n        outputs = torch.tanh(inputs)\r\n        print(outputs.requires_grad)\r\n        return torch.autograd.grad(outputs, inputs, grad_outputs, retain_graph=False)[0]\r\n\r\n\r\nif __name__ == '__main__':\r\n    x = torch.randn(2, 2).requires_grad_(True)\r\n\r\n    y = TanhWrapper.apply(x)\r\n    y.sum().backward()\r\n    print(x.grad)\r\n```\r\n\r\nThis prints out \r\n```\r\nTrue\r\nFalse\r\n...\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```"}