{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390430104", "html_url": "https://github.com/pytorch/pytorch/issues/7698#issuecomment-390430104", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7698", "id": 390430104, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDQzMDEwNA==", "user": {"login": "rtqichen", "id": 3375899, "node_id": "MDQ6VXNlcjMzNzU4OTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/3375899?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rtqichen", "html_url": "https://github.com/rtqichen", "followers_url": "https://api.github.com/users/rtqichen/followers", "following_url": "https://api.github.com/users/rtqichen/following{/other_user}", "gists_url": "https://api.github.com/users/rtqichen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rtqichen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rtqichen/subscriptions", "organizations_url": "https://api.github.com/users/rtqichen/orgs", "repos_url": "https://api.github.com/users/rtqichen/repos", "events_url": "https://api.github.com/users/rtqichen/events{/privacy}", "received_events_url": "https://api.github.com/users/rtqichen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-19T20:19:58Z", "updated_at": "2018-05-19T20:26:39Z", "author_association": "NONE", "body_html": "<p>Sorry, I don't think I provided enough to quite get my point across. In the backward call I'm finding that even though a tensor might have <code>requires_grad=True</code> after applying <code>torch.tanh</code> the output has <code>requires_grad=False</code>.</p>\n<p>The example is a nonsense but the idea to eventually be able to implement something like this fixed-point solver from autograd as a Function (<a href=\"https://github.com/HIPS/autograd/blob/master/autograd/misc/fixed_points.py\">https://github.com/HIPS/autograd/blob/master/autograd/misc/fixed_points.py</a>). The idea is that the backward pass doesn't directly differentiate through the forward pass, but solves another fixed-point problem.</p>\n<p><code>autograd.Function</code> also doesn't support non-Tensor inputs so implementing this is still pretty far away, but being able to take gradients inside the backward call makes sense to me.</p>\n<p>For now, I think using 0.4's no_grad and manually performing backward will suffice. It just would be much more convenient if torch.autograd could wrap this inside a Function.</p>", "body_text": "Sorry, I don't think I provided enough to quite get my point across. In the backward call I'm finding that even though a tensor might have requires_grad=True after applying torch.tanh the output has requires_grad=False.\nThe example is a nonsense but the idea to eventually be able to implement something like this fixed-point solver from autograd as a Function (https://github.com/HIPS/autograd/blob/master/autograd/misc/fixed_points.py). The idea is that the backward pass doesn't directly differentiate through the forward pass, but solves another fixed-point problem.\nautograd.Function also doesn't support non-Tensor inputs so implementing this is still pretty far away, but being able to take gradients inside the backward call makes sense to me.\nFor now, I think using 0.4's no_grad and manually performing backward will suffice. It just would be much more convenient if torch.autograd could wrap this inside a Function.", "body": "Sorry, I don't think I provided enough to quite get my point across. In the backward call I'm finding that even though a tensor might have `requires_grad=True` after applying `torch.tanh` the output has `requires_grad=False`.\r\n\r\nThe example is a nonsense but the idea to eventually be able to implement something like this fixed-point solver from autograd as a Function (https://github.com/HIPS/autograd/blob/master/autograd/misc/fixed_points.py). The idea is that the backward pass doesn't directly differentiate through the forward pass, but solves another fixed-point problem.\r\n\r\n`autograd.Function` also doesn't support non-Tensor inputs so implementing this is still pretty far away, but being able to take gradients inside the backward call makes sense to me.\r\n\r\nFor now, I think using 0.4's no_grad and manually performing backward will suffice. It just would be much more convenient if torch.autograd could wrap this inside a Function."}