{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83598480", "pull_request_review_id": 4426897, "id": 83598480, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDgzNTk4NDgw", "diff_hunk": "@@ -0,0 +1,408 @@\n+import torch.cuda\n+import torch.backends.cudnn as cudnn\n+from torch.backends.cudnn import check_error\n+import ctypes\n+\n+\n+def initDropoutDescriptor(fn, handle):\n+    dropout_desc = cudnn.DropoutDescriptor()\n+\n+    dropout_states_size = ctypes.c_long()\n+    check_error(cudnn.lib.cudnnDropoutGetStatesSize(\n+        handle,\n+        ctypes.byref(dropout_states_size)))\n+\n+    dropout_states = torch.cuda.ByteTensor(dropout_states_size.value)\n+    dropout_desc.set(\n+        handle,\n+        fn.dropout,\n+        dropout_states,\n+        fn.seed\n+    )\n+    return dropout_desc\n+\n+\n+def initRNNDescriptor(fn):\n+    rnn_desc = cudnn.RNNDescriptor()\n+\n+    rnn_desc.set(\n+        fn.hidden_size,\n+        fn.num_layers,\n+        fn.dropout_desc,\n+        fn.input_mode,\n+        fn.bidirectional,\n+        fn.mode,\n+        fn.datatype\n+    )\n+    return rnn_desc\n+\n+\n+def initWeightDescriptor(fn, weight):\n+    w_desc = cudnn.FilterDescriptor()\n+    w_view = weight.view(-1, 1, 1)  # seems that filters require >=3 dimensions\n+    w_desc.set(w_view)\n+    return w_desc\n+\n+\n+def _inputSize(fn):\n+    return (fn.seq_length, fn.mini_batch, fn.input_size)\n+\n+\n+def _hiddenSize(fn):\n+    return (fn.num_layers * fn.num_directions, fn.mini_batch, fn.hidden_size)\n+\n+\n+def _outputSize(fn):\n+    return (fn.seq_length, fn.mini_batch, fn.hidden_size * fn.num_directions)\n+\n+\n+def getNumWeights(handle, rnn_desc, x_desc, datatype):\n+    weight_size = ctypes.c_long()\n+    check_error(cudnn.lib.cudnnGetRNNParamsSize(\n+        handle,\n+        rnn_desc,\n+        x_desc,\n+        ctypes.byref(weight_size),\n+        datatype\n+    ))\n+    elem_size = cudnn._sizeofmap[datatype]\n+    assert(weight_size.value % elem_size == 0)\n+    return weight_size.value // elem_size\n+\n+\n+def getParameters(fn, handle, weight_buf):\n+\n+    cudnn_methods = [\n+        cudnn.lib.cudnnGetRNNLinLayerMatrixParams,\n+        cudnn.lib.cudnnGetRNNLinLayerBiasParams\n+    ]\n+\n+    # if fn.mode == cudnn.CUDNN_RNN_RELU or fn.mode == cudnn.CUDNN_RNN_TANH:\n+    #     linear_name = [\"ih\", \"hh\"]\n+    # elif fn.mode == cudnn.CUDNN_LSTM:\n+    #     linear_name = [\"ii\", \"if\", \"ic\", \"io\", \"hi\", \"hf\", \"hc\", \"ho\"]\n+    # elif fn.mode == cudnn.CUDNN_GRU:\n+    #     linear_name = [\"ir\", \"iu\", \"ic\", \"hr\", \"hu\", \"hc\"]\n+    # else:\n+    #     raise Exception(\"Unknown mode: {}\".format(fn.mode))\n+\n+    params = []\n+    num_linear_layers = _numLinearLayers(fn)\n+    num_layers = fn.num_directions * fn.num_layers\n+    for layer in range(num_layers):\n+        layer_params = []\n+        for cudnn_method in cudnn_methods:\n+            for linear_id in range(num_linear_layers):\n+                lin_layer_mat_desc = cudnn.FilterDescriptor()\n+                matrix_pointer = ctypes.c_void_p()\n+                check_error(cudnn_method(\n+                    handle,\n+                    fn.rnn_desc,\n+                    layer,\n+                    fn.x_descs[0],\n+                    fn.w_desc,\n+                    ctypes.c_void_p(weight_buf.data_ptr()),\n+                    linear_id,\n+                    lin_layer_mat_desc,\n+                    ctypes.byref(matrix_pointer)))\n+\n+                data_type = ctypes.c_int()\n+                format = ctypes.c_int()\n+                nb_dims = ctypes.c_int()\n+                min_dim = 3\n+                filter_dim_a = torch.IntTensor(min_dim)\n+                check_error(cudnn.lib.cudnnGetFilterNdDescriptor(\n+                    lin_layer_mat_desc,\n+                    min_dim,\n+                    ctypes.byref(data_type),\n+                    ctypes.byref(format),\n+                    ctypes.byref(nb_dims),\n+                    ctypes.c_void_p(filter_dim_a.data_ptr())))\n+\n+                filter_dim_a.resize_(nb_dims.value)\n+                elem_size = cudnn._sizeofmap[fn.datatype]\n+                offset_bytes = (matrix_pointer.value - weight_buf.data_ptr())\n+                assert(offset_bytes % elem_size == 0)\n+                offset = offset_bytes // elem_size\n+\n+                # for all the RNN types provided by CUDNN, all the ih weights\n+                # are the same size and are allocated in a contiguous chunk\n+                # (same for the hh weights, and the ih and hh biases).\n+                # Since we're storing all the weights in a single tensor anyway,\n+                # might as well merge the CUDNN ones into a single tensor as well\n+                if linear_id == 0 or linear_id == num_linear_layers / 2:\n+                    assert(filter_dim_a.prod() == filter_dim_a[0])\n+                    param = fn.weight_buf.new().set_(\n+                        weight_buf.storage(), offset,\n+                        filter_dim_a[0] * num_linear_layers / 2, filter_dim_a[2])\n+                    layer_params.append(param)\n+                else:\n+                    assert(cur_offset == offset)\n+\n+                cur_offset = offset + filter_dim_a[0]\n+\n+\n+        params.append(layer_params)\n+\n+    return params\n+\n+\n+def _copyParams(params_from, params_to):\n+    for layer_params_from, layer_params_to in zip(params_from, params_to):", "path": "torch/backends/cudnn/rnn.py", "position": 169, "original_position": 151, "commit_id": "b5d13296c65e4b3cd5aa9715cf58df0fc043454e", "original_commit_id": "ccb1f401ff482f1fb25251272656149899758d4a", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Uh, `params_from` and `params_to` are iterables of iterables?\n", "created_at": "2016-10-17T08:43:37Z", "updated_at": "2018-11-23T15:31:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/129#discussion_r83598480", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/129", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83598480"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/129#discussion_r83598480"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/129"}}, "body_html": "<p>Uh, <code>params_from</code> and <code>params_to</code> are iterables of iterables?</p>", "body_text": "Uh, params_from and params_to are iterables of iterables?"}