{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83594558", "pull_request_review_id": 4426897, "id": 83594558, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDgzNTk0NTU4", "diff_hunk": "@@ -596,6 +605,115 @@ def test_MaxUnpool2d_output_size(self):\n                             mu(output_small, indices_small, (h, w)))\n \n \n+    def test_RNN_cell(self):\n+        # this is just a smoke test; these modules are implemented through\n+        # autograd so no Jacobian test is needed\n+        for module in (nn.rnn.cell.RNN, nn.rnn.cell.RNNReLU, nn.rnn.cell.GRU):\n+            for bias in (True, False):\n+                input = Variable(torch.randn(3, 10))\n+                hx = Variable(torch.randn(3, 20))\n+                cell = module(10, 20, bias=bias)\n+                for i in range(6):\n+                    hx = cell(input, hx)\n+\n+                hx.sum().backward()\n+\n+    def test_LSTM_cell(self):\n+        # this is just a smoke test; these modules are implemented through\n+        # autograd so no Jacobian test is needed\n+        for bias in (True, False):\n+            input = Variable(torch.randn(3, 10))\n+            hx = Variable(torch.randn(3, 20))\n+            cx = Variable(torch.randn(3, 20))\n+            lstm = nn.rnn.cell.LSTM(10, 20, bias=bias)\n+            for i in range(6):\n+                hx, cx = lstm(input, (hx, cx))\n+\n+            (hx+cx).sum().backward()\n+\n+    @unittest.skipIf(not TEST_CUDNN, \"needs cudnn\")\n+    def test_RNN_cpu_vs_cudnn(self):\n+\n+        def forward_backward(cuda, module, bias, input_val, hx_val, weights_val):\n+            rnn = module(input_size, hidden_size, num_layers, bias=bias)\n+            is_lstm = module == nn.rnn.LSTM\n+\n+            for x_layer, y_layer in zip(rnn.all_weights, weights_val):\n+                for x, y in zip(x_layer, y_layer):\n+                    x.data.copy_(y.data)\n+\n+            input = Variable(input_val.clone(), requires_grad=True)\n+            if is_lstm:\n+                hx = (Variable(hx_val.clone(), requires_grad=True),\n+                      Variable(hx_val.add(1), requires_grad=True))\n+            else:\n+                hx = Variable(hx_val.clone(), requires_grad=True)\n+\n+            if cuda:\n+                rnn.cuda()\n+                input.data = input.data.cuda()\n+                if is_lstm:\n+                    hx[0].data = hx[0].data.cuda()\n+                    hx[1].data = hx[1].data.cuda()\n+                else:\n+                    hx.data = hx.data.cuda()\n+\n+            output, hy = rnn(input, hx)\n+            # FIXME this is because of a pytorch bug\n+            if is_lstm:\n+                fake_loss = 0*(hy[0] + hy[1]).sum()\n+            else:\n+                fake_loss = 0*hy.sum()\n+\n+            loss = output.sum() + fake_loss\n+            loss.backward()\n+\n+            return {'output': output.data,\n+                    'hy': hy[0].data if is_lstm else hy.data,\n+                    'weights': rnn.all_weights,\n+                    'grad_input': input.grad,\n+                    'grad_hx': hx[0].grad if is_lstm else hx.grad,\n+                    'cy': hy[1].data if is_lstm else None,\n+                    'grad_cx': hx[1].grad if is_lstm else None}\n+\n+        def diff(t_cpu, t_gpu, name):\n+            self.assertTrue(torch.is_tensor(t_cpu))\n+            self.assertTrue(torch.is_tensor(t_gpu))\n+            delta = t_gpu.cpu().add(-1, t_cpu).abs().max()\n+            # print(\"{:30s} cpu: {:10g} gpu: {:10g} diff: {:10g}\".format(name, t_cpu.abs().max(), t_gpu.abs().max(), delta))\n+            self.assertLess(delta, 2 * PRECISION)\n+\n+        input_size = 10\n+        hidden_size = 20\n+        num_layers = 2\n+        seq_length = 7\n+        batch = 5\n+\n+        # FIXME: we can't use torch.cuda.DoubleTensor because sum() is not yet defined on it\n+        with set_default_tensor_type('torch.FloatTensor'):\n+            for module in (nn.rnn.RNNTanh, nn.rnn.RNNReLU, nn.rnn.LSTM, nn.rnn.GRU):\n+                for bias in (True, False):\n+                    input_val = torch.randn(seq_length, batch, input_size)\n+                    hx_val = torch.randn(num_layers, batch, hidden_size)\n+\n+                    weights_val = module(input_size, hidden_size, num_layers).all_weights\n+\n+                    outputs_cpu = forward_backward(False, module, bias, input_val, hx_val, weights_val)\n+                    outputs_gpu = forward_backward(True,  module, bias, input_val, hx_val, weights_val)\n+\n+                    diff(outputs_cpu['output'], outputs_gpu['output'], 'output')", "path": "test/test_nn.py", "position": null, "original_position": 127, "commit_id": "b5d13296c65e4b3cd5aa9715cf58df0fc043454e", "original_commit_id": "ccb1f401ff482f1fb25251272656149899758d4a", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "``` python\nself.assertEqual(list(outputs_cpu.keys()), list(outputs_gpu.keys()))\nfor out_cpu, out_gpu in zip(outputs_cpu.values(), outputs_gpu.values()):\n    self.assertEqual(out_cpu, out_gpu)\n```\n", "created_at": "2016-10-17T08:18:11Z", "updated_at": "2018-11-23T15:31:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/129#discussion_r83594558", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/129", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83594558"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/129#discussion_r83594558"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/129"}}, "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">self</span>.assertEqual(<span class=\"pl-c1\">list</span>(outputs_cpu.keys()), <span class=\"pl-c1\">list</span>(outputs_gpu.keys()))\n<span class=\"pl-k\">for</span> out_cpu, out_gpu <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(outputs_cpu.values(), outputs_gpu.values()):\n    <span class=\"pl-c1\">self</span>.assertEqual(out_cpu, out_gpu)</pre></div>", "body_text": "self.assertEqual(list(outputs_cpu.keys()), list(outputs_gpu.keys()))\nfor out_cpu, out_gpu in zip(outputs_cpu.values(), outputs_gpu.values()):\n    self.assertEqual(out_cpu, out_gpu)"}