{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83686578", "pull_request_review_id": 4513938, "id": 83686578, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDgzNjg2NTc4", "diff_hunk": "@@ -0,0 +1,408 @@\n+import torch.cuda\n+import torch.backends.cudnn as cudnn\n+from torch.backends.cudnn import check_error\n+import ctypes\n+\n+\n+def initDropoutDescriptor(fn, handle):\n+    dropout_desc = cudnn.DropoutDescriptor()\n+\n+    dropout_states_size = ctypes.c_long()\n+    check_error(cudnn.lib.cudnnDropoutGetStatesSize(\n+        handle,\n+        ctypes.byref(dropout_states_size)))\n+\n+    dropout_states = torch.cuda.ByteTensor(dropout_states_size.value)\n+    dropout_desc.set(\n+        handle,\n+        fn.dropout,\n+        dropout_states,\n+        fn.seed\n+    )\n+    return dropout_desc\n+\n+\n+def initRNNDescriptor(fn):\n+    rnn_desc = cudnn.RNNDescriptor()\n+\n+    rnn_desc.set(\n+        fn.hidden_size,\n+        fn.num_layers,\n+        fn.dropout_desc,\n+        fn.input_mode,\n+        fn.bidirectional,\n+        fn.mode,\n+        fn.datatype\n+    )\n+    return rnn_desc\n+\n+\n+def initWeightDescriptor(fn, weight):\n+    w_desc = cudnn.FilterDescriptor()\n+    w_view = weight.view(-1, 1, 1)  # seems that filters require >=3 dimensions\n+    w_desc.set(w_view)\n+    return w_desc\n+\n+\n+def _inputSize(fn):\n+    return (fn.seq_length, fn.mini_batch, fn.input_size)\n+\n+\n+def _hiddenSize(fn):\n+    return (fn.num_layers * fn.num_directions, fn.mini_batch, fn.hidden_size)\n+\n+\n+def _outputSize(fn):\n+    return (fn.seq_length, fn.mini_batch, fn.hidden_size * fn.num_directions)\n+\n+\n+def getNumWeights(handle, rnn_desc, x_desc, datatype):\n+    weight_size = ctypes.c_long()\n+    check_error(cudnn.lib.cudnnGetRNNParamsSize(\n+        handle,\n+        rnn_desc,\n+        x_desc,\n+        ctypes.byref(weight_size),\n+        datatype\n+    ))\n+    elem_size = cudnn._sizeofmap[datatype]\n+    assert(weight_size.value % elem_size == 0)\n+    return weight_size.value // elem_size\n+\n+\n+def getParameters(fn, handle, weight_buf):\n+\n+    cudnn_methods = [\n+        cudnn.lib.cudnnGetRNNLinLayerMatrixParams,\n+        cudnn.lib.cudnnGetRNNLinLayerBiasParams\n+    ]\n+\n+    # if fn.mode == cudnn.CUDNN_RNN_RELU or fn.mode == cudnn.CUDNN_RNN_TANH:\n+    #     linear_name = [\"ih\", \"hh\"]\n+    # elif fn.mode == cudnn.CUDNN_LSTM:\n+    #     linear_name = [\"ii\", \"if\", \"ic\", \"io\", \"hi\", \"hf\", \"hc\", \"ho\"]\n+    # elif fn.mode == cudnn.CUDNN_GRU:\n+    #     linear_name = [\"ir\", \"iu\", \"ic\", \"hr\", \"hu\", \"hc\"]\n+    # else:\n+    #     raise Exception(\"Unknown mode: {}\".format(fn.mode))\n+\n+    params = []\n+    num_linear_layers = _numLinearLayers(fn)\n+    num_layers = fn.num_directions * fn.num_layers\n+    for layer in range(num_layers):\n+        layer_params = []\n+        for cudnn_method in cudnn_methods:\n+            for linear_id in range(num_linear_layers):\n+                lin_layer_mat_desc = cudnn.FilterDescriptor()\n+                matrix_pointer = ctypes.c_void_p()\n+                check_error(cudnn_method(\n+                    handle,\n+                    fn.rnn_desc,\n+                    layer,\n+                    fn.x_descs[0],\n+                    fn.w_desc,\n+                    ctypes.c_void_p(weight_buf.data_ptr()),\n+                    linear_id,\n+                    lin_layer_mat_desc,\n+                    ctypes.byref(matrix_pointer)))\n+\n+                data_type = ctypes.c_int()\n+                format = ctypes.c_int()\n+                nb_dims = ctypes.c_int()\n+                min_dim = 3\n+                filter_dim_a = torch.IntTensor(min_dim)\n+                check_error(cudnn.lib.cudnnGetFilterNdDescriptor(\n+                    lin_layer_mat_desc,\n+                    min_dim,\n+                    ctypes.byref(data_type),\n+                    ctypes.byref(format),\n+                    ctypes.byref(nb_dims),\n+                    ctypes.c_void_p(filter_dim_a.data_ptr())))\n+\n+                filter_dim_a.resize_(nb_dims.value)\n+                elem_size = cudnn._sizeofmap[fn.datatype]\n+                offset_bytes = (matrix_pointer.value - weight_buf.data_ptr())\n+                assert(offset_bytes % elem_size == 0)\n+                offset = offset_bytes // elem_size\n+\n+                # for all the RNN types provided by CUDNN, all the ih weights\n+                # are the same size and are allocated in a contiguous chunk\n+                # (same for the hh weights, and the ih and hh biases).\n+                # Since we're storing all the weights in a single tensor anyway,\n+                # might as well merge the CUDNN ones into a single tensor as well\n+                if linear_id == 0 or linear_id == num_linear_layers / 2:\n+                    assert(filter_dim_a.prod() == filter_dim_a[0])\n+                    param = fn.weight_buf.new().set_(\n+                        weight_buf.storage(), offset,\n+                        filter_dim_a[0] * num_linear_layers / 2, filter_dim_a[2])\n+                    layer_params.append(param)\n+                else:\n+                    assert(cur_offset == offset)\n+\n+                cur_offset = offset + filter_dim_a[0]\n+\n+\n+        params.append(layer_params)\n+\n+    return params\n+\n+\n+def _copyParams(params_from, params_to):\n+    for layer_params_from, layer_params_to in zip(params_from, params_to):\n+        for param_from, param_to in zip(layer_params_from, layer_params_to):\n+            assert(param_from.type() == param_to.type())\n+            param_to.copy_(param_from)\n+\n+\n+def forward(fn, input, hx, weight, output, hy):\n+    with torch.cuda.device_of(input):\n+        lib = cudnn.lib\n+        handle = cudnn.get_handle()\n+        fn.datatype = cudnn._typemap[input.type()]\n+\n+        if fn.mode == cudnn.CUDNN_LSTM:\n+            hx, cx = hx\n+            hy, cy = hy\n+        else:\n+            cx, cy = None, None\n+\n+        if fn.batch_first:\n+            input = input.transpose(0, 1)\n+\n+        if input.dim() != 3:\n+            raise Exception(\n+                'input must have 3 dimensions: seq_length, mini_batch, input_size')\n+        if fn.input_size != input.size(2):\n+            raise Exception('input.size(2) must be equal to input_size provided')\n+        if fn.dropout != 0 and cudnn.lib.version < 5103:\n+            raise Exception('dropout supported only in cudnn v5.1 and above')\n+\n+        fn.seq_length, fn.mini_batch, fn.input_size = input.size()\n+        hidden_size = _hiddenSize(fn)\n+        output_size = _outputSize(fn)\n+        x = input.contiguous()\n+        output.resize_(*output_size)\n+        hy.resize_(*hidden_size).zero_()\n+        if cy:\n+            cy.resize_(*hidden_size).zero_()\n+        y = output\n+\n+        # init descriptors\n+        fn.dropout_desc = initDropoutDescriptor(fn, handle)\n+        fn.rnn_desc     = initRNNDescriptor(fn)", "path": "torch/backends/cudnn/rnn.py", "position": null, "original_position": 192, "commit_id": "b5d13296c65e4b3cd5aa9715cf58df0fc043454e", "original_commit_id": "ccb1f401ff482f1fb25251272656149899758d4a", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "body": "Well, that was the worry, but based on my experiments from https://github.com/pytorch/examples/pull/4 , which is a very cheap RNN, it seems like all the overhead doesn't add more than 5% relative to lua-torch\n", "created_at": "2016-10-17T16:54:21Z", "updated_at": "2018-11-23T15:31:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/129#discussion_r83686578", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/129", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83686578"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/129#discussion_r83686578"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/129"}}, "body_html": "<p>Well, that was the worry, but based on my experiments from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"181225788\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/examples/issues/4\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/examples/pull/4/hovercard\" href=\"https://github.com/pytorch/examples/pull/4\">pytorch/examples#4</a> , which is a very cheap RNN, it seems like all the overhead doesn't add more than 5% relative to lua-torch</p>", "body_text": "Well, that was the worry, but based on my experiments from pytorch/examples#4 , which is a very cheap RNN, it seems like all the overhead doesn't add more than 5% relative to lua-torch", "in_reply_to_id": 83599289}