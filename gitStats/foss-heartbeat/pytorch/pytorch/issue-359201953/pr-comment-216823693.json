{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216823693", "pull_request_review_id": 154395617, "id": 216823693, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjgyMzY5Mw==", "diff_hunk": "@@ -531,13 +531,252 @@ Python-defined Constants\n Debugging\n ~~~~~~~~~\n \n-Print things\n+Interpreting Graphs\n+    TorchScript uses a static single assignment (SSA) intermediate representation\n+    (IR) to represent computation. The instructions in this format consist of\n+    ATen operators and other primitive operators, including control flow\n+    operators for loops and conditionals. As an example::\n \n-Use ``USE_PYTHON=0`` to debug in normal python mode\n+        @torch.jit.script\n+        def foo(len):\n+          # type: (int) -> torch.Tensor\n+          rv = torch.zeros(3, 4)\n+          for i in range(len):\n+            if i < 10:\n+                rv = rv - 1.0\n+            else:\n+                rv = rv + 1.0\n+          return rv\n+\n+        print(foo.graph)\n+\n+    A ``ScriptModule`` with a single ``forward`` method will have an attribute\n+    ``graph``, which you can use to inspect the IR representing the computation.\n+    If the ScriptModule has more than one method, you will need to access\n+    ``.graph`` on the method itself and not the module. We can inspect the\n+    graph of a method named ``bar`` on a ScriptModule by accessing ``.bar.graph``.\n+\n+    The example script above produces the graph::\n+\n+        graph(%len : int) {\n+          %13 : float = prim::Constant[value=1]()\n+          %10 : int = prim::Constant[value=10]()\n+          %2 : int = prim::Constant[value=4]()\n+          %1 : int = prim::Constant[value=3]()\n+          %3 : int[] = prim::ListConstruct(%1, %2)\n+          %4 : int = prim::Constant[value=6]()\n+          %5 : int = prim::Constant[value=0]()\n+          %6 : int[] = prim::Constant[value=[0, -1]]()\n+          %rv.1 : Dynamic = aten::zeros(%3, %4, %5, %6)\n+          %8 : int = prim::Constant[value=1]()\n+          %rv : Dynamic = prim::Loop(%len, %8, %rv.1)\n+            block0(%i : int, %12 : Dynamic) {\n+              %11 : int = aten::lt(%i, %10)\n+              %rv.4 : Dynamic = prim::If(%11)\n+                block0() {\n+                  %14 : int = prim::Constant[value=1]()\n+                  %rv.2 : Dynamic = aten::sub(%12, %13, %14)\n+                  -> (%rv.2)\n+                }\n+                block1() {\n+                  %16 : int = prim::Constant[value=1]()\n+                  %rv.3 : Dynamic = aten::add(%12, %13, %16)\n+                  -> (%rv.3)\n+                }\n+              %19 : int = prim::Constant[value=1]()\n+              -> (%19, %rv.4)\n+            }\n+          return (%rv);\n+        }\n+\n+    Take the instruction ``%rv.1 : Dynamic = aten::zeros(%3, %4, %5, %6)`` for\n+    example. ``%rv.1 : Dynamic`` means we assign the output to a (unique)\n+    value named ``rv.1``, and that value is of ``Dynamic`` type, i.e. we do\n+    not know its concrete shape. ``aten::zeros`` is the operator (equivalent\n+    to ``torch.zeros``) and the input list ``(%3, %4, %5, %6)`` specifies which\n+    values in scope should be passed as inputs. The schema for built-in functions\n+    like ``aten::zeros`` can be found at `Builtin Functions`_.\n+\n+    Notice that operators can also have associated ``blocks``, namely the\n+    ``prim::Loop`` and ``prim::If`` operators. In the graph print-out, these\n+    operators are formatted to reflect their equivalent source code forms\n+    to facilitate easy debugging.\n+\n+    Graphs can be inspected as shown to confirm that the computation described\n+    by a ``ScriptModule`` is correct, in both automated and manual fashion, as\n+    described below.\n+\n+\n+Tracing Edge Cases\n+    There are some edge cases that exist where the trace of a given Python\n+    function/module will not be representative of the underlying code. These\n+    cases can include:\n+\n+    * Tracing of control flow that is dependent on inputs (e.g. tensor shapes)\n+    * Tracing of in-place operations of tensor views (e.g. indexing on the\n+      left-hand side of an assignment)\n+\n+    Note that these cases may in fact be traceable in the future.\n+\n+\n+Automatic Trace Checking\n+    A way to automatically detect such edge cases is by using ``check_inputs``\n+    on the ``torch.jit.trace()`` API. ``check_inputs`` takes a list of tuples\n+    of inputs that will be used to re-trace the computation and verify the\n+    results. For example::\n+\n+        def loop_in_traced_fn(x):\n+            result = x[0]\n+            for i in range(x.size(0)):\n+                result = result * x[i]\n+            return result\n+\n+        inputs = (torch.rand(3, 4, 5),)\n+        check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n+\n+        traced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\n+\n+    Gives us the following diagnostic information::\n+\n+        ERROR: Graphs differed across invocations!\n+        Graph diff:\n+            graph(%0 : Dynamic) {\n+                  %1 : int = prim::Constant[value=0]()\n+                  %2 : int = prim::Constant[value=0]()\n+                  %3 : Dynamic = aten::select(%0, %1, %2)\n+                  %4 : int = prim::Constant[value=0]()\n+                  %5 : int = prim::Constant[value=0]()\n+                  %6 : Dynamic = aten::select(%0, %4, %5)\n+                  %7 : Dynamic = aten::mul(%3, %6)\n+                  %8 : int = prim::Constant[value=0]()\n+                  %9 : int = prim::Constant[value=1]()\n+                  %10 : Dynamic = aten::select(%0, %8, %9)\n+                  %11 : Dynamic = aten::mul(%7, %10)\n+                  %12 : int = prim::Constant[value=0]()\n+                  %13 : int = prim::Constant[value=2]()\n+                  %14 : Dynamic = aten::select(%0, %12, %13)\n+                  %15 : Dynamic = aten::mul(%11, %14)\n+              +   %16 : int = prim::Constant[value=0]()\n+              +   %17 : int = prim::Constant[value=3]()\n+              +   %18 : Dynamic = aten::select(%0, %16, %17)\n+              +   %19 : Dynamic = aten::mul(%15, %18)\n+              -   return (%15);\n+              ?             ^\n+              +   return (%19);\n+              ?             ^\n+            }\n+\n+\n+    This message indicates to us that the computation differed between when\n+    we first traced it and when we traced it with the ``check_inputs``. Indeed,\n+    the loop within the body of ``loop_in_traced_fn`` depends on the shape\n+    of the input ``x``, and thus when we try another ``x`` with a different\n+    shape, the trace differs.\n+\n+    In this case, data-dependent control flow like this can be captured using\n+    script instead::\n+\n+        def fn(x):\n+            result = x[0]\n+            for i in range(x.size(0)):\n+                result = result * x[i]\n+            return result\n+\n+        inputs = (torch.rand(3, 4, 5),)\n+        check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n+\n+        scripted_fn = torch.jit.script(fn)\n+        print(scripted_fn.graph)\n+\n+        for input_tuple in [inputs] + check_inputs:\n+            torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))\n+\n+\n+    Which produces::\n+\n+        graph(%x : Dynamic) {\n+          %1 : int = prim::Constant[value=0]()\n+          %2 : int = prim::Constant[value=0]()\n+          %result.1 : Dynamic = aten::select(%x, %2, %1)\n+          %4 : int = aten::size(%x, %1)\n+          %5 : int = prim::Constant[value=1]()\n+          %result : Dynamic = prim::Loop(%4, %5, %result.1)\n+            block0(%i : int, %7 : Dynamic) {\n+              %9 : int = prim::Constant[value=0]()\n+              %10 : Dynamic = aten::select(%x, %9, %i)\n+              %result.2 : Dynamic = aten::mul(%7, %10)\n+              %12 : int = prim::Constant[value=1]()\n+              -> (%12, %result.2)\n+            }\n+          return (%result);\n+        }\n+\n+\n+Tracer Warnings\n+    The tracer produces warnings for several problematic patterns in traced\n+    computation. As an example, take a trace of a function that contains an\n+    in-place assignment on a slice (a view) of a Tensor::\n+\n+        def fill_row_zero(x):\n+            x[0] = torch.rand(*x.shape[1:2])\n+            return x\n+\n+        traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\n+        print(traced.graph)\n+\n+\n+    Produces several warnings and a graph which simply returns the input::\n+\n+        update_view.py:4: TracerWarning: There are 2 live references to the tensor being modified when tracing in-place operator copy_ (possibly due to an assignment) which  might cause the trace to be incorrect. We can't record the data flow of  Python values, which means the trace might not generalize to other inputs.\n+          x[0] = torch.rand(*x.shape[1:2])\n+        update_view.py:7: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n+        Not within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.8348613381385803 vs. 0.09187901020050049) and 3 other locations (33.00%)\n+          traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\n+        graph(%0 : Float(3, 4)) {\n+          return (%0);\n+        }\n+\n+    We can fix this by modifying the code to not use the in-place update, but\n+    rather build up the result tensor out-of-place with `torch.cat`::\n+\n+        def fill_row_zero(x):\n+            x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)\n+            return x\n+\n+        traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\n+        print(traced.graph)\n+        traced(torch.rand(5, 6))\n+\n+\n+Disable JIT for Debugging", "path": "docs/source/jit.rst", "position": null, "original_position": 223, "commit_id": "29e8cf0aa61dffa88f49bd06d662521eedb951f2", "original_commit_id": "46d9797b3dab7a644228c1715f01ff579799e1e9", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "That should appear much higher in this list", "created_at": "2018-09-11T21:10:07Z", "updated_at": "2018-11-23T15:51:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/11540#discussion_r216823693", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11540", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216823693"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11540#discussion_r216823693"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11540"}}, "body_html": "<p>That should appear much higher in this list</p>", "body_text": "That should appear much higher in this list"}