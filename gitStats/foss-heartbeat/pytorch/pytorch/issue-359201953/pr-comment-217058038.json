{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217058038", "pull_request_review_id": 154681097, "id": 217058038, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzA1ODAzOA==", "diff_hunk": "@@ -531,13 +531,253 @@ Python-defined Constants\n Debugging\n ~~~~~~~~~\n \n-Print things\n+Disable JIT for Debugging\n+    If you want to disable all JIT modes (tracing and scripting) so you can\n+    debug your program in raw Python, you can use the ``PYTORCH_JIT`` environment\n+    variable. ``PYTORCH_JIT`` can be used to globally disable the\n+    JIT by setting its value to ``0``. Given an example script::\n \n-Use ``USE_PYTHON=0`` to debug in normal python mode\n+        @torch.jit.script\n+        def scripted_fn(x : torch.Tensor):\n+            for i in range(12):\n+                x = x + x\n+            return x\n+\n+\n+        def fn(x):\n+            x = torch.neg(x)\n+            import pdb; pdb.set_trace()\n+            return scripted_fn(x)\n+\n+        traced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\n+\n+        traced_fn(torch.rand(3, 4))\n \n-Look at the graph\n+    Debugging this script with PDB works except for when we invoke the @script\n+    function. We can globally disable JIT, so that we can call the @script\n+    function as a normal python function and not compile it. If the above script\n+    is called ``disable_jit_example.py``, we can invoke it like so::\n \n-Pay attention to tracer warnings\n+        $ PYTORCH_JIT=0 python disable_jit_example.py\n+\n+    and we will be able to step into the @script function as a normal Python\n+    function.\n+\n+\n+Interpreting Graphs\n+    TorchScript uses a static single assignment (SSA) intermediate representation\n+    (IR) to represent computation. The instructions in this format consist of\n+    ATen (the C++ backend of PyTorch) operators and other primitive operators,\n+    including control flow operators for loops and conditionals. As an example::\n+\n+        @torch.jit.script\n+        def foo(len):\n+          # type: (int) -> torch.Tensor\n+          rv = torch.zeros(3, 4)\n+          for i in range(len):\n+            if i < 10:\n+                rv = rv - 1.0\n+            else:\n+                rv = rv + 1.0\n+          return rv\n+\n+        print(foo.graph)\n+\n+    A ``ScriptModule`` with a single ``forward`` method will have an attribute\n+    ``graph``, which you can use to inspect the IR representing the computation.\n+    If the ScriptModule has more than one method, you will need to access\n+    ``.graph`` on the method itself and not the module. We can inspect the\n+    graph of a method named ``bar`` on a ScriptModule by accessing ``.bar.graph``.\n+\n+    The example script above produces the graph::\n+\n+        graph(%len : int) {\n+          %13 : float = prim::Constant[value=1]()\n+          %10 : int = prim::Constant[value=10]()\n+          %2 : int = prim::Constant[value=4]()\n+          %1 : int = prim::Constant[value=3]()\n+          %3 : int[] = prim::ListConstruct(%1, %2)\n+          %4 : int = prim::Constant[value=6]()\n+          %5 : int = prim::Constant[value=0]()\n+          %6 : int[] = prim::Constant[value=[0, -1]]()\n+          %rv.1 : Dynamic = aten::zeros(%3, %4, %5, %6)\n+          %8 : int = prim::Constant[value=1]()\n+          %rv : Dynamic = prim::Loop(%len, %8, %rv.1)\n+            block0(%i : int, %12 : Dynamic) {\n+              %11 : int = aten::lt(%i, %10)\n+              %rv.4 : Dynamic = prim::If(%11)\n+                block0() {\n+                  %14 : int = prim::Constant[value=1]()\n+                  %rv.2 : Dynamic = aten::sub(%12, %13, %14)\n+                  -> (%rv.2)\n+                }\n+                block1() {\n+                  %16 : int = prim::Constant[value=1]()\n+                  %rv.3 : Dynamic = aten::add(%12, %13, %16)\n+                  -> (%rv.3)\n+                }\n+              %19 : int = prim::Constant[value=1]()\n+              -> (%19, %rv.4)\n+            }\n+          return (%rv);\n+        }\n+\n+    Take the instruction ``%rv.1 : Dynamic = aten::zeros(%3, %4, %5, %6)`` for\n+    example. ``%rv.1 : Dynamic`` means we assign the output to a (unique)\n+    value named ``rv.1``, and that value is of ``Dynamic`` type, i.e. we do\n+    not know its concrete shape. ``aten::zeros`` is the operator (equivalent\n+    to ``torch.zeros``) and the input list ``(%3, %4, %5, %6)`` specifies which\n+    values in scope should be passed as inputs. The schema for built-in functions\n+    like ``aten::zeros`` can be found at `Builtin Functions`_.\n+\n+    Notice that operators can also have associated ``blocks``, namely the\n+    ``prim::Loop`` and ``prim::If`` operators. In the graph print-out, these\n+    operators are formatted to reflect their equivalent source code forms\n+    to facilitate easy debugging.\n+\n+    Graphs can be inspected as shown to confirm that the computation described\n+    by a ``ScriptModule`` is correct, in both automated and manual fashion, as\n+    described below.\n+\n+\n+Tracing Edge Cases\n+    There are some edge cases that exist where the trace of a given Python\n+    function/module will not be representative of the underlying code. These\n+    cases can include:\n+\n+    * Tracing of control flow that is dependent on inputs (e.g. tensor shapes)\n+    * Tracing of in-place operations of tensor views (e.g. indexing on the\n+      left-hand side of an assignment)\n+\n+    Note that these cases may in fact be traceable in the future.\n+\n+\n+Automatic Trace Checking\n+    One way to automatically catch many errors in traces is by using ``check_inputs``\n+    on the ``torch.jit.trace()`` API. ``check_inputs`` takes a list of tuples\n+    of inputs that will be used to re-trace the computation and verify the\n+    results. For example::\n+\n+        def loop_in_traced_fn(x):\n+            result = x[0]\n+            for i in range(x.size(0)):\n+                result = result * x[i]\n+            return result\n+\n+        inputs = (torch.rand(3, 4, 5),)\n+        check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n+\n+        traced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\n+\n+    Gives us the following diagnostic information::\n+\n+        ERROR: Graphs differed across invocations!\n+        Graph diff:\n+            graph(%0 : Dynamic) {\n+                  %1 : int = prim::Constant[value=0]()\n+                  %2 : int = prim::Constant[value=0]()\n+                  %3 : Dynamic = aten::select(%0, %1, %2)\n+                  %4 : int = prim::Constant[value=0]()\n+                  %5 : int = prim::Constant[value=0]()\n+                  %6 : Dynamic = aten::select(%0, %4, %5)\n+                  %7 : Dynamic = aten::mul(%3, %6)\n+                  %8 : int = prim::Constant[value=0]()\n+                  %9 : int = prim::Constant[value=1]()\n+                  %10 : Dynamic = aten::select(%0, %8, %9)\n+                  %11 : Dynamic = aten::mul(%7, %10)\n+                  %12 : int = prim::Constant[value=0]()\n+                  %13 : int = prim::Constant[value=2]()\n+                  %14 : Dynamic = aten::select(%0, %12, %13)\n+                  %15 : Dynamic = aten::mul(%11, %14)\n+              +   %16 : int = prim::Constant[value=0]()\n+              +   %17 : int = prim::Constant[value=3]()\n+              +   %18 : Dynamic = aten::select(%0, %16, %17)\n+              +   %19 : Dynamic = aten::mul(%15, %18)\n+              -   return (%15);\n+              ?             ^\n+              +   return (%19);\n+              ?             ^\n+            }\n+\n+\n+    This message indicates to us that the computation differed between when\n+    we first traced it and when we traced it with the ``check_inputs``. Indeed,\n+    the loop within the body of ``loop_in_traced_fn`` depends on the shape\n+    of the input ``x``, and thus when we try another ``x`` with a different\n+    shape, the trace differs.\n+\n+    In this case, data-dependent control flow like this can be captured using\n+    script instead::\n+\n+        def fn(x):\n+            result = x[0]\n+            for i in range(x.size(0)):\n+                result = result * x[i]\n+            return result\n+\n+        inputs = (torch.rand(3, 4, 5),)\n+        check_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n+\n+        scripted_fn = torch.jit.script(fn)\n+        print(scripted_fn.graph)\n+\n+        for input_tuple in [inputs] + check_inputs:\n+            torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))\n+\n+\n+    Which produces::\n+\n+        graph(%x : Dynamic) {\n+          %1 : int = prim::Constant[value=0]()\n+          %2 : int = prim::Constant[value=0]()\n+          %result.1 : Dynamic = aten::select(%x, %2, %1)\n+          %4 : int = aten::size(%x, %1)\n+          %5 : int = prim::Constant[value=1]()\n+          %result : Dynamic = prim::Loop(%4, %5, %result.1)\n+            block0(%i : int, %7 : Dynamic) {\n+              %9 : int = prim::Constant[value=0]()\n+              %10 : Dynamic = aten::select(%x, %9, %i)\n+              %result.2 : Dynamic = aten::mul(%7, %10)\n+              %12 : int = prim::Constant[value=1]()\n+              -> (%12, %result.2)\n+            }\n+          return (%result);\n+        }\n+\n+\n+Tracer Warnings\n+    The tracer produces warnings for several problematic patterns in traced\n+    computation. As an example, take a trace of a function that contains an\n+    in-place assignment on a slice (a view) of a Tensor::\n+\n+        def fill_row_zero(x):\n+            x[0] = torch.rand(*x.shape[1:2])\n+            return x\n+\n+        traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\n+        print(traced.graph)\n+\n+\n+    Produces several warnings and a graph which simply returns the input::\n+\n+        update_view.py:4: TracerWarning: There are 2 live references to the tensor being modified when tracing in-place operator copy_ (possibly due to an assignment) which  might cause the trace to be incorrect. We can't record the data flow of  Python values, which means the trace might not generalize to other inputs.", "path": "docs/source/jit.rst", "position": null, "original_position": 237, "commit_id": "29e8cf0aa61dffa88f49bd06d662521eedb951f2", "original_commit_id": "d6a12fe58ccb40c3304f3fe4ef044d6eb6fd573d", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "The warning PR has already landed.", "created_at": "2018-09-12T14:35:10Z", "updated_at": "2018-11-23T15:51:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/11540#discussion_r217058038", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11540", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217058038"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11540#discussion_r217058038"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11540"}}, "body_html": "<p>The warning PR has already landed.</p>", "body_text": "The warning PR has already landed.", "in_reply_to_id": 216856668}