{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/294306868", "html_url": "https://github.com/pytorch/pytorch/issues/488#issuecomment-294306868", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/488", "id": 294306868, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDMwNjg2OA==", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-15T17:25:45Z", "updated_at": "2017-04-16T08:19:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Update on cuda-on-cl: I've added in some cudnn api implementations, notably pooling, convolutions, sigmoid, tanh, relu activations, and softmax forward, which I think is sufficient now to build and run <a href=\"https://github.com/tbennun/cudnn-training\">a cudnn implementation of lenet</a>, with some <a href=\"https://github.com/tbennun/cudnn-training/compare/master...hughperkins:opencl\">minor CMakeLists.txt tweaks</a>. I dont imagine this is sufficient for pytorch to simply build and run for OpenCL yet :-) , but it's a few more steps in that general direction.</p>\n<p><del>(I think if pytorch is anything like luatorch, one key not-hard-but-need-to-do thing is linking. currently each cuda file is considered in a standalone way, whereas in lua torch, it's important to be able to run cuda functions from other modules. this is already partially implemented, by providing a facility for each module to register its source-code <a href=\"https://github.com/hughperkins/cuda-on-cl/blob/c15328d49dc8c5700230cd8ec4fd3efe2ada7a79/src/cocl_clsources.cpp\">https://github.com/hughperkins/cuda-on-cl/blob/c15328d49dc8c5700230cd8ec4fd3efe2ada7a79/src/cocl_clsources.cpp</a> , but I never quite got round to hooking this into each module's startup, I think. This would happen by patching the llvm hostside code to call the registration method in its global initializers. <a href=\"https://github.com/hughperkins/cuda-on-cl/blob/master/src/patch-hostside.cpp\">patchhostside.cpp</a> is the code that handles hostside llvm patching, and would need to be extended to handle this.<br>\n)</del></p>\n<p>Edit: realized that actually probably dont need cross-module linking in fact, since reduce etc are in templated headerfiles, which should be already handled ok.</p>", "body_text": "Update on cuda-on-cl: I've added in some cudnn api implementations, notably pooling, convolutions, sigmoid, tanh, relu activations, and softmax forward, which I think is sufficient now to build and run a cudnn implementation of lenet, with some minor CMakeLists.txt tweaks. I dont imagine this is sufficient for pytorch to simply build and run for OpenCL yet :-) , but it's a few more steps in that general direction.\n(I think if pytorch is anything like luatorch, one key not-hard-but-need-to-do thing is linking. currently each cuda file is considered in a standalone way, whereas in lua torch, it's important to be able to run cuda functions from other modules. this is already partially implemented, by providing a facility for each module to register its source-code https://github.com/hughperkins/cuda-on-cl/blob/c15328d49dc8c5700230cd8ec4fd3efe2ada7a79/src/cocl_clsources.cpp , but I never quite got round to hooking this into each module's startup, I think. This would happen by patching the llvm hostside code to call the registration method in its global initializers. patchhostside.cpp is the code that handles hostside llvm patching, and would need to be extended to handle this.\n)\nEdit: realized that actually probably dont need cross-module linking in fact, since reduce etc are in templated headerfiles, which should be already handled ok.", "body": "Update on cuda-on-cl: I've added in some cudnn api implementations, notably pooling, convolutions, sigmoid, tanh, relu activations, and softmax forward, which I think is sufficient now to build and run [a cudnn implementation of lenet](https://github.com/tbennun/cudnn-training), with some [minor CMakeLists.txt tweaks](https://github.com/tbennun/cudnn-training/compare/master...hughperkins:opencl). I dont imagine this is sufficient for pytorch to simply build and run for OpenCL yet :-) , but it's a few more steps in that general direction.\r\n\r\n~~(I think if pytorch is anything like luatorch, one key not-hard-but-need-to-do thing is linking. currently each cuda file is considered in a standalone way, whereas in lua torch, it's important to be able to run cuda functions from other modules. this is already partially implemented, by providing a facility for each module to register its source-code https://github.com/hughperkins/cuda-on-cl/blob/c15328d49dc8c5700230cd8ec4fd3efe2ada7a79/src/cocl_clsources.cpp , but I never quite got round to hooking this into each module's startup, I think. This would happen by patching the llvm hostside code to call the registration method in its global initializers. [patchhostside.cpp](https://github.com/hughperkins/cuda-on-cl/blob/master/src/patch-hostside.cpp) is the code that handles hostside llvm patching, and would need to be extended to handle this.\r\n)~~\r\n\r\nEdit: realized that actually probably dont need cross-module linking in fact, since reduce etc are in templated headerfiles, which should be already handled ok."}