{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/273626736", "html_url": "https://github.com/pytorch/pytorch/issues/488#issuecomment-273626736", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/488", "id": 273626736, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzYyNjczNg==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-18T22:47:21Z", "updated_at": "2017-01-18T22:47:21Z", "author_association": "MEMBER", "body_html": "<p>I understand where you are coming from.</p>\n<p>We officially are not planning any OpenCL work because:</p>\n<ul>\n<li>AMD itself seems to be moving towards HIP / GPUOpen which has a CUDA transpiler (and they've done some work on transpiling Torch's backend)</li>\n<li>Intel is moving it's speed and optimization value into MKLDNN</li>\n<li>Generic OpenCL support has strictly worse performance than using CUDA/HIP/MKLDNN where appropriate.</li>\n</ul>\n<p>Hugh's OpenCL port is limited to the most popular layers, and does not port the 250+ C functions that need to be ported to get a fully functional OpenCL backend going. It makes sense for an extension to port part of the core value as there is no expectation of full parity, but the official / core distribution cannot get away with this -- users expect full API parity.</p>\n<p>So, considering that it's a humongous effort to do OpenCL support, and considering the weight of the upside, we wont be working on it.</p>\n<p>However, if anyone in the community wants to give a go, feel free to :)</p>", "body_text": "I understand where you are coming from.\nWe officially are not planning any OpenCL work because:\n\nAMD itself seems to be moving towards HIP / GPUOpen which has a CUDA transpiler (and they've done some work on transpiling Torch's backend)\nIntel is moving it's speed and optimization value into MKLDNN\nGeneric OpenCL support has strictly worse performance than using CUDA/HIP/MKLDNN where appropriate.\n\nHugh's OpenCL port is limited to the most popular layers, and does not port the 250+ C functions that need to be ported to get a fully functional OpenCL backend going. It makes sense for an extension to port part of the core value as there is no expectation of full parity, but the official / core distribution cannot get away with this -- users expect full API parity.\nSo, considering that it's a humongous effort to do OpenCL support, and considering the weight of the upside, we wont be working on it.\nHowever, if anyone in the community wants to give a go, feel free to :)", "body": "I understand where you are coming from. \r\n\r\nWe officially are not planning any OpenCL work because:\r\n- AMD itself seems to be moving towards HIP / GPUOpen which has a CUDA transpiler (and they've done some work on transpiling Torch's backend)\r\n- Intel is moving it's speed and optimization value into MKLDNN\r\n- Generic OpenCL support has strictly worse performance than using CUDA/HIP/MKLDNN where appropriate.\r\n\r\nHugh's OpenCL port is limited to the most popular layers, and does not port the 250+ C functions that need to be ported to get a fully functional OpenCL backend going. It makes sense for an extension to port part of the core value as there is no expectation of full parity, but the official / core distribution cannot get away with this -- users expect full API parity.\r\n\r\nSo, considering that it's a humongous effort to do OpenCL support, and considering the weight of the upside, we wont be working on it.\r\n\r\nHowever, if anyone in the community wants to give a go, feel free to :)\r\n\r\n"}