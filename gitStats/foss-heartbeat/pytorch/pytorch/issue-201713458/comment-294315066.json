{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/294315066", "html_url": "https://github.com/pytorch/pytorch/issues/488#issuecomment-294315066", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/488", "id": 294315066, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDMxNTA2Ng==", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-15T20:06:34Z", "updated_at": "2017-04-15T20:57:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p><del>Oh, will need the issue with <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGeneral.h.in\">https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGeneral.h.in</a> handling somehow (ie, it gets macrod by things like <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGenerateFloatTypes.h\">https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGenerateFloatTypes.h</a> )</del></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/123560/25066512/5e5f6a44-221f-11e7-802e-deb2a16abc20.png\"><img src=\"https://cloud.githubusercontent.com/assets/123560/25066512/5e5f6a44-221f-11e7-802e-deb2a16abc20.png\" alt=\"will_need_the_thcgeneral_h_in_handling_somehow\" style=\"max-width:100%;\"></a></p>\n<p>We need <code>THGeneral.h</code>, which we can can get like:</p>\n<pre><code># assume cloned pytorch as ~/git/pytorch, so:\nPTR=$HOME/git/pytorch\n\ncd $PTR/torch/lib/TH\nmkdir build\ncd build\nccmake .. -D\n# change CMAKE_INSTALL_PREFIX  to be ~/pytorch directory for you (change ~ to whatever your homedirectory is)\n# c then c then g\nmake -j 4\nmake install\n</code></pre>\n<p>... and it'll be in <code>~/pytorch/include/TH</code>. <del>But we also need THCGeneral.h, and trying the same technique gets stuck on configure:</del></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/123560/25066565/314d30a2-2221-11e7-871f-fc31d7219a30.png\"><img src=\"https://cloud.githubusercontent.com/assets/123560/25066565/314d30a2-2221-11e7-871f-fc31d7219a30.png\" alt=\"thc_cmake_fails\" style=\"max-width:100%;\"></a></p>\n<p><del>TorchConfig.cmake et al are not in $HOME/pytorch:</del></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/123560/25066567/488f7360-2221-11e7-9fa9-6da75bc5950d.png\"><img src=\"https://cloud.githubusercontent.com/assets/123560/25066567/488f7360-2221-11e7-9fa9-6da75bc5950d.png\" alt=\"pytorch_no_torchconfig_files\" style=\"max-width:100%;\"></a></p>\n<p>We also need <code>THCGeneral.h</code>. We can get this by doing:</p>\n<pre><code>cp $PTR/torch/lib/THC/THCGeneral.h.in ~/pytorch/include/THCGeneral.h\n# by hand, comment out line 13 of THCgeneral.h, ie this line:\n# // #cmakedefine USE_MAGMA\n</code></pre>\n<p>Then retry as follows:</p>\n<pre><code>cd $PTR/torch/lib/THCUNN\ncocl -I ~/pytorch/include/TH/ -I ~/pytorch/include -I .. -I ../THC/ -c Sigmoid.cu \n</code></pre>\n<p>This gives a bunch of undefined errors, stuff that needs adding to cuda-on-cl, ie <code>cudaTextureObject_t</code>. These should be added to one of the include files in <code>include/cocl</code> directory of cuda-on-cl project. There are a bunch of similar defines there that can be used as a basis/model/template.</p>\n<p>Current output:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/123560/25066634/bddd5978-2223-11e7-98cd-633429635e57.png\"><img src=\"https://cloud.githubusercontent.com/assets/123560/25066634/bddd5978-2223-11e7-98cd-633429635e57.png\" alt=\"output_1\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/123560/25066635/c1404e04-2223-11e7-9586-94285ff348b0.png\"><img src=\"https://cloud.githubusercontent.com/assets/123560/25066635/c1404e04-2223-11e7-9586-94285ff348b0.png\" alt=\"output_2\" style=\"max-width:100%;\"></a></p>\n<p>Edit 2: we can fix that by simply commenting out line 130 of <code>torch/lib/THC/generic/THCTensor.h</code>:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/123560/25066669/fa00968a-2224-11e7-9289-7ca5c0ba0332.png\"><img src=\"https://cloud.githubusercontent.com/assets/123560/25066669/fa00968a-2224-11e7-9289-7ca5c0ba0332.png\" alt=\"comment_out_line_130\" style=\"max-width:100%;\"></a></p>\n<p>New output:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/123560/25066672/08e0aeba-2225-11e7-94ad-97d5979ae5c4.png\"><img src=\"https://cloud.githubusercontent.com/assets/123560/25066672/08e0aeba-2225-11e7-94ad-97d5979ae5c4.png\" alt=\"thcunn_h_not_found\" style=\"max-width:100%;\"></a></p>\n<p>Edit 3: Add <code>-I</code> for current directory:</p>\n<pre><code>cocl -I ~/pytorch/include/TH/ -I ~/pytorch/include -I .. -I ../THC/ -I . -c Sigmoid.cu\n</code></pre>\n<p>New output:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/123560/25066687/49ef3962-2225-11e7-9ee5-94a759e459ef.png\"><img src=\"https://cloud.githubusercontent.com/assets/123560/25066687/49ef3962-2225-11e7-9ee5-94a759e459ef.png\" alt=\"output_3_1\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/123560/25066688/4c1c6534-2225-11e7-9026-d6cd8189055c.png\"><img src=\"https://cloud.githubusercontent.com/assets/123560/25066688/4c1c6534-2225-11e7-9026-d6cd8189055c.png\" alt=\"output_3_2\" style=\"max-width:100%;\"></a></p>\n<p>... reached some hacky cuda-on-cl stuff. The issue is it's tricky to come up with a definition of <code>min</code> that works in all the possible use-cases. There are a bunch of corner-cases to deal with. Some of the relevant codes/hacks is here:</p>\n<p><a href=\"https://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L106-L109\">https://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L106-L109</a></p>\n<pre><code>double max(double in1, double in2);\ndouble min(double in1, double in2);\nfloat max(float in1, float in2);\nfloat min(float in1, float in2);\n</code></pre>\n<p>various other previous attempts: <a href=\"https://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L87-L99\">https://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L87-L99</a></p>\n<pre><code>namespace cocl {\n   // double max(double in1, double in2);\n   // double min(double in, double in2);\n}\n// using cocl::max;\n// using cocl::min;\n\nextern \"C\" {\n// double our_pretend_tanh(double in);\n// double our_pretend_log(double in);\n// double our_pretend_exp(double in);\n// double our_pretend_max(double in1, double in2);\n// double our_pretend_min(double in1, double in2);\n</code></pre>\n<p><a href=\"https://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L114-L115\">https://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L114-L115</a></p>\n<pre><code>// #define max cocl::max\n// #define min cocl::min\n</code></pre>\n<p>So, first challenge is, figure out <del>a horrible hack</del> an elegant plan to get the <code>min</code>s to compile across all use-cases.</p>\n<p>Edit 4: looks like this use-case is using <code>min</code> against <code>long long</code>s. So might be sufficient to create a declaration of <code>min</code> that uses <code>long long</code>s. I think I might leave this to someone else to try though :-)</p>", "body_text": "Oh, will need the issue with https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGeneral.h.in handling somehow (ie, it gets macrod by things like https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGenerateFloatTypes.h )\n\nWe need THGeneral.h, which we can can get like:\n# assume cloned pytorch as ~/git/pytorch, so:\nPTR=$HOME/git/pytorch\n\ncd $PTR/torch/lib/TH\nmkdir build\ncd build\nccmake .. -D\n# change CMAKE_INSTALL_PREFIX  to be ~/pytorch directory for you (change ~ to whatever your homedirectory is)\n# c then c then g\nmake -j 4\nmake install\n\n... and it'll be in ~/pytorch/include/TH. But we also need THCGeneral.h, and trying the same technique gets stuck on configure:\n\nTorchConfig.cmake et al are not in $HOME/pytorch:\n\nWe also need THCGeneral.h. We can get this by doing:\ncp $PTR/torch/lib/THC/THCGeneral.h.in ~/pytorch/include/THCGeneral.h\n# by hand, comment out line 13 of THCgeneral.h, ie this line:\n# // #cmakedefine USE_MAGMA\n\nThen retry as follows:\ncd $PTR/torch/lib/THCUNN\ncocl -I ~/pytorch/include/TH/ -I ~/pytorch/include -I .. -I ../THC/ -c Sigmoid.cu \n\nThis gives a bunch of undefined errors, stuff that needs adding to cuda-on-cl, ie cudaTextureObject_t. These should be added to one of the include files in include/cocl directory of cuda-on-cl project. There are a bunch of similar defines there that can be used as a basis/model/template.\nCurrent output:\n\n\nEdit 2: we can fix that by simply commenting out line 130 of torch/lib/THC/generic/THCTensor.h:\n\nNew output:\n\nEdit 3: Add -I for current directory:\ncocl -I ~/pytorch/include/TH/ -I ~/pytorch/include -I .. -I ../THC/ -I . -c Sigmoid.cu\n\nNew output:\n\n\n... reached some hacky cuda-on-cl stuff. The issue is it's tricky to come up with a definition of min that works in all the possible use-cases. There are a bunch of corner-cases to deal with. Some of the relevant codes/hacks is here:\nhttps://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L106-L109\ndouble max(double in1, double in2);\ndouble min(double in1, double in2);\nfloat max(float in1, float in2);\nfloat min(float in1, float in2);\n\nvarious other previous attempts: https://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L87-L99\nnamespace cocl {\n   // double max(double in1, double in2);\n   // double min(double in, double in2);\n}\n// using cocl::max;\n// using cocl::min;\n\nextern \"C\" {\n// double our_pretend_tanh(double in);\n// double our_pretend_log(double in);\n// double our_pretend_exp(double in);\n// double our_pretend_max(double in1, double in2);\n// double our_pretend_min(double in1, double in2);\n\nhttps://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L114-L115\n// #define max cocl::max\n// #define min cocl::min\n\nSo, first challenge is, figure out a horrible hack an elegant plan to get the mins to compile across all use-cases.\nEdit 4: looks like this use-case is using min against long longs. So might be sufficient to create a declaration of min that uses long longs. I think I might leave this to someone else to try though :-)", "body": "~~Oh, will need the issue with https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGeneral.h.in handling somehow (ie, it gets macrod by things like https://github.com/pytorch/pytorch/blob/master/torch/lib/TH/THGenerateFloatTypes.h )~~\r\n\r\n![will_need_the_thcgeneral_h_in_handling_somehow](https://cloud.githubusercontent.com/assets/123560/25066512/5e5f6a44-221f-11e7-802e-deb2a16abc20.png)\r\n\r\nWe need `THGeneral.h`, which we can can get like:\r\n\r\n```\r\n# assume cloned pytorch as ~/git/pytorch, so:\r\nPTR=$HOME/git/pytorch\r\n\r\ncd $PTR/torch/lib/TH\r\nmkdir build\r\ncd build\r\nccmake .. -D\r\n# change CMAKE_INSTALL_PREFIX  to be ~/pytorch directory for you (change ~ to whatever your homedirectory is)\r\n# c then c then g\r\nmake -j 4\r\nmake install\r\n```\r\n... and it'll be in `~/pytorch/include/TH`. ~~But we also need THCGeneral.h, and trying the same technique gets stuck on configure:~~\r\n\r\n![thc_cmake_fails](https://cloud.githubusercontent.com/assets/123560/25066565/314d30a2-2221-11e7-871f-fc31d7219a30.png)\r\n\r\n~~TorchConfig.cmake et al are not in $HOME/pytorch:~~\r\n\r\n![pytorch_no_torchconfig_files](https://cloud.githubusercontent.com/assets/123560/25066567/488f7360-2221-11e7-9fa9-6da75bc5950d.png)\r\n\r\nWe also need `THCGeneral.h`. We can get this by doing:\r\n```\r\ncp $PTR/torch/lib/THC/THCGeneral.h.in ~/pytorch/include/THCGeneral.h\r\n# by hand, comment out line 13 of THCgeneral.h, ie this line:\r\n# // #cmakedefine USE_MAGMA\r\n```\r\nThen retry as follows:\r\n```\r\ncd $PTR/torch/lib/THCUNN\r\ncocl -I ~/pytorch/include/TH/ -I ~/pytorch/include -I .. -I ../THC/ -c Sigmoid.cu \r\n```\r\nThis gives a bunch of undefined errors, stuff that needs adding to cuda-on-cl, ie `cudaTextureObject_t`. These should be added to one of the include files in `include/cocl` directory of cuda-on-cl project. There are a bunch of similar defines there that can be used as a basis/model/template.\r\n\r\nCurrent output:\r\n\r\n![output_1](https://cloud.githubusercontent.com/assets/123560/25066634/bddd5978-2223-11e7-98cd-633429635e57.png)\r\n![output_2](https://cloud.githubusercontent.com/assets/123560/25066635/c1404e04-2223-11e7-9586-94285ff348b0.png)\r\n\r\nEdit 2: we can fix that by simply commenting out line 130 of `torch/lib/THC/generic/THCTensor.h`:\r\n\r\n![comment_out_line_130](https://cloud.githubusercontent.com/assets/123560/25066669/fa00968a-2224-11e7-9289-7ca5c0ba0332.png)\r\n\r\nNew output:\r\n\r\n![thcunn_h_not_found](https://cloud.githubusercontent.com/assets/123560/25066672/08e0aeba-2225-11e7-94ad-97d5979ae5c4.png)\r\n\r\nEdit 3: Add `-I` for current directory:\r\n\r\n```\r\ncocl -I ~/pytorch/include/TH/ -I ~/pytorch/include -I .. -I ../THC/ -I . -c Sigmoid.cu\r\n```\r\n\r\nNew output:\r\n\r\n![output_3_1](https://cloud.githubusercontent.com/assets/123560/25066687/49ef3962-2225-11e7-9ee5-94a759e459ef.png)\r\n![output_3_2](https://cloud.githubusercontent.com/assets/123560/25066688/4c1c6534-2225-11e7-9026-d6cd8189055c.png)\r\n\r\n... reached some hacky cuda-on-cl stuff. The issue is it's tricky to come up with a definition of `min` that works in all the possible use-cases. There are a bunch of corner-cases to deal with. Some of the relevant codes/hacks is here:\r\n\r\nhttps://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L106-L109\r\n\r\n```\r\ndouble max(double in1, double in2);\r\ndouble min(double in1, double in2);\r\nfloat max(float in1, float in2);\r\nfloat min(float in1, float in2);\r\n```\r\n\r\nvarious other previous attempts: https://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L87-L99\r\n```\r\nnamespace cocl {\r\n   // double max(double in1, double in2);\r\n   // double min(double in, double in2);\r\n}\r\n// using cocl::max;\r\n// using cocl::min;\r\n\r\nextern \"C\" {\r\n// double our_pretend_tanh(double in);\r\n// double our_pretend_log(double in);\r\n// double our_pretend_exp(double in);\r\n// double our_pretend_max(double in1, double in2);\r\n// double our_pretend_min(double in1, double in2);\r\n```\r\nhttps://github.com/hughperkins/cuda-on-cl/blob/master/include/cocl/fake_funcs.h#L114-L115\r\n```\r\n// #define max cocl::max\r\n// #define min cocl::min\r\n```\r\nSo, first challenge is, figure out ~~a horrible hack~~ an elegant plan to get the `min`s to compile across all use-cases.\r\n\r\nEdit 4: looks like this use-case is using `min` against `long long`s. So might be sufficient to create a declaration of `min` that uses `long long`s. I think I might leave this to someone else to try though :-)"}