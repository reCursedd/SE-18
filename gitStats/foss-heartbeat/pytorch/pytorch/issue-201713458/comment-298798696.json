{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/298798696", "html_url": "https://github.com/pytorch/pytorch/issues/488#issuecomment-298798696", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/488", "id": 298798696, "node_id": "MDEyOklzc3VlQ29tbWVudDI5ODc5ODY5Ng==", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-03T00:36:36Z", "updated_at": "2017-05-03T11:16:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>well, I <del>created a <code>pytorch</code> branch of cuda-on-cl</del> tweaked cuda-on-cl <code>master</code> branch a bit, to add the min/max functions above, and also exp10 and exp10f.</p>\n<p>That gets as far as:</p>\n<pre><code>7 warnings generated.\n+ /usr/local/bin/patch-hostside --hostrawfile ./Sigmoid-hostraw.ll --devicellfile ./Sigmoid-device.ll --hostpatchedfile ./Sigmoid-hostpatched.ll\nAssertion failed: (isa&lt;X&gt;(Val) &amp;&amp; \"cast&lt;Ty&gt;() argument of incompatible type!\"), function cast, file /usr/local/opt/llvm-3.8/include/llvm/Support/Casting.h, line 237.\n/usr/local/bin/cocl_wrapped: line 389: 17959 Abort trap: 6           ${COCL_BIN}/patch-hostside --hostrawfile ${OUTPUTBASEPATH}-hostraw.ll --devicellfile ${OUTPUTBASEPATH}-device.ll --hostpatchedfile ${OUTPUTBASEPATH}-hostpatched.ll\n</code></pre>\n<p>Edit: note, the command to try, after doing the earlier steps, like copying THCGeneral.h.in etc, is:</p>\n<pre><code>cd ~/git/pytorch/torch/lib/THCUNN\ncocl -I ~/pytorch/include/TH/ --devicell-opt inline --devicell-opt mem2reg \\\n--devicell-opt instcombine --devicell-opt O2 \\\n-I ~/pytorch/include -I .. -I ../THC/ -I . -c Sigmoid.cu\n</code></pre>\n<p>(assuming pytorch is cloned as <code>~/git/pytorch</code>)</p>\n<p>(by the way, I'm testing on a Mac. It's totally possible this will get mildly further on Ubuntu 16.04. Or not. Hard to say :-) )</p>", "body_text": "well, I created a pytorch branch of cuda-on-cl tweaked cuda-on-cl master branch a bit, to add the min/max functions above, and also exp10 and exp10f.\nThat gets as far as:\n7 warnings generated.\n+ /usr/local/bin/patch-hostside --hostrawfile ./Sigmoid-hostraw.ll --devicellfile ./Sigmoid-device.ll --hostpatchedfile ./Sigmoid-hostpatched.ll\nAssertion failed: (isa<X>(Val) && \"cast<Ty>() argument of incompatible type!\"), function cast, file /usr/local/opt/llvm-3.8/include/llvm/Support/Casting.h, line 237.\n/usr/local/bin/cocl_wrapped: line 389: 17959 Abort trap: 6           ${COCL_BIN}/patch-hostside --hostrawfile ${OUTPUTBASEPATH}-hostraw.ll --devicellfile ${OUTPUTBASEPATH}-device.ll --hostpatchedfile ${OUTPUTBASEPATH}-hostpatched.ll\n\nEdit: note, the command to try, after doing the earlier steps, like copying THCGeneral.h.in etc, is:\ncd ~/git/pytorch/torch/lib/THCUNN\ncocl -I ~/pytorch/include/TH/ --devicell-opt inline --devicell-opt mem2reg \\\n--devicell-opt instcombine --devicell-opt O2 \\\n-I ~/pytorch/include -I .. -I ../THC/ -I . -c Sigmoid.cu\n\n(assuming pytorch is cloned as ~/git/pytorch)\n(by the way, I'm testing on a Mac. It's totally possible this will get mildly further on Ubuntu 16.04. Or not. Hard to say :-) )", "body": "well, I ~~created a `pytorch` branch of cuda-on-cl~~ tweaked cuda-on-cl `master` branch a bit, to add the min/max functions above, and also exp10 and exp10f.\r\n\r\nThat gets as far as:\r\n\r\n```\r\n7 warnings generated.\r\n+ /usr/local/bin/patch-hostside --hostrawfile ./Sigmoid-hostraw.ll --devicellfile ./Sigmoid-device.ll --hostpatchedfile ./Sigmoid-hostpatched.ll\r\nAssertion failed: (isa<X>(Val) && \"cast<Ty>() argument of incompatible type!\"), function cast, file /usr/local/opt/llvm-3.8/include/llvm/Support/Casting.h, line 237.\r\n/usr/local/bin/cocl_wrapped: line 389: 17959 Abort trap: 6           ${COCL_BIN}/patch-hostside --hostrawfile ${OUTPUTBASEPATH}-hostraw.ll --devicellfile ${OUTPUTBASEPATH}-device.ll --hostpatchedfile ${OUTPUTBASEPATH}-hostpatched.ll\r\n```\r\n\r\nEdit: note, the command to try, after doing the earlier steps, like copying THCGeneral.h.in etc, is:\r\n```\r\ncd ~/git/pytorch/torch/lib/THCUNN\r\ncocl -I ~/pytorch/include/TH/ --devicell-opt inline --devicell-opt mem2reg \\\r\n--devicell-opt instcombine --devicell-opt O2 \\\r\n-I ~/pytorch/include -I .. -I ../THC/ -I . -c Sigmoid.cu\r\n```\r\n(assuming pytorch is cloned as `~/git/pytorch`)\r\n\r\n(by the way, I'm testing on a Mac. It's totally possible this will get mildly further on Ubuntu 16.04. Or not. Hard to say :-) )"}