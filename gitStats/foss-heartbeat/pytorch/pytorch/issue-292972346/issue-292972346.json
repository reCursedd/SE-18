{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4952", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4952/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4952/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4952/events", "html_url": "https://github.com/pytorch/pytorch/issues/4952", "id": 292972346, "node_id": "MDU6SXNzdWUyOTI5NzIzNDY=", "number": 4952, "title": "[Feature request] Optimize autograd/ATen when a gradient is clearly zero", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-30T23:09:18Z", "updated_at": "2018-05-14T19:00:12Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>When an input parameter <em><strong>clearly</strong></em> has zero gradient (not due to numerical coincidence), the current ATen guideline requires the backward function to return a zero tensor. Since there is no special treatment of zero tensors, autograd engine will still trace back the entire graph supporting that variable. This can happen easily when ATen functions have multiple outputs (e.g. many double backward functions). For example,</p>\n<div class=\"highlight highlight-source-python\"><pre>i <span class=\"pl-k\">=</span> deep_nn_1(<span class=\"pl-c1\">...</span>)\nw <span class=\"pl-k\">=</span> deep_nn_2(<span class=\"pl-c1\">...</span>)\no <span class=\"pl-k\">=</span> conv(i, w)\nloss <span class=\"pl-k\">=</span> autograd.grad(o.sum(), i, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>).sum()\nloss.backward()</pre></div>\n<p>In conv double backward, <code>gI</code> only depends on <code>gO</code> and <code>ggW</code>, and <code>gW</code> only depends on <code>gO</code> and <code>ggI</code>. In this case, <code>ggW</code> is zero when doing the double backward, which means that <code>gI</code> should be zero and <code>deep_nn_2</code> doesn't need to be traversed at all. However, ATen conv double backward still <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Convolution.cpp#L686-L688\">would output a zero <code>gI</code></a> even if it already has the check on <code>ggW.defined()</code> to compute <code>gI</code>. Then the autograd engine will go through <code>deep_nn_2</code> unnecessarily.</p>", "body_text": "When an input parameter clearly has zero gradient (not due to numerical coincidence), the current ATen guideline requires the backward function to return a zero tensor. Since there is no special treatment of zero tensors, autograd engine will still trace back the entire graph supporting that variable. This can happen easily when ATen functions have multiple outputs (e.g. many double backward functions). For example,\ni = deep_nn_1(...)\nw = deep_nn_2(...)\no = conv(i, w)\nloss = autograd.grad(o.sum(), i, create_graph=True).sum()\nloss.backward()\nIn conv double backward, gI only depends on gO and ggW, and gW only depends on gO and ggI. In this case, ggW is zero when doing the double backward, which means that gI should be zero and deep_nn_2 doesn't need to be traversed at all. However, ATen conv double backward still would output a zero gI even if it already has the check on ggW.defined() to compute gI. Then the autograd engine will go through deep_nn_2 unnecessarily.", "body": "When an input parameter ***clearly*** has zero gradient (not due to numerical coincidence), the current ATen guideline requires the backward function to return a zero tensor. Since there is no special treatment of zero tensors, autograd engine will still trace back the entire graph supporting that variable. This can happen easily when ATen functions have multiple outputs (e.g. many double backward functions). For example, \r\n\r\n```python\r\ni = deep_nn_1(...)\r\nw = deep_nn_2(...)\r\no = conv(i, w)\r\nloss = autograd.grad(o.sum(), i, create_graph=True).sum()\r\nloss.backward()\r\n```\r\n\r\nIn conv double backward, `gI` only depends on `gO` and `ggW`, and `gW` only depends on `gO` and `ggI`. In this case, `ggW` is zero when doing the double backward, which means that `gI` should be zero and `deep_nn_2` doesn't need to be traversed at all. However, ATen conv double backward still [would output a zero `gI`](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Convolution.cpp#L686-L688) even if it already has the check on `ggW.defined()` to compute `gI`. Then the autograd engine will go through `deep_nn_2` unnecessarily. \r\n"}