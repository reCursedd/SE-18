{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11742", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11742/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11742/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11742/events", "html_url": "https://github.com/pytorch/pytorch/issues/11742", "id": 360675406, "node_id": "MDU6SXNzdWUzNjA2NzU0MDY=", "number": 11742, "title": "runtime error when combining DataParallel with register_buffer", "user": {"login": "xuanqing94", "id": 8935605, "node_id": "MDQ6VXNlcjg5MzU2MDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/8935605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuanqing94", "html_url": "https://github.com/xuanqing94", "followers_url": "https://api.github.com/users/xuanqing94/followers", "following_url": "https://api.github.com/users/xuanqing94/following{/other_user}", "gists_url": "https://api.github.com/users/xuanqing94/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuanqing94/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuanqing94/subscriptions", "organizations_url": "https://api.github.com/users/xuanqing94/orgs", "repos_url": "https://api.github.com/users/xuanqing94/repos", "events_url": "https://api.github.com/users/xuanqing94/events{/privacy}", "received_events_url": "https://api.github.com/users/xuanqing94/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-09-16T20:30:38Z", "updated_at": "2018-09-17T00:04:55Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>PyTorch 0.4.0 or 0.4.1 from conda</p>\n<p>Minimal code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MyLinear</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">in_features</span>, <span class=\"pl-smi\">out_features</span>):\n        <span class=\"pl-c1\">super</span>(MyLinear, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.in_features <span class=\"pl-k\">=</span> in_features\n        <span class=\"pl-c1\">self</span>.out_features <span class=\"pl-k\">=</span> out_features\n        <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(out_features, in_features))\n        <span class=\"pl-c1\">self</span>.noise1 <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(out_features, in_features))\n        <span class=\"pl-c1\">self</span>.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>eps_weight<span class=\"pl-pds\">'</span></span>, torch.Tensor(out_features, in_features))\n\n\n        <span class=\"pl-c1\">self</span>.bias <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(out_features))\n        <span class=\"pl-c1\">self</span>.noise2 <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(out_features))\n        <span class=\"pl-c1\">self</span>.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>eps_bias<span class=\"pl-pds\">'</span></span>, torch.Tensor(out_features))\n\n        <span class=\"pl-c1\">self</span>.reset_parameters()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">reset_parameters</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        stdv <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>. <span class=\"pl-k\">/</span> math.sqrt(<span class=\"pl-c1\">self</span>.weight.size(<span class=\"pl-c1\">1</span>))\n        <span class=\"pl-c1\">self</span>.weight.data.uniform_(<span class=\"pl-k\">-</span>stdv, stdv)\n        <span class=\"pl-c1\">self</span>.bias.data.uniform_(<span class=\"pl-k\">-</span>stdv, stdv)\n        <span class=\"pl-c1\">self</span>.noise1.data.zero_()\n        <span class=\"pl-c1\">self</span>.noise2.data.zero_()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        weight <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.noise1 <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.eps_weight.normal_()\n        bias <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.bias <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.noise2 <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.eps_bias.normal_()\n        out <span class=\"pl-k\">=</span> F.linear(<span class=\"pl-c1\">input</span>, weight, bias)\n        <span class=\"pl-k\">return</span> out\n\nlayer <span class=\"pl-k\">=</span> MyLinear(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">1</span>) \n<span class=\"pl-k\">&gt;&gt;&gt;&gt;</span> layer <span class=\"pl-k\">=</span> nn.DataParallel(layer, <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>))\nlayer.cuda()\nx <span class=\"pl-k\">=</span> torch.FloatTensor(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">10</span>).normal_().cuda()\ny <span class=\"pl-k\">=</span> torch.FloatTensor(<span class=\"pl-c1\">20</span>).normal_().cuda()\nout <span class=\"pl-k\">=</span> layer(x).squeeze()\ndiff <span class=\"pl-k\">=</span> out <span class=\"pl-k\">-</span> y\nloss <span class=\"pl-k\">=</span> torch.sum(diff <span class=\"pl-k\">*</span> diff) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">0.5</span>\nloss.backward()</pre></div>\n<p>The code can be executed without error, but when changing the line</p>\n<pre><code>layer = nn.DataParallel(layer, device_ids=range(1))\n</code></pre>\n<p>to</p>\n<pre><code>layer = nn.DataParallel(layer, device_ids=range(2))\n</code></pre>\n<p>then it says \"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\"</p>", "body_text": "PyTorch 0.4.0 or 0.4.1 from conda\nMinimal code:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MyLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(MyLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.noise1 = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.register_buffer('eps_weight', torch.Tensor(out_features, in_features))\n\n\n        self.bias = nn.Parameter(torch.Tensor(out_features))\n        self.noise2 = nn.Parameter(torch.Tensor(out_features))\n        self.register_buffer('eps_bias', torch.Tensor(out_features))\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        self.bias.data.uniform_(-stdv, stdv)\n        self.noise1.data.zero_()\n        self.noise2.data.zero_()\n\n    def forward(self, input):\n        weight = self.weight + self.noise1 * self.eps_weight.normal_()\n        bias = self.bias + self.noise2 * self.eps_bias.normal_()\n        out = F.linear(input, weight, bias)\n        return out\n\nlayer = MyLinear(10, 1) \n>>>> layer = nn.DataParallel(layer, device_ids=range(1))\nlayer.cuda()\nx = torch.FloatTensor(20, 10).normal_().cuda()\ny = torch.FloatTensor(20).normal_().cuda()\nout = layer(x).squeeze()\ndiff = out - y\nloss = torch.sum(diff * diff) * 0.5\nloss.backward()\nThe code can be executed without error, but when changing the line\nlayer = nn.DataParallel(layer, device_ids=range(1))\n\nto\nlayer = nn.DataParallel(layer, device_ids=range(2))\n\nthen it says \"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\"", "body": "PyTorch 0.4.0 or 0.4.1 from conda\r\n\r\nMinimal code:\r\n```Python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass MyLinear(nn.Module):\r\n    def __init__(self, in_features, out_features):\r\n        super(MyLinear, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\r\n        self.noise1 = nn.Parameter(torch.Tensor(out_features, in_features))\r\n        self.register_buffer('eps_weight', torch.Tensor(out_features, in_features))\r\n\r\n\r\n        self.bias = nn.Parameter(torch.Tensor(out_features))\r\n        self.noise2 = nn.Parameter(torch.Tensor(out_features))\r\n        self.register_buffer('eps_bias', torch.Tensor(out_features))\r\n\r\n        self.reset_parameters()\r\n\r\n    def reset_parameters(self):\r\n        stdv = 1. / math.sqrt(self.weight.size(1))\r\n        self.weight.data.uniform_(-stdv, stdv)\r\n        self.bias.data.uniform_(-stdv, stdv)\r\n        self.noise1.data.zero_()\r\n        self.noise2.data.zero_()\r\n\r\n    def forward(self, input):\r\n        weight = self.weight + self.noise1 * self.eps_weight.normal_()\r\n        bias = self.bias + self.noise2 * self.eps_bias.normal_()\r\n        out = F.linear(input, weight, bias)\r\n        return out\r\n\r\nlayer = MyLinear(10, 1) \r\n>>>> layer = nn.DataParallel(layer, device_ids=range(1))\r\nlayer.cuda()\r\nx = torch.FloatTensor(20, 10).normal_().cuda()\r\ny = torch.FloatTensor(20).normal_().cuda()\r\nout = layer(x).squeeze()\r\ndiff = out - y\r\nloss = torch.sum(diff * diff) * 0.5\r\nloss.backward()\r\n```\r\nThe code can be executed without error, but when changing the line\r\n```\r\nlayer = nn.DataParallel(layer, device_ids=range(1))\r\n```\r\nto\r\n```\r\nlayer = nn.DataParallel(layer, device_ids=range(2))\r\n```\r\nthen it says \"RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\""}