{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13833", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13833/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13833/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13833/events", "html_url": "https://github.com/pytorch/pytorch/issues/13833", "id": 379645130, "node_id": "MDU6SXNzdWUzNzk2NDUxMzA=", "number": 13833, "title": "JIT autodiff Differentiate doesn't handle the change of graphs having Tuple outputs correctly", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-12T06:57:28Z", "updated_at": "2018-11-12T22:52:57Z", "closed_at": "2018-11-12T22:52:57Z", "author_association": "MEMBER", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>It looks like graphs return multiple outputs as Tuples by default now.<br>\nHowever, the <code>Differentiate</code> function in autodiff.h / autodiff.cpp does not handle this correctly when returning multiple outputs.<br>\nIt incorrectly adds extra <code>gradOutput</code> nodes for all outputs that are not <code>output[0]</code> separately in the graph.</p>\n<p>Here's an example:</p>\n<p>Graph: here <code>%7</code> and <code>%8</code> are the finput and fgradinput buffers, so they are not part of <code>f_real_outputs</code></p>\n<pre><code>graph(%self : Dynamic\n      %weight : Dynamic\n      %bias : Dynamic) {\n  %0 : int[] = prim::Constant[value=[3, 5]]()\n  %1 : int[] = prim::Constant[value=[1, 1]]()\n  %2 : int[] = prim::Constant[value=[1, 1]]()\n  %6 : Dynamic, %7 : Dynamic, %8 : Dynamic = aten::thnn_conv2d_forward(%self, %weight, %0, %bias, %1, %2)\n  %9 : Tuple = prim::TupleConstruct(%6, %7, %8)\n  return (%9);\n}\n</code></pre>\n<p>You can construct this graph via:</p>\n<div class=\"highlight highlight-source-c++\"><pre>  std::vector&lt;<span class=\"pl-c1\">int64_t</span>&gt; input_size = {<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">7</span>}; <span class=\"pl-c\"><span class=\"pl-c\">//</span> B x C x H x W</span>\n  std::vector&lt;<span class=\"pl-c1\">int64_t</span>&gt; kernel_size = {<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>};\n  std::vector&lt;<span class=\"pl-c1\">int64_t</span>&gt; stride = {<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>};\n  std::vector&lt;<span class=\"pl-c1\">int64_t</span>&gt; padding = {<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>};\n  <span class=\"pl-k\">constexpr</span> <span class=\"pl-k\">int</span> out_channels = <span class=\"pl-c1\">5</span>;\n\n  <span class=\"pl-k\">auto</span> graph = std::make_shared&lt;Graph&gt;();\n  <span class=\"pl-k\">auto</span> ksz_val = graph-&gt;<span class=\"pl-en\">insertConstant</span>(IValue(kernel_size));\n  <span class=\"pl-k\">auto</span> kst_val = graph-&gt;<span class=\"pl-en\">insertConstant</span>(IValue(stride));\n  <span class=\"pl-k\">auto</span> pad_val = graph-&gt;<span class=\"pl-en\">insertConstant</span>(IValue(padding));\n\n  <span class=\"pl-k\">auto</span> input = graph-&gt;<span class=\"pl-en\">addInput</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>self<span class=\"pl-pds\">\"</span></span>);\n  <span class=\"pl-k\">auto</span> weight = graph-&gt;<span class=\"pl-en\">addInput</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weight<span class=\"pl-pds\">\"</span></span>);\n  <span class=\"pl-k\">auto</span> bias = graph-&gt;<span class=\"pl-en\">addInput</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bias<span class=\"pl-pds\">\"</span></span>);\n\n  Value* conv = graph-&gt;<span class=\"pl-en\">insert</span>(aten::thnn_conv2d_forward, {input, weight, ksz_val, bias, kst_val, pad_val});\n  <span class=\"pl-k\">auto</span> outputs = conv-&gt;<span class=\"pl-en\">node</span>()-&gt;outputs();\n  <span class=\"pl-k\">for</span> (<span class=\"pl-k\">auto</span> output : outputs) {\n    graph-&gt;<span class=\"pl-c1\">registerOutput</span>(output);\n  }\n  graph-&gt;<span class=\"pl-en\">lint</span>();</pre></div>\n<p>Differentiate this graph using:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-en\">EliminateDeadCode</span>(graph); <span class=\"pl-c\"><span class=\"pl-c\">//</span> Tracing of some ops depends on the DCE trick</span>\n<span class=\"pl-en\">ConstantPropagation</span>(graph);\n<span class=\"pl-k\">auto</span> grad_spec = differentiate(graph);\n<span class=\"pl-en\">LowerGradOf</span>(*grad_spec.df);</pre></div>\n<p>The <code>df</code> graph is:</p>\n<pre><code>graph(%0 : Tuple\n      %1 : Dynamic\n      %2 : Dynamic\n      %self : Dynamic\n      %weight : Dynamic\n      %5 : Dynamic\n      %6 : Dynamic) {\n  %7 : int[] = prim::Constant[value=[1, 1]]()\n  %8 : int[] = prim::Constant[value=[3, 5]]()\n  %24 : int = prim::AnyDefined(%0)\n  %25 : Dynamic, %26 : Dynamic, %27 : Dynamic = prim::If(%24)\n    block0() {\n      %28 : Dynamic, %29 : Dynamic, %30 : Dynamic = prim::TupleUnpack(%0)\n      -&gt; (%28, %29, %30)\n    }\n    block1() {\n      %31 : Dynamic = prim::Undefined()\n      -&gt; (%31, %31, %31)\n    }\n  %15 : Dynamic = prim::AutogradAdd(%2, %27)\n  %16 : Dynamic = prim::AutogradAdd(%1, %26)\n  %32 : int = prim::AnyDefined(%25, %16, %15)\n  %33 : Dynamic, %34 : Dynamic, %35 : Dynamic = prim::If(%32)\n    block0() {\n      %36 : bool[] = prim::Constant[value=[1, 1, 1]]()\n      %37 : Dynamic, %38 : Dynamic, %39 : Dynamic = aten::thnn_conv2d_backward(%25, %self, %weight, %8, %7, %7, %5, %6, %36)\n      -&gt; (%37, %38, %39)\n    }\n    block1() {\n      %40 : Dynamic = prim::Undefined()\n      -&gt; (%40, %40, %40)\n    }\n  return (%33, %34, %35);\n}\n</code></pre>\n<p>Now follow along <code>%1</code>, <code>%2</code>, which are <code>gradout_finput</code> and <code>gradout_fgradinput</code>. They shouldn't have been there as inputs at all, because <code>%0</code> as a Tuple has these as <code>%0[1]</code> and <code>%0[2]</code>. In fact, if you see the graph, the unpacked version <code>%0[1]</code> is <code>AutogradAdd</code>ed to <code>%1</code> in the line <code>%16 : Dynamic = prim::AutogradAdd(%1, %26)</code></p>\n<h2>Environment</h2>\n<p>pytorch master</p>\n<p>You will need my patches adding autodiff formulas for <code>TupleConstruct</code> and <code>aten::thnn_conv2d_forward</code> here: <a href=\"https://gist.github.com/soumith/f28ab0dd859f8772ff98b1c0f683acbc\">https://gist.github.com/soumith/f28ab0dd859f8772ff98b1c0f683acbc</a></p>", "body_text": "\ud83d\udc1b Bug\nIt looks like graphs return multiple outputs as Tuples by default now.\nHowever, the Differentiate function in autodiff.h / autodiff.cpp does not handle this correctly when returning multiple outputs.\nIt incorrectly adds extra gradOutput nodes for all outputs that are not output[0] separately in the graph.\nHere's an example:\nGraph: here %7 and %8 are the finput and fgradinput buffers, so they are not part of f_real_outputs\ngraph(%self : Dynamic\n      %weight : Dynamic\n      %bias : Dynamic) {\n  %0 : int[] = prim::Constant[value=[3, 5]]()\n  %1 : int[] = prim::Constant[value=[1, 1]]()\n  %2 : int[] = prim::Constant[value=[1, 1]]()\n  %6 : Dynamic, %7 : Dynamic, %8 : Dynamic = aten::thnn_conv2d_forward(%self, %weight, %0, %bias, %1, %2)\n  %9 : Tuple = prim::TupleConstruct(%6, %7, %8)\n  return (%9);\n}\n\nYou can construct this graph via:\n  std::vector<int64_t> input_size = {4, 3, 5, 7}; // B x C x H x W\n  std::vector<int64_t> kernel_size = {3, 5};\n  std::vector<int64_t> stride = {1, 1};\n  std::vector<int64_t> padding = {1, 1};\n  constexpr int out_channels = 5;\n\n  auto graph = std::make_shared<Graph>();\n  auto ksz_val = graph->insertConstant(IValue(kernel_size));\n  auto kst_val = graph->insertConstant(IValue(stride));\n  auto pad_val = graph->insertConstant(IValue(padding));\n\n  auto input = graph->addInput(\"self\");\n  auto weight = graph->addInput(\"weight\");\n  auto bias = graph->addInput(\"bias\");\n\n  Value* conv = graph->insert(aten::thnn_conv2d_forward, {input, weight, ksz_val, bias, kst_val, pad_val});\n  auto outputs = conv->node()->outputs();\n  for (auto output : outputs) {\n    graph->registerOutput(output);\n  }\n  graph->lint();\nDifferentiate this graph using:\nEliminateDeadCode(graph); // Tracing of some ops depends on the DCE trick\nConstantPropagation(graph);\nauto grad_spec = differentiate(graph);\nLowerGradOf(*grad_spec.df);\nThe df graph is:\ngraph(%0 : Tuple\n      %1 : Dynamic\n      %2 : Dynamic\n      %self : Dynamic\n      %weight : Dynamic\n      %5 : Dynamic\n      %6 : Dynamic) {\n  %7 : int[] = prim::Constant[value=[1, 1]]()\n  %8 : int[] = prim::Constant[value=[3, 5]]()\n  %24 : int = prim::AnyDefined(%0)\n  %25 : Dynamic, %26 : Dynamic, %27 : Dynamic = prim::If(%24)\n    block0() {\n      %28 : Dynamic, %29 : Dynamic, %30 : Dynamic = prim::TupleUnpack(%0)\n      -> (%28, %29, %30)\n    }\n    block1() {\n      %31 : Dynamic = prim::Undefined()\n      -> (%31, %31, %31)\n    }\n  %15 : Dynamic = prim::AutogradAdd(%2, %27)\n  %16 : Dynamic = prim::AutogradAdd(%1, %26)\n  %32 : int = prim::AnyDefined(%25, %16, %15)\n  %33 : Dynamic, %34 : Dynamic, %35 : Dynamic = prim::If(%32)\n    block0() {\n      %36 : bool[] = prim::Constant[value=[1, 1, 1]]()\n      %37 : Dynamic, %38 : Dynamic, %39 : Dynamic = aten::thnn_conv2d_backward(%25, %self, %weight, %8, %7, %7, %5, %6, %36)\n      -> (%37, %38, %39)\n    }\n    block1() {\n      %40 : Dynamic = prim::Undefined()\n      -> (%40, %40, %40)\n    }\n  return (%33, %34, %35);\n}\n\nNow follow along %1, %2, which are gradout_finput and gradout_fgradinput. They shouldn't have been there as inputs at all, because %0 as a Tuple has these as %0[1] and %0[2]. In fact, if you see the graph, the unpacked version %0[1] is AutogradAdded to %1 in the line %16 : Dynamic = prim::AutogradAdd(%1, %26)\nEnvironment\npytorch master\nYou will need my patches adding autodiff formulas for TupleConstruct and aten::thnn_conv2d_forward here: https://gist.github.com/soumith/f28ab0dd859f8772ff98b1c0f683acbc", "body": "## \ud83d\udc1b Bug\r\n\r\nIt looks like graphs return multiple outputs as Tuples by default now.\r\nHowever, the `Differentiate` function in autodiff.h / autodiff.cpp does not handle this correctly when returning multiple outputs.\r\nIt incorrectly adds extra `gradOutput` nodes for all outputs that are not `output[0]` separately in the graph.\r\n\r\nHere's an example:\r\n\r\nGraph: here `%7` and `%8` are the finput and fgradinput buffers, so they are not part of `f_real_outputs`\r\n\r\n```\r\ngraph(%self : Dynamic\r\n      %weight : Dynamic\r\n      %bias : Dynamic) {\r\n  %0 : int[] = prim::Constant[value=[3, 5]]()\r\n  %1 : int[] = prim::Constant[value=[1, 1]]()\r\n  %2 : int[] = prim::Constant[value=[1, 1]]()\r\n  %6 : Dynamic, %7 : Dynamic, %8 : Dynamic = aten::thnn_conv2d_forward(%self, %weight, %0, %bias, %1, %2)\r\n  %9 : Tuple = prim::TupleConstruct(%6, %7, %8)\r\n  return (%9);\r\n}\r\n```\r\n\r\nYou can construct this graph via:\r\n\r\n```cpp\r\n  std::vector<int64_t> input_size = {4, 3, 5, 7}; // B x C x H x W\r\n  std::vector<int64_t> kernel_size = {3, 5};\r\n  std::vector<int64_t> stride = {1, 1};\r\n  std::vector<int64_t> padding = {1, 1};\r\n  constexpr int out_channels = 5;\r\n\r\n  auto graph = std::make_shared<Graph>();\r\n  auto ksz_val = graph->insertConstant(IValue(kernel_size));\r\n  auto kst_val = graph->insertConstant(IValue(stride));\r\n  auto pad_val = graph->insertConstant(IValue(padding));\r\n\r\n  auto input = graph->addInput(\"self\");\r\n  auto weight = graph->addInput(\"weight\");\r\n  auto bias = graph->addInput(\"bias\");\r\n\r\n  Value* conv = graph->insert(aten::thnn_conv2d_forward, {input, weight, ksz_val, bias, kst_val, pad_val});\r\n  auto outputs = conv->node()->outputs();\r\n  for (auto output : outputs) {\r\n    graph->registerOutput(output);\r\n  }\r\n  graph->lint();\r\n```\r\n\r\n\r\nDifferentiate this graph using:\r\n\r\n```cpp\r\nEliminateDeadCode(graph); // Tracing of some ops depends on the DCE trick\r\nConstantPropagation(graph);\r\nauto grad_spec = differentiate(graph);\r\nLowerGradOf(*grad_spec.df);\r\n```\r\n\r\nThe `df` graph is:\r\n\r\n```\r\ngraph(%0 : Tuple\r\n      %1 : Dynamic\r\n      %2 : Dynamic\r\n      %self : Dynamic\r\n      %weight : Dynamic\r\n      %5 : Dynamic\r\n      %6 : Dynamic) {\r\n  %7 : int[] = prim::Constant[value=[1, 1]]()\r\n  %8 : int[] = prim::Constant[value=[3, 5]]()\r\n  %24 : int = prim::AnyDefined(%0)\r\n  %25 : Dynamic, %26 : Dynamic, %27 : Dynamic = prim::If(%24)\r\n    block0() {\r\n      %28 : Dynamic, %29 : Dynamic, %30 : Dynamic = prim::TupleUnpack(%0)\r\n      -> (%28, %29, %30)\r\n    }\r\n    block1() {\r\n      %31 : Dynamic = prim::Undefined()\r\n      -> (%31, %31, %31)\r\n    }\r\n  %15 : Dynamic = prim::AutogradAdd(%2, %27)\r\n  %16 : Dynamic = prim::AutogradAdd(%1, %26)\r\n  %32 : int = prim::AnyDefined(%25, %16, %15)\r\n  %33 : Dynamic, %34 : Dynamic, %35 : Dynamic = prim::If(%32)\r\n    block0() {\r\n      %36 : bool[] = prim::Constant[value=[1, 1, 1]]()\r\n      %37 : Dynamic, %38 : Dynamic, %39 : Dynamic = aten::thnn_conv2d_backward(%25, %self, %weight, %8, %7, %7, %5, %6, %36)\r\n      -> (%37, %38, %39)\r\n    }\r\n    block1() {\r\n      %40 : Dynamic = prim::Undefined()\r\n      -> (%40, %40, %40)\r\n    }\r\n  return (%33, %34, %35);\r\n}\r\n```\r\n\r\nNow follow along `%1`, `%2`, which are `gradout_finput` and `gradout_fgradinput`. They shouldn't have been there as inputs at all, because `%0` as a Tuple has these as `%0[1]` and `%0[2]`. In fact, if you see the graph, the unpacked version `%0[1]` is `AutogradAdd`ed to `%1` in the line `%16 : Dynamic = prim::AutogradAdd(%1, %26)`\r\n\r\n## Environment\r\n\r\npytorch master\r\n\r\nYou will need my patches adding autodiff formulas for `TupleConstruct` and `aten::thnn_conv2d_forward` here: https://gist.github.com/soumith/f28ab0dd859f8772ff98b1c0f683acbc"}