{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9942", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9942/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9942/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9942/events", "html_url": "https://github.com/pytorch/pytorch/issues/9942", "id": 345316915, "node_id": "MDU6SXNzdWUzNDUzMTY5MTU=", "number": 9942, "title": "Potential 0.4.1 Memory Leak for a Fairseq Model", "user": {"login": "hmc-cs-mdrissi", "id": 16809055, "node_id": "MDQ6VXNlcjE2ODA5MDU1", "avatar_url": "https://avatars0.githubusercontent.com/u/16809055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hmc-cs-mdrissi", "html_url": "https://github.com/hmc-cs-mdrissi", "followers_url": "https://api.github.com/users/hmc-cs-mdrissi/followers", "following_url": "https://api.github.com/users/hmc-cs-mdrissi/following{/other_user}", "gists_url": "https://api.github.com/users/hmc-cs-mdrissi/gists{/gist_id}", "starred_url": "https://api.github.com/users/hmc-cs-mdrissi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hmc-cs-mdrissi/subscriptions", "organizations_url": "https://api.github.com/users/hmc-cs-mdrissi/orgs", "repos_url": "https://api.github.com/users/hmc-cs-mdrissi/repos", "events_url": "https://api.github.com/users/hmc-cs-mdrissi/events{/privacy}", "received_events_url": "https://api.github.com/users/hmc-cs-mdrissi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-07-27T18:05:00Z", "updated_at": "2018-07-30T18:03:13Z", "closed_at": "2018-07-30T17:47:24Z", "author_association": "NONE", "body_html": "<p>Yesterday, when I was on 0.4.0 I was training a fairseq model (self_att_wp) I could train it fine with batch size of around 4 (technically I control the number of tokens fed into the model). After upgrading to 0.4.1, when training the same model with the same arguments it runs out of memory. Even decreasing the batch size it still runs out of memory after a bit of time with memory just increasing after every couple of batches.</p>\n<p>There's an issue in fairseq mention this as well, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"345081547\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/fairseq/issues/232\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/fairseq/issues/232/hovercard\" href=\"https://github.com/pytorch/fairseq/issues/232\">pytorch/fairseq#232</a>. The issue mentions the memory leak with a different fairseq model, so I'd guess that multiple fairseq models now have this issue. The command used to train the fairseq model I've been using are:</p>\n<p>python3.6 train.py data-bin/wikitext_outline_to_target -<br>\na fconv_self_att_wp --lr 0.25 --clip-norm 0.1 --max-tokens 4000 --lr-scheduler reduce_lr_on_plateau --source-lang wikitext_outline --target-lang wikitext_target --max-epoch 25 --no-epoch-checkpoints --save-di<br>\nr model4_checkpoints/</p>\n<p>You'll need to replace source-lang/target-lang and the data-bin argument by whatever dataset you end up using. The readme here, <a href=\"https://github.com/pytorch/fairseq/tree/master/examples/stories\">https://github.com/pytorch/fairseq/tree/master/examples/stories</a>, describes the commands in some more detail to cover that part and trains the same architecture (some variation in exact arguments, but I don't think they'll matter).</p>\n<p>edit: More specifically the error was a cuda out of memory error and doing nvidia-smi I could see the memory increasing over time. I also had upgraded to cuda 9.2/cudnn 7.1.4 so it might be an issue there. The OS was ubuntu 16.04.</p>", "body_text": "Yesterday, when I was on 0.4.0 I was training a fairseq model (self_att_wp) I could train it fine with batch size of around 4 (technically I control the number of tokens fed into the model). After upgrading to 0.4.1, when training the same model with the same arguments it runs out of memory. Even decreasing the batch size it still runs out of memory after a bit of time with memory just increasing after every couple of batches.\nThere's an issue in fairseq mention this as well, pytorch/fairseq#232. The issue mentions the memory leak with a different fairseq model, so I'd guess that multiple fairseq models now have this issue. The command used to train the fairseq model I've been using are:\npython3.6 train.py data-bin/wikitext_outline_to_target -\na fconv_self_att_wp --lr 0.25 --clip-norm 0.1 --max-tokens 4000 --lr-scheduler reduce_lr_on_plateau --source-lang wikitext_outline --target-lang wikitext_target --max-epoch 25 --no-epoch-checkpoints --save-di\nr model4_checkpoints/\nYou'll need to replace source-lang/target-lang and the data-bin argument by whatever dataset you end up using. The readme here, https://github.com/pytorch/fairseq/tree/master/examples/stories, describes the commands in some more detail to cover that part and trains the same architecture (some variation in exact arguments, but I don't think they'll matter).\nedit: More specifically the error was a cuda out of memory error and doing nvidia-smi I could see the memory increasing over time. I also had upgraded to cuda 9.2/cudnn 7.1.4 so it might be an issue there. The OS was ubuntu 16.04.", "body": "Yesterday, when I was on 0.4.0 I was training a fairseq model (self_att_wp) I could train it fine with batch size of around 4 (technically I control the number of tokens fed into the model). After upgrading to 0.4.1, when training the same model with the same arguments it runs out of memory. Even decreasing the batch size it still runs out of memory after a bit of time with memory just increasing after every couple of batches.\r\n\r\nThere's an issue in fairseq mention this as well, https://github.com/pytorch/fairseq/issues/232. The issue mentions the memory leak with a different fairseq model, so I'd guess that multiple fairseq models now have this issue. The command used to train the fairseq model I've been using are:\r\n\r\npython3.6 train.py data-bin/wikitext_outline_to_target -\r\na fconv_self_att_wp --lr 0.25 --clip-norm 0.1 --max-tokens 4000 --lr-scheduler reduce_lr_on_plateau --source-lang wikitext_outline --target-lang wikitext_target --max-epoch 25 --no-epoch-checkpoints --save-di\r\nr model4_checkpoints/\r\n\r\nYou'll need to replace source-lang/target-lang and the data-bin argument by whatever dataset you end up using. The readme here, https://github.com/pytorch/fairseq/tree/master/examples/stories, describes the commands in some more detail to cover that part and trains the same architecture (some variation in exact arguments, but I don't think they'll matter).\r\n\r\nedit: More specifically the error was a cuda out of memory error and doing nvidia-smi I could see the memory increasing over time. I also had upgraded to cuda 9.2/cudnn 7.1.4 so it might be an issue there. The OS was ubuntu 16.04."}