{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/408954196", "html_url": "https://github.com/pytorch/pytorch/issues/9942#issuecomment-408954196", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9942", "id": 408954196, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODk1NDE5Ng==", "user": {"login": "hmc-cs-mdrissi", "id": 16809055, "node_id": "MDQ6VXNlcjE2ODA5MDU1", "avatar_url": "https://avatars0.githubusercontent.com/u/16809055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hmc-cs-mdrissi", "html_url": "https://github.com/hmc-cs-mdrissi", "followers_url": "https://api.github.com/users/hmc-cs-mdrissi/followers", "following_url": "https://api.github.com/users/hmc-cs-mdrissi/following{/other_user}", "gists_url": "https://api.github.com/users/hmc-cs-mdrissi/gists{/gist_id}", "starred_url": "https://api.github.com/users/hmc-cs-mdrissi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hmc-cs-mdrissi/subscriptions", "organizations_url": "https://api.github.com/users/hmc-cs-mdrissi/orgs", "repos_url": "https://api.github.com/users/hmc-cs-mdrissi/repos", "events_url": "https://api.github.com/users/hmc-cs-mdrissi/events{/privacy}", "received_events_url": "https://api.github.com/users/hmc-cs-mdrissi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-30T17:57:54Z", "updated_at": "2018-07-30T18:03:13Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>, If you look at the edit, while I don't think it was caused solely by 0.4.1, I think it was mainly caused by either it or cuda 9.2. I didn't check to see which of the two was having the harmful effect (mostly due to a deadline in a few days that needs working code). I can investigate it more properly this weekend. I guess a full short description is:</p>\n<p>0.4.1 + cuda 9.2 + nvidia 396.45 -&gt; fairseq model, self_att_wp, crashes and leaks gpu memory<br>\n0.4.0 + cuda 9.0 + nvidia 396.45 -&gt; fairseq model, self_att_wp, a lot of out of memory warnings, but no crash and it doesn't appear to be a leak just a bit more memory usage than the last case<br>\n0.4.0 + cuda 9.0 + nvidia 384.130 -&gt; fairseq model, self_att_wp, rare out of memory warnings</p>\n<p>Honestly the difference between the last two is probably minor as I tried to get the memory close the max by playing with the parameter controlling the number of elements per batch. So the second case is likely a small memory regression in the driver. The model crashing in the first case is a much bigger issue.</p>", "body_text": "@zou3519, If you look at the edit, while I don't think it was caused solely by 0.4.1, I think it was mainly caused by either it or cuda 9.2. I didn't check to see which of the two was having the harmful effect (mostly due to a deadline in a few days that needs working code). I can investigate it more properly this weekend. I guess a full short description is:\n0.4.1 + cuda 9.2 + nvidia 396.45 -> fairseq model, self_att_wp, crashes and leaks gpu memory\n0.4.0 + cuda 9.0 + nvidia 396.45 -> fairseq model, self_att_wp, a lot of out of memory warnings, but no crash and it doesn't appear to be a leak just a bit more memory usage than the last case\n0.4.0 + cuda 9.0 + nvidia 384.130 -> fairseq model, self_att_wp, rare out of memory warnings\nHonestly the difference between the last two is probably minor as I tried to get the memory close the max by playing with the parameter controlling the number of elements per batch. So the second case is likely a small memory regression in the driver. The model crashing in the first case is a much bigger issue.", "body": "@zou3519, If you look at the edit, while I don't think it was caused solely by 0.4.1, I think it was mainly caused by either it or cuda 9.2. I didn't check to see which of the two was having the harmful effect (mostly due to a deadline in a few days that needs working code). I can investigate it more properly this weekend. I guess a full short description is:\r\n\r\n0.4.1 + cuda 9.2 + nvidia 396.45 -> fairseq model, self_att_wp, crashes and leaks gpu memory\r\n0.4.0 + cuda 9.0 + nvidia 396.45 -> fairseq model, self_att_wp, a lot of out of memory warnings, but no crash and it doesn't appear to be a leak just a bit more memory usage than the last case\r\n0.4.0 + cuda 9.0 + nvidia 384.130 -> fairseq model, self_att_wp, rare out of memory warnings\r\n\r\nHonestly the difference between the last two is probably minor as I tried to get the memory close the max by playing with the parameter controlling the number of elements per batch. So the second case is likely a small memory regression in the driver. The model crashing in the first case is a much bigger issue."}