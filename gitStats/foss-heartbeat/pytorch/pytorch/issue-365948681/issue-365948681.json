{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12260", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12260/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12260/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12260/events", "html_url": "https://github.com/pytorch/pytorch/issues/12260", "id": 365948681, "node_id": "MDU6SXNzdWUzNjU5NDg2ODE=", "number": 12260, "title": "torch.multinomial without replacement returns repetitive values when all non-zero items are exhausted ", "user": {"login": "ankitvgupta", "id": 5473865, "node_id": "MDQ6VXNlcjU0NzM4NjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5473865?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ankitvgupta", "html_url": "https://github.com/ankitvgupta", "followers_url": "https://api.github.com/users/ankitvgupta/followers", "following_url": "https://api.github.com/users/ankitvgupta/following{/other_user}", "gists_url": "https://api.github.com/users/ankitvgupta/gists{/gist_id}", "starred_url": "https://api.github.com/users/ankitvgupta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ankitvgupta/subscriptions", "organizations_url": "https://api.github.com/users/ankitvgupta/orgs", "repos_url": "https://api.github.com/users/ankitvgupta/repos", "events_url": "https://api.github.com/users/ankitvgupta/events{/privacy}", "received_events_url": "https://api.github.com/users/ankitvgupta/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1002715609, "node_id": "MDU6TGFiZWwxMDAyNzE1NjA5", "url": "https://api.github.com/repos/pytorch/pytorch/labels/blocker", "name": "blocker", "color": "b60205", "default": false}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-10-02T15:17:15Z", "updated_at": "2018-10-11T03:40:27Z", "closed_at": "2018-10-11T03:40:27Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n\n<p>When given a one-hot probability distribution, with replacement=False, torch.multinomial returns 0s for all indices after the first one. While I understand that it is weird to sample &gt; 1 value without replacement if the distribution is one-hot, I think this is less desirable behavior than randomly sampling the remaining values without replacement.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.multinomial(torch.FloatTensor([<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>]), <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">replacement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\ntensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>])</pre></div>\n<p>This seems undesired, because replacement=False implies to me that I should expect my outputs to all be different. My understanding is that this was discussed in a recent issue about the GPU kernel for multinomial not matching the CPU here: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"337195683\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9062\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9062/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9062\">#9062</a></p>\n<h2>Expected behavior</h2>\n<p>I expected not to get the index \"0\" in the output more than once.</p>\n<h2>Environment</h2>\n<p>Please copy and paste the output from our<br>\n<a href=\"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\" rel=\"nofollow\">environment collection script</a><br>\n(or fill out the checklist below manually).</p>\n<p>You can get the script and run it with:</p>\n<pre><code>wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n</code></pre>\n<ul>\n<li>PyTorch Version (e.g., 1.0): 0.4.0/0.4.1</li>\n<li>OS (e.g., Linux): 16.04</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): pip</li>\n<li>Build command you used (if compiling from source): N/A</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 9</li>\n<li>GPU models and configuration: K80</li>\n<li>Any other relevant information: N/A</li>\n</ul>\n<h2>Additional context</h2>\n", "body_text": "\ud83d\udc1b Bug\n\nWhen given a one-hot probability distribution, with replacement=False, torch.multinomial returns 0s for all indices after the first one. While I understand that it is weird to sample > 1 value without replacement if the distribution is one-hot, I think this is less desirable behavior than randomly sampling the remaining values without replacement.\nTo Reproduce\nSteps to reproduce the behavior:\n>>> torch.multinomial(torch.FloatTensor([0,1,0,0]), 3, replacement=False)\ntensor([1, 0, 0])\nThis seems undesired, because replacement=False implies to me that I should expect my outputs to all be different. My understanding is that this was discussed in a recent issue about the GPU kernel for multinomial not matching the CPU here: #9062\nExpected behavior\nI expected not to get the index \"0\" in the output more than once.\nEnvironment\nPlease copy and paste the output from our\nenvironment collection script\n(or fill out the checklist below manually).\nYou can get the script and run it with:\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n\n\nPyTorch Version (e.g., 1.0): 0.4.0/0.4.1\nOS (e.g., Linux): 16.04\nHow you installed PyTorch (conda, pip, source): pip\nBuild command you used (if compiling from source): N/A\nPython version: 3.6\nCUDA/cuDNN version: 9\nGPU models and configuration: K80\nAny other relevant information: N/A\n\nAdditional context", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen given a one-hot probability distribution, with replacement=False, torch.multinomial returns 0s for all indices after the first one. While I understand that it is weird to sample > 1 value without replacement if the distribution is one-hot, I think this is less desirable behavior than randomly sampling the remaining values without replacement.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n``` python\r\n>>> torch.multinomial(torch.FloatTensor([0,1,0,0]), 3, replacement=False)\r\ntensor([1, 0, 0])\r\n```\r\nThis seems undesired, because replacement=False implies to me that I should expect my outputs to all be different. My understanding is that this was discussed in a recent issue about the GPU kernel for multinomial not matching the CPU here: https://github.com/pytorch/pytorch/issues/9062\r\n\r\n## Expected behavior\r\n\r\nI expected not to get the index \"0\" in the output more than once.\r\n\r\n## Environment\r\n\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 0.4.0/0.4.1\r\n - OS (e.g., Linux): 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source): N/A\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 9\r\n - GPU models and configuration: K80\r\n - Any other relevant information: N/A\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}