{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/382626571", "html_url": "https://github.com/pytorch/pytorch/pull/6699#issuecomment-382626571", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6699", "id": 382626571, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjYyNjU3MQ==", "user": {"login": "jgong5", "id": 8359223, "node_id": "MDQ6VXNlcjgzNTkyMjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/8359223?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgong5", "html_url": "https://github.com/jgong5", "followers_url": "https://api.github.com/users/jgong5/followers", "following_url": "https://api.github.com/users/jgong5/following{/other_user}", "gists_url": "https://api.github.com/users/jgong5/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgong5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgong5/subscriptions", "organizations_url": "https://api.github.com/users/jgong5/orgs", "repos_url": "https://api.github.com/users/jgong5/repos", "events_url": "https://api.github.com/users/jgong5/events{/privacy}", "received_events_url": "https://api.github.com/users/jgong5/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-19T06:36:17Z", "updated_at": "2018-04-19T06:36:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1100089\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yinghai\">@yinghai</a> The sub-graph level optimization is still being developed internally and we will update you as soon as it is available. It is transparent to the framework (i.e. no Caffe2 code changes needed) and iDeep will handle all the details internally. An example of such optimizations is operator fusion such as computing convolution and ReLU together. MKL-DNN library has such fusion primitive support but we don't want Caffe2 users to care about such a backend-dependent optimization.</p>\n<p>Please check the following table for the performance comparison on two CNN models: ResNet-50 and GoogleNet-v3. They were measured on Skylake 8180 (28 cores, 2.5GHz). The columns marked \"Projected\" are what we will achieve with sub-graph optimizations. The low-precision 8-bit inference (int8) will be added later as well.</p>\n<table>\n<thead>\n<tr>\n<th><em>ResNet-50</em></th>\n<th>\u00a0  MKL-ML FP32</th>\n<th>MKL-DNN FP32</th>\n<th>Projected MKL-DNN   FP32</th>\n<th>Projected MKL-DNN   INT8</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Latency (BS=1),   milliseconds, lower better</td>\n<td>10.4ms</td>\n<td>38ms</td>\n<td>6.4ms</td>\n<td>2.6ms</td>\n</tr>\n<tr>\n<td>Throughput   (BS=128), images/sec, higher better</td>\n<td>178</td>\n<td>201</td>\n<td>390</td>\n<td>660</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th><em>GoogleNet-v3</em></th>\n<th>\u00a0  MKL-ML FP32</th>\n<th>MKL-DNN FP32</th>\n<th>Projected MKL-DNN   FP32</th>\n<th>Projected MKL-DNN   INT8</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Latency (BS=1),   milliseconds, lower better</td>\n<td>16ms</td>\n<td>55ms</td>\n<td>9.9ms</td>\n<td>5.5ms</td>\n</tr>\n<tr>\n<td>Throughput   (BS=128), images/sec, higher better</td>\n<td>152</td>\n<td>158</td>\n<td>230</td>\n<td>390</td>\n</tr>\n</tbody>\n</table>", "body_text": "@yinghai The sub-graph level optimization is still being developed internally and we will update you as soon as it is available. It is transparent to the framework (i.e. no Caffe2 code changes needed) and iDeep will handle all the details internally. An example of such optimizations is operator fusion such as computing convolution and ReLU together. MKL-DNN library has such fusion primitive support but we don't want Caffe2 users to care about such a backend-dependent optimization.\nPlease check the following table for the performance comparison on two CNN models: ResNet-50 and GoogleNet-v3. They were measured on Skylake 8180 (28 cores, 2.5GHz). The columns marked \"Projected\" are what we will achieve with sub-graph optimizations. The low-precision 8-bit inference (int8) will be added later as well.\n\n\n\nResNet-50\n\u00a0  MKL-ML FP32\nMKL-DNN FP32\nProjected MKL-DNN   FP32\nProjected MKL-DNN   INT8\n\n\n\n\nLatency (BS=1),   milliseconds, lower better\n10.4ms\n38ms\n6.4ms\n2.6ms\n\n\nThroughput   (BS=128), images/sec, higher better\n178\n201\n390\n660\n\n\n\n\n\n\nGoogleNet-v3\n\u00a0  MKL-ML FP32\nMKL-DNN FP32\nProjected MKL-DNN   FP32\nProjected MKL-DNN   INT8\n\n\n\n\nLatency (BS=1),   milliseconds, lower better\n16ms\n55ms\n9.9ms\n5.5ms\n\n\nThroughput   (BS=128), images/sec, higher better\n152\n158\n230\n390", "body": "@yinghai The sub-graph level optimization is still being developed internally and we will update you as soon as it is available. It is transparent to the framework (i.e. no Caffe2 code changes needed) and iDeep will handle all the details internally. An example of such optimizations is operator fusion such as computing convolution and ReLU together. MKL-DNN library has such fusion primitive support but we don't want Caffe2 users to care about such a backend-dependent optimization.\r\n\r\nPlease check the following table for the performance comparison on two CNN models: ResNet-50 and GoogleNet-v3. They were measured on Skylake 8180 (28 cores, 2.5GHz). The columns marked \"Projected\" are what we will achieve with sub-graph optimizations. The low-precision 8-bit inference (int8) will be added later as well.\r\n\r\n*ResNet-50*\r\n\u00a0  MKL-ML FP32 | MKL-DNN FP32 | Projected MKL-DNN   FP32 | Projected MKL-DNN   INT8\r\n-- | -- | -- | -- | --\r\nLatency (BS=1),   milliseconds, lower better | 10.4ms | 38ms | 6.4ms | 2.6ms\r\nThroughput   (BS=128), images/sec, higher better | 178 | 201 | 390 | 660\r\n\r\n*GoogleNet-v3*\r\n\u00a0  MKL-ML FP32 | MKL-DNN FP32 | Projected MKL-DNN   FP32 | Projected MKL-DNN   INT8\r\n-- | -- | -- | -- | --\r\nLatency (BS=1),   milliseconds, lower better | 16ms | 55ms | 9.9ms | 5.5ms\r\nThroughput   (BS=128), images/sec, higher better | 152 | 158 | 230 | 390\r\n"}