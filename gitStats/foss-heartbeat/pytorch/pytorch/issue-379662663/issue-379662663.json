{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13834", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13834/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13834/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13834/events", "html_url": "https://github.com/pytorch/pytorch/issues/13834", "id": 379662663, "node_id": "MDU6SXNzdWUzNzk2NjI2NjM=", "number": 13834, "title": "Fail to export custom operator with tuple output to ONNX", "user": {"login": "0wu", "id": 13663916, "node_id": "MDQ6VXNlcjEzNjYzOTE2", "avatar_url": "https://avatars2.githubusercontent.com/u/13663916?v=4", "gravatar_id": "", "url": "https://api.github.com/users/0wu", "html_url": "https://github.com/0wu", "followers_url": "https://api.github.com/users/0wu/followers", "following_url": "https://api.github.com/users/0wu/following{/other_user}", "gists_url": "https://api.github.com/users/0wu/gists{/gist_id}", "starred_url": "https://api.github.com/users/0wu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/0wu/subscriptions", "organizations_url": "https://api.github.com/users/0wu/orgs", "repos_url": "https://api.github.com/users/0wu/repos", "events_url": "https://api.github.com/users/0wu/events{/privacy}", "received_events_url": "https://api.github.com/users/0wu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx", "name": "onnx", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-12T08:07:42Z", "updated_at": "2018-11-12T18:21:26Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>When exporting a tuple-output custom Function to ONNX, the LowerAllTuple optimization pass will fail because Symb::PythonOP is not in the whitelist. The same export works in 0.4.1 but not master.</p>\n<p>The failing assert is at<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/8752214fb7534d3d3f83c1a459f24c57db86cd10/torch/csrc/jit/passes/lower_tuples.cpp#L109\">pytorch/torch/csrc/jit/passes/lower_tuples.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 109\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/8752214fb7534d3d3f83c1a459f24c57db86cd10\">8752214</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L109\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"109\"></td>\n          <td id=\"LC109\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">JIT_ASSERTM</span>(white_list.<span class=\"pl-c1\">count</span>(n-&gt;<span class=\"pl-c1\">kind</span>()) &gt; <span class=\"pl-c1\">0</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tuple appears in op that does not forward tuples<span class=\"pl-pds\">\"</span></span>); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\ninvoked by <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/8752214fb7534d3d3f83c1a459f24c57db86cd10/torch/onnx/utils.py#L150\">pytorch/torch/onnx/utils.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 150\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/8752214fb7534d3d3f83c1a459f24c57db86cd10\">8752214</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L150\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"150\"></td>\n          <td id=\"LC150\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> torch._C._jit_pass_lower_all_tuples(graph) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>By adding Symbol::PythonOp into the whiltelist, at <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/8752214fb7534d3d3f83c1a459f24c57db86cd10/torch/csrc/jit/passes/lower_tuples.cpp#L13\">pytorch/torch/csrc/jit/passes/lower_tuples.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 13\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/8752214fb7534d3d3f83c1a459f24c57db86cd10\">8752214</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L13\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"13\"></td>\n          <td id=\"LC13\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> std::unordered_set&lt;Symbol&gt; white_list = { </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\nwe can export normally again.</p>\n<h2>To Reproduce</h2>\n<p>Here's a sample 1-input 2-output custom operator</p>\n<pre><code>import torch\n\nclass Split(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, input):\n        return g.op('Split', input, outputs=2)\n\n    @staticmethod\n    def forward(ctx, input):\n        return input[0], input[1]\n\ndef test_onnx_export():\n    class MyModule(torch.nn.Module):\n        def forward(self, input):\n            return Split().apply(input)\n\n    model_string = torch.onnx.export_to_pretty_string(\n                   MyModule(),\n                   (torch.tensor([0, 1])),\n                   \"/tmp/custom_op.onnx\")\n    print(model_string)\n\ntest_onnx_export()\n</code></pre>\n<p>which can be exported correctly in pytorch-0.4.1</p>\n<pre><code>(pytorch41)# python split.py \nModelProto {\n  producer_name: \"pytorch\"\n  domain: \"\"\n  doc_string: \"\"\n  graph:\n    GraphProto {\n      name: \"torch-jit-export\"\n      inputs: [{name: \"0\", type:Tensor dims: 2}]\n      outputs: [{name: \"1\", type:Tensor dims: },{name: \"2\", type:Tensor dims: }]\n      initializers: []\n      nodes: [\n        Node {type: \"Split\", inputs: [0], outputs: [1,2], attributes: []}\n      ]\n    }\n  opset_import: [OperatorSetIdProto { domain: }],\n}\n</code></pre>\n<p>But in pytorch-1.0+ <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/4b86a215cad4bbada9fdb52a63f68bc6db52d696/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/4b86a215cad4bbada9fdb52a63f68bc6db52d696\"><tt>4b86a21</tt></a></p>\n<pre><code>(pytorch10) # python /workspace/split.py \nTraceback (most recent call last):\n  File \"/workspace/split.py\", line 23, in &lt;module&gt;\n    test_onnx_export()\n  File \"/workspace/split.py\", line 20, in test_onnx_export\n    \"/tmp/custom_op.onnx\")\n  File \"/opt/pytorch/torch/onnx/__init__.py\", line 32, in export_to_pretty_string\n    return utils.export_to_pretty_string(*args, **kwargs)\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 255, in export_to_pretty_string\n    export_type, example_outputs, propagate, google_printer)\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 265, in _export_to_pretty_string\n    example_outputs, propagate)\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 227, in _model_to_graph\n    graph = _optimize_graph(graph, operator_export_type)\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 150, in _optimize_graph\n    torch._C._jit_pass_lower_all_tuples(graph)\nRuntimeError: tuple appears in op that does not forward tuples (VisitNode at /opt/pytorch/torch/csrc/jit/passes/lower_tuples.cpp:109)\nframe #0: &lt;unknown function&gt; + 0x2f835 (0x7f94b46e8835 in /opt/pytorch/torch/lib/libc10.so)\nframe #1: std::function&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; ()&gt;::operator()() const + 0x4c (0x7f94b46e9622 in /opt/pytorch/torch/lib/libc10.so)\nframe #2: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) + 0x37 (0x7f94b46e7f4b in /opt/pytorch/torch/lib/libc10.so)\nframe #3: &lt;unknown function&gt; + 0x1152c19 (0x7f94b3ba5c19 in /opt/pytorch/torch/lib/libtorch.so.1)\nframe #4: &lt;unknown function&gt; + 0x1153084 (0x7f94b3ba6084 in /opt/pytorch/torch/lib/libtorch.so.1)\nframe #5: torch::jit::LowerAllTuples(std::shared_ptr&lt;torch::jit::Graph&gt;&amp;) + 0x28 (0x7f94b3ba640c in /opt/pytorch/torch/lib/libtorch.so.1)\nframe #6: &lt;unknown function&gt; + 0x92cced (0x7f94b9c23ced in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #7: &lt;unknown function&gt; + 0x925984 (0x7f94b9c1c984 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #8: &lt;unknown function&gt; + 0x91d5b3 (0x7f94b9c145b3 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #9: &lt;unknown function&gt; + 0x91d709 (0x7f94b9c14709 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #10: &lt;unknown function&gt; + 0x610e54 (0x7f94b9907e54 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\n&lt;omitting python frames&gt;\nframe #43: __libc_start_main + 0xf0 (0x7f94c86c5830 in /lib/x86_64-linux-gnu/libc.so.6)\n\n#\n</code></pre>\n<p>After adding PythonOp into the white list:</p>\n<pre><code>diff --git a/torch/csrc/jit/passes/lower_tuples.cpp b/torch/csrc/jit/passes/lower_tuples.cpp\nindex 3e1eebc..ac6bacc 100644\n--- a/torch/csrc/jit/passes/lower_tuples.cpp\n+++ b/torch/csrc/jit/passes/lower_tuples.cpp\n@@ -19,6 +19,7 @@ std::unordered_set&lt;Symbol&gt; white_list = {\n   prim::TupleSlice,\n   prim::Param,\n   prim::Return,\n+  prim::PythonOp,\n };\n\n</code></pre>\n<p>We can export correctly again:</p>\n<pre><code>ModelProto {\n  producer_name: \"pytorch\"\n  domain: \"\"\n  doc_string: \"\"\n  graph:\n    GraphProto {\n      name: \"torch-jit-export\"\n      inputs: [{name: \"input\", type:Tensor dims: 2}]\n      outputs: [{name: \"1\", type:Tensor dims: },{name: \"2\", type:Tensor dims: }]\n      initializers: []\n      nodes: [\n        Node {type: \"Split\", inputs: [input], outputs: [1,2], attributes: []}\n      ]\n    }\n  opset_import: [OperatorSetIdProto { domain: }],\n}\n</code></pre>\n<p>I am not sure what I have is a fix or workaround. I can submit a PR if this is the right fix.</p>\n<h2>Environment</h2>\n<pre><code>PyTorch version: 1.0.0a0+4b86a21\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] torch                     1.0.0a0+4b86a21           &lt;pip&gt;\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n[conda] umbotorch                 0.0.1+e7d7225             &lt;pip&gt;\n[conda] umbotorch                 0.0.1+4b94be3             &lt;pip&gt;\n</code></pre>\n<h2>Additional context</h2>\n", "body_text": "\ud83d\udc1b Bug\nWhen exporting a tuple-output custom Function to ONNX, the LowerAllTuple optimization pass will fail because Symb::PythonOP is not in the whitelist. The same export works in 0.4.1 but not master.\nThe failing assert is at\n\n  \n    \n      pytorch/torch/csrc/jit/passes/lower_tuples.cpp\n    \n    \n         Line 109\n      in\n      8752214\n    \n    \n    \n    \n\n        \n          \n           JIT_ASSERTM(white_list.count(n->kind()) > 0, \"tuple appears in op that does not forward tuples\"); \n        \n    \n  \n\n\ninvoked by \n  \n    \n      pytorch/torch/onnx/utils.py\n    \n    \n         Line 150\n      in\n      8752214\n    \n    \n    \n    \n\n        \n          \n           torch._C._jit_pass_lower_all_tuples(graph) \n        \n    \n  \n\n\nBy adding Symbol::PythonOp into the whiltelist, at \n  \n    \n      pytorch/torch/csrc/jit/passes/lower_tuples.cpp\n    \n    \n         Line 13\n      in\n      8752214\n    \n    \n    \n    \n\n        \n          \n           std::unordered_set<Symbol> white_list = { \n        \n    \n  \n\n\nwe can export normally again.\nTo Reproduce\nHere's a sample 1-input 2-output custom operator\nimport torch\n\nclass Split(torch.autograd.Function):\n    @staticmethod\n    def symbolic(g, input):\n        return g.op('Split', input, outputs=2)\n\n    @staticmethod\n    def forward(ctx, input):\n        return input[0], input[1]\n\ndef test_onnx_export():\n    class MyModule(torch.nn.Module):\n        def forward(self, input):\n            return Split().apply(input)\n\n    model_string = torch.onnx.export_to_pretty_string(\n                   MyModule(),\n                   (torch.tensor([0, 1])),\n                   \"/tmp/custom_op.onnx\")\n    print(model_string)\n\ntest_onnx_export()\n\nwhich can be exported correctly in pytorch-0.4.1\n(pytorch41)# python split.py \nModelProto {\n  producer_name: \"pytorch\"\n  domain: \"\"\n  doc_string: \"\"\n  graph:\n    GraphProto {\n      name: \"torch-jit-export\"\n      inputs: [{name: \"0\", type:Tensor dims: 2}]\n      outputs: [{name: \"1\", type:Tensor dims: },{name: \"2\", type:Tensor dims: }]\n      initializers: []\n      nodes: [\n        Node {type: \"Split\", inputs: [0], outputs: [1,2], attributes: []}\n      ]\n    }\n  opset_import: [OperatorSetIdProto { domain: }],\n}\n\nBut in pytorch-1.0+ 4b86a21\n(pytorch10) # python /workspace/split.py \nTraceback (most recent call last):\n  File \"/workspace/split.py\", line 23, in <module>\n    test_onnx_export()\n  File \"/workspace/split.py\", line 20, in test_onnx_export\n    \"/tmp/custom_op.onnx\")\n  File \"/opt/pytorch/torch/onnx/__init__.py\", line 32, in export_to_pretty_string\n    return utils.export_to_pretty_string(*args, **kwargs)\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 255, in export_to_pretty_string\n    export_type, example_outputs, propagate, google_printer)\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 265, in _export_to_pretty_string\n    example_outputs, propagate)\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 227, in _model_to_graph\n    graph = _optimize_graph(graph, operator_export_type)\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 150, in _optimize_graph\n    torch._C._jit_pass_lower_all_tuples(graph)\nRuntimeError: tuple appears in op that does not forward tuples (VisitNode at /opt/pytorch/torch/csrc/jit/passes/lower_tuples.cpp:109)\nframe #0: <unknown function> + 0x2f835 (0x7f94b46e8835 in /opt/pytorch/torch/lib/libc10.so)\nframe #1: std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > ()>::operator()() const + 0x4c (0x7f94b46e9622 in /opt/pytorch/torch/lib/libc10.so)\nframe #2: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x37 (0x7f94b46e7f4b in /opt/pytorch/torch/lib/libc10.so)\nframe #3: <unknown function> + 0x1152c19 (0x7f94b3ba5c19 in /opt/pytorch/torch/lib/libtorch.so.1)\nframe #4: <unknown function> + 0x1153084 (0x7f94b3ba6084 in /opt/pytorch/torch/lib/libtorch.so.1)\nframe #5: torch::jit::LowerAllTuples(std::shared_ptr<torch::jit::Graph>&) + 0x28 (0x7f94b3ba640c in /opt/pytorch/torch/lib/libtorch.so.1)\nframe #6: <unknown function> + 0x92cced (0x7f94b9c23ced in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #7: <unknown function> + 0x925984 (0x7f94b9c1c984 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #8: <unknown function> + 0x91d5b3 (0x7f94b9c145b3 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #9: <unknown function> + 0x91d709 (0x7f94b9c14709 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #10: <unknown function> + 0x610e54 (0x7f94b9907e54 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\n<omitting python frames>\nframe #43: __libc_start_main + 0xf0 (0x7f94c86c5830 in /lib/x86_64-linux-gnu/libc.so.6)\n\n#\n\nAfter adding PythonOp into the white list:\ndiff --git a/torch/csrc/jit/passes/lower_tuples.cpp b/torch/csrc/jit/passes/lower_tuples.cpp\nindex 3e1eebc..ac6bacc 100644\n--- a/torch/csrc/jit/passes/lower_tuples.cpp\n+++ b/torch/csrc/jit/passes/lower_tuples.cpp\n@@ -19,6 +19,7 @@ std::unordered_set<Symbol> white_list = {\n   prim::TupleSlice,\n   prim::Param,\n   prim::Return,\n+  prim::PythonOp,\n };\n\n\nWe can export correctly again:\nModelProto {\n  producer_name: \"pytorch\"\n  domain: \"\"\n  doc_string: \"\"\n  graph:\n    GraphProto {\n      name: \"torch-jit-export\"\n      inputs: [{name: \"input\", type:Tensor dims: 2}]\n      outputs: [{name: \"1\", type:Tensor dims: },{name: \"2\", type:Tensor dims: }]\n      initializers: []\n      nodes: [\n        Node {type: \"Split\", inputs: [input], outputs: [1,2], attributes: []}\n      ]\n    }\n  opset_import: [OperatorSetIdProto { domain: }],\n}\n\nI am not sure what I have is a fix or workaround. I can submit a PR if this is the right fix.\nEnvironment\nPyTorch version: 1.0.0a0+4b86a21\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] torch                     1.0.0a0+4b86a21           <pip>\n[conda] torchvision               0.2.1                     <pip>\n[conda] umbotorch                 0.0.1+e7d7225             <pip>\n[conda] umbotorch                 0.0.1+4b94be3             <pip>\n\nAdditional context", "body": "## \ud83d\udc1b Bug\r\nWhen exporting a tuple-output custom Function to ONNX, the LowerAllTuple optimization pass will fail because Symb::PythonOP is not in the whitelist. The same export works in 0.4.1 but not master.\r\n\r\nThe failing assert is at \r\nhttps://github.com/pytorch/pytorch/blob/8752214fb7534d3d3f83c1a459f24c57db86cd10/torch/csrc/jit/passes/lower_tuples.cpp#L109\r\ninvoked by https://github.com/pytorch/pytorch/blob/8752214fb7534d3d3f83c1a459f24c57db86cd10/torch/onnx/utils.py#L150 \r\n\r\nBy adding Symbol::PythonOp into the whiltelist, at https://github.com/pytorch/pytorch/blob/8752214fb7534d3d3f83c1a459f24c57db86cd10/torch/csrc/jit/passes/lower_tuples.cpp#L13\r\nwe can export normally again.\r\n\r\n## To Reproduce\r\nHere's a sample 1-input 2-output custom operator\r\n```\r\nimport torch\r\n\r\nclass Split(torch.autograd.Function):\r\n    @staticmethod\r\n    def symbolic(g, input):\r\n        return g.op('Split', input, outputs=2)\r\n\r\n    @staticmethod\r\n    def forward(ctx, input):\r\n        return input[0], input[1]\r\n\r\ndef test_onnx_export():\r\n    class MyModule(torch.nn.Module):\r\n        def forward(self, input):\r\n            return Split().apply(input)\r\n\r\n    model_string = torch.onnx.export_to_pretty_string(\r\n                   MyModule(),\r\n                   (torch.tensor([0, 1])),\r\n                   \"/tmp/custom_op.onnx\")\r\n    print(model_string)\r\n\r\ntest_onnx_export()\r\n```\r\nwhich can be exported correctly in pytorch-0.4.1\r\n```\r\n(pytorch41)# python split.py \r\nModelProto {\r\n  producer_name: \"pytorch\"\r\n  domain: \"\"\r\n  doc_string: \"\"\r\n  graph:\r\n    GraphProto {\r\n      name: \"torch-jit-export\"\r\n      inputs: [{name: \"0\", type:Tensor dims: 2}]\r\n      outputs: [{name: \"1\", type:Tensor dims: },{name: \"2\", type:Tensor dims: }]\r\n      initializers: []\r\n      nodes: [\r\n        Node {type: \"Split\", inputs: [0], outputs: [1,2], attributes: []}\r\n      ]\r\n    }\r\n  opset_import: [OperatorSetIdProto { domain: }],\r\n}\r\n```\r\n\r\nBut in pytorch-1.0+ 4b86a215\r\n```\r\n(pytorch10) # python /workspace/split.py \r\nTraceback (most recent call last):\r\n  File \"/workspace/split.py\", line 23, in <module>\r\n    test_onnx_export()\r\n  File \"/workspace/split.py\", line 20, in test_onnx_export\r\n    \"/tmp/custom_op.onnx\")\r\n  File \"/opt/pytorch/torch/onnx/__init__.py\", line 32, in export_to_pretty_string\r\n    return utils.export_to_pretty_string(*args, **kwargs)\r\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 255, in export_to_pretty_string\r\n    export_type, example_outputs, propagate, google_printer)\r\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 265, in _export_to_pretty_string\r\n    example_outputs, propagate)\r\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 227, in _model_to_graph\r\n    graph = _optimize_graph(graph, operator_export_type)\r\n  File \"/opt/pytorch/torch/onnx/utils.py\", line 150, in _optimize_graph\r\n    torch._C._jit_pass_lower_all_tuples(graph)\r\nRuntimeError: tuple appears in op that does not forward tuples (VisitNode at /opt/pytorch/torch/csrc/jit/passes/lower_tuples.cpp:109)\r\nframe #0: <unknown function> + 0x2f835 (0x7f94b46e8835 in /opt/pytorch/torch/lib/libc10.so)\r\nframe #1: std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > ()>::operator()() const + 0x4c (0x7f94b46e9622 in /opt/pytorch/torch/lib/libc10.so)\r\nframe #2: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x37 (0x7f94b46e7f4b in /opt/pytorch/torch/lib/libc10.so)\r\nframe #3: <unknown function> + 0x1152c19 (0x7f94b3ba5c19 in /opt/pytorch/torch/lib/libtorch.so.1)\r\nframe #4: <unknown function> + 0x1153084 (0x7f94b3ba6084 in /opt/pytorch/torch/lib/libtorch.so.1)\r\nframe #5: torch::jit::LowerAllTuples(std::shared_ptr<torch::jit::Graph>&) + 0x28 (0x7f94b3ba640c in /opt/pytorch/torch/lib/libtorch.so.1)\r\nframe #6: <unknown function> + 0x92cced (0x7f94b9c23ced in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\r\nframe #7: <unknown function> + 0x925984 (0x7f94b9c1c984 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\r\nframe #8: <unknown function> + 0x91d5b3 (0x7f94b9c145b3 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\r\nframe #9: <unknown function> + 0x91d709 (0x7f94b9c14709 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\r\nframe #10: <unknown function> + 0x610e54 (0x7f94b9907e54 in /opt/pytorch/torch/_C.cpython-36m-x86_64-linux-gnu.so)\r\n<omitting python frames>\r\nframe #43: __libc_start_main + 0xf0 (0x7f94c86c5830 in /lib/x86_64-linux-gnu/libc.so.6)\r\n\r\n#\r\n```\r\n\r\nAfter adding PythonOp into the white list:\r\n```\r\ndiff --git a/torch/csrc/jit/passes/lower_tuples.cpp b/torch/csrc/jit/passes/lower_tuples.cpp\r\nindex 3e1eebc..ac6bacc 100644\r\n--- a/torch/csrc/jit/passes/lower_tuples.cpp\r\n+++ b/torch/csrc/jit/passes/lower_tuples.cpp\r\n@@ -19,6 +19,7 @@ std::unordered_set<Symbol> white_list = {\r\n   prim::TupleSlice,\r\n   prim::Param,\r\n   prim::Return,\r\n+  prim::PythonOp,\r\n };\r\n\r\n```\r\nWe can export correctly again:\r\n```\r\nModelProto {\r\n  producer_name: \"pytorch\"\r\n  domain: \"\"\r\n  doc_string: \"\"\r\n  graph:\r\n    GraphProto {\r\n      name: \"torch-jit-export\"\r\n      inputs: [{name: \"input\", type:Tensor dims: 2}]\r\n      outputs: [{name: \"1\", type:Tensor dims: },{name: \"2\", type:Tensor dims: }]\r\n      initializers: []\r\n      nodes: [\r\n        Node {type: \"Split\", inputs: [input], outputs: [1,2], attributes: []}\r\n      ]\r\n    }\r\n  opset_import: [OperatorSetIdProto { domain: }],\r\n}\r\n```\r\n\r\nI am not sure what I have is a fix or workaround. I can submit a PR if this is the right fix.\r\n\r\n## Environment\r\n```\r\nPyTorch version: 1.0.0a0+4b86a21\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] torch                     1.0.0a0+4b86a21           <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n[conda] umbotorch                 0.0.1+e7d7225             <pip>\r\n[conda] umbotorch                 0.0.1+4b94be3             <pip>\r\n```\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}