{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379838226", "html_url": "https://github.com/pytorch/pytorch/issues/6411#issuecomment-379838226", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6411", "id": 379838226, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTgzODIyNg==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-09T17:54:17Z", "updated_at": "2018-04-09T17:54:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We can't require <code>padding_idx</code> embedding to always be 0 because</p>\n<ol>\n<li>The weight matrix is just a tensor, which is the same as any other tensor. In fact, one can use any tensor with proper shape as the weight matrix (both in module form and functional form of embedding). So it will be difficult and doesn't make sense to impose such a constraint</li>\n<li>Sometimes people may actually want to set <code>padding_idx</code> to some specific embedding.</li>\n</ol>\n<p>I'll add a note about this in our document.</p>", "body_text": "We can't require padding_idx embedding to always be 0 because\n\nThe weight matrix is just a tensor, which is the same as any other tensor. In fact, one can use any tensor with proper shape as the weight matrix (both in module form and functional form of embedding). So it will be difficult and doesn't make sense to impose such a constraint\nSometimes people may actually want to set padding_idx to some specific embedding.\n\nI'll add a note about this in our document.", "body": "We can't require `padding_idx` embedding to always be 0 because \r\n1. The weight matrix is just a tensor, which is the same as any other tensor. In fact, one can use any tensor with proper shape as the weight matrix (both in module form and functional form of embedding). So it will be difficult and doesn't make sense to impose such a constraint\r\n2. Sometimes people may actually want to set `padding_idx` to some specific embedding.\r\n\r\nI'll add a note about this in our document. "}