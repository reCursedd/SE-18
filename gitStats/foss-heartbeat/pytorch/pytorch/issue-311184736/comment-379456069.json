{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379456069", "html_url": "https://github.com/pytorch/pytorch/issues/6267#issuecomment-379456069", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6267", "id": 379456069, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTQ1NjA2OQ==", "user": {"login": "KaiyuYue", "id": 19852297, "node_id": "MDQ6VXNlcjE5ODUyMjk3", "avatar_url": "https://avatars1.githubusercontent.com/u/19852297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KaiyuYue", "html_url": "https://github.com/KaiyuYue", "followers_url": "https://api.github.com/users/KaiyuYue/followers", "following_url": "https://api.github.com/users/KaiyuYue/following{/other_user}", "gists_url": "https://api.github.com/users/KaiyuYue/gists{/gist_id}", "starred_url": "https://api.github.com/users/KaiyuYue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KaiyuYue/subscriptions", "organizations_url": "https://api.github.com/users/KaiyuYue/orgs", "repos_url": "https://api.github.com/users/KaiyuYue/repos", "events_url": "https://api.github.com/users/KaiyuYue/events{/privacy}", "received_events_url": "https://api.github.com/users/KaiyuYue/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-07T09:21:32Z", "updated_at": "2018-04-07T09:36:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a> , sorry about my re-comment in this issue, I am sure about that the function <code>torch.nn.parallel.DistributedDataParallel</code> has bugs, it will hang from the beginning. The code is kept the same with <a href=\"https://github.com/pytorch/examples/blob/master/imagenet/main.py\">distributed example</a>. Instead of using <code>torch.nn.parallel.DistributedDataParallel</code>, <code>torch.nn.DataParallel</code> works well. So using <code>torch.nn.DataParallel</code> with the distributed sampler <code>torch.utils.data.distributed.DistributedSampler</code> can play a same effect as the situation of using <code>torch.nn.parallel.DistributedDataParallel</code>, right? Thanks.</p>", "body_text": "Hi, @teng-li , sorry about my re-comment in this issue, I am sure about that the function torch.nn.parallel.DistributedDataParallel has bugs, it will hang from the beginning. The code is kept the same with distributed example. Instead of using torch.nn.parallel.DistributedDataParallel, torch.nn.DataParallel works well. So using torch.nn.DataParallel with the distributed sampler torch.utils.data.distributed.DistributedSampler can play a same effect as the situation of using torch.nn.parallel.DistributedDataParallel, right? Thanks.", "body": "Hi, @teng-li , sorry about my re-comment in this issue, I am sure about that the function `torch.nn.parallel.DistributedDataParallel` has bugs, it will hang from the beginning. The code is kept the same with [distributed example](https://github.com/pytorch/examples/blob/master/imagenet/main.py). Instead of using `torch.nn.parallel.DistributedDataParallel`, `torch.nn.DataParallel` works well. So using `torch.nn.DataParallel` with the distributed sampler `torch.utils.data.distributed.DistributedSampler` can play a same effect as the situation of using `torch.nn.parallel.DistributedDataParallel`, right? Thanks."}