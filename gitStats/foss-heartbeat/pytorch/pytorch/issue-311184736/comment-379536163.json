{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379536163", "html_url": "https://github.com/pytorch/pytorch/issues/6267#issuecomment-379536163", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6267", "id": 379536163, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTUzNjE2Mw==", "user": {"login": "KaiyuYue", "id": 19852297, "node_id": "MDQ6VXNlcjE5ODUyMjk3", "avatar_url": "https://avatars1.githubusercontent.com/u/19852297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KaiyuYue", "html_url": "https://github.com/KaiyuYue", "followers_url": "https://api.github.com/users/KaiyuYue/followers", "following_url": "https://api.github.com/users/KaiyuYue/following{/other_user}", "gists_url": "https://api.github.com/users/KaiyuYue/gists{/gist_id}", "starred_url": "https://api.github.com/users/KaiyuYue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KaiyuYue/subscriptions", "organizations_url": "https://api.github.com/users/KaiyuYue/orgs", "repos_url": "https://api.github.com/users/KaiyuYue/repos", "events_url": "https://api.github.com/users/KaiyuYue/events{/privacy}", "received_events_url": "https://api.github.com/users/KaiyuYue/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-08T09:38:22Z", "updated_at": "2018-04-08T09:41:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a>:</p>\n<p>The number of data loader workers I used is <code>4 * (gpu_nums per single_machine)</code>, our single machine has 8 gpus, so it is 32.</p>\n<p>According to the <a href=\"http://pytorch.org/docs/master/notes/multiprocessing.html#sharing-cuda-tensors\" rel=\"nofollow\">doc</a> about multiprocessing, I add these codes at the beginning of <code>train.py</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> multiprocessing <span class=\"pl-k\">as</span> mp\n<span class=\"pl-k\">try</span>:\n    mp.set_start_method(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>spawn<span class=\"pl-pds\">'</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> spawn, forkserver, and fork</span>\n<span class=\"pl-k\">except</span> <span class=\"pl-c1\">RuntimeError</span>:\n    <span class=\"pl-k\">pass</span></pre></div>\n<p>and try the different methods (<code>spawn</code>, <code>forkserver</code>, and <code>fork</code>) with using <code>torch.nn.parallel.DistributedDataParallel</code>. No good news, training still fails.</p>\n<p>Moreover, I change the num of data loader workers into <code>1</code>, as the same result that they still can't work.</p>", "body_text": "Hi, @teng-li:\nThe number of data loader workers I used is 4 * (gpu_nums per single_machine), our single machine has 8 gpus, so it is 32.\nAccording to the doc about multiprocessing, I add these codes at the beginning of train.py:\nimport multiprocessing as mp\ntry:\n    mp.set_start_method('spawn') # spawn, forkserver, and fork\nexcept RuntimeError:\n    pass\nand try the different methods (spawn, forkserver, and fork) with using torch.nn.parallel.DistributedDataParallel. No good news, training still fails.\nMoreover, I change the num of data loader workers into 1, as the same result that they still can't work.", "body": "Hi, @teng-li:\r\n\r\nThe number of data loader workers I used is `4 * (gpu_nums per single_machine)`, our single machine has 8 gpus, so it is 32. \r\n\r\nAccording to the [doc](http://pytorch.org/docs/master/notes/multiprocessing.html#sharing-cuda-tensors) about multiprocessing, I add these codes at the beginning of `train.py`:\r\n```python\r\nimport multiprocessing as mp\r\ntry:\r\n    mp.set_start_method('spawn') # spawn, forkserver, and fork\r\nexcept RuntimeError:\r\n    pass\r\n```\r\nand try the different methods (`spawn`, `forkserver`, and `fork`) with using `torch.nn.parallel.DistributedDataParallel`. No good news, training still fails. \r\n\r\nMoreover, I change the num of data loader workers into `1`, as the same result that they still can't work."}