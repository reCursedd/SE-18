{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6267", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6267/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6267/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6267/events", "html_url": "https://github.com/pytorch/pytorch/issues/6267", "id": 311184736, "node_id": "MDU6SXNzdWUzMTExODQ3MzY=", "number": 6267, "title": "[Distributed PyTorch] Gloo hang at the initialization ", "user": {"login": "KaiyuYue", "id": 19852297, "node_id": "MDQ6VXNlcjE5ODUyMjk3", "avatar_url": "https://avatars1.githubusercontent.com/u/19852297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KaiyuYue", "html_url": "https://github.com/KaiyuYue", "followers_url": "https://api.github.com/users/KaiyuYue/followers", "following_url": "https://api.github.com/users/KaiyuYue/following{/other_user}", "gists_url": "https://api.github.com/users/KaiyuYue/gists{/gist_id}", "starred_url": "https://api.github.com/users/KaiyuYue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KaiyuYue/subscriptions", "organizations_url": "https://api.github.com/users/KaiyuYue/orgs", "repos_url": "https://api.github.com/users/KaiyuYue/repos", "events_url": "https://api.github.com/users/KaiyuYue/events{/privacy}", "received_events_url": "https://api.github.com/users/KaiyuYue/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}, {"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 21, "created_at": "2018-04-04T11:08:41Z", "updated_at": "2018-05-12T14:45:59Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>I change the issue, the problem before is solved. Sorry to bother. The only problem is this runtime error:</p>\n<pre><code>Traceback (most recent call last): \nFile \"dist_m.py\", line 881, in \nTraceback (most recent call last): \nFile \"dist_m.py\", line 881, in \ndist_main() \ndist_main() \nFile \"dist_m.py\", line 421, in dist_main \nFile \"dist_m.py\", line 421, in dist_main \nmodel = modules.build_model(args) \nmodel = modules.build_model(args) \nFile \"/home/slurm/job/tmp/job-21086/modules.py\", line 134, in build_model \ngroups = args.groups \nFile \"/home/slurm/job/tmp/job-21086/models/c_resnet_1_nl.py\", line 873, in resnet50 \nFile \"/home/slurm/job/tmp/job-21086/modules.py\", line 134, in build_model \ngroups = args.groups \nFile \"/home/slurm/job/tmp/job-21086/models/c_resnet_1_nl.py\", line 873, in resnet50 \nmodel = torch.nn.parallel.DistributedDataParallel(model.cuda(), device_ids=device_ids) \nmodel = torch.nn.parallel.DistributedDataParallel(model.cuda(), device_ids=device_ids) \nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 97, in __init__ \nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 97, in __init__ \ndist.broadcast(p, 0) \nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/distributed/__init__.py\", line 290, in broadcast \ndist.broadcast(p, 0) \nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/distributed/__init__.py\", line 290, in broadcast \nreturn torch._C._dist_broadcast(tensor, src, group) \nRuntimeError: Invalid broadcastT function type \nreturn torch._C._dist_broadcast(tensor, src, group) \nRuntimeError: Invalid broadcastT function type \nterminate called after throwing an instance of 'gloo::EnforceNotMet' \nwhat(): [enforce fail at /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down \nterminate called after throwing an instance of 'gloo::EnforceNotMet' \nwhat(): [enforce fail at /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down \n</code></pre>\n<p>So, What is the type of tensor not supported by gloo? I check my code, the types of functions I use include <code>torch.scatter_add_()</code>, <code>torch.gather()</code>, <code>torch.addcmul()</code>,  and <code>fft function</code> from <a href=\"https://github.com/locuslab/pytorch_fft\">here</a>. Is it <code>fft functions</code> not supported?</p>", "body_text": "I change the issue, the problem before is solved. Sorry to bother. The only problem is this runtime error:\nTraceback (most recent call last): \nFile \"dist_m.py\", line 881, in \nTraceback (most recent call last): \nFile \"dist_m.py\", line 881, in \ndist_main() \ndist_main() \nFile \"dist_m.py\", line 421, in dist_main \nFile \"dist_m.py\", line 421, in dist_main \nmodel = modules.build_model(args) \nmodel = modules.build_model(args) \nFile \"/home/slurm/job/tmp/job-21086/modules.py\", line 134, in build_model \ngroups = args.groups \nFile \"/home/slurm/job/tmp/job-21086/models/c_resnet_1_nl.py\", line 873, in resnet50 \nFile \"/home/slurm/job/tmp/job-21086/modules.py\", line 134, in build_model \ngroups = args.groups \nFile \"/home/slurm/job/tmp/job-21086/models/c_resnet_1_nl.py\", line 873, in resnet50 \nmodel = torch.nn.parallel.DistributedDataParallel(model.cuda(), device_ids=device_ids) \nmodel = torch.nn.parallel.DistributedDataParallel(model.cuda(), device_ids=device_ids) \nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 97, in __init__ \nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 97, in __init__ \ndist.broadcast(p, 0) \nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/distributed/__init__.py\", line 290, in broadcast \ndist.broadcast(p, 0) \nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/distributed/__init__.py\", line 290, in broadcast \nreturn torch._C._dist_broadcast(tensor, src, group) \nRuntimeError: Invalid broadcastT function type \nreturn torch._C._dist_broadcast(tensor, src, group) \nRuntimeError: Invalid broadcastT function type \nterminate called after throwing an instance of 'gloo::EnforceNotMet' \nwhat(): [enforce fail at /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down \nterminate called after throwing an instance of 'gloo::EnforceNotMet' \nwhat(): [enforce fail at /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down \n\nSo, What is the type of tensor not supported by gloo? I check my code, the types of functions I use include torch.scatter_add_(), torch.gather(), torch.addcmul(),  and fft function from here. Is it fft functions not supported?", "body": "I change the issue, the problem before is solved. Sorry to bother. The only problem is this runtime error:\r\n```\r\nTraceback (most recent call last): \r\nFile \"dist_m.py\", line 881, in \r\nTraceback (most recent call last): \r\nFile \"dist_m.py\", line 881, in \r\ndist_main() \r\ndist_main() \r\nFile \"dist_m.py\", line 421, in dist_main \r\nFile \"dist_m.py\", line 421, in dist_main \r\nmodel = modules.build_model(args) \r\nmodel = modules.build_model(args) \r\nFile \"/home/slurm/job/tmp/job-21086/modules.py\", line 134, in build_model \r\ngroups = args.groups \r\nFile \"/home/slurm/job/tmp/job-21086/models/c_resnet_1_nl.py\", line 873, in resnet50 \r\nFile \"/home/slurm/job/tmp/job-21086/modules.py\", line 134, in build_model \r\ngroups = args.groups \r\nFile \"/home/slurm/job/tmp/job-21086/models/c_resnet_1_nl.py\", line 873, in resnet50 \r\nmodel = torch.nn.parallel.DistributedDataParallel(model.cuda(), device_ids=device_ids) \r\nmodel = torch.nn.parallel.DistributedDataParallel(model.cuda(), device_ids=device_ids) \r\nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 97, in __init__ \r\nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/distributed.py\", line 97, in __init__ \r\ndist.broadcast(p, 0) \r\nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/distributed/__init__.py\", line 290, in broadcast \r\ndist.broadcast(p, 0) \r\nFile \"/home/slurm/job/tmp/job-21086/anaconda2/lib/python2.7/site-packages/torch/distributed/__init__.py\", line 290, in broadcast \r\nreturn torch._C._dist_broadcast(tensor, src, group) \r\nRuntimeError: Invalid broadcastT function type \r\nreturn torch._C._dist_broadcast(tensor, src, group) \r\nRuntimeError: Invalid broadcastT function type \r\nterminate called after throwing an instance of 'gloo::EnforceNotMet' \r\nwhat(): [enforce fail at /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down \r\nterminate called after throwing an instance of 'gloo::EnforceNotMet' \r\nwhat(): [enforce fail at /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /home/yuekaiyu/code/distributed-pytorch/distributed-pytorch-0.4.0/pytorch/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down \r\n```\r\nSo, What is the type of tensor not supported by gloo? I check my code, the types of functions I use include `torch.scatter_add_()`, `torch.gather()`, `torch.addcmul()`,  and `fft function` from [here](https://github.com/locuslab/pytorch_fft). Is it `fft functions` not supported?"}