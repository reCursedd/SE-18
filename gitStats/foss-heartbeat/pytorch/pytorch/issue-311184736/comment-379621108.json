{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379621108", "html_url": "https://github.com/pytorch/pytorch/issues/6267#issuecomment-379621108", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6267", "id": 379621108, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTYyMTEwOA==", "user": {"login": "KaiyuYue", "id": 19852297, "node_id": "MDQ6VXNlcjE5ODUyMjk3", "avatar_url": "https://avatars1.githubusercontent.com/u/19852297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KaiyuYue", "html_url": "https://github.com/KaiyuYue", "followers_url": "https://api.github.com/users/KaiyuYue/followers", "following_url": "https://api.github.com/users/KaiyuYue/following{/other_user}", "gists_url": "https://api.github.com/users/KaiyuYue/gists{/gist_id}", "starred_url": "https://api.github.com/users/KaiyuYue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KaiyuYue/subscriptions", "organizations_url": "https://api.github.com/users/KaiyuYue/orgs", "repos_url": "https://api.github.com/users/KaiyuYue/repos", "events_url": "https://api.github.com/users/KaiyuYue/events{/privacy}", "received_events_url": "https://api.github.com/users/KaiyuYue/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-09T03:42:17Z", "updated_at": "2018-04-09T03:47:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a>, I have a quick question. I use three machines to do the distributed task, as the <a href=\"http://pytorch.org/docs/0.3.1/distributed.html#basics\" rel=\"nofollow\">doc</a> said, it provides the <code>synchronous </code> distributed training. In my practical experiments, it seems like that each machine does its own training and the epoch number of these three machines cannot be synced to be the same at the same time (e.g. one is faster than the others). So I want to ask when does the master rank machine gather/collect the gradients of three machines? Or I have the experimental error in my distributed training. Thanks.</p>", "body_text": "Hi, @fmassa, I have a quick question. I use three machines to do the distributed task, as the doc said, it provides the synchronous  distributed training. In my practical experiments, it seems like that each machine does its own training and the epoch number of these three machines cannot be synced to be the same at the same time (e.g. one is faster than the others). So I want to ask when does the master rank machine gather/collect the gradients of three machines? Or I have the experimental error in my distributed training. Thanks.", "body": "Hi, @fmassa, I have a quick question. I use three machines to do the distributed task, as the [doc](http://pytorch.org/docs/0.3.1/distributed.html#basics) said, it provides the `synchronous ` distributed training. In my practical experiments, it seems like that each machine does its own training and the epoch number of these three machines cannot be synced to be the same at the same time (e.g. one is faster than the others). So I want to ask when does the master rank machine gather/collect the gradients of three machines? Or I have the experimental error in my distributed training. Thanks."}