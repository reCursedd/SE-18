{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4274", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4274/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4274/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4274/events", "html_url": "https://github.com/pytorch/pytorch/issues/4274", "id": 283600787, "node_id": "MDU6SXNzdWUyODM2MDA3ODc=", "number": 4274, "title": "Stop using undefined tensors to represent zero gradients in engine", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-20T15:30:07Z", "updated_at": "2017-12-20T17:29:10Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p><strong>Motivation.</strong> The treatment of undefined tensors in the autograd engine has been a continual source of confusion in PyTorch internals. Instances of such bugs: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"270138240\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3412\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3412/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3412\">#3412</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"282717605\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4215\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4215/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4215\">#4215</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"282537782\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4198\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4198/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4198\">#4198</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"274653738\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3743\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3743/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3743\">#3743</a>. Also see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"275524673\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3801\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3801/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3801\">#3801</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"241235871\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2003\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2003/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2003\">#2003</a>. (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"282739000\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4217\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4217/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4217\">#4217</a> and <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"282739000\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4217\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4217/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4217\">#4217</a> are also undefined tensor bugs but not quite related.) The crux of the problem is that we're using undefined tensors to mean two things:</p>\n<ol>\n<li>\n<p>A value for a default initialized tensor, for which we need a non-allocating \"null pointer-like\" construct suitable for use as the default in optional arguments and all of those places (<code>std::vector</code>, <code>std::tuple</code>) where C++ makes life very difficult if you don't have a default constructor.</p>\n</li>\n<li>\n<p>When passing around gradients in the autograd engine, we need an efficient mechanism of representing zero gradients.  Semantically, a zero gradient is equivalent to creating a zero filled tensor of the appropriate size and type, but actually materializing such a tensor involves doing a bunch of unnecessary allocations.</p>\n</li>\n</ol>\n<p>These two use-cases are often in tension with each other. For example, it would be no problem to use a null pointer to represent undefined tensors in case (1) (as segfaults in these situations are easy to detect at develop time, and are simply a matter of good hygiene) whereas we have made enough errors in case (2) that we introduced a patch to turn undefined tensors into a full-fledged object, <em>simply</em> to avoid segfaulting when we got it wrong; see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"271143246\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3482\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3482/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3482\">#3482</a>.</p>\n<p><strong>Proposal.</strong> We propose to <em>distinguish</em> between these two use-cases, introducing a new concept of zero-filled tensor. By not eliminating the concept of an undefined tensor, we avoid many of the objections levied against earlier versions of this proposal. There are a number of possible implementation strategies, I'll describe two of them:</p>\n<ul>\n<li><strong>Zero-filled tensor as immutable zero-dim tensor</strong>\n<ul>\n<li>For every tensor type, we define a lazily initialized, singleton tensor instance, which is zero-dimensional and zero-filled. As a singleton tensor, there is a pointer-equality test to see if your tensor is zero-filled.</li>\n<li>This tensor's backing data is placed in read-only memory, so that attempts to \"overwrite zero\" will result in a fault</li>\n<li>Autograd engine tests to see if a tensor is zero-filled anywhere it previously tested for undefined tensors, and uses this to skip additions when possible.</li>\n<li>Can zero filled tensors be passed on to further backwards functions?  The hazard pointed out by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6359743\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/albanD\">@albanD</a> (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"272055539\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/zdevito/ATen/issues/163\" data-hovercard-type=\"issue\" data-hovercard-url=\"/zdevito/ATen/issues/163/hovercard?comment_id=342772547&amp;comment_type=issue_comment\" href=\"https://github.com/zdevito/ATen/issues/163#issuecomment-342772547\">zdevito/ATen#163 (comment)</a>) is that some operations require contiguous tensors. Additionally, it is only valid to pass a zero-dim tensor in place of regular tensor if the operation in question is broadcasting. I see two possible resolutions for this problem:\n<ul>\n<li>We accept as a contract that we never pass on zero-dimensional gradients to subsequent backward functions. In this case, the engine is expected to materialize the zero-filled tensor into a full tensor if it needs to pass on. This may happen already with InputBuffer.</li>\n<li>We adjust all backwards functions to broadcast gradients, and all implementations are responsible for expanding and calling contiguous as necessary.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>Zero-filled tensor as a new Type</strong>\n<ul>\n<li>The previous strategy is parsimonious, but it has the problem that zero-dim tensors are not semantically equivalent an n-dim zero-filled tensor. An alternate strategy which doesn't have this problem is to introduce another Type which <em>is</em> semantically equivalent to an n-dim zero-filled tensor, but with a more efficient backing implementation.</li>\n<li>As a new Type, we must provide implementations for all of the operations. As a first approximation, all of these operations could be left as exception raising, NYI (similar to how an undefined tensor is defined today), but we could slowly build out support as is necessary.</li>\n<li>(Not exactly sure how to dispatch CPUDoubleType + ZeroFilledType. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a>, maybe you have ideas?)</li>\n</ul>\n</li>\n</ul>\n<p>Tensors continue to default construct into undefined tensors. After this change is in place, we will revert <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"271143246\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3482\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3482/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3482\">#3482</a>, restoring undefined tensors into actual null pointers which will segfault if you attempt to access them.</p>\n<p>This proposal obsoletes <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"272055539\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/zdevito/ATen/issues/163\" data-hovercard-type=\"issue\" data-hovercard-url=\"/zdevito/ATen/issues/163/hovercard\" href=\"https://github.com/zdevito/ATen/issues/163\">zdevito/ATen#163</a></p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>", "body_text": "Motivation. The treatment of undefined tensors in the autograd engine has been a continual source of confusion in PyTorch internals. Instances of such bugs: #3412, #4215, #4198, #3743. Also see #3801, #2003. (#4217 and #4217 are also undefined tensor bugs but not quite related.) The crux of the problem is that we're using undefined tensors to mean two things:\n\n\nA value for a default initialized tensor, for which we need a non-allocating \"null pointer-like\" construct suitable for use as the default in optional arguments and all of those places (std::vector, std::tuple) where C++ makes life very difficult if you don't have a default constructor.\n\n\nWhen passing around gradients in the autograd engine, we need an efficient mechanism of representing zero gradients.  Semantically, a zero gradient is equivalent to creating a zero filled tensor of the appropriate size and type, but actually materializing such a tensor involves doing a bunch of unnecessary allocations.\n\n\nThese two use-cases are often in tension with each other. For example, it would be no problem to use a null pointer to represent undefined tensors in case (1) (as segfaults in these situations are easy to detect at develop time, and are simply a matter of good hygiene) whereas we have made enough errors in case (2) that we introduced a patch to turn undefined tensors into a full-fledged object, simply to avoid segfaulting when we got it wrong; see #3482.\nProposal. We propose to distinguish between these two use-cases, introducing a new concept of zero-filled tensor. By not eliminating the concept of an undefined tensor, we avoid many of the objections levied against earlier versions of this proposal. There are a number of possible implementation strategies, I'll describe two of them:\n\nZero-filled tensor as immutable zero-dim tensor\n\nFor every tensor type, we define a lazily initialized, singleton tensor instance, which is zero-dimensional and zero-filled. As a singleton tensor, there is a pointer-equality test to see if your tensor is zero-filled.\nThis tensor's backing data is placed in read-only memory, so that attempts to \"overwrite zero\" will result in a fault\nAutograd engine tests to see if a tensor is zero-filled anywhere it previously tested for undefined tensors, and uses this to skip additions when possible.\nCan zero filled tensors be passed on to further backwards functions?  The hazard pointed out by @albanD (zdevito/ATen#163 (comment)) is that some operations require contiguous tensors. Additionally, it is only valid to pass a zero-dim tensor in place of regular tensor if the operation in question is broadcasting. I see two possible resolutions for this problem:\n\nWe accept as a contract that we never pass on zero-dimensional gradients to subsequent backward functions. In this case, the engine is expected to materialize the zero-filled tensor into a full tensor if it needs to pass on. This may happen already with InputBuffer.\nWe adjust all backwards functions to broadcast gradients, and all implementations are responsible for expanding and calling contiguous as necessary.\n\n\n\n\nZero-filled tensor as a new Type\n\nThe previous strategy is parsimonious, but it has the problem that zero-dim tensors are not semantically equivalent an n-dim zero-filled tensor. An alternate strategy which doesn't have this problem is to introduce another Type which is semantically equivalent to an n-dim zero-filled tensor, but with a more efficient backing implementation.\nAs a new Type, we must provide implementations for all of the operations. As a first approximation, all of these operations could be left as exception raising, NYI (similar to how an undefined tensor is defined today), but we could slowly build out support as is necessary.\n(Not exactly sure how to dispatch CPUDoubleType + ZeroFilledType. @zdevito, maybe you have ideas?)\n\n\n\nTensors continue to default construct into undefined tensors. After this change is in place, we will revert #3482, restoring undefined tensors into actual null pointers which will segfault if you attempt to access them.\nThis proposal obsoletes zdevito/ATen#163\nCC @gchanan @zdevito @colesbury @apaszke @soumith", "body": "**Motivation.** The treatment of undefined tensors in the autograd engine has been a continual source of confusion in PyTorch internals. Instances of such bugs: #3412, #4215, #4198, #3743. Also see #3801, #2003. (#4217 and #4217 are also undefined tensor bugs but not quite related.) The crux of the problem is that we're using undefined tensors to mean two things:\r\n\r\n1. A value for a default initialized tensor, for which we need a non-allocating \"null pointer-like\" construct suitable for use as the default in optional arguments and all of those places (`std::vector`, `std::tuple`) where C++ makes life very difficult if you don't have a default constructor.\r\n\r\n2. When passing around gradients in the autograd engine, we need an efficient mechanism of representing zero gradients.  Semantically, a zero gradient is equivalent to creating a zero filled tensor of the appropriate size and type, but actually materializing such a tensor involves doing a bunch of unnecessary allocations.\r\n\r\nThese two use-cases are often in tension with each other. For example, it would be no problem to use a null pointer to represent undefined tensors in case (1) (as segfaults in these situations are easy to detect at develop time, and are simply a matter of good hygiene) whereas we have made enough errors in case (2) that we introduced a patch to turn undefined tensors into a full-fledged object, *simply* to avoid segfaulting when we got it wrong; see #3482.\r\n\r\n**Proposal.** We propose to *distinguish* between these two use-cases, introducing a new concept of zero-filled tensor. By not eliminating the concept of an undefined tensor, we avoid many of the objections levied against earlier versions of this proposal. There are a number of possible implementation strategies, I'll describe two of them:\r\n\r\n* **Zero-filled tensor as immutable zero-dim tensor**\r\n  * For every tensor type, we define a lazily initialized, singleton tensor instance, which is zero-dimensional and zero-filled. As a singleton tensor, there is a pointer-equality test to see if your tensor is zero-filled.\r\n  * This tensor's backing data is placed in read-only memory, so that attempts to \"overwrite zero\" will result in a fault\r\n  * Autograd engine tests to see if a tensor is zero-filled anywhere it previously tested for undefined tensors, and uses this to skip additions when possible.\r\n  * Can zero filled tensors be passed on to further backwards functions?  The hazard pointed out by @albanD (https://github.com/zdevito/ATen/issues/163#issuecomment-342772547) is that some operations require contiguous tensors. Additionally, it is only valid to pass a zero-dim tensor in place of regular tensor if the operation in question is broadcasting. I see two possible resolutions for this problem:\r\n    * We accept as a contract that we never pass on zero-dimensional gradients to subsequent backward functions. In this case, the engine is expected to materialize the zero-filled tensor into a full tensor if it needs to pass on. This may happen already with InputBuffer.\r\n    * We adjust all backwards functions to broadcast gradients, and all implementations are responsible for expanding and calling contiguous as necessary.\r\n* **Zero-filled tensor as a new Type**\r\n  * The previous strategy is parsimonious, but it has the problem that zero-dim tensors are not semantically equivalent an n-dim zero-filled tensor. An alternate strategy which doesn't have this problem is to introduce another Type which *is* semantically equivalent to an n-dim zero-filled tensor, but with a more efficient backing implementation.\r\n  * As a new Type, we must provide implementations for all of the operations. As a first approximation, all of these operations could be left as exception raising, NYI (similar to how an undefined tensor is defined today), but we could slowly build out support as is necessary.\r\n  * (Not exactly sure how to dispatch CPUDoubleType + ZeroFilledType. @zdevito, maybe you have ideas?)\r\n\r\nTensors continue to default construct into undefined tensors. After this change is in place, we will revert #3482, restoring undefined tensors into actual null pointers which will segfault if you attempt to access them.\r\n\r\nThis proposal obsoletes https://github.com/zdevito/ATen/issues/163\r\n\r\nCC @gchanan @zdevito @colesbury @apaszke @soumith "}