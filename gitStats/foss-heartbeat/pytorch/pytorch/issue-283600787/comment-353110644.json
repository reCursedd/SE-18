{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/353110644", "html_url": "https://github.com/pytorch/pytorch/issues/4274#issuecomment-353110644", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4274", "id": 353110644, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MzExMDY0NA==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-20T16:25:30Z", "updated_at": "2017-12-20T16:25:30Z", "author_association": "COLLABORATOR", "body_html": "<p>I think that is a very interesting change !</p>\n<p>I think I would tend toward the second proposition even though it might be a bit of work.</p>\n<p>In particular there is one point that is missing from your comments above (I think):<br>\nThis point is partially true: <code>Autograd engine tests to see if a tensor is zero-filled anywhere it previously tested for undefined tensors, and uses this to skip additions when possible.</code>.<br>\nIndeed, in the case where we actually want to create the graph during backward (for higher order gradients), the fact that a gradient is zero does not mean that it's own gradient is going to be 0, so the operation (even if we know the output is going to be 0) must exist in the created graph. By using a specific type, we can easily have the <code>CPUDoubleTensor * ZeroFilledType</code> be an autograd op that returns another <code>ZeroFilledType</code> and pass it to the rest of the gradient computation.</p>\n<p>I got a bit lost on how the dispatch for autograd ops works, but if it is possible to create a <code>conv</code> for example that takes <code>ZeroFilledType</code> as input as well, we can greatly simplify the implementation of backward functions. There would be no need to check which input is zero or not and some ops will automatically become no-ops when provided with <code>ZeroFilledType</code> (while creating the correct backward of backward graph).</p>\n<p>Another (minor) advantage would be some slight performance increase for people using <code>torch.zeros()</code> to create buffers that they then fill.</p>", "body_text": "I think that is a very interesting change !\nI think I would tend toward the second proposition even though it might be a bit of work.\nIn particular there is one point that is missing from your comments above (I think):\nThis point is partially true: Autograd engine tests to see if a tensor is zero-filled anywhere it previously tested for undefined tensors, and uses this to skip additions when possible..\nIndeed, in the case where we actually want to create the graph during backward (for higher order gradients), the fact that a gradient is zero does not mean that it's own gradient is going to be 0, so the operation (even if we know the output is going to be 0) must exist in the created graph. By using a specific type, we can easily have the CPUDoubleTensor * ZeroFilledType be an autograd op that returns another ZeroFilledType and pass it to the rest of the gradient computation.\nI got a bit lost on how the dispatch for autograd ops works, but if it is possible to create a conv for example that takes ZeroFilledType as input as well, we can greatly simplify the implementation of backward functions. There would be no need to check which input is zero or not and some ops will automatically become no-ops when provided with ZeroFilledType (while creating the correct backward of backward graph).\nAnother (minor) advantage would be some slight performance increase for people using torch.zeros() to create buffers that they then fill.", "body": "I think that is a very interesting change !\r\n\r\nI think I would tend toward the second proposition even though it might be a bit of work.\r\n\r\nIn particular there is one point that is missing from your comments above (I think):\r\nThis point is partially true: `Autograd engine tests to see if a tensor is zero-filled anywhere it previously tested for undefined tensors, and uses this to skip additions when possible.`.\r\nIndeed, in the case where we actually want to create the graph during backward (for higher order gradients), the fact that a gradient is zero does not mean that it's own gradient is going to be 0, so the operation (even if we know the output is going to be 0) must exist in the created graph. By using a specific type, we can easily have the `CPUDoubleTensor * ZeroFilledType` be an autograd op that returns another `ZeroFilledType` and pass it to the rest of the gradient computation.\r\n\r\nI got a bit lost on how the dispatch for autograd ops works, but if it is possible to create a `conv` for example that takes `ZeroFilledType` as input as well, we can greatly simplify the implementation of backward functions. There would be no need to check which input is zero or not and some ops will automatically become no-ops when provided with `ZeroFilledType` (while creating the correct backward of backward graph).\r\n\r\nAnother (minor) advantage would be some slight performance increase for people using `torch.zeros()` to create buffers that they then fill."}