{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/387620068", "html_url": "https://github.com/pytorch/pytorch/pull/6625#issuecomment-387620068", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6625", "id": 387620068, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzYyMDA2OA==", "user": {"login": "Jorghi12", "id": 8586039, "node_id": "MDQ6VXNlcjg1ODYwMzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/8586039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jorghi12", "html_url": "https://github.com/Jorghi12", "followers_url": "https://api.github.com/users/Jorghi12/followers", "following_url": "https://api.github.com/users/Jorghi12/following{/other_user}", "gists_url": "https://api.github.com/users/Jorghi12/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jorghi12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jorghi12/subscriptions", "organizations_url": "https://api.github.com/users/Jorghi12/orgs", "repos_url": "https://api.github.com/users/Jorghi12/repos", "events_url": "https://api.github.com/users/Jorghi12/events{/privacy}", "received_events_url": "https://api.github.com/users/Jorghi12/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-09T04:57:24Z", "updated_at": "2018-05-15T19:53:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Updates:<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I removed the hip files completely and left THCTensorRandom.cu in a minimal patch form. (Static casts are added due to the issue with the implementation of hipKernelLaunch.)</p>\n<p>Top level CMake Changes:</p>\n<ul>\n<li>The LoadHIP.cmake script has been added to cmake/public since it'll be useful for Caffe2 build as well.</li>\n<li>The command torch_cuda_based_add_executable was added inside of the cmake/public/utils.cmake. Similarly done for torch_cuda_based_add_library.</li>\n</ul>\n<p>Once ATen is separated into ATen_cpu and ATen_gpu, then I'll be able to build GPU code using the HCC compiler and continue on using the default compiler for CPU code. Thanks for the great work on that front, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> !!!</p>\n<p>Remaining patches:</p>\n<ul>\n<li>\n<p><strong>HCC has difficulty generating machine code</strong> (a_aten_src_THC_THCTensorMathReduce.cuh.patch)<br>\nPreviously when building this section of code using HCC, we'd get an \"LLVM error: Cannot Select\" which is an indication that the instruction selection cannot find a way to convert this into machine instructions.</p>\n</li>\n<li>\n<p><strong>HCC has trouble with kernel launches</strong>  (a_aten_src_THC_generic_THCTensorRandom.cu.patch)<br>\nAt compile time there needs to be a way for the compiler running in host compilation path to \"understand\" what kernels are actually produced in kernel compilation path so that it can adjust its overloading logic. The fundamental issue with ROCm stack is that the host compilation path is actually executed prior to kernel compilation path. So in other words, this information isn't available ahead of time. Looking at hipLaunchKernelGGL, it appears to be taking the kernel arguments prepared on the host side into a vector, (uses ELFIO lib to locate the kernel), then passes the vector as the kernel argument. Due to the above issue, i.e. host compilation path doesn't take into account kernel compilation path, there isn't any compile-time type checking here. (I got around most of these issues by writing a script to automatically parse kernel definitions during the transpilation stage - covers majority of scenarios). I sent an issue to AMD for fixing the compiler driver. Though it appears it's not on their road map and not going to be any time soon.</p>\n</li>\n<li>\n<p><strong>HIP doesn't support shfl ops for type double</strong> (a_aten_src_THC_THCDeviceUtils.cuh.patch)<br>\nLooking at AMDGPU's LLVM code, shfl operations only work with i32 primitive types.<br>\n<a href=\"https://github.com/RadeonOpenCompute/ROCm-Device-Libs/blob/46d82ed0d6d53c5b1f1fb542cb61221ed1c44afb/hc/src/hc_amdgcn.ll#L122\">https://github.com/RadeonOpenCompute/ROCm-Device-Libs/blob/46d82ed0d6d53c5b1f1fb542cb61221ed1c44afb/hc/src/hc_amdgcn.ll#L122</a><br>\nThe current workaround is to cast from double and use the float shfl operation and then return type double.</p>\n</li>\n<li>\n<p><strong>Need to add template keyword</strong> (a_aten_src_THCUNN_FeatureLPPooling.cu.patch)<br>\nChange from RegisterUtils&lt;T, Width&gt;::shiftLeft(in); TO RegisterUtils&lt;T, Width&gt;::<strong>template</strong> shiftLeft(in);</p>\n</li>\n<li>\n<p><strong>Don't load cudart on HIP</strong>  (a_torch_cuda___init__.py.patch)<br>\nThis patch just prevents the loading of cuda run time when using PyTorch AMD. Temporary.</p>\n</li>\n</ul>", "body_text": "Updates:\n@apaszke I removed the hip files completely and left THCTensorRandom.cu in a minimal patch form. (Static casts are added due to the issue with the implementation of hipKernelLaunch.)\nTop level CMake Changes:\n\nThe LoadHIP.cmake script has been added to cmake/public since it'll be useful for Caffe2 build as well.\nThe command torch_cuda_based_add_executable was added inside of the cmake/public/utils.cmake. Similarly done for torch_cuda_based_add_library.\n\nOnce ATen is separated into ATen_cpu and ATen_gpu, then I'll be able to build GPU code using the HCC compiler and continue on using the default compiler for CPU code. Thanks for the great work on that front, @ezyang !!!\nRemaining patches:\n\n\nHCC has difficulty generating machine code (a_aten_src_THC_THCTensorMathReduce.cuh.patch)\nPreviously when building this section of code using HCC, we'd get an \"LLVM error: Cannot Select\" which is an indication that the instruction selection cannot find a way to convert this into machine instructions.\n\n\nHCC has trouble with kernel launches  (a_aten_src_THC_generic_THCTensorRandom.cu.patch)\nAt compile time there needs to be a way for the compiler running in host compilation path to \"understand\" what kernels are actually produced in kernel compilation path so that it can adjust its overloading logic. The fundamental issue with ROCm stack is that the host compilation path is actually executed prior to kernel compilation path. So in other words, this information isn't available ahead of time. Looking at hipLaunchKernelGGL, it appears to be taking the kernel arguments prepared on the host side into a vector, (uses ELFIO lib to locate the kernel), then passes the vector as the kernel argument. Due to the above issue, i.e. host compilation path doesn't take into account kernel compilation path, there isn't any compile-time type checking here. (I got around most of these issues by writing a script to automatically parse kernel definitions during the transpilation stage - covers majority of scenarios). I sent an issue to AMD for fixing the compiler driver. Though it appears it's not on their road map and not going to be any time soon.\n\n\nHIP doesn't support shfl ops for type double (a_aten_src_THC_THCDeviceUtils.cuh.patch)\nLooking at AMDGPU's LLVM code, shfl operations only work with i32 primitive types.\nhttps://github.com/RadeonOpenCompute/ROCm-Device-Libs/blob/46d82ed0d6d53c5b1f1fb542cb61221ed1c44afb/hc/src/hc_amdgcn.ll#L122\nThe current workaround is to cast from double and use the float shfl operation and then return type double.\n\n\nNeed to add template keyword (a_aten_src_THCUNN_FeatureLPPooling.cu.patch)\nChange from RegisterUtils<T, Width>::shiftLeft(in); TO RegisterUtils<T, Width>::template shiftLeft(in);\n\n\nDon't load cudart on HIP  (a_torch_cuda___init__.py.patch)\nThis patch just prevents the loading of cuda run time when using PyTorch AMD. Temporary.", "body": "Updates:\r\n@apaszke I removed the hip files completely and left THCTensorRandom.cu in a minimal patch form. (Static casts are added due to the issue with the implementation of hipKernelLaunch.)\r\n\r\nTop level CMake Changes:\r\n- The LoadHIP.cmake script has been added to cmake/public since it'll be useful for Caffe2 build as well.\r\n- The command torch_cuda_based_add_executable was added inside of the cmake/public/utils.cmake. Similarly done for torch_cuda_based_add_library.\r\n\r\nOnce ATen is separated into ATen_cpu and ATen_gpu, then I'll be able to build GPU code using the HCC compiler and continue on using the default compiler for CPU code. Thanks for the great work on that front, @ezyang !!!\r\n\r\nRemaining patches:\r\n- **HCC has difficulty generating machine code** (a_aten_src_THC_THCTensorMathReduce.cuh.patch) \r\nPreviously when building this section of code using HCC, we'd get an \"LLVM error: Cannot Select\" which is an indication that the instruction selection cannot find a way to convert this into machine instructions.\r\n\r\n- **HCC has trouble with kernel launches**  (a_aten_src_THC_generic_THCTensorRandom.cu.patch) \r\nAt compile time there needs to be a way for the compiler running in host compilation path to \"understand\" what kernels are actually produced in kernel compilation path so that it can adjust its overloading logic. The fundamental issue with ROCm stack is that the host compilation path is actually executed prior to kernel compilation path. So in other words, this information isn't available ahead of time. Looking at hipLaunchKernelGGL, it appears to be taking the kernel arguments prepared on the host side into a vector, (uses ELFIO lib to locate the kernel), then passes the vector as the kernel argument. Due to the above issue, i.e. host compilation path doesn't take into account kernel compilation path, there isn't any compile-time type checking here. (I got around most of these issues by writing a script to automatically parse kernel definitions during the transpilation stage - covers majority of scenarios). I sent an issue to AMD for fixing the compiler driver. Though it appears it's not on their road map and not going to be any time soon.\r\n\r\n- **HIP doesn't support shfl ops for type double** (a_aten_src_THC_THCDeviceUtils.cuh.patch)\r\nLooking at AMDGPU's LLVM code, shfl operations only work with i32 primitive types. \r\nhttps://github.com/RadeonOpenCompute/ROCm-Device-Libs/blob/46d82ed0d6d53c5b1f1fb542cb61221ed1c44afb/hc/src/hc_amdgcn.ll#L122\r\nThe current workaround is to cast from double and use the float shfl operation and then return type double.\r\n\r\n- **Need to add template keyword** (a_aten_src_THCUNN_FeatureLPPooling.cu.patch)\r\nChange from RegisterUtils<T, Width>::shiftLeft<Stride>(in); TO RegisterUtils<T, Width>::**template** shiftLeft<Stride>(in);\r\n \r\n- **Don't load cudart on HIP**  (a_torch_cuda___init__.py.patch) \r\nThis patch just prevents the loading of cuda run time when using PyTorch AMD. Temporary."}