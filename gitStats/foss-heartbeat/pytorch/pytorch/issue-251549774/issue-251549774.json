{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2496", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2496/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2496/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2496/events", "html_url": "https://github.com/pytorch/pytorch/issues/2496", "id": 251549774, "node_id": "MDU6SXNzdWUyNTE1NDk3NzQ=", "number": 2496, "title": "PyTorch 0.2.0_1 Freezes at nn.Conv2d() ", "user": {"login": "Jiankai-Sun", "id": 25477396, "node_id": "MDQ6VXNlcjI1NDc3Mzk2", "avatar_url": "https://avatars2.githubusercontent.com/u/25477396?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jiankai-Sun", "html_url": "https://github.com/Jiankai-Sun", "followers_url": "https://api.github.com/users/Jiankai-Sun/followers", "following_url": "https://api.github.com/users/Jiankai-Sun/following{/other_user}", "gists_url": "https://api.github.com/users/Jiankai-Sun/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jiankai-Sun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jiankai-Sun/subscriptions", "organizations_url": "https://api.github.com/users/Jiankai-Sun/orgs", "repos_url": "https://api.github.com/users/Jiankai-Sun/repos", "events_url": "https://api.github.com/users/Jiankai-Sun/events{/privacy}", "received_events_url": "https://api.github.com/users/Jiankai-Sun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2017-08-21T04:55:27Z", "updated_at": "2018-08-01T23:50:59Z", "closed_at": "2018-08-01T23:50:59Z", "author_association": "NONE", "body_html": "<p>System:<br>\nUbuntu 16.04 64-bit<br>\nPython 2.7<br>\npytorch '0.2.0_1</p>\n<p>When I run the code which works correctly with <code>pytorch 0.1.12</code>, the code freezes at <code>x = F.relu(self.conv1(x))</code>(To be more specific, it freezes at <code>self.conv1(x)</code> ) in function <code>def forward(self, x, hidden)</code>, <code>class A3CLSTMNet(nn.Module)</code>. No error  information reports. Just no response. I can stop the program using Ctrl+C.</p>\n<p>Some code are show below. If you need any more details, feel free to tell me.</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        test_p <span class=\"pl-k\">=</span> mp.Process(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.test)\n        <span class=\"pl-c1\">self</span>.jobs.append(test_p)\n        <span class=\"pl-c1\">self</span>.args.train_step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>  \n        <span class=\"pl-k\">for</span> job <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.jobs:\n            job.start()\n\n        <span class=\"pl-k\">for</span> job <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.jobs:\n            job.join()\n\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">render_</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        test_env <span class=\"pl-k\">=</span> AtariEnv(gym.make(<span class=\"pl-c1\">self</span>.args.game),<span class=\"pl-c1\">self</span>.args.frame_seq,<span class=\"pl-c1\">self</span>.args.frame_skip,<span class=\"pl-v\">render</span> <span class=\"pl-k\">=</span> render_)\n        test_model <span class=\"pl-k\">=</span> A3CLSTMNet(<span class=\"pl-c1\">self</span>.env.state_shape, <span class=\"pl-c1\">self</span>.env.action_dim)\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            terminal <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n            reward_ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n            lstm_h <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">256</span>), <span class=\"pl-v\">volatile</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n            lstm_c <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">256</span>), <span class=\"pl-v\">volatile</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n            test_env.reset_env()\n            <span class=\"pl-k\">if</span> (<span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">self</span>.main_update_step.value)) <span class=\"pl-k\">%</span> <span class=\"pl-c1\">500</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                <span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>step: <span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">self</span>.main_update_step.value))\n                episode_length <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n                <span class=\"pl-c1\">self</span>.save_model(<span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">self</span>.main_update_step.value))\n                test_model.load_state_dict(<span class=\"pl-c1\">self</span>.shared_model.state_dict())\n                <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> terminal:\n                    state_tensor <span class=\"pl-k\">=</span> Variable(\n                            torch.from_numpy(test_env.state).float())\n              \n                    pl, v, (lstm_h,lstm_c) <span class=\"pl-k\">=</span> test_model(state_tensor, (lstm_h, lstm_c))\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span>print pl.data.numpy()[0]</span>\n                    action <span class=\"pl-k\">=</span> pl.max(<span class=\"pl-c1\">1</span>)[<span class=\"pl-c1\">1</span>].data.numpy()[<span class=\"pl-c1\">0</span>]\n                    _, reward, terminal <span class=\"pl-k\">=</span> test_env.forward_action(action)\n                    reward_ <span class=\"pl-k\">+=</span> reward\n                    episode_length <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span>img_ = (test_env.state.copy().reshape(42,42)*256)</span>\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span>img_ = cv2.resize(img_, (160,160))</span>\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span>img_ = np.stack((img_,)*3)</span>\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span>self.test_win = self.vis.image(img_, </span>\n                            <span class=\"pl-c\"><span class=\"pl-c\">#</span>win = self.test_win)</span>\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Reward: <span class=\"pl-pds\">\"</span></span>, reward_)\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>episode_length<span class=\"pl-pds\">\"</span></span>, episode_length)</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">A3CLSTMNet</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">state_shape</span>, <span class=\"pl-smi\">action_dim</span>):\n        <span class=\"pl-c1\">super</span>(A3CLSTMNet, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.state_shape <span class=\"pl-k\">=</span> state_shape \n        <span class=\"pl-c1\">self</span>.action_dim <span class=\"pl-k\">=</span> action_dim\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">self</span>.state_shape[<span class=\"pl-c1\">0</span>],<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.conv3 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.conv4 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.lstm <span class=\"pl-k\">=</span> nn.LSTMCell(<span class=\"pl-c1\">3</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">3</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">256</span>,<span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> hang policy output</span>\n        <span class=\"pl-c1\">self</span>.linear_policy_1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">256</span>,<span class=\"pl-c1\">self</span>.action_dim)\n        <span class=\"pl-c1\">self</span>.softmax_policy <span class=\"pl-k\">=</span> nn.Softmax()\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> hang value output</span>\n        <span class=\"pl-c1\">self</span>.linear_value_1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">256</span>,<span class=\"pl-c1\">1</span>)\n        \n        <span class=\"pl-c1\">self</span>.apply(weights_init)\n        <span class=\"pl-c1\">self</span>.linear_policy_1.weight.data <span class=\"pl-k\">=</span> normalized_columns_initializer(\n            <span class=\"pl-c1\">self</span>.linear_policy_1.weight.data, <span class=\"pl-c1\">0.01</span>)\n        <span class=\"pl-c1\">self</span>.linear_policy_1.bias.data.fill_(<span class=\"pl-c1\">0</span>)\n        <span class=\"pl-c1\">self</span>.linear_value_1.weight.data <span class=\"pl-k\">=</span> normalized_columns_initializer(\n            <span class=\"pl-c1\">self</span>.linear_value_1.weight.data, <span class=\"pl-c1\">1.0</span>)\n        <span class=\"pl-c1\">self</span>.linear_value_1.bias.data.fill_(<span class=\"pl-c1\">0</span>)\n\n        <span class=\"pl-c1\">self</span>.lstm.bias_ih.data.fill_(<span class=\"pl-c1\">0</span>)\n        <span class=\"pl-c1\">self</span>.lstm.bias_hh.data.fill_(<span class=\"pl-c1\">0</span>)\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">hidden</span>):\n\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.state_shape[<span class=\"pl-c1\">0</span>],\n                <span class=\"pl-c1\">self</span>.state_shape[<span class=\"pl-c1\">1</span>],<span class=\"pl-c1\">self</span>.state_shape[<span class=\"pl-c1\">2</span>])\n        <span class=\"pl-c1\">print</span>(x)\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv1(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv2(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv3(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv4(x))\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">3</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">32</span>) \n        x,c <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lstm(x, (hidden[<span class=\"pl-c1\">0</span>],hidden[<span class=\"pl-c1\">1</span>]))\n        pl <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear_policy_1(x)\n        pl <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.softmax_policy(pl)\n        v <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear_value_1(x)\n        <span class=\"pl-k\">return</span> pl,v,(x,c)</pre></div>\n<p>I get the contents of variable <code>x</code> by <code>print(x)</code>. Since I use <code>torch.multiprocessing</code>, there are more than one <code>x</code> below:</p>\n<pre><code>Variable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\n[2017-08-21 12:33:00,077] Making new env: PongDeterministic-v4\n('step: ', 0)\nafter test_p\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n</code></pre>\n<p>After debugging, I find it freezes at <code>self.conv1(x)</code>. I also make a test in python console (PyTorch version 0.2.0_1, same with the above code running environment), the following code works correctly without freezing.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n\nconv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\nstate <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">42</span>, <span class=\"pl-c1\">42</span>)\nstate_tensor<span class=\"pl-k\">=</span>Variable(torch.from_numpy(state).float())\nstate1 <span class=\"pl-k\">=</span> state_tensor.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">42</span>, <span class=\"pl-c1\">42</span>)\n<span class=\"pl-c1\">print</span>(state1)\ntmp <span class=\"pl-k\">=</span> conv1(state1)\n<span class=\"pl-c1\">print</span>(tmp)</pre></div>\n<p>The outpus is</p>\n<pre><code>Variable containing:\n(0 ,0 ,.,.) = \n -1.0531 -0.1796  0.3587  ...   0.2259  0.3968 -1.3910\n  0.0166  2.0252  0.7346  ...   0.3473  0.0885  0.6998\n  0.7673 -0.8903  0.6876  ...  -2.2238 -0.3563 -0.4267\n           ...             \u22f1             ...          \n  0.6304  0.6472 -0.4925  ...   0.0982 -0.3930 -0.8110\n -0.6198  0.1078 -1.3479  ...   0.7241 -0.0109  1.6495\n -0.4501  0.8786  0.0325  ...  -1.5324 -0.7801 -0.8719\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  4.9092e-01  3.7111e-01  8.6216e-01  ...  -3.4242e-01  2.9049e-02  8.0934e-01\n  3.4515e-02 -4.2537e-01  1.7428e-01  ...   1.1093e-01 -1.0738e+00  6.0881e-01\n  3.1217e-01 -8.8080e-03 -5.6441e-02  ...   5.4400e-01 -1.0658e-02  6.8474e-01\n                 ...                   \u22f1                   ...                \n  7.0461e-01  9.7259e-02 -1.1624e+00  ...  -4.2550e-01  1.1152e+00  8.4051e-01\n  7.4105e-01  2.6353e-01 -1.4116e-01  ...   2.2952e-01 -3.0020e-01  4.3274e-01\n  9.1155e-01  4.7228e-01 -4.2916e-01  ...   5.2318e-01  3.3947e-01  4.5746e-02\n\n(0 ,1 ,.,.) = \n -2.3114e-01 -6.7370e-01 -1.1178e+00  ...  -1.4957e+00 -2.5461e-01 -8.0390e-01\n  5.7831e-01 -3.8049e-01 -2.2143e-01  ...   1.4796e+00 -1.4711e+00  1.9418e-01\n -1.2247e+00 -1.0017e+00  4.5575e-02  ...  -4.6029e-01  1.4457e-01 -8.9094e-01\n                 ...                   \u22f1                   ...                \n  2.4875e-01  1.1358e+00 -2.1782e-03  ...  -4.8295e-01  7.9160e-01 -1.2784e+00\n -6.3711e-01  1.7278e+00  9.5453e-02  ...  -5.7657e-01 -9.4344e-01  3.3424e-01\n -1.1142e+00 -9.6311e-01 -4.1400e-01  ...  -8.5177e-02 -1.7373e-01 -3.7225e-01\n\n(0 ,2 ,.,.) = \n  3.7729e-02 -4.4649e-01  1.7628e-01  ...  -2.0471e-01  1.1540e-01  9.1261e-01\n  3.3155e-02 -1.2039e-02  6.7443e-01  ...  -7.5356e-01  1.7888e-01  1.0874e+00\n  8.5223e-02  3.9133e-01  3.7567e-02  ...  -7.7139e-01 -4.8861e-01  3.9544e-01\n                 ...                   \u22f1                   ...                \n  5.6189e-01 -1.8068e-01 -3.5824e-01  ...  -2.2752e-01 -6.7575e-02 -7.5511e-01\n  2.4132e-01 -2.8088e-01 -1.0904e+00  ...  -3.3897e-01 -1.7998e-01  4.9224e-02\n  1.6513e-01  5.3980e-02 -1.3517e-01  ...   1.8532e-01 -7.9412e-03 -3.4100e-01\n   ...\n\n(0 ,29,.,.) = \n  1.0052e+00 -1.1017e-01 -1.2992e-01  ...   4.7833e-01 -3.1562e-01  2.4085e-01\n -6.0557e-02  8.5560e-01  6.3823e-01  ...   1.7778e+00  9.0973e-01 -4.5352e-01\n -1.2317e-01  1.1153e+00  1.2399e+00  ...  -1.0785e-01  3.8565e-01  4.1936e-02\n                 ...                   \u22f1                   ...                \n -1.3574e+00  1.1036e+00  2.1086e+00  ...   1.4253e+00 -1.0863e+00  8.4242e-01\n -7.7115e-01 -2.7149e-01  4.6326e-01  ...   1.0213e+00  8.8335e-01 -1.7714e-01\n -8.9065e-01  1.6536e-01  5.7247e-01  ...  -7.0598e-01 -3.1186e-01  6.3970e-02\n\n(0 ,30,.,.) = \n  1.3632e-03 -5.4865e-01  2.2106e-01  ...   3.6347e-02  5.3167e-01  6.2587e-01\n -4.6164e-01 -5.2589e-01 -5.7386e-01  ...  -2.0609e-01 -1.2341e+00  4.0797e-01\n -3.8703e-01  7.1140e-01  2.7122e-01  ...   3.5770e-01 -4.8863e-03 -2.6595e-01\n                 ...                   \u22f1                   ...                \n -5.9267e-01  9.7627e-02 -7.3991e-01  ...   2.4554e-01 -1.7257e-01  2.1498e-01\n -5.8195e-02 -9.4493e-02 -5.5493e-01  ...  -4.2769e-01 -9.1936e-01 -6.1112e-01\n -4.7519e-01  3.4992e-01 -1.6242e-01  ...  -6.2562e-01 -1.7873e-01 -2.6058e-01\n\n(0 ,31,.,.) = \n -3.5324e-01  6.1532e-01 -3.3209e-01  ...   7.6112e-01 -1.8485e-01 -8.4187e-01\n  2.5945e-01  6.1765e-01  2.1825e-01  ...  -3.5352e-01  1.8842e+00 -6.1050e-01\n  9.1983e-02 -2.4436e-01 -2.2283e-01  ...  -1.0150e-01  6.2999e-01 -6.0950e-01\n                 ...                   \u22f1                   ...                \n -1.1329e-01 -4.7121e-01  1.3504e+00  ...   2.5476e-01 -5.5622e-01 -2.3621e-01\n -9.8706e-01 -6.2796e-01  1.5716e+00  ...   5.0247e-01  7.7165e-01 -5.9040e-02\n -1.7598e-01 -3.7907e-01  8.6386e-01  ...   1.6770e-01 -1.6873e-01  1.7422e-01\n[torch.FloatTensor of size 1x32x20x20]\n</code></pre>\n<p>I wonder why the original code works correctly using <code>pytorch '0.1.12</code> but incorrectly in <code>pytorch '0.2.0_1</code>? Do you have any suggestions? Thank you?</p>", "body_text": "System:\nUbuntu 16.04 64-bit\nPython 2.7\npytorch '0.2.0_1\nWhen I run the code which works correctly with pytorch 0.1.12, the code freezes at x = F.relu(self.conv1(x))(To be more specific, it freezes at self.conv1(x) ) in function def forward(self, x, hidden), class A3CLSTMNet(nn.Module). No error  information reports. Just no response. I can stop the program using Ctrl+C.\nSome code are show below. If you need any more details, feel free to tell me.\n    def train(self):\n        test_p = mp.Process(target=self.test)\n        self.jobs.append(test_p)\n        self.args.train_step = 0  \n        for job in self.jobs:\n            job.start()\n\n        for job in self.jobs:\n            job.join()\n\n\n    def test(self, render_=False):\n        test_env = AtariEnv(gym.make(self.args.game),self.args.frame_seq,self.args.frame_skip,render = render_)\n        test_model = A3CLSTMNet(self.env.state_shape, self.env.action_dim)\n        while True:\n            terminal = False\n            reward_ = 0\n            lstm_h = Variable(torch.zeros(1,256), volatile=True)\n            lstm_c = Variable(torch.zeros(1,256), volatile=True)\n            test_env.reset_env()\n            if (int(self.main_update_step.value)) % 500 == 0:\n                print (\"step: \", int(self.main_update_step.value))\n                episode_length = 0\n                self.save_model(int(self.main_update_step.value))\n                test_model.load_state_dict(self.shared_model.state_dict())\n                while not terminal:\n                    state_tensor = Variable(\n                            torch.from_numpy(test_env.state).float())\n              \n                    pl, v, (lstm_h,lstm_c) = test_model(state_tensor, (lstm_h, lstm_c))\n                    #print pl.data.numpy()[0]\n                    action = pl.max(1)[1].data.numpy()[0]\n                    _, reward, terminal = test_env.forward_action(action)\n                    reward_ += reward\n                    episode_length += 1\n                    #img_ = (test_env.state.copy().reshape(42,42)*256)\n                    #img_ = cv2.resize(img_, (160,160))\n                    #img_ = np.stack((img_,)*3)\n                    #self.test_win = self.vis.image(img_, \n                            #win = self.test_win)\n                print(\"Reward: \", reward_)\n                print(\"episode_length\", episode_length)\nclass A3CLSTMNet(nn.Module):\n\n    def __init__(self, state_shape, action_dim):\n        super(A3CLSTMNet, self).__init__()\n        self.state_shape = state_shape \n        self.action_dim = action_dim\n        self.conv1 = nn.Conv2d(self.state_shape[0],32,3,stride=2)\n        self.conv2 = nn.Conv2d(32,32,3,stride=2, padding = 1)\n        self.conv3 = nn.Conv2d(32,32,3,stride=2, padding = 1)\n        self.conv4 = nn.Conv2d(32,32,3,stride=2, padding = 1)\n        self.lstm = nn.LSTMCell(3*3*32,256,1)\n        # hang policy output\n        self.linear_policy_1 = nn.Linear(256,self.action_dim)\n        self.softmax_policy = nn.Softmax()\n        # hang value output\n        self.linear_value_1 = nn.Linear(256,1)\n        \n        self.apply(weights_init)\n        self.linear_policy_1.weight.data = normalized_columns_initializer(\n            self.linear_policy_1.weight.data, 0.01)\n        self.linear_policy_1.bias.data.fill_(0)\n        self.linear_value_1.weight.data = normalized_columns_initializer(\n            self.linear_value_1.weight.data, 1.0)\n        self.linear_value_1.bias.data.fill_(0)\n\n        self.lstm.bias_ih.data.fill_(0)\n        self.lstm.bias_hh.data.fill_(0)\n    \n    def forward(self, x, hidden):\n\n        x = x.view(-1, self.state_shape[0],\n                self.state_shape[1],self.state_shape[2])\n        print(x)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = x.view(-1, 3*3*32) \n        x,c = self.lstm(x, (hidden[0],hidden[1]))\n        pl = self.linear_policy_1(x)\n        pl = self.softmax_policy(pl)\n        v = self.linear_value_1(x)\n        return pl,v,(x,c)\nI get the contents of variable x by print(x). Since I use torch.multiprocessing, there are more than one x below:\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\n[2017-08-21 12:33:00,077] Making new env: PongDeterministic-v4\n('step: ', 0)\nafter test_p\nVariable containing:\n(0 ,0 ,.,.) = \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n           ...             \u22f1             ...          \n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\n[torch.FloatTensor of size 1x1x42x42]\n\nAfter debugging, I find it freezes at self.conv1(x). I also make a test in python console (PyTorch version 0.2.0_1, same with the above code running environment), the following code works correctly without freezing.\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.autograd import Variable\n\n\nconv1 = nn.Conv2d(1,32,3,stride=2)\nstate = np.random.randn(1, 42, 42)\nstate_tensor=Variable(torch.from_numpy(state).float())\nstate1 = state_tensor.view(-1, 1, 42, 42)\nprint(state1)\ntmp = conv1(state1)\nprint(tmp)\nThe outpus is\nVariable containing:\n(0 ,0 ,.,.) = \n -1.0531 -0.1796  0.3587  ...   0.2259  0.3968 -1.3910\n  0.0166  2.0252  0.7346  ...   0.3473  0.0885  0.6998\n  0.7673 -0.8903  0.6876  ...  -2.2238 -0.3563 -0.4267\n           ...             \u22f1             ...          \n  0.6304  0.6472 -0.4925  ...   0.0982 -0.3930 -0.8110\n -0.6198  0.1078 -1.3479  ...   0.7241 -0.0109  1.6495\n -0.4501  0.8786  0.0325  ...  -1.5324 -0.7801 -0.8719\n[torch.FloatTensor of size 1x1x42x42]\n\nVariable containing:\n(0 ,0 ,.,.) = \n  4.9092e-01  3.7111e-01  8.6216e-01  ...  -3.4242e-01  2.9049e-02  8.0934e-01\n  3.4515e-02 -4.2537e-01  1.7428e-01  ...   1.1093e-01 -1.0738e+00  6.0881e-01\n  3.1217e-01 -8.8080e-03 -5.6441e-02  ...   5.4400e-01 -1.0658e-02  6.8474e-01\n                 ...                   \u22f1                   ...                \n  7.0461e-01  9.7259e-02 -1.1624e+00  ...  -4.2550e-01  1.1152e+00  8.4051e-01\n  7.4105e-01  2.6353e-01 -1.4116e-01  ...   2.2952e-01 -3.0020e-01  4.3274e-01\n  9.1155e-01  4.7228e-01 -4.2916e-01  ...   5.2318e-01  3.3947e-01  4.5746e-02\n\n(0 ,1 ,.,.) = \n -2.3114e-01 -6.7370e-01 -1.1178e+00  ...  -1.4957e+00 -2.5461e-01 -8.0390e-01\n  5.7831e-01 -3.8049e-01 -2.2143e-01  ...   1.4796e+00 -1.4711e+00  1.9418e-01\n -1.2247e+00 -1.0017e+00  4.5575e-02  ...  -4.6029e-01  1.4457e-01 -8.9094e-01\n                 ...                   \u22f1                   ...                \n  2.4875e-01  1.1358e+00 -2.1782e-03  ...  -4.8295e-01  7.9160e-01 -1.2784e+00\n -6.3711e-01  1.7278e+00  9.5453e-02  ...  -5.7657e-01 -9.4344e-01  3.3424e-01\n -1.1142e+00 -9.6311e-01 -4.1400e-01  ...  -8.5177e-02 -1.7373e-01 -3.7225e-01\n\n(0 ,2 ,.,.) = \n  3.7729e-02 -4.4649e-01  1.7628e-01  ...  -2.0471e-01  1.1540e-01  9.1261e-01\n  3.3155e-02 -1.2039e-02  6.7443e-01  ...  -7.5356e-01  1.7888e-01  1.0874e+00\n  8.5223e-02  3.9133e-01  3.7567e-02  ...  -7.7139e-01 -4.8861e-01  3.9544e-01\n                 ...                   \u22f1                   ...                \n  5.6189e-01 -1.8068e-01 -3.5824e-01  ...  -2.2752e-01 -6.7575e-02 -7.5511e-01\n  2.4132e-01 -2.8088e-01 -1.0904e+00  ...  -3.3897e-01 -1.7998e-01  4.9224e-02\n  1.6513e-01  5.3980e-02 -1.3517e-01  ...   1.8532e-01 -7.9412e-03 -3.4100e-01\n   ...\n\n(0 ,29,.,.) = \n  1.0052e+00 -1.1017e-01 -1.2992e-01  ...   4.7833e-01 -3.1562e-01  2.4085e-01\n -6.0557e-02  8.5560e-01  6.3823e-01  ...   1.7778e+00  9.0973e-01 -4.5352e-01\n -1.2317e-01  1.1153e+00  1.2399e+00  ...  -1.0785e-01  3.8565e-01  4.1936e-02\n                 ...                   \u22f1                   ...                \n -1.3574e+00  1.1036e+00  2.1086e+00  ...   1.4253e+00 -1.0863e+00  8.4242e-01\n -7.7115e-01 -2.7149e-01  4.6326e-01  ...   1.0213e+00  8.8335e-01 -1.7714e-01\n -8.9065e-01  1.6536e-01  5.7247e-01  ...  -7.0598e-01 -3.1186e-01  6.3970e-02\n\n(0 ,30,.,.) = \n  1.3632e-03 -5.4865e-01  2.2106e-01  ...   3.6347e-02  5.3167e-01  6.2587e-01\n -4.6164e-01 -5.2589e-01 -5.7386e-01  ...  -2.0609e-01 -1.2341e+00  4.0797e-01\n -3.8703e-01  7.1140e-01  2.7122e-01  ...   3.5770e-01 -4.8863e-03 -2.6595e-01\n                 ...                   \u22f1                   ...                \n -5.9267e-01  9.7627e-02 -7.3991e-01  ...   2.4554e-01 -1.7257e-01  2.1498e-01\n -5.8195e-02 -9.4493e-02 -5.5493e-01  ...  -4.2769e-01 -9.1936e-01 -6.1112e-01\n -4.7519e-01  3.4992e-01 -1.6242e-01  ...  -6.2562e-01 -1.7873e-01 -2.6058e-01\n\n(0 ,31,.,.) = \n -3.5324e-01  6.1532e-01 -3.3209e-01  ...   7.6112e-01 -1.8485e-01 -8.4187e-01\n  2.5945e-01  6.1765e-01  2.1825e-01  ...  -3.5352e-01  1.8842e+00 -6.1050e-01\n  9.1983e-02 -2.4436e-01 -2.2283e-01  ...  -1.0150e-01  6.2999e-01 -6.0950e-01\n                 ...                   \u22f1                   ...                \n -1.1329e-01 -4.7121e-01  1.3504e+00  ...   2.5476e-01 -5.5622e-01 -2.3621e-01\n -9.8706e-01 -6.2796e-01  1.5716e+00  ...   5.0247e-01  7.7165e-01 -5.9040e-02\n -1.7598e-01 -3.7907e-01  8.6386e-01  ...   1.6770e-01 -1.6873e-01  1.7422e-01\n[torch.FloatTensor of size 1x32x20x20]\n\nI wonder why the original code works correctly using pytorch '0.1.12 but incorrectly in pytorch '0.2.0_1? Do you have any suggestions? Thank you?", "body": "System:\r\nUbuntu 16.04 64-bit\r\nPython 2.7\r\npytorch '0.2.0_1\r\n\r\nWhen I run the code which works correctly with `pytorch 0.1.12`, the code freezes at `x = F.relu(self.conv1(x))`(To be more specific, it freezes at `self.conv1(x)` ) in function `def forward(self, x, hidden)`, `class A3CLSTMNet(nn.Module)`. No error  information reports. Just no response. I can stop the program using Ctrl+C.\r\n\r\nSome code are show below. If you need any more details, feel free to tell me.\r\n```python\r\n    def train(self):\r\n        test_p = mp.Process(target=self.test)\r\n        self.jobs.append(test_p)\r\n        self.args.train_step = 0  \r\n        for job in self.jobs:\r\n            job.start()\r\n\r\n        for job in self.jobs:\r\n            job.join()\r\n\r\n\r\n    def test(self, render_=False):\r\n        test_env = AtariEnv(gym.make(self.args.game),self.args.frame_seq,self.args.frame_skip,render = render_)\r\n        test_model = A3CLSTMNet(self.env.state_shape, self.env.action_dim)\r\n        while True:\r\n            terminal = False\r\n            reward_ = 0\r\n            lstm_h = Variable(torch.zeros(1,256), volatile=True)\r\n            lstm_c = Variable(torch.zeros(1,256), volatile=True)\r\n            test_env.reset_env()\r\n            if (int(self.main_update_step.value)) % 500 == 0:\r\n                print (\"step: \", int(self.main_update_step.value))\r\n                episode_length = 0\r\n                self.save_model(int(self.main_update_step.value))\r\n                test_model.load_state_dict(self.shared_model.state_dict())\r\n                while not terminal:\r\n                    state_tensor = Variable(\r\n                            torch.from_numpy(test_env.state).float())\r\n              \r\n                    pl, v, (lstm_h,lstm_c) = test_model(state_tensor, (lstm_h, lstm_c))\r\n                    #print pl.data.numpy()[0]\r\n                    action = pl.max(1)[1].data.numpy()[0]\r\n                    _, reward, terminal = test_env.forward_action(action)\r\n                    reward_ += reward\r\n                    episode_length += 1\r\n                    #img_ = (test_env.state.copy().reshape(42,42)*256)\r\n                    #img_ = cv2.resize(img_, (160,160))\r\n                    #img_ = np.stack((img_,)*3)\r\n                    #self.test_win = self.vis.image(img_, \r\n                            #win = self.test_win)\r\n                print(\"Reward: \", reward_)\r\n                print(\"episode_length\", episode_length)\r\n```\r\n```python\r\nclass A3CLSTMNet(nn.Module):\r\n\r\n    def __init__(self, state_shape, action_dim):\r\n        super(A3CLSTMNet, self).__init__()\r\n        self.state_shape = state_shape \r\n        self.action_dim = action_dim\r\n        self.conv1 = nn.Conv2d(self.state_shape[0],32,3,stride=2)\r\n        self.conv2 = nn.Conv2d(32,32,3,stride=2, padding = 1)\r\n        self.conv3 = nn.Conv2d(32,32,3,stride=2, padding = 1)\r\n        self.conv4 = nn.Conv2d(32,32,3,stride=2, padding = 1)\r\n        self.lstm = nn.LSTMCell(3*3*32,256,1)\r\n        # hang policy output\r\n        self.linear_policy_1 = nn.Linear(256,self.action_dim)\r\n        self.softmax_policy = nn.Softmax()\r\n        # hang value output\r\n        self.linear_value_1 = nn.Linear(256,1)\r\n        \r\n        self.apply(weights_init)\r\n        self.linear_policy_1.weight.data = normalized_columns_initializer(\r\n            self.linear_policy_1.weight.data, 0.01)\r\n        self.linear_policy_1.bias.data.fill_(0)\r\n        self.linear_value_1.weight.data = normalized_columns_initializer(\r\n            self.linear_value_1.weight.data, 1.0)\r\n        self.linear_value_1.bias.data.fill_(0)\r\n\r\n        self.lstm.bias_ih.data.fill_(0)\r\n        self.lstm.bias_hh.data.fill_(0)\r\n    \r\n    def forward(self, x, hidden):\r\n\r\n        x = x.view(-1, self.state_shape[0],\r\n                self.state_shape[1],self.state_shape[2])\r\n        print(x)\r\n        x = F.relu(self.conv1(x))\r\n        x = F.relu(self.conv2(x))\r\n        x = F.relu(self.conv3(x))\r\n        x = F.relu(self.conv4(x))\r\n        x = x.view(-1, 3*3*32) \r\n        x,c = self.lstm(x, (hidden[0],hidden[1]))\r\n        pl = self.linear_policy_1(x)\r\n        pl = self.softmax_policy(pl)\r\n        v = self.linear_value_1(x)\r\n        return pl,v,(x,c)\r\n```\r\nI get the contents of variable `x` by `print(x)`. Since I use `torch.multiprocessing`, there are more than one `x` below:\r\n```\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\n[2017-08-21 12:33:00,077] Making new env: PongDeterministic-v4\r\n('step: ', 0)\r\nafter test_p\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3451  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n           ...             \u22f1             ...          \r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n  0.3529  0.3529  0.3529  ...   0.3529  0.3529  0.3529\r\n[torch.FloatTensor of size 1x1x42x42]\r\n```\r\nAfter debugging, I find it freezes at `self.conv1(x)`. I also make a test in python console (PyTorch version 0.2.0_1, same with the above code running environment), the following code works correctly without freezing.\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\n\r\n\r\nconv1 = nn.Conv2d(1,32,3,stride=2)\r\nstate = np.random.randn(1, 42, 42)\r\nstate_tensor=Variable(torch.from_numpy(state).float())\r\nstate1 = state_tensor.view(-1, 1, 42, 42)\r\nprint(state1)\r\ntmp = conv1(state1)\r\nprint(tmp)\r\n```\r\nThe outpus is \r\n```\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n -1.0531 -0.1796  0.3587  ...   0.2259  0.3968 -1.3910\r\n  0.0166  2.0252  0.7346  ...   0.3473  0.0885  0.6998\r\n  0.7673 -0.8903  0.6876  ...  -2.2238 -0.3563 -0.4267\r\n           ...             \u22f1             ...          \r\n  0.6304  0.6472 -0.4925  ...   0.0982 -0.3930 -0.8110\r\n -0.6198  0.1078 -1.3479  ...   0.7241 -0.0109  1.6495\r\n -0.4501  0.8786  0.0325  ...  -1.5324 -0.7801 -0.8719\r\n[torch.FloatTensor of size 1x1x42x42]\r\n\r\nVariable containing:\r\n(0 ,0 ,.,.) = \r\n  4.9092e-01  3.7111e-01  8.6216e-01  ...  -3.4242e-01  2.9049e-02  8.0934e-01\r\n  3.4515e-02 -4.2537e-01  1.7428e-01  ...   1.1093e-01 -1.0738e+00  6.0881e-01\r\n  3.1217e-01 -8.8080e-03 -5.6441e-02  ...   5.4400e-01 -1.0658e-02  6.8474e-01\r\n                 ...                   \u22f1                   ...                \r\n  7.0461e-01  9.7259e-02 -1.1624e+00  ...  -4.2550e-01  1.1152e+00  8.4051e-01\r\n  7.4105e-01  2.6353e-01 -1.4116e-01  ...   2.2952e-01 -3.0020e-01  4.3274e-01\r\n  9.1155e-01  4.7228e-01 -4.2916e-01  ...   5.2318e-01  3.3947e-01  4.5746e-02\r\n\r\n(0 ,1 ,.,.) = \r\n -2.3114e-01 -6.7370e-01 -1.1178e+00  ...  -1.4957e+00 -2.5461e-01 -8.0390e-01\r\n  5.7831e-01 -3.8049e-01 -2.2143e-01  ...   1.4796e+00 -1.4711e+00  1.9418e-01\r\n -1.2247e+00 -1.0017e+00  4.5575e-02  ...  -4.6029e-01  1.4457e-01 -8.9094e-01\r\n                 ...                   \u22f1                   ...                \r\n  2.4875e-01  1.1358e+00 -2.1782e-03  ...  -4.8295e-01  7.9160e-01 -1.2784e+00\r\n -6.3711e-01  1.7278e+00  9.5453e-02  ...  -5.7657e-01 -9.4344e-01  3.3424e-01\r\n -1.1142e+00 -9.6311e-01 -4.1400e-01  ...  -8.5177e-02 -1.7373e-01 -3.7225e-01\r\n\r\n(0 ,2 ,.,.) = \r\n  3.7729e-02 -4.4649e-01  1.7628e-01  ...  -2.0471e-01  1.1540e-01  9.1261e-01\r\n  3.3155e-02 -1.2039e-02  6.7443e-01  ...  -7.5356e-01  1.7888e-01  1.0874e+00\r\n  8.5223e-02  3.9133e-01  3.7567e-02  ...  -7.7139e-01 -4.8861e-01  3.9544e-01\r\n                 ...                   \u22f1                   ...                \r\n  5.6189e-01 -1.8068e-01 -3.5824e-01  ...  -2.2752e-01 -6.7575e-02 -7.5511e-01\r\n  2.4132e-01 -2.8088e-01 -1.0904e+00  ...  -3.3897e-01 -1.7998e-01  4.9224e-02\r\n  1.6513e-01  5.3980e-02 -1.3517e-01  ...   1.8532e-01 -7.9412e-03 -3.4100e-01\r\n   ...\r\n\r\n(0 ,29,.,.) = \r\n  1.0052e+00 -1.1017e-01 -1.2992e-01  ...   4.7833e-01 -3.1562e-01  2.4085e-01\r\n -6.0557e-02  8.5560e-01  6.3823e-01  ...   1.7778e+00  9.0973e-01 -4.5352e-01\r\n -1.2317e-01  1.1153e+00  1.2399e+00  ...  -1.0785e-01  3.8565e-01  4.1936e-02\r\n                 ...                   \u22f1                   ...                \r\n -1.3574e+00  1.1036e+00  2.1086e+00  ...   1.4253e+00 -1.0863e+00  8.4242e-01\r\n -7.7115e-01 -2.7149e-01  4.6326e-01  ...   1.0213e+00  8.8335e-01 -1.7714e-01\r\n -8.9065e-01  1.6536e-01  5.7247e-01  ...  -7.0598e-01 -3.1186e-01  6.3970e-02\r\n\r\n(0 ,30,.,.) = \r\n  1.3632e-03 -5.4865e-01  2.2106e-01  ...   3.6347e-02  5.3167e-01  6.2587e-01\r\n -4.6164e-01 -5.2589e-01 -5.7386e-01  ...  -2.0609e-01 -1.2341e+00  4.0797e-01\r\n -3.8703e-01  7.1140e-01  2.7122e-01  ...   3.5770e-01 -4.8863e-03 -2.6595e-01\r\n                 ...                   \u22f1                   ...                \r\n -5.9267e-01  9.7627e-02 -7.3991e-01  ...   2.4554e-01 -1.7257e-01  2.1498e-01\r\n -5.8195e-02 -9.4493e-02 -5.5493e-01  ...  -4.2769e-01 -9.1936e-01 -6.1112e-01\r\n -4.7519e-01  3.4992e-01 -1.6242e-01  ...  -6.2562e-01 -1.7873e-01 -2.6058e-01\r\n\r\n(0 ,31,.,.) = \r\n -3.5324e-01  6.1532e-01 -3.3209e-01  ...   7.6112e-01 -1.8485e-01 -8.4187e-01\r\n  2.5945e-01  6.1765e-01  2.1825e-01  ...  -3.5352e-01  1.8842e+00 -6.1050e-01\r\n  9.1983e-02 -2.4436e-01 -2.2283e-01  ...  -1.0150e-01  6.2999e-01 -6.0950e-01\r\n                 ...                   \u22f1                   ...                \r\n -1.1329e-01 -4.7121e-01  1.3504e+00  ...   2.5476e-01 -5.5622e-01 -2.3621e-01\r\n -9.8706e-01 -6.2796e-01  1.5716e+00  ...   5.0247e-01  7.7165e-01 -5.9040e-02\r\n -1.7598e-01 -3.7907e-01  8.6386e-01  ...   1.6770e-01 -1.6873e-01  1.7422e-01\r\n[torch.FloatTensor of size 1x32x20x20]\r\n``` \r\nI wonder why the original code works correctly using `pytorch '0.1.12` but incorrectly in `pytorch '0.2.0_1`? Do you have any suggestions? Thank you?"}