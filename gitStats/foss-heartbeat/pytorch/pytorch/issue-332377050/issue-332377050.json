{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8480", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8480/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8480/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8480/events", "html_url": "https://github.com/pytorch/pytorch/issues/8480", "id": 332377050, "node_id": "MDU6SXNzdWUzMzIzNzcwNTA=", "number": 8480, "title": "pytorch 0.4.0 always allocates memory on GPU:0 when the model and data are on other GPU.", "user": {"login": "yezhejack", "id": 9005542, "node_id": "MDQ6VXNlcjkwMDU1NDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/9005542?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yezhejack", "html_url": "https://github.com/yezhejack", "followers_url": "https://api.github.com/users/yezhejack/followers", "following_url": "https://api.github.com/users/yezhejack/following{/other_user}", "gists_url": "https://api.github.com/users/yezhejack/gists{/gist_id}", "starred_url": "https://api.github.com/users/yezhejack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yezhejack/subscriptions", "organizations_url": "https://api.github.com/users/yezhejack/orgs", "repos_url": "https://api.github.com/users/yezhejack/repos", "events_url": "https://api.github.com/users/yezhejack/events{/privacy}", "received_events_url": "https://api.github.com/users/yezhejack/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-14T12:18:38Z", "updated_at": "2018-06-14T12:20:53Z", "closed_at": "2018-06-14T12:20:53Z", "author_association": "NONE", "body_html": "<p>I want to only use GPU:1 to train my model. I put the gru layer and input tensor to the cuda:1. After I feed the data into gru layer there, pytorch will allocate some memory on GPU:0. As a result, it will use two GPUs. The following code will reproduce the problem.</p>\n<pre><code>import torch\nwith torch.cuda.device(1):\n    gru = torch.nn.GRU(10, 10).to(1)\ni = torch.ones(1, 1, 10)\nwith torch.cuda.device(1):\n    i = i.to(1)\n\ninput('check the nvidia-smi')\nk = gru(i)\ninput('check the nvidia-smi')\n</code></pre>", "body_text": "I want to only use GPU:1 to train my model. I put the gru layer and input tensor to the cuda:1. After I feed the data into gru layer there, pytorch will allocate some memory on GPU:0. As a result, it will use two GPUs. The following code will reproduce the problem.\nimport torch\nwith torch.cuda.device(1):\n    gru = torch.nn.GRU(10, 10).to(1)\ni = torch.ones(1, 1, 10)\nwith torch.cuda.device(1):\n    i = i.to(1)\n\ninput('check the nvidia-smi')\nk = gru(i)\ninput('check the nvidia-smi')", "body": "I want to only use GPU:1 to train my model. I put the gru layer and input tensor to the cuda:1. After I feed the data into gru layer there, pytorch will allocate some memory on GPU:0. As a result, it will use two GPUs. The following code will reproduce the problem.\r\n```\r\nimport torch\r\nwith torch.cuda.device(1):\r\n    gru = torch.nn.GRU(10, 10).to(1)\r\ni = torch.ones(1, 1, 10)\r\nwith torch.cuda.device(1):\r\n    i = i.to(1)\r\n\r\ninput('check the nvidia-smi')\r\nk = gru(i)\r\ninput('check the nvidia-smi')\r\n```\r\n\r\n\r\n\r\n"}