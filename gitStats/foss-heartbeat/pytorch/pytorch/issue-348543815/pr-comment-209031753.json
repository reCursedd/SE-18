{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209031753", "pull_request_review_id": 144966762, "id": 209031753, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTAzMTc1Mw==", "diff_hunk": "@@ -39,4 +40,18 @@ static inline int64_t matrixStride(const Tensor& batched_matrices) {\n   return batched_matrices.size(-1) * batched_matrices.size(-2);\n }\n \n+// Returns the epsilon value for floating types\n+static inline Tensor _get_epsilon(const Type& type) {\n+  switch (type.scalarType()) {\n+    case at::ScalarType::Half:\n+      return type.tensor({}).fill_(std::numeric_limits<at::Half>::epsilon());", "path": "aten/src/ATen/native/LinearAlgebraUtils.h", "position": null, "original_position": 14, "commit_id": "c9052b5bf6893bc9c5fafb56d0ae3ea8dff3250b", "original_commit_id": "08466d5de9505ef8d3dd631400d856d0ececbc87", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "Please don't use `Type::tensor` anymore, it doesn't retain the GPU device of the source. You can write this as `at::full({}, value, type)` or even `at::tensor(value, type)` (the latter is like `torch.tensor`)", "created_at": "2018-08-09T18:21:38Z", "updated_at": "2018-11-23T15:49:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/10338#discussion_r209031753", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10338", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209031753"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10338#discussion_r209031753"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10338"}}, "body_html": "<p>Please don't use <code>Type::tensor</code> anymore, it doesn't retain the GPU device of the source. You can write this as <code>at::full({}, value, type)</code> or even <code>at::tensor(value, type)</code> (the latter is like <code>torch.tensor</code>)</p>", "body_text": "Please don't use Type::tensor anymore, it doesn't retain the GPU device of the source. You can write this as at::full({}, value, type) or even at::tensor(value, type) (the latter is like torch.tensor)"}