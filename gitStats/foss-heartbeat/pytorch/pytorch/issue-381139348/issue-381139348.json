{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14028", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14028/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14028/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14028/events", "html_url": "https://github.com/pytorch/pytorch/issues/14028", "id": 381139348, "node_id": "MDU6SXNzdWUzODExMzkzNDg=", "number": 14028, "title": "[feature request] Shifted Softplus", "user": {"login": "Tony-Y", "id": 11532812, "node_id": "MDQ6VXNlcjExNTMyODEy", "avatar_url": "https://avatars1.githubusercontent.com/u/11532812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tony-Y", "html_url": "https://github.com/Tony-Y", "followers_url": "https://api.github.com/users/Tony-Y/followers", "following_url": "https://api.github.com/users/Tony-Y/following{/other_user}", "gists_url": "https://api.github.com/users/Tony-Y/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tony-Y/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tony-Y/subscriptions", "organizations_url": "https://api.github.com/users/Tony-Y/orgs", "repos_url": "https://api.github.com/users/Tony-Y/repos", "events_url": "https://api.github.com/users/Tony-Y/events{/privacy}", "received_events_url": "https://api.github.com/users/Tony-Y/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-15T12:27:17Z", "updated_at": "2018-11-22T05:07:21Z", "closed_at": "2018-11-19T18:46:39Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>The shifted softplus activation function</p>\n<p>SSP(x) = Softplus(x) - Softplus(0)</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/11532812/48595087-a6f15480-e996-11e8-9141-9ca29b8187d9.png\"><img src=\"https://user-images.githubusercontent.com/11532812/48595087-a6f15480-e996-11e8-9141-9ca29b8187d9.png\" alt=\"fig_ssp\" style=\"max-width:100%;\"></a></p>\n<h2>Motivation</h2>\n<p>SchNet (<a href=\"https://arxiv.org/abs/1712.06113\" rel=\"nofollow\">https://arxiv.org/abs/1712.06113</a>) uses a shifted softplus<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/11532812/48593239-4b6f9880-e98f-11e8-9a9c-342002293dc9.png\"><img width=\"408\" alt=\"ssp\" src=\"https://user-images.githubusercontent.com/11532812/48593239-4b6f9880-e98f-11e8-9a9c-342002293dc9.png\" style=\"max-width:100%;\"></a><br>\nas activation functions. This is like ELU(alpha=log2), but allows the use of the smooth derivatives of any order for training because it have infinite order of continuity. The SSP activation function would be usable instead of ELU where ELU is effective and the smooth derivates are required. Generally, SSP and Softplus are preferable for the following problem:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/11532812/48655443-ef257b00-ea59-11e8-9211-45e82b9dfd70.png\"><img width=\"828\" alt=\"fitting_problem\" src=\"https://user-images.githubusercontent.com/11532812/48655443-ef257b00-ea59-11e8-9211-45e82b9dfd70.png\" style=\"max-width:100%;\"></a></p>\n<p>Sample code: <a href=\"https://github.com/Tony-Y/PotFit.git\">https://github.com/Tony-Y/PotFit.git</a></p>\n<h2>Pitch</h2>\n<p>It can be defined in torch.nn.modules.activation as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">SSP</span>(<span class=\"pl-e\">Softplus</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">beta</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">threshold</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">20</span>):\n        <span class=\"pl-c1\">super</span>(<span class=\"pl-c1\">SSP</span>, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>(beta, threshold)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        sp0 <span class=\"pl-k\">=</span> F.softplus(torch.zeros(<span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">self</span>.beta, <span class=\"pl-c1\">self</span>.threshold).item()\n        <span class=\"pl-k\">return</span> F.softplus(<span class=\"pl-c1\">input</span>, <span class=\"pl-c1\">self</span>.beta, <span class=\"pl-c1\">self</span>.threshold) <span class=\"pl-k\">-</span> sp0</pre></div>\n<h2>Alternatives</h2>\n<p>torch.nn.functional.softplus is defined as</p>\n<div class=\"highlight highlight-source-python\"><pre>softplus <span class=\"pl-k\">=</span> _add_docstr(torch._C._nn.softplus, <span class=\"pl-sr\"><span class=\"pl-k\">r</span><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-sr\">softplus<span class=\"pl-c1\">(</span>input, beta=1, threshold=20<span class=\"pl-c1\">)</span> -&gt; Tensor</span>\n<span class=\"pl-sr\"><span class=\"pl-pds\">\"\"\"</span></span>)</pre></div>\n<p>I think its sources are<br>\npytorch/aten/src/THNN/generic/SoftPlus.c<br>\npytorch/aten/src/THCUNN/SoftPlus.cu<br>\npytorch/aten/src/THCUNN/generic/SoftPlus.cu</p>\n<p>So, we can also add SSP.c and SSP.cu to ATEN, but I don\u2019t  have enough knowledge of ATEN. It might be preferable that SSP is implemented as an option of Softplus like <code>Softplus(shifted=True)</code>.</p>\n<p>(The text below was added on Nov. 22, 2018)</p>\n<h2>Additional Resources</h2>\n<p>I noticed SSP with the origin shift of 0.5 is more similar to ELU.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">SSP</span>(<span class=\"pl-e\">Softplus</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">beta</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">origin</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>, <span class=\"pl-smi\">threshold</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">20</span>):\n        <span class=\"pl-c1\">super</span>(<span class=\"pl-c1\">SSP</span>, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>(beta, threshold)\n        <span class=\"pl-c1\">self</span>.origin <span class=\"pl-k\">=</span> origin\n        <span class=\"pl-c1\">self</span>.sp0 <span class=\"pl-k\">=</span> F.softplus(torch.zeros(<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.origin, <span class=\"pl-c1\">self</span>.beta, <span class=\"pl-c1\">self</span>.threshold).item()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-k\">return</span> F.softplus(<span class=\"pl-c1\">input</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.origin, <span class=\"pl-c1\">self</span>.beta, <span class=\"pl-c1\">self</span>.threshold) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">self</span>.sp0</pre></div>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/11532812/48882416-7f4e3080-ee5d-11e8-8126-e24b1bd504de.png\"><img src=\"https://user-images.githubusercontent.com/11532812/48882416-7f4e3080-ee5d-11e8-8126-e24b1bd504de.png\" alt=\"fig_ssp_v2\" style=\"max-width:100%;\"></a></p>", "body_text": "\ud83d\ude80 Feature\nThe shifted softplus activation function\nSSP(x) = Softplus(x) - Softplus(0)\n\nMotivation\nSchNet (https://arxiv.org/abs/1712.06113) uses a shifted softplus\n\nas activation functions. This is like ELU(alpha=log2), but allows the use of the smooth derivatives of any order for training because it have infinite order of continuity. The SSP activation function would be usable instead of ELU where ELU is effective and the smooth derivates are required. Generally, SSP and Softplus are preferable for the following problem:\n\nSample code: https://github.com/Tony-Y/PotFit.git\nPitch\nIt can be defined in torch.nn.modules.activation as follows:\nclass SSP(Softplus):\n    def __init__(self, beta=1, threshold=20):\n        super(SSP, self).__init__(beta, threshold)\n    def forward(self, input):\n        sp0 = F.softplus(torch.zeros(1), self.beta, self.threshold).item()\n        return F.softplus(input, self.beta, self.threshold) - sp0\nAlternatives\ntorch.nn.functional.softplus is defined as\nsoftplus = _add_docstr(torch._C._nn.softplus, r\"\"\"\nsoftplus(input, beta=1, threshold=20) -> Tensor\n\"\"\")\nI think its sources are\npytorch/aten/src/THNN/generic/SoftPlus.c\npytorch/aten/src/THCUNN/SoftPlus.cu\npytorch/aten/src/THCUNN/generic/SoftPlus.cu\nSo, we can also add SSP.c and SSP.cu to ATEN, but I don\u2019t  have enough knowledge of ATEN. It might be preferable that SSP is implemented as an option of Softplus like Softplus(shifted=True).\n(The text below was added on Nov. 22, 2018)\nAdditional Resources\nI noticed SSP with the origin shift of 0.5 is more similar to ELU.\nclass SSP(Softplus):\n    def __init__(self, beta=1, origin=0.5, threshold=20):\n        super(SSP, self).__init__(beta, threshold)\n        self.origin = origin\n        self.sp0 = F.softplus(torch.zeros(1) + self.origin, self.beta, self.threshold).item()\n\n    def forward(self, input):\n        return F.softplus(input + self.origin, self.beta, self.threshold) - self.sp0", "body": "## \ud83d\ude80 Feature\r\nThe shifted softplus activation function\r\n\r\nSSP(x) = Softplus(x) - Softplus(0)\r\n\r\n![fig_ssp](https://user-images.githubusercontent.com/11532812/48595087-a6f15480-e996-11e8-9141-9ca29b8187d9.png)\r\n\r\n## Motivation\r\nSchNet (https://arxiv.org/abs/1712.06113) uses a shifted softplus\r\n<img width=\"408\" alt=\"ssp\" src=\"https://user-images.githubusercontent.com/11532812/48593239-4b6f9880-e98f-11e8-9a9c-342002293dc9.png\">\r\nas activation functions. This is like ELU(alpha=log2), but allows the use of the smooth derivatives of any order for training because it have infinite order of continuity. The SSP activation function would be usable instead of ELU where ELU is effective and the smooth derivates are required. Generally, SSP and Softplus are preferable for the following problem: \r\n\r\n<img width=\"828\" alt=\"fitting_problem\" src=\"https://user-images.githubusercontent.com/11532812/48655443-ef257b00-ea59-11e8-9211-45e82b9dfd70.png\">\r\n\r\nSample code: https://github.com/Tony-Y/PotFit.git\r\n\r\n## Pitch\r\nIt can be defined in torch.nn.modules.activation as follows:\r\n```python\r\nclass SSP(Softplus):\r\n    def __init__(self, beta=1, threshold=20):\r\n        super(SSP, self).__init__(beta, threshold)\r\n    def forward(self, input):\r\n        sp0 = F.softplus(torch.zeros(1), self.beta, self.threshold).item()\r\n        return F.softplus(input, self.beta, self.threshold) - sp0\r\n```\r\n\r\n## Alternatives\r\ntorch.nn.functional.softplus is defined as\r\n```python\r\nsoftplus = _add_docstr(torch._C._nn.softplus, r\"\"\"\r\nsoftplus(input, beta=1, threshold=20) -> Tensor\r\n\"\"\")\r\n```\r\nI think its sources are\r\npytorch/aten/src/THNN/generic/SoftPlus.c\r\npytorch/aten/src/THCUNN/SoftPlus.cu\r\npytorch/aten/src/THCUNN/generic/SoftPlus.cu\r\n\r\nSo, we can also add SSP.c and SSP.cu to ATEN, but I don\u2019t  have enough knowledge of ATEN. It might be preferable that SSP is implemented as an option of Softplus like `Softplus(shifted=True)`.\r\n\r\n(The text below was added on Nov. 22, 2018)\r\n## Additional Resources\r\nI noticed SSP with the origin shift of 0.5 is more similar to ELU.\r\n```python\r\nclass SSP(Softplus):\r\n    def __init__(self, beta=1, origin=0.5, threshold=20):\r\n        super(SSP, self).__init__(beta, threshold)\r\n        self.origin = origin\r\n        self.sp0 = F.softplus(torch.zeros(1) + self.origin, self.beta, self.threshold).item()\r\n\r\n    def forward(self, input):\r\n        return F.softplus(input + self.origin, self.beta, self.threshold) - self.sp0\r\n```\r\n\r\n![fig_ssp_v2](https://user-images.githubusercontent.com/11532812/48882416-7f4e3080-ee5d-11e8-8126-e24b1bd504de.png)"}