{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146480671", "pull_request_review_id": 71426769, "id": 146480671, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjQ4MDY3MQ==", "diff_hunk": "@@ -44,6 +44,68 @@ Below you can find a small example showcasing this::\n Best practices\n --------------\n \n+Device-agnostic code\n+^^^^^^^^^^^^^^^^^^^^\n+\n+Due to the structure of PyTorch, you may need to explicitly write\n+device-agnostic (CPU or GPU) code; an example may be creating a new tensor as\n+the initial hidden state of a recurrent neural network. \n+\n+The first step is to determine whether the GPU should be used or not. A common\n+pattern is to use Python's `argparse` module to read in user arguments, and\n+have a flag that can be used to disable CUDA, in combination with\n+`torch.cuda.is_available()`. In the following, `args.cuda` results in a flag\n+that can be used to cast tensors and modules to CUDA if desired::\n+\n+    import argparse\n+    import torch\n+\n+    parser = argparse.ArgumentParser(description='PyTorch Example')\n+    parser.add_argument('--disable-cuda', action='store_true',\n+                        help='Disable CUDA')\n+    args = parser.parse_args()\n+    args.cuda = not args.disable_cuda and torch.cuda.is_available()\n+\n+If modules or tensors need to be sent to the GPU, `args.cuda` can be used as\n+follows::\n+\n+    x = torch.Tensor(8, 42)\n+    net = Network()\n+    if args.cuda:\n+      x = x.cuda()\n+      net.cuda()\n+\n+To avoid the if statement for tensors, an alternative is to have a default\n+datatype defined, and cast all tensors using that. An example when using a\n+dataloader would be as follows::\n+\n+    dtype = torch.cuda.FloatTensor\n+    for i, x in enumerate(train_loader):\n+        x = Variable(x.type(dtype))\n+\n+If you have a tensor and would like to create a new tensor of the same type,\n+then you can use the `.new()` function, which acts the same as a normal tensor\n+constructor. This is the recommended practice when creating modules in which\n+new tensors/variables need to be created internally during the forward pass::", "path": "docs/source/notes/cuda.rst", "position": null, "original_position": 46, "commit_id": "6e601f3b99d6c2ade77ecfbb2805f640e697b124", "original_commit_id": "6891cb5cad60ec7081d08ba080afab68ddf1fb1e", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "body": "Good point - I rarely have access to more than 1 GPU and just use DataParallel if I do. Added context management in now, let me know if there's anything else worth mentioning.", "created_at": "2017-10-24T08:08:12Z", "updated_at": "2018-11-23T15:35:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/3227#discussion_r146480671", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3227", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/146480671"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3227#discussion_r146480671"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3227"}}, "body_html": "<p>Good point - I rarely have access to more than 1 GPU and just use DataParallel if I do. Added context management in now, let me know if there's anything else worth mentioning.</p>", "body_text": "Good point - I rarely have access to more than 1 GPU and just use DataParallel if I do. Added context management in now, let me know if there's anything else worth mentioning.", "in_reply_to_id": 146347080}