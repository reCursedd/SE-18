{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103354033", "pull_request_review_id": 24137407, "id": 103354033, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMzM1NDAzMw==", "diff_hunk": "@@ -0,0 +1,54 @@\n+from collections import namedtuple\n+import torch\n+from torch.autograd import Variable\n+\n+\n+PackedSequence = namedtuple('PackedSequence', ['data', 'batch_sizes'])\n+\n+\n+def pack_padded_sequence(tensor, lengths, batch_first=False):\n+    if batch_first:\n+        tensor = tensor.transpose(0, 1)\n+\n+    steps = []\n+    batch_sizes = []\n+    lengths_iter = reversed(lengths)\n+    current_length = next(lengths_iter)\n+    batch_size = tensor.size(1)\n+    if len(lengths) != batch_size:\n+        raise ValueError(\"lengths array has incorrect size\")\n+\n+    for step, step_value in enumerate(tensor, 1):\n+        steps.append(step_value[:batch_size])\n+        batch_sizes.append(batch_size)\n+\n+        while step == current_length:\n+            try:\n+                new_length = next(lengths_iter)\n+            except StopIteration:\n+                current_length = None\n+                break\n+\n+            if current_length > new_length:  # remember that new_length is the preceding length in the array\n+                raise ValueError(\"lengths array has to be sorted in decreasing order\")\n+            batch_size -= 1\n+            current_length = new_length\n+        if current_length is None:\n+            break\n+    return PackedSequence(torch.cat(steps), batch_sizes)\n+\n+\n+def pad_packed_sequence(sequence, batch_first=False):\n+    var_data, batch_sizes = sequence\n+    max_batch_size = batch_sizes[0]\n+    output = var_data.data.new(len(batch_sizes), max_batch_size, *var_data.size()[1:]).zero_()\n+    output = Variable(output)\n+\n+    data_offset = 0\n+    for i, batch_size in enumerate(batch_sizes):\n+        output[i,:batch_size] = var_data[data_offset:data_offset + batch_size]\n+        data_offset += batch_size\n+\n+    if batch_first:\n+        output = output.transpose(0, 1)\n+    return output", "path": "torch/nn/utils/rnn.py", "position": null, "original_position": 54, "commit_id": "15ec2af142f4e0424f15a6fd5985f11f9ae3cedc", "original_commit_id": "c0fc07eca031c47a75faf36e23a296bb897fd80d", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "body": "Would be great if this could return `output, lengths` so it fully inverts `pack_padded_sequence`", "created_at": "2017-02-28T01:07:30Z", "updated_at": "2018-11-23T15:32:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/873#discussion_r103354033", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/873", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103354033"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/873#discussion_r103354033"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/873"}}, "body_html": "<p>Would be great if this could return <code>output, lengths</code> so it fully inverts <code>pack_padded_sequence</code></p>", "body_text": "Would be great if this could return output, lengths so it fully inverts pack_padded_sequence"}