{"url": "https://api.github.com/repos/pytorch/pytorch/issues/666", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/666/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/666/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/666/events", "html_url": "https://github.com/pytorch/pytorch/issues/666", "id": 204647494, "node_id": "MDU6SXNzdWUyMDQ2NDc0OTQ=", "number": 666, "title": "nn.Module not importing parameters contained in lists", "user": {"login": "psmaragdis", "id": 4094887, "node_id": "MDQ6VXNlcjQwOTQ4ODc=", "avatar_url": "https://avatars2.githubusercontent.com/u/4094887?v=4", "gravatar_id": "", "url": "https://api.github.com/users/psmaragdis", "html_url": "https://github.com/psmaragdis", "followers_url": "https://api.github.com/users/psmaragdis/followers", "following_url": "https://api.github.com/users/psmaragdis/following{/other_user}", "gists_url": "https://api.github.com/users/psmaragdis/gists{/gist_id}", "starred_url": "https://api.github.com/users/psmaragdis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/psmaragdis/subscriptions", "organizations_url": "https://api.github.com/users/psmaragdis/orgs", "repos_url": "https://api.github.com/users/psmaragdis/repos", "events_url": "https://api.github.com/users/psmaragdis/events{/privacy}", "received_events_url": "https://api.github.com/users/psmaragdis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-02-01T17:18:50Z", "updated_at": "2017-02-01T17:34:11Z", "closed_at": "2017-02-01T17:34:11Z", "author_association": "NONE", "body_html": "<p>I'm not sure if this is the intended behavior or not. <code>nn.Module</code> does not look for parameters inside lists. This makes it hard to e.g. initialize a network, provide the number of layers as an argument, and then store these layers in a list.</p>\n<p>For example:</p>\n<pre><code>class net( nn.Module):\n    def __init__( self, num_layers):\n        super( net, self).__init__()\n\n        self.layer = nn.Linear( 1, 1)\n        self.list_of_layers = [nn.Linear( 1, 1) for i in range( num_layers)]\n\nn = net( 4)\nprint n\n</code></pre>\n<p>Returns:</p>\n<pre><code>net (\n  (layer): Linear (1 -&gt; 1)\n)\n</code></pre>\n<p>I would have expected the parameters in self.list_of_layers to be have been inherited as the network parameters too.</p>\n<p>Is there another recommended way that allows storage of an arbitrary number layers?</p>\n<p>Thanks</p>", "body_text": "I'm not sure if this is the intended behavior or not. nn.Module does not look for parameters inside lists. This makes it hard to e.g. initialize a network, provide the number of layers as an argument, and then store these layers in a list.\nFor example:\nclass net( nn.Module):\n    def __init__( self, num_layers):\n        super( net, self).__init__()\n\n        self.layer = nn.Linear( 1, 1)\n        self.list_of_layers = [nn.Linear( 1, 1) for i in range( num_layers)]\n\nn = net( 4)\nprint n\n\nReturns:\nnet (\n  (layer): Linear (1 -> 1)\n)\n\nI would have expected the parameters in self.list_of_layers to be have been inherited as the network parameters too.\nIs there another recommended way that allows storage of an arbitrary number layers?\nThanks", "body": "I'm not sure if this is the intended behavior or not. ```nn.Module``` does not look for parameters inside lists. This makes it hard to e.g. initialize a network, provide the number of layers as an argument, and then store these layers in a list.\r\n\r\nFor example:\r\n```\r\nclass net( nn.Module):\r\n    def __init__( self, num_layers):\r\n        super( net, self).__init__()\r\n\r\n        self.layer = nn.Linear( 1, 1)\r\n        self.list_of_layers = [nn.Linear( 1, 1) for i in range( num_layers)]\r\n\r\nn = net( 4)\r\nprint n\r\n```\r\n\r\nReturns:\r\n```\r\nnet (\r\n  (layer): Linear (1 -> 1)\r\n)\r\n```\r\n\r\nI would have expected the parameters in self.list_of_layers to be have been inherited as the network parameters too.\r\n\r\nIs there another recommended way that allows storage of an arbitrary number layers?\r\n\r\nThanks\r\n"}