{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187680187", "pull_request_review_id": 119515539, "id": 187680187, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NzY4MDE4Nw==", "diff_hunk": "@@ -430,6 +443,12 @@ struct GraphExecutorImpl {\n   // symbolically differentiable.\n   bool symbolically_differentiable;\n \n+  // some ops, including python operations, can intorduce requires_grad=True\n+  // variables even though no inputs to this graph are availiable, if\n+  // the graph includes those operators then needGradient must be true\n+  // regardles of input state.\n+  bool may_introduce_gradient;", "path": "torch/csrc/jit/graph_executor.cpp", "position": 123, "original_position": 123, "commit_id": "ba574c6b8fe9cef23df54f42b658729cf5c63b5b", "original_commit_id": "3715ff61a72d1845f90361f20171025396484c88", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "I agree, these things are getting added because of actual bugs.  This one is because we changed an invariant we used to have: that the python code we call is functional. Now python code we call may be a call to a module forward with some hidden module weights that do require grad. This is pretty common when the input tensor does not requires grad but all the weights do.", "created_at": "2018-05-11T17:27:23Z", "updated_at": "2018-11-23T15:43:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/7489#discussion_r187680187", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7489", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/187680187"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7489#discussion_r187680187"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7489"}}, "body_html": "<p>I agree, these things are getting added because of actual bugs.  This one is because we changed an invariant we used to have: that the python code we call is functional. Now python code we call may be a call to a module forward with some hidden module weights that do require grad. This is pretty common when the input tensor does not requires grad but all the weights do.</p>", "body_text": "I agree, these things are getting added because of actual bugs.  This one is because we changed an invariant we used to have: that the python code we call is functional. Now python code we call may be a call to a module forward with some hidden module weights that do require grad. This is pretty common when the input tensor does not requires grad but all the weights do.", "in_reply_to_id": 187561863}