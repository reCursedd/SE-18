{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4375", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4375/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4375/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4375/events", "html_url": "https://github.com/pytorch/pytorch/issues/4375", "id": 284843930, "node_id": "MDU6SXNzdWUyODQ4NDM5MzA=", "number": 4375, "title": "RuntimeError: ONNX export failed: Couldn't export Python operator Scatter", "user": {"login": "jasw1001", "id": 20208199, "node_id": "MDQ6VXNlcjIwMjA4MTk5", "avatar_url": "https://avatars2.githubusercontent.com/u/20208199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jasw1001", "html_url": "https://github.com/jasw1001", "followers_url": "https://api.github.com/users/jasw1001/followers", "following_url": "https://api.github.com/users/jasw1001/following{/other_user}", "gists_url": "https://api.github.com/users/jasw1001/gists{/gist_id}", "starred_url": "https://api.github.com/users/jasw1001/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jasw1001/subscriptions", "organizations_url": "https://api.github.com/users/jasw1001/orgs", "repos_url": "https://api.github.com/users/jasw1001/repos", "events_url": "https://api.github.com/users/jasw1001/events{/privacy}", "received_events_url": "https://api.github.com/users/jasw1001/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-12-28T05:23:33Z", "updated_at": "2018-10-09T09:35:39Z", "closed_at": "2017-12-28T15:17:07Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nCould anyone help me to figure out the following error?<br>\nMany Thanks!!!!!</p>\n<p>The Error:</p>\n<p>RuntimeError: ONNX export failed: Couldn't export Python operator Scatter</p>\n<p>Graph we tried to export:<br>\ngraph(%0 : Float(1, 1, 128, 128, 128)<br>\n%1 : Float(64, 1, 3, 3, 3)<br>\n%2 : Float(64)<br>\n%3 : Float(64)<br>\n%4 : Float(64)<br>\n%5 : Float(64)<br>\n%6 : Float(64)<br>\n%7 : Float(64, 64, 3, 3, 3)<br>\n%8 : Float(64)<br>\n%9 : Float(64)<br>\n%10 : Float(64)<br>\n%11 : Float(64)<br>\n%12 : Float(64)<br>\n%13 : Float(64, 64, 2, 2, 2)<br>\n%14 : Float(64)<br>\n%15 : Float(128, 64, 3, 3, 3)<br>\n%16 : Float(128)<br>\n%17 : Float(128)<br>\n%18 : Float(128)<br>\n%19 : Float(128)<br>\n%20 : Float(128)<br>\n%21 : Float(128, 128, 3, 3, 3)<br>\n%22 : Float(128)<br>\n%23 : Float(128)<br>\n%24 : Float(128)<br>\n%25 : Float(128)<br>\n%26 : Float(128)<br>\n%27 : Float(128, 128, 2, 2, 2)<br>\n%28 : Float(128)<br>\n%29 : Float(256, 128, 3, 3, 3)<br>\n%30 : Float(256)<br>\n%31 : Float(256)<br>\n%32 : Float(256)<br>\n%33 : Float(256)<br>\n%34 : Float(256)<br>\n%35 : Float(256, 256, 3, 3, 3)<br>\n%36 : Float(256)<br>\n%37 : Float(256)<br>\n%38 : Float(256)<br>\n%39 : Float(256)<br>\n%40 : Float(256)<br>\n%41 : Float(256, 256, 3, 3, 3)<br>\n%42 : Float(256)<br>\n%43 : Float(256)<br>\n%44 : Float(256)<br>\n%45 : Float(256)<br>\n%46 : Float(256)<br>\n%47 : Float(256, 256, 2, 2, 2)<br>\n%48 : Float(256)<br>\n%49 : Float(512, 256, 3, 3, 3)<br>\n%50 : Float(512)<br>\n%51 : Float(512)<br>\n%52 : Float(512)<br>\n%53 : Float(512)<br>\n%54 : Float(512)<br>\n%55 : Float(512, 512, 3, 3, 3)<br>\n%56 : Float(512)<br>\n%57 : Float(512)<br>\n%58 : Float(512)<br>\n%59 : Float(512)<br>\n%60 : Float(512)<br>\n%61 : Float(512, 512, 3, 3, 3)<br>\n%62 : Float(512)<br>\n%63 : Float(512)<br>\n%64 : Float(512)<br>\n%65 : Float(512)<br>\n%66 : Float(512)<br>\n%67 : Float(512, 512, 2, 2, 2)<br>\n%68 : Float(512)<br>\n%69 : Float(512, 512, 3, 3, 3)<br>\n%70 : Float(512)<br>\n%71 : Float(512)<br>\n%72 : Float(512)<br>\n%73 : Float(512)<br>\n%74 : Float(512)<br>\n%75 : Float(512, 512, 3, 3, 3)<br>\n%76 : Float(512)<br>\n%77 : Float(512)<br>\n%78 : Float(512)<br>\n%79 : Float(512)<br>\n%80 : Float(512)<br>\n%81 : Float(512, 512, 3, 3, 3)<br>\n%82 : Float(512)<br>\n%83 : Float(512)<br>\n%84 : Float(512)<br>\n%85 : Float(512)<br>\n%86 : Float(512)<br>\n%87 : Float(512, 512, 3, 3, 3)<br>\n%88 : Float(512)<br>\n%89 : Float(15, 512, 1, 1, 1)<br>\n%90 : Float(15)) {<br>\n%91 : Float(1, 1, 128, 128, 128), %92 : Handle = ^Scatter([0], None, 0)(%0), scope: DataParallel<br>\n%93 : Float(1, 64, 128, 128, 128) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%91, %1, %2), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[0]<br>\n%94 : Float(1, 64, 128, 128, 128) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%93, %3, %4, %5, %6), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[1]<br>\n%95 : Float(1, 64, 128, 128, 128) = Relu(%94), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[2]<br>\n%96 : Float(1, 64, 128, 128, 128) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%95, %7, %8), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[3]<br>\n%97 : Float(1, 64, 128, 128, 128) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%96, %9, %10, %11, %12), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[4]<br>\n%98 : Float(1, 64, 128, 128, 128) = Relu(%97), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[5]<br>\n%99 : Float(1, 64, 64, 64, 64) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%98, %13, %14), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[6]<br>\n%100 : Float(1, 128, 64, 64, 64) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%99, %15, %16), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[7]<br>\n%101 : Float(1, 128, 64, 64, 64) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%100, %17, %18, %19, %20), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[8]<br>\n%102 : Float(1, 128, 64, 64, 64) = Relu(%101), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[9]<br>\n%103 : Float(1, 128, 64, 64, 64) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%102, %21, %22), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[10]<br>\n%104 : Float(1, 128, 64, 64, 64) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%103, %23, %24, %25, %26), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[11]<br>\n%105 : Float(1, 128, 64, 64, 64) = Relu(%104), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[12]<br>\n%106 : Float(1, 128, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%105, %27, %28), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[13]<br>\n%107 : Float(1, 256, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%106, %29, %30), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[14]<br>\n%108 : Float(1, 256, 32, 32, 32) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%107, %31, %32, %33, %34), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[15]<br>\n%109 : Float(1, 256, 32, 32, 32) = Relu(%108), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[16]<br>\n%110 : Float(1, 256, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%109, %35, %36), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[17]<br>\n%111 : Float(1, 256, 32, 32, 32) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%110, %37, %38, %39, %40), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[18]<br>\n%112 : Float(1, 256, 32, 32, 32) = Relu(%111), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[19]<br>\n%113 : Float(1, 256, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%112, %41, %42), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[20]<br>\n%114 : Float(1, 256, 32, 32, 32) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%113, %43, %44, %45, %46), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[21]<br>\n%115 : Float(1, 256, 32, 32, 32) = Relu(%114), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[22]<br>\n%116 : Float(1, 256, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%115, %47, %48), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[23]<br>\n%117 : Float(1, 512, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%116, %49, %50), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[24]<br>\n%118 : Float(1, 512, 16, 16, 16) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%117, %51, %52, %53, %54), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[25]<br>\n%119 : Float(1, 512, 16, 16, 16) = Relu(%118), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[26]<br>\n%120 : Float(1, 512, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%119, %55, %56), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[27]<br>\n%121 : Float(1, 512, 16, 16, 16) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%120, %57, %58, %59, %60), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[28]<br>\n%122 : Float(1, 512, 16, 16, 16) = Relu(%121), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[29]<br>\n%123 : Float(1, 512, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%122, %61, %62), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[30]<br>\n%124 : Float(1, 512, 16, 16, 16) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%123, %63, %64, %65, %66), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[31]<br>\n%125 : Float(1, 512, 16, 16, 16) = Relu(%124), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[32]<br>\n%126 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%125, %67, %68), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[33]<br>\n%127 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%126, %69, %70), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[34]<br>\n%128 : Float(1, 512, 8, 8, 8) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%127, %71, %72, %73, %74), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[35]<br>\n%129 : Float(1, 512, 8, 8, 8) = Relu(%128), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[36]<br>\n%130 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%129, %75, %76), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[37]<br>\n%131 : Float(1, 512, 8, 8, 8) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%130, %77, %78, %79, %80), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[38]<br>\n%132 : Float(1, 512, 8, 8, 8) = Relu(%131), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[39]<br>\n%133 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%132, %81, %82), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[40]<br>\n%134 : Float(1, 512, 8, 8, 8) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%133, %83, %84, %85, %86), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[41]<br>\n%135 : Float(1, 512, 8, 8, 8) = Relu(%134), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[42]<br>\n%136 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%135, %87, %88), scope: DataParallel/Net[module]/Conv3d[rpn_conv33]<br>\n%137 : Float(1, 512, 8, 8, 8) = Relu(%136), scope: DataParallel/Net[module]/ReLU[rpn_relu33]<br>\n%138 : Float(1, 15, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[1, 1, 1], pads=[0, 0, 0, 0, 0, 0], strides=[1, 1, 1]](%137, %89, %90), scope: DataParallel/Net[module]/Conv3d[rpn_score]<br>\n%139 : Float(1, 15, 512) = Reshape<a href=\"%138\">shape=[1, 15, -1]</a>, scope: DataParallel/Net[module]<br>\n%140 : Float(1!, 512!, 15!) = Transpose<a href=\"%139\">perm=[0, 2, 1]</a>, scope: DataParallel/Net[module]<br>\n%141 : Float(1, 8, 8, 8, 3, 5) = Reshape<a href=\"%140\">shape=[1, 8, 8, 8, 3, 5]</a>, scope: DataParallel/Net[module]<br>\nreturn (%141);<br>\n}</p>", "body_text": "Hi,\nCould anyone help me to figure out the following error?\nMany Thanks!!!!!\nThe Error:\nRuntimeError: ONNX export failed: Couldn't export Python operator Scatter\nGraph we tried to export:\ngraph(%0 : Float(1, 1, 128, 128, 128)\n%1 : Float(64, 1, 3, 3, 3)\n%2 : Float(64)\n%3 : Float(64)\n%4 : Float(64)\n%5 : Float(64)\n%6 : Float(64)\n%7 : Float(64, 64, 3, 3, 3)\n%8 : Float(64)\n%9 : Float(64)\n%10 : Float(64)\n%11 : Float(64)\n%12 : Float(64)\n%13 : Float(64, 64, 2, 2, 2)\n%14 : Float(64)\n%15 : Float(128, 64, 3, 3, 3)\n%16 : Float(128)\n%17 : Float(128)\n%18 : Float(128)\n%19 : Float(128)\n%20 : Float(128)\n%21 : Float(128, 128, 3, 3, 3)\n%22 : Float(128)\n%23 : Float(128)\n%24 : Float(128)\n%25 : Float(128)\n%26 : Float(128)\n%27 : Float(128, 128, 2, 2, 2)\n%28 : Float(128)\n%29 : Float(256, 128, 3, 3, 3)\n%30 : Float(256)\n%31 : Float(256)\n%32 : Float(256)\n%33 : Float(256)\n%34 : Float(256)\n%35 : Float(256, 256, 3, 3, 3)\n%36 : Float(256)\n%37 : Float(256)\n%38 : Float(256)\n%39 : Float(256)\n%40 : Float(256)\n%41 : Float(256, 256, 3, 3, 3)\n%42 : Float(256)\n%43 : Float(256)\n%44 : Float(256)\n%45 : Float(256)\n%46 : Float(256)\n%47 : Float(256, 256, 2, 2, 2)\n%48 : Float(256)\n%49 : Float(512, 256, 3, 3, 3)\n%50 : Float(512)\n%51 : Float(512)\n%52 : Float(512)\n%53 : Float(512)\n%54 : Float(512)\n%55 : Float(512, 512, 3, 3, 3)\n%56 : Float(512)\n%57 : Float(512)\n%58 : Float(512)\n%59 : Float(512)\n%60 : Float(512)\n%61 : Float(512, 512, 3, 3, 3)\n%62 : Float(512)\n%63 : Float(512)\n%64 : Float(512)\n%65 : Float(512)\n%66 : Float(512)\n%67 : Float(512, 512, 2, 2, 2)\n%68 : Float(512)\n%69 : Float(512, 512, 3, 3, 3)\n%70 : Float(512)\n%71 : Float(512)\n%72 : Float(512)\n%73 : Float(512)\n%74 : Float(512)\n%75 : Float(512, 512, 3, 3, 3)\n%76 : Float(512)\n%77 : Float(512)\n%78 : Float(512)\n%79 : Float(512)\n%80 : Float(512)\n%81 : Float(512, 512, 3, 3, 3)\n%82 : Float(512)\n%83 : Float(512)\n%84 : Float(512)\n%85 : Float(512)\n%86 : Float(512)\n%87 : Float(512, 512, 3, 3, 3)\n%88 : Float(512)\n%89 : Float(15, 512, 1, 1, 1)\n%90 : Float(15)) {\n%91 : Float(1, 1, 128, 128, 128), %92 : Handle = ^Scatter([0], None, 0)(%0), scope: DataParallel\n%93 : Float(1, 64, 128, 128, 128) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%91, %1, %2), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[0]\n%94 : Float(1, 64, 128, 128, 128) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%93, %3, %4, %5, %6), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[1]\n%95 : Float(1, 64, 128, 128, 128) = Relu(%94), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[2]\n%96 : Float(1, 64, 128, 128, 128) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%95, %7, %8), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[3]\n%97 : Float(1, 64, 128, 128, 128) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%96, %9, %10, %11, %12), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[4]\n%98 : Float(1, 64, 128, 128, 128) = Relu(%97), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[5]\n%99 : Float(1, 64, 64, 64, 64) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%98, %13, %14), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[6]\n%100 : Float(1, 128, 64, 64, 64) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%99, %15, %16), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[7]\n%101 : Float(1, 128, 64, 64, 64) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%100, %17, %18, %19, %20), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[8]\n%102 : Float(1, 128, 64, 64, 64) = Relu(%101), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[9]\n%103 : Float(1, 128, 64, 64, 64) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%102, %21, %22), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[10]\n%104 : Float(1, 128, 64, 64, 64) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%103, %23, %24, %25, %26), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[11]\n%105 : Float(1, 128, 64, 64, 64) = Relu(%104), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[12]\n%106 : Float(1, 128, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%105, %27, %28), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[13]\n%107 : Float(1, 256, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%106, %29, %30), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[14]\n%108 : Float(1, 256, 32, 32, 32) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%107, %31, %32, %33, %34), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[15]\n%109 : Float(1, 256, 32, 32, 32) = Relu(%108), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[16]\n%110 : Float(1, 256, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%109, %35, %36), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[17]\n%111 : Float(1, 256, 32, 32, 32) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%110, %37, %38, %39, %40), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[18]\n%112 : Float(1, 256, 32, 32, 32) = Relu(%111), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[19]\n%113 : Float(1, 256, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%112, %41, %42), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[20]\n%114 : Float(1, 256, 32, 32, 32) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%113, %43, %44, %45, %46), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[21]\n%115 : Float(1, 256, 32, 32, 32) = Relu(%114), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[22]\n%116 : Float(1, 256, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%115, %47, %48), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[23]\n%117 : Float(1, 512, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%116, %49, %50), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[24]\n%118 : Float(1, 512, 16, 16, 16) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%117, %51, %52, %53, %54), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[25]\n%119 : Float(1, 512, 16, 16, 16) = Relu(%118), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[26]\n%120 : Float(1, 512, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%119, %55, %56), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[27]\n%121 : Float(1, 512, 16, 16, 16) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%120, %57, %58, %59, %60), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[28]\n%122 : Float(1, 512, 16, 16, 16) = Relu(%121), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[29]\n%123 : Float(1, 512, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%122, %61, %62), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[30]\n%124 : Float(1, 512, 16, 16, 16) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%123, %63, %64, %65, %66), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[31]\n%125 : Float(1, 512, 16, 16, 16) = Relu(%124), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[32]\n%126 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%125, %67, %68), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[33]\n%127 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%126, %69, %70), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[34]\n%128 : Float(1, 512, 8, 8, 8) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%127, %71, %72, %73, %74), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[35]\n%129 : Float(1, 512, 8, 8, 8) = Relu(%128), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[36]\n%130 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%129, %75, %76), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[37]\n%131 : Float(1, 512, 8, 8, 8) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%130, %77, %78, %79, %80), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[38]\n%132 : Float(1, 512, 8, 8, 8) = Relu(%131), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[39]\n%133 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%132, %81, %82), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[40]\n%134 : Float(1, 512, 8, 8, 8) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%133, %83, %84, %85, %86), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[41]\n%135 : Float(1, 512, 8, 8, 8) = Relu(%134), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[42]\n%136 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%135, %87, %88), scope: DataParallel/Net[module]/Conv3d[rpn_conv33]\n%137 : Float(1, 512, 8, 8, 8) = Relu(%136), scope: DataParallel/Net[module]/ReLU[rpn_relu33]\n%138 : Float(1, 15, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[1, 1, 1], pads=[0, 0, 0, 0, 0, 0], strides=[1, 1, 1]](%137, %89, %90), scope: DataParallel/Net[module]/Conv3d[rpn_score]\n%139 : Float(1, 15, 512) = Reshapeshape=[1, 15, -1], scope: DataParallel/Net[module]\n%140 : Float(1!, 512!, 15!) = Transposeperm=[0, 2, 1], scope: DataParallel/Net[module]\n%141 : Float(1, 8, 8, 8, 3, 5) = Reshapeshape=[1, 8, 8, 8, 3, 5], scope: DataParallel/Net[module]\nreturn (%141);\n}", "body": "Hi,\r\nCould anyone help me to figure out the following error?\r\nMany Thanks!!!!!\r\n\r\n\r\nThe Error: \r\n\r\nRuntimeError: ONNX export failed: Couldn't export Python operator Scatter\r\n\r\nGraph we tried to export:\r\ngraph(%0 : Float(1, 1, 128, 128, 128)\r\n      %1 : Float(64, 1, 3, 3, 3)\r\n      %2 : Float(64)\r\n      %3 : Float(64)\r\n      %4 : Float(64)\r\n      %5 : Float(64)\r\n      %6 : Float(64)\r\n      %7 : Float(64, 64, 3, 3, 3)\r\n      %8 : Float(64)\r\n      %9 : Float(64)\r\n      %10 : Float(64)\r\n      %11 : Float(64)\r\n      %12 : Float(64)\r\n      %13 : Float(64, 64, 2, 2, 2)\r\n      %14 : Float(64)\r\n      %15 : Float(128, 64, 3, 3, 3)\r\n      %16 : Float(128)\r\n      %17 : Float(128)\r\n      %18 : Float(128)\r\n      %19 : Float(128)\r\n      %20 : Float(128)\r\n      %21 : Float(128, 128, 3, 3, 3)\r\n      %22 : Float(128)\r\n      %23 : Float(128)\r\n      %24 : Float(128)\r\n      %25 : Float(128)\r\n      %26 : Float(128)\r\n      %27 : Float(128, 128, 2, 2, 2)\r\n      %28 : Float(128)\r\n      %29 : Float(256, 128, 3, 3, 3)\r\n      %30 : Float(256)\r\n      %31 : Float(256)\r\n      %32 : Float(256)\r\n      %33 : Float(256)\r\n      %34 : Float(256)\r\n      %35 : Float(256, 256, 3, 3, 3)\r\n      %36 : Float(256)\r\n      %37 : Float(256)\r\n      %38 : Float(256)\r\n      %39 : Float(256)\r\n      %40 : Float(256)\r\n      %41 : Float(256, 256, 3, 3, 3)\r\n      %42 : Float(256)\r\n      %43 : Float(256)\r\n      %44 : Float(256)\r\n      %45 : Float(256)\r\n      %46 : Float(256)\r\n      %47 : Float(256, 256, 2, 2, 2)\r\n      %48 : Float(256)\r\n      %49 : Float(512, 256, 3, 3, 3)\r\n      %50 : Float(512)\r\n      %51 : Float(512)\r\n      %52 : Float(512)\r\n      %53 : Float(512)\r\n      %54 : Float(512)\r\n      %55 : Float(512, 512, 3, 3, 3)\r\n      %56 : Float(512)\r\n      %57 : Float(512)\r\n      %58 : Float(512)\r\n      %59 : Float(512)\r\n      %60 : Float(512)\r\n      %61 : Float(512, 512, 3, 3, 3)\r\n      %62 : Float(512)\r\n      %63 : Float(512)\r\n      %64 : Float(512)\r\n      %65 : Float(512)\r\n      %66 : Float(512)\r\n      %67 : Float(512, 512, 2, 2, 2)\r\n      %68 : Float(512)\r\n      %69 : Float(512, 512, 3, 3, 3)\r\n      %70 : Float(512)\r\n      %71 : Float(512)\r\n      %72 : Float(512)\r\n      %73 : Float(512)\r\n      %74 : Float(512)\r\n      %75 : Float(512, 512, 3, 3, 3)\r\n      %76 : Float(512)\r\n      %77 : Float(512)\r\n      %78 : Float(512)\r\n      %79 : Float(512)\r\n      %80 : Float(512)\r\n      %81 : Float(512, 512, 3, 3, 3)\r\n      %82 : Float(512)\r\n      %83 : Float(512)\r\n      %84 : Float(512)\r\n      %85 : Float(512)\r\n      %86 : Float(512)\r\n      %87 : Float(512, 512, 3, 3, 3)\r\n      %88 : Float(512)\r\n      %89 : Float(15, 512, 1, 1, 1)\r\n      %90 : Float(15)) {\r\n  %91 : Float(1, 1, 128, 128, 128), %92 : Handle = ^Scatter([0], None, 0)(%0), scope: DataParallel\r\n  %93 : Float(1, 64, 128, 128, 128) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%91, %1, %2), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[0]\r\n  %94 : Float(1, 64, 128, 128, 128) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%93, %3, %4, %5, %6), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[1]\r\n  %95 : Float(1, 64, 128, 128, 128) = Relu(%94), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[2]\r\n  %96 : Float(1, 64, 128, 128, 128) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%95, %7, %8), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[3]\r\n  %97 : Float(1, 64, 128, 128, 128) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%96, %9, %10, %11, %12), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[4]\r\n  %98 : Float(1, 64, 128, 128, 128) = Relu(%97), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[5]\r\n  %99 : Float(1, 64, 64, 64, 64) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%98, %13, %14), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[6]\r\n  %100 : Float(1, 128, 64, 64, 64) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%99, %15, %16), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[7]\r\n  %101 : Float(1, 128, 64, 64, 64) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%100, %17, %18, %19, %20), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[8]\r\n  %102 : Float(1, 128, 64, 64, 64) = Relu(%101), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[9]\r\n  %103 : Float(1, 128, 64, 64, 64) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%102, %21, %22), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[10]\r\n  %104 : Float(1, 128, 64, 64, 64) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%103, %23, %24, %25, %26), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[11]\r\n  %105 : Float(1, 128, 64, 64, 64) = Relu(%104), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[12]\r\n  %106 : Float(1, 128, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%105, %27, %28), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[13]\r\n  %107 : Float(1, 256, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%106, %29, %30), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[14]\r\n  %108 : Float(1, 256, 32, 32, 32) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%107, %31, %32, %33, %34), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[15]\r\n  %109 : Float(1, 256, 32, 32, 32) = Relu(%108), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[16]\r\n  %110 : Float(1, 256, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%109, %35, %36), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[17]\r\n  %111 : Float(1, 256, 32, 32, 32) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%110, %37, %38, %39, %40), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[18]\r\n  %112 : Float(1, 256, 32, 32, 32) = Relu(%111), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[19]\r\n  %113 : Float(1, 256, 32, 32, 32) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%112, %41, %42), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[20]\r\n  %114 : Float(1, 256, 32, 32, 32) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%113, %43, %44, %45, %46), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[21]\r\n  %115 : Float(1, 256, 32, 32, 32) = Relu(%114), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[22]\r\n  %116 : Float(1, 256, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%115, %47, %48), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[23]\r\n  %117 : Float(1, 512, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%116, %49, %50), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[24]\r\n  %118 : Float(1, 512, 16, 16, 16) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%117, %51, %52, %53, %54), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[25]\r\n  %119 : Float(1, 512, 16, 16, 16) = Relu(%118), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[26]\r\n  %120 : Float(1, 512, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%119, %55, %56), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[27]\r\n  %121 : Float(1, 512, 16, 16, 16) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%120, %57, %58, %59, %60), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[28]\r\n  %122 : Float(1, 512, 16, 16, 16) = Relu(%121), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[29]\r\n  %123 : Float(1, 512, 16, 16, 16) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%122, %61, %62), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[30]\r\n  %124 : Float(1, 512, 16, 16, 16) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%123, %63, %64, %65, %66), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[31]\r\n  %125 : Float(1, 512, 16, 16, 16) = Relu(%124), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[32]\r\n  %126 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[2, 2, 2], pads=[0, 0, 0, 0, 0, 0], strides=[2, 2, 2]](%125, %67, %68), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[33]\r\n  %127 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%126, %69, %70), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[34]\r\n  %128 : Float(1, 512, 8, 8, 8) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%127, %71, %72, %73, %74), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[35]\r\n  %129 : Float(1, 512, 8, 8, 8) = Relu(%128), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[36]\r\n  %130 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%129, %75, %76), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[37]\r\n  %131 : Float(1, 512, 8, 8, 8) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%130, %77, %78, %79, %80), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[38]\r\n  %132 : Float(1, 512, 8, 8, 8) = Relu(%131), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[39]\r\n  %133 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%132, %81, %82), scope: DataParallel/Net[module]/Sequential[layer]/Conv3d[40]\r\n  %134 : Float(1, 512, 8, 8, 8) = BatchNormalization[consumed_inputs=[0, 0, 0, 1, 1], epsilon=1e-05, is_test=1, momentum=0.9](%133, %83, %84, %85, %86), scope: DataParallel/Net[module]/Sequential[layer]/BatchNorm3d[41]\r\n  %135 : Float(1, 512, 8, 8, 8) = Relu(%134), scope: DataParallel/Net[module]/Sequential[layer]/ReLU[42]\r\n  %136 : Float(1, 512, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[3, 3, 3], pads=[1, 1, 1, 1, 1, 1], strides=[1, 1, 1]](%135, %87, %88), scope: DataParallel/Net[module]/Conv3d[rpn_conv33]\r\n  %137 : Float(1, 512, 8, 8, 8) = Relu(%136), scope: DataParallel/Net[module]/ReLU[rpn_relu33]\r\n  %138 : Float(1, 15, 8, 8, 8) = Conv[dilations=[1, 1, 1], group=1, kernel_shape=[1, 1, 1], pads=[0, 0, 0, 0, 0, 0], strides=[1, 1, 1]](%137, %89, %90), scope: DataParallel/Net[module]/Conv3d[rpn_score]\r\n  %139 : Float(1, 15, 512) = Reshape[shape=[1, 15, -1]](%138), scope: DataParallel/Net[module]\r\n  %140 : Float(1!, 512!, 15!) = Transpose[perm=[0, 2, 1]](%139), scope: DataParallel/Net[module]\r\n  %141 : Float(1, 8, 8, 8, 3, 5) = Reshape[shape=[1, 8, 8, 8, 3, 5]](%140), scope: DataParallel/Net[module]\r\n  return (%141);\r\n}\r\n"}