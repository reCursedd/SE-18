{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326510790", "html_url": "https://github.com/pytorch/pytorch/issues/2178#issuecomment-326510790", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2178", "id": 326510790, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjUxMDc5MA==", "user": {"login": "benjamin-work", "id": 29862381, "node_id": "MDQ6VXNlcjI5ODYyMzgx", "avatar_url": "https://avatars1.githubusercontent.com/u/29862381?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benjamin-work", "html_url": "https://github.com/benjamin-work", "followers_url": "https://api.github.com/users/benjamin-work/followers", "following_url": "https://api.github.com/users/benjamin-work/following{/other_user}", "gists_url": "https://api.github.com/users/benjamin-work/gists{/gist_id}", "starred_url": "https://api.github.com/users/benjamin-work/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benjamin-work/subscriptions", "organizations_url": "https://api.github.com/users/benjamin-work/orgs", "repos_url": "https://api.github.com/users/benjamin-work/repos", "events_url": "https://api.github.com/users/benjamin-work/events{/privacy}", "received_events_url": "https://api.github.com/users/benjamin-work/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-01T07:30:01Z", "updated_at": "2017-09-01T07:30:01Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>A fix / workaround is to make <code>y</code> to be <code>torch.zeros(100, 1)</code> and then squeeze the last dimension once you get the mini-batch.</p>\n</blockquote>\n<p>This is my current workaround.</p>\n<blockquote>\n<p>We can solve this once we get scalars</p>\n</blockquote>\n<p>Nice, then I will close this issue.</p>", "body_text": "A fix / workaround is to make y to be torch.zeros(100, 1) and then squeeze the last dimension once you get the mini-batch.\n\nThis is my current workaround.\n\nWe can solve this once we get scalars\n\nNice, then I will close this issue.", "body": "> A fix / workaround is to make `y` to be `torch.zeros(100, 1)` and then squeeze the last dimension once you get the mini-batch.\r\n\r\nThis is my current workaround.\r\n\r\n> We can solve this once we get scalars\r\n\r\nNice, then I will close this issue.\r\n"}