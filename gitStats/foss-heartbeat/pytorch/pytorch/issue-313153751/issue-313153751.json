{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6495", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6495/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6495/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6495/events", "html_url": "https://github.com/pytorch/pytorch/issues/6495", "id": 313153751, "node_id": "MDU6SXNzdWUzMTMxNTM3NTE=", "number": 6495, "title": "Embedding custom init with padding_idx fails to zero out padding_idx", "user": {"login": "williamFalcon", "id": 3640001, "node_id": "MDQ6VXNlcjM2NDAwMDE=", "avatar_url": "https://avatars1.githubusercontent.com/u/3640001?v=4", "gravatar_id": "", "url": "https://api.github.com/users/williamFalcon", "html_url": "https://github.com/williamFalcon", "followers_url": "https://api.github.com/users/williamFalcon/followers", "following_url": "https://api.github.com/users/williamFalcon/following{/other_user}", "gists_url": "https://api.github.com/users/williamFalcon/gists{/gist_id}", "starred_url": "https://api.github.com/users/williamFalcon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/williamFalcon/subscriptions", "organizations_url": "https://api.github.com/users/williamFalcon/orgs", "repos_url": "https://api.github.com/users/williamFalcon/repos", "events_url": "https://api.github.com/users/williamFalcon/events{/privacy}", "received_events_url": "https://api.github.com/users/williamFalcon/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-11T02:40:45Z", "updated_at": "2018-04-11T03:20:51Z", "closed_at": "2018-04-11T03:20:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The following fails to zero out padding_idx when performing the embedding op.<br>\nShouldn't the padding_idx also respect the custom init?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">WeightNormalizedEmbedding</span>(<span class=\"pl-smi\">num_embeddings</span>, <span class=\"pl-smi\">embedding_dim</span>, <span class=\"pl-smi\">padding_idx</span>):\n    m <span class=\"pl-k\">=</span> nn.Embedding(num_embeddings, embedding_dim, <span class=\"pl-v\">padding_idx</span><span class=\"pl-k\">=</span>padding_idx)\n    m.weight.data.normal_(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0.1</span>)\n    <span class=\"pl-k\">return</span> m\n\nx <span class=\"pl-k\">=</span> emb(x)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>        0.3452  0.4937 -0.9361</span>\n          <span class=\"pl-c1\">0.3452</span>  <span class=\"pl-c1\">0.4937</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.9361</span>\n          <span class=\"pl-c1\">0.3452</span>  <span class=\"pl-c1\">0.4937</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.9361</span>\n          <span class=\"pl-c1\">0.0706</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.1962</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6276</span></pre></div>\n<p>But it should be:</p>\n<div class=\"highlight highlight-source-python\"><pre>          <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>\n          <span class=\"pl-c1\">0.3452</span>  <span class=\"pl-c1\">0.4937</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.9361</span>\n          <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>  <span class=\"pl-c1\">0.0000</span>\n          <span class=\"pl-c1\">0.0706</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.1962</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6276</span></pre></div>", "body_text": "The following fails to zero out padding_idx when performing the embedding op.\nShouldn't the padding_idx also respect the custom init?\ndef WeightNormalizedEmbedding(num_embeddings, embedding_dim, padding_idx):\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    m.weight.data.normal_(0, 0.1)\n    return m\n\nx = emb(x)\n#        0.3452  0.4937 -0.9361\n          0.3452  0.4937 -0.9361\n          0.3452  0.4937 -0.9361\n          0.0706 -2.1962 -0.6276\nBut it should be:\n          0.0000  0.0000  0.0000\n          0.3452  0.4937 -0.9361\n          0.0000  0.0000  0.0000\n          0.0706 -2.1962 -0.6276", "body": "The following fails to zero out padding_idx when performing the embedding op. \r\nShouldn't the padding_idx also respect the custom init?\r\n\r\n```python   \r\ndef WeightNormalizedEmbedding(num_embeddings, embedding_dim, padding_idx):\r\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\r\n    m.weight.data.normal_(0, 0.1)\r\n    return m\r\n\r\nx = emb(x)\r\n#        0.3452  0.4937 -0.9361\r\n          0.3452  0.4937 -0.9361\r\n          0.3452  0.4937 -0.9361\r\n          0.0706 -2.1962 -0.6276\r\n```   \r\n\r\nBut it should be:     \r\n```python    \r\n          0.0000  0.0000  0.0000\r\n          0.3452  0.4937 -0.9361\r\n          0.0000  0.0000  0.0000\r\n          0.0706 -2.1962 -0.6276\r\n```"}