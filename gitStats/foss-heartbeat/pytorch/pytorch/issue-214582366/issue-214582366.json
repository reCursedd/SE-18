{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1010", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1010/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1010/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1010/events", "html_url": "https://github.com/pytorch/pytorch/issues/1010", "id": 214582366, "node_id": "MDU6SXNzdWUyMTQ1ODIzNjY=", "number": 1010, "title": "GPU goes away after an error occurs", "user": {"login": "jihunchoi", "id": 1898501, "node_id": "MDQ6VXNlcjE4OTg1MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1898501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jihunchoi", "html_url": "https://github.com/jihunchoi", "followers_url": "https://api.github.com/users/jihunchoi/followers", "following_url": "https://api.github.com/users/jihunchoi/following{/other_user}", "gists_url": "https://api.github.com/users/jihunchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jihunchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jihunchoi/subscriptions", "organizations_url": "https://api.github.com/users/jihunchoi/orgs", "repos_url": "https://api.github.com/users/jihunchoi/repos", "events_url": "https://api.github.com/users/jihunchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/jihunchoi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-03-16T02:35:47Z", "updated_at": "2018-09-09T15:30:35Z", "closed_at": "2017-03-19T12:07:03Z", "author_association": "CONTRIBUTOR", "body_html": "<ol>\n<li></li>\n</ol>\n<p>After upgrading to the latest source-code version, weird error messages occur when I write some wrong code.<br>\nFor example, when I run the below CPU-only codes, neat error messages occur:</p>\n<pre><code>emb = nn.Embedding(10, 10)\ninds = Variable(torch.LongTensor([1, -1]))\nemb(inds)\n</code></pre>\n<pre><code>RuntimeError: index out of range at /home/my_name/pytorch/torch/lib/TH/generic/THTensorMath.c:273\n</code></pre>\n<p>However, when I run the codes in GPU, the error messages become:</p>\n<pre><code>emb.cuda()\nemb(inds.cuda())\n</code></pre>\n<pre><code>RuntimeError: cuda runtime error (59) : device-side assert triggered at /home/my_name/pytorch/torch/lib/THC/generic/THCTensorCopy.c:65\n</code></pre>\n<p>which is much less comprehensible than the CPU-version codes.</p>\n<p>After some googling, I found the related issue submitted on cutorch repo: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"210355046\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/torch/cutorch/issues/708\" data-hovercard-type=\"issue\" data-hovercard-url=\"/torch/cutorch/issues/708/hovercard\" href=\"https://github.com/torch/cutorch/issues/708\">torch/cutorch#708</a>.</p>\n<ol start=\"2\">\n<li></li>\n</ol>\n<p>The first problem is OK, since I can anyway debug codes and fix the codes by adding some environment variables.<br>\nHowever, when I terminate the Python process where the error occurred, OS starts to ignore the GPU device, and the only way (as far as I know) to revive the GPU is to reboot the entire system.<br>\nSpecifically, when I run <code>nvidia-smi</code> after terminating the process, OS cannot find the device:</p>\n<pre><code>RuntimeError: cuda runtime error (59) : device-side assert triggered at /home/my_name/pytorch/torch/lib/THC/generic/THCTensorCopy.c:65\n\nIn [10]: !nvidia-smi\nThu Mar 16 11:27:20 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1070    Off  | 0000:01:00.0     Off |                  N/A |\n|  0%   46C    P2    45W / 166W |    237MiB /  8113MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1802    C   /home/my_name/anaconda3/bin/python              235MiB |\n+-----------------------------------------------------------------------------+\n\nIn [11]:\nDo you really want to exit ([y]/n)? y\n\n\n~ \u231a 11:27:32\n$ nvidia-smi\nNo devices were found\n</code></pre>", "body_text": "After upgrading to the latest source-code version, weird error messages occur when I write some wrong code.\nFor example, when I run the below CPU-only codes, neat error messages occur:\nemb = nn.Embedding(10, 10)\ninds = Variable(torch.LongTensor([1, -1]))\nemb(inds)\n\nRuntimeError: index out of range at /home/my_name/pytorch/torch/lib/TH/generic/THTensorMath.c:273\n\nHowever, when I run the codes in GPU, the error messages become:\nemb.cuda()\nemb(inds.cuda())\n\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /home/my_name/pytorch/torch/lib/THC/generic/THCTensorCopy.c:65\n\nwhich is much less comprehensible than the CPU-version codes.\nAfter some googling, I found the related issue submitted on cutorch repo: torch/cutorch#708.\n\n\n\nThe first problem is OK, since I can anyway debug codes and fix the codes by adding some environment variables.\nHowever, when I terminate the Python process where the error occurred, OS starts to ignore the GPU device, and the only way (as far as I know) to revive the GPU is to reboot the entire system.\nSpecifically, when I run nvidia-smi after terminating the process, OS cannot find the device:\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /home/my_name/pytorch/torch/lib/THC/generic/THCTensorCopy.c:65\n\nIn [10]: !nvidia-smi\nThu Mar 16 11:27:20 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1070    Off  | 0000:01:00.0     Off |                  N/A |\n|  0%   46C    P2    45W / 166W |    237MiB /  8113MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1802    C   /home/my_name/anaconda3/bin/python              235MiB |\n+-----------------------------------------------------------------------------+\n\nIn [11]:\nDo you really want to exit ([y]/n)? y\n\n\n~ \u231a 11:27:32\n$ nvidia-smi\nNo devices were found", "body": "1)\r\nAfter upgrading to the latest source-code version, weird error messages occur when I write some wrong code.\r\nFor example, when I run the below CPU-only codes, neat error messages occur:\r\n```\r\nemb = nn.Embedding(10, 10)\r\ninds = Variable(torch.LongTensor([1, -1]))\r\nemb(inds)\r\n```\r\n\r\n```\r\nRuntimeError: index out of range at /home/my_name/pytorch/torch/lib/TH/generic/THTensorMath.c:273\r\n```\r\n\r\nHowever, when I run the codes in GPU, the error messages become:\r\n```\r\nemb.cuda()\r\nemb(inds.cuda())\r\n```\r\n\r\n```\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /home/my_name/pytorch/torch/lib/THC/generic/THCTensorCopy.c:65\r\n```\r\nwhich is much less comprehensible than the CPU-version codes.\r\n\r\nAfter some googling, I found the related issue submitted on cutorch repo: https://github.com/torch/cutorch/issues/708.\r\n\r\n2)\r\nThe first problem is OK, since I can anyway debug codes and fix the codes by adding some environment variables.\r\nHowever, when I terminate the Python process where the error occurred, OS starts to ignore the GPU device, and the only way (as far as I know) to revive the GPU is to reboot the entire system.\r\nSpecifically, when I run `nvidia-smi` after terminating the process, OS cannot find the device:\r\n\r\n```\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /home/my_name/pytorch/torch/lib/THC/generic/THCTensorCopy.c:65\r\n\r\nIn [10]: !nvidia-smi\r\nThu Mar 16 11:27:20 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.44                 Driver Version: 367.44                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    Off  | 0000:01:00.0     Off |                  N/A |\r\n|  0%   46C    P2    45W / 166W |    237MiB /  8113MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1802    C   /home/my_name/anaconda3/bin/python              235MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nIn [11]:\r\nDo you really want to exit ([y]/n)? y\r\n\r\n\r\n~ \u231a 11:27:32\r\n$ nvidia-smi\r\nNo devices were found\r\n```"}