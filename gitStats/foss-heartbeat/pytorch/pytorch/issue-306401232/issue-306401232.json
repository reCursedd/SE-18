{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5871", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5871/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5871/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5871/events", "html_url": "https://github.com/pytorch/pytorch/issues/5871", "id": 306401232, "node_id": "MDU6SXNzdWUzMDY0MDEyMzI=", "number": 5871, "title": "Error in Function backward when forward output is in-place of forward input and input is not leaf", "user": {"login": "garibarba", "id": 13115728, "node_id": "MDQ6VXNlcjEzMTE1NzI4", "avatar_url": "https://avatars3.githubusercontent.com/u/13115728?v=4", "gravatar_id": "", "url": "https://api.github.com/users/garibarba", "html_url": "https://github.com/garibarba", "followers_url": "https://api.github.com/users/garibarba/followers", "following_url": "https://api.github.com/users/garibarba/following{/other_user}", "gists_url": "https://api.github.com/users/garibarba/gists{/gist_id}", "starred_url": "https://api.github.com/users/garibarba/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/garibarba/subscriptions", "organizations_url": "https://api.github.com/users/garibarba/orgs", "repos_url": "https://api.github.com/users/garibarba/repos", "events_url": "https://api.github.com/users/garibarba/events{/privacy}", "received_events_url": "https://api.github.com/users/garibarba/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-19T10:14:27Z", "updated_at": "2018-03-19T16:14:48Z", "closed_at": "2018-03-19T15:32:02Z", "author_association": "NONE", "body_html": "<ul>\n<li>OS: Win10</li>\n<li>PyTorch version: '0.3.1.post2'</li>\n<li>How you installed PyTorch (conda, pip, source):  <code>conda install -c peterjc123 pytorch</code></li>\n<li>Python version: Python 3.6.4 :: Anaconda custom (64-bit)</li>\n</ul>\n<p>I'm using in-place addition on slices to save memory. To do that I'm defining my own <code>Function</code>.<br>\nHere is a minimal example reproducing my problem, and the workaround I've found.</p>\n<pre><code>import torch\nfrom torch.autograd import Variable, Function\n\nclass InPlaceAdd(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        a[1:2] += b\n        return a\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output, grad_output[1:2].view(1)\n\nadd_in_place = InPlaceAdd.apply\n</code></pre>\n<p>This fails to compute the gradient on <code>b</code>:</p>\n<pre><code>a = Variable(torch.ones(2), requires_grad=True)\nb = Variable(torch.ones(1), requires_grad=True)\n\nc = add_in_place(a + 1, b)\n\nc.sum().backward()\n\nprint(a.grad, b.grad)\n</code></pre>\n<ul>\n<li>output:</li>\n</ul>\n<pre><code>Variable containing:\n 1\n 1\n[torch.FloatTensor of size 2]\n None\n</code></pre>\n<p>while this works just fine:</p>\n<pre><code>a = Variable(torch.ones(2), requires_grad=True)\nb = Variable(torch.ones(1), requires_grad=True)\n\nc = add_in_place(a, b)\n\nc.sum().backward()\n\nprint(a.grad, b.grad)\n</code></pre>\n<ul>\n<li>output:</li>\n</ul>\n<pre><code>Variable containing:\n 1\n 1\n[torch.FloatTensor of size 2]\n Variable containing:\n 1\n[torch.FloatTensor of size 1]\n</code></pre>\n<hr>\n<p>The workaround I've found is returning a <code>view</code> to the output <code>Tensor</code> in <code>forward</code> instead of the input <code>tensor</code> itself. With this change, the problem is gone.<br>\n<code>return a</code> =&gt; <code>return a.view_as(a)</code></p>", "body_text": "OS: Win10\nPyTorch version: '0.3.1.post2'\nHow you installed PyTorch (conda, pip, source):  conda install -c peterjc123 pytorch\nPython version: Python 3.6.4 :: Anaconda custom (64-bit)\n\nI'm using in-place addition on slices to save memory. To do that I'm defining my own Function.\nHere is a minimal example reproducing my problem, and the workaround I've found.\nimport torch\nfrom torch.autograd import Variable, Function\n\nclass InPlaceAdd(Function):\n    @staticmethod\n    def forward(ctx, a, b):\n        a[1:2] += b\n        return a\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output, grad_output[1:2].view(1)\n\nadd_in_place = InPlaceAdd.apply\n\nThis fails to compute the gradient on b:\na = Variable(torch.ones(2), requires_grad=True)\nb = Variable(torch.ones(1), requires_grad=True)\n\nc = add_in_place(a + 1, b)\n\nc.sum().backward()\n\nprint(a.grad, b.grad)\n\n\noutput:\n\nVariable containing:\n 1\n 1\n[torch.FloatTensor of size 2]\n None\n\nwhile this works just fine:\na = Variable(torch.ones(2), requires_grad=True)\nb = Variable(torch.ones(1), requires_grad=True)\n\nc = add_in_place(a, b)\n\nc.sum().backward()\n\nprint(a.grad, b.grad)\n\n\noutput:\n\nVariable containing:\n 1\n 1\n[torch.FloatTensor of size 2]\n Variable containing:\n 1\n[torch.FloatTensor of size 1]\n\n\nThe workaround I've found is returning a view to the output Tensor in forward instead of the input tensor itself. With this change, the problem is gone.\nreturn a => return a.view_as(a)", "body": "- OS: Win10\r\n- PyTorch version: '0.3.1.post2'\r\n- How you installed PyTorch (conda, pip, source):  `conda install -c peterjc123 pytorch` \r\n- Python version: Python 3.6.4 :: Anaconda custom (64-bit)\r\n\r\nI'm using in-place addition on slices to save memory. To do that I'm defining my own `Function`.\r\nHere is a minimal example reproducing my problem, and the workaround I've found.\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable, Function\r\n\r\nclass InPlaceAdd(Function):\r\n    @staticmethod\r\n    def forward(ctx, a, b):\r\n        a[1:2] += b\r\n        return a\r\n    \r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        return grad_output, grad_output[1:2].view(1)\r\n\r\nadd_in_place = InPlaceAdd.apply\r\n```\r\n\r\nThis fails to compute the gradient on `b`:\r\n```\r\na = Variable(torch.ones(2), requires_grad=True)\r\nb = Variable(torch.ones(1), requires_grad=True)\r\n\r\nc = add_in_place(a + 1, b)\r\n\r\nc.sum().backward()\r\n\r\nprint(a.grad, b.grad)\r\n```\r\n* output:\r\n```\r\nVariable containing:\r\n 1\r\n 1\r\n[torch.FloatTensor of size 2]\r\n None\r\n```\r\n\r\n\r\nwhile this works just fine:\r\n```\r\na = Variable(torch.ones(2), requires_grad=True)\r\nb = Variable(torch.ones(1), requires_grad=True)\r\n\r\nc = add_in_place(a, b)\r\n\r\nc.sum().backward()\r\n\r\nprint(a.grad, b.grad)\r\n```\r\n* output:\r\n```\r\nVariable containing:\r\n 1\r\n 1\r\n[torch.FloatTensor of size 2]\r\n Variable containing:\r\n 1\r\n[torch.FloatTensor of size 1]\r\n```\r\n\r\n---------------------------------\r\nThe workaround I've found is returning a `view` to the output `Tensor` in `forward` instead of the input `tensor` itself. With this change, the problem is gone.\r\n`return a` => `return a.view_as(a)`"}