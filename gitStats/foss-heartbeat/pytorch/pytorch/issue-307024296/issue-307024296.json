{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5912", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5912/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5912/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5912/events", "html_url": "https://github.com/pytorch/pytorch/issues/5912", "id": 307024296, "node_id": "MDU6SXNzdWUzMDcwMjQyOTY=", "number": 5912, "title": "Conv-RNN combination slow in backward pass", "user": {"login": "adaitche", "id": 25004501, "node_id": "MDQ6VXNlcjI1MDA0NTAx", "avatar_url": "https://avatars0.githubusercontent.com/u/25004501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adaitche", "html_url": "https://github.com/adaitche", "followers_url": "https://api.github.com/users/adaitche/followers", "following_url": "https://api.github.com/users/adaitche/following{/other_user}", "gists_url": "https://api.github.com/users/adaitche/gists{/gist_id}", "starred_url": "https://api.github.com/users/adaitche/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adaitche/subscriptions", "organizations_url": "https://api.github.com/users/adaitche/orgs", "repos_url": "https://api.github.com/users/adaitche/repos", "events_url": "https://api.github.com/users/adaitche/events{/privacy}", "received_events_url": "https://api.github.com/users/adaitche/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-03-20T20:22:47Z", "updated_at": "2018-05-14T19:40:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Models used for processing audio signals often combine a Conv layer with a GRU/LSTM layer. In pytorch, this kind of combination seems to yield an unexpectedly long backward pass. Here is a minimal model of this kind:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">PyTorchModel</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_channels</span>, <span class=\"pl-smi\">kernel_size</span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.conv <span class=\"pl-k\">=</span> nn.Conv1d(num_channels, num_channels, kernel_size, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>((kernel_size <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span>))\n        <span class=\"pl-c1\">self</span>.gru <span class=\"pl-k\">=</span> nn.GRU(num_channels, num_channels, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.fc <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.Linear(num_channels, <span class=\"pl-c1\">1</span>),\n            nn.Sigmoid()\n        )\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inp</span>, <span class=\"pl-smi\">do_conv</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">do_gru</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv(inp)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> (batchsize, num_channels, seq_length) -&gt; (batchsize, seq_length, num_channels)</span>\n        out <span class=\"pl-k\">=</span> out.transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>)\n        out, _ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.gru(out)\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc(out)\n        <span class=\"pl-k\">return</span> out</pre></div>\n<p>The script provided at the end benchmarks this model and compares the execution times to keras (with tensorflow backend). Here are the benchmark results (obtained on a MacBook Pro with the CPU i7-4980HQ):</p>\n<pre><code>data for pytorch: X.shape=(32, 128, 2048), Y.shape=(32, 2048, 1)\ndata for keras: X.shape=(32, 2048, 128), Y.shape=(32, 2048, 1)\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\nTesting Conv and GRU together\npytorch: forward_pass=1.3s backward_pass=40.0s\nkeras: forward_pass=0.7s backward_pass=2.5s\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\nTesting Conv only\npytorch: forward_pass=0.5s backward_pass=0.2s\nkeras: forward_pass=0.3s backward_pass=0.6s\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\nTesting GRU only\npytorch: forward_pass=0.8s backward_pass=0.9s\nkeras: forward_pass=0.4s backward_pass=1.8s\n&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n</code></pre>\n<p>The backward pass for the Conv-GRU model takes 16 times longer in pytorch as compared to keras. There is no such strong difference for a model with only a Conv or a GRU layer. Hence the problems seems to stem from the combination.</p>\n<p>An apparent difference between the pytorch and keras implementations is that in pytorch there is a transpose operation between the Conv and the GRU layers. My initial guess was that this is the source of the problem. However, in the benchmark above the cases \"Conv only\" and \"GRU only\" also contain a transpose operation (see benchmark script), which doesn't seem to influence the performance negatively.</p>\n<p>The results are similar when the GRU is replaced by a LSTM or RNN layer.</p>\n<p>Information about the used setup:</p>\n<ul>\n<li>OS: OSX (but tested also on Linux, with similar results)</li>\n<li>PyTorch version: 0.3.1.post2</li>\n<li>How you installed PyTorch: <code>conda install pytorch -c pytorch </code></li>\n<li>Python version: 3.6.3</li>\n<li>CUDA/cuDNN version: no CUDA installed</li>\n</ul>\n<p>Script used for benchmarking:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n\n<span class=\"pl-k\">import</span> keras.models\n<span class=\"pl-k\">from</span> keras.layers <span class=\"pl-k\">import</span> Input, Conv1D, <span class=\"pl-c1\">GRU</span>, Dense, TimeDistributed\n<span class=\"pl-k\">from</span> keras.optimizers <span class=\"pl-k\">import</span> <span class=\"pl-c1\">SGD</span>\n\n\nbatchsize <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nseq_length <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2048</span>\nnum_channels <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\nkernel_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">15</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">generate_training_data</span>():\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> for pytorch</span>\n    X <span class=\"pl-k\">=</span> torch.randn((batchsize, num_channels, seq_length))\n    Y <span class=\"pl-k\">=</span> torch.rand((batchsize, seq_length, <span class=\"pl-c1\">1</span>)).round()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> for keray</span>\n    Xnp <span class=\"pl-k\">=</span> X.transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>).clone().numpy()\n    Ynp <span class=\"pl-k\">=</span> Y.clone().numpy()\n\n    <span class=\"pl-k\">return</span> X, Y, Xnp, Ynp\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">PyTorchModel</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.conv <span class=\"pl-k\">=</span> nn.Conv1d(num_channels, num_channels, kernel_size, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>((kernel_size <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span>))\n        <span class=\"pl-c1\">self</span>.gru <span class=\"pl-k\">=</span> nn.GRU(num_channels, num_channels, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.fc <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.Linear(num_channels, <span class=\"pl-c1\">1</span>),\n            nn.Sigmoid()\n        )\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inp</span>, <span class=\"pl-smi\">do_conv</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">do_gru</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-k\">if</span> do_conv:\n            out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv(inp)\n        <span class=\"pl-k\">else</span>:\n            out <span class=\"pl-k\">=</span> inp\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> (batchsize, num_channels, seq_length) -&gt; (batchsize, seq_length, num_channels)</span>\n        out <span class=\"pl-k\">=</span> out.transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>)\n\n        <span class=\"pl-k\">if</span> do_gru:\n            out, _ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.gru(out)\n\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc(out)\n        <span class=\"pl-k\">return</span> out\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_keras_model</span>(<span class=\"pl-smi\">do_conv</span>, <span class=\"pl-smi\">do_gru</span>):\n    inp <span class=\"pl-k\">=</span> Input((seq_length, num_channels))\n    out <span class=\"pl-k\">=</span> inp\n\n    <span class=\"pl-k\">if</span> do_conv:\n        out <span class=\"pl-k\">=</span> Conv1D(num_channels, kernel_size, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>same<span class=\"pl-pds\">\"</span></span>)(out)\n\n    <span class=\"pl-k\">if</span> do_gru:\n        out <span class=\"pl-k\">=</span> GRU(num_channels, <span class=\"pl-v\">return_sequences</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)(out)\n\n    out <span class=\"pl-k\">=</span> TimeDistributed(Dense(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>sigmoid<span class=\"pl-pds\">\"</span></span>))(out)\n\n    <span class=\"pl-k\">return</span> keras.models.Model(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>inp, <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>out)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">benchmark_pytorch</span>(<span class=\"pl-smi\">X</span>, <span class=\"pl-smi\">Y</span>, <span class=\"pl-smi\">do_conv</span>, <span class=\"pl-smi\">do_gru</span>, <span class=\"pl-smi\">num_iter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Do the foward and backward pass `num_iter` times and return the average runtime.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    loss_fn <span class=\"pl-k\">=</span> nn.BCELoss()\n    model <span class=\"pl-k\">=</span> PyTorchModel()\n\n    forward_time <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    backward_time <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_iter):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> do forward pass</span>\n        start <span class=\"pl-k\">=</span> time.time()\n        out <span class=\"pl-k\">=</span> model(Variable(X), <span class=\"pl-v\">do_gru</span><span class=\"pl-k\">=</span>do_gru, <span class=\"pl-v\">do_conv</span><span class=\"pl-k\">=</span>do_conv)\n        forward_time <span class=\"pl-k\">+=</span> time.time() <span class=\"pl-k\">-</span> start\n\n        loss <span class=\"pl-k\">=</span> loss_fn(out, Variable(Y))\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> do backward pass</span>\n        start <span class=\"pl-k\">=</span> time.time()\n        loss.backward()\n        backward_time <span class=\"pl-k\">+=</span> time.time() <span class=\"pl-k\">-</span> start\n\n    <span class=\"pl-k\">return</span> forward_time <span class=\"pl-k\">/</span> num_iter, backward_time <span class=\"pl-k\">/</span> num_iter\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">benchmark_keras</span>(<span class=\"pl-smi\">X</span>, <span class=\"pl-smi\">Y</span>, <span class=\"pl-smi\">do_conv</span>, <span class=\"pl-smi\">do_gru</span>, <span class=\"pl-smi\">num_iter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Do the foward and backward pass `num_iter` times and return the average runtime.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    forward_time <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    backward_time <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    model <span class=\"pl-k\">=</span> get_keras_model(<span class=\"pl-v\">do_conv</span><span class=\"pl-k\">=</span>do_conv, <span class=\"pl-v\">do_gru</span><span class=\"pl-k\">=</span>do_gru)\n    model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>binary_crossentropy<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>SGD(<span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>))\n\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_iter):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> do forward pass</span>\n        start <span class=\"pl-k\">=</span> time.time()\n        model.predict(X)\n        forward_time <span class=\"pl-k\">+=</span> time.time() <span class=\"pl-k\">-</span> start\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> do backward pass</span>\n        start <span class=\"pl-k\">=</span> time.time()\n        model.train_on_batch(X, Y)\n        backward_time <span class=\"pl-k\">+=</span> time.time() <span class=\"pl-k\">-</span> start\n\n    <span class=\"pl-k\">return</span> forward_time <span class=\"pl-k\">/</span> num_iter, backward_time <span class=\"pl-k\">/</span> num_iter\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    X, Y, Xnp, Ynp <span class=\"pl-k\">=</span> generate_training_data()\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>data for pytorch: X.shape=<span class=\"pl-c1\">{}</span>, Y.shape=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-c1\">tuple</span>(X.shape), <span class=\"pl-c1\">tuple</span>(Y.shape)))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>data for keras: X.shape=<span class=\"pl-c1\">{}</span>, Y.shape=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-c1\">tuple</span>(Xnp.shape), <span class=\"pl-c1\">tuple</span>(Ynp.shape)))\n\n    parameters <span class=\"pl-k\">=</span> [\n        (<span class=\"pl-c1\">True</span>, <span class=\"pl-c1\">True</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Testing Conv and GRU together<span class=\"pl-pds\">\"</span></span>),\n        (<span class=\"pl-c1\">True</span>, <span class=\"pl-c1\">False</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Testing Conv only<span class=\"pl-pds\">\"</span></span>),\n        (<span class=\"pl-c1\">False</span>, <span class=\"pl-c1\">True</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Testing GRU only<span class=\"pl-pds\">\"</span></span>),\n    ]\n\n    <span class=\"pl-k\">for</span> do_conv, do_gru, description <span class=\"pl-k\">in</span> parameters:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&gt;<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">50</span>)\n        <span class=\"pl-c1\">print</span>(description)\n\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>pytorch: forward_pass=<span class=\"pl-c1\">{<span class=\"pl-k\">:.1f</span>}</span>s backward_pass=<span class=\"pl-c1\">{<span class=\"pl-k\">:.1f</span>}</span>s<span class=\"pl-pds\">\"</span></span>.format(\n            <span class=\"pl-k\">*</span>benchmark_pytorch(X, Y, <span class=\"pl-v\">do_conv</span><span class=\"pl-k\">=</span>do_conv, <span class=\"pl-v\">do_gru</span><span class=\"pl-k\">=</span>do_gru)\n        ))\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>keras: forward_pass=<span class=\"pl-c1\">{<span class=\"pl-k\">:.1f</span>}</span>s backward_pass=<span class=\"pl-c1\">{<span class=\"pl-k\">:.1f</span>}</span>s<span class=\"pl-pds\">\"</span></span>.format(\n            <span class=\"pl-k\">*</span>benchmark_keras(Xnp, Ynp, <span class=\"pl-v\">do_conv</span><span class=\"pl-k\">=</span>do_conv, <span class=\"pl-v\">do_gru</span><span class=\"pl-k\">=</span>do_gru)\n        ))\n\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">50</span>)</pre></div>", "body_text": "Models used for processing audio signals often combine a Conv layer with a GRU/LSTM layer. In pytorch, this kind of combination seems to yield an unexpectedly long backward pass. Here is a minimal model of this kind:\nclass PyTorchModel(nn.Module):\n    def __init__(self, num_channels, kernel_size):\n        super().__init__()\n\n        self.conv = nn.Conv1d(num_channels, num_channels, kernel_size, padding=int((kernel_size - 1) / 2))\n        self.gru = nn.GRU(num_channels, num_channels, batch_first=True)\n        self.fc = nn.Sequential(\n            nn.Linear(num_channels, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, inp, do_conv=True, do_gru=True):\n        out = self.conv(inp)\n        # (batchsize, num_channels, seq_length) -> (batchsize, seq_length, num_channels)\n        out = out.transpose(1, 2)\n        out, _ = self.gru(out)\n        out = self.fc(out)\n        return out\nThe script provided at the end benchmarks this model and compares the execution times to keras (with tensorflow backend). Here are the benchmark results (obtained on a MacBook Pro with the CPU i7-4980HQ):\ndata for pytorch: X.shape=(32, 128, 2048), Y.shape=(32, 2048, 1)\ndata for keras: X.shape=(32, 2048, 128), Y.shape=(32, 2048, 1)\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nTesting Conv and GRU together\npytorch: forward_pass=1.3s backward_pass=40.0s\nkeras: forward_pass=0.7s backward_pass=2.5s\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nTesting Conv only\npytorch: forward_pass=0.5s backward_pass=0.2s\nkeras: forward_pass=0.3s backward_pass=0.6s\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nTesting GRU only\npytorch: forward_pass=0.8s backward_pass=0.9s\nkeras: forward_pass=0.4s backward_pass=1.8s\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\nThe backward pass for the Conv-GRU model takes 16 times longer in pytorch as compared to keras. There is no such strong difference for a model with only a Conv or a GRU layer. Hence the problems seems to stem from the combination.\nAn apparent difference between the pytorch and keras implementations is that in pytorch there is a transpose operation between the Conv and the GRU layers. My initial guess was that this is the source of the problem. However, in the benchmark above the cases \"Conv only\" and \"GRU only\" also contain a transpose operation (see benchmark script), which doesn't seem to influence the performance negatively.\nThe results are similar when the GRU is replaced by a LSTM or RNN layer.\nInformation about the used setup:\n\nOS: OSX (but tested also on Linux, with similar results)\nPyTorch version: 0.3.1.post2\nHow you installed PyTorch: conda install pytorch -c pytorch \nPython version: 3.6.3\nCUDA/cuDNN version: no CUDA installed\n\nScript used for benchmarking:\nimport time\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\n\nimport keras.models\nfrom keras.layers import Input, Conv1D, GRU, Dense, TimeDistributed\nfrom keras.optimizers import SGD\n\n\nbatchsize = 32\nseq_length = 2048\nnum_channels = 128\nkernel_size = 15\n\n\ndef generate_training_data():\n    # for pytorch\n    X = torch.randn((batchsize, num_channels, seq_length))\n    Y = torch.rand((batchsize, seq_length, 1)).round()\n\n    # for keray\n    Xnp = X.transpose(1, 2).clone().numpy()\n    Ynp = Y.clone().numpy()\n\n    return X, Y, Xnp, Ynp\n\n\nclass PyTorchModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.conv = nn.Conv1d(num_channels, num_channels, kernel_size, padding=int((kernel_size - 1) / 2))\n        self.gru = nn.GRU(num_channels, num_channels, batch_first=True)\n        self.fc = nn.Sequential(\n            nn.Linear(num_channels, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, inp, do_conv=True, do_gru=True):\n        if do_conv:\n            out = self.conv(inp)\n        else:\n            out = inp\n\n        # (batchsize, num_channels, seq_length) -> (batchsize, seq_length, num_channels)\n        out = out.transpose(1, 2)\n\n        if do_gru:\n            out, _ = self.gru(out)\n\n        out = self.fc(out)\n        return out\n\n\ndef get_keras_model(do_conv, do_gru):\n    inp = Input((seq_length, num_channels))\n    out = inp\n\n    if do_conv:\n        out = Conv1D(num_channels, kernel_size, padding=\"same\")(out)\n\n    if do_gru:\n        out = GRU(num_channels, return_sequences=True)(out)\n\n    out = TimeDistributed(Dense(1, activation=\"sigmoid\"))(out)\n\n    return keras.models.Model(inputs=inp, outputs=out)\n\n\ndef benchmark_pytorch(X, Y, do_conv, do_gru, num_iter=5):\n    \"\"\"\n    Do the foward and backward pass `num_iter` times and return the average runtime.\n    \"\"\"\n    loss_fn = nn.BCELoss()\n    model = PyTorchModel()\n\n    forward_time = 0\n    backward_time = 0\n\n    for _ in range(num_iter):\n        # do forward pass\n        start = time.time()\n        out = model(Variable(X), do_gru=do_gru, do_conv=do_conv)\n        forward_time += time.time() - start\n\n        loss = loss_fn(out, Variable(Y))\n\n        # do backward pass\n        start = time.time()\n        loss.backward()\n        backward_time += time.time() - start\n\n    return forward_time / num_iter, backward_time / num_iter\n\n\ndef benchmark_keras(X, Y, do_conv, do_gru, num_iter=5):\n    \"\"\"\n    Do the foward and backward pass `num_iter` times and return the average runtime.\n    \"\"\"\n    forward_time = 0\n    backward_time = 0\n\n    model = get_keras_model(do_conv=do_conv, do_gru=do_gru)\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.001))\n\n    for _ in range(num_iter):\n        # do forward pass\n        start = time.time()\n        model.predict(X)\n        forward_time += time.time() - start\n\n        # do backward pass\n        start = time.time()\n        model.train_on_batch(X, Y)\n        backward_time += time.time() - start\n\n    return forward_time / num_iter, backward_time / num_iter\n\n\nif __name__ == '__main__':\n    X, Y, Xnp, Ynp = generate_training_data()\n\n    print(\"\")\n    print(\"data for pytorch: X.shape={}, Y.shape={}\".format(tuple(X.shape), tuple(Y.shape)))\n    print(\"data for keras: X.shape={}, Y.shape={}\".format(tuple(Xnp.shape), tuple(Ynp.shape)))\n\n    parameters = [\n        (True, True, \"Testing Conv and GRU together\"),\n        (True, False, \"Testing Conv only\"),\n        (False, True, \"Testing GRU only\"),\n    ]\n\n    for do_conv, do_gru, description in parameters:\n        print(\"\\n\" + \">\" * 50)\n        print(description)\n\n        print(\"pytorch: forward_pass={:.1f}s backward_pass={:.1f}s\".format(\n            *benchmark_pytorch(X, Y, do_conv=do_conv, do_gru=do_gru)\n        ))\n        print(\"keras: forward_pass={:.1f}s backward_pass={:.1f}s\".format(\n            *benchmark_keras(Xnp, Ynp, do_conv=do_conv, do_gru=do_gru)\n        ))\n\n        print(\"<\" * 50)", "body": "Models used for processing audio signals often combine a Conv layer with a GRU/LSTM layer. In pytorch, this kind of combination seems to yield an unexpectedly long backward pass. Here is a minimal model of this kind:\r\n\r\n```python\r\nclass PyTorchModel(nn.Module):\r\n    def __init__(self, num_channels, kernel_size):\r\n        super().__init__()\r\n\r\n        self.conv = nn.Conv1d(num_channels, num_channels, kernel_size, padding=int((kernel_size - 1) / 2))\r\n        self.gru = nn.GRU(num_channels, num_channels, batch_first=True)\r\n        self.fc = nn.Sequential(\r\n            nn.Linear(num_channels, 1),\r\n            nn.Sigmoid()\r\n        )\r\n\r\n    def forward(self, inp, do_conv=True, do_gru=True):\r\n        out = self.conv(inp)\r\n        # (batchsize, num_channels, seq_length) -> (batchsize, seq_length, num_channels)\r\n        out = out.transpose(1, 2)\r\n        out, _ = self.gru(out)\r\n        out = self.fc(out)\r\n        return out\r\n```\r\n\r\nThe script provided at the end benchmarks this model and compares the execution times to keras (with tensorflow backend). Here are the benchmark results (obtained on a MacBook Pro with the CPU i7-4980HQ):\r\n\r\n```\r\ndata for pytorch: X.shape=(32, 128, 2048), Y.shape=(32, 2048, 1)\r\ndata for keras: X.shape=(32, 2048, 128), Y.shape=(32, 2048, 1)\r\n\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\nTesting Conv and GRU together\r\npytorch: forward_pass=1.3s backward_pass=40.0s\r\nkeras: forward_pass=0.7s backward_pass=2.5s\r\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\nTesting Conv only\r\npytorch: forward_pass=0.5s backward_pass=0.2s\r\nkeras: forward_pass=0.3s backward_pass=0.6s\r\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n\r\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\r\nTesting GRU only\r\npytorch: forward_pass=0.8s backward_pass=0.9s\r\nkeras: forward_pass=0.4s backward_pass=1.8s\r\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n```\r\n\r\nThe backward pass for the Conv-GRU model takes 16 times longer in pytorch as compared to keras. There is no such strong difference for a model with only a Conv or a GRU layer. Hence the problems seems to stem from the combination. \r\n\r\nAn apparent difference between the pytorch and keras implementations is that in pytorch there is a transpose operation between the Conv and the GRU layers. My initial guess was that this is the source of the problem. However, in the benchmark above the cases \"Conv only\" and \"GRU only\" also contain a transpose operation (see benchmark script), which doesn't seem to influence the performance negatively.\r\n\r\nThe results are similar when the GRU is replaced by a LSTM or RNN layer.\r\n\r\nInformation about the used setup:\r\n- OS: OSX (but tested also on Linux, with similar results)\r\n- PyTorch version: 0.3.1.post2\r\n- How you installed PyTorch: `conda install pytorch -c pytorch `\r\n- Python version: 3.6.3\r\n- CUDA/cuDNN version: no CUDA installed\r\n\r\nScript used for benchmarking:\r\n```python\r\nimport time\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\nfrom torch import nn\r\n\r\nimport keras.models\r\nfrom keras.layers import Input, Conv1D, GRU, Dense, TimeDistributed\r\nfrom keras.optimizers import SGD\r\n\r\n\r\nbatchsize = 32\r\nseq_length = 2048\r\nnum_channels = 128\r\nkernel_size = 15\r\n\r\n\r\ndef generate_training_data():\r\n    # for pytorch\r\n    X = torch.randn((batchsize, num_channels, seq_length))\r\n    Y = torch.rand((batchsize, seq_length, 1)).round()\r\n\r\n    # for keray\r\n    Xnp = X.transpose(1, 2).clone().numpy()\r\n    Ynp = Y.clone().numpy()\r\n\r\n    return X, Y, Xnp, Ynp\r\n\r\n\r\nclass PyTorchModel(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.conv = nn.Conv1d(num_channels, num_channels, kernel_size, padding=int((kernel_size - 1) / 2))\r\n        self.gru = nn.GRU(num_channels, num_channels, batch_first=True)\r\n        self.fc = nn.Sequential(\r\n            nn.Linear(num_channels, 1),\r\n            nn.Sigmoid()\r\n        )\r\n\r\n    def forward(self, inp, do_conv=True, do_gru=True):\r\n        if do_conv:\r\n            out = self.conv(inp)\r\n        else:\r\n            out = inp\r\n\r\n        # (batchsize, num_channels, seq_length) -> (batchsize, seq_length, num_channels)\r\n        out = out.transpose(1, 2)\r\n\r\n        if do_gru:\r\n            out, _ = self.gru(out)\r\n\r\n        out = self.fc(out)\r\n        return out\r\n\r\n\r\ndef get_keras_model(do_conv, do_gru):\r\n    inp = Input((seq_length, num_channels))\r\n    out = inp\r\n\r\n    if do_conv:\r\n        out = Conv1D(num_channels, kernel_size, padding=\"same\")(out)\r\n\r\n    if do_gru:\r\n        out = GRU(num_channels, return_sequences=True)(out)\r\n\r\n    out = TimeDistributed(Dense(1, activation=\"sigmoid\"))(out)\r\n\r\n    return keras.models.Model(inputs=inp, outputs=out)\r\n\r\n\r\ndef benchmark_pytorch(X, Y, do_conv, do_gru, num_iter=5):\r\n    \"\"\"\r\n    Do the foward and backward pass `num_iter` times and return the average runtime.\r\n    \"\"\"\r\n    loss_fn = nn.BCELoss()\r\n    model = PyTorchModel()\r\n\r\n    forward_time = 0\r\n    backward_time = 0\r\n\r\n    for _ in range(num_iter):\r\n        # do forward pass\r\n        start = time.time()\r\n        out = model(Variable(X), do_gru=do_gru, do_conv=do_conv)\r\n        forward_time += time.time() - start\r\n\r\n        loss = loss_fn(out, Variable(Y))\r\n\r\n        # do backward pass\r\n        start = time.time()\r\n        loss.backward()\r\n        backward_time += time.time() - start\r\n\r\n    return forward_time / num_iter, backward_time / num_iter\r\n\r\n\r\ndef benchmark_keras(X, Y, do_conv, do_gru, num_iter=5):\r\n    \"\"\"\r\n    Do the foward and backward pass `num_iter` times and return the average runtime.\r\n    \"\"\"\r\n    forward_time = 0\r\n    backward_time = 0\r\n\r\n    model = get_keras_model(do_conv=do_conv, do_gru=do_gru)\r\n    model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.001))\r\n\r\n    for _ in range(num_iter):\r\n        # do forward pass\r\n        start = time.time()\r\n        model.predict(X)\r\n        forward_time += time.time() - start\r\n\r\n        # do backward pass\r\n        start = time.time()\r\n        model.train_on_batch(X, Y)\r\n        backward_time += time.time() - start\r\n\r\n    return forward_time / num_iter, backward_time / num_iter\r\n\r\n\r\nif __name__ == '__main__':\r\n    X, Y, Xnp, Ynp = generate_training_data()\r\n\r\n    print(\"\")\r\n    print(\"data for pytorch: X.shape={}, Y.shape={}\".format(tuple(X.shape), tuple(Y.shape)))\r\n    print(\"data for keras: X.shape={}, Y.shape={}\".format(tuple(Xnp.shape), tuple(Ynp.shape)))\r\n\r\n    parameters = [\r\n        (True, True, \"Testing Conv and GRU together\"),\r\n        (True, False, \"Testing Conv only\"),\r\n        (False, True, \"Testing GRU only\"),\r\n    ]\r\n\r\n    for do_conv, do_gru, description in parameters:\r\n        print(\"\\n\" + \">\" * 50)\r\n        print(description)\r\n\r\n        print(\"pytorch: forward_pass={:.1f}s backward_pass={:.1f}s\".format(\r\n            *benchmark_pytorch(X, Y, do_conv=do_conv, do_gru=do_gru)\r\n        ))\r\n        print(\"keras: forward_pass={:.1f}s backward_pass={:.1f}s\".format(\r\n            *benchmark_keras(Xnp, Ynp, do_conv=do_conv, do_gru=do_gru)\r\n        ))\r\n\r\n        print(\"<\" * 50)\r\n```"}