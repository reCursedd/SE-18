{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11364", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11364/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11364/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11364/events", "html_url": "https://github.com/pytorch/pytorch/issues/11364", "id": 357897994, "node_id": "MDU6SXNzdWUzNTc4OTc5OTQ=", "number": 11364, "title": "test_get_rank_size_group is flaky", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-09-07T02:38:53Z", "updated_at": "2018-11-15T22:27:23Z", "closed_at": "2018-11-15T22:27:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The error complains of broken pipe:</p>\n<pre><code>02:21:52 test_get_rank_size_full_group (__main__.TestDistBackend) ... ok\n02:21:53 test_get_rank_size_group (__main__.TestDistBackend) ... Process process 1:\n02:21:53 Traceback (most recent call last):\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n02:21:53     self.run()\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n02:21:53     self._target(*self._args, **self._kwargs)\n02:21:53   File \"test_distributed.py\", line 1274, in _run\n02:21:53     getattr(self, self.id().split(\".\")[2])()\n02:21:53   File \"test_distributed.py\", line 1216, in wrapper\n02:21:53     fn(self)\n02:21:53   File \"test_distributed.py\", line 252, in test_get_rank_size_group\n02:21:53     group_id = dist.new_group(group)\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 1075, in new_group\n02:21:53     input_ranks)\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 275, in _new_process_group_helper\n02:21:53     pg = ProcessGroupGloo(store, rank, world_size)\n02:21:53 RuntimeError: Broken pipe\n02:21:53 Process process 2:\n02:21:53 Traceback (most recent call last):\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n02:21:53     self.run()\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n02:21:53     self._target(*self._args, **self._kwargs)\n02:21:53   File \"test_distributed.py\", line 1274, in _run\n02:21:53     getattr(self, self.id().split(\".\")[2])()\n02:21:53   File \"test_distributed.py\", line 1216, in wrapper\n02:21:53     fn(self)\n02:21:53   File \"test_distributed.py\", line 252, in test_get_rank_size_group\n02:21:53     group_id = dist.new_group(group)\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 1075, in new_group\n02:21:53     input_ranks)\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 275, in _new_process_group_helper\n02:21:53     pg = ProcessGroupGloo(store, rank, world_size)\n02:21:53 RuntimeError: Broken pipe\n02:21:53 FAIL\n02:21:53 test_irecv (__main__.TestDistBackend) ... skipped 'Gloo does not support irecv'\n02:21:53 test_isend (__main__.TestDistBackend) ... skipped 'Gloo does not support isend'\n02:21:53 test_reduce_full_group_max (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_full_group_min (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_full_group_product (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_full_group_sum (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_group_max (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_group_min (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_group_product (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_group_sum (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_max (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_min (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_multigpu (__main__.TestDistBackend) ... skipped 'Only Nccl backend supports reduce multigpu'\n02:21:53 test_reduce_product (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_sum (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_sum_cuda (__main__.TestDistBackend) ... skipped 'Only Nccl supports CUDA reduce'\n02:21:53 test_scatter (__main__.TestDistBackend) ... skipped 'Gloo does not support scatter'\n02:21:53 test_scatter_full_group (__main__.TestDistBackend) ... skipped 'Gloo does not support scatter'\n02:21:53 test_scatter_group (__main__.TestDistBackend) ... skipped 'Gloo does not support scatter'\n02:21:53 test_send_recv (__main__.TestDistBackend) ... skipped 'Gloo does not support send/recv'\n02:21:53 test_send_recv_any_source (__main__.TestDistBackend) ... skipped 'Gloo does not support send/recv from any source'\n02:21:53 \n02:21:53 ======================================================================\n02:21:53 FAIL: test_get_rank_size_group (__main__.TestDistBackend)\n02:21:53 ----------------------------------------------------------------------\n02:21:53 Traceback (most recent call last):\n02:21:53   File \"test_distributed.py\", line 1214, in wrapper\n02:21:53     self._join_and_reduce(fn)\n02:21:53   File \"test_distributed.py\", line 1290, in _join_and_reduce\n02:21:53     self.assertEqual(p.exitcode, first_process.exitcode)\n02:21:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 370, in assertEqual\n02:21:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n02:21:53 AssertionError: 1 not less than or equal to 1e-05 : \n02:21:53 \n</code></pre>\n<p>Sample log: <a href=\"https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-cuda8-cudnn6-py3-multigpu-test/12694//console\" rel=\"nofollow\">https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-cuda8-cudnn6-py3-multigpu-test/12694//console</a></p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9845\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pietern\">@pietern</a></p>", "body_text": "The error complains of broken pipe:\n02:21:52 test_get_rank_size_full_group (__main__.TestDistBackend) ... ok\n02:21:53 test_get_rank_size_group (__main__.TestDistBackend) ... Process process 1:\n02:21:53 Traceback (most recent call last):\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n02:21:53     self.run()\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n02:21:53     self._target(*self._args, **self._kwargs)\n02:21:53   File \"test_distributed.py\", line 1274, in _run\n02:21:53     getattr(self, self.id().split(\".\")[2])()\n02:21:53   File \"test_distributed.py\", line 1216, in wrapper\n02:21:53     fn(self)\n02:21:53   File \"test_distributed.py\", line 252, in test_get_rank_size_group\n02:21:53     group_id = dist.new_group(group)\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 1075, in new_group\n02:21:53     input_ranks)\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 275, in _new_process_group_helper\n02:21:53     pg = ProcessGroupGloo(store, rank, world_size)\n02:21:53 RuntimeError: Broken pipe\n02:21:53 Process process 2:\n02:21:53 Traceback (most recent call last):\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n02:21:53     self.run()\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n02:21:53     self._target(*self._args, **self._kwargs)\n02:21:53   File \"test_distributed.py\", line 1274, in _run\n02:21:53     getattr(self, self.id().split(\".\")[2])()\n02:21:53   File \"test_distributed.py\", line 1216, in wrapper\n02:21:53     fn(self)\n02:21:53   File \"test_distributed.py\", line 252, in test_get_rank_size_group\n02:21:53     group_id = dist.new_group(group)\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 1075, in new_group\n02:21:53     input_ranks)\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 275, in _new_process_group_helper\n02:21:53     pg = ProcessGroupGloo(store, rank, world_size)\n02:21:53 RuntimeError: Broken pipe\n02:21:53 FAIL\n02:21:53 test_irecv (__main__.TestDistBackend) ... skipped 'Gloo does not support irecv'\n02:21:53 test_isend (__main__.TestDistBackend) ... skipped 'Gloo does not support isend'\n02:21:53 test_reduce_full_group_max (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_full_group_min (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_full_group_product (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_full_group_sum (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_group_max (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_group_min (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_group_product (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_group_sum (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_max (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_min (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_multigpu (__main__.TestDistBackend) ... skipped 'Only Nccl backend supports reduce multigpu'\n02:21:53 test_reduce_product (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_sum (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\n02:21:53 test_reduce_sum_cuda (__main__.TestDistBackend) ... skipped 'Only Nccl supports CUDA reduce'\n02:21:53 test_scatter (__main__.TestDistBackend) ... skipped 'Gloo does not support scatter'\n02:21:53 test_scatter_full_group (__main__.TestDistBackend) ... skipped 'Gloo does not support scatter'\n02:21:53 test_scatter_group (__main__.TestDistBackend) ... skipped 'Gloo does not support scatter'\n02:21:53 test_send_recv (__main__.TestDistBackend) ... skipped 'Gloo does not support send/recv'\n02:21:53 test_send_recv_any_source (__main__.TestDistBackend) ... skipped 'Gloo does not support send/recv from any source'\n02:21:53 \n02:21:53 ======================================================================\n02:21:53 FAIL: test_get_rank_size_group (__main__.TestDistBackend)\n02:21:53 ----------------------------------------------------------------------\n02:21:53 Traceback (most recent call last):\n02:21:53   File \"test_distributed.py\", line 1214, in wrapper\n02:21:53     self._join_and_reduce(fn)\n02:21:53   File \"test_distributed.py\", line 1290, in _join_and_reduce\n02:21:53     self.assertEqual(p.exitcode, first_process.exitcode)\n02:21:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 370, in assertEqual\n02:21:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\n02:21:53 AssertionError: 1 not less than or equal to 1e-05 : \n02:21:53 \n\nSample log: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-cuda8-cudnn6-py3-multigpu-test/12694//console\nCC @teng-li @pietern", "body": "The error complains of broken pipe:\r\n\r\n```\r\n02:21:52 test_get_rank_size_full_group (__main__.TestDistBackend) ... ok\r\n02:21:53 test_get_rank_size_group (__main__.TestDistBackend) ... Process process 1:\r\n02:21:53 Traceback (most recent call last):\r\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n02:21:53     self.run()\r\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\r\n02:21:53     self._target(*self._args, **self._kwargs)\r\n02:21:53   File \"test_distributed.py\", line 1274, in _run\r\n02:21:53     getattr(self, self.id().split(\".\")[2])()\r\n02:21:53   File \"test_distributed.py\", line 1216, in wrapper\r\n02:21:53     fn(self)\r\n02:21:53   File \"test_distributed.py\", line 252, in test_get_rank_size_group\r\n02:21:53     group_id = dist.new_group(group)\r\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 1075, in new_group\r\n02:21:53     input_ranks)\r\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 275, in _new_process_group_helper\r\n02:21:53     pg = ProcessGroupGloo(store, rank, world_size)\r\n02:21:53 RuntimeError: Broken pipe\r\n02:21:53 Process process 2:\r\n02:21:53 Traceback (most recent call last):\r\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\r\n02:21:53     self.run()\r\n02:21:53   File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\r\n02:21:53     self._target(*self._args, **self._kwargs)\r\n02:21:53   File \"test_distributed.py\", line 1274, in _run\r\n02:21:53     getattr(self, self.id().split(\".\")[2])()\r\n02:21:53   File \"test_distributed.py\", line 1216, in wrapper\r\n02:21:53     fn(self)\r\n02:21:53   File \"test_distributed.py\", line 252, in test_get_rank_size_group\r\n02:21:53     group_id = dist.new_group(group)\r\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 1075, in new_group\r\n02:21:53     input_ranks)\r\n02:21:53   File \"/opt/conda/lib/python3.6/site-packages/torch/distributed/c10d/distributed_c10d.py\", line 275, in _new_process_group_helper\r\n02:21:53     pg = ProcessGroupGloo(store, rank, world_size)\r\n02:21:53 RuntimeError: Broken pipe\r\n02:21:53 FAIL\r\n02:21:53 test_irecv (__main__.TestDistBackend) ... skipped 'Gloo does not support irecv'\r\n02:21:53 test_isend (__main__.TestDistBackend) ... skipped 'Gloo does not support isend'\r\n02:21:53 test_reduce_full_group_max (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_full_group_min (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_full_group_product (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_full_group_sum (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_group_max (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_group_min (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_group_product (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_group_sum (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_max (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_min (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_multigpu (__main__.TestDistBackend) ... skipped 'Only Nccl backend supports reduce multigpu'\r\n02:21:53 test_reduce_product (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_sum (__main__.TestDistBackend) ... skipped 'Gloo does not support reduce'\r\n02:21:53 test_reduce_sum_cuda (__main__.TestDistBackend) ... skipped 'Only Nccl supports CUDA reduce'\r\n02:21:53 test_scatter (__main__.TestDistBackend) ... skipped 'Gloo does not support scatter'\r\n02:21:53 test_scatter_full_group (__main__.TestDistBackend) ... skipped 'Gloo does not support scatter'\r\n02:21:53 test_scatter_group (__main__.TestDistBackend) ... skipped 'Gloo does not support scatter'\r\n02:21:53 test_send_recv (__main__.TestDistBackend) ... skipped 'Gloo does not support send/recv'\r\n02:21:53 test_send_recv_any_source (__main__.TestDistBackend) ... skipped 'Gloo does not support send/recv from any source'\r\n02:21:53 \r\n02:21:53 ======================================================================\r\n02:21:53 FAIL: test_get_rank_size_group (__main__.TestDistBackend)\r\n02:21:53 ----------------------------------------------------------------------\r\n02:21:53 Traceback (most recent call last):\r\n02:21:53   File \"test_distributed.py\", line 1214, in wrapper\r\n02:21:53     self._join_and_reduce(fn)\r\n02:21:53   File \"test_distributed.py\", line 1290, in _join_and_reduce\r\n02:21:53     self.assertEqual(p.exitcode, first_process.exitcode)\r\n02:21:53   File \"/var/lib/jenkins/workspace/test/common.py\", line 370, in assertEqual\r\n02:21:53     super(TestCase, self).assertLessEqual(abs(x - y), prec, message)\r\n02:21:53 AssertionError: 1 not less than or equal to 1e-05 : \r\n02:21:53 \r\n```\r\n\r\nSample log: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-cuda8-cudnn6-py3-multigpu-test/12694//console\r\n\r\nCC @teng-li @pietern "}