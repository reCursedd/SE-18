{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/380681555", "html_url": "https://github.com/pytorch/pytorch/issues/755#issuecomment-380681555", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/755", "id": 380681555, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDY4MTU1NQ==", "user": {"login": "Roger-luo", "id": 8445510, "node_id": "MDQ6VXNlcjg0NDU1MTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/8445510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Roger-luo", "html_url": "https://github.com/Roger-luo", "followers_url": "https://api.github.com/users/Roger-luo/followers", "following_url": "https://api.github.com/users/Roger-luo/following{/other_user}", "gists_url": "https://api.github.com/users/Roger-luo/gists{/gist_id}", "starred_url": "https://api.github.com/users/Roger-luo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Roger-luo/subscriptions", "organizations_url": "https://api.github.com/users/Roger-luo/orgs", "repos_url": "https://api.github.com/users/Roger-luo/repos", "events_url": "https://api.github.com/users/Roger-luo/events{/privacy}", "received_events_url": "https://api.github.com/users/Roger-luo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-12T05:21:18Z", "updated_at": "2018-04-12T12:49:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a>  If you lack of manpower, I could try to maintain the complex tensor part in the future, I just started my PhD (and this is my gap year actually), and therefore I won't have the problem for writing my thesis in recent years at least.  But I really cannot keep contributing without any feedback from pytorch team. I do think there should be a <strong>roadmap</strong> for this big extension. And we could add complex support smoothly so your guys won't need to review a large PR and it will ease developers' efforts on tracking master branch.</p>\n<p>Firstly, I think the main problem about complex support would be the CUDA part. It is quite easy to support CPU part with ATen or any other libraries, I can rewrite the CPU part in just a few days if the there is any feedback. There is a few problems I might concern for CUDA part, and I think this might lead to two different approach:</p>\n<ol>\n<li>Use <code>float2</code>, etc. to simulate a single complex value like <code>cuComplex</code> do in the CUDA part.</li>\n<li>Use existing <code>FloatTensor</code> and <code>DoubleTensor</code> to simulate a complex tensor in ATen's C++ part.</li>\n</ol>\n<p>The reason for second approach is because in <code>THC</code>, pytorch use some tricks to accelerate map/reduce operations and it is not suitable for <code>cuComplex</code> trivially because <code>cuComplex</code> is actually <code>float2</code>, but <code>__shfl_xxx</code> functions does not natively support <code>float2</code>. I'm not sure how to efficiently simulate such a function for <code>float2</code> at the moment.</p>\n<p>The second approach would be easier because we now don't need to care about the hardware, and we can make our new complex extension work on old devices much easier. However, this might cause some overhead due to in-contiguous memory address.</p>\n<p>Besides, I found that to integrate complex number into ATen, we might have to handle four different types that is actually the same on hardware: <code>std::complex</code>, <code>thrust::complex</code>, <code>cuComplex</code>, <code>float2</code> which might be dangerous sometimes. (in fact, I met this problem last year, and <code>reinterpreter_cast</code> was the solution).</p>\n<p>I personally would prefer to write everything more native though.</p>\n<p>And I think we probably need a timeframe or roadmap, and we can pick up each small part and work together so I won't need to track master myself which is totally impossible...</p>\n<p>there was a <a href=\"https://github.com/Roger-luo/TH/blob/master/ChangeLog.md\">ChangeLog</a> when I was trying to implement the CPU backend, I classified functions needs to be modified for complex numbers in the log. We could write a road map based on this log.</p>\n<p>Besides, since my visa was just rejected (by Australia), I have to start a gap year, if you need someone keep working on this, I could apply for an internship.</p>", "body_text": "@ezyang  If you lack of manpower, I could try to maintain the complex tensor part in the future, I just started my PhD (and this is my gap year actually), and therefore I won't have the problem for writing my thesis in recent years at least.  But I really cannot keep contributing without any feedback from pytorch team. I do think there should be a roadmap for this big extension. And we could add complex support smoothly so your guys won't need to review a large PR and it will ease developers' efforts on tracking master branch.\nFirstly, I think the main problem about complex support would be the CUDA part. It is quite easy to support CPU part with ATen or any other libraries, I can rewrite the CPU part in just a few days if the there is any feedback. There is a few problems I might concern for CUDA part, and I think this might lead to two different approach:\n\nUse float2, etc. to simulate a single complex value like cuComplex do in the CUDA part.\nUse existing FloatTensor and DoubleTensor to simulate a complex tensor in ATen's C++ part.\n\nThe reason for second approach is because in THC, pytorch use some tricks to accelerate map/reduce operations and it is not suitable for cuComplex trivially because cuComplex is actually float2, but __shfl_xxx functions does not natively support float2. I'm not sure how to efficiently simulate such a function for float2 at the moment.\nThe second approach would be easier because we now don't need to care about the hardware, and we can make our new complex extension work on old devices much easier. However, this might cause some overhead due to in-contiguous memory address.\nBesides, I found that to integrate complex number into ATen, we might have to handle four different types that is actually the same on hardware: std::complex, thrust::complex, cuComplex, float2 which might be dangerous sometimes. (in fact, I met this problem last year, and reinterpreter_cast was the solution).\nI personally would prefer to write everything more native though.\nAnd I think we probably need a timeframe or roadmap, and we can pick up each small part and work together so I won't need to track master myself which is totally impossible...\nthere was a ChangeLog when I was trying to implement the CPU backend, I classified functions needs to be modified for complex numbers in the log. We could write a road map based on this log.\nBesides, since my visa was just rejected (by Australia), I have to start a gap year, if you need someone keep working on this, I could apply for an internship.", "body": "@ezyang  If you lack of manpower, I could try to maintain the complex tensor part in the future, I just started my PhD (and this is my gap year actually), and therefore I won't have the problem for writing my thesis in recent years at least.  But I really cannot keep contributing without any feedback from pytorch team. I do think there should be a **roadmap** for this big extension. And we could add complex support smoothly so your guys won't need to review a large PR and it will ease developers' efforts on tracking master branch.\r\n\r\nFirstly, I think the main problem about complex support would be the CUDA part. It is quite easy to support CPU part with ATen or any other libraries, I can rewrite the CPU part in just a few days if the there is any feedback. There is a few problems I might concern for CUDA part, and I think this might lead to two different approach:\r\n\r\n1. Use `float2`, etc. to simulate a single complex value like `cuComplex` do in the CUDA part.\r\n2. Use existing `FloatTensor` and `DoubleTensor` to simulate a complex tensor in ATen's C++ part.\r\n\r\nThe reason for second approach is because in `THC`, pytorch use some tricks to accelerate map/reduce operations and it is not suitable for `cuComplex` trivially because `cuComplex` is actually `float2`, but `__shfl_xxx` functions does not natively support `float2`. I'm not sure how to efficiently simulate such a function for `float2` at the moment.\r\n\r\nThe second approach would be easier because we now don't need to care about the hardware, and we can make our new complex extension work on old devices much easier. However, this might cause some overhead due to in-contiguous memory address. \r\n\r\nBesides, I found that to integrate complex number into ATen, we might have to handle four different types that is actually the same on hardware: `std::complex`, `thrust::complex`, `cuComplex`, `float2` which might be dangerous sometimes. (in fact, I met this problem last year, and `reinterpreter_cast` was the solution).\r\n\r\nI personally would prefer to write everything more native though.\r\n\r\nAnd I think we probably need a timeframe or roadmap, and we can pick up each small part and work together so I won't need to track master myself which is totally impossible...\r\n\r\nthere was a [ChangeLog](https://github.com/Roger-luo/TH/blob/master/ChangeLog.md) when I was trying to implement the CPU backend, I classified functions needs to be modified for complex numbers in the log. We could write a road map based on this log.\r\n\r\nBesides, since my visa was just rejected (by Australia), I have to start a gap year, if you need someone keep working on this, I could apply for an internship."}