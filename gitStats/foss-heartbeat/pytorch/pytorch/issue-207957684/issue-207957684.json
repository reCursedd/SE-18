{"url": "https://api.github.com/repos/pytorch/pytorch/issues/755", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/755/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/755/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/755/events", "html_url": "https://github.com/pytorch/pytorch/issues/755", "id": 207957684, "node_id": "MDU6SXNzdWUyMDc5NTc2ODQ=", "number": 755, "title": "Integrating complex tensors", "user": {"login": "PhilippPelz", "id": 1299153, "node_id": "MDQ6VXNlcjEyOTkxNTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/1299153?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhilippPelz", "html_url": "https://github.com/PhilippPelz", "followers_url": "https://api.github.com/users/PhilippPelz/followers", "following_url": "https://api.github.com/users/PhilippPelz/following{/other_user}", "gists_url": "https://api.github.com/users/PhilippPelz/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhilippPelz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhilippPelz/subscriptions", "organizations_url": "https://api.github.com/users/PhilippPelz/orgs", "repos_url": "https://api.github.com/users/PhilippPelz/repos", "events_url": "https://api.github.com/users/PhilippPelz/events{/privacy}", "received_events_url": "https://api.github.com/users/PhilippPelz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 586699385, "node_id": "MDU6TGFiZWw1ODY2OTkzODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/24hr+", "name": "24hr+", "color": "d4a5d9", "default": false}, {"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 59, "created_at": "2017-02-15T23:06:57Z", "updated_at": "2018-10-17T14:43:08Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>New description from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a>:</p>\n<p>Work is in progress at <a href=\"https://github.com/Roger-luo/pytorch-complex\">https://github.com/Roger-luo/pytorch-complex</a></p>\n<h2>Organizational principles</h2>\n<ul>\n<li>Complex tensor support is important to PyTorch, and we will accept patches to core which add small amounts of code to make adding complex support.</li>\n<li>Adding complex involves writing a lot of new kernels and code: we'd like this code to initially live out of repo, so it is easier for people to iterate quickly on them without having to go through the PyTorch main code review process. We will <em>NOT</em> commit to reviewing large new kernels in the short term, but eventually we would like all the kernels to come back to PyTorch.</li>\n<li>The external library will be buildable separately from PyTorch, so you will be able to maintain it as a separate repository without having to merge with PyTorch (and deal with loads of merge conflicts).\n<ul>\n<li>PyTorch may occasionally make breaking changes in C++ API; if you bring these to our attention we will do our utmost to help solve these problems.</li>\n</ul>\n</li>\n<li>The hooks needed for this will NOT ship with PyTorch 1.0, but they will ship with a released version of PyTorch in the not too distant future.</li>\n</ul>\n<h2>How will I work on complex kernels?</h2>\n<p>Here is what the workflow will look like in the steady state.</p>\n<p><strong>PyTorch will natively contain APIs for referring to the complex dtype, but they won't do anything by default.</strong> PyTorch defines torch.complex64 and torch.complex128 referring to complex tensors. However, if you try to construct a tensor this way, by default, PyTorch will error:</p>\n<pre><code>&gt;&gt;&gt; torch.zeros({2,2}, dtype=torch.complex64)\nRuntimeError: complex64 not supported by PyTorch\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> provided a patch which adds these dtypes to PyTorch. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"356214300\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11173\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11173/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11173\">#11173</a></p>\n<p>In the mid-term, we will merge support for basic functionality (like allocating a tensor of zeros) to be supported by PyTorch natively. A reasonable proxy for what support is \u201cbasic\u201d is PyTorch's native support for CPU half tensors (which are extremely impoverished).</p>\n<p><strong>PyTorch publishes an interface for registering an implementation of complex tensors.</strong> The implementation inherits from the TypeDefault class (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"355311062\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11013\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11013/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11013\">#11013</a>) and will override methods on this class to define implementations of functions for which we have complex implementations. It will look something like this:</p>\n<pre><code>struct CPUComplexFloatType final : public TypeDefault {\n  virtual Tensor add(const Tensor &amp; self, const Tensor &amp; other, Scalar alpha=1) const override {\n    // Your implementation of add for complex tensors\n  }\n  // ...\n}\n</code></pre>\n<p>This class will override exactly the types which are supported for complex; all other implementations are provided by TypeDefault and will error by default.</p>\n<p>There will be a canonical listing of methods supported on Type (the overall interface) as an autogenerated file that is checked into the PyTorch source repository; we'll communicate API changes by diffs to this file. In general, the methods are in one-to-one correspondence with their corresponding names in the PyTorch frontend.</p>\n<p>In general, when you use an operation which you haven't implemented yet,</p>\n<p><strong>WARNING:</strong> We intend to refactor Type away into a new system that also supports open registration of new operations (this obviously doesn't work if you have a single superclass that defines all the methods you might possibly want to support). Thus, try not to get too tied to the particular implementation strategy of writing Type as a subclass.</p>\n<p><strong>To publish new, complex only operations, you will use the C++ extension API.</strong> The C++ extension API is documented at <a href=\"https://pytorch.org/tutorials/advanced/cpp_extension.html\" rel=\"nofollow\">https://pytorch.org/tutorials/advanced/cpp_extension.html</a> Essentially, you can write a C++ function like:</p>\n<pre><code>at::Tensor imag(at::Tensor z) {\n  ...\n}\n</code></pre>\n<p>And then the C++ extension API will generate a Python binding so that you invoke this function from Python.</p>\n<p><strong>Some operations will be \u201ceasy\u201d to integrate into PyTorch as it exists today.</strong> For example, for implementation of binary operations, it probably makes more sense to extend add_kernel in BinaryOpsKernel.cpp so that it dispatches over complex types (and then you get it for free, because std::complex implements addition). As long as these patches are small and self-contained, we promise to merge them on a timely basis.</p>\n<p>It should ALWAYS be possible to unblock, by just writing an override on Type instead of using existing infrastructure, and doing liberal copy pasting. But let's avoid it when it's easy!</p>\n<p><strong>Autograd.</strong> As long as you're working on operations which already have derivative formulas defined for them, you will \u201cautomatically\u201d get autograd support, as long as you implement complex support for all the constituent functions which are invoked in the backwards implementation from derivatives.yaml.</p>\n<p>In some cases, we may need to adjust autograd formulas so that they work for complex numbers; e.g., the gradient of 'abs' isn't 'grad . self.sign()'. In these cases, all we need to do is upstream fix of changing the autograd formula of 'abs' to 'abs_backward', which is a function that can be overridden.</p>\n<p>For general complex valued back propagation, there are some references:</p>\n<ol>\n<li><em>Akira\u2019s \u201cComplex Valued Neural Networks\u201d.</em></li>\n<li><a href=\"https://giggleliu.github.io/2018/02/01/complex_bp.html\" rel=\"nofollow\">https://giggleliu.github.io/2018/02/01/complex_bp.html</a></li>\n</ol>\n<p>Generally, we won't need to modify the autograd since in most cases we only calculate the derivatives of a real-valued function (the loss).</p>\n<h2>Work plan</h2>\n<p>Many of the necessary pieces are in place today, but they are not put together in an end-to-end way. Here is what needs to be done.</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Codemod TH to not ifdef real <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"356132836\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11163\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11163/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11163\">#11163</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Built-in support for torch.complex64 and torch.complex128 dtypes. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"356214300\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11173\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11173/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11173\">#11173</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> An interface for registering CPUComplexType, etc., so that this implementation is invoked when you request a complex tensor with dtype=torch.complex64 or do an operation on complex tensors.</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Land <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"355311062\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11013\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11013/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11013\">#11013</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> An end-to-end example, including working build system, of a separately compileable C++ program that links against libtorch and uses the aforementioned interface to implement complex tensor allocation.</li>\n</ul>\n<p>Short term integration plan. These operations are \u201ceasy\u201d to implement, and so we should mainline them in PyTorch as soon as possible.</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\" checked=\"\"> Basic tensor factories: torch.empty, torch.zeros, torch.ones</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> CPU binary operations: add, sub, mul, div <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"359948632\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11641\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11641/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11641\">#11641</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> FFT</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> ???</li>\n</ul>\n<p>Kernel implementation:</p>\n<p>TODO: Generate a list based on <a href=\"https://github.com/Roger-luo/TH/blob/master/ChangeLog.md\">https://github.com/Roger-luo/TH/blob/master/ChangeLog.md</a></p>\n<p>Other complex related tasks:</p>\n<ul class=\"contains-task-list\">\n<li class=\"task-list-item\"><input type=\"checkbox\" id=\"\" disabled=\"\" class=\"task-list-item-checkbox\"> Figure out the type promotion rules for complex tensors, and implement it in promoteTypes <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"359948632\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11641\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/11641/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/11641\">#11641</a></li>\n</ul>\n<h2>Historical issue content</h2>\n<p>Original comment from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1299153\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PhilippPelz\">@PhilippPelz</a></p>\n<p>I was wondering if there is interest in incorporating complex tensors into pytorch.<br>\nFor CPU support there is ztorch and I have written z-cutorch ( <a href=\"https://github.com/PhilippPelz/z-cutorch\">https://github.com/PhilippPelz/z-cutorch</a> ) a while ago. It is a fork off cutorch before the refactoring for CudaHalfTensor (don't have the hardware yet).<br>\nIf it's not too much work, I would like to slowly integrate it with pytorch. I am using matplotlib for plotting via fb.ptyhon and it turns out a huge pain every time I reinstall my system (compiling all the dependencies), plus it seems pytorch will work under Windows soon, which one of my experiment PCs runs on.<br>\nI would also need complex gradients, so I would sooner or later touch autograd as well.<br>\nWhile tf supports complex tensors per se, it seems many ops don't support it yet (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"153518419\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2255\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2255/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2255\">tensorflow/tensorflow#2255</a>), plus it seems a bit heavyweight for my purposes.</p>\n<p>Maybe someone could say a few words how and where to start with this, if it's a welcome idea.</p>", "body_text": "New description from @ezyang:\nWork is in progress at https://github.com/Roger-luo/pytorch-complex\nOrganizational principles\n\nComplex tensor support is important to PyTorch, and we will accept patches to core which add small amounts of code to make adding complex support.\nAdding complex involves writing a lot of new kernels and code: we'd like this code to initially live out of repo, so it is easier for people to iterate quickly on them without having to go through the PyTorch main code review process. We will NOT commit to reviewing large new kernels in the short term, but eventually we would like all the kernels to come back to PyTorch.\nThe external library will be buildable separately from PyTorch, so you will be able to maintain it as a separate repository without having to merge with PyTorch (and deal with loads of merge conflicts).\n\nPyTorch may occasionally make breaking changes in C++ API; if you bring these to our attention we will do our utmost to help solve these problems.\n\n\nThe hooks needed for this will NOT ship with PyTorch 1.0, but they will ship with a released version of PyTorch in the not too distant future.\n\nHow will I work on complex kernels?\nHere is what the workflow will look like in the steady state.\nPyTorch will natively contain APIs for referring to the complex dtype, but they won't do anything by default. PyTorch defines torch.complex64 and torch.complex128 referring to complex tensors. However, if you try to construct a tensor this way, by default, PyTorch will error:\n>>> torch.zeros({2,2}, dtype=torch.complex64)\nRuntimeError: complex64 not supported by PyTorch\n\n@ezyang provided a patch which adds these dtypes to PyTorch. #11173\nIn the mid-term, we will merge support for basic functionality (like allocating a tensor of zeros) to be supported by PyTorch natively. A reasonable proxy for what support is \u201cbasic\u201d is PyTorch's native support for CPU half tensors (which are extremely impoverished).\nPyTorch publishes an interface for registering an implementation of complex tensors. The implementation inherits from the TypeDefault class (#11013) and will override methods on this class to define implementations of functions for which we have complex implementations. It will look something like this:\nstruct CPUComplexFloatType final : public TypeDefault {\n  virtual Tensor add(const Tensor & self, const Tensor & other, Scalar alpha=1) const override {\n    // Your implementation of add for complex tensors\n  }\n  // ...\n}\n\nThis class will override exactly the types which are supported for complex; all other implementations are provided by TypeDefault and will error by default.\nThere will be a canonical listing of methods supported on Type (the overall interface) as an autogenerated file that is checked into the PyTorch source repository; we'll communicate API changes by diffs to this file. In general, the methods are in one-to-one correspondence with their corresponding names in the PyTorch frontend.\nIn general, when you use an operation which you haven't implemented yet,\nWARNING: We intend to refactor Type away into a new system that also supports open registration of new operations (this obviously doesn't work if you have a single superclass that defines all the methods you might possibly want to support). Thus, try not to get too tied to the particular implementation strategy of writing Type as a subclass.\nTo publish new, complex only operations, you will use the C++ extension API. The C++ extension API is documented at https://pytorch.org/tutorials/advanced/cpp_extension.html Essentially, you can write a C++ function like:\nat::Tensor imag(at::Tensor z) {\n  ...\n}\n\nAnd then the C++ extension API will generate a Python binding so that you invoke this function from Python.\nSome operations will be \u201ceasy\u201d to integrate into PyTorch as it exists today. For example, for implementation of binary operations, it probably makes more sense to extend add_kernel in BinaryOpsKernel.cpp so that it dispatches over complex types (and then you get it for free, because std::complex implements addition). As long as these patches are small and self-contained, we promise to merge them on a timely basis.\nIt should ALWAYS be possible to unblock, by just writing an override on Type instead of using existing infrastructure, and doing liberal copy pasting. But let's avoid it when it's easy!\nAutograd. As long as you're working on operations which already have derivative formulas defined for them, you will \u201cautomatically\u201d get autograd support, as long as you implement complex support for all the constituent functions which are invoked in the backwards implementation from derivatives.yaml.\nIn some cases, we may need to adjust autograd formulas so that they work for complex numbers; e.g., the gradient of 'abs' isn't 'grad . self.sign()'. In these cases, all we need to do is upstream fix of changing the autograd formula of 'abs' to 'abs_backward', which is a function that can be overridden.\nFor general complex valued back propagation, there are some references:\n\nAkira\u2019s \u201cComplex Valued Neural Networks\u201d.\nhttps://giggleliu.github.io/2018/02/01/complex_bp.html\n\nGenerally, we won't need to modify the autograd since in most cases we only calculate the derivatives of a real-valued function (the loss).\nWork plan\nMany of the necessary pieces are in place today, but they are not put together in an end-to-end way. Here is what needs to be done.\n\n Codemod TH to not ifdef real #11163\n Built-in support for torch.complex64 and torch.complex128 dtypes. #11173\n An interface for registering CPUComplexType, etc., so that this implementation is invoked when you request a complex tensor with dtype=torch.complex64 or do an operation on complex tensors.\n Land #11013\n An end-to-end example, including working build system, of a separately compileable C++ program that links against libtorch and uses the aforementioned interface to implement complex tensor allocation.\n\nShort term integration plan. These operations are \u201ceasy\u201d to implement, and so we should mainline them in PyTorch as soon as possible.\n\n Basic tensor factories: torch.empty, torch.zeros, torch.ones\n CPU binary operations: add, sub, mul, div #11641\n FFT\n ???\n\nKernel implementation:\nTODO: Generate a list based on https://github.com/Roger-luo/TH/blob/master/ChangeLog.md\nOther complex related tasks:\n\n Figure out the type promotion rules for complex tensors, and implement it in promoteTypes #11641\n\nHistorical issue content\nOriginal comment from @PhilippPelz\nI was wondering if there is interest in incorporating complex tensors into pytorch.\nFor CPU support there is ztorch and I have written z-cutorch ( https://github.com/PhilippPelz/z-cutorch ) a while ago. It is a fork off cutorch before the refactoring for CudaHalfTensor (don't have the hardware yet).\nIf it's not too much work, I would like to slowly integrate it with pytorch. I am using matplotlib for plotting via fb.ptyhon and it turns out a huge pain every time I reinstall my system (compiling all the dependencies), plus it seems pytorch will work under Windows soon, which one of my experiment PCs runs on.\nI would also need complex gradients, so I would sooner or later touch autograd as well.\nWhile tf supports complex tensors per se, it seems many ops don't support it yet (tensorflow/tensorflow#2255), plus it seems a bit heavyweight for my purposes.\nMaybe someone could say a few words how and where to start with this, if it's a welcome idea.", "body": "New description from @ezyang:\r\n\r\nWork is in progress at https://github.com/Roger-luo/pytorch-complex\r\n\r\n## Organizational principles\r\n\r\n* Complex tensor support is important to PyTorch, and we will accept patches to core which add small amounts of code to make adding complex support.\r\n* Adding complex involves writing a lot of new kernels and code: we'd like this code to initially live out of repo, so it is easier for people to iterate quickly on them without having to go through the PyTorch main code review process. We will *NOT* commit to reviewing large new kernels in the short term, but eventually we would like all the kernels to come back to PyTorch.\r\n* The external library will be buildable separately from PyTorch, so you will be able to maintain it as a separate repository without having to merge with PyTorch (and deal with loads of merge conflicts).\r\n    * PyTorch may occasionally make breaking changes in C++ API; if you bring these to our attention we will do our utmost to help solve these problems.\r\n* The hooks needed for this will NOT ship with PyTorch 1.0, but they will ship with a released version of PyTorch in the not too distant future.\r\n\r\n## How will I work on complex kernels?\r\n\r\nHere is what the workflow will look like in the steady state.\r\n\r\n**PyTorch will natively contain APIs for referring to the complex dtype, but they won't do anything by default.** PyTorch defines torch.complex64 and torch.complex128 referring to complex tensors. However, if you try to construct a tensor this way, by default, PyTorch will error:\r\n\r\n```\r\n>>> torch.zeros({2,2}, dtype=torch.complex64)\r\nRuntimeError: complex64 not supported by PyTorch\r\n```\r\n\r\n@ezyang provided a patch which adds these dtypes to PyTorch. https://github.com/pytorch/pytorch/pull/11173\r\n\r\nIn the mid-term, we will merge support for basic functionality (like allocating a tensor of zeros) to be supported by PyTorch natively. A reasonable proxy for what support is \u201cbasic\u201d is PyTorch's native support for CPU half tensors (which are extremely impoverished).\r\n\r\n**PyTorch publishes an interface for registering an implementation of complex tensors.** The implementation inherits from the TypeDefault class (https://github.com/pytorch/pytorch/pull/11013) and will override methods on this class to define implementations of functions for which we have complex implementations. It will look something like this:\r\n\r\n```\r\nstruct CPUComplexFloatType final : public TypeDefault {\r\n  virtual Tensor add(const Tensor & self, const Tensor & other, Scalar alpha=1) const override {\r\n    // Your implementation of add for complex tensors\r\n  }\r\n  // ...\r\n}\r\n```\r\n\r\nThis class will override exactly the types which are supported for complex; all other implementations are provided by TypeDefault and will error by default.\r\n\r\nThere will be a canonical listing of methods supported on Type (the overall interface) as an autogenerated file that is checked into the PyTorch source repository; we'll communicate API changes by diffs to this file. In general, the methods are in one-to-one correspondence with their corresponding names in the PyTorch frontend.\r\n\r\nIn general, when you use an operation which you haven't implemented yet, \r\n\r\n**WARNING:** We intend to refactor Type away into a new system that also supports open registration of new operations (this obviously doesn't work if you have a single superclass that defines all the methods you might possibly want to support). Thus, try not to get too tied to the particular implementation strategy of writing Type as a subclass.\r\n\r\n**To publish new, complex only operations, you will use the C++ extension API.** The C++ extension API is documented at https://pytorch.org/tutorials/advanced/cpp_extension.html Essentially, you can write a C++ function like:\r\n\r\n```\r\nat::Tensor imag(at::Tensor z) {\r\n  ...\r\n}\r\n```\r\n\r\nAnd then the C++ extension API will generate a Python binding so that you invoke this function from Python.\r\n\r\n**Some operations will be \u201ceasy\u201d to integrate into PyTorch as it exists today.** For example, for implementation of binary operations, it probably makes more sense to extend add_kernel in BinaryOpsKernel.cpp so that it dispatches over complex types (and then you get it for free, because std::complex implements addition). As long as these patches are small and self-contained, we promise to merge them on a timely basis.\r\n\r\nIt should ALWAYS be possible to unblock, by just writing an override on Type instead of using existing infrastructure, and doing liberal copy pasting. But let's avoid it when it's easy!\r\n\r\n**Autograd.** As long as you're working on operations which already have derivative formulas defined for them, you will \u201cautomatically\u201d get autograd support, as long as you implement complex support for all the constituent functions which are invoked in the backwards implementation from derivatives.yaml.\r\n\r\nIn some cases, we may need to adjust autograd formulas so that they work for complex numbers; e.g., the gradient of 'abs' isn't 'grad . self.sign()'. In these cases, all we need to do is upstream fix of changing the autograd formula of 'abs' to 'abs_backward', which is a function that can be overridden.\r\n\r\nFor general complex valued back propagation, there are some references:\r\n\r\n1. *Akira\u2019s \u201cComplex Valued Neural Networks\u201d.*\r\n2. https://giggleliu.github.io/2018/02/01/complex_bp.html\r\n\r\nGenerally, we won't need to modify the autograd since in most cases we only calculate the derivatives of a real-valued function (the loss). \r\n\r\n## Work plan\r\n\r\nMany of the necessary pieces are in place today, but they are not put together in an end-to-end way. Here is what needs to be done.\r\n\r\n- [X] Codemod TH to not ifdef real https://github.com/pytorch/pytorch/pull/11163\r\n- [X] Built-in support for torch.complex64 and torch.complex128 dtypes. https://github.com/pytorch/pytorch/pull/11173\r\n- [X] An interface for registering CPUComplexType, etc., so that this implementation is invoked when you request a complex tensor with dtype=torch.complex64 or do an operation on complex tensors. \r\n- [X] Land https://github.com/pytorch/pytorch/pull/11013\r\n- [X] An end-to-end example, including working build system, of a separately compileable C++ program that links against libtorch and uses the aforementioned interface to implement complex tensor allocation.\r\n\r\nShort term integration plan. These operations are \u201ceasy\u201d to implement, and so we should mainline them in PyTorch as soon as possible.\r\n\r\n- [X] Basic tensor factories: torch.empty, torch.zeros, torch.ones\r\n- [ ] CPU binary operations: add, sub, mul, div #11641\r\n- [ ] FFT\r\n- [ ] ???\r\n\r\nKernel implementation:\r\n\r\nTODO: Generate a list based on https://github.com/Roger-luo/TH/blob/master/ChangeLog.md\r\n\r\nOther complex related tasks:\r\n\r\n- [ ] Figure out the type promotion rules for complex tensors, and implement it in promoteTypes #11641\r\n\r\n## Historical issue content\r\n\r\nOriginal comment from @PhilippPelz \r\n\r\nI was wondering if there is interest in incorporating complex tensors into pytorch.\r\nFor CPU support there is ztorch and I have written z-cutorch ( https://github.com/PhilippPelz/z-cutorch ) a while ago. It is a fork off cutorch before the refactoring for CudaHalfTensor (don't have the hardware yet). \r\nIf it's not too much work, I would like to slowly integrate it with pytorch. I am using matplotlib for plotting via fb.ptyhon and it turns out a huge pain every time I reinstall my system (compiling all the dependencies), plus it seems pytorch will work under Windows soon, which one of my experiment PCs runs on.\r\nI would also need complex gradients, so I would sooner or later touch autograd as well. \r\nWhile tf supports complex tensors per se, it seems many ops don't support it yet (https://github.com/tensorflow/tensorflow/issues/2255), plus it seems a bit heavyweight for my purposes.\r\n\r\nMaybe someone could say a few words how and where to start with this, if it's a welcome idea.\r\n"}