{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/380844405", "html_url": "https://github.com/pytorch/pytorch/issues/755#issuecomment-380844405", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/755", "id": 380844405, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDg0NDQwNQ==", "user": {"login": "Roger-luo", "id": 8445510, "node_id": "MDQ6VXNlcjg0NDU1MTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/8445510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Roger-luo", "html_url": "https://github.com/Roger-luo", "followers_url": "https://api.github.com/users/Roger-luo/followers", "following_url": "https://api.github.com/users/Roger-luo/following{/other_user}", "gists_url": "https://api.github.com/users/Roger-luo/gists{/gist_id}", "starred_url": "https://api.github.com/users/Roger-luo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Roger-luo/subscriptions", "organizations_url": "https://api.github.com/users/Roger-luo/orgs", "repos_url": "https://api.github.com/users/Roger-luo/repos", "events_url": "https://api.github.com/users/Roger-luo/events{/privacy}", "received_events_url": "https://api.github.com/users/Roger-luo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-12T15:23:50Z", "updated_at": "2018-04-12T15:24:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Agree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1299153\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/PhilippPelz\">@PhilippPelz</a> , we might lose a lot performance since we will lose the complex support from BLAS, cublas and MAGMA. But I'm not sure about it. However, to be clear, complex Tensor is something completely <strong>different from sparse tensor</strong>, most libraries like <code>scipy.sparse</code>, and Julia's <code>SparseArrays</code> treat sparse array as a composition of fundamental multi-dimensional arrays. But nobody treat a multi-dimensional array with complex type by composite two real arrays... (nobody here I mean tensorflow, arrayfire, numpy and Julia). Though in MXNet, the FFT is accomplished by a composition of two real tensors indeed, they do not support complex... It seems that tensorflow implemented a DataType as a wrapper around differnet types including <code>complex64</code> and <code>complex128</code> see <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/types.proto\">types.proto</a></p>\n<h4>About the performance loss</h4>\n<p>Firstly, the element-wise functions (functions calls map/reduce) will not have large performance loss (at least, the memory for these operations will be contiguous). But I think we should try to benchmark the some BLAS functions first, to see if a composition of <code>FloatTensor</code> have similar performance with <code>Complex64Tensor</code> on GPU, and how much we will lose on the performance with a draft implementation, like:</p>\n<ul>\n<li><code>gemm</code></li>\n<li><code>gemv</code></li>\n</ul>\n<p>A composited complex tensor would be something looks like (or just use <code>shared_ptr</code>):</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">ComplexTensor</span> {\n    FloatTensor *real;\n    FloatTensor *imag;\n};</pre></div>\n<p>However, as I mentioned in the disadvantage of first approach, functions like  <code>__shfl_xxx</code> also looks like an obstacle if we want to do this more native.</p>", "body_text": "Agree with @PhilippPelz , we might lose a lot performance since we will lose the complex support from BLAS, cublas and MAGMA. But I'm not sure about it. However, to be clear, complex Tensor is something completely different from sparse tensor, most libraries like scipy.sparse, and Julia's SparseArrays treat sparse array as a composition of fundamental multi-dimensional arrays. But nobody treat a multi-dimensional array with complex type by composite two real arrays... (nobody here I mean tensorflow, arrayfire, numpy and Julia). Though in MXNet, the FFT is accomplished by a composition of two real tensors indeed, they do not support complex... It seems that tensorflow implemented a DataType as a wrapper around differnet types including complex64 and complex128 see types.proto\nAbout the performance loss\nFirstly, the element-wise functions (functions calls map/reduce) will not have large performance loss (at least, the memory for these operations will be contiguous). But I think we should try to benchmark the some BLAS functions first, to see if a composition of FloatTensor have similar performance with Complex64Tensor on GPU, and how much we will lose on the performance with a draft implementation, like:\n\ngemm\ngemv\n\nA composited complex tensor would be something looks like (or just use shared_ptr):\nclass ComplexTensor {\n    FloatTensor *real;\n    FloatTensor *imag;\n};\nHowever, as I mentioned in the disadvantage of first approach, functions like  __shfl_xxx also looks like an obstacle if we want to do this more native.", "body": "Agree with @PhilippPelz , we might lose a lot performance since we will lose the complex support from BLAS, cublas and MAGMA. But I'm not sure about it. However, to be clear, complex Tensor is something completely **different from sparse tensor**, most libraries like `scipy.sparse`, and Julia's `SparseArrays` treat sparse array as a composition of fundamental multi-dimensional arrays. But nobody treat a multi-dimensional array with complex type by composite two real arrays... (nobody here I mean tensorflow, arrayfire, numpy and Julia). Though in MXNet, the FFT is accomplished by a composition of two real tensors indeed, they do not support complex... It seems that tensorflow implemented a DataType as a wrapper around differnet types including `complex64` and `complex128` see [types.proto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/types.proto)\r\n\r\n#### About the performance loss\r\nFirstly, the element-wise functions (functions calls map/reduce) will not have large performance loss (at least, the memory for these operations will be contiguous). But I think we should try to benchmark the some BLAS functions first, to see if a composition of `FloatTensor` have similar performance with `Complex64Tensor` on GPU, and how much we will lose on the performance with a draft implementation, like:\r\n\r\n- `gemm`\r\n- `gemv`\r\n\r\nA composited complex tensor would be something looks like (or just use `shared_ptr`):\r\n\r\n```c++\r\nclass ComplexTensor {\r\n    FloatTensor *real;\r\n    FloatTensor *imag;\r\n};\r\n```\r\n\r\nHowever, as I mentioned in the disadvantage of first approach, functions like  `__shfl_xxx` also looks like an obstacle if we want to do this more native."}