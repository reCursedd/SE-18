{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/380815753", "html_url": "https://github.com/pytorch/pytorch/issues/755#issuecomment-380815753", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/755", "id": 380815753, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDgxNTc1Mw==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-12T14:01:56Z", "updated_at": "2018-04-12T14:01:56Z", "author_association": "MEMBER", "body_html": "<p>I thought a lot about this over the last day. It's a little sad that we couldn't merge Roger's effort as-is, but I thought to myself</p>\n<blockquote>\n<p>\"how can we build out complex Tensor support, while keeping low maintenance overheads?\"</p>\n</blockquote>\n<p>This is what I am laying out as an effective plan from the above goal:</p>\n<ul>\n<li>Complex Tensors shouldn't be a fundamental new Tensor type, like <code>sparse</code> Tensors. Adding a fundamental type causes a lot of maintenance overhead and cross-cutting changes. The maintenance overhead is not about \"who maintains the complex bits?\", but more of \"now all core-devs should be aware of this complex type when doing any fundamental changes, any ATen changes, etc.\"\n<ul>\n<li>Instead, they should always be [Tensor Shape x 2] or [2 x TensorShape], i.e. the Tensor should have one extra dimension with a size 2.</li>\n</ul>\n</li>\n<li>Complex Tensors should be a small file / folder of ~2k lines of simple C++ that are built on top of ATen Tensor API.\n<ul>\n<li>For example, as <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"313439689\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6514\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/6514/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/6514\">#6514</a> suggests, complex multiplication should be implemented as <code>torch.stack([real1 * real2 - imag1 * imag2, real1 * imag2 + imag1 * real2], dim = -1)</code> where <code>real1 = input1[:, :, :, ..., 0]</code></li>\n<li>This hurts performance: yes we wont get as much perf as if we inline everything. However, the question is: \"by how much?\". I think we should aim for 20% lower performance in exchange for a healthy and full-featured + maintained complex support.</li>\n<li>The most used complex functions can start getting dedicated kernels, so that where performance is taking greater than 20% hit on a frequently used function, we step in.</li>\n</ul>\n</li>\n</ul>", "body_text": "I thought a lot about this over the last day. It's a little sad that we couldn't merge Roger's effort as-is, but I thought to myself\n\n\"how can we build out complex Tensor support, while keeping low maintenance overheads?\"\n\nThis is what I am laying out as an effective plan from the above goal:\n\nComplex Tensors shouldn't be a fundamental new Tensor type, like sparse Tensors. Adding a fundamental type causes a lot of maintenance overhead and cross-cutting changes. The maintenance overhead is not about \"who maintains the complex bits?\", but more of \"now all core-devs should be aware of this complex type when doing any fundamental changes, any ATen changes, etc.\"\n\nInstead, they should always be [Tensor Shape x 2] or [2 x TensorShape], i.e. the Tensor should have one extra dimension with a size 2.\n\n\nComplex Tensors should be a small file / folder of ~2k lines of simple C++ that are built on top of ATen Tensor API.\n\nFor example, as #6514 suggests, complex multiplication should be implemented as torch.stack([real1 * real2 - imag1 * imag2, real1 * imag2 + imag1 * real2], dim = -1) where real1 = input1[:, :, :, ..., 0]\nThis hurts performance: yes we wont get as much perf as if we inline everything. However, the question is: \"by how much?\". I think we should aim for 20% lower performance in exchange for a healthy and full-featured + maintained complex support.\nThe most used complex functions can start getting dedicated kernels, so that where performance is taking greater than 20% hit on a frequently used function, we step in.", "body": "I thought a lot about this over the last day. It's a little sad that we couldn't merge Roger's effort as-is, but I thought to myself\r\n\r\n> \"how can we build out complex Tensor support, while keeping low maintenance overheads?\"\r\n\r\nThis is what I am laying out as an effective plan from the above goal:\r\n\r\n- Complex Tensors shouldn't be a fundamental new Tensor type, like `sparse` Tensors. Adding a fundamental type causes a lot of maintenance overhead and cross-cutting changes. The maintenance overhead is not about \"who maintains the complex bits?\", but more of \"now all core-devs should be aware of this complex type when doing any fundamental changes, any ATen changes, etc.\"\r\n  - Instead, they should always be [Tensor Shape x 2] or [2 x TensorShape], i.e. the Tensor should have one extra dimension with a size 2.\r\n- Complex Tensors should be a small file / folder of ~2k lines of simple C++ that are built on top of ATen Tensor API.\r\n  - For example, as https://github.com/pytorch/pytorch/issues/6514 suggests, complex multiplication should be implemented as `torch.stack([real1 * real2 - imag1 * imag2, real1 * imag2 + imag1 * real2], dim = -1)` where `real1 = input1[:, :, :, ..., 0]`\r\n  - This hurts performance: yes we wont get as much perf as if we inline everything. However, the question is: \"by how much?\". I think we should aim for 20% lower performance in exchange for a healthy and full-featured + maintained complex support.\r\n  - The most used complex functions can start getting dedicated kernels, so that where performance is taking greater than 20% hit on a frequently used function, we step in."}