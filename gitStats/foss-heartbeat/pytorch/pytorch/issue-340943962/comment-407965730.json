{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/407965730", "html_url": "https://github.com/pytorch/pytorch/issues/9418#issuecomment-407965730", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9418", "id": 407965730, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzk2NTczMA==", "user": {"login": "Stonesjtu", "id": 4556044, "node_id": "MDQ6VXNlcjQ1NTYwNDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4556044?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Stonesjtu", "html_url": "https://github.com/Stonesjtu", "followers_url": "https://api.github.com/users/Stonesjtu/followers", "following_url": "https://api.github.com/users/Stonesjtu/following{/other_user}", "gists_url": "https://api.github.com/users/Stonesjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Stonesjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Stonesjtu/subscriptions", "organizations_url": "https://api.github.com/users/Stonesjtu/orgs", "repos_url": "https://api.github.com/users/Stonesjtu/repos", "events_url": "https://api.github.com/users/Stonesjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/Stonesjtu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-26T03:34:30Z", "updated_at": "2018-07-26T03:34:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Reproduced on openmpi-1.10</p>\n<p>I debug the program via <code>mpirun -n 2 cuda-memcheck python test.py</code>. It reports that the receive buffer's pointer was on CPU, which is unexpected at all. (<a href=\"https://www.mail-archive.com/users@lists.open-mpi.org/msg29456.html\" rel=\"nofollow\">https://www.mail-archive.com/users@lists.open-mpi.org/msg29456.html</a>)<br>\nHowever if you add an extra <code>torch.cuda.synchronize</code> before all the mpi calls, the snippet runs without error.</p>\n<p>So there may be some bugs under the hood.</p>\n<p>Another workaround is adding <code>torch.cuda.set_device(rank)</code> at the first, because <code>torch.manual_seed</code> initializes all the processes on GPU:0, which is not desired.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch.distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">debug_fun</span>():\n    torch.cuda.set_device(dist.get_rank())\n    torch.manual_seed(dist.get_rank())\n    device <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda:<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(dist.get_rank())\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> device = \"cpu\"</span>\n    param <span class=\"pl-k\">=</span> torch.FloatTensor(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>).to(device)\n\n    param.uniform_()\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Rank <span class=\"pl-c1\">{}</span>, before sync: <span class=\"pl-c1\">{}</span> at <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(dist.get_rank(),param.cpu().data.numpy(), param.device))\n    dist.broadcast(param.data,<span class=\"pl-c1\">0</span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Rank <span class=\"pl-c1\">{}</span>, after sync : <span class=\"pl-c1\">{}</span> at <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(dist.get_rank(),param.cpu().data.numpy(), param.device))\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    dist.init_process_group(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mpi<span class=\"pl-pds\">\"</span></span>)\n    debug_fun()</pre></div>", "body_text": "Reproduced on openmpi-1.10\nI debug the program via mpirun -n 2 cuda-memcheck python test.py. It reports that the receive buffer's pointer was on CPU, which is unexpected at all. (https://www.mail-archive.com/users@lists.open-mpi.org/msg29456.html)\nHowever if you add an extra torch.cuda.synchronize before all the mpi calls, the snippet runs without error.\nSo there may be some bugs under the hood.\nAnother workaround is adding torch.cuda.set_device(rank) at the first, because torch.manual_seed initializes all the processes on GPU:0, which is not desired.\nimport torch.distributed as dist\nimport torch\n\ndef debug_fun():\n    torch.cuda.set_device(dist.get_rank())\n    torch.manual_seed(dist.get_rank())\n    device = \"cuda:{}\".format(dist.get_rank())\n    # device = \"cpu\"\n    param = torch.FloatTensor(1,1).to(device)\n\n    param.uniform_()\n    print(\"Rank {}, before sync: {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\n    dist.broadcast(param.data,0)\n    print(\"Rank {}, after sync : {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\n\n\nif __name__ == '__main__':\n    dist.init_process_group(\"mpi\")\n    debug_fun()", "body": "Reproduced on openmpi-1.10\r\n\r\nI debug the program via `mpirun -n 2 cuda-memcheck python test.py`. It reports that the receive buffer's pointer was on CPU, which is unexpected at all. (https://www.mail-archive.com/users@lists.open-mpi.org/msg29456.html)\r\nHowever if you add an extra `torch.cuda.synchronize` before all the mpi calls, the snippet runs without error.\r\n\r\nSo there may be some bugs under the hood.\r\n\r\nAnother workaround is adding `torch.cuda.set_device(rank)` at the first, because `torch.manual_seed` initializes all the processes on GPU:0, which is not desired.\r\n```python\r\nimport torch.distributed as dist\r\nimport torch\r\n\r\ndef debug_fun():\r\n    torch.cuda.set_device(dist.get_rank())\r\n    torch.manual_seed(dist.get_rank())\r\n    device = \"cuda:{}\".format(dist.get_rank())\r\n    # device = \"cpu\"\r\n    param = torch.FloatTensor(1,1).to(device)\r\n\r\n    param.uniform_()\r\n    print(\"Rank {}, before sync: {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\r\n    dist.broadcast(param.data,0)\r\n    print(\"Rank {}, after sync : {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\r\n\r\n\r\nif __name__ == '__main__':\r\n    dist.init_process_group(\"mpi\")\r\n    debug_fun()\r\n```\r\n"}