{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9418", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9418/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9418/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9418/events", "html_url": "https://github.com/pytorch/pytorch/issues/9418", "id": 340943962, "node_id": "MDU6SXNzdWUzNDA5NDM5NjI=", "number": 9418, "title": "Segmentation Fault using dist.broadcast() with openmpi", "user": {"login": "matthiasreisser", "id": 8858505, "node_id": "MDQ6VXNlcjg4NTg1MDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/8858505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matthiasreisser", "html_url": "https://github.com/matthiasreisser", "followers_url": "https://api.github.com/users/matthiasreisser/followers", "following_url": "https://api.github.com/users/matthiasreisser/following{/other_user}", "gists_url": "https://api.github.com/users/matthiasreisser/gists{/gist_id}", "starred_url": "https://api.github.com/users/matthiasreisser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matthiasreisser/subscriptions", "organizations_url": "https://api.github.com/users/matthiasreisser/orgs", "repos_url": "https://api.github.com/users/matthiasreisser/repos", "events_url": "https://api.github.com/users/matthiasreisser/events{/privacy}", "received_events_url": "https://api.github.com/users/matthiasreisser/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-07-13T09:19:04Z", "updated_at": "2018-07-31T19:25:46Z", "closed_at": "2018-07-31T19:25:46Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I am trying to run distributed pytorch using the mpi backend. In this specific minimal example, I am trying to broadcast a tensor from node 0 to node 1.<br>\nThe example works fine when the tensor is living on cpu, however when trying to exchange the tensor between GPUs, I get a Segmentation fault. Please see below.</p>\n<h2>Code example</h2>\n<pre><code>import torch.distributed as dist\nimport torch\n\ndef debug_fun():\n    torch.manual_seed(dist.get_rank())\n    device = \"cuda:{}\".format(dist.get_rank())\n    # device = \"cpu\"\n    param = torch.FloatTensor(1,1).to(device)\n\n    param.uniform_()\n    print(\"Rank {}, before sync: {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\n    dist.broadcast(param.data,0)\n    print(\"Rank {}, after sync : {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\n\n\nif __name__ == '__main__':\n    dist.init_process_group(\"mpi\", rank=0, world_size=0)\n    debug_fun()\n</code></pre>\n<p>When running <code>mpirun -n 2 segfault_demo.py</code> with <code>device = \"cpu\"</code> uncommented, the output is, as can be expected:</p>\n<pre><code>Rank 0, before sync: [[0.4962566]] at cpu\nRank 1, before sync: [[0.7576316]] at cpu\nRank 0, after sync : [[0.4962566]] at cpu\nRank 1, after sync : [[0.4962566]] at cpu\n</code></pre>\n<p>However, when moving the tensor to the respective gpu, I have the following error message (home directory path has been replaced):</p>\n<pre><code>Rank 0, before sync: [[0.08403993]] at cuda:0\nRank 1, before sync: [[0.29211983]] at cuda:1\n[node219:15444] *** Process received signal ***\n[node219:15444] Signal: Segmentation fault (11)\n[node219:15444] Signal code: Invalid permissions (2)\n[node219:15444] Failing at address: 0x1050de00000\n[node219:15444] [ 0] /lib64/libpthread.so.0(+0xf5e0)[0x2aaaaacde5e0]\n[node219:15444] [ 1] /lib64/libc.so.6(+0x14cf13)[0x2aaaab037f13]\n[node219:15444] [ 2] /HOME/lib/libopen-pal.so.40(opal_convertor_unpack+0x10a)[0x2aab47ad74fa]\n[node219:15444] [ 3] /HOME/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_recv_frag_callback_match+0x41f)[0x2aab6451e3ff]\n[node219:15444] [ 4] /HOME/lib/openmpi/mca_btl_smcuda.so(mca_btl_smcuda_component_progress+0x53b)[0x2aab630d2e5b]\n[node219:15444] [ 5] /HOME/lib/libopen-pal.so.40(opal_progress+0x2c)[0x2aab47ac66dc]\n[node219:15444] [ 6] /HOME/lib/libopen-pal.so.40(ompi_sync_wait_mt+0xb5)[0x2aab47acd395]\n[node219:15444] [ 7] Rank 0, after sync : [[0.08403993]] at cuda:0\n/HOME/lib/libmpi.so.40(ompi_request_default_wait+0x1ce)[0x2aab1e45f9ce]\n[node219:15444] [ 8] /HOME/lib/libmpi.so.40(ompi_coll_base_bcast_intra_generic+0x4ee)[0x2aab1e4a903e]\n[node219:15444] [ 9] /HOME/lib/libmpi.so.40(ompi_coll_base_bcast_intra_binomial+0xb7)[0x2aab1e4a94b7]\n[node219:15444] [10] /HOME/lib/openmpi/mca_coll_tuned.so(ompi_coll_tuned_bcast_intra_dec_fixed+0xcc)[0x2aab6537a8ac]\n[node219:15444] [11] /HOME/lib/libmpi.so.40(MPI_Bcast+0x139)[0x2aab1e474ee9]\n[node219:15444] [12] /HOME/anaconda3/envs/fbi_env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN3thd14DataChannelMPI9broadcastERN2at6TensorEji+0x15f)[0x2aaac5b7b5cf]\n[node219:15444] [13] /HOME/anaconda3/envs/fbi_env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(THDBroadcast+0x28)[0x2aaac5b53958]\n[node219:15444] [14] /HOME/anaconda3/envs/fbi_env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_Z20THDPModule_broadcastP7_objectS0_+0xcb)[0x2aaac59ef95b]\n[node219:15444] [15] python(_PyCFunction_FastCallDict+0x91)[0x555555663921]\n[node219:15444] [16] python(+0x19cdfc)[0x5555556f0dfc]\n[node219:15444] [17] python(_PyEval_EvalFrameDefault+0x2fa)[0x55555571594a]\n[node219:15444] [18] python(+0x196206)[0x5555556ea206]\n[node219:15444] [19] python(+0x1971cf)[0x5555556eb1cf]\n[node219:15444] [20] python(+0x19ced5)[0x5555556f0ed5]\n[node219:15444] [21] python(_PyEval_EvalFrameDefault+0x2fa)[0x55555571594a]\n[node219:15444] [22] python(+0x196f8b)[0x5555556eaf8b]\n[node219:15444] [23] python(+0x19ced5)[0x5555556f0ed5]\n[node219:15444] [24] python(_PyEval_EvalFrameDefault+0x2fa)[0x55555571594a]\n[node219:15444] [25] python(PyEval_EvalCodeEx+0x329)[0x5555556ebcb9]\n[node219:15444] [26] python(PyEval_EvalCode+0x1c)[0x5555556eca4c]\n[node219:15444] [27] python(+0x214c44)[0x555555768c44]\n[node219:15444] [28] python(PyRun_FileExFlags+0xa1)[0x555555769041]\n[node219:15444] [29] python(PyRun_SimpleFileExFlags+0x1c4)[0x555555769244]\n[node219:15444] *** End of error message ***\n-------------------------------------------------------\nPrimary job  terminated normally, but 1 process returned\na non-zero exit code. Per user-direction, the job has been aborted.\n-------------------------------------------------------\n--------------------------------------------------------------------------\nmpirun noticed that process rank 1 with PID 0 on node node219 exited on signal 11 (Segmentation fault).\n--------------------------------------------------------------------------\n</code></pre>\n<p>Please note that in the above error message, we have the line <code>[node219:15444] [ 7] Rank 0, after sync : [[0.08403993]] at cuda:0</code>, showing that the sender successfully returned from the broadcast call. (this print-out is mixed into the error-message).</p>\n<p>Furthermore, when explicitly setting <code>device = \"cuda:0\"</code> such that both processes access gpu 0, the code runs fine:</p>\n<pre><code>Rank 1, before sync: [[0.29211983]] at cuda:0\nRank 0, before sync: [[0.08403993]] at cuda:0\nRank 0, after sync : [[0.08403993]] at cuda:0\nRank 1, after sync : [[0.08403993]] at cuda:0\n</code></pre>\n<p>However when setting <code>device = \"cuda:1\"</code>, it consistently fails with the same error-message</p>\n<h2>System Info</h2>\n<pre><code>Collecting environment information...\nPyTorch version: 0.5.0a0+e186377\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: CentOS Linux release 7.4.1708 (Core)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\nCMake version: version 3.11.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\nGPU 3: GeForce GTX 1080 Ti\n\nNvidia driver version: 384.111\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] numpy (1.14.5)\n[pip] torch (0.5.0a0+e186377)\n[pip] torchvision (0.2.1)\n[conda] magma-cuda90              2.3.0                         1    pytorch\n[conda] pytorch                   0.4.0            py36hdf912b8_0\n[conda] torch                     0.5.0a0+04a7fc1           &lt;pip&gt;\n[conda] torch                     0.5.0a0+e186377           &lt;pip&gt;\n[conda] torchvision               0.2.1                    py36_0\n</code></pre>\n<p>In addition to the above, cuDNN version is v7 for cuda9.0.<br>\npytorch was built from source from within anaconda3<br>\nFurthermore:<br>\nmpirun (Open MPI) 3.1.0<br>\nnccl_2.2.13-1+cuda9.0  (I am not entirely sure this was found during build, since using \"nccl\" during initialization fails). If you think this is relevant, I will recompile pytorch and double check.</p>\n<p>Any help would be appreciated</p>\n<p>Matthias</p>", "body_text": "Issue description\nI am trying to run distributed pytorch using the mpi backend. In this specific minimal example, I am trying to broadcast a tensor from node 0 to node 1.\nThe example works fine when the tensor is living on cpu, however when trying to exchange the tensor between GPUs, I get a Segmentation fault. Please see below.\nCode example\nimport torch.distributed as dist\nimport torch\n\ndef debug_fun():\n    torch.manual_seed(dist.get_rank())\n    device = \"cuda:{}\".format(dist.get_rank())\n    # device = \"cpu\"\n    param = torch.FloatTensor(1,1).to(device)\n\n    param.uniform_()\n    print(\"Rank {}, before sync: {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\n    dist.broadcast(param.data,0)\n    print(\"Rank {}, after sync : {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\n\n\nif __name__ == '__main__':\n    dist.init_process_group(\"mpi\", rank=0, world_size=0)\n    debug_fun()\n\nWhen running mpirun -n 2 segfault_demo.py with device = \"cpu\" uncommented, the output is, as can be expected:\nRank 0, before sync: [[0.4962566]] at cpu\nRank 1, before sync: [[0.7576316]] at cpu\nRank 0, after sync : [[0.4962566]] at cpu\nRank 1, after sync : [[0.4962566]] at cpu\n\nHowever, when moving the tensor to the respective gpu, I have the following error message (home directory path has been replaced):\nRank 0, before sync: [[0.08403993]] at cuda:0\nRank 1, before sync: [[0.29211983]] at cuda:1\n[node219:15444] *** Process received signal ***\n[node219:15444] Signal: Segmentation fault (11)\n[node219:15444] Signal code: Invalid permissions (2)\n[node219:15444] Failing at address: 0x1050de00000\n[node219:15444] [ 0] /lib64/libpthread.so.0(+0xf5e0)[0x2aaaaacde5e0]\n[node219:15444] [ 1] /lib64/libc.so.6(+0x14cf13)[0x2aaaab037f13]\n[node219:15444] [ 2] /HOME/lib/libopen-pal.so.40(opal_convertor_unpack+0x10a)[0x2aab47ad74fa]\n[node219:15444] [ 3] /HOME/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_recv_frag_callback_match+0x41f)[0x2aab6451e3ff]\n[node219:15444] [ 4] /HOME/lib/openmpi/mca_btl_smcuda.so(mca_btl_smcuda_component_progress+0x53b)[0x2aab630d2e5b]\n[node219:15444] [ 5] /HOME/lib/libopen-pal.so.40(opal_progress+0x2c)[0x2aab47ac66dc]\n[node219:15444] [ 6] /HOME/lib/libopen-pal.so.40(ompi_sync_wait_mt+0xb5)[0x2aab47acd395]\n[node219:15444] [ 7] Rank 0, after sync : [[0.08403993]] at cuda:0\n/HOME/lib/libmpi.so.40(ompi_request_default_wait+0x1ce)[0x2aab1e45f9ce]\n[node219:15444] [ 8] /HOME/lib/libmpi.so.40(ompi_coll_base_bcast_intra_generic+0x4ee)[0x2aab1e4a903e]\n[node219:15444] [ 9] /HOME/lib/libmpi.so.40(ompi_coll_base_bcast_intra_binomial+0xb7)[0x2aab1e4a94b7]\n[node219:15444] [10] /HOME/lib/openmpi/mca_coll_tuned.so(ompi_coll_tuned_bcast_intra_dec_fixed+0xcc)[0x2aab6537a8ac]\n[node219:15444] [11] /HOME/lib/libmpi.so.40(MPI_Bcast+0x139)[0x2aab1e474ee9]\n[node219:15444] [12] /HOME/anaconda3/envs/fbi_env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN3thd14DataChannelMPI9broadcastERN2at6TensorEji+0x15f)[0x2aaac5b7b5cf]\n[node219:15444] [13] /HOME/anaconda3/envs/fbi_env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(THDBroadcast+0x28)[0x2aaac5b53958]\n[node219:15444] [14] /HOME/anaconda3/envs/fbi_env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_Z20THDPModule_broadcastP7_objectS0_+0xcb)[0x2aaac59ef95b]\n[node219:15444] [15] python(_PyCFunction_FastCallDict+0x91)[0x555555663921]\n[node219:15444] [16] python(+0x19cdfc)[0x5555556f0dfc]\n[node219:15444] [17] python(_PyEval_EvalFrameDefault+0x2fa)[0x55555571594a]\n[node219:15444] [18] python(+0x196206)[0x5555556ea206]\n[node219:15444] [19] python(+0x1971cf)[0x5555556eb1cf]\n[node219:15444] [20] python(+0x19ced5)[0x5555556f0ed5]\n[node219:15444] [21] python(_PyEval_EvalFrameDefault+0x2fa)[0x55555571594a]\n[node219:15444] [22] python(+0x196f8b)[0x5555556eaf8b]\n[node219:15444] [23] python(+0x19ced5)[0x5555556f0ed5]\n[node219:15444] [24] python(_PyEval_EvalFrameDefault+0x2fa)[0x55555571594a]\n[node219:15444] [25] python(PyEval_EvalCodeEx+0x329)[0x5555556ebcb9]\n[node219:15444] [26] python(PyEval_EvalCode+0x1c)[0x5555556eca4c]\n[node219:15444] [27] python(+0x214c44)[0x555555768c44]\n[node219:15444] [28] python(PyRun_FileExFlags+0xa1)[0x555555769041]\n[node219:15444] [29] python(PyRun_SimpleFileExFlags+0x1c4)[0x555555769244]\n[node219:15444] *** End of error message ***\n-------------------------------------------------------\nPrimary job  terminated normally, but 1 process returned\na non-zero exit code. Per user-direction, the job has been aborted.\n-------------------------------------------------------\n--------------------------------------------------------------------------\nmpirun noticed that process rank 1 with PID 0 on node node219 exited on signal 11 (Segmentation fault).\n--------------------------------------------------------------------------\n\nPlease note that in the above error message, we have the line [node219:15444] [ 7] Rank 0, after sync : [[0.08403993]] at cuda:0, showing that the sender successfully returned from the broadcast call. (this print-out is mixed into the error-message).\nFurthermore, when explicitly setting device = \"cuda:0\" such that both processes access gpu 0, the code runs fine:\nRank 1, before sync: [[0.29211983]] at cuda:0\nRank 0, before sync: [[0.08403993]] at cuda:0\nRank 0, after sync : [[0.08403993]] at cuda:0\nRank 1, after sync : [[0.08403993]] at cuda:0\n\nHowever when setting device = \"cuda:1\", it consistently fails with the same error-message\nSystem Info\nCollecting environment information...\nPyTorch version: 0.5.0a0+e186377\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: CentOS Linux release 7.4.1708 (Core)\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\nCMake version: version 3.11.1\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0: GeForce GTX 1080 Ti\nGPU 1: GeForce GTX 1080 Ti\nGPU 2: GeForce GTX 1080 Ti\nGPU 3: GeForce GTX 1080 Ti\n\nNvidia driver version: 384.111\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] numpy (1.14.5)\n[pip] torch (0.5.0a0+e186377)\n[pip] torchvision (0.2.1)\n[conda] magma-cuda90              2.3.0                         1    pytorch\n[conda] pytorch                   0.4.0            py36hdf912b8_0\n[conda] torch                     0.5.0a0+04a7fc1           <pip>\n[conda] torch                     0.5.0a0+e186377           <pip>\n[conda] torchvision               0.2.1                    py36_0\n\nIn addition to the above, cuDNN version is v7 for cuda9.0.\npytorch was built from source from within anaconda3\nFurthermore:\nmpirun (Open MPI) 3.1.0\nnccl_2.2.13-1+cuda9.0  (I am not entirely sure this was found during build, since using \"nccl\" during initialization fails). If you think this is relevant, I will recompile pytorch and double check.\nAny help would be appreciated\nMatthias", "body": "## Issue description\r\n\r\nI am trying to run distributed pytorch using the mpi backend. In this specific minimal example, I am trying to broadcast a tensor from node 0 to node 1. \r\nThe example works fine when the tensor is living on cpu, however when trying to exchange the tensor between GPUs, I get a Segmentation fault. Please see below. \r\n\r\n## Code example\r\n\r\n```\r\nimport torch.distributed as dist\r\nimport torch\r\n\r\ndef debug_fun():\r\n    torch.manual_seed(dist.get_rank())\r\n    device = \"cuda:{}\".format(dist.get_rank())\r\n    # device = \"cpu\"\r\n    param = torch.FloatTensor(1,1).to(device)\r\n\r\n    param.uniform_()\r\n    print(\"Rank {}, before sync: {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\r\n    dist.broadcast(param.data,0)\r\n    print(\"Rank {}, after sync : {} at {}\".format(dist.get_rank(),param.cpu().data.numpy(), param.device))\r\n\r\n\r\nif __name__ == '__main__':\r\n    dist.init_process_group(\"mpi\", rank=0, world_size=0)\r\n    debug_fun()\r\n```\r\n\r\nWhen running `mpirun -n 2 segfault_demo.py` with `device = \"cpu\"` uncommented, the output is, as can be expected:\r\n```\r\nRank 0, before sync: [[0.4962566]] at cpu\r\nRank 1, before sync: [[0.7576316]] at cpu\r\nRank 0, after sync : [[0.4962566]] at cpu\r\nRank 1, after sync : [[0.4962566]] at cpu\r\n``` \r\n\r\nHowever, when moving the tensor to the respective gpu, I have the following error message (home directory path has been replaced):\r\n```\r\nRank 0, before sync: [[0.08403993]] at cuda:0\r\nRank 1, before sync: [[0.29211983]] at cuda:1\r\n[node219:15444] *** Process received signal ***\r\n[node219:15444] Signal: Segmentation fault (11)\r\n[node219:15444] Signal code: Invalid permissions (2)\r\n[node219:15444] Failing at address: 0x1050de00000\r\n[node219:15444] [ 0] /lib64/libpthread.so.0(+0xf5e0)[0x2aaaaacde5e0]\r\n[node219:15444] [ 1] /lib64/libc.so.6(+0x14cf13)[0x2aaaab037f13]\r\n[node219:15444] [ 2] /HOME/lib/libopen-pal.so.40(opal_convertor_unpack+0x10a)[0x2aab47ad74fa]\r\n[node219:15444] [ 3] /HOME/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_recv_frag_callback_match+0x41f)[0x2aab6451e3ff]\r\n[node219:15444] [ 4] /HOME/lib/openmpi/mca_btl_smcuda.so(mca_btl_smcuda_component_progress+0x53b)[0x2aab630d2e5b]\r\n[node219:15444] [ 5] /HOME/lib/libopen-pal.so.40(opal_progress+0x2c)[0x2aab47ac66dc]\r\n[node219:15444] [ 6] /HOME/lib/libopen-pal.so.40(ompi_sync_wait_mt+0xb5)[0x2aab47acd395]\r\n[node219:15444] [ 7] Rank 0, after sync : [[0.08403993]] at cuda:0\r\n/HOME/lib/libmpi.so.40(ompi_request_default_wait+0x1ce)[0x2aab1e45f9ce]\r\n[node219:15444] [ 8] /HOME/lib/libmpi.so.40(ompi_coll_base_bcast_intra_generic+0x4ee)[0x2aab1e4a903e]\r\n[node219:15444] [ 9] /HOME/lib/libmpi.so.40(ompi_coll_base_bcast_intra_binomial+0xb7)[0x2aab1e4a94b7]\r\n[node219:15444] [10] /HOME/lib/openmpi/mca_coll_tuned.so(ompi_coll_tuned_bcast_intra_dec_fixed+0xcc)[0x2aab6537a8ac]\r\n[node219:15444] [11] /HOME/lib/libmpi.so.40(MPI_Bcast+0x139)[0x2aab1e474ee9]\r\n[node219:15444] [12] /HOME/anaconda3/envs/fbi_env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_ZN3thd14DataChannelMPI9broadcastERN2at6TensorEji+0x15f)[0x2aaac5b7b5cf]\r\n[node219:15444] [13] /HOME/anaconda3/envs/fbi_env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(THDBroadcast+0x28)[0x2aaac5b53958]\r\n[node219:15444] [14] /HOME/anaconda3/envs/fbi_env/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so(_Z20THDPModule_broadcastP7_objectS0_+0xcb)[0x2aaac59ef95b]\r\n[node219:15444] [15] python(_PyCFunction_FastCallDict+0x91)[0x555555663921]\r\n[node219:15444] [16] python(+0x19cdfc)[0x5555556f0dfc]\r\n[node219:15444] [17] python(_PyEval_EvalFrameDefault+0x2fa)[0x55555571594a]\r\n[node219:15444] [18] python(+0x196206)[0x5555556ea206]\r\n[node219:15444] [19] python(+0x1971cf)[0x5555556eb1cf]\r\n[node219:15444] [20] python(+0x19ced5)[0x5555556f0ed5]\r\n[node219:15444] [21] python(_PyEval_EvalFrameDefault+0x2fa)[0x55555571594a]\r\n[node219:15444] [22] python(+0x196f8b)[0x5555556eaf8b]\r\n[node219:15444] [23] python(+0x19ced5)[0x5555556f0ed5]\r\n[node219:15444] [24] python(_PyEval_EvalFrameDefault+0x2fa)[0x55555571594a]\r\n[node219:15444] [25] python(PyEval_EvalCodeEx+0x329)[0x5555556ebcb9]\r\n[node219:15444] [26] python(PyEval_EvalCode+0x1c)[0x5555556eca4c]\r\n[node219:15444] [27] python(+0x214c44)[0x555555768c44]\r\n[node219:15444] [28] python(PyRun_FileExFlags+0xa1)[0x555555769041]\r\n[node219:15444] [29] python(PyRun_SimpleFileExFlags+0x1c4)[0x555555769244]\r\n[node219:15444] *** End of error message ***\r\n-------------------------------------------------------\r\nPrimary job  terminated normally, but 1 process returned\r\na non-zero exit code. Per user-direction, the job has been aborted.\r\n-------------------------------------------------------\r\n--------------------------------------------------------------------------\r\nmpirun noticed that process rank 1 with PID 0 on node node219 exited on signal 11 (Segmentation fault).\r\n--------------------------------------------------------------------------\r\n```\r\nPlease note that in the above error message, we have the line `[node219:15444] [ 7] Rank 0, after sync : [[0.08403993]] at cuda:0`, showing that the sender successfully returned from the broadcast call. (this print-out is mixed into the error-message).\r\n\r\nFurthermore, when explicitly setting `device = \"cuda:0\"` such that both processes access gpu 0, the code runs fine:\r\n```\r\nRank 1, before sync: [[0.29211983]] at cuda:0\r\nRank 0, before sync: [[0.08403993]] at cuda:0\r\nRank 0, after sync : [[0.08403993]] at cuda:0\r\nRank 1, after sync : [[0.08403993]] at cuda:0\r\n```\r\nHowever when setting `device = \"cuda:1\"`, it consistently fails with the same error-message\r\n\r\n## System Info\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.5.0a0+e186377\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: CentOS Linux release 7.4.1708 (Core)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX 1080 Ti\r\nGPU 1: GeForce GTX 1080 Ti\r\nGPU 2: GeForce GTX 1080 Ti\r\nGPU 3: GeForce GTX 1080 Ti\r\n\r\nNvidia driver version: 384.111\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.5)\r\n[pip] torch (0.5.0a0+e186377)\r\n[pip] torchvision (0.2.1)\r\n[conda] magma-cuda90              2.3.0                         1    pytorch\r\n[conda] pytorch                   0.4.0            py36hdf912b8_0\r\n[conda] torch                     0.5.0a0+04a7fc1           <pip>\r\n[conda] torch                     0.5.0a0+e186377           <pip>\r\n[conda] torchvision               0.2.1                    py36_0\r\n``` \r\nIn addition to the above, cuDNN version is v7 for cuda9.0.\r\npytorch was built from source from within anaconda3\r\nFurthermore:\r\nmpirun (Open MPI) 3.1.0\r\nnccl_2.2.13-1+cuda9.0  (I am not entirely sure this was found during build, since using \"nccl\" during initialization fails). If you think this is relevant, I will recompile pytorch and double check.\r\n\r\n\r\n\r\nAny help would be appreciated\r\n\r\nMatthias\r\n"}