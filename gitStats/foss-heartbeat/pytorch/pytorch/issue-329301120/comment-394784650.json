{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/394784650", "html_url": "https://github.com/pytorch/pytorch/pull/8151#issuecomment-394784650", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8151", "id": 394784650, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDc4NDY1MA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-05T16:58:55Z", "updated_at": "2018-06-05T16:58:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Adam is right, any_sync should go into <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCDeviceUtils.cuh\">https://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCDeviceUtils.cuh</a>.<br>\nIt also has a mask argument that cannot be blindly used, you have to make sure that the threads in the mask did not actually exit. So in embedding using 0xffffffff might lead to hangs, mask has to be intelligently constructed. I did not check the code in detail though, may be there's no warp divergence in which case 0xffffffff will do.</p>", "body_text": "Adam is right, any_sync should go into https://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCDeviceUtils.cuh.\nIt also has a mask argument that cannot be blindly used, you have to make sure that the threads in the mask did not actually exit. So in embedding using 0xffffffff might lead to hangs, mask has to be intelligently constructed. I did not check the code in detail though, may be there's no warp divergence in which case 0xffffffff will do.", "body": "Adam is right, any_sync should go into https://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCDeviceUtils.cuh. \r\nIt also has a mask argument that cannot be blindly used, you have to make sure that the threads in the mask did not actually exit. So in embedding using 0xffffffff might lead to hangs, mask has to be intelligently constructed. I did not check the code in detail though, may be there's no warp divergence in which case 0xffffffff will do. "}