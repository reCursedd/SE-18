{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12478", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12478/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12478/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12478/events", "html_url": "https://github.com/pytorch/pytorch/issues/12478", "id": 368140230, "node_id": "MDU6SXNzdWUzNjgxNDAyMzA=", "number": 12478, "title": "Decaying learning rate with Adam optimizer", "user": {"login": "vinaykumar2491", "id": 12234745, "node_id": "MDQ6VXNlcjEyMjM0NzQ1", "avatar_url": "https://avatars0.githubusercontent.com/u/12234745?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vinaykumar2491", "html_url": "https://github.com/vinaykumar2491", "followers_url": "https://api.github.com/users/vinaykumar2491/followers", "following_url": "https://api.github.com/users/vinaykumar2491/following{/other_user}", "gists_url": "https://api.github.com/users/vinaykumar2491/gists{/gist_id}", "starred_url": "https://api.github.com/users/vinaykumar2491/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vinaykumar2491/subscriptions", "organizations_url": "https://api.github.com/users/vinaykumar2491/orgs", "repos_url": "https://api.github.com/users/vinaykumar2491/repos", "events_url": "https://api.github.com/users/vinaykumar2491/events{/privacy}", "received_events_url": "https://api.github.com/users/vinaykumar2491/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-09T10:20:40Z", "updated_at": "2018-10-16T07:20:31Z", "closed_at": "2018-10-15T03:19:14Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nI'm trying to decay the learning rate using <code>optim.lr_scheduler.ExponentialLR()</code> with <code>optim.Adam()</code> optimizer.</p>\n<p>My loss suddenly starts increasing.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/12234745/46663142-f0ea5c00-cbda-11e8-9bfb-f0ff624edd0a.png\"><img src=\"https://user-images.githubusercontent.com/12234745/46663142-f0ea5c00-cbda-11e8-9bfb-f0ff624edd0a.png\" alt=\"loss_vocals_decay_comparision\" style=\"max-width:100%;\"></a></p>\n<p>What should I do for a better learning?</p>", "body_text": "Hi,\nI'm trying to decay the learning rate using optim.lr_scheduler.ExponentialLR() with optim.Adam() optimizer.\nMy loss suddenly starts increasing.\n\nWhat should I do for a better learning?", "body": "Hi,\r\nI'm trying to decay the learning rate using `optim.lr_scheduler.ExponentialLR()` with `optim.Adam()` optimizer.\r\n\r\nMy loss suddenly starts increasing.\r\n![loss_vocals_decay_comparision](https://user-images.githubusercontent.com/12234745/46663142-f0ea5c00-cbda-11e8-9bfb-f0ff624edd0a.png)\r\n\r\nWhat should I do for a better learning? "}