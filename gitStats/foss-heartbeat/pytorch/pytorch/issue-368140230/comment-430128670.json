{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430128670", "html_url": "https://github.com/pytorch/pytorch/issues/12478#issuecomment-430128670", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12478", "id": 430128670, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDEyODY3MA==", "user": {"login": "vinaykumar2491", "id": 12234745, "node_id": "MDQ6VXNlcjEyMjM0NzQ1", "avatar_url": "https://avatars0.githubusercontent.com/u/12234745?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vinaykumar2491", "html_url": "https://github.com/vinaykumar2491", "followers_url": "https://api.github.com/users/vinaykumar2491/followers", "following_url": "https://api.github.com/users/vinaykumar2491/following{/other_user}", "gists_url": "https://api.github.com/users/vinaykumar2491/gists{/gist_id}", "starred_url": "https://api.github.com/users/vinaykumar2491/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vinaykumar2491/subscriptions", "organizations_url": "https://api.github.com/users/vinaykumar2491/orgs", "repos_url": "https://api.github.com/users/vinaykumar2491/repos", "events_url": "https://api.github.com/users/vinaykumar2491/events{/privacy}", "received_events_url": "https://api.github.com/users/vinaykumar2491/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-16T07:20:31Z", "updated_at": "2018-10-16T07:20:31Z", "author_association": "NONE", "body_html": "<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12234745\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vinaykumar2491\">@vinaykumar2491</a> what value of <code>gamma</code> are you using for the scheduler? Also is this training loss or validation loss?</p>\n</blockquote>\n<p>Hi, I am having a discussion on this issue here: <a href=\"https://discuss.pytorch.org/t/loss-jumps-abruptly-whenever-learning-rate-is-decayed-in-adam-optimizer/26096/9\" rel=\"nofollow\">https://discuss.pytorch.org/t/loss-jumps-abruptly-whenever-learning-rate-is-decayed-in-adam-optimizer/26096/9</a></p>\n<p>I raised an issue here as I was not able to find any support there. But as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> suggested please discuss here.<br>\nThanks</p>", "body_text": "@vinaykumar2491 what value of gamma are you using for the scheduler? Also is this training loss or validation loss?\n\nHi, I am having a discussion on this issue here: https://discuss.pytorch.org/t/loss-jumps-abruptly-whenever-learning-rate-is-decayed-in-adam-optimizer/26096/9\nI raised an issue here as I was not able to find any support there. But as @soumith suggested please discuss here.\nThanks", "body": "> @vinaykumar2491 what value of `gamma` are you using for the scheduler? Also is this training loss or validation loss?\r\n\r\nHi, I am having a discussion on this issue here: https://discuss.pytorch.org/t/loss-jumps-abruptly-whenever-learning-rate-is-decayed-in-adam-optimizer/26096/9\r\n\r\nI raised an issue here as I was not able to find any support there. But as @soumith suggested please discuss here.\r\nThanks"}