{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/361768948", "html_url": "https://github.com/pytorch/pytorch/issues/4939#issuecomment-361768948", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4939", "id": 361768948, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTc2ODk0OA==", "user": {"login": "jrdurrant", "id": 9134971, "node_id": "MDQ6VXNlcjkxMzQ5NzE=", "avatar_url": "https://avatars0.githubusercontent.com/u/9134971?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jrdurrant", "html_url": "https://github.com/jrdurrant", "followers_url": "https://api.github.com/users/jrdurrant/followers", "following_url": "https://api.github.com/users/jrdurrant/following{/other_user}", "gists_url": "https://api.github.com/users/jrdurrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/jrdurrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jrdurrant/subscriptions", "organizations_url": "https://api.github.com/users/jrdurrant/orgs", "repos_url": "https://api.github.com/users/jrdurrant/repos", "events_url": "https://api.github.com/users/jrdurrant/events{/privacy}", "received_events_url": "https://api.github.com/users/jrdurrant/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T23:22:47Z", "updated_at": "2018-01-30T23:22:47Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> Yes exactly, and that certainly explains why I can't seem to find any information elsewhere either! I have an implementation of the paper <a href=\"http://www.cs.huji.ac.il/labs/cglab/projects/convpyr/\" rel=\"nofollow\">Convolution Pyramids</a> - where their 'pyramid' is effectively an encoder-decoder network with skip connections - that I think this makes most sense for.</p>\n<p>In order for their method to work correctly the convolutions must be 'full' convolutions in the encoder part of the network. This means the image is being padded such that it actually ends up larger (before being downsampled) at each layer. In order for the sizes of the equivalent layers with skip connections to work this means doing the inverse of this during the decoder part - this is the negative padding. So although the edges of the image are being discarded, there is no actual <em>information</em> being lost.</p>", "body_text": "@fmassa Yes exactly, and that certainly explains why I can't seem to find any information elsewhere either! I have an implementation of the paper Convolution Pyramids - where their 'pyramid' is effectively an encoder-decoder network with skip connections - that I think this makes most sense for.\nIn order for their method to work correctly the convolutions must be 'full' convolutions in the encoder part of the network. This means the image is being padded such that it actually ends up larger (before being downsampled) at each layer. In order for the sizes of the equivalent layers with skip connections to work this means doing the inverse of this during the decoder part - this is the negative padding. So although the edges of the image are being discarded, there is no actual information being lost.", "body": "@fmassa Yes exactly, and that certainly explains why I can't seem to find any information elsewhere either! I have an implementation of the paper [Convolution Pyramids](http://www.cs.huji.ac.il/labs/cglab/projects/convpyr/) - where their 'pyramid' is effectively an encoder-decoder network with skip connections - that I think this makes most sense for. \r\n\r\nIn order for their method to work correctly the convolutions must be 'full' convolutions in the encoder part of the network. This means the image is being padded such that it actually ends up larger (before being downsampled) at each layer. In order for the sizes of the equivalent layers with skip connections to work this means doing the inverse of this during the decoder part - this is the negative padding. So although the edges of the image are being discarded, there is no actual _information_ being lost. "}