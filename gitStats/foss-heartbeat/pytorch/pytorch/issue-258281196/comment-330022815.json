{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/330022815", "html_url": "https://github.com/pytorch/pytorch/issues/2759#issuecomment-330022815", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2759", "id": 330022815, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMDAyMjgxNQ==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-17T05:41:44Z", "updated_at": "2017-09-17T05:41:44Z", "author_association": "MEMBER", "body_html": "<p>As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> says here: <a href=\"https://discuss.pytorch.org/t/pytorch-performance/3079/7?u=smth\" rel=\"nofollow\">https://discuss.pytorch.org/t/pytorch-performance/3079/7?u=smth</a></p>\n<blockquote>\n<p>In benchmark mode, for each input size, cudnn will perform a bunch of computations to infer the fastest algorithm for that specific case, and caches the result. This brings some overhead, and if your input dimensions change all the time, using benchmark will actually slow down things because of this overhead.</p>\n</blockquote>", "body_text": "As @fmassa says here: https://discuss.pytorch.org/t/pytorch-performance/3079/7?u=smth\n\nIn benchmark mode, for each input size, cudnn will perform a bunch of computations to infer the fastest algorithm for that specific case, and caches the result. This brings some overhead, and if your input dimensions change all the time, using benchmark will actually slow down things because of this overhead.", "body": "As @fmassa says here: https://discuss.pytorch.org/t/pytorch-performance/3079/7?u=smth\r\n\r\n> In benchmark mode, for each input size, cudnn will perform a bunch of computations to infer the fastest algorithm for that specific case, and caches the result. This brings some overhead, and if your input dimensions change all the time, using benchmark will actually slow down things because of this overhead."}