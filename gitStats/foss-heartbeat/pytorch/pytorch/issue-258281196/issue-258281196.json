{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2759", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2759/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2759/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2759/events", "html_url": "https://github.com/pytorch/pytorch/issues/2759", "id": 258281196, "node_id": "MDU6SXNzdWUyNTgyODExOTY=", "number": 2759, "title": "Variable input size training is slow", "user": {"login": "kuangliu", "id": 10502826, "node_id": "MDQ6VXNlcjEwNTAyODI2", "avatar_url": "https://avatars2.githubusercontent.com/u/10502826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kuangliu", "html_url": "https://github.com/kuangliu", "followers_url": "https://api.github.com/users/kuangliu/followers", "following_url": "https://api.github.com/users/kuangliu/following{/other_user}", "gists_url": "https://api.github.com/users/kuangliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/kuangliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kuangliu/subscriptions", "organizations_url": "https://api.github.com/users/kuangliu/orgs", "repos_url": "https://api.github.com/users/kuangliu/repos", "events_url": "https://api.github.com/users/kuangliu/events{/privacy}", "received_events_url": "https://api.github.com/users/kuangliu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-09-17T05:32:37Z", "updated_at": "2017-09-17T05:42:43Z", "closed_at": "2017-09-17T05:42:43Z", "author_association": "NONE", "body_html": "<p>I have a model modified from resnet50, just remove the last <code>avgpool</code> &amp; <code>fc</code>.<br>\nI found if I constantly changing the input size during training, the speed is slow.</p>\n<p>Minimal code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.backends.cudnn <span class=\"pl-k\">as</span> cudnn\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ... remove avgpool &amp; fc from resnet50 here</span>\nnet <span class=\"pl-k\">=</span> resnet50()\nnet.cuda()\nnet <span class=\"pl-k\">=</span> torch.nn.DataParallel(net, <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">range</span>(torch.cuda.device_count()))\ncudnn.benchmark <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n    h <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">400</span>,<span class=\"pl-c1\">600</span>)\n    w <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">400</span>,<span class=\"pl-c1\">600</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> or fix h = w = 600</span>\n    x <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">3</span>,h,w)).cuda()\n\n    t1 <span class=\"pl-k\">=</span> time.time()\n    y <span class=\"pl-k\">=</span> net(x)\n    t2 <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-c1\">print</span>(t2<span class=\"pl-k\">-</span>t1)</pre></div>\n<ol>\n<li>If I fix input size to [600,600], what I got on my 8 Nvidia P40 machine is:</li>\n</ol>\n<pre><code>3.14512705803\n0.11568403244\n0.0255229473114\n0.0228650569916\n0.0235478878021\n0.0225219726562\n0.0436158180237\n0.0222969055176\n0.0223350524902\n0.0227248668671\n</code></pre>\n<ol start=\"2\">\n<li>If I change the input size randomly from [400,600], I got:</li>\n</ol>\n<pre><code>3.12573313713\n0.670918941498\n2.32590889931\n2.3486700058\n2.31507301331\n0.593285083771\n0.68169093132\n2.34181690216\n0.597991943359\n1.74615192413\n</code></pre>\n<p>I also trained with only CPU, both works OK. So I think the reason might related to CUDA overhead. Any ideas to fix this?</p>", "body_text": "I have a model modified from resnet50, just remove the last avgpool & fc.\nI found if I constantly changing the input size during training, the speed is slow.\nMinimal code:\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nfrom torch.autograd import Variable\n\n# ... remove avgpool & fc from resnet50 here\nnet = resnet50()\nnet.cuda()\nnet = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\ncudnn.benchmark = True\n\nfor i in range(10):\n    h = np.random.randint(400,600)\n    w = np.random.randint(400,600)\n    # or fix h = w = 600\n    x = Variable(torch.randn(1,3,h,w)).cuda()\n\n    t1 = time.time()\n    y = net(x)\n    t2 = time.time()\n    print(t2-t1)\n\nIf I fix input size to [600,600], what I got on my 8 Nvidia P40 machine is:\n\n3.14512705803\n0.11568403244\n0.0255229473114\n0.0228650569916\n0.0235478878021\n0.0225219726562\n0.0436158180237\n0.0222969055176\n0.0223350524902\n0.0227248668671\n\n\nIf I change the input size randomly from [400,600], I got:\n\n3.12573313713\n0.670918941498\n2.32590889931\n2.3486700058\n2.31507301331\n0.593285083771\n0.68169093132\n2.34181690216\n0.597991943359\n1.74615192413\n\nI also trained with only CPU, both works OK. So I think the reason might related to CUDA overhead. Any ideas to fix this?", "body": "I have a model modified from resnet50, just remove the last `avgpool` & `fc`.\r\nI found if I constantly changing the input size during training, the speed is slow.\r\n\r\nMinimal code:\r\n```Python\r\nimport time\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.backends.cudnn as cudnn\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\n\r\n# ... remove avgpool & fc from resnet50 here\r\nnet = resnet50()\r\nnet.cuda()\r\nnet = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\r\ncudnn.benchmark = True\r\n\r\nfor i in range(10):\r\n    h = np.random.randint(400,600)\r\n    w = np.random.randint(400,600)\r\n    # or fix h = w = 600\r\n    x = Variable(torch.randn(1,3,h,w)).cuda()\r\n\r\n    t1 = time.time()\r\n    y = net(x)\r\n    t2 = time.time()\r\n    print(t2-t1)\r\n```\r\n1. If I fix input size to [600,600], what I got on my 8 Nvidia P40 machine is:\r\n```\r\n3.14512705803\r\n0.11568403244\r\n0.0255229473114\r\n0.0228650569916\r\n0.0235478878021\r\n0.0225219726562\r\n0.0436158180237\r\n0.0222969055176\r\n0.0223350524902\r\n0.0227248668671\r\n```\r\n\r\n2. If I change the input size randomly from [400,600], I got:\r\n```\r\n3.12573313713\r\n0.670918941498\r\n2.32590889931\r\n2.3486700058\r\n2.31507301331\r\n0.593285083771\r\n0.68169093132\r\n2.34181690216\r\n0.597991943359\r\n1.74615192413\r\n```\r\n\r\nI also trained with only CPU, both works OK. So I think the reason might related to CUDA overhead. Any ideas to fix this?\r\n"}