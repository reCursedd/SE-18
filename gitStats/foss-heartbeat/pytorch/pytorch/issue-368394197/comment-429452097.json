{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/429452097", "html_url": "https://github.com/pytorch/pytorch/issues/12501#issuecomment-429452097", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12501", "id": 429452097, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTQ1MjA5Nw==", "user": {"login": "harrysummer", "id": 1215413, "node_id": "MDQ6VXNlcjEyMTU0MTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1215413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harrysummer", "html_url": "https://github.com/harrysummer", "followers_url": "https://api.github.com/users/harrysummer/followers", "following_url": "https://api.github.com/users/harrysummer/following{/other_user}", "gists_url": "https://api.github.com/users/harrysummer/gists{/gist_id}", "starred_url": "https://api.github.com/users/harrysummer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harrysummer/subscriptions", "organizations_url": "https://api.github.com/users/harrysummer/orgs", "repos_url": "https://api.github.com/users/harrysummer/repos", "events_url": "https://api.github.com/users/harrysummer/events{/privacy}", "received_events_url": "https://api.github.com/users/harrysummer/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-12T20:25:14Z", "updated_at": "2018-10-12T20:25:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>By running your code here, I found out the reason of my GPU memory leak, which is because I commented out the cudaStreamDestroy calls in ThreadLocalCUDAObject (<a href=\"https://github.com/pytorch/pytorch/blob/49256ddb4a80d5385779c9d60c49004a39535d6a/caffe2/core/context_gpu.h#L118\">link</a>). The reason I commented it, is that when the program is shutting down, the cudaStreamDestroy call will be after the CUDA driver shutting down, and error will be throw before the program exits. Instead of commenting that, now I use the following patch:</p>\n<div class=\"highlight highlight-source-diff\"><pre><span class=\"pl-md\"><span class=\"pl-md\">-</span>      for (auto&amp; stream : cuda_streams_[i]) {</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>        if (stream) {</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>          CUDA_CHECK(cudaStreamDestroy(stream));</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>        }</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>      }</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>      for (auto&amp; stream : cuda_streams_[i]) {</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        if (stream) {</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>          cudaError_t err = cudaStreamDestroy(stream);</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>          if (err != cudaErrorCudartUnloading) {</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>            CUDA_CHECK(err);</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>          }</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        }</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>      }</span></pre></div>\n<p>Did you have any modification of caffe2 code, especially related with thread local variables?</p>", "body_text": "By running your code here, I found out the reason of my GPU memory leak, which is because I commented out the cudaStreamDestroy calls in ThreadLocalCUDAObject (link). The reason I commented it, is that when the program is shutting down, the cudaStreamDestroy call will be after the CUDA driver shutting down, and error will be throw before the program exits. Instead of commenting that, now I use the following patch:\n-      for (auto& stream : cuda_streams_[i]) {\n-        if (stream) {\n-          CUDA_CHECK(cudaStreamDestroy(stream));\n-        }\n-      }\n+      for (auto& stream : cuda_streams_[i]) {\n+        if (stream) {\n+          cudaError_t err = cudaStreamDestroy(stream);\n+          if (err != cudaErrorCudartUnloading) {\n+            CUDA_CHECK(err);\n+          }\n+        }\n+      }\nDid you have any modification of caffe2 code, especially related with thread local variables?", "body": "By running your code here, I found out the reason of my GPU memory leak, which is because I commented out the cudaStreamDestroy calls in ThreadLocalCUDAObject ([link](https://github.com/pytorch/pytorch/blob/49256ddb4a80d5385779c9d60c49004a39535d6a/caffe2/core/context_gpu.h#L118)). The reason I commented it, is that when the program is shutting down, the cudaStreamDestroy call will be after the CUDA driver shutting down, and error will be throw before the program exits. Instead of commenting that, now I use the following patch:\r\n```diff\r\n-      for (auto& stream : cuda_streams_[i]) {\r\n-        if (stream) {\r\n-          CUDA_CHECK(cudaStreamDestroy(stream));\r\n-        }\r\n-      }\r\n+      for (auto& stream : cuda_streams_[i]) {\r\n+        if (stream) {\r\n+          cudaError_t err = cudaStreamDestroy(stream);\r\n+          if (err != cudaErrorCudartUnloading) {\r\n+            CUDA_CHECK(err);\r\n+          }\r\n+        }\r\n+      }\r\n```\r\n\r\n Did you have any modification of caffe2 code, especially related with thread local variables?"}