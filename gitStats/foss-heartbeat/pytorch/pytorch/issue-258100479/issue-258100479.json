{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2749", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2749/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2749/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2749/events", "html_url": "https://github.com/pytorch/pytorch/issues/2749", "id": 258100479, "node_id": "MDU6SXNzdWUyNTgxMDA0Nzk=", "number": 2749, "title": "Incorrect torch.triu of subtensor on CUDA", "user": {"login": "OyvindTafjord", "id": 6453366, "node_id": "MDQ6VXNlcjY0NTMzNjY=", "avatar_url": "https://avatars2.githubusercontent.com/u/6453366?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OyvindTafjord", "html_url": "https://github.com/OyvindTafjord", "followers_url": "https://api.github.com/users/OyvindTafjord/followers", "following_url": "https://api.github.com/users/OyvindTafjord/following{/other_user}", "gists_url": "https://api.github.com/users/OyvindTafjord/gists{/gist_id}", "starred_url": "https://api.github.com/users/OyvindTafjord/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OyvindTafjord/subscriptions", "organizations_url": "https://api.github.com/users/OyvindTafjord/orgs", "repos_url": "https://api.github.com/users/OyvindTafjord/repos", "events_url": "https://api.github.com/users/OyvindTafjord/events{/privacy}", "received_events_url": "https://api.github.com/users/OyvindTafjord/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-09-15T16:46:24Z", "updated_at": "2017-10-06T19:28:58Z", "closed_at": "2017-10-06T19:28:58Z", "author_association": "NONE", "body_html": "<p>Here's an example showing how <code>torch.triu</code> (similar for <code>torch.tril</code>) will consistently be off using CUDA (on an AWS P2 instance, Python 3.6, PyTorch 0.2.0):</p>\n<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; my_tensor = torch.ones(3,4,4)\n&gt;&gt;&gt; torch.triu(my_tensor[1])\n 1  1  1  1\n 0  1  1  1\n 0  0  1  1\n 0  0  0  1\n[torch.FloatTensor of size 4x4]\n&gt;&gt;&gt; torch.triu(my_tensor.cuda()[1])\n 1  1  1  1\n 1  0  1  1\n 1  0  0  1\n 1  0  0  0\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n</code></pre>\n<p>Adding a <code>.contiguous()</code>, in case that was a silent issue, does not help:</p>\n<pre><code>&gt;&gt;&gt; torch.triu(my_tensor.cuda()[1].contiguous())\n\n 1  1  1  1\n 1  0  1  1\n 1  0  0  1\n 1  0  0  0\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n</code></pre>\n<p>It gives different results for each index, the first one being correct:</p>\n<pre><code>&gt;&gt;&gt; [torch.triu(t) for t in my_tensor.cuda()]\n[\n 1  1  1  1\n 0  1  1  1\n 0  0  1  1\n 0  0  0  1\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n, \n 1  1  1  1\n 1  0  1  1\n 1  0  0  1\n 1  0  0  0\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n, \n 1  1  1  1\n 1  1  1  1\n 1  1  1  1\n 1  1  1  1\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n]\n</code></pre>\n<p>Here's the same for <code>torch.tril</code>, basically the complement:</p>\n<pre><code>&gt;&gt;&gt; [torch.tril(t) for t in my_tensor.cuda()]\n[\n 1  0  0  0\n 1  1  0  0\n 1  1  1  0\n 1  1  1  1\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n, \n 0  1  0  0\n 0  1  1  0\n 0  1  1  1\n 0  1  1  1\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n, \n 0  0  0  0\n 0  0  0  0\n 0  0  0  0\n 0  0  0  0\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n]\n</code></pre>", "body_text": "Here's an example showing how torch.triu (similar for torch.tril) will consistently be off using CUDA (on an AWS P2 instance, Python 3.6, PyTorch 0.2.0):\n>>> import torch\n>>> my_tensor = torch.ones(3,4,4)\n>>> torch.triu(my_tensor[1])\n 1  1  1  1\n 0  1  1  1\n 0  0  1  1\n 0  0  0  1\n[torch.FloatTensor of size 4x4]\n>>> torch.triu(my_tensor.cuda()[1])\n 1  1  1  1\n 1  0  1  1\n 1  0  0  1\n 1  0  0  0\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n\nAdding a .contiguous(), in case that was a silent issue, does not help:\n>>> torch.triu(my_tensor.cuda()[1].contiguous())\n\n 1  1  1  1\n 1  0  1  1\n 1  0  0  1\n 1  0  0  0\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n\nIt gives different results for each index, the first one being correct:\n>>> [torch.triu(t) for t in my_tensor.cuda()]\n[\n 1  1  1  1\n 0  1  1  1\n 0  0  1  1\n 0  0  0  1\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n, \n 1  1  1  1\n 1  0  1  1\n 1  0  0  1\n 1  0  0  0\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n, \n 1  1  1  1\n 1  1  1  1\n 1  1  1  1\n 1  1  1  1\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n]\n\nHere's the same for torch.tril, basically the complement:\n>>> [torch.tril(t) for t in my_tensor.cuda()]\n[\n 1  0  0  0\n 1  1  0  0\n 1  1  1  0\n 1  1  1  1\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n, \n 0  1  0  0\n 0  1  1  0\n 0  1  1  1\n 0  1  1  1\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n, \n 0  0  0  0\n 0  0  0  0\n 0  0  0  0\n 0  0  0  0\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\n]", "body": "Here's an example showing how `torch.triu` (similar for `torch.tril`) will consistently be off using CUDA (on an AWS P2 instance, Python 3.6, PyTorch 0.2.0):\r\n```\r\n>>> import torch\r\n>>> my_tensor = torch.ones(3,4,4)\r\n>>> torch.triu(my_tensor[1])\r\n 1  1  1  1\r\n 0  1  1  1\r\n 0  0  1  1\r\n 0  0  0  1\r\n[torch.FloatTensor of size 4x4]\r\n>>> torch.triu(my_tensor.cuda()[1])\r\n 1  1  1  1\r\n 1  0  1  1\r\n 1  0  0  1\r\n 1  0  0  0\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n```\r\nAdding a `.contiguous()`, in case that was a silent issue, does not help:\r\n```\r\n>>> torch.triu(my_tensor.cuda()[1].contiguous())\r\n\r\n 1  1  1  1\r\n 1  0  1  1\r\n 1  0  0  1\r\n 1  0  0  0\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n```\r\n\r\nIt gives different results for each index, the first one being correct:\r\n```\r\n>>> [torch.triu(t) for t in my_tensor.cuda()]\r\n[\r\n 1  1  1  1\r\n 0  1  1  1\r\n 0  0  1  1\r\n 0  0  0  1\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n, \r\n 1  1  1  1\r\n 1  0  1  1\r\n 1  0  0  1\r\n 1  0  0  0\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n, \r\n 1  1  1  1\r\n 1  1  1  1\r\n 1  1  1  1\r\n 1  1  1  1\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n]\r\n```\r\nHere's the same for `torch.tril`, basically the complement:\r\n```\r\n>>> [torch.tril(t) for t in my_tensor.cuda()]\r\n[\r\n 1  0  0  0\r\n 1  1  0  0\r\n 1  1  1  0\r\n 1  1  1  1\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n, \r\n 0  1  0  0\r\n 0  1  1  0\r\n 0  1  1  1\r\n 0  1  1  1\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n, \r\n 0  0  0  0\r\n 0  0  0  0\r\n 0  0  0  0\r\n 0  0  0  0\r\n[torch.cuda.FloatTensor of size 4x4 (GPU 0)]\r\n]\r\n```\r\n"}