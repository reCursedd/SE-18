{"url": "https://api.github.com/repos/pytorch/pytorch/issues/543", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/543/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/543/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/543/events", "html_url": "https://github.com/pytorch/pytorch/issues/543", "id": 202363392, "node_id": "MDU6SXNzdWUyMDIzNjMzOTI=", "number": 543, "title": "Error when backprop Conv1d", "user": {"login": "jazzsaxmafia", "id": 8408608, "node_id": "MDQ6VXNlcjg0MDg2MDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/8408608?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jazzsaxmafia", "html_url": "https://github.com/jazzsaxmafia", "followers_url": "https://api.github.com/users/jazzsaxmafia/followers", "following_url": "https://api.github.com/users/jazzsaxmafia/following{/other_user}", "gists_url": "https://api.github.com/users/jazzsaxmafia/gists{/gist_id}", "starred_url": "https://api.github.com/users/jazzsaxmafia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jazzsaxmafia/subscriptions", "organizations_url": "https://api.github.com/users/jazzsaxmafia/orgs", "repos_url": "https://api.github.com/users/jazzsaxmafia/repos", "events_url": "https://api.github.com/users/jazzsaxmafia/events{/privacy}", "received_events_url": "https://api.github.com/users/jazzsaxmafia/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2017-01-22T06:16:09Z", "updated_at": "2017-01-23T02:53:18Z", "closed_at": "2017-01-23T02:53:18Z", "author_association": "NONE", "body_html": "<p>Hello, Thank you for building this great Library.<br>\nI was going through the <a href=\"https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb\">tutorial</a>'s nn example (CNN), and when I changed <code>Conv2d()</code> to <code>Conv1d()</code>, an error was emitted when the <code>loss.backward()</code> was called. The code below runs successfully up to right before <code>loss.backward()</code> (last line).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n     <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n         <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()                                                                                                                     \n         <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv1d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">5</span>)                                                                                                                                                                                                                                                                      \n         <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv1d(<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">5</span>)\n         <span class=\"pl-c1\">self</span>.fc1   <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">384</span>, <span class=\"pl-c1\">120</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> an affine operation: y = Wx + b</span>\n         <span class=\"pl-c1\">self</span>.fc2   <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">120</span>, <span class=\"pl-c1\">84</span>)\n         <span class=\"pl-c1\">self</span>.fc3   <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">84</span>, <span class=\"pl-c1\">10</span>)\n\n     <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n         x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv1(x)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Max pooling over a (2, 2) window</span>\n\n         x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv2(x)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> If the size is a square you can only specify a single number</span>\n\n         x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.num_flat_features(x))\n         x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc1(x))\n         x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc2(x))\n         x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc3(x)\n         <span class=\"pl-k\">return</span> x\n\n     <span class=\"pl-k\">def</span> <span class=\"pl-en\">num_flat_features</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n         size <span class=\"pl-k\">=</span> x.size()[<span class=\"pl-c1\">1</span>:] <span class=\"pl-c\"><span class=\"pl-c\">#</span> all dimensions except the batch dimension</span>\n         num_features <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n         <span class=\"pl-k\">for</span> s <span class=\"pl-k\">in</span> size:\n             num_features <span class=\"pl-k\">*=</span> s\n         <span class=\"pl-k\">return</span> num_features\n\nnet <span class=\"pl-k\">=</span> Net()\n\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>))\nout <span class=\"pl-k\">=</span> net(<span class=\"pl-c1\">input</span>)\n\nnet.zero_grad()\ntarget <span class=\"pl-k\">=</span> Variable(torch.range(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> a dummy target, for example</span>\ncriterion <span class=\"pl-k\">=</span> nn.MSELoss()\nloss <span class=\"pl-k\">=</span> criterion(out, target)\n\nloss.backward()</pre></div>\n<p>The error is <code>AttributeError: 'NoneType' object has no attribute 'dim'</code>, and is from <code>_view3d() </code>function in <code>nn/_functions/conv.py</code>. If I set <code>requires_grad=True</code> for the input <code>Variable</code>, this error disappears, but this doesn't seem like a right solution.</p>\n<p>Thank you.</p>", "body_text": "Hello, Thank you for building this great Library.\nI was going through the tutorial's nn example (CNN), and when I changed Conv2d() to Conv1d(), an error was emitted when the loss.backward() was called. The code below runs successfully up to right before loss.backward() (last line).\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass Net(nn.Module):\n     def __init__(self):\n         super(Net, self).__init__()                                                                                                                     \n         self.conv1 = nn.Conv1d(1, 6, 5)                                                                                                                                                                                                                                                                      \n         self.conv2 = nn.Conv1d(6, 16, 5)\n         self.fc1   = nn.Linear(384, 120) # an affine operation: y = Wx + b\n         self.fc2   = nn.Linear(120, 84)\n         self.fc3   = nn.Linear(84, 10)\n\n     def forward(self, x):\n         x = F.relu(self.conv1(x)) # Max pooling over a (2, 2) window\n\n         x = F.relu(self.conv2(x)) # If the size is a square you can only specify a single number\n\n         x = x.view(-1, self.num_flat_features(x))\n         x = F.relu(self.fc1(x))\n         x = F.relu(self.fc2(x))\n         x = self.fc3(x)\n         return x\n\n     def num_flat_features(self, x):\n         size = x.size()[1:] # all dimensions except the batch dimension\n         num_features = 1\n         for s in size:\n             num_features *= s\n         return num_features\n\nnet = Net()\n\ninput = Variable(torch.randn(1, 1, 32))\nout = net(input)\n\nnet.zero_grad()\ntarget = Variable(torch.range(1, 10))  # a dummy target, for example\ncriterion = nn.MSELoss()\nloss = criterion(out, target)\n\nloss.backward()\nThe error is AttributeError: 'NoneType' object has no attribute 'dim', and is from _view3d() function in nn/_functions/conv.py. If I set requires_grad=True for the input Variable, this error disappears, but this doesn't seem like a right solution.\nThank you.", "body": "Hello, Thank you for building this great Library.\r\nI was going through the [tutorial](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb)'s nn example (CNN), and when I changed `Conv2d()` to `Conv1d()`, an error was emitted when the `loss.backward()` was called. The code below runs successfully up to right before `loss.backward()` (last line).\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\n\r\nclass Net(nn.Module):\r\n     def __init__(self):\r\n         super(Net, self).__init__()                                                                                                                     \r\n         self.conv1 = nn.Conv1d(1, 6, 5)                                                                                                                                                                                                                                                                      \r\n         self.conv2 = nn.Conv1d(6, 16, 5)\r\n         self.fc1   = nn.Linear(384, 120) # an affine operation: y = Wx + b\r\n         self.fc2   = nn.Linear(120, 84)\r\n         self.fc3   = nn.Linear(84, 10)\r\n\r\n     def forward(self, x):\r\n         x = F.relu(self.conv1(x)) # Max pooling over a (2, 2) window\r\n\r\n         x = F.relu(self.conv2(x)) # If the size is a square you can only specify a single number\r\n\r\n         x = x.view(-1, self.num_flat_features(x))\r\n         x = F.relu(self.fc1(x))\r\n         x = F.relu(self.fc2(x))\r\n         x = self.fc3(x)\r\n         return x\r\n\r\n     def num_flat_features(self, x):\r\n         size = x.size()[1:] # all dimensions except the batch dimension\r\n         num_features = 1\r\n         for s in size:\r\n             num_features *= s\r\n         return num_features\r\n\r\nnet = Net()\r\n\r\ninput = Variable(torch.randn(1, 1, 32))\r\nout = net(input)\r\n\r\nnet.zero_grad()\r\ntarget = Variable(torch.range(1, 10))  # a dummy target, for example\r\ncriterion = nn.MSELoss()\r\nloss = criterion(out, target)\r\n\r\nloss.backward()\r\n```\r\n\r\nThe error is `AttributeError: 'NoneType' object has no attribute 'dim'`, and is from `_view3d() `function in `nn/_functions/conv.py`. If I set `requires_grad=True` for the input `Variable`, this error disappears, but this doesn't seem like a right solution.\r\n\r\nThank you."}