{"url": "https://api.github.com/repos/pytorch/pytorch/issues/205", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/205/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/205/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/205/events", "html_url": "https://github.com/pytorch/pytorch/pull/205", "id": 187605675, "node_id": "MDExOlB1bGxSZXF1ZXN0OTI1MTE1MjM=", "number": 205, "title": "adding __repr__ for most modules in nn", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-11-07T01:11:41Z", "updated_at": "2018-11-23T15:31:54Z", "closed_at": "2016-11-07T21:18:51Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/205", "html_url": "https://github.com/pytorch/pytorch/pull/205", "diff_url": "https://github.com/pytorch/pytorch/pull/205.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/205.patch"}, "body_html": "<ul>\n<li>also fixes a bug in ConvTranspose2d</li>\n<li>also adds .apply(func) with the same-ish semantics as in torch</li>\n</ul>\n<p>printing a net will now look more informative, like this:</p>\n<pre><code>&gt;&gt;&gt; print(netG) # a sequential network\nSequential {\n  (0): ConvTranspose2d (100 -&gt; 512, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (1): BatchNorm2d (nChannels: 512    eps: 1e-05 momentum: 0.1 affine: True)\n  (2): ReLU (inplace)\n  (3): ConvTranspose2d (512 -&gt; 256, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (4): BatchNorm2d (nChannels: 256    eps: 1e-05 momentum: 0.1 affine: True)\n  (5): ReLU (inplace)\n  (6): ConvTranspose2d (256 -&gt; 128, size: (4, 4), stride: (4, 4), padding: (2, 2))\n  (7): BatchNorm2d (nChannels: 128    eps: 1e-05 momentum: 0.1 affine: True)\n  (8): ReLU (inplace)\n  (9): ConvTranspose2d (128 -&gt; 64, size: (4, 4), stride: (4, 4), padding: (2, 2))\n  (10): BatchNorm2d (nChannels: 64    eps: 1e-05 momentum: 0.1 affine: True)\n  (11): ReLU (inplace)\n  (12): ConvTranspose2d (64 -&gt; 3, size: (4, 4), stride: (4, 4), padding: (4, 4))\n  (13): Tanh ()\n}\n\n&gt;&gt;&gt; print(netD) # a container with a sequential module called \"main\"\n\n_netD {\n  (main): Sequential {\n  (0): Conv2d (3 -&gt; 64, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (1): LeakyReLU (negative slope: 0.2, inplace)\n  (2): Conv2d (64 -&gt; 128, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (3): BatchNorm2d (nChannels: 128    eps: 1e-05 momentum: 0.1 affine: True)\n  (4): LeakyReLU (negative slope: 0.2, inplace)\n  (5): Conv2d (128 -&gt; 256, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (6): BatchNorm2d (nChannels: 256    eps: 1e-05 momentum: 0.1 affine: True)\n  (7): LeakyReLU (negative slope: 0.2, inplace)\n  (8): Conv2d (256 -&gt; 512, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (9): BatchNorm2d (nChannels: 512    eps: 1e-05 momentum: 0.1 affine: True)\n  (10): LeakyReLU (negative slope: 0.2, inplace)\n  (11): Conv2d (512 -&gt; 1, size: (4, 4), stride: (1, 1))\n  (12): Sigmoid ()\n}\n}\n</code></pre>", "body_text": "also fixes a bug in ConvTranspose2d\nalso adds .apply(func) with the same-ish semantics as in torch\n\nprinting a net will now look more informative, like this:\n>>> print(netG) # a sequential network\nSequential {\n  (0): ConvTranspose2d (100 -> 512, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (1): BatchNorm2d (nChannels: 512    eps: 1e-05 momentum: 0.1 affine: True)\n  (2): ReLU (inplace)\n  (3): ConvTranspose2d (512 -> 256, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (4): BatchNorm2d (nChannels: 256    eps: 1e-05 momentum: 0.1 affine: True)\n  (5): ReLU (inplace)\n  (6): ConvTranspose2d (256 -> 128, size: (4, 4), stride: (4, 4), padding: (2, 2))\n  (7): BatchNorm2d (nChannels: 128    eps: 1e-05 momentum: 0.1 affine: True)\n  (8): ReLU (inplace)\n  (9): ConvTranspose2d (128 -> 64, size: (4, 4), stride: (4, 4), padding: (2, 2))\n  (10): BatchNorm2d (nChannels: 64    eps: 1e-05 momentum: 0.1 affine: True)\n  (11): ReLU (inplace)\n  (12): ConvTranspose2d (64 -> 3, size: (4, 4), stride: (4, 4), padding: (4, 4))\n  (13): Tanh ()\n}\n\n>>> print(netD) # a container with a sequential module called \"main\"\n\n_netD {\n  (main): Sequential {\n  (0): Conv2d (3 -> 64, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (1): LeakyReLU (negative slope: 0.2, inplace)\n  (2): Conv2d (64 -> 128, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (3): BatchNorm2d (nChannels: 128    eps: 1e-05 momentum: 0.1 affine: True)\n  (4): LeakyReLU (negative slope: 0.2, inplace)\n  (5): Conv2d (128 -> 256, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (6): BatchNorm2d (nChannels: 256    eps: 1e-05 momentum: 0.1 affine: True)\n  (7): LeakyReLU (negative slope: 0.2, inplace)\n  (8): Conv2d (256 -> 512, size: (4, 4), stride: (2, 2), padding: (1, 1))\n  (9): BatchNorm2d (nChannels: 512    eps: 1e-05 momentum: 0.1 affine: True)\n  (10): LeakyReLU (negative slope: 0.2, inplace)\n  (11): Conv2d (512 -> 1, size: (4, 4), stride: (1, 1))\n  (12): Sigmoid ()\n}\n}", "body": "- also fixes a bug in ConvTranspose2d\r\n- also adds .apply(func) with the same-ish semantics as in torch\r\n\r\nprinting a net will now look more informative, like this:\r\n\r\n```\r\n>>> print(netG) # a sequential network\r\nSequential {\r\n  (0): ConvTranspose2d (100 -> 512, size: (4, 4), stride: (2, 2), padding: (1, 1))\r\n  (1): BatchNorm2d (nChannels: 512    eps: 1e-05 momentum: 0.1 affine: True)\r\n  (2): ReLU (inplace)\r\n  (3): ConvTranspose2d (512 -> 256, size: (4, 4), stride: (2, 2), padding: (1, 1))\r\n  (4): BatchNorm2d (nChannels: 256    eps: 1e-05 momentum: 0.1 affine: True)\r\n  (5): ReLU (inplace)\r\n  (6): ConvTranspose2d (256 -> 128, size: (4, 4), stride: (4, 4), padding: (2, 2))\r\n  (7): BatchNorm2d (nChannels: 128    eps: 1e-05 momentum: 0.1 affine: True)\r\n  (8): ReLU (inplace)\r\n  (9): ConvTranspose2d (128 -> 64, size: (4, 4), stride: (4, 4), padding: (2, 2))\r\n  (10): BatchNorm2d (nChannels: 64    eps: 1e-05 momentum: 0.1 affine: True)\r\n  (11): ReLU (inplace)\r\n  (12): ConvTranspose2d (64 -> 3, size: (4, 4), stride: (4, 4), padding: (4, 4))\r\n  (13): Tanh ()\r\n}\r\n\r\n>>> print(netD) # a container with a sequential module called \"main\"\r\n\r\n_netD {\r\n  (main): Sequential {\r\n  (0): Conv2d (3 -> 64, size: (4, 4), stride: (2, 2), padding: (1, 1))\r\n  (1): LeakyReLU (negative slope: 0.2, inplace)\r\n  (2): Conv2d (64 -> 128, size: (4, 4), stride: (2, 2), padding: (1, 1))\r\n  (3): BatchNorm2d (nChannels: 128    eps: 1e-05 momentum: 0.1 affine: True)\r\n  (4): LeakyReLU (negative slope: 0.2, inplace)\r\n  (5): Conv2d (128 -> 256, size: (4, 4), stride: (2, 2), padding: (1, 1))\r\n  (6): BatchNorm2d (nChannels: 256    eps: 1e-05 momentum: 0.1 affine: True)\r\n  (7): LeakyReLU (negative slope: 0.2, inplace)\r\n  (8): Conv2d (256 -> 512, size: (4, 4), stride: (2, 2), padding: (1, 1))\r\n  (9): BatchNorm2d (nChannels: 512    eps: 1e-05 momentum: 0.1 affine: True)\r\n  (10): LeakyReLU (negative slope: 0.2, inplace)\r\n  (11): Conv2d (512 -> 1, size: (4, 4), stride: (1, 1))\r\n  (12): Sigmoid ()\r\n}\r\n}\r\n```"}