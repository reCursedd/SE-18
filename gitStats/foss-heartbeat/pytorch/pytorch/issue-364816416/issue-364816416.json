{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12166", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12166/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12166/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12166/events", "html_url": "https://github.com/pytorch/pytorch/issues/12166", "id": 364816416, "node_id": "MDU6SXNzdWUzNjQ4MTY0MTY=", "number": 12166, "title": "GPU hangs after killing the program using DistributedDataparallel Model", "user": {"login": "boxwh1", "id": 41627739, "node_id": "MDQ6VXNlcjQxNjI3NzM5", "avatar_url": "https://avatars2.githubusercontent.com/u/41627739?v=4", "gravatar_id": "", "url": "https://api.github.com/users/boxwh1", "html_url": "https://github.com/boxwh1", "followers_url": "https://api.github.com/users/boxwh1/followers", "following_url": "https://api.github.com/users/boxwh1/following{/other_user}", "gists_url": "https://api.github.com/users/boxwh1/gists{/gist_id}", "starred_url": "https://api.github.com/users/boxwh1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/boxwh1/subscriptions", "organizations_url": "https://api.github.com/users/boxwh1/orgs", "repos_url": "https://api.github.com/users/boxwh1/repos", "events_url": "https://api.github.com/users/boxwh1/events{/privacy}", "received_events_url": "https://api.github.com/users/boxwh1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-09-28T09:43:47Z", "updated_at": "2018-11-13T06:12:45Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2></h2>\n<p>GPU hangs after we kill training program by Ctrl+C.<br>\nThe problem has occured three times when training ResNet-50 with the distributed examples forked from <a href=\"https://github.com/NVIDIA/apex\">https://github.com/NVIDIA/apex</a>.<br>\nWhen GPU hanged, we checked the status of processes.<br>\nWe found there was one  process with the status of D, so we could not kill it even with \"kill -9\" or \"kill -18\".<br>\nThe nvidia-smi also became not available, and we could not use GPUs.<br>\nThen we must reboot the system to use GPUs again......<br>\nBesides, since all gpus hangs, we can not get more information when the problem happens.</p>\n<p>We found the problem is similar with <a href=\"https://discuss.pytorch.org/t/when-i-shut-down-the-pytorch-program-by-kill-i-encountered-the-problem-with-the-gpu/6315\" rel=\"nofollow\">https://discuss.pytorch.org/t/when-i-shut-down-the-pytorch-program-by-kill-i-encountered-the-problem-with-the-gpu/6315</a>.</p>\n\n<h2>To Reproduce</h2>\n<p>The problem happens occasionally. So we can not always reproduce it......</p>\n\n<h2>Expected behavior</h2>\n\n<h2>Environment</h2>\n<p>[environment collection script]</p>\n<ul>\n<li>PyTorch Version (e.g., 1.0): 0.4.1</li>\n<li>OS (e.g., Linux): ubuntu 16.04</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): pip</li>\n<li>Build command you used (if compiling from source):</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 9.0/7.1.2</li>\n<li>GPU models and configuration: P100-PCIe * 8/per nodes, two nodes connected with 100G infiniband</li>\n<li>Any other relevant information:</li>\n</ul>\n<h2>Additional context</h2>\n", "body_text": "GPU hangs after we kill training program by Ctrl+C.\nThe problem has occured three times when training ResNet-50 with the distributed examples forked from https://github.com/NVIDIA/apex.\nWhen GPU hanged, we checked the status of processes.\nWe found there was one  process with the status of D, so we could not kill it even with \"kill -9\" or \"kill -18\".\nThe nvidia-smi also became not available, and we could not use GPUs.\nThen we must reboot the system to use GPUs again......\nBesides, since all gpus hangs, we can not get more information when the problem happens.\nWe found the problem is similar with https://discuss.pytorch.org/t/when-i-shut-down-the-pytorch-program-by-kill-i-encountered-the-problem-with-the-gpu/6315.\n\nTo Reproduce\nThe problem happens occasionally. So we can not always reproduce it......\n\nExpected behavior\n\nEnvironment\n[environment collection script]\n\nPyTorch Version (e.g., 1.0): 0.4.1\nOS (e.g., Linux): ubuntu 16.04\nHow you installed PyTorch (conda, pip, source): pip\nBuild command you used (if compiling from source):\nPython version: 3.6\nCUDA/cuDNN version: 9.0/7.1.2\nGPU models and configuration: P100-PCIe * 8/per nodes, two nodes connected with 100G infiniband\nAny other relevant information:\n\nAdditional context", "body": "## \r\nGPU hangs after we kill training program by Ctrl+C.\r\nThe problem has occured three times when training ResNet-50 with the distributed examples forked from https://github.com/NVIDIA/apex.\r\nWhen GPU hanged, we checked the status of processes.\r\nWe found there was one  process with the status of D, so we could not kill it even with \"kill -9\" or \"kill -18\".\r\nThe nvidia-smi also became not available, and we could not use GPUs.\r\nThen we must reboot the system to use GPUs again......\r\nBesides, since all gpus hangs, we can not get more information when the problem happens.\r\n\r\nWe found the problem is similar with https://discuss.pytorch.org/t/when-i-shut-down-the-pytorch-program-by-kill-i-encountered-the-problem-with-the-gpu/6315.\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\nThe problem happens occasionally. So we can not always reproduce it......\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n## Expected behavior\r\n\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\n[environment collection script]\r\n\r\n - PyTorch Version (e.g., 1.0): 0.4.1\r\n - OS (e.g., Linux): ubuntu 16.04\r\n - How you installed PyTorch (`conda`, `pip`, source): pip\r\n - Build command you used (if compiling from source):\r\n - Python version: 3.6\r\n - CUDA/cuDNN version: 9.0/7.1.2\r\n - GPU models and configuration: P100-PCIe * 8/per nodes, two nodes connected with 100G infiniband\r\n - Any other relevant information:\r\n\r\n## Additional context\r\n\r\n<!-- Add any other context about the problem here. -->\r\n"}