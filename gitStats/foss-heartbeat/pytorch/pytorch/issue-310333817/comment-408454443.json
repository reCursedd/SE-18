{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/408454443", "html_url": "https://github.com/pytorch/pytorch/issues/6171#issuecomment-408454443", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6171", "id": 408454443, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODQ1NDQ0Mw==", "user": {"login": "matthieuheitz", "id": 2835332, "node_id": "MDQ6VXNlcjI4MzUzMzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/2835332?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matthieuheitz", "html_url": "https://github.com/matthieuheitz", "followers_url": "https://api.github.com/users/matthieuheitz/followers", "following_url": "https://api.github.com/users/matthieuheitz/following{/other_user}", "gists_url": "https://api.github.com/users/matthieuheitz/gists{/gist_id}", "starred_url": "https://api.github.com/users/matthieuheitz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matthieuheitz/subscriptions", "organizations_url": "https://api.github.com/users/matthieuheitz/orgs", "repos_url": "https://api.github.com/users/matthieuheitz/repos", "events_url": "https://api.github.com/users/matthieuheitz/events{/privacy}", "received_events_url": "https://api.github.com/users/matthieuheitz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-27T15:30:00Z", "updated_at": "2018-07-27T15:30:00Z", "author_association": "NONE", "body_html": "<p>Starting from <code>master</code>, my first attempt consists of splitting this function: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/bc66d982482e6d6c586d12de5176d9d4bf38eec5/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu#L44\">pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 44\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/bc66d982482e6d6c586d12de5176d9d4bf38eec5\">bc66d98</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L44\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"44\"></td>\n          <td id=\"LC44\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> Tensor&amp; <span class=\"pl-en\">s_addmm_out_sparse_dense_cuda</span>(Tensor&amp; r_, <span class=\"pl-k\">const</span> Tensor&amp; t, <span class=\"pl-k\">const</span> SparseTensor&amp; sparse_, <span class=\"pl-k\">const</span> Tensor&amp; dense, Scalar beta, Scalar alpha) { </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>in 2 parts (see code below), so that I can call the first part at the very beginning, and then the second part every time I need to do a matmat product.</p>\n<p>I know it's not very pretty, but I'm just trying to write something that could work, while waiting for the next release of PyTorch.<br>\nNow I need to do the wrapping in order to be able to call <code>sp_coo_to_csr</code> from PyTorch, and replace the calls to <code>s_addmm_out_sparse_dense_cuda</code> by <code>s_addmm_out_spcsr_dense_cuda</code>, and I'm not sure how to do that...</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a>, can I have some guidance on if my functions could potentially work, and if the wrapping would be easy ? Also, do you have a timeline for when this issue will be adressed ?</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> Takes a SparseTensor (in COO) and returns the CSR representation as a tuple (or array) of Tensors </span>\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> sp_sizes is a vector containing the important sizes of the sparse matrix : [m, k, nnz]</span>\nArrayRef&lt;Tensor&gt; <span class=\"pl-en\">sp_coo_to_csr</span>(<span class=\"pl-k\">const</span> SparseTensor&amp; sparse_, ArrayRef&lt;<span class=\"pl-c1\">int64_t</span>&gt;* sp_sizes) {\n#<span class=\"pl-k\">ifndef</span> __HIP_PLATFORM_HCC__\n  <span class=\"pl-c1\">AT_CHECK</span>(sparse_.<span class=\"pl-c1\">is_cuda</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm: expected 'mat1' to be CUDA, but got CPU<span class=\"pl-pds\">\"</span></span>);\n\n  <span class=\"pl-c1\">AT_CHECK</span>(<span class=\"pl-c1\">_check_device</span>({sparse_}));\n\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> TODO: This error message seems awfully opaque</span>\n  <span class=\"pl-c1\">AT_CHECK</span>(sparse_.<span class=\"pl-c1\">_sparseDims</span>() == <span class=\"pl-c1\">2</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm: 2D tensor expected, got <span class=\"pl-pds\">\"</span></span>, sparse_.<span class=\"pl-c1\">_sparseDims</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>D tensor<span class=\"pl-pds\">\"</span></span>);\n  <span class=\"pl-c1\">AT_CHECK</span>(sparse_.<span class=\"pl-c1\">_denseDims</span>() == <span class=\"pl-c1\">0</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm: scalar values expected, got <span class=\"pl-pds\">\"</span></span>, sparse_.<span class=\"pl-c1\">_denseDims</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>D values<span class=\"pl-pds\">\"</span></span>);\n\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> mxk * kxn = mxn</span>\n  <span class=\"pl-c1\">int64_t</span> m = sparse_.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">0</span>);\n  <span class=\"pl-c1\">int64_t</span> k = sparse_.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">1</span>);\n  <span class=\"pl-c1\">int64_t</span> nnz = sparse.<span class=\"pl-c1\">_nnz</span>();\n\n  sp_sizes[<span class=\"pl-c1\">0</span>] = m;\n  sp_sizes[<span class=\"pl-c1\">1</span>] = k;\n  sp_sizes[<span class=\"pl-c1\">2</span>] = nnz;\n\n  SparseTensor sparse = sparse_.<span class=\"pl-c1\">coalesce</span>();\n\n  LongTensor indices = sparse.<span class=\"pl-c1\">_indices</span>();\n  Tensor values = sparse.<span class=\"pl-c1\">_values</span>();\n\n  LongTensor rowIndices = indices.<span class=\"pl-c1\">select</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>);\n  LongTensor colIndices = indices.<span class=\"pl-c1\">select</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>);\n  IntTensor csr = <span class=\"pl-c1\">_to_csr_int</span>(rowIndices, m, nnz);\n  IntTensor colIndicesInt = <span class=\"pl-c1\">at::empty</span>({colIndices.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">0</span>)}, indices.<span class=\"pl-c1\">type</span>().<span class=\"pl-c1\">toScalarType</span>(<span class=\"pl-c1\">kInt</span>));\n  colIndicesInt.<span class=\"pl-c1\">copy_</span>(colIndices);\n\n  <span class=\"pl-k\">return</span> {colIndicesInt, csr, values};\n#<span class=\"pl-k\">else</span>\n  <span class=\"pl-c1\">AT_ERROR</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>s_addmm_out_sparse_dense_cuda: HIP not supported<span class=\"pl-pds\">\"</span></span>);\n#<span class=\"pl-k\">endif</span>\n}\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> Takes the sparse matrix as a array of Tensors, and an array of sizes</span>\nTensor&amp; <span class=\"pl-en\">s_addmm_out_spcsr_dense_cuda</span>(Tensor&amp; r_, <span class=\"pl-k\">const</span> Tensor&amp; t, <span class=\"pl-k\">const</span> ArrayRef&lt;Tensor&gt; sparse_, <span class=\"pl-k\">const</span> ArrayRef&lt;<span class=\"pl-c1\">int64_t</span>&gt; sp_sizes, <span class=\"pl-k\">const</span> Tensor&amp; dense, Scalar beta, Scalar alpha) {\n#<span class=\"pl-k\">ifndef</span> __HIP_PLATFORM_HCC__\n  <span class=\"pl-c1\">AT_ASSERT</span>(t.<span class=\"pl-c1\">is_cuda</span>()); <span class=\"pl-c\"><span class=\"pl-c\">//</span> dispatch argument</span>\n  <span class=\"pl-c1\">AT_CHECK</span>(r_.<span class=\"pl-c1\">is_cuda</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm: expected 'out' to be CUDA, but got CPU<span class=\"pl-pds\">\"</span></span>);\n  <span class=\"pl-c1\">AT_CHECK</span>(dense.<span class=\"pl-c1\">is_cuda</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm: expected 'mat2' to be CUDA, but got CPU<span class=\"pl-pds\">\"</span></span>);\n\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> Get back the info on the sparse matrix</span>\n  IntTensor colIndicesInt = sparse_[<span class=\"pl-c1\">0</span>];\n  IntTensor csr = sparse_[<span class=\"pl-c1\">1</span>];\n  Tensor values = sparse_[<span class=\"pl-c1\">2</span>];\n\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> mxk * kxn = mxn</span>\n  <span class=\"pl-c1\">int64_t</span> m = sp_sizes[<span class=\"pl-c1\">0</span>];\n  <span class=\"pl-c1\">int64_t</span> k = sp_sizes[<span class=\"pl-c1\">1</span>];\n  <span class=\"pl-c1\">int64_t</span> nnz = sp_sizes[<span class=\"pl-c1\">2</span>];\n  <span class=\"pl-c1\">int64_t</span> n = dense.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">1</span>);\n\n  <span class=\"pl-c1\">AT_CHECK</span>(<span class=\"pl-c1\">_check_device</span>({colIndicesInt,csr,values}));\n  <span class=\"pl-c1\">AT_CHECK</span>(<span class=\"pl-c1\">_check_device</span>({r_, t, dense}));\n\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> TODO: This error message seems awfully opaque</span>\n  <span class=\"pl-c1\">AT_CHECK</span>(dense.<span class=\"pl-c1\">dim</span>() == <span class=\"pl-c1\">2</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm: 2D tensor expected, got <span class=\"pl-pds\">\"</span></span>, dense.<span class=\"pl-c1\">dim</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>D tensor<span class=\"pl-pds\">\"</span></span>);\n\n\n  <span class=\"pl-c1\">AT_CHECK</span>(t.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">0</span>) == m,\n\t  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm: Argument #1 (t): Expected dim 0 size <span class=\"pl-pds\">\"</span></span>, m, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>, got <span class=\"pl-pds\">\"</span></span>, t.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">0</span>));\n  <span class=\"pl-c1\">AT_CHECK</span>(t.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">1</span>) == n,\n\t  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm: Argument #1 (t): Expected dim 1 size <span class=\"pl-pds\">\"</span></span>, n, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>, got <span class=\"pl-pds\">\"</span></span>, t.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">1</span>));\n  <span class=\"pl-c1\">AT_CHECK</span>(dense.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">0</span>) == k,\n\t  <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm: Argument #3 (dense): Expected dim 0 size <span class=\"pl-pds\">\"</span></span>, k, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>, got <span class=\"pl-pds\">\"</span></span>, dense.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">0</span>));\n\n  r_.<span class=\"pl-c1\">resize_</span>({m, n});\n\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> No half support, so we don't have to use CUDATypeConversion</span>\n  Tensor r__;\n  <span class=\"pl-c1\">AT_DISPATCH_FLOATING_TYPES</span>(\n\t  values.<span class=\"pl-c1\">type</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>addmm_sparse_cuda<span class=\"pl-pds\">\"</span></span>, [&amp;] {\n\t\t<span class=\"pl-c1\">scalar_t</span> cast_beta = beta.<span class=\"pl-smi\">to</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;();\n\t\t<span class=\"pl-c1\">scalar_t</span> cast_alpha = alpha.<span class=\"pl-smi\">to</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;();\n\t\t<span class=\"pl-k\">if</span> (cast_beta == <span class=\"pl-c1\">0</span>) {\n\t\t  r_.<span class=\"pl-c1\">zero_</span>();\n\t\t} <span class=\"pl-k\">else</span> <span class=\"pl-k\">if</span> (cast_beta == <span class=\"pl-c1\">1</span>) {\n\t\t  <span class=\"pl-k\">if</span> (!<span class=\"pl-c1\">isSameTensor</span>(t, r_)) {\n\t\t\tr_.<span class=\"pl-c1\">copy_</span>(t);\n\t\t  }\n\t\t} <span class=\"pl-k\">else</span> {\n\t\t  <span class=\"pl-c1\">at::mul_out</span>(r_, t, beta);\n\t\t}\n\n\t\t<span class=\"pl-c\"><span class=\"pl-c\">/*</span> r_ <span class=\"pl-c\">*/</span></span>\n\t\t<span class=\"pl-k\">if</span>(r_.<span class=\"pl-c1\">stride</span>(<span class=\"pl-c1\">0</span>) == <span class=\"pl-c1\">1</span> &amp;&amp; r_.<span class=\"pl-c1\">stride</span>(<span class=\"pl-c1\">1</span>) == r_.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">0</span>)) {\n\t\t  r__ = r_;\n\t\t} <span class=\"pl-k\">else</span> {\n\t\t  <span class=\"pl-c\"><span class=\"pl-c\">//</span> TODO: how... strange</span>\n\t\t  r__ = r_.<span class=\"pl-c1\">transpose</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>).<span class=\"pl-c1\">clone</span>();\n\t\t  r__.<span class=\"pl-c1\">transpose_</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>);\n\t\t}\n\n\t\t<span class=\"pl-c\"><span class=\"pl-c\">/*</span> dense <span class=\"pl-c\">*/</span></span>\n\t\tTensor dense_;\n\t\t<span class=\"pl-k\">char</span> transpose_dense;\n\t\t<span class=\"pl-k\">if</span>(dense.<span class=\"pl-c1\">stride</span>(<span class=\"pl-c1\">0</span>) == <span class=\"pl-c1\">1</span> &amp;&amp; dense.<span class=\"pl-c1\">stride</span>(<span class=\"pl-c1\">1</span>) == dense.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">0</span>)) {\n\t\t  transpose_dense = <span class=\"pl-s\"><span class=\"pl-pds\">'</span>n<span class=\"pl-pds\">'</span></span>;\n\t\t  dense_ = dense;\n\t\t} <span class=\"pl-k\">else</span> <span class=\"pl-k\">if</span>(dense.<span class=\"pl-c1\">stride</span>(<span class=\"pl-c1\">1</span>) == <span class=\"pl-c1\">1</span> &amp;&amp; dense.<span class=\"pl-c1\">stride</span>(<span class=\"pl-c1\">0</span>) != dense.<span class=\"pl-c1\">size</span>(<span class=\"pl-c1\">1</span>)) {\n\t\t  transpose_dense = <span class=\"pl-s\"><span class=\"pl-pds\">'</span>t<span class=\"pl-pds\">'</span></span>;\n\t\t  dense_ = dense;\n\t\t} <span class=\"pl-k\">else</span> {\n\t\t  transpose_dense = <span class=\"pl-s\"><span class=\"pl-pds\">'</span>t<span class=\"pl-pds\">'</span></span>;\n\t\t  dense_ = dense.<span class=\"pl-c1\">contiguous</span>();\n\t\t}\n\n\t\t<span class=\"pl-c1\">sparse::cuda::csrmm2</span>(\n\t\t  <span class=\"pl-s\"><span class=\"pl-pds\">'</span>n<span class=\"pl-pds\">'</span></span>,\n\t\t  transpose_dense,\n\t\t  m,\n\t\t  n,\n\t\t  k,\n\t\t  nnz,\n\t\t  cast_alpha,\n\t\t  values.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;(),\n\t\t  csr.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">int32_t</span>&gt;(),\n\t\t  colIndicesInt.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">int32_t</span>&gt;(),\n\t\t  dense_.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;(),\n\t\t  (transpose_dense == <span class=\"pl-s\"><span class=\"pl-pds\">'</span>n<span class=\"pl-pds\">'</span></span> ? dense_.<span class=\"pl-c1\">stride</span>(<span class=\"pl-c1\">1</span>) : dense_.<span class=\"pl-c1\">stride</span>(<span class=\"pl-c1\">0</span>)),\n\t\t  cast_beta,\n\t\t  r__.<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;(),\n\t\t  r__.<span class=\"pl-c1\">stride</span>(<span class=\"pl-c1\">1</span>));\n\n\t  });\n\n  r_.<span class=\"pl-c1\">copy_</span>(r__);\n  <span class=\"pl-k\">return</span> r_;\n#<span class=\"pl-k\">else</span>\n  <span class=\"pl-c1\">AT_ERROR</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>s_addmm_out_sparse_dense_cuda: HIP not supported<span class=\"pl-pds\">\"</span></span>);\n#<span class=\"pl-k\">endif</span>\n}\n</pre></div>", "body_text": "Starting from master, my first attempt consists of splitting this function: \n  \n    \n      pytorch/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu\n    \n    \n         Line 44\n      in\n      bc66d98\n    \n    \n    \n    \n\n        \n          \n           Tensor& s_addmm_out_sparse_dense_cuda(Tensor& r_, const Tensor& t, const SparseTensor& sparse_, const Tensor& dense, Scalar beta, Scalar alpha) { \n        \n    \n  \n\n\nin 2 parts (see code below), so that I can call the first part at the very beginning, and then the second part every time I need to do a matmat product.\nI know it's not very pretty, but I'm just trying to write something that could work, while waiting for the next release of PyTorch.\nNow I need to do the wrapping in order to be able to call sp_coo_to_csr from PyTorch, and replace the calls to s_addmm_out_sparse_dense_cuda by s_addmm_out_spcsr_dense_cuda, and I'm not sure how to do that...\n@ezyang, can I have some guidance on if my functions could potentially work, and if the wrapping would be easy ? Also, do you have a timeline for when this issue will be adressed ?\n// Takes a SparseTensor (in COO) and returns the CSR representation as a tuple (or array) of Tensors \n// sp_sizes is a vector containing the important sizes of the sparse matrix : [m, k, nnz]\nArrayRef<Tensor> sp_coo_to_csr(const SparseTensor& sparse_, ArrayRef<int64_t>* sp_sizes) {\n#ifndef __HIP_PLATFORM_HCC__\n  AT_CHECK(sparse_.is_cuda(), \"addmm: expected 'mat1' to be CUDA, but got CPU\");\n\n  AT_CHECK(_check_device({sparse_}));\n\n  // TODO: This error message seems awfully opaque\n  AT_CHECK(sparse_._sparseDims() == 2, \"addmm: 2D tensor expected, got \", sparse_._sparseDims(), \"D tensor\");\n  AT_CHECK(sparse_._denseDims() == 0, \"addmm: scalar values expected, got \", sparse_._denseDims(), \"D values\");\n\n  // mxk * kxn = mxn\n  int64_t m = sparse_.size(0);\n  int64_t k = sparse_.size(1);\n  int64_t nnz = sparse._nnz();\n\n  sp_sizes[0] = m;\n  sp_sizes[1] = k;\n  sp_sizes[2] = nnz;\n\n  SparseTensor sparse = sparse_.coalesce();\n\n  LongTensor indices = sparse._indices();\n  Tensor values = sparse._values();\n\n  LongTensor rowIndices = indices.select(0, 0);\n  LongTensor colIndices = indices.select(0, 1);\n  IntTensor csr = _to_csr_int(rowIndices, m, nnz);\n  IntTensor colIndicesInt = at::empty({colIndices.size(0)}, indices.type().toScalarType(kInt));\n  colIndicesInt.copy_(colIndices);\n\n  return {colIndicesInt, csr, values};\n#else\n  AT_ERROR(\"s_addmm_out_sparse_dense_cuda: HIP not supported\");\n#endif\n}\n\n// Takes the sparse matrix as a array of Tensors, and an array of sizes\nTensor& s_addmm_out_spcsr_dense_cuda(Tensor& r_, const Tensor& t, const ArrayRef<Tensor> sparse_, const ArrayRef<int64_t> sp_sizes, const Tensor& dense, Scalar beta, Scalar alpha) {\n#ifndef __HIP_PLATFORM_HCC__\n  AT_ASSERT(t.is_cuda()); // dispatch argument\n  AT_CHECK(r_.is_cuda(), \"addmm: expected 'out' to be CUDA, but got CPU\");\n  AT_CHECK(dense.is_cuda(), \"addmm: expected 'mat2' to be CUDA, but got CPU\");\n\n  // Get back the info on the sparse matrix\n  IntTensor colIndicesInt = sparse_[0];\n  IntTensor csr = sparse_[1];\n  Tensor values = sparse_[2];\n\n  // mxk * kxn = mxn\n  int64_t m = sp_sizes[0];\n  int64_t k = sp_sizes[1];\n  int64_t nnz = sp_sizes[2];\n  int64_t n = dense.size(1);\n\n  AT_CHECK(_check_device({colIndicesInt,csr,values}));\n  AT_CHECK(_check_device({r_, t, dense}));\n\n  // TODO: This error message seems awfully opaque\n  AT_CHECK(dense.dim() == 2, \"addmm: 2D tensor expected, got \", dense.dim(), \"D tensor\");\n\n\n  AT_CHECK(t.size(0) == m,\n\t  \"addmm: Argument #1 (t): Expected dim 0 size \", m, \", got \", t.size(0));\n  AT_CHECK(t.size(1) == n,\n\t  \"addmm: Argument #1 (t): Expected dim 1 size \", n, \", got \", t.size(1));\n  AT_CHECK(dense.size(0) == k,\n\t  \"addmm: Argument #3 (dense): Expected dim 0 size \", k, \", got \", dense.size(0));\n\n  r_.resize_({m, n});\n\n  // No half support, so we don't have to use CUDATypeConversion\n  Tensor r__;\n  AT_DISPATCH_FLOATING_TYPES(\n\t  values.type(), \"addmm_sparse_cuda\", [&] {\n\t\tscalar_t cast_beta = beta.to<scalar_t>();\n\t\tscalar_t cast_alpha = alpha.to<scalar_t>();\n\t\tif (cast_beta == 0) {\n\t\t  r_.zero_();\n\t\t} else if (cast_beta == 1) {\n\t\t  if (!isSameTensor(t, r_)) {\n\t\t\tr_.copy_(t);\n\t\t  }\n\t\t} else {\n\t\t  at::mul_out(r_, t, beta);\n\t\t}\n\n\t\t/* r_ */\n\t\tif(r_.stride(0) == 1 && r_.stride(1) == r_.size(0)) {\n\t\t  r__ = r_;\n\t\t} else {\n\t\t  // TODO: how... strange\n\t\t  r__ = r_.transpose(0, 1).clone();\n\t\t  r__.transpose_(0, 1);\n\t\t}\n\n\t\t/* dense */\n\t\tTensor dense_;\n\t\tchar transpose_dense;\n\t\tif(dense.stride(0) == 1 && dense.stride(1) == dense.size(0)) {\n\t\t  transpose_dense = 'n';\n\t\t  dense_ = dense;\n\t\t} else if(dense.stride(1) == 1 && dense.stride(0) != dense.size(1)) {\n\t\t  transpose_dense = 't';\n\t\t  dense_ = dense;\n\t\t} else {\n\t\t  transpose_dense = 't';\n\t\t  dense_ = dense.contiguous();\n\t\t}\n\n\t\tsparse::cuda::csrmm2(\n\t\t  'n',\n\t\t  transpose_dense,\n\t\t  m,\n\t\t  n,\n\t\t  k,\n\t\t  nnz,\n\t\t  cast_alpha,\n\t\t  values.data<scalar_t>(),\n\t\t  csr.data<int32_t>(),\n\t\t  colIndicesInt.data<int32_t>(),\n\t\t  dense_.data<scalar_t>(),\n\t\t  (transpose_dense == 'n' ? dense_.stride(1) : dense_.stride(0)),\n\t\t  cast_beta,\n\t\t  r__.data<scalar_t>(),\n\t\t  r__.stride(1));\n\n\t  });\n\n  r_.copy_(r__);\n  return r_;\n#else\n  AT_ERROR(\"s_addmm_out_sparse_dense_cuda: HIP not supported\");\n#endif\n}", "body": "Starting from `master`, my first attempt consists of splitting this function: https://github.com/pytorch/pytorch/blob/bc66d982482e6d6c586d12de5176d9d4bf38eec5/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu#L44\r\n\r\nin 2 parts (see code below), so that I can call the first part at the very beginning, and then the second part every time I need to do a matmat product.\r\n\r\nI know it's not very pretty, but I'm just trying to write something that could work, while waiting for the next release of PyTorch.\r\nNow I need to do the wrapping in order to be able to call `sp_coo_to_csr` from PyTorch, and replace the calls to `s_addmm_out_sparse_dense_cuda` by `s_addmm_out_spcsr_dense_cuda`, and I'm not sure how to do that...\r\n\r\n@ezyang, can I have some guidance on if my functions could potentially work, and if the wrapping would be easy ? Also, do you have a timeline for when this issue will be adressed ?\r\n\r\n```c++\r\n// Takes a SparseTensor (in COO) and returns the CSR representation as a tuple (or array) of Tensors \r\n// sp_sizes is a vector containing the important sizes of the sparse matrix : [m, k, nnz]\r\nArrayRef<Tensor> sp_coo_to_csr(const SparseTensor& sparse_, ArrayRef<int64_t>* sp_sizes) {\r\n#ifndef __HIP_PLATFORM_HCC__\r\n  AT_CHECK(sparse_.is_cuda(), \"addmm: expected 'mat1' to be CUDA, but got CPU\");\r\n\r\n  AT_CHECK(_check_device({sparse_}));\r\n\r\n  // TODO: This error message seems awfully opaque\r\n  AT_CHECK(sparse_._sparseDims() == 2, \"addmm: 2D tensor expected, got \", sparse_._sparseDims(), \"D tensor\");\r\n  AT_CHECK(sparse_._denseDims() == 0, \"addmm: scalar values expected, got \", sparse_._denseDims(), \"D values\");\r\n\r\n  // mxk * kxn = mxn\r\n  int64_t m = sparse_.size(0);\r\n  int64_t k = sparse_.size(1);\r\n  int64_t nnz = sparse._nnz();\r\n\r\n  sp_sizes[0] = m;\r\n  sp_sizes[1] = k;\r\n  sp_sizes[2] = nnz;\r\n\r\n  SparseTensor sparse = sparse_.coalesce();\r\n\r\n  LongTensor indices = sparse._indices();\r\n  Tensor values = sparse._values();\r\n\r\n  LongTensor rowIndices = indices.select(0, 0);\r\n  LongTensor colIndices = indices.select(0, 1);\r\n  IntTensor csr = _to_csr_int(rowIndices, m, nnz);\r\n  IntTensor colIndicesInt = at::empty({colIndices.size(0)}, indices.type().toScalarType(kInt));\r\n  colIndicesInt.copy_(colIndices);\r\n\r\n  return {colIndicesInt, csr, values};\r\n#else\r\n  AT_ERROR(\"s_addmm_out_sparse_dense_cuda: HIP not supported\");\r\n#endif\r\n}\r\n\r\n// Takes the sparse matrix as a array of Tensors, and an array of sizes\r\nTensor& s_addmm_out_spcsr_dense_cuda(Tensor& r_, const Tensor& t, const ArrayRef<Tensor> sparse_, const ArrayRef<int64_t> sp_sizes, const Tensor& dense, Scalar beta, Scalar alpha) {\r\n#ifndef __HIP_PLATFORM_HCC__\r\n  AT_ASSERT(t.is_cuda()); // dispatch argument\r\n  AT_CHECK(r_.is_cuda(), \"addmm: expected 'out' to be CUDA, but got CPU\");\r\n  AT_CHECK(dense.is_cuda(), \"addmm: expected 'mat2' to be CUDA, but got CPU\");\r\n\r\n  // Get back the info on the sparse matrix\r\n  IntTensor colIndicesInt = sparse_[0];\r\n  IntTensor csr = sparse_[1];\r\n  Tensor values = sparse_[2];\r\n\r\n  // mxk * kxn = mxn\r\n  int64_t m = sp_sizes[0];\r\n  int64_t k = sp_sizes[1];\r\n  int64_t nnz = sp_sizes[2];\r\n  int64_t n = dense.size(1);\r\n\r\n  AT_CHECK(_check_device({colIndicesInt,csr,values}));\r\n  AT_CHECK(_check_device({r_, t, dense}));\r\n\r\n  // TODO: This error message seems awfully opaque\r\n  AT_CHECK(dense.dim() == 2, \"addmm: 2D tensor expected, got \", dense.dim(), \"D tensor\");\r\n\r\n\r\n  AT_CHECK(t.size(0) == m,\r\n\t  \"addmm: Argument #1 (t): Expected dim 0 size \", m, \", got \", t.size(0));\r\n  AT_CHECK(t.size(1) == n,\r\n\t  \"addmm: Argument #1 (t): Expected dim 1 size \", n, \", got \", t.size(1));\r\n  AT_CHECK(dense.size(0) == k,\r\n\t  \"addmm: Argument #3 (dense): Expected dim 0 size \", k, \", got \", dense.size(0));\r\n\r\n  r_.resize_({m, n});\r\n\r\n  // No half support, so we don't have to use CUDATypeConversion\r\n  Tensor r__;\r\n  AT_DISPATCH_FLOATING_TYPES(\r\n\t  values.type(), \"addmm_sparse_cuda\", [&] {\r\n\t\tscalar_t cast_beta = beta.to<scalar_t>();\r\n\t\tscalar_t cast_alpha = alpha.to<scalar_t>();\r\n\t\tif (cast_beta == 0) {\r\n\t\t  r_.zero_();\r\n\t\t} else if (cast_beta == 1) {\r\n\t\t  if (!isSameTensor(t, r_)) {\r\n\t\t\tr_.copy_(t);\r\n\t\t  }\r\n\t\t} else {\r\n\t\t  at::mul_out(r_, t, beta);\r\n\t\t}\r\n\r\n\t\t/* r_ */\r\n\t\tif(r_.stride(0) == 1 && r_.stride(1) == r_.size(0)) {\r\n\t\t  r__ = r_;\r\n\t\t} else {\r\n\t\t  // TODO: how... strange\r\n\t\t  r__ = r_.transpose(0, 1).clone();\r\n\t\t  r__.transpose_(0, 1);\r\n\t\t}\r\n\r\n\t\t/* dense */\r\n\t\tTensor dense_;\r\n\t\tchar transpose_dense;\r\n\t\tif(dense.stride(0) == 1 && dense.stride(1) == dense.size(0)) {\r\n\t\t  transpose_dense = 'n';\r\n\t\t  dense_ = dense;\r\n\t\t} else if(dense.stride(1) == 1 && dense.stride(0) != dense.size(1)) {\r\n\t\t  transpose_dense = 't';\r\n\t\t  dense_ = dense;\r\n\t\t} else {\r\n\t\t  transpose_dense = 't';\r\n\t\t  dense_ = dense.contiguous();\r\n\t\t}\r\n\r\n\t\tsparse::cuda::csrmm2(\r\n\t\t  'n',\r\n\t\t  transpose_dense,\r\n\t\t  m,\r\n\t\t  n,\r\n\t\t  k,\r\n\t\t  nnz,\r\n\t\t  cast_alpha,\r\n\t\t  values.data<scalar_t>(),\r\n\t\t  csr.data<int32_t>(),\r\n\t\t  colIndicesInt.data<int32_t>(),\r\n\t\t  dense_.data<scalar_t>(),\r\n\t\t  (transpose_dense == 'n' ? dense_.stride(1) : dense_.stride(0)),\r\n\t\t  cast_beta,\r\n\t\t  r__.data<scalar_t>(),\r\n\t\t  r__.stride(1));\r\n\r\n\t  });\r\n\r\n  r_.copy_(r__);\r\n  return r_;\r\n#else\r\n  AT_ERROR(\"s_addmm_out_sparse_dense_cuda: HIP not supported\");\r\n#endif\r\n}\r\n\r\n```\r\n"}