{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1506", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1506/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1506/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1506/events", "html_url": "https://github.com/pytorch/pytorch/pull/1506", "id": 226888572, "node_id": "MDExOlB1bGxSZXF1ZXN0MTE5MzUxOTA0", "number": 1506, "title": "Autograd bugfixes and improvements", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-05-07T19:53:15Z", "updated_at": "2017-05-10T14:43:18Z", "closed_at": "2017-05-10T14:43:15Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1506", "html_url": "https://github.com/pytorch/pytorch/pull/1506", "diff_url": "https://github.com/pytorch/pytorch/pull/1506.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1506.patch"}, "body_html": "<ul>\n<li>Turns out we can't really take a short path for volatile inputs. The flags aren't propagated correctly in case a non-volatile input is modified in an in-place op with a volatile Variable -- e.g. <code>x[2] = y</code>, where <code>x</code> is not volatile, and <code>y</code> is. Also, volatile ops didn't mark inputs as dirty.</li>\n<li>Replace deprecated <code>retain_variables</code> with <code>retain_graph</code></li>\n<li>Add new flags to <code>Variable.backward</code> (and delegate to <code>torch.autograd.backward</code> to simplify the code)</li>\n<li>Minor fix for <code>Prod</code> backward (<code>grad_input</code> wasn't volatile, even if <code>grad_output</code> was)</li>\n<li>If a Variable has non-volatile gradient (i.e. when higher order grads are used), <code>model.zero_grad()</code> now replaces the grad with a new zero-filled Variable. This is useful if one wants to zero grad but used the old Variable to compute sth. Additionally assignments to <code>.grad</code> are now allowed (new grad is checked for its device, type and size).</li>\n<li>Exposed <code>.variable</code> attribute from <code>AccumulateGrad</code> nodes (cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4953728\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/szagoruyko\">@szagoruyko</a>)</li>\n</ul>", "body_text": "Turns out we can't really take a short path for volatile inputs. The flags aren't propagated correctly in case a non-volatile input is modified in an in-place op with a volatile Variable -- e.g. x[2] = y, where x is not volatile, and y is. Also, volatile ops didn't mark inputs as dirty.\nReplace deprecated retain_variables with retain_graph\nAdd new flags to Variable.backward (and delegate to torch.autograd.backward to simplify the code)\nMinor fix for Prod backward (grad_input wasn't volatile, even if grad_output was)\nIf a Variable has non-volatile gradient (i.e. when higher order grads are used), model.zero_grad() now replaces the grad with a new zero-filled Variable. This is useful if one wants to zero grad but used the old Variable to compute sth. Additionally assignments to .grad are now allowed (new grad is checked for its device, type and size).\nExposed .variable attribute from AccumulateGrad nodes (cc @szagoruyko)", "body": "* Turns out we can't really take a short path for volatile inputs. The flags aren't propagated correctly in case a non-volatile input is modified in an in-place op with a volatile Variable -- e.g. `x[2] = y`, where `x` is not volatile, and `y` is. Also, volatile ops didn't mark inputs as dirty.\r\n* Replace deprecated `retain_variables` with `retain_graph` \r\n* Add new flags to `Variable.backward` (and delegate to `torch.autograd.backward` to simplify the code)\r\n* Minor fix for `Prod` backward (`grad_input` wasn't volatile, even if `grad_output` was)\r\n* If a Variable has non-volatile gradient (i.e. when higher order grads are used), `model.zero_grad()` now replaces the grad with a new zero-filled Variable. This is useful if one wants to zero grad but used the old Variable to compute sth. Additionally assignments to `.grad` are now allowed (new grad is checked for its device, type and size).\r\n* Exposed `.variable` attribute from `AccumulateGrad` nodes (cc @szagoruyko)"}