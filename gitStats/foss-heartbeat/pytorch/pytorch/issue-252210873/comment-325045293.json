{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/325045293", "html_url": "https://github.com/pytorch/pytorch/issues/2517#issuecomment-325045293", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2517", "id": 325045293, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNTA0NTI5Mw==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-25T22:10:23Z", "updated_at": "2017-08-25T22:10:23Z", "author_association": "MEMBER", "body_html": "<p>The problem is that <code>torch.manual_seed</code> initializes the CUDA driver, which breaks horribly when re-initialized across forks. Any cuda call, including <code>cudaGetDeviceCount</code>, seems to initialize the driver.</p>\n<p>We need to avoid the <code>cudaGetDeviceCount</code> call in <code>manual_seed</code>. We should check if we've already initialized CUDA (i.e. <code>THCState *state</code> is not null). If we haven't created a <code>THCState</code> yet, we should just store the seed somwehere internally.</p>\n<p>There's a similar problem in <code>engine.cpp</code> because it calls <code>cudaGetDeviceCount</code>. We'll need to avoid that call too if the <code>THCState*</code> is null.</p>", "body_text": "The problem is that torch.manual_seed initializes the CUDA driver, which breaks horribly when re-initialized across forks. Any cuda call, including cudaGetDeviceCount, seems to initialize the driver.\nWe need to avoid the cudaGetDeviceCount call in manual_seed. We should check if we've already initialized CUDA (i.e. THCState *state is not null). If we haven't created a THCState yet, we should just store the seed somwehere internally.\nThere's a similar problem in engine.cpp because it calls cudaGetDeviceCount. We'll need to avoid that call too if the THCState* is null.", "body": "The problem is that `torch.manual_seed` initializes the CUDA driver, which breaks horribly when re-initialized across forks. Any cuda call, including `cudaGetDeviceCount`, seems to initialize the driver.\r\n\r\nWe need to avoid the `cudaGetDeviceCount` call in `manual_seed`. We should check if we've already initialized CUDA (i.e. `THCState *state` is not null). If we haven't created a `THCState` yet, we should just store the seed somwehere internally.\r\n\r\nThere's a similar problem in `engine.cpp` because it calls `cudaGetDeviceCount`. We'll need to avoid that call too if the `THCState*` is null."}