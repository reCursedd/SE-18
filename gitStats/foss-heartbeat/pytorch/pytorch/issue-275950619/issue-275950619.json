{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3828", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3828/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3828/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3828/events", "html_url": "https://github.com/pytorch/pytorch/issues/3828", "id": 275950619, "node_id": "MDU6SXNzdWUyNzU5NTA2MTk=", "number": 3828, "title": "build failed in with cuda 9.0 & python 3.6 & gcc 6.4", "user": {"login": "snowzjy", "id": 949681, "node_id": "MDQ6VXNlcjk0OTY4MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/949681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/snowzjy", "html_url": "https://github.com/snowzjy", "followers_url": "https://api.github.com/users/snowzjy/followers", "following_url": "https://api.github.com/users/snowzjy/following{/other_user}", "gists_url": "https://api.github.com/users/snowzjy/gists{/gist_id}", "starred_url": "https://api.github.com/users/snowzjy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/snowzjy/subscriptions", "organizations_url": "https://api.github.com/users/snowzjy/orgs", "repos_url": "https://api.github.com/users/snowzjy/repos", "events_url": "https://api.github.com/users/snowzjy/events{/privacy}", "received_events_url": "https://api.github.com/users/snowzjy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-22T05:39:55Z", "updated_at": "2017-12-12T14:02:15Z", "closed_at": "2017-12-12T14:02:15Z", "author_association": "NONE", "body_html": "<p>$git clone --recursive <a href=\"https://github.com/pytorch/pytorch\">https://github.com/pytorch/pytorch</a><br>\n$cd pytorch<br>\n$python setup.py install<br>\nrunning install<br>\nrunning build_deps<br>\n-- The C compiler identification is GNU 6.4.0<br>\n-- The CXX compiler identification is GNU 6.4.0<br>\n-- Check for working C compiler: /usr/bin/cc<br>\n-- Check for working C compiler: /usr/bin/cc -- works<br>\n-- Detecting C compiler ABI info<br>\n-- Detecting C compiler ABI info - done<br>\n-- Detecting C compile features<br>\n-- Detecting C compile features - done<br>\n-- Check for working CXX compiler: /usr/bin/c++<br>\n-- Check for working CXX compiler: /usr/bin/c++ -- works<br>\n-- Detecting CXX compiler ABI info<br>\n-- Detecting CXX compiler ABI info - done<br>\n-- Detecting CXX compile features<br>\n-- Detecting CXX compile features - done<br>\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"7.0\")<br>\n-- Configuring done<br>\n-- Generating done<br>\n-- Build files have been written to: /home/zjy/program/pytorch/torch/lib/build/nccl<br>\nScanning dependencies of target nccl<br>\n[100%] Generating lib/libnccl.so<br>\nGrabbing  src/nccl.h                          &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/include/nccl.h<br>\nCompiling src/libwrap.cu                      &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/obj/libwrap.o<br>\nCompiling src/core.cu                         &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/obj/core.o<br>\nCompiling src/all_gather.cu                   &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/obj/all_gather.o<br>\nCompiling src/all_reduce.cu                   &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/obj/all_reduce.o<br>\nCompiling src/broadcast.cu                    &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/obj/broadcast.o<br>\nCompiling src/reduce.cu                       &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/obj/reduce.o<br>\nCompiling src/reduce_scatter.cu               &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/obj/reduce_scatter.o<br>\nptxas warning : Too big maxrregcount value specified 96, will be ignored<br>\nptxas warning : Too big maxrregcount value specified 96, will be ignored<br>\nptxas warning : Too big maxrregcount value specified 96, will be ignored<br>\nptxas warning : Too big maxrregcount value specified 96, will be ignored<br>\nptxas warning : Too big maxrregcount value specified 96, will be ignored<br>\nptxas warning : Too big maxrregcount value specified 96, will be ignored<br>\nptxas warning : Too big maxrregcount value specified 96, will be ignored<br>\nLinking   libnccl.so.1.3.5                    &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/lib/libnccl.so.1.3.5<br>\nArchiving libnccl_static.a                    &gt; /home/zjy/program/pytorch/torch/lib/build/nccl/lib/libnccl_static.a<br>\n[100%] Built target nccl<br>\nInstall the project...<br>\n-- Install configuration: \"Release\"<br>\n-- Installing: /home/zjy/program/pytorch/torch/lib/tmp_install/include/nccl.h<br>\n-- The C compiler identification is GNU 6.4.0<br>\n-- The CXX compiler identification is GNU 6.4.0<br>\n-- Check for working C compiler: /usr/bin/cc<br>\n-- Check for working C compiler: /usr/bin/cc -- works<br>\n-- Detecting C compiler ABI info<br>\n-- Detecting C compiler ABI info - done<br>\n-- Detecting C compile features<br>\n-- Detecting C compile features - done<br>\n-- Check for working CXX compiler: /usr/bin/c++<br>\n-- Check for working CXX compiler: /usr/bin/c++ -- works<br>\n-- Detecting CXX compiler ABI info<br>\n-- Detecting CXX compiler ABI info - done<br>\n-- Detecting CXX compile features<br>\n-- Detecting CXX compile features - done<br>\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"5.5\")<br>\n-- Autodetected CUDA architecture(s): 6.1<br>\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor<br>\n-- Removing -DNDEBUG from compile flags<br>\nCMake Warning (dev) at /home/zjy/anaconda3/share/cmake-3.9/Modules/FindOpenMP.cmake:200 (if):<br>\nPolicy CMP0054 is not set: Only interpret if() arguments as variables or<br>\nkeywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy<br>\ndetails.  Use the cmake_policy command to set the policy and suppress this<br>\nwarning.</p>\n<p>Quoted variables like \"c\" will no longer be dereferenced when the policy is<br>\nset to NEW.  Since the policy is not set the OLD behavior will be used.<br>\nCall Stack (most recent call first):<br>\n/home/zjy/anaconda3/share/cmake-3.9/Modules/FindOpenMP.cmake:324 (_OPENMP_GET_FLAGS)<br>\nCMakeLists.txt:130 (FIND_PACKAGE)<br>\nThis warning is for project developers.  Use -Wno-dev to suppress it.</p>\n<p>-- Found OpenMP_C: -fopenmp (found version \"4.5\")<br>\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")<br>\n-- Compiling with OpenMP support<br>\n-- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2 - True<br>\n-- Compiling with MAGMA support<br>\n-- MAGMA INCLUDE DIRECTORIES: /home/zjy/anaconda3/include<br>\n-- MAGMA LIBRARIES: /home/zjy/anaconda3/lib/libmagma.a<br>\n-- MAGMA V2 check: 1<br>\n-- Could not find hardware support for NEON on this machine.<br>\n-- No OMAP3 processor on this machine.<br>\n-- No OMAP4 processor on this machine.<br>\n-- Looking for cpuid.h<br>\n-- Looking for cpuid.h - found<br>\n-- Performing Test HAVE_GCC_GET_CPUID<br>\n-- Performing Test HAVE_GCC_GET_CPUID - Success<br>\n-- Performing Test NO_GCC_EBX_FPIC_BUG<br>\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success<br>\n-- Performing Test C_HAS_SSE1_1<br>\n-- Performing Test C_HAS_SSE1_1 - Success<br>\n-- Performing Test C_HAS_SSE2_1<br>\n-- Performing Test C_HAS_SSE2_1 - Success<br>\n-- Performing Test C_HAS_SSE3_1<br>\n-- Performing Test C_HAS_SSE3_1 - Failed<br>\n-- Performing Test C_HAS_SSE3_2<br>\n-- Performing Test C_HAS_SSE3_2 - Success<br>\n-- Performing Test C_HAS_SSE4_1_1<br>\n-- Performing Test C_HAS_SSE4_1_1 - Failed<br>\n-- Performing Test C_HAS_SSE4_1_2<br>\n-- Performing Test C_HAS_SSE4_1_2 - Success<br>\n-- Performing Test C_HAS_SSE4_2_1<br>\n-- Performing Test C_HAS_SSE4_2_1 - Failed<br>\n-- Performing Test C_HAS_SSE4_2_2<br>\n-- Performing Test C_HAS_SSE4_2_2 - Success<br>\n-- Performing Test C_HAS_AVX_1<br>\n-- Performing Test C_HAS_AVX_1 - Failed<br>\n-- Performing Test C_HAS_AVX_2<br>\n-- Performing Test C_HAS_AVX_2 - Success<br>\n-- Performing Test C_HAS_AVX2_1<br>\n-- Performing Test C_HAS_AVX2_1 - Failed<br>\n-- Performing Test C_HAS_AVX2_2<br>\n-- Performing Test C_HAS_AVX2_2 - Success<br>\n-- Performing Test CXX_HAS_SSE1_1<br>\n-- Performing Test CXX_HAS_SSE1_1 - Success<br>\n-- Performing Test CXX_HAS_SSE2_1<br>\n-- Performing Test CXX_HAS_SSE2_1 - Success<br>\n-- Performing Test CXX_HAS_SSE3_1<br>\n-- Performing Test CXX_HAS_SSE3_1 - Failed<br>\n-- Performing Test CXX_HAS_SSE3_2<br>\n-- Performing Test CXX_HAS_SSE3_2 - Success<br>\n-- Performing Test CXX_HAS_SSE4_1_1<br>\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed<br>\n-- Performing Test CXX_HAS_SSE4_1_2<br>\n-- Performing Test CXX_HAS_SSE4_1_2 - Success<br>\n-- Performing Test CXX_HAS_SSE4_2_1<br>\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed<br>\n-- Performing Test CXX_HAS_SSE4_2_2<br>\n-- Performing Test CXX_HAS_SSE4_2_2 - Success<br>\n-- Performing Test CXX_HAS_AVX_1<br>\n-- Performing Test CXX_HAS_AVX_1 - Failed<br>\n-- Performing Test CXX_HAS_AVX_2<br>\n-- Performing Test CXX_HAS_AVX_2 - Success<br>\n-- Performing Test CXX_HAS_AVX2_1<br>\n-- Performing Test CXX_HAS_AVX2_1 - Failed<br>\n-- Performing Test CXX_HAS_AVX2_2<br>\n-- Performing Test CXX_HAS_AVX2_2 - Success<br>\n-- SSE2 Found<br>\n-- SSE3 Found<br>\n-- AVX Found<br>\n-- AVX2 Found<br>\n-- Performing Test HAS_C11_ATOMICS<br>\n-- Performing Test HAS_C11_ATOMICS - Failed<br>\n-- Performing Test HAS_MSC_ATOMICS<br>\n-- Performing Test HAS_MSC_ATOMICS - Failed<br>\n-- Performing Test HAS_GCC_ATOMICS<br>\n-- Performing Test HAS_GCC_ATOMICS - Success<br>\n-- Atomics: using GCC intrinsics<br>\n-- Looking for sys/types.h<br>\n-- Looking for sys/types.h - found<br>\n-- Looking for stdint.h<br>\n-- Looking for stdint.h - found<br>\n-- Looking for stddef.h<br>\n-- Looking for stddef.h - found<br>\n-- Check size of void*<br>\n-- Check size of void* - done<br>\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]<br>\n--   Library mkl_gf_lp64: /home/zjy/anaconda3/lib/libmkl_gf_lp64.so<br>\n--   Library mkl_gnu_thread: /home/zjy/anaconda3/lib/libmkl_gnu_thread.so<br>\n--   Library mkl_core: /home/zjy/anaconda3/lib/libmkl_core.so<br>\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")<br>\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")<br>\n--   Library gomp: -fopenmp<br>\n--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.so<br>\n--   Library m: /usr/lib/x86_64-linux-gnu/libm.so<br>\n--   Library dl: /usr/lib/x86_64-linux-gnu/libdl.so<br>\n-- Looking for cblas_sgemm<br>\n-- Looking for cblas_sgemm - found<br>\n-- MKL library found<br>\n-- Performing Test BLAS_F2C_DOUBLE_WORKS<br>\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed<br>\n-- Performing Test BLAS_F2C_FLOAT_WORKS<br>\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success<br>\n-- Performing Test BLAS_USE_CBLAS_DOT<br>\n-- Performing Test BLAS_USE_CBLAS_DOT - Success<br>\n-- Found a library with BLAS API (mkl).<br>\n-- Found a library with LAPACK API. (mkl)<br>\n-- Found CUDNN: /usr/local/cuda/include<br>\n-- Found cuDNN: v7.0.4  (include: /usr/local/cuda/include, library: /usr/local/cuda/lib64/libcudnn.so)<br>\nCMake Deprecation Warning at src/ATen/CMakeLists.txt:7 (CMAKE_POLICY):<br>\nThe OLD behavior for policy CMP0026 will be removed from a future version<br>\nof CMake.</p>\n<p>The cmake-policies(7) manual explains that the OLD behaviors of all<br>\npolicies are deprecated and that a policy should be set to OLD only under<br>\nspecific short-term circumstances.  Projects should be ported to the NEW<br>\nbehavior and not rely on setting a policy to OLD.</p>\n<p>-- Using python found in /home/zjy/anaconda3/bin/python<br>\n['/home/zjy/program/pytorch/aten/src/THNN/generic/THNN.h', '/home/zjy/program/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/zjy/program/pytorch/aten/src/ATen/nn.yaml']<br>\nATen Excluded: {'bernoulli', 'bernoulli_'}<br>\n-- Looking for clock_gettime in rt<br>\n-- Looking for clock_gettime in rt - found<br>\n-- Looking for mmap<br>\n-- Looking for mmap - found<br>\n-- Looking for shm_open<br>\n-- Looking for shm_open - found<br>\n-- Looking for shm_unlink<br>\n-- Looking for shm_unlink - found<br>\n-- Looking for malloc_usable_size<br>\n-- Looking for malloc_usable_size - found<br>\n-- Performing Test C_HAS_THREAD<br>\n-- Performing Test C_HAS_THREAD - Success<br>\ndisable contrib because ATEN_NO_CONTRIB is set<br>\n-- Configuring done<br>\n-- Generating done<br>\n-- Build files have been written to: /home/zjy/program/pytorch/torch/lib/build/aten<br>\n[  0%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCBlas.cu.o<br>\n[  0%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensorMathBlas.cu.o<br>\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCSleep.cu.o<br>\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensorMath.cu.o<br>\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCStorage.cu.o<br>\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensor.cu.o<br>\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCReduceApplyUtils.cu.o<br>\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensorCopy.cu.o<br>\n[  3%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPU[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCStorageCopy.cu.o<br>\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensorMath2.cu.o<br>\nShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/CUDAByteStorage.cpp, ATen/CUDAByteStorage.h, ATen/CUDAByteType.cpp, ATen/CUDAByteType.h, ATen/CUDAByteTensor.cpp, ATen/CUDAByteTensor.h, ATen/CUDACharStorage.cpp, ATen/CUDACharStorage.h, ATen/CUDACharType.cpp, ATen/CUDACharType.h, ATen/CUDACharTensor.cpp, ATen/CUDACharTensor.h, ATen/CUDADoubleStorage.cpp, ATen/CUDADoubleStorage.h, ATen/CUDADoubleType.cpp, ATen/CUDADoubleType.h, ATen/CUDADoubleTensor.cpp, ATen/CUDADoubleTensor.h, ATen/CUDAFloatStorage.cpp, ATen/CUDAFloatStorage.h, ATen/CUDAFloatType.cpp, ATen/CUDAFloatType.h, ATen/CUDAFloatTensor.cpp, ATen/CUDAFloatTensor.h, ATen/CUDAIntStorage.cpp, ATen/CUDAIntStorage.h, ATen/CUDAIntType.cpp, ATen/CUDAIntType.h, ATen/CUDAIntTensor.cpp, ATen/CUDAIntTensor.h, ATen/CUDALongStorage.cpp, ATen/CUDALongStorage.h, ATen/CUDALongType.cpp, ATen/CUDALongType.h, ATen/CUDALongTensor.cpp, ATen/CUDALongTensor.h, ATen/CUDAShortStorage.cpp, ATen/CUDAShortStorage.h, ATen/CUDAShortType.cpp, ATen/CUDAShortType.h, ATen/CUDAShortTensor.cpp, ATen/CUDAShortTensor.h, ATen/CUDAHalfStorage.cpp, ATen/CUDAHalfStorage.h, ATen/CUDAHalfType.cpp, ATen/CUDAHalfType.h, ATen/CUDAHalfTensor.cpp, ATen/CUDAHalfTensor.h, ATen/SparseCUDAByteType.cpp, ATen/SparseCUDAByteType.h, ATen/SparseCUDAByteTensor.cpp, ATen/SparseCUDAByteTensor.h, ATen/SparseCUDACharType.cpp, ATen/SparseCUDACharType.h, ATen/SparseCUDACharTensor.cpp, ATen/SparseCUDACharTensor.h, ATen/SparseCUDADoubleType.cpp, ATen/SparseCUDADoubleType.h, ATen/SparseCUDADoubleTensor.cpp, ATen/SparseCUDADoubleTensor.h, ATen/SparseCUDAFloatType.cpp, ATen/SparseCUDAFloatType.h, ATen/SparseCUDAFloatTensor.cpp, ATen/SparseCUDAFloatTensor.h, ATen/SparseCUDAIntType.cpp, ATen/SparseCUDAIntType.h, ATen/SparseCUDAIntTensor.cpp, ATen/SparseCUDAIntTensor.h, ATen/SparseCUDALongType.cpp, ATen/SparseCUDALongType.h, ATen/SparseCUDALongTensor.cpp, ATen/SparseCUDALongTensor.h, ATen/SparseCUDAShortType.cpp, ATen/SparseCUDAShortType.h, ATen/SparseCUDAShortTensor.cpp, ATen/SparseCUDAShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp, ATen/NativeFunctions.h<br>\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCTensorMathMagma.cu.o<br>\n['/home/zjy/program/pytorch/aten/src/THNN/generic/THNN.h', '/home/zjy/program/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/zjy/program/pytorch/aten/src/ATen/nn.yaml']<br>\nATen Excluded: {'bernoulli_', 'bernoulli'}<br>\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensorMathPairwise.cu.o<br>\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu:105:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nTHCublasCheck(cublasSgemv(handle, op, i_m, i_n, &amp;alpha, a, i_lda, x, i_incx, &amp;beta, y, i_incy));<br>\n~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu:135:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nTHCublasCheck(cublasDgemv(handle, op, i_m, i_n, &amp;alpha, a, i_lda, x, i_incx, &amp;beta, y, i_incy));<br>\n~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCTensorMathReduce.cu.o<br>\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensorMathScan.cu.o<br>\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCTensorIndex.cu.o<br>\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensorConv.cu.o<br>\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCTensorRandom.cu.o<br>\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensorScatterGather.cu.o<br>\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCTensorTopK.cu.o<br>\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCTensorSort.cu.o<br>\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCTensorTypeUtils.cu.o<br>\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCSortUtils.cu.o<br>\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/ATen_generated_THCTensorMode.cu.o<br>\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorSortByte.cu.o<br>\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareTByte.cu.o<br>\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathPointwiseByte.cu.o<br>\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareByte.cu.o<br>\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathReduceByte.cu.o<br>\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMaskedByte.cu.o<br>\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorSortChar.cu.o<br>\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareTChar.cu.o<br>\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathPointwiseChar.cu.o<br>\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareChar.cu.o<br>\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathReduceChar.cu.o<br>\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMaskedChar.cu.o<br>\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorSortShort.cu.o<br>\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareTShort.cu.o<br>\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathPointwiseShort.cu.o<br>\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareShort.cu.o<br>\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathReduceShort.cu.o<br>\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMaskedShort.cu.o<br>\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorSortInt.cu.o<br>\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareTInt.cu.o<br>\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathPointwiseInt.cu.o<br>\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareInt.cu.o<br>\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathReduceInt.cu.o<br>\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMaskedInt.cu.o<br>\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorSortLong.cu.o<br>\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareTLong.cu.o<br>\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathPointwiseLong.cu.o<br>\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareLong.cu.o<br>\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathReduceLong.cu.o<br>\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMaskedLong.cu.o<br>\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorSortHalf.cu.o<br>\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareTHalf.cu.o<br>\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathPointwiseHalf.cu.o<br>\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareHalf.cu.o<br>\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathReduceHalf.cu.o<br>\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMaskedHalf.cu.o<br>\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorSortFloat.cu.o<br>\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareTFloat.cu.o<br>\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathPointwiseFloat.cu.o<br>\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareFloat.cu.o<br>\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathReduceFloat.cu.o<br>\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMaskedFloat.cu.o<br>\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorSortDouble.cu.o<br>\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareTDouble.cu.o<br>\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathPointwiseDouble.cu.o<br>\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMathCompareDouble.cu.o<br>\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/generated/ATen_generated_THCTensorMathReduceDouble.cu.o<br>\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THC/generated/ATen_generated_THCTensorMaskedDouble.cu.o<br>\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THC/ATen_generated_THCHalf.cu.o<br>\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_AbsCriterion.cu.o<br>\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_Abs.cu.o<br>\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_BatchNormalization.cu.o<br>\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_BCECriterion.cu.o<br>\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_ClassNLLCriterion.cu.o<br>\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_DistKLDivCriterion.cu.o<br>\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_ELU.cu.o<br>\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_FeatureLPPooling.cu.o<br>\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_FusedRNNKernel.cu.o<br>\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_GatedLinearUnit.cu.o<br>\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_HardTanh.cu.o<br>\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_IndexLinear.cu.o<br>\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_L1Cost.cu.o<br>\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_LeakyReLU.cu.o<br>\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_LogSigmoid.cu.o<br>\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_LogSoftMax.cu.o<br>\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_LookupTableBag.cu.o<br>\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_LookupTable.cu.o<br>\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_MarginCriterion.cu.o<br>\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_MSECriterion.cu.o<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/LookupTable.cu(25): warning: function \"__shfl(int, int, int)\"<br>\n/usr/local/cuda/include/sm_30_intrinsics.hpp(152): here was declared deprecated (\"__shfl() is deprecated in favor of __shfl_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")</p>\n<p>/home/zjy/program/pytorch/aten/src/THCUNN/LookupTable.cu(42): warning: function \"__any\"<br>\n/usr/local/cuda/include/device_atomic_functions.h(180): here was declared deprecated (\"__any() is deprecated in favor of __any_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")</p>\n<p>[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfLSTM_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:96:   required from here<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size<em>4 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size</em>4 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfGRU_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795<g-emoji class=\"g-emoji\" alias=\"100\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f4af.png\">\ud83d\udcaf</g-emoji>   required from here<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size<em>3 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size</em>3 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaLSTM_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:92:   required from here<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size<em>4 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size</em>4 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaGRU_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:96:   required from here<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size<em>3 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size</em>3 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleLSTM_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:98:   required from here<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size<em>4 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size</em>4 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleGRU_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:102:   required from here<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size<em>3 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]<br>\nTHAssertMsg( hid_size</em>3 == THCTensor_(nElement)(state, bias1) &amp;&amp;<br>\n^<br>\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_MultiMarginCriterion.cu.o<br>\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_PReLU.cu.o<br>\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_RReLU.cu.o<br>\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_Sigmoid.cu.o<br>\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SmoothL1Criterion.cu.o<br>\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SoftMarginCriterion.cu.o<br>\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SoftMax.cu.o<br>\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SoftPlus.cu.o<br>\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SoftShrink.cu.o<br>\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SparseLinear.cu.o<br>\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialAdaptiveAveragePooling.cu.o<br>\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialAdaptiveMaxPooling.cu.o<br>\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialAveragePooling.cu.o<br>\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialClassNLLCriterion.cu.o<br>\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialConvolutionLocal.cu.o<br>\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialConvolutionMM.cu.o<br>\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialCrossMapLRN.cu.o<br>\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialDepthwiseConvolution.cu.o<br>\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialDilatedConvolution.cu.o<br>\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialDilatedMaxPooling.cu.o<br>\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialFractionalMaxPooling.cu.o<br>\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialFullConvolution.cu.o<br>\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialFullDilatedConvolution.cu.o<br>\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialGridSamplerBilinear.cu.o<br>\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialMaxPooling.cu.o<br>\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialMaxUnpooling.cu.o<br>\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialReflectionPadding.cu.o<br>\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialReplicationPadding.cu.o<br>\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialSubSampling.cu.o<br>\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_SpatialUpSamplingBilinear.cu.o<br>\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_SpatialUpSamplingNearest.cu.o<br>\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_Sqrt.cu.o<br>\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_Square.cu.o<br>\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_Tanh.cu.o<br>\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_TemporalConvolution.cu.o<br>\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_TemporalMaxPooling.cu.o<br>\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_TemporalReflectionPadding.cu.o<br>\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_TemporalReplicationPadding.cu.o<br>\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_TemporalRowConvolution.cu.o<br>\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_TemporalUpSamplingLinear.cu.o<br>\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_TemporalUpSamplingNearest.cu.o<br>\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_Threshold.cu.o<br>\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_VolumetricAdaptiveAveragePooling.cu.o<br>\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_VolumetricAdaptiveMaxPooling.cu.o<br>\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_VolumetricAveragePooling.cu.o<br>\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_VolumetricConvolution.cu.o<br>\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_VolumetricDilatedConvolution.cu.o<br>\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_VolumetricDilatedMaxPooling.cu.o<br>\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_VolumetricFractionalMaxPooling.cu.o<br>\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_VolumetricFullConvolution.cu.o<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)<br>\n~~~~~~~~~~~~^~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here<br>\nint inputWidth;<br>\n^~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputHeight - 1)*dH &gt;= inputHeight + padH)<br>\n~~~~~~~~~~~~~^~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here<br>\nint inputHeight;<br>\n^~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputTime   - 1)*dT &gt;= inputTime   + padT)<br>\n~~~~~~~~~~~^~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here<br>\nint inputTime;<br>\n^~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:425: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nTHCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here<br>\nint inputSlices;<br>\n^~~~~~~~~~~<br>\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_VolumetricFullDilatedConvolution.cu.o<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputWidth  - 1)<em>dW &gt;= inputWidth  + padW)<br>\n~~~~~~~~~~~~^~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here<br>\nint inputWidth;<br>\n^~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputHeight - 1)<em>dH &gt;= inputHeight + padH)<br>\n~~~~~~~~~~~~~^~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here<br>\nint inputHeight;<br>\n^~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputTime   - 1)<em>dT &gt;= inputTime   + padT)<br>\n~~~~~~~~~~~^~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here<br>\nint inputTime;<br>\n^~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:413: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nTHCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here<br>\nint inputSlices;<br>\n^~~~~~~~~~~<br>\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_VolumetricMaxPooling.cu.o<br>\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_VolumetricMaxUnpooling.cu.o<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState</em>, THCudaDoubleTensor</em>, THCudaDoubleTensor</em>, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputWidth  - 1)<em>dW &gt;= inputWidth  + padW)<br>\n~~~~~~~~~~~~^~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here<br>\nint inputWidth;<br>\n^~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputHeight - 1)<em>dH &gt;= inputHeight + padH)<br>\n~~~~~~~~~~~~~^~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here<br>\nint inputHeight;<br>\n^~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputTime   - 1)<em>dT &gt;= inputTime   + padT)<br>\n~~~~~~~~~~~^~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here<br>\nint inputTime;<br>\n^~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:431: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nTHCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here<br>\nint inputSlices;<br>\n^~~~~~~~~~~<br>\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_VolumetricReplicationPadding.cu.o<br>\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCUNN/ATen_generated_VolumetricUpSamplingNearest.cu.o<br>\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/<strong>/THCUNN/ATen_generated_VolumetricUpSamplingTrilinear.cu.o<br>\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/</strong>/THCS/ATen_generated_THCSTensor.cu.o<br>\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_shapeCheck(THCState</em>, THCudaHalfTensor</em>, THCudaHalfTensor</em>, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputWidth  - 1)<em>dW &gt;= inputWidth  + padW)<br>\n~~~~~~~~~~~~^~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here<br>\nint inputWidth;<br>\n^~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputHeight - 1)<em>dH &gt;= inputHeight + padH)<br>\n~~~~~~~~~~~~~^~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here<br>\nint inputHeight;<br>\n^~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputTime - 1)<em>dT &gt;= inputTime + padT)<br>\n~~~~~~~~~~~^~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here<br>\nint inputTime;<br>\n^~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nTHCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here<br>\nint inputSlices;<br>\n^~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_shapeCheck(THCState</em>, THCudaTensor</em>, THCudaTensor</em>, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputWidth  - 1)<em>dW &gt;= inputWidth  + padW)<br>\n~~~~~~~~~~~~^~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here<br>\nint inputWidth;<br>\n^~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputHeight - 1)<em>dH &gt;= inputHeight + padH)<br>\n~~~~~~~~~~~~~^~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here<br>\nint inputHeight;<br>\n^~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputTime - 1)<em>dT &gt;= inputTime + padT)<br>\n~~~~~~~~~~~^~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here<br>\nint inputTime;<br>\n^~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nTHCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here<br>\nint inputSlices;<br>\n^~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_shapeCheck(THCState</em>, THCudaDoubleTensor</em>, THCudaDoubleTensor</em>, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputWidth  - 1)*dW &gt;= inputWidth  + padW)<br>\n~~~~~~~~~~~~^~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here<br>\nint inputWidth;<br>\n^~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputHeight - 1)*dH &gt;= inputHeight + padH)<br>\n~~~~~~~~~~~~~^~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here<br>\nint inputHeight;<br>\n^~~~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nif ((outputTime - 1)*dT &gt;= inputTime + padT)<br>\n~~~~~~~~~~~^~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here<br>\nint inputTime;<br>\n^~~~~~~~~<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]<br>\nTHCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);<br>\n^<br>\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here<br>\nint inputSlices;<br>\n^~~~~~~~~~~<br>\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o<br>\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_MoveConstructibleTuple() [with _UElements = {std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;}; bool  = true; _Elements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}]\u2019:<br>\n/usr/include/c++/6/tuple:626:248:   required by substitution of \u2018template&lt;class ... _UElements, typename std::enable_if&lt;(((std::_TC&lt;(sizeof... (_UElements) == 1), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_NotSameTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; (3ul &gt;= 1)), bool&gt;::type  &gt; constexpr std::tuple&lt;  &gt;::tuple(_UElements&amp;&amp; ...) [with _UElements = {std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;}; typename std::enable_if&lt;(((std::_TC&lt;(sizeof... (_UElements) == 1), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_NotSameTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; (3ul &gt;= 1)), bool&gt;::type  = ]\u2019<br>\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here<br>\n/usr/include/c++/6/tuple:483:67: error: mismatched argument pack lengths while expanding \u2018std::is_constructible&lt;_Elements, _UElements&amp;&amp;&gt;\u2019<br>\nreturn _<em>and</em>&lt;is_constructible&lt;_Elements, _UElements&amp;&amp;&gt;...&gt;::value;<br>\n^~~~~<br>\n/usr/include/c++/6/tuple:484:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_MoveConstructibleTuple() [with _UElements = {std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;}; bool  = true; _Elements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}]\u2019 not a return-statement<br>\n}<br>\n^<br>\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;}; bool  = true; _Elements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}]\u2019:<br>\n/usr/include/c++/6/tuple:626:362:   required by substitution of \u2018template&lt;class ... _UElements, typename std::enable_if&lt;(((std::_TC&lt;(sizeof... (_UElements) == 1), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_NotSameTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; (3ul &gt;= 1)), bool&gt;::type  &gt; constexpr std::tuple&lt;  &gt;::tuple(_UElements&amp;&amp; ...) [with _UElements = {std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;}; typename std::enable_if&lt;(((std::_TC&lt;(sizeof... (_UElements) == 1), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_NotSameTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; (3ul &gt;= 1)), bool&gt;::type  = ]\u2019<br>\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here<br>\n/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding \u2018std::is_convertible&lt;_UElements&amp;&amp;, _Elements&gt;\u2019<br>\nreturn _<em>and</em>&lt;is_convertible&lt;_UElements&amp;&amp;, _Elements&gt;...&gt;::value;<br>\n^~~~~<br>\n/usr/include/c++/6/tuple:490:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;}; bool  = true; _Elements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}]\u2019 not a return-statement<br>\n}<br>\n^<br>\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = const std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;&amp;; bool  = true; _Elements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}]\u2019:<br>\n/usr/include/c++/6/tuple:662:419:   required by substitution of \u2018template&lt;class ... _UElements, class _Dummy, typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ImplicitlyConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_NonNestedTuple&lt;const tuple&lt;_Elements ...&gt;&amp;&gt;()), bool&gt;::type  &gt; constexpr std::tuple&lt;  &gt;::tuple(const std::tuple&lt;_Args1 ...&gt;&amp;) [with _UElements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}; _Dummy = void; typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ImplicitlyConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_NonNestedTuple&lt;const tuple&lt;_Elements ...&gt;&amp;&gt;()), bool&gt;::type  = ]\u2019<br>\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here<br>\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)<br>\nreturn  _<em>and</em>&lt;_<em>not</em>&lt;is_same&lt;tuple&lt;_Elements...&gt;,<br>\n^<br>\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template&lt;class _From, class _To&gt; struct std::is_convertible\u2019<br>\nstruct is_convertible<br>\n^~~~~~~~~~~~~~<br>\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = const std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;&amp;; bool  = true; _Elements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}]\u2019 not a return-statement<br>\n}<br>\n^<br>\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;&amp;&amp;; bool  = true; _Elements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}]\u2019:<br>\n/usr/include/c++/6/tuple:686:422:   required by substitution of \u2018template&lt;class ... _UElements, class _Dummy, typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_NonNestedTuple&lt;tuple&lt;_Elements ...&gt;&amp;&amp;&gt;()), bool&gt;::type  &gt; constexpr std::tuple&lt;  &gt;::tuple(std::tuple&lt;_Args1 ...&gt;&amp;&amp;) [with _UElements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}; _Dummy = void; typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;::_NonNestedTuple&lt;tuple&lt;_Elements ...&gt;&amp;&amp;&gt;()), bool&gt;::type  = ]\u2019<br>\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here<br>\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)<br>\nreturn  _<em>and</em>&lt;_<em>not</em>&lt;is_same&lt;tuple&lt;_Elements...&gt;,<br>\n^<br>\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template&lt;class _From, class _To&gt; struct std::is_convertible\u2019<br>\nstruct is_convertible<br>\n^~~~~~~~~~~~~~<br>\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = std::tuple&lt;at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;&gt;&amp;&amp;; bool  = true; _Elements = {at::Tensor&amp;, at::Tensor&amp;, at::Tensor&amp;}]\u2019 not a return-statement<br>\n}<br>\n^<br>\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_MoveConstructibleTuple() [with _UElements = {std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;}; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:<br>\n/usr/include/c++/6/tuple:626:248:   required by substitution of \u2018template&lt;class ... _UElements, typename std::enable_if&lt;(((std::_TC&lt;(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor&gt;::_NotSameTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; (3ul &gt;= 1)), bool&gt;::type  &gt; constexpr std::tuple&lt;  &gt;::tuple(_UElements&amp;&amp; ...) [with _UElements = {std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;}; typename std::enable_if&lt;(((std::_TC&lt;(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor&gt;::_NotSameTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; (3ul &gt;= 1)), bool&gt;::type  = ]\u2019<br>\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here<br>\n/usr/include/c++/6/tuple:483:67: error: mismatched argument pack lengths while expanding \u2018std::is_constructible&lt;_Elements, _UElements&amp;&amp;&gt;\u2019<br>\nreturn _<em>and</em>&lt;is_constructible&lt;_Elements, _UElements&amp;&amp;&gt;...&gt;::value;<br>\n^~~~~<br>\n/usr/include/c++/6/tuple:484:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_MoveConstructibleTuple() [with _UElements = {std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;}; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement<br>\n}<br>\n^<br>\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;}; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:<br>\n/usr/include/c++/6/tuple:626:362:   required by substitution of \u2018template&lt;class ... _UElements, typename std::enable_if&lt;(((std::_TC&lt;(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor&gt;::_NotSameTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; (3ul &gt;= 1)), bool&gt;::type  &gt; constexpr std::tuple&lt;  &gt;::tuple(_UElements&amp;&amp; ...) [with _UElements = {std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;}; typename std::enable_if&lt;(((std::_TC&lt;(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor&gt;::_NotSameTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; (3ul &gt;= 1)), bool&gt;::type  = ]\u2019<br>\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here<br>\n/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding \u2018std::is_convertible&lt;_UElements&amp;&amp;, _Elements&gt;\u2019<br>\nreturn _<em>and</em>&lt;is_convertible&lt;_UElements&amp;&amp;, _Elements&gt;...&gt;::value;<br>\n^~~~~<br>\n/usr/include/c++/6/tuple:490:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;}; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement<br>\n}<br>\n^<br>\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = const std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;&amp;; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:<br>\n/usr/include/c++/6/tuple:662:419:   required by substitution of \u2018template&lt;class ... _UElements, class _Dummy, typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ImplicitlyConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), at::Tensor, at::Tensor, at::Tensor&gt;::_NonNestedTuple&lt;const tuple&lt;_Elements ...&gt;&amp;&gt;()), bool&gt;::type  &gt; constexpr std::tuple&lt;  &gt;::tuple(const std::tuple&lt;_Args1 ...&gt;&amp;) [with _UElements = {at::Tensor, at::Tensor, at::Tensor}; _Dummy = void; typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ImplicitlyConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), at::Tensor, at::Tensor, at::Tensor&gt;::_NonNestedTuple&lt;const tuple&lt;_Elements ...&gt;&amp;&gt;()), bool&gt;::type  = ]\u2019<br>\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here<br>\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)<br>\nreturn  _<em>and</em>&lt;_<em>not</em>&lt;is_same&lt;tuple&lt;_Elements...&gt;,<br>\n^<br>\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template&lt;class _From, class _To&gt; struct std::is_convertible\u2019<br>\nstruct is_convertible<br>\n^~~~~~~~~~~~~~<br>\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = const std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;&amp;; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement<br>\n}<br>\n^<br>\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;&amp;&amp;; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:<br>\n/usr/include/c++/6/tuple:686:422:   required by substitution of \u2018template&lt;class ... _UElements, class _Dummy, typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), at::Tensor, at::Tensor, at::Tensor&gt;::_NonNestedTuple&lt;tuple&lt;_Elements ...&gt;&amp;&amp;&gt;()), bool&gt;::type  &gt; constexpr std::tuple&lt;  &gt;::tuple(std::tuple&lt;_Args1 ...&gt;&amp;&amp;) [with _UElements = {at::Tensor, at::Tensor, at::Tensor}; _Dummy = void; typename std::enable_if&lt;((std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_MoveConstructibleTuple&lt;_UElements ...&gt;() &amp;&amp; std::_TC&lt;(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor&gt;::_ImplicitlyMoveConvertibleTuple&lt;_UElements ...&gt;()) &amp;&amp; std::_TC&lt;(std::is_same&lt;_Dummy, void&gt;::value &amp;&amp; (1ul == 1)), at::Tensor, at::Tensor, at::Tensor&gt;::_NonNestedTuple&lt;tuple&lt;_Elements ...&gt;&amp;&amp;&gt;()), bool&gt;::type  = ]\u2019<br>\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here<br>\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)<br>\nreturn  _<em>and</em>&lt;_<em>not</em>&lt;is_same&lt;tuple&lt;_Elements...&gt;,<br>\n^<br>\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template&lt;class _From, class _To&gt; struct std::is_convertible\u2019<br>\nstruct is_convertible<br>\n^~~~~~~~~~~~~~<br>\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC&lt;, _Elements&gt;::_NonNestedTuple() [with _SrcTuple = std::tuple&lt;at::Tensor, at::Tensor, at::Tensor&gt;&amp;&amp;; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement<br>\n}<br>\n^<br>\nCMake Error at ATen_generated_NativeFunctionsCuda.cu.o.cmake:267 (message):<br>\nError generating file<br>\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/native/cuda/./ATen_generated_NativeFunctionsCuda.cu.o</p>\n<p>src/ATen/CMakeFiles/ATen.dir/build.make:1120: recipe for target 'src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o' failed<br>\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o] Error 1<br>\nmake[2]: *** Waiting for unfinished jobs....<br>\nCMakeFiles/Makefile2:193: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed<br>\nmake[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2<br>\nMakefile:129: recipe for target 'all' failed<br>\nmake: *** [all] Error 2</p>", "body_text": "$git clone --recursive https://github.com/pytorch/pytorch\n$cd pytorch\n$python setup.py install\nrunning install\nrunning build_deps\n-- The C compiler identification is GNU 6.4.0\n-- The CXX compiler identification is GNU 6.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"7.0\")\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/zjy/program/pytorch/torch/lib/build/nccl\nScanning dependencies of target nccl\n[100%] Generating lib/libnccl.so\nGrabbing  src/nccl.h                          > /home/zjy/program/pytorch/torch/lib/build/nccl/include/nccl.h\nCompiling src/libwrap.cu                      > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/libwrap.o\nCompiling src/core.cu                         > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/core.o\nCompiling src/all_gather.cu                   > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/all_gather.o\nCompiling src/all_reduce.cu                   > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/all_reduce.o\nCompiling src/broadcast.cu                    > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/broadcast.o\nCompiling src/reduce.cu                       > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/reduce.o\nCompiling src/reduce_scatter.cu               > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/reduce_scatter.o\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nptxas warning : Too big maxrregcount value specified 96, will be ignored\nLinking   libnccl.so.1.3.5                    > /home/zjy/program/pytorch/torch/lib/build/nccl/lib/libnccl.so.1.3.5\nArchiving libnccl_static.a                    > /home/zjy/program/pytorch/torch/lib/build/nccl/lib/libnccl_static.a\n[100%] Built target nccl\nInstall the project...\n-- Install configuration: \"Release\"\n-- Installing: /home/zjy/program/pytorch/torch/lib/tmp_install/include/nccl.h\n-- The C compiler identification is GNU 6.4.0\n-- The CXX compiler identification is GNU 6.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"5.5\")\n-- Autodetected CUDA architecture(s): 6.1\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\n-- Removing -DNDEBUG from compile flags\nCMake Warning (dev) at /home/zjy/anaconda3/share/cmake-3.9/Modules/FindOpenMP.cmake:200 (if):\nPolicy CMP0054 is not set: Only interpret if() arguments as variables or\nkeywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\ndetails.  Use the cmake_policy command to set the policy and suppress this\nwarning.\nQuoted variables like \"c\" will no longer be dereferenced when the policy is\nset to NEW.  Since the policy is not set the OLD behavior will be used.\nCall Stack (most recent call first):\n/home/zjy/anaconda3/share/cmake-3.9/Modules/FindOpenMP.cmake:324 (_OPENMP_GET_FLAGS)\nCMakeLists.txt:130 (FIND_PACKAGE)\nThis warning is for project developers.  Use -Wno-dev to suppress it.\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Compiling with OpenMP support\n-- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2 - True\n-- Compiling with MAGMA support\n-- MAGMA INCLUDE DIRECTORIES: /home/zjy/anaconda3/include\n-- MAGMA LIBRARIES: /home/zjy/anaconda3/lib/libmagma.a\n-- MAGMA V2 check: 1\n-- Could not find hardware support for NEON on this machine.\n-- No OMAP3 processor on this machine.\n-- No OMAP4 processor on this machine.\n-- Looking for cpuid.h\n-- Looking for cpuid.h - found\n-- Performing Test HAVE_GCC_GET_CPUID\n-- Performing Test HAVE_GCC_GET_CPUID - Success\n-- Performing Test NO_GCC_EBX_FPIC_BUG\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\n-- Performing Test C_HAS_SSE1_1\n-- Performing Test C_HAS_SSE1_1 - Success\n-- Performing Test C_HAS_SSE2_1\n-- Performing Test C_HAS_SSE2_1 - Success\n-- Performing Test C_HAS_SSE3_1\n-- Performing Test C_HAS_SSE3_1 - Failed\n-- Performing Test C_HAS_SSE3_2\n-- Performing Test C_HAS_SSE3_2 - Success\n-- Performing Test C_HAS_SSE4_1_1\n-- Performing Test C_HAS_SSE4_1_1 - Failed\n-- Performing Test C_HAS_SSE4_1_2\n-- Performing Test C_HAS_SSE4_1_2 - Success\n-- Performing Test C_HAS_SSE4_2_1\n-- Performing Test C_HAS_SSE4_2_1 - Failed\n-- Performing Test C_HAS_SSE4_2_2\n-- Performing Test C_HAS_SSE4_2_2 - Success\n-- Performing Test C_HAS_AVX_1\n-- Performing Test C_HAS_AVX_1 - Failed\n-- Performing Test C_HAS_AVX_2\n-- Performing Test C_HAS_AVX_2 - Success\n-- Performing Test C_HAS_AVX2_1\n-- Performing Test C_HAS_AVX2_1 - Failed\n-- Performing Test C_HAS_AVX2_2\n-- Performing Test C_HAS_AVX2_2 - Success\n-- Performing Test CXX_HAS_SSE1_1\n-- Performing Test CXX_HAS_SSE1_1 - Success\n-- Performing Test CXX_HAS_SSE2_1\n-- Performing Test CXX_HAS_SSE2_1 - Success\n-- Performing Test CXX_HAS_SSE3_1\n-- Performing Test CXX_HAS_SSE3_1 - Failed\n-- Performing Test CXX_HAS_SSE3_2\n-- Performing Test CXX_HAS_SSE3_2 - Success\n-- Performing Test CXX_HAS_SSE4_1_1\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\n-- Performing Test CXX_HAS_SSE4_1_2\n-- Performing Test CXX_HAS_SSE4_1_2 - Success\n-- Performing Test CXX_HAS_SSE4_2_1\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\n-- Performing Test CXX_HAS_SSE4_2_2\n-- Performing Test CXX_HAS_SSE4_2_2 - Success\n-- Performing Test CXX_HAS_AVX_1\n-- Performing Test CXX_HAS_AVX_1 - Failed\n-- Performing Test CXX_HAS_AVX_2\n-- Performing Test CXX_HAS_AVX_2 - Success\n-- Performing Test CXX_HAS_AVX2_1\n-- Performing Test CXX_HAS_AVX2_1 - Failed\n-- Performing Test CXX_HAS_AVX2_2\n-- Performing Test CXX_HAS_AVX2_2 - Success\n-- SSE2 Found\n-- SSE3 Found\n-- AVX Found\n-- AVX2 Found\n-- Performing Test HAS_C11_ATOMICS\n-- Performing Test HAS_C11_ATOMICS - Failed\n-- Performing Test HAS_MSC_ATOMICS\n-- Performing Test HAS_MSC_ATOMICS - Failed\n-- Performing Test HAS_GCC_ATOMICS\n-- Performing Test HAS_GCC_ATOMICS - Success\n-- Atomics: using GCC intrinsics\n-- Looking for sys/types.h\n-- Looking for sys/types.h - found\n-- Looking for stdint.h\n-- Looking for stdint.h - found\n-- Looking for stddef.h\n-- Looking for stddef.h - found\n-- Check size of void*\n-- Check size of void* - done\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\n--   Library mkl_gf_lp64: /home/zjy/anaconda3/lib/libmkl_gf_lp64.so\n--   Library mkl_gnu_thread: /home/zjy/anaconda3/lib/libmkl_gnu_thread.so\n--   Library mkl_core: /home/zjy/anaconda3/lib/libmkl_core.so\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n--   Library gomp: -fopenmp\n--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.so\n--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\n--   Library dl: /usr/lib/x86_64-linux-gnu/libdl.so\n-- Looking for cblas_sgemm\n-- Looking for cblas_sgemm - found\n-- MKL library found\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\n-- Performing Test BLAS_F2C_FLOAT_WORKS\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\n-- Performing Test BLAS_USE_CBLAS_DOT\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\n-- Found a library with BLAS API (mkl).\n-- Found a library with LAPACK API. (mkl)\n-- Found CUDNN: /usr/local/cuda/include\n-- Found cuDNN: v7.0.4  (include: /usr/local/cuda/include, library: /usr/local/cuda/lib64/libcudnn.so)\nCMake Deprecation Warning at src/ATen/CMakeLists.txt:7 (CMAKE_POLICY):\nThe OLD behavior for policy CMP0026 will be removed from a future version\nof CMake.\nThe cmake-policies(7) manual explains that the OLD behaviors of all\npolicies are deprecated and that a policy should be set to OLD only under\nspecific short-term circumstances.  Projects should be ported to the NEW\nbehavior and not rely on setting a policy to OLD.\n-- Using python found in /home/zjy/anaconda3/bin/python\n['/home/zjy/program/pytorch/aten/src/THNN/generic/THNN.h', '/home/zjy/program/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/zjy/program/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: {'bernoulli', 'bernoulli_'}\n-- Looking for clock_gettime in rt\n-- Looking for clock_gettime in rt - found\n-- Looking for mmap\n-- Looking for mmap - found\n-- Looking for shm_open\n-- Looking for shm_open - found\n-- Looking for shm_unlink\n-- Looking for shm_unlink - found\n-- Looking for malloc_usable_size\n-- Looking for malloc_usable_size - found\n-- Performing Test C_HAS_THREAD\n-- Performing Test C_HAS_THREAD - Success\ndisable contrib because ATEN_NO_CONTRIB is set\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/zjy/program/pytorch/torch/lib/build/aten\n[  0%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCBlas.cu.o\n[  0%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorMathBlas.cu.o\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCSleep.cu.o\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorMath.cu.o\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCStorage.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensor.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCReduceApplyUtils.cu.o\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorCopy.cu.o\n[  3%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPU[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCStorageCopy.cu.o\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorMath2.cu.o\nShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/CUDAByteStorage.cpp, ATen/CUDAByteStorage.h, ATen/CUDAByteType.cpp, ATen/CUDAByteType.h, ATen/CUDAByteTensor.cpp, ATen/CUDAByteTensor.h, ATen/CUDACharStorage.cpp, ATen/CUDACharStorage.h, ATen/CUDACharType.cpp, ATen/CUDACharType.h, ATen/CUDACharTensor.cpp, ATen/CUDACharTensor.h, ATen/CUDADoubleStorage.cpp, ATen/CUDADoubleStorage.h, ATen/CUDADoubleType.cpp, ATen/CUDADoubleType.h, ATen/CUDADoubleTensor.cpp, ATen/CUDADoubleTensor.h, ATen/CUDAFloatStorage.cpp, ATen/CUDAFloatStorage.h, ATen/CUDAFloatType.cpp, ATen/CUDAFloatType.h, ATen/CUDAFloatTensor.cpp, ATen/CUDAFloatTensor.h, ATen/CUDAIntStorage.cpp, ATen/CUDAIntStorage.h, ATen/CUDAIntType.cpp, ATen/CUDAIntType.h, ATen/CUDAIntTensor.cpp, ATen/CUDAIntTensor.h, ATen/CUDALongStorage.cpp, ATen/CUDALongStorage.h, ATen/CUDALongType.cpp, ATen/CUDALongType.h, ATen/CUDALongTensor.cpp, ATen/CUDALongTensor.h, ATen/CUDAShortStorage.cpp, ATen/CUDAShortStorage.h, ATen/CUDAShortType.cpp, ATen/CUDAShortType.h, ATen/CUDAShortTensor.cpp, ATen/CUDAShortTensor.h, ATen/CUDAHalfStorage.cpp, ATen/CUDAHalfStorage.h, ATen/CUDAHalfType.cpp, ATen/CUDAHalfType.h, ATen/CUDAHalfTensor.cpp, ATen/CUDAHalfTensor.h, ATen/SparseCUDAByteType.cpp, ATen/SparseCUDAByteType.h, ATen/SparseCUDAByteTensor.cpp, ATen/SparseCUDAByteTensor.h, ATen/SparseCUDACharType.cpp, ATen/SparseCUDACharType.h, ATen/SparseCUDACharTensor.cpp, ATen/SparseCUDACharTensor.h, ATen/SparseCUDADoubleType.cpp, ATen/SparseCUDADoubleType.h, ATen/SparseCUDADoubleTensor.cpp, ATen/SparseCUDADoubleTensor.h, ATen/SparseCUDAFloatType.cpp, ATen/SparseCUDAFloatType.h, ATen/SparseCUDAFloatTensor.cpp, ATen/SparseCUDAFloatTensor.h, ATen/SparseCUDAIntType.cpp, ATen/SparseCUDAIntType.h, ATen/SparseCUDAIntTensor.cpp, ATen/SparseCUDAIntTensor.h, ATen/SparseCUDALongType.cpp, ATen/SparseCUDALongType.h, ATen/SparseCUDALongTensor.cpp, ATen/SparseCUDALongTensor.h, ATen/SparseCUDAShortType.cpp, ATen/SparseCUDAShortType.h, ATen/SparseCUDAShortTensor.cpp, ATen/SparseCUDAShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp, ATen/NativeFunctions.h\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorMathMagma.cu.o\n['/home/zjy/program/pytorch/aten/src/THNN/generic/THNN.h', '/home/zjy/program/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/zjy/program/pytorch/aten/src/ATen/nn.yaml']\nATen Excluded: {'bernoulli_', 'bernoulli'}\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorMathPairwise.cu.o\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu:105:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nTHCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\n~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu:135:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nTHCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\n~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorMathReduce.cu.o\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorMathScan.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorIndex.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorConv.cu.o\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorRandom.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorScatterGather.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorTopK.cu.o\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorSort.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorTypeUtils.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCSortUtils.cu.o\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCTensorMode.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorSortByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareTByte.cu.o\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathPointwiseByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathReduceByte.cu.o\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMaskedByte.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorSortChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareTChar.cu.o\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathPointwiseChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathReduceChar.cu.o\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMaskedChar.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorSortShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareTShort.cu.o\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathPointwiseShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathReduceShort.cu.o\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMaskedShort.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorSortInt.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareTInt.cu.o\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathPointwiseInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathReduceInt.cu.o\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMaskedInt.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorSortLong.cu.o\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareTLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathPointwiseLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareLong.cu.o\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathReduceLong.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMaskedLong.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorSortHalf.cu.o\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareTHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathPointwiseHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareHalf.cu.o\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathReduceHalf.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMaskedHalf.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorSortFloat.cu.o\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareTFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathPointwiseFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareFloat.cu.o\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathReduceFloat.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMaskedFloat.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorSortDouble.cu.o\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareTDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathPointwiseDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathCompareDouble.cu.o\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMathReduceDouble.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/generated/ATen_generated_THCTensorMaskedDouble.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THC/ATen_generated_THCHalf.cu.o\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_AbsCriterion.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_Abs.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_BatchNormalization.cu.o\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_BCECriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_ClassNLLCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_DistKLDivCriterion.cu.o\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_ELU.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_FeatureLPPooling.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_FusedRNNKernel.cu.o\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_GatedLinearUnit.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_HardTanh.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_IndexLinear.cu.o\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_L1Cost.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_LeakyReLU.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_LogSigmoid.cu.o\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_LogSoftMax.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_LookupTableBag.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_LookupTable.cu.o\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_MarginCriterion.cu.o\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_MSECriterion.cu.o\n/home/zjy/program/pytorch/aten/src/THCUNN/LookupTable.cu(25): warning: function \"__shfl(int, int, int)\"\n/usr/local/cuda/include/sm_30_intrinsics.hpp(152): here was declared deprecated (\"__shfl() is deprecated in favor of __shfl_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\n/home/zjy/program/pytorch/aten/src/THCUNN/LookupTable.cu(42): warning: function \"__any\"\n/usr/local/cuda/include/device_atomic_functions.h(180): here was declared deprecated (\"__any() is deprecated in favor of __any_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfLSTM_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:96:   required from here\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size4 == THCTensor_(nElement)(state, bias1) &&\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size4 == THCTensor_(nElement)(state, bias1) &&\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfGRU_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795\ud83d\udcaf   required from here\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size3 == THCTensor_(nElement)(state, bias1) &&\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size3 == THCTensor_(nElement)(state, bias1) &&\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaLSTM_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:92:   required from here\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size4 == THCTensor_(nElement)(state, bias1) &&\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size4 == THCTensor_(nElement)(state, bias1) &&\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaGRU_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:96:   required from here\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size3 == THCTensor_(nElement)(state, bias1) &&\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size3 == THCTensor_(nElement)(state, bias1) &&\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleLSTM_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:98:   required from here\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size4 == THCTensor_(nElement)(state, bias1) &&\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size4 == THCTensor_(nElement)(state, bias1) &&\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleGRU_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:102:   required from here\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size3 == THCTensor_(nElement)(state, bias1) &&\n~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nTHAssertMsg( hid_size3 == THCTensor_(nElement)(state, bias1) &&\n^\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_MultiMarginCriterion.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_PReLU.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_RReLU.cu.o\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_Sigmoid.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SmoothL1Criterion.cu.o\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SoftMarginCriterion.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SoftMax.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SoftPlus.cu.o\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SoftShrink.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SparseLinear.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialAdaptiveAveragePooling.cu.o\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialAdaptiveMaxPooling.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialAveragePooling.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialClassNLLCriterion.cu.o\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialConvolutionLocal.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialConvolutionMM.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialCrossMapLRN.cu.o\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialDepthwiseConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialDilatedConvolution.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialDilatedMaxPooling.cu.o\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialFractionalMaxPooling.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialFullConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialFullDilatedConvolution.cu.o\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialGridSamplerBilinear.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialMaxPooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialMaxUnpooling.cu.o\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialReflectionPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialReplicationPadding.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialSubSampling.cu.o\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialUpSamplingBilinear.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_SpatialUpSamplingNearest.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_Sqrt.cu.o\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_Square.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_Tanh.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_TemporalConvolution.cu.o\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_TemporalMaxPooling.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_TemporalReflectionPadding.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_TemporalReplicationPadding.cu.o\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_TemporalRowConvolution.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_TemporalUpSamplingLinear.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_TemporalUpSamplingNearest.cu.o\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_Threshold.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricAdaptiveAveragePooling.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricAdaptiveMaxPooling.cu.o\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricAveragePooling.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricConvolution.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricDilatedConvolution.cu.o\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricDilatedMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricFractionalMaxPooling.cu.o\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricFullConvolution.cu.o\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputWidth  - 1)*dW >= inputWidth  + padW)\n~~~~~~~~~~~~^~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\nint inputWidth;\n^~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputHeight - 1)*dH >= inputHeight + padH)\n~~~~~~~~~~~~~^~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\nint inputHeight;\n^~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputTime   - 1)*dT >= inputTime   + padT)\n~~~~~~~~~~~^~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\nint inputTime;\n^~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:425: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nTHCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\nint inputSlices;\n^~~~~~~~~~~\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricFullDilatedConvolution.cu.o\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputWidth  - 1)dW >= inputWidth  + padW)\n~~~~~~~~~~~~^~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\nint inputWidth;\n^~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputHeight - 1)dH >= inputHeight + padH)\n~~~~~~~~~~~~~^~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\nint inputHeight;\n^~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputTime   - 1)dT >= inputTime   + padT)\n~~~~~~~~~~~^~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\nint inputTime;\n^~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:413: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nTHCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\nint inputSlices;\n^~~~~~~~~~~\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricMaxPooling.cu.o\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricMaxUnpooling.cu.o\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState, THCudaDoubleTensor, THCudaDoubleTensor, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputWidth  - 1)dW >= inputWidth  + padW)\n~~~~~~~~~~~~^~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\nint inputWidth;\n^~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputHeight - 1)dH >= inputHeight + padH)\n~~~~~~~~~~~~~^~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\nint inputHeight;\n^~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputTime   - 1)dT >= inputTime   + padT)\n~~~~~~~~~~~^~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\nint inputTime;\n^~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:431: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nTHCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\nint inputSlices;\n^~~~~~~~~~~\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricReplicationPadding.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricUpSamplingNearest.cu.o\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCUNN/ATen_generated_VolumetricUpSamplingTrilinear.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir//THCS/ATen_generated_THCSTensor.cu.o\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_shapeCheck(THCState, THCudaHalfTensor, THCudaHalfTensor, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputWidth  - 1)dW >= inputWidth  + padW)\n~~~~~~~~~~~~^~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\nint inputWidth;\n^~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputHeight - 1)dH >= inputHeight + padH)\n~~~~~~~~~~~~~^~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\nint inputHeight;\n^~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputTime - 1)dT >= inputTime + padT)\n~~~~~~~~~~~^~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\nint inputTime;\n^~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nTHCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\nint inputSlices;\n^~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_shapeCheck(THCState, THCudaTensor, THCudaTensor, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputWidth  - 1)dW >= inputWidth  + padW)\n~~~~~~~~~~~~^~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\nint inputWidth;\n^~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputHeight - 1)dH >= inputHeight + padH)\n~~~~~~~~~~~~~^~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\nint inputHeight;\n^~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputTime - 1)dT >= inputTime + padT)\n~~~~~~~~~~~^~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\nint inputTime;\n^~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nTHCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\nint inputSlices;\n^~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_shapeCheck(THCState, THCudaDoubleTensor, THCudaDoubleTensor, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputWidth  - 1)*dW >= inputWidth  + padW)\n~~~~~~~~~~~~^~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\nint inputWidth;\n^~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputHeight - 1)*dH >= inputHeight + padH)\n~~~~~~~~~~~~~^~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\nint inputHeight;\n^~~~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nif ((outputTime - 1)*dT >= inputTime + padT)\n~~~~~~~~~~~^~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\nint inputTime;\n^~~~~~~~~\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\nTHCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\n^\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\nint inputSlices;\n^~~~~~~~~~~\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; bool  = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019:\n/usr/include/c++/6/tuple:626:248:   required by substitution of \u2018template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor&, at::Tensor&, at::Tensor&>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type  > constexpr std::tuple<  >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor&, at::Tensor&, at::Tensor&>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type  = ]\u2019\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here\n/usr/include/c++/6/tuple:483:67: error: mismatched argument pack lengths while expanding \u2018std::is_constructible<_Elements, _UElements&&>\u2019\nreturn _and<is_constructible<_Elements, _UElements&&>...>::value;\n^~~~~\n/usr/include/c++/6/tuple:484:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; bool  = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019 not a return-statement\n}\n^\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; bool  = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019:\n/usr/include/c++/6/tuple:626:362:   required by substitution of \u2018template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor&, at::Tensor&, at::Tensor&>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type  > constexpr std::tuple<  >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor&, at::Tensor&, at::Tensor&>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type  = ]\u2019\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here\n/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding \u2018std::is_convertible<_UElements&&, _Elements>\u2019\nreturn _and<is_convertible<_UElements&&, _Elements>...>::value;\n^~~~~\n/usr/include/c++/6/tuple:490:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; bool  = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019 not a return-statement\n}\n^\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>&; bool  = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019:\n/usr/include/c++/6/tuple:662:419:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor&, at::Tensor&, at::Tensor&>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type  > constexpr std::tuple<  >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {at::Tensor&, at::Tensor&, at::Tensor&}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor&, at::Tensor&, at::Tensor&>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type  = ]\u2019\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\nreturn  _and<_not<is_same<tuple<_Elements...>,\n^\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\nstruct is_convertible\n^~~~~~~~~~~~~~\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>&; bool  = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019 not a return-statement\n}\n^\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>&&; bool  = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019:\n/usr/include/c++/6/tuple:686:422:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor&, at::Tensor&, at::Tensor&>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type  > constexpr std::tuple<  >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {at::Tensor&, at::Tensor&, at::Tensor&}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor&, at::Tensor&, at::Tensor&>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type  = ]\u2019\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\nreturn  _and<_not<is_same<tuple<_Elements...>,\n^\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\nstruct is_convertible\n^~~~~~~~~~~~~~\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>&&; bool  = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019 not a return-statement\n}\n^\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:\n/usr/include/c++/6/tuple:626:248:   required by substitution of \u2018template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type  > constexpr std::tuple<  >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type  = ]\u2019\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here\n/usr/include/c++/6/tuple:483:67: error: mismatched argument pack lengths while expanding \u2018std::is_constructible<_Elements, _UElements&&>\u2019\nreturn _and<is_constructible<_Elements, _UElements&&>...>::value;\n^~~~~\n/usr/include/c++/6/tuple:484:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement\n}\n^\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:\n/usr/include/c++/6/tuple:626:362:   required by substitution of \u2018template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type  > constexpr std::tuple<  >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type  = ]\u2019\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here\n/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding \u2018std::is_convertible<_UElements&&, _Elements>\u2019\nreturn _and<is_convertible<_UElements&&, _Elements>...>::value;\n^~~~~\n/usr/include/c++/6/tuple:490:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement\n}\n^\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<at::Tensor, at::Tensor, at::Tensor>&; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:\n/usr/include/c++/6/tuple:662:419:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor, at::Tensor, at::Tensor>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type  > constexpr std::tuple<  >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {at::Tensor, at::Tensor, at::Tensor}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor, at::Tensor, at::Tensor>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type  = ]\u2019\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\nreturn  _and<_not<is_same<tuple<_Elements...>,\n^\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\nstruct is_convertible\n^~~~~~~~~~~~~~\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<at::Tensor, at::Tensor, at::Tensor>&; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement\n}\n^\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor, at::Tensor, at::Tensor>&&; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:\n/usr/include/c++/6/tuple:686:422:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor, at::Tensor, at::Tensor>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type  > constexpr std::tuple<  >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {at::Tensor, at::Tensor, at::Tensor}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor, at::Tensor, at::Tensor>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type  = ]\u2019\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\nreturn  _and<_not<is_same<tuple<_Elements...>,\n^\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\nstruct is_convertible\n^~~~~~~~~~~~~~\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor, at::Tensor, at::Tensor>&&; bool  = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement\n}\n^\nCMake Error at ATen_generated_NativeFunctionsCuda.cu.o.cmake:267 (message):\nError generating file\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/native/cuda/./ATen_generated_NativeFunctionsCuda.cu.o\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1120: recipe for target 'src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o' failed\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\nCMakeFiles/Makefile2:193: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed\nmake[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2\nMakefile:129: recipe for target 'all' failed\nmake: *** [all] Error 2", "body": "$git clone --recursive https://github.com/pytorch/pytorch \r\n$cd pytorch\r\n$python setup.py install                                                                                                                                                                                                      \r\nrunning install\r\nrunning build_deps\r\n-- The C compiler identification is GNU 6.4.0\r\n-- The CXX compiler identification is GNU 6.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"7.0\") \r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/zjy/program/pytorch/torch/lib/build/nccl\r\nScanning dependencies of target nccl\r\n[100%] Generating lib/libnccl.so\r\nGrabbing  src/nccl.h                          > /home/zjy/program/pytorch/torch/lib/build/nccl/include/nccl.h\r\nCompiling src/libwrap.cu                      > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/libwrap.o\r\nCompiling src/core.cu                         > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/core.o\r\nCompiling src/all_gather.cu                   > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/all_gather.o\r\nCompiling src/all_reduce.cu                   > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/all_reduce.o\r\nCompiling src/broadcast.cu                    > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/broadcast.o\r\nCompiling src/reduce.cu                       > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/reduce.o\r\nCompiling src/reduce_scatter.cu               > /home/zjy/program/pytorch/torch/lib/build/nccl/obj/reduce_scatter.o\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nptxas warning : Too big maxrregcount value specified 96, will be ignored\r\nLinking   libnccl.so.1.3.5                    > /home/zjy/program/pytorch/torch/lib/build/nccl/lib/libnccl.so.1.3.5\r\nArchiving libnccl_static.a                    > /home/zjy/program/pytorch/torch/lib/build/nccl/lib/libnccl_static.a\r\n[100%] Built target nccl\r\nInstall the project...\r\n-- Install configuration: \"Release\"\r\n-- Installing: /home/zjy/program/pytorch/torch/lib/tmp_install/include/nccl.h\r\n-- The C compiler identification is GNU 6.4.0\r\n-- The CXX compiler identification is GNU 6.4.0\r\n-- Check for working C compiler: /usr/bin/cc\r\n-- Check for working C compiler: /usr/bin/cc -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: /usr/bin/c++\r\n-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Found CUDA: /usr/local/cuda (found suitable version \"9.0\", minimum required is \"5.5\") \r\n-- Autodetected CUDA architecture(s): 6.1\r\n-- Found CUDA with FP16 support, compiling with torch.CudaHalfTensor\r\n-- Removing -DNDEBUG from compile flags\r\nCMake Warning (dev) at /home/zjy/anaconda3/share/cmake-3.9/Modules/FindOpenMP.cmake:200 (if):\r\n  Policy CMP0054 is not set: Only interpret if() arguments as variables or\r\n  keywords when unquoted.  Run \"cmake --help-policy CMP0054\" for policy\r\n  details.  Use the cmake_policy command to set the policy and suppress this\r\n  warning.\r\n\r\n  Quoted variables like \"c\" will no longer be dereferenced when the policy is\r\n  set to NEW.  Since the policy is not set the OLD behavior will be used.\r\nCall Stack (most recent call first):\r\n  /home/zjy/anaconda3/share/cmake-3.9/Modules/FindOpenMP.cmake:324 (_OPENMP_GET_FLAGS)\r\n  CMakeLists.txt:130 (FIND_PACKAGE)\r\nThis warning is for project developers.  Use -Wno-dev to suppress it.\r\n\r\n-- Found OpenMP_C: -fopenmp (found version \"4.5\") \r\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \r\n-- Compiling with OpenMP support\r\n-- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2 - True\r\n-- Compiling with MAGMA support\r\n-- MAGMA INCLUDE DIRECTORIES: /home/zjy/anaconda3/include\r\n-- MAGMA LIBRARIES: /home/zjy/anaconda3/lib/libmagma.a\r\n-- MAGMA V2 check: 1\r\n-- Could not find hardware support for NEON on this machine.\r\n-- No OMAP3 processor on this machine.\r\n-- No OMAP4 processor on this machine.\r\n-- Looking for cpuid.h\r\n-- Looking for cpuid.h - found\r\n-- Performing Test HAVE_GCC_GET_CPUID\r\n-- Performing Test HAVE_GCC_GET_CPUID - Success\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG\r\n-- Performing Test NO_GCC_EBX_FPIC_BUG - Success\r\n-- Performing Test C_HAS_SSE1_1\r\n-- Performing Test C_HAS_SSE1_1 - Success\r\n-- Performing Test C_HAS_SSE2_1\r\n-- Performing Test C_HAS_SSE2_1 - Success\r\n-- Performing Test C_HAS_SSE3_1\r\n-- Performing Test C_HAS_SSE3_1 - Failed\r\n-- Performing Test C_HAS_SSE3_2\r\n-- Performing Test C_HAS_SSE3_2 - Success\r\n-- Performing Test C_HAS_SSE4_1_1\r\n-- Performing Test C_HAS_SSE4_1_1 - Failed\r\n-- Performing Test C_HAS_SSE4_1_2\r\n-- Performing Test C_HAS_SSE4_1_2 - Success\r\n-- Performing Test C_HAS_SSE4_2_1\r\n-- Performing Test C_HAS_SSE4_2_1 - Failed\r\n-- Performing Test C_HAS_SSE4_2_2\r\n-- Performing Test C_HAS_SSE4_2_2 - Success\r\n-- Performing Test C_HAS_AVX_1\r\n-- Performing Test C_HAS_AVX_1 - Failed\r\n-- Performing Test C_HAS_AVX_2\r\n-- Performing Test C_HAS_AVX_2 - Success\r\n-- Performing Test C_HAS_AVX2_1\r\n-- Performing Test C_HAS_AVX2_1 - Failed\r\n-- Performing Test C_HAS_AVX2_2\r\n-- Performing Test C_HAS_AVX2_2 - Success\r\n-- Performing Test CXX_HAS_SSE1_1\r\n-- Performing Test CXX_HAS_SSE1_1 - Success\r\n-- Performing Test CXX_HAS_SSE2_1\r\n-- Performing Test CXX_HAS_SSE2_1 - Success\r\n-- Performing Test CXX_HAS_SSE3_1\r\n-- Performing Test CXX_HAS_SSE3_1 - Failed\r\n-- Performing Test CXX_HAS_SSE3_2\r\n-- Performing Test CXX_HAS_SSE3_2 - Success\r\n-- Performing Test CXX_HAS_SSE4_1_1\r\n-- Performing Test CXX_HAS_SSE4_1_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_1_2\r\n-- Performing Test CXX_HAS_SSE4_1_2 - Success\r\n-- Performing Test CXX_HAS_SSE4_2_1\r\n-- Performing Test CXX_HAS_SSE4_2_1 - Failed\r\n-- Performing Test CXX_HAS_SSE4_2_2\r\n-- Performing Test CXX_HAS_SSE4_2_2 - Success\r\n-- Performing Test CXX_HAS_AVX_1\r\n-- Performing Test CXX_HAS_AVX_1 - Failed\r\n-- Performing Test CXX_HAS_AVX_2\r\n-- Performing Test CXX_HAS_AVX_2 - Success\r\n-- Performing Test CXX_HAS_AVX2_1\r\n-- Performing Test CXX_HAS_AVX2_1 - Failed\r\n-- Performing Test CXX_HAS_AVX2_2\r\n-- Performing Test CXX_HAS_AVX2_2 - Success\r\n-- SSE2 Found\r\n-- SSE3 Found\r\n-- AVX Found\r\n-- AVX2 Found\r\n-- Performing Test HAS_C11_ATOMICS\r\n-- Performing Test HAS_C11_ATOMICS - Failed\r\n-- Performing Test HAS_MSC_ATOMICS\r\n-- Performing Test HAS_MSC_ATOMICS - Failed\r\n-- Performing Test HAS_GCC_ATOMICS\r\n-- Performing Test HAS_GCC_ATOMICS - Success\r\n-- Atomics: using GCC intrinsics\r\n-- Looking for sys/types.h\r\n-- Looking for sys/types.h - found\r\n-- Looking for stdint.h\r\n-- Looking for stdint.h - found\r\n-- Looking for stddef.h\r\n-- Looking for stddef.h - found\r\n-- Check size of void*\r\n-- Check size of void* - done\r\n-- Checking for [mkl_gf_lp64 - mkl_gnu_thread - mkl_core - gomp - pthread - m - dl]\r\n--   Library mkl_gf_lp64: /home/zjy/anaconda3/lib/libmkl_gf_lp64.so\r\n--   Library mkl_gnu_thread: /home/zjy/anaconda3/lib/libmkl_gnu_thread.so\r\n--   Library mkl_core: /home/zjy/anaconda3/lib/libmkl_core.so\r\n-- Found OpenMP_C: -fopenmp (found version \"4.5\") \r\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \r\n--   Library gomp: -fopenmp\r\n--   Library pthread: /usr/lib/x86_64-linux-gnu/libpthread.so\r\n--   Library m: /usr/lib/x86_64-linux-gnu/libm.so\r\n--   Library dl: /usr/lib/x86_64-linux-gnu/libdl.so\r\n-- Looking for cblas_sgemm\r\n-- Looking for cblas_sgemm - found\r\n-- MKL library found\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS\r\n-- Performing Test BLAS_F2C_DOUBLE_WORKS - Failed\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS\r\n-- Performing Test BLAS_F2C_FLOAT_WORKS - Success\r\n-- Performing Test BLAS_USE_CBLAS_DOT\r\n-- Performing Test BLAS_USE_CBLAS_DOT - Success\r\n-- Found a library with BLAS API (mkl).\r\n-- Found a library with LAPACK API. (mkl)\r\n-- Found CUDNN: /usr/local/cuda/include  \r\n-- Found cuDNN: v7.0.4  (include: /usr/local/cuda/include, library: /usr/local/cuda/lib64/libcudnn.so)\r\nCMake Deprecation Warning at src/ATen/CMakeLists.txt:7 (CMAKE_POLICY):\r\n  The OLD behavior for policy CMP0026 will be removed from a future version\r\n  of CMake.\r\n\r\n  The cmake-policies(7) manual explains that the OLD behaviors of all\r\n  policies are deprecated and that a policy should be set to OLD only under\r\n  specific short-term circumstances.  Projects should be ported to the NEW\r\n  behavior and not rely on setting a policy to OLD.\r\n\r\n\r\n-- Using python found in /home/zjy/anaconda3/bin/python\r\n['/home/zjy/program/pytorch/aten/src/THNN/generic/THNN.h', '/home/zjy/program/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/zjy/program/pytorch/aten/src/ATen/nn.yaml']\r\nATen Excluded: {'bernoulli', 'bernoulli_'}\r\n-- Looking for clock_gettime in rt\r\n-- Looking for clock_gettime in rt - found\r\n-- Looking for mmap\r\n-- Looking for mmap - found\r\n-- Looking for shm_open\r\n-- Looking for shm_open - found\r\n-- Looking for shm_unlink\r\n-- Looking for shm_unlink - found\r\n-- Looking for malloc_usable_size\r\n-- Looking for malloc_usable_size - found\r\n-- Performing Test C_HAS_THREAD\r\n-- Performing Test C_HAS_THREAD - Success\r\ndisable contrib because ATEN_NO_CONTRIB is set\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: /home/zjy/program/pytorch/torch/lib/build/aten\r\n[  0%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCBlas.cu.o\r\n[  0%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathBlas.cu.o\r\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSleep.cu.o\r\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath.cu.o\r\n[  1%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorage.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensor.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCReduceApplyUtils.cu.o\r\n[  2%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorCopy.cu.o\r\n[  3%] Generating ATen/CPUGenerator.h, ATen/CUDAGenerator.h, ATen/Declarations.yaml, ATen/CPUByteStorage.cpp, ATen/CPUByteStorage.h, ATen/CPUByteType.cpp, ATen/CPUByteType.h, ATen/CPUByteTensor.cpp, ATen/CPUByteTensor.h, ATen/CPUCharStorage.cpp, ATen/CPUCharStorage.h, ATen/CPUCharType.cpp, ATen/CPUCharType.h, ATen/CPUCharTensor.cpp, ATen/CPUCharTensor.h, ATen/CPUDoubleStorage.cpp, ATen/CPUDoubleStorage.h, ATen/CPUDoubleType.cpp, ATen/CPUDoubleType.h, ATen/CPUDoubleTensor.cpp, ATen/CPUDoubleTensor.h, ATen/CPUFloatStorage.cpp, ATen/CPUFloatStorage.h, ATen/CPUFloatType.cpp, ATen/CPUFloatType.h, ATen/CPUFloatTensor.cpp, ATen/CPUFloatTensor.h, ATen/CPUIntStorage.cpp, ATen/CPUIntStorage.h, ATen/CPUIntType.cpp, ATen/CPUIntType.h, ATen/CPUIntTensor.cpp, ATen/CPUIntTensor.h, ATen/CPULongStorage.cpp, ATen/CPULongStorage.h, ATen/CPULongType.cpp, ATen/CPULongType.h, ATen/CPULongTensor.cpp, ATen/CPULongTensor.h, ATen/CPUShortStorage.cpp, ATen/CPUShortStorage.h, ATen/CPUShortType.cpp, ATen/CPUShortType.h, ATen/CPU[  3%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCStorageCopy.cu.o\r\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMath2.cu.o\r\nShortTensor.cpp, ATen/CPUShortTensor.h, ATen/CPUHalfStorage.cpp, ATen/CPUHalfStorage.h, ATen/CPUHalfType.cpp, ATen/CPUHalfType.h, ATen/CPUHalfTensor.cpp, ATen/CPUHalfTensor.h, ATen/SparseCPUByteType.cpp, ATen/SparseCPUByteType.h, ATen/SparseCPUByteTensor.cpp, ATen/SparseCPUByteTensor.h, ATen/SparseCPUCharType.cpp, ATen/SparseCPUCharType.h, ATen/SparseCPUCharTensor.cpp, ATen/SparseCPUCharTensor.h, ATen/SparseCPUDoubleType.cpp, ATen/SparseCPUDoubleType.h, ATen/SparseCPUDoubleTensor.cpp, ATen/SparseCPUDoubleTensor.h, ATen/SparseCPUFloatType.cpp, ATen/SparseCPUFloatType.h, ATen/SparseCPUFloatTensor.cpp, ATen/SparseCPUFloatTensor.h, ATen/SparseCPUIntType.cpp, ATen/SparseCPUIntType.h, ATen/SparseCPUIntTensor.cpp, ATen/SparseCPUIntTensor.h, ATen/SparseCPULongType.cpp, ATen/SparseCPULongType.h, ATen/SparseCPULongTensor.cpp, ATen/SparseCPULongTensor.h, ATen/SparseCPUShortType.cpp, ATen/SparseCPUShortType.h, ATen/SparseCPUShortTensor.cpp, ATen/SparseCPUShortTensor.h, ATen/CUDAByteStorage.cpp, ATen/CUDAByteStorage.h, ATen/CUDAByteType.cpp, ATen/CUDAByteType.h, ATen/CUDAByteTensor.cpp, ATen/CUDAByteTensor.h, ATen/CUDACharStorage.cpp, ATen/CUDACharStorage.h, ATen/CUDACharType.cpp, ATen/CUDACharType.h, ATen/CUDACharTensor.cpp, ATen/CUDACharTensor.h, ATen/CUDADoubleStorage.cpp, ATen/CUDADoubleStorage.h, ATen/CUDADoubleType.cpp, ATen/CUDADoubleType.h, ATen/CUDADoubleTensor.cpp, ATen/CUDADoubleTensor.h, ATen/CUDAFloatStorage.cpp, ATen/CUDAFloatStorage.h, ATen/CUDAFloatType.cpp, ATen/CUDAFloatType.h, ATen/CUDAFloatTensor.cpp, ATen/CUDAFloatTensor.h, ATen/CUDAIntStorage.cpp, ATen/CUDAIntStorage.h, ATen/CUDAIntType.cpp, ATen/CUDAIntType.h, ATen/CUDAIntTensor.cpp, ATen/CUDAIntTensor.h, ATen/CUDALongStorage.cpp, ATen/CUDALongStorage.h, ATen/CUDALongType.cpp, ATen/CUDALongType.h, ATen/CUDALongTensor.cpp, ATen/CUDALongTensor.h, ATen/CUDAShortStorage.cpp, ATen/CUDAShortStorage.h, ATen/CUDAShortType.cpp, ATen/CUDAShortType.h, ATen/CUDAShortTensor.cpp, ATen/CUDAShortTensor.h, ATen/CUDAHalfStorage.cpp, ATen/CUDAHalfStorage.h, ATen/CUDAHalfType.cpp, ATen/CUDAHalfType.h, ATen/CUDAHalfTensor.cpp, ATen/CUDAHalfTensor.h, ATen/SparseCUDAByteType.cpp, ATen/SparseCUDAByteType.h, ATen/SparseCUDAByteTensor.cpp, ATen/SparseCUDAByteTensor.h, ATen/SparseCUDACharType.cpp, ATen/SparseCUDACharType.h, ATen/SparseCUDACharTensor.cpp, ATen/SparseCUDACharTensor.h, ATen/SparseCUDADoubleType.cpp, ATen/SparseCUDADoubleType.h, ATen/SparseCUDADoubleTensor.cpp, ATen/SparseCUDADoubleTensor.h, ATen/SparseCUDAFloatType.cpp, ATen/SparseCUDAFloatType.h, ATen/SparseCUDAFloatTensor.cpp, ATen/SparseCUDAFloatTensor.h, ATen/SparseCUDAIntType.cpp, ATen/SparseCUDAIntType.h, ATen/SparseCUDAIntTensor.cpp, ATen/SparseCUDAIntTensor.h, ATen/SparseCUDALongType.cpp, ATen/SparseCUDALongType.h, ATen/SparseCUDALongTensor.cpp, ATen/SparseCUDALongTensor.h, ATen/SparseCUDAShortType.cpp, ATen/SparseCUDAShortType.h, ATen/SparseCUDAShortTensor.cpp, ATen/SparseCUDAShortTensor.h, ATen/Type.h, ATen/Type.cpp, ATen/Tensor.h, ATen/TensorMethods.h, ATen/Functions.h, ATen/Dispatch.h, ATen/Copy.cpp, ATen/NativeFunctions.h\r\n[  4%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathMagma.cu.o\r\n['/home/zjy/program/pytorch/aten/src/THNN/generic/THNN.h', '/home/zjy/program/pytorch/aten/src/THCUNN/generic/THCUNN.h', '/home/zjy/program/pytorch/aten/src/ATen/nn.yaml']\r\nATen Excluded: {'bernoulli_', 'bernoulli'}\r\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathPairwise.cu.o\r\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Sgemv(THCState*, char, int64_t, int64_t, float, float*, int64_t, float*, int64_t, float, float*, int64_t)\u2019:\r\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu:105:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCublasCheck(cublasSgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\r\n     ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                          \r\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu: In function \u2018void THCudaBlas_Dgemv(THCState*, char, int64_t, int64_t, double, double*, int64_t, double*, int64_t, double, double*, int64_t)\u2019:\r\n/home/zjy/program/pytorch/aten/src/THC/THCBlas.cu:135:16: warning: \u2018op\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCublasCheck(cublasDgemv(handle, op, i_m, i_n, &alpha, a, i_lda, x, i_incx, &beta, y, i_incy));\r\n     ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                          \r\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathReduce.cu.o\r\n[  5%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMathScan.cu.o\r\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorIndex.cu.o\r\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorConv.cu.o\r\n[  6%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorRandom.cu.o\r\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorScatterGather.cu.o\r\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTopK.cu.o\r\n[  7%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorSort.cu.o\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorTypeUtils.cu.o\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCSortUtils.cu.o\r\n[  8%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCTensorMode.cu.o\r\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortByte.cu.o\r\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTByte.cu.o\r\n[  9%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseByte.cu.o\r\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareByte.cu.o\r\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceByte.cu.o\r\n[ 10%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedByte.cu.o\r\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortChar.cu.o\r\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTChar.cu.o\r\n[ 11%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseChar.cu.o\r\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareChar.cu.o\r\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceChar.cu.o\r\n[ 12%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedChar.cu.o\r\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortShort.cu.o\r\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTShort.cu.o\r\n[ 13%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseShort.cu.o\r\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareShort.cu.o\r\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceShort.cu.o\r\n[ 14%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedShort.cu.o\r\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortInt.cu.o\r\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTInt.cu.o\r\n[ 15%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseInt.cu.o\r\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareInt.cu.o\r\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceInt.cu.o\r\n[ 16%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedInt.cu.o\r\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortLong.cu.o\r\n[ 17%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTLong.cu.o\r\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseLong.cu.o\r\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareLong.cu.o\r\n[ 18%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceLong.cu.o\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedLong.cu.o\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortHalf.cu.o\r\n[ 19%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTHalf.cu.o\r\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseHalf.cu.o\r\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareHalf.cu.o\r\n[ 20%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceHalf.cu.o\r\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedHalf.cu.o\r\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortFloat.cu.o\r\n[ 21%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTFloat.cu.o\r\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseFloat.cu.o\r\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareFloat.cu.o\r\n[ 22%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceFloat.cu.o\r\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedFloat.cu.o\r\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorSortDouble.cu.o\r\n[ 23%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareTDouble.cu.o\r\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathPointwiseDouble.cu.o\r\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathCompareDouble.cu.o\r\n[ 24%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMathReduceDouble.cu.o\r\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/generated/ATen_generated_THCTensorMaskedDouble.cu.o\r\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THC/ATen_generated_THCHalf.cu.o\r\n[ 25%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_AbsCriterion.cu.o\r\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Abs.cu.o\r\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BatchNormalization.cu.o\r\n[ 26%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_BCECriterion.cu.o\r\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ClassNLLCriterion.cu.o\r\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_DistKLDivCriterion.cu.o\r\n[ 27%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_ELU.cu.o\r\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FeatureLPPooling.cu.o\r\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_FusedRNNKernel.cu.o\r\n[ 28%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_GatedLinearUnit.cu.o\r\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_HardTanh.cu.o\r\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_IndexLinear.cu.o\r\n[ 29%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_L1Cost.cu.o\r\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LeakyReLU.cu.o\r\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSigmoid.cu.o\r\n[ 30%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LogSoftMax.cu.o\r\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTableBag.cu.o\r\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_LookupTable.cu.o\r\n[ 31%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MarginCriterion.cu.o\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MSECriterion.cu.o\r\n/home/zjy/program/pytorch/aten/src/THCUNN/LookupTable.cu(25): warning: function \"__shfl(int, int, int)\"\r\n/usr/local/cuda/include/sm_30_intrinsics.hpp(152): here was declared deprecated (\"__shfl() is deprecated in favor of __shfl_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\r\n\r\n/home/zjy/program/pytorch/aten/src/THCUNN/LookupTable.cu(42): warning: function \"__any\"\r\n/usr/local/cuda/include/device_atomic_functions.h(180): here was declared deprecated (\"__any() is deprecated in favor of __any_sync() and may be removed in a future release (Use -Wno-deprecated-declarations to suppress this warning).\")\r\n\r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiLabelMarginCriterion.cu.o\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfLSTM_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:96:   required from here\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n            ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n                                                                                           ^                                          \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaHalfGRU_forw_ind_wrap(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaHalfTensor = THCudaHalfTensor]\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:100:   required from here\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n            ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:91: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n                                                                                           ^                                          \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaLSTM_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:92:   required from here\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n            ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n                                                                                       ^                                      \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaGRU_forw_ind_wrap(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*, THCudaTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaTensor = THCudaTensor]\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:96:   required from here\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n            ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:87: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n                                                                                       ^                                      \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleLSTM_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:595:98:   required from here\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n            ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:536:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*4 == THCTensor_(nElement)(state, bias1) &&\r\n                                                                                             ^                                            \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu: In instantiation of \u2018void THNN_CudaDoubleGRU_forw_ind_wrap(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*) [with INDTYPE = long unsigned int; THCState = THCState; THCudaDoubleTensor = THCudaDoubleTensor]\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:795:102:   required from here\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:28: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n            ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/FusedRNNKernel.cu:731:93: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\r\n     THAssertMsg( hid_size*3 == THCTensor_(nElement)(state, bias1) &&\r\n                                                                                             ^                                            \r\n[ 32%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_MultiMarginCriterion.cu.o\r\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_PReLU.cu.o\r\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_RReLU.cu.o\r\n[ 33%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sigmoid.cu.o\r\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SmoothL1Criterion.cu.o\r\n[ 34%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMarginCriterion.cu.o\r\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftMax.cu.o\r\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftPlus.cu.o\r\n[ 35%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SoftShrink.cu.o\r\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SparseLinear.cu.o\r\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveAveragePooling.cu.o\r\n[ 36%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAdaptiveMaxPooling.cu.o\r\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialAveragePooling.cu.o\r\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialClassNLLCriterion.cu.o\r\n[ 37%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionLocal.cu.o\r\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialConvolutionMM.cu.o\r\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialCrossMapLRN.cu.o\r\n[ 38%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDepthwiseConvolution.cu.o\r\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedConvolution.cu.o\r\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialDilatedMaxPooling.cu.o\r\n[ 39%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFractionalMaxPooling.cu.o\r\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullConvolution.cu.o\r\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialFullDilatedConvolution.cu.o\r\n[ 40%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialGridSamplerBilinear.cu.o\r\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxPooling.cu.o\r\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialMaxUnpooling.cu.o\r\n[ 41%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReflectionPadding.cu.o\r\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialReplicationPadding.cu.o\r\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialSubSampling.cu.o\r\n[ 42%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingBilinear.cu.o\r\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_SpatialUpSamplingNearest.cu.o\r\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Sqrt.cu.o\r\n[ 43%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Square.cu.o\r\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Tanh.cu.o\r\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalConvolution.cu.o\r\n[ 44%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalMaxPooling.cu.o\r\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReflectionPadding.cu.o\r\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalReplicationPadding.cu.o\r\n[ 45%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalRowConvolution.cu.o\r\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingLinear.cu.o\r\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_TemporalUpSamplingNearest.cu.o\r\n[ 46%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_Threshold.cu.o\r\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveAveragePooling.cu.o\r\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAdaptiveMaxPooling.cu.o\r\n[ 47%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricAveragePooling.cu.o\r\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricConvolution.cu.o\r\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedConvolution.cu.o\r\n[ 48%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricDilatedMaxPooling.cu.o\r\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFractionalMaxPooling.cu.o\r\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullConvolution.cu.o\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaHalfVolumetricAveragePooling_updateGradInput(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaHalfTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                 ~~~~~~~~~~~~^~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                  ~~~~~~~~~~~~~^~~~~~ \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                ~~~~~~~~~~~^~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:425: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                                         ^\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^~~~~~~~~~~\r\n[ 49%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricFullDilatedConvolution.cu.o\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaVolumetricAveragePooling_updateGradInput(THCState*, THCudaTensor*, THCudaTensor*, THCudaTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                 ~~~~~~~~~~~~^~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                  ~~~~~~~~~~~~~^~~~~~ \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                ~~~~~~~~~~~^~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:413: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                             ^\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^~~~~~~~~~~\r\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxPooling.cu.o\r\n[ 50%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricMaxUnpooling.cu.o\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu: In function \u2018void THNN_CudaDoubleVolumetricAveragePooling_updateGradInput(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaDoubleTensor*, int, int, int, int, int, int, int, int, int, bool, bool)\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                 ~~~~~~~~~~~~^~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:17:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                  ~~~~~~~~~~~~~^~~~~~ \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:16:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime   - 1)*dT >= inputTime   + padT)\r\n                                ~~~~~~~~~~~^~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:15:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:104:431: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n      THCUNN_check_dim_size(state, gradOutput, ndim, dimN, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                                               ^\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricAveragePooling.cu:14:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^~~~~~~~~~~\r\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricReplicationPadding.cu.o\r\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingNearest.cu.o\r\n[ 51%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCUNN/ATen_generated_VolumetricUpSamplingTrilinear.cu.o\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSTensor.cu.o\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/__/THCS/ATen_generated_THCSparse.cu.o\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaHalfVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaHalfTensor*, THCudaHalfTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                 ~~~~~~~~~~~~^~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                  ~~~~~~~~~~~~~^~~~~~ \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                ~~~~~~~~~~~^~~~~~ \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                          ^\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaTensor*, THCudaTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                 ~~~~~~~~~~~~^~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                  ~~~~~~~~~~~~~^~~~~~ \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                ~~~~~~~~~~~^~~~~~ \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                          ^\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu: In function \u2018void THNN_CudaDoubleVolumetricDilatedMaxPooling_shapeCheck(THCState*, THCudaDoubleTensor*, THCudaDoubleTensor*, THCudaLongTensor*, int, int, int, int, int, int, int, int, int, int, int, int, bool)\u2019:\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:98:45: warning: \u2018inputWidth\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputWidth  - 1)*dW >= inputWidth  + padW)\r\n                                 ~~~~~~~~~~~~^~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:26:5: note: \u2018inputWidth\u2019 was declared here\r\n   int inputWidth;\r\n     ^~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:96:47: warning: \u2018inputHeight\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputHeight - 1)*dH >= inputHeight + padH)\r\n                                  ~~~~~~~~~~~~~^~~~~~ \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:25:5: note: \u2018inputHeight\u2019 was declared here\r\n   int inputHeight;\r\n     ^~~~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:94:43: warning: \u2018inputTime\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     if ((outputTime - 1)*dT >= inputTime + padT)\r\n                                ~~~~~~~~~~~^~~~~~ \r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:24:5: note: \u2018inputTime\u2019 was declared here\r\n   int inputTime;\r\n     ^~~~~~~~~\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:113:410: warning: \u2018inputSlices\u2019 may be used uninitialized in this function [-Wmaybe-uninitialized]\r\n     THCUNN_check_dim_size_indices(state, indices, ndim, dimf, inputSlices);\r\n                                                                                                                                                                                                                                                                                                                                                                                                                          ^\r\n/home/zjy/program/pytorch/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu:23:5: note: \u2018inputSlices\u2019 was declared here\r\n   int inputSlices;\r\n     ^~~~~~~~~~~\r\n[ 52%] Building NVCC (Device) object src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o\r\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; bool <anonymous> = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019:\r\n/usr/include/c++/6/tuple:626:248:   required by substitution of \u2018template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor&, at::Tensor&, at::Tensor&>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor&, at::Tensor&, at::Tensor&>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]\u2019\r\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here\r\n/usr/include/c++/6/tuple:483:67: error: mismatched argument pack lengths while expanding \u2018std::is_constructible<_Elements, _UElements&&>\u2019\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/include/c++/6/tuple:484:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; bool <anonymous> = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019 not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; bool <anonymous> = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019:\r\n/usr/include/c++/6/tuple:626:362:   required by substitution of \u2018template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor&, at::Tensor&, at::Tensor&>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor&, at::Tensor&, at::Tensor&>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]\u2019\r\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here\r\n/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding \u2018std::is_convertible<_UElements&&, _Elements>\u2019\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/include/c++/6/tuple:490:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>}; bool <anonymous> = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019 not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>&; bool <anonymous> = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019:\r\n/usr/include/c++/6/tuple:662:419:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor&, at::Tensor&, at::Tensor&>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {at::Tensor&, at::Tensor&, at::Tensor&}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor&, at::Tensor&, at::Tensor&>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> = <missing>]\u2019\r\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here\r\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>&; bool <anonymous> = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019 not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>&&; bool <anonymous> = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019:\r\n/usr/include/c++/6/tuple:686:422:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor&, at::Tensor&, at::Tensor&>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {at::Tensor&, at::Tensor&, at::Tensor&}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor&, at::Tensor&, at::Tensor&>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor&, at::Tensor&, at::Tensor&>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]\u2019\r\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1416:61:   required from here\r\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor&, at::Tensor&, at::Tensor&>&&; bool <anonymous> = true; _Elements = {at::Tensor&, at::Tensor&, at::Tensor&}]\u2019 not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:\r\n/usr/include/c++/6/tuple:626:248:   required by substitution of \u2018template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]\u2019\r\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here\r\n/usr/include/c++/6/tuple:483:67: error: mismatched argument pack lengths while expanding \u2018std::is_constructible<_Elements, _UElements&&>\u2019\r\n       return __and_<is_constructible<_Elements, _UElements&&>...>::value;\r\n                                                                   ^~~~~\r\n/usr/include/c++/6/tuple:484:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_MoveConstructibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:\r\n/usr/include/c++/6/tuple:626:362:   required by substitution of \u2018template<class ... _UElements, typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; typename std::enable_if<(((std::_TC<(sizeof... (_UElements) == 1), at::Tensor, at::Tensor, at::Tensor>::_NotSameTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>()) && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && (3ul >= 1)), bool>::type <anonymous> = <missing>]\u2019\r\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here\r\n/usr/include/c++/6/tuple:489:65: error: mismatched argument pack lengths while expanding \u2018std::is_convertible<_UElements&&, _Elements>\u2019\r\n       return __and_<is_convertible<_UElements&&, _Elements>...>::value;\r\n                                                                 ^~~~~\r\n/usr/include/c++/6/tuple:490:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {std::tuple<at::Tensor, at::Tensor, at::Tensor>}; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<at::Tensor, at::Tensor, at::Tensor>&; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:\r\n/usr/include/c++/6/tuple:662:419:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor, at::Tensor, at::Tensor>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(const std::tuple<_Args1 ...>&) [with _UElements = {at::Tensor, at::Tensor, at::Tensor}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor, at::Tensor, at::Tensor>::_NonNestedTuple<const tuple<_Elements ...>&>()), bool>::type <anonymous> = <missing>]\u2019\r\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here\r\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = const std::tuple<at::Tensor, at::Tensor, at::Tensor>&; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement\r\n     }\r\n ^\r\n/usr/include/c++/6/tuple: In instantiation of \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor, at::Tensor, at::Tensor>&&; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019:\r\n/usr/include/c++/6/tuple:686:422:   required by substitution of \u2018template<class ... _UElements, class _Dummy, typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor, at::Tensor, at::Tensor>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> > constexpr std::tuple< <template-parameter-1-1> >::tuple(std::tuple<_Args1 ...>&&) [with _UElements = {at::Tensor, at::Tensor, at::Tensor}; _Dummy = void; typename std::enable_if<((std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_MoveConstructibleTuple<_UElements ...>() && std::_TC<(1ul == sizeof... (_UElements)), at::Tensor, at::Tensor, at::Tensor>::_ImplicitlyMoveConvertibleTuple<_UElements ...>()) && std::_TC<(std::is_same<_Dummy, void>::value && (1ul == 1)), at::Tensor, at::Tensor, at::Tensor>::_NonNestedTuple<tuple<_Elements ...>&&>()), bool>::type <anonymous> = <missing>]\u2019\r\n/home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/ATen/Functions.h:1419:39:   required from here\r\n/usr/include/c++/6/tuple:495:244: error: wrong number of template arguments (4, should be 2)\r\n       return  __and_<__not_<is_same<tuple<_Elements...>,\r\n                                                                                                                                                                                                                                                    ^    \r\n/usr/include/c++/6/type_traits:1558:8: note: provided for \u2018template<class _From, class _To> struct std::is_convertible\u2019\r\n     struct is_convertible\r\n        ^~~~~~~~~~~~~~\r\n/usr/include/c++/6/tuple:502:1: error: body of constexpr function \u2018static constexpr bool std::_TC<<anonymous>, _Elements>::_NonNestedTuple() [with _SrcTuple = std::tuple<at::Tensor, at::Tensor, at::Tensor>&&; bool <anonymous> = true; _Elements = {at::Tensor, at::Tensor, at::Tensor}]\u2019 not a return-statement\r\n     }\r\n ^\r\nCMake Error at ATen_generated_NativeFunctionsCuda.cu.o.cmake:267 (message):\r\n  Error generating file\r\n  /home/zjy/program/pytorch/torch/lib/build/aten/src/ATen/CMakeFiles/ATen.dir/native/cuda/./ATen_generated_NativeFunctionsCuda.cu.o\r\n\r\n\r\nsrc/ATen/CMakeFiles/ATen.dir/build.make:1120: recipe for target 'src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o' failed\r\nmake[2]: *** [src/ATen/CMakeFiles/ATen.dir/native/cuda/ATen_generated_NativeFunctionsCuda.cu.o] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\nCMakeFiles/Makefile2:193: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed\r\nmake[1]: *** [src/ATen/CMakeFiles/ATen.dir/all] Error 2\r\nMakefile:129: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\n\r\n"}