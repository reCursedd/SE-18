{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6864", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6864/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6864/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6864/events", "html_url": "https://github.com/pytorch/pytorch/issues/6864", "id": 316857129, "node_id": "MDU6SXNzdWUzMTY4NTcxMjk=", "number": 6864, "title": "`softmax` and `log_softmax` returns `nan` even when only 1 `Inf` is present", "user": {"login": "sytrus-in-github", "id": 12224616, "node_id": "MDQ6VXNlcjEyMjI0NjE2", "avatar_url": "https://avatars0.githubusercontent.com/u/12224616?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sytrus-in-github", "html_url": "https://github.com/sytrus-in-github", "followers_url": "https://api.github.com/users/sytrus-in-github/followers", "following_url": "https://api.github.com/users/sytrus-in-github/following{/other_user}", "gists_url": "https://api.github.com/users/sytrus-in-github/gists{/gist_id}", "starred_url": "https://api.github.com/users/sytrus-in-github/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sytrus-in-github/subscriptions", "organizations_url": "https://api.github.com/users/sytrus-in-github/orgs", "repos_url": "https://api.github.com/users/sytrus-in-github/repos", "events_url": "https://api.github.com/users/sytrus-in-github/events{/privacy}", "received_events_url": "https://api.github.com/users/sytrus-in-github/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-04-23T15:12:14Z", "updated_at": "2018-04-24T12:10:46Z", "closed_at": "2018-04-24T08:46:18Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p><code>F.softmax</code> should return one-hot representation when only 1 value is <code>Inf</code> and the others are all finite or <code>-Inf</code>. Instead, they return <code>nan</code> for all entries.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.softmax(torch.Tensor([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-inf<span class=\"pl-pds\">'</span></span>)]), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\ntensor([ <span class=\"pl-c1\">1.0000</span>,  <span class=\"pl-c1\">0.0000</span>])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.softmax(torch.Tensor([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>inf<span class=\"pl-pds\">'</span></span>)]), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> should give [0.0, 1.0]</span>\ntensor([    nan,     nan])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.log_softmax(torch.Tensor([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-inf<span class=\"pl-pds\">'</span></span>)]), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\ntensor([ <span class=\"pl-c1\">0.0000</span>,    <span class=\"pl-k\">-</span>inf])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.log_softmax(torch.Tensor([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>inf<span class=\"pl-pds\">'</span></span>)]), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\ntensor([    nan,     nan])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.softmax(torch.Tensor([<span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-inf<span class=\"pl-pds\">'</span></span>), <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-inf<span class=\"pl-pds\">'</span></span>)]), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\ntensor([ <span class=\"pl-c1\">0.0000</span>,  <span class=\"pl-c1\">1.0000</span>,  <span class=\"pl-c1\">0.0000</span>])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.softmax(torch.Tensor([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>inf<span class=\"pl-pds\">'</span></span>), <span class=\"pl-c1\">0</span>]), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> should give [0.0, 1.0, 0.0]</span>\ntensor([    nan,     nan,     nan])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> F.softmax(torch.Tensor([<span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-inf<span class=\"pl-pds\">'</span></span>), <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">float</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>inf<span class=\"pl-pds\">'</span></span>)]), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> should give [0.0, 0.0, 1.0]</span>\ntensor([    nan,     nan,     nan])</pre></div>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): conda install pytorch-nightly -c pytorch</li>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: nightly 2018.04.20</li>\n<li>Python version: 3.6.3</li>\n<li>CUDA/cuDNN version: N/A</li>\n<li>GPU models and configuration: N/A</li>\n</ul>", "body_text": "Issue description\nF.softmax should return one-hot representation when only 1 value is Inf and the others are all finite or -Inf. Instead, they return nan for all entries.\nCode example\n>>> import torch\n>>> import torch.nn.functional as F\n>>> F.softmax(torch.Tensor([0, float('-inf')]), -1)\ntensor([ 1.0000,  0.0000])\n>>> F.softmax(torch.Tensor([0, float('inf')]), -1)  # should give [0.0, 1.0]\ntensor([    nan,     nan])\n>>> F.log_softmax(torch.Tensor([0, float('-inf')]), -1)\ntensor([ 0.0000,    -inf])\n>>> F.log_softmax(torch.Tensor([0, float('inf')]), -1)\ntensor([    nan,     nan])\n>>> F.softmax(torch.Tensor([float('-inf'), 0, float('-inf')]), -1)\ntensor([ 0.0000,  1.0000,  0.0000])\n>>> F.softmax(torch.Tensor([0, float('inf'), 0]), -1)  # should give [0.0, 1.0, 0.0]\ntensor([    nan,     nan,     nan])\n>>> F.softmax(torch.Tensor([float('-inf'), 0, float('inf')]), -1)  # should give [0.0, 0.0, 1.0]\ntensor([    nan,     nan,     nan])\nSystem Info\n\nPyTorch or Caffe2: PyTorch\nHow you installed PyTorch (conda, pip, source): conda install pytorch-nightly -c pytorch\nOS: Ubuntu 16.04\nPyTorch version: nightly 2018.04.20\nPython version: 3.6.3\nCUDA/cuDNN version: N/A\nGPU models and configuration: N/A", "body": "## Issue description\r\n\r\n`F.softmax` should return one-hot representation when only 1 value is `Inf` and the others are all finite or `-Inf`. Instead, they return `nan` for all entries.\r\n\r\n## Code example\r\n```python\r\n>>> import torch\r\n>>> import torch.nn.functional as F\r\n>>> F.softmax(torch.Tensor([0, float('-inf')]), -1)\r\ntensor([ 1.0000,  0.0000])\r\n>>> F.softmax(torch.Tensor([0, float('inf')]), -1)  # should give [0.0, 1.0]\r\ntensor([    nan,     nan])\r\n>>> F.log_softmax(torch.Tensor([0, float('-inf')]), -1)\r\ntensor([ 0.0000,    -inf])\r\n>>> F.log_softmax(torch.Tensor([0, float('inf')]), -1)\r\ntensor([    nan,     nan])\r\n>>> F.softmax(torch.Tensor([float('-inf'), 0, float('-inf')]), -1)\r\ntensor([ 0.0000,  1.0000,  0.0000])\r\n>>> F.softmax(torch.Tensor([0, float('inf'), 0]), -1)  # should give [0.0, 1.0, 0.0]\r\ntensor([    nan,     nan,     nan])\r\n>>> F.softmax(torch.Tensor([float('-inf'), 0, float('inf')]), -1)  # should give [0.0, 0.0, 1.0]\r\ntensor([    nan,     nan,     nan])\r\n```\r\n\r\n## System Info\r\n\r\n- PyTorch or Caffe2: PyTorch\r\n- How you installed PyTorch (conda, pip, source): conda install pytorch-nightly -c pytorch\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: nightly 2018.04.20\r\n- Python version: 3.6.3\r\n- CUDA/cuDNN version: N/A\r\n- GPU models and configuration: N/A\r\n\r\n"}