{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383694920", "html_url": "https://github.com/pytorch/pytorch/issues/6864#issuecomment-383694920", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6864", "id": 383694920, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzY5NDkyMA==", "user": {"login": "sytrus-in-github", "id": 12224616, "node_id": "MDQ6VXNlcjEyMjI0NjE2", "avatar_url": "https://avatars0.githubusercontent.com/u/12224616?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sytrus-in-github", "html_url": "https://github.com/sytrus-in-github", "followers_url": "https://api.github.com/users/sytrus-in-github/followers", "following_url": "https://api.github.com/users/sytrus-in-github/following{/other_user}", "gists_url": "https://api.github.com/users/sytrus-in-github/gists{/gist_id}", "starred_url": "https://api.github.com/users/sytrus-in-github/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sytrus-in-github/subscriptions", "organizations_url": "https://api.github.com/users/sytrus-in-github/orgs", "repos_url": "https://api.github.com/users/sytrus-in-github/repos", "events_url": "https://api.github.com/users/sytrus-in-github/events{/privacy}", "received_events_url": "https://api.github.com/users/sytrus-in-github/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-23T19:31:15Z", "updated_at": "2018-04-23T19:43:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thank you for your reply!<br>\nOf course I understand that we are in a numerically tricky case. But since pytorch is trying to be friendly with edge cases: e.g. supporting <code>inf</code> and <code>-inf</code> for ops, enabling sub-gradient etc. this might be a nice edge case to cover. I have no idea how hard it is to implement this or how bad the performance regression will be, though.<br>\nThe case might occur if e.g. you first do operations with log likelihood and then try to recover the resulting probability distribution by taking <code>softmax</code>.</p>", "body_text": "Thank you for your reply!\nOf course I understand that we are in a numerically tricky case. But since pytorch is trying to be friendly with edge cases: e.g. supporting inf and -inf for ops, enabling sub-gradient etc. this might be a nice edge case to cover. I have no idea how hard it is to implement this or how bad the performance regression will be, though.\nThe case might occur if e.g. you first do operations with log likelihood and then try to recover the resulting probability distribution by taking softmax.", "body": "Thank you for your reply!\r\nOf course I understand that we are in a numerically tricky case. But since pytorch is trying to be friendly with edge cases: e.g. supporting `inf` and `-inf` for ops, enabling sub-gradient etc. this might be a nice edge case to cover. I have no idea how hard it is to implement this or how bad the performance regression will be, though.\r\nThe case might occur if e.g. you first do operations with log likelihood and then try to recover the resulting probability distribution by taking `softmax`."}