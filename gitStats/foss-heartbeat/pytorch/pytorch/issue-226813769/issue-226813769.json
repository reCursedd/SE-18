{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1500", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1500/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1500/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1500/events", "html_url": "https://github.com/pytorch/pytorch/issues/1500", "id": 226813769, "node_id": "MDU6SXNzdWUyMjY4MTM3Njk=", "number": 1500, "title": "Using running mean in forward pass when model is parallelized", "user": {"login": "johnwlambert", "id": 16724970, "node_id": "MDQ6VXNlcjE2NzI0OTcw", "avatar_url": "https://avatars2.githubusercontent.com/u/16724970?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johnwlambert", "html_url": "https://github.com/johnwlambert", "followers_url": "https://api.github.com/users/johnwlambert/followers", "following_url": "https://api.github.com/users/johnwlambert/following{/other_user}", "gists_url": "https://api.github.com/users/johnwlambert/gists{/gist_id}", "starred_url": "https://api.github.com/users/johnwlambert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johnwlambert/subscriptions", "organizations_url": "https://api.github.com/users/johnwlambert/orgs", "repos_url": "https://api.github.com/users/johnwlambert/repos", "events_url": "https://api.github.com/users/johnwlambert/events{/privacy}", "received_events_url": "https://api.github.com/users/johnwlambert/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-06T23:54:31Z", "updated_at": "2017-05-07T21:50:47Z", "closed_at": "2017-05-07T21:34:47Z", "author_association": "NONE", "body_html": "<p>Hi, in my <em>init</em> function of my nn.Module I define variables for a running mean. I then parallelize the model with data_parallel and place the model onto the GPUs I have with .cuda(). I use the running mean during my forward pass, but encounter an error telling me that tensors are located on different GPUs. What's the best way to overcome this sort of obstacle in the PyTorch paradigm? Making only certain parts of my forward pass parallelized, and calling .cuda(0) before the section with the running mean to place everything on GPU 0, for example?</p>\n<p>Are there any good examples for this? I assume that you all did something similar when computing running means and variances for your batch norm implementation but couldn't find anything easily.</p>", "body_text": "Hi, in my init function of my nn.Module I define variables for a running mean. I then parallelize the model with data_parallel and place the model onto the GPUs I have with .cuda(). I use the running mean during my forward pass, but encounter an error telling me that tensors are located on different GPUs. What's the best way to overcome this sort of obstacle in the PyTorch paradigm? Making only certain parts of my forward pass parallelized, and calling .cuda(0) before the section with the running mean to place everything on GPU 0, for example?\nAre there any good examples for this? I assume that you all did something similar when computing running means and variances for your batch norm implementation but couldn't find anything easily.", "body": "Hi, in my _init_ function of my nn.Module I define variables for a running mean. I then parallelize the model with data_parallel and place the model onto the GPUs I have with .cuda(). I use the running mean during my forward pass, but encounter an error telling me that tensors are located on different GPUs. What's the best way to overcome this sort of obstacle in the PyTorch paradigm? Making only certain parts of my forward pass parallelized, and calling .cuda(0) before the section with the running mean to place everything on GPU 0, for example?\r\n\r\nAre there any good examples for this? I assume that you all did something similar when computing running means and variances for your batch norm implementation but couldn't find anything easily."}