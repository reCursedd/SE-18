{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208280290", "pull_request_review_id": 144056928, "id": 208280290, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwODI4MDI5MA==", "diff_hunk": "@@ -0,0 +1,601 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n+#include \"ATen/TensorUtils.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+#include <THC/THCNumerics.cuh>\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+using at::cuda::detail::TensorInfo;\n+using at::cuda::detail::getTensorInfo;\n+using at::cuda::detail::IndexToOffset;\n+using at::cuda::detail::canUse32BitIndexMath;\n+\n+// Factor will be 3 for GRU and 4 for LSTM\n+void checkSizes(CheckedFrom c,\n+                const TensorArg& input_gates, const TensorArg& hidden_gates,\n+                const TensorArg& input_bias, const TensorArg& hidden_bias,\n+                int64_t factor, const TensorArg& prev_hidden) {\n+  checkDim(c, input_gates, 2);\n+  checkSameSize(c, input_gates, hidden_gates);\n+  int64_t gates_size = input_gates->size(1);\n+\n+  if (input_bias->defined()) {\n+    checkDim(c, input_bias, 1);\n+    checkNumel(c, input_bias, gates_size);\n+    checkSameSize(c, input_bias, hidden_bias);\n+  }\n+\n+  checkDim(c, prev_hidden, 2);\n+  checkNumel(c, prev_hidden, input_gates->size(0) * gates_size / factor);\n+\n+  checkAllSameGPU(c, {input_gates, hidden_gates, input_bias, hidden_bias, prev_hidden});\n+}\n+\n+bool allContiguous(at::TensorList tensors) {\n+  return std::all_of(tensors.begin(), tensors.end(),\n+                     [](const at::Tensor& t) { return !t.defined() || t.is_contiguous(); });\n+}\n+\n+void getLaunchConfig(dim3& block, dim3& grid, int64_t numel) {\n+  int curDevice = -1;\n+  cudaGetDevice(&curDevice);\n+  block = cuda::getApplyBlock();\n+  AT_ASSERTM(cuda::getApplyGrid(numel, grid, curDevice),\n+             \"Could not get grid size for pointwise apply.\");\n+}\n+\n+template<typename T, typename T2>\n+TensorInfo<T, T2> tryGetTensorInfo(const at::Tensor& t) {\n+  return t.defined() ? getTensorInfo<T, T2>(t) : TensorInfo<T, T2>{};\n+}\n+\n+void collapseDims() {};\n+template<typename T, typename T2, typename... Args>\n+void collapseDims(TensorInfo<T, T2>& info, Args&... infos) {\n+  info.collapseDims();\n+  collapseDims(infos...);\n+}\n+\n+#define DEVICE_LINEAR_GET(D_TENSOR, INDEX)                              \\\n+  D_TENSOR.data[IndexToOffset<scalar_t, index_type, indexing_kind>::get(INDEX, D_TENSOR)]\n+\n+// Biases are always 1D\n+#define DEVICE_BIAS_GET(D_TENSOR, INDEX)                              \\\n+  D_TENSOR.data[IndexToOffset<scalar_t, index_type, 1>::get(INDEX, D_TENSOR)]\n+\n+#define H2F(input) ScalarConvert<scalar_t, accscalar_t>::to(input)\n+#define F2H(input) ScalarConvert<accscalar_t, scalar_t>::to(input)\n+\n+template<typename T>\n+__device__ __forceinline__\n+T sigmoid(T in)  {\n+  T one = static_cast<T>(1.0);\n+  return one / (one + THCNumerics<T>::exp(-in));\n+}\n+\n+namespace kernel {\n+\n+template <typename scalar_t, typename accscalar_t, typename index_type, int indexing_kind>\n+#if __CUDA_ARCH__ >= 350\n+__launch_bounds__(32 * 16, 4)\n+#endif\n+__global__ void lstm_cell_forward(\n+            TensorInfo<scalar_t, index_type> input,\n+            TensorInfo<scalar_t, index_type> hidden,\n+            TensorInfo<scalar_t, index_type> bias1,\n+            TensorInfo<scalar_t, index_type> bias2,\n+            TensorInfo<scalar_t, index_type> _cx,\n+            TensorInfo<scalar_t, index_type> _hy,\n+            TensorInfo<scalar_t, index_type> _cy,\n+            TensorInfo<scalar_t, index_type> workspace,\n+            index_type hsz,\n+            index_type totalElements) {\n+    bool has_bias = (bias1.data != NULL);\n+    for (index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+      index_type offset = (linearIndex/hsz)*4*hsz+linearIndex%hsz;\n+\n+      scalar_t iig = DEVICE_LINEAR_GET(input, offset+0*hsz);\n+      scalar_t ifg = DEVICE_LINEAR_GET(input, offset+1*hsz);\n+      scalar_t icg = DEVICE_LINEAR_GET(input, offset+2*hsz);\n+      scalar_t iog = DEVICE_LINEAR_GET(input, offset+3*hsz);\n+\n+      scalar_t hig = DEVICE_LINEAR_GET(hidden, offset+0*hsz);\n+      scalar_t hfg = DEVICE_LINEAR_GET(hidden, offset+1*hsz);\n+      scalar_t hcg = DEVICE_LINEAR_GET(hidden,  offset+2*hsz);\n+      scalar_t hog = DEVICE_LINEAR_GET(hidden,  offset+3*hsz);\n+\n+      scalar_t* wig = &DEVICE_LINEAR_GET(workspace, offset+0*hsz);\n+      scalar_t* wfg = &DEVICE_LINEAR_GET(workspace, offset+1*hsz);\n+      scalar_t* wcg = &DEVICE_LINEAR_GET(workspace, offset+2*hsz);\n+      scalar_t* wog = &DEVICE_LINEAR_GET(workspace, offset+3*hsz);\n+\n+      scalar_t cx = DEVICE_LINEAR_GET(_cx, linearIndex);\n+\n+      scalar_t* hy = &DEVICE_LINEAR_GET(_hy, linearIndex);\n+      scalar_t* cy = &DEVICE_LINEAR_GET(_cy, linearIndex);\n+\n+      scalar_t b1i, b1f, b1c, b1o;\n+      scalar_t b2i, b2f, b2c, b2o;\n+\n+      if (has_bias) {\n+        b1i = DEVICE_BIAS_GET(bias1, linearIndex % hsz + 0 * hsz);\n+        b1f = DEVICE_BIAS_GET(bias1, linearIndex % hsz + 1 * hsz);\n+        b1c = DEVICE_BIAS_GET(bias1, linearIndex % hsz + 2 * hsz);\n+        b1o = DEVICE_BIAS_GET(bias1, linearIndex % hsz + 3 * hsz);\n+\n+        b2i = DEVICE_BIAS_GET(bias2, linearIndex % hsz + 0 * hsz);\n+        b2f = DEVICE_BIAS_GET(bias2, linearIndex % hsz + 1 * hsz);\n+        b2c = DEVICE_BIAS_GET(bias2, linearIndex % hsz + 2 * hsz);\n+        b2o = DEVICE_BIAS_GET(bias2, linearIndex % hsz + 3 * hsz);\n+      } else {\n+#ifndef THC_REAL_IS_HALF\n+        b1i = 0.0; b1f = 0.0; b1c = 0.0; b1o = 0.0;\n+        b2i = 0.0; b2f = 0.0; b2c = 0.0; b2o = 0.0;\n+#else\n+        b1i = F2H(0.0); b1f = F2H(0.0); b1c = F2H(0.0); b1o = F2H(0.0);\n+        b2i = F2H(0.0); b2f = F2H(0.0); b2c = F2H(0.0); b2o = F2H(0.0);\n+#endif\n+      }\n+\n+      accscalar_t ig, fg, cg, og;\n+      accscalar_t f_hy, f_cy;\n+\n+      ig = sigmoid(H2F(iig) + H2F(hig) + H2F(b1i) + H2F(b2i));\n+      fg = sigmoid(H2F(ifg) + H2F(hfg) + H2F(b1f) + H2F(b2f));\n+      cg = THCNumerics<accscalar_t>::tanh(H2F(icg) + H2F(hcg) + H2F(b1c) + H2F(b2c));\n+      og = sigmoid(H2F(iog) + H2F(hog) + H2F(b1o) + H2F(b2o));\n+\n+      f_cy = (fg * H2F(cx)) + (ig * cg);\n+      f_hy = og * THCNumerics<accscalar_t>::tanh(f_cy);\n+\n+      *hy = F2H(f_hy);\n+      *cy = F2H(f_cy);\n+\n+      //SAVE FOR BACKWARDS\n+      //Also need cy and cx but can be saved easily in python\n+      *wig = F2H(ig);\n+      *wfg = F2H(fg);\n+      *wcg = F2H(cg);\n+      *wog = F2H(og);\n+    }\n+}\n+\n+template <typename scalar_t, typename accscalar_t, typename index_type, int indexing_kind>\n+#if __CUDA_ARCH__ >= 350\n+__launch_bounds__(32 * 16, 4)\n+#endif\n+__global__ void lstm_cell_backward(\n+              TensorInfo<scalar_t, index_type> storage,\n+              TensorInfo<scalar_t, index_type> gradInGates,\n+              TensorInfo<scalar_t, index_type> _cx,\n+              TensorInfo<scalar_t, index_type> _cy,\n+              TensorInfo<scalar_t, index_type> gradoutput,\n+              TensorInfo<scalar_t, index_type> gradoutputcell,\n+              TensorInfo<scalar_t, index_type> gradInputCx,\n+              index_type hsz,\n+              index_type totalElements) {\n+  for (index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+    index_type offset = (linearIndex/hsz)*4*hsz+linearIndex%hsz;\n+\n+    scalar_t ig = DEVICE_LINEAR_GET(storage, offset+0*hsz);\n+    scalar_t fg = DEVICE_LINEAR_GET(storage, offset+1*hsz);\n+    scalar_t cg = DEVICE_LINEAR_GET(storage, offset+2*hsz);\n+    scalar_t og = DEVICE_LINEAR_GET(storage, offset+3*hsz);\n+\n+    scalar_t* ih = &DEVICE_LINEAR_GET(gradInGates, offset+0*hsz);\n+    scalar_t* fh = &DEVICE_LINEAR_GET(gradInGates, offset+1*hsz);\n+    scalar_t* ch = &DEVICE_LINEAR_GET(gradInGates, offset+2*hsz);\n+    scalar_t* oh = &DEVICE_LINEAR_GET(gradInGates, offset+3*hsz);\n+\n+    //will return hidden grads here\n+    scalar_t cx = DEVICE_LINEAR_GET(_cx, linearIndex);\n+    scalar_t cy = DEVICE_LINEAR_GET(_cy, linearIndex);\n+\n+    scalar_t* gi = &DEVICE_LINEAR_GET(gradInputCx, linearIndex);\n+\n+    scalar_t go  = DEVICE_LINEAR_GET(gradoutput, linearIndex);\n+    scalar_t goc = DEVICE_LINEAR_GET(gradoutputcell, linearIndex);\n+\n+    accscalar_t gcx = THCNumerics<accscalar_t>::tanh(H2F(cy));\n+\n+    accscalar_t gog = H2F(go) * gcx;\n+    gcx = H2F(go) * H2F(og) * ( 1 - gcx*gcx) + H2F(goc);\n+\n+    accscalar_t gig = gcx * H2F(cg);\n+    accscalar_t gfg = gcx * H2F(cx);\n+    accscalar_t gcg = gcx * H2F(ig);\n+\n+    gcx = gcx * H2F(fg);\n+\n+    gig = gig * (1-H2F(ig)) * H2F(ig);\n+    gfg = gfg * (1-H2F(fg)) * H2F(fg);\n+    gcg = gcg * (1-H2F(cg)*H2F(cg));\n+    gog = gog * (1-H2F(og)) * H2F(og);\n+\n+    *ih = F2H(gig);\n+    *fh = F2H(gfg);\n+    *ch = F2H(gcg);\n+    *oh = F2H(gog);\n+\n+    *gi = F2H(gcx);\n+  }\n+}\n+\n+template <typename scalar_t, typename accscalar_t, typename index_type, int indexing_kind>\n+#if __CUDA_ARCH__ >= 350\n+__launch_bounds__(32 * 16, 4)\n+#endif\n+__global__ void gru_cell_forward(\n+            TensorInfo<scalar_t, index_type> Input,\n+            TensorInfo<scalar_t, index_type> Hidden,\n+            TensorInfo<scalar_t, index_type> Bias1,\n+            TensorInfo<scalar_t, index_type> Bias2,\n+            TensorInfo<scalar_t, index_type> _hx,\n+            TensorInfo<scalar_t, index_type> _hy,\n+            TensorInfo<scalar_t, index_type> storage,\n+            index_type hsz,\n+            index_type totalElements) {\n+  bool has_bias = (Bias1.data != NULL);\n+  for (index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+      index_type offset = (linearIndex/hsz)*3*hsz+linearIndex%hsz;\n+\n+      scalar_t ir = DEVICE_LINEAR_GET(Input, offset+0*hsz);\n+      scalar_t ii = DEVICE_LINEAR_GET(Input, offset+1*hsz);\n+      scalar_t in = DEVICE_LINEAR_GET(Input, offset+2*hsz);\n+      scalar_t hr = DEVICE_LINEAR_GET(Hidden,offset+0*hsz);\n+      scalar_t hi = DEVICE_LINEAR_GET(Hidden,offset+1*hsz);\n+      scalar_t hn = DEVICE_LINEAR_GET(Hidden,  offset+2*hsz);\n+\n+      scalar_t hx = DEVICE_LINEAR_GET(_hx, linearIndex);\n+      scalar_t* hy = &DEVICE_LINEAR_GET(_hy, linearIndex);\n+\n+      scalar_t b1r, b1i, b1n, b2r, b2i, b2n;\n+\n+      if (has_bias) {\n+        b1r = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+0*hsz);\n+        b1i = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+1*hsz);\n+        b1n = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+2*hsz);\n+\n+        b2r = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+0*hsz);\n+        b2i = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+1*hsz);\n+        b2n = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+2*hsz);\n+      } else {\n+#ifndef THC_REAL_IS_HALF\n+        b1r = 0.0; b1i = 0.0; b1n = 0.0;\n+        b2r = 0.0; b2i = 0.0; b2n = 0.0;\n+#else\n+        b1r = F2H(0.0); b1i = F2H(0.0); b1n = F2H(0.0);\n+        b2r = F2H(0.0); b2i = F2H(0.0); b2n = F2H(0.0);\n+#endif\n+      }\n+\n+      offset = (linearIndex/hsz)*5*hsz+linearIndex%hsz;\n+\n+      accscalar_t rg, ig, ng;\n+\n+      rg = sigmoid(H2F(ir) + H2F(hr) + H2F(b1r) + H2F(b2r));\n+      ig = sigmoid(H2F(ii) + H2F(hi) + H2F(b1i) + H2F(b2i));\n+\n+      ng = H2F(in) + H2F(b1n) + rg*( H2F(hn)+H2F(b2n) );\n+      ng = THCNumerics<accscalar_t>::tanh(ng);\n+      *hy = F2H( ng + ig * ( H2F(hx)-ng ) );\n+\n+      //SAVE FOR BACKWARDS\n+      DEVICE_LINEAR_GET(storage, offset+0*hsz) = F2H(rg);\n+      DEVICE_LINEAR_GET(storage, offset+1*hsz) = F2H(ig);\n+      DEVICE_LINEAR_GET(storage, offset+2*hsz) = F2H(ng);\n+      DEVICE_LINEAR_GET(storage, offset+3*hsz) = hx;\n+      DEVICE_LINEAR_GET(storage, offset+4*hsz) = F2H(H2F(hn) + H2F(b2n));\n+    }\n+}\n+\n+template <typename scalar_t, typename accscalar_t, typename index_type, int indexing_kind>\n+#if __CUDA_ARCH__ >= 350\n+__launch_bounds__(32 * 16, 4)\n+#endif\n+__global__ void gru_cell_backward(\n+             TensorInfo<scalar_t, index_type> gradInInput,\n+             TensorInfo<scalar_t, index_type> gradInHidden,\n+             TensorInfo<scalar_t, index_type> gradOutput,\n+             TensorInfo<scalar_t, index_type> gradInputHx,\n+             TensorInfo<scalar_t, index_type> storage,\n+             index_type hsz,\n+             index_type totalElements) {\n+  for (index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+    index_type offset = (linearIndex/hsz)*5*hsz+linearIndex%hsz;\n+\n+    scalar_t rg = DEVICE_LINEAR_GET(storage, offset+0*hsz);\n+    scalar_t ig = DEVICE_LINEAR_GET(storage, offset+1*hsz);\n+    scalar_t ng = DEVICE_LINEAR_GET(storage, offset+2*hsz);\n+    scalar_t hx = DEVICE_LINEAR_GET(storage, offset+3*hsz);\n+    scalar_t hn = DEVICE_LINEAR_GET(storage, offset+4*hsz);\n+\n+    scalar_t go = DEVICE_LINEAR_GET(gradOutput, linearIndex);\n+\n+    offset = (linearIndex/hsz)*3*hsz+linearIndex%hsz;\n+\n+    accscalar_t gig = H2F(go)*( H2F(hx)-H2F(ng) )*( 1-H2F(ig) )*H2F(ig);\n+    accscalar_t ghx = H2F(go)*H2F(ig);\n+    accscalar_t gin = H2F(go)*( 1-H2F(ig) )*( 1-H2F(ng)*H2F(ng) );\n+    accscalar_t ghn = gin * H2F(rg);\n+    accscalar_t grg = gin *H2F(hn)*( 1-H2F(rg) )*H2F(rg);\n+\n+    DEVICE_LINEAR_GET(gradInInput, offset+0*hsz) = F2H(grg);\n+    DEVICE_LINEAR_GET(gradInInput, offset+1*hsz) = F2H(gig);\n+    DEVICE_LINEAR_GET(gradInInput, offset+2*hsz) = F2H(gin);\n+\n+    DEVICE_LINEAR_GET(gradInHidden, offset+0*hsz) = F2H(grg);\n+    DEVICE_LINEAR_GET(gradInHidden, offset+1*hsz) = F2H(gig);\n+    DEVICE_LINEAR_GET(gradInHidden, offset+2*hsz) = F2H(ghn);\n+    DEVICE_LINEAR_GET(gradInputHx, linearIndex) = F2H(ghx);\n+  }\n+}\n+\n+#undef DEVICE_LINEAR_GET\n+#undef DEVICE_BIAS_GET\n+#undef H2F\n+#undef F2H\n+\n+} // namespace kernel\n+\n+template<typename scalar_t, typename index_type>\n+void lstm_forward_index(const Tensor& input_gates, const Tensor& hidden_gates,", "path": "aten/src/ATen/native/cuda/RNN.cu", "position": null, "original_position": 355, "commit_id": "1a78db4286a2bc422312f29247b0d50584855c29", "original_commit_id": "9f423f29740c6c30d7251b1565ba862bdeea5473", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "`LSTM_forw_ind_wrap` I guess.", "created_at": "2018-08-07T15:35:19Z", "updated_at": "2018-11-23T15:48:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/10305#discussion_r208280290", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10305", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208280290"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10305#discussion_r208280290"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10305"}}, "body_html": "<p><code>LSTM_forw_ind_wrap</code> I guess.</p>", "body_text": "LSTM_forw_ind_wrap I guess.", "in_reply_to_id": 208279112}