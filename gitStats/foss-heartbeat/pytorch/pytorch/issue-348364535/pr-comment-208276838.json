{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208276838", "pull_request_review_id": 144052553, "id": 208276838, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwODI3NjgzOA==", "diff_hunk": "@@ -0,0 +1,601 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n+#include \"ATen/TensorUtils.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+#include <THC/THCNumerics.cuh>\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+using at::cuda::detail::TensorInfo;\n+using at::cuda::detail::getTensorInfo;\n+using at::cuda::detail::IndexToOffset;\n+using at::cuda::detail::canUse32BitIndexMath;\n+\n+// Factor will be 3 for GRU and 4 for LSTM\n+void checkSizes(CheckedFrom c,\n+                const TensorArg& input_gates, const TensorArg& hidden_gates,\n+                const TensorArg& input_bias, const TensorArg& hidden_bias,\n+                int64_t factor, const TensorArg& prev_hidden) {\n+  checkDim(c, input_gates, 2);\n+  checkSameSize(c, input_gates, hidden_gates);\n+  int64_t gates_size = input_gates->size(1);\n+\n+  if (input_bias->defined()) {\n+    checkDim(c, input_bias, 1);\n+    checkNumel(c, input_bias, gates_size);\n+    checkSameSize(c, input_bias, hidden_bias);\n+  }\n+\n+  checkDim(c, prev_hidden, 2);\n+  checkNumel(c, prev_hidden, input_gates->size(0) * gates_size / factor);\n+\n+  checkAllSameGPU(c, {input_gates, hidden_gates, input_bias, hidden_bias, prev_hidden});\n+}\n+\n+bool allContiguous(at::TensorList tensors) {\n+  return std::all_of(tensors.begin(), tensors.end(),\n+                     [](const at::Tensor& t) { return !t.defined() || t.is_contiguous(); });\n+}\n+\n+void getLaunchConfig(dim3& block, dim3& grid, int64_t numel) {\n+  int curDevice = -1;\n+  cudaGetDevice(&curDevice);\n+  block = cuda::getApplyBlock();\n+  AT_ASSERTM(cuda::getApplyGrid(numel, grid, curDevice),\n+             \"Could not get grid size for pointwise apply.\");\n+}\n+\n+template<typename T, typename T2>\n+TensorInfo<T, T2> tryGetTensorInfo(const at::Tensor& t) {\n+  return t.defined() ? getTensorInfo<T, T2>(t) : TensorInfo<T, T2>{};\n+}\n+\n+void collapseDims() {};\n+template<typename T, typename T2, typename... Args>\n+void collapseDims(TensorInfo<T, T2>& info, Args&... infos) {\n+  info.collapseDims();\n+  collapseDims(infos...);\n+}\n+\n+#define DEVICE_LINEAR_GET(D_TENSOR, INDEX)                              \\", "path": "aten/src/ATen/native/cuda/RNN.cu", "position": 64, "original_position": 64, "commit_id": "1a78db4286a2bc422312f29247b0d50584855c29", "original_commit_id": "9f423f29740c6c30d7251b1565ba862bdeea5473", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I would like to have them as functions, but they would still need those template arguments (they can't really be inferred from the argument types), and so I left them as macros.", "created_at": "2018-08-07T15:26:15Z", "updated_at": "2018-11-23T15:48:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/10305#discussion_r208276838", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10305", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208276838"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10305#discussion_r208276838"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10305"}}, "body_html": "<p>I would like to have them as functions, but they would still need those template arguments (they can't really be inferred from the argument types), and so I left them as macros.</p>", "body_text": "I would like to have them as functions, but they would still need those template arguments (they can't really be inferred from the argument types), and so I left them as macros.", "in_reply_to_id": 208276460}