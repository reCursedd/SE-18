{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208276739", "pull_request_review_id": 144052431, "id": 208276739, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwODI3NjczOQ==", "diff_hunk": "@@ -0,0 +1,601 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/AccumulateType.h\"\n+#include \"ATen/TensorUtils.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+#include <THC/THCNumerics.cuh>\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+using at::cuda::detail::TensorInfo;\n+using at::cuda::detail::getTensorInfo;\n+using at::cuda::detail::IndexToOffset;\n+using at::cuda::detail::canUse32BitIndexMath;\n+\n+// Factor will be 3 for GRU and 4 for LSTM\n+void checkSizes(CheckedFrom c,\n+                const TensorArg& input_gates, const TensorArg& hidden_gates,\n+                const TensorArg& input_bias, const TensorArg& hidden_bias,\n+                int64_t factor, const TensorArg& prev_hidden) {\n+  checkDim(c, input_gates, 2);\n+  checkSameSize(c, input_gates, hidden_gates);\n+  int64_t gates_size = input_gates->size(1);\n+\n+  if (input_bias->defined()) {\n+    checkDim(c, input_bias, 1);\n+    checkNumel(c, input_bias, gates_size);\n+    checkSameSize(c, input_bias, hidden_bias);\n+  }\n+\n+  checkDim(c, prev_hidden, 2);\n+  checkNumel(c, prev_hidden, input_gates->size(0) * gates_size / factor);\n+\n+  checkAllSameGPU(c, {input_gates, hidden_gates, input_bias, hidden_bias, prev_hidden});\n+}\n+\n+bool allContiguous(at::TensorList tensors) {\n+  return std::all_of(tensors.begin(), tensors.end(),\n+                     [](const at::Tensor& t) { return !t.defined() || t.is_contiguous(); });\n+}\n+\n+void getLaunchConfig(dim3& block, dim3& grid, int64_t numel) {\n+  int curDevice = -1;\n+  cudaGetDevice(&curDevice);\n+  block = cuda::getApplyBlock();\n+  AT_ASSERTM(cuda::getApplyGrid(numel, grid, curDevice),\n+             \"Could not get grid size for pointwise apply.\");\n+}\n+\n+template<typename T, typename T2>\n+TensorInfo<T, T2> tryGetTensorInfo(const at::Tensor& t) {\n+  return t.defined() ? getTensorInfo<T, T2>(t) : TensorInfo<T, T2>{};\n+}\n+\n+void collapseDims() {};\n+template<typename T, typename T2, typename... Args>\n+void collapseDims(TensorInfo<T, T2>& info, Args&... infos) {\n+  info.collapseDims();\n+  collapseDims(infos...);\n+}\n+\n+#define DEVICE_LINEAR_GET(D_TENSOR, INDEX)                              \\\n+  D_TENSOR.data[IndexToOffset<scalar_t, index_type, indexing_kind>::get(INDEX, D_TENSOR)]\n+\n+// Biases are always 1D\n+#define DEVICE_BIAS_GET(D_TENSOR, INDEX)                              \\\n+  D_TENSOR.data[IndexToOffset<scalar_t, index_type, 1>::get(INDEX, D_TENSOR)]\n+\n+#define H2F(input) ScalarConvert<scalar_t, accscalar_t>::to(input)\n+#define F2H(input) ScalarConvert<accscalar_t, scalar_t>::to(input)\n+\n+template<typename T>\n+__device__ __forceinline__\n+T sigmoid(T in)  {\n+  T one = static_cast<T>(1.0);\n+  return one / (one + THCNumerics<T>::exp(-in));\n+}\n+\n+namespace kernel {\n+\n+template <typename scalar_t, typename accscalar_t, typename index_type, int indexing_kind>\n+#if __CUDA_ARCH__ >= 350\n+__launch_bounds__(32 * 16, 4)\n+#endif\n+__global__ void lstm_cell_forward(\n+            TensorInfo<scalar_t, index_type> input,\n+            TensorInfo<scalar_t, index_type> hidden,\n+            TensorInfo<scalar_t, index_type> bias1,\n+            TensorInfo<scalar_t, index_type> bias2,\n+            TensorInfo<scalar_t, index_type> _cx,\n+            TensorInfo<scalar_t, index_type> _hy,\n+            TensorInfo<scalar_t, index_type> _cy,\n+            TensorInfo<scalar_t, index_type> workspace,\n+            index_type hsz,\n+            index_type totalElements) {\n+    bool has_bias = (bias1.data != NULL);", "path": "aten/src/ATen/native/cuda/RNN.cu", "position": null, "original_position": 98, "commit_id": "1a78db4286a2bc422312f29247b0d50584855c29", "original_commit_id": "9f423f29740c6c30d7251b1565ba862bdeea5473", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "nit: nullptr", "created_at": "2018-08-07T15:26:00Z", "updated_at": "2018-11-23T15:48:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/10305#discussion_r208276739", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10305", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208276739"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10305#discussion_r208276739"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10305"}}, "body_html": "<p>nit: nullptr</p>", "body_text": "nit: nullptr"}