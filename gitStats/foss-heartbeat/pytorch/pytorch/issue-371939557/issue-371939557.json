{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12871", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12871/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12871/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12871/events", "html_url": "https://github.com/pytorch/pytorch/issues/12871", "id": 371939557, "node_id": "MDU6SXNzdWUzNzE5Mzk1NTc=", "number": 12871, "title": "torch.nn.functional works incorrectly with backward() for non-scalars", "user": {"login": "olyaromanyuk", "id": 15520877, "node_id": "MDQ6VXNlcjE1NTIwODc3", "avatar_url": "https://avatars0.githubusercontent.com/u/15520877?v=4", "gravatar_id": "", "url": "https://api.github.com/users/olyaromanyuk", "html_url": "https://github.com/olyaromanyuk", "followers_url": "https://api.github.com/users/olyaromanyuk/followers", "following_url": "https://api.github.com/users/olyaromanyuk/following{/other_user}", "gists_url": "https://api.github.com/users/olyaromanyuk/gists{/gist_id}", "starred_url": "https://api.github.com/users/olyaromanyuk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/olyaromanyuk/subscriptions", "organizations_url": "https://api.github.com/users/olyaromanyuk/orgs", "repos_url": "https://api.github.com/users/olyaromanyuk/repos", "events_url": "https://api.github.com/users/olyaromanyuk/events{/privacy}", "received_events_url": "https://api.github.com/users/olyaromanyuk/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-19T12:37:43Z", "updated_at": "2018-11-06T00:49:56Z", "closed_at": "2018-11-05T18:37:09Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>Trying to calculate gradients with respect to input for a simple network that processes batches. Backpropagation crashes with RuntimeError: output and gradOutput have different number of elements: output[3 x 1] has 3 elements, while gradOutput[1 x 1] has 1 elements at /pytorch/aten/src/THCUNN/generic/Sigmoid.cu:22.<br>\nSame backpropagation without sigmoid as a last step in the network works ok.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:<br>\nHere is a minimal example:</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self,x):\n        y = torch.sum(x, dim=1)\n        y = torch.sum(y, dim=1, keepdim=True)\n        print(y)\n        y=F.sigmoid(y)\n        return y\n\n\nmodel = Model()\nmodel.cuda()\n\nx = Variable(torch.zeros(3, 5, 10), requires_grad=True)\nx_cuda = x.cuda()\nprint(x)\n\ny = model(x_cuda)\n\nprint(y.shape)\nprint(y)\ny.backward(torch.cuda.FloatTensor([[1]]))\n\nprint(x.grad.shape)\nprint(x.grad)\n</code></pre>\n<ol>\n<li>Run the code above. Execution ends with the following error:</li>\n</ol>\n<pre><code>Traceback (most recent call last):\n  File \"/home/olha/experiments/non_scalar_gradients.py\", line 33, in &lt;module&gt;\n    y.backward(torch.cuda.FloatTensor([[1]]))\n  File \"/home/olha/.local/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/olha/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 89, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: output and gradOutput have different number of elements: output[3 x 1] has 3 elements, while gradOutput[1 x 1] has 1 elements at /pytorch/aten/src/THCUNN/generic/Sigmoid.cu:22\n</code></pre>\n<ol start=\"2\">\n<li>Run the same code, but comment out the line y=F.sigmoid(y). Code runs ok, gradients are correct.</li>\n</ol>\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version 0.4.0</li>\n<li>Ubuntu 18.04</li>\n<li>How you installed PyTorch - pip</li>\n<li>Python version 3.6</li>\n<li>CUDA 9.2/cuDNN 7.3</li>\n</ul>\n<h2>Additional context</h2>\n<p>In the code I use sigmoid, but the issue is not limited to it. I've tried tanh with the same results.</p>", "body_text": "\ud83d\udc1b Bug\nTrying to calculate gradients with respect to input for a simple network that processes batches. Backpropagation crashes with RuntimeError: output and gradOutput have different number of elements: output[3 x 1] has 3 elements, while gradOutput[1 x 1] has 1 elements at /pytorch/aten/src/THCUNN/generic/Sigmoid.cu:22.\nSame backpropagation without sigmoid as a last step in the network works ok.\nTo Reproduce\nSteps to reproduce the behavior:\nHere is a minimal example:\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self,x):\n        y = torch.sum(x, dim=1)\n        y = torch.sum(y, dim=1, keepdim=True)\n        print(y)\n        y=F.sigmoid(y)\n        return y\n\n\nmodel = Model()\nmodel.cuda()\n\nx = Variable(torch.zeros(3, 5, 10), requires_grad=True)\nx_cuda = x.cuda()\nprint(x)\n\ny = model(x_cuda)\n\nprint(y.shape)\nprint(y)\ny.backward(torch.cuda.FloatTensor([[1]]))\n\nprint(x.grad.shape)\nprint(x.grad)\n\n\nRun the code above. Execution ends with the following error:\n\nTraceback (most recent call last):\n  File \"/home/olha/experiments/non_scalar_gradients.py\", line 33, in <module>\n    y.backward(torch.cuda.FloatTensor([[1]]))\n  File \"/home/olha/.local/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/olha/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 89, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: output and gradOutput have different number of elements: output[3 x 1] has 3 elements, while gradOutput[1 x 1] has 1 elements at /pytorch/aten/src/THCUNN/generic/Sigmoid.cu:22\n\n\nRun the same code, but comment out the line y=F.sigmoid(y). Code runs ok, gradients are correct.\n\nEnvironment\n\nPyTorch Version 0.4.0\nUbuntu 18.04\nHow you installed PyTorch - pip\nPython version 3.6\nCUDA 9.2/cuDNN 7.3\n\nAdditional context\nIn the code I use sigmoid, but the issue is not limited to it. I've tried tanh with the same results.", "body": "## \ud83d\udc1b Bug\r\n\r\nTrying to calculate gradients with respect to input for a simple network that processes batches. Backpropagation crashes with RuntimeError: output and gradOutput have different number of elements: output[3 x 1] has 3 elements, while gradOutput[1 x 1] has 1 elements at /pytorch/aten/src/THCUNN/generic/Sigmoid.cu:22.\r\nSame backpropagation without sigmoid as a last step in the network works ok.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\nHere is a minimal example:\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n\r\n    def forward(self,x):\r\n        y = torch.sum(x, dim=1)\r\n        y = torch.sum(y, dim=1, keepdim=True)\r\n        print(y)\r\n        y=F.sigmoid(y)\r\n        return y\r\n\r\n\r\nmodel = Model()\r\nmodel.cuda()\r\n\r\nx = Variable(torch.zeros(3, 5, 10), requires_grad=True)\r\nx_cuda = x.cuda()\r\nprint(x)\r\n\r\ny = model(x_cuda)\r\n\r\nprint(y.shape)\r\nprint(y)\r\ny.backward(torch.cuda.FloatTensor([[1]]))\r\n\r\nprint(x.grad.shape)\r\nprint(x.grad)\r\n```\r\n\r\n1. Run the code above. Execution ends with the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/olha/experiments/non_scalar_gradients.py\", line 33, in <module>\r\n    y.backward(torch.cuda.FloatTensor([[1]]))\r\n  File \"/home/olha/.local/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/olha/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 89, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: output and gradOutput have different number of elements: output[3 x 1] has 3 elements, while gradOutput[1 x 1] has 1 elements at /pytorch/aten/src/THCUNN/generic/Sigmoid.cu:22\r\n```\r\n2.  Run the same code, but comment out the line y=F.sigmoid(y). Code runs ok, gradients are correct.\r\n\r\n## Environment\r\n - PyTorch Version 0.4.0\r\n - Ubuntu 18.04\r\n - How you installed PyTorch - pip\r\n - Python version 3.6\r\n - CUDA 9.2/cuDNN 7.3 \r\n\r\n## Additional context\r\n\r\nIn the code I use sigmoid, but the issue is not limited to it. I've tried tanh with the same results."}