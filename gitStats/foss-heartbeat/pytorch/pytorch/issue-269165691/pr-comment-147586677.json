{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/147586677", "pull_request_review_id": 72702614, "id": 147586677, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NzU4NjY3Nw==", "diff_hunk": "@@ -220,19 +220,123 @@ void encodeModel(onnx::ModelProto* p_m, const std::shared_ptr<Graph>& g,\n   encodeGraph(p_g, g, initializers);\n }\n \n+// Broadcasting operators have the following property:\n+// They support a 'broadcast' flag, which enables broadcasting\n+// on the last argument.  ATM this is not full-Numpy broadcasting,\n+// only left-size extension (no size 1 to size n broadcast)\n+std::unordered_set<NodeKind> broadcasting = {\n+  kAdd,\n+  kDiv,\n+  kMul,\n+  kPow,\n+  kSub,\n+  kGemm,\n+};\n+\n+bool isBroadcasting(Node *node) {\n+  return broadcasting.count(node->kind());\n+}\n+\n+// When iterating over the dimension sizes, starting at the trailing dimension,\n+// the dimension sizes must either be equal, or one of them does not exist.\n+//\n+//  equivalently:\n+//\n+// Test that 'from' is a suffix of 'to'.\n+bool fusibleExpandTo(at::IntList from, at::IntList to) {\n+  auto f = from.rbegin();\n+  auto t = to.rbegin();\n+  for (; f != from.rend() && t != to.rend(); f++, t++) {\n+    // TODO: if 1->n expansion is supported, adjust this conditional.\n+    if (*f != *t) return false;\n+  }\n+  return f == from.rend();\n+}\n+\n+// This optimization fuses expand calls into ONNX operators, because it is\n+// easier for non-strided backends to more efficiently do broadcasts if this is\n+// local information.  This optimization is not useful for PyTorch as 'expand'\n+// is free.\n+void fuseBroadcast(const std::shared_ptr<Graph>& graph) {\n+  for (auto it = graph->nodes().begin(); it != graph->nodes().end(); ++it) {\n+    auto* n = *it;\n+\n+    // Can't fuse into nodes that don't support broadcasting\n+    if (!isBroadcasting(n)) continue;\n+\n+    // If the node already broadcasts, can't \"rebroadcast\"\n+    // TODO: Actually, maybe you can, if there is a broadcast for some\n+    // dims, and then another broadcast for the rest.  But this will\n+    // never happen in practice so I didn't implement it.\n+    if (n->hasAttribute(kbroadcast) && n->i(kbroadcast)) continue;\n+    JIT_ASSERT(!n->hasAttribute(kaxis));\n+\n+    // TODO: switch ATen tracing to not insert selects for single output.\n+    auto* rhs = n->inputs().at(n->inputs().size() - 1);\n+\n+    // The rhs input isn't actually an expand, so no fusion available\n+    if (rhs->kind() != kExpand) continue;\n+\n+    auto* new_rhs = rhs->input();", "path": "torch/csrc/jit/export.cpp", "position": 61, "original_position": 61, "commit_id": "dcb4a12576ffc1c85f540057c6b755d5d486f49b", "original_commit_id": "dcb4a12576ffc1c85f540057c6b755d5d486f49b", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Sure, I'll do \"unexpanded_rhs\" and \"expanded_rhs\"", "created_at": "2017-10-29T16:29:58Z", "updated_at": "2018-11-23T15:35:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/3325#discussion_r147586677", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3325", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/147586677"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3325#discussion_r147586677"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3325"}}, "body_html": "<p>Sure, I'll do \"unexpanded_rhs\" and \"expanded_rhs\"</p>", "body_text": "Sure, I'll do \"unexpanded_rhs\" and \"expanded_rhs\"", "in_reply_to_id": 147545134}