{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/420828187", "html_url": "https://github.com/pytorch/pytorch/pull/11533#issuecomment-420828187", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11533", "id": 420828187, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDgyODE4Nw==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-12T23:10:43Z", "updated_at": "2018-09-12T23:10:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I wrote an attempt at doing this at the CUDA initialization level, but this ran into a reentrancy problem. It looks something like this:</p>\n<ol>\n<li>User triggers access of CUDA type somehow</li>\n<li>This triggers initCUDA in Context</li>\n<li>This triggers Python lazy initialization of CUDA</li>\n<li>As part of lazy initialization, the Python lazy initialization triggers an initialization of CUDA type</li>\n<li>SAD PANDA</li>\n</ol>\n<p>There were a few things I could have tried, but it seems very delicate. I think the old patch will be better. Maybe we can try again later.</p>", "body_text": "I wrote an attempt at doing this at the CUDA initialization level, but this ran into a reentrancy problem. It looks something like this:\n\nUser triggers access of CUDA type somehow\nThis triggers initCUDA in Context\nThis triggers Python lazy initialization of CUDA\nAs part of lazy initialization, the Python lazy initialization triggers an initialization of CUDA type\nSAD PANDA\n\nThere were a few things I could have tried, but it seems very delicate. I think the old patch will be better. Maybe we can try again later.", "body": "I wrote an attempt at doing this at the CUDA initialization level, but this ran into a reentrancy problem. It looks something like this:\r\n\r\n1. User triggers access of CUDA type somehow\r\n2. This triggers initCUDA in Context\r\n3. This triggers Python lazy initialization of CUDA\r\n4. As part of lazy initialization, the Python lazy initialization triggers an initialization of CUDA type\r\n5. SAD PANDA\r\n\r\nThere were a few things I could have tried, but it seems very delicate. I think the old patch will be better. Maybe we can try again later."}