{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217039707", "pull_request_review_id": 154658720, "id": 217039707, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzAzOTcwNw==", "diff_hunk": "@@ -427,9 +427,9 @@ def append_actuals_formals(actual, formal):\n         env['actuals'] = actuals\n \n         if has_tensor_options:\n-            env['initialize_cuda'] = 'maybe_initialize_cuda(at::getType(options));'\n+            env['initialize_cuda'] = 'maybe_initialize_cuda(options);'\n         else:\n-            env['initialize_cuda'] = 'maybe_initialize_cuda({});'.format(type_args[0]['name']) if type_args else ''\n+            env['initialize_cuda'] = ''", "path": "tools/autograd/gen_python_functions.py", "position": 8, "original_position": 8, "commit_id": "ffbe8124459acea52f0ff3568a8c40d45d9b560d", "original_commit_id": "eb2d6fc606b472895873a8c6f76afce8858c6dbd", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "> VariableType::getVariabletTypeFromBaseType\r\n\r\nThis doesn't work because VariableType lives in the Python agnostic part of Torch, whereas the initializer has to call into Python code.\r\n\r\nI can introduce yet another function for getting types... but there will still be multiple places I would have to guard; for example, if someone calls `x.cuda()` I have no logical interposition point (this will go straight to the `cuda()` variable method) or they call `type.toBackend(kCUDA)` (this will go straight to the Type toBackend method).\r\n\r\nI *suppose* what we could do instead is add a second CUDA initialization hook, so that when we require CUDA initialization, we call the first hook (which no-ops if it's not set), and then we call the second hook. That would definitively prevent any errors of this sort.", "created_at": "2018-09-12T13:53:08Z", "updated_at": "2018-11-23T15:51:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/11533#discussion_r217039707", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11533", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217039707"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11533#discussion_r217039707"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11533"}}, "body_html": "<blockquote>\n<p>VariableType::getVariabletTypeFromBaseType</p>\n</blockquote>\n<p>This doesn't work because VariableType lives in the Python agnostic part of Torch, whereas the initializer has to call into Python code.</p>\n<p>I can introduce yet another function for getting types... but there will still be multiple places I would have to guard; for example, if someone calls <code>x.cuda()</code> I have no logical interposition point (this will go straight to the <code>cuda()</code> variable method) or they call <code>type.toBackend(kCUDA)</code> (this will go straight to the Type toBackend method).</p>\n<p>I <em>suppose</em> what we could do instead is add a second CUDA initialization hook, so that when we require CUDA initialization, we call the first hook (which no-ops if it's not set), and then we call the second hook. That would definitively prevent any errors of this sort.</p>", "body_text": "VariableType::getVariabletTypeFromBaseType\n\nThis doesn't work because VariableType lives in the Python agnostic part of Torch, whereas the initializer has to call into Python code.\nI can introduce yet another function for getting types... but there will still be multiple places I would have to guard; for example, if someone calls x.cuda() I have no logical interposition point (this will go straight to the cuda() variable method) or they call type.toBackend(kCUDA) (this will go straight to the Type toBackend method).\nI suppose what we could do instead is add a second CUDA initialization hook, so that when we require CUDA initialization, we call the first hook (which no-ops if it's not set), and then we call the second hook. That would definitively prevent any errors of this sort.", "in_reply_to_id": 216831602}