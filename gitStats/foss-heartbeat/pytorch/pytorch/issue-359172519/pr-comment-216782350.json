{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216782350", "pull_request_review_id": 154346900, "id": 216782350, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjc4MjM1MA==", "diff_hunk": "@@ -1,11 +1,23 @@\n #pragma once\n \n-// It initially lies in torch/csrc/cuda, but to unconditionlly compile it\n-// we have to put it here.\n+// cuda_lazy_init() is always compiled, even for CPU-only builds.\n+// Thus, it does not live in the cuda/ folder.\n \n namespace torch {\n namespace utils {\n \n+// The INVARIANT is that this function MUST be called before you attempt", "path": "torch/csrc/utils/cuda_lazy_init.h", "position": 11, "original_position": 11, "commit_id": "ffbe8124459acea52f0ff3568a8c40d45d9b560d", "original_commit_id": "64fc00fcd21073c7c8119a08da782d57c08510a1", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "This doesn't square with https://github.com/pytorch/pytorch/blob/64fc00fcd21073c7c8119a08da782d57c08510a1/tools/autograd/templates/python_torch_functions_dispatch.h#L31-L35 -- either the message is wrong or that function is bogus.", "created_at": "2018-09-11T18:54:40Z", "updated_at": "2018-11-23T15:51:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/11533#discussion_r216782350", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11533", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216782350"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11533#discussion_r216782350"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11533"}}, "body_html": "<p>This doesn't square with <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/64fc00fcd21073c7c8119a08da782d57c08510a1/tools/autograd/templates/python_torch_functions_dispatch.h#L31-L35\">pytorch/tools/autograd/templates/python_torch_functions_dispatch.h</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 31 to 35\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/64fc00fcd21073c7c8119a08da782d57c08510a1\">64fc00f</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L31\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"31\"></td>\n          <td id=\"LC31\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">static</span> <span class=\"pl-k\">void</span> <span class=\"pl-en\">maybe_initialize_cuda</span>(<span class=\"pl-k\">const</span> at::Type &amp;type) { </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L32\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"32\"></td>\n          <td id=\"LC32\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">if</span> (type.<span class=\"pl-c1\">is_cuda</span>()) { </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L33\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"33\"></td>\n          <td id=\"LC33\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c1\">torch::utils::cuda_lazy_init</span>(); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L34\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"34\"></td>\n          <td id=\"LC34\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   } </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L35\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"35\"></td>\n          <td id=\"LC35\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> } </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n -- either the message is wrong or that function is bogus.</p>", "body_text": "This doesn't square with \n  \n    \n      pytorch/tools/autograd/templates/python_torch_functions_dispatch.h\n    \n    \n        Lines 31 to 35\n      in\n      64fc00f\n    \n    \n    \n    \n\n        \n          \n           static void maybe_initialize_cuda(const at::Type &type) { \n        \n\n        \n          \n             if (type.is_cuda()) { \n        \n\n        \n          \n               torch::utils::cuda_lazy_init(); \n        \n\n        \n          \n             } \n        \n\n        \n          \n           } \n        \n    \n  \n\n -- either the message is wrong or that function is bogus."}