{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1826", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1826/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1826/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1826/events", "html_url": "https://github.com/pytorch/pytorch/pull/1826", "id": 236584071, "node_id": "MDExOlB1bGxSZXF1ZXN0MTI2MTA3ODYx", "number": 1826, "title": "Fix autograd tracking of pointwise fallback and broadcasted shapes", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-16T20:20:59Z", "updated_at": "2018-11-23T15:33:55Z", "closed_at": "2017-06-17T13:38:29Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1826", "html_url": "https://github.com/pytorch/pytorch/pull/1826", "diff_url": "https://github.com/pytorch/pytorch/pull/1826.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1826.patch"}, "body_html": "<ol>\n<li>\n<p>Fixes pointwise fallback autograd shape tracking<br>\nThis refers to cases where the number of elements are the same, but the shapes don't match and broadcasting didn't happen (i.e. this predates broadcasting).  Autograd did this correctly for add, sub, mul, div, but didn't do it for the other ops with this behavior, e.g: addcmul, addcdiv, lerp, le, lt, ge, gt, ne, eq, min, max, masked_scatter.  Also added some shape check code to test_autograd.</p>\n</li>\n<li>\n<p>Fix broadcast autograd shape tracking<br>\nThis is equivalent to the case above, but for cases where broadcasting occurred.  This includes the functions in 1), but also functions that don't have pointwise fallback, but broadcast, e.g. addmm, addmv, addbmm, baddbmm, addr.</p>\n</li>\n<li>\n<p>Fixes other autograd shape tracking bugs<br>\nexpand and masked_scatter had bugs, these should be fixed.</p>\n</li>\n<li>\n<p>Disallows div_ with tensor or variable parameter<br>\nThis is now the same as mul_; before, it would call DivConstant which was incorrect.</p>\n</li>\n</ol>", "body_text": "Fixes pointwise fallback autograd shape tracking\nThis refers to cases where the number of elements are the same, but the shapes don't match and broadcasting didn't happen (i.e. this predates broadcasting).  Autograd did this correctly for add, sub, mul, div, but didn't do it for the other ops with this behavior, e.g: addcmul, addcdiv, lerp, le, lt, ge, gt, ne, eq, min, max, masked_scatter.  Also added some shape check code to test_autograd.\n\n\nFix broadcast autograd shape tracking\nThis is equivalent to the case above, but for cases where broadcasting occurred.  This includes the functions in 1), but also functions that don't have pointwise fallback, but broadcast, e.g. addmm, addmv, addbmm, baddbmm, addr.\n\n\nFixes other autograd shape tracking bugs\nexpand and masked_scatter had bugs, these should be fixed.\n\n\nDisallows div_ with tensor or variable parameter\nThis is now the same as mul_; before, it would call DivConstant which was incorrect.", "body": "1) Fixes pointwise fallback autograd shape tracking\r\nThis refers to cases where the number of elements are the same, but the shapes don't match and broadcasting didn't happen (i.e. this predates broadcasting).  Autograd did this correctly for add, sub, mul, div, but didn't do it for the other ops with this behavior, e.g: addcmul, addcdiv, lerp, le, lt, ge, gt, ne, eq, min, max, masked_scatter.  Also added some shape check code to test_autograd.\r\n\r\n2) Fix broadcast autograd shape tracking\r\nThis is equivalent to the case above, but for cases where broadcasting occurred.  This includes the functions in 1), but also functions that don't have pointwise fallback, but broadcast, e.g. addmm, addmv, addbmm, baddbmm, addr.\r\n\r\n3) Fixes other autograd shape tracking bugs\r\nexpand and masked_scatter had bugs, these should be fixed.\r\n\r\n4) Disallows div_ with tensor or variable parameter\r\nThis is now the same as mul_; before, it would call DivConstant which was incorrect."}