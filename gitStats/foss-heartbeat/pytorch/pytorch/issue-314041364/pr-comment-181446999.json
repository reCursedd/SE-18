{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/181446999", "pull_request_review_id": 112082656, "id": 181446999, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MTQ0Njk5OQ==", "diff_hunk": "@@ -400,26 +400,31 @@ struct AutogradHandle : public ContainerTensor {\n // behave differently depending on whether a future handle needs to be\n // created.\n struct HandleBuilder {\n-  HandleBuilder(bool requires_handle) {\n+  HandleBuilder(bool requires_handle, bool values_are_variables)\n+    : values_are_variables(values_are_variables) {\n     if(requires_handle) {\n       handle = new AutogradHandle();\n       handle->forward_inputs = std::make_shared<DummyFunction>();\n     }\n   }\n   autograd::Variable addInput(at::Tensor && input, const VariableFlags & flags_) {\n-    if(handle && flags_.requires_grad) {\n-      auto variable = autograd::make_variable(std::move(input), /*requires_grad=*/false);\n+    if (handle && flags_.requires_grad) {\n+      auto variable = values_are_variables ?\n+        static_cast<autograd::Variable&&>(input).detach() :", "path": "torch/csrc/jit/interpreter.cpp", "position": 17, "original_position": 17, "commit_id": "807badfeec64c72eb4cfce7b467053d2d29eae11", "original_commit_id": "807badfeec64c72eb4cfce7b467053d2d29eae11", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "This won't work correctly with the GraphExecutor pipeline because doesn't support handles correctly and detach will break the autograd gradients (in fact, we really should start removing the Handle functionality entirely).  When we are tracing using `torch.jit.trace`, which will turn into a GraphExecutor, we should be creating python functions with no handle outputs, and with  `tracing_autograd_python_function=false`.\r\n", "created_at": "2018-04-13T16:47:35Z", "updated_at": "2018-11-23T15:42:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/6583#discussion_r181446999", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6583", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/181446999"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6583#discussion_r181446999"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6583"}}, "body_html": "<p>This won't work correctly with the GraphExecutor pipeline because doesn't support handles correctly and detach will break the autograd gradients (in fact, we really should start removing the Handle functionality entirely).  When we are tracing using <code>torch.jit.trace</code>, which will turn into a GraphExecutor, we should be creating python functions with no handle outputs, and with  <code>tracing_autograd_python_function=false</code>.</p>", "body_text": "This won't work correctly with the GraphExecutor pipeline because doesn't support handles correctly and detach will break the autograd gradients (in fact, we really should start removing the Handle functionality entirely).  When we are tracing using torch.jit.trace, which will turn into a GraphExecutor, we should be creating python functions with no handle outputs, and with  tracing_autograd_python_function=false."}