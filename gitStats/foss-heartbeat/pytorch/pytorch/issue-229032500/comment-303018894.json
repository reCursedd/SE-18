{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/303018894", "html_url": "https://github.com/pytorch/pytorch/issues/1571#issuecomment-303018894", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1571", "id": 303018894, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzAxODg5NA==", "user": {"login": "peterGuang", "id": 3926656, "node_id": "MDQ6VXNlcjM5MjY2NTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/3926656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/peterGuang", "html_url": "https://github.com/peterGuang", "followers_url": "https://api.github.com/users/peterGuang/followers", "following_url": "https://api.github.com/users/peterGuang/following{/other_user}", "gists_url": "https://api.github.com/users/peterGuang/gists{/gist_id}", "starred_url": "https://api.github.com/users/peterGuang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/peterGuang/subscriptions", "organizations_url": "https://api.github.com/users/peterGuang/orgs", "repos_url": "https://api.github.com/users/peterGuang/repos", "events_url": "https://api.github.com/users/peterGuang/events{/privacy}", "received_events_url": "https://api.github.com/users/peterGuang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-22T07:19:13Z", "updated_at": "2017-05-22T07:19:13Z", "author_association": "NONE", "body_html": "<p>I have got the problem when using Bilinear layer. How to solve it?</p>\n<p>#!coding=utf8<br>\nimport torch<br>\nfrom torch.autograd import Variable<br>\nimport torch.nn as nn<br>\nimport torch.nn.functional as F<br>\ndim = 5<br>\nclass Net(nn.Module):<br>\ndef <strong>init</strong>(self):<br>\nsuper(Net, self).<strong>init</strong>()<br>\nself.T1 = nn.Parameter(torch.randn(dim,dim,dim),requires_grad=True)<br>\ndef forward(self,x1,x2):<br>\nreturn F.tanh(F.bilinear(x1,x2,self.T1))<br>\nimport torch.optim as optim<br>\nnet = Net()<br>\noptimizer = optim.SGD(net.parameters(), lr = 0.01)<br>\nfor p in net.parameters():<br>\nprint(p.size())<br>\nfor x in [1]:<br>\noptimizer.zero_grad()<br>\nx1 = Variable(torch.randn(1,5))<br>\nx2 = Variable(torch.randn(5,1))<br>\nloss = net(x1,x2)<br>\nprint(loss)<br>\nloss.backward(torch.randn([1,5]))<br>\noptimizer.step()</p>\n<p>The output:</p>\n<p>(5L, 5L, 5L)<br>\nVariable containing:<br>\n0.3141  0.9993  1.0000  0.9987 -0.9995<br>\n[torch.FloatTensor of size 1x5]</p>\n<p>Traceback (most recent call last):<br>\nFile \"/home/peter/workspace/stocktrend/model/test_torch.py\", line 25, in <br>\nloss.backward(torch.randn([1,5]))<br>\nFile \"/home/peter/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 145, in backward<br>\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)<br>\nFile \"/home/peter/anaconda2/lib/python2.7/site-packages/torch/autograd/<strong>init</strong>.py\", line 98, in backward<br>\nvariables, grad_variables, retain_graph)<br>\nFile \"/home/peter/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/linear.py\", line 80, in backward<br>\ngrad_weight = torch.mm(buff.t(), input2)<br>\nRuntimeError: size mismatch, m1: [5 x 1], m2: [5 x 1] at /home/peter/\u4e0b\u8f7d/pytorch-master/torch/lib/TH/generic/THTensorMath.c:1237</p>\n<p>Process finished with exit code 1</p>", "body_text": "I have got the problem when using Bilinear layer. How to solve it?\n#!coding=utf8\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\ndim = 5\nclass Net(nn.Module):\ndef init(self):\nsuper(Net, self).init()\nself.T1 = nn.Parameter(torch.randn(dim,dim,dim),requires_grad=True)\ndef forward(self,x1,x2):\nreturn F.tanh(F.bilinear(x1,x2,self.T1))\nimport torch.optim as optim\nnet = Net()\noptimizer = optim.SGD(net.parameters(), lr = 0.01)\nfor p in net.parameters():\nprint(p.size())\nfor x in [1]:\noptimizer.zero_grad()\nx1 = Variable(torch.randn(1,5))\nx2 = Variable(torch.randn(5,1))\nloss = net(x1,x2)\nprint(loss)\nloss.backward(torch.randn([1,5]))\noptimizer.step()\nThe output:\n(5L, 5L, 5L)\nVariable containing:\n0.3141  0.9993  1.0000  0.9987 -0.9995\n[torch.FloatTensor of size 1x5]\nTraceback (most recent call last):\nFile \"/home/peter/workspace/stocktrend/model/test_torch.py\", line 25, in \nloss.backward(torch.randn([1,5]))\nFile \"/home/peter/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 145, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\nFile \"/home/peter/anaconda2/lib/python2.7/site-packages/torch/autograd/init.py\", line 98, in backward\nvariables, grad_variables, retain_graph)\nFile \"/home/peter/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/linear.py\", line 80, in backward\ngrad_weight = torch.mm(buff.t(), input2)\nRuntimeError: size mismatch, m1: [5 x 1], m2: [5 x 1] at /home/peter/\u4e0b\u8f7d/pytorch-master/torch/lib/TH/generic/THTensorMath.c:1237\nProcess finished with exit code 1", "body": "I have got the problem when using Bilinear layer. How to solve it?\r\n\r\n#!coding=utf8\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\ndim = 5\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.T1 = nn.Parameter(torch.randn(dim,dim,dim),requires_grad=True)\r\n    def forward(self,x1,x2):\r\n        return F.tanh(F.bilinear(x1,x2,self.T1))\r\nimport torch.optim as optim\r\nnet = Net()\r\noptimizer = optim.SGD(net.parameters(), lr = 0.01)\r\nfor p in net.parameters():\r\n    print(p.size())\r\nfor x in [1]:\r\n    optimizer.zero_grad()\r\n    x1 = Variable(torch.randn(1,5))\r\n    x2 = Variable(torch.randn(5,1))\r\n    loss = net(x1,x2)\r\n    print(loss)\r\n    loss.backward(torch.randn([1,5]))\r\n    optimizer.step()\r\n\r\n\r\nThe output:\r\n\r\n(5L, 5L, 5L)\r\nVariable containing:\r\n 0.3141  0.9993  1.0000  0.9987 -0.9995\r\n[torch.FloatTensor of size 1x5]\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/peter/workspace/stocktrend/model/test_torch.py\", line 25, in <module>\r\n    loss.backward(torch.randn([1,5]))\r\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 145, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\r\n    variables, grad_variables, retain_graph)\r\n  File \"/home/peter/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/linear.py\", line 80, in backward\r\n    grad_weight = torch.mm(buff.t(), input2)\r\nRuntimeError: size mismatch, m1: [5 x 1], m2: [5 x 1] at /home/peter/\u4e0b\u8f7d/pytorch-master/torch/lib/TH/generic/THTensorMath.c:1237\r\n\r\nProcess finished with exit code 1\r\n\r\n"}