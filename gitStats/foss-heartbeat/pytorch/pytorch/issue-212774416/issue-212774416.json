{"url": "https://api.github.com/repos/pytorch/pytorch/issues/959", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/959/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/959/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/959/events", "html_url": "https://github.com/pytorch/pytorch/issues/959", "id": 212774416, "node_id": "MDU6SXNzdWUyMTI3NzQ0MTY=", "number": 959, "title": "Segmentation fault (segfault) when returning a None gradient for a part of a graph that requires gradient", "user": {"login": "MatthiasKohl", "id": 344856, "node_id": "MDQ6VXNlcjM0NDg1Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/344856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MatthiasKohl", "html_url": "https://github.com/MatthiasKohl", "followers_url": "https://api.github.com/users/MatthiasKohl/followers", "following_url": "https://api.github.com/users/MatthiasKohl/following{/other_user}", "gists_url": "https://api.github.com/users/MatthiasKohl/gists{/gist_id}", "starred_url": "https://api.github.com/users/MatthiasKohl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MatthiasKohl/subscriptions", "organizations_url": "https://api.github.com/users/MatthiasKohl/orgs", "repos_url": "https://api.github.com/users/MatthiasKohl/repos", "events_url": "https://api.github.com/users/MatthiasKohl/events{/privacy}", "received_events_url": "https://api.github.com/users/MatthiasKohl/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-08T15:55:51Z", "updated_at": "2017-03-10T17:36:33Z", "closed_at": "2017-03-10T17:36:33Z", "author_association": "NONE", "body_html": "<p>This is more of an enhancement, but the following code will produce a segfault:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Function, Variable\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Foo1</span>(<span class=\"pl-e\">Function</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Foo1, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">label</span>):\n        <span class=\"pl-c1\">self</span>.save_for_backward(i, label)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> return -i.sum()</span>\n        <span class=\"pl-k\">return</span> <span class=\"pl-k\">-</span>i.sum(<span class=\"pl-c1\">0</span>).sum(<span class=\"pl-c1\">1</span>).view(<span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        i, label <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.saved_tensors\n        <span class=\"pl-k\">return</span> <span class=\"pl-k\">-</span>grad_output.expand_as(i), <span class=\"pl-c1\">None</span>\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Foo1Loss</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Foo1Loss, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">label</span>):\n        <span class=\"pl-k\">return</span> Foo1()(i, label)\n\n\nf <span class=\"pl-k\">=</span> Foo1Loss()\nv_in <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nv_lab <span class=\"pl-k\">=</span> Variable(torch.Tensor([<span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nv_out <span class=\"pl-k\">=</span> f(v_in, v_lab)\nv_out.backward()</pre></div>\n<p>I followed the implementation of a custom Loss/Module closely as in the docs/source code. This code will produce a segfault on my machine (Linux Debian 3.16.36, torch version <code>0.1.10_2</code>). The segfault might be related to issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"202286539\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/531\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/531/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/531\">#531</a> .</p>\n<p>By the way, when uncommenting the <code>return -i.sum()</code> line in <code>forward</code> of <code>Foo1</code>, the error is <code>RuntimeError: data must be a Tensor</code>. This is of course because <code>forward</code> is not returning a <code>Tensor</code> but the error description is not so obvious as it happens only when the <code>Foo1Loss</code> module returns that value, so it's not that obvious what the error means. Maybe changing it to <code>returned data must be a Tensor</code> would be a bit clearer, but I guess I'm being pedantic :)</p>", "body_text": "This is more of an enhancement, but the following code will produce a segfault:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function, Variable\n\n\nclass Foo1(Function):\n\n    def __init__(self):\n        super(Foo1, self).__init__()\n\n    def forward(self, i, label):\n        self.save_for_backward(i, label)\n        # return -i.sum()\n        return -i.sum(0).sum(1).view(1)\n\n    def backward(self, grad_output):\n        i, label = self.saved_tensors\n        return -grad_output.expand_as(i), None\n\n\nclass Foo1Loss(nn.Module):\n\n    def __init__(self):\n        super(Foo1Loss, self).__init__()\n\n    def forward(self, i, label):\n        return Foo1()(i, label)\n\n\nf = Foo1Loss()\nv_in = Variable(torch.randn(1, 10), requires_grad=True)\nv_lab = Variable(torch.Tensor([1]), requires_grad=True)\nv_out = f(v_in, v_lab)\nv_out.backward()\nI followed the implementation of a custom Loss/Module closely as in the docs/source code. This code will produce a segfault on my machine (Linux Debian 3.16.36, torch version 0.1.10_2). The segfault might be related to issue #531 .\nBy the way, when uncommenting the return -i.sum() line in forward of Foo1, the error is RuntimeError: data must be a Tensor. This is of course because forward is not returning a Tensor but the error description is not so obvious as it happens only when the Foo1Loss module returns that value, so it's not that obvious what the error means. Maybe changing it to returned data must be a Tensor would be a bit clearer, but I guess I'm being pedantic :)", "body": "This is more of an enhancement, but the following code will produce a segfault:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Function, Variable\r\n\r\n\r\nclass Foo1(Function):\r\n\r\n    def __init__(self):\r\n        super(Foo1, self).__init__()\r\n\r\n    def forward(self, i, label):\r\n        self.save_for_backward(i, label)\r\n        # return -i.sum()\r\n        return -i.sum(0).sum(1).view(1)\r\n\r\n    def backward(self, grad_output):\r\n        i, label = self.saved_tensors\r\n        return -grad_output.expand_as(i), None\r\n\r\n\r\nclass Foo1Loss(nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Foo1Loss, self).__init__()\r\n\r\n    def forward(self, i, label):\r\n        return Foo1()(i, label)\r\n\r\n\r\nf = Foo1Loss()\r\nv_in = Variable(torch.randn(1, 10), requires_grad=True)\r\nv_lab = Variable(torch.Tensor([1]), requires_grad=True)\r\nv_out = f(v_in, v_lab)\r\nv_out.backward()\r\n```\r\n\r\nI followed the implementation of a custom Loss/Module closely as in the docs/source code. This code will produce a segfault on my machine (Linux Debian 3.16.36, torch version `0.1.10_2`). The segfault might be related to issue #531 .\r\n\r\nBy the way, when uncommenting the `return -i.sum()` line in `forward` of `Foo1`, the error is `RuntimeError: data must be a Tensor`. This is of course because `forward` is not returning a `Tensor` but the error description is not so obvious as it happens only when the `Foo1Loss` module returns that value, so it's not that obvious what the error means. Maybe changing it to `returned data must be a Tensor` would be a bit clearer, but I guess I'm being pedantic :)"}