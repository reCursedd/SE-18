{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6421", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6421/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6421/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6421/events", "html_url": "https://github.com/pytorch/pytorch/issues/6421", "id": 312508977, "node_id": "MDU6SXNzdWUzMTI1MDg5Nzc=", "number": 6421, "title": "[PyTorch]nn.LSTM.cuda() leads to CuDNNError: 8 when adding dropout parameter", "user": {"login": "iamlockelightning", "id": 12706469, "node_id": "MDQ6VXNlcjEyNzA2NDY5", "avatar_url": "https://avatars3.githubusercontent.com/u/12706469?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iamlockelightning", "html_url": "https://github.com/iamlockelightning", "followers_url": "https://api.github.com/users/iamlockelightning/followers", "following_url": "https://api.github.com/users/iamlockelightning/following{/other_user}", "gists_url": "https://api.github.com/users/iamlockelightning/gists{/gist_id}", "starred_url": "https://api.github.com/users/iamlockelightning/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iamlockelightning/subscriptions", "organizations_url": "https://api.github.com/users/iamlockelightning/orgs", "repos_url": "https://api.github.com/users/iamlockelightning/repos", "events_url": "https://api.github.com/users/iamlockelightning/events{/privacy}", "received_events_url": "https://api.github.com/users/iamlockelightning/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-04-09T12:22:52Z", "updated_at": "2018-09-17T03:54:23Z", "closed_at": "2018-05-22T12:22:31Z", "author_association": "NONE", "body_html": "<h2>Information</h2>\n<ul>\n<li>PyTorch or Caffe2: PyTorch</li>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: 0.3.0.post4</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Python version: 3.5</li>\n<li>CUDA/cuDNN version:</li>\n</ul>\n<blockquote>\n<p>nvcc --version<br>\nnvcc: NVIDIA (R) Cuda compiler driver<br>\nCopyright (c) 2005-2015 NVIDIA Corporation<br>\nBuilt on Tue_Aug_11_14:27:32_CDT_2015<br>\nCuda compilation tools, release 7.5, V7.5.17<br>\ncat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2<br>\n#define CUDNN_MAJOR      6<br>\n#define CUDNN_MINOR      0<br>\n#define CUDNN_PATCHLEVEL 21<br>\n#define CUDNN_VERSION    (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)<br>\n#include \"driver_types.h\"</p>\n</blockquote>\n<ul>\n<li>GPU models and configuration: NVIDIA-SMI 384.111 Driver Version: 384.111</li>\n<li>GCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)</li>\n</ul>\n<h2>Code:</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">RNN</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">hidden_size</span>, <span class=\"pl-smi\">num_layers</span>, <span class=\"pl-smi\">num_classes</span>):\n        <span class=\"pl-c1\">super</span>(<span class=\"pl-c1\">RNN</span>, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.hidden_size <span class=\"pl-k\">=</span> hidden_size\n        <span class=\"pl-c1\">self</span>.num_layers <span class=\"pl-k\">=</span> num_layers\n        <span class=\"pl-c1\">self</span>.lstm <span class=\"pl-k\">=</span> nn.LSTM(input_size, hidden_size, num_layers, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span>, dropout=0.2</span>\n        <span class=\"pl-c1\">self</span>.fc <span class=\"pl-k\">=</span> nn.Linear(hidden_size, num_classes)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        h0 <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">self</span>.num_layers, x.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">self</span>.hidden_size).cuda())\n        c0 <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">self</span>.num_layers, x.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">self</span>.hidden_size).cuda())\n        out, _ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lstm(x, (h0, c0))\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc(out[:, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, :])\n        <span class=\"pl-k\">return</span> out\n\nrnn <span class=\"pl-k\">=</span> RNN(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>)\nrnn.cuda()</pre></div>\n<p>A simple rnn model. And it works.<br>\nBut if I add <strong>\"dropout=0.2\"</strong> in nn.LSTM, when run rnn.cuda(), I get nothing but:</p>\n<div class=\"highlight highlight-source-shell\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test.py<span class=\"pl-pds\">\"</span></span>, line 37, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    <span class=\"pl-en\">rnn.cuda</span>()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line 216, <span class=\"pl-k\">in</span> cuda\n    <span class=\"pl-k\">return</span> self._apply(lambda t: t.cuda(device))\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py<span class=\"pl-pds\">\"</span></span>, line 146, <span class=\"pl-k\">in</span> _apply\n    module._apply(fn)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py<span class=\"pl-pds\">\"</span></span>, line 123, <span class=\"pl-k\">in</span> _apply\n    <span class=\"pl-en\">self.flatten_parameters</span>()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py<span class=\"pl-pds\">\"</span></span>, line 102, <span class=\"pl-k\">in</span> flatten_parameters\n    fn.rnn_desc = rnn.init_rnn_descriptor(fn, handle)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/rnn.py<span class=\"pl-pds\">\"</span></span>, line 42, <span class=\"pl-k\">in</span> init_rnn_descriptor\n    cudnn.DropoutDescriptor(handle, dropout_p, fn.dropout_seed)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py<span class=\"pl-pds\">\"</span></span>, line 207, <span class=\"pl-k\">in</span> __init__\n    self._set(dropout, seed)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py<span class=\"pl-pds\">\"</span></span>, line 232, <span class=\"pl-k\">in</span> _set\n    ctypes.c_ulonglong(seed),\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py<span class=\"pl-pds\">\"</span></span>, line 283, <span class=\"pl-k\">in</span> check_error\n    raise CuDNNError(status)\ntorch.backends.cudnn.CuDNNError: 8: b<span class=\"pl-s\"><span class=\"pl-pds\">'</span>CUDNN_STATUS_EXECUTION_FAILED<span class=\"pl-pds\">'</span></span></pre></div>\n<h2>Question</h2>\n<p>I wonder why? Or any other method I could use to add dropout after each layer of the lstm?<br>\nThanks!</p>", "body_text": "Information\n\nPyTorch or Caffe2: PyTorch\nOS: Ubuntu 16.04\nPyTorch version: 0.3.0.post4\nHow you installed PyTorch (conda, pip, source): pip\nPython version: 3.5\nCUDA/cuDNN version:\n\n\nnvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\nCuda compilation tools, release 7.5, V7.5.17\ncat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n#define CUDNN_MAJOR      6\n#define CUDNN_MINOR      0\n#define CUDNN_PATCHLEVEL 21\n#define CUDNN_VERSION    (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n#include \"driver_types.h\"\n\n\nGPU models and configuration: NVIDIA-SMI 384.111 Driver Version: 384.111\nGCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\n\nCode:\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) #, dropout=0.2\n        self.fc = nn.Linear(hidden_size, num_classes)\n    def forward(self, x):\n        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda())\n        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda())\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\nrnn = RNN(100, 50, 3, 2)\nrnn.cuda()\nA simple rnn model. And it works.\nBut if I add \"dropout=0.2\" in nn.LSTM, when run rnn.cuda(), I get nothing but:\nTraceback (most recent call last):\n  File \"test.py\", line 37, in <module>\n    rnn.cuda()\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 216, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 146, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\", line 123, in _apply\n    self.flatten_parameters()\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\", line 102, in flatten_parameters\n    fn.rnn_desc = rnn.init_rnn_descriptor(fn, handle)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/rnn.py\", line 42, in init_rnn_descriptor\n    cudnn.DropoutDescriptor(handle, dropout_p, fn.dropout_seed)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py\", line 207, in __init__\n    self._set(dropout, seed)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py\", line 232, in _set\n    ctypes.c_ulonglong(seed),\n  File \"/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py\", line 283, in check_error\n    raise CuDNNError(status)\ntorch.backends.cudnn.CuDNNError: 8: b'CUDNN_STATUS_EXECUTION_FAILED'\nQuestion\nI wonder why? Or any other method I could use to add dropout after each layer of the lstm?\nThanks!", "body": "## Information\r\n- PyTorch or Caffe2: PyTorch\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.3.0.post4\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Python version: 3.5\r\n- CUDA/cuDNN version:\r\n>nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2015 NVIDIA Corporation\r\nBuilt on Tue_Aug_11_14:27:32_CDT_2015\r\nCuda compilation tools, release 7.5, V7.5.17\r\n>cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\r\n#define CUDNN_MAJOR      6\r\n#define CUDNN_MINOR      0\r\n#define CUDNN_PATCHLEVEL 21\r\n#define CUDNN_VERSION    (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\r\n#include \"driver_types.h\"\r\n- GPU models and configuration: NVIDIA-SMI 384.111 Driver Version: 384.111\r\n- GCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.9)\r\n\r\n## Code:\r\n``` Python\r\nclass RNN(nn.Module):\r\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\r\n        super(RNN, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.num_layers = num_layers\r\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) #, dropout=0.2\r\n        self.fc = nn.Linear(hidden_size, num_classes)\r\n    def forward(self, x):\r\n        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda())\r\n        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size).cuda())\r\n        out, _ = self.lstm(x, (h0, c0))\r\n        out = self.fc(out[:, -1, :])\r\n        return out\r\n\r\nrnn = RNN(100, 50, 3, 2)\r\nrnn.cuda()\r\n```\r\n\r\nA simple rnn model. And it works.\r\nBut if I add **\"dropout=0.2\"** in nn.LSTM, when run rnn.cuda(), I get nothing but:\r\n``` Shell\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 37, in <module>\r\n    rnn.cuda()\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 216, in cuda\r\n    return self._apply(lambda t: t.cuda(device))\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 146, in _apply\r\n    module._apply(fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\", line 123, in _apply\r\n    self.flatten_parameters()\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\", line 102, in flatten_parameters\r\n    fn.rnn_desc = rnn.init_rnn_descriptor(fn, handle)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/rnn.py\", line 42, in init_rnn_descriptor\r\n    cudnn.DropoutDescriptor(handle, dropout_p, fn.dropout_seed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py\", line 207, in __init__\r\n    self._set(dropout, seed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py\", line 232, in _set\r\n    ctypes.c_ulonglong(seed),\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py\", line 283, in check_error\r\n    raise CuDNNError(status)\r\ntorch.backends.cudnn.CuDNNError: 8: b'CUDNN_STATUS_EXECUTION_FAILED'\r\n```\r\n## Question\r\nI wonder why? Or any other method I could use to add dropout after each layer of the lstm?\r\nThanks!"}