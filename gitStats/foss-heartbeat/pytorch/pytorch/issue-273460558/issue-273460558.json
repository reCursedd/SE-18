{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3667", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3667/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3667/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3667/events", "html_url": "https://github.com/pytorch/pytorch/issues/3667", "id": 273460558, "node_id": "MDU6SXNzdWUyNzM0NjA1NTg=", "number": 3667, "title": "Exposing CuDNN benchmark strategy selection ", "user": {"login": "silvandeleemput", "id": 3715102, "node_id": "MDQ6VXNlcjM3MTUxMDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3715102?v=4", "gravatar_id": "", "url": "https://api.github.com/users/silvandeleemput", "html_url": "https://github.com/silvandeleemput", "followers_url": "https://api.github.com/users/silvandeleemput/followers", "following_url": "https://api.github.com/users/silvandeleemput/following{/other_user}", "gists_url": "https://api.github.com/users/silvandeleemput/gists{/gist_id}", "starred_url": "https://api.github.com/users/silvandeleemput/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/silvandeleemput/subscriptions", "organizations_url": "https://api.github.com/users/silvandeleemput/orgs", "repos_url": "https://api.github.com/users/silvandeleemput/repos", "events_url": "https://api.github.com/users/silvandeleemput/events{/privacy}", "received_events_url": "https://api.github.com/users/silvandeleemput/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 806617721, "node_id": "MDU6TGFiZWw4MDY2MTc3MjE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cudnn", "name": "cudnn", "color": "fbca04", "default": false}, {"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-13T15:02:46Z", "updated_at": "2018-01-16T17:27:54Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>It would be very convenient to have an option to set the convolution algorithm preference to prefer memory over speed when enabling the cuDNN benchmark. For a lot of applications memory efficient convolutions are preferred to the fastest approach. So having a way to set this would be welcome.</p>\n<p><code>torch.backends.cudnn.benchmark = True</code></p>\n<p>The <a href=\"https://docs.rs/cudnn-sys/0.0.3/cudnn_sys/enum.cudnnConvolutionFwdPreference_t.html\" rel=\"nofollow\">cudnnConvolutionFwdPreference</a> during the benchmark knows several settings:</p>\n<ul>\n<li>CUDNN_CONVOLUTION_FWD_NO_WORKSPACE</li>\n<li>CUDNN_CONVOLUTION_FWD_PREFER_FASTEST</li>\n<li>CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT</li>\n</ul>\n<p>Yet, in the current <a href=\"https://github.com/pytorch/pytorch/blob/8fbe003d4ed946804d67a6d3bcd84eb6c3df9a4a/torch/csrc/cudnn/Conv.cpp\">implementation</a> this appears to be fixed to: CUDNN_CONVOLUTION_FWD_PREFER_FASTEST</p>\n<p>This is of course similar for:</p>\n<ul>\n<li>cudnnConvolutionBwdDataPreference_t</li>\n<li>cudnnConvolutionBwdFilterPreference_t</li>\n<li>cudnnConvolutionFwdPreference_t</li>\n</ul>\n<p>My initial proposal would be to include a property to the <code>torch.backends.cudnn</code> which can be set to pick a different preference, i.e.:<br>\n<code>torch.backends.cudnn.benchmark_memory_limit = None</code> (FASTEST / default value)<br>\n<code>torch.backends.cudnn.benchmark_memory_limit = 0</code> (NO_WORKSPACE)<br>\n<code>torch.backends.cudnn.benchmark_memory_limit = 12</code> (SPECIFY_WORKSPACE_LIMIT)</p>\n<p>The numbers could represent GPU memory in GB/MB or even bytes.</p>", "body_text": "It would be very convenient to have an option to set the convolution algorithm preference to prefer memory over speed when enabling the cuDNN benchmark. For a lot of applications memory efficient convolutions are preferred to the fastest approach. So having a way to set this would be welcome.\ntorch.backends.cudnn.benchmark = True\nThe cudnnConvolutionFwdPreference during the benchmark knows several settings:\n\nCUDNN_CONVOLUTION_FWD_NO_WORKSPACE\nCUDNN_CONVOLUTION_FWD_PREFER_FASTEST\nCUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n\nYet, in the current implementation this appears to be fixed to: CUDNN_CONVOLUTION_FWD_PREFER_FASTEST\nThis is of course similar for:\n\ncudnnConvolutionBwdDataPreference_t\ncudnnConvolutionBwdFilterPreference_t\ncudnnConvolutionFwdPreference_t\n\nMy initial proposal would be to include a property to the torch.backends.cudnn which can be set to pick a different preference, i.e.:\ntorch.backends.cudnn.benchmark_memory_limit = None (FASTEST / default value)\ntorch.backends.cudnn.benchmark_memory_limit = 0 (NO_WORKSPACE)\ntorch.backends.cudnn.benchmark_memory_limit = 12 (SPECIFY_WORKSPACE_LIMIT)\nThe numbers could represent GPU memory in GB/MB or even bytes.", "body": "It would be very convenient to have an option to set the convolution algorithm preference to prefer memory over speed when enabling the cuDNN benchmark. For a lot of applications memory efficient convolutions are preferred to the fastest approach. So having a way to set this would be welcome.\r\n\r\n`torch.backends.cudnn.benchmark = True`\r\n\r\nThe [cudnnConvolutionFwdPreference](https://docs.rs/cudnn-sys/0.0.3/cudnn_sys/enum.cudnnConvolutionFwdPreference_t.html) during the benchmark knows several settings: \r\n* CUDNN_CONVOLUTION_FWD_NO_WORKSPACE\t\t\r\n* CUDNN_CONVOLUTION_FWD_PREFER_FASTEST\t\t\r\n* CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\r\n\r\nYet, in the current [implementation](https://github.com/pytorch/pytorch/blob/8fbe003d4ed946804d67a6d3bcd84eb6c3df9a4a/torch/csrc/cudnn/Conv.cpp) this appears to be fixed to: CUDNN_CONVOLUTION_FWD_PREFER_FASTEST\r\n\r\nThis is of course similar for:\r\n* cudnnConvolutionBwdDataPreference_t\r\n* cudnnConvolutionBwdFilterPreference_t \r\n* cudnnConvolutionFwdPreference_t\r\n\r\n\r\nMy initial proposal would be to include a property to the `torch.backends.cudnn` which can be set to pick a different preference, i.e.:\r\n`torch.backends.cudnn.benchmark_memory_limit = None` (FASTEST / default value)\r\n`torch.backends.cudnn.benchmark_memory_limit = 0` (NO_WORKSPACE)\r\n`torch.backends.cudnn.benchmark_memory_limit = 12` (SPECIFY_WORKSPACE_LIMIT)\r\n\r\nThe numbers could represent GPU memory in GB/MB or even bytes.\r\n\r\n\r\n\r\n"}