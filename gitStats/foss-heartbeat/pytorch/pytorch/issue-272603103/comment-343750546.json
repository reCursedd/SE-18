{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/343750546", "html_url": "https://github.com/pytorch/pytorch/issues/3600#issuecomment-343750546", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3600", "id": 343750546, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Mzc1MDU0Ng==", "user": {"login": "kyunghyuncho", "id": 4028979, "node_id": "MDQ6VXNlcjQwMjg5Nzk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4028979?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kyunghyuncho", "html_url": "https://github.com/kyunghyuncho", "followers_url": "https://api.github.com/users/kyunghyuncho/followers", "following_url": "https://api.github.com/users/kyunghyuncho/following{/other_user}", "gists_url": "https://api.github.com/users/kyunghyuncho/gists{/gist_id}", "starred_url": "https://api.github.com/users/kyunghyuncho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kyunghyuncho/subscriptions", "organizations_url": "https://api.github.com/users/kyunghyuncho/orgs", "repos_url": "https://api.github.com/users/kyunghyuncho/repos", "events_url": "https://api.github.com/users/kyunghyuncho/events{/privacy}", "received_events_url": "https://api.github.com/users/kyunghyuncho/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-12T16:51:40Z", "updated_at": "2017-11-12T16:51:40Z", "author_association": "NONE", "body_html": "<p>the trouble i see with device-agnostic code is that one might want to mix in variables on both gpu and cpu. wouldn't it be easier to choose a default device to be cpu and let cuda fall back to doing nothing? in this case, if the code was intentionally implemented to mix variables on gpu and cpu, it would do so as intended with gpu is available, and otherwise, both types of variables would be on cpu.</p>\n<p>using <code>.type()</code> sounds reasonable, but i can't think of how to make it automatic for all the different tensor types..</p>", "body_text": "the trouble i see with device-agnostic code is that one might want to mix in variables on both gpu and cpu. wouldn't it be easier to choose a default device to be cpu and let cuda fall back to doing nothing? in this case, if the code was intentionally implemented to mix variables on gpu and cpu, it would do so as intended with gpu is available, and otherwise, both types of variables would be on cpu.\nusing .type() sounds reasonable, but i can't think of how to make it automatic for all the different tensor types..", "body": "the trouble i see with device-agnostic code is that one might want to mix in variables on both gpu and cpu. wouldn't it be easier to choose a default device to be cpu and let cuda fall back to doing nothing? in this case, if the code was intentionally implemented to mix variables on gpu and cpu, it would do so as intended with gpu is available, and otherwise, both types of variables would be on cpu.\r\n\r\nusing ``.type()`` sounds reasonable, but i can't think of how to make it automatic for all the different tensor types.."}