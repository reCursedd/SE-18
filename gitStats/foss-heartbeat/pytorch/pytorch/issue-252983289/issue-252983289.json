{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2539", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2539/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2539/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2539/events", "html_url": "https://github.com/pytorch/pytorch/issues/2539", "id": 252983289, "node_id": "MDU6SXNzdWUyNTI5ODMyODk=", "number": 2539, "title": "DataLoader converts cuda FloatTensor into cpu DoubleTensor when shape is (n,)", "user": {"login": "jpopham91", "id": 11764867, "node_id": "MDQ6VXNlcjExNzY0ODY3", "avatar_url": "https://avatars0.githubusercontent.com/u/11764867?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jpopham91", "html_url": "https://github.com/jpopham91", "followers_url": "https://api.github.com/users/jpopham91/followers", "following_url": "https://api.github.com/users/jpopham91/following{/other_user}", "gists_url": "https://api.github.com/users/jpopham91/gists{/gist_id}", "starred_url": "https://api.github.com/users/jpopham91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jpopham91/subscriptions", "organizations_url": "https://api.github.com/users/jpopham91/orgs", "repos_url": "https://api.github.com/users/jpopham91/repos", "events_url": "https://api.github.com/users/jpopham91/events{/privacy}", "received_events_url": "https://api.github.com/users/jpopham91/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-08-25T18:27:41Z", "updated_at": "2018-02-06T21:19:51Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>In version 0.2.0, it appears that the DataLoader will convert a cuda.FloatTensor into a DoubleTensor when the shape is (n,)</p>\n<div class=\"highlight highlight-source-python\"><pre>X <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">8</span>).cuda()\ny <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">64</span>).cuda()\ndataset <span class=\"pl-k\">=</span> TensorDataset(X,y)\nloader <span class=\"pl-k\">=</span> DataLoader(dataset)\n<span class=\"pl-c1\">next</span>((y <span class=\"pl-k\">for</span> (X,y) <span class=\"pl-k\">in</span> loader))</pre></div>\n<pre><code>0.1309\n[torch.DoubleTensor of size 1x1 (GPU 0)]\n</code></pre>\n<p>Using <code>unsqueeze</code> to convert the (n,) tensor to (n,1) fixes the issue.</p>\n<div class=\"highlight highlight-source-python\"><pre>X <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">8</span>).cuda()\ny <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">64</span>).cuda().unsqueeze(<span class=\"pl-c1\">1</span>)\ndataset <span class=\"pl-k\">=</span> TensorDataset(X,y)\nloader <span class=\"pl-k\">=</span> DataLoader(dataset)\n<span class=\"pl-c1\">next</span>((y <span class=\"pl-k\">for</span> (X,y) <span class=\"pl-k\">in</span> loader))</pre></div>\n<pre><code>0.8397\n[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\n</code></pre>\n<p>The workaround is easy enough, but I'm assuming this is a bug with either TensorDataset or DataLoader.</p>\n<p>EDIT:<br>\nThis seems to be independent of CUDA, as it still converts the FloatTensor to DoubleTensor in cpu mode.</p>\n<div class=\"highlight highlight-source-python\"><pre>X <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">8</span>)\ny <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">64</span>)\ndataset <span class=\"pl-k\">=</span> TensorDataset(X,y)\nloader <span class=\"pl-k\">=</span> DataLoader(dataset)\n<span class=\"pl-c1\">next</span>((y <span class=\"pl-k\">for</span> (X,y) <span class=\"pl-k\">in</span> loader))</pre></div>\n<pre><code> 0.4935\n[torch.DoubleTensor of size 1]\n</code></pre>\n<div class=\"highlight highlight-source-python\"><pre>X <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">8</span>)\ny <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">64</span>).unsqueeze(<span class=\"pl-c1\">1</span>)\ndataset <span class=\"pl-k\">=</span> TensorDataset(X,y)\nloader <span class=\"pl-k\">=</span> DataLoader(dataset)\n<span class=\"pl-c1\">next</span>((y <span class=\"pl-k\">for</span> (X,y) <span class=\"pl-k\">in</span> loader))</pre></div>\n<pre><code> 0.9419\n[torch.FloatTensor of size 1x1]\n</code></pre>", "body_text": "In version 0.2.0, it appears that the DataLoader will convert a cuda.FloatTensor into a DoubleTensor when the shape is (n,)\nX = torch.rand(64,8).cuda()\ny = torch.rand(64).cuda()\ndataset = TensorDataset(X,y)\nloader = DataLoader(dataset)\nnext((y for (X,y) in loader))\n0.1309\n[torch.DoubleTensor of size 1x1 (GPU 0)]\n\nUsing unsqueeze to convert the (n,) tensor to (n,1) fixes the issue.\nX = torch.rand(64,8).cuda()\ny = torch.rand(64).cuda().unsqueeze(1)\ndataset = TensorDataset(X,y)\nloader = DataLoader(dataset)\nnext((y for (X,y) in loader))\n0.8397\n[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\n\nThe workaround is easy enough, but I'm assuming this is a bug with either TensorDataset or DataLoader.\nEDIT:\nThis seems to be independent of CUDA, as it still converts the FloatTensor to DoubleTensor in cpu mode.\nX = torch.rand(64,8)\ny = torch.rand(64)\ndataset = TensorDataset(X,y)\nloader = DataLoader(dataset)\nnext((y for (X,y) in loader))\n 0.4935\n[torch.DoubleTensor of size 1]\n\nX = torch.rand(64,8)\ny = torch.rand(64).unsqueeze(1)\ndataset = TensorDataset(X,y)\nloader = DataLoader(dataset)\nnext((y for (X,y) in loader))\n 0.9419\n[torch.FloatTensor of size 1x1]", "body": "In version 0.2.0, it appears that the DataLoader will convert a cuda.FloatTensor into a DoubleTensor when the shape is (n,)\r\n\r\n```python\r\nX = torch.rand(64,8).cuda()\r\ny = torch.rand(64).cuda()\r\ndataset = TensorDataset(X,y)\r\nloader = DataLoader(dataset)\r\nnext((y for (X,y) in loader))\r\n```\r\n```\r\n0.1309\r\n[torch.DoubleTensor of size 1x1 (GPU 0)]\r\n```\r\n\r\nUsing `unsqueeze` to convert the (n,) tensor to (n,1) fixes the issue.\r\n```python\r\nX = torch.rand(64,8).cuda()\r\ny = torch.rand(64).cuda().unsqueeze(1)\r\ndataset = TensorDataset(X,y)\r\nloader = DataLoader(dataset)\r\nnext((y for (X,y) in loader))\r\n```\r\n```\r\n0.8397\r\n[torch.cuda.FloatTensor of size 1x1 (GPU 0)]\r\n```\r\nThe workaround is easy enough, but I'm assuming this is a bug with either TensorDataset or DataLoader.\r\n\r\nEDIT:\r\nThis seems to be independent of CUDA, as it still converts the FloatTensor to DoubleTensor in cpu mode.\r\n```python\r\nX = torch.rand(64,8)\r\ny = torch.rand(64)\r\ndataset = TensorDataset(X,y)\r\nloader = DataLoader(dataset)\r\nnext((y for (X,y) in loader))\r\n```\r\n```\r\n 0.4935\r\n[torch.DoubleTensor of size 1]\r\n```\r\n```python\r\nX = torch.rand(64,8)\r\ny = torch.rand(64).unsqueeze(1)\r\ndataset = TensorDataset(X,y)\r\nloader = DataLoader(dataset)\r\nnext((y for (X,y) in loader))\r\n```\r\n```\r\n 0.9419\r\n[torch.FloatTensor of size 1x1]\r\n```\r\n"}