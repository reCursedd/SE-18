{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/376169085", "html_url": "https://github.com/pytorch/pytorch/pull/4429#issuecomment-376169085", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4429", "id": 376169085, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjE2OTA4NQ==", "user": {"login": "jpeg729", "id": 3158606, "node_id": "MDQ6VXNlcjMxNTg2MDY=", "avatar_url": "https://avatars0.githubusercontent.com/u/3158606?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jpeg729", "html_url": "https://github.com/jpeg729", "followers_url": "https://api.github.com/users/jpeg729/followers", "following_url": "https://api.github.com/users/jpeg729/following{/other_user}", "gists_url": "https://api.github.com/users/jpeg729/gists{/gist_id}", "starred_url": "https://api.github.com/users/jpeg729/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jpeg729/subscriptions", "organizations_url": "https://api.github.com/users/jpeg729/orgs", "repos_url": "https://api.github.com/users/jpeg729/repos", "events_url": "https://api.github.com/users/jpeg729/events{/privacy}", "received_events_url": "https://api.github.com/users/jpeg729/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-26T13:39:33Z", "updated_at": "2018-03-26T13:39:33Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8100\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kashif\">@kashif</a> You are right. <code>eta</code> is the Scheduler.<br>\nBut, I think that there is a problem here that requires some thought.<br>\nIn the paper at time t the effective learning rate is <code>eta * alpha</code>, and the effective weight_decay is <code>eta * w</code>.</p>\n<p>So if someone uses a learning rate scheduler with these optimisers, <em>then they have to manually apply a similar scheduler to the weight decay</em> in order to replicate the methods in the paper. Otherwise the grad updates would decay but not the weight decay updates, and eventually the grad updates would be swamped by the weight decay updates.</p>\n<p>One fix would be to add <code>param_group['eta']</code> to each optimiser and change the schedulers to modify <code>param_group['eta']</code> instead of <code>param_group['lr']</code>.</p>", "body_text": "@kashif You are right. eta is the Scheduler.\nBut, I think that there is a problem here that requires some thought.\nIn the paper at time t the effective learning rate is eta * alpha, and the effective weight_decay is eta * w.\nSo if someone uses a learning rate scheduler with these optimisers, then they have to manually apply a similar scheduler to the weight decay in order to replicate the methods in the paper. Otherwise the grad updates would decay but not the weight decay updates, and eventually the grad updates would be swamped by the weight decay updates.\nOne fix would be to add param_group['eta'] to each optimiser and change the schedulers to modify param_group['eta'] instead of param_group['lr'].", "body": "@kashif You are right. `eta` is the Scheduler.\r\nBut, I think that there is a problem here that requires some thought.\r\nIn the paper at time t the effective learning rate is `eta * alpha`, and the effective weight_decay is `eta * w`.\r\n\r\nSo if someone uses a learning rate scheduler with these optimisers, _then they have to manually apply a similar scheduler to the weight decay_ in order to replicate the methods in the paper. Otherwise the grad updates would decay but not the weight decay updates, and eventually the grad updates would be swamped by the weight decay updates.\r\n\r\nOne fix would be to add `param_group['eta']` to each optimiser and change the schedulers to modify `param_group['eta']` instead of `param_group['lr']`."}