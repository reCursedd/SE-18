{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/376163578", "html_url": "https://github.com/pytorch/pytorch/pull/4429#issuecomment-376163578", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4429", "id": 376163578, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjE2MzU3OA==", "user": {"login": "kashif", "id": 8100, "node_id": "MDQ6VXNlcjgxMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/8100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kashif", "html_url": "https://github.com/kashif", "followers_url": "https://api.github.com/users/kashif/followers", "following_url": "https://api.github.com/users/kashif/following{/other_user}", "gists_url": "https://api.github.com/users/kashif/gists{/gist_id}", "starred_url": "https://api.github.com/users/kashif/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kashif/subscriptions", "organizations_url": "https://api.github.com/users/kashif/orgs", "repos_url": "https://api.github.com/users/kashif/repos", "events_url": "https://api.github.com/users/kashif/events{/privacy}", "received_events_url": "https://api.github.com/users/kashif/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-26T13:22:08Z", "updated_at": "2018-03-26T13:24:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3158606\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jpeg729\">@jpeg729</a> ok so I fixed the negatives:</p>\n<pre><code>if weight_decay != 0:\n    p.data.add_(-weight_decay, p.data) # p.data = p.data - weight_decay * p.data\n\np.data.add_(-group['lr'], d_p) # p.data = p.data - lr * d_p = p.data -lr * d_p - weight_decay * p.data\n</code></pre>\n<p>which is essentially line 9 in Algorithm 1 of the paper. Could it be that you are thinking of the <code>eta_t</code> in the paper as the learning rate? <code>eta_t</code> in the paper is the learning rate schedular which here is just 1.</p>", "body_text": "@jpeg729 ok so I fixed the negatives:\nif weight_decay != 0:\n    p.data.add_(-weight_decay, p.data) # p.data = p.data - weight_decay * p.data\n\np.data.add_(-group['lr'], d_p) # p.data = p.data - lr * d_p = p.data -lr * d_p - weight_decay * p.data\n\nwhich is essentially line 9 in Algorithm 1 of the paper. Could it be that you are thinking of the eta_t in the paper as the learning rate? eta_t in the paper is the learning rate schedular which here is just 1.", "body": "@jpeg729 ok so I fixed the negatives:\r\n\r\n```\r\nif weight_decay != 0:\r\n    p.data.add_(-weight_decay, p.data) # p.data = p.data - weight_decay * p.data\r\n\r\np.data.add_(-group['lr'], d_p) # p.data = p.data - lr * d_p = p.data -lr * d_p - weight_decay * p.data\r\n```\r\n\r\nwhich is essentially line 9 in Algorithm 1 of the paper. Could it be that you are thinking of the `eta_t` in the paper as the learning rate? `eta_t` in the paper is the learning rate schedular which here is just 1."}