{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357508516", "html_url": "https://github.com/pytorch/pytorch/pull/4429#issuecomment-357508516", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4429", "id": 357508516, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzUwODUxNg==", "user": {"login": "lucasb-eyer", "id": 1476029, "node_id": "MDQ6VXNlcjE0NzYwMjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1476029?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucasb-eyer", "html_url": "https://github.com/lucasb-eyer", "followers_url": "https://api.github.com/users/lucasb-eyer/followers", "following_url": "https://api.github.com/users/lucasb-eyer/following{/other_user}", "gists_url": "https://api.github.com/users/lucasb-eyer/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucasb-eyer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucasb-eyer/subscriptions", "organizations_url": "https://api.github.com/users/lucasb-eyer/orgs", "repos_url": "https://api.github.com/users/lucasb-eyer/repos", "events_url": "https://api.github.com/users/lucasb-eyer/events{/privacy}", "received_events_url": "https://api.github.com/users/lucasb-eyer/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-14T12:29:28Z", "updated_at": "2018-01-14T12:29:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I would like to see this merged, as I think that paper's result was an important one. I also prefer \"fixing\" the existing optimizers as in this PR over the huge copy-paste in the other PR.</p>\n<p>As it currently is implemented, I have never encountered a case where weight-decay has helped me in any of the non-SGD/Momentum optimizers, nor has anyone I have talked to.</p>\n<p>That being said, we might want to keep the original behaviour by default, at the very least for SGD/Momentum. Imagine all the training codes where the performance changes (for the better or the worse) after an update, and how near-impossible it would be to find out that this PR is why! Then, read the paper, and fix the factors accordingly. So I'd rather see a new flag for it, something like <code>separate_weight_decay=False</code> by default.</p>\n<p>Or maybe print a (one-time per session) warning whenever the optimizer is created with non-zero <code>weight_decay</code> for the next handful of versions.</p>", "body_text": "I would like to see this merged, as I think that paper's result was an important one. I also prefer \"fixing\" the existing optimizers as in this PR over the huge copy-paste in the other PR.\nAs it currently is implemented, I have never encountered a case where weight-decay has helped me in any of the non-SGD/Momentum optimizers, nor has anyone I have talked to.\nThat being said, we might want to keep the original behaviour by default, at the very least for SGD/Momentum. Imagine all the training codes where the performance changes (for the better or the worse) after an update, and how near-impossible it would be to find out that this PR is why! Then, read the paper, and fix the factors accordingly. So I'd rather see a new flag for it, something like separate_weight_decay=False by default.\nOr maybe print a (one-time per session) warning whenever the optimizer is created with non-zero weight_decay for the next handful of versions.", "body": "I would like to see this merged, as I think that paper's result was an important one. I also prefer \"fixing\" the existing optimizers as in this PR over the huge copy-paste in the other PR.\r\n\r\nAs it currently is implemented, I have never encountered a case where weight-decay has helped me in any of the non-SGD/Momentum optimizers, nor has anyone I have talked to.\r\n\r\nThat being said, we might want to keep the original behaviour by default, at the very least for SGD/Momentum. Imagine all the training codes where the performance changes (for the better or the worse) after an update, and how near-impossible it would be to find out that this PR is why! Then, read the paper, and fix the factors accordingly. So I'd rather see a new flag for it, something like `separate_weight_decay=False` by default.\r\n\r\nOr maybe print a (one-time per session) warning whenever the optimizer is created with non-zero `weight_decay` for the next handful of versions."}