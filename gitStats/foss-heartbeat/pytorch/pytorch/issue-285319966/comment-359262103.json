{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/359262103", "html_url": "https://github.com/pytorch/pytorch/pull/4429#issuecomment-359262103", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4429", "id": 359262103, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTI2MjEwMw==", "user": {"login": "kashif", "id": 8100, "node_id": "MDQ6VXNlcjgxMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/8100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kashif", "html_url": "https://github.com/kashif", "followers_url": "https://api.github.com/users/kashif/followers", "following_url": "https://api.github.com/users/kashif/following{/other_user}", "gists_url": "https://api.github.com/users/kashif/gists{/gist_id}", "starred_url": "https://api.github.com/users/kashif/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kashif/subscriptions", "organizations_url": "https://api.github.com/users/kashif/orgs", "repos_url": "https://api.github.com/users/kashif/repos", "events_url": "https://api.github.com/users/kashif/events{/privacy}", "received_events_url": "https://api.github.com/users/kashif/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-21T16:47:20Z", "updated_at": "2018-01-21T16:49:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1476029\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lucasb-eyer\">@lucasb-eyer</a> as far as I am aware only chainer head has this implemented currently for the adam optimizer by default. No other optimizers have weight decay so they have not added it to them, like I did here...</p>\n<p>In terms of experiments, the ones we ran for chainer are here:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3026734/33929214-fefd80c4-e02b-11e7-9624-b00a1442810e.png\"><img src=\"https://user-images.githubusercontent.com/3026734/33929214-fefd80c4-e02b-11e7-9624-b00a1442810e.png\" alt=\"\" style=\"max-width:100%;\"></a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3026734/33929219-030b34a4-e02c-11e7-925a-95f54e9bd049.png\"><img src=\"https://user-images.githubusercontent.com/3026734/33929219-030b34a4-e02c-11e7-925a-95f54e9bd049.png\" alt=\"\" style=\"max-width:100%;\"></a></p>", "body_text": "@lucasb-eyer as far as I am aware only chainer head has this implemented currently for the adam optimizer by default. No other optimizers have weight decay so they have not added it to them, like I did here...\nIn terms of experiments, the ones we ran for chainer are here:", "body": "@lucasb-eyer as far as I am aware only chainer head has this implemented currently for the adam optimizer by default. No other optimizers have weight decay so they have not added it to them, like I did here...\r\n\r\nIn terms of experiments, the ones we ran for chainer are here:\r\n\r\n\r\n![](https://user-images.githubusercontent.com/3026734/33929214-fefd80c4-e02b-11e7-9624-b00a1442810e.png)\r\n\r\n![](https://user-images.githubusercontent.com/3026734/33929219-030b34a4-e02c-11e7-925a-95f54e9bd049.png)\r\n"}