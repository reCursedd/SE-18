{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/376884944", "html_url": "https://github.com/pytorch/pytorch/pull/4429#issuecomment-376884944", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4429", "id": 376884944, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Njg4NDk0NA==", "user": {"login": "untom", "id": 3627551, "node_id": "MDQ6VXNlcjM2Mjc1NTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/3627551?v=4", "gravatar_id": "", "url": "https://api.github.com/users/untom", "html_url": "https://github.com/untom", "followers_url": "https://api.github.com/users/untom/followers", "following_url": "https://api.github.com/users/untom/following{/other_user}", "gists_url": "https://api.github.com/users/untom/gists{/gist_id}", "starred_url": "https://api.github.com/users/untom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/untom/subscriptions", "organizations_url": "https://api.github.com/users/untom/orgs", "repos_url": "https://api.github.com/users/untom/repos", "events_url": "https://api.github.com/users/untom/events{/privacy}", "received_events_url": "https://api.github.com/users/untom/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-28T13:23:56Z", "updated_at": "2018-03-28T13:23:56Z", "author_association": "NONE", "body_html": "<p>Just jumping in as longtime pytorch user: I think these improvements should definitely be added to pytorch, even if it means changing how learning rate scheduling works on a larger scale.  After all, Pytorch is still in the 0.x version phase, so I think \"breaking\" changes to the API should still be allowed (this likely wouldn't even break too many thinks).</p>\n<p>Additionally, since we know that \"normal\" Adam is faulty its handling of L2 reg, it doesn't make sense to keep doing it the wrong way. That's also why I think for having \"fixed L2 reg\" as the default instead of the backwards-compatible option. But that's besides the point. The really important thing is that pytorch will have an option to use adamw/sgdw in the future, because that's something that a lot of us are really looking forward to!</p>", "body_text": "Just jumping in as longtime pytorch user: I think these improvements should definitely be added to pytorch, even if it means changing how learning rate scheduling works on a larger scale.  After all, Pytorch is still in the 0.x version phase, so I think \"breaking\" changes to the API should still be allowed (this likely wouldn't even break too many thinks).\nAdditionally, since we know that \"normal\" Adam is faulty its handling of L2 reg, it doesn't make sense to keep doing it the wrong way. That's also why I think for having \"fixed L2 reg\" as the default instead of the backwards-compatible option. But that's besides the point. The really important thing is that pytorch will have an option to use adamw/sgdw in the future, because that's something that a lot of us are really looking forward to!", "body": "Just jumping in as longtime pytorch user: I think these improvements should definitely be added to pytorch, even if it means changing how learning rate scheduling works on a larger scale.  After all, Pytorch is still in the 0.x version phase, so I think \"breaking\" changes to the API should still be allowed (this likely wouldn't even break too many thinks).\r\n \r\nAdditionally, since we know that \"normal\" Adam is faulty its handling of L2 reg, it doesn't make sense to keep doing it the wrong way. That's also why I think for having \"fixed L2 reg\" as the default instead of the backwards-compatible option. But that's besides the point. The really important thing is that pytorch will have an option to use adamw/sgdw in the future, because that's something that a lot of us are really looking forward to!"}