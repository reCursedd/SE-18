{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373667450", "html_url": "https://github.com/pytorch/pytorch/issues/5812#issuecomment-373667450", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5812", "id": 373667450, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzY2NzQ1MA==", "user": {"login": "leftthomas", "id": 9991443, "node_id": "MDQ6VXNlcjk5OTE0NDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/9991443?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leftthomas", "html_url": "https://github.com/leftthomas", "followers_url": "https://api.github.com/users/leftthomas/followers", "following_url": "https://api.github.com/users/leftthomas/following{/other_user}", "gists_url": "https://api.github.com/users/leftthomas/gists{/gist_id}", "starred_url": "https://api.github.com/users/leftthomas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leftthomas/subscriptions", "organizations_url": "https://api.github.com/users/leftthomas/orgs", "repos_url": "https://api.github.com/users/leftthomas/repos", "events_url": "https://api.github.com/users/leftthomas/events{/privacy}", "received_events_url": "https://api.github.com/users/leftthomas/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-16T10:15:01Z", "updated_at": "2018-03-16T10:15:01Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> I have tested <strong>put the train loop in a separate function</strong> as you said, it still have the problem! <strong>It is a bug of PyTorch, not my code wrong</strong>. The script to reproduce the bug:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch.optim <span class=\"pl-k\">import</span> Adam\n<span class=\"pl-k\">from</span> tqdm <span class=\"pl-k\">import</span> tqdm\n<span class=\"pl-k\">import</span> torchvision.transforms <span class=\"pl-k\">as</span> transforms\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n<span class=\"pl-k\">from</span> torchvision.datasets <span class=\"pl-k\">import</span> <span class=\"pl-c1\">MNIST</span>\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MNISTNet</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(MNISTNet, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.features <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">7</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>),\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">512</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">512</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">512</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        )\n        <span class=\"pl-c1\">self</span>.classifier <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-v\">in_features</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">512</span>, <span class=\"pl-v\">out_features</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.features(x)\n        out <span class=\"pl-k\">=</span> out.view(out.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.classifier(out)\n        <span class=\"pl-k\">return</span> classes\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">optimizer</span>):\n    train_data <span class=\"pl-k\">=</span> MNIST(<span class=\"pl-v\">root</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.ToTensor(), <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    train_dl <span class=\"pl-k\">=</span> DataLoader(<span class=\"pl-v\">dataset</span><span class=\"pl-k\">=</span>train_data, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)\n\n    <span class=\"pl-k\">for</span> data, labels <span class=\"pl-k\">in</span> train_dl:\n        data <span class=\"pl-k\">=</span> Variable(data).cuda()\n        labels <span class=\"pl-k\">=</span> Variable(labels).cuda()\n        classes <span class=\"pl-k\">=</span> model(data)\n        loss <span class=\"pl-k\">=</span> F.cross_entropy(classes, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>(<span class=\"pl-smi\">model</span>):\n    test_data <span class=\"pl-k\">=</span> MNIST(<span class=\"pl-v\">root</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.ToTensor(), <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    test_dl <span class=\"pl-k\">=</span> DataLoader(<span class=\"pl-v\">dataset</span><span class=\"pl-k\">=</span>test_data, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)\n    <span class=\"pl-k\">for</span> data, labels <span class=\"pl-k\">in</span> test_dl:\n        data <span class=\"pl-k\">=</span> Variable(data).cuda()\n        model(data)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    model <span class=\"pl-k\">=</span> MNISTNet().cuda()\n    optimizer <span class=\"pl-k\">=</span> Adam(model.parameters())\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> tqdm(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>)):\n        train(model, optimizer)\n        test(model)\n</pre></div>", "body_text": "@fmassa I have tested put the train loop in a separate function as you said, it still have the problem! It is a bug of PyTorch, not my code wrong. The script to reproduce the bug:\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nimport torch.nn.functional as F\n\n\nclass MNISTNet(nn.Module):\n    def __init__(self):\n        super(MNISTNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n        )\n        self.classifier = nn.Linear(in_features=2 * 2 * 512, out_features=10)\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        classes = self.classifier(out)\n        return classes\n\n\ndef train(model, optimizer):\n    train_data = MNIST(root='./', train=True, transform=transforms.ToTensor(), download=True)\n    train_dl = DataLoader(dataset=train_data, batch_size=64, shuffle=True, num_workers=4)\n\n    for data, labels in train_dl:\n        data = Variable(data).cuda()\n        labels = Variable(labels).cuda()\n        classes = model(data)\n        loss = F.cross_entropy(classes, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\ndef test(model):\n    test_data = MNIST(root='./', train=False, transform=transforms.ToTensor(), download=True)\n    test_dl = DataLoader(dataset=test_data, batch_size=64, num_workers=4)\n    for data, labels in test_dl:\n        data = Variable(data).cuda()\n        model(data)\n\n\nif __name__ == '__main__':\n    model = MNISTNet().cuda()\n    optimizer = Adam(model.parameters())\n    for epoch in tqdm(range(2)):\n        train(model, optimizer)\n        test(model)", "body": "@fmassa I have tested **put the train loop in a separate function** as you said, it still have the problem! **It is a bug of PyTorch, not my code wrong**. The script to reproduce the bug:\r\n```python\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch.optim import Adam\r\nfrom tqdm import tqdm\r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass MNISTNet(nn.Module):\r\n    def __init__(self):\r\n        super(MNISTNet, self).__init__()\r\n        self.features = nn.Sequential(\r\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3),\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\r\n        )\r\n        self.classifier = nn.Linear(in_features=2 * 2 * 512, out_features=10)\r\n\r\n    def forward(self, x):\r\n        out = self.features(x)\r\n        out = out.view(out.size(0), -1)\r\n        classes = self.classifier(out)\r\n        return classes\r\n\r\n\r\ndef train(model, optimizer):\r\n    train_data = MNIST(root='./', train=True, transform=transforms.ToTensor(), download=True)\r\n    train_dl = DataLoader(dataset=train_data, batch_size=64, shuffle=True, num_workers=4)\r\n\r\n    for data, labels in train_dl:\r\n        data = Variable(data).cuda()\r\n        labels = Variable(labels).cuda()\r\n        classes = model(data)\r\n        loss = F.cross_entropy(classes, labels)\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n\r\ndef test(model):\r\n    test_data = MNIST(root='./', train=False, transform=transforms.ToTensor(), download=True)\r\n    test_dl = DataLoader(dataset=test_data, batch_size=64, num_workers=4)\r\n    for data, labels in test_dl:\r\n        data = Variable(data).cuda()\r\n        model(data)\r\n\r\n\r\nif __name__ == '__main__':\r\n    model = MNISTNet().cuda()\r\n    optimizer = Adam(model.parameters())\r\n    for epoch in tqdm(range(2)):\r\n        train(model, optimizer)\r\n        test(model)\r\n\r\n```"}