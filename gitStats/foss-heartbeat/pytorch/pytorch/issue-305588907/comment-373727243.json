{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373727243", "html_url": "https://github.com/pytorch/pytorch/issues/5812#issuecomment-373727243", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5812", "id": 373727243, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzcyNzI0Mw==", "user": {"login": "leftthomas", "id": 9991443, "node_id": "MDQ6VXNlcjk5OTE0NDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/9991443?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leftthomas", "html_url": "https://github.com/leftthomas", "followers_url": "https://api.github.com/users/leftthomas/followers", "following_url": "https://api.github.com/users/leftthomas/following{/other_user}", "gists_url": "https://api.github.com/users/leftthomas/gists{/gist_id}", "starred_url": "https://api.github.com/users/leftthomas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leftthomas/subscriptions", "organizations_url": "https://api.github.com/users/leftthomas/orgs", "repos_url": "https://api.github.com/users/leftthomas/repos", "events_url": "https://api.github.com/users/leftthomas/events{/privacy}", "received_events_url": "https://api.github.com/users/leftthomas/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-16T14:18:46Z", "updated_at": "2018-03-16T14:18:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> But the memory grow up to three times! Does it really normal? And I have asked more than five friends to test this code, we have tested it on PyTorch 0.2.0, 0.3.0, 0.3.1, CUDA8.0,CUDA9.0, NVIDIA GTX 1060, 1070, 1080, TITAN X, they all have saw this issue. The most strange thing is when the <code>batch_size</code> set to other number such as 100, it won't encounter this problem, but if <code>batch_size</code>  is 32,64,128,256, the problem happened, I don't think it's fine, it's really the natural behavior of PyTorch?</p>", "body_text": "@fmassa But the memory grow up to three times! Does it really normal? And I have asked more than five friends to test this code, we have tested it on PyTorch 0.2.0, 0.3.0, 0.3.1, CUDA8.0,CUDA9.0, NVIDIA GTX 1060, 1070, 1080, TITAN X, they all have saw this issue. The most strange thing is when the batch_size set to other number such as 100, it won't encounter this problem, but if batch_size  is 32,64,128,256, the problem happened, I don't think it's fine, it's really the natural behavior of PyTorch?", "body": "@fmassa But the memory grow up to three times! Does it really normal? And I have asked more than five friends to test this code, we have tested it on PyTorch 0.2.0, 0.3.0, 0.3.1, CUDA8.0,CUDA9.0, NVIDIA GTX 1060, 1070, 1080, TITAN X, they all have saw this issue. The most strange thing is when the `batch_size` set to other number such as 100, it won't encounter this problem, but if `batch_size`  is 32,64,128,256, the problem happened, I don't think it's fine, it's really the natural behavior of PyTorch?"}