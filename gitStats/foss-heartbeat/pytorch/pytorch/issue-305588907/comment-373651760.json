{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373651760", "html_url": "https://github.com/pytorch/pytorch/issues/5812#issuecomment-373651760", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5812", "id": 373651760, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzY1MTc2MA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-16T09:20:41Z", "updated_at": "2018-03-16T09:20:41Z", "author_association": "MEMBER", "body_html": "<p>The reason in your last snippet is that the loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts <code>del loss</code> and <code>del classes</code>, or put the train loop in a separate function (and thus the scope ends with the function)</p>", "body_text": "The reason in your last snippet is that the loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts del loss and del classes, or put the train loop in a separate function (and thus the scope ends with the function)", "body": "The reason in your last snippet is that the loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts `del loss` and `del classes`, or put the train loop in a separate function (and thus the scope ends with the function)"}