{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373699732", "html_url": "https://github.com/pytorch/pytorch/issues/5812#issuecomment-373699732", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5812", "id": 373699732, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzY5OTczMg==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-16T12:33:13Z", "updated_at": "2018-03-16T12:33:13Z", "author_association": "MEMBER", "body_html": "<p>I tried your snippet and I don't see a memory leak, but a small increase in memory usage when the testing part happens, but it then stabilizes after the first epoch.</p>\n<p>What I think is happening (but I might be wrong here) is that the caching allocator is not being able to recover part of the memory that was allocated during training, maybe because it got fragmented at a later stage of the network during training (for example during backprop) ?</p>\n<p>In the end, I think the behavior you are seing is normal, and you'd see expected results if you cleared the caching allocator after training, but that would imply slowdowns on every epoch.</p>\n<p>I think this is not a bug actually.</p>", "body_text": "I tried your snippet and I don't see a memory leak, but a small increase in memory usage when the testing part happens, but it then stabilizes after the first epoch.\nWhat I think is happening (but I might be wrong here) is that the caching allocator is not being able to recover part of the memory that was allocated during training, maybe because it got fragmented at a later stage of the network during training (for example during backprop) ?\nIn the end, I think the behavior you are seing is normal, and you'd see expected results if you cleared the caching allocator after training, but that would imply slowdowns on every epoch.\nI think this is not a bug actually.", "body": "I tried your snippet and I don't see a memory leak, but a small increase in memory usage when the testing part happens, but it then stabilizes after the first epoch.\r\n\r\nWhat I think is happening (but I might be wrong here) is that the caching allocator is not being able to recover part of the memory that was allocated during training, maybe because it got fragmented at a later stage of the network during training (for example during backprop) ?\r\n\r\nIn the end, I think the behavior you are seing is normal, and you'd see expected results if you cleared the caching allocator after training, but that would imply slowdowns on every epoch.\r\n\r\nI think this is not a bug actually."}