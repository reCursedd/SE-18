{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5812", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5812/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5812/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5812/events", "html_url": "https://github.com/pytorch/pytorch/issues/5812", "id": 305588907, "node_id": "MDU6SXNzdWUzMDU1ODg5MDc=", "number": 5812, "title": "memory leaky on DataLoader", "user": {"login": "leftthomas", "id": 9991443, "node_id": "MDQ6VXNlcjk5OTE0NDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/9991443?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leftthomas", "html_url": "https://github.com/leftthomas", "followers_url": "https://api.github.com/users/leftthomas/followers", "following_url": "https://api.github.com/users/leftthomas/following{/other_user}", "gists_url": "https://api.github.com/users/leftthomas/gists{/gist_id}", "starred_url": "https://api.github.com/users/leftthomas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leftthomas/subscriptions", "organizations_url": "https://api.github.com/users/leftthomas/orgs", "repos_url": "https://api.github.com/users/leftthomas/repos", "events_url": "https://api.github.com/users/leftthomas/events{/privacy}", "received_events_url": "https://api.github.com/users/leftthomas/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 18, "created_at": "2018-03-15T14:59:56Z", "updated_at": "2018-03-20T04:22:30Z", "closed_at": "2018-03-16T12:44:00Z", "author_association": "NONE", "body_html": "<ul>\n<li>OS: Ubuntu16.04</li>\n<li>PyTorch version: 0.3.1</li>\n<li>PyTorchNet version: 0.0.1</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version: CUDA 9.0.176/cuDNN 7.0.5.15</li>\n</ul>\n<p><strong>The script to reproduce the bug:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch.optim <span class=\"pl-k\">import</span> Adam\n<span class=\"pl-k\">from</span> torchnet.engine <span class=\"pl-k\">import</span> Engine\n<span class=\"pl-k\">from</span> tqdm <span class=\"pl-k\">import</span> tqdm\n<span class=\"pl-k\">import</span> torchvision.transforms <span class=\"pl-k\">as</span> transforms\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> DataLoader\n<span class=\"pl-k\">from</span> torchvision.datasets <span class=\"pl-k\">import</span> <span class=\"pl-c1\">MNIST</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">MNISTNet</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(MNISTNet, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.features <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">7</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>),\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">256</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">512</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n            nn.Conv2d(<span class=\"pl-v\">in_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">512</span>, <span class=\"pl-v\">out_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">512</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        )\n        <span class=\"pl-c1\">self</span>.classifier <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-v\">in_features</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">512</span>, <span class=\"pl-v\">out_features</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.features(x)\n        out <span class=\"pl-k\">=</span> out.view(out.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        classes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.classifier(out)\n        <span class=\"pl-k\">return</span> classes\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_iterator</span>(<span class=\"pl-smi\">mode</span>):\n    data <span class=\"pl-k\">=</span> MNIST(<span class=\"pl-v\">root</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span>mode, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.ToTensor(), <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">return</span> DataLoader(<span class=\"pl-v\">dataset</span><span class=\"pl-k\">=</span>data, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span>mode, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">processor</span>(<span class=\"pl-smi\">sample</span>):\n    data, labels, training <span class=\"pl-k\">=</span> sample\n    data <span class=\"pl-k\">=</span> Variable(data).cuda()\n    labels <span class=\"pl-k\">=</span> Variable(labels).cuda()\n    classes <span class=\"pl-k\">=</span> model(data)\n    loss <span class=\"pl-k\">=</span> loss_criterion(classes, labels)\n    <span class=\"pl-k\">return</span> loss, classes\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">on_sample</span>(<span class=\"pl-smi\">state</span>):\n    state[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sample<span class=\"pl-pds\">'</span></span>].append(state[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>])\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">on_start_epoch</span>(<span class=\"pl-smi\">state</span>):\n    state[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>iterator<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> tqdm(state[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>iterator<span class=\"pl-pds\">'</span></span>])\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">on_end_epoch</span>(<span class=\"pl-smi\">state</span>):\n    engine.test(processor, get_iterator(<span class=\"pl-c1\">False</span>))\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n\n    model <span class=\"pl-k\">=</span> MNISTNet().cuda()\n    loss_criterion <span class=\"pl-k\">=</span> nn.CrossEntropyLoss().cuda()\n\n    optimizer <span class=\"pl-k\">=</span> Adam(model.parameters())\n    engine <span class=\"pl-k\">=</span> Engine()\n    engine.hooks[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>on_sample<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> on_sample\n    engine.hooks[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>on_start_epoch<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> on_start_epoch\n    engine.hooks[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>on_end_epoch<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> on_end_epoch\n\n    engine.train(processor, get_iterator(<span class=\"pl-c1\">True</span>), <span class=\"pl-v\">maxepoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>optimizer)</pre></div>\n<p><strong>Error messages</strong></p>\n<ul>\n<li>\n<p>The <code>batch_size</code> of <code>DataLoader</code> is set to 64, then on epoch 1, before <code>on_end_epoch</code>:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/9991443/37463453-30928920-2890-11e8-93d0-d0dbad043997.png\"><img src=\"https://user-images.githubusercontent.com/9991443/37463453-30928920-2890-11e8-93d0-d0dbad043997.png\" alt=\"screenshot from 2018-03-15 20-23-31\" style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p>After <code>on_end_epoch</code>, you could see the memory is grow up to about 3 times:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/9991443/37463507-5f8d2992-2890-11e8-9f7d-b6a754314c6a.png\"><img src=\"https://user-images.githubusercontent.com/9991443/37463507-5f8d2992-2890-11e8-9f7d-b6a754314c6a.png\" alt=\"screenshot from 2018-03-15 20-24-00\" style=\"max-width:100%;\"></a></p>\n</li>\n<li>\n<p>Furthermore, if I change the <code>batch_size</code> of <code>DataLoader</code> to 1000, then on epoch 1, before <code>on_end_epoch</code>, you could see the memory use is just only <strong>715MB</strong>, it's strange:</p>\n</li>\n</ul>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/9991443/37471232-3815a042-28a4-11e8-89e5-b971a9e3af93.png\"><img width=\"867\" alt=\"qq20180315-225225 2x\" src=\"https://user-images.githubusercontent.com/9991443/37471232-3815a042-28a4-11e8-89e5-b971a9e3af93.png\" style=\"max-width:100%;\"></a></p>\n<ul>\n<li>After <code>on_end_epoch</code>, you could see the memory haven't change:</li>\n</ul>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/9991443/37471400-8e578920-28a4-11e8-8c51-34bb35c80e6c.png\"><img width=\"748\" alt=\"qq20180315-225238 2x\" src=\"https://user-images.githubusercontent.com/9991443/37471400-8e578920-28a4-11e8-8c51-34bb35c80e6c.png\" style=\"max-width:100%;\"></a></p>", "body_text": "OS: Ubuntu16.04\nPyTorch version: 0.3.1\nPyTorchNet version: 0.0.1\nHow you installed PyTorch (conda, pip, source): conda\nPython version: 3.6.4\nCUDA/cuDNN version: CUDA 9.0.176/cuDNN 7.0.5.15\n\nThe script to reproduce the bug:\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nfrom torchnet.engine import Engine\nfrom tqdm import tqdm\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nclass MNISTNet(nn.Module):\n    def __init__(self):\n        super(MNISTNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n        )\n        self.classifier = nn.Linear(in_features=2 * 2 * 512, out_features=10)\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        classes = self.classifier(out)\n        return classes\n\n\ndef get_iterator(mode):\n    data = MNIST(root='./', train=mode, transform=transforms.ToTensor(), download=True)\n    return DataLoader(dataset=data, batch_size=64, shuffle=mode, num_workers=4)\n\n\ndef processor(sample):\n    data, labels, training = sample\n    data = Variable(data).cuda()\n    labels = Variable(labels).cuda()\n    classes = model(data)\n    loss = loss_criterion(classes, labels)\n    return loss, classes\n\n\ndef on_sample(state):\n    state['sample'].append(state['train'])\n\n\ndef on_start_epoch(state):\n    state['iterator'] = tqdm(state['iterator'])\n\n\ndef on_end_epoch(state):\n    engine.test(processor, get_iterator(False))\n\n\nif __name__ == '__main__':\n\n    model = MNISTNet().cuda()\n    loss_criterion = nn.CrossEntropyLoss().cuda()\n\n    optimizer = Adam(model.parameters())\n    engine = Engine()\n    engine.hooks['on_sample'] = on_sample\n    engine.hooks['on_start_epoch'] = on_start_epoch\n    engine.hooks['on_end_epoch'] = on_end_epoch\n\n    engine.train(processor, get_iterator(True), maxepoch=100, optimizer=optimizer)\nError messages\n\n\nThe batch_size of DataLoader is set to 64, then on epoch 1, before on_end_epoch:\n\n\n\nAfter on_end_epoch, you could see the memory is grow up to about 3 times:\n\n\n\nFurthermore, if I change the batch_size of DataLoader to 1000, then on epoch 1, before on_end_epoch, you could see the memory use is just only 715MB, it's strange:\n\n\n\n\nAfter on_end_epoch, you could see the memory haven't change:", "body": "- OS: Ubuntu16.04\r\n- PyTorch version: 0.3.1\r\n- PyTorchNet version: 0.0.1\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: CUDA 9.0.176/cuDNN 7.0.5.15\r\n\r\n**The script to reproduce the bug:** \r\n``` python\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\nfrom torch.optim import Adam\r\nfrom torchnet.engine import Engine\r\nfrom tqdm import tqdm\r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision.datasets import MNIST\r\n\r\nclass MNISTNet(nn.Module):\r\n    def __init__(self):\r\n        super(MNISTNet, self).__init__()\r\n        self.features = nn.Sequential(\r\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3),\r\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1),\r\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\r\n        )\r\n        self.classifier = nn.Linear(in_features=2 * 2 * 512, out_features=10)\r\n\r\n    def forward(self, x):\r\n        out = self.features(x)\r\n        out = out.view(out.size(0), -1)\r\n        classes = self.classifier(out)\r\n        return classes\r\n\r\n\r\ndef get_iterator(mode):\r\n    data = MNIST(root='./', train=mode, transform=transforms.ToTensor(), download=True)\r\n    return DataLoader(dataset=data, batch_size=64, shuffle=mode, num_workers=4)\r\n\r\n\r\ndef processor(sample):\r\n    data, labels, training = sample\r\n    data = Variable(data).cuda()\r\n    labels = Variable(labels).cuda()\r\n    classes = model(data)\r\n    loss = loss_criterion(classes, labels)\r\n    return loss, classes\r\n\r\n\r\ndef on_sample(state):\r\n    state['sample'].append(state['train'])\r\n\r\n\r\ndef on_start_epoch(state):\r\n    state['iterator'] = tqdm(state['iterator'])\r\n\r\n\r\ndef on_end_epoch(state):\r\n    engine.test(processor, get_iterator(False))\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    model = MNISTNet().cuda()\r\n    loss_criterion = nn.CrossEntropyLoss().cuda()\r\n\r\n    optimizer = Adam(model.parameters())\r\n    engine = Engine()\r\n    engine.hooks['on_sample'] = on_sample\r\n    engine.hooks['on_start_epoch'] = on_start_epoch\r\n    engine.hooks['on_end_epoch'] = on_end_epoch\r\n\r\n    engine.train(processor, get_iterator(True), maxepoch=100, optimizer=optimizer)\r\n```\r\n**Error messages**\r\n\r\n- The `batch_size` of `DataLoader` is set to 64, then on epoch 1, before `on_end_epoch`:\r\n![screenshot from 2018-03-15 20-23-31](https://user-images.githubusercontent.com/9991443/37463453-30928920-2890-11e8-93d0-d0dbad043997.png)\r\n\r\n- After `on_end_epoch`, you could see the memory is grow up to about 3 times:\r\n![screenshot from 2018-03-15 20-24-00](https://user-images.githubusercontent.com/9991443/37463507-5f8d2992-2890-11e8-9f7d-b6a754314c6a.png)\r\n\r\n- Furthermore, if I change the `batch_size` of `DataLoader` to 1000, then on epoch 1, before `on_end_epoch`, you could see the memory use is just only **715MB**, it's strange:\r\n<img width=\"867\" alt=\"qq20180315-225225 2x\" src=\"https://user-images.githubusercontent.com/9991443/37471232-3815a042-28a4-11e8-89e5-b971a9e3af93.png\">\r\n\r\n- After `on_end_epoch`, you could see the memory haven't change:\r\n<img width=\"748\" alt=\"qq20180315-225238 2x\" src=\"https://user-images.githubusercontent.com/9991443/37471400-8e578920-28a4-11e8-8c51-34bb35c80e6c.png\">\r\n"}