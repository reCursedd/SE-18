{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10751", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10751/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10751/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10751/events", "html_url": "https://github.com/pytorch/pytorch/issues/10751", "id": 352706189, "node_id": "MDU6SXNzdWUzNTI3MDYxODk=", "number": 10751, "title": "[docs] Error in documentation for fft normalization", "user": {"login": "cranmer", "id": 4458890, "node_id": "MDQ6VXNlcjQ0NTg4OTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/4458890?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cranmer", "html_url": "https://github.com/cranmer", "followers_url": "https://api.github.com/users/cranmer/followers", "following_url": "https://api.github.com/users/cranmer/following{/other_user}", "gists_url": "https://api.github.com/users/cranmer/gists{/gist_id}", "starred_url": "https://api.github.com/users/cranmer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cranmer/subscriptions", "organizations_url": "https://api.github.com/users/cranmer/orgs", "repos_url": "https://api.github.com/users/cranmer/repos", "events_url": "https://api.github.com/users/cranmer/events{/privacy}", "received_events_url": "https://api.github.com/users/cranmer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-08-21T20:54:05Z", "updated_at": "2018-08-22T04:10:21Z", "closed_at": "2018-08-22T04:10:21Z", "author_association": "NONE", "body_html": "<p>The primary fft equation in the documentation <a href=\"https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5303\">here</a> is</p>\n<pre lang=\"X[\\omega_1,\" data-meta=\"\\dots, \\omega_d] =\"><code>        \\frac{1}{\\prod_{i=1}^d N_i} \\sum_{n_1=0}^{N_1} \\dots \\sum_{n_d=0}^{N_d} x[n_1, \\dots, n_d]\n         e^{-j\\ 2 \\pi \\sum_{i=0}^d \\frac{\\omega_i n_i}{N_i}},\n</code></pre>\n<p>However, from experimentation it seems the  1/N factor <code>\\frac{1}{\\prod_{i=1}^d N_i}</code> should not be there for the forward transform.</p>\n<p>Here is the relevant line:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5303\">pytorch/torch/_torch_docs.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 5303\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/8013dac43d2acb592cab75317f17d4f9c5b9eb6a\">8013dac</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L5303\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"5303\"></td>\n          <td id=\"LC5303\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-sr\">        <span class=\"pl-cce\">\\f</span>rac<span class=\"pl-k\">{1}</span>{<span class=\"pl-cce\">\\p</span>rod_{i=1}<span class=\"pl-c1\">^</span>d N_i} <span class=\"pl-c1\">\\s</span>um_{n_1=0}<span class=\"pl-c1\">^</span>{N_1} <span class=\"pl-c1\">\\d</span>ots <span class=\"pl-c1\">\\s</span>um_{n_d=0}<span class=\"pl-c1\">^</span>{N_d} x[<span class=\"pl-c1\">n_1, </span><span class=\"pl-c1\">\\d</span><span class=\"pl-c1\">ots, n_d</span>]</span> </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>The inverse transform equation (with the 1/N) is consistent with the behavior of the implementation.</p>\n<p>Note, the documentation is accurate for <code>normalized=True</code>, <code>If normalized is set to True, this normalizes the result by dividing by [1/sqrt(N)]</code>, but this only makes sense when if that 1/N factor isn't there in the main equation above.</p>\n<p>Note, the documentation for <code>ifft</code> is also accurate for <code>normalized=True</code>, but again this only makes sense if the 1/N factor isn't there in the main equation above.</p>\n<p>Lastly, the documentation for <code>rfft</code> is a mix of two problems. <a href=\"https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5493\">This line</a> says<br>\n<code>If normalized is set to True, this normalizes the result by multiplying it with [sqrt(N)]</code><br>\nThis says \"multiply\" instead of \"divide\" (as in fft). This is consistent with the primary equation, but it is not consistent with the implementation. It should be <code>divide</code></p>", "body_text": "The primary fft equation in the documentation here is\n        \\frac{1}{\\prod_{i=1}^d N_i} \\sum_{n_1=0}^{N_1} \\dots \\sum_{n_d=0}^{N_d} x[n_1, \\dots, n_d]\n         e^{-j\\ 2 \\pi \\sum_{i=0}^d \\frac{\\omega_i n_i}{N_i}},\n\nHowever, from experimentation it seems the  1/N factor \\frac{1}{\\prod_{i=1}^d N_i} should not be there for the forward transform.\nHere is the relevant line:\n\n  \n    \n      pytorch/torch/_torch_docs.py\n    \n    \n         Line 5303\n      in\n      8013dac\n    \n    \n    \n    \n\n        \n          \n                   \\frac{1}{\\prod_{i=1}^d N_i} \\sum_{n_1=0}^{N_1} \\dots \\sum_{n_d=0}^{N_d} x[n_1, \\dots, n_d] \n        \n    \n  \n\n\nThe inverse transform equation (with the 1/N) is consistent with the behavior of the implementation.\nNote, the documentation is accurate for normalized=True, If normalized is set to True, this normalizes the result by dividing by [1/sqrt(N)], but this only makes sense when if that 1/N factor isn't there in the main equation above.\nNote, the documentation for ifft is also accurate for normalized=True, but again this only makes sense if the 1/N factor isn't there in the main equation above.\nLastly, the documentation for rfft is a mix of two problems. This line says\nIf normalized is set to True, this normalizes the result by multiplying it with [sqrt(N)]\nThis says \"multiply\" instead of \"divide\" (as in fft). This is consistent with the primary equation, but it is not consistent with the implementation. It should be divide", "body": "The primary fft equation in the documentation [here](https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5303) is \r\n```   X[\\omega_1, \\dots, \\omega_d] =\r\n        \\frac{1}{\\prod_{i=1}^d N_i} \\sum_{n_1=0}^{N_1} \\dots \\sum_{n_d=0}^{N_d} x[n_1, \\dots, n_d]\r\n         e^{-j\\ 2 \\pi \\sum_{i=0}^d \\frac{\\omega_i n_i}{N_i}},\r\n```\r\nHowever, from experimentation it seems the  1/N factor `\\frac{1}{\\prod_{i=1}^d N_i}` should not be there for the forward transform. \r\n\r\nHere is the relevant line:\r\nhttps://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5303\r\n\r\nThe inverse transform equation (with the 1/N) is consistent with the behavior of the implementation.\r\n\r\nNote, the documentation is accurate for `normalized=True`, `If normalized is set to True, this normalizes the result by dividing by [1/sqrt(N)]`, but this only makes sense when if that 1/N factor isn't there in the main equation above. \r\n\r\nNote, the documentation for `ifft` is also accurate for `normalized=True`, but again this only makes sense if the 1/N factor isn't there in the main equation above.\r\n\r\nLastly, the documentation for `rfft` is a mix of two problems. [This line](https://github.com/pytorch/pytorch/blob/8013dac43d2acb592cab75317f17d4f9c5b9eb6a/torch/_torch_docs.py#L5493) says \r\n```If normalized is set to True, this normalizes the result by multiplying it with [sqrt(N)]```\r\nThis says \"multiply\" instead of \"divide\" (as in fft). This is consistent with the primary equation, but it is not consistent with the implementation. It should be `divide`"}