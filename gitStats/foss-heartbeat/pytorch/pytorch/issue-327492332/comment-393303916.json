{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393303916", "html_url": "https://github.com/pytorch/pytorch/pull/7932#issuecomment-393303916", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7932", "id": 393303916, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzMwMzkxNg==", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-30T20:17:59Z", "updated_at": "2018-05-30T20:17:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Offline discussion with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a>: there is a possibility that this is dead code since we require shapes to symbolically differentiate. This led into a discussion of generalizing symbolic differentiation and we pondered how to remove two things:</p>\n<ol>\n<li>Need to know statically whether we should accumulate gradients across a dimension due to broadcasting</li>\n<li>Creating zero gradient tensors for unused outputs</li>\n</ol>", "body_text": "Offline discussion with @zdevito: there is a possibility that this is dead code since we require shapes to symbolically differentiate. This led into a discussion of generalizing symbolic differentiation and we pondered how to remove two things:\n\nNeed to know statically whether we should accumulate gradients across a dimension due to broadcasting\nCreating zero gradient tensors for unused outputs", "body": "Offline discussion with @zdevito: there is a possibility that this is dead code since we require shapes to symbolically differentiate. This led into a discussion of generalizing symbolic differentiation and we pondered how to remove two things:\r\n1) Need to know statically whether we should accumulate gradients across a dimension due to broadcasting\r\n2) Creating zero gradient tensors for unused outputs"}