{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10290", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10290/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10290/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10290/events", "html_url": "https://github.com/pytorch/pytorch/issues/10290", "id": 348138602, "node_id": "MDU6SXNzdWUzNDgxMzg2MDI=", "number": 10290, "title": "FP16 Grouped convolution is too slow on Titan V", "user": {"login": "irishev", "id": 23737152, "node_id": "MDQ6VXNlcjIzNzM3MTUy", "avatar_url": "https://avatars0.githubusercontent.com/u/23737152?v=4", "gravatar_id": "", "url": "https://api.github.com/users/irishev", "html_url": "https://github.com/irishev", "followers_url": "https://api.github.com/users/irishev/followers", "following_url": "https://api.github.com/users/irishev/following{/other_user}", "gists_url": "https://api.github.com/users/irishev/gists{/gist_id}", "starred_url": "https://api.github.com/users/irishev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/irishev/subscriptions", "organizations_url": "https://api.github.com/users/irishev/orgs", "repos_url": "https://api.github.com/users/irishev/repos", "events_url": "https://api.github.com/users/irishev/events{/privacy}", "received_events_url": "https://api.github.com/users/irishev/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-07T01:58:57Z", "updated_at": "2018-08-07T01:58:57Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I tested a ResNeXt29_8x64d model (CIFAR-10) and found iteration on a convolution modulelist is always much faster than grouped convolution. It seems grouped convolution doesn't utilize the tensor core.</p>\n<h3>Code of grouped convolution</h3>\n<pre><code>class Block(nn.Module):\n    '''Grouped convolution block.'''\n    expansion = 4\n\n    def __init__(self, in_planes, cardinality=8, bottleneck_width=64, stride=1):\n        super(Block, self).__init__()\n        self.bwidth = bottleneck_width\n        group_width = cardinality * bottleneck_width\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(group_width)\n        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n        self.bn2 = nn.BatchNorm2d(group_width)\n        self.conv3 = nn.Conv2d(group_width, self.expansion*bottleneck_width, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*bottleneck_width)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*bottleneck_width:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*bottleneck_width, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*bottleneck_width)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n</code></pre>\n<h3>Code of modulelist iteration</h3>\n<pre><code>class Block(nn.Module):\n    '''Grouped convolution block.'''\n    expansion = 4\n\n    def __init__(self, in_planes, cardinality=8, bottleneck_width=64, stride=1):\n        super(Block, self).__init__()\n        self.bwidth = bottleneck_width\n        group_width = cardinality * bottleneck_width\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(group_width)\n        self.conv2 = nn.ModuleList([nn.Conv2d(bottleneck_width, bottleneck_width, kernel_size=3, stride=stride, padding=1, bias=False) for _ in range(cardinality)])\n        self.bn2 = nn.BatchNorm2d(group_width)\n        self.conv3 = nn.Conv2d(group_width, self.expansion*bottleneck_width, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*bottleneck_width)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*bottleneck_width:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*bottleneck_width, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*bottleneck_width)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(torch.cat([m(out[:,n*self.bwidth:(n+1)*self.bwidth,:,:]) for n,m in enumerate(self.conv2)], 1)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n</code></pre>\n<h3>Result</h3>\n<p>Grouped convolution</p>\n<p>[loss : 1.78515625]: 391/391 [03:38&lt;00:00]<br>\n[loss : 1.599609375]: 391/391 [03:29&lt;00:00]<br>\n[loss : 1.0234375]: 391/391 [03:28&lt;00:00]</p>\n<p>Modulelist iteration</p>\n<p>[loss : 1.8349609375]:  391/391 [01:53&lt;00:00]<br>\n[loss : 1.6005859375]:  391/391 [01:48&lt;00:00]<br>\n[loss : 1.5390625]: 391/391 [01:47&lt;00:00]</p>\n<h3>System Info</h3>\n<p>PyTorch version: 0.4.0a0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration: GPU 0: TITAN V<br>\nNvidia driver version: 390.77<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2</p>\n<p>Versions of relevant libraries:<br>\n[pip] msgpack-numpy (0.4.1)<br>\n[pip] numpy (1.14.2)<br>\n[pip] torch (0.4.0a0)<br>\n[pip] torchtext (0.2.1)<br>\n[pip] torchvision (0.2.0)<br>\n[conda] Could not collect</p>", "body_text": "I tested a ResNeXt29_8x64d model (CIFAR-10) and found iteration on a convolution modulelist is always much faster than grouped convolution. It seems grouped convolution doesn't utilize the tensor core.\nCode of grouped convolution\nclass Block(nn.Module):\n    '''Grouped convolution block.'''\n    expansion = 4\n\n    def __init__(self, in_planes, cardinality=8, bottleneck_width=64, stride=1):\n        super(Block, self).__init__()\n        self.bwidth = bottleneck_width\n        group_width = cardinality * bottleneck_width\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(group_width)\n        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n        self.bn2 = nn.BatchNorm2d(group_width)\n        self.conv3 = nn.Conv2d(group_width, self.expansion*bottleneck_width, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*bottleneck_width)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*bottleneck_width:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*bottleneck_width, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*bottleneck_width)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nCode of modulelist iteration\nclass Block(nn.Module):\n    '''Grouped convolution block.'''\n    expansion = 4\n\n    def __init__(self, in_planes, cardinality=8, bottleneck_width=64, stride=1):\n        super(Block, self).__init__()\n        self.bwidth = bottleneck_width\n        group_width = cardinality * bottleneck_width\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(group_width)\n        self.conv2 = nn.ModuleList([nn.Conv2d(bottleneck_width, bottleneck_width, kernel_size=3, stride=stride, padding=1, bias=False) for _ in range(cardinality)])\n        self.bn2 = nn.BatchNorm2d(group_width)\n        self.conv3 = nn.Conv2d(group_width, self.expansion*bottleneck_width, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*bottleneck_width)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*bottleneck_width:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*bottleneck_width, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*bottleneck_width)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(torch.cat([m(out[:,n*self.bwidth:(n+1)*self.bwidth,:,:]) for n,m in enumerate(self.conv2)], 1)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\nResult\nGrouped convolution\n[loss : 1.78515625]: 391/391 [03:38<00:00]\n[loss : 1.599609375]: 391/391 [03:29<00:00]\n[loss : 1.0234375]: 391/391 [03:28<00:00]\nModulelist iteration\n[loss : 1.8349609375]:  391/391 [01:53<00:00]\n[loss : 1.6005859375]:  391/391 [01:48<00:00]\n[loss : 1.5390625]: 391/391 [01:47<00:00]\nSystem Info\nPyTorch version: 0.4.0a0\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration: GPU 0: TITAN V\nNvidia driver version: 390.77\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2\nVersions of relevant libraries:\n[pip] msgpack-numpy (0.4.1)\n[pip] numpy (1.14.2)\n[pip] torch (0.4.0a0)\n[pip] torchtext (0.2.1)\n[pip] torchvision (0.2.0)\n[conda] Could not collect", "body": "I tested a ResNeXt29_8x64d model (CIFAR-10) and found iteration on a convolution modulelist is always much faster than grouped convolution. It seems grouped convolution doesn't utilize the tensor core.\r\n\r\n### Code of grouped convolution\r\n\r\n```\r\nclass Block(nn.Module):\r\n    '''Grouped convolution block.'''\r\n    expansion = 4\r\n\r\n    def __init__(self, in_planes, cardinality=8, bottleneck_width=64, stride=1):\r\n        super(Block, self).__init__()\r\n        self.bwidth = bottleneck_width\r\n        group_width = cardinality * bottleneck_width\r\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(group_width)\r\n        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(group_width)\r\n        self.conv3 = nn.Conv2d(group_width, self.expansion*bottleneck_width, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(self.expansion*bottleneck_width)\r\n\r\n        self.shortcut = nn.Sequential()\r\n        if stride != 1 or in_planes != self.expansion*bottleneck_width:\r\n            self.shortcut = nn.Sequential(\r\n                nn.Conv2d(in_planes, self.expansion*bottleneck_width, kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(self.expansion*bottleneck_width)\r\n            )\r\n\r\n    def forward(self, x):\r\n        out = F.relu(self.bn1(self.conv1(x)))\r\n        out = F.relu(self.bn2(self.conv2(out)))\r\n        out = self.bn3(self.conv3(out))\r\n        out += self.shortcut(x)\r\n        out = F.relu(out)\r\n        return out\r\n```\r\n\r\n\r\n### Code of modulelist iteration\r\n```\r\nclass Block(nn.Module):\r\n    '''Grouped convolution block.'''\r\n    expansion = 4\r\n\r\n    def __init__(self, in_planes, cardinality=8, bottleneck_width=64, stride=1):\r\n        super(Block, self).__init__()\r\n        self.bwidth = bottleneck_width\r\n        group_width = cardinality * bottleneck_width\r\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(group_width)\r\n        self.conv2 = nn.ModuleList([nn.Conv2d(bottleneck_width, bottleneck_width, kernel_size=3, stride=stride, padding=1, bias=False) for _ in range(cardinality)])\r\n        self.bn2 = nn.BatchNorm2d(group_width)\r\n        self.conv3 = nn.Conv2d(group_width, self.expansion*bottleneck_width, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(self.expansion*bottleneck_width)\r\n\r\n        self.shortcut = nn.Sequential()\r\n        if stride != 1 or in_planes != self.expansion*bottleneck_width:\r\n            self.shortcut = nn.Sequential(\r\n                nn.Conv2d(in_planes, self.expansion*bottleneck_width, kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(self.expansion*bottleneck_width)\r\n            )\r\n\r\n    def forward(self, x):\r\n        out = F.relu(self.bn1(self.conv1(x)))\r\n        out = F.relu(self.bn2(torch.cat([m(out[:,n*self.bwidth:(n+1)*self.bwidth,:,:]) for n,m in enumerate(self.conv2)], 1)))\r\n        out = self.bn3(self.conv3(out))\r\n        out += self.shortcut(x)\r\n        out = F.relu(out)\r\n        return out\r\n```\r\n\r\n### Result\r\n\r\nGrouped convolution\r\n\r\n[loss : 1.78515625]: 391/391 [03:38<00:00]\r\n[loss : 1.599609375]: 391/391 [03:29<00:00]\r\n[loss : 1.0234375]: 391/391 [03:28<00:00]\r\n\r\nModulelist iteration\r\n\r\n[loss : 1.8349609375]:  391/391 [01:53<00:00]\r\n[loss : 1.6005859375]:  391/391 [01:48<00:00]\r\n[loss : 1.5390625]: 391/391 [01:47<00:00]\r\n\r\n\r\n\r\n### System Info\r\nPyTorch version: 0.4.0a0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: TITAN V\r\nNvidia driver version: 390.77\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2\r\n\r\nVersions of relevant libraries:\r\n[pip] msgpack-numpy (0.4.1)\r\n[pip] numpy (1.14.2)\r\n[pip] torch (0.4.0a0)\r\n[pip] torchtext (0.2.1)\r\n[pip] torchvision (0.2.0)\r\n[conda] Could not collect"}