{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/438215942", "html_url": "https://github.com/pytorch/pytorch/issues/13386#issuecomment-438215942", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13386", "id": 438215942, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODIxNTk0Mg==", "user": {"login": "f0k", "id": 629706, "node_id": "MDQ6VXNlcjYyOTcwNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/629706?v=4", "gravatar_id": "", "url": "https://api.github.com/users/f0k", "html_url": "https://github.com/f0k", "followers_url": "https://api.github.com/users/f0k/followers", "following_url": "https://api.github.com/users/f0k/following{/other_user}", "gists_url": "https://api.github.com/users/f0k/gists{/gist_id}", "starred_url": "https://api.github.com/users/f0k/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/f0k/subscriptions", "organizations_url": "https://api.github.com/users/f0k/orgs", "repos_url": "https://api.github.com/users/f0k/repos", "events_url": "https://api.github.com/users/f0k/events{/privacy}", "received_events_url": "https://api.github.com/users/f0k/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-13T10:29:53Z", "updated_at": "2018-11-18T17:50:14Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>Yup... looks like we're using floating point calculation:</p>\n</blockquote>\n<p>Thanks for pointing out the source, I must admit that I was a bit lost.</p>\n<p>For the GPU, the conversion to float is placed slightly differently, that's why the results do not always match up:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/26a8bb62ee359d3fa444dc55133f11a4a42f0479/aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu#L48-L53\">pytorch/aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 48 to 53\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/26a8bb62ee359d3fa444dc55133f11a4a42f0479\">26a8bb6</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L48\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"48\"></td>\n          <td id=\"LC48\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   nOutputCols = <span class=\"pl-c1\">ceil</span>(<span class=\"pl-c1\">float</span>(nInputCols - (dilationW * (<span class=\"pl-c1\">kW</span> - <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">2</span>*padW) / <span class=\"pl-c1\">float</span>(dW)) + <span class=\"pl-c1\">1</span>; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L49\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"49\"></td>\n          <td id=\"LC49\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   nOutputRows = <span class=\"pl-c1\">ceil</span>(<span class=\"pl-c1\">float</span>(nInputRows - (dilationH * (<span class=\"pl-c1\">kH</span> - <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">2</span>*padH) / <span class=\"pl-c1\">float</span>(dH)) + <span class=\"pl-c1\">1</span>; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L50\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"50\"></td>\n          <td id=\"LC50\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> } </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L51\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"51\"></td>\n          <td id=\"LC51\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">else</span> { </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L52\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"52\"></td>\n          <td id=\"LC52\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   nOutputCols = <span class=\"pl-c1\">floor</span>(<span class=\"pl-c1\">float</span>(nInputCols - (dilationW * (<span class=\"pl-c1\">kW</span> - <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">2</span>*padW) / <span class=\"pl-c1\">float</span>(dW)) + <span class=\"pl-c1\">1</span>; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L53\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"53\"></td>\n          <td id=\"LC53\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   nOutputRows = <span class=\"pl-c1\">floor</span>(<span class=\"pl-c1\">float</span>(nInputRows - (dilationH * (<span class=\"pl-c1\">kH</span> - <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">2</span>*padH) / <span class=\"pl-c1\">float</span>(dH)) + <span class=\"pl-c1\">1</span>; </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<blockquote>\n<p>This problem also occurs for <code>avg_pool1d</code>:</p>\n</blockquote>\n<p>All pooling operations will be affected, because somebody thought that's the way to distinguish the <code>ceil</code>/<code>floor</code> cases.</p>\n<p>Convolution does not have a <code>ceil</code> mode, so it's done in int64 as it should:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/26a8bb62ee359d3fa444dc55133f11a4a42f0479/aten/src/THNN/generic/SpatialDilatedConvolution.c#L243-L244\">pytorch/aten/src/THNN/generic/SpatialDilatedConvolution.c</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 243 to 244\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/26a8bb62ee359d3fa444dc55133f11a4a42f0479\">26a8bb6</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L243\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"243\"></td>\n          <td id=\"LC243\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">int64_t</span> outputWidth  = (inputWidth + <span class=\"pl-c1\">2</span>*padW - (dilationW * (<span class=\"pl-c1\">kW</span> - <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">1</span>)) / dW + <span class=\"pl-c1\">1</span>; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L244\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"244\"></td>\n          <td id=\"LC244\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">int64_t</span> outputHeight = (inputHeight + <span class=\"pl-c1\">2</span>*padH - (dilationH * (<span class=\"pl-c1\">kH</span> - <span class=\"pl-c1\">1</span>) + <span class=\"pl-c1\">1</span>)) / dH + <span class=\"pl-c1\">1</span>; </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>So the actual fix will be easy, I'd be happy to do it, something like:</p>\n<div class=\"highlight highlight-source-c\"><pre><span class=\"pl-c1\">int64_t</span> outp = (inp + <span class=\"pl-c1\">2</span>*pad - dilation * (kernel - <span class=\"pl-c1\">1</span>) - <span class=\"pl-c1\">1</span> + (ceilmode ? stride - <span class=\"pl-c1\">1</span> : <span class=\"pl-c1\">0</span>)) / stride + <span class=\"pl-c1\">1</span>;</pre></div>\n<p>But I also need to learn where to add a corresponding test.</p>", "body_text": "Yup... looks like we're using floating point calculation:\n\nThanks for pointing out the source, I must admit that I was a bit lost.\nFor the GPU, the conversion to float is placed slightly differently, that's why the results do not always match up:\n\n  \n    \n      pytorch/aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu\n    \n    \n        Lines 48 to 53\n      in\n      26a8bb6\n    \n    \n    \n    \n\n        \n          \n             nOutputCols = ceil(float(nInputCols - (dilationW * (kW - 1) + 1) + 2*padW) / float(dW)) + 1; \n        \n\n        \n          \n             nOutputRows = ceil(float(nInputRows - (dilationH * (kH - 1) + 1) + 2*padH) / float(dH)) + 1; \n        \n\n        \n          \n           } \n        \n\n        \n          \n           else { \n        \n\n        \n          \n             nOutputCols = floor(float(nInputCols - (dilationW * (kW - 1) + 1) + 2*padW) / float(dW)) + 1; \n        \n\n        \n          \n             nOutputRows = floor(float(nInputRows - (dilationH * (kH - 1) + 1) + 2*padH) / float(dH)) + 1; \n        \n    \n  \n\n\n\nThis problem also occurs for avg_pool1d:\n\nAll pooling operations will be affected, because somebody thought that's the way to distinguish the ceil/floor cases.\nConvolution does not have a ceil mode, so it's done in int64 as it should:\n\n  \n    \n      pytorch/aten/src/THNN/generic/SpatialDilatedConvolution.c\n    \n    \n        Lines 243 to 244\n      in\n      26a8bb6\n    \n    \n    \n    \n\n        \n          \n           int64_t outputWidth  = (inputWidth + 2*padW - (dilationW * (kW - 1) + 1)) / dW + 1; \n        \n\n        \n          \n           int64_t outputHeight = (inputHeight + 2*padH - (dilationH * (kH - 1) + 1)) / dH + 1; \n        \n    \n  \n\n\nSo the actual fix will be easy, I'd be happy to do it, something like:\nint64_t outp = (inp + 2*pad - dilation * (kernel - 1) - 1 + (ceilmode ? stride - 1 : 0)) / stride + 1;\nBut I also need to learn where to add a corresponding test.", "body": "> Yup... looks like we're using floating point calculation:\r\n\r\nThanks for pointing out the source, I must admit that I was a bit lost.\r\n\r\nFor the GPU, the conversion to float is placed slightly differently, that's why the results do not always match up:\r\nhttps://github.com/pytorch/pytorch/blob/26a8bb62ee359d3fa444dc55133f11a4a42f0479/aten/src/THCUNN/generic/SpatialDilatedMaxPooling.cu#L48-L53\r\n\r\n> This problem also occurs for `avg_pool1d`:\r\n\r\nAll pooling operations will be affected, because somebody thought that's the way to distinguish the `ceil`/`floor` cases.\r\n\r\nConvolution does not have a `ceil` mode, so it's done in int64 as it should:\r\nhttps://github.com/pytorch/pytorch/blob/26a8bb62ee359d3fa444dc55133f11a4a42f0479/aten/src/THNN/generic/SpatialDilatedConvolution.c#L243-L244\r\n\r\nSo the actual fix will be easy, I'd be happy to do it, something like:\r\n```C\r\nint64_t outp = (inp + 2*pad - dilation * (kernel - 1) - 1 + (ceilmode ? stride - 1 : 0)) / stride + 1;\r\n```\r\nBut I also need to learn where to add a corresponding test."}