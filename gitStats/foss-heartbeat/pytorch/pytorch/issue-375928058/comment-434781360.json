{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434781360", "html_url": "https://github.com/pytorch/pytorch/issues/13386#issuecomment-434781360", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13386", "id": 434781360, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDc4MTM2MA==", "user": {"login": "f0k", "id": 629706, "node_id": "MDQ6VXNlcjYyOTcwNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/629706?v=4", "gravatar_id": "", "url": "https://api.github.com/users/f0k", "html_url": "https://github.com/f0k", "followers_url": "https://api.github.com/users/f0k/followers", "following_url": "https://api.github.com/users/f0k/following{/other_user}", "gists_url": "https://api.github.com/users/f0k/gists{/gist_id}", "starred_url": "https://api.github.com/users/f0k/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/f0k/subscriptions", "organizations_url": "https://api.github.com/users/f0k/orgs", "repos_url": "https://api.github.com/users/f0k/repos", "events_url": "https://api.github.com/users/f0k/events{/privacy}", "received_events_url": "https://api.github.com/users/f0k/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-31T17:39:52Z", "updated_at": "2018-10-31T17:39:52Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Follow-up</h2>\n<p>It's the same with <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/e475d3ede37cfec2892cf7e94a3109b6ec1b3cde/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/e475d3ede37cfec2892cf7e94a3109b6ec1b3cde\"><tt>e475d3e</tt></a>. And I have a horrifying suspicion what you're doing:</p>\n<pre><code>In [1]: inp = 19199998\nIn [2]: pad = 1921//2\nIn [3]: dil = 1\nIn [4]: krn = 1921\nIn [5]: std = 1\nIn [6]: one = 1\nIn [7]: two = 2\nIn [8]: (inp + two*pad - dil*(krn-one) - one) // std + one\nOut[8]: 19199998\nIn [9]: inp, pad, dil, krn, std, one, two = map(np.float32, (inp, pad, dil, krn, std, one, two))\nIn [10]: (inp + two*pad - dil*(krn-one) - one) // std + one\nOut[10]: 19199996.0\n</code></pre>\n<p>So my theory is you're using floating point for computing the output shape. This wouldn't explain the CPU/GPU difference, but it would explain why the size can be off in both directions, and only for large inputs.</p>\n<p>This should be a blocker for 1.0.</p>", "body_text": "Follow-up\nIt's the same with e475d3e. And I have a horrifying suspicion what you're doing:\nIn [1]: inp = 19199998\nIn [2]: pad = 1921//2\nIn [3]: dil = 1\nIn [4]: krn = 1921\nIn [5]: std = 1\nIn [6]: one = 1\nIn [7]: two = 2\nIn [8]: (inp + two*pad - dil*(krn-one) - one) // std + one\nOut[8]: 19199998\nIn [9]: inp, pad, dil, krn, std, one, two = map(np.float32, (inp, pad, dil, krn, std, one, two))\nIn [10]: (inp + two*pad - dil*(krn-one) - one) // std + one\nOut[10]: 19199996.0\n\nSo my theory is you're using floating point for computing the output shape. This wouldn't explain the CPU/GPU difference, but it would explain why the size can be off in both directions, and only for large inputs.\nThis should be a blocker for 1.0.", "body": "## Follow-up\r\n\r\nIt's the same with e475d3e. And I have a horrifying suspicion what you're doing:\r\n```\r\nIn [1]: inp = 19199998\r\nIn [2]: pad = 1921//2\r\nIn [3]: dil = 1\r\nIn [4]: krn = 1921\r\nIn [5]: std = 1\r\nIn [6]: one = 1\r\nIn [7]: two = 2\r\nIn [8]: (inp + two*pad - dil*(krn-one) - one) // std + one\r\nOut[8]: 19199998\r\nIn [9]: inp, pad, dil, krn, std, one, two = map(np.float32, (inp, pad, dil, krn, std, one, two))\r\nIn [10]: (inp + two*pad - dil*(krn-one) - one) // std + one\r\nOut[10]: 19199996.0\r\n```\r\nSo my theory is you're using floating point for computing the output shape. This wouldn't explain the CPU/GPU difference, but it would explain why the size can be off in both directions, and only for large inputs.\r\n\r\nThis should be a blocker for 1.0."}