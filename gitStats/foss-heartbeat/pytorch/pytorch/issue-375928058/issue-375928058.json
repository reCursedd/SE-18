{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13386", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13386/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13386/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13386/events", "html_url": "https://github.com/pytorch/pytorch/issues/13386", "id": 375928058, "node_id": "MDU6SXNzdWUzNzU5MjgwNTg=", "number": 13386, "title": "Incorrect output shape for max-pooling", "user": {"login": "f0k", "id": 629706, "node_id": "MDQ6VXNlcjYyOTcwNg==", "avatar_url": "https://avatars3.githubusercontent.com/u/629706?v=4", "gravatar_id": "", "url": "https://api.github.com/users/f0k", "html_url": "https://github.com/f0k", "followers_url": "https://api.github.com/users/f0k/followers", "following_url": "https://api.github.com/users/f0k/following{/other_user}", "gists_url": "https://api.github.com/users/f0k/gists{/gist_id}", "starred_url": "https://api.github.com/users/f0k/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/f0k/subscriptions", "organizations_url": "https://api.github.com/users/f0k/orgs", "repos_url": "https://api.github.com/users/f0k/repos", "events_url": "https://api.github.com/users/f0k/events{/privacy}", "received_events_url": "https://api.github.com/users/f0k/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "open", "locked": false, "assignee": {"login": "nairbv", "id": 582713, "node_id": "MDQ6VXNlcjU4MjcxMw==", "avatar_url": "https://avatars1.githubusercontent.com/u/582713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairbv", "html_url": "https://github.com/nairbv", "followers_url": "https://api.github.com/users/nairbv/followers", "following_url": "https://api.github.com/users/nairbv/following{/other_user}", "gists_url": "https://api.github.com/users/nairbv/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairbv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairbv/subscriptions", "organizations_url": "https://api.github.com/users/nairbv/orgs", "repos_url": "https://api.github.com/users/nairbv/repos", "events_url": "https://api.github.com/users/nairbv/events{/privacy}", "received_events_url": "https://api.github.com/users/nairbv/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "nairbv", "id": 582713, "node_id": "MDQ6VXNlcjU4MjcxMw==", "avatar_url": "https://avatars1.githubusercontent.com/u/582713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairbv", "html_url": "https://github.com/nairbv", "followers_url": "https://api.github.com/users/nairbv/followers", "following_url": "https://api.github.com/users/nairbv/following{/other_user}", "gists_url": "https://api.github.com/users/nairbv/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairbv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairbv/subscriptions", "organizations_url": "https://api.github.com/users/nairbv/orgs", "repos_url": "https://api.github.com/users/nairbv/repos", "events_url": "https://api.github.com/users/nairbv/events{/privacy}", "received_events_url": "https://api.github.com/users/nairbv/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-10-31T11:59:41Z", "updated_at": "2018-11-18T17:50:14Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>For some combinations of input size, pooling size and padding, the output size of <code>max_pool1d</code> is wrong. Furthermore, it is different between CPU and CUDA.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<p>On GPU:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nx <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">19200000</span> <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float32, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda:0<span class=\"pl-pds\">'</span></span>)\nwidth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1921</span>\ny <span class=\"pl-k\">=</span> torch.nn.functional.max_pool1d(x[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>], width, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span>width<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>)[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]\n<span class=\"pl-c1\">print</span>(x.shape)\n<span class=\"pl-c1\">print</span>(y.shape)</pre></div>\n<p>I get:</p>\n<pre><code>torch.Size([19199998])\ntorch.Size([19199996])\n</code></pre>\n<p>On CPU:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nx <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">19200000</span> <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float32, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>)\nwidth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1921</span>\ny <span class=\"pl-k\">=</span> torch.nn.functional.max_pool1d(x[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>], width, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span>width<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>)[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]\n<span class=\"pl-c1\">print</span>(x.shape)\n<span class=\"pl-c1\">print</span>(y.shape)</pre></div>\n<p>I get:</p>\n<pre><code>torch.Size([19199998])\ntorch.Size([19199997])\n</code></pre>\n<p>For an input length of <code>19200000</code>, I get an output length of <code>19200000</code> on GPU and <code>19200001</code> on CPU.</p>\n<h2>Expected behavior</h2>\n<p>In all cases, the output shape should match the input shape -- it's an uneven pooling window with stride 1 and matching symmetric padding. Also the output shape should be the same regardless of the device. Note that it works fine on both CUDA and CPU for an input length of <code>1920000 - 2</code> (one order of magnitude smaller).</p>\n<h2>Environment</h2>\n<p>I can reproduce this both with a precompiled PyTorch 0.4.1 and a self-compiled PyTorch 1.0.</p>\n<p>Pre-compiled:</p>\n<pre><code>Collecting environment information...\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: \nGPU 0: TITAN X (Pascal)\nGPU 1: TITAN X (Pascal)\n\nNvidia driver version: 390.30\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n</code></pre>\n<p>Self-compiled:</p>\n<pre><code>PyTorch version: 1.0.0a0+710191e\nIs debug build: No\nCUDA used to build PyTorch: 10.0.130\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: 10.0.130\nGPU models and configuration: GPU 0: GeForce GTX 1060\nNvidia driver version: 410.57\ncuDNN version: Probably one of the following:\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn_static.a\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n</code></pre>\n<h2>Additional context</h2>\n<p>For what it's worth, it works fine using Lasagne/Theano both on CPU and CUDA.</p>", "body_text": "\ud83d\udc1b Bug\nFor some combinations of input size, pooling size and padding, the output size of max_pool1d is wrong. Furthermore, it is different between CPU and CUDA.\nTo Reproduce\nSteps to reproduce the behavior:\nOn GPU:\nimport torch\nx = torch.rand(19200000 - 2, dtype=torch.float32, device='cuda:0')\nwidth = 1921\ny = torch.nn.functional.max_pool1d(x[None, None], width, stride=1, padding=width//2)[0, 0]\nprint(x.shape)\nprint(y.shape)\nI get:\ntorch.Size([19199998])\ntorch.Size([19199996])\n\nOn CPU:\nimport torch\nx = torch.rand(19200000 - 2, dtype=torch.float32, device='cpu')\nwidth = 1921\ny = torch.nn.functional.max_pool1d(x[None, None], width, stride=1, padding=width//2)[0, 0]\nprint(x.shape)\nprint(y.shape)\nI get:\ntorch.Size([19199998])\ntorch.Size([19199997])\n\nFor an input length of 19200000, I get an output length of 19200000 on GPU and 19200001 on CPU.\nExpected behavior\nIn all cases, the output shape should match the input shape -- it's an uneven pooling window with stride 1 and matching symmetric padding. Also the output shape should be the same regardless of the device. Note that it works fine on both CUDA and CPU for an input length of 1920000 - 2 (one order of magnitude smaller).\nEnvironment\nI can reproduce this both with a precompiled PyTorch 0.4.1 and a self-compiled PyTorch 1.0.\nPre-compiled:\nCollecting environment information...\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: \nGPU 0: TITAN X (Pascal)\nGPU 1: TITAN X (Pascal)\n\nNvidia driver version: 390.30\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n\nSelf-compiled:\nPyTorch version: 1.0.0a0+710191e\nIs debug build: No\nCUDA used to build PyTorch: 10.0.130\n\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\n\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: 10.0.130\nGPU models and configuration: GPU 0: GeForce GTX 1060\nNvidia driver version: 410.57\ncuDNN version: Probably one of the following:\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn_static.a\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] Could not collect\n\nAdditional context\nFor what it's worth, it works fine using Lasagne/Theano both on CPU and CUDA.", "body": "## \ud83d\udc1b Bug\r\n\r\nFor some combinations of input size, pooling size and padding, the output size of `max_pool1d` is wrong. Furthermore, it is different between CPU and CUDA.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nOn GPU:\r\n```python\r\nimport torch\r\nx = torch.rand(19200000 - 2, dtype=torch.float32, device='cuda:0')\r\nwidth = 1921\r\ny = torch.nn.functional.max_pool1d(x[None, None], width, stride=1, padding=width//2)[0, 0]\r\nprint(x.shape)\r\nprint(y.shape)\r\n```\r\nI get:\r\n```\r\ntorch.Size([19199998])\r\ntorch.Size([19199996])\r\n```\r\n\r\nOn CPU:\r\n```python\r\nimport torch\r\nx = torch.rand(19200000 - 2, dtype=torch.float32, device='cpu')\r\nwidth = 1921\r\ny = torch.nn.functional.max_pool1d(x[None, None], width, stride=1, padding=width//2)[0, 0]\r\nprint(x.shape)\r\nprint(y.shape)\r\n```\r\nI get:\r\n```\r\ntorch.Size([19199998])\r\ntorch.Size([19199997])\r\n```\r\n\r\nFor an input length of `19200000`, I get an output length of `19200000` on GPU and `19200001` on CPU.\r\n\r\n## Expected behavior\r\n\r\nIn all cases, the output shape should match the input shape -- it's an uneven pooling window with stride 1 and matching symmetric padding. Also the output shape should be the same regardless of the device. Note that it works fine on both CUDA and CPU for an input length of `1920000 - 2` (one order of magnitude smaller).\r\n\r\n## Environment\r\n\r\nI can reproduce this both with a precompiled PyTorch 0.4.1 and a self-compiled PyTorch 1.0.\r\n\r\nPre-compiled:\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\n\r\nNvidia driver version: 390.30\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\nSelf-compiled:\r\n```\r\nPyTorch version: 1.0.0a0+710191e\r\nIs debug build: No\r\nCUDA used to build PyTorch: 10.0.130\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 10.0.130\r\nGPU models and configuration: GPU 0: GeForce GTX 1060\r\nNvidia driver version: 410.57\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn.so.7.3.1\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn.so.7.1.4\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] Could not collect\r\n```\r\n\r\n## Additional context\r\n\r\nFor what it's worth, it works fine using Lasagne/Theano both on CPU and CUDA."}