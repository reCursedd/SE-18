{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2809", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2809/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2809/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2809/events", "html_url": "https://github.com/pytorch/pytorch/issues/2809", "id": 259297551, "node_id": "MDU6SXNzdWUyNTkyOTc1NTE=", "number": 2809, "title": "Feature Request: More general learning-rate scheduling", "user": {"login": "rdipietro", "id": 5150559, "node_id": "MDQ6VXNlcjUxNTA1NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5150559?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rdipietro", "html_url": "https://github.com/rdipietro", "followers_url": "https://api.github.com/users/rdipietro/followers", "following_url": "https://api.github.com/users/rdipietro/following{/other_user}", "gists_url": "https://api.github.com/users/rdipietro/gists{/gist_id}", "starred_url": "https://api.github.com/users/rdipietro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rdipietro/subscriptions", "organizations_url": "https://api.github.com/users/rdipietro/orgs", "repos_url": "https://api.github.com/users/rdipietro/repos", "events_url": "https://api.github.com/users/rdipietro/events{/privacy}", "received_events_url": "https://api.github.com/users/rdipietro/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-20T20:49:59Z", "updated_at": "2017-09-29T03:38:24Z", "closed_at": "2017-09-29T03:38:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi,</p>\n<p>I recently learned of <code>torch.optim.lr_scheduler</code>.</p>\n<p>First a comment: Despite how common it is to tweak learning rates during training, this package isn't easy to find. Would it be worth importing in <code>torch.optim.__init__.py</code> so that (for example) <code>torch.optim.&lt;tab&gt;</code> in most IDEs would make this common behavior more visible?</p>\n<p>Feature request: <code>LambdaLR</code> currently has strange behavior: it accepts a function <code>f(step)</code> which returns a learning-rate multiplicative factor as a function of <code>step</code>. Given the name <code>LambdaLR</code>, it seems that it should instead accept a function <code>f(step)</code> which simply computes the learning rate itself as a function of <code>step</code>. Thoughts?</p>\n<p>Or, if a bug-inducing change like that is too much, maybe we should just introduce <code>NonMultiplicativeLambdaLR</code> or <code>GeneralLambdaLR</code> instead?</p>", "body_text": "Hi,\nI recently learned of torch.optim.lr_scheduler.\nFirst a comment: Despite how common it is to tweak learning rates during training, this package isn't easy to find. Would it be worth importing in torch.optim.__init__.py so that (for example) torch.optim.<tab> in most IDEs would make this common behavior more visible?\nFeature request: LambdaLR currently has strange behavior: it accepts a function f(step) which returns a learning-rate multiplicative factor as a function of step. Given the name LambdaLR, it seems that it should instead accept a function f(step) which simply computes the learning rate itself as a function of step. Thoughts?\nOr, if a bug-inducing change like that is too much, maybe we should just introduce NonMultiplicativeLambdaLR or GeneralLambdaLR instead?", "body": "Hi,\r\n\r\nI recently learned of `torch.optim.lr_scheduler`.\r\n\r\nFirst a comment: Despite how common it is to tweak learning rates during training, this package isn't easy to find. Would it be worth importing in `torch.optim.__init__.py` so that (for example) `torch.optim.<tab>` in most IDEs would make this common behavior more visible?\r\n\r\nFeature request: `LambdaLR` currently has strange behavior: it accepts a function `f(step)` which returns a learning-rate multiplicative factor as a function of `step`. Given the name `LambdaLR`, it seems that it should instead accept a function `f(step)` which simply computes the learning rate itself as a function of `step`. Thoughts?\r\n\r\nOr, if a bug-inducing change like that is too much, maybe we should just introduce `NonMultiplicativeLambdaLR` or `GeneralLambdaLR` instead?"}