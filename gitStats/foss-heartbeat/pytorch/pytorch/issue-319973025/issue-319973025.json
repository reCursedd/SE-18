{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7243", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7243/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7243/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7243/events", "html_url": "https://github.com/pytorch/pytorch/issues/7243", "id": 319973025, "node_id": "MDU6SXNzdWUzMTk5NzMwMjU=", "number": 7243, "title": "`autograd` seems have a problem in new version pytorch(0.4.0)", "user": {"login": "huguyuehuhu", "id": 25881545, "node_id": "MDQ6VXNlcjI1ODgxNTQ1", "avatar_url": "https://avatars2.githubusercontent.com/u/25881545?v=4", "gravatar_id": "", "url": "https://api.github.com/users/huguyuehuhu", "html_url": "https://github.com/huguyuehuhu", "followers_url": "https://api.github.com/users/huguyuehuhu/followers", "following_url": "https://api.github.com/users/huguyuehuhu/following{/other_user}", "gists_url": "https://api.github.com/users/huguyuehuhu/gists{/gist_id}", "starred_url": "https://api.github.com/users/huguyuehuhu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/huguyuehuhu/subscriptions", "organizations_url": "https://api.github.com/users/huguyuehuhu/orgs", "repos_url": "https://api.github.com/users/huguyuehuhu/repos", "events_url": "https://api.github.com/users/huguyuehuhu/events{/privacy}", "received_events_url": "https://api.github.com/users/huguyuehuhu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-03T15:35:16Z", "updated_at": "2018-05-04T01:30:51Z", "closed_at": "2018-05-04T01:30:51Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>The <code>autograd</code> seems have a problem in new version pytorch(0.4.0), but I don't sure. Because it okay in  pytorch(0.3.0); for details:<br>\nIf  when use GPU( <code>cuda = True</code>) in following  code, it will cause an  error: <code>one of the variables needed for gradient computation has been modified by an inplace operation</code>. but don't use GPU\uff08<code>cuda = False</code> \uff09has no error. Also no error in pytorch(0.3.0) for both cases.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">test_model</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,\n                 <span class=\"pl-smi\">in_channel</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>,\n                 <span class=\"pl-smi\">num_class</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">60</span>,\n                 <span class=\"pl-smi\">num_joint</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">25</span>,\n                 <span class=\"pl-smi\">hidden_size</span> <span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>,\n                 <span class=\"pl-smi\">window_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>,\n                 ):\n        <span class=\"pl-c1\">super</span>(test_model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.lstm <span class=\"pl-k\">=</span> nn.LSTM(<span class=\"pl-v\">input_size</span><span class=\"pl-k\">=</span>in_channel <span class=\"pl-k\">*</span> num_joint,\n                             <span class=\"pl-v\">hidden_size</span><span class=\"pl-k\">=</span>hidden_size, <span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>,\n                             <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.fc <span class=\"pl-k\">=</span> nn.Linear(hidden_size<span class=\"pl-k\">*</span>window_size, num_class)\n\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        N, C, T, V, M <span class=\"pl-k\">=</span> x.size()\n        logits <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):\n            out <span class=\"pl-k\">=</span> x[:, :, :, :, i].permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>).contiguous().view(N,T,C<span class=\"pl-k\">*</span>V)\n            <span class=\"pl-c1\">self</span>.lstm.flatten_parameters()\n            out,_ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lstm(out)\n            logits.append(out)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> out = torch.max(logits[0], logits[1])</span>\n        out <span class=\"pl-k\">=</span>  logits[<span class=\"pl-c1\">1</span>]<span class=\"pl-k\">+</span>logits[<span class=\"pl-c1\">0</span>]\n        out <span class=\"pl-k\">=</span> out.contiguous().view(out.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc(out)\n        <span class=\"pl-k\">return</span> out\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> cronstruct a dataset</span>\n    data <span class=\"pl-k\">=</span> []\n    label <span class=\"pl-k\">=</span> []\n    epoches <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    cuda <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> cuda = True</span>\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(epoches):\n        data.append(torch.randn((<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">25</span>, <span class=\"pl-c1\">2</span>)))\n        label.append(torch.randn((<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">60</span>)))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> model</span>\n    model <span class=\"pl-k\">=</span> test_model()\n    <span class=\"pl-k\">if</span> cuda:\n        model<span class=\"pl-k\">=</span> model.cuda()\n    optimizer <span class=\"pl-k\">=</span> optim.Adam(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> cre = nn.CrossEntropyLoss()</span>\n    cre <span class=\"pl-k\">=</span> nn.MSELoss()\n\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(epoches):\n        <span class=\"pl-k\">if</span> cuda:\n            x <span class=\"pl-k\">=</span> Variable(data[i]).cuda()\n            y <span class=\"pl-k\">=</span> Variable(label[i]).cuda()\n        <span class=\"pl-k\">else</span>:\n            x <span class=\"pl-k\">=</span> Variable(data[i])\n            y <span class=\"pl-k\">=</span> Variable(label[i])\n\n        out_put <span class=\"pl-k\">=</span> model(x)\n        loss <span class=\"pl-k\">=</span> cre(out_put, y)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> print(loss)</span>\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n     <span class=\"pl-c\"><span class=\"pl-c\">#</span>print('end')</span></pre></div>", "body_text": "Issue description\nThe autograd seems have a problem in new version pytorch(0.4.0), but I don't sure. Because it okay in  pytorch(0.3.0); for details:\nIf  when use GPU( cuda = True) in following  code, it will cause an  error: one of the variables needed for gradient computation has been modified by an inplace operation. but don't use GPU\uff08cuda = False \uff09has no error. Also no error in pytorch(0.3.0) for both cases.\nCode example\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nclass test_model(nn.Module):\n    def __init__(self,\n                 in_channel=3,\n                 num_class=60,\n                 num_joint=25,\n                 hidden_size =100,\n                 window_size=64,\n                 ):\n        super(test_model, self).__init__()\n        self.lstm = nn.LSTM(input_size=in_channel * num_joint,\n                             hidden_size=hidden_size, num_layers=3,\n                             batch_first=True)\n        self.fc = nn.Linear(hidden_size*window_size, num_class)\n\n\n    def forward(self, x):\n        N, C, T, V, M = x.size()\n        logits = []\n        for i in range(2):\n            out = x[:, :, :, :, i].permute(0, 2, 1, 3).contiguous().view(N,T,C*V)\n            self.lstm.flatten_parameters()\n            out,_ = self.lstm(out)\n            logits.append(out)\n\n        # out = torch.max(logits[0], logits[1])\n        out =  logits[1]+logits[0]\n        out = out.contiguous().view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\nif __name__ == '__main__':\n    # cronstruct a dataset\n    data = []\n    label = []\n    epoches = 10\n    cuda = False\n    # cuda = True\n    for i in range(epoches):\n        data.append(torch.randn((2, 3, 64, 25, 2)))\n        label.append(torch.randn((2, 60)))\n\n    # model\n    model = test_model()\n    if cuda:\n        model= model.cuda()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # cre = nn.CrossEntropyLoss()\n    cre = nn.MSELoss()\n\n    for i in range(epoches):\n        if cuda:\n            x = Variable(data[i]).cuda()\n            y = Variable(label[i]).cuda()\n        else:\n            x = Variable(data[i])\n            y = Variable(label[i])\n\n        out_put = model(x)\n        loss = cre(out_put, y)\n        # print(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n     #print('end')", "body": "## Issue description\r\nThe `autograd` seems have a problem in new version pytorch(0.4.0), but I don't sure. Because it okay in  pytorch(0.3.0); for details:\r\nIf  when use GPU( `cuda = True`) in following  code, it will cause an  error: `one of the variables needed for gradient computation has been modified by an inplace operation`. but don't use GPU\uff08`cuda = False` \uff09has no error. Also no error in pytorch(0.3.0) for both cases.\r\n## Code example\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\n\r\nclass test_model(nn.Module):\r\n    def __init__(self,\r\n                 in_channel=3,\r\n                 num_class=60,\r\n                 num_joint=25,\r\n                 hidden_size =100,\r\n                 window_size=64,\r\n                 ):\r\n        super(test_model, self).__init__()\r\n        self.lstm = nn.LSTM(input_size=in_channel * num_joint,\r\n                             hidden_size=hidden_size, num_layers=3,\r\n                             batch_first=True)\r\n        self.fc = nn.Linear(hidden_size*window_size, num_class)\r\n\r\n\r\n    def forward(self, x):\r\n        N, C, T, V, M = x.size()\r\n        logits = []\r\n        for i in range(2):\r\n            out = x[:, :, :, :, i].permute(0, 2, 1, 3).contiguous().view(N,T,C*V)\r\n            self.lstm.flatten_parameters()\r\n            out,_ = self.lstm(out)\r\n            logits.append(out)\r\n\r\n        # out = torch.max(logits[0], logits[1])\r\n        out =  logits[1]+logits[0]\r\n        out = out.contiguous().view(out.size(0), -1)\r\n        out = self.fc(out)\r\n        return out\r\n\r\nif __name__ == '__main__':\r\n    # cronstruct a dataset\r\n    data = []\r\n    label = []\r\n    epoches = 10\r\n    cuda = False\r\n    # cuda = True\r\n    for i in range(epoches):\r\n        data.append(torch.randn((2, 3, 64, 25, 2)))\r\n        label.append(torch.randn((2, 60)))\r\n\r\n    # model\r\n    model = test_model()\r\n    if cuda:\r\n        model= model.cuda()\r\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n\r\n    # cre = nn.CrossEntropyLoss()\r\n    cre = nn.MSELoss()\r\n\r\n    for i in range(epoches):\r\n        if cuda:\r\n            x = Variable(data[i]).cuda()\r\n            y = Variable(label[i]).cuda()\r\n        else:\r\n            x = Variable(data[i])\r\n            y = Variable(label[i])\r\n\r\n        out_put = model(x)\r\n        loss = cre(out_put, y)\r\n        # print(loss)\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n     #print('end')\r\n```\r\n"}