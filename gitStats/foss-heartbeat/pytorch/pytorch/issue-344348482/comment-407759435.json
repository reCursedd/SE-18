{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/407759435", "html_url": "https://github.com/pytorch/pytorch/issues/9811#issuecomment-407759435", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9811", "id": 407759435, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzc1OTQzNQ==", "user": {"login": "sharpsy", "id": 7761488, "node_id": "MDQ6VXNlcjc3NjE0ODg=", "avatar_url": "https://avatars2.githubusercontent.com/u/7761488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sharpsy", "html_url": "https://github.com/sharpsy", "followers_url": "https://api.github.com/users/sharpsy/followers", "following_url": "https://api.github.com/users/sharpsy/following{/other_user}", "gists_url": "https://api.github.com/users/sharpsy/gists{/gist_id}", "starred_url": "https://api.github.com/users/sharpsy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sharpsy/subscriptions", "organizations_url": "https://api.github.com/users/sharpsy/orgs", "repos_url": "https://api.github.com/users/sharpsy/repos", "events_url": "https://api.github.com/users/sharpsy/events{/privacy}", "received_events_url": "https://api.github.com/users/sharpsy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-25T13:46:12Z", "updated_at": "2018-07-25T13:46:12Z", "author_association": "NONE", "body_html": "<p>It is true, the loss computation could increase the training time. My hunch is that loss calculation is usually the fast part of the model training. I'm thinking of all other computation that takes place besides the loss calculation - forward pass, gradient calculation, weight updates...</p>\n<p>To check this, I just benchmarked <code>CrossEntropy</code> on 10000 items and 10000 classes and loss computation took 2.16ms. I used the following benchmark:<br>\n<code>python3 -m timeit -s 'import torch as t; dev=\"cuda\"; loss=t.nn.CrossEntropyLoss().to(dev); inp=t.randn(10000, 10000).to(dev); tgt=t.empty(10000, dtype=t.long).random_(10000).to(dev)' 'loss(inp, tgt)'</code></p>\n<p>It would seem that this is unlikely to observably influence the training time.</p>\n<p>Even if it does, implementation that distributes the loss computation over the GPUs and then calculates the mean of the returned vector is considerably slower.<br>\nTry it:<br>\n<code>python3 -m timeit -s 'import torch as t; dev=\"cuda\"; loss=t.nn.DataParallel(t.nn.CrossEntropyLoss(reduce=False)).to(dev); inp=t.randn(10000, 10000, requires_grad=True).to(dev); tgt=t.empty(10000, dtype=t.long).random_(10000).to(dev)' 't.mean(loss(inp, tgt))'</code></p>\n<p>Regardless of the performance impact, the <code>DataParallel</code> has a nice property that the <code>model(input)</code> will be equal to <code>DataParallel(model)(input)</code>. The proposed change would break this property.</p>", "body_text": "It is true, the loss computation could increase the training time. My hunch is that loss calculation is usually the fast part of the model training. I'm thinking of all other computation that takes place besides the loss calculation - forward pass, gradient calculation, weight updates...\nTo check this, I just benchmarked CrossEntropy on 10000 items and 10000 classes and loss computation took 2.16ms. I used the following benchmark:\npython3 -m timeit -s 'import torch as t; dev=\"cuda\"; loss=t.nn.CrossEntropyLoss().to(dev); inp=t.randn(10000, 10000).to(dev); tgt=t.empty(10000, dtype=t.long).random_(10000).to(dev)' 'loss(inp, tgt)'\nIt would seem that this is unlikely to observably influence the training time.\nEven if it does, implementation that distributes the loss computation over the GPUs and then calculates the mean of the returned vector is considerably slower.\nTry it:\npython3 -m timeit -s 'import torch as t; dev=\"cuda\"; loss=t.nn.DataParallel(t.nn.CrossEntropyLoss(reduce=False)).to(dev); inp=t.randn(10000, 10000, requires_grad=True).to(dev); tgt=t.empty(10000, dtype=t.long).random_(10000).to(dev)' 't.mean(loss(inp, tgt))'\nRegardless of the performance impact, the DataParallel has a nice property that the model(input) will be equal to DataParallel(model)(input). The proposed change would break this property.", "body": "It is true, the loss computation could increase the training time. My hunch is that loss calculation is usually the fast part of the model training. I'm thinking of all other computation that takes place besides the loss calculation - forward pass, gradient calculation, weight updates...\r\n\r\nTo check this, I just benchmarked `CrossEntropy` on 10000 items and 10000 classes and loss computation took 2.16ms. I used the following benchmark:\r\n`python3 -m timeit -s 'import torch as t; dev=\"cuda\"; loss=t.nn.CrossEntropyLoss().to(dev); inp=t.randn(10000, 10000).to(dev); tgt=t.empty(10000, dtype=t.long).random_(10000).to(dev)' 'loss(inp, tgt)'`\r\n\r\nIt would seem that this is unlikely to observably influence the training time.\r\n\r\nEven if it does, implementation that distributes the loss computation over the GPUs and then calculates the mean of the returned vector is considerably slower.\r\nTry it:\r\n`python3 -m timeit -s 'import torch as t; dev=\"cuda\"; loss=t.nn.DataParallel(t.nn.CrossEntropyLoss(reduce=False)).to(dev); inp=t.randn(10000, 10000, requires_grad=True).to(dev); tgt=t.empty(10000, dtype=t.long).random_(10000).to(dev)' 't.mean(loss(inp, tgt))'`\r\n\r\nRegardless of the performance impact, the `DataParallel` has a nice property that the `model(input)` will be equal to `DataParallel(model)(input)`. The proposed change would break this property."}