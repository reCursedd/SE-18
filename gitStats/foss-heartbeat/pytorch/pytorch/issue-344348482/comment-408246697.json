{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/408246697", "html_url": "https://github.com/pytorch/pytorch/issues/9811#issuecomment-408246697", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9811", "id": 408246697, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODI0NjY5Nw==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-26T21:53:18Z", "updated_at": "2018-07-26T21:53:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11853089\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/0phoff\">@0phoff</a> Here is what it used to be</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.<span class=\"pl-c1\">__version__</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'</span>0.3.1.post3<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n<span class=\"pl-c1\">...</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n<span class=\"pl-c1\">...</span>         <span class=\"pl-k\">return</span> F.mse_loss(x, torch.autograd.Variable(torch.ones(x.shape).cuda()))\n<span class=\"pl-c1\">...</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> m <span class=\"pl-k\">=</span> nn.DataParallel(Model(), [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">print</span>(m(torch.autograd.Variable(torch.zeros(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>))))\nVariable containing:\n <span class=\"pl-c1\">1</span>\n <span class=\"pl-c1\">1</span>\n[torch.cuda.FloatTensor of size <span class=\"pl-c1\">2</span> (<span class=\"pl-c1\">GPU</span> <span class=\"pl-c1\">0</span>)]\n</pre></div>\n<p>I didn't really add any new behavior.</p>", "body_text": "@apaszke @0phoff Here is what it used to be\n>>> import torch\n>>> torch.__version__\n'0.3.1.post3'\n>>> import torch.nn as nn\n>>> import torch.nn.functional as F\n>>>\n>>> class Model(nn.Module):\n...     def forward(self, x):\n...         return F.mse_loss(x, torch.autograd.Variable(torch.ones(x.shape).cuda()))\n...\n>>> m = nn.DataParallel(Model(), [0, 1])\n>>> print(m(torch.autograd.Variable(torch.zeros(4, 3))))\nVariable containing:\n 1\n 1\n[torch.cuda.FloatTensor of size 2 (GPU 0)]\n\nI didn't really add any new behavior.", "body": "@apaszke @0phoff Here is what it used to be \r\n```py\r\n>>> import torch\r\n>>> torch.__version__\r\n'0.3.1.post3'\r\n>>> import torch.nn as nn\r\n>>> import torch.nn.functional as F\r\n>>>\r\n>>> class Model(nn.Module):\r\n...     def forward(self, x):\r\n...         return F.mse_loss(x, torch.autograd.Variable(torch.ones(x.shape).cuda()))\r\n...\r\n>>> m = nn.DataParallel(Model(), [0, 1])\r\n>>> print(m(torch.autograd.Variable(torch.zeros(4, 3))))\r\nVariable containing:\r\n 1\r\n 1\r\n[torch.cuda.FloatTensor of size 2 (GPU 0)]\r\n\r\n```\r\n\r\nI didn't really add any new behavior."}