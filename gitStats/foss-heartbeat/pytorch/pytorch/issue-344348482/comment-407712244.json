{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/407712244", "html_url": "https://github.com/pytorch/pytorch/issues/9811#issuecomment-407712244", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9811", "id": 407712244, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzcxMjI0NA==", "user": {"login": "sharpsy", "id": 7761488, "node_id": "MDQ6VXNlcjc3NjE0ODg=", "avatar_url": "https://avatars2.githubusercontent.com/u/7761488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sharpsy", "html_url": "https://github.com/sharpsy", "followers_url": "https://api.github.com/users/sharpsy/followers", "following_url": "https://api.github.com/users/sharpsy/following{/other_user}", "gists_url": "https://api.github.com/users/sharpsy/gists{/gist_id}", "starred_url": "https://api.github.com/users/sharpsy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sharpsy/subscriptions", "organizations_url": "https://api.github.com/users/sharpsy/orgs", "repos_url": "https://api.github.com/users/sharpsy/repos", "events_url": "https://api.github.com/users/sharpsy/events{/privacy}", "received_events_url": "https://api.github.com/users/sharpsy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-25T10:40:46Z", "updated_at": "2018-07-25T10:41:28Z", "author_association": "NONE", "body_html": "<p>I believe that it is not a good idea for PyTorch to combine those return values in a new dimension. This change would make a shape of the return value dependent on the number of GPU devices used to calculate the result. As I see the <code>DataParallel</code> class, its job is to transparently split the workload on available GPUs so the result is indistinguishable from the result you would get from running the model without the <code>DataParallel</code>.</p>\n<p>I have not used models that calculate the loss in the forward pass so I do not have an experience for a valid advice - to me it looks like something to avoid. What you can do to implement it with <code>DataParallel</code> is to include <code>DataParallel</code> as part of your model. For example, look at this code snippet for the general idea: <a href=\"https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html#dataparallel\" rel=\"nofollow\">https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html#dataparallel</a>. You could do the same, have all layers in (a single) <code>DataParallel</code> and then calculate the loss after this <code>DataParallel</code> finishes.</p>", "body_text": "I believe that it is not a good idea for PyTorch to combine those return values in a new dimension. This change would make a shape of the return value dependent on the number of GPU devices used to calculate the result. As I see the DataParallel class, its job is to transparently split the workload on available GPUs so the result is indistinguishable from the result you would get from running the model without the DataParallel.\nI have not used models that calculate the loss in the forward pass so I do not have an experience for a valid advice - to me it looks like something to avoid. What you can do to implement it with DataParallel is to include DataParallel as part of your model. For example, look at this code snippet for the general idea: https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html#dataparallel. You could do the same, have all layers in (a single) DataParallel and then calculate the loss after this DataParallel finishes.", "body": "I believe that it is not a good idea for PyTorch to combine those return values in a new dimension. This change would make a shape of the return value dependent on the number of GPU devices used to calculate the result. As I see the `DataParallel` class, its job is to transparently split the workload on available GPUs so the result is indistinguishable from the result you would get from running the model without the `DataParallel`.\r\n\r\nI have not used models that calculate the loss in the forward pass so I do not have an experience for a valid advice - to me it looks like something to avoid. What you can do to implement it with `DataParallel` is to include `DataParallel` as part of your model. For example, look at this code snippet for the general idea: https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html#dataparallel. You could do the same, have all layers in (a single) `DataParallel` and then calculate the loss after this `DataParallel` finishes.\r\n"}