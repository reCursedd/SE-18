{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/408382786", "html_url": "https://github.com/pytorch/pytorch/issues/9811#issuecomment-408382786", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9811", "id": 408382786, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODM4Mjc4Ng==", "user": {"login": "sharpsy", "id": 7761488, "node_id": "MDQ6VXNlcjc3NjE0ODg=", "avatar_url": "https://avatars2.githubusercontent.com/u/7761488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sharpsy", "html_url": "https://github.com/sharpsy", "followers_url": "https://api.github.com/users/sharpsy/followers", "following_url": "https://api.github.com/users/sharpsy/following{/other_user}", "gists_url": "https://api.github.com/users/sharpsy/gists{/gist_id}", "starred_url": "https://api.github.com/users/sharpsy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sharpsy/subscriptions", "organizations_url": "https://api.github.com/users/sharpsy/orgs", "repos_url": "https://api.github.com/users/sharpsy/repos", "events_url": "https://api.github.com/users/sharpsy/events{/privacy}", "received_events_url": "https://api.github.com/users/sharpsy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-27T10:45:09Z", "updated_at": "2018-07-27T10:45:09Z", "author_association": "NONE", "body_html": "<p>Current behavior leads to fickle code. If backwards compatibility is the reason to leave it as it is - version 1.0 is a great time to solve this.<br>\nIn <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"327934105\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7973\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7973/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/7973\">#7973</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> wrote down other reasons for this behavior.</p>\n<blockquote>\n<p>If people want to calculate their losses at each GPU, they can't treat DP as non-existent and will have to face this problem no matter what.</p>\n</blockquote>\n<p>Parallel loss calculation is a valuable feature, and we want to keep it. As the example I wrote showed, automatic loss function reduction in the <code>DataParallel</code> can easily lead to wrong results. The (only) way for it to work is with argument <code>average=False</code> and then user sums all losses and divide it with the whole batch size. The problem here is that broadcasting scalars to a tensor will be done implicitly and the user needs to combine them explicitly. To do it correctly, user will need to understand what happened in the <code>DataParallel</code> implementation. If he understands <code>DataParallel</code> implementation, it should not be a problem to come up with <code>return scalar.unsqueeze(0)</code> to have current behavior. If he doesn't understand - he will probably have a bug in the code.</p>\n<blockquote>\n<p>Even if people use .unsqueeze(0), the output shape will still be number_of_GPU-dependent.</p>\n</blockquote>\n<p>Yes, but this is the point - for them to request this behavior explicitly. In that case, we should assume that they know what they are doing.</p>\n<blockquote>\n<p>This pattern works before 0.4.</p>\n</blockquote>\n<p>The backwards compatibility is a reason to keep this behavior. If it is the only reason, version 1.0 is a great time to deprecate it.</p>\n<blockquote>\n<p>The pattern <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> and I were thinking about is calculating loss with size_average=False in Module forward, and sum on the resulting vector from DP, and backward.</p>\n</blockquote>\n<p>This is a valid pattern and it should stay possible. And it will be possible even if scalars are not broadcasted to tensor (ie. with the <code>.unsqueeze(0)</code> and similar tricks).</p>\n<p>If outright removal of this 'feature' is currently not an option, I would suggest deprecating it as soon as possible.</p>", "body_text": "Current behavior leads to fickle code. If backwards compatibility is the reason to leave it as it is - version 1.0 is a great time to solve this.\nIn #7973, @SsnL wrote down other reasons for this behavior.\n\nIf people want to calculate their losses at each GPU, they can't treat DP as non-existent and will have to face this problem no matter what.\n\nParallel loss calculation is a valuable feature, and we want to keep it. As the example I wrote showed, automatic loss function reduction in the DataParallel can easily lead to wrong results. The (only) way for it to work is with argument average=False and then user sums all losses and divide it with the whole batch size. The problem here is that broadcasting scalars to a tensor will be done implicitly and the user needs to combine them explicitly. To do it correctly, user will need to understand what happened in the DataParallel implementation. If he understands DataParallel implementation, it should not be a problem to come up with return scalar.unsqueeze(0) to have current behavior. If he doesn't understand - he will probably have a bug in the code.\n\nEven if people use .unsqueeze(0), the output shape will still be number_of_GPU-dependent.\n\nYes, but this is the point - for them to request this behavior explicitly. In that case, we should assume that they know what they are doing.\n\nThis pattern works before 0.4.\n\nThe backwards compatibility is a reason to keep this behavior. If it is the only reason, version 1.0 is a great time to deprecate it.\n\nThe pattern @colesbury and I were thinking about is calculating loss with size_average=False in Module forward, and sum on the resulting vector from DP, and backward.\n\nThis is a valid pattern and it should stay possible. And it will be possible even if scalars are not broadcasted to tensor (ie. with the .unsqueeze(0) and similar tricks).\nIf outright removal of this 'feature' is currently not an option, I would suggest deprecating it as soon as possible.", "body": "Current behavior leads to fickle code. If backwards compatibility is the reason to leave it as it is - version 1.0 is a great time to solve this.\r\nIn https://github.com/pytorch/pytorch/pull/7973, @SsnL wrote down other reasons for this behavior.\r\n\r\n> If people want to calculate their losses at each GPU, they can't treat DP as non-existent and will have to face this problem no matter what.\r\n\r\nParallel loss calculation is a valuable feature, and we want to keep it. As the example I wrote showed, automatic loss function reduction in the `DataParallel` can easily lead to wrong results. The (only) way for it to work is with argument `average=False` and then user sums all losses and divide it with the whole batch size. The problem here is that broadcasting scalars to a tensor will be done implicitly and the user needs to combine them explicitly. To do it correctly, user will need to understand what happened in the `DataParallel` implementation. If he understands `DataParallel` implementation, it should not be a problem to come up with `return scalar.unsqueeze(0)` to have current behavior. If he doesn't understand - he will probably have a bug in the code.\r\n\r\n> Even if people use .unsqueeze(0), the output shape will still be number_of_GPU-dependent.\r\n\r\nYes, but this is the point - for them to request this behavior explicitly. In that case, we should assume that they know what they are doing.\r\n\r\n> This pattern works before 0.4.\r\n\r\nThe backwards compatibility is a reason to keep this behavior. If it is the only reason, version 1.0 is a great time to deprecate it.\r\n\r\n> The pattern @colesbury and I were thinking about is calculating loss with size_average=False in Module forward, and sum on the resulting vector from DP, and backward.\r\n\r\nThis is a valid pattern and it should stay possible. And it will be possible even if scalars are not broadcasted to tensor (ie. with the `.unsqueeze(0)` and similar tricks).\r\n\r\nIf outright removal of this 'feature' is currently not an option, I would suggest deprecating it as soon as possible."}