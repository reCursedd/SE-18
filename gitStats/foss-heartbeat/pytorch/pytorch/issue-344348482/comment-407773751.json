{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/407773751", "html_url": "https://github.com/pytorch/pytorch/issues/9811#issuecomment-407773751", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9811", "id": 407773751, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzc3Mzc1MQ==", "user": {"login": "0phoff", "id": 11853089, "node_id": "MDQ6VXNlcjExODUzMDg5", "avatar_url": "https://avatars3.githubusercontent.com/u/11853089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/0phoff", "html_url": "https://github.com/0phoff", "followers_url": "https://api.github.com/users/0phoff/followers", "following_url": "https://api.github.com/users/0phoff/following{/other_user}", "gists_url": "https://api.github.com/users/0phoff/gists{/gist_id}", "starred_url": "https://api.github.com/users/0phoff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/0phoff/subscriptions", "organizations_url": "https://api.github.com/users/0phoff/orgs", "repos_url": "https://api.github.com/users/0phoff/repos", "events_url": "https://api.github.com/users/0phoff/events{/privacy}", "received_events_url": "https://api.github.com/users/0phoff/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-25T14:27:21Z", "updated_at": "2018-07-25T14:44:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7761488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sharpsy\">@sharpsy</a>, thanks for the thorough analysis. Helps to put this issue in perspective! I will probably move the CrossEntropyLoss outside of <code>DataParallel</code>.<br>\nAnd the idea of adding an extra dimension for stacking scalar tensors is just that, an idea.<br>\nI would understand it if you would not implement that part, but adding <code>reduce</code> and <code>size_average</code> (or probable just the new <code>reduction</code>) parameters, would still allow to have this nice property by default, but allow advanced users to have a different behaviour if they want.</p>\n<p>While it indeed might be faster to compute CrossEntropyLoss on one GPU, there are some loss functions that are quite more complicated, and that require much more computations. Paralellizing these computations does have a benefit! (eg. the RegionLoss of the YOLO detectors is quite an intricate loss function)</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a>, what exactly did you fix?</p>\n<p><strong>EDIT</strong>: Had an afterthought</p>", "body_text": "@sharpsy, thanks for the thorough analysis. Helps to put this issue in perspective! I will probably move the CrossEntropyLoss outside of DataParallel.\nAnd the idea of adding an extra dimension for stacking scalar tensors is just that, an idea.\nI would understand it if you would not implement that part, but adding reduce and size_average (or probable just the new reduction) parameters, would still allow to have this nice property by default, but allow advanced users to have a different behaviour if they want.\nWhile it indeed might be faster to compute CrossEntropyLoss on one GPU, there are some loss functions that are quite more complicated, and that require much more computations. Paralellizing these computations does have a benefit! (eg. the RegionLoss of the YOLO detectors is quite an intricate loss function)\n@SsnL, what exactly did you fix?\nEDIT: Had an afterthought", "body": "@sharpsy, thanks for the thorough analysis. Helps to put this issue in perspective! I will probably move the CrossEntropyLoss outside of `DataParallel`.  \r\nAnd the idea of adding an extra dimension for stacking scalar tensors is just that, an idea.\r\nI would understand it if you would not implement that part, but adding `reduce` and `size_average` (or probable just the new `reduction`) parameters, would still allow to have this nice property by default, but allow advanced users to have a different behaviour if they want.\r\n\r\nWhile it indeed might be faster to compute CrossEntropyLoss on one GPU, there are some loss functions that are quite more complicated, and that require much more computations. Paralellizing these computations does have a benefit! (eg. the RegionLoss of the YOLO detectors is quite an intricate loss function)\r\n\r\n@SsnL, what exactly did you fix? \r\n\r\n__EDIT__: Had an afterthought"}