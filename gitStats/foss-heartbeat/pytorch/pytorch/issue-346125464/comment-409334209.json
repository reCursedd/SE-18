{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/409334209", "html_url": "https://github.com/pytorch/pytorch/issues/10065#issuecomment-409334209", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10065", "id": 409334209, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTMzNDIwOQ==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-31T19:09:16Z", "updated_at": "2018-07-31T19:09:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11974176\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/GZHermit\">@GZHermit</a> ,</p>\n<blockquote>\n<p>w = torch.tensor([1, 2, 3], requires_grad=True)</p>\n</blockquote>\n<p>is a leaf tensor, while the next line,</p>\n<blockquote>\n<p>w1 = w.type(torch.float)</p>\n</blockquote>\n<p>creates a non-leaf tensor <code>w1</code>. Gradients only accumulate in leaf tensors (i.e., <code>w</code> in this case).</p>\n<p>To fix the problem, you can do the following:</p>\n<pre><code>w1 = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float).\n</code></pre>\n<p>I'll leave this issue open so that we will add a note to the FAQ and/or tutorials about this point.</p>", "body_text": "@GZHermit ,\n\nw = torch.tensor([1, 2, 3], requires_grad=True)\n\nis a leaf tensor, while the next line,\n\nw1 = w.type(torch.float)\n\ncreates a non-leaf tensor w1. Gradients only accumulate in leaf tensors (i.e., w in this case).\nTo fix the problem, you can do the following:\nw1 = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float).\n\nI'll leave this issue open so that we will add a note to the FAQ and/or tutorials about this point.", "body": "@GZHermit ,\r\n\r\n> w = torch.tensor([1, 2, 3], requires_grad=True)\r\n\r\nis a leaf tensor, while the next line,\r\n\r\n> w1 = w.type(torch.float)\r\n\r\ncreates a non-leaf tensor `w1`. Gradients only accumulate in leaf tensors (i.e., `w` in this case).\r\n\r\nTo fix the problem, you can do the following:\r\n\r\n```\r\nw1 = torch.tensor([1, 2, 3], requires_grad=True, dtype=torch.float).\r\n```\r\nI'll leave this issue open so that we will add a note to the FAQ and/or tutorials about this point."}