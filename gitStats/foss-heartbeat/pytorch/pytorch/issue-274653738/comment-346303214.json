{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/346303214", "html_url": "https://github.com/pytorch/pytorch/issues/3743#issuecomment-346303214", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3743", "id": 346303214, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NjMwMzIxNA==", "user": {"login": "dmarnerides", "id": 7605917, "node_id": "MDQ6VXNlcjc2MDU5MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/7605917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dmarnerides", "html_url": "https://github.com/dmarnerides", "followers_url": "https://api.github.com/users/dmarnerides/followers", "following_url": "https://api.github.com/users/dmarnerides/following{/other_user}", "gists_url": "https://api.github.com/users/dmarnerides/gists{/gist_id}", "starred_url": "https://api.github.com/users/dmarnerides/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dmarnerides/subscriptions", "organizations_url": "https://api.github.com/users/dmarnerides/orgs", "repos_url": "https://api.github.com/users/dmarnerides/repos", "events_url": "https://api.github.com/users/dmarnerides/events{/privacy}", "received_events_url": "https://api.github.com/users/dmarnerides/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-22T10:07:12Z", "updated_at": "2017-11-22T10:07:12Z", "author_association": "NONE", "body_html": "<p>I can reproduce the memory leak with this:</p>\n<pre><code>import gc\nimport resource\nimport torch\nfrom torch import nn, autograd\nfrom torch.autograd import Variable\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 1, 1, 1),\n            nn.Conv2d(1, 1, 1, 1),\n        )\n    def forward(self, v_x):\n        return self.main(v_x).view(v_x.size(0), 1)\n\nnet = Network()\n\ni = 0\nwhile True:\n    v_in = Variable(torch.Tensor(2,1,1,1), requires_grad=True)\n    grad_out = Variable(torch.ones(2,1,1,1))\n\n    gradient = autograd.grad(outputs=net(v_in), inputs=v_in,\n                             grad_outputs=grad_out,\n                             create_graph=True, retain_graph=True, \n                             only_inputs=True)[0]\n    gradient.mean().backward()\n\n    i += 1\n    if i % 512 == 0:\n        gc.collect()\n        max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n        print(\"{:.2f} MB\".format(max_mem_used / 1024))\n</code></pre>\n<p>However the leak disappears when only one convolution is used:</p>\n<pre><code>class Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 1, 1, 1),\n        )\n    def forward(self, v_x):\n        return self.main(v_x).view(v_x.size(0), 1)\n</code></pre>\n<p>Also, if nn.Linear is used, there is no leak.</p>\n<p>Torch version: 0.4.0a0+8ebf18b</p>", "body_text": "I can reproduce the memory leak with this:\nimport gc\nimport resource\nimport torch\nfrom torch import nn, autograd\nfrom torch.autograd import Variable\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 1, 1, 1),\n            nn.Conv2d(1, 1, 1, 1),\n        )\n    def forward(self, v_x):\n        return self.main(v_x).view(v_x.size(0), 1)\n\nnet = Network()\n\ni = 0\nwhile True:\n    v_in = Variable(torch.Tensor(2,1,1,1), requires_grad=True)\n    grad_out = Variable(torch.ones(2,1,1,1))\n\n    gradient = autograd.grad(outputs=net(v_in), inputs=v_in,\n                             grad_outputs=grad_out,\n                             create_graph=True, retain_graph=True, \n                             only_inputs=True)[0]\n    gradient.mean().backward()\n\n    i += 1\n    if i % 512 == 0:\n        gc.collect()\n        max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n        print(\"{:.2f} MB\".format(max_mem_used / 1024))\n\nHowever the leak disappears when only one convolution is used:\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 1, 1, 1),\n        )\n    def forward(self, v_x):\n        return self.main(v_x).view(v_x.size(0), 1)\n\nAlso, if nn.Linear is used, there is no leak.\nTorch version: 0.4.0a0+8ebf18b", "body": "I can reproduce the memory leak with this:\r\n\r\n```\r\nimport gc\r\nimport resource\r\nimport torch\r\nfrom torch import nn, autograd\r\nfrom torch.autograd import Variable\r\n\r\nclass Network(nn.Module):\r\n    def __init__(self):\r\n        super(Network, self).__init__()\r\n        self.main = nn.Sequential(\r\n            nn.Conv2d(1, 1, 1, 1),\r\n            nn.Conv2d(1, 1, 1, 1),\r\n        )\r\n    def forward(self, v_x):\r\n        return self.main(v_x).view(v_x.size(0), 1)\r\n\r\nnet = Network()\r\n\r\ni = 0\r\nwhile True:\r\n    v_in = Variable(torch.Tensor(2,1,1,1), requires_grad=True)\r\n    grad_out = Variable(torch.ones(2,1,1,1))\r\n\r\n    gradient = autograd.grad(outputs=net(v_in), inputs=v_in,\r\n                             grad_outputs=grad_out,\r\n                             create_graph=True, retain_graph=True, \r\n                             only_inputs=True)[0]\r\n    gradient.mean().backward()\r\n\r\n    i += 1\r\n    if i % 512 == 0:\r\n        gc.collect()\r\n        max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n        print(\"{:.2f} MB\".format(max_mem_used / 1024))\r\n```\r\n\r\nHowever the leak disappears when only one convolution is used:\r\n```\r\nclass Network(nn.Module):\r\n    def __init__(self):\r\n        super(Network, self).__init__()\r\n        self.main = nn.Sequential(\r\n            nn.Conv2d(1, 1, 1, 1),\r\n        )\r\n    def forward(self, v_x):\r\n        return self.main(v_x).view(v_x.size(0), 1)\r\n```\r\n\r\nAlso, if nn.Linear is used, there is no leak.\r\n\r\nTorch version: 0.4.0a0+8ebf18b"}