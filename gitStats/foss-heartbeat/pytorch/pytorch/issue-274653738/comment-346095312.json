{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/346095312", "html_url": "https://github.com/pytorch/pytorch/issues/3743#issuecomment-346095312", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3743", "id": 346095312, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NjA5NTMxMg==", "user": {"login": "aelnouby", "id": 8495451, "node_id": "MDQ6VXNlcjg0OTU0NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/8495451?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aelnouby", "html_url": "https://github.com/aelnouby", "followers_url": "https://api.github.com/users/aelnouby/followers", "following_url": "https://api.github.com/users/aelnouby/following{/other_user}", "gists_url": "https://api.github.com/users/aelnouby/gists{/gist_id}", "starred_url": "https://api.github.com/users/aelnouby/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aelnouby/subscriptions", "organizations_url": "https://api.github.com/users/aelnouby/orgs", "repos_url": "https://api.github.com/users/aelnouby/repos", "events_url": "https://api.github.com/users/aelnouby/events{/privacy}", "received_events_url": "https://api.github.com/users/aelnouby/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-21T17:10:36Z", "updated_at": "2017-11-21T17:10:36Z", "author_association": "NONE", "body_html": "<p>I am having an out of memory error as well, only when applying backward to the output of this method. I am using 3 titans GPUs so memory is not a problem I would assume.</p>\n<pre><code>def compute_GP(netD, real_data, real_embed, fake_data, LAMBDA):\n    BATCH_SIZE = real_data.size(0)\n    alpha = torch.rand(BATCH_SIZE, 1)\n    alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement() / BATCH_SIZE)).contiguous().view(BATCH_SIZE, 3, 64, 64)\n    alpha = alpha.cuda()\n    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n    interpolates = interpolates.cuda()\n    interpolates = autograd.Variable(interpolates, requires_grad=True)\n    disc_interpolates, _ = netD(interpolates, real_embed)\n    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n    return gradient_penalty\n</code></pre>\n<p>`</p>", "body_text": "I am having an out of memory error as well, only when applying backward to the output of this method. I am using 3 titans GPUs so memory is not a problem I would assume.\ndef compute_GP(netD, real_data, real_embed, fake_data, LAMBDA):\n    BATCH_SIZE = real_data.size(0)\n    alpha = torch.rand(BATCH_SIZE, 1)\n    alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement() / BATCH_SIZE)).contiguous().view(BATCH_SIZE, 3, 64, 64)\n    alpha = alpha.cuda()\n    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n    interpolates = interpolates.cuda()\n    interpolates = autograd.Variable(interpolates, requires_grad=True)\n    disc_interpolates, _ = netD(interpolates, real_embed)\n    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                              grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n    return gradient_penalty\n\n`", "body": "I am having an out of memory error as well, only when applying backward to the output of this method. I am using 3 titans GPUs so memory is not a problem I would assume.\r\n\r\n    def compute_GP(netD, real_data, real_embed, fake_data, LAMBDA):\r\n        BATCH_SIZE = real_data.size(0)\r\n        alpha = torch.rand(BATCH_SIZE, 1)\r\n        alpha = alpha.expand(BATCH_SIZE, int(real_data.nelement() / BATCH_SIZE)).contiguous().view(BATCH_SIZE, 3, 64, 64)\r\n        alpha = alpha.cuda()\r\n        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\r\n        interpolates = interpolates.cuda()\r\n        interpolates = autograd.Variable(interpolates, requires_grad=True)\r\n        disc_interpolates, _ = netD(interpolates, real_embed)\r\n        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\r\n                                  grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\r\n                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\r\n        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\r\n        return gradient_penalty\r\n`\r\n"}