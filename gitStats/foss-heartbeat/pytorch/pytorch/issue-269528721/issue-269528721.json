{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3363", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3363/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3363/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3363/events", "html_url": "https://github.com/pytorch/pytorch/issues/3363", "id": 269528721, "node_id": "MDU6SXNzdWUyNjk1Mjg3MjE=", "number": 3363, "title": "When I was training a CNN+GRU model with CTC loss, I got the nan loss after several batches.", "user": {"login": "jiayi-wei", "id": 18025870, "node_id": "MDQ6VXNlcjE4MDI1ODcw", "avatar_url": "https://avatars3.githubusercontent.com/u/18025870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiayi-wei", "html_url": "https://github.com/jiayi-wei", "followers_url": "https://api.github.com/users/jiayi-wei/followers", "following_url": "https://api.github.com/users/jiayi-wei/following{/other_user}", "gists_url": "https://api.github.com/users/jiayi-wei/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiayi-wei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiayi-wei/subscriptions", "organizations_url": "https://api.github.com/users/jiayi-wei/orgs", "repos_url": "https://api.github.com/users/jiayi-wei/repos", "events_url": "https://api.github.com/users/jiayi-wei/events{/privacy}", "received_events_url": "https://api.github.com/users/jiayi-wei/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-30T09:19:36Z", "updated_at": "2018-11-23T11:28:45Z", "closed_at": "2017-10-30T09:31:40Z", "author_association": "NONE", "body_html": "<p>My model:<br>\n`class CTCModel(nn.Module):<br>\ndef <strong>init</strong>(self,output_size,rnn_hidden_size=128, num_rnn_layers=1, dropout=0):<br>\nsuper(CTCModel, self).<strong>init</strong>()<br>\nself.num_rnn_layers = num_rnn_layers<br>\nself.rnn_hidden_size = rnn_hidden_size<br>\nself.output_size = output_size<br>\nself.layer1 = nn.Sequential(<br>\n#ResBlock(3,32),<br>\nnn.Conv2d(3,32, kernel_size=(3,3),stride=(1,1),padding=(1,1)),<br>\nnn.ReLU(),<br>\nnn.Conv2d(32, 32, kernel_size=(3, 4), stride=(3, 2)),<br>\nnn.BatchNorm2d(32),<br>\nnn.ReLU(),<br>\n#nn.Dropout2d(dropout)<br>\n)<br>\nself.layer2 = nn.Sequential(<br>\n#ResBlock(32,32),<br>\nnn.Conv2d(32,32, kernel_size=(3,3),stride=(1,1),padding=(1,1)),<br>\nnn.ReLU(),<br>\nnn.Conv2d(32, 32, kernel_size=(4, 3), stride=(4, 2)),<br>\nnn.BatchNorm2d(32),<br>\nnn.ReLU(),<br>\n#nn.Dropout2d(dropout)<br>\n)<br>\nself.layer3 = nn.Sequential(<br>\n#ResBlock(32,32),<br>\nnn.Conv2d(32, 32, kernel_size=(3, 3),stride=(1,1),padding=(1,1)),<br>\nnn.ReLU(),<br>\nnn.Conv2d(32, 32, kernel_size=(4, 2), stride=(2, 2)),<br>\n#nn.BatchNorm2d(32),<br>\nnn.ReLU(),<br>\nnn.Conv2d(32, 32, kernel_size=(3, 2), stride=(1, 1)),<br>\nnn.BatchNorm2d(32),<br>\nnn.ReLU(),<br>\n)<br>\nself.gru = nn.GRU(32, rnn_hidden_size, num_rnn_layers,<br>\nbatch_first=True,<br>\ndropout=dropout,bidirectional=True)<br>\nself.linear = nn.Linear(rnn_hidden_size*2,output_size)</p>\n<pre><code>def forward(self, x, hidden):\n    h0 = hidden\n    #print h0.size()\n    out = self.layer1(x)\n    #print out.size()\n    out = self.layer2(out)\n    #print out.size()\n    out = self.layer3(out).squeeze()\n    #print out.size()\n    out = out.transpose(1, 2)\n    #print out.size()\n    #quit()\n    out, hidden = self.gru(out, h0)\n    out = self.linear(out)\n    return out\n\ndef initHidden(self,batch_size,use_cuda=False):\n    h0 = Variable(torch.zeros(self.num_rnn_layers*2,batch_size,self.rnn_hidden_size))\n    if use_cuda:\n        return (h0.cuda())\n    else:\n        return h0`\n</code></pre>\n<p>Training module:<br>\n`def CTCtrain(inputs,targets,lens,ctc,ctc_optimizer,criterion,clip,use_cuda=False):<br>\nif use_cuda:<br>\ninputs = inputs.cuda()<br>\nloss = 0<br>\nctc_optimizer.zero_grad()<br>\nbatch_size = inputs.size()[0]<br>\ninit_hidden = ctc.initHidden(batch_size,use_cuda=use_cuda)<br>\nctc_outputs = ctc(inputs,init_hidden)<br>\n#print ctc_outputs.size(), inputs.size(), init_hidden.size()<br>\n#quit()<br>\nctcloss_inputs = ctc_outputs.transpose(0,1) #SeqLen * BatchSize * Hidden<br>\nlabel_lens = lens<br>\nact_lens = Variable(torch.IntTensor(batch_size*[ctc_outputs.size()[1]]),requires_grad=False)<br>\n#print ctcloss_inputs, targets, act_lens, label_lens<br>\n#quit()<br>\nloss = criterion(ctcloss_inputs,targets,act_lens,label_lens)</p>\n<pre><code>loss.backward()\ntorch.nn.utils.clip_grad_norm(ctc.parameters(), clip)\nctc_optimizer.step()\n\n#TODO\ndecoded_outputs = decode_ctc_outputs(ctc_outputs)\ndecoded_targets = np.split(targets.data.numpy(),lens.data.numpy().cumsum())[:-1]\naccuracy = np.array([np.array_equal(decoded_targets[i],decoded_outputs[i])\n                     for i in range(batch_size)]).mean()\n\nreturn loss.data[0],accuracy`\n</code></pre>\n<p>The size of input image is 100*300. The CTCLoss could update the weight for a while. Then, the loss while become nan</p>\n<p>Could someone tell me the reason? THX!!</p>", "body_text": "My model:\n`class CTCModel(nn.Module):\ndef init(self,output_size,rnn_hidden_size=128, num_rnn_layers=1, dropout=0):\nsuper(CTCModel, self).init()\nself.num_rnn_layers = num_rnn_layers\nself.rnn_hidden_size = rnn_hidden_size\nself.output_size = output_size\nself.layer1 = nn.Sequential(\n#ResBlock(3,32),\nnn.Conv2d(3,32, kernel_size=(3,3),stride=(1,1),padding=(1,1)),\nnn.ReLU(),\nnn.Conv2d(32, 32, kernel_size=(3, 4), stride=(3, 2)),\nnn.BatchNorm2d(32),\nnn.ReLU(),\n#nn.Dropout2d(dropout)\n)\nself.layer2 = nn.Sequential(\n#ResBlock(32,32),\nnn.Conv2d(32,32, kernel_size=(3,3),stride=(1,1),padding=(1,1)),\nnn.ReLU(),\nnn.Conv2d(32, 32, kernel_size=(4, 3), stride=(4, 2)),\nnn.BatchNorm2d(32),\nnn.ReLU(),\n#nn.Dropout2d(dropout)\n)\nself.layer3 = nn.Sequential(\n#ResBlock(32,32),\nnn.Conv2d(32, 32, kernel_size=(3, 3),stride=(1,1),padding=(1,1)),\nnn.ReLU(),\nnn.Conv2d(32, 32, kernel_size=(4, 2), stride=(2, 2)),\n#nn.BatchNorm2d(32),\nnn.ReLU(),\nnn.Conv2d(32, 32, kernel_size=(3, 2), stride=(1, 1)),\nnn.BatchNorm2d(32),\nnn.ReLU(),\n)\nself.gru = nn.GRU(32, rnn_hidden_size, num_rnn_layers,\nbatch_first=True,\ndropout=dropout,bidirectional=True)\nself.linear = nn.Linear(rnn_hidden_size*2,output_size)\ndef forward(self, x, hidden):\n    h0 = hidden\n    #print h0.size()\n    out = self.layer1(x)\n    #print out.size()\n    out = self.layer2(out)\n    #print out.size()\n    out = self.layer3(out).squeeze()\n    #print out.size()\n    out = out.transpose(1, 2)\n    #print out.size()\n    #quit()\n    out, hidden = self.gru(out, h0)\n    out = self.linear(out)\n    return out\n\ndef initHidden(self,batch_size,use_cuda=False):\n    h0 = Variable(torch.zeros(self.num_rnn_layers*2,batch_size,self.rnn_hidden_size))\n    if use_cuda:\n        return (h0.cuda())\n    else:\n        return h0`\n\nTraining module:\n`def CTCtrain(inputs,targets,lens,ctc,ctc_optimizer,criterion,clip,use_cuda=False):\nif use_cuda:\ninputs = inputs.cuda()\nloss = 0\nctc_optimizer.zero_grad()\nbatch_size = inputs.size()[0]\ninit_hidden = ctc.initHidden(batch_size,use_cuda=use_cuda)\nctc_outputs = ctc(inputs,init_hidden)\n#print ctc_outputs.size(), inputs.size(), init_hidden.size()\n#quit()\nctcloss_inputs = ctc_outputs.transpose(0,1) #SeqLen * BatchSize * Hidden\nlabel_lens = lens\nact_lens = Variable(torch.IntTensor(batch_size*[ctc_outputs.size()[1]]),requires_grad=False)\n#print ctcloss_inputs, targets, act_lens, label_lens\n#quit()\nloss = criterion(ctcloss_inputs,targets,act_lens,label_lens)\nloss.backward()\ntorch.nn.utils.clip_grad_norm(ctc.parameters(), clip)\nctc_optimizer.step()\n\n#TODO\ndecoded_outputs = decode_ctc_outputs(ctc_outputs)\ndecoded_targets = np.split(targets.data.numpy(),lens.data.numpy().cumsum())[:-1]\naccuracy = np.array([np.array_equal(decoded_targets[i],decoded_outputs[i])\n                     for i in range(batch_size)]).mean()\n\nreturn loss.data[0],accuracy`\n\nThe size of input image is 100*300. The CTCLoss could update the weight for a while. Then, the loss while become nan\nCould someone tell me the reason? THX!!", "body": "My model:\r\n`class CTCModel(nn.Module):\r\n    def __init__(self,output_size,rnn_hidden_size=128, num_rnn_layers=1, dropout=0):\r\n        super(CTCModel, self).__init__()\r\n        self.num_rnn_layers = num_rnn_layers\r\n        self.rnn_hidden_size = rnn_hidden_size\r\n        self.output_size = output_size\r\n        self.layer1 = nn.Sequential(\r\n            #ResBlock(3,32),\r\n            nn.Conv2d(3,32, kernel_size=(3,3),stride=(1,1),padding=(1,1)),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 32, kernel_size=(3, 4), stride=(3, 2)),\r\n            nn.BatchNorm2d(32),\r\n            nn.ReLU(),\r\n            #nn.Dropout2d(dropout)\r\n        )\r\n        self.layer2 = nn.Sequential(\r\n            #ResBlock(32,32),\r\n            nn.Conv2d(32,32, kernel_size=(3,3),stride=(1,1),padding=(1,1)),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 32, kernel_size=(4, 3), stride=(4, 2)),\r\n            nn.BatchNorm2d(32),\r\n            nn.ReLU(),\r\n            #nn.Dropout2d(dropout)\r\n        )\r\n        self.layer3 = nn.Sequential(\r\n            #ResBlock(32,32),\r\n            nn.Conv2d(32, 32, kernel_size=(3, 3),stride=(1,1),padding=(1,1)),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 32, kernel_size=(4, 2), stride=(2, 2)),\r\n            #nn.BatchNorm2d(32),\r\n            nn.ReLU(),\r\n            nn.Conv2d(32, 32, kernel_size=(3, 2), stride=(1, 1)),\r\n            nn.BatchNorm2d(32),\r\n\t\t\tnn.ReLU(),\r\n        )\r\n        self.gru = nn.GRU(32, rnn_hidden_size, num_rnn_layers,\r\n                          batch_first=True,\r\n                          dropout=dropout,bidirectional=True)\r\n        self.linear = nn.Linear(rnn_hidden_size*2,output_size)\r\n\r\n    def forward(self, x, hidden):\r\n        h0 = hidden\r\n        #print h0.size()\r\n        out = self.layer1(x)\r\n        #print out.size()\r\n        out = self.layer2(out)\r\n        #print out.size()\r\n        out = self.layer3(out).squeeze()\r\n        #print out.size()\r\n        out = out.transpose(1, 2)\r\n        #print out.size()\r\n        #quit()\r\n        out, hidden = self.gru(out, h0)\r\n        out = self.linear(out)\r\n        return out\r\n\r\n    def initHidden(self,batch_size,use_cuda=False):\r\n        h0 = Variable(torch.zeros(self.num_rnn_layers*2,batch_size,self.rnn_hidden_size))\r\n        if use_cuda:\r\n            return (h0.cuda())\r\n        else:\r\n            return h0`\r\n\r\nTraining module:\r\n`def CTCtrain(inputs,targets,lens,ctc,ctc_optimizer,criterion,clip,use_cuda=False):\r\n    if use_cuda:\r\n        inputs = inputs.cuda()\r\n    loss = 0\r\n    ctc_optimizer.zero_grad()\r\n    batch_size = inputs.size()[0]\r\n    init_hidden = ctc.initHidden(batch_size,use_cuda=use_cuda)\r\n    ctc_outputs = ctc(inputs,init_hidden)\r\n    #print ctc_outputs.size(), inputs.size(), init_hidden.size()\r\n    #quit() \r\n    ctcloss_inputs = ctc_outputs.transpose(0,1) #SeqLen * BatchSize * Hidden\r\n    label_lens = lens\r\n    act_lens = Variable(torch.IntTensor(batch_size*[ctc_outputs.size()[1]]),requires_grad=False)\r\n    #print ctcloss_inputs, targets, act_lens, label_lens\r\n    #quit()\r\n    loss = criterion(ctcloss_inputs,targets,act_lens,label_lens)\r\n\r\n    loss.backward()\r\n    torch.nn.utils.clip_grad_norm(ctc.parameters(), clip)\r\n    ctc_optimizer.step()\r\n\r\n    #TODO\r\n    decoded_outputs = decode_ctc_outputs(ctc_outputs)\r\n    decoded_targets = np.split(targets.data.numpy(),lens.data.numpy().cumsum())[:-1]\r\n    accuracy = np.array([np.array_equal(decoded_targets[i],decoded_outputs[i])\r\n                         for i in range(batch_size)]).mean()\r\n\r\n    return loss.data[0],accuracy`\r\n\r\nThe size of input image is 100*300. The CTCLoss could update the weight for a while. Then, the loss while become nan\r\n\r\nCould someone tell me the reason? THX!!"}