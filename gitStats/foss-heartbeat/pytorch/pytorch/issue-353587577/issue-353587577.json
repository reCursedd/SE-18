{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10838", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10838/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10838/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10838/events", "html_url": "https://github.com/pytorch/pytorch/issues/10838", "id": 353587577, "node_id": "MDU6SXNzdWUzNTM1ODc1Nzc=", "number": 10838, "title": "[ONNX] Incorrect handling of Bidirectional RNN output format?", "user": {"login": "tengyifei", "id": 2877531, "node_id": "MDQ6VXNlcjI4Nzc1MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2877531?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tengyifei", "html_url": "https://github.com/tengyifei", "followers_url": "https://api.github.com/users/tengyifei/followers", "following_url": "https://api.github.com/users/tengyifei/following{/other_user}", "gists_url": "https://api.github.com/users/tengyifei/gists{/gist_id}", "starred_url": "https://api.github.com/users/tengyifei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tengyifei/subscriptions", "organizations_url": "https://api.github.com/users/tengyifei/orgs", "repos_url": "https://api.github.com/users/tengyifei/repos", "events_url": "https://api.github.com/users/tengyifei/events{/privacy}", "received_events_url": "https://api.github.com/users/tengyifei/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx", "name": "onnx", "color": "e99695", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-23T23:12:56Z", "updated_at": "2018-09-09T00:11:15Z", "closed_at": "2018-09-09T00:11:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/b4684db6985f9941a27e8d1b6ed9689b3eb19fc3/torch/onnx/symbolic.py#L1187-L1194\">pytorch/torch/onnx/symbolic.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 1187 to 1194\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/b4684db6985f9941a27e8d1b6ed9689b3eb19fc3\">b4684db</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L1187\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1187\"></td>\n          <td id=\"LC1187\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">#</span> The ONNX RNN/GRU/LSTM produce an output of dimensions</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1188\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1188\"></td>\n          <td id=\"LC1188\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">#</span>   seq_len, num_directions, batch, hidden_size</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1189\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1189\"></td>\n          <td id=\"LC1189\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">#</span> We have to convert to match pytorch's expected</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1190\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1190\"></td>\n          <td id=\"LC1190\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">#</span>   seq_len, batch, hidden_size * num_directions</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1191\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1191\"></td>\n          <td id=\"LC1191\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">#</span> by first moving num_directions to the end with</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1192\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1192\"></td>\n          <td id=\"LC1192\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">#</span> Transpose, and then combining it with hidden_size</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1193\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1193\"></td>\n          <td id=\"LC1193\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">#</span> with Reshape.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L1194\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1194\"></td>\n          <td id=\"LC1194\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> prev_output <span class=\"pl-k\">=</span> g.op(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Transpose<span class=\"pl-pds\">'</span></span>, prev_output, <span class=\"pl-v\">perm_i</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>]) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\nThe code converts the <code>[seq_len, num_directions, batch, hidden_size]</code> format in ONNX to <code>[seq_len, batch, hidden_size, num_directions]</code>, but from the PyTorch docs it should have been <code>[seq_len, batch, num_directions, hidden_size]</code>? i.e. <code>perm_i=[0, 2, 1, 3]</code> instead of <code>perm_i=[0, 2, 3, 1]</code></p>\n<p>There's a similar operation in Caffe2<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/b4684db6985f9941a27e8d1b6ed9689b3eb19fc3/caffe2/python/onnx/backend.py#L521\">pytorch/caffe2/python/onnx/backend.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 521\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/b4684db6985f9941a27e8d1b6ed9689b3eb19fc3\">b4684db</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L521\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"521\"></td>\n          <td id=\"LC521\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> pred_mh.net.Transpose(reshaped_output, n.outputs[<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">axes</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>]) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\nwhich undoes the previous transformation, making it work with Caffe2 despite not following the docs.</p>", "body_text": "In \n  \n    \n      pytorch/torch/onnx/symbolic.py\n    \n    \n        Lines 1187 to 1194\n      in\n      b4684db\n    \n    \n    \n    \n\n        \n          \n           # The ONNX RNN/GRU/LSTM produce an output of dimensions \n        \n\n        \n          \n           #   seq_len, num_directions, batch, hidden_size \n        \n\n        \n          \n           # We have to convert to match pytorch's expected \n        \n\n        \n          \n           #   seq_len, batch, hidden_size * num_directions \n        \n\n        \n          \n           # by first moving num_directions to the end with \n        \n\n        \n          \n           # Transpose, and then combining it with hidden_size \n        \n\n        \n          \n           # with Reshape. \n        \n\n        \n          \n           prev_output = g.op('Transpose', prev_output, perm_i=[0, 2, 3, 1]) \n        \n    \n  \n\n\nThe code converts the [seq_len, num_directions, batch, hidden_size] format in ONNX to [seq_len, batch, hidden_size, num_directions], but from the PyTorch docs it should have been [seq_len, batch, num_directions, hidden_size]? i.e. perm_i=[0, 2, 1, 3] instead of perm_i=[0, 2, 3, 1]\nThere's a similar operation in Caffe2\n\n  \n    \n      pytorch/caffe2/python/onnx/backend.py\n    \n    \n         Line 521\n      in\n      b4684db\n    \n    \n    \n    \n\n        \n          \n           pred_mh.net.Transpose(reshaped_output, n.outputs[0], axes=[0,3,1,2]) \n        \n    \n  \n\n\nwhich undoes the previous transformation, making it work with Caffe2 despite not following the docs.", "body": "In https://github.com/pytorch/pytorch/blob/b4684db6985f9941a27e8d1b6ed9689b3eb19fc3/torch/onnx/symbolic.py#L1187-L1194\r\nThe code converts the `[seq_len, num_directions, batch, hidden_size]` format in ONNX to `[seq_len, batch, hidden_size, num_directions]`, but from the PyTorch docs it should have been `[seq_len, batch, num_directions, hidden_size]`? i.e. `perm_i=[0, 2, 1, 3]` instead of `perm_i=[0, 2, 3, 1]`\r\n\r\nThere's a similar operation in Caffe2\r\nhttps://github.com/pytorch/pytorch/blob/b4684db6985f9941a27e8d1b6ed9689b3eb19fc3/caffe2/python/onnx/backend.py#L521\r\nwhich undoes the previous transformation, making it work with Caffe2 despite not following the docs.\r\n"}