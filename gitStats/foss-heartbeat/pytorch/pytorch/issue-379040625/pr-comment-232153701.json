{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232153701", "pull_request_review_id": 173287835, "id": 232153701, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjE1MzcwMQ==", "diff_hunk": "@@ -1,516 +1,7 @@\n-#include <THC/THCDeviceUtils.cuh>\n-#include <THC/THCGeneral.h>\n-#include \"ATen/ATen.h\"\n-#include \"ATen/AccumulateType.h\"\n-#include \"ATen/cuda/CUDAContext.h\"\n-#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+#include \"ATen/native/cuda/Normalization.cuh\"\n \n namespace at { namespace native {\n \n-namespace {\n-\n-\n-#if defined(__HIP_PLATFORM_HCC__)\n-constexpr int WARP_SIZE = 64;\n-\n-// take these out when ROCm implements std:: math functions\n-#include <math.h>\n-template <typename scalar_t>\n-static __forceinline__ __device__ scalar_t device_sqrt(scalar_t val);\n-\n-template <>\n-__forceinline__ __device__ float device_sqrt(float val) {\n-  return ::sqrtf(val);\n-}\n-\n-template <>\n-__forceinline__ __device__ double device_sqrt(double val) {\n-  return ::sqrt(val);\n-}\n-\n-#else\n-constexpr int WARP_SIZE = 32;\n-\n-template<typename scalar_t>\n-__forceinline__ __device__ double device_sqrt(scalar_t val) {\n-  return std::sqrt(val);\n-}\n-#endif\n-\n-// The maximum number of threads in a block\n-#if defined(__HIP_PLATFORM_HCC__)\n-constexpr int MAX_BLOCK_SIZE = 256;\n-#else\n-constexpr int MAX_BLOCK_SIZE = 512;\n-#endif\n-\n-// Number of threads in a block given an input size up to MAX_BLOCK_SIZE\n-static int getNumThreads(int nElem) {\n-#if defined(__HIP_PLATFORM_HCC__)\n-  int threadSizes[5] = { 16, 32, 64, 128, MAX_BLOCK_SIZE };\n-#else\n-  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };\n-#endif\n-  for (int i = 0; i != 5; ++i) {\n-    if (nElem <= threadSizes[i]) {\n-      return threadSizes[i];\n-    }\n-  }\n-  return MAX_BLOCK_SIZE;\n-}\n-\n-// Returns the index of the most significant 1 bit in `val`.\n-__device__ __forceinline__ int getMSB(int val) {\n-  return 31 - __clz(val);\n-}\n-\n-template <typename scalar_t, typename accscalar_t>\n-struct Float2 {\n-  accscalar_t v1, v2;\n-  __device__ Float2() {}\n-  __device__ Float2(scalar_t v1, scalar_t v2) : v1(static_cast<accscalar_t>(v1)), v2(static_cast<accscalar_t>(v2)) {}\n-  __device__ Float2(int v) : v1(static_cast<accscalar_t>(v)), v2(static_cast<accscalar_t>(v)) {}\n-  __device__ Float2& operator+=(const Float2& a) {\n-    v1 += a.v1;\n-    v2 += a.v2;\n-    return *this;\n-  }\n-};\n-\n-template <typename scalar_t, typename accscalar_t, typename PTA>\n-struct SumOp {\n-  __device__ SumOp(const PTA& t) : tensor(t) {}\n-  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n-    return static_cast<accscalar_t>(tensor[batch][plane][n]);\n-  }\n-  const PTA& tensor;\n-};\n-\n-  template <typename scalar_t, typename accscalar_t, typename PTA>\n-struct VarOp {\n-  __device__ VarOp(accscalar_t m, const PTA& t) : mean(m), tensor(t) {}\n-  __device__ __forceinline__ accscalar_t operator()(int batch, int plane, int n) {\n-    accscalar_t val = tensor[batch][plane][n];\n-    return (val - mean) * (val - mean);\n-  }\n-  const accscalar_t mean;\n-  const PTA& tensor;\n-};\n-\n-template <typename scalar_t, typename accscalar_t, typename PTA>\n-struct GradOp {\n-  __device__ GradOp(accscalar_t m, const PTA& i, const PTA& g)\n-    : mean(m), input(i), grad_output(g) {}\n-  __device__ __forceinline__ Float2<scalar_t, accscalar_t> operator()(int batch, int plane, int n) {\n-    accscalar_t g = grad_output[batch][plane][n];\n-    accscalar_t c = static_cast<accscalar_t>(input[batch][plane][n]) - mean;\n-    return Float2<scalar_t, accscalar_t>(g, g * c);\n-  }\n-  const accscalar_t mean;\n-  const PTA& input;\n-  const PTA& grad_output;\n-};\n-\n-// Sum across all threads within a warp\n-template <typename T>\n-static __device__ __forceinline__ T warpSum(T val) {\n-  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n-    val += WARP_SHFL_XOR(val, 1 << i, WARP_SIZE);\n-  }\n-  return val;\n-}\n-\n-template <typename scalar_t, typename accscalar_t>\n-static __device__ __forceinline__ Float2<scalar_t, accscalar_t> warpSum(Float2<scalar_t, accscalar_t> value) {\n-  value.v1 = warpSum(value.v1);\n-  value.v2 = warpSum(value.v2);\n-  return value;\n-}\n-\n-// Sum across (batch, x/y/z) applying Op() pointwise\n-// this works by first having each thread sum it's part\n-// of the data. Then there is a double-shuffeling reduction.\n-// First each warp (of WARP_SIZE threads) uses warpSum to reduce its\n-// data to the \"warp leader\", who writes its value into shared memory.\n-// Then a single warp reads the remaining (at most WARP_SIZE) items\n-// and reduces them using another warpSum.\n-// The implicit assumption is that there are no more\n-// than WARP_SIZE**2 threads.\n-template<typename scalar_t, typename Op, typename PTA>\n-__device__ scalar_t reduce(Op op, PTA tensor, int plane) {\n-  // first the reductions each thread does separately\n-  scalar_t sum = static_cast<scalar_t>(0);\n-  for (int batch = threadIdx.y; batch < tensor.size(0); batch += blockDim.y) {\n-    for (int x = threadIdx.x; x < tensor.size(2); x += blockDim.x) {\n-      sum += op(batch, plane, x);\n-    }\n-  }\n-\n-  // first warpSum to get one value per thread to\n-  // one value per warp\n-  sum = warpSum(sum);\n-\n-  // this writes each warps  item into shared memory\n-  // there are at most WARP_SIZE items left because\n-  // there are at most WARP_SIZE**2 threads at the beginning\n-  __shared__ scalar_t shared[WARP_SIZE];\n-  __syncthreads();\n-  int tid = threadIdx.x + threadIdx.y * blockDim.x;\n-  if (tid % WARP_SIZE == 0) {\n-    shared[tid / WARP_SIZE] = sum;\n-  }\n-  if (tid >= blockDim.x * blockDim.y / WARP_SIZE && tid < WARP_SIZE) {\n-    // zero out the other entries in shared\n-    shared[tid] = (scalar_t)0;\n-  }\n-  __syncthreads();\n-  // now have a second warpSum to reduce the intermediate values\n-  // from shared memory to a single number. The very first\n-  // thread writes it to shared memory.\n-\n-  if (tid / WARP_SIZE == 0) {\n-    sum = warpSum(shared[tid]);\n-    if (tid == 0) {\n-      shared[0] = sum;\n-    }\n-  }\n-  __syncthreads();\n-\n-  // Everyone picks it up, should be broadcast into the whole grad_input\n-  return shared[0];\n-}\n-\n-template <typename scalar_t, typename accscalar_t, bool train, typename index_t>\n-__global__ void batch_norm_transform_input_kernel(\n-    const PackedTensorAccessor<scalar_t, 3, RestrictPtrTraits, index_t> input,\n-    PackedTensorAccessor<scalar_t, 3, RestrictPtrTraits, index_t> output,\n-    const PackedTensorAccessor<typename std::conditional<train, accscalar_t, scalar_t>::type, 1, RestrictPtrTraits, index_t> mean_,\n-    const PackedTensorAccessor<typename std::conditional<train, accscalar_t, scalar_t>::type, 1, RestrictPtrTraits, index_t> var_or_invstd,\n-    const PackedTensorAccessor<scalar_t, 1, RestrictPtrTraits, index_t> weight,\n-    const PackedTensorAccessor<scalar_t, 1, RestrictPtrTraits, index_t> bias,\n-    accscalar_t epsilon) {\n-\n-  index_t plane = blockIdx.x;\n-\n-  if (plane >= input.size(1)) {\n-    return;\n-  }\n-\n-  accscalar_t gamma = weight.size(0) > 0 ? static_cast<accscalar_t>(weight[plane]) : static_cast<accscalar_t>(1);\n-  accscalar_t beta = bias.size(0) > 0 ? static_cast<accscalar_t>(bias[plane]) : static_cast<accscalar_t>(0);\n-  accscalar_t mean = static_cast<accscalar_t>(mean_[plane]);\n-  accscalar_t invstd;\n-  if (train) {\n-    invstd = var_or_invstd[plane];\n-  } else {\n-    invstd = static_cast<accscalar_t>(1) / device_sqrt(static_cast<accscalar_t>(var_or_invstd[plane]) + epsilon);\n-  }\n-\n-  index_t bs = input.size(0);\n-  index_t fs = input.size(2);\n-\n-  index_t bstep  = blockDim.y * gridDim.y;\n-  for (index_t batch = threadIdx.y + blockIdx.y * blockDim.y; batch < bs; batch += bstep) {\n-    auto o = output[batch][plane];\n-    auto i = input[batch][plane];\n-    for (index_t feature = threadIdx.x; feature < fs; feature += blockDim.x) {\n-      o[feature] = static_cast<scalar_t>(gamma * (i[feature] - mean) * invstd + beta);\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, typename accscalar_t, typename index_t>\n-__global__ void batch_norm_collect_statistics_kernel(\n-    const PackedTensorAccessor<scalar_t, 3, RestrictPtrTraits, index_t> input,\n-    const accscalar_t epsilon,\n-    const accscalar_t momentum,\n-    PackedTensorAccessor<scalar_t, 1, RestrictPtrTraits, index_t> running_mean,\n-    PackedTensorAccessor<scalar_t, 1, RestrictPtrTraits, index_t> running_var,\n-    PackedTensorAccessor<accscalar_t, 1, RestrictPtrTraits, index_t> save_mean,\n-    PackedTensorAccessor<accscalar_t, 1, RestrictPtrTraits, index_t> save_invstd) {\n-\n-  __shared__ int shared_n[2 * 2 * WARP_SIZE + WARP_SIZE];\n-\n-  int plane = blockIdx.x;\n-  int N = input.size(0) * input.size(2);\n-  int tid = threadIdx.x + threadIdx.y * blockDim.x;\n-\n-  // Compute the mean and variance across (batch, x/y/z)\n-  // this uses the Welford (in the for loop)/parallel algorithm (to sum across the block)\n-  // https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_Online_algorithm\n-  // and the parallel algorithm on the same page.\n-  // We use two shuffles to reduce across the entire block.\n-  // https://devblogs.nvidia.com/faster-parallel-reductions-kepler/ has a description.\n-  accscalar_t* shared_avg_var = (accscalar_t*) &shared_n[WARP_SIZE];\n-\n-  // first the reductions each thread does separately\n-  accscalar_t avg = 0;\n-  accscalar_t var_n = 0;\n-  int n = 0;\n-  for (int batch = threadIdx.y; batch < input.size(0); batch += blockDim.y) {\n-    for (int x = threadIdx.x; x < input.size(2); x += blockDim.x) {\n-      accscalar_t v = input[batch][plane][x];\n-      accscalar_t d1 = v - avg;\n-      n++;\n-      avg += d1 / n;\n-      var_n += d1 * (v - avg);\n-    }\n-  }\n-\n-  // first warpSum to get one value per thread to\n-  // one value per warp\n-  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n-    accscalar_t o_avg = WARP_SHFL_XOR(avg, 1 << i, WARP_SIZE);\n-    int o_n = WARP_SHFL_XOR(n, 1 << i, WARP_SIZE);\n-    if (n + o_n > 0) {\n-      var_n += WARP_SHFL_XOR(var_n, 1 << i, WARP_SIZE) + ((avg - o_avg) * (avg - o_avg) * n * o_n) / (n + o_n);\n-      avg = (n * avg + o_n * o_avg)/(n+o_n);\n-      n += o_n;\n-    }\n-  }\n-\n-  // this writes each warps  item into shared memory\n-  // there are at most WARP_SIZE items left because\n-  // there are at most WARP_SIZE**2 threads at the beginning  \n-  __syncthreads();\n-  if (tid % WARP_SIZE == 0) {\n-    shared_n[tid / WARP_SIZE] = n;\n-    shared_avg_var[tid / WARP_SIZE * 2] = avg;\n-    shared_avg_var[tid / WARP_SIZE * 2 + 1] = var_n;\n-  }\n-  __syncthreads();\n-  // now have a second warpSum to reduce the intermediate values\n-  // from shared memory to a single number. The very first\n-  // thread writes it to shared memory.\n-\n-  if (tid < WARP_SIZE) {\n-    n = (tid < blockDim.x * blockDim.y / WARP_SIZE ? shared_n[tid] : 0);\n-    avg = (tid < blockDim.x * blockDim.y  / WARP_SIZE ? shared_avg_var[2 * tid] : 0);\n-    var_n = (tid < blockDim.x * blockDim.y  / WARP_SIZE ? shared_avg_var[2 * tid + 1] : 0);\n-  }\n-  for (int i = 0; i < getMSB(WARP_SIZE); ++i) {\n-    accscalar_t o_avg = WARP_SHFL_XOR(avg, 1 << i, WARP_SIZE);\n-    int o_n = WARP_SHFL_XOR(n, 1 << i, WARP_SIZE);\n-    if (n + o_n > 0) {", "path": "aten/src/ATen/native/cuda/Normalization.cu", "position": 296, "original_position": 296, "commit_id": "63b76ab23154dbb0c3cd99291a03489d27f90e39", "original_commit_id": "63b76ab23154dbb0c3cd99291a03489d27f90e39", "user": {"login": "jjsjann123", "id": 3709243, "node_id": "MDQ6VXNlcjM3MDkyNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjsjann123", "html_url": "https://github.com/jjsjann123", "followers_url": "https://api.github.com/users/jjsjann123/followers", "following_url": "https://api.github.com/users/jjsjann123/following{/other_user}", "gists_url": "https://api.github.com/users/jjsjann123/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjsjann123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjsjann123/subscriptions", "organizations_url": "https://api.github.com/users/jjsjann123/orgs", "repos_url": "https://api.github.com/users/jjsjann123/repos", "events_url": "https://api.github.com/users/jjsjann123/events{/privacy}", "received_events_url": "https://api.github.com/users/jjsjann123/received_events", "type": "User", "site_admin": false}, "body": "Same as above", "created_at": "2018-11-09T06:37:40Z", "updated_at": "2018-11-23T15:54:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/13765#discussion_r232153701", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13765", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/232153701"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13765#discussion_r232153701"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13765"}}, "body_html": "<p>Same as above</p>", "body_text": "Same as above"}