{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7449", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7449/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7449/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7449/events", "html_url": "https://github.com/pytorch/pytorch/issues/7449", "id": 321769924, "node_id": "MDU6SXNzdWUzMjE3Njk5MjQ=", "number": 7449, "title": "RuntimeError: No grad accumulator for a saved leaf!", "user": {"login": "minghao-wu", "id": 17817832, "node_id": "MDQ6VXNlcjE3ODE3ODMy", "avatar_url": "https://avatars3.githubusercontent.com/u/17817832?v=4", "gravatar_id": "", "url": "https://api.github.com/users/minghao-wu", "html_url": "https://github.com/minghao-wu", "followers_url": "https://api.github.com/users/minghao-wu/followers", "following_url": "https://api.github.com/users/minghao-wu/following{/other_user}", "gists_url": "https://api.github.com/users/minghao-wu/gists{/gist_id}", "starred_url": "https://api.github.com/users/minghao-wu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/minghao-wu/subscriptions", "organizations_url": "https://api.github.com/users/minghao-wu/orgs", "repos_url": "https://api.github.com/users/minghao-wu/repos", "events_url": "https://api.github.com/users/minghao-wu/events{/privacy}", "received_events_url": "https://api.github.com/users/minghao-wu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-10T00:29:58Z", "updated_at": "2018-06-18T18:14:30Z", "closed_at": "2018-05-10T01:00:35Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>When I run the code, an error was reported as follows:</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-89-0bfb9354ae29&gt; in &lt;module&gt;()\n      1 train_model(model=model, optimizer=optimizer, scheduler=step_scheduler, \n      2             num_epochs=num_epochs, train_loader=train_loader, dev_loader=dev_loader,\n----&gt; 3             test_loader=test_loader, device=device)\n\n&lt;ipython-input-86-615427f40473&gt; in train_model(model, optimizer, scheduler, num_epochs, train_loader, dev_loader, test_loader, device)\n    161             loss2 = F.cross_entropy(pred2, a[:, 0])\n    162             loss = loss1 + loss2\n--&gt; 163             loss.backward()\n    164             optimizer.step()\n    165 \n\n~/anaconda3/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\n     91                 products. Defaults to ``False``.\n     92         \"\"\"\n---&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n     94 \n     95     def register_hook(self, hook):\n\n~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     87     Variable._execution_engine.run_backward(\n     88         tensors, grad_tensors, retain_graph, create_graph,\n---&gt; 89         allow_unreachable=True)  # allow_unreachable flag\n     90 \n     91 \n\nRuntimeError: No grad accumulator for a saved leaf!\n</code></pre>\n<h2>Code example</h2>\n<p>I'm using <code>torch 0.4.0</code> and <code>Cw, Cc, Qw, Qc</code> are elements in a batch with <code>requires_grad=True</code>. <code>a</code> is the label, a tensor with <code>requires_grad=False</code>.</p>\n<p>my code looks like as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> ids, Cw, Cc, Qw, Qc, a <span class=\"pl-k\">in</span> train_loader:\n    optimizer.zero_grad()\n    pred1, pred2 <span class=\"pl-k\">=</span> model(Cw, Cc, Qw, Qc)\n    loss1 <span class=\"pl-k\">=</span> F.cross_entropy(pred1, a[:, <span class=\"pl-c1\">0</span>])\n    loss2 <span class=\"pl-k\">=</span> F.cross_entropy(pred2, a[:, <span class=\"pl-c1\">0</span>])\n    loss <span class=\"pl-k\">=</span> loss1 <span class=\"pl-k\">+</span> loss2\n    loss.backward()\n    optimizer.step()</pre></div>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: None</p>\n<p>OS: Mac OSX 10.12.6<br>\nGCC version: Could not collect<br>\nCMake version: version 3.9.6</p>\n<p>Python version: 3.6<br>\nIs CUDA available: No<br>\nCUDA runtime version: No CUDA<br>\nGPU models and configuration: No CUDA<br>\nNvidia driver version: No CUDA<br>\ncuDNN version: No CUDA</p>\n<p>Versions of relevant libraries:<br>\n[pip3] msgpack-numpy (0.4.1)<br>\n[pip3] numpy (1.14.2)<br>\n[pip3] numpydoc (0.7.0)<br>\n[pip3] pytorchviz (0.0.1)<br>\n[pip3] tensorboard-pytorch (0.7.1)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchfile (0.1.0)<br>\n[pip3] torchtext (0.2.3)<br>\n[pip3] torchvision (0.2.1)<br>\n[conda] pytorch                   0.4.0           py36_cuda0.0_cudnn0.0_1    pytorch<br>\n[conda] pytorchviz                0.0.1                     <br>\n[conda] tensorboard-pytorch       0.7.1                     <br>\n[conda] torchfile                 0.1.0                     <br>\n[conda] torchtext                 0.2.3                     <br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>", "body_text": "Issue description\nWhen I run the code, an error was reported as follows:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-89-0bfb9354ae29> in <module>()\n      1 train_model(model=model, optimizer=optimizer, scheduler=step_scheduler, \n      2             num_epochs=num_epochs, train_loader=train_loader, dev_loader=dev_loader,\n----> 3             test_loader=test_loader, device=device)\n\n<ipython-input-86-615427f40473> in train_model(model, optimizer, scheduler, num_epochs, train_loader, dev_loader, test_loader, device)\n    161             loss2 = F.cross_entropy(pred2, a[:, 0])\n    162             loss = loss1 + loss2\n--> 163             loss.backward()\n    164             optimizer.step()\n    165 \n\n~/anaconda3/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\n     91                 products. Defaults to ``False``.\n     92         \"\"\"\n---> 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n     94 \n     95     def register_hook(self, hook):\n\n~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     87     Variable._execution_engine.run_backward(\n     88         tensors, grad_tensors, retain_graph, create_graph,\n---> 89         allow_unreachable=True)  # allow_unreachable flag\n     90 \n     91 \n\nRuntimeError: No grad accumulator for a saved leaf!\n\nCode example\nI'm using torch 0.4.0 and Cw, Cc, Qw, Qc are elements in a batch with requires_grad=True. a is the label, a tensor with requires_grad=False.\nmy code looks like as follows:\nfor ids, Cw, Cc, Qw, Qc, a in train_loader:\n    optimizer.zero_grad()\n    pred1, pred2 = model(Cw, Cc, Qw, Qc)\n    loss1 = F.cross_entropy(pred1, a[:, 0])\n    loss2 = F.cross_entropy(pred2, a[:, 0])\n    loss = loss1 + loss2\n    loss.backward()\n    optimizer.step()\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: None\nOS: Mac OSX 10.12.6\nGCC version: Could not collect\nCMake version: version 3.9.6\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nVersions of relevant libraries:\n[pip3] msgpack-numpy (0.4.1)\n[pip3] numpy (1.14.2)\n[pip3] numpydoc (0.7.0)\n[pip3] pytorchviz (0.0.1)\n[pip3] tensorboard-pytorch (0.7.1)\n[pip3] torch (0.4.0)\n[pip3] torchfile (0.1.0)\n[pip3] torchtext (0.2.3)\n[pip3] torchvision (0.2.1)\n[conda] pytorch                   0.4.0           py36_cuda0.0_cudnn0.0_1    pytorch\n[conda] pytorchviz                0.0.1                     \n[conda] tensorboard-pytorch       0.7.1                     \n[conda] torchfile                 0.1.0                     \n[conda] torchtext                 0.2.3                     \n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "## Issue description\r\n\r\nWhen I run the code, an error was reported as follows:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-89-0bfb9354ae29> in <module>()\r\n      1 train_model(model=model, optimizer=optimizer, scheduler=step_scheduler, \r\n      2             num_epochs=num_epochs, train_loader=train_loader, dev_loader=dev_loader,\r\n----> 3             test_loader=test_loader, device=device)\r\n\r\n<ipython-input-86-615427f40473> in train_model(model, optimizer, scheduler, num_epochs, train_loader, dev_loader, test_loader, device)\r\n    161             loss2 = F.cross_entropy(pred2, a[:, 0])\r\n    162             loss = loss1 + loss2\r\n--> 163             loss.backward()\r\n    164             optimizer.step()\r\n    165 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n     91                 products. Defaults to ``False``.\r\n     92         \"\"\"\r\n---> 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n     94 \r\n     95     def register_hook(self, hook):\r\n\r\n~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     87     Variable._execution_engine.run_backward(\r\n     88         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 89         allow_unreachable=True)  # allow_unreachable flag\r\n     90 \r\n     91 \r\n\r\nRuntimeError: No grad accumulator for a saved leaf!\r\n```\r\n\r\n## Code example\r\nI'm using ``torch 0.4.0`` and ``Cw, Cc, Qw, Qc`` are elements in a batch with ``requires_grad=True``. ``a`` is the label, a tensor with ``requires_grad=False``.\r\n\r\nmy code looks like as follows:\r\n```python\r\nfor ids, Cw, Cc, Qw, Qc, a in train_loader:\r\n    optimizer.zero_grad()\r\n    pred1, pred2 = model(Cw, Cc, Qw, Qc)\r\n    loss1 = F.cross_entropy(pred1, a[:, 0])\r\n    loss2 = F.cross_entropy(pred2, a[:, 0])\r\n    loss = loss1 + loss2\r\n    loss.backward()\r\n    optimizer.step()\r\n```\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.12.6\r\nGCC version: Could not collect\r\nCMake version: version 3.9.6\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy (0.4.1)\r\n[pip3] numpy (1.14.2)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] pytorchviz (0.0.1)\r\n[pip3] tensorboard-pytorch (0.7.1)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchfile (0.1.0)\r\n[pip3] torchtext (0.2.3)\r\n[pip3] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda0.0_cudnn0.0_1    pytorch\r\n[conda] pytorchviz                0.0.1                     <pip>\r\n[conda] tensorboard-pytorch       0.7.1                     <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchtext                 0.2.3                     <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch"}