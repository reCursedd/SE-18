{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/339266229", "html_url": "https://github.com/pytorch/pytorch/issues/3264#issuecomment-339266229", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3264", "id": 339266229, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTI2NjIyOQ==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-25T09:10:50Z", "updated_at": "2017-10-25T09:10:50Z", "author_association": "COLLABORATOR", "body_html": "<p>Hi,<br>\nThe problem is that you use the norm 1, not the sum in this case (there is an absolute value difference).<br>\nThe problem is that the current implementation of the norms with p &lt; 1 has a <code>nan</code> gradient at 0 (I guess you were expecting it to return the subgradient of 0).<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> should we give this a similar treatment as <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"250308860\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2421\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2421/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2421\">#2421</a> ?</p>", "body_text": "Hi,\nThe problem is that you use the norm 1, not the sum in this case (there is an absolute value difference).\nThe problem is that the current implementation of the norms with p < 1 has a nan gradient at 0 (I guess you were expecting it to return the subgradient of 0).\n@soumith should we give this a similar treatment as #2421 ?", "body": "Hi,\r\nThe problem is that you use the norm 1, not the sum in this case (there is an absolute value difference).\r\nThe problem is that the current implementation of the norms with p < 1 has a `nan` gradient at 0 (I guess you were expecting it to return the subgradient of 0).\r\n@soumith should we give this a similar treatment as #2421 ?"}