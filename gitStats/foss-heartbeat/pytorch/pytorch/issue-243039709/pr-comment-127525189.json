{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/127525189", "pull_request_review_id": 50124774, "id": 127525189, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNzUyNTE4OQ==", "diff_hunk": "@@ -115,4 +116,99 @@ def __repr__(self):\n             + ', in2_features=' + str(self.in2_features) \\\n             + ', out_features=' + str(self.out_features) + ')'\n \n+\n+class NoisyLinear(Module):\n+    \"\"\"Applies a noisy linear transformation to the incoming data:\n+        :math:`y = (mu_w + sigma_w \\cdot epsilon_w)x\n+            + mu_b + sigma_b \\cdot epsilon_b`\n+    More details can be found in the paper `Noisy Networks for Exploration` _ .\n+    Args:\n+        in_features: size of each input sample\n+        out_features: size of each output sample\n+        bias: If set to False, the layer will not learn an additive bias.\n+            Default: True\n+        factorised: whether or not to use factorised noise.\n+            Default: True\n+        std_init: initialization constant for standard deviation component of\n+            weights. If None, defaults to 0.017 for independent and 0.4 for\n+            factorised. Default: None\n+    Shape:\n+        - Input: :math:`(N, in\\_features)`\n+        - Output: :math:`(N, out\\_features)`\n+    Attributes:\n+        weight: the learnable weights of the module of shape\n+            (out_features x in_features)\n+        bias:   the learnable bias of the module of shape (out_features)\n+    Examples::\n+        >>> m = nn.NoisyLinear(20, 30)\n+        >>> input = autograd.Variable(torch.randn(128, 20))\n+        >>> output = m(input)\n+        >>> print(output.size())\n+    \"\"\"\n+    def __init__(self, in_features, out_features, bias=True, factorised=True, std_init=None):\n+        super(NoisyLinear, self).__init__()\n+        self.in_features = in_features\n+        self.out_features = out_features\n+        self.factorised = factorised\n+        self.weight_mu = Parameter(torch.Tensor(out_features, in_features))\n+        self.weight_sigma = Parameter(torch.Tensor(out_features, in_features))\n+        if bias:\n+            self.bias_mu = Parameter(torch.Tensor(out_features))\n+            self.bias_sigma = Parameter(torch.Tensor(out_features))\n+        else:\n+            self.register_parameter('bias', None)\n+        if not std_init:\n+            if self.factorised:\n+                self.std_init = 0.4\n+            else:\n+                self.std_init = 0.017\n+        else:\n+            self.std_init = std_init\n+        self.reset_parameters(bias)\n+        self.reset_noise()\n+\n+    def reset_parameters(self, bias):\n+        if self.factorised:\n+            mu_range = 1. / math.sqrt(self.weight_mu.size(1))\n+            self.weight_mu.data.uniform_(-mu_range, mu_range)\n+            self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.weight_sigma.size(1)))\n+            if bias:\n+                self.bias_mu.data.uniform_(-mu_range, mu_range)\n+                self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.bias_sigma.size(0)))\n+        else:\n+            mu_range = math.sqrt(3. / self.weight_mu.size(1))\n+            self.weight_mu.data.uniform_(-mu_range, mu_range)\n+            self.weight_sigma.data.fill_(self.std_init)\n+            if bias:\n+                self.bias_mu.data.uniform_(-mu_range, mu_range)\n+                self.bias_sigma.data.fill_(self.std_init)\n+\n+    def scale_noise(self, size):\n+        x = torch.Tensor(size).normal_()\n+        x = x.sign().mul(x.abs().sqrt())\n+        return x\n+\n+    def reset_noise(self):\n+        if self.factorised:\n+            epsilon_in = self.scale_noise(self.in_features)\n+            epsilon_out = self.scale_noise(self.out_features)\n+            self.weight_epsilon = Variable(epsilon_out.ger(epsilon_in))\n+            self.bias_epsilon = Variable(self.scale_noise(self.out_features))\n+        else:\n+            self.weight_epsilon = Variable(torch.Tensor((self.out_features, self.in_features)).normal_())\n+            self.bias_epsilon = Variable(torch.Tensor(self.out_features).normal_())\n+\n+    def forward(self, input):\n+        if self.training:\n+            return F.linear(input,\n+                            self.weight_mu + self.weight_sigma.mul(self.weight_epsilon),\n+                            self.bias_mu + self.bias_sigma.mul(self.bias_epsilon))\n+        else:\n+            return F.linear(input, self.weight_mu, self.bias_mu)\n+\n+    def __repr__(self):\n+        return self.__class__.__name__ + ' (' \\\n+            + str(self.in_features) + ' -> ' \\\n+            + str(self.out_features) + ')'", "path": "torch/nn/modules/linear.py", "position": null, "original_position": 114, "commit_id": "82337532f2880ab4f42cfa98f9596c351fa90974", "original_commit_id": "4652c7fd02c1ddea7c6da0f21c5e3448ce933c9f", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "body": "It's probably worth including `self.factorised` in this as well. `self.bias` is left out of `nn.Linear`, and `self.std_init` is only applicable at init.", "created_at": "2017-07-14T18:51:25Z", "updated_at": "2018-11-23T15:34:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/2103#discussion_r127525189", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2103", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/127525189"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2103#discussion_r127525189"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2103"}}, "body_html": "<p>It's probably worth including <code>self.factorised</code> in this as well. <code>self.bias</code> is left out of <code>nn.Linear</code>, and <code>self.std_init</code> is only applicable at init.</p>", "body_text": "It's probably worth including self.factorised in this as well. self.bias is left out of nn.Linear, and self.std_init is only applicable at init."}