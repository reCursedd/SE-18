{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/316362638", "html_url": "https://github.com/pytorch/pytorch/pull/2103#issuecomment-316362638", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2103", "id": 316362638, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjM2MjYzOA==", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-19T11:57:20Z", "updated_at": "2017-07-19T11:57:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=687194\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alykhantejani\">@alykhantejani</a> Apart from the dict tests I think 2 custom tests would be good:</p>\n<ul>\n<li>In training mode, show that output 1, given a fixed input, is reasonably different from output 2, given the same input, but after <code>reset_noise</code> has been called. There is already an <code>assertNotEqual</code> function available to use.</li>\n<li>In evaluation mode, the output matches a linear layer with the same weights and biases.</li>\n</ul>\n<p>It is pretty critical that the module shows these behaviours beyond simply passing the automatic differentiation checks.</p>", "body_text": "@alykhantejani Apart from the dict tests I think 2 custom tests would be good:\n\nIn training mode, show that output 1, given a fixed input, is reasonably different from output 2, given the same input, but after reset_noise has been called. There is already an assertNotEqual function available to use.\nIn evaluation mode, the output matches a linear layer with the same weights and biases.\n\nIt is pretty critical that the module shows these behaviours beyond simply passing the automatic differentiation checks.", "body": "@alykhantejani Apart from the dict tests I think 2 custom tests would be good:\r\n\r\n- In training mode, show that output 1, given a fixed input, is reasonably different from output 2, given the same input, but after `reset_noise` has been called. There is already an `assertNotEqual` function available to use.\r\n- In evaluation mode, the output matches a linear layer with the same weights and biases.\r\n\r\nIt is pretty critical that the module shows these behaviours beyond simply passing the automatic differentiation checks."}