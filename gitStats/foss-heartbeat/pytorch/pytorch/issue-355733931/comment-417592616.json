{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/417592616", "html_url": "https://github.com/pytorch/pytorch/pull/11104#issuecomment-417592616", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11104", "id": 417592616, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzU5MjYxNg==", "user": {"login": "striajan", "id": 1244058, "node_id": "MDQ6VXNlcjEyNDQwNTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1244058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/striajan", "html_url": "https://github.com/striajan", "followers_url": "https://api.github.com/users/striajan/followers", "following_url": "https://api.github.com/users/striajan/following{/other_user}", "gists_url": "https://api.github.com/users/striajan/gists{/gist_id}", "starred_url": "https://api.github.com/users/striajan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/striajan/subscriptions", "organizations_url": "https://api.github.com/users/striajan/orgs", "repos_url": "https://api.github.com/users/striajan/repos", "events_url": "https://api.github.com/users/striajan/events{/privacy}", "received_events_url": "https://api.github.com/users/striajan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-31T08:28:27Z", "updated_at": "2018-08-31T08:36:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I also want to comment on the different implementations of the <a href=\"https://arxiv.org/pdf/1608.03983.pdf\" rel=\"nofollow\">equation 5</a>. Let me remind that in the paper T_cur iterates from 0 to T_i in each run. In my and TensorFlow implementation it iterates from 0 to T_i - 1. In <a href=\"https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/python/training/learning_rate_decay.py#L586\">TensorFlow</a> they use T_cur / T_i. In my code I use T_cur / (T_i - 1) to reach eta_min in the last iteration of each run. However, since eta_min defaults to 0, the gradients don't get updated in the last iteration. Therefore it might be better to use TensorFlow approach. Please let me know.</p>", "body_text": "I also want to comment on the different implementations of the equation 5. Let me remind that in the paper T_cur iterates from 0 to T_i in each run. In my and TensorFlow implementation it iterates from 0 to T_i - 1. In TensorFlow they use T_cur / T_i. In my code I use T_cur / (T_i - 1) to reach eta_min in the last iteration of each run. However, since eta_min defaults to 0, the gradients don't get updated in the last iteration. Therefore it might be better to use TensorFlow approach. Please let me know.", "body": "I also want to comment on the different implementations of the [equation 5](https://arxiv.org/pdf/1608.03983.pdf). Let me remind that in the paper T_cur iterates from 0 to T_i in each run. In my and TensorFlow implementation it iterates from 0 to T_i - 1. In [TensorFlow](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/python/training/learning_rate_decay.py#L586) they use T_cur / T_i. In my code I use T_cur / (T_i - 1) to reach eta_min in the last iteration of each run. However, since eta_min defaults to 0, the gradients don't get updated in the last iteration. Therefore it might be better to use TensorFlow approach. Please let me know."}