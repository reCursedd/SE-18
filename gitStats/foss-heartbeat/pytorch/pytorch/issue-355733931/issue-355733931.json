{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11104", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11104/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11104/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11104/events", "html_url": "https://github.com/pytorch/pytorch/pull/11104", "id": 355733931, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEyMTk1MDY2", "number": 11104, "title": "Cosine annealing with restarts", "user": {"login": "striajan", "id": 1244058, "node_id": "MDQ6VXNlcjEyNDQwNTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1244058?v=4", "gravatar_id": "", "url": "https://api.github.com/users/striajan", "html_url": "https://github.com/striajan", "followers_url": "https://api.github.com/users/striajan/followers", "following_url": "https://api.github.com/users/striajan/following{/other_user}", "gists_url": "https://api.github.com/users/striajan/gists{/gist_id}", "starred_url": "https://api.github.com/users/striajan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/striajan/subscriptions", "organizations_url": "https://api.github.com/users/striajan/orgs", "repos_url": "https://api.github.com/users/striajan/repos", "events_url": "https://api.github.com/users/striajan/events{/privacy}", "received_events_url": "https://api.github.com/users/striajan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-08-30T20:04:01Z", "updated_at": "2018-09-21T20:35:07Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11104", "html_url": "https://github.com/pytorch/pytorch/pull/11104", "diff_url": "https://github.com/pytorch/pytorch/pull/11104.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11104.patch"}, "body_html": "<p>I implemented cosine annealing with warm restarts. The implementation is stateless, computing number of already performed restarts and epoch of the last restart directly from the <code>self.last_epoch</code> variable. This resolves issues in the implementations <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"310071130\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6130\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6130/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/6130\">#6130</a> and <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"326268517\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7821\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7821/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/7821\">#7821</a>. It was inspired by TensorFlow <a href=\"https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/python/training/learning_rate_decay.py#L514\">implementation</a>, but it was implemented in more readable style.</p>\n<p>There is one important difference compared to the original <a href=\"https://arxiv.org/abs/1608.03983\" rel=\"nofollow\">paper</a>. The authors let the variable T_cur go from 0 to T_i (i-th run has T_i + 1 iterations). In my implementation, T_cur goes from 0 to T_i - 1 (i-th run has T_i iterations only). This is more consistent with <code>StepLR</code>, where the decay happens at <code>step_size</code>. It is also more convenient to use T_max (and T_i computed from it) to define length of the period between restarts. This approach is used in TensorFlow as well. The difference is that they use T_cur / T_i instead of T_cur / (T_i - 1), therefore never reaching the minimum LR. If the TensorFlow approach is more preferred, I can update my code easily.</p>\n<p>I have also included a possibility of decaying the initial LR after each restart by the multiplicative factor gamma. This was inspired by TensorFlow implementation. The paper doesn't mention this explicitly, although it defines the minimum and maximum eta for the i-th run.</p>\n<p>Here are some example plots of generated LR schedules. It might be nice to have them in the documentation, as we do have plots for various activation functions. I can add the generating script to docs/src/scripts if desired.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1244058/44874831-af41c980-ac9c-11e8-9942-e520a5edd7be.png\"><img src=\"https://user-images.githubusercontent.com/1244058/44874831-af41c980-ac9c-11e8-9942-e520a5edd7be.png\" alt=\"cosineannealingrestartslr\" style=\"max-width:100%;\"></a></p>\n<p>I implemented the schedule in a separate class <code>CosineAnnealingRestartsLR</code> to avoid possible inconsistencies with <code>CosineAnnealingLR</code>, which were mentioned in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"310071130\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6130\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6130/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/6130\">#6130</a>. TensorFlow has two independent implementations as well.</p>\n<p>I am not completely happy with naming of variables. In my opinion, <code>eta_min</code> should be named <code>lr_min</code>, since PyTorch uses <code>lr</code> and <code>base_lr</code>. I also don't like names <code>T_max</code>, <code>T_mult</code> and <code>T_i</code> which violate <a href=\"https://www.python.org/dev/peps/pep-0008/\" rel=\"nofollow\">PEP 8</a>. I used these names to be consistent with <code>CosineAnnealingLR</code> and with the paper. I can update them if desired.</p>", "body_text": "I implemented cosine annealing with warm restarts. The implementation is stateless, computing number of already performed restarts and epoch of the last restart directly from the self.last_epoch variable. This resolves issues in the implementations #6130 and #7821. It was inspired by TensorFlow implementation, but it was implemented in more readable style.\nThere is one important difference compared to the original paper. The authors let the variable T_cur go from 0 to T_i (i-th run has T_i + 1 iterations). In my implementation, T_cur goes from 0 to T_i - 1 (i-th run has T_i iterations only). This is more consistent with StepLR, where the decay happens at step_size. It is also more convenient to use T_max (and T_i computed from it) to define length of the period between restarts. This approach is used in TensorFlow as well. The difference is that they use T_cur / T_i instead of T_cur / (T_i - 1), therefore never reaching the minimum LR. If the TensorFlow approach is more preferred, I can update my code easily.\nI have also included a possibility of decaying the initial LR after each restart by the multiplicative factor gamma. This was inspired by TensorFlow implementation. The paper doesn't mention this explicitly, although it defines the minimum and maximum eta for the i-th run.\nHere are some example plots of generated LR schedules. It might be nice to have them in the documentation, as we do have plots for various activation functions. I can add the generating script to docs/src/scripts if desired.\n\nI implemented the schedule in a separate class CosineAnnealingRestartsLR to avoid possible inconsistencies with CosineAnnealingLR, which were mentioned in #6130. TensorFlow has two independent implementations as well.\nI am not completely happy with naming of variables. In my opinion, eta_min should be named lr_min, since PyTorch uses lr and base_lr. I also don't like names T_max, T_mult and T_i which violate PEP 8. I used these names to be consistent with CosineAnnealingLR and with the paper. I can update them if desired.", "body": "I implemented cosine annealing with warm restarts. The implementation is stateless, computing number of already performed restarts and epoch of the last restart directly from the `self.last_epoch` variable. This resolves issues in the implementations #6130 and #7821. It was inspired by TensorFlow [implementation](https://github.com/tensorflow/tensorflow/blob/25c197e02393bd44f50079945409009dd4d434f8/tensorflow/python/training/learning_rate_decay.py#L514), but it was implemented in more readable style.\r\n\r\nThere is one important difference compared to the original [paper](https://arxiv.org/abs/1608.03983). The authors let the variable T_cur go from 0 to T_i (i-th run has T_i + 1 iterations). In my implementation, T_cur goes from 0 to T_i - 1 (i-th run has T_i iterations only). This is more consistent with `StepLR`, where the decay happens at `step_size`. It is also more convenient to use T_max (and T_i computed from it) to define length of the period between restarts. This approach is used in TensorFlow as well. The difference is that they use T_cur / T_i instead of T_cur / (T_i - 1), therefore never reaching the minimum LR. If the TensorFlow approach is more preferred, I can update my code easily.\r\n\r\nI have also included a possibility of decaying the initial LR after each restart by the multiplicative factor gamma. This was inspired by TensorFlow implementation. The paper doesn't mention this explicitly, although it defines the minimum and maximum eta for the i-th run.\r\n\r\nHere are some example plots of generated LR schedules. It might be nice to have them in the documentation, as we do have plots for various activation functions. I can add the generating script to docs/src/scripts if desired.\r\n\r\n![cosineannealingrestartslr](https://user-images.githubusercontent.com/1244058/44874831-af41c980-ac9c-11e8-9942-e520a5edd7be.png)\r\n\r\nI implemented the schedule in a separate class `CosineAnnealingRestartsLR` to avoid possible inconsistencies with `CosineAnnealingLR`, which were mentioned in #6130. TensorFlow has two independent implementations as well.\r\n\r\nI am not completely happy with naming of variables. In my opinion, `eta_min` should be named `lr_min`, since PyTorch uses `lr` and `base_lr`. I also don't like names `T_max`, `T_mult` and `T_i` which violate [PEP 8](https://www.python.org/dev/peps/pep-0008/). I used these names to be consistent with `CosineAnnealingLR` and with the paper. I can update them if desired."}