{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195768159", "pull_request_review_id": 129202440, "id": 195768159, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTc2ODE1OQ==", "diff_hunk": "@@ -0,0 +1,469 @@\n+#include <ATen/ATen.h>\n+#include <ATen/NativeFunctions.h>\n+#include <ATen/SparseTensorRef.h>\n+#include <ATen/ExpandUtils.h>\n+\n+namespace at { namespace native {\n+\n+namespace {\n+  // NB: Even though some of the functions we have ported are CUDA\n+  // friendly, flipping the switch between native and non-native is\n+  // an all or nothing affair, because the internal representation\n+  // is different\n+  static bool _has_native(const Tensor& self) {\n+    return self.is_sparse() && !self.is_cuda();\n+  }\n+\n+  static bool _type_has_native(const Type& dtype) {\n+    return dtype.is_sparse() && !dtype.is_cuda();\n+  }\n+}\n+\n+// These native operations are not \"really\" native; they're actually just bridge\n+// functions that decide whether or not to call native sparse functions, or\n+// TH functions.  This file should be temporary; when all of TH gets ported, we\n+// can just use the native mechanism straight.\n+\n+// TODO: Maybe the foo_ variants should call th_foo_\n+\n+Tensor norm(const Tensor & self, Scalar p) {\n+  if (_has_native(self)) {\n+    return native_norm(self, p);\n+  } else {\n+    return th_norm(self, p);\n+  }\n+}\n+\n+Tensor clone(const Tensor& self) {\n+  if (_has_native(self)) {\n+    return native_clone(self);\n+  } else {\n+    return th_clone(self);\n+  }\n+}\n+\n+Tensor& resize_as_(Tensor& self, const Tensor& the_template) {\n+  if (_has_native(self)) {\n+    return native_resize_as_(self, the_template);\n+  } else {\n+    return th_resize_as_(self, the_template);\n+  }\n+}\n+\n+Tensor& pow_out(Tensor& result, const Tensor& self, Scalar exponent) {\n+  if (_has_native(self)) {\n+    return native_pow_out(result, self, exponent);\n+  } else {\n+    return th_pow_out(result, self, exponent);\n+  }\n+}\n+\n+Tensor pow(const Tensor& self, Scalar exponent) {\n+  if (_has_native(self)) {\n+    return native_pow(self, exponent);\n+  } else {\n+    return th_pow(self, exponent);\n+  }\n+}\n+\n+Tensor& zero_(Tensor& self) {\n+  if (_has_native(self)) {\n+    return native_zero_(self);\n+  } else {\n+    return th_zero_(self);\n+  }\n+}\n+\n+// Note [CPU sparse is globally native]\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+// The current state of affairs is as follows:\n+//\n+//  - CPU sparse functionality is implemented natively\n+//  - CUDA sparse functionality, and all other functionality, are implemented\n+//    in TH.\n+//\n+// Thus, we need these trampoline functions, to help us decide whether or\n+// not we can go to native implementations or not.  We expect the trampolines\n+// to go away when things get ported to native for real.\n+\n+// Note [Multiple dispatch to sparse]\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+// In principle, there are some degrees of freedom in how we could gotten to our\n+// native CPU sparse implementations.  In particular, the handling of how we are\n+// simulating multiple dispatch is asymmetric with how th_add_out handles the\n+// multiple dispatch.  We justify this in two ways: (1) knowing when to", "path": "aten/src/ATen/native/LegacyBridge.cpp", "position": null, "original_position": 94, "commit_id": "cdf42cdb1df7b90da99aa5914ec208ab1d396d2e", "original_commit_id": "557a724094be5fd8737c29bee24b7c8b9a4f2e82", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "```\r\n// Note [Multiple dispatch to sparse]\r\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n// In an ideal world, we would use direct support for multiple dispatch to\r\n// say that add(Dense, Dense) should dispatch to one function, while\r\n// add(Dense, Sparse) should dispatch to another function.\r\n//\r\n// In a world where we only have single dispatch, we can single dispatch on\r\n// the first function, and then do an is_sparse() test on the second argument\r\n// to direct ourselves to the correct argument.\r\n//\r\n// We are in neither of those worlds.  Instead, we have a th_add function\r\n// which has legacy implementations in the single dispatch world, BUT our\r\n// actual add function needs to call s_native_add if the function *would have*\r\n// utilized a kernel implemented in THS (CPU sparse tensors).\r\n//\r\n// th_add is \"good old single dispatch\" which internally handles the is_sparse()\r\n// test and also handles broadcasting.  s_native_add works asymmetrically:\r\n// it doesn't handle broadcasting at all, and it ASSUMES that the relevant\r\n// argument is a CPU sparse tensor.  Why the asymmetry?  It turns out it is not\r\n// so easy to figure out if a kernel is implemented in THS; it's not as simple\r\n// as testing if the first argument is CPU and sparse, because, e.g.,\r\n// in add(Dense, Sparse), the sparse kernel is in the second argument.  So,\r\n// the trampoline function is going to know about the overloads *anyway*; it\r\n// might as well also handle is_sparse() and broadcasting while it's at it.\r\n//\r\n// Why not change TH to follow this new scheme?  We could... but since it's\r\n// all going away when we finish porting the TH functions to ATen, we haven't\r\n```\r\n\r\nNew comment", "created_at": "2018-06-15T14:58:45Z", "updated_at": "2018-11-23T15:45:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/8409#discussion_r195768159", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8409", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195768159"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8409#discussion_r195768159"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8409"}}, "body_html": "<pre><code>// Note [Multiple dispatch to sparse]\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n// In an ideal world, we would use direct support for multiple dispatch to\n// say that add(Dense, Dense) should dispatch to one function, while\n// add(Dense, Sparse) should dispatch to another function.\n//\n// In a world where we only have single dispatch, we can single dispatch on\n// the first function, and then do an is_sparse() test on the second argument\n// to direct ourselves to the correct argument.\n//\n// We are in neither of those worlds.  Instead, we have a th_add function\n// which has legacy implementations in the single dispatch world, BUT our\n// actual add function needs to call s_native_add if the function *would have*\n// utilized a kernel implemented in THS (CPU sparse tensors).\n//\n// th_add is \"good old single dispatch\" which internally handles the is_sparse()\n// test and also handles broadcasting.  s_native_add works asymmetrically:\n// it doesn't handle broadcasting at all, and it ASSUMES that the relevant\n// argument is a CPU sparse tensor.  Why the asymmetry?  It turns out it is not\n// so easy to figure out if a kernel is implemented in THS; it's not as simple\n// as testing if the first argument is CPU and sparse, because, e.g.,\n// in add(Dense, Sparse), the sparse kernel is in the second argument.  So,\n// the trampoline function is going to know about the overloads *anyway*; it\n// might as well also handle is_sparse() and broadcasting while it's at it.\n//\n// Why not change TH to follow this new scheme?  We could... but since it's\n// all going away when we finish porting the TH functions to ATen, we haven't\n</code></pre>\n<p>New comment</p>", "body_text": "// Note [Multiple dispatch to sparse]\n// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n// In an ideal world, we would use direct support for multiple dispatch to\n// say that add(Dense, Dense) should dispatch to one function, while\n// add(Dense, Sparse) should dispatch to another function.\n//\n// In a world where we only have single dispatch, we can single dispatch on\n// the first function, and then do an is_sparse() test on the second argument\n// to direct ourselves to the correct argument.\n//\n// We are in neither of those worlds.  Instead, we have a th_add function\n// which has legacy implementations in the single dispatch world, BUT our\n// actual add function needs to call s_native_add if the function *would have*\n// utilized a kernel implemented in THS (CPU sparse tensors).\n//\n// th_add is \"good old single dispatch\" which internally handles the is_sparse()\n// test and also handles broadcasting.  s_native_add works asymmetrically:\n// it doesn't handle broadcasting at all, and it ASSUMES that the relevant\n// argument is a CPU sparse tensor.  Why the asymmetry?  It turns out it is not\n// so easy to figure out if a kernel is implemented in THS; it's not as simple\n// as testing if the first argument is CPU and sparse, because, e.g.,\n// in add(Dense, Sparse), the sparse kernel is in the second argument.  So,\n// the trampoline function is going to know about the overloads *anyway*; it\n// might as well also handle is_sparse() and broadcasting while it's at it.\n//\n// Why not change TH to follow this new scheme?  We could... but since it's\n// all going away when we finish porting the TH functions to ATen, we haven't\n\nNew comment", "in_reply_to_id": 195561030}