{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195770867", "pull_request_review_id": 129205670, "id": 195770867, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTc3MDg2Nw==", "diff_hunk": "@@ -65,218 +51,81 @@ THTensor *THSTensor_(newValues)(const THSTensor *self) {\n /*** Helper methods ***/\n static void THSTensor_(rawInit)(THSTensor *self)\n {\n-  new (&self->refcount) std::atomic<int>(1);\n-  self->size = NULL;\n-  self->indices = THLongTensor_new();\n-  self->values = THTensor_(new)();\n-  self->nDimensionI = 0;\n-  self->nDimensionV = 0;\n-  self->coalesced = 0;\n-  self->nnz = 0;\n-  // self->flag = TH_TENSOR_REFCOUNTED;\n+  THError(\"Internal error! THSTensor_(rawInit)(self) shouldn't be called; dtype.tensor() allocated sparse tensors should already be initialized\");\n }\n \n THSTensor* THSTensor_(rawResize)(THSTensor *self, int nDimI, int nDimV, int64_t *size) {\n-  // Only resize valid sizes into tensor.\n-  self->size = (int64_t *)THRealloc(self->size, sizeof(int64_t)*(nDimI + nDimV));\n-\n-  for (int64_t d = 0; d < nDimI + nDimV; d++) {\n-    self->size[d] = size[d];\n-  }\n-  self->nDimensionI = nDimI;\n-  self->nDimensionV = nDimV;\n-\n-  return self;\n+  THError(\"Internal error! THSTensor_(rawResize)(self, nDimI, nDimV, size) shouldn't be called; use _get_sparse_impl(self)->raw_resize_(dimI, dimV, size) instead\");\n }\n \n // directly assign without cloning or retaining (internal method)\n THSTensor* THSTensor_(_move)(THSTensor *self, THLongTensor *indices, THTensor *values) {\n-  int empty = THTensor_(nDimension)(values) == 0;\n-  if (!empty) {\n-    THArgCheck(THLongTensor_nDimension(indices) == 2, 1,\n-        \"indices must be nDim x nnz\");\n-    THArgCheck(THLongTensor_size(indices, 1) == THTensor_(size)(values, 0), 1,\n-        \"indices and values must have same nnz\");\n-    THArgCheck(THLongTensor_size(indices, 0) == self->nDimensionI, 2,\n-        \"indices has incorrect first dimension, expected %d, got %d\", self->nDimensionI, THLongTensor_size(indices, 0));\n-    THArgCheck(THTensor_(nDimension)(values) == self->nDimensionV + 1, 3,\n-        \"values has incorrect number of dimensions, expected %d, got %d\", self->nDimensionV + 1, THTensor_(nDimension)(values));\n-  } else {\n-    THArgCheck(THLongTensor_nDimension(indices) == 0, 2,\n-        \"if values is empty, indices must be empty too\");\n-  }\n-  THLongTensor_free(self->indices);\n-  THTensor_(free)(self->values);\n-  self->indices = indices;\n-  self->values = values;\n-  self->nnz = empty ? 0 : THTensor_(size)(values, 0);\n-  self->coalesced = 0;\n-\n-  return self;\n+  THError(\"Internal error! THSTensor_(_move)(self, indices, values) shouldn't be called; use _alias_into_sparse(self, indices, values) instead\");\n }\n \n THSTensor* THSTensor_(_set)(THSTensor *self, THLongTensor *indices, THTensor *values) {\n-  // Note: Not like torch.set, this is an internal method\n-  return THSTensor_(_move)(\n-    self, THLongTensor_newClone(indices), THTensor_(newClone)(values));\n+  THError(\"Internal error! THSTensor_(_set)(self, indices, values) shouldn't be called; use _copy_into_sparse(self, indices, values) instead\");\n }\n \n static inline THSTensor* THSTensor_(_newWithDimsAndTensor)(int64_t nDimI, int64_t nDimV, int64_t *sizes, THLongTensor *indices, THTensor *values) {\n-  THSTensor *self = THSTensor_(new)();\n-  THSTensor_(rawResize)(self, nDimI, nDimV, sizes);\n-\n-  // NB: by default, we do NOT clone indices/values into the sparse tensor.\n-  // Efficient API by default!\n-  THSTensor_(_move)(self, THLongTensor_newWithTensor(indices), THTensor_(newWithTensor)(values));\n-  return self;\n+  THError(\"Internal error! THSTensor_(_newWithDimsAndTensor)(nDimI, nDimV, sizes, indices, values) shouldn't be called; use _new_with_dims_and_tensor_sparse(dtype, nDimI, nDimV, sizes, indices, values) instead\");\n }\n \n /*** end helper methods ***/\n \n /* Empty init */\n THSTensor *THSTensor_(new)(void)\n {\n-  THSTensor *self = (THSTensor *)THAlloc(sizeof(THSTensor));\n-  THSTensor_(rawInit)(self);\n-  return self;\n+  THError(\"Internal error! THSTensor_(new)() shouldn't be called; use dtype.tensor() instead\");", "path": "aten/src/THS/generic/THSTensor.cpp", "position": 147, "original_position": 145, "commit_id": "cdf42cdb1df7b90da99aa5914ec208ab1d396d2e", "original_commit_id": "557a724094be5fd8737c29bee24b7c8b9a4f2e82", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "I kept it so that, if we eventually have these call ATen, all of the templates will still be here so it's an easier patch to do. Otherwise whoever does it has to dig out the old files, wire them all back up, and then start doing replacements.", "created_at": "2018-06-15T15:06:33Z", "updated_at": "2018-11-23T15:45:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/8409#discussion_r195770867", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8409", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195770867"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8409#discussion_r195770867"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8409"}}, "body_html": "<p>I kept it so that, if we eventually have these call ATen, all of the templates will still be here so it's an easier patch to do. Otherwise whoever does it has to dig out the old files, wire them all back up, and then start doing replacements.</p>", "body_text": "I kept it so that, if we eventually have these call ATen, all of the templates will still be here so it's an easier patch to do. Otherwise whoever does it has to dig out the old files, wire them all back up, and then start doing replacements.", "in_reply_to_id": 195565177}