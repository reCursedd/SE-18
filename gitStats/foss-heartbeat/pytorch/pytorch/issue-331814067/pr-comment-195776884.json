{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195776884", "pull_request_review_id": 129213434, "id": 195776884, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTc3Njg4NA==", "diff_hunk": "@@ -1164,3 +1164,383 @@\n   dispatch:\n     CPU: _s_poisson_cpu\n     CUDA: _s_poisson_cuda\n+\n+# When more variants get ported to native, this dispatch will get more\n+# complicated\n+\n+- func: native_norm(Tensor self, Scalar p=2) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: norm_sparse\n+\n+- func: norm(Tensor self, Scalar p=2) -> Tensor\n+  variants: method, function\n+\n+- func: native_clone(Tensor self) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: clone_sparse\n+\n+- func: clone(Tensor self) -> Tensor\n+\n+- func: native_resize_as_(Tensor self, Tensor the_template) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: resize_as_sparse_\n+\n+- func: resize_as_(Tensor self, Tensor the_template) -> Tensor\n+\n+- func: native_pow_out(Tensor result, Tensor self, Scalar exponent) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: pow_out_sparse_scalar\n+\n+- func: native_pow(Tensor self, Scalar exponent) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: pow_sparse_scalar\n+\n+- func: pow_out(Tensor result, Tensor self, Scalar exponent) -> Tensor\n+  variants: function\n+\n+- func: pow(Tensor self, Scalar exponent) -> Tensor\n+  variants: method, function\n+\n+- func: native_zero_(Tensor self) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: zero_sparse_\n+\n+- func: zero_(Tensor self) -> Tensor\n+\n+\n+\n+- func: s_native_add_out(Tensor result, Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: s_add_out_sparse_cpu\n+\n+- func: native_add_out(Tensor result, Tensor self, SparseTensorRef other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    CPU: add_out_dense_sparse_cpu\n+\n+- func: s_native_add(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: s_add_sparse_cpu\n+\n+- func: native_add(Tensor self, SparseTensorRef other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    CPU: add_dense_sparse_cpu\n+\n+- func: s_native_add_(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: s_add_sparse_cpu_\n+\n+- func: native_add_(Tensor self, SparseTensorRef other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    CPU: add_dense_sparse_cpu_\n+\n+- func: add_out(Tensor result, Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+\n+- func: add(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: method, function\n+\n+- func: add_(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: method\n+\n+\n+\n+- func: s_native_sub_out(Tensor result, Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: s_sub_out_sparse_cpu\n+\n+- func: s_native_sub(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: s_sub_sparse_cpu\n+\n+- func: s_native_sub_(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: s_sub_sparse_cpu_\n+\n+- func: sub_out(Tensor result, Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: function\n+\n+- func: sub(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: method, function\n+\n+- func: sub_(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\n+  variants: method\n+\n+\n+\n+- func: s_native_mul_out(Tensor result, Tensor self, Tensor other) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: s_mul_out_sparse_cpu\n+\n+- func: s_native_mul(Tensor self, Tensor other) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: s_mul_sparse_cpu\n+\n+- func: s_native_mul_(Tensor self, Tensor other) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: s_mul_sparse_cpu_\n+\n+- func: native_mul_out(Tensor result, Tensor self, Scalar other) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: mul_out_sparse_scalar\n+\n+- func: native_mul(Tensor self, Scalar other) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: mul_sparse_scalar\n+\n+- func: native_mul_(Tensor self, Scalar other) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: mul_sparse_scalar_\n+\n+- func: mul_out(Tensor result, Tensor self, Tensor other) -> Tensor\n+  variants: function\n+\n+- func: mul_out(Tensor result, Tensor self, Scalar other) -> Tensor\n+  variants: function\n+\n+- func: mul(Tensor self, Tensor other) -> Tensor\n+  variants: method, function\n+\n+- func: mul(Tensor self, Scalar other) -> Tensor\n+  variants: method, function\n+\n+- func: mul_(Tensor self, Tensor other) -> Tensor\n+  variants: method\n+\n+- func: mul_(Tensor self, Scalar other) -> Tensor\n+  variants: method\n+\n+\n+\n+- func: native_div_out(Tensor result, Tensor self, Scalar other) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: div_out_sparse_scalar\n+\n+- func: native_div(Tensor self, Scalar other) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: div_sparse_scalar\n+\n+- func: native_div_(Tensor self, Scalar other) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: div_sparse_scalar_\n+\n+- func: div_out(Tensor result, Tensor self, Scalar other) -> Tensor\n+  variants: function\n+\n+- func: div(Tensor self, Scalar other) -> Tensor\n+  variants: method, function\n+\n+- func: div_(Tensor self, Scalar other) -> Tensor\n+  variants: method\n+\n+\n+- func: s_native_addmm_out(Tensor result, Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    CPU: s_addmm_out_sparse_dense_cpu\n+\n+- func: s_native_addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    CPU: s_addmm_sparse_dense_cpu\n+\n+- func: s_native_addmm_(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n+  variants: function\n+  dispatch:\n+    CPU: s_addmm_sparse_dense_cpu_\n+\n+- func: addmm_out(Tensor result, Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n+  variants: function\n+\n+- func: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n+  variants: method, function\n+\n+- func: addmm_(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\n+  variants: method\n+\n+\n+- func: native_tensor(Type self_ty) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: new_sparse\n+\n+- func: native_tensor(Type self_ty, IntList size) -> Tensor\n+  variants: function\n+  dispatch:\n+    SparseCPU: new_with_size_sparse\n+\n+- func: tensor(Type dtype) -> Tensor\n+  variants: function\n+\n+- func: tensor(Type dtype, IntList size) -> Tensor\n+  variants: function\n+\n+\n+# NB: The function overloads are removed to avoid a nasty bug where\n+# you say at::native_sparse_coo_tensor(indices, values), and then\n+# it does the dispatch based on indices (wrong wrong wrong!)  Without\n+# the variants, you must call this on type directly.\n+#\n+# Maybe this meant we were supposed to take a dtype as an argument here.\n+# Hmmmmm.\n+\n+- func: native_sparse_coo_tensor(IndexTensor indices, Tensor values) -> Tensor\n+  variants: []", "path": "aten/src/ATen/native/native_functions.yaml", "position": 248, "original_position": 248, "commit_id": "cdf42cdb1df7b90da99aa5914ec208ab1d396d2e", "original_commit_id": "557a724094be5fd8737c29bee24b7c8b9a4f2e82", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "I think we should consider methods on types to be an implementation detail, it doesn't match up with the (non-legacy) pytorch API.", "created_at": "2018-06-15T15:27:23Z", "updated_at": "2018-11-23T15:45:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/8409#discussion_r195776884", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8409", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195776884"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8409#discussion_r195776884"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8409"}}, "body_html": "<p>I think we should consider methods on types to be an implementation detail, it doesn't match up with the (non-legacy) pytorch API.</p>", "body_text": "I think we should consider methods on types to be an implementation detail, it doesn't match up with the (non-legacy) pytorch API.", "in_reply_to_id": 195563476}