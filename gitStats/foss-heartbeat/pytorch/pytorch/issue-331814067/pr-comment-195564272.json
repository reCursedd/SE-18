{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195564272", "pull_request_review_id": 128956733, "id": 195564272, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTU2NDI3Mg==", "diff_hunk": "@@ -0,0 +1,449 @@\n+// Basic functions on sparse tensors\n+\n+#include <ATen/ATen.h>\n+#include <ATen/SparseTensorImpl.h>\n+#include <ATen/native/BlasUtils.h>\n+#include <ATen/NativeFunctions.h>\n+\n+namespace at { namespace native {\n+\n+// Just for documentary purposes\n+using SparseTensor = Tensor;\n+using LongTensor = Tensor;\n+using SparseType = Type;\n+\n+namespace {\n+  // This is an internal utility function for getting at the SparseTensorImpl,\n+  // so that we can write sparse tensor specific accessors for special fields\n+  // in SparseTensor.  You should only use this for writing low level\n+  // setters/getters for SparseTensorImpl fields; otherwise, you should use\n+  // the low level setters/getters that were implemented using this.\n+  //\n+  // This may be called repeatedly, so make sure it's pretty cheap.\n+  SparseTensorImpl* _get_sparse_impl(const SparseTensor& self) {\n+    if (!self.is_sparse()) AT_ERROR(\"_internal_get_SparseTensorImpl: not a sparse tensor\");\n+    return static_cast<SparseTensorImpl*>(self.unsafeGetTensorImpl());\n+  }\n+}\n+\n+/******************************************************************************\n+ * access methods\n+ ******************************************************************************/\n+\n+int64_t _dimI_sparse(const SparseTensor& self) {\n+  return _get_sparse_impl(self)->dimI();\n+}\n+\n+int64_t _dimV_sparse(const SparseTensor& self) {\n+  return _get_sparse_impl(self)->dimV();\n+}\n+\n+bool is_coalesced_sparse(const SparseTensor& self) {\n+  return _get_sparse_impl(self)->coalesced();\n+}\n+\n+int64_t _nnz_sparse(const SparseTensor& self) {\n+  return _get_sparse_impl(self)->nnz();\n+}\n+\n+// TODO: This is wrong: if nnz == 0 but indices/values is not\n+// empty then we'll return all the values, even the ones that\n+// are \"masked out\" by nnz\n+\n+Tensor _indices_sparse(const SparseTensor& self) {\n+  auto nnz = self._nnz();\n+  if (nnz == 0) {\n+    // Narrows don't work on 0-length tensors\n+    // TODO: When we handle zero-size dims correctly, this will work and\n+    // we can remove the special case.\n+    return _get_sparse_impl(self)->indices();\n+  }\n+  return _get_sparse_impl(self)->indices().narrow(1, 0, nnz);\n+}\n+\n+Tensor _values_sparse(const SparseTensor& self) {\n+  // See indices for some relevant notes\n+  auto nnz = self._nnz();\n+  if (nnz == 0) {\n+    return _get_sparse_impl(self)->values();\n+  }\n+  return _get_sparse_impl(self)->values().narrow(0, 0, nnz);\n+}\n+\n+/******************************************************************************\n+ * creation methods\n+ ******************************************************************************/\n+\n+/* Empty init */\n+SparseTensor new_sparse(const SparseType& dtype) {\n+  AT_ASSERT(!dtype.is_undefined());\n+  AT_ASSERT(!dtype.is_variable());\n+  AT_ASSERT(dtype.is_sparse());\n+  // TODO: Hmm... this const_cast business seems a bit dodgy\n+  return SparseTensor(new SparseTensorImpl(const_cast<SparseType*>(&dtype)), /* retain */ false);\n+}\n+\n+/*** Helper methods ***/\n+\n+namespace {\n+  void _raw_resize_sparse(const SparseTensor& self, int64_t dimI, int64_t dimV, ArrayRef<int64_t> size) {\n+    _get_sparse_impl(self)->raw_resize_(dimI, dimV, size);\n+  }\n+\n+  // Takes indices and values and directly puts them into the sparse tensor, no\n+  // copy.  This used to be called THSTensor_(_move)\n+  void _alias_into_sparse(const SparseTensor& self, const LongTensor& indices, const Tensor& values) {\n+    _get_sparse_impl(self)->set_indices_and_values(indices, values);\n+  }\n+\n+  // Take indices and values and makes a (data) copy of them to put into the sparse\n+  // indices/values.  This used to be called THSTensor_(_set)\n+  void _copy_into_sparse(const SparseTensor& self, const LongTensor& indices, const Tensor& values) {\n+    _alias_into_sparse(self, indices.clone(), values.clone());\n+  }\n+\n+  // Does NOT make copies of indices/values\n+  SparseTensor _new_with_dims_and_tensor_sparse(\n+      const SparseType& dtype,\n+      int64_t dimI,\n+      int64_t dimV,\n+      ArrayRef<int64_t> sizes,\n+      const LongTensor& indices,\n+      const Tensor& values) {\n+    SparseTensor self = new_sparse(dtype);\n+    _raw_resize_sparse(self, dimI, dimV, sizes);\n+    _alias_into_sparse(self, indices, values);\n+    return self;\n+  }\n+\n+  // TODO: put this into the public API\n+  bool isSameTensor(const Tensor& lhs, const Tensor& rhs) {\n+    return lhs.unsafeGetTensorImpl() == rhs.unsafeGetTensorImpl();\n+  }\n+}\n+\n+/* Pointer-copy init */\n+SparseTensor new_with_tensor_sparse(const LongTensor& indices, const Tensor& values_) {\n+  Tensor values;\n+  if (values_.dim() == 0) {\n+    // Mimic Numpy behavior here and treat it as a 1D tensor\n+    values = values_.expand({1});\n+  } else {\n+    values = values_;\n+  }\n+\n+  // TODO: This is a temporary test until we support zero-size dims.\n+  // I'm NOT adding the \"obvious\" bypass code, because it wasn't supported\n+  // previously\n+  AT_CHECK(indices.numel() != 0, \"cannot construct sparse tensor with empty indices; use the nullary constructor instead\");\n+\n+  const SparseType& dtype = values.type().toSparse();\n+\n+  // If sizes are not given, it is inferred as max index of each dim.\n+  int64_t dimI = indices.size(0);\n+  int64_t dimV = values.dim() - 1;\n+\n+  std::vector<int64_t> computed_sizes(dimI + dimV);\n+  // NB: It used to keepdim. I think that was wrong.\n+  LongTensor computed_indices_sizes = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n+  computed_indices_sizes.add_(1); // len = max_index + 1\n+  auto computed_indices_sizes_accessor = computed_indices_sizes.accessor<int64_t, 1>();\n+  for (int64_t d = 0; d < dimI; d++) {\n+    computed_sizes[static_cast<size_t>(d)] = computed_indices_sizes_accessor[d];\n+  }\n+  for (int64_t d = 0; d < dimV; d++) {\n+    computed_sizes[static_cast<size_t>(dimI + d)] = values.size(d+1);\n+  }\n+  return _new_with_dims_and_tensor_sparse(dtype, dimI, dimV, computed_sizes, indices, values);\n+}\n+\n+SparseTensor new_with_size_sparse(const SparseType& dtype, ArrayRef<int64_t> size) {\n+  SparseTensor self = new_sparse(dtype);\n+  _raw_resize_sparse(self, size.size(), 0, size);\n+  return self;\n+}\n+\n+// NB: Got rid of the sizes == NULL case\n+SparseTensor new_with_tensor_and_size_unsafe_sparse(const LongTensor& indices, const Tensor& values_, ArrayRef<int64_t> sizes) {\n+  Tensor values;\n+  if (values_.dim() == 0) {\n+    // Mimic Numpy behavior here and treat it as a 1D tensor\n+    values = values_.expand({1});\n+  } else {\n+    values = values_;\n+  }\n+\n+  const SparseType& dtype = values.type().toSparse();\n+  // NB: used to be a dim() == 0 test, but that's legacy TH semantics\n+  if (indices.numel() == 0 && values.numel() == 0) {\n+    return new_with_size_sparse(dtype, sizes);\n+  }\n+\n+  int64_t dimI = indices.size(0);\n+  int64_t dimV = values.dim() - 1;\n+  return _new_with_dims_and_tensor_sparse(dtype, dimI, dimV, sizes, indices, values);\n+}\n+\n+// NB: Got rid of the sizes == NULL case\n+SparseTensor new_with_tensor_and_size_sparse(const LongTensor& indices, const Tensor& values_, ArrayRef<int64_t> sizes) {\n+  Tensor values;\n+  if (values_.dim() == 0) {\n+    // Mimic Numpy behavior here and treat it as a 1D tensor\n+    values = values_.expand({1});\n+  } else {\n+    values = values_;\n+  }\n+\n+  const SparseType& dtype = values.type().toSparse();\n+  // NB: This used to be dims, but mumble TH handling zero-sized tensors\n+  // incorrectly\n+  if (indices.numel() == 0 && values.numel() == 0) {\n+    return new_with_size_sparse(dtype, sizes);\n+  }\n+\n+  int64_t dimI = indices.size(0);\n+  int64_t dimV = values.dim() - 1;\n+  AT_CHECK(sizes.size() == dimI + dimV, \"number of dimensions must be dimI (\", dimI, \") + dimV (\", dimV, \"), but got \", sizes);\n+\n+  LongTensor max_indices = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n+  auto max_indices_accessor = max_indices.accessor<int64_t, 1>();\n+  for (int64_t d = 0; d < dimI; d++) {\n+    int64_t max_index_in_dim = max_indices_accessor[d];\n+    int64_t dim_size = sizes[static_cast<size_t>(d)];\n+    AT_CHECK(max_index_in_dim < dim_size,\n+             \"sizes is inconsistent with indices: for dim \", d, \", size is \", dim_size, \" but found index \", max_index_in_dim);\n+  }\n+  for (int64_t d = 0; d < dimV; d++) {\n+    int64_t values_size = values.size(d+1);\n+    int64_t specified_size = sizes[static_cast<size_t>(dimI + d)];\n+    AT_CHECK(values_size <= specified_size,\n+             \"values and sizes are inconsistent: sizes[\", d + dimI, \"] is \", specified_size,\n+             \" but values.size(\", d + 1, \") is \", values_size);\n+  }\n+  return _new_with_dims_and_tensor_sparse(dtype, dimI, dimV, sizes, indices, values);\n+}\n+\n+// NB: Deleted newWithSizeNd variants\n+\n+SparseTensor clone_sparse(const SparseTensor& self) {\n+  SparseTensor other = new_sparse(self.type());\n+  _raw_resize_sparse(other, self._dimI(), self._dimV(), self.sizes());\n+  // NB: This seems to preserve the size of the UN-narrowed indices and\n+  // values.  Veeery interesting.\n+  _copy_into_sparse(other, _get_sparse_impl(self)->indices(), _get_sparse_impl(self)->values());\n+  _get_sparse_impl(other)->set_coalesced(self.is_coalesced());\n+  _get_sparse_impl(other)->set_nnz(self._nnz());\n+  return other;\n+}\n+\n+SparseTensor transpose_sparse(const SparseTensor& self, int64_t d1, int64_t d2) {\n+  SparseTensor other = clone_sparse(self);\n+  other.transpose_(d1, d2);\n+  return other;\n+}\n+\n+/******************************************************************************\n+ * reshaping methods\n+ ******************************************************************************/\n+\n+/*\n+// We should implement a utility function which: (1) sets nnz and (2) resizes\n+// indices/values to hold enough space to fit nnz, if nnz is larger than\n+// the previous amount.  This ensures that we maintain the nnz invariant.\n+void _resize_nnz_(const SparseTensor& self, int64_t nnz) {\n+}\n+*/\n+\n+void resize_sparse(const SparseTensor& self, ArrayRef<int64_t> size) {\n+  _raw_resize_sparse(self, size.size(), 0, size);\n+}\n+\n+SparseTensor& raw_resize_sparse_(SparseTensor& self, ArrayRef<int64_t> size, int64_t dimI, int64_t dimV) {\n+  if (dimI == -1) {\n+    dimI = self._indices().size(0);\n+  }\n+  if (dimV == -1) {\n+    dimV = self._values().dim() - 1;\n+  }\n+  _raw_resize_sparse(self, dimI, dimV, size);\n+  return self;\n+}\n+\n+namespace {\n+  bool _is_same_size_as_sparse(const SparseTensor& self, const SparseTensor& src) {\n+    return self._dimI() == src._dimI() && self._dimV() == src._dimV() && self.sizes().equals(src.sizes());", "path": "aten/src/ATen/native/sparse/SparseTensor.cpp", "position": null, "original_position": 274, "commit_id": "cdf42cdb1df7b90da99aa5914ec208ab1d396d2e", "original_commit_id": "557a724094be5fd8737c29bee24b7c8b9a4f2e82", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "I think I called this `is_congruent` somewhere.", "created_at": "2018-06-14T20:41:51Z", "updated_at": "2018-11-23T15:45:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/8409#discussion_r195564272", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8409", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195564272"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8409#discussion_r195564272"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8409"}}, "body_html": "<p>I think I called this <code>is_congruent</code> somewhere.</p>", "body_text": "I think I called this is_congruent somewhere."}