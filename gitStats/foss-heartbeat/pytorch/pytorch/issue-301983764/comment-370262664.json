{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/370262664", "html_url": "https://github.com/pytorch/pytorch/pull/5541#issuecomment-370262664", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5541", "id": 370262664, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDI2MjY2NA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-04T20:48:39Z", "updated_at": "2018-03-04T20:48:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This looks great!<br>\nI have couple questions</p>\n<ol>\n<li>cuda kernels operate on raw data pointers, which would be pretty annoying for non-contiguous tensors, as it would require recreating collapseDims, IndexToOffset and similar functions currently in ATen/cuda/detail/TensorInfo.cuh or its THC copy (ATen/src/THC/THCTensorInfo.cuh). What would be the best way of exposing TensorInfo for use in custom kernels and setup functions?</li>\n<li>How would one go about creating an extension where tensor arguments are of the different type? E.g. it may be desirable to have outputs of some reductions and norms on half tensor to be a float tensor. AT_DISPATCH_FLOATING_TYPES does not handle this situation now, it would require two type arguments, say scalar_t and accScalar_t.</li>\n</ol>", "body_text": "This looks great!\nI have couple questions\n\ncuda kernels operate on raw data pointers, which would be pretty annoying for non-contiguous tensors, as it would require recreating collapseDims, IndexToOffset and similar functions currently in ATen/cuda/detail/TensorInfo.cuh or its THC copy (ATen/src/THC/THCTensorInfo.cuh). What would be the best way of exposing TensorInfo for use in custom kernels and setup functions?\nHow would one go about creating an extension where tensor arguments are of the different type? E.g. it may be desirable to have outputs of some reductions and norms on half tensor to be a float tensor. AT_DISPATCH_FLOATING_TYPES does not handle this situation now, it would require two type arguments, say scalar_t and accScalar_t.", "body": "This looks great!\r\nI have couple questions\r\n1) cuda kernels operate on raw data pointers, which would be pretty annoying for non-contiguous tensors, as it would require recreating collapseDims, IndexToOffset and similar functions currently in ATen/cuda/detail/TensorInfo.cuh or its THC copy (ATen/src/THC/THCTensorInfo.cuh). What would be the best way of exposing TensorInfo for use in custom kernels and setup functions?\r\n2) How would one go about creating an extension where tensor arguments are of the different type? E.g. it may be desirable to have outputs of some reductions and norms on half tensor to be a float tensor. AT_DISPATCH_FLOATING_TYPES does not handle this situation now, it would require two type arguments, say scalar_t and accScalar_t."}