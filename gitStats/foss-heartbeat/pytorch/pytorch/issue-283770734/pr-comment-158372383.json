{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/158372383", "pull_request_review_id": 85175315, "id": 158372383, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODM3MjM4Mw==", "diff_hunk": "@@ -1,111 +0,0 @@\n-from torch.autograd import Variable\n-from torch.autograd.function import Function\n-from torch._thnn import type2backend\n-\n-from . import _all_functions\n-from torch.nn.modules.utils import _single\n-\n-\n-# NB: Looking for MaxPool2d or AvgPool2d?  They're natively implemented by ATen.\n-# Look at tools/autograd/derivatives.yaml\n-\n-\n-class FractionalMaxPool2d(Function):\n-\n-    @staticmethod\n-    def forward(ctx, input, kh, kw, output_size=None, output_ratio=None,\n-                _random_samples=None):\n-        # Pool size (how wide the pooling for each output unit is)\n-        ctx.kw, ctx.kh = kw, kh\n-\n-        # Random samples are drawn for all\n-        # batch * plane * (height, width; i.e., 2) points. This determines\n-        # the 2d \"pseudorandom\" overlapping pooling regions for each\n-        # (batch element x input plane).\n-        ctx.random_samples = _random_samples\n-\n-        if output_size is not None:\n-            ctx.oh, ctx.ow = output_size\n-            ctx.rh, ctx.rw = None, None\n-        elif output_ratio is not None:\n-            ctx.oh, ctx.ow = None, None\n-            ctx.rh, ctx.rw = output_ratio\n-            assert 0 < ctx.rh < 1\n-            assert 0 < ctx.rw < 1\n-        else:\n-            assert False\n-\n-        if ctx.random_samples is None:\n-            random_samples = input.new().resize_(input.size(0),\n-                                                 input.size(1), 2).uniform_()\n-        else:\n-            random_samples = ctx.random_samples\n-            ctx.random_samples = None\n-\n-        if ctx.oh is None:\n-            ctx.oh = int(input.size(2) * ctx.rh)\n-            ctx.ow = int(input.size(3) * ctx.rw)\n-        assert isinstance(ctx.oh, int) and isinstance(ctx.ow, int)\n-\n-        indices = input.new().long()\n-        output = input.new()\n-        ctx._backend = type2backend[type(input)]\n-        ctx._backend.SpatialFractionalMaxPooling_updateOutput(\n-            ctx._backend.library_state,\n-            input,\n-            output,\n-            ctx.ow, ctx.oh,\n-            ctx.kw, ctx.kh,\n-            indices,\n-            random_samples\n-        )\n-\n-        ctx.random_samples = None  # Free unnecessary buffers\n-        ctx.save_for_backward(input, indices)\n-        ctx.mark_non_differentiable(indices)\n-        return output, indices\n-\n-    @staticmethod\n-    def backward(ctx, grad_output, _grad_indices=None):\n-        input, indices = ctx.saved_variables\n-\n-        return (FractionalMaxPool2dBackward.apply(input, indices, grad_output, ctx.oh, ctx.ow, ctx.kh, ctx.kw),\n-                None, None, None, None, None, None)\n-\n-\n-class FractionalMaxPool2dBackward(Function):\n-\n-    @staticmethod\n-    def forward(ctx, input, indices, grad_output, oh, ow, kh, kw):\n-        ctx._backend = type2backend[type(input)]\n-        ctx.oh = oh\n-        ctx.ow = ow\n-        ctx.kh = kh\n-        ctx.kw = kw\n-        ctx.save_for_backward(indices)\n-\n-        grad_input = grad_output.new()\n-        ctx._backend.SpatialFractionalMaxPooling_updateGradInput(\n-            ctx._backend.library_state,\n-            input,\n-            grad_output,\n-            grad_input,\n-            ctx.ow, ctx.oh,\n-            ctx.kw, ctx.kh,\n-            indices)\n-\n-        return grad_input\n-\n-    @staticmethod\n-    def backward(ctx, ggI):\n-        indices, = ctx.saved_variables\n-\n-        gI = Variable(ggI.data.new(ggI.size()).zero_())\n-        # ggO is equivalent to the 1d case, but the indices are given wrt the last two dimensions combined\n-        indices_view = indices.view(indices.size()[:-2] + (-1,))\n-        ggO = ggI.contiguous().view(ggI.size()[:-2] + (-1,)).gather(dim=2, index=indices_view).view_as(indices)", "path": "torch/nn/_functions/thnn/pooling.py", "position": 106, "original_position": 106, "commit_id": "7fe50b9c8703a201ad267331a4994f8ae3c5e187", "original_commit_id": "bbd685275a600b9879737de0e2064759de09d95e", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "this was dim=2, now it's dim=-1, and this accepts inputs of 3d or 4d, so this is changed (although now the same as adaptive).  So I'm guessing this was a bug before?  Can we add a test?", "created_at": "2017-12-21T20:34:38Z", "updated_at": "2018-11-23T15:37:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/4290#discussion_r158372383", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4290", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/158372383"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4290#discussion_r158372383"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4290"}}, "body_html": "<p>this was dim=2, now it's dim=-1, and this accepts inputs of 3d or 4d, so this is changed (although now the same as adaptive).  So I'm guessing this was a bug before?  Can we add a test?</p>", "body_text": "this was dim=2, now it's dim=-1, and this accepts inputs of 3d or 4d, so this is changed (although now the same as adaptive).  So I'm guessing this was a bug before?  Can we add a test?"}