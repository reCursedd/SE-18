{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/411809067", "html_url": "https://github.com/pytorch/pytorch/issues/1485#issuecomment-411809067", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1485", "id": 411809067, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMTgwOTA2Nw==", "user": {"login": "fcalvet", "id": 38648518, "node_id": "MDQ6VXNlcjM4NjQ4NTE4", "avatar_url": "https://avatars0.githubusercontent.com/u/38648518?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fcalvet", "html_url": "https://github.com/fcalvet", "followers_url": "https://api.github.com/users/fcalvet/followers", "following_url": "https://api.github.com/users/fcalvet/following{/other_user}", "gists_url": "https://api.github.com/users/fcalvet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fcalvet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fcalvet/subscriptions", "organizations_url": "https://api.github.com/users/fcalvet/orgs", "repos_url": "https://api.github.com/users/fcalvet/repos", "events_url": "https://api.github.com/users/fcalvet/events{/privacy}", "received_events_url": "https://api.github.com/users/fcalvet/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-09T16:01:00Z", "updated_at": "2018-08-09T16:01:00Z", "author_association": "NONE", "body_html": "<p>Run into a similar issue when switching from a tensor with shape [2,1,8,512,512] to one with shape [2,1,512,512,8] broke my script.</p>\n<p>Could this be somehow related to the expected shape for convolution being <code>minibatch\u00d7in_channels\u00d7iT\u00d7iH\u00d7iW </code>and not <code>minibatch\u00d7in_channels\u00d7iH\u00d7iW\u00d7iT</code>, with T&lt;W and H ?<br>\nIf so should a check be implemented to warn the user, regardless of the fix for this bug ?</p>\n<p>The following error is raised, interestingly not in the forward pass but during the backward one:</p>\n<pre><code>`  File \"train.py\", line 90, in train_epoch\n    loss.backward()\n  File \"/home/fcalvet/pytorch/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/fcalvet/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: CuDNN error: CUDNN_STATUS_EXECUTION_FAILED\n</code></pre>\n<p>The fix <code>torch.backends.cudnn.benchmark=True</code> did work. So I would be curious to know more about this \"cudnnGet* is returning the wrong algorithm\" and how it could for example affect performance.</p>\n<p>For reference, I'm using PyTorch 0.4.1 with CUDA 9.2 installed with pip on Linux</p>", "body_text": "Run into a similar issue when switching from a tensor with shape [2,1,8,512,512] to one with shape [2,1,512,512,8] broke my script.\nCould this be somehow related to the expected shape for convolution being minibatch\u00d7in_channels\u00d7iT\u00d7iH\u00d7iW and not minibatch\u00d7in_channels\u00d7iH\u00d7iW\u00d7iT, with T<W and H ?\nIf so should a check be implemented to warn the user, regardless of the fix for this bug ?\nThe following error is raised, interestingly not in the forward pass but during the backward one:\n`  File \"train.py\", line 90, in train_epoch\n    loss.backward()\n  File \"/home/fcalvet/pytorch/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/fcalvet/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: CuDNN error: CUDNN_STATUS_EXECUTION_FAILED\n\nThe fix torch.backends.cudnn.benchmark=True did work. So I would be curious to know more about this \"cudnnGet* is returning the wrong algorithm\" and how it could for example affect performance.\nFor reference, I'm using PyTorch 0.4.1 with CUDA 9.2 installed with pip on Linux", "body": "Run into a similar issue when switching from a tensor with shape [2,1,8,512,512] to one with shape [2,1,512,512,8] broke my script. \r\n\r\nCould this be somehow related to the expected shape for convolution being `minibatch\u00d7in_channels\u00d7iT\u00d7iH\u00d7iW `and not `minibatch\u00d7in_channels\u00d7iH\u00d7iW\u00d7iT`, with T<W and H ?\r\nIf so should a check be implemented to warn the user, regardless of the fix for this bug ?\r\n\r\nThe following error is raised, interestingly not in the forward pass but during the backward one:\r\n```\r\n`  File \"train.py\", line 90, in train_epoch\r\n    loss.backward()\r\n  File \"/home/fcalvet/pytorch/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/fcalvet/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: CuDNN error: CUDNN_STATUS_EXECUTION_FAILED\r\n````\r\n\r\nThe fix `torch.backends.cudnn.benchmark=True` did work. So I would be curious to know more about this \"cudnnGet* is returning the wrong algorithm\" and how it could for example affect performance.\r\n\r\nFor reference, I'm using PyTorch 0.4.1 with CUDA 9.2 installed with pip on Linux"}