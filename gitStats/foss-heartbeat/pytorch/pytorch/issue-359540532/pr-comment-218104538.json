{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218104538", "pull_request_review_id": 155968730, "id": 218104538, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxODEwNDUzOA==", "diff_hunk": "@@ -106,6 +107,19 @@ static std::vector<Value*> gradientForNode(Node* node, ArrayRef<Value*> grad_val\n     } else if (node->matches(\"aten::relu(Tensor self) -> Tensor\")) {\n       return {grads.at(0) * (outputs.at(0) > at::Scalar(0)).type_as(outputs.at(0))};\n \n+    } else if (node->matches(\"aten::clamp(Tensor self, Scalar min, Scalar max) -> Tensor\")) {\n+      // we do two type_as as it's free (hopefully) and the \"*\" only works with float\n+      // the \"! (val > min)\" is chosen such that the gradient is 0 on the\n+      // boundary and the factor is 1 when the boundary is NaN\n+      // the ! is expressed as \"1-\" for lack of a \"not\" function and\n+      // the the fuser insisting on float\n+      // it would be prettier to return NaN as gradient for NaN,\n+      // but that is hard to reliably code here, so we have 0 as gradient\n+      // when the input is NaN (unless grads is NaN or infinite)\n+      return {grads.at(0)\n+\t      * (1-(inputs.at(0).isnan()).type_as(inputs.at(0)))", "path": "torch/csrc/jit/autodiff.cpp", "position": null, "original_position": 22, "commit_id": "30be1e7983687052b37e4261489878d50e223a3b", "original_commit_id": "be5b5e64f27227b11abaee3839c099869f656fc6", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This term is missing from the derivative formula we usually use for clamp. I'm not sure why do we want it. I don't think it's worth sacrifising performance just to change the NaN handling.", "created_at": "2018-09-17T15:00:34Z", "updated_at": "2018-11-23T15:51:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/11574#discussion_r218104538", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11574", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218104538"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11574#discussion_r218104538"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11574"}}, "body_html": "<p>This term is missing from the derivative formula we usually use for clamp. I'm not sure why do we want it. I don't think it's worth sacrifising performance just to change the NaN handling.</p>", "body_text": "This term is missing from the derivative formula we usually use for clamp. I'm not sure why do we want it. I don't think it's worth sacrifising performance just to change the NaN handling."}