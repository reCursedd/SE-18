{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217511755", "pull_request_review_id": 155243553, "id": 217511755, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzUxMTc1NQ==", "diff_hunk": "@@ -119,9 +119,13 @@ static std::vector<Value*> gradientForNode(Node* node, ArrayRef<Value*> grad_val\n \n     } else if (node->matches(\"aten::clamp(Tensor self, Scalar min, Scalar max) -> Tensor\")) {\n       // we do two type_as as it's free (hopefully) and the \"*\" only works with float\n-      return {grads.at(0)\n-\t      * (inputs.at(0) > inputs.at(1)).type_as(inputs.at(0))\n-\t      * (inputs.at(0) < inputs.at(2)).type_as(inputs.at(0)), nullptr, nullptr};\n+      // the \"! (val > min)\" is chosen such that the gradient is 0 on the\n+      // boundary and the factor is 1 when the boundary is NaN\n+      // the ! is expressed as \"1-\" for lack of a \"not\" function and\n+      // the the fuser insisting on float\n+      return {(inputs.at(0).isnan() ? inputs.at(0) : grads.at(0))", "path": "torch/csrc/jit/autodiff.cpp", "position": null, "original_position": 11, "commit_id": "30be1e7983687052b37e4261489878d50e223a3b", "original_commit_id": "b744d5fbc7ddd34823f10e8d7df8aba3cb61d952", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "You can have byte inputs (at least you could at some point), but you'd have to add some special casing in the autodiff pass and fusion pass. ", "created_at": "2018-09-13T19:48:42Z", "updated_at": "2018-11-23T15:51:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/11574#discussion_r217511755", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11574", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217511755"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11574#discussion_r217511755"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11574"}}, "body_html": "<p>You can have byte inputs (at least you could at some point), but you'd have to add some special casing in the autodiff pass and fusion pass.</p>", "body_text": "You can have byte inputs (at least you could at some point), but you'd have to add some special casing in the autodiff pass and fusion pass.", "in_reply_to_id": 217445994}