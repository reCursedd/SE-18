{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217096947", "pull_request_review_id": 154730416, "id": 217096947, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzA5Njk0Nw==", "diff_hunk": "@@ -481,6 +481,8 @@ std::string encodeRHS(Node * n) {\n     {aten::add, \"${0} + ${2}*${1}\"},\n     {aten::sub, \"(${0} - ${2}*${1})\"},\n     {aten::rand_like, \"uniform(rnd())\"},\n+    //min, max\n+    {aten::clamp, \"fmaxf(fminf(${0}, ${2}), ${1})\"},", "path": "torch/csrc/jit/fusion_compiler.cpp", "position": null, "original_position": 5, "commit_id": "30be1e7983687052b37e4261489878d50e223a3b", "original_commit_id": "838e55a0027b071772c9157bf2086be6c81de708", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "I did not look up how clamp is implemented in the core, but this won't propagate nans (fmaxf and fminf return other when comparing nan with other according to standard) and we usually try to propagate nans. A few lines above, aten::relu is implemented as `{aten::relu, \"${0} < 0 ? 0.f : ${0} \"},` for this reason. ", "created_at": "2018-09-12T16:12:57Z", "updated_at": "2018-11-23T15:51:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/11574#discussion_r217096947", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11574", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217096947"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11574#discussion_r217096947"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11574"}}, "body_html": "<p>I did not look up how clamp is implemented in the core, but this won't propagate nans (fmaxf and fminf return other when comparing nan with other according to standard) and we usually try to propagate nans. A few lines above, aten::relu is implemented as <code>{aten::relu, \"${0} &lt; 0 ? 0.f : ${0} \"},</code> for this reason.</p>", "body_text": "I did not look up how clamp is implemented in the core, but this won't propagate nans (fmaxf and fminf return other when comparing nan with other according to standard) and we usually try to propagate nans. A few lines above, aten::relu is implemented as {aten::relu, \"${0} < 0 ? 0.f : ${0} \"}, for this reason."}