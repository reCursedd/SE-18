{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11750", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11750/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11750/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11750/events", "html_url": "https://github.com/pytorch/pytorch/issues/11750", "id": 360782129, "node_id": "MDU6SXNzdWUzNjA3ODIxMjk=", "number": 11750, "title": "ctc_loss backward doesn't deal well with grad_out", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-17T09:16:02Z", "updated_at": "2018-09-17T16:56:08Z", "closed_at": "2018-09-17T16:56:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Several bits of the equations aren't multiplied by gradient out, both for CPU and GPU.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\ntorch.manual_seed(<span class=\"pl-c1\">1234</span>)\n\ndev <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>\nlog_probs <span class=\"pl-k\">=</span> torch.softmax(torch.randn(<span class=\"pl-c1\">150</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">101</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>dev), <span class=\"pl-c1\">2</span>)\ncodes <span class=\"pl-k\">=</span> torch.randint(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">100</span>, (<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">15</span>), <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>dev, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.long)\n\ns <span class=\"pl-k\">=</span> log_probs.size(<span class=\"pl-c1\">0</span>)\ninput_lengths <span class=\"pl-k\">=</span> [s <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(log_probs.size(<span class=\"pl-c1\">1</span>))]\nt <span class=\"pl-k\">=</span> codes.size(<span class=\"pl-c1\">1</span>)\ntarget_lengths <span class=\"pl-k\">=</span> [t <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(log_probs.size(<span class=\"pl-c1\">1</span>))]\n\nlp1 <span class=\"pl-k\">=</span> log_probs.detach().requires_grad_()\nlp2 <span class=\"pl-k\">=</span> log_probs.detach().requires_grad_()\n\nl1 <span class=\"pl-k\">=</span> torch.nn.functional.ctc_loss(lp1, codes, input_lengths, target_lengths, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum<span class=\"pl-pds\">'</span></span>)\nl2 <span class=\"pl-k\">=</span> torch.nn.functional.ctc_loss(lp2, codes, input_lengths, target_lengths, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum<span class=\"pl-pds\">'</span></span>)<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>\n\nl1.backward()\nl2.backward()\n\n<span class=\"pl-c1\">print</span>((lp1.grad<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span><span class=\"pl-k\">-</span>lp2.grad).abs().max().item())</pre></div>\n<p>Expect: 0, but will see all sorts of things.</p>\n<p>This also affects the various reductions, in particularly the default <code>elementwise_mean</code>.</p>\n<p>I have a fix but want to add autograd tests...</p>", "body_text": "Several bits of the equations aren't multiplied by gradient out, both for CPU and GPU.\nimport torch\n\ntorch.manual_seed(1234)\n\ndev = 'cuda'\nlog_probs = torch.softmax(torch.randn(150, 64, 101, device=dev), 2)\ncodes = torch.randint(1, 100, (64, 15), device=dev, dtype=torch.long)\n\ns = log_probs.size(0)\ninput_lengths = [s for _ in range(log_probs.size(1))]\nt = codes.size(1)\ntarget_lengths = [t for _ in range(log_probs.size(1))]\n\nlp1 = log_probs.detach().requires_grad_()\nlp2 = log_probs.detach().requires_grad_()\n\nl1 = torch.nn.functional.ctc_loss(lp1, codes, input_lengths, target_lengths, reduction='sum')\nl2 = torch.nn.functional.ctc_loss(lp2, codes, input_lengths, target_lengths, reduction='sum')*2\n\nl1.backward()\nl2.backward()\n\nprint((lp1.grad*2-lp2.grad).abs().max().item())\nExpect: 0, but will see all sorts of things.\nThis also affects the various reductions, in particularly the default elementwise_mean.\nI have a fix but want to add autograd tests...", "body": "Several bits of the equations aren't multiplied by gradient out, both for CPU and GPU.\r\n\r\n```python\r\nimport torch\r\n\r\ntorch.manual_seed(1234)\r\n\r\ndev = 'cuda'\r\nlog_probs = torch.softmax(torch.randn(150, 64, 101, device=dev), 2)\r\ncodes = torch.randint(1, 100, (64, 15), device=dev, dtype=torch.long)\r\n\r\ns = log_probs.size(0)\r\ninput_lengths = [s for _ in range(log_probs.size(1))]\r\nt = codes.size(1)\r\ntarget_lengths = [t for _ in range(log_probs.size(1))]\r\n\r\nlp1 = log_probs.detach().requires_grad_()\r\nlp2 = log_probs.detach().requires_grad_()\r\n\r\nl1 = torch.nn.functional.ctc_loss(lp1, codes, input_lengths, target_lengths, reduction='sum')\r\nl2 = torch.nn.functional.ctc_loss(lp2, codes, input_lengths, target_lengths, reduction='sum')*2\r\n\r\nl1.backward()\r\nl2.backward()\r\n\r\nprint((lp1.grad*2-lp2.grad).abs().max().item())\r\n```\r\nExpect: 0, but will see all sorts of things.\r\n\r\nThis also affects the various reductions, in particularly the default `elementwise_mean`.\r\n\r\nI have a fix but want to add autograd tests..."}