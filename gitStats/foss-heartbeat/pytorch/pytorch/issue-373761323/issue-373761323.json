{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13107", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13107/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13107/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13107/events", "html_url": "https://github.com/pytorch/pytorch/issues/13107", "id": 373761323, "node_id": "MDU6SXNzdWUzNzM3NjEzMjM=", "number": 13107, "title": "How to Implement Attention?", "user": {"login": "mourafa", "id": 44455976, "node_id": "MDQ6VXNlcjQ0NDU1OTc2", "avatar_url": "https://avatars3.githubusercontent.com/u/44455976?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mourafa", "html_url": "https://github.com/mourafa", "followers_url": "https://api.github.com/users/mourafa/followers", "following_url": "https://api.github.com/users/mourafa/following{/other_user}", "gists_url": "https://api.github.com/users/mourafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/mourafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mourafa/subscriptions", "organizations_url": "https://api.github.com/users/mourafa/orgs", "repos_url": "https://api.github.com/users/mourafa/repos", "events_url": "https://api.github.com/users/mourafa/events{/privacy}", "received_events_url": "https://api.github.com/users/mourafa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-25T03:43:48Z", "updated_at": "2018-10-25T03:57:34Z", "closed_at": "2018-10-25T03:54:35Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I have just started working with neural networks. I am having trouble in mapping of concepts given in paper to code.</p>\n<p>I wanted to ask some questions:</p>\n<ol>\n<li>\n<p>The nn module in PyTorch accepts a batch as its parameter by default? i.e. I prepare my input in the format of batch x features x feature_length form and then when defining my network class I can just ignore the batch parameter?</p>\n</li>\n<li>\n<p>Isn't seq2seq model just a normal lstm with a softmax decoder? I can put another layer before softmax and it would still be an encoder, what is the difference here b/w seq2seq and just more layers?</p>\n</li>\n<li>\n<p>How do i implement attention, is it a layer? I think in keras they just take the dot product of the hidden states of lstm and get scores which is then multiplied back to inputs and added to the final hidden state. Is attention any different? I don't think it has any tunable parameters.</p>\n</li>\n<li>\n<p>How do I implement self attention? Here i don't even have a clue? Is it a linear layer, how to do this?</p>\n</li>\n</ol>\n<p>This is very hard it appears, thanks for helping!</p>", "body_text": "Hi,\nI have just started working with neural networks. I am having trouble in mapping of concepts given in paper to code.\nI wanted to ask some questions:\n\n\nThe nn module in PyTorch accepts a batch as its parameter by default? i.e. I prepare my input in the format of batch x features x feature_length form and then when defining my network class I can just ignore the batch parameter?\n\n\nIsn't seq2seq model just a normal lstm with a softmax decoder? I can put another layer before softmax and it would still be an encoder, what is the difference here b/w seq2seq and just more layers?\n\n\nHow do i implement attention, is it a layer? I think in keras they just take the dot product of the hidden states of lstm and get scores which is then multiplied back to inputs and added to the final hidden state. Is attention any different? I don't think it has any tunable parameters.\n\n\nHow do I implement self attention? Here i don't even have a clue? Is it a linear layer, how to do this?\n\n\nThis is very hard it appears, thanks for helping!", "body": "Hi,\r\n\r\nI have just started working with neural networks. I am having trouble in mapping of concepts given in paper to code.\r\n\r\nI wanted to ask some questions:\r\n\r\n1. The nn module in PyTorch accepts a batch as its parameter by default? i.e. I prepare my input in the format of batch x features x feature_length form and then when defining my network class I can just ignore the batch parameter?\r\n\r\n2. Isn't seq2seq model just a normal lstm with a softmax decoder? I can put another layer before softmax and it would still be an encoder, what is the difference here b/w seq2seq and just more layers? \r\n\r\n3. How do i implement attention, is it a layer? I think in keras they just take the dot product of the hidden states of lstm and get scores which is then multiplied back to inputs and added to the final hidden state. Is attention any different? I don't think it has any tunable parameters. \r\n\r\n4. How do I implement self attention? Here i don't even have a clue? Is it a linear layer, how to do this?\r\n\r\nThis is very hard it appears, thanks for helping!"}