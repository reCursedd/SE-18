{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12382", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12382/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12382/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12382/events", "html_url": "https://github.com/pytorch/pytorch/pull/12382", "id": 367328372, "node_id": "MDExOlB1bGxSZXF1ZXN0MjIwNzkzMTQ5", "number": 12382, "title": "implement rowwise quantization for fp16", "user": {"login": "hyuen", "id": 38995, "node_id": "MDQ6VXNlcjM4OTk1", "avatar_url": "https://avatars1.githubusercontent.com/u/38995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hyuen", "html_url": "https://github.com/hyuen", "followers_url": "https://api.github.com/users/hyuen/followers", "following_url": "https://api.github.com/users/hyuen/following{/other_user}", "gists_url": "https://api.github.com/users/hyuen/gists{/gist_id}", "starred_url": "https://api.github.com/users/hyuen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hyuen/subscriptions", "organizations_url": "https://api.github.com/users/hyuen/orgs", "repos_url": "https://api.github.com/users/hyuen/repos", "events_url": "https://api.github.com/users/hyuen/events{/privacy}", "received_events_url": "https://api.github.com/users/hyuen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-05T18:53:29Z", "updated_at": "2018-10-12T20:59:07Z", "closed_at": "2018-10-12T20:59:07Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/12382", "html_url": "https://github.com/pytorch/pytorch/pull/12382", "diff_url": "https://github.com/pytorch/pytorch/pull/12382.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/12382.patch"}, "body_html": "<p>Summary:<br>\nimplement fp16-&gt; (uint8 + scale and bias in fp32)</p>\n<p>this is similar to fp32 rowwise quantization</p>\n<p>we could have done scale and bias in fp16 but not too motivated since we are not saving much and those datatypes have to be converted to fp32 to process since x86 doesn't support half float operations anyways</p>\n<p>Differential Revision: D10220463</p>", "body_text": "Summary:\nimplement fp16-> (uint8 + scale and bias in fp32)\nthis is similar to fp32 rowwise quantization\nwe could have done scale and bias in fp16 but not too motivated since we are not saving much and those datatypes have to be converted to fp32 to process since x86 doesn't support half float operations anyways\nDifferential Revision: D10220463", "body": "Summary:\nimplement fp16-> (uint8 + scale and bias in fp32)\n\nthis is similar to fp32 rowwise quantization\n\nwe could have done scale and bias in fp16 but not too motivated since we are not saving much and those datatypes have to be converted to fp32 to process since x86 doesn't support half float operations anyways\n\nDifferential Revision: D10220463\n"}