{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1233", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1233/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1233/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1233/events", "html_url": "https://github.com/pytorch/pytorch/issues/1233", "id": 221029518, "node_id": "MDU6SXNzdWUyMjEwMjk1MTg=", "number": 1233, "title": "torch::autograd::ReadyQueue::~ReadyQueue deadlock in atexit", "user": {"login": "eklitzke", "id": 2734, "node_id": "MDQ6VXNlcjI3MzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eklitzke", "html_url": "https://github.com/eklitzke", "followers_url": "https://api.github.com/users/eklitzke/followers", "following_url": "https://api.github.com/users/eklitzke/following{/other_user}", "gists_url": "https://api.github.com/users/eklitzke/gists{/gist_id}", "starred_url": "https://api.github.com/users/eklitzke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eklitzke/subscriptions", "organizations_url": "https://api.github.com/users/eklitzke/orgs", "repos_url": "https://api.github.com/users/eklitzke/repos", "events_url": "https://api.github.com/users/eklitzke/events{/privacy}", "received_events_url": "https://api.github.com/users/eklitzke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-04-11T17:48:27Z", "updated_at": "2017-04-13T04:08:53Z", "closed_at": "2017-04-13T04:08:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I have a neural network I've built using pytorch. I've built pytorch from the latest tip of master (using miniconda). When I run my network, the program hangs while exiting. From GDB, I can see that the program is running <code>atexit()</code> hooks, and in these hooks it's encountering a deadlock when deleting the <code>torch::autograd::ReadyQueue</code>.</p>\n<p>If necessary I can try to simplify my code to something that will reproduce the issue, but I'm hoping that this back trace from GDB will be sufficient for someone who understands the autograd internals to understand what's happening:</p>\n<pre><code>(gdb) bt\n#0  0x00007f2d7da2d221 in futex_wait (private=&lt;optimized out&gt;, expected=12, futex_word=0x1a7ad14)\n    at ../sysdeps/unix/sysv/linux/futex-internal.h:61\n#1  futex_wait_simple (private=&lt;optimized out&gt;, expected=12, futex_word=0x1a7ad14) at ../sysdeps/nptl/futex-internal.h:135\n#2  __pthread_cond_destroy (cond=0x1a7acf0) at pthread_cond_destroy.c:54\n#3  0x00007f2d6ddc482e in torch::autograd::ReadyQueue::~ReadyQueue (this=0x1a7aca0, __in_chrg=&lt;optimized out&gt;)\n    at torch/csrc/autograd/engine.cpp:36\n#4  std::default_delete&lt;torch::autograd::ReadyQueue&gt;::operator() (this=&lt;optimized out&gt;, __ptr=0x1a7aca0)\n    at /home/evan/miniconda3/gcc/include/c++/bits/unique_ptr.h:67\n#5  std::unique_ptr&lt;torch::autograd::ReadyQueue, std::default_delete&lt;torch::autograd::ReadyQueue&gt; &gt;::~unique_ptr (this=0x1a7ac80, \n    __in_chrg=&lt;optimized out&gt;) at /home/evan/miniconda3/gcc/include/c++/bits/unique_ptr.h:184\n#6  std::_Destroy&lt;std::unique_ptr&lt;torch::autograd::ReadyQueue&gt; &gt; (__pointer=0x1a7ac80)\n    at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:93\n#7  std::_Destroy_aux&lt;false&gt;::__destroy&lt;std::unique_ptr&lt;torch::autograd::ReadyQueue&gt;*&gt; (__last=0x1a7ac88, __first=0x1a7ac80)\n    at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:103\n#8  std::_Destroy&lt;std::unique_ptr&lt;torch::autograd::ReadyQueue&gt;*&gt; (__last=0x1a7ac88, __first=&lt;optimized out&gt;)\n    at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:126\n#9  std::_Destroy&lt;std::unique_ptr&lt;torch::autograd::ReadyQueue&gt;*, std::unique_ptr&lt;torch::autograd::ReadyQueue&gt; &gt; (__last=0x1a7ac88, \n    __first=&lt;optimized out&gt;) at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:151\n#10 std::vector&lt;std::unique_ptr&lt;torch::autograd::ReadyQueue, std::default_delete&lt;torch::autograd::ReadyQueue&gt; &gt;, std::allocator&lt;std::unique_ptr&lt;torch::autograd::ReadyQueue, std::default_delete&lt;torch::autograd::ReadyQueue&gt; &gt; &gt; &gt;::~vector (this=0x7f2d6e1041c8 &lt;engine+8&gt;, \n    __in_chrg=&lt;optimized out&gt;) at /home/evan/miniconda3/gcc/include/c++/bits/stl_vector.h:415\n#11 torch::autograd::Engine::~Engine (this=0x7f2d6e1041c0 &lt;engine&gt;, __in_chrg=&lt;optimized out&gt;)\n    at /home/evan/code/pytorch/torch/csrc/autograd/engine.h:21\n#12 0x00007f2d7cd64dc8 in __run_exit_handlers (status=0, listp=0x7f2d7d0f55b8 &lt;__exit_funcs&gt;, run_list_atexit=run_list_atexit@entry=true, \n    run_dtors=run_dtors@entry=true) at exit.c:83\n#13 0x00007f2d7cd64e1a in __GI_exit (status=&lt;optimized out&gt;) at exit.c:105\n#14 0x00007f2d7cd4a605 in __libc_start_main (main=0x400ab0 &lt;main&gt;, argc=4, argv=0x7fff26fd2f68, init=&lt;optimized out&gt;, fini=&lt;optimized out&gt;, \n    rtld_fini=&lt;optimized out&gt;, stack_end=0x7fff26fd2f58) at ../csu/libc-start.c:329\n#15 0x00000000004009e9 in _start ()\n</code></pre>", "body_text": "I have a neural network I've built using pytorch. I've built pytorch from the latest tip of master (using miniconda). When I run my network, the program hangs while exiting. From GDB, I can see that the program is running atexit() hooks, and in these hooks it's encountering a deadlock when deleting the torch::autograd::ReadyQueue.\nIf necessary I can try to simplify my code to something that will reproduce the issue, but I'm hoping that this back trace from GDB will be sufficient for someone who understands the autograd internals to understand what's happening:\n(gdb) bt\n#0  0x00007f2d7da2d221 in futex_wait (private=<optimized out>, expected=12, futex_word=0x1a7ad14)\n    at ../sysdeps/unix/sysv/linux/futex-internal.h:61\n#1  futex_wait_simple (private=<optimized out>, expected=12, futex_word=0x1a7ad14) at ../sysdeps/nptl/futex-internal.h:135\n#2  __pthread_cond_destroy (cond=0x1a7acf0) at pthread_cond_destroy.c:54\n#3  0x00007f2d6ddc482e in torch::autograd::ReadyQueue::~ReadyQueue (this=0x1a7aca0, __in_chrg=<optimized out>)\n    at torch/csrc/autograd/engine.cpp:36\n#4  std::default_delete<torch::autograd::ReadyQueue>::operator() (this=<optimized out>, __ptr=0x1a7aca0)\n    at /home/evan/miniconda3/gcc/include/c++/bits/unique_ptr.h:67\n#5  std::unique_ptr<torch::autograd::ReadyQueue, std::default_delete<torch::autograd::ReadyQueue> >::~unique_ptr (this=0x1a7ac80, \n    __in_chrg=<optimized out>) at /home/evan/miniconda3/gcc/include/c++/bits/unique_ptr.h:184\n#6  std::_Destroy<std::unique_ptr<torch::autograd::ReadyQueue> > (__pointer=0x1a7ac80)\n    at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:93\n#7  std::_Destroy_aux<false>::__destroy<std::unique_ptr<torch::autograd::ReadyQueue>*> (__last=0x1a7ac88, __first=0x1a7ac80)\n    at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:103\n#8  std::_Destroy<std::unique_ptr<torch::autograd::ReadyQueue>*> (__last=0x1a7ac88, __first=<optimized out>)\n    at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:126\n#9  std::_Destroy<std::unique_ptr<torch::autograd::ReadyQueue>*, std::unique_ptr<torch::autograd::ReadyQueue> > (__last=0x1a7ac88, \n    __first=<optimized out>) at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:151\n#10 std::vector<std::unique_ptr<torch::autograd::ReadyQueue, std::default_delete<torch::autograd::ReadyQueue> >, std::allocator<std::unique_ptr<torch::autograd::ReadyQueue, std::default_delete<torch::autograd::ReadyQueue> > > >::~vector (this=0x7f2d6e1041c8 <engine+8>, \n    __in_chrg=<optimized out>) at /home/evan/miniconda3/gcc/include/c++/bits/stl_vector.h:415\n#11 torch::autograd::Engine::~Engine (this=0x7f2d6e1041c0 <engine>, __in_chrg=<optimized out>)\n    at /home/evan/code/pytorch/torch/csrc/autograd/engine.h:21\n#12 0x00007f2d7cd64dc8 in __run_exit_handlers (status=0, listp=0x7f2d7d0f55b8 <__exit_funcs>, run_list_atexit=run_list_atexit@entry=true, \n    run_dtors=run_dtors@entry=true) at exit.c:83\n#13 0x00007f2d7cd64e1a in __GI_exit (status=<optimized out>) at exit.c:105\n#14 0x00007f2d7cd4a605 in __libc_start_main (main=0x400ab0 <main>, argc=4, argv=0x7fff26fd2f68, init=<optimized out>, fini=<optimized out>, \n    rtld_fini=<optimized out>, stack_end=0x7fff26fd2f58) at ../csu/libc-start.c:329\n#15 0x00000000004009e9 in _start ()", "body": "I have a neural network I've built using pytorch. I've built pytorch from the latest tip of master (using miniconda). When I run my network, the program hangs while exiting. From GDB, I can see that the program is running `atexit()` hooks, and in these hooks it's encountering a deadlock when deleting the `torch::autograd::ReadyQueue`.\r\n\r\nIf necessary I can try to simplify my code to something that will reproduce the issue, but I'm hoping that this back trace from GDB will be sufficient for someone who understands the autograd internals to understand what's happening:\r\n\r\n```\r\n(gdb) bt\r\n#0  0x00007f2d7da2d221 in futex_wait (private=<optimized out>, expected=12, futex_word=0x1a7ad14)\r\n    at ../sysdeps/unix/sysv/linux/futex-internal.h:61\r\n#1  futex_wait_simple (private=<optimized out>, expected=12, futex_word=0x1a7ad14) at ../sysdeps/nptl/futex-internal.h:135\r\n#2  __pthread_cond_destroy (cond=0x1a7acf0) at pthread_cond_destroy.c:54\r\n#3  0x00007f2d6ddc482e in torch::autograd::ReadyQueue::~ReadyQueue (this=0x1a7aca0, __in_chrg=<optimized out>)\r\n    at torch/csrc/autograd/engine.cpp:36\r\n#4  std::default_delete<torch::autograd::ReadyQueue>::operator() (this=<optimized out>, __ptr=0x1a7aca0)\r\n    at /home/evan/miniconda3/gcc/include/c++/bits/unique_ptr.h:67\r\n#5  std::unique_ptr<torch::autograd::ReadyQueue, std::default_delete<torch::autograd::ReadyQueue> >::~unique_ptr (this=0x1a7ac80, \r\n    __in_chrg=<optimized out>) at /home/evan/miniconda3/gcc/include/c++/bits/unique_ptr.h:184\r\n#6  std::_Destroy<std::unique_ptr<torch::autograd::ReadyQueue> > (__pointer=0x1a7ac80)\r\n    at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:93\r\n#7  std::_Destroy_aux<false>::__destroy<std::unique_ptr<torch::autograd::ReadyQueue>*> (__last=0x1a7ac88, __first=0x1a7ac80)\r\n    at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:103\r\n#8  std::_Destroy<std::unique_ptr<torch::autograd::ReadyQueue>*> (__last=0x1a7ac88, __first=<optimized out>)\r\n    at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:126\r\n#9  std::_Destroy<std::unique_ptr<torch::autograd::ReadyQueue>*, std::unique_ptr<torch::autograd::ReadyQueue> > (__last=0x1a7ac88, \r\n    __first=<optimized out>) at /home/evan/miniconda3/gcc/include/c++/bits/stl_construct.h:151\r\n#10 std::vector<std::unique_ptr<torch::autograd::ReadyQueue, std::default_delete<torch::autograd::ReadyQueue> >, std::allocator<std::unique_ptr<torch::autograd::ReadyQueue, std::default_delete<torch::autograd::ReadyQueue> > > >::~vector (this=0x7f2d6e1041c8 <engine+8>, \r\n    __in_chrg=<optimized out>) at /home/evan/miniconda3/gcc/include/c++/bits/stl_vector.h:415\r\n#11 torch::autograd::Engine::~Engine (this=0x7f2d6e1041c0 <engine>, __in_chrg=<optimized out>)\r\n    at /home/evan/code/pytorch/torch/csrc/autograd/engine.h:21\r\n#12 0x00007f2d7cd64dc8 in __run_exit_handlers (status=0, listp=0x7f2d7d0f55b8 <__exit_funcs>, run_list_atexit=run_list_atexit@entry=true, \r\n    run_dtors=run_dtors@entry=true) at exit.c:83\r\n#13 0x00007f2d7cd64e1a in __GI_exit (status=<optimized out>) at exit.c:105\r\n#14 0x00007f2d7cd4a605 in __libc_start_main (main=0x400ab0 <main>, argc=4, argv=0x7fff26fd2f68, init=<optimized out>, fini=<optimized out>, \r\n    rtld_fini=<optimized out>, stack_end=0x7fff26fd2f58) at ../csu/libc-start.c:329\r\n#15 0x00000000004009e9 in _start ()\r\n```"}