{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14104", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14104/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14104/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14104/events", "html_url": "https://github.com/pytorch/pytorch/issues/14104", "id": 381687676, "node_id": "MDU6SXNzdWUzODE2ODc2NzY=", "number": 14104, "title": "[Caffe2] ONNX Caffe2Backend.prepare() initializes input as float64", "user": {"login": "laggui", "id": 7225623, "node_id": "MDQ6VXNlcjcyMjU2MjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/7225623?v=4", "gravatar_id": "", "url": "https://api.github.com/users/laggui", "html_url": "https://github.com/laggui", "followers_url": "https://api.github.com/users/laggui/followers", "following_url": "https://api.github.com/users/laggui/following{/other_user}", "gists_url": "https://api.github.com/users/laggui/gists{/gist_id}", "starred_url": "https://api.github.com/users/laggui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/laggui/subscriptions", "organizations_url": "https://api.github.com/users/laggui/orgs", "repos_url": "https://api.github.com/users/laggui/repos", "events_url": "https://api.github.com/users/laggui/events{/privacy}", "received_events_url": "https://api.github.com/users/laggui/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-16T17:13:37Z", "updated_at": "2018-11-22T21:55:08Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>When working with an ONNX model, preparing the backend <a href=\"https://github.com/pytorch/pytorch/blob/master/caffe2/python/onnx/backend.py#L647\">automatically initializes</a> the graph inputs as float64, which then throws out the following warning when specifying a CUDA device:<br>\n<code>CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: 0 type: float64</code></p>\n<p>This is because <code>numpy.ones</code> has default datatype numpy.float64.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<p>In my case, I initially converted a pre-trained PyTorch model to ONNX with torch.onnx._export(), but I believe this warning should still appear when working with any ONNX model.</p>\n<ol>\n<li>Convert PyTorch model to ONNX with <code>torch.onnx._export(torch_model, example_input, 'model.onnx', export_params=True)</code></li>\n<li>Load ONNX model with <a href=\"https://github.com/onnx/onnx\">onnx</a>: <code>onnx.load('model.onnx')</code></li>\n<li>Prepare the caffe2 backend: <code>caffe2.python.onnx.backend.prepare(model, device='CUDA:0')</code></li>\n</ol>\n<p>Sample code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> Prepare backend</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Preparing backend...<span class=\"pl-pds\">'</span></span>)\nprep_t0 <span class=\"pl-k\">=</span> time.time()\nprepared_backend <span class=\"pl-k\">=</span> caffe2.python.onnx.backend.prepare(model, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>CUDA:0<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Backend prepared in <span class=\"pl-c1\">{}</span> seconds<span class=\"pl-pds\">'</span></span>.format(time.time() <span class=\"pl-k\">-</span> prep_t0))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the ONNX model with Caffe2</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Running inference..<span class=\"pl-pds\">'</span></span>)\nfwd_t0 <span class=\"pl-k\">=</span> time.time()\noutputs <span class=\"pl-k\">=</span> prepared_backend.run(img_arr.astype(np.float32))[<span class=\"pl-c1\">0</span>]\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Foward pass time: <span class=\"pl-c1\">{}</span> seconds<span class=\"pl-pds\">'</span></span>.format(time.time() <span class=\"pl-k\">-</span> fwd_t0))</pre></div>\n<p>Outputs:</p>\n<pre><code>Preparing backend...\nCUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: 0 type: float64\nBackend prepared in 1.4734389781951904 seconds\nRunning inference..\nFoward pass time: 0.04503035545349121 seconds\n</code></pre>\n<h2>Expected behavior</h2>\n<p>The behavior is as expected apart from the warning. The model still seems to be loaded on GPU and inference is executed on GPU as well. But I was wondering why I was getting a warning for such a simple task.</p>\n<h2>Environment</h2>\n<ul>\n<li>PyTorch Version (e.g., 1.0): 1.0.0.dev20181105</li>\n<li>OS (e.g., Linux): Linux</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): conda</li>\n<li>Python version: 3.6.6</li>\n<li>CUDA/cuDNN version: 9.2.148/7104</li>\n<li>GPU models and configuration: GeForce GTX 1080 Ti with drivers 410.73</li>\n</ul>\n<h2>Additional context</h2>\n<p>As mentioned above, this error is thrown when preparing the caffe2 backend for CUDA device, and I managed to remove the warning by modifying the source <a href=\"https://github.com/pytorch/pytorch/blob/master/caffe2/python/onnx/backend.py#L647\">here</a>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@</span><span class=\"pl-c1\">classmethod</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_direct_initialize_inputs</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">cls</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">initialized</span>, <span class=\"pl-smi\">ws</span>, <span class=\"pl-smi\">device_option</span>):\n     <span class=\"pl-k\">for</span> value_info <span class=\"pl-k\">in</span> inputs:\n         <span class=\"pl-k\">if</span> value_info.name <span class=\"pl-k\">in</span> initialized:\n             <span class=\"pl-k\">continue</span>\n         shape <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(d.dim_value <span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> value_info.type.tensor_type.shape.dim)\n         ws.FeedBlob(value_info.name, np.ones(shape, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32), device_option)</pre></div>\n<p>Unless this is the expected behavior? But I figured since it's just the initialization process for the graph inputs, and they're initialized to ones, I really don't see a particular reason as to why it cannot be float32 instead.</p>\n<p>Thanks</p>", "body_text": "\ud83d\udc1b Bug\nWhen working with an ONNX model, preparing the backend automatically initializes the graph inputs as float64, which then throws out the following warning when specifying a CUDA device:\nCUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: 0 type: float64\nThis is because numpy.ones has default datatype numpy.float64.\nTo Reproduce\nSteps to reproduce the behavior:\nIn my case, I initially converted a pre-trained PyTorch model to ONNX with torch.onnx._export(), but I believe this warning should still appear when working with any ONNX model.\n\nConvert PyTorch model to ONNX with torch.onnx._export(torch_model, example_input, 'model.onnx', export_params=True)\nLoad ONNX model with onnx: onnx.load('model.onnx')\nPrepare the caffe2 backend: caffe2.python.onnx.backend.prepare(model, device='CUDA:0')\n\nSample code:\n# Prepare backend\nprint('Preparing backend...')\nprep_t0 = time.time()\nprepared_backend = caffe2.python.onnx.backend.prepare(model, device='CUDA:0')\nprint('Backend prepared in {} seconds'.format(time.time() - prep_t0))\n# Run the ONNX model with Caffe2\nprint('Running inference..')\nfwd_t0 = time.time()\noutputs = prepared_backend.run(img_arr.astype(np.float32))[0]\nprint('Foward pass time: {} seconds'.format(time.time() - fwd_t0))\nOutputs:\nPreparing backend...\nCUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: 0 type: float64\nBackend prepared in 1.4734389781951904 seconds\nRunning inference..\nFoward pass time: 0.04503035545349121 seconds\n\nExpected behavior\nThe behavior is as expected apart from the warning. The model still seems to be loaded on GPU and inference is executed on GPU as well. But I was wondering why I was getting a warning for such a simple task.\nEnvironment\n\nPyTorch Version (e.g., 1.0): 1.0.0.dev20181105\nOS (e.g., Linux): Linux\nHow you installed PyTorch (conda, pip, source): conda\nPython version: 3.6.6\nCUDA/cuDNN version: 9.2.148/7104\nGPU models and configuration: GeForce GTX 1080 Ti with drivers 410.73\n\nAdditional context\nAs mentioned above, this error is thrown when preparing the caffe2 backend for CUDA device, and I managed to remove the warning by modifying the source here:\n@classmethod\ndef _direct_initialize_inputs(cls, inputs, initialized, ws, device_option):\n     for value_info in inputs:\n         if value_info.name in initialized:\n             continue\n         shape = list(d.dim_value for d in value_info.type.tensor_type.shape.dim)\n         ws.FeedBlob(value_info.name, np.ones(shape, dtype=np.float32), device_option)\nUnless this is the expected behavior? But I figured since it's just the initialization process for the graph inputs, and they're initialized to ones, I really don't see a particular reason as to why it cannot be float32 instead.\nThanks", "body": "## \ud83d\udc1b Bug\r\n\r\nWhen working with an ONNX model, preparing the backend [automatically initializes](https://github.com/pytorch/pytorch/blob/master/caffe2/python/onnx/backend.py#L647) the graph inputs as float64, which then throws out the following warning when specifying a CUDA device: \r\n`CUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: 0 type: float64`\r\n\r\nThis is because `numpy.ones` has default datatype numpy.float64.\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nIn my case, I initially converted a pre-trained PyTorch model to ONNX with torch.onnx._export(), but I believe this warning should still appear when working with any ONNX model.\r\n\r\n1. Convert PyTorch model to ONNX with `torch.onnx._export(torch_model, example_input, 'model.onnx', export_params=True)` \r\n1. Load ONNX model with [onnx](https://github.com/onnx/onnx): `onnx.load('model.onnx')`\r\n1. Prepare the caffe2 backend: `caffe2.python.onnx.backend.prepare(model, device='CUDA:0')`\r\n\r\nSample code:\r\n```python\r\n# Prepare backend\r\nprint('Preparing backend...')\r\nprep_t0 = time.time()\r\nprepared_backend = caffe2.python.onnx.backend.prepare(model, device='CUDA:0')\r\nprint('Backend prepared in {} seconds'.format(time.time() - prep_t0))\r\n# Run the ONNX model with Caffe2\r\nprint('Running inference..')\r\nfwd_t0 = time.time()\r\noutputs = prepared_backend.run(img_arr.astype(np.float32))[0]\r\nprint('Foward pass time: {} seconds'.format(time.time() - fwd_t0))\r\n```\r\n\r\nOutputs:\r\n```\r\nPreparing backend...\r\nCUDA operators do not support 64-bit doubles, please use arr.astype(np.float32) or np.int32 for ints. Blob: 0 type: float64\r\nBackend prepared in 1.4734389781951904 seconds\r\nRunning inference..\r\nFoward pass time: 0.04503035545349121 seconds\r\n```\r\n## Expected behavior\r\n\r\nThe behavior is as expected apart from the warning. The model still seems to be loaded on GPU and inference is executed on GPU as well. But I was wondering why I was getting a warning for such a simple task.\r\n\r\n## Environment\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0.0.dev20181105\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): conda\r\n - Python version: 3.6.6\r\n - CUDA/cuDNN version: 9.2.148/7104\r\n - GPU models and configuration: GeForce GTX 1080 Ti with drivers 410.73\r\n\r\n## Additional context\r\n\r\nAs mentioned above, this error is thrown when preparing the caffe2 backend for CUDA device, and I managed to remove the warning by modifying the source [here](https://github.com/pytorch/pytorch/blob/master/caffe2/python/onnx/backend.py#L647):\r\n\r\n```py\r\n@classmethod\r\ndef _direct_initialize_inputs(cls, inputs, initialized, ws, device_option):\r\n     for value_info in inputs:\r\n         if value_info.name in initialized:\r\n             continue\r\n         shape = list(d.dim_value for d in value_info.type.tensor_type.shape.dim)\r\n         ws.FeedBlob(value_info.name, np.ones(shape, dtype=np.float32), device_option)\r\n```\r\nUnless this is the expected behavior? But I figured since it's just the initialization process for the graph inputs, and they're initialized to ones, I really don't see a particular reason as to why it cannot be float32 instead.\r\n\r\n\r\nThanks"}