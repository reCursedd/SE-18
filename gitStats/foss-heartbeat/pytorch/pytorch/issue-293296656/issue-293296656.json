{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4968", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4968/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4968/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4968/events", "html_url": "https://github.com/pytorch/pytorch/issues/4968", "id": 293296656, "node_id": "MDU6SXNzdWUyOTMyOTY2NTY=", "number": 4968, "title": "We should track implicit scalar broadcasts for backward", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-01-31T20:15:44Z", "updated_at": "2018-01-31T20:15:44Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Today, we 'implicitly' broadcast scalars to 1-dimensional tensors when calling into TH; it's implicit because that's just how the tensors are represented in TH.  Take for example <code>torch.ger</code>; if I call this on two scalars, they are both implicitly broadcasted to size [1] tensors/variables and the result is a [1,1] tensor/variable; this behavior matches numpy.  But the backwards need to be written to take this implicit broadcast into account, i.e. this backwards declaration in derivatives.yaml is incorrect:<br>\n<code>self: grad.mv(vec2)</code><br>\nbecause the result has to be 0-d if self was 0-d.  So, we need to write backwards to take the broadcast into account, which we don't normally have to do (because we explicitly broadcast elsewhere and that gets captured by autograd).</p>\n<p>Note this only matters for functions that call into TH/THC.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a></p>", "body_text": "Today, we 'implicitly' broadcast scalars to 1-dimensional tensors when calling into TH; it's implicit because that's just how the tensors are represented in TH.  Take for example torch.ger; if I call this on two scalars, they are both implicitly broadcasted to size [1] tensors/variables and the result is a [1,1] tensor/variable; this behavior matches numpy.  But the backwards need to be written to take this implicit broadcast into account, i.e. this backwards declaration in derivatives.yaml is incorrect:\nself: grad.mv(vec2)\nbecause the result has to be 0-d if self was 0-d.  So, we need to write backwards to take the broadcast into account, which we don't normally have to do (because we explicitly broadcast elsewhere and that gets captured by autograd).\nNote this only matters for functions that call into TH/THC.\nCC @colesbury @ezyang", "body": "Today, we 'implicitly' broadcast scalars to 1-dimensional tensors when calling into TH; it's implicit because that's just how the tensors are represented in TH.  Take for example `torch.ger`; if I call this on two scalars, they are both implicitly broadcasted to size [1] tensors/variables and the result is a [1,1] tensor/variable; this behavior matches numpy.  But the backwards need to be written to take this implicit broadcast into account, i.e. this backwards declaration in derivatives.yaml is incorrect:\r\n`\r\nself: grad.mv(vec2)\r\n`\r\nbecause the result has to be 0-d if self was 0-d.  So, we need to write backwards to take the broadcast into account, which we don't normally have to do (because we explicitly broadcast elsewhere and that gets captured by autograd).\r\n\r\nNote this only matters for functions that call into TH/THC.\r\n\r\nCC @colesbury @ezyang "}