{"url": "https://api.github.com/repos/pytorch/pytorch/issues/890", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/890/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/890/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/890/events", "html_url": "https://github.com/pytorch/pytorch/issues/890", "id": 211183536, "node_id": "MDU6SXNzdWUyMTExODM1MzY=", "number": 890, "title": "Hanging on myTensor.cuda()", "user": {"login": "alexbw", "id": 161935, "node_id": "MDQ6VXNlcjE2MTkzNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161935?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexbw", "html_url": "https://github.com/alexbw", "followers_url": "https://api.github.com/users/alexbw/followers", "following_url": "https://api.github.com/users/alexbw/following{/other_user}", "gists_url": "https://api.github.com/users/alexbw/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexbw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexbw/subscriptions", "organizations_url": "https://api.github.com/users/alexbw/orgs", "repos_url": "https://api.github.com/users/alexbw/repos", "events_url": "https://api.github.com/users/alexbw/events{/privacy}", "received_events_url": "https://api.github.com/users/alexbw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-01T19:29:53Z", "updated_at": "2017-03-01T19:46:06Z", "closed_at": "2017-03-01T19:46:06Z", "author_association": "NONE", "body_html": "<p>Hey all, good looking library.</p>\n<p>I'm running into the following issue, and I'm sure it's a configuration problem on my end, but I don't know how to begin to debug it.</p>\n<p>I have two cards, a Titan X, and an older Quadro (to drive a monitor). Titan X is device 0, and Quadro is device 1. I'm on Python 3.5. I've got CUDA 8.0 installed.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\na <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>)\na_gpu <span class=\"pl-k\">=</span> a.cuda() <span class=\"pl-c\"><span class=\"pl-c\">#</span> &lt;-- hangs forever here.</span></pre></div>\n<p>One core of my CPU is pinned at 100%, doing something. Restarting the machine does not help. Other CUDA-backed systems, e.g. Cupy, do not have a problem.</p>\n<p>If I interrupt the kernel, here's the traceback:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-ii\">--------------------------------------------------------------------------</span><span class=\"pl-k\">-</span>\n<span class=\"pl-c1\">KeyboardInterrupt</span>                         Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>ca988235a575<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n      <span class=\"pl-c1\">1</span> <span class=\"pl-k\">import</span> torch\n      <span class=\"pl-c1\">2</span> a <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>)\n<span class=\"pl-ii\">----</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">3</span> a_gpu <span class=\"pl-k\">=</span> a.cuda() <span class=\"pl-c\"><span class=\"pl-c\">#</span> &lt;-- hangs forever here</span>\n\n<span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>alexbw<span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.5<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>_utils.py <span class=\"pl-k\">in</span> _cuda(<span class=\"pl-c1\">self</span>, device, <span class=\"pl-k\">async</span>)\n     <span class=\"pl-c1\">49</span>             device <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>\n     <span class=\"pl-c1\">50</span>         <span class=\"pl-k\">with</span> torch.cuda.device(device):\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">51</span>             <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.type(<span class=\"pl-c1\">getattr</span>(torch.cuda, <span class=\"pl-c1\">self</span>.<span class=\"pl-c1\">__class__</span>.<span class=\"pl-c1\">__name__</span>), <span class=\"pl-k\">async</span>)\n     <span class=\"pl-c1\">52</span> \n     <span class=\"pl-c1\">53</span> \n\n<span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>alexbw<span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.5<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>_utils.py <span class=\"pl-k\">in</span> _type(<span class=\"pl-c1\">self</span>, new_type, <span class=\"pl-k\">async</span>)\n     <span class=\"pl-c1\">22</span>     <span class=\"pl-k\">if</span> new_type <span class=\"pl-k\">==</span> <span class=\"pl-c1\">type</span>(<span class=\"pl-c1\">self</span>):\n     <span class=\"pl-c1\">23</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">24</span>     <span class=\"pl-k\">return</span> new_type(<span class=\"pl-c1\">self</span>.size()).copy_(<span class=\"pl-c1\">self</span>, <span class=\"pl-k\">async</span>)\n     <span class=\"pl-c1\">25</span> \n     <span class=\"pl-c1\">26</span> \n\n<span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>alexbw<span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.5<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>cuda<span class=\"pl-k\">/</span><span class=\"pl-c1\">__init__</span>.py <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__new__</span>(<span class=\"pl-c1\">cls</span>, <span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">255</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> We need this method only for lazy init, so we can remove it</span>\n    <span class=\"pl-c1\">256</span>         <span class=\"pl-k\">del</span> _CudaBase.<span class=\"pl-c1\">__new__</span>\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">257</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">super</span>(_CudaBase, <span class=\"pl-c1\">cls</span>).<span class=\"pl-c1\">__new__</span>(<span class=\"pl-c1\">cls</span>, <span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">258</span> \n    <span class=\"pl-c1\">259</span> \n\n<span class=\"pl-c1\">KeyboardInterrupt</span>: </pre></div>\n<p>However, if I don't use the Titan X, everything works fine.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\ntorch.cuda.set_device(<span class=\"pl-c1\">1</span>)\na <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>)\na_gpu <span class=\"pl-k\">=</span> a.cuda() <span class=\"pl-c\"><span class=\"pl-c\">#</span> works just fine</span></pre></div>\n<p>Any idea why this is? Any pointers for how to get to the bottom of it? Should I downgrade to CUDA 7.5?</p>", "body_text": "Hey all, good looking library.\nI'm running into the following issue, and I'm sure it's a configuration problem on my end, but I don't know how to begin to debug it.\nI have two cards, a Titan X, and an older Quadro (to drive a monitor). Titan X is device 0, and Quadro is device 1. I'm on Python 3.5. I've got CUDA 8.0 installed.\nimport torch\na = torch.randn(3,3)\na_gpu = a.cuda() # <-- hangs forever here.\nOne core of my CPU is pinned at 100%, doing something. Restarting the machine does not help. Other CUDA-backed systems, e.g. Cupy, do not have a problem.\nIf I interrupt the kernel, here's the traceback:\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n<ipython-input-1-ca988235a575> in <module>()\n      1 import torch\n      2 a = torch.randn(3,3)\n----> 3 a_gpu = a.cuda() # <-- hangs forever here\n\n/home/alexbw/anaconda/lib/python3.5/site-packages/torch/_utils.py in _cuda(self, device, async)\n     49             device = -1\n     50         with torch.cuda.device(device):\n---> 51             return self.type(getattr(torch.cuda, self.__class__.__name__), async)\n     52 \n     53 \n\n/home/alexbw/anaconda/lib/python3.5/site-packages/torch/_utils.py in _type(self, new_type, async)\n     22     if new_type == type(self):\n     23         return self\n---> 24     return new_type(self.size()).copy_(self, async)\n     25 \n     26 \n\n/home/alexbw/anaconda/lib/python3.5/site-packages/torch/cuda/__init__.py in __new__(cls, *args, **kwargs)\n    255         # We need this method only for lazy init, so we can remove it\n    256         del _CudaBase.__new__\n--> 257         return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\n    258 \n    259 \n\nKeyboardInterrupt: \nHowever, if I don't use the Titan X, everything works fine.\nimport torch\ntorch.cuda.set_device(1)\na = torch.randn(3,3)\na_gpu = a.cuda() # works just fine\nAny idea why this is? Any pointers for how to get to the bottom of it? Should I downgrade to CUDA 7.5?", "body": "Hey all, good looking library.\r\n\r\nI'm running into the following issue, and I'm sure it's a configuration problem on my end, but I don't know how to begin to debug it.\r\n\r\nI have two cards, a Titan X, and an older Quadro (to drive a monitor). Titan X is device 0, and Quadro is device 1. I'm on Python 3.5. I've got CUDA 8.0 installed.\r\n\r\n```python\r\nimport torch\r\na = torch.randn(3,3)\r\na_gpu = a.cuda() # <-- hangs forever here.\r\n```\r\n\r\nOne core of my CPU is pinned at 100%, doing something. Restarting the machine does not help. Other CUDA-backed systems, e.g. Cupy, do not have a problem.\r\n\r\nIf I interrupt the kernel, here's the traceback:\r\n```python\r\n---------------------------------------------------------------------------\r\nKeyboardInterrupt                         Traceback (most recent call last)\r\n<ipython-input-1-ca988235a575> in <module>()\r\n      1 import torch\r\n      2 a = torch.randn(3,3)\r\n----> 3 a_gpu = a.cuda() # <-- hangs forever here\r\n\r\n/home/alexbw/anaconda/lib/python3.5/site-packages/torch/_utils.py in _cuda(self, device, async)\r\n     49             device = -1\r\n     50         with torch.cuda.device(device):\r\n---> 51             return self.type(getattr(torch.cuda, self.__class__.__name__), async)\r\n     52 \r\n     53 \r\n\r\n/home/alexbw/anaconda/lib/python3.5/site-packages/torch/_utils.py in _type(self, new_type, async)\r\n     22     if new_type == type(self):\r\n     23         return self\r\n---> 24     return new_type(self.size()).copy_(self, async)\r\n     25 \r\n     26 \r\n\r\n/home/alexbw/anaconda/lib/python3.5/site-packages/torch/cuda/__init__.py in __new__(cls, *args, **kwargs)\r\n    255         # We need this method only for lazy init, so we can remove it\r\n    256         del _CudaBase.__new__\r\n--> 257         return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\r\n    258 \r\n    259 \r\n\r\nKeyboardInterrupt: \r\n```\r\n\r\n\r\n\r\nHowever, if I don't use the Titan X, everything works fine.\r\n```python\r\nimport torch\r\ntorch.cuda.set_device(1)\r\na = torch.randn(3,3)\r\na_gpu = a.cuda() # works just fine\r\n```\r\n\r\nAny idea why this is? Any pointers for how to get to the bottom of it? Should I downgrade to CUDA 7.5?"}