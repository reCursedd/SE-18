{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10843", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10843/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10843/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10843/events", "html_url": "https://github.com/pytorch/pytorch/pull/10843", "id": 353612678, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEwNjE4MTkw", "number": 10843, "title": "Prevent JIT from overspecializing to every single size configuration", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-24T01:41:10Z", "updated_at": "2018-11-23T15:49:53Z", "closed_at": "2018-08-24T01:41:22Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10843", "html_url": "https://github.com/pytorch/pytorch/pull/10843", "diff_url": "https://github.com/pytorch/pytorch/pull/10843.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10843.patch"}, "body_html": "<p>Summary of changes:</p>\n<ul>\n<li>Renamed <code>TensorType</code> to <code>CompleteTensorType</code> and added <code>TensorType</code> which records only the scalar type, number of dimensions, and device of a value. The argument behind the rename is to encourage people to use <code>CompleteTensorType</code> less, as most passes will only have limited information available. To make transition easier <code>complete_type-&gt;cast&lt;TensorType&gt;()</code> works, and makes our passes work with both kinds of specialization if they don't need extra the extra detail.</li>\n<li>Shape analysis can process graphs with both <code>CompleteTensorType</code> and <code>TensorType</code>.</li>\n<li>Fuser was a part that heavily relied on full shape information being available. Now, we simply try to fuse the largest possible graphs, and have to do run-time checks to make sure they match the code we generate. If they don't, we fall back to regular interpretation. The shape checks are implementing using an optimized method exploiting algebraic properties of shapes with broadcasting, and the relations of broadcasting with pointwise ops. A full written proof of correctness of the shape checking algorithm is included in a comment in <code>fusion_compiler.cpp</code>.</li>\n</ul>", "body_text": "Summary of changes:\n\nRenamed TensorType to CompleteTensorType and added TensorType which records only the scalar type, number of dimensions, and device of a value. The argument behind the rename is to encourage people to use CompleteTensorType less, as most passes will only have limited information available. To make transition easier complete_type->cast<TensorType>() works, and makes our passes work with both kinds of specialization if they don't need extra the extra detail.\nShape analysis can process graphs with both CompleteTensorType and TensorType.\nFuser was a part that heavily relied on full shape information being available. Now, we simply try to fuse the largest possible graphs, and have to do run-time checks to make sure they match the code we generate. If they don't, we fall back to regular interpretation. The shape checks are implementing using an optimized method exploiting algebraic properties of shapes with broadcasting, and the relations of broadcasting with pointwise ops. A full written proof of correctness of the shape checking algorithm is included in a comment in fusion_compiler.cpp.", "body": "Summary of changes:\r\n\r\n- Renamed `TensorType` to `CompleteTensorType` and added `TensorType` which records only the scalar type, number of dimensions, and device of a value. The argument behind the rename is to encourage people to use `CompleteTensorType` less, as most passes will only have limited information available. To make transition easier `complete_type->cast<TensorType>()` works, and makes our passes work with both kinds of specialization if they don't need extra the extra detail.\r\n- Shape analysis can process graphs with both `CompleteTensorType` and `TensorType`.\r\n- Fuser was a part that heavily relied on full shape information being available. Now, we simply try to fuse the largest possible graphs, and have to do run-time checks to make sure they match the code we generate. If they don't, we fall back to regular interpretation. The shape checks are implementing using an optimized method exploiting algebraic properties of shapes with broadcasting, and the relations of broadcasting with pointwise ops. A full written proof of correctness of the shape checking algorithm is included in a comment in `fusion_compiler.cpp`."}