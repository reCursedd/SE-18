{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3835", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3835/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3835/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3835/events", "html_url": "https://github.com/pytorch/pytorch/issues/3835", "id": 276076844, "node_id": "MDU6SXNzdWUyNzYwNzY4NDQ=", "number": 3835, "title": "ConvNd function leaks memory", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-11-22T13:58:03Z", "updated_at": "2018-06-28T00:17:12Z", "closed_at": "2018-06-28T00:17:12Z", "author_association": "MEMBER", "body_html": "<p>Originally reported by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7605917\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dmarnerides\">@dmarnerides</a> in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"274653738\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3743\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3743/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3743\">#3743</a>:</p>\n<hr>\n<p>I can reproduce the memory leak with this:</p>\n<pre><code>import gc\nimport resource\nimport torch\nfrom torch import nn, autograd\nfrom torch.autograd import Variable\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 1, 1, 1),\n            nn.Conv2d(1, 1, 1, 1),\n        )\n    def forward(self, v_x):\n        return self.main(v_x).view(v_x.size(0), 1)\n\nnet = Network()\n\ni = 0\nwhile True:\n    v_in = Variable(torch.Tensor(2,1,1,1), requires_grad=True)\n    grad_out = Variable(torch.ones(2,1,1,1))\n\n    gradient = autograd.grad(outputs=net(v_in), inputs=v_in,\n                             grad_outputs=grad_out,\n                             create_graph=True, retain_graph=True, \n                             only_inputs=True)[0]\n    gradient.mean().backward()\n\n    i += 1\n    if i % 512 == 0:\n        gc.collect()\n        max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n        print(\"{:.2f} MB\".format(max_mem_used / 1024))\n</code></pre>\n<p>However the leak disappears when only one convolution is used:</p>\n<pre><code>class Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 1, 1, 1),\n        )\n    def forward(self, v_x):\n        return self.main(v_x).view(v_x.size(0), 1)\n</code></pre>\n<p>Also, if nn.Linear is used, there is no leak.</p>\n<p>Torch version: 0.4.0a0+8ebf18b</p>", "body_text": "Originally reported by @dmarnerides in #3743:\n\nI can reproduce the memory leak with this:\nimport gc\nimport resource\nimport torch\nfrom torch import nn, autograd\nfrom torch.autograd import Variable\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 1, 1, 1),\n            nn.Conv2d(1, 1, 1, 1),\n        )\n    def forward(self, v_x):\n        return self.main(v_x).view(v_x.size(0), 1)\n\nnet = Network()\n\ni = 0\nwhile True:\n    v_in = Variable(torch.Tensor(2,1,1,1), requires_grad=True)\n    grad_out = Variable(torch.ones(2,1,1,1))\n\n    gradient = autograd.grad(outputs=net(v_in), inputs=v_in,\n                             grad_outputs=grad_out,\n                             create_graph=True, retain_graph=True, \n                             only_inputs=True)[0]\n    gradient.mean().backward()\n\n    i += 1\n    if i % 512 == 0:\n        gc.collect()\n        max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n        print(\"{:.2f} MB\".format(max_mem_used / 1024))\n\nHowever the leak disappears when only one convolution is used:\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(1, 1, 1, 1),\n        )\n    def forward(self, v_x):\n        return self.main(v_x).view(v_x.size(0), 1)\n\nAlso, if nn.Linear is used, there is no leak.\nTorch version: 0.4.0a0+8ebf18b", "body": "Originally reported by @dmarnerides in #3743:\r\n\r\n---\r\n\r\nI can reproduce the memory leak with this:\r\n\r\n```\r\nimport gc\r\nimport resource\r\nimport torch\r\nfrom torch import nn, autograd\r\nfrom torch.autograd import Variable\r\n\r\nclass Network(nn.Module):\r\n    def __init__(self):\r\n        super(Network, self).__init__()\r\n        self.main = nn.Sequential(\r\n            nn.Conv2d(1, 1, 1, 1),\r\n            nn.Conv2d(1, 1, 1, 1),\r\n        )\r\n    def forward(self, v_x):\r\n        return self.main(v_x).view(v_x.size(0), 1)\r\n\r\nnet = Network()\r\n\r\ni = 0\r\nwhile True:\r\n    v_in = Variable(torch.Tensor(2,1,1,1), requires_grad=True)\r\n    grad_out = Variable(torch.ones(2,1,1,1))\r\n\r\n    gradient = autograd.grad(outputs=net(v_in), inputs=v_in,\r\n                             grad_outputs=grad_out,\r\n                             create_graph=True, retain_graph=True, \r\n                             only_inputs=True)[0]\r\n    gradient.mean().backward()\r\n\r\n    i += 1\r\n    if i % 512 == 0:\r\n        gc.collect()\r\n        max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n        print(\"{:.2f} MB\".format(max_mem_used / 1024))\r\n```\r\n\r\nHowever the leak disappears when only one convolution is used:\r\n```\r\nclass Network(nn.Module):\r\n    def __init__(self):\r\n        super(Network, self).__init__()\r\n        self.main = nn.Sequential(\r\n            nn.Conv2d(1, 1, 1, 1),\r\n        )\r\n    def forward(self, v_x):\r\n        return self.main(v_x).view(v_x.size(0), 1)\r\n```\r\n\r\nAlso, if nn.Linear is used, there is no leak.\r\n\r\nTorch version: 0.4.0a0+8ebf18b"}