{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/411409465", "html_url": "https://github.com/pytorch/pytorch/issues/10276#issuecomment-411409465", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10276", "id": 411409465, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMTQwOTQ2NQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-08T13:41:55Z", "updated_at": "2018-08-08T13:41:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So the fundamental reason is that <code>clamp_out</code> is implemented using <a href=\"https://github.com/pytorch/pytorch/blob/0d03219a421310c2c8c0f287284e27e31ee74562/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp#L17\"><code>copy_</code> + <code>clamp_</code></a>, when the <a href=\"https://github.com/pytorch/pytorch/blob/0d03219a421310c2c8c0f287284e27e31ee74562/aten/src/THC/generic/THCTensorMathPointwise.cu#L92\">original THC clamp</a> used to just copy while clamping.<br>\nI'd say the best way to fix this might be to move the clamp code and kernel to ATen proper and not punting it to TH/THC, but one could also replace <code>_th_clamp_</code> (and min max friends) with <code>_th_clamp_out</code> and call it with self, self when we want <code>_th_clamp_</code>.</p>", "body_text": "So the fundamental reason is that clamp_out is implemented using copy_ + clamp_, when the original THC clamp used to just copy while clamping.\nI'd say the best way to fix this might be to move the clamp code and kernel to ATen proper and not punting it to TH/THC, but one could also replace _th_clamp_ (and min max friends) with _th_clamp_out and call it with self, self when we want _th_clamp_.", "body": "So the fundamental reason is that `clamp_out` is implemented using [`copy_` + `clamp_`](https://github.com/pytorch/pytorch/blob/0d03219a421310c2c8c0f287284e27e31ee74562/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp#L17), when the [original THC clamp](https://github.com/pytorch/pytorch/blob/0d03219a421310c2c8c0f287284e27e31ee74562/aten/src/THC/generic/THCTensorMathPointwise.cu#L92) used to just copy while clamping.\r\nI'd say the best way to fix this might be to move the clamp code and kernel to ATen proper and not punting it to TH/THC, but one could also replace `_th_clamp_` (and min max friends) with `_th_clamp_out` and call it with self, self when we want `_th_clamp_`."}