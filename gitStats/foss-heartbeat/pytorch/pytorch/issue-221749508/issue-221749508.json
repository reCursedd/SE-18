{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1260", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1260/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1260/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1260/events", "html_url": "https://github.com/pytorch/pytorch/issues/1260", "id": 221749508, "node_id": "MDU6SXNzdWUyMjE3NDk1MDg=", "number": 1260, "title": "Any easy approach to apply nn.Softmax() along each dimension?", "user": {"login": "yuandong-tian", "id": 2973937, "node_id": "MDQ6VXNlcjI5NzM5Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/2973937?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuandong-tian", "html_url": "https://github.com/yuandong-tian", "followers_url": "https://api.github.com/users/yuandong-tian/followers", "following_url": "https://api.github.com/users/yuandong-tian/following{/other_user}", "gists_url": "https://api.github.com/users/yuandong-tian/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuandong-tian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuandong-tian/subscriptions", "organizations_url": "https://api.github.com/users/yuandong-tian/orgs", "repos_url": "https://api.github.com/users/yuandong-tian/repos", "events_url": "https://api.github.com/users/yuandong-tian/events{/privacy}", "received_events_url": "https://api.github.com/users/yuandong-tian/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-14T06:15:18Z", "updated_at": "2017-04-14T14:51:18Z", "closed_at": "2017-04-14T14:51:18Z", "author_association": "NONE", "body_html": "<p>Currently nn.Softmax() can only be applied on 2D tensor. Is that possible we can specify dimension? E.g.,</p>\n<pre><code>x = torch.FloatTensor(128, 5, 27)\nsoftmax = nn.Softmax(axis=1)\ny = softmax(Variable(x))\ny.size()\n</code></pre>\n<p><code>y.size() == x.size()</code> but <code>y</code>'s second dimension (5) has been applied softmax operation.</p>", "body_text": "Currently nn.Softmax() can only be applied on 2D tensor. Is that possible we can specify dimension? E.g.,\nx = torch.FloatTensor(128, 5, 27)\nsoftmax = nn.Softmax(axis=1)\ny = softmax(Variable(x))\ny.size()\n\ny.size() == x.size() but y's second dimension (5) has been applied softmax operation.", "body": "Currently nn.Softmax() can only be applied on 2D tensor. Is that possible we can specify dimension? E.g., \r\n\r\n```\r\nx = torch.FloatTensor(128, 5, 27)\r\nsoftmax = nn.Softmax(axis=1)\r\ny = softmax(Variable(x))\r\ny.size()\r\n```\r\n`y.size() == x.size()` but `y`'s second dimension (5) has been applied softmax operation.  \r\n"}