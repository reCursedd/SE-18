{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396569717", "html_url": "https://github.com/pytorch/pytorch/pull/8371#issuecomment-396569717", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8371", "id": 396569717, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjU2OTcxNw==", "user": {"login": "bombs-kim", "id": 11001573, "node_id": "MDQ6VXNlcjExMDAxNTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/11001573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bombs-kim", "html_url": "https://github.com/bombs-kim", "followers_url": "https://api.github.com/users/bombs-kim/followers", "following_url": "https://api.github.com/users/bombs-kim/following{/other_user}", "gists_url": "https://api.github.com/users/bombs-kim/gists{/gist_id}", "starred_url": "https://api.github.com/users/bombs-kim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bombs-kim/subscriptions", "organizations_url": "https://api.github.com/users/bombs-kim/orgs", "repos_url": "https://api.github.com/users/bombs-kim/repos", "events_url": "https://api.github.com/users/bombs-kim/events{/privacy}", "received_events_url": "https://api.github.com/users/bombs-kim/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-12T12:22:09Z", "updated_at": "2018-06-15T02:28:01Z", "author_association": "CONTRIBUTOR", "body_html": "<h1>Results</h1>\n<h3>Environment</h3>\n<p>OS: Ubuntu<br>\nCPU cores: 32<br>\nGPU: Geforce GTX 1080 x 1</p>\n<h3>Test script</h3>\n<details><summary>click</summary>\n<p>\n</p><div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">import</span> torchvision.datasets <span class=\"pl-k\">as</span> datasets\n<span class=\"pl-k\">import</span> torchvision.transforms <span class=\"pl-k\">as</span> transforms\n\n\nto_tensor <span class=\"pl-k\">=</span> transforms.ToTensor()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">some_augmentation</span>(<span class=\"pl-smi\">img</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Simulating doing some heavy computation<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>.<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">end</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>); sys.stdout.flush()\n\n    t <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">while</span> time.time() <span class=\"pl-k\">-</span> t <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0.1</span>:\n        <span class=\"pl-k\">pass</span>\n    <span class=\"pl-k\">return</span> to_tensor(img)\n\n<span class=\"pl-c1\">DATASET_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\ndataset <span class=\"pl-k\">=</span> datasets.MNIST(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>../data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>some_augmentation, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ndataset.train_data <span class=\"pl-k\">=</span> dataset.train_data[:<span class=\"pl-c1\">DATASET_SIZE</span>]\ndataset.train_labels <span class=\"pl-k\">=</span> dataset.train_labels[:<span class=\"pl-c1\">DATASET_SIZE</span>]\n\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\n<span class=\"pl-c1\">NUM_WORKERS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">16</span>\n\ntrain_loader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">NUM_WORKERS</span>)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.linear0 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">28</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">10000</span>)\n        <span class=\"pl-c1\">self</span>.linear1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">10000</span>)\n        <span class=\"pl-c1\">self</span>.linear2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">28</span>)\n        x <span class=\"pl-k\">=</span> F.sigmoid(<span class=\"pl-c1\">self</span>.linear0(x))\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">50</span>):\n            x <span class=\"pl-k\">=</span> F.sigmoid(<span class=\"pl-c1\">self</span>.linear1(x))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear2(x)\n        <span class=\"pl-k\">return</span> F.log_softmax(x, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\nmodel <span class=\"pl-k\">=</span> Net()\nmodel.cuda()\n\ncriterion <span class=\"pl-k\">=</span> nn.CrossEntropyLoss().cuda()\n\nmodel.train()\nt <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">for</span> i, (<span class=\"pl-c1\">input</span>, target) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n    <span class=\"pl-k\">if</span> i <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        time_to_first_batch <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> t\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\n</span>batch index <span class=\"pl-pds\">\"</span></span>, i)\n    <span class=\"pl-c1\">input</span>, target <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.cuda(), target.cuda()\n    target <span class=\"pl-k\">=</span> target.cuda()\n    output <span class=\"pl-k\">=</span> model(<span class=\"pl-c1\">input</span>)\n    loss <span class=\"pl-k\">=</span> criterion(output, target)\n    loss.backward()\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\n</span>done<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> skip optimizer</span>\n\ntotal_time <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> t\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time to fisrt batch: <span class=\"pl-pds\">\"</span></span>, time_to_first_batch)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>total time: <span class=\"pl-pds\">\"</span></span>, total_time)</pre></div>\n<p></p>\n</details>\n<h3>Measurements</h3>\n<h4>previous version</h4>\n<p>time to fisrt batch:  12.95819354057312<br>\ntotal time:  18.71925663948059</p>\n<h4>new version</h4>\n<p>time to fisrt batch:  2.630634069442749<br>\ntotal time:  10.540151596069336</p>", "body_text": "Results\nEnvironment\nOS: Ubuntu\nCPU cores: 32\nGPU: Geforce GTX 1080 x 1\nTest script\nclick\n\nimport sys\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\n\nto_tensor = transforms.ToTensor()\ndef some_augmentation(img):\n    \"\"\"Simulating doing some heavy computation\"\"\"\n    print('.', end=''); sys.stdout.flush()\n\n    t = time.time()\n    while time.time() - t < 0.1:\n        pass\n    return to_tensor(img)\n\nDATASET_SIZE = 1000\ndataset = datasets.MNIST('../data', transform=some_augmentation, train=True, download=True)\ndataset.train_data = dataset.train_data[:DATASET_SIZE]\ndataset.train_labels = dataset.train_labels[:DATASET_SIZE]\n\nBATCH_SIZE = 128\nNUM_WORKERS = 16\n\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear0 = nn.Linear(28*28, 10000)\n        self.linear1 = nn.Linear(10000, 10000)\n        self.linear2 = nn.Linear(10000, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28*28)\n        x = F.sigmoid(self.linear0(x))\n        for i in range(50):\n            x = F.sigmoid(self.linear1(x))\n        x = self.linear2(x)\n        return F.log_softmax(x, dim=1)\n\nmodel = Net()\nmodel.cuda()\n\ncriterion = nn.CrossEntropyLoss().cuda()\n\nmodel.train()\nt = time.time()\nfor i, (input, target) in enumerate(train_loader):\n    if i == 0:\n        time_to_first_batch = time.time() - t\n    print(\"\\nbatch index \", i)\n    input, target = input.cuda(), target.cuda()\n    target = target.cuda()\n    output = model(input)\n    loss = criterion(output, target)\n    loss.backward()\n    print(\"\\ndone\")\n    # skip optimizer\n\ntotal_time = time.time() - t\nprint(\"time to fisrt batch: \", time_to_first_batch)\nprint(\"total time: \", total_time)\n\n\nMeasurements\nprevious version\ntime to fisrt batch:  12.95819354057312\ntotal time:  18.71925663948059\nnew version\ntime to fisrt batch:  2.630634069442749\ntotal time:  10.540151596069336", "body": "# Results\r\n\r\n### Environment\r\nOS: Ubuntu\r\nCPU cores: 32\r\nGPU: Geforce GTX 1080 x 1\r\n\r\n\r\n### Test script\r\n\r\n<details><summary>click</summary>\r\n<p>\r\n\r\n```python\r\nimport sys\r\nimport time\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torchvision.datasets as datasets\r\nimport torchvision.transforms as transforms\r\n\r\n\r\nto_tensor = transforms.ToTensor()\r\ndef some_augmentation(img):\r\n    \"\"\"Simulating doing some heavy computation\"\"\"\r\n    print('.', end=''); sys.stdout.flush()\r\n\r\n    t = time.time()\r\n    while time.time() - t < 0.1:\r\n        pass\r\n    return to_tensor(img)\r\n\r\nDATASET_SIZE = 1000\r\ndataset = datasets.MNIST('../data', transform=some_augmentation, train=True, download=True)\r\ndataset.train_data = dataset.train_data[:DATASET_SIZE]\r\ndataset.train_labels = dataset.train_labels[:DATASET_SIZE]\r\n\r\nBATCH_SIZE = 128\r\nNUM_WORKERS = 16\r\n\r\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear0 = nn.Linear(28*28, 10000)\r\n        self.linear1 = nn.Linear(10000, 10000)\r\n        self.linear2 = nn.Linear(10000, 10)\r\n\r\n    def forward(self, x):\r\n        x = x.view(-1, 28*28)\r\n        x = F.sigmoid(self.linear0(x))\r\n        for i in range(50):\r\n            x = F.sigmoid(self.linear1(x))\r\n        x = self.linear2(x)\r\n        return F.log_softmax(x, dim=1)\r\n\r\nmodel = Net()\r\nmodel.cuda()\r\n\r\ncriterion = nn.CrossEntropyLoss().cuda()\r\n\r\nmodel.train()\r\nt = time.time()\r\nfor i, (input, target) in enumerate(train_loader):\r\n    if i == 0:\r\n        time_to_first_batch = time.time() - t\r\n    print(\"\\nbatch index \", i)\r\n    input, target = input.cuda(), target.cuda()\r\n    target = target.cuda()\r\n    output = model(input)\r\n    loss = criterion(output, target)\r\n    loss.backward()\r\n    print(\"\\ndone\")\r\n    # skip optimizer\r\n\r\ntotal_time = time.time() - t\r\nprint(\"time to fisrt batch: \", time_to_first_batch)\r\nprint(\"total time: \", total_time)\r\n```\r\n</p>\r\n</details>\r\n\r\n\r\n### Measurements\r\n\r\n#### previous version\r\ntime to fisrt batch:  12.95819354057312\r\ntotal time:  18.71925663948059\r\n\r\n#### new version\r\ntime to fisrt batch:  2.630634069442749\r\ntotal time:  10.540151596069336\r\n\r\n"}