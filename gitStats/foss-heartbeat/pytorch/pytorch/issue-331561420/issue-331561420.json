{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8371", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8371/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8371/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8371/events", "html_url": "https://github.com/pytorch/pytorch/pull/8371", "id": 331561420, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk0MjU5NjE5", "number": 8371, "title": "Making dataloader workers work on each datapoint, not each batch, to reduce the time to first batch", "user": {"login": "bombs-kim", "id": 11001573, "node_id": "MDQ6VXNlcjExMDAxNTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/11001573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bombs-kim", "html_url": "https://github.com/bombs-kim", "followers_url": "https://api.github.com/users/bombs-kim/followers", "following_url": "https://api.github.com/users/bombs-kim/following{/other_user}", "gists_url": "https://api.github.com/users/bombs-kim/gists{/gist_id}", "starred_url": "https://api.github.com/users/bombs-kim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bombs-kim/subscriptions", "organizations_url": "https://api.github.com/users/bombs-kim/orgs", "repos_url": "https://api.github.com/users/bombs-kim/repos", "events_url": "https://api.github.com/users/bombs-kim/events{/privacy}", "received_events_url": "https://api.github.com/users/bombs-kim/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 559719279, "node_id": "MDU6TGFiZWw1NTk3MTkyNzk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ready%20for%20review", "name": "ready for review", "color": "b60205", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2018-06-12T12:14:05Z", "updated_at": "2018-11-23T15:45:23Z", "closed_at": "2018-08-16T23:42:27Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8371", "html_url": "https://github.com/pytorch/pytorch/pull/8371", "diff_url": "https://github.com/pytorch/pytorch/pull/8371.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8371.patch"}, "body_html": "<h1>Abstract</h1>\n<p>Current implementation of dataloader is not optimal in terms of time to first batch(TTFB). Also multi-process utilization is bad especially when the number of batches(not the batch size) is not a lot. I suggest making workers work on each datapoint, not on each mini-batch, to reduce TTFB and fully utilize  sub-processes spawned.</p>\n<h1>Rationale</h1>\n<p>With the current implementation of dataloader, you can only utilize as many processes as the number of (mini)batches. If the batch size is big and the dataset is relatively small, you may not be able to fully utilize worker processes. Let's say you have 10 batches from a dataset, and you have 32 workers.  22 workers are idle because they don't have any batch to work on.<br>\nAlso, in many cases, what really matters is only the TTFB not the general performance of dataloader. With the current implementation, multiple batches tend to finish at roughly the same time, and it's suboptimal in terms of <a href=\"https://en.wikipedia.org/wiki/Pipeline_(computing)\" rel=\"nofollow\">pipelining</a>.</p>\n<h1>Details</h1>\n<p>Each worker gets an an index at a time. Also, I added a dedicated process to collate samples spread. I assumed heavy computation should be only done within <code>dataset[sample_idx]</code> not in <code>collater_fn()</code>. So I thought single process will do for _collater_loop. Here is the simplified code.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_worker_loop</span>(...):\n    <span class=\"pl-c1\">...</span>\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        r <span class=\"pl-k\">=</span> index_queue.get()\n        batch_idx, inbatch_idx, sample_idx <span class=\"pl-k\">=</span> r  <span class=\"pl-c\"><span class=\"pl-c\">#</span># inbatch_idx added</span>\n        sample <span class=\"pl-k\">=</span> dataset[sample_idx]\n        result_queue.put((batch_idx, inbatch_idx, sample))\n        <span class=\"pl-c1\">...</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_collator_loop</span>(<span class=\"pl-smi\">result_queue</span>, <span class=\"pl-smi\">batch_queue</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">collate_fn</span>, <span class=\"pl-smi\">send_idx</span>):\n    <span class=\"pl-k\">from</span> collections <span class=\"pl-k\">import</span> defaultdict\n    batches <span class=\"pl-k\">=</span> defaultdict(<span class=\"pl-k\">lambda</span>: [<span class=\"pl-c1\">None</span>]<span class=\"pl-k\">*</span>batch_size)\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        r <span class=\"pl-k\">=</span> result_queue.get()\n        batch_idx, inbatch_idx, sample <span class=\"pl-k\">=</span> r\n        batches[batch_idx][inbatch_idx] <span class=\"pl-k\">=</span> sample\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Check if the batch is full</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">all</span>(batches[batch_idx]):\n            batch <span class=\"pl-k\">=</span> collate_fn(batches[batch_idx])\n            batch_queue.put((batch_idx, batch))\n            <span class=\"pl-k\">del</span> batches[batch_idx]</pre></div>\n<h3>A few more points</h3>\n<ul>\n<li>I had to remove <code>index_queues</code> and revert to the point when there was only one <code>index_queue</code>  while writing my code. I would like to discuss about it with others if necessary.</li>\n<li>In the actual code, a new sentinel object(Ellipsis) is used to indicate the end of the dataset</li>\n<li>Exception handling in <code>_collater_loop</code> is ad-hoc and can be improved.</li>\n</ul>\n<h1>Definitions</h1>\n<h3>Time to first batch(TTFB)</h3>\n<p>The time spent from the point an instance of _DataLoaderIter is created to the point when you can retrieve next batch by finishing calling <code>next(iterator)</code>.</p>\n<h1>Related pull request</h1>\n<p><a href=\"https://github.com/pytorch/pytorch/pull/4640\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4640/hovercard\">Fixed non-determinate preprocessing on DataLoader</a></p>\n<h1>Related issues open</h1>\n<p><a href=\"https://github.com/pytorch/pytorch/issues/8126\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8126/hovercard\">PyTorch multiprocessing using single CPU core</a><br>\n<a href=\"https://github.com/pytorch/pytorch/issues/8154\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/8154/hovercard\">Performance drop on small models trained on CPU between 0.3.1 and 0.4</a></p>", "body_text": "Abstract\nCurrent implementation of dataloader is not optimal in terms of time to first batch(TTFB). Also multi-process utilization is bad especially when the number of batches(not the batch size) is not a lot. I suggest making workers work on each datapoint, not on each mini-batch, to reduce TTFB and fully utilize  sub-processes spawned.\nRationale\nWith the current implementation of dataloader, you can only utilize as many processes as the number of (mini)batches. If the batch size is big and the dataset is relatively small, you may not be able to fully utilize worker processes. Let's say you have 10 batches from a dataset, and you have 32 workers.  22 workers are idle because they don't have any batch to work on.\nAlso, in many cases, what really matters is only the TTFB not the general performance of dataloader. With the current implementation, multiple batches tend to finish at roughly the same time, and it's suboptimal in terms of pipelining.\nDetails\nEach worker gets an an index at a time. Also, I added a dedicated process to collate samples spread. I assumed heavy computation should be only done within dataset[sample_idx] not in collater_fn(). So I thought single process will do for _collater_loop. Here is the simplified code.\ndef _worker_loop(...):\n    ...\n    while True:\n        r = index_queue.get()\n        batch_idx, inbatch_idx, sample_idx = r  ## inbatch_idx added\n        sample = dataset[sample_idx]\n        result_queue.put((batch_idx, inbatch_idx, sample))\n        ...\n\ndef _collator_loop(result_queue, batch_queue, batch_size, collate_fn, send_idx):\n    from collections import defaultdict\n    batches = defaultdict(lambda: [None]*batch_size)\n    while True:\n        r = result_queue.get()\n        batch_idx, inbatch_idx, sample = r\n        batches[batch_idx][inbatch_idx] = sample\n\n        # Check if the batch is full\n        if all(batches[batch_idx]):\n            batch = collate_fn(batches[batch_idx])\n            batch_queue.put((batch_idx, batch))\n            del batches[batch_idx]\nA few more points\n\nI had to remove index_queues and revert to the point when there was only one index_queue  while writing my code. I would like to discuss about it with others if necessary.\nIn the actual code, a new sentinel object(Ellipsis) is used to indicate the end of the dataset\nException handling in _collater_loop is ad-hoc and can be improved.\n\nDefinitions\nTime to first batch(TTFB)\nThe time spent from the point an instance of _DataLoaderIter is created to the point when you can retrieve next batch by finishing calling next(iterator).\nRelated pull request\nFixed non-determinate preprocessing on DataLoader\nRelated issues open\nPyTorch multiprocessing using single CPU core\nPerformance drop on small models trained on CPU between 0.3.1 and 0.4", "body": "# Abstract\r\n\r\nCurrent implementation of dataloader is not optimal in terms of time to first batch(TTFB). Also multi-process utilization is bad especially when the number of batches(not the batch size) is not a lot. I suggest making workers work on each datapoint, not on each mini-batch, to reduce TTFB and fully utilize  sub-processes spawned.\r\n\r\n# Rationale\r\nWith the current implementation of dataloader, you can only utilize as many processes as the number of (mini)batches. If the batch size is big and the dataset is relatively small, you may not be able to fully utilize worker processes. Let's say you have 10 batches from a dataset, and you have 32 workers.  22 workers are idle because they don't have any batch to work on.\r\nAlso, in many cases, what really matters is only the TTFB not the general performance of dataloader. With the current implementation, multiple batches tend to finish at roughly the same time, and it's suboptimal in terms of [pipelining](https://en.wikipedia.org/wiki/Pipeline_(computing)).\r\n\r\n# Details\r\n\r\nEach worker gets an an index at a time. Also, I added a dedicated process to collate samples spread. I assumed heavy computation should be only done within `dataset[sample_idx]` not in `collater_fn()`. So I thought single process will do for _collater_loop. Here is the simplified code.\r\n\r\n```python\r\ndef _worker_loop(...):\r\n    ...\r\n    while True:\r\n        r = index_queue.get()\r\n        batch_idx, inbatch_idx, sample_idx = r  ## inbatch_idx added\r\n        sample = dataset[sample_idx]\r\n        result_queue.put((batch_idx, inbatch_idx, sample))\r\n        ...\r\n\r\ndef _collator_loop(result_queue, batch_queue, batch_size, collate_fn, send_idx):\r\n    from collections import defaultdict\r\n    batches = defaultdict(lambda: [None]*batch_size)\r\n    while True:\r\n        r = result_queue.get()\r\n        batch_idx, inbatch_idx, sample = r\r\n        batches[batch_idx][inbatch_idx] = sample\r\n\r\n        # Check if the batch is full\r\n        if all(batches[batch_idx]):\r\n            batch = collate_fn(batches[batch_idx])\r\n            batch_queue.put((batch_idx, batch))\r\n            del batches[batch_idx]\r\n```\r\n\r\n### A few more points\r\n- I had to remove `index_queues` and revert to the point when there was only one `index_queue`  while writing my code. I would like to discuss about it with others if necessary.\r\n- In the actual code, a new sentinel object(Ellipsis) is used to indicate the end of the dataset\r\n- Exception handling in `_collater_loop` is ad-hoc and can be improved.\r\n\r\n\r\n# Definitions\r\n\r\n### Time to first batch(TTFB)\r\n  The time spent from the point an instance of _DataLoaderIter is created to the point when you can retrieve next batch by finishing calling `next(iterator)`.\r\n\r\n\r\n# Related pull request\r\n[Fixed non-determinate preprocessing on DataLoader](https://github.com/pytorch/pytorch/pull/4640)\r\n\r\n# Related issues open\r\n[PyTorch multiprocessing using single CPU core](https://github.com/pytorch/pytorch/issues/8126)\r\n[Performance drop on small models trained on CPU between 0.3.1 and 0.4](https://github.com/pytorch/pytorch/issues/8154)"}