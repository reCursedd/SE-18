{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397268487", "html_url": "https://github.com/pytorch/pytorch/pull/8371#issuecomment-397268487", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8371", "id": 397268487, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzI2ODQ4Nw==", "user": {"login": "bombs-kim", "id": 11001573, "node_id": "MDQ6VXNlcjExMDAxNTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/11001573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bombs-kim", "html_url": "https://github.com/bombs-kim", "followers_url": "https://api.github.com/users/bombs-kim/followers", "following_url": "https://api.github.com/users/bombs-kim/following{/other_user}", "gists_url": "https://api.github.com/users/bombs-kim/gists{/gist_id}", "starred_url": "https://api.github.com/users/bombs-kim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bombs-kim/subscriptions", "organizations_url": "https://api.github.com/users/bombs-kim/orgs", "repos_url": "https://api.github.com/users/bombs-kim/repos", "events_url": "https://api.github.com/users/bombs-kim/events{/privacy}", "received_events_url": "https://api.github.com/users/bombs-kim/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-14T11:56:24Z", "updated_at": "2018-08-15T06:01:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> The current experiment that I'm working on is to classify biological cells into different classes. I have about 1000 voxel images sized (150, 150, 68). Considering my GPUs' capacity, the optimal batch size is 256. One image takes roughly one second to be processed and TTFB is longer than 4 minutes. So I had to reduce the batch size to 36, and TTFB is less than a minute now. But I still don't think it's ideal.</p>\n<p>Actually I came up with <strong>a new approach</strong> that can remove unnecessary TTFB delay without largely  changing the current architecture. That is <strong>to have a queue of data loader iterators and prime the next iterator when the previous iterators loaded all of their portion whether or not you are actually in the training/eval epoch with which the next iterator is associated</strong>. I'm still testing on it, but it seems promising by effectively removing TTFB from the second epoch. I would like to  share the result here if you are interested. I guess this can be helpful for edge case experiments like mine and other prototyping phase experiments.</p>\n<h1>For people who are reading this PR later, this PR is not about \"Making dataloader workers work on each datapoint\" any more. I came up with a new approach. Sorry for making this PR complicated.</h1>", "body_text": "@apaszke The current experiment that I'm working on is to classify biological cells into different classes. I have about 1000 voxel images sized (150, 150, 68). Considering my GPUs' capacity, the optimal batch size is 256. One image takes roughly one second to be processed and TTFB is longer than 4 minutes. So I had to reduce the batch size to 36, and TTFB is less than a minute now. But I still don't think it's ideal.\nActually I came up with a new approach that can remove unnecessary TTFB delay without largely  changing the current architecture. That is to have a queue of data loader iterators and prime the next iterator when the previous iterators loaded all of their portion whether or not you are actually in the training/eval epoch with which the next iterator is associated. I'm still testing on it, but it seems promising by effectively removing TTFB from the second epoch. I would like to  share the result here if you are interested. I guess this can be helpful for edge case experiments like mine and other prototyping phase experiments.\nFor people who are reading this PR later, this PR is not about \"Making dataloader workers work on each datapoint\" any more. I came up with a new approach. Sorry for making this PR complicated.", "body": "@apaszke The current experiment that I'm working on is to classify biological cells into different classes. I have about 1000 voxel images sized (150, 150, 68). Considering my GPUs' capacity, the optimal batch size is 256. One image takes roughly one second to be processed and TTFB is longer than 4 minutes. So I had to reduce the batch size to 36, and TTFB is less than a minute now. But I still don't think it's ideal.\r\n\r\nActually I came up with **a new approach** that can remove unnecessary TTFB delay without largely  changing the current architecture. That is **to have a queue of data loader iterators and prime the next iterator when the previous iterators loaded all of their portion whether or not you are actually in the training/eval epoch with which the next iterator is associated**. I'm still testing on it, but it seems promising by effectively removing TTFB from the second epoch. I would like to  share the result here if you are interested. I guess this can be helpful for edge case experiments like mine and other prototyping phase experiments.\r\n\r\n# For people who are reading this PR later, this PR is not about \"Making dataloader workers work on each datapoint\" any more. I came up with a new approach. Sorry for making this PR complicated."}