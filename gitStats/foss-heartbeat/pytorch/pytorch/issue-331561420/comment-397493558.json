{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397493558", "html_url": "https://github.com/pytorch/pytorch/pull/8371#issuecomment-397493558", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8371", "id": 397493558, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzQ5MzU1OA==", "user": {"login": "bombs-kim", "id": 11001573, "node_id": "MDQ6VXNlcjExMDAxNTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/11001573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bombs-kim", "html_url": "https://github.com/bombs-kim", "followers_url": "https://api.github.com/users/bombs-kim/followers", "following_url": "https://api.github.com/users/bombs-kim/following{/other_user}", "gists_url": "https://api.github.com/users/bombs-kim/gists{/gist_id}", "starred_url": "https://api.github.com/users/bombs-kim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bombs-kim/subscriptions", "organizations_url": "https://api.github.com/users/bombs-kim/orgs", "repos_url": "https://api.github.com/users/bombs-kim/repos", "events_url": "https://api.github.com/users/bombs-kim/events{/privacy}", "received_events_url": "https://api.github.com/users/bombs-kim/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-15T02:17:47Z", "updated_at": "2018-06-16T08:42:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The new approach has been implemented in the last commits. I added <code>prepare_iters_early </code> option for <code>DataLoader</code>  and I think it works! It effectively solved all the problems that I mentioned.</p>\n<h3>Environment</h3>\n<p>OS: Ubuntu<br>\nCPU cores: 32</p>\n<h3>Test script</h3>\n<details><summary>click</summary>\n<p>\n</p><div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span># python3</span>\n<span class=\"pl-k\">from</span> itertools <span class=\"pl-k\">import</span> cycle\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">import</span> torchvision.datasets <span class=\"pl-k\">as</span> datasets\n<span class=\"pl-k\">import</span> torchvision.transforms <span class=\"pl-k\">as</span> transforms\n\n<span class=\"pl-k\">from</span> dataloader <span class=\"pl-k\">import</span> DataLoader, _DataLoaderIter\n\n\n<span class=\"pl-c1\">DATASET_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\n<span class=\"pl-c1\">NUM_WORKERS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\n<span class=\"pl-c1\">IMGS_PATH</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/data2/dev_out/imagenet/<span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-c1\">EXPECTED_AUGMENTATION_TIME</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>\n<span class=\"pl-c1\">NUM_EPOCHS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\nto_tensor <span class=\"pl-k\">=</span> transforms.ToTensor()\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">some_augmentation</span>(<span class=\"pl-smi\">img</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Simulate doing some heavy computation<span class=\"pl-pds\">\"\"\"</span></span>\n    t <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">while</span> time.time() <span class=\"pl-k\">-</span> t <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">EXPECTED_AUGMENTATION_TIME</span>:\n        <span class=\"pl-k\">pass</span>\n    <span class=\"pl-k\">return</span> to_tensor(img)\n\n\ndataset <span class=\"pl-k\">=</span> datasets.ImageFolder(<span class=\"pl-c1\">IMGS_PATH</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>some_augmentation)\ndataset.samples <span class=\"pl-k\">=</span> dataset.samples[:<span class=\"pl-c1\">DATASET_SIZE</span>]\n\niter_dict <span class=\"pl-k\">=</span> {}\nchars <span class=\"pl-k\">=</span> cycle(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>abcdefghijk<span class=\"pl-pds\">'</span></span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">wrap</span>(<span class=\"pl-smi\">put_indices_fn</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">wrapper</span>(<span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwds</span>):\n        <span class=\"pl-c1\">self</span> <span class=\"pl-k\">=</span> args[<span class=\"pl-c1\">0</span>]\n        success <span class=\"pl-k\">=</span> put_indices_fn(<span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwds)\n        <span class=\"pl-k\">if</span> success:\n            id_ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">id</span>(<span class=\"pl-c1\">self</span>)\n            <span class=\"pl-k\">if</span> id_ <span class=\"pl-k\">not</span> <span class=\"pl-k\">in</span> iter_dict:\n                iter_dict[id_] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">next</span>(chars)\n            <span class=\"pl-c1\">print</span>(iter_dict[id_], <span class=\"pl-v\">end</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">return</span> success\n    <span class=\"pl-k\">return</span> wrapper\n\n_DataLoaderIter.put_indices <span class=\"pl-k\">=</span> wrap(_DataLoaderIter.put_indices)\n\ntrain_loader <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">NUM_WORKERS</span>, <span class=\"pl-v\">prepare_iters_early</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n\nt0 <span class=\"pl-k\">=</span> time.time()\ntimes_to_first_batches <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">NUM_EPOCHS</span>):\n    t1 <span class=\"pl-k\">=</span> time.time()\n    <span class=\"pl-k\">for</span> j, (<span class=\"pl-c1\">input</span>, target) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span>training epoch <span class=\"pl-c1\">{}</span> batch <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(i, j))\n        <span class=\"pl-k\">if</span> j <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            times_to_first_batches.append(time.time() <span class=\"pl-k\">-</span> t1)\n        time.sleep(<span class=\"pl-c1\">1</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Simulate training/testing</span>\n\ntotal_time <span class=\"pl-k\">=</span> time.time() <span class=\"pl-k\">-</span> t0\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\n</span>times to first batches: <span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>, <span class=\"pl-pds\">\"</span></span>.join(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">%.4f</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> t <span class=\"pl-k\">for</span> t <span class=\"pl-k\">in</span> times_to_first_batches))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>total time: <span class=\"pl-pds\">\"</span></span>, total_time)</pre></div>\n<p></p>\n</details>\n<h3>Measurements</h3>\n<h4>prepare_iters_early = False</h4>\n<pre><code>aaaa\ntraining epoch 0 batch 0\n\ntraining epoch 0 batch 1\n\ntraining epoch 0 batch 2\n\ntraining epoch 0 batch 3\naaaa\ntraining epoch 1 batch 0\n\n...\n\naaaa\ntraining epoch 9 batch 0\n\ntraining epoch 9 batch 1\n\ntraining epoch 9 batch 2\n\ntraining epoch 9 batch 3\n\ntimes to first batches:  27.0132, 27.0762, 27.0608, 27.0480, 27.0565, 27.0517, 27.0842, 27.0356, 27.0749, 27.0580\ntotal time:  311.82125639915466\n\n</code></pre>\n<h4>prepare_iters_early = True</h4>\n<pre><code>\naaaabbbbccccddddeeeeffffgggghhhhi\ntraining epoch 0 batch 0\ni\ntraining epoch 0 batch 1\ni\ntraining epoch 0 batch 2\ni\n\n...\n\nf\ntraining epoch 9 batch 0\nf\ntraining epoch 9 batch 1\nf\ntraining epoch 9 batch 2\nf\ntraining epoch 9 batch 3\n\ntimes to first batches:  27.4209, 0.1829, 0.1792, 0.1858, 0.2096, 0.2046, 0.1625, 0.1802, 0.1998, 0.2046\ntotal time:  69.70392274856567\n</code></pre>", "body_text": "The new approach has been implemented in the last commits. I added prepare_iters_early  option for DataLoader  and I think it works! It effectively solved all the problems that I mentioned.\nEnvironment\nOS: Ubuntu\nCPU cores: 32\nTest script\nclick\n\n## python3\nfrom itertools import cycle\nimport sys\nimport time\n\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nfrom dataloader import DataLoader, _DataLoaderIter\n\n\nDATASET_SIZE = 1000\nBATCH_SIZE = 256\nNUM_WORKERS = 32\nIMGS_PATH = \"/data2/dev_out/imagenet/\"\nEXPECTED_AUGMENTATION_TIME = 0.1\nNUM_EPOCHS = 10\n\nto_tensor = transforms.ToTensor()\ndef some_augmentation(img):\n    \"\"\"Simulate doing some heavy computation\"\"\"\n    t = time.time()\n    while time.time() - t < EXPECTED_AUGMENTATION_TIME:\n        pass\n    return to_tensor(img)\n\n\ndataset = datasets.ImageFolder(IMGS_PATH, transform=some_augmentation)\ndataset.samples = dataset.samples[:DATASET_SIZE]\n\niter_dict = {}\nchars = cycle('abcdefghijk')\n\n\ndef wrap(put_indices_fn):\n    def wrapper(*args, **kwds):\n        self = args[0]\n        success = put_indices_fn(*args, **kwds)\n        if success:\n            id_ = id(self)\n            if id_ not in iter_dict:\n                iter_dict[id_] = next(chars)\n            print(iter_dict[id_], end='')\n        return success\n    return wrapper\n\n_DataLoaderIter.put_indices = wrap(_DataLoaderIter.put_indices)\n\ntrain_loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, prepare_iters_early=True)\n\n\nt0 = time.time()\ntimes_to_first_batches = []\nfor i in range(NUM_EPOCHS):\n    t1 = time.time()\n    for j, (input, target) in enumerate(train_loader):\n        print('\\ntraining epoch {} batch {}'.format(i, j))\n        if j == 0:\n            times_to_first_batches.append(time.time() - t1)\n        time.sleep(1)  # Simulate training/testing\n\ntotal_time = time.time() - t0\nprint(\"\\ntimes to first batches: \", \", \".join(\"%.4f\" % t for t in times_to_first_batches))\nprint(\"total time: \", total_time)\n\n\nMeasurements\nprepare_iters_early = False\naaaa\ntraining epoch 0 batch 0\n\ntraining epoch 0 batch 1\n\ntraining epoch 0 batch 2\n\ntraining epoch 0 batch 3\naaaa\ntraining epoch 1 batch 0\n\n...\n\naaaa\ntraining epoch 9 batch 0\n\ntraining epoch 9 batch 1\n\ntraining epoch 9 batch 2\n\ntraining epoch 9 batch 3\n\ntimes to first batches:  27.0132, 27.0762, 27.0608, 27.0480, 27.0565, 27.0517, 27.0842, 27.0356, 27.0749, 27.0580\ntotal time:  311.82125639915466\n\n\nprepare_iters_early = True\n\naaaabbbbccccddddeeeeffffgggghhhhi\ntraining epoch 0 batch 0\ni\ntraining epoch 0 batch 1\ni\ntraining epoch 0 batch 2\ni\n\n...\n\nf\ntraining epoch 9 batch 0\nf\ntraining epoch 9 batch 1\nf\ntraining epoch 9 batch 2\nf\ntraining epoch 9 batch 3\n\ntimes to first batches:  27.4209, 0.1829, 0.1792, 0.1858, 0.2096, 0.2046, 0.1625, 0.1802, 0.1998, 0.2046\ntotal time:  69.70392274856567", "body": "The new approach has been implemented in the last commits. I added `prepare_iters_early ` option for `DataLoader`  and I think it works! It effectively solved all the problems that I mentioned.\r\n\r\n### Environment\r\nOS: Ubuntu\r\nCPU cores: 32\r\n\r\n### Test script\r\n<details><summary>click</summary>\r\n<p>\r\n\r\n```python\r\n## python3\r\nfrom itertools import cycle\r\nimport sys\r\nimport time\r\n\r\nimport torchvision.datasets as datasets\r\nimport torchvision.transforms as transforms\r\n\r\nfrom dataloader import DataLoader, _DataLoaderIter\r\n\r\n\r\nDATASET_SIZE = 1000\r\nBATCH_SIZE = 256\r\nNUM_WORKERS = 32\r\nIMGS_PATH = \"/data2/dev_out/imagenet/\"\r\nEXPECTED_AUGMENTATION_TIME = 0.1\r\nNUM_EPOCHS = 10\r\n\r\nto_tensor = transforms.ToTensor()\r\ndef some_augmentation(img):\r\n    \"\"\"Simulate doing some heavy computation\"\"\"\r\n    t = time.time()\r\n    while time.time() - t < EXPECTED_AUGMENTATION_TIME:\r\n        pass\r\n    return to_tensor(img)\r\n\r\n\r\ndataset = datasets.ImageFolder(IMGS_PATH, transform=some_augmentation)\r\ndataset.samples = dataset.samples[:DATASET_SIZE]\r\n\r\niter_dict = {}\r\nchars = cycle('abcdefghijk')\r\n\r\n\r\ndef wrap(put_indices_fn):\r\n    def wrapper(*args, **kwds):\r\n        self = args[0]\r\n        success = put_indices_fn(*args, **kwds)\r\n        if success:\r\n            id_ = id(self)\r\n            if id_ not in iter_dict:\r\n                iter_dict[id_] = next(chars)\r\n            print(iter_dict[id_], end='')\r\n        return success\r\n    return wrapper\r\n\r\n_DataLoaderIter.put_indices = wrap(_DataLoaderIter.put_indices)\r\n\r\ntrain_loader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, prepare_iters_early=True)\r\n\r\n\r\nt0 = time.time()\r\ntimes_to_first_batches = []\r\nfor i in range(NUM_EPOCHS):\r\n    t1 = time.time()\r\n    for j, (input, target) in enumerate(train_loader):\r\n        print('\\ntraining epoch {} batch {}'.format(i, j))\r\n        if j == 0:\r\n            times_to_first_batches.append(time.time() - t1)\r\n        time.sleep(1)  # Simulate training/testing\r\n\r\ntotal_time = time.time() - t0\r\nprint(\"\\ntimes to first batches: \", \", \".join(\"%.4f\" % t for t in times_to_first_batches))\r\nprint(\"total time: \", total_time)\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n### Measurements\r\n#### prepare_iters_early = False\r\n```\r\naaaa\r\ntraining epoch 0 batch 0\r\n\r\ntraining epoch 0 batch 1\r\n\r\ntraining epoch 0 batch 2\r\n\r\ntraining epoch 0 batch 3\r\naaaa\r\ntraining epoch 1 batch 0\r\n\r\n...\r\n\r\naaaa\r\ntraining epoch 9 batch 0\r\n\r\ntraining epoch 9 batch 1\r\n\r\ntraining epoch 9 batch 2\r\n\r\ntraining epoch 9 batch 3\r\n\r\ntimes to first batches:  27.0132, 27.0762, 27.0608, 27.0480, 27.0565, 27.0517, 27.0842, 27.0356, 27.0749, 27.0580\r\ntotal time:  311.82125639915466\r\n\r\n```\r\n\r\n\r\n#### prepare_iters_early = True\r\n```\r\n\r\naaaabbbbccccddddeeeeffffgggghhhhi\r\ntraining epoch 0 batch 0\r\ni\r\ntraining epoch 0 batch 1\r\ni\r\ntraining epoch 0 batch 2\r\ni\r\n\r\n...\r\n\r\nf\r\ntraining epoch 9 batch 0\r\nf\r\ntraining epoch 9 batch 1\r\nf\r\ntraining epoch 9 batch 2\r\nf\r\ntraining epoch 9 batch 3\r\n\r\ntimes to first batches:  27.4209, 0.1829, 0.1792, 0.1858, 0.2096, 0.2046, 0.1625, 0.1802, 0.1998, 0.2046\r\ntotal time:  69.70392274856567\r\n```\r\n"}