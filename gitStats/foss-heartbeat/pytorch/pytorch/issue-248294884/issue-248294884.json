{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2309", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2309/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2309/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2309/events", "html_url": "https://github.com/pytorch/pytorch/issues/2309", "id": 248294884, "node_id": "MDU6SXNzdWUyNDgyOTQ4ODQ=", "number": 2309, "title": "A bug in batchnorm double backward", "user": {"login": "yunjey", "id": 15663219, "node_id": "MDQ6VXNlcjE1NjYzMjE5", "avatar_url": "https://avatars2.githubusercontent.com/u/15663219?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yunjey", "html_url": "https://github.com/yunjey", "followers_url": "https://api.github.com/users/yunjey/followers", "following_url": "https://api.github.com/users/yunjey/following{/other_user}", "gists_url": "https://api.github.com/users/yunjey/gists{/gist_id}", "starred_url": "https://api.github.com/users/yunjey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yunjey/subscriptions", "organizations_url": "https://api.github.com/users/yunjey/orgs", "repos_url": "https://api.github.com/users/yunjey/repos", "events_url": "https://api.github.com/users/yunjey/events{/privacy}", "received_events_url": "https://api.github.com/users/yunjey/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-07T02:53:20Z", "updated_at": "2017-08-07T03:24:47Z", "closed_at": "2017-08-07T03:24:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am working on adding batchnorm in the discriminator in WGAN-GP. However, I encountered a bug where <strong>gpu memory continues to increase</strong> when using <strong>batchnorm double backprop</strong>. This bug only occurs when using batchnorm. If i remove batchnorm from the model, the bug doesn't occur.</p>\n<p>Here's the code you can experiment with.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Model (partial discriminator)</span>\nD <span class=\"pl-k\">=</span> nn.Sequential(\n    nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n    nn.BatchNorm2d(<span class=\"pl-c1\">64</span>),   <span class=\"pl-c\"><span class=\"pl-c\">#</span> if you remove this, the bug does not occur.</span>\n    nn.LeakyReLU(<span class=\"pl-c1\">0.2</span>))\n\nD.cuda()\n\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Input </span>\n    x <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    out <span class=\"pl-k\">=</span> D(x)\n    \n    grad <span class=\"pl-k\">=</span> torch.autograd.grad(<span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>out,\n                               <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>x,\n                               <span class=\"pl-v\">grad_outputs</span><span class=\"pl-k\">=</span>torch.ones(out.size()).cuda(),\n                               <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                               <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                               <span class=\"pl-v\">only_inputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]\n    \n    grad_norm <span class=\"pl-k\">=</span> grad.pow(<span class=\"pl-c1\">2</span>).sum().sqrt()\n    loss <span class=\"pl-k\">=</span> torch.mean((grad_norm <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>)<span class=\"pl-k\">**</span><span class=\"pl-c1\">2</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Reset grad and backprop</span>\n    D.zero_grad()\n    loss.backward()\n    \n    <span class=\"pl-k\">if</span> (i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>) <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-c1\">print</span> (i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)</pre></div>", "body_text": "I am working on adding batchnorm in the discriminator in WGAN-GP. However, I encountered a bug where gpu memory continues to increase when using batchnorm double backprop. This bug only occurs when using batchnorm. If i remove batchnorm from the model, the bug doesn't occur.\nHere's the code you can experiment with.\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n# Model (partial discriminator)\nD = nn.Sequential(\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n    nn.BatchNorm2d(64),   # if you remove this, the bug does not occur.\n    nn.LeakyReLU(0.2))\n\nD.cuda()\n\n\nfor i in range(1000):\n    # Input \n    x = Variable(torch.randn(10, 3, 128, 128).cuda(), requires_grad=True)\n    out = D(x)\n    \n    grad = torch.autograd.grad(outputs=out,\n                               inputs=x,\n                               grad_outputs=torch.ones(out.size()).cuda(),\n                               retain_graph=True,\n                               create_graph=True,\n                               only_inputs=True)[0]\n    \n    grad_norm = grad.pow(2).sum().sqrt()\n    loss = torch.mean((grad_norm - 1)**2)\n\n    # Reset grad and backprop\n    D.zero_grad()\n    loss.backward()\n    \n    if (i+1) % 10 == 0:\n        print (i+1)", "body": "I am working on adding batchnorm in the discriminator in WGAN-GP. However, I encountered a bug where **gpu memory continues to increase** when using **batchnorm double backprop**. This bug only occurs when using batchnorm. If i remove batchnorm from the model, the bug doesn't occur.\r\n\r\nHere's the code you can experiment with.\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\n# Model (partial discriminator)\r\nD = nn.Sequential(\r\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\r\n    nn.BatchNorm2d(64),   # if you remove this, the bug does not occur.\r\n    nn.LeakyReLU(0.2))\r\n\r\nD.cuda()\r\n\r\n\r\nfor i in range(1000):\r\n    # Input \r\n    x = Variable(torch.randn(10, 3, 128, 128).cuda(), requires_grad=True)\r\n    out = D(x)\r\n    \r\n    grad = torch.autograd.grad(outputs=out,\r\n                               inputs=x,\r\n                               grad_outputs=torch.ones(out.size()).cuda(),\r\n                               retain_graph=True,\r\n                               create_graph=True,\r\n                               only_inputs=True)[0]\r\n    \r\n    grad_norm = grad.pow(2).sum().sqrt()\r\n    loss = torch.mean((grad_norm - 1)**2)\r\n\r\n    # Reset grad and backprop\r\n    D.zero_grad()\r\n    loss.backward()\r\n    \r\n    if (i+1) % 10 == 0:\r\n        print (i+1)\r\n```\r\n"}