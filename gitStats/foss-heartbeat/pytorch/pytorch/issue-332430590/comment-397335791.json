{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397335791", "html_url": "https://github.com/pytorch/pytorch/issues/8484#issuecomment-397335791", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8484", "id": 397335791, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzMzNTc5MQ==", "user": {"login": "vishwakftw", "id": 23639302, "node_id": "MDQ6VXNlcjIzNjM5MzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/23639302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishwakftw", "html_url": "https://github.com/vishwakftw", "followers_url": "https://api.github.com/users/vishwakftw/followers", "following_url": "https://api.github.com/users/vishwakftw/following{/other_user}", "gists_url": "https://api.github.com/users/vishwakftw/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishwakftw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishwakftw/subscriptions", "organizations_url": "https://api.github.com/users/vishwakftw/orgs", "repos_url": "https://api.github.com/users/vishwakftw/repos", "events_url": "https://api.github.com/users/vishwakftw/events{/privacy}", "received_events_url": "https://api.github.com/users/vishwakftw/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-14T15:25:28Z", "updated_at": "2018-06-14T15:39:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p>If this is the case, shouldn't <code>rrelu</code> behave the same way as a <code>relu</code>, because the negative slope would be sampled from U(0, 0) = {0}?</p>\n<p>It doesn't seem to do something like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.nn.functional.rrelu(a)\ntensor([<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.2292</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> a = 0.2292</span></pre></div>\n<p>Edit: they don't seem to work when directly used on tensors</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.rrelu(a)\ntensor([<span class=\"pl-k\">-</span><span class=\"pl-c1\">0</span>.])</pre></div>\n<p>This reason it works in <code>torch.nn.functional</code> is because the default values are given again to the <code>torch.rrelu</code> function.</p>", "body_text": "If this is the case, shouldn't rrelu behave the same way as a relu, because the negative slope would be sampled from U(0, 0) = {0}?\nIt doesn't seem to do something like this:\n>>> a = torch.ones(1) - 2\n>>> torch.nn.functional.rrelu(a)\ntensor([-0.2292])  # a = 0.2292\nEdit: they don't seem to work when directly used on tensors\n>>> a = torch.ones(1) - 2\n>>> torch.rrelu(a)\ntensor([-0.])\nThis reason it works in torch.nn.functional is because the default values are given again to the torch.rrelu function.", "body": "If this is the case, shouldn't `rrelu` behave the same way as a `relu`, because the negative slope would be sampled from U(0, 0) = {0}?\r\n\r\nIt doesn't seem to do something like this:\r\n```python\r\n>>> a = torch.ones(1) - 2\r\n>>> torch.nn.functional.rrelu(a)\r\ntensor([-0.2292])  # a = 0.2292\r\n```\r\n\r\nEdit: they don't seem to work when directly used on tensors\r\n```python\r\n>>> a = torch.ones(1) - 2\r\n>>> torch.rrelu(a)\r\ntensor([-0.])\r\n```\r\nThis reason it works in `torch.nn.functional` is because the default values are given again to the `torch.rrelu` function."}