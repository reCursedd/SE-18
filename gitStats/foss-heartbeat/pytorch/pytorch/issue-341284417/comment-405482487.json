{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/405482487", "html_url": "https://github.com/pytorch/pytorch/issues/9453#issuecomment-405482487", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9453", "id": 405482487, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTQ4MjQ4Nw==", "user": {"login": "jacksonly", "id": 10672549, "node_id": "MDQ6VXNlcjEwNjcyNTQ5", "avatar_url": "https://avatars0.githubusercontent.com/u/10672549?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacksonly", "html_url": "https://github.com/jacksonly", "followers_url": "https://api.github.com/users/jacksonly/followers", "following_url": "https://api.github.com/users/jacksonly/following{/other_user}", "gists_url": "https://api.github.com/users/jacksonly/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacksonly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacksonly/subscriptions", "organizations_url": "https://api.github.com/users/jacksonly/orgs", "repos_url": "https://api.github.com/users/jacksonly/repos", "events_url": "https://api.github.com/users/jacksonly/events{/privacy}", "received_events_url": "https://api.github.com/users/jacksonly/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-17T07:09:46Z", "updated_at": "2018-07-19T04:48:05Z", "author_association": "NONE", "body_html": "<p>Okay, this is my complete script.</p>\n<pre><code>#!/usr/bin/env python\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport time\nimport random\nfrom math import ceil\nfrom random import Random\nfrom torch.multiprocessing import Process\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\n\nclass Partition(object):\n    \"\"\" Dataset-like object, but only access a subset of it. \"\"\"\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n    \"\"\" Partitions a dataset into different chuncks. \"\"\"\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])\n\n\nclass Net(nn.Module):\n    \"\"\" Network architecture. \"\"\"\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\ndef partition_dataset():\n    \"\"\" Partitioning MNIST \"\"\"\n    dataset = datasets.MNIST(\n        r'./data',\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    size = dist.get_world_size()\n    bsz = 128 / float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(\n        partition, batch_size=bsz, shuffle=True)\n    return train_set, bsz\n\n\ndef average_gradients(model):\n    \"\"\" Gradient averaging. \"\"\"\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)\n        param.grad.data /= size\n\n\ndef run(rank, size):\n    \"\"\" Distributed Synchronous SGD Example \"\"\"\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n#    model = model\n    model = model.cuda(rank)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n#            data, target = Variable(data), Variable(target)\n            data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank))\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.data[0]\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        print('Rank ',\n              dist.get_rank(), ', epoch ', epoch, ': ',\n              epoch_loss / num_batches)\n\ndef mpi_get_rank():\n    if 'OMPI_COMM_WORLD_RANK' in os.environ:\n        return int(os.environ['OMPI_COMM_WORLD_RANK'])\n    else:\n        return None\n\ndef mpi_get_world_size():\n    if 'OMPI_COMM_WORLD_SIZE' in os.environ:\n        return int(os.environ['OMPI_COMM_WORLD_SIZE'])\n    else:\n        return None\n\ndef init_processes(rank, size, fn, backend='gloo'):\n    if mpi_get_rank() != None:\n        time.sleep(mpi_get_rank())\n    else:\n        time.sleep(random.randint(0, 4 * size))\n    \"\"\" Initialize the distributed environment. \"\"\"\n    print('*'*10)\n    print('Before Initialize:')\n    print('*'*10)\n    dist.init_process_group(backend=backend, init_method=\"file:///xxx/shared/{}\".format('shared_file_common'), world_size=size)\n    print('*'*10)\n    print('Finished Initialization!!!')\n    print('*'*10)\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    processes = []\n    for rank in range(size):\n        p = Process(target=init_processes, args=(rank, size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()\n</code></pre>", "body_text": "Okay, this is my complete script.\n#!/usr/bin/env python\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport time\nimport random\nfrom math import ceil\nfrom random import Random\nfrom torch.multiprocessing import Process\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\n\nclass Partition(object):\n    \"\"\" Dataset-like object, but only access a subset of it. \"\"\"\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n    \"\"\" Partitions a dataset into different chuncks. \"\"\"\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])\n\n\nclass Net(nn.Module):\n    \"\"\" Network architecture. \"\"\"\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\n\ndef partition_dataset():\n    \"\"\" Partitioning MNIST \"\"\"\n    dataset = datasets.MNIST(\n        r'./data',\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    size = dist.get_world_size()\n    bsz = 128 / float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(\n        partition, batch_size=bsz, shuffle=True)\n    return train_set, bsz\n\n\ndef average_gradients(model):\n    \"\"\" Gradient averaging. \"\"\"\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)\n        param.grad.data /= size\n\n\ndef run(rank, size):\n    \"\"\" Distributed Synchronous SGD Example \"\"\"\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n#    model = model\n    model = model.cuda(rank)\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n#            data, target = Variable(data), Variable(target)\n            data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank))\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.data[0]\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        print('Rank ',\n              dist.get_rank(), ', epoch ', epoch, ': ',\n              epoch_loss / num_batches)\n\ndef mpi_get_rank():\n    if 'OMPI_COMM_WORLD_RANK' in os.environ:\n        return int(os.environ['OMPI_COMM_WORLD_RANK'])\n    else:\n        return None\n\ndef mpi_get_world_size():\n    if 'OMPI_COMM_WORLD_SIZE' in os.environ:\n        return int(os.environ['OMPI_COMM_WORLD_SIZE'])\n    else:\n        return None\n\ndef init_processes(rank, size, fn, backend='gloo'):\n    if mpi_get_rank() != None:\n        time.sleep(mpi_get_rank())\n    else:\n        time.sleep(random.randint(0, 4 * size))\n    \"\"\" Initialize the distributed environment. \"\"\"\n    print('*'*10)\n    print('Before Initialize:')\n    print('*'*10)\n    dist.init_process_group(backend=backend, init_method=\"file:///xxx/shared/{}\".format('shared_file_common'), world_size=size)\n    print('*'*10)\n    print('Finished Initialization!!!')\n    print('*'*10)\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    processes = []\n    for rank in range(size):\n        p = Process(target=init_processes, args=(rank, size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()", "body": "Okay, this is my complete script. \r\n```\r\n#!/usr/bin/env python\r\n\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nimport time\r\nimport random\r\nfrom math import ceil\r\nfrom random import Random\r\nfrom torch.multiprocessing import Process\r\nfrom torch.autograd import Variable\r\nfrom torchvision import datasets, transforms\r\n\r\n\r\nclass Partition(object):\r\n    \"\"\" Dataset-like object, but only access a subset of it. \"\"\"\r\n\r\n    def __init__(self, data, index):\r\n        self.data = data\r\n        self.index = index\r\n\r\n    def __len__(self):\r\n        return len(self.index)\r\n\r\n    def __getitem__(self, index):\r\n        data_idx = self.index[index]\r\n        return self.data[data_idx]\r\n\r\n\r\nclass DataPartitioner(object):\r\n    \"\"\" Partitions a dataset into different chuncks. \"\"\"\r\n\r\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\r\n        self.data = data\r\n        self.partitions = []\r\n        rng = Random()\r\n        rng.seed(seed)\r\n        data_len = len(data)\r\n        indexes = [x for x in range(0, data_len)]\r\n        rng.shuffle(indexes)\r\n\r\n        for frac in sizes:\r\n            part_len = int(frac * data_len)\r\n            self.partitions.append(indexes[0:part_len])\r\n            indexes = indexes[part_len:]\r\n\r\n    def use(self, partition):\r\n        return Partition(self.data, self.partitions[partition])\r\n\r\n\r\nclass Net(nn.Module):\r\n    \"\"\" Network architecture. \"\"\"\r\n\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.conv2_drop = nn.Dropout2d()\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\r\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n        x = x.view(-1, 320)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x)\r\n\r\n\r\ndef partition_dataset():\r\n    \"\"\" Partitioning MNIST \"\"\"\r\n    dataset = datasets.MNIST(\r\n        r'./data',\r\n        transform=transforms.Compose([\r\n            transforms.ToTensor(),\r\n            transforms.Normalize((0.1307, ), (0.3081, ))\r\n        ]))\r\n    size = dist.get_world_size()\r\n    bsz = 128 / float(size)\r\n    partition_sizes = [1.0 / size for _ in range(size)]\r\n    partition = DataPartitioner(dataset, partition_sizes)\r\n    partition = partition.use(dist.get_rank())\r\n    train_set = torch.utils.data.DataLoader(\r\n        partition, batch_size=bsz, shuffle=True)\r\n    return train_set, bsz\r\n\r\n\r\ndef average_gradients(model):\r\n    \"\"\" Gradient averaging. \"\"\"\r\n    size = float(dist.get_world_size())\r\n    for param in model.parameters():\r\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)\r\n        param.grad.data /= size\r\n\r\n\r\ndef run(rank, size):\r\n    \"\"\" Distributed Synchronous SGD Example \"\"\"\r\n    torch.manual_seed(1234)\r\n    train_set, bsz = partition_dataset()\r\n    model = Net()\r\n#    model = model\r\n    model = model.cuda(rank)\r\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\r\n\r\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\r\n    for epoch in range(10):\r\n        epoch_loss = 0.0\r\n        for data, target in train_set:\r\n#            data, target = Variable(data), Variable(target)\r\n            data, target = Variable(data.cuda(rank)), Variable(target.cuda(rank))\r\n            optimizer.zero_grad()\r\n            output = model(data)\r\n            loss = F.nll_loss(output, target)\r\n            epoch_loss += loss.data[0]\r\n            loss.backward()\r\n            average_gradients(model)\r\n            optimizer.step()\r\n        print('Rank ',\r\n              dist.get_rank(), ', epoch ', epoch, ': ',\r\n              epoch_loss / num_batches)\r\n\r\ndef mpi_get_rank():\r\n    if 'OMPI_COMM_WORLD_RANK' in os.environ:\r\n        return int(os.environ['OMPI_COMM_WORLD_RANK'])\r\n    else:\r\n        return None\r\n\r\ndef mpi_get_world_size():\r\n    if 'OMPI_COMM_WORLD_SIZE' in os.environ:\r\n        return int(os.environ['OMPI_COMM_WORLD_SIZE'])\r\n    else:\r\n        return None\r\n\r\ndef init_processes(rank, size, fn, backend='gloo'):\r\n    if mpi_get_rank() != None:\r\n        time.sleep(mpi_get_rank())\r\n    else:\r\n        time.sleep(random.randint(0, 4 * size))\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    print('*'*10)\r\n    print('Before Initialize:')\r\n    print('*'*10)\r\n    dist.init_process_group(backend=backend, init_method=\"file:///xxx/shared/{}\".format('shared_file_common'), world_size=size)\r\n    print('*'*10)\r\n    print('Finished Initialization!!!')\r\n    print('*'*10)\r\n    fn(rank, size)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    size = 2\r\n    processes = []\r\n    for rank in range(size):\r\n        p = Process(target=init_processes, args=(rank, size, run))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n```"}