{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1374", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1374/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1374/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1374/events", "html_url": "https://github.com/pytorch/pytorch/issues/1374", "id": 224666894, "node_id": "MDU6SXNzdWUyMjQ2NjY4OTQ=", "number": 1374, "title": "Segmentation fault (Python 3.6.0, Anaconda 4.3.0, Ubuntu 16.04.01)", "user": {"login": "aizvorski", "id": 5302763, "node_id": "MDQ6VXNlcjUzMDI3NjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5302763?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aizvorski", "html_url": "https://github.com/aizvorski", "followers_url": "https://api.github.com/users/aizvorski/followers", "following_url": "https://api.github.com/users/aizvorski/following{/other_user}", "gists_url": "https://api.github.com/users/aizvorski/gists{/gist_id}", "starred_url": "https://api.github.com/users/aizvorski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aizvorski/subscriptions", "organizations_url": "https://api.github.com/users/aizvorski/orgs", "repos_url": "https://api.github.com/users/aizvorski/repos", "events_url": "https://api.github.com/users/aizvorski/events{/privacy}", "received_events_url": "https://api.github.com/users/aizvorski/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": {"url": "https://api.github.com/repos/pytorch/pytorch/milestones/2", "html_url": "https://github.com/pytorch/pytorch/milestone/2", "labels_url": "https://api.github.com/repos/pytorch/pytorch/milestones/2/labels", "id": 2536200, "node_id": "MDk6TWlsZXN0b25lMjUzNjIwMA==", "number": 2, "title": "v0.2", "description": "", "creator": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "open_issues": 2, "closed_issues": 34, "state": "closed", "created_at": "2017-05-22T18:19:28Z", "updated_at": "2018-08-06T21:16:06Z", "due_on": "2017-06-04T07:00:00Z", "closed_at": "2018-08-06T21:16:06Z"}, "comments": 1, "created_at": "2017-04-27T04:39:51Z", "updated_at": "2017-05-25T18:50:09Z", "closed_at": "2017-05-25T18:50:09Z", "author_association": "NONE", "body_html": "<p>Steps to reproduce:</p>\n<ul>\n<li>Install Ubuntu 16.04.01 LTS</li>\n<li>Install Anaconda 4.3.0 (<a href=\"https://repo.continuum.io/archive/Anaconda3-4.3.0-Linux-x86_64.sh\" rel=\"nofollow\">https://repo.continuum.io/archive/Anaconda3-4.3.0-Linux-x86_64.sh</a>)</li>\n<li>Install PyTorch (cpu version) with <code>conda install pytorch torchvision -c soumith</code></li>\n<li>Get ResNeXt code with <code>git clone https://github.com/prlz77/ResNeXt.pytorch/</code></li>\n<li>Apply this patch to enable running on cpu:</li>\n</ul>\n<pre><code>diff --git a/train.py b/train.py\nindex dc5a31d..ceec980 100644\n--- a/train.py\n+++ b/train.py\n@@ -84,9 +84,9 @@ if __name__ == '__main__':\n         test_data = dset.CIFAR100(args.data_path, train=False, transform=test_transform, download=True)\n         nlabels = 100\n     train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True,\n-                                               num_workers=args.prefetch, pin_memory=True)\n+                                               num_workers=args.prefetch, pin_memory=False)\n     test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False,\n-                                              num_workers=args.prefetch, pin_memory=True)\n+                                              num_workers=args.prefetch, pin_memory=False)\n \n     # Init checkpoints\n     if not os.path.isdir(args.save):\n@@ -109,7 +109,7 @@ if __name__ == '__main__':\n         net.train()\n         loss_avg = 0.0\n         for batch_idx, (data, target) in enumerate(train_loader):\n-            data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n+            data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n \n             # forward\n             output = net(data)\n</code></pre>\n<ul>\n<li>Run with <code>python train.py --ngpu 0 --batch_size 8 data cifar10</code></li>\n</ul>\n<p>Result: <code>Segmentation fault (core dumped)</code></p>\n<p>Debugging info:</p>\n<pre><code>ResNeXt.pytorch$ gdb python\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\nCopyright (C) 2016 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n&lt;http://www.gnu.org/software/gdb/bugs/&gt;.\nFind the GDB manual and other documentation resources online at:\n&lt;http://www.gnu.org/software/gdb/documentation/&gt;.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from python...done.\n(gdb) r train.py --ngpu 0 --batch_size 8 data cifar10\nStarting program: /home/alex/anaconda3/bin/python train.py --ngpu 0 --batch_size 8 data cifar10\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nFiles already downloaded and verified\nFiles already downloaded and verified\nCifarResNeXt (\n  (conv_1_3x3): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n  (stage_1): Sequential (\n    (stage_1_bottleneck_0): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n        (shortcut_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (shortcut_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n      )\n    )\n    (stage_1_bottleneck_1): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n    (stage_1_bottleneck_2): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n  )\n  (stage_2): Sequential (\n    (stage_2_bottleneck_0): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n        (shortcut_conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (shortcut_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      )\n    )\n    (stage_2_bottleneck_1): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n    (stage_2_bottleneck_2): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n  )\n  (stage_3): Sequential (\n    (stage_3_bottleneck_0): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n        (shortcut_conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (shortcut_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      )\n    )\n    (stage_3_bottleneck_1): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n    (stage_3_bottleneck_2): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n  )\n  (classifier): Linear (1024 -&gt; 10)\n)\n[New Thread 0x7fffbefa5780 (LWP 18919)]\n[New Thread 0x7fffbeba4800 (LWP 18920)]\n[New Thread 0x7fffbe7a3880 (LWP 18921)]\n[New Thread 0x7fffab9b1700 (LWP 18923)]\n[New Thread 0x7fffab1af980 (LWP 18924)]\n[New Thread 0x7fffaadaea00 (LWP 18925)]\n[New Thread 0x7fffaa9ada80 (LWP 18926)]\n\nThread 5 \"python\" received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7fffab9b1700 (LWP 18923)]\n0x00007fffedba4d04 in torch::autograd::cat (tensors=..., dim=dim@entry=0) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n(gdb) where\n#0  0x00007fffedba4d04 in torch::autograd::cat (tensors=..., dim=dim@entry=0) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#1  0x00007fffedba6f1c in torch::autograd::ConvBackward::apply (this=0x2db1a278, grad_outputs=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#2  0x00007fffedb8d138 in torch::autograd::call_function (task=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#3  torch::autograd::Engine::evaluate_function (this=this@entry=0x7fffee408d00 &lt;engine&gt;, task=...) at torch/csrc/autograd/engine.cpp:136\n#4  0x00007fffedb8ed3a in torch::autograd::Engine::thread_main (this=this@entry=0x7fffee408d00 &lt;engine&gt;, queue=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#5  0x00007fffedb9f89a in PythonEngine::thread_main (this=0x7fffee408d00 &lt;engine&gt;, queue=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#6  0x00007fffd20a5870 in ?? () from /home/alex/anaconda3/lib/python3.6/site-packages/torch/lib/../../../../libstdc++.so.6\n#7  0x00007ffff76bc6ba in start_thread (arg=0x7fffab9b1700) at pthread_create.c:333\n#8  0x00007ffff6ada82d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n</code></pre>\n<p>Versions:</p>\n<pre><code>$ python --version\nPython 3.6.0 :: Anaconda 4.3.0 (64-bit)\n$ python -c \"import torch; print(torch.__version__)\"\n0.1.11+b13b701\n</code></pre>\n<p>Note: other networks seem to work okay, for example some of the pytorch examples.  It is just <em>this</em> network that crashes.</p>", "body_text": "Steps to reproduce:\n\nInstall Ubuntu 16.04.01 LTS\nInstall Anaconda 4.3.0 (https://repo.continuum.io/archive/Anaconda3-4.3.0-Linux-x86_64.sh)\nInstall PyTorch (cpu version) with conda install pytorch torchvision -c soumith\nGet ResNeXt code with git clone https://github.com/prlz77/ResNeXt.pytorch/\nApply this patch to enable running on cpu:\n\ndiff --git a/train.py b/train.py\nindex dc5a31d..ceec980 100644\n--- a/train.py\n+++ b/train.py\n@@ -84,9 +84,9 @@ if __name__ == '__main__':\n         test_data = dset.CIFAR100(args.data_path, train=False, transform=test_transform, download=True)\n         nlabels = 100\n     train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True,\n-                                               num_workers=args.prefetch, pin_memory=True)\n+                                               num_workers=args.prefetch, pin_memory=False)\n     test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False,\n-                                              num_workers=args.prefetch, pin_memory=True)\n+                                              num_workers=args.prefetch, pin_memory=False)\n \n     # Init checkpoints\n     if not os.path.isdir(args.save):\n@@ -109,7 +109,7 @@ if __name__ == '__main__':\n         net.train()\n         loss_avg = 0.0\n         for batch_idx, (data, target) in enumerate(train_loader):\n-            data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\n+            data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n \n             # forward\n             output = net(data)\n\n\nRun with python train.py --ngpu 0 --batch_size 8 data cifar10\n\nResult: Segmentation fault (core dumped)\nDebugging info:\nResNeXt.pytorch$ gdb python\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\nCopyright (C) 2016 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\nand \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>.\nFind the GDB manual and other documentation resources online at:\n<http://www.gnu.org/software/gdb/documentation/>.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from python...done.\n(gdb) r train.py --ngpu 0 --batch_size 8 data cifar10\nStarting program: /home/alex/anaconda3/bin/python train.py --ngpu 0 --batch_size 8 data cifar10\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nFiles already downloaded and verified\nFiles already downloaded and verified\nCifarResNeXt (\n  (conv_1_3x3): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n  (stage_1): Sequential (\n    (stage_1_bottleneck_0): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n        (shortcut_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (shortcut_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n      )\n    )\n    (stage_1_bottleneck_1): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n    (stage_1_bottleneck_2): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n  )\n  (stage_2): Sequential (\n    (stage_2_bottleneck_0): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n        (shortcut_conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (shortcut_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      )\n    )\n    (stage_2_bottleneck_1): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n    (stage_2_bottleneck_2): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n  )\n  (stage_3): Sequential (\n    (stage_3_bottleneck_0): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n        (shortcut_conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (shortcut_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      )\n    )\n    (stage_3_bottleneck_1): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n    (stage_3_bottleneck_2): ResNeXtBottleneck (\n      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n      (shortcut): Sequential (\n      )\n    )\n  )\n  (classifier): Linear (1024 -> 10)\n)\n[New Thread 0x7fffbefa5780 (LWP 18919)]\n[New Thread 0x7fffbeba4800 (LWP 18920)]\n[New Thread 0x7fffbe7a3880 (LWP 18921)]\n[New Thread 0x7fffab9b1700 (LWP 18923)]\n[New Thread 0x7fffab1af980 (LWP 18924)]\n[New Thread 0x7fffaadaea00 (LWP 18925)]\n[New Thread 0x7fffaa9ada80 (LWP 18926)]\n\nThread 5 \"python\" received signal SIGSEGV, Segmentation fault.\n[Switching to Thread 0x7fffab9b1700 (LWP 18923)]\n0x00007fffedba4d04 in torch::autograd::cat (tensors=..., dim=dim@entry=0) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n(gdb) where\n#0  0x00007fffedba4d04 in torch::autograd::cat (tensors=..., dim=dim@entry=0) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#1  0x00007fffedba6f1c in torch::autograd::ConvBackward::apply (this=0x2db1a278, grad_outputs=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#2  0x00007fffedb8d138 in torch::autograd::call_function (task=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#3  torch::autograd::Engine::evaluate_function (this=this@entry=0x7fffee408d00 <engine>, task=...) at torch/csrc/autograd/engine.cpp:136\n#4  0x00007fffedb8ed3a in torch::autograd::Engine::thread_main (this=this@entry=0x7fffee408d00 <engine>, queue=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#5  0x00007fffedb9f89a in PythonEngine::thread_main (this=0x7fffee408d00 <engine>, queue=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\n#6  0x00007fffd20a5870 in ?? () from /home/alex/anaconda3/lib/python3.6/site-packages/torch/lib/../../../../libstdc++.so.6\n#7  0x00007ffff76bc6ba in start_thread (arg=0x7fffab9b1700) at pthread_create.c:333\n#8  0x00007ffff6ada82d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n\nVersions:\n$ python --version\nPython 3.6.0 :: Anaconda 4.3.0 (64-bit)\n$ python -c \"import torch; print(torch.__version__)\"\n0.1.11+b13b701\n\nNote: other networks seem to work okay, for example some of the pytorch examples.  It is just this network that crashes.", "body": "Steps to reproduce:\r\n\r\n- Install Ubuntu 16.04.01 LTS\r\n- Install Anaconda 4.3.0 (https://repo.continuum.io/archive/Anaconda3-4.3.0-Linux-x86_64.sh)\r\n- Install PyTorch (cpu version) with `conda install pytorch torchvision -c soumith`\r\n- Get ResNeXt code with `git clone https://github.com/prlz77/ResNeXt.pytorch/`\r\n- Apply this patch to enable running on cpu:\r\n\r\n```\r\ndiff --git a/train.py b/train.py\r\nindex dc5a31d..ceec980 100644\r\n--- a/train.py\r\n+++ b/train.py\r\n@@ -84,9 +84,9 @@ if __name__ == '__main__':\r\n         test_data = dset.CIFAR100(args.data_path, train=False, transform=test_transform, download=True)\r\n         nlabels = 100\r\n     train_loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=True,\r\n-                                               num_workers=args.prefetch, pin_memory=True)\r\n+                                               num_workers=args.prefetch, pin_memory=False)\r\n     test_loader = torch.utils.data.DataLoader(test_data, batch_size=args.test_bs, shuffle=False,\r\n-                                              num_workers=args.prefetch, pin_memory=True)\r\n+                                              num_workers=args.prefetch, pin_memory=False)\r\n \r\n     # Init checkpoints\r\n     if not os.path.isdir(args.save):\r\n@@ -109,7 +109,7 @@ if __name__ == '__main__':\r\n         net.train()\r\n         loss_avg = 0.0\r\n         for batch_idx, (data, target) in enumerate(train_loader):\r\n-            data, target = torch.autograd.Variable(data.cuda()), torch.autograd.Variable(target.cuda())\r\n+            data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\r\n \r\n             # forward\r\n             output = net(data)\r\n```\r\n\r\n- Run with `python train.py --ngpu 0 --batch_size 8 data cifar10`\r\n\r\nResult: `Segmentation fault (core dumped)`\r\n\r\nDebugging info:\r\n\r\n```\r\nResNeXt.pytorch$ gdb python\r\nGNU gdb (Ubuntu 7.11.1-0ubuntu1~16.04) 7.11.1\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.  Type \"show copying\"\r\nand \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-linux-gnu\".\r\nType \"show configuration\" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n<http://www.gnu.org/software/gdb/documentation/>.\r\nFor help, type \"help\".\r\nType \"apropos word\" to search for commands related to \"word\"...\r\nReading symbols from python...done.\r\n(gdb) r train.py --ngpu 0 --batch_size 8 data cifar10\r\nStarting program: /home/alex/anaconda3/bin/python train.py --ngpu 0 --batch_size 8 data cifar10\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\nFiles already downloaded and verified\r\nFiles already downloaded and verified\r\nCifarResNeXt (\r\n  (conv_1_3x3): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\r\n  (bn_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\r\n  (stage_1): Sequential (\r\n    (stage_1_bottleneck_0): ResNeXtBottleneck (\r\n      (conv_reduce): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\r\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\r\n      (shortcut): Sequential (\r\n        (shortcut_conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n        (shortcut_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\r\n      )\r\n    )\r\n    (stage_1_bottleneck_1): ResNeXtBottleneck (\r\n      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\r\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\r\n      (shortcut): Sequential (\r\n      )\r\n    )\r\n    (stage_1_bottleneck_2): ResNeXtBottleneck (\r\n      (conv_reduce): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_reduce): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\r\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_expand): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_expand): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\r\n      (shortcut): Sequential (\r\n      )\r\n    )\r\n  )\r\n  (stage_2): Sequential (\r\n    (stage_2_bottleneck_0): ResNeXtBottleneck (\r\n      (conv_reduce): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\r\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      (shortcut): Sequential (\r\n        (shortcut_conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\r\n        (shortcut_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      )\r\n    )\r\n    (stage_2_bottleneck_1): ResNeXtBottleneck (\r\n      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\r\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      (shortcut): Sequential (\r\n      )\r\n    )\r\n    (stage_2_bottleneck_2): ResNeXtBottleneck (\r\n      (conv_reduce): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_reduce): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\r\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_expand): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_expand): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\r\n      (shortcut): Sequential (\r\n      )\r\n    )\r\n  )\r\n  (stage_3): Sequential (\r\n    (stage_3_bottleneck_0): ResNeXtBottleneck (\r\n      (conv_reduce): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=8, bias=False)\r\n      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      (shortcut): Sequential (\r\n        (shortcut_conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\r\n        (shortcut_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      )\r\n    )\r\n    (stage_3_bottleneck_1): ResNeXtBottleneck (\r\n      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\r\n      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      (shortcut): Sequential (\r\n      )\r\n    )\r\n    (stage_3_bottleneck_2): ResNeXtBottleneck (\r\n      (conv_reduce): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_reduce): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\r\n      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\r\n      (conv_expand): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      (bn_expand): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\r\n      (shortcut): Sequential (\r\n      )\r\n    )\r\n  )\r\n  (classifier): Linear (1024 -> 10)\r\n)\r\n[New Thread 0x7fffbefa5780 (LWP 18919)]\r\n[New Thread 0x7fffbeba4800 (LWP 18920)]\r\n[New Thread 0x7fffbe7a3880 (LWP 18921)]\r\n[New Thread 0x7fffab9b1700 (LWP 18923)]\r\n[New Thread 0x7fffab1af980 (LWP 18924)]\r\n[New Thread 0x7fffaadaea00 (LWP 18925)]\r\n[New Thread 0x7fffaa9ada80 (LWP 18926)]\r\n\r\nThread 5 \"python\" received signal SIGSEGV, Segmentation fault.\r\n[Switching to Thread 0x7fffab9b1700 (LWP 18923)]\r\n0x00007fffedba4d04 in torch::autograd::cat (tensors=..., dim=dim@entry=0) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\r\n(gdb) where\r\n#0  0x00007fffedba4d04 in torch::autograd::cat (tensors=..., dim=dim@entry=0) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\r\n#1  0x00007fffedba6f1c in torch::autograd::ConvBackward::apply (this=0x2db1a278, grad_outputs=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\r\n#2  0x00007fffedb8d138 in torch::autograd::call_function (task=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\r\n#3  torch::autograd::Engine::evaluate_function (this=this@entry=0x7fffee408d00 <engine>, task=...) at torch/csrc/autograd/engine.cpp:136\r\n#4  0x00007fffedb8ed3a in torch::autograd::Engine::thread_main (this=this@entry=0x7fffee408d00 <engine>, queue=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\r\n#5  0x00007fffedb9f89a in PythonEngine::thread_main (this=0x7fffee408d00 <engine>, queue=...) from /home/alex/anaconda3/lib/python3.6/site-packages/torch/_C.cpython-36m-x86_64-linux-gnu.so\r\n#6  0x00007fffd20a5870 in ?? () from /home/alex/anaconda3/lib/python3.6/site-packages/torch/lib/../../../../libstdc++.so.6\r\n#7  0x00007ffff76bc6ba in start_thread (arg=0x7fffab9b1700) at pthread_create.c:333\r\n#8  0x00007ffff6ada82d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\r\n```\r\n\r\nVersions:\r\n```\r\n$ python --version\r\nPython 3.6.0 :: Anaconda 4.3.0 (64-bit)\r\n$ python -c \"import torch; print(torch.__version__)\"\r\n0.1.11+b13b701\r\n```\r\n\r\nNote: other networks seem to work okay, for example some of the pytorch examples.  It is just *this* network that crashes."}