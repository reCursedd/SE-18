{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/322381053", "html_url": "https://github.com/pytorch/pytorch/issues/2418#issuecomment-322381053", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2418", "id": 322381053, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjM4MTA1Mw==", "user": {"login": "santisy", "id": 8842080, "node_id": "MDQ6VXNlcjg4NDIwODA=", "avatar_url": "https://avatars2.githubusercontent.com/u/8842080?v=4", "gravatar_id": "", "url": "https://api.github.com/users/santisy", "html_url": "https://github.com/santisy", "followers_url": "https://api.github.com/users/santisy/followers", "following_url": "https://api.github.com/users/santisy/following{/other_user}", "gists_url": "https://api.github.com/users/santisy/gists{/gist_id}", "starred_url": "https://api.github.com/users/santisy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/santisy/subscriptions", "organizations_url": "https://api.github.com/users/santisy/orgs", "repos_url": "https://api.github.com/users/santisy/repos", "events_url": "https://api.github.com/users/santisy/events{/privacy}", "received_events_url": "https://api.github.com/users/santisy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-15T05:32:24Z", "updated_at": "2017-08-15T05:32:24Z", "author_association": "NONE", "body_html": "<p>I think the problem lies in the <code>scatter_kwargs</code> function. <a href=\"https://github.com/pytorch/pytorch/blob/421607a935813b93362d3e1fc5f481efe27885c1/torch/nn/parallel/data_parallel.py#L100\">scatter_kwargs in data_parallel</a>. It seems that <code>scatter_kwargs</code> only split the variables in the first dimension, however the <code>hidden_state</code> input to RNNs requires format <code>(num_layers * num_directions, batch, hidden_size)</code>, which contradicts with the rule of <code>scatter_kwargs</code>. One temporary solution is to swap the dimension of <code>hidden_state</code> by wrapping <code>GRU</code>.</p>", "body_text": "I think the problem lies in the scatter_kwargs function. scatter_kwargs in data_parallel. It seems that scatter_kwargs only split the variables in the first dimension, however the hidden_state input to RNNs requires format (num_layers * num_directions, batch, hidden_size), which contradicts with the rule of scatter_kwargs. One temporary solution is to swap the dimension of hidden_state by wrapping GRU.", "body": "I think the problem lies in the `scatter_kwargs` function. [scatter_kwargs in data_parallel](https://github.com/pytorch/pytorch/blob/421607a935813b93362d3e1fc5f481efe27885c1/torch/nn/parallel/data_parallel.py#L100). It seems that `scatter_kwargs` only split the variables in the first dimension, however the `hidden_state` input to RNNs requires format `(num_layers * num_directions, batch, hidden_size)`, which contradicts with the rule of `scatter_kwargs`. One temporary solution is to swap the dimension of `hidden_state` by wrapping `GRU`.  "}