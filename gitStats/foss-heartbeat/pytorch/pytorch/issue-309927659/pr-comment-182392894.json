{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/182392894", "pull_request_review_id": 113175214, "id": 182392894, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MjM5Mjg5NA==", "diff_hunk": "@@ -22,289 +27,348 @@ namespace at {\n  * B[1][0]\n  * B[1][1]\n  *\n- * We set the offset into the underlying storage as (storageOffset + stride_B * index_B),\n- * i.e. basically we compute the offset into the storage as we would normally for a\n- * Tensor. But because we are guaranteed the subsequent data is contiguous in memory, we\n- * can simply loop for sizeof(A) iterations and perform the operation, without having to\n- * follow the order described by the strides of A.\n+ * We set the offset into the underlying storage as (storageOffset + stride_B *\n+ * index_B), i.e. basically we compute the offset into the storage as we would\n+ * normally for a Tensor. But because we are guaranteed the subsequent data is\n+ * contiguous in memory, we can simply loop for sizeof(A) iterations and perform\n+ * the operation, without having to follow the order described by the strides of\n+ * A.\n  *\n- * 3. As an optimization, we merge dimensions of A that are contiguous in memory. For\n- * example, if A is a 3x3x3x3 tensor narrowed from a 3x3x4x3 tensor, then the first two\n- * dimensions can be merged for the purposes of APPLY, reducing the number of nested\n- * loops.\n+ * 3. As an optimization, we merge dimensions of A that are contiguous in\n+ * memory. For example, if A is a 3x3x3x3 tensor narrowed from a 3x3x4x3 tensor,\n+ * then the first two dimensions can be merged for the purposes of APPLY,\n+ * reducing the number of nested loops.\n  */\n \n-// TODO: turn this macro into a proper template\n-#define __ATH_TENSOR_APPLYX_PREAMBLE(TYPE, ATENSOR, DIM, ALLOW_CONTIGUOUS) \\\n-  TYPE *ATENSOR##_data = NULL; \\\n-  int64_t *ATENSOR##_counter = NULL, *ATENSOR##_sizes = NULL, *ATENSOR##_strides = NULL, *ATENSOR##_dimOffset = NULL; \\\n-  int64_t ATENSOR##_stride = 0, ATENSOR##_size = 0, ATENSOR##_dim = 0, ATENSOR##_i; \\\n-  int ATENSOR##_contiguous = ALLOW_CONTIGUOUS && DIM < 0; \\\n-\\\n-  if(ATENSOR.sizes().equals({0})) \\\n-    TH_TENSOR_APPLY_hasFinished = true; \\\n-  else \\\n-  { \\\n-    ATENSOR##_data = ATENSOR.data<TYPE>(); \\\n-    ATENSOR##_size = 1; \\\n-    ATENSOR##_stride = 1; \\\n-    for(ATENSOR##_i = ATENSOR.dim() - 1; ATENSOR##_i >= 0; ATENSOR##_i--) { \\\n-      if(ATENSOR.sizes()[ATENSOR##_i] != 1) { \\\n-        if(ATENSOR.strides()[ATENSOR##_i] == ATENSOR##_size && ATENSOR##_i != DIM) \\\n-          ATENSOR##_size *= ATENSOR.sizes()[ATENSOR##_i]; \\\n-        else{ \\\n-          ATENSOR##_contiguous = 0; \\\n-          break; \\\n-        } \\\n-      } \\\n-    } \\\n-    if (!ATENSOR##_contiguous) { \\\n-      /* Find the dimension of contiguous sections */ \\\n-      ATENSOR##_dim = 1; \\\n-      for(ATENSOR##_i = ATENSOR.dim() - 2; ATENSOR##_i >= 0; ATENSOR##_i--) \\\n-      { \\\n-        if(ATENSOR.strides()[ATENSOR##_i] != ATENSOR.strides()[ATENSOR##_i+1] * ATENSOR.sizes()[ATENSOR##_i+1] || ATENSOR##_i == DIM || ATENSOR##_i+1 == DIM) \\\n-          ATENSOR##_dim++; \\\n-      } \\\n-      /* Allocate an array of 3*dim elements, where dim is the number of contiguous sections */ \\\n-      ATENSOR##_counter = new int64_t[3*ATENSOR##_dim]; \\\n-      ATENSOR##_sizes = ATENSOR##_counter + ATENSOR##_dim; \\\n-      ATENSOR##_strides = ATENSOR##_counter + 2*ATENSOR##_dim; \\\n-      TH_TENSOR_dim_index = ATENSOR##_dim-1; \\\n-      ATENSOR##_dimOffset = (DIM == ATENSOR.dim()-1) ? &ATENSOR##_i : &ATENSOR##_counter[DIM]; \\\n-      ATENSOR##_sizes[TH_TENSOR_dim_index] = ATENSOR.sizes()[ATENSOR.dim()-1]; \\\n-      ATENSOR##_strides[TH_TENSOR_dim_index] = ATENSOR.strides()[ATENSOR.dim()-1]; \\\n-      /* ATENSOR##_counter tracks where we are in the storage. The offset into the */ \\\n-      /* storage is given by storage_offset + (i * j), where i is the stride */ \\\n-      /* vector and j is tensor_counter vector. This sets the starting position for the loop. */ \\\n-      for(ATENSOR##_i = ATENSOR##_dim-1; ATENSOR##_i >= 0; --ATENSOR##_i) { \\\n-        ATENSOR##_counter[ATENSOR##_i] = 0; \\\n-      } \\\n-      for(ATENSOR##_i = ATENSOR.dim()-2; ATENSOR##_i >= 0; --ATENSOR##_i) { \\\n-        if (ATENSOR.strides()[ATENSOR##_i] == ATENSOR.strides()[ATENSOR##_i+1] * ATENSOR.sizes()[ATENSOR##_i+1] && ATENSOR##_i != DIM && ATENSOR##_i+1 != DIM) { \\\n-          ATENSOR##_sizes[TH_TENSOR_dim_index] = ATENSOR.sizes()[ATENSOR##_i] * ATENSOR##_sizes[TH_TENSOR_dim_index]; \\\n-          if (DIM != ATENSOR.dim()-1 && ATENSOR##_i < DIM) \\\n-            ATENSOR##_dimOffset--; \\\n-        } else { \\\n-          --TH_TENSOR_dim_index; \\\n-          ATENSOR##_sizes[TH_TENSOR_dim_index] = ATENSOR.sizes()[ATENSOR##_i]; \\\n-          ATENSOR##_strides[TH_TENSOR_dim_index] = ATENSOR.strides()[ATENSOR##_i]; \\\n-        } \\\n-      } \\\n-      /* Size of the inner most section */ \\\n-      ATENSOR##_size = ATENSOR##_sizes[ATENSOR##_dim-1]; \\\n-      /* Stride of the inner most section */ \\\n-      ATENSOR##_stride = ATENSOR##_strides[ATENSOR##_dim-1]; \\\n-    } \\\n-  } \\\n-  ATENSOR##_i = 0;\n-\n-// TODO: turn this macro into a proper template\n-#define  __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(ATENSOR, ALWAYS_UPDATE) \\\n-  if(ATENSOR##_i == ATENSOR##_size || ALWAYS_UPDATE) \\\n-  { \\\n-    if(ATENSOR##_contiguous) \\\n-      break; \\\n-\\\n-    if(ATENSOR##_dim == 1) \\\n-       break; \\\n-\\\n-    /* Reset pointer to beginning of loop */ \\\n-    ATENSOR##_data -= ATENSOR##_size*ATENSOR##_stride; \\\n-    for(ATENSOR##_i = ATENSOR##_dim-2; ATENSOR##_i >= 0; ATENSOR##_i--) \\\n-    { \\\n-      ATENSOR##_counter[ATENSOR##_i]++; \\\n-      /* Jump ahread by the stride of this dimension */ \\\n-      ATENSOR##_data += ATENSOR##_strides[ATENSOR##_i]; \\\n-\\\n-      if(ATENSOR##_counter[ATENSOR##_i]  == ATENSOR##_sizes[ATENSOR##_i]) \\\n-      { \\\n-        if(ATENSOR##_i == 0) \\\n-        { \\\n-          TH_TENSOR_APPLY_hasFinished = true; \\\n-          break; \\\n-        } \\\n-          else \\\n-        { \\\n-          /* Reset the pointer to the beginning of the chunk defined by this dimension */ \\\n-          ATENSOR##_data -= ATENSOR##_counter[ATENSOR##_i]*ATENSOR##_strides[ATENSOR##_i]; \\\n-          ATENSOR##_counter[ATENSOR##_i] = 0; \\\n-        } \\\n-      } \\\n-      else \\\n-        break; \\\n-    } \\\n-    ATENSOR##_i = 0; \\\n+template <typename T, int N>\n+struct AT_API strided_tensor_iter_fixed {\n+ public:\n+  T* data_ = NULL;\n+  int64_t dim_;\n+\n+  int64_t counter_[N];\n+  int64_t sizes_[N];\n+  int64_t strides_[N];\n+\n+  strided_tensor_iter_fixed(strided_tensor_iter_fixed const&) = delete;\n+  void operator=(strided_tensor_iter_fixed const& x) = delete;\n+  strided_tensor_iter_fixed(strided_tensor_iter_fixed&&) = default;\n+  strided_tensor_iter_fixed(Tensor& tensor, bool sort_strides = false)\n+      : data_(tensor.data<T>()), dim_(tensor.ndimension()) {\n+    memset(counter_, 0, sizeof(int64_t) * N);\n+    int64_t max_dim = tensor.ndimension();\n+    dim_ = 0;\n+    for (int64_t i = 0; i < max_dim; i++) {\n+      int64_t size = tensor.size(i);\n+      int64_t stride = tensor.stride(i);\n+      while (i + 1 < max_dim &&\n+             tensor.stride(i) == tensor.size(i + 1) * tensor.stride(i + 1)) {\n+        size = size * tensor.size(i + 1);\n+        stride = tensor.stride(i + 1);\n+        i++;\n+      }\n+      sizes_[dim_] = size;\n+      strides_[dim_] = stride;\n+      dim_++;\n+    }\n   }\n+};\n \n-template <typename scalar1, typename scalar2, typename Op>\n-void CPU_tensor_apply2_dim(Tensor& tensor1, Tensor& tensor2, int64_t dim, Op op) {\n-  checkBackend(\"CPU_tensor_apply2\", {tensor1, tensor2}, Backend::CPU);\n-  bool TH_TENSOR_APPLY_hasFinished = false;\n-  int64_t TH_TENSOR_dim_index = 0;\n-  __ATH_TENSOR_APPLYX_PREAMBLE(scalar1, tensor1, dim, 1)\n-  __ATH_TENSOR_APPLYX_PREAMBLE(scalar2, tensor2, dim, 1)\n-  auto t1_numel = tensor1.numel();\n-  auto t2_numel = tensor2.numel();\n-  if(t1_numel != t2_numel) {\n-    std::ostringstream oss;\n-    oss << \"inconsistent tensor size, expected \" << tensor1.sizes() << \" and \" << tensor2.sizes()\n-        << \" to have the same number of elements, but got \" << t1_numel << \" and \" << t2_numel << \" elements respectively\";\n-    throw std::runtime_error(oss.str());\n+template <typename T>\n+struct AT_API strided_tensor_iter {\n+ private:\n+ public:\n+  T* data_ = NULL;\n+  const int64_t dim_;\n+\n+  std::vector<int64_t> counter_;\n+  std::vector<int64_t> sizes_;\n+  std::vector<int64_t> strides_;\n+\n+  strided_tensor_iter(strided_tensor_iter const&) = delete;\n+  void operator=(strided_tensor_iter const& x) = delete;\n+  strided_tensor_iter(strided_tensor_iter&&) = default;\n+  strided_tensor_iter(Tensor& tensor)\n+      : data_(tensor.data<T>()),\n+        dim_(tensor.ndimension()),\n+        counter_(dim_, 0),\n+        sizes_(tensor.sizes()),\n+        strides_(tensor.strides()) {}\n+};\n+\n+template <typename Arg>\n+AT_API inline void forward(int64_t offset, Arg& ut) {\n+  int64_t multi = offset;\n+  for (int64_t i = ut.dim_ - 1; i >= 0; i--) {\n+    int64_t inc = multi % ut.sizes_[i];\n+    multi = multi / ut.sizes_[i];\n+    ut.data_ = ut.data_ + inc * ut.strides_[i];\n+    ut.counter_[i] += inc;\n   }\n-  while(!TH_TENSOR_APPLY_hasFinished)\n-  {\n-    /* Loop through the inner most region of the Tensor */\n-    for(; tensor1_i < tensor1_size && tensor2_i < tensor2_size; tensor1_i++, tensor2_i++, tensor1_data += tensor1_stride, tensor2_data += tensor2_stride)\n-    {\n-      op(*tensor1_data, *tensor2_data);\n+}\n+\n+template <typename Arg>\n+AT_API inline void iterate(Arg& ut) {\n+  if (ut.counter_[ut.dim_ - 1] == ut.sizes_[ut.dim_ - 1]) {\n+    for (int64_t i = ut.dim_ - 1; i > 0; i--) {\n+      if (ut.counter_[i] == ut.sizes_[i]) {\n+        ut.counter_[i] = 0;\n+        ut.counter_[i - 1]++;\n+        ut.data_ =\n+            ut.data_ - (ut.sizes_[i] * ut.strides_[i]) + ut.strides_[i - 1];\n+      }\n     }\n-    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(tensor1, 0)\n-    __ATH_TENSOR_APPLYX_UPDATE_COUNTERS(tensor2, 0)\n   }\n-  if(tensor1_counter != NULL)\n-    delete [] tensor1_counter;\n-  if(tensor2_counter != NULL)\n-    delete [] tensor2_counter;\n }\n \n-/*\n-  Apply a pointwise operator to two tensors.\n-\n-  The calling convention for op is a function/functor that takes takes two references to\n-  type scalar; at least one of these references should be non-const in order to write the output.\n-  For example, to compute a = b^2, op would be of the form:\n-  [](scalar &a_val, const scalar &b_val) { a_val = b_val * b_val; };\n-*/\n-template<typename scalar1, typename scalar2, typename Op>\n-void CPU_tensor_apply2(Tensor tensor1, Tensor tensor2, Op op) {\n-  CPU_tensor_apply2_dim<scalar1, scalar2, Op>(tensor1, tensor2, -1, op);\n+AT_API inline bool _all_equal_numel(at::ArrayRef<Tensor> tensors) {\n+  if (tensors.size() == 0)\n+    return true;\n+  int64_t all_numel = tensors[0].numel();\n+  for (size_t i = 1; i < tensors.size(); i++) {\n+    if (tensors[i].numel() != all_numel)\n+      return false;\n+  }\n+  return true;\n }\n \n-template<typename scalar1, typename scalar2, typename scalar3, typename Op>\n-void CPU_tensor_apply3_dim(Tensor &tensor1, Tensor& tensor2, Tensor& tensor3, int64_t dim, Op op) {\n-  checkBackend(\"CPU_tensor_apply3\", {tensor1, tensor2, tensor3}, Backend::CPU);\n-  bool TH_TENSOR_APPLY_hasFinished = false;\n-  int64_t TH_TENSOR_dim_index = 0;\n-  __ATH_TENSOR_APPLYX_PREAMBLE(scalar1, tensor1, dim, 1)\n-  __ATH_TENSOR_APPLYX_PREAMBLE(scalar2, tensor2, dim, 1)\n-  __ATH_TENSOR_APPLYX_PREAMBLE(scalar3, tensor3, dim, 1)\n-\n-  int elements_equal = 1;\n-  auto t1_numel = tensor1.numel();\n-  auto t2_numel = tensor2.numel();\n-  auto t3_numel = tensor3.numel();\n-  if(t1_numel!= t2_numel) {\n-    elements_equal = 0;\n-  } else if(t1_numel != t3_numel) {\n-    elements_equal = 0;\n+AT_API inline std::string _all_equal_numel_error(at::ArrayRef<Tensor> tensors) {\n+  std::ostringstream oss;\n+  oss << \"inconsistent tensor size, expected \";\n+  for (size_t i = 0; i < tensors.size() - 1; i++) {\n+    oss << tensors[i].sizes() << \", \";\n   }\n-  if (elements_equal == 0) {\n-    std::ostringstream oss;\n-    oss << \"inconsistent tensor size, expected \" << tensor1.sizes() << \", \" << tensor2.sizes() << \", and \" << tensor3.sizes()\n-        << \" to have the same number of elements, but got \" << t1_numel << \", \" << t2_numel << \", and \" << t3_numel << \" elements respectively\";\n-    throw std::runtime_error(oss.str());\n+  oss << \"and \" << tensors[tensors.size() - 1]\n+      << \" to have the same number of elements, but got \";\n+  for (size_t i = 0; i < tensors.size() - 1; i++) {\n+    oss << tensors[i].numel() << \", \";\n   }\n+  oss << \"and \" << tensors[tensors.size() - 1].numel()\n+      << \" elements respectively\";\n+  return oss.str();\n+}\n \n-  while(!TH_TENSOR_APPLY_hasFinished)\n-  {\n-    /* Loop through the inner most region of the Tensor */\n-    for(; tensor1_i <  tensor1_size && tensor2_i < tensor2_size && tensor3_i < tensor3_size; tensor1_i++, tensor2_i++, tensor3_i++, tensor1_data += tensor1_stride, tensor2_data += tensor2_stride, tensor3_data += tensor3_stride)\n-    {\n-      op(*tensor1_data, *tensor2_data, *tensor3_data);\n+AT_API inline bool _apply_preamble(ArrayRef<Tensor> tensors) {\n+  checkBackend(\"CPU_tensor_apply\", tensors, Backend::CPU);\n+  if (!_all_equal_numel(tensors))\n+    throw std::runtime_error(_all_equal_numel_error(tensors));\n+  // An empty tensor has no elements\n+  for (auto& t : tensors)\n+    if (t.sizes().equals({0}))\n+      return false;\n+  internal::init_tbb_num_threads();\n+  return true;\n+}\n+\n+AT_API inline void iterate_all(){};\n+\n+template <typename Arg, typename... Args>\n+AT_API inline void iterate_all(Arg& iter, Args&... iter_tail) {\n+  iterate(iter);\n+  iterate_all(iter_tail...);\n+}\n+\n+AT_API inline void forward_all(int64_t offset){};\n+\n+template <typename Arg, typename... Args>\n+AT_API inline void forward_all(int64_t offset, Arg& iter, Args&... iter_tail) {\n+  forward(offset, iter);\n+  forward_all(offset, iter_tail...);\n+}\n+\n+AT_API inline bool continue_loop() {\n+  return true;\n+}\n+\n+template <typename Arg, typename... Args>\n+AT_API inline bool continue_loop(Arg& iter, Args&... iter_tail) {\n+  return iter.counter_[iter.dim_ - 1] < iter.sizes_[iter.dim_ - 1] &&\n+      continue_loop(iter_tail...);", "path": "aten/src/ATen/CPUApplyUtils.h", "position": null, "original_position": 364, "commit_id": "ca55c40ec722ecb64659682bdb81caa0cfb82e36", "original_commit_id": "5a48d570764ff93638dbcace66485f97806ebb65", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Do we really need to check this condition on all tensors? We've already checked the have an equal number of elements, so it should only be the case that all of those conditions are false or all are true. Is that correct?", "created_at": "2018-04-18T11:25:30Z", "updated_at": "2018-11-23T15:42:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/6119#discussion_r182392894", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6119", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/182392894"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6119#discussion_r182392894"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6119"}}, "body_html": "<p>Do we really need to check this condition on all tensors? We've already checked the have an equal number of elements, so it should only be the case that all of those conditions are false or all are true. Is that correct?</p>", "body_text": "Do we really need to check this condition on all tensors? We've already checked the have an equal number of elements, so it should only be the case that all of those conditions are false or all are true. Is that correct?"}