{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/382362325", "html_url": "https://github.com/pytorch/pytorch/pull/6119#issuecomment-382362325", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6119", "id": 382362325, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjM2MjMyNQ==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-18T11:59:35Z", "updated_at": "2018-04-18T12:19:44Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> so, let me try to explain what I meant by returning non-contiguous tensors (was writing from my phone yesterday).</p>\n<p>So, the current way we do things is to always return a contiguous tensor, for every operation.<br>\nThis means that the access pattern of the input tensor and the output tensor can be different if the input tensor is non-contiguous.</p>\n<p>If we can return non-contiguous tensors, then we can make the access pattern of the output the same as the input tensor, which means that in some cases the computation time is exactly the same for non-contiguous tensors as for contiguous tensors (if by sorting the strides, the resulting tensor is contiguous).</p>\n<p>For example:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)\nb <span class=\"pl-k\">=</span> a.transpose((<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>))\n\n<span class=\"pl-k\">%</span>timeit a <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">291</span> \u00b5s \u00b1 <span class=\"pl-c1\">19.1</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> same timing for non-contiguous array</span>\n<span class=\"pl-k\">%</span>timeit b <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">289</span> \u00b5s \u00b1 <span class=\"pl-c1\">20.3</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\n<span class=\"pl-c1\">print</span>((a <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>).strides)\n(<span class=\"pl-c1\">80000</span>, <span class=\"pl-c1\">800</span>, <span class=\"pl-c1\">8</span>)\n\n<span class=\"pl-c1\">print</span>((b <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>).strides)\n(<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">80000</span>, <span class=\"pl-c1\">800</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> same as b</span></pre></div>\n<p>While in PyTorch master (single thread), we have</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">%</span>timeit a <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">294</span> \u00b5s \u00b1 <span class=\"pl-c1\">22.1</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\n<span class=\"pl-k\">%</span>timeit b <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">2.62</span> ms \u00b1 <span class=\"pl-c1\">282</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100</span> loops each)</pre></div>\n<p>Does this actually matter outside micro-benchmarks? I'm not sure though, because the cost for more complex operations can get amortized (as the first operation converts the tensor to a contiguous tensor, and the rest is fast):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> in PyTorch</span>\n<span class=\"pl-k\">%</span>timeit (a <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">6</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>\n<span class=\"pl-c1\">1.22</span> ms \u00b1 <span class=\"pl-c1\">2.64</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\n<span class=\"pl-k\">%</span>timeit (b <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">6</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>\n<span class=\"pl-c1\">3.21</span> ms \u00b1 <span class=\"pl-c1\">138</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100</span> loops each)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> in numpy</span>\n<span class=\"pl-k\">%</span>timeit (aa <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">6</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>\n<span class=\"pl-c1\">881</span> \u00b5s \u00b1 <span class=\"pl-c1\">709</span> ns per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)\n\n<span class=\"pl-k\">%</span>timeit (bb <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">6</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>\n<span class=\"pl-c1\">893</span> \u00b5s \u00b1 <span class=\"pl-c1\">3.16</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">1000</span> loops each)</pre></div>", "body_text": "@apaszke so, let me try to explain what I meant by returning non-contiguous tensors (was writing from my phone yesterday).\nSo, the current way we do things is to always return a contiguous tensor, for every operation.\nThis means that the access pattern of the input tensor and the output tensor can be different if the input tensor is non-contiguous.\nIf we can return non-contiguous tensors, then we can make the access pattern of the output the same as the input tensor, which means that in some cases the computation time is exactly the same for non-contiguous tensors as for contiguous tensors (if by sorting the strides, the resulting tensor is contiguous).\nFor example:\na = np.random.rand(100, 100, 100)\nb = a.transpose((2, 0, 1))\n\n%timeit a + 1\n291 \u00b5s \u00b1 19.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n# same timing for non-contiguous array\n%timeit b + 1\n289 \u00b5s \u00b1 20.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nprint((a + 1).strides)\n(80000, 800, 8)\n\nprint((b + 1).strides)\n(8, 80000, 800)  # same as b\nWhile in PyTorch master (single thread), we have\n%timeit a + 1\n294 \u00b5s \u00b1 22.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n%timeit b + 1\n2.62 ms \u00b1 282 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\nDoes this actually matter outside micro-benchmarks? I'm not sure though, because the cost for more complex operations can get amortized (as the first operation converts the tensor to a contiguous tensor, and the rest is fast):\n# in PyTorch\n%timeit (a + 1) ** 2 * 6 + 2\n1.22 ms \u00b1 2.64 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n%timeit (b + 1) ** 2 * 6 + 2\n3.21 ms \u00b1 138 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n# in numpy\n%timeit (aa + 1) ** 2 * 6 + 2\n881 \u00b5s \u00b1 709 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\n%timeit (bb + 1) ** 2 * 6 + 2\n893 \u00b5s \u00b1 3.16 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)", "body": "@apaszke so, let me try to explain what I meant by returning non-contiguous tensors (was writing from my phone yesterday).\r\n\r\nSo, the current way we do things is to always return a contiguous tensor, for every operation.\r\nThis means that the access pattern of the input tensor and the output tensor can be different if the input tensor is non-contiguous.\r\n\r\nIf we can return non-contiguous tensors, then we can make the access pattern of the output the same as the input tensor, which means that in some cases the computation time is exactly the same for non-contiguous tensors as for contiguous tensors (if by sorting the strides, the resulting tensor is contiguous).\r\n\r\nFor example:\r\n\r\n```python\r\na = np.random.rand(100, 100, 100)\r\nb = a.transpose((2, 0, 1))\r\n\r\n%timeit a + 1\r\n291 \u00b5s \u00b1 19.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n# same timing for non-contiguous array\r\n%timeit b + 1\r\n289 \u00b5s \u00b1 20.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\nprint((a + 1).strides)\r\n(80000, 800, 8)\r\n\r\nprint((b + 1).strides)\r\n(8, 80000, 800)  # same as b\r\n```\r\n\r\nWhile in PyTorch master (single thread), we have\r\n```python\r\n%timeit a + 1\r\n294 \u00b5s \u00b1 22.1 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n%timeit b + 1\r\n2.62 ms \u00b1 282 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\nDoes this actually matter outside micro-benchmarks? I'm not sure though, because the cost for more complex operations can get amortized (as the first operation converts the tensor to a contiguous tensor, and the rest is fast):\r\n\r\n```python\r\n# in PyTorch\r\n%timeit (a + 1) ** 2 * 6 + 2\r\n1.22 ms \u00b1 2.64 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n%timeit (b + 1) ** 2 * 6 + 2\r\n3.21 ms \u00b1 138 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n\r\n# in numpy\r\n%timeit (aa + 1) ** 2 * 6 + 2\r\n881 \u00b5s \u00b1 709 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n%timeit (bb + 1) ** 2 * 6 + 2\r\n893 \u00b5s \u00b1 3.16 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\r\n```"}