{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/380669884", "html_url": "https://github.com/pytorch/pytorch/pull/6119#issuecomment-380669884", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6119", "id": 380669884, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDY2OTg4NA==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-12T03:52:17Z", "updated_at": "2018-04-12T04:22:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for repasting this <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a>.</p>\n<p>We figured out the perf problems. On a high level, it's related to <a href=\"https://software.intel.com/en-us/articles/avoiding-avx-sse-transition-penalties\" rel=\"nofollow\">sse to avx transitions</a> and it's runtime penalties. We fixed this by moving all code that doesn't explicitly benefit from -mavx2 into UnaryOps.cpp instead of UnaryOpsKernel.cpp. And just as you just suggested, writing the benchmark in C++ actually brought this to light.</p>\n<p>At this point we're close to reproducing the perf of the macro on master using this branch. I have various perf improvements in mind (such as collapsing dimensions and figuring out how to best use SmallVector etc.), that should now allow us to easily close the gap. As evidence getting rid of std::vector and replacing it with stack allocated arrays significantly improves the timings (almost closing the gap).</p>\n<p>Current timings on single core using <a href=\"https://paste.fedoraproject.org/paste/yG5Cb4wPgPWSMJl5-ijyBw\" rel=\"nofollow\">this script</a>.</p>\n<pre><code>Master\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.31 type: torch.FloatTensor    dim: 3\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   1.68 type: torch.FloatTensor    dim: 3\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:  10.58 type: torch.DoubleTensor   dim: 3\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   3.20 type: torch.DoubleTensor   dim: 3\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.54 type: torch.FloatTensor    dim: 3\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   1.74 type: torch.FloatTensor    dim: 3\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.70 type: torch.DoubleTensor   dim: 3\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   1.86 type: torch.DoubleTensor   dim: 3\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   1.33 type: torch.FloatTensor    dim: 4\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.18 type: torch.FloatTensor    dim: 4\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.93 type: torch.DoubleTensor   dim: 4\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   2.48 type: torch.DoubleTensor   dim: 4\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   1.65 type: torch.FloatTensor    dim: 4\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.39 type: torch.FloatTensor    dim: 4\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   1.79 type: torch.DoubleTensor   dim: 4\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.51 type: torch.DoubleTensor   dim: 4\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.00 type: torch.FloatTensor    dim: 5\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   1.34 type: torch.FloatTensor    dim: 5\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   2.25 type: torch.DoubleTensor   dim: 5\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.91 type: torch.DoubleTensor   dim: 5\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.28 type: torch.FloatTensor    dim: 5\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   1.65 type: torch.FloatTensor    dim: 5\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.36 type: torch.DoubleTensor   dim: 5\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   1.78 type: torch.DoubleTensor   dim: 5\n\n\nThis branch\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.95 type: torch.FloatTensor    dim: 3\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   2.27 type: torch.FloatTensor    dim: 3\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:  11.34 type: torch.DoubleTensor   dim: 3\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   4.09 type: torch.DoubleTensor   dim: 3\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   2.22 type: torch.FloatTensor    dim: 3\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   2.46 type: torch.FloatTensor    dim: 3\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   2.44 type: torch.DoubleTensor   dim: 3\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   2.56 type: torch.DoubleTensor   dim: 3\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.21 type: torch.FloatTensor    dim: 4\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.77 type: torch.FloatTensor    dim: 4\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   4.12 type: torch.DoubleTensor   dim: 4\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   3.37 type: torch.DoubleTensor   dim: 4\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.53 type: torch.FloatTensor    dim: 4\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   2.04 type: torch.FloatTensor    dim: 4\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.61 type: torch.DoubleTensor   dim: 4\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   2.11 type: torch.DoubleTensor   dim: 4\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.84 type: torch.FloatTensor    dim: 5\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.23 type: torch.FloatTensor    dim: 5\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   3.37 type: torch.DoubleTensor   dim: 5\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   4.13 type: torch.DoubleTensor   dim: 5\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   2.14 type: torch.FloatTensor    dim: 5\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.59 type: torch.FloatTensor    dim: 5\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   2.12 type: torch.DoubleTensor   dim: 5\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.65 type: torch.DoubleTensor   dim: 5\n</code></pre>", "body_text": "Thanks for repasting this @ezyang.\nWe figured out the perf problems. On a high level, it's related to sse to avx transitions and it's runtime penalties. We fixed this by moving all code that doesn't explicitly benefit from -mavx2 into UnaryOps.cpp instead of UnaryOpsKernel.cpp. And just as you just suggested, writing the benchmark in C++ actually brought this to light.\nAt this point we're close to reproducing the perf of the macro on master using this branch. I have various perf improvements in mind (such as collapsing dimensions and figuring out how to best use SmallVector etc.), that should now allow us to easily close the gap. As evidence getting rid of std::vector and replacing it with stack allocated arrays significantly improves the timings (almost closing the gap).\nCurrent timings on single core using this script.\nMaster\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.31 type: torch.FloatTensor    dim: 3\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   1.68 type: torch.FloatTensor    dim: 3\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:  10.58 type: torch.DoubleTensor   dim: 3\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   3.20 type: torch.DoubleTensor   dim: 3\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.54 type: torch.FloatTensor    dim: 3\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   1.74 type: torch.FloatTensor    dim: 3\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.70 type: torch.DoubleTensor   dim: 3\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   1.86 type: torch.DoubleTensor   dim: 3\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   1.33 type: torch.FloatTensor    dim: 4\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.18 type: torch.FloatTensor    dim: 4\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.93 type: torch.DoubleTensor   dim: 4\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   2.48 type: torch.DoubleTensor   dim: 4\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   1.65 type: torch.FloatTensor    dim: 4\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.39 type: torch.FloatTensor    dim: 4\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   1.79 type: torch.DoubleTensor   dim: 4\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.51 type: torch.DoubleTensor   dim: 4\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.00 type: torch.FloatTensor    dim: 5\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   1.34 type: torch.FloatTensor    dim: 5\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   2.25 type: torch.DoubleTensor   dim: 5\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.91 type: torch.DoubleTensor   dim: 5\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.28 type: torch.FloatTensor    dim: 5\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   1.65 type: torch.FloatTensor    dim: 5\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.36 type: torch.DoubleTensor   dim: 5\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   1.78 type: torch.DoubleTensor   dim: 5\n\n\nThis branch\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.95 type: torch.FloatTensor    dim: 3\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   2.27 type: torch.FloatTensor    dim: 3\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:  11.34 type: torch.DoubleTensor   dim: 3\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   4.09 type: torch.DoubleTensor   dim: 3\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   2.22 type: torch.FloatTensor    dim: 3\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   2.46 type: torch.FloatTensor    dim: 3\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   2.44 type: torch.DoubleTensor   dim: 3\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   2.56 type: torch.DoubleTensor   dim: 3\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.21 type: torch.FloatTensor    dim: 4\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.77 type: torch.FloatTensor    dim: 4\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   4.12 type: torch.DoubleTensor   dim: 4\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   3.37 type: torch.DoubleTensor   dim: 4\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.53 type: torch.FloatTensor    dim: 4\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   2.04 type: torch.FloatTensor    dim: 4\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.61 type: torch.DoubleTensor   dim: 4\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   2.11 type: torch.DoubleTensor   dim: 4\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.84 type: torch.FloatTensor    dim: 5\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.23 type: torch.FloatTensor    dim: 5\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   3.37 type: torch.DoubleTensor   dim: 5\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   4.13 type: torch.DoubleTensor   dim: 5\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   2.14 type: torch.FloatTensor    dim: 5\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.59 type: torch.FloatTensor    dim: 5\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   2.12 type: torch.DoubleTensor   dim: 5\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.65 type: torch.DoubleTensor   dim: 5", "body": "Thanks for repasting this @ezyang.\r\n\r\nWe figured out the perf problems. On a high level, it's related to [sse to avx transitions](https://software.intel.com/en-us/articles/avoiding-avx-sse-transition-penalties) and it's runtime penalties. We fixed this by moving all code that doesn't explicitly benefit from -mavx2 into UnaryOps.cpp instead of UnaryOpsKernel.cpp. And just as you just suggested, writing the benchmark in C++ actually brought this to light.\r\n\r\nAt this point we're close to reproducing the perf of the macro on master using this branch. I have various perf improvements in mind (such as collapsing dimensions and figuring out how to best use SmallVector etc.), that should now allow us to easily close the gap. As evidence getting rid of std::vector and replacing it with stack allocated arrays significantly improves the timings (almost closing the gap).\r\n\r\nCurrent timings on single core using [this script](https://paste.fedoraproject.org/paste/yG5Cb4wPgPWSMJl5-ijyBw).\r\n\r\n```\r\nMaster\r\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.31 type: torch.FloatTensor    dim: 3\r\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   1.68 type: torch.FloatTensor    dim: 3\r\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:  10.58 type: torch.DoubleTensor   dim: 3\r\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   3.20 type: torch.DoubleTensor   dim: 3\r\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.54 type: torch.FloatTensor    dim: 3\r\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   1.74 type: torch.FloatTensor    dim: 3\r\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.70 type: torch.DoubleTensor   dim: 3\r\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   1.86 type: torch.DoubleTensor   dim: 3\r\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   1.33 type: torch.FloatTensor    dim: 4\r\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.18 type: torch.FloatTensor    dim: 4\r\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.93 type: torch.DoubleTensor   dim: 4\r\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   2.48 type: torch.DoubleTensor   dim: 4\r\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   1.65 type: torch.FloatTensor    dim: 4\r\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.39 type: torch.FloatTensor    dim: 4\r\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   1.79 type: torch.DoubleTensor   dim: 4\r\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.51 type: torch.DoubleTensor   dim: 4\r\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.00 type: torch.FloatTensor    dim: 5\r\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   1.34 type: torch.FloatTensor    dim: 5\r\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   2.25 type: torch.DoubleTensor   dim: 5\r\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.91 type: torch.DoubleTensor   dim: 5\r\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.28 type: torch.FloatTensor    dim: 5\r\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   1.65 type: torch.FloatTensor    dim: 5\r\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.36 type: torch.DoubleTensor   dim: 5\r\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   1.78 type: torch.DoubleTensor   dim: 5\r\n\r\n\r\nThis branch\r\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   1.95 type: torch.FloatTensor    dim: 3\r\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   2.27 type: torch.FloatTensor    dim: 3\r\nsin   size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:  11.34 type: torch.DoubleTensor   dim: 3\r\nsin   size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   4.09 type: torch.DoubleTensor   dim: 3\r\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   2.22 type: torch.FloatTensor    dim: 3\r\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   2.46 type: torch.FloatTensor    dim: 3\r\nsqrt  size: 10^1   count: 10000  size: [21, 21, 21]         stride: (9261, 441, 21)                numel: 9261      elapsed:   2.44 type: torch.DoubleTensor   dim: 3\r\nsqrt  size: 10^2   count: 1000   size: [46, 46, 46]         stride: (97336, 2116, 46)              numel: 97336     elapsed:   2.56 type: torch.DoubleTensor   dim: 3\r\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.21 type: torch.FloatTensor    dim: 4\r\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   1.77 type: torch.FloatTensor    dim: 4\r\nsin   size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   4.12 type: torch.DoubleTensor   dim: 4\r\nsin   size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   3.37 type: torch.DoubleTensor   dim: 4\r\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.53 type: torch.FloatTensor    dim: 4\r\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   2.04 type: torch.FloatTensor    dim: 4\r\nsqrt  size: 10^1   count: 10000  size: [10, 10, 10, 10]     stride: (10000, 1000, 100, 10)         numel: 10000     elapsed:   2.61 type: torch.DoubleTensor   dim: 4\r\nsqrt  size: 10^2   count: 1000   size: [17, 17, 17, 17]     stride: (83521, 4913, 289, 17)         numel: 83521     elapsed:   2.11 type: torch.DoubleTensor   dim: 4\r\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   1.84 type: torch.FloatTensor    dim: 5\r\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.23 type: torch.FloatTensor    dim: 5\r\nsin   size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   3.37 type: torch.DoubleTensor   dim: 5\r\nsin   size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   4.13 type: torch.DoubleTensor   dim: 5\r\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   2.14 type: torch.FloatTensor    dim: 5\r\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.59 type: torch.FloatTensor    dim: 5\r\nsqrt  size: 10^1   count: 10000  size: [6, 6, 6, 6, 6]      stride: (7776, 1296, 216, 36, 6)       numel: 7776      elapsed:   2.12 type: torch.DoubleTensor   dim: 5\r\nsqrt  size: 10^2   count: 1000   size: [10, 10, 10, 10, 10] stride: (100000, 10000, 1000, 100, 10) numel: 100000    elapsed:   2.65 type: torch.DoubleTensor   dim: 5\r\n```"}