{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/380665097", "html_url": "https://github.com/pytorch/pytorch/pull/6119#issuecomment-380665097", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6119", "id": 380665097, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDY2NTA5Nw==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-12T03:23:03Z", "updated_at": "2018-04-12T03:23:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I have a very interesting comment from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a> in my inbox, which I cannot find on this issue anymore. Assuming that GitHub ate it, I am pasting it here.</p>\n<pre><code>  After spending quite a bit of time on this in terms of performance optimization on a single core\n  there appears to be a few interesting topics\n\n  1) The capability dispatch code appears to cause some initial overhead. When timing the code with [m\n  script](https://paste.fedoraproject.org/paste/nKfovmrDZZ4KEGqKzwSL2g) I found that it is important t\n  run a small benchmark first. If I don't run that first small benchmark (call to ```run```) I see\n  around an 8x slowndown for the first actual benchmark, no matter the number of counts (!) within the\n  benchmark itself. Further, if I run this script with ```ATEN_DISABLE_AVX=1 ATEN_DISABLE_AVX2=1```\n  that slowdown also disappears. There appears to be something about our dispatch code that appears to\n  cause a slow-down or there is something wrong with this benchmark code, so I'd be very happy to hear\n  some insights on that if someone has some time. I'm wondering if it's related to how the dispatch is\n  currently implemented. Note that it's not necessary to warm up the benchmark with any such call for\n  Master and 0.3.\n\n  2) The benchmark scripts gives significantly better timings when the strided tensor uses stack\n  allocated, fixed sized memory for counter_, sizes_, etc. instead of std::vector. This appears to hav\n  a lot more impact than optimizing the calculation of data_ (i.e. the iteration script) etc. I\n  implemented a rudimentary version of a small vector (called small_vector), but it's not as efficient\n  as using a fixed sized array. I'm going to try using the LLVM SmallVector once it's available.\n\n  3) It's important to wrap the inner loop into a simple for loop that just deals with raw pointers.\n\n  4) It's important to get the compiler to inline the calculations. This can be done using lambdas of\n  signature [](args...).\n\n  There's another stylistic thing I haven't gotten to work yet, which is consolidating all the\n  apply2/3/4 functions into a single apply.\n\n  So far the goal is to get the same perf as the current implementation on a single core. Then it\n  should be, hopefully, easy to get speed similar to OMP using some basic parallelization with TBB.\n  After that we might be able to get some real gains over what's currently being done.\n</code></pre>\n<p>A few notes:</p>\n<blockquote>\n<p>I  implemented a rudimentary version of a small vector (called small_vector), but it's not as efficient  as using a fixed sized array. I'm going to try using the LLVM SmallVector once it's available.</p>\n</blockquote>\n<p>We absolutely need small vector for c10. If you bring this into ATen (both LLVM and folly SmallVector seem to have lots of dependencies) I can import this code into the c10 efforts.</p>\n<blockquote>\n<p>There appears to be something about our dispatch code that appears to cause a slow-down or there is something wrong with this benchmark code, so I'd be very happy to hear some insights on that if someone has some time.</p>\n</blockquote>\n<p>I noticed the benchmark script is written in Python. I'd suggest rerunning it from C++ and seeing if that makes a difference.</p>", "body_text": "I have a very interesting comment from @cpuhrsch in my inbox, which I cannot find on this issue anymore. Assuming that GitHub ate it, I am pasting it here.\n  After spending quite a bit of time on this in terms of performance optimization on a single core\n  there appears to be a few interesting topics\n\n  1) The capability dispatch code appears to cause some initial overhead. When timing the code with [m\n  script](https://paste.fedoraproject.org/paste/nKfovmrDZZ4KEGqKzwSL2g) I found that it is important t\n  run a small benchmark first. If I don't run that first small benchmark (call to ```run```) I see\n  around an 8x slowndown for the first actual benchmark, no matter the number of counts (!) within the\n  benchmark itself. Further, if I run this script with ```ATEN_DISABLE_AVX=1 ATEN_DISABLE_AVX2=1```\n  that slowdown also disappears. There appears to be something about our dispatch code that appears to\n  cause a slow-down or there is something wrong with this benchmark code, so I'd be very happy to hear\n  some insights on that if someone has some time. I'm wondering if it's related to how the dispatch is\n  currently implemented. Note that it's not necessary to warm up the benchmark with any such call for\n  Master and 0.3.\n\n  2) The benchmark scripts gives significantly better timings when the strided tensor uses stack\n  allocated, fixed sized memory for counter_, sizes_, etc. instead of std::vector. This appears to hav\n  a lot more impact than optimizing the calculation of data_ (i.e. the iteration script) etc. I\n  implemented a rudimentary version of a small vector (called small_vector), but it's not as efficient\n  as using a fixed sized array. I'm going to try using the LLVM SmallVector once it's available.\n\n  3) It's important to wrap the inner loop into a simple for loop that just deals with raw pointers.\n\n  4) It's important to get the compiler to inline the calculations. This can be done using lambdas of\n  signature [](args...).\n\n  There's another stylistic thing I haven't gotten to work yet, which is consolidating all the\n  apply2/3/4 functions into a single apply.\n\n  So far the goal is to get the same perf as the current implementation on a single core. Then it\n  should be, hopefully, easy to get speed similar to OMP using some basic parallelization with TBB.\n  After that we might be able to get some real gains over what's currently being done.\n\nA few notes:\n\nI  implemented a rudimentary version of a small vector (called small_vector), but it's not as efficient  as using a fixed sized array. I'm going to try using the LLVM SmallVector once it's available.\n\nWe absolutely need small vector for c10. If you bring this into ATen (both LLVM and folly SmallVector seem to have lots of dependencies) I can import this code into the c10 efforts.\n\nThere appears to be something about our dispatch code that appears to cause a slow-down or there is something wrong with this benchmark code, so I'd be very happy to hear some insights on that if someone has some time.\n\nI noticed the benchmark script is written in Python. I'd suggest rerunning it from C++ and seeing if that makes a difference.", "body": "I have a very interesting comment from @cpuhrsch in my inbox, which I cannot find on this issue anymore. Assuming that GitHub ate it, I am pasting it here.\r\n\r\n```\r\n  After spending quite a bit of time on this in terms of performance optimization on a single core\r\n  there appears to be a few interesting topics\r\n\r\n  1) The capability dispatch code appears to cause some initial overhead. When timing the code with [m\r\n  script](https://paste.fedoraproject.org/paste/nKfovmrDZZ4KEGqKzwSL2g) I found that it is important t\r\n  run a small benchmark first. If I don't run that first small benchmark (call to ```run```) I see\r\n  around an 8x slowndown for the first actual benchmark, no matter the number of counts (!) within the\r\n  benchmark itself. Further, if I run this script with ```ATEN_DISABLE_AVX=1 ATEN_DISABLE_AVX2=1```\r\n  that slowdown also disappears. There appears to be something about our dispatch code that appears to\r\n  cause a slow-down or there is something wrong with this benchmark code, so I'd be very happy to hear\r\n  some insights on that if someone has some time. I'm wondering if it's related to how the dispatch is\r\n  currently implemented. Note that it's not necessary to warm up the benchmark with any such call for\r\n  Master and 0.3.\r\n\r\n  2) The benchmark scripts gives significantly better timings when the strided tensor uses stack\r\n  allocated, fixed sized memory for counter_, sizes_, etc. instead of std::vector. This appears to hav\r\n  a lot more impact than optimizing the calculation of data_ (i.e. the iteration script) etc. I\r\n  implemented a rudimentary version of a small vector (called small_vector), but it's not as efficient\r\n  as using a fixed sized array. I'm going to try using the LLVM SmallVector once it's available.\r\n\r\n  3) It's important to wrap the inner loop into a simple for loop that just deals with raw pointers.\r\n\r\n  4) It's important to get the compiler to inline the calculations. This can be done using lambdas of\r\n  signature [](args...).\r\n\r\n  There's another stylistic thing I haven't gotten to work yet, which is consolidating all the\r\n  apply2/3/4 functions into a single apply.\r\n\r\n  So far the goal is to get the same perf as the current implementation on a single core. Then it\r\n  should be, hopefully, easy to get speed similar to OMP using some basic parallelization with TBB.\r\n  After that we might be able to get some real gains over what's currently being done.\r\n```\r\n\r\nA few notes:\r\n\r\n> I  implemented a rudimentary version of a small vector (called small_vector), but it's not as efficient  as using a fixed sized array. I'm going to try using the LLVM SmallVector once it's available.\r\n\r\nWe absolutely need small vector for c10. If you bring this into ATen (both LLVM and folly SmallVector seem to have lots of dependencies) I can import this code into the c10 efforts.\r\n\r\n> There appears to be something about our dispatch code that appears to cause a slow-down or there is something wrong with this benchmark code, so I'd be very happy to hear some insights on that if someone has some time.\r\n\r\nI noticed the benchmark script is written in Python. I'd suggest rerunning it from C++ and seeing if that makes a difference."}