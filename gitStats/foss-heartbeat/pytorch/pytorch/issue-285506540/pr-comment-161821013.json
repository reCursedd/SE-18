{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/161821013", "pull_request_review_id": 89169885, "id": 161821013, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MTgyMTAxMw==", "diff_hunk": "@@ -139,314 +137,58 @@ static void _check_inputs(TensorList inputs, TensorList outputs, int input_multi\n   }\n }\n \n-static ncclDataType_t _get_data_type(const Type& type) {\n-  if (type.backend() != kCUDA) {\n-    throw std::runtime_error(\"Unconvertible NCCL type\");\n-  }\n-  switch (type.scalarType()) {\n-  case at::kFloat   : return ncclFloat;\n-  case at::kHalf    : return ncclHalf;\n-  case at::kDouble  : return ncclDouble;\n-  case at::kLong    : return ncclInt64;\n-  case at::kInt     : return ncclInt;\n-  case at::kChar    : return ncclChar;\n-  case at::kByte    : return ncclChar;\n-  default: throw std::runtime_error(\"Unconvertible NCCL type\");\n+} // namespace detail\n+\n+bool is_available(const TensorList& tensors) {\n+#ifdef WITH_NCCL\n+  device_set devices;\n+  for (auto & tensor : tensors) {\n+    auto & type = tensor.type();\n+    if (!type.is_cuda() || type.is_sparse())\n+      return false;\n+    if (!tensor.is_contiguous())\n+      return false;\n+    auto device = tensor.get_device();\n+    if (devices[device])\n+      return false;\n+    devices[device] = true;\n   }\n+  return true;\n+#else\n+  return false;\n+#endif\n }\n \n-PyObject * THCPModule_nccl_version(PyObject *self, PyObject *args) {\n+std::uint64_t version() {\n #if defined(NCCL_MAJOR)\n-  return PyInt_FromLong(NCCL_MAJOR * 1000 + NCCL_MINOR * 100 + NCCL_PATCH);\n+  return NCCL_MAJOR * 1000 + NCCL_MINOR * 100 + NCCL_PATCH;\n+#elif defined(WITH_NCCL)\n+  return 1000;\n #else\n-  return PyInt_FromLong(1000);  // assume NCCL 1.0\n+  return 0;\n #endif\n }\n \n-PyObject * THCPModule_nccl_unique_id(PyObject *self, PyObject *args) {\n-  HANDLE_TH_ERRORS\n-  ncclUniqueId id;\n-  CHECK(ncclGetUniqueId(&id));\n-  return PyBytes_FromStringAndSize((char*)&id, NCCL_UNIQUE_ID_BYTES);\n-  END_HANDLE_TH_ERRORS\n-}\n-\n-static ncclComm_t unpack_nccl_comm(PyObject* capsule) {\n-  ncclComm_t comm = (ncclComm_t)PyCapsule_GetPointer(capsule, COMM_CAPSULE_NAME);\n-  if (!comm) throw python_error();\n-  return comm;\n-}\n-\n-static void destroy_nccl_comm(PyObject* capsule) {\n-  HANDLE_TH_ERRORS\n-  ncclComm_t comm = unpack_nccl_comm(capsule);\n-  with_no_gil([&]{\n-    ncclCommDestroy(comm);\n-  });\n-  END_HANDLE_TH_ERRORS_RET()\n-}\n-\n-static std::vector<THCStream*> unpack_streams(PyObject* obj, size_t size) {\n-  if (obj == Py_None) {\n-    return std::vector<THCStream*>(size, nullptr);\n-  }\n-  auto streams = THPUtils_PySequence_to_THCStreamList(obj);\n-  if (streams.size() != size) {\n-    throw std::runtime_error(\"number of streams is not equal to number of inputs\");\n-  }\n-  return streams;\n-}\n-\n-static std::vector<ncclComm_t> unpack_comms(PyObject* obj, size_t size) {\n-  if (obj == Py_None) {\n-    return std::vector<ncclComm_t>();\n-  }\n-  std::vector<ncclComm_t> comms;\n-  if (PyCapsule_CheckExact(obj)) {\n-    comms = { unpack_nccl_comm(obj) };\n-  } else {\n-    auto seq = THPObjectPtr(PySequence_Fast(obj, \"comm is not a sequence\"));\n-    if (!seq) throw python_error();\n-    auto size = PySequence_Fast_GET_SIZE(seq.get());\n-    comms = std::vector<ncclComm_t>(size);\n-    for (int64_t i = 0; i < size; i++) {\n-      comms[i] = unpack_nccl_comm(PySequence_Fast_GET_ITEM(seq.get(), i));\n-    }\n-  }\n-  if (comms.size() != size) {\n-    throw std::runtime_error(\"number of communicators is not equal to number of inputs\");\n-  }\n-  return comms;\n-}\n-\n-PyObject * THCPModule_nccl_init_rank(PyObject *self, PyObject *args) {\n-  HANDLE_TH_ERRORS\n-  int nranks;\n-  const char* id;\n-  Py_ssize_t id_len;\n-  int rank;\n+void broadcast(const TensorList& tensors, const stream_list& streams, const comm_list& user_comms) {", "path": "torch/csrc/cuda/nccl.cpp", "position": null, "original_position": 229, "commit_id": "a297a5469f60513dcb937ad0a0826208448ad1dd", "original_commit_id": "3ba7c34f24812edb03eea38246fcef086f7be86d", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "`TensorList tensors`", "created_at": "2018-01-16T16:59:07Z", "updated_at": "2018-11-23T15:38:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/4443#discussion_r161821013", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4443", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/161821013"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4443#discussion_r161821013"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4443"}}, "body_html": "<p><code>TensorList tensors</code></p>", "body_text": "TensorList tensors"}