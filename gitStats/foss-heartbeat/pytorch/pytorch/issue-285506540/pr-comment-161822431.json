{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/161822431", "pull_request_review_id": 89169885, "id": 161822431, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MTgyMjQzMQ==", "diff_hunk": "@@ -4,70 +4,16 @@\n     _flatten_sparse_tensors, _unflatten_dense_tensors, \\\n     _unflatten_sparse_tensors, _reorder_tensors_as\n \n-\n-def broadcast(tensor, devices):\n-    \"\"\"Broadcasts a tensor to a number of GPUs.\n-\n-    Arguments:\n-        tensor (Tensor): tensor to broadcast.\n-        devices (Iterable): an iterable of devices among which to broadcast.\n-          Note that it should be like (src, dst1, dst2, ...), the first element\n-          of which is the source device to broadcast from.\n-\n-    Returns:\n-        A tuple containing copies of the ``tensor``, placed on devices\n-        corresponding to indices from ``devices``.\n-    \"\"\"\n-    tensors = [tensor]\n-    if nccl.is_available(tensors) and len(set(devices)) == len(devices):\n-        for device in devices[1:]:\n-            with torch.cuda.device(device):\n-                tensors.append(type(tensor)(tensor.size()))\n-        nccl.broadcast(tensors)\n-        return tuple(tensors)\n-\n-    return tuple(tensor.cuda(gpu, async=True) for gpu in devices)\n-\n-\n-def broadcast_coalesced(tensors, devices, buffer_size=10485760):\n-    \"\"\"Broadcasts a sequence tensors to the specified GPUs.\n-\n-    Small tensors are first coalesced into a buffer to reduce the number\n-    of synchronizations.\n-\n-    Arguments:\n-        tensors (sequence): tensors to broadcast.\n-        devices (Iterable): an iterable of devices among which to broadcast.\n-          Note that it should be like (src, dst1, dst2, ...), the first element\n-          of which is the source device to broadcast from.\n-        buffer_size (int): maximum size of the buffer used for coalescing\n-\n-    Returns:\n-        A tuple containing copies of the ``tensor``, placed on devices\n-        corresponding to indices from ``devices``.\n-    \"\"\"\n-    for tensor in tensors:\n-        if tensor.get_device() != devices[0]:\n-            raise RuntimeError('all tensors must be on devices[0]')\n-    outputs = [[] for _ in devices]\n-    # use the original tensors for the first device\n-    outputs[0].extend(tensors)\n-    for chunk in _take_tensors(tensors, buffer_size):\n-        if chunk[0].is_sparse:\n-            flat_indices, flat_values = _flatten_sparse_tensors(chunk)\n-            result_indices = broadcast(flat_indices, devices)\n-            result_values = broadcast(flat_values, devices)\n-            unflat_results = tuple(_unflatten_sparse_tensors(iv, chunk) for iv in zip(result_indices, result_values))\n-        else:\n-            flat = _flatten_dense_tensors(chunk)\n-            results = broadcast(flat, devices)\n-            unflat_results = tuple(_unflatten_dense_tensors(tensor, chunk) for tensor in results)\n-        # use the broadcasted tensors for the remaining devices\n-        for dst, unflat_res in zip(outputs[1:], unflat_results[1:]):\n-            dst.extend(unflat_res)\n-    for i, output in enumerate(outputs):\n-        outputs[i] = _reorder_tensors_as(output, tensors)\n-    return tuple(outputs)\n+try:\n+    from torch._C import _add_docstr as add_docstr\n+    from torch._C import _broadcast as broadcast\n+    from torch._C import _broadcast_coalesced as broadcast_coalesced\n+except ImportError:\n+    def broadcast(*args, **kwargs):", "path": "torch/cuda/comm.py", "position": null, "original_position": 73, "commit_id": "a297a5469f60513dcb937ad0a0826208448ad1dd", "original_commit_id": "3ba7c34f24812edb03eea38246fcef086f7be86d", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I'm not a fan of this pattern. Among other things it means that if you compile without NCCL support you won't have proper docs.\r\n\r\nJust define `def broadcast(tensor, devices):` and `broadcast_coalesced` in Python, with the doc strings and call `torch._C.broadcast` inside them.", "created_at": "2018-01-16T17:03:56Z", "updated_at": "2018-11-23T15:38:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/4443#discussion_r161822431", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4443", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/161822431"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4443#discussion_r161822431"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4443"}}, "body_html": "<p>I'm not a fan of this pattern. Among other things it means that if you compile without NCCL support you won't have proper docs.</p>\n<p>Just define <code>def broadcast(tensor, devices):</code> and <code>broadcast_coalesced</code> in Python, with the doc strings and call <code>torch._C.broadcast</code> inside them.</p>", "body_text": "I'm not a fan of this pattern. Among other things it means that if you compile without NCCL support you won't have proper docs.\nJust define def broadcast(tensor, devices): and broadcast_coalesced in Python, with the doc strings and call torch._C.broadcast inside them."}