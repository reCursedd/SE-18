{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159988883", "pull_request_review_id": 87034182, "id": 159988883, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1OTk4ODg4Mw==", "diff_hunk": "@@ -0,0 +1,18 @@\n+#include \"torch/csrc/utils/pybind.h\"\n+#include \"torch/csrc/cuda/comm.h\"\n+\n+#include <chrono>\n+\n+namespace torch { namespace cuda { namespace python {\n+\n+void initCommMethods(PyObject *module) {\n+  auto m = py::cast<py::module>(module);\n+  m.def(\"_broadcast_coalesced\", [](std::vector<at::Tensor>& tensors, std::vector<int64_t> devices, std::size_t buffer_size) {", "path": "torch/csrc/cuda/python_comm.cpp", "position": 10, "original_position": 10, "commit_id": "a297a5469f60513dcb937ad0a0826208448ad1dd", "original_commit_id": "1f392e8c78e427442cb37da78ba221ad2e2b830c", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "There are some examples in the template `torch/csrc/autograd/generated/python_variable_methods.cpp` and more uses in the generated version of that file.\r\n\r\n_broadcast_colaesced would looks like:\r\n\r\n```c++\r\n  HANDLE_TH_ERRORS\r\n  static PythonArgParser parser({\r\n    \"_broadcast_coalesced(TensorList tensors, IntList devices, int64_t buffer_size=10485760)\",\r\n  });\r\n  PyObject* parsed_args[3];\r\n  parser.parse(args, kwargs, parsed_args);\r\n  return wrap(dispatch_broadcast_coalesced(r.tensorlist(0), r.intlist(1), r.toInt64(2)));\r\n  END_HANDLE_TH_ERRORS\r\n```\r\n\r\n```c++\r\nstatic TensorList dispatch_broadcast_coalesced(TensorList tensors, IntList devices, int64_t buffer_size) {\r\n  AutoNoGIL no_gil;\r\n  return broadcast_coalesced(...);\r\n}\r\n```\r\n  \r\nAgain, I don't think the choice of binding is particularly important. It's just convenient that it could be written with PythonArgParser because that means that if we push NCCL down to ATen we can generate the Python bindings automatically.", "created_at": "2018-01-05T21:55:58Z", "updated_at": "2018-11-23T15:37:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/4443#discussion_r159988883", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4443", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159988883"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4443#discussion_r159988883"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4443"}}, "body_html": "<p>There are some examples in the template <code>torch/csrc/autograd/generated/python_variable_methods.cpp</code> and more uses in the generated version of that file.</p>\n<p>_broadcast_colaesced would looks like:</p>\n<div class=\"highlight highlight-source-c++\"><pre>  HANDLE_TH_ERRORS\n  <span class=\"pl-k\">static</span> PythonArgParser <span class=\"pl-en\">parser</span>({\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_broadcast_coalesced(TensorList tensors, IntList devices, int64_t buffer_size=10485760)<span class=\"pl-pds\">\"</span></span>,\n  });\n  PyObject* parsed_args[<span class=\"pl-c1\">3</span>];\n  parser.parse(args, kwargs, parsed_args);\n  <span class=\"pl-k\">return</span> wrap(dispatch_broadcast_coalesced(r.tensorlist(<span class=\"pl-c1\">0</span>), r.intlist(<span class=\"pl-c1\">1</span>), r.toInt64(<span class=\"pl-c1\">2</span>)));\n  END_HANDLE_TH_ERRORS</pre></div>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">static</span> TensorList <span class=\"pl-en\">dispatch_broadcast_coalesced</span>(TensorList tensors, IntList devices, <span class=\"pl-c1\">int64_t</span> buffer_size) {\n  AutoNoGIL no_gil;\n  <span class=\"pl-k\">return</span> <span class=\"pl-c1\">broadcast_coalesced</span>(...);\n}</pre></div>\n<p>Again, I don't think the choice of binding is particularly important. It's just convenient that it could be written with PythonArgParser because that means that if we push NCCL down to ATen we can generate the Python bindings automatically.</p>", "body_text": "There are some examples in the template torch/csrc/autograd/generated/python_variable_methods.cpp and more uses in the generated version of that file.\n_broadcast_colaesced would looks like:\n  HANDLE_TH_ERRORS\n  static PythonArgParser parser({\n    \"_broadcast_coalesced(TensorList tensors, IntList devices, int64_t buffer_size=10485760)\",\n  });\n  PyObject* parsed_args[3];\n  parser.parse(args, kwargs, parsed_args);\n  return wrap(dispatch_broadcast_coalesced(r.tensorlist(0), r.intlist(1), r.toInt64(2)));\n  END_HANDLE_TH_ERRORS\nstatic TensorList dispatch_broadcast_coalesced(TensorList tensors, IntList devices, int64_t buffer_size) {\n  AutoNoGIL no_gil;\n  return broadcast_coalesced(...);\n}\nAgain, I don't think the choice of binding is particularly important. It's just convenient that it could be written with PythonArgParser because that means that if we push NCCL down to ATen we can generate the Python bindings automatically.", "in_reply_to_id": 159961375}