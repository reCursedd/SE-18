{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/299908876", "html_url": "https://github.com/pytorch/pytorch/pull/1492#issuecomment-299908876", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1492", "id": 299908876, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTkwODg3Ng==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-08T15:56:02Z", "updated_at": "2017-05-08T15:56:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for bringing up what the default should be <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>.</p>\n<p>I don't think the argument is really \"To have correct broadcasting without introducing user bugs, we have to introduce squeezing dims on reduction\", but rather \"broadcasting is already going to break backwards compatibility, so let's pay the price once to get numpy-style semantics.\"</p>\n<p>To wit:</p>\n<ul>\n<li>Broadcasting already breaks backwards compatibility -- i.e. a (1,4) x (4,1) tensor op becomes a (4,4) with broadcasting, compared to a (1,4) as we have now.</li>\n<li>As mentioned above, automatically squeezing the dimensions is also backwards incompatible, but there are cases (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"193133052\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/289\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/289/hovercard?comment_id=298705616&amp;comment_type=issue_comment\" href=\"https://github.com/pytorch/pytorch/issues/289#issuecomment-298705616\">#289 (comment)</a>) where introducing it with broadcasting obviates the need for user-level changes compared to introducing broadcasting alone.  I don't have a good understanding for how prevalent this is, unfortunately.</li>\n<li>Getting close to numpy semantics (broadcasting) without going all the way (or most of the way, I'm sure we are missing some things) seems more confusing, i.e. if the difference between PyTorch and numpy semantics are totally different I can probably keep them straight, but if they are really close, it's actually more difficult.</li>\n<li>Removing the dimension by default is the consistent behavior: i.e. doing a reduction on a 1 dimension tensor without specify the dimension yields a scalar/0-dimensional tensor (torch.sum(x)) , but doing a reduction on a 1-dimensional tensor while specify the only dimensions yields a 1-dimensional tensor (torch.sum(x,0)).  Note that this requires <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"225700604\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1433\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1433/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1433\">#1433</a> to get totally consistent semantics.</li>\n<li>Making backwards incompatible changes are only going to get more difficult as usage of PyTorch increases and the code becomes more stable.  This, combined with us breaking backwards compatibility for broadcasting suggests this is the best time.</li>\n</ul>\n<p>That being said, I understand the other side of the argument; as a user, dealing with backwards incompatibilities is a real pain.</p>\n<p>One possibility is deferring the decision until broadcasting is ready.  It's not difficult to change the default at this point (now that I found the places that <em>need</em> keepdims=True), so I could change the default for now, we can get this in, and we can re-evaluate with broadcasting.  That may give us more information on how prevalent the case mentioned in (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"193133052\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/289\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/289/hovercard?comment_id=298705616&amp;comment_type=issue_comment\" href=\"https://github.com/pytorch/pytorch/issues/289#issuecomment-298705616\">#289 (comment)</a>) is.</p>\n<p>Thoughts?</p>", "body_text": "Thanks for bringing up what the default should be @apaszke and @soumith.\nI don't think the argument is really \"To have correct broadcasting without introducing user bugs, we have to introduce squeezing dims on reduction\", but rather \"broadcasting is already going to break backwards compatibility, so let's pay the price once to get numpy-style semantics.\"\nTo wit:\n\nBroadcasting already breaks backwards compatibility -- i.e. a (1,4) x (4,1) tensor op becomes a (4,4) with broadcasting, compared to a (1,4) as we have now.\nAs mentioned above, automatically squeezing the dimensions is also backwards incompatible, but there are cases (#289 (comment)) where introducing it with broadcasting obviates the need for user-level changes compared to introducing broadcasting alone.  I don't have a good understanding for how prevalent this is, unfortunately.\nGetting close to numpy semantics (broadcasting) without going all the way (or most of the way, I'm sure we are missing some things) seems more confusing, i.e. if the difference between PyTorch and numpy semantics are totally different I can probably keep them straight, but if they are really close, it's actually more difficult.\nRemoving the dimension by default is the consistent behavior: i.e. doing a reduction on a 1 dimension tensor without specify the dimension yields a scalar/0-dimensional tensor (torch.sum(x)) , but doing a reduction on a 1-dimensional tensor while specify the only dimensions yields a 1-dimensional tensor (torch.sum(x,0)).  Note that this requires #1433 to get totally consistent semantics.\nMaking backwards incompatible changes are only going to get more difficult as usage of PyTorch increases and the code becomes more stable.  This, combined with us breaking backwards compatibility for broadcasting suggests this is the best time.\n\nThat being said, I understand the other side of the argument; as a user, dealing with backwards incompatibilities is a real pain.\nOne possibility is deferring the decision until broadcasting is ready.  It's not difficult to change the default at this point (now that I found the places that need keepdims=True), so I could change the default for now, we can get this in, and we can re-evaluate with broadcasting.  That may give us more information on how prevalent the case mentioned in (#289 (comment)) is.\nThoughts?", "body": "Thanks for bringing up what the default should be @apaszke and @soumith.\r\n\r\nI don't think the argument is really \"To have correct broadcasting without introducing user bugs, we have to introduce squeezing dims on reduction\", but rather \"broadcasting is already going to break backwards compatibility, so let's pay the price once to get numpy-style semantics.\"\r\n\r\nTo wit:\r\n- Broadcasting already breaks backwards compatibility -- i.e. a (1,4) x (4,1) tensor op becomes a (4,4) with broadcasting, compared to a (1,4) as we have now.\r\n- As mentioned above, automatically squeezing the dimensions is also backwards incompatible, but there are cases (https://github.com/pytorch/pytorch/issues/289#issuecomment-298705616) where introducing it with broadcasting obviates the need for user-level changes compared to introducing broadcasting alone.  I don't have a good understanding for how prevalent this is, unfortunately.\r\n- Getting close to numpy semantics (broadcasting) without going all the way (or most of the way, I'm sure we are missing some things) seems more confusing, i.e. if the difference between PyTorch and numpy semantics are totally different I can probably keep them straight, but if they are really close, it's actually more difficult.\r\n- Removing the dimension by default is the consistent behavior: i.e. doing a reduction on a 1 dimension tensor without specify the dimension yields a scalar/0-dimensional tensor (torch.sum(x)) , but doing a reduction on a 1-dimensional tensor while specify the only dimensions yields a 1-dimensional tensor (torch.sum(x,0)).  Note that this requires https://github.com/pytorch/pytorch/issues/1433 to get totally consistent semantics.\r\n- Making backwards incompatible changes are only going to get more difficult as usage of PyTorch increases and the code becomes more stable.  This, combined with us breaking backwards compatibility for broadcasting suggests this is the best time.\r\n\r\nThat being said, I understand the other side of the argument; as a user, dealing with backwards incompatibilities is a real pain.\r\n\r\nOne possibility is deferring the decision until broadcasting is ready.  It's not difficult to change the default at this point (now that I found the places that _need_ keepdims=True), so I could change the default for now, we can get this in, and we can re-evaluate with broadcasting.  That may give us more information on how prevalent the case mentioned in (https://github.com/pytorch/pytorch/issues/289#issuecomment-298705616) is.\r\n\r\nThoughts?"}