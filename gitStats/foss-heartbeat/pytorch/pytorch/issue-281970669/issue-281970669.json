{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4166", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4166/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4166/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4166/events", "html_url": "https://github.com/pytorch/pytorch/issues/4166", "id": 281970669, "node_id": "MDU6SXNzdWUyODE5NzA2Njk=", "number": 4166, "title": "Can we use dynamic MaxPool1d over the max length of the input.", "user": {"login": "wabyking", "id": 9322285, "node_id": "MDQ6VXNlcjkzMjIyODU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9322285?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wabyking", "html_url": "https://github.com/wabyking", "followers_url": "https://api.github.com/users/wabyking/followers", "following_url": "https://api.github.com/users/wabyking/following{/other_user}", "gists_url": "https://api.github.com/users/wabyking/gists{/gist_id}", "starred_url": "https://api.github.com/users/wabyking/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wabyking/subscriptions", "organizations_url": "https://api.github.com/users/wabyking/orgs", "repos_url": "https://api.github.com/users/wabyking/repos", "events_url": "https://api.github.com/users/wabyking/events{/privacy}", "received_events_url": "https://api.github.com/users/wabyking/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-14T03:30:52Z", "updated_at": "2017-12-14T04:17:31Z", "closed_at": "2017-12-14T04:17:31Z", "author_association": "NONE", "body_html": "<p><em>MaxPool1d</em>  have a MUST-REQUIRED parameter named <em>kernel_size</em> \uff0c But when our input have different length in different batch. We should padding all the Batches with a fixed length to set a static size of <em>kernel_size</em>  before we feed our data to this models!</p>\n<pre><code>conv = nn.Sequential(\n            nn.Conv1d(in_channels = self.embedding_dim,\n                      out_channels = self.content_dim,\n                      kernel_size = self.kernel_size),\n            nn.ReLU(),\n            nn.MaxPool1d(**kernel_size = (self.max_seq_len - self.kernel_size + 1)**)\n        )\n</code></pre>\n<p>Instead, we may have a alternative method if we have a dynamic MaxPool1d without pre-set the max_seq_len.</p>\n<pre><code>conv = nn.Sequential(\n            nn.Conv1d(in_channels = self.embedding_dim,\n                      out_channels = self.content_dim,\n                      kernel_size = self.kernel_size),\n            nn.ReLU(),\n            nn.MaxPool1d()\n        )\n</code></pre>\n<p>We do not have any padding operation for my input. It means more efficient than before\uff01</p>", "body_text": "MaxPool1d  have a MUST-REQUIRED parameter named kernel_size \uff0c But when our input have different length in different batch. We should padding all the Batches with a fixed length to set a static size of kernel_size  before we feed our data to this models!\nconv = nn.Sequential(\n            nn.Conv1d(in_channels = self.embedding_dim,\n                      out_channels = self.content_dim,\n                      kernel_size = self.kernel_size),\n            nn.ReLU(),\n            nn.MaxPool1d(**kernel_size = (self.max_seq_len - self.kernel_size + 1)**)\n        )\n\nInstead, we may have a alternative method if we have a dynamic MaxPool1d without pre-set the max_seq_len.\nconv = nn.Sequential(\n            nn.Conv1d(in_channels = self.embedding_dim,\n                      out_channels = self.content_dim,\n                      kernel_size = self.kernel_size),\n            nn.ReLU(),\n            nn.MaxPool1d()\n        )\n\nWe do not have any padding operation for my input. It means more efficient than before\uff01", "body": "_MaxPool1d_  have a MUST-REQUIRED parameter named _kernel_size_ \uff0c But when our input have different length in different batch. We should padding all the Batches with a fixed length to set a static size of _kernel_size_  before we feed our data to this models!\r\n\r\n```\r\nconv = nn.Sequential(\r\n            nn.Conv1d(in_channels = self.embedding_dim,\r\n                      out_channels = self.content_dim,\r\n                      kernel_size = self.kernel_size),\r\n            nn.ReLU(),\r\n            nn.MaxPool1d(**kernel_size = (self.max_seq_len - self.kernel_size + 1)**)\r\n        )\r\n```\r\n\r\nInstead, we may have a alternative method if we have a dynamic MaxPool1d without pre-set the max_seq_len.\r\n```\r\nconv = nn.Sequential(\r\n            nn.Conv1d(in_channels = self.embedding_dim,\r\n                      out_channels = self.content_dim,\r\n                      kernel_size = self.kernel_size),\r\n            nn.ReLU(),\r\n            nn.MaxPool1d()\r\n        )\r\n```\r\n\r\nWe do not have any padding operation for my input. It means more efficient than before\uff01 \r\n "}