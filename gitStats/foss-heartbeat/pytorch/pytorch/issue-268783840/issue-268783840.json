{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3305", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3305/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3305/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3305/events", "html_url": "https://github.com/pytorch/pytorch/issues/3305", "id": 268783840, "node_id": "MDU6SXNzdWUyNjg3ODM4NDA=", "number": 3305, "title": "[feature request] Support SoftMax on bare Tensors", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-26T14:30:57Z", "updated_at": "2017-10-26T15:08:09Z", "closed_at": "2017-10-26T14:44:19Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre>torch.nn.functional.softmax(torch.rand(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>), <span class=\"pl-v\">dim</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Traceback (most recent call last):</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>  File \".../python2.7/site-packages/torch/nn/functional.py\", line 660, in softmax</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>    return torch._C._nn.softmax(input, dim)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>RuntimeError: softmax(): argument 'input' (position 1) must be Variable, not torch.FloatTensor</span>\n\ntorch.nn.functional.normalize(torch.rand(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>), <span class=\"pl-v\">dim</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> works</span></pre></div>\n<p>The workaround is easy, but it'd be nice to have. I guess this may work automagically when Variables and Tensors are unified?</p>", "body_text": "torch.nn.functional.softmax(torch.rand(5, 5), dim = 0)\n#Traceback (most recent call last):\n#  File \"<stdin>\", line 1, in <module>\n#  File \".../python2.7/site-packages/torch/nn/functional.py\", line 660, in softmax\n#    return torch._C._nn.softmax(input, dim)\n#RuntimeError: softmax(): argument 'input' (position 1) must be Variable, not torch.FloatTensor\n\ntorch.nn.functional.normalize(torch.rand(5, 5), dim = 0) # works\nThe workaround is easy, but it'd be nice to have. I guess this may work automagically when Variables and Tensors are unified?", "body": "```python\r\ntorch.nn.functional.softmax(torch.rand(5, 5), dim = 0)\r\n#Traceback (most recent call last):\r\n#  File \"<stdin>\", line 1, in <module>\r\n#  File \".../python2.7/site-packages/torch/nn/functional.py\", line 660, in softmax\r\n#    return torch._C._nn.softmax(input, dim)\r\n#RuntimeError: softmax(): argument 'input' (position 1) must be Variable, not torch.FloatTensor\r\n\r\ntorch.nn.functional.normalize(torch.rand(5, 5), dim = 0) # works\r\n```\r\n\r\nThe workaround is easy, but it'd be nice to have. I guess this may work automagically when Variables and Tensors are unified?"}