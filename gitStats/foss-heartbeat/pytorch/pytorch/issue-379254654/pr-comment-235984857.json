{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/235984857", "pull_request_review_id": 177979060, "id": 235984857, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNTk4NDg1Nw==", "diff_hunk": "@@ -0,0 +1,366 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/WrapDimUtilsMulti.h\"\n+\n+#ifdef USE_FBGEMM\n+#include \"fbgemm/Fbgemm.h\"\n+#endif // USE_FBGEMM\n+\n+#include <array>\n+#include <cctype>\n+#include <cmath>\n+#include <cstddef>\n+#include <sstream>\n+#include <string>\n+#include <vector>\n+\n+namespace at { namespace native {\n+\n+#ifdef USE_FBGEMM\n+namespace {\n+\n+void FindMinMax(const float *a, float* min, float* max, int len) {\n+  if (len <= 0) {\n+    *min = 0.0f;\n+    *max = 0.0f;\n+    return;\n+  }\n+\n+  float temp_min = *a, temp_max = *a;\n+  int i = 0;\n+\n+#ifdef __AVX__\n+  __m256 min_v = _mm256_set1_ps(*a), max_v = _mm256_set1_ps(*a);\n+  constexpr int VLEN = 8;\n+  if (len >= VLEN) {\n+    for ( ; i < len / VLEN * VLEN; i += VLEN) {\n+      min_v = _mm256_min_ps(min_v, _mm256_loadu_ps(a + i));\n+      max_v = _mm256_max_ps(max_v, _mm256_loadu_ps(a + i));\n+    }\n+\n+    float min_buf[VLEN], max_buf[VLEN];\n+    _mm256_storeu_ps(min_buf, min_v);\n+    _mm256_storeu_ps(max_buf, max_v);\n+    for (int j = 0; j < VLEN; ++j) {\n+      temp_min = std::min(temp_min, min_buf[j]);\n+      temp_max = std::max(temp_max, max_buf[j]);\n+    }\n+  }\n+#endif\n+\n+  for ( ; i < len; i++) {\n+    temp_min = std::min(temp_min, a[i]);\n+    temp_max = std::max(temp_max, a[i]);\n+  }\n+  *min = temp_min;\n+  *max = temp_max;\n+}\n+\n+struct TensorQuantizationParams {\n+  float scale;\n+  std::int32_t zero_point;\n+  int precision;\n+  float Min() const;\n+  float Max() const;\n+};\n+\n+TensorQuantizationParams ChooseQuantizationParams(\n+    float min, float max,\n+    int32_t qmin, int32_t qmax, bool preserve_sparsity) {\n+\n+  if (min < 0 && max > 0 && preserve_sparsity) {\n+    int symmetric_qmin = -((qmax - qmin) / 2 + 1);\n+    int symmetric_qmax = (qmax - qmin) / 2;\n+    double max_scale =\n+      std::max(fabs(min / symmetric_qmin), fabs(max / symmetric_qmax));\n+    min = max_scale * symmetric_qmin;\n+    max = max_scale * symmetric_qmax;\n+  }\n+\n+  double scale =\n+    (std::max(max, 0.f) - std::min(min, 0.f)) / ((double)qmax - qmin);\n+  if (scale == 0) scale = 0.1;\n+    // If scale is 0, we arbitrary adjust the scale to 0.1\n+  assert(scale > 0);\n+\n+  // We extend the [min, max] interval to ensure that it contains 0.\n+  // Otherwise, we would not meet the requirement that 0 be an exactly\n+  // representable value.\n+  min = std::min(min, 0.f);\n+  max = std::max(max, 0.f);\n+\n+  // if (force_scale_power_of_two_) {\n+  //   if (scale < 1) {\n+  //     scale = 1./(1 << (int)floor(std::log2(1/scale)));\n+  //   }\n+  //   else {\n+  //     scale = 1 << (int)ceil(std::log2(scale));\n+  //   }\n+  // }\n+\n+  // Zero-point computation.\n+  // First the initial floating-point computation. The zero-point can be\n+  // determined from solving an affine equation for any known pair\n+  // (real value, corresponding quantized value).\n+  // We know two such pairs: (rmin, qmin) and (rmax, qmax).\n+  // The arithmetic error on the zero point computed from either pair\n+  // will be roughly machine_epsilon * (sum of absolute values of terms)\n+  // so we want to use the variant that adds the smaller terms.\n+  double zero_point_from_min = qmin - min / scale;\n+  double zero_point_from_max = qmax - max / scale;\n+  double zero_point_from_min_error = std::abs(qmin) + std::abs(min / scale);\n+  double zero_point_from_max_error = std::abs(qmax) + std::abs(max / scale);\n+  double initial_zero_point =\n+    zero_point_from_min_error < zero_point_from_max_error\n+    ? zero_point_from_min : zero_point_from_max;\n+\n+  // for symmetric quantization (min == -max), we force zero_point to 128\n+  // to model signed integer (FIXME: this is a workaround that gemmlowp\n+  // doesn't support signed int AFAIK. Once we have an (efficient) gemm for\n+  // signed as well, we can just use signed int with zero_point = 0\n+  if (min < 0 && max > 0 && preserve_sparsity) {\n+    initial_zero_point = (qmin + qmax) / 2 + 1;\n+  }\n+\n+  // Now we need to nudge the zero point to be an integer\n+  // (our zero points are integer, and this is motivated by the requirement\n+  // to be able to represent the real value \"0\" exactly as a quantized value,\n+  // which is required in multiple places, for example in Im2col with SAME\n+  // padding).\n+  int32_t nudged_zero_point = 0;\n+  if (initial_zero_point < qmin) {\n+    nudged_zero_point = qmin;\n+  } else if (initial_zero_point > qmax) {\n+    nudged_zero_point = qmax;\n+  } else {\n+    nudged_zero_point = static_cast<int32_t>(std::round(initial_zero_point));\n+  }\n+\n+  TensorQuantizationParams result;\n+  result.scale = scale;\n+  result.zero_point = nudged_zero_point;\n+  return result;\n+}\n+\n+/// Clamp src in T1 to the desired precision and convert it to T2\n+template <typename T1, typename T2 = std::uint8_t>\n+T2 clamp(T1 src, int precision, bool is_signed = false)\n+// TODO: T26263653 fix signed-integer-overflow undefined behavior\n+#if defined(__has_feature)\n+#if __has_feature(__address_sanitizer__)\n+    __attribute__((__no_sanitize__(\"signed-integer-overflow\")))\n+#endif\n+#endif\n+{\n+  std::int32_t min = is_signed ? -(1LL << (precision - 1)) : 0;\n+  std::int32_t max =\n+    is_signed ? ((1LL << (precision - 1)) - 1) : (1LL << precision) - 1;\n+\n+  // Make sure T1 and T2 can represent the precision\n+  assert(min >= std::numeric_limits<T1>::lowest());\n+  assert(min >= std::numeric_limits<T2>::lowest());\n+  assert(max <= std::numeric_limits<T1>::max());\n+  assert(max <= std::numeric_limits<T2>::max());\n+\n+  return std::min<T1>(std::max<T1>(src, min), max);\n+}\n+\n+/// Quantize src using zero_point and scale, clamp to the specified precision,\n+/// and convert it to type T\n+template <typename T>\n+T Quantize(float src, std::int32_t zero_point, float scale,\n+           int result_precision,\n+           bool result_is_signed = std::is_signed<T>::value) {\n+  const float transformed_val = zero_point + src / scale;\n+  return clamp<std::int64_t, T>(\n+      (std::int64_t)std::round(transformed_val),\n+      result_precision, result_is_signed);\n+}\n+\n+template <typename T>\n+T Quantize(float src, const TensorQuantizationParams& qparams) {\n+  return Quantize<T>(\n+    src, qparams.zero_point, qparams.scale, qparams.precision);\n+}\n+\n+template <typename T>\n+void Quantize(\n+    const float* src,\n+    T* dst,\n+    int len,\n+    const TensorQuantizationParams& qparams) {\n+\n+#if defined(__AVX2__) && defined(__FMA__)\n+  caffe2::CpuId cpuid = caffe2::GetCpuId();\n+  bool avx2_support = cpuid.avx2();\n+  bool fma_support = cpuid.fma();\n+  if (avx2_support && fma_support && qparams.precision == 8 &&\n+      std::is_same<T, uint8_t>::value) {\n+    // fast path\n+    constexpr int VLEN = 8;\n+    std::size_t i = 0;\n+    __m256 inverse_scale_v = _mm256_set1_ps(1.f / qparams.scale);\n+    for (; i < len / VLEN * VLEN; i += VLEN) {\n+      __m256 src_v = _mm256_loadu_ps(src + i);\n+      __m256 transformed_v = _mm256_fmadd_ps(\n+          src_v, inverse_scale_v, _mm256_set1_ps(qparams.zero_point));\n+      __m256 clipped_v = _mm256_min_ps(\n+          _mm256_max_ps(transformed_v, _mm256_set1_ps(0.f)),\n+          _mm256_set1_ps(255.f));\n+      __m256i rounded_v = _mm256_cvtps_epi32(clipped_v);\n+      std::int32_t temp_int32[VLEN] __attribute__((aligned(64)));\n+      _mm256_store_si256((__m256i*)temp_int32, rounded_v);\n+      for (int j = 0; j < VLEN; ++j) {\n+        dst[i + j] = temp_int32[j];\n+      }\n+    }\n+\n+    for (; i < len; ++i) {\n+      float transformed = qparams.zero_point + src[i] / qparams.scale;\n+      float clipped = std::min(std::max(transformed, 0.f), 255.f);\n+      dst[i] = round(clipped);\n+    }\n+  } else\n+#endif\n+  {\n+    for (std::size_t i = 0; i < len; ++i) {\n+      dst[i] = Quantize<T>(src[i], qparams);\n+    }\n+  }\n+}\n+\n+}  // namespace\n+\n+Tensor linear_int8_weight(const Tensor& input, const Tensor& weight, Scalar weight_scale, Scalar weight_zero_point, const Tensor& bias) {\n+  // Dynamic dispatch for machines that don't support AVX2\n+  // Note that we assume `weight` is unquantized and scale and zero_point\n+  // are dont-care\n+  if (!fbgemm::fbgemmSupportedCPU()) {\n+    return at::native::linear(input, weight, bias);\n+  }\n+  auto *input_ptr = input.data<float>();\n+  auto *weight_ptr = (int8_t*)weight.data<uint8_t>();\n+\n+  AT_ASSERT(input.dim() == 2);\n+  AT_ASSERT(weight.dim() == 2);\n+  auto M = input.size(0), K = input.size(1);\n+  AT_ASSERT(K == weight.size(1));\n+  auto N = weight.size(0);\n+  AT_ASSERT(bias.dim() == 1);\n+  AT_ASSERT(bias.size(0) == N);\n+  AT_ASSERT(weight_scale.isFloatingPoint());\n+  AT_ASSERT(weight_zero_point.isIntegral());\n+\n+  float x_min, x_max;\n+  FindMinMax(input_ptr, &x_min, &x_max, input.numel());\n+\n+  static constexpr int precision = 8;\n+  static constexpr bool is_signed = false;\n+\n+  auto q_params = ChooseQuantizationParams(\n+    x_min,\n+    x_max,\n+    is_signed ? -(1 << (precision - 1)) : 0,\n+    is_signed ? ((1 << (precision - 1)) - 1) : (1 << precision) - 1,\n+    false);\n+\n+  q_params.precision = precision;\n+\n+  using namespace fbgemm;\n+\n+  std::vector<int32_t> row_offsets_, column_offsets_;\n+  column_offsets_.resize(M * N);\n+  row_offsets_.resize(\n+      PackAWithQuantRowOffset<uint8_t>::rowOffsetBufferSize());\n+  std::vector<uint8_t> X_pack_buf_;\n+  X_pack_buf_.resize(\n+      PackAWithQuantRowOffset<uint8_t>::packedBufferSize());\n+  PackAWithQuantRowOffset<uint8_t> packA(\n+      matrix_op_t::NoTranspose,\n+      M,\n+      K,\n+      input_ptr,\n+      K,\n+      X_pack_buf_.data(), // buffer for packed matrix\n+      q_params.scale,\n+      q_params.zero_point,\n+      1, // groups\n+      row_offsets_.data());\n+\n+  DoNothing<float, float> doNothingObj{};\n+  ReQuantizeForFloat<false /* FUSE_RELU*/> outputProcObj(\n+      doNothingObj,\n+      q_params.scale,\n+      weight_scale.to<double>(),\n+      q_params.zero_point,\n+      weight_zero_point.to<int64_t>(),\n+      packA.getRowOffsetBuffer(),\n+      column_offsets_.data(),\n+      bias.data<float>()); // bias\n+\n+  fbgemm::PackBMatrix<int8_t> packB(\n+    fbgemm::matrix_op_t::Transpose,\n+    K,\n+    N,\n+    weight_ptr,\n+    K,\n+    nullptr,\n+    1,\n+    weight_zero_point.to<int64_t>());\n+\n+  auto output = bias.to(at::kFloat).expand({M, N}).contiguous();\n+\n+  fbgemmPacked(\n+      packA,\n+      packB,\n+      output.data<float>(),\n+      (int32_t*)output.data<float>(),\n+      N,\n+      outputProcObj,\n+      0, // thread_id\n+      1); // num_threads\n+\n+  return output;\n+}\n+\n+std::tuple<Tensor, double, int64_t> linear_quantize_weight(const Tensor& input) {", "path": "aten/src/ATen/native/QuantizedLinear.cpp", "position": 326, "original_position": 326, "commit_id": "b284c1280e5dbfad8b3f221aa72bf482c4cf0f99", "original_commit_id": "b284c1280e5dbfad8b3f221aa72bf482c4cf0f99", "user": {"login": "harouwu", "id": 5057263, "node_id": "MDQ6VXNlcjUwNTcyNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/5057263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harouwu", "html_url": "https://github.com/harouwu", "followers_url": "https://api.github.com/users/harouwu/followers", "following_url": "https://api.github.com/users/harouwu/following{/other_user}", "gists_url": "https://api.github.com/users/harouwu/gists{/gist_id}", "starred_url": "https://api.github.com/users/harouwu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harouwu/subscriptions", "organizations_url": "https://api.github.com/users/harouwu/orgs", "repos_url": "https://api.github.com/users/harouwu/repos", "events_url": "https://api.github.com/users/harouwu/events{/privacy}", "received_events_url": "https://api.github.com/users/harouwu/received_events", "type": "User", "site_admin": false}, "body": "according to planning doc, we want to call it:\r\n```\r\ntorch.quantized_tensor(tensor, min, max, rounding, dtype, ..) \r\n```", "created_at": "2018-11-23T16:19:20Z", "updated_at": "2018-11-23T16:24:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/13777#discussion_r235984857", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13777", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/235984857"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13777#discussion_r235984857"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13777"}}, "body_html": "<p>according to planning doc, we want to call it:</p>\n<pre><code>torch.quantized_tensor(tensor, min, max, rounding, dtype, ..) \n</code></pre>", "body_text": "according to planning doc, we want to call it:\ntorch.quantized_tensor(tensor, min, max, rounding, dtype, ..)"}