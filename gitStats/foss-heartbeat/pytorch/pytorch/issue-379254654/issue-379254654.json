{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13777", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13777/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13777/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13777/events", "html_url": "https://github.com/pytorch/pytorch/pull/13777", "id": 379254654, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI5NzY2NDM2", "number": 13777, "title": "[WIP] Direct FBGEMM integraton into ATen", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-11-09T17:32:32Z", "updated_at": "2018-11-23T17:36:54Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/13777", "html_url": "https://github.com/pytorch/pytorch/pull/13777", "diff_url": "https://github.com/pytorch/pytorch/pull/13777.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/13777.patch"}, "body_html": "<p>This is a first attempt at integrating FBGEMM into ATen where we have fp32 inputs and outputs but int8 quantized weights.</p>\n<p>This is missing a bunch of stuff, but I'm putting this up to help with debugging. In particular we're missing:</p>\n<ul>\n<li>Caching of the packed weight matrix so we're not repacking on every iteration</li>\n<li>(will add more as I think of them)</li>\n</ul>", "body_text": "This is a first attempt at integrating FBGEMM into ATen where we have fp32 inputs and outputs but int8 quantized weights.\nThis is missing a bunch of stuff, but I'm putting this up to help with debugging. In particular we're missing:\n\nCaching of the packed weight matrix so we're not repacking on every iteration\n(will add more as I think of them)", "body": "This is a first attempt at integrating FBGEMM into ATen where we have fp32 inputs and outputs but int8 quantized weights. \r\n\r\nThis is missing a bunch of stuff, but I'm putting this up to help with debugging. In particular we're missing:\r\n\r\n* Caching of the packed weight matrix so we're not repacking on every iteration\r\n* (will add more as I think of them)\r\n"}