{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9646", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9646/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9646/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9646/events", "html_url": "https://github.com/pytorch/pytorch/issues/9646", "id": 343207895, "node_id": "MDU6SXNzdWUzNDMyMDc4OTU=", "number": 9646, "title": "Perf overhead of creating and destroying CUDA streams", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-20T18:36:02Z", "updated_at": "2018-09-08T17:10:23Z", "closed_at": "2018-09-08T17:10:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There is a cost to creating, retaining, and destroying CUDA streams in PyTorch master. In particular:</p>\n<ul>\n<li>Tracking CUDA streams requires atomic refcounting</li>\n<li>Destroying a CUDA stream can (rarely) cause implicit device synchronization</li>\n</ul>\n<p>The refcounting issue has been raised as a concern for expanding stream tracing to allow streaming backwards, for example, and it's clearly best to avoid implicit device synchronization as it causes an often unexpected performance degradation.</p>\n<p>For static frameworks the recommended best practice is to create all the needed streams upfront and destroy them after the work is done. This pattern is not immediately applicable to PyTorch, but a per device stream pool would achieve a similar effect. In particular, it would:</p>\n<ul>\n<li>Eliminate refcounting</li>\n<li>Move stream destruction to shutdown</li>\n<li>Allow for efficient stream tracing</li>\n<li>Keep the same interface</li>\n</ul>\n<p>These per-device pools could consist of the default stream, 32 low priority streams, and 32 high priority streams.</p>\n<p>Even with the same interface, a stream pool will cause some behavior changes:</p>\n<ul>\n<li>Users who are profiling networks that use many streams (like 33 non-default low priority streams) will see their profiles change to reflect only the actual 32 streams in the pool.</li>\n<li>Streams X and X + 32 will actually use the same cudaStream and will not run concurrently.</li>\n</ul>\n<p>While any change may sound worrying, I don't think either will be a problem in practice. When profiles involve so many streams reducing the number that appear may even be a perk, and if a network requires a long-lived stream then it can use the default stream or a high priority stream to avoid the latter issue. As long as streams are created and used as needed (something that a pool will make incredibly fast), the latter issue will not occur, and if a user needs even more control for some reason then they can easily implement their own user-level stream pool.</p>\n<p>I think a stream pool is a good way to get the benefits of allocating resources upfront and destroying them after while working in a dynamic environment as it will let us use streams freely with no performance concerns.</p>\n<p>I am creating this issue at the recommendation of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> for public review.</p>", "body_text": "There is a cost to creating, retaining, and destroying CUDA streams in PyTorch master. In particular:\n\nTracking CUDA streams requires atomic refcounting\nDestroying a CUDA stream can (rarely) cause implicit device synchronization\n\nThe refcounting issue has been raised as a concern for expanding stream tracing to allow streaming backwards, for example, and it's clearly best to avoid implicit device synchronization as it causes an often unexpected performance degradation.\nFor static frameworks the recommended best practice is to create all the needed streams upfront and destroy them after the work is done. This pattern is not immediately applicable to PyTorch, but a per device stream pool would achieve a similar effect. In particular, it would:\n\nEliminate refcounting\nMove stream destruction to shutdown\nAllow for efficient stream tracing\nKeep the same interface\n\nThese per-device pools could consist of the default stream, 32 low priority streams, and 32 high priority streams.\nEven with the same interface, a stream pool will cause some behavior changes:\n\nUsers who are profiling networks that use many streams (like 33 non-default low priority streams) will see their profiles change to reflect only the actual 32 streams in the pool.\nStreams X and X + 32 will actually use the same cudaStream and will not run concurrently.\n\nWhile any change may sound worrying, I don't think either will be a problem in practice. When profiles involve so many streams reducing the number that appear may even be a perk, and if a network requires a long-lived stream then it can use the default stream or a high priority stream to avoid the latter issue. As long as streams are created and used as needed (something that a pool will make incredibly fast), the latter issue will not occur, and if a user needs even more control for some reason then they can easily implement their own user-level stream pool.\nI think a stream pool is a good way to get the benefits of allocating resources upfront and destroying them after while working in a dynamic environment as it will let us use streams freely with no performance concerns.\nI am creating this issue at the recommendation of @colesbury for public review.", "body": "There is a cost to creating, retaining, and destroying CUDA streams in PyTorch master. In particular:\r\n\r\n- Tracking CUDA streams requires atomic refcounting\r\n- Destroying a CUDA stream can (rarely) cause implicit device synchronization\r\n\r\nThe refcounting issue has been raised as a concern for expanding stream tracing to allow streaming backwards, for example, and it's clearly best to avoid implicit device synchronization as it causes an often unexpected performance degradation.\r\n\r\nFor static frameworks the recommended best practice is to create all the needed streams upfront and destroy them after the work is done. This pattern is not immediately applicable to PyTorch, but a per device stream pool would achieve a similar effect. In particular, it would:\r\n\r\n- Eliminate refcounting\r\n- Move stream destruction to shutdown\r\n- Allow for efficient stream tracing\r\n- Keep the same interface\r\n\r\nThese per-device pools could consist of the default stream, 32 low priority streams, and 32 high priority streams.\r\n\r\nEven with the same interface, a stream pool will cause some behavior changes:\r\n\r\n- Users who are profiling networks that use many streams (like 33 non-default low priority streams) will see their profiles change to reflect only the actual 32 streams in the pool.\r\n- Streams X and X + 32 will actually use the same cudaStream and will not run concurrently. \r\n\r\nWhile any change may sound worrying, I don't think either will be a problem in practice. When profiles involve so many streams reducing the number that appear may even be a perk, and if a network requires a long-lived stream then it can use the default stream or a high priority stream to avoid the latter issue. As long as streams are created and used as needed (something that a pool will make incredibly fast), the latter issue will not occur, and if a user needs even more control for some reason then they can easily implement their own user-level stream pool.\r\n\r\nI think a stream pool is a good way to get the benefits of allocating resources upfront and destroying them after while working in a dynamic environment as it will let us use streams freely with no performance concerns. \r\n\r\nI am creating this issue at the recommendation of @colesbury for public review. "}