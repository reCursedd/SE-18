{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12297", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12297/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12297/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12297/events", "html_url": "https://github.com/pytorch/pytorch/issues/12297", "id": 366538884, "node_id": "MDU6SXNzdWUzNjY1Mzg4ODQ=", "number": 12297, "title": "[caffe2] Inference using multiple GPU", "user": {"login": "nanma56", "id": 19244104, "node_id": "MDQ6VXNlcjE5MjQ0MTA0", "avatar_url": "https://avatars1.githubusercontent.com/u/19244104?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nanma56", "html_url": "https://github.com/nanma56", "followers_url": "https://api.github.com/users/nanma56/followers", "following_url": "https://api.github.com/users/nanma56/following{/other_user}", "gists_url": "https://api.github.com/users/nanma56/gists{/gist_id}", "starred_url": "https://api.github.com/users/nanma56/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nanma56/subscriptions", "organizations_url": "https://api.github.com/users/nanma56/orgs", "repos_url": "https://api.github.com/users/nanma56/repos", "events_url": "https://api.github.com/users/nanma56/events{/privacy}", "received_events_url": "https://api.github.com/users/nanma56/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-03T21:34:49Z", "updated_at": "2018-10-03T21:34:50Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I would like to load the same model on different GPUs to process the images from different cameras in parallel. However, when I use the following code to force the GPU locations for the model, the model only lives on the GPU defined by the last run of the code.</p>\n<p>In this case, what is the right way to load the same model on different GPUs running in different process?</p>\n<p>Thank you</p>\n<pre><code># set the device options to CUDA\ndevice_opts1 = caffe2_pb2.DeviceOption()\ndevice_opts1.device_type = caffe2_pb2.CUDA\ndevice_opts1.cuda_gpu_id = 0\n\n# init net\ninit_def1 = caffe2_pb2.NetDef()\nwith open(INIT_NET, 'rb') as f:\n    init_def1.ParseFromString(f.read())\n    init_def1.device_option.CopyFrom(device_opts1)\n    workspace.RunNetOnce(init_def1)\n\n# create net\nnet_def1 = caffe2_pb2.NetDef()\nwith open(PREDICT_NET, 'rb') as f:\n    net_def1.ParseFromString(f.read())\n    net_def1.device_option.CopyFrom(device_opts1)\n    workspace.CreateNet(net_def1, overwrite=True)\n</code></pre>", "body_text": "I would like to load the same model on different GPUs to process the images from different cameras in parallel. However, when I use the following code to force the GPU locations for the model, the model only lives on the GPU defined by the last run of the code.\nIn this case, what is the right way to load the same model on different GPUs running in different process?\nThank you\n# set the device options to CUDA\ndevice_opts1 = caffe2_pb2.DeviceOption()\ndevice_opts1.device_type = caffe2_pb2.CUDA\ndevice_opts1.cuda_gpu_id = 0\n\n# init net\ninit_def1 = caffe2_pb2.NetDef()\nwith open(INIT_NET, 'rb') as f:\n    init_def1.ParseFromString(f.read())\n    init_def1.device_option.CopyFrom(device_opts1)\n    workspace.RunNetOnce(init_def1)\n\n# create net\nnet_def1 = caffe2_pb2.NetDef()\nwith open(PREDICT_NET, 'rb') as f:\n    net_def1.ParseFromString(f.read())\n    net_def1.device_option.CopyFrom(device_opts1)\n    workspace.CreateNet(net_def1, overwrite=True)", "body": "I would like to load the same model on different GPUs to process the images from different cameras in parallel. However, when I use the following code to force the GPU locations for the model, the model only lives on the GPU defined by the last run of the code. \r\n\r\nIn this case, what is the right way to load the same model on different GPUs running in different process?\r\n\r\nThank you\r\n\r\n```\r\n# set the device options to CUDA\r\ndevice_opts1 = caffe2_pb2.DeviceOption()\r\ndevice_opts1.device_type = caffe2_pb2.CUDA\r\ndevice_opts1.cuda_gpu_id = 0\r\n\r\n# init net\r\ninit_def1 = caffe2_pb2.NetDef()\r\nwith open(INIT_NET, 'rb') as f:\r\n    init_def1.ParseFromString(f.read())\r\n    init_def1.device_option.CopyFrom(device_opts1)\r\n    workspace.RunNetOnce(init_def1)\r\n\r\n# create net\r\nnet_def1 = caffe2_pb2.NetDef()\r\nwith open(PREDICT_NET, 'rb') as f:\r\n    net_def1.ParseFromString(f.read())\r\n    net_def1.device_option.CopyFrom(device_opts1)\r\n    workspace.CreateNet(net_def1, overwrite=True)\r\n```\r\n"}