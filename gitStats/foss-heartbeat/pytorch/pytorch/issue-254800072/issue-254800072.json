{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2598", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2598/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2598/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2598/events", "html_url": "https://github.com/pytorch/pytorch/issues/2598", "id": 254800072, "node_id": "MDU6SXNzdWUyNTQ4MDAwNzI=", "number": 2598, "title": "Upgrading from 0.1.9 to 0.2.0 silently breaks policy gradient code?", "user": {"login": "p-morais", "id": 3759402, "node_id": "MDQ6VXNlcjM3NTk0MDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3759402?v=4", "gravatar_id": "", "url": "https://api.github.com/users/p-morais", "html_url": "https://github.com/p-morais", "followers_url": "https://api.github.com/users/p-morais/followers", "following_url": "https://api.github.com/users/p-morais/following{/other_user}", "gists_url": "https://api.github.com/users/p-morais/gists{/gist_id}", "starred_url": "https://api.github.com/users/p-morais/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/p-morais/subscriptions", "organizations_url": "https://api.github.com/users/p-morais/orgs", "repos_url": "https://api.github.com/users/p-morais/repos", "events_url": "https://api.github.com/users/p-morais/events{/privacy}", "received_events_url": "https://api.github.com/users/p-morais/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-02T09:07:14Z", "updated_at": "2017-09-05T22:27:03Z", "closed_at": "2017-09-05T22:27:03Z", "author_association": "NONE", "body_html": "<p>I was implementing Proximal Policy Optimization when I noticed that my Pytorch version was outdated, so I updated. To my surprise, the code I was running which worked fine in 0.1.9 was completely broken in 0.2.0 (i.e. it ran without any errors or warnings, but the policy updates suddenly weren't improving the surrogate loss or expected reward at all).</p>\n<p>Here is a minimal example that reproduces the problem:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch.optim <span class=\"pl-k\">import</span> Adam\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> gym\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">GaussianMLP</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">obs_dim</span>, <span class=\"pl-smi\">action_dim</span>):\n\n        <span class=\"pl-c1\">super</span>(GaussianMLP, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.hidden_layer <span class=\"pl-k\">=</span> nn.Linear(obs_dim, <span class=\"pl-c1\">8</span>)\n\n        <span class=\"pl-c1\">self</span>.means <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">8</span>, action_dim)\n\n        <span class=\"pl-c1\">self</span>.log_stds <span class=\"pl-k\">=</span> nn.Parameter(\n            torch.ones(<span class=\"pl-c1\">1</span>, action_dim)\n        )\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> F.tanh(<span class=\"pl-c1\">self</span>.hidden_layer(x))\n\n        means <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.means(x)\n\n        log_stds <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.log_stds.expand_as(means)\n\n        stds <span class=\"pl-k\">=</span> torch.exp(log_stds)\n\n        <span class=\"pl-k\">return</span> means, log_stds, stds\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">log_likelihood</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">means</span>, <span class=\"pl-smi\">log_stds</span>, <span class=\"pl-smi\">stds</span>):\n        var <span class=\"pl-k\">=</span> stds.pow(<span class=\"pl-c1\">2</span>)\n\n        log_density <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>(x <span class=\"pl-k\">-</span> means).pow(<span class=\"pl-c1\">2</span>) <span class=\"pl-k\">/</span> (\n            <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> var) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">0.5</span> <span class=\"pl-k\">*</span> np.log(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> np.pi) <span class=\"pl-k\">-</span> log_stds\n\n        <span class=\"pl-k\">return</span> log_density.sum(<span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_action</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">means</span>, <span class=\"pl-smi\">stds</span>):\n        action <span class=\"pl-k\">=</span> torch.normal(means, stds)\n        <span class=\"pl-k\">return</span> action.detach()\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">n_itr</span>,\n          <span class=\"pl-smi\">n_trj</span>,\n          <span class=\"pl-smi\">max_trj_len</span>,\n          <span class=\"pl-smi\">env</span>,\n          <span class=\"pl-smi\">policy</span>,\n          <span class=\"pl-smi\">optimizer</span>,\n          <span class=\"pl-smi\">discount</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.99</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">rollout</span>(<span class=\"pl-smi\">env</span>, <span class=\"pl-smi\">policy</span>, <span class=\"pl-smi\">max_trj_len</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Collect a single rollout.<span class=\"pl-pds\">\"\"\"</span></span>\n        rewards <span class=\"pl-k\">=</span> []\n        observations <span class=\"pl-k\">=</span> []\n        actions <span class=\"pl-k\">=</span> []\n        returns <span class=\"pl-k\">=</span> []\n\n        obs <span class=\"pl-k\">=</span> env.reset().ravel()[<span class=\"pl-c1\">None</span>, :] <span class=\"pl-c\"><span class=\"pl-c\">#</span> reshape because of OpenAI gym inconsistencies</span>\n        <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(max_trj_len):\n            obs_var <span class=\"pl-k\">=</span> Variable(torch.Tensor(obs))\n\n            means, log_stds, stds <span class=\"pl-k\">=</span> policy(obs_var)\n            action <span class=\"pl-k\">=</span> policy.get_action(means, stds)\n\n            next_obs, reward, done, _ <span class=\"pl-k\">=</span> env.step(action.data.numpy().ravel())\n\n            observations.append(obs_var)\n            actions.append(action)\n            rewards.append(Variable(torch.Tensor([reward])))\n\n            obs <span class=\"pl-k\">=</span> next_obs.ravel()[<span class=\"pl-c1\">None</span>, :]\n\n            <span class=\"pl-k\">if</span> done:\n                <span class=\"pl-k\">break</span>\n\n        R <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">reversed</span>(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(rewards))):\n            R <span class=\"pl-k\">=</span> discount <span class=\"pl-k\">*</span> R <span class=\"pl-k\">+</span> rewards[i]\n            returns.append(R)\n\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">dict</span>(\n            <span class=\"pl-v\">rewards</span><span class=\"pl-k\">=</span>torch.stack(rewards),\n            <span class=\"pl-v\">returns</span><span class=\"pl-k\">=</span>torch.cat(returns[::<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]),\n            <span class=\"pl-v\">observations</span><span class=\"pl-k\">=</span>torch.cat(observations),\n            <span class=\"pl-v\">actions</span><span class=\"pl-k\">=</span>torch.cat(actions)\n        )\n\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n_itr):\n        paths <span class=\"pl-k\">=</span> [rollout(env, policy, max_trj_len) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n_trj)]\n\n        observations <span class=\"pl-k\">=</span> torch.cat([p[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>observations<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> paths])\n        actions <span class=\"pl-k\">=</span> torch.cat([p[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>actions<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> paths])\n        returns <span class=\"pl-k\">=</span> torch.cat([p[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>returns<span class=\"pl-pds\">\"</span></span>] <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> paths])\n\n        means, log_stds, stds <span class=\"pl-k\">=</span> policy(observations)\n\n        logprobs <span class=\"pl-k\">=</span> policy.log_likelihood(actions, means, log_stds, stds)\n\n        policy_loss <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>(logprobs <span class=\"pl-k\">*</span> returns).mean()\n\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        <span class=\"pl-c1\">print</span>(\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Average Return: <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span>\n            np.mean(([p[<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rewards<span class=\"pl-pds\">\"</span></span>].sum().data.numpy() <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> paths]))\n        )\n\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    env <span class=\"pl-k\">=</span> gym.make(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Hopper-v1<span class=\"pl-pds\">\"</span></span>)\n\n    obs_dim <span class=\"pl-k\">=</span> env.observation_space.shape[<span class=\"pl-c1\">0</span>]\n    action_dim <span class=\"pl-k\">=</span> env.action_space.shape[<span class=\"pl-c1\">0</span>]\n\n    policy <span class=\"pl-k\">=</span> GaussianMLP(obs_dim, action_dim)\n\n    train(\n        <span class=\"pl-v\">n_itr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>,\n        <span class=\"pl-v\">n_trj</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>,\n        <span class=\"pl-v\">max_trj_len</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>,\n        <span class=\"pl-v\">env</span><span class=\"pl-k\">=</span>env,\n        <span class=\"pl-v\">policy</span><span class=\"pl-k\">=</span>policy,\n        <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>Adam(policy.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\n    )</pre></div>\n<p>At least on my machine running the above code in 0.1.9 yields stable reward improvement, whereas in 0.2.0 the rewards stays the same, with minor fluctuations (indicating the gradient is wrong?). What is going on?</p>", "body_text": "I was implementing Proximal Policy Optimization when I noticed that my Pytorch version was outdated, so I updated. To my surprise, the code I was running which worked fine in 0.1.9 was completely broken in 0.2.0 (i.e. it ran without any errors or warnings, but the policy updates suddenly weren't improving the surrogate loss or expected reward at all).\nHere is a minimal example that reproduces the problem:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nimport torch.nn.functional as F\n\n\nimport numpy as np\nimport gym\n\nclass GaussianMLP(nn.Module):\n    def __init__(self, obs_dim, action_dim):\n\n        super(GaussianMLP, self).__init__()\n\n        self.hidden_layer = nn.Linear(obs_dim, 8)\n\n        self.means = nn.Linear(8, action_dim)\n\n        self.log_stds = nn.Parameter(\n            torch.ones(1, action_dim)\n        )\n\n    def forward(self, x):\n        x = F.tanh(self.hidden_layer(x))\n\n        means = self.means(x)\n\n        log_stds = self.log_stds.expand_as(means)\n\n        stds = torch.exp(log_stds)\n\n        return means, log_stds, stds\n\n    def log_likelihood(self, x, means, log_stds, stds):\n        var = stds.pow(2)\n\n        log_density = -(x - means).pow(2) / (\n            2 * var) - 0.5 * np.log(2 * np.pi) - log_stds\n\n        return log_density.sum(1)\n\n    def get_action(self, means, stds):\n        action = torch.normal(means, stds)\n        return action.detach()\n\n\ndef train(n_itr,\n          n_trj,\n          max_trj_len,\n          env,\n          policy,\n          optimizer,\n          discount=0.99):\n\n    def rollout(env, policy, max_trj_len):\n        \"\"\"Collect a single rollout.\"\"\"\n        rewards = []\n        observations = []\n        actions = []\n        returns = []\n\n        obs = env.reset().ravel()[None, :] # reshape because of OpenAI gym inconsistencies\n        for _ in range(max_trj_len):\n            obs_var = Variable(torch.Tensor(obs))\n\n            means, log_stds, stds = policy(obs_var)\n            action = policy.get_action(means, stds)\n\n            next_obs, reward, done, _ = env.step(action.data.numpy().ravel())\n\n            observations.append(obs_var)\n            actions.append(action)\n            rewards.append(Variable(torch.Tensor([reward])))\n\n            obs = next_obs.ravel()[None, :]\n\n            if done:\n                break\n\n        R = Variable(torch.zeros(1, 1))\n        for i in reversed(range(len(rewards))):\n            R = discount * R + rewards[i]\n            returns.append(R)\n\n        return dict(\n            rewards=torch.stack(rewards),\n            returns=torch.cat(returns[::-1]),\n            observations=torch.cat(observations),\n            actions=torch.cat(actions)\n        )\n\n    for _ in range(n_itr):\n        paths = [rollout(env, policy, max_trj_len) for _ in range(n_trj)]\n\n        observations = torch.cat([p[\"observations\"] for p in paths])\n        actions = torch.cat([p[\"actions\"] for p in paths])\n        returns = torch.cat([p[\"returns\"] for p in paths])\n\n        means, log_stds, stds = policy(observations)\n\n        logprobs = policy.log_likelihood(actions, means, log_stds, stds)\n\n        policy_loss = -(logprobs * returns).mean()\n\n        optimizer.zero_grad()\n        policy_loss.backward()\n        optimizer.step()\n\n        print(\n            'Average Return: %f' %\n            np.mean(([p[\"rewards\"].sum().data.numpy() for p in paths]))\n        )\n\n\n\nif __name__ == \"__main__\":\n    env = gym.make(\"Hopper-v1\")\n\n    obs_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n\n    policy = GaussianMLP(obs_dim, action_dim)\n\n    train(\n        n_itr=100,\n        n_trj=200,\n        max_trj_len=100,\n        env=env,\n        policy=policy,\n        optimizer=Adam(policy.parameters(), lr=0.01)\n    )\nAt least on my machine running the above code in 0.1.9 yields stable reward improvement, whereas in 0.2.0 the rewards stays the same, with minor fluctuations (indicating the gradient is wrong?). What is going on?", "body": "I was implementing Proximal Policy Optimization when I noticed that my Pytorch version was outdated, so I updated. To my surprise, the code I was running which worked fine in 0.1.9 was completely broken in 0.2.0 (i.e. it ran without any errors or warnings, but the policy updates suddenly weren't improving the surrogate loss or expected reward at all). \r\n\r\nHere is a minimal example that reproduces the problem:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nfrom torch.optim import Adam\r\nimport torch.nn.functional as F\r\n\r\n\r\nimport numpy as np\r\nimport gym\r\n\r\nclass GaussianMLP(nn.Module):\r\n    def __init__(self, obs_dim, action_dim):\r\n\r\n        super(GaussianMLP, self).__init__()\r\n\r\n        self.hidden_layer = nn.Linear(obs_dim, 8)\r\n\r\n        self.means = nn.Linear(8, action_dim)\r\n\r\n        self.log_stds = nn.Parameter(\r\n            torch.ones(1, action_dim)\r\n        )\r\n\r\n    def forward(self, x):\r\n        x = F.tanh(self.hidden_layer(x))\r\n\r\n        means = self.means(x)\r\n\r\n        log_stds = self.log_stds.expand_as(means)\r\n\r\n        stds = torch.exp(log_stds)\r\n\r\n        return means, log_stds, stds\r\n\r\n    def log_likelihood(self, x, means, log_stds, stds):\r\n        var = stds.pow(2)\r\n\r\n        log_density = -(x - means).pow(2) / (\r\n            2 * var) - 0.5 * np.log(2 * np.pi) - log_stds\r\n\r\n        return log_density.sum(1)\r\n\r\n    def get_action(self, means, stds):\r\n        action = torch.normal(means, stds)\r\n        return action.detach()\r\n\r\n\r\ndef train(n_itr,\r\n          n_trj,\r\n          max_trj_len,\r\n          env,\r\n          policy,\r\n          optimizer,\r\n          discount=0.99):\r\n\r\n    def rollout(env, policy, max_trj_len):\r\n        \"\"\"Collect a single rollout.\"\"\"\r\n        rewards = []\r\n        observations = []\r\n        actions = []\r\n        returns = []\r\n\r\n        obs = env.reset().ravel()[None, :] # reshape because of OpenAI gym inconsistencies\r\n        for _ in range(max_trj_len):\r\n            obs_var = Variable(torch.Tensor(obs))\r\n\r\n            means, log_stds, stds = policy(obs_var)\r\n            action = policy.get_action(means, stds)\r\n\r\n            next_obs, reward, done, _ = env.step(action.data.numpy().ravel())\r\n\r\n            observations.append(obs_var)\r\n            actions.append(action)\r\n            rewards.append(Variable(torch.Tensor([reward])))\r\n\r\n            obs = next_obs.ravel()[None, :]\r\n\r\n            if done:\r\n                break\r\n\r\n        R = Variable(torch.zeros(1, 1))\r\n        for i in reversed(range(len(rewards))):\r\n            R = discount * R + rewards[i]\r\n            returns.append(R)\r\n\r\n        return dict(\r\n            rewards=torch.stack(rewards),\r\n            returns=torch.cat(returns[::-1]),\r\n            observations=torch.cat(observations),\r\n            actions=torch.cat(actions)\r\n        )\r\n\r\n    for _ in range(n_itr):\r\n        paths = [rollout(env, policy, max_trj_len) for _ in range(n_trj)]\r\n\r\n        observations = torch.cat([p[\"observations\"] for p in paths])\r\n        actions = torch.cat([p[\"actions\"] for p in paths])\r\n        returns = torch.cat([p[\"returns\"] for p in paths])\r\n\r\n        means, log_stds, stds = policy(observations)\r\n\r\n        logprobs = policy.log_likelihood(actions, means, log_stds, stds)\r\n\r\n        policy_loss = -(logprobs * returns).mean()\r\n\r\n        optimizer.zero_grad()\r\n        policy_loss.backward()\r\n        optimizer.step()\r\n\r\n        print(\r\n            'Average Return: %f' %\r\n            np.mean(([p[\"rewards\"].sum().data.numpy() for p in paths]))\r\n        )\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    env = gym.make(\"Hopper-v1\")\r\n\r\n    obs_dim = env.observation_space.shape[0]\r\n    action_dim = env.action_space.shape[0]\r\n\r\n    policy = GaussianMLP(obs_dim, action_dim)\r\n\r\n    train(\r\n        n_itr=100,\r\n        n_trj=200,\r\n        max_trj_len=100,\r\n        env=env,\r\n        policy=policy,\r\n        optimizer=Adam(policy.parameters(), lr=0.01)\r\n    )\r\n```\r\nAt least on my machine running the above code in 0.1.9 yields stable reward improvement, whereas in 0.2.0 the rewards stays the same, with minor fluctuations (indicating the gradient is wrong?). What is going on?"}