{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/343935136", "html_url": "https://github.com/pytorch/pytorch/issues/3653#issuecomment-343935136", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3653", "id": 343935136, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MzkzNTEzNg==", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-13T14:28:55Z", "updated_at": "2017-11-13T20:45:18Z", "author_association": "NONE", "body_html": "<p>That's what I currently do.</p>\n<p>Though <code>B</code> will always be non-contiguous and an unnecessary copy of <code>B</code> will happen, since transpose must be applied first (instead of blas call with right transa, transb arguments):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bmm</span>(<span class=\"pl-smi\">A</span>, <span class=\"pl-smi\">B</span>):\n    <span class=\"pl-c1\">print</span>(A.is_contiguous(), B.is_contiguous())\n    <span class=\"pl-k\">return</span> torch.bmm(A.contiguous().view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, A.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>), A.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)), B.contiguous().view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, B.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>), B.size(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))).view(A.size()[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>] <span class=\"pl-k\">+</span> (<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, ))\n\na <span class=\"pl-k\">=</span> torch.Tensor(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>)\nbmm(a, a.transpose(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> prints (True, False)</span></pre></div>\n<p>If <code>view</code> is augmented to support the way <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> described better than I did, then maybe <code>.is_contiguous(dim = [0,1,2])</code> support also makes sense, so that the caller can find out if the tensor is contiguous in the given subspace of dimensions and <code>.contiguous(dim = [0,1,2])</code> to force it so.</p>", "body_text": "That's what I currently do.\nThough B will always be non-contiguous and an unnecessary copy of B will happen, since transpose must be applied first (instead of blas call with right transa, transb arguments):\nimport torch\ndef bmm(A, B):\n    print(A.is_contiguous(), B.is_contiguous())\n    return torch.bmm(A.contiguous().view(-1, A.size(-2), A.size(-1)), B.contiguous().view(-1, B.size(-2), B.size(-1))).view(A.size()[:-1] + (-1, ))\n\na = torch.Tensor(16, 8, 32, 64)\nbmm(a, a.transpose(-1, -2)) # prints (True, False)\nIf view is augmented to support the way @fmassa described better than I did, then maybe .is_contiguous(dim = [0,1,2]) support also makes sense, so that the caller can find out if the tensor is contiguous in the given subspace of dimensions and .contiguous(dim = [0,1,2]) to force it so.", "body": "That's what I currently do.\r\n\r\nThough `B` will always be non-contiguous and an unnecessary copy of `B` will happen, since transpose must be applied first (instead of blas call with right transa, transb arguments):\r\n\r\n```python\r\nimport torch\r\ndef bmm(A, B):\r\n    print(A.is_contiguous(), B.is_contiguous())\r\n    return torch.bmm(A.contiguous().view(-1, A.size(-2), A.size(-1)), B.contiguous().view(-1, B.size(-2), B.size(-1))).view(A.size()[:-1] + (-1, ))\r\n\r\na = torch.Tensor(16, 8, 32, 64)\r\nbmm(a, a.transpose(-1, -2)) # prints (True, False)\r\n```\r\n\r\nIf `view` is augmented to support the way @fmassa described better than I did, then maybe `.is_contiguous(dim = [0,1,2])` support also makes sense, so that the caller can find out if the tensor is contiguous in the given subspace of dimensions and `.contiguous(dim = [0,1,2])` to force it so."}