{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4746", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4746/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4746/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4746/events", "html_url": "https://github.com/pytorch/pytorch/pull/4746", "id": 290089877, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY0MDU1MDk2", "number": 4746, "title": "Heuristic-based autograd execution order", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-01-19T19:57:16Z", "updated_at": "2018-11-23T15:38:22Z", "closed_at": "2018-01-24T04:45:34Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4746", "html_url": "https://github.com/pytorch/pytorch/pull/4746", "diff_url": "https://github.com/pytorch/pytorch/pull/4746.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4746.patch"}, "body_html": "<p><strong>tl;dr</strong>: This PR implements the heuristic-based autograd execution order, where the tasks for each thread are ordered by the time the <code>Function</code> is created. <code>Function</code>s created later are executed earlier.</p>\n<p>The current breadth-first (BFS) order can cause huge memory usage in certain models. This was discovered using a real model that uses a <code>Linear</code> module multiple times in forward. After <code>Linear</code> is decomposed into <code>Transpose</code>+<code>Addmm</code> (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"239323099\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1935\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/1935/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/1935\">#1935</a>), the BFS orders all <code>TBackward</code> tasks at last to execute, causing huge amount of memory occupied by intermediate results.</p>\n<p>With help from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> , <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> , <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> , I have benchmarked three autograd execution orders:</p>\n<ol>\n<li>\n<p>BFS (the current scheme):<br>\nWithin a thread, tasks are fetched from a FIFO queue.<br>\nSample diff: None.</p>\n</li>\n<li>\n<p>DFS:<br>\nWithin a thread, tasks are fetched from a LIFO stack.<br>\nSample diff: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/SsnL/pytorch/commit/ac5a97dca0caef88ed0c2d00e9c4b90663a3f2cc/hovercard\" href=\"https://github.com/SsnL/pytorch/commit/ac5a97dca0caef88ed0c2d00e9c4b90663a3f2cc\">SsnL@<tt>ac5a97d</tt></a></p>\n</li>\n<li>\n<p>HEAP (Heuristic based):<br>\nWithin a thread, tasks are fetched from a max-heap, ordered by the time each autograd Function is created.<br>\nSample diff: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/SsnL/pytorch/commit/44d91b1ea11b0cda2487d9f0ef68706365aabd94/hovercard\" href=\"https://github.com/SsnL/pytorch/commit/44d91b1ea11b0cda2487d9f0ef68706365aabd94\">SsnL@<tt>44d91b1</tt></a></p>\n</li>\n</ol>\n<p>The benchmark code is applying the above diffs on master at <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/2dd7039b6badcc362fb6da62a33c49418acd5c5d/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/2dd7039b6badcc362fb6da62a33c49418acd5c5d\"><tt>2dd7039</tt></a>, with code from <a href=\"https://github.com/pytorch/pytorch/pull/4511\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4511/hovercard\">#4511 (Methods for checking CUDA memory usage)</a> manually added.</p>\n<p>The benchmarked tasks include: ImageNet on ResNet50, Open-NMT, word language model, CycleGAN, the mini-model with which we discovered the issue (*), and a model specifically crafted to make DFS and HEAP perform worse (**).</p>\n<p>The benchmark details and results can be found <a href=\"https://github.com/pytorch/pytorch/files/1647739/autograd.pdf\">here</a>. Roughly speaking, the performance for three approaches are similar, except on (*) and (**).</p>\n<p>Basing on the results and following discussions, we think it would be reasonable to switch to HEAP ordering, because:</p>\n<ol>\n<li>\n<p>There is no substantial slowdown in some common models from benchmark results.</p>\n</li>\n<li>\n<p>All three orders have edges cases that make them bad. But HEAP and DFS need a particular bad way of creating multi-device graph, which should be very uncommon. Yet BFS case can be encountered in real models, especially for models with many Linear layers (T+Addmm). HEAP generally performs slightly better than DFS in benchmark results.</p>\n</li>\n<li>\n<p>This is probably a weaker point. For BFS and DFS, the actual backward order depends on the order of operator args. For HEAP, it instead depends on the creation order of ops, which I think is easier to reason about. Although we probably shouldn't encourage user to tune the ordering, it will be helpful for us to fix some OOM models without releasing new binaries.</p>\n</li>\n</ol>\n<p>Furthermore, after benchmarking on a tiny CPU model, we don't see obvious overheads for maintaining the heap.</p>", "body_text": "tl;dr: This PR implements the heuristic-based autograd execution order, where the tasks for each thread are ordered by the time the Function is created. Functions created later are executed earlier.\nThe current breadth-first (BFS) order can cause huge memory usage in certain models. This was discovered using a real model that uses a Linear module multiple times in forward. After Linear is decomposed into Transpose+Addmm (#1935), the BFS orders all TBackward tasks at last to execute, causing huge amount of memory occupied by intermediate results.\nWith help from @ezyang , @apaszke , @zdevito and @colesbury , I have benchmarked three autograd execution orders:\n\n\nBFS (the current scheme):\nWithin a thread, tasks are fetched from a FIFO queue.\nSample diff: None.\n\n\nDFS:\nWithin a thread, tasks are fetched from a LIFO stack.\nSample diff: SsnL@ac5a97d\n\n\nHEAP (Heuristic based):\nWithin a thread, tasks are fetched from a max-heap, ordered by the time each autograd Function is created.\nSample diff: SsnL@44d91b1\n\n\nThe benchmark code is applying the above diffs on master at 2dd7039, with code from #4511 (Methods for checking CUDA memory usage) manually added.\nThe benchmarked tasks include: ImageNet on ResNet50, Open-NMT, word language model, CycleGAN, the mini-model with which we discovered the issue (*), and a model specifically crafted to make DFS and HEAP perform worse (**).\nThe benchmark details and results can be found here. Roughly speaking, the performance for three approaches are similar, except on (*) and (**).\nBasing on the results and following discussions, we think it would be reasonable to switch to HEAP ordering, because:\n\n\nThere is no substantial slowdown in some common models from benchmark results.\n\n\nAll three orders have edges cases that make them bad. But HEAP and DFS need a particular bad way of creating multi-device graph, which should be very uncommon. Yet BFS case can be encountered in real models, especially for models with many Linear layers (T+Addmm). HEAP generally performs slightly better than DFS in benchmark results.\n\n\nThis is probably a weaker point. For BFS and DFS, the actual backward order depends on the order of operator args. For HEAP, it instead depends on the creation order of ops, which I think is easier to reason about. Although we probably shouldn't encourage user to tune the ordering, it will be helpful for us to fix some OOM models without releasing new binaries.\n\n\nFurthermore, after benchmarking on a tiny CPU model, we don't see obvious overheads for maintaining the heap.", "body": "**tl;dr**: This PR implements the heuristic-based autograd execution order, where the tasks for each thread are ordered by the time the `Function` is created. `Function`s created later are executed earlier.\r\n\r\nThe current breadth-first (BFS) order can cause huge memory usage in certain models. This was discovered using a real model that uses a `Linear` module multiple times in forward. After `Linear` is decomposed into `Transpose`+`Addmm` (https://github.com/pytorch/pytorch/pull/1935), the BFS orders all `TBackward` tasks at last to execute, causing huge amount of memory occupied by intermediate results. \r\n\r\nWith help from @ezyang , @apaszke , @zdevito and @colesbury , I have benchmarked three autograd execution orders:\r\n\r\n1. BFS (the current scheme): \r\n  Within a thread, tasks are fetched from a FIFO queue.\r\n  Sample diff: None.\r\n\r\n2. DFS:\r\n  Within a thread, tasks are fetched from a LIFO stack.\r\n  Sample diff: https://github.com/SsnL/pytorch/commit/ac5a97dca0caef88ed0c2d00e9c4b90663a3f2cc\r\n\r\n3. HEAP (Heuristic based):\r\n  Within a thread, tasks are fetched from a max-heap, ordered by the time each autograd Function is created.\r\n  Sample diff: https://github.com/SsnL/pytorch/commit/44d91b1ea11b0cda2487d9f0ef68706365aabd94\r\n\r\nThe benchmark code is applying the above diffs on master at https://github.com/pytorch/pytorch/commit/2dd7039b6badcc362fb6da62a33c49418acd5c5d, with code from [#4511 (Methods for checking CUDA memory usage)](https://github.com/pytorch/pytorch/pull/4511) manually added.\r\n\r\nThe benchmarked tasks include: ImageNet on ResNet50, Open-NMT, word language model, CycleGAN, the mini-model with which we discovered the issue (*), and a model specifically crafted to make DFS and HEAP perform worse (**).\r\n\r\nThe benchmark details and results can be found [here](https://github.com/pytorch/pytorch/files/1647739/autograd.pdf). Roughly speaking, the performance for three approaches are similar, except on (*) and (**).\r\n\r\nBasing on the results and following discussions, we think it would be reasonable to switch to HEAP ordering, because:\r\n\r\n1. There is no substantial slowdown in some common models from benchmark results.\r\n\r\n2. All three orders have edges cases that make them bad. But HEAP and DFS need a particular bad way of creating multi-device graph, which should be very uncommon. Yet BFS case can be encountered in real models, especially for models with many Linear layers (T+Addmm). HEAP generally performs slightly better than DFS in benchmark results.\r\n\r\n3. This is probably a weaker point. For BFS and DFS, the actual backward order depends on the order of operator args. For HEAP, it instead depends on the creation order of ops, which I think is easier to reason about. Although we probably shouldn't encourage user to tune the ordering, it will be helpful for us to fix some OOM models without releasing new binaries.\r\n\r\nFurthermore, after benchmarking on a tiny CPU model, we don't see obvious overheads for maintaining the heap."}