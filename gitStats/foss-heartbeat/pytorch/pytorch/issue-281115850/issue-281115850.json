{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4116", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4116/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4116/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4116/events", "html_url": "https://github.com/pytorch/pytorch/pull/4116", "id": 281115850, "node_id": "MDExOlB1bGxSZXF1ZXN0MTU3NjMzOTc5", "number": 4116, "title": "Convolution derivatives in ATen", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-12-11T18:08:42Z", "updated_at": "2018-11-23T15:37:34Z", "closed_at": "2017-12-20T19:19:27Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4116", "html_url": "https://github.com/pytorch/pytorch/pull/4116", "diff_url": "https://github.com/pytorch/pytorch/pull/4116.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4116.patch"}, "body_html": "<p>This PR introduces ATen implementation of convolution, which dispatches to THNN/CuDNN/nnpack based on input parameters. The general strategy is to compose this function out of the various forward-backward pairs of specific implementations, rather than write a monolithic function with backwards (which is what we did before because the boilerplate of doing it otherwise would have been very high.) The new API provides the following functions:</p>\n<ul>\n<li><code>_convolution</code>, which is a fully generic, native convolution implementation that dispatches to various other convolution implementations depending on input characteristics. This is prefixed with an underscore because it explicitly takes <code>benchmark</code>, <code>deterministic</code> and <code>cudnn_enabled</code> which are implementation details for CuDNN. The intent is to eventually provide a <code>convolution</code> that reads these parameters out of the context using <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"280824647\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4104\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4104/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4104\">#4104</a>.</li>\n<li><code>_convolution_nogroup</code> is a convolution implementation for non-CuDNN algorithms which don't support group convolution natively.</li>\n<li><code>_convolution_double_backward</code> is the generic double-backwards implementation for convolution.</li>\n</ul>\n<p>This depends on <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"280661552\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4096\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4096/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4096\">#4096</a> (commits included).</p>\n<p>In more detail:</p>\n<ul>\n<li>Most functionality from <code>torch/csrc/autograd/functions/convolution.cpp</code> has been moved into <code>aten/src/ATen/native/Convolution.cpp</code></li>\n<li>We continue to make use of <code>ConvParams</code>, but we now construct the parameters upon entry to a function from the function signature (which does not use <code>ConvParams</code>; having <code>convolution</code> take <code>ConvParams</code> directly would require teaching the code generator how to accept these as parameters, complicating ATen's API model) and destruct them when making subprocedure calls.</li>\n<li>I introduce a new idiom, <code>input_r</code>, which represents a <code>const Tensor&amp;</code> reference, which will subsequently be assigned to a local <code>Tensor input</code>. This is helpful because a lot of the existing algorithms relied on being able to assign to locals, which is not permitted with a const reference.</li>\n<li>The native argument parser now supports <code>std::array&lt;bool,2&gt;</code> inputs (NB: there MUST NOT be a space; this is the same hack as is applied to derivatives.yaml)</li>\n<li>Native parser now supports <code>Tensor?</code> arguments, which indicates a nullable tensor. Previously this function was only used by NN methods.</li>\n<li>Documentation updates on THNN library</li>\n<li>I added an extra <code>fgradInput</code> argument to <code>VolumetricConvolutionMM_updateOutput</code> and <code>VolumetricConvolutionMM_accGradParameters</code> so that its buffer list lines up with the backward argument list. This makes it possible to write derivative for <code>conv2d</code> which previously was not supported (commented out in <code>derivatives.yaml</code>)</li>\n<li>Extra <code>double_backward</code> declarations for all convolution backwards functions was added.</li>\n<li>Python argument parsing and <code>make_variable</code> augmented to handle undefined.</li>\n</ul>\n<p>TODO:</p>\n<ul>\n<li>I bet NNPack doesn't work correctly</li>\n</ul>", "body_text": "This PR introduces ATen implementation of convolution, which dispatches to THNN/CuDNN/nnpack based on input parameters. The general strategy is to compose this function out of the various forward-backward pairs of specific implementations, rather than write a monolithic function with backwards (which is what we did before because the boilerplate of doing it otherwise would have been very high.) The new API provides the following functions:\n\n_convolution, which is a fully generic, native convolution implementation that dispatches to various other convolution implementations depending on input characteristics. This is prefixed with an underscore because it explicitly takes benchmark, deterministic and cudnn_enabled which are implementation details for CuDNN. The intent is to eventually provide a convolution that reads these parameters out of the context using #4104.\n_convolution_nogroup is a convolution implementation for non-CuDNN algorithms which don't support group convolution natively.\n_convolution_double_backward is the generic double-backwards implementation for convolution.\n\nThis depends on #4096 (commits included).\nIn more detail:\n\nMost functionality from torch/csrc/autograd/functions/convolution.cpp has been moved into aten/src/ATen/native/Convolution.cpp\nWe continue to make use of ConvParams, but we now construct the parameters upon entry to a function from the function signature (which does not use ConvParams; having convolution take ConvParams directly would require teaching the code generator how to accept these as parameters, complicating ATen's API model) and destruct them when making subprocedure calls.\nI introduce a new idiom, input_r, which represents a const Tensor& reference, which will subsequently be assigned to a local Tensor input. This is helpful because a lot of the existing algorithms relied on being able to assign to locals, which is not permitted with a const reference.\nThe native argument parser now supports std::array<bool,2> inputs (NB: there MUST NOT be a space; this is the same hack as is applied to derivatives.yaml)\nNative parser now supports Tensor? arguments, which indicates a nullable tensor. Previously this function was only used by NN methods.\nDocumentation updates on THNN library\nI added an extra fgradInput argument to VolumetricConvolutionMM_updateOutput and VolumetricConvolutionMM_accGradParameters so that its buffer list lines up with the backward argument list. This makes it possible to write derivative for conv2d which previously was not supported (commented out in derivatives.yaml)\nExtra double_backward declarations for all convolution backwards functions was added.\nPython argument parsing and make_variable augmented to handle undefined.\n\nTODO:\n\nI bet NNPack doesn't work correctly", "body": "This PR introduces ATen implementation of convolution, which dispatches to THNN/CuDNN/nnpack based on input parameters. The general strategy is to compose this function out of the various forward-backward pairs of specific implementations, rather than write a monolithic function with backwards (which is what we did before because the boilerplate of doing it otherwise would have been very high.) The new API provides the following functions:\r\n\r\n- `_convolution`, which is a fully generic, native convolution implementation that dispatches to various other convolution implementations depending on input characteristics. This is prefixed with an underscore because it explicitly takes `benchmark`, `deterministic` and `cudnn_enabled` which are implementation details for CuDNN. The intent is to eventually provide a `convolution` that reads these parameters out of the context using #4104.\r\n- `_convolution_nogroup` is a convolution implementation for non-CuDNN algorithms which don't support group convolution natively.\r\n- `_convolution_double_backward` is the generic double-backwards implementation for convolution.\r\n\r\nThis depends on #4096 (commits included).\r\n\r\nIn more detail:\r\n\r\n- Most functionality from `torch/csrc/autograd/functions/convolution.cpp` has been moved into `aten/src/ATen/native/Convolution.cpp`\r\n- We continue to make use of `ConvParams`, but we now construct the parameters upon entry to a function from the function signature (which does not use `ConvParams`; having `convolution` take `ConvParams` directly would require teaching the code generator how to accept these as parameters, complicating ATen's API model) and destruct them when making subprocedure calls.\r\n- I introduce a new idiom, `input_r`, which represents a `const Tensor&` reference, which will subsequently be assigned to a local `Tensor input`. This is helpful because a lot of the existing algorithms relied on being able to assign to locals, which is not permitted with a const reference.\r\n- The native argument parser now supports `std::array<bool,2>` inputs (NB: there MUST NOT be a space; this is the same hack as is applied to derivatives.yaml)\r\n- Native parser now supports `Tensor?` arguments, which indicates a nullable tensor. Previously this function was only used by NN methods.\r\n- Documentation updates on THNN library\r\n- I added an extra `fgradInput` argument to `VolumetricConvolutionMM_updateOutput` and `VolumetricConvolutionMM_accGradParameters` so that its buffer list lines up with the backward argument list. This makes it possible to write derivative for `conv2d` which previously was not supported (commented out in `derivatives.yaml`)\r\n- Extra `double_backward` declarations for all convolution backwards functions was added.\r\n- Python argument parsing and `make_variable` augmented to handle undefined.\r\n\r\nTODO:\r\n- I bet NNPack doesn't work correctly"}