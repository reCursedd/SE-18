{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/299670431", "html_url": "https://github.com/pytorch/pytorch/issues/1499#issuecomment-299670431", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1499", "id": 299670431, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTY3MDQzMQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-06T22:44:40Z", "updated_at": "2017-05-06T22:44:40Z", "author_association": "MEMBER", "body_html": "<p>This looks ok for me - the reason why you're seeing that is because parameters for all ResNet clones are permanently stored on the first device, but are broadcast to the second one only when DataParallel is is forward, and are freed immediately afterwards. Since you only run at most a single forward at a time, the memory usage on the first GPU scales nearly linearly with the number of models (storage for parameters), while usage on the second one should stay constant (storage for a single copy of parameters).</p>", "body_text": "This looks ok for me - the reason why you're seeing that is because parameters for all ResNet clones are permanently stored on the first device, but are broadcast to the second one only when DataParallel is is forward, and are freed immediately afterwards. Since you only run at most a single forward at a time, the memory usage on the first GPU scales nearly linearly with the number of models (storage for parameters), while usage on the second one should stay constant (storage for a single copy of parameters).", "body": "This looks ok for me - the reason why you're seeing that is because parameters for all ResNet clones are permanently stored on the first device, but are broadcast to the second one only when DataParallel is is forward, and are freed immediately afterwards. Since you only run at most a single forward at a time, the memory usage on the first GPU scales nearly linearly with the number of models (storage for parameters), while usage on the second one should stay constant (storage for a single copy of parameters)."}