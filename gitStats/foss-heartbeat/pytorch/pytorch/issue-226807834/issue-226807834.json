{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1499", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1499/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1499/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1499/events", "html_url": "https://github.com/pytorch/pytorch/issues/1499", "id": 226807834, "node_id": "MDU6SXNzdWUyMjY4MDc4MzQ=", "number": 1499, "title": "DataParallel does not effectively use GPU RAM on device_id 1", "user": {"login": "elistevens", "id": 138016, "node_id": "MDQ6VXNlcjEzODAxNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/138016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elistevens", "html_url": "https://github.com/elistevens", "followers_url": "https://api.github.com/users/elistevens/followers", "following_url": "https://api.github.com/users/elistevens/following{/other_user}", "gists_url": "https://api.github.com/users/elistevens/gists{/gist_id}", "starred_url": "https://api.github.com/users/elistevens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elistevens/subscriptions", "organizations_url": "https://api.github.com/users/elistevens/orgs", "repos_url": "https://api.github.com/users/elistevens/repos", "events_url": "https://api.github.com/users/elistevens/events{/privacy}", "received_events_url": "https://api.github.com/users/elistevens/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-05-06T22:09:26Z", "updated_at": "2018-08-18T23:37:02Z", "closed_at": "2017-05-06T22:44:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Example program here: <a href=\"https://gist.github.com/elistevens/d20debf24b82d57b06913cfa34da2662\">https://gist.github.com/elistevens/d20debf24b82d57b06913cfa34da2662</a></p>\n<p>Note that the GPU 1 RAM used does not significantly change with calling <code>loss_var.backward()</code> with any combination of <code>0</code> or <code>1</code> <code>Variables</code>, nor with increasing the number of models.</p>\n<p>I don't think the problem is explicitly with <code>.backward()</code> since the problem still happens just by calling <code>model.forward()</code> (in the <code>... in {None}</code> cases from the gist).</p>\n<p>The end result is that training bottlenecks on GPU 0 RAM usage, even though there is free GPU RAM on the other device(s).</p>\n<p>I'm using:<br>\n<a href=\"http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp35-cp35m-linux_x86_64.whl\" rel=\"nofollow\">http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp35-cp35m-linux_x86_64.whl</a><br>\nUbuntu 16.04<br>\nnvidia-375 from the graphics-drivers PPA<br>\n2x GTX 1080</p>", "body_text": "Example program here: https://gist.github.com/elistevens/d20debf24b82d57b06913cfa34da2662\nNote that the GPU 1 RAM used does not significantly change with calling loss_var.backward() with any combination of 0 or 1 Variables, nor with increasing the number of models.\nI don't think the problem is explicitly with .backward() since the problem still happens just by calling model.forward() (in the ... in {None} cases from the gist).\nThe end result is that training bottlenecks on GPU 0 RAM usage, even though there is free GPU RAM on the other device(s).\nI'm using:\nhttp://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp35-cp35m-linux_x86_64.whl\nUbuntu 16.04\nnvidia-375 from the graphics-drivers PPA\n2x GTX 1080", "body": "Example program here: https://gist.github.com/elistevens/d20debf24b82d57b06913cfa34da2662\r\n\r\nNote that the GPU 1 RAM used does not significantly change with calling `loss_var.backward()` with any combination of `0` or `1` `Variables`, nor with increasing the number of models.\r\n\r\nI don't think the problem is explicitly with `.backward()` since the problem still happens just by calling `model.forward()` (in the `... in {None}` cases from the gist).\r\n\r\nThe end result is that training bottlenecks on GPU 0 RAM usage, even though there is free GPU RAM on the other device(s).\r\n\r\nI'm using:\r\nhttp://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp35-cp35m-linux_x86_64.whl\r\nUbuntu 16.04\r\nnvidia-375 from the graphics-drivers PPA\r\n2x GTX 1080\r\n"}