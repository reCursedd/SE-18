{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/299672228", "html_url": "https://github.com/pytorch/pytorch/issues/1499#issuecomment-299672228", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1499", "id": 299672228, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTY3MjIyOA==", "user": {"login": "elistevens", "id": 138016, "node_id": "MDQ6VXNlcjEzODAxNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/138016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elistevens", "html_url": "https://github.com/elistevens", "followers_url": "https://api.github.com/users/elistevens/followers", "following_url": "https://api.github.com/users/elistevens/following{/other_user}", "gists_url": "https://api.github.com/users/elistevens/gists{/gist_id}", "starred_url": "https://api.github.com/users/elistevens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elistevens/subscriptions", "organizations_url": "https://api.github.com/users/elistevens/orgs", "repos_url": "https://api.github.com/users/elistevens/repos", "events_url": "https://api.github.com/users/elistevens/events{/privacy}", "received_events_url": "https://api.github.com/users/elistevens/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-06T23:30:23Z", "updated_at": "2017-05-06T23:30:38Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>parameters for all ResNet clones are permanently stored on the first device</p>\n</blockquote>\n<p>Right, this is the issue. I want to be able to tell DataParallel to use a different device (or keep the model cpu in pinned memory or something, whatever doesn't bottle neck on one device's RAM).</p>\n<p>I'm guessing that all of the models being on GPU 0 is also why calling <code>.backward()</code> also only seems to use GPU 0 RAM.</p>", "body_text": "parameters for all ResNet clones are permanently stored on the first device\n\nRight, this is the issue. I want to be able to tell DataParallel to use a different device (or keep the model cpu in pinned memory or something, whatever doesn't bottle neck on one device's RAM).\nI'm guessing that all of the models being on GPU 0 is also why calling .backward() also only seems to use GPU 0 RAM.", "body": "> parameters for all ResNet clones are permanently stored on the first device\r\n\r\nRight, this is the issue. I want to be able to tell DataParallel to use a different device (or keep the model cpu in pinned memory or something, whatever doesn't bottle neck on one device's RAM).\r\n\r\nI'm guessing that all of the models being on GPU 0 is also why calling `.backward()` also only seems to use GPU 0 RAM."}