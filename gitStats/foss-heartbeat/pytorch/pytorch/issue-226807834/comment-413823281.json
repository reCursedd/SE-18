{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413823281", "html_url": "https://github.com/pytorch/pytorch/issues/1499#issuecomment-413823281", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1499", "id": 413823281, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzgyMzI4MQ==", "user": {"login": "xu-ji", "id": 2847967, "node_id": "MDQ6VXNlcjI4NDc5Njc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2847967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xu-ji", "html_url": "https://github.com/xu-ji", "followers_url": "https://api.github.com/users/xu-ji/followers", "following_url": "https://api.github.com/users/xu-ji/following{/other_user}", "gists_url": "https://api.github.com/users/xu-ji/gists{/gist_id}", "starred_url": "https://api.github.com/users/xu-ji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xu-ji/subscriptions", "organizations_url": "https://api.github.com/users/xu-ji/orgs", "repos_url": "https://api.github.com/users/xu-ji/repos", "events_url": "https://api.github.com/users/xu-ji/events{/privacy}", "received_events_url": "https://api.github.com/users/xu-ji/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-17T10:21:17Z", "updated_at": "2018-08-17T10:21:17Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Is there an update on this? I'm also having issues with the unevenness of device utilisation with DataParallel. This behaviour actually seems quite limiting because regardless of how many GPU devices you have, the batch size seems to be bottlenecked not by total memory, but the memory of the first device with wasted space in the others.</p>\n<p>For example, there's about 14GB of memory I can't use here, though I'd like to be able to use it to increase the batch size:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2847967/44261250-fc3e8e00-a20e-11e8-8974-2e9009c9a994.png\"><img src=\"https://user-images.githubusercontent.com/2847967/44261250-fc3e8e00-a20e-11e8-8974-2e9009c9a994.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>", "body_text": "@apaszke Is there an update on this? I'm also having issues with the unevenness of device utilisation with DataParallel. This behaviour actually seems quite limiting because regardless of how many GPU devices you have, the batch size seems to be bottlenecked not by total memory, but the memory of the first device with wasted space in the others.\nFor example, there's about 14GB of memory I can't use here, though I'd like to be able to use it to increase the batch size:", "body": "@apaszke Is there an update on this? I'm also having issues with the unevenness of device utilisation with DataParallel. This behaviour actually seems quite limiting because regardless of how many GPU devices you have, the batch size seems to be bottlenecked not by total memory, but the memory of the first device with wasted space in the others.\r\n\r\nFor example, there's about 14GB of memory I can't use here, though I'd like to be able to use it to increase the batch size:\r\n![image](https://user-images.githubusercontent.com/2847967/44261250-fc3e8e00-a20e-11e8-8974-2e9009c9a994.png)\r\n\r\n\r\n\r\n\r\n\r\n"}