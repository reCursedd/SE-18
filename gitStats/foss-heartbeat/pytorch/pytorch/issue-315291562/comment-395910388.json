{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/395910388", "html_url": "https://github.com/pytorch/pytorch/pull/6688#issuecomment-395910388", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6688", "id": 395910388, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NTkxMDM4OA==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-08T22:35:41Z", "updated_at": "2018-06-08T22:35:41Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Cool! I\u2019ll do another pass tomorrow.\n\nAbout your K40, out of curiosity, do you see memory occupied in nvidia-smi?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Jun 8, 2018 at 18:33 Chintak Sheth ***@***.***&gt; wrote:\n Benchmark with sync\n\n Following offline discussion with <a class=\"user-mention\" href=\"https://github.com/zou3519\">@zou3519</a> &lt;<a href=\"https://github.com/zou3519\">https://github.com/zou3519</a>&gt;,\n there was an error with my earlier benchmarking, in that, I wasn't\n explicitly synchronizing the GPU after torch.bincount call. This has been\n now fixed. Here is the benchmark.py &lt;<a href=\"https://pastebin.com/pci0nQtv\">https://pastebin.com/pci0nQtv</a>&gt; file\n used.\n CPU\n input size time\n 100K 0.0055\n 1M 0.052\n 10M 0.52 Shared memory usage\n input size 10 100 500 1000 2500 5000 7500 10000\n 100K 0.0135 0.0007 0.0006 0.0005 0.0004 0.0004 NA NA\n 1M 0.0013 0.0073 0.0048 0.003 0.0014 0.0016 NA NA\n 10M 0.0087 0.0826 0.0454 0.0268 0.0101 0.0125 NA NA Multi Block memory\n usage\n input size 10 100 500 1000 2500 5000 7500 10000\n 100K 0.0148 0.0006 0.0007 0.0006 0.0005 0.0006 0.0007 0.0008\n 1M 0.004 0.004 0.0049 0.0028 0.0018 0.0023 0.003 0.0038\n 10M 0.0315 0.0308 0.0421 0.0246 0.0134 0.0183 0.0248 0.032 Global memory\n usage\n input size 10 100 500 1000 2500 5000 7500 10000\n 100K 0.1677 0.005 0.0008 0.0004 0.0003 0.0002 0.0002 0.0002\n 1M 1.8555 0.0625 0.0071 0.0022 0.0011 0.0007 0.0006 0.0006\n 10M 18.5349 0.6453 0.0694 0.019 0.0086 0.0048 0.0038 0.0038 Conclusion\n\n    1. For small number of bins, 10-100, shared memory implementation\n    performs the best.\n    2. For the intermediate range of bins, 100-500, multi-block memory\n    implementation wins over the other three.\n    3. For larger number of bins, 1000+, global memory implementation\n    performs the best.\n\n cc <a class=\"user-mention\" href=\"https://github.com/zou3519\">@zou3519</a> &lt;<a href=\"https://github.com/zou3519\">https://github.com/zou3519</a>&gt;, <a class=\"user-mention\" href=\"https://github.com/SsnL\">@SsnL</a> &lt;<a href=\"https://github.com/SsnL\">https://github.com/SsnL</a>&gt;\n\n P.S.:\n\n    1. All time reported in sec.\n    2. Experiments performed on Tesla M40 (12 Gb) (K40 seems to be having\n    a bad day - out of memory errors even though nothing was running).\n\n \u2014\n You are receiving this because you were mentioned.\n\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"315291562\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6688\" href=\"https://github.com/pytorch/pytorch/pull/6688#issuecomment-395909908\">#6688 (comment)</a>&gt;, or mute\n the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AFaWZZwu0DQPpfZrBMjjagfNQYIqE7ZUks5t6vutgaJpZM4TZQmp\">https://github.com/notifications/unsubscribe-auth/AFaWZZwu0DQPpfZrBMjjagfNQYIqE7ZUks5t6vutgaJpZM4TZQmp</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Cool! I\u2019ll do another pass tomorrow.\n\nAbout your K40, out of curiosity, do you see memory occupied in nvidia-smi?\n\u2026\nOn Fri, Jun 8, 2018 at 18:33 Chintak Sheth ***@***.***> wrote:\n Benchmark with sync\n\n Following offline discussion with @zou3519 <https://github.com/zou3519>,\n there was an error with my earlier benchmarking, in that, I wasn't\n explicitly synchronizing the GPU after torch.bincount call. This has been\n now fixed. Here is the benchmark.py <https://pastebin.com/pci0nQtv> file\n used.\n CPU\n input size time\n 100K 0.0055\n 1M 0.052\n 10M 0.52 Shared memory usage\n input size 10 100 500 1000 2500 5000 7500 10000\n 100K 0.0135 0.0007 0.0006 0.0005 0.0004 0.0004 NA NA\n 1M 0.0013 0.0073 0.0048 0.003 0.0014 0.0016 NA NA\n 10M 0.0087 0.0826 0.0454 0.0268 0.0101 0.0125 NA NA Multi Block memory\n usage\n input size 10 100 500 1000 2500 5000 7500 10000\n 100K 0.0148 0.0006 0.0007 0.0006 0.0005 0.0006 0.0007 0.0008\n 1M 0.004 0.004 0.0049 0.0028 0.0018 0.0023 0.003 0.0038\n 10M 0.0315 0.0308 0.0421 0.0246 0.0134 0.0183 0.0248 0.032 Global memory\n usage\n input size 10 100 500 1000 2500 5000 7500 10000\n 100K 0.1677 0.005 0.0008 0.0004 0.0003 0.0002 0.0002 0.0002\n 1M 1.8555 0.0625 0.0071 0.0022 0.0011 0.0007 0.0006 0.0006\n 10M 18.5349 0.6453 0.0694 0.019 0.0086 0.0048 0.0038 0.0038 Conclusion\n\n    1. For small number of bins, 10-100, shared memory implementation\n    performs the best.\n    2. For the intermediate range of bins, 100-500, multi-block memory\n    implementation wins over the other three.\n    3. For larger number of bins, 1000+, global memory implementation\n    performs the best.\n\n cc @zou3519 <https://github.com/zou3519>, @SsnL <https://github.com/SsnL>\n\n P.S.:\n\n    1. All time reported in sec.\n    2. Experiments performed on Tesla M40 (12 Gb) (K40 seems to be having\n    a bad day - out of memory errors even though nothing was running).\n\n \u2014\n You are receiving this because you were mentioned.\n\n Reply to this email directly, view it on GitHub\n <#6688 (comment)>, or mute\n the thread\n <https://github.com/notifications/unsubscribe-auth/AFaWZZwu0DQPpfZrBMjjagfNQYIqE7ZUks5t6vutgaJpZM4TZQmp>\n .", "body": "Cool! I\u2019ll do another pass tomorrow.\n\nAbout your K40, out of curiosity, do you see memory occupied in nvidia-smi?\n\nOn Fri, Jun 8, 2018 at 18:33 Chintak Sheth <notifications@github.com> wrote:\n\n> Benchmark with sync\n>\n> Following offline discussion with @zou3519 <https://github.com/zou3519>,\n> there was an error with my earlier benchmarking, in that, I wasn't\n> explicitly synchronizing the GPU after torch.bincount call. This has been\n> now fixed. Here is the benchmark.py <https://pastebin.com/pci0nQtv> file\n> used.\n> CPU\n> input size time\n> 100K 0.0055\n> 1M 0.052\n> 10M 0.52 Shared memory usage\n> input size 10 100 500 1000 2500 5000 7500 10000\n> 100K 0.0135 0.0007 0.0006 0.0005 0.0004 0.0004 NA NA\n> 1M 0.0013 0.0073 0.0048 0.003 0.0014 0.0016 NA NA\n> 10M 0.0087 0.0826 0.0454 0.0268 0.0101 0.0125 NA NA Multi Block memory\n> usage\n> input size 10 100 500 1000 2500 5000 7500 10000\n> 100K 0.0148 0.0006 0.0007 0.0006 0.0005 0.0006 0.0007 0.0008\n> 1M 0.004 0.004 0.0049 0.0028 0.0018 0.0023 0.003 0.0038\n> 10M 0.0315 0.0308 0.0421 0.0246 0.0134 0.0183 0.0248 0.032 Global memory\n> usage\n> input size 10 100 500 1000 2500 5000 7500 10000\n> 100K 0.1677 0.005 0.0008 0.0004 0.0003 0.0002 0.0002 0.0002\n> 1M 1.8555 0.0625 0.0071 0.0022 0.0011 0.0007 0.0006 0.0006\n> 10M 18.5349 0.6453 0.0694 0.019 0.0086 0.0048 0.0038 0.0038 Conclusion\n>\n>    1. For small number of bins, 10-100, shared memory implementation\n>    performs the best.\n>    2. For the intermediate range of bins, 100-500, multi-block memory\n>    implementation wins over the other three.\n>    3. For larger number of bins, 1000+, global memory implementation\n>    performs the best.\n>\n> cc @zou3519 <https://github.com/zou3519>, @SsnL <https://github.com/SsnL>\n>\n> P.S.:\n>\n>    1. All time reported in sec.\n>    2. Experiments performed on Tesla M40 (12 Gb) (K40 seems to be having\n>    a bad day - out of memory errors even though nothing was running).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n>\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/pull/6688#issuecomment-395909908>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/AFaWZZwu0DQPpfZrBMjjagfNQYIqE7ZUks5t6vutgaJpZM4TZQmp>\n> .\n>\n"}