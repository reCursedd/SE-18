{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383254607", "html_url": "https://github.com/pytorch/pytorch/pull/6688#issuecomment-383254607", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6688", "id": 383254607, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzI1NDYwNw==", "user": {"login": "chintak", "id": 3398558, "node_id": "MDQ6VXNlcjMzOTg1NTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3398558?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chintak", "html_url": "https://github.com/chintak", "followers_url": "https://api.github.com/users/chintak/followers", "following_url": "https://api.github.com/users/chintak/following{/other_user}", "gists_url": "https://api.github.com/users/chintak/gists{/gist_id}", "starred_url": "https://api.github.com/users/chintak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chintak/subscriptions", "organizations_url": "https://api.github.com/users/chintak/orgs", "repos_url": "https://api.github.com/users/chintak/repos", "events_url": "https://api.github.com/users/chintak/events{/privacy}", "received_events_url": "https://api.github.com/users/chintak/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-21T00:42:29Z", "updated_at": "2018-04-21T00:44:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm trying to implement the CUDA version. I see <code>at::cuda::CUDA_tensor_apply2</code>, which requires input and output size to be equal since indexing is 1:1 <code>a[i] &lt;-&gt; b[i]</code>(see <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/CUDAApplyUtils.cuh#L152\">CUDAApplyUtils.cuh#L152</a>).</p>\n<p>For implementing bincount, however, I need more flexibility in terms of indexing. Specifically, I need to be able to do <code>atomicAdd(&amp;a[b[i]], 1)</code>. Further, I want to create an intermediary 2D device tensor which can hold partial histogram per block, followed by a reduceDim with sum op to get the final histogram. (<a href=\"https://gist.github.com/chintak/239602d79bf425e1a790e35ca3a787fd#file-histogram-cu-L48-L67\">reference vanilla CUDA implementation</a>)</p>\n<p>Are there wrappers for doing similar kernel launch? Any pointers to how I can create such a wrapper?<br>\nOr any other approaches to calculating bincount?</p>", "body_text": "I'm trying to implement the CUDA version. I see at::cuda::CUDA_tensor_apply2, which requires input and output size to be equal since indexing is 1:1 a[i] <-> b[i](see CUDAApplyUtils.cuh#L152).\nFor implementing bincount, however, I need more flexibility in terms of indexing. Specifically, I need to be able to do atomicAdd(&a[b[i]], 1). Further, I want to create an intermediary 2D device tensor which can hold partial histogram per block, followed by a reduceDim with sum op to get the final histogram. (reference vanilla CUDA implementation)\nAre there wrappers for doing similar kernel launch? Any pointers to how I can create such a wrapper?\nOr any other approaches to calculating bincount?", "body": "I'm trying to implement the CUDA version. I see ``at::cuda::CUDA_tensor_apply2``, which requires input and output size to be equal since indexing is 1:1 ``a[i] <-> b[i]``(see [CUDAApplyUtils.cuh#L152](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cuda/CUDAApplyUtils.cuh#L152)).\r\n\r\nFor implementing bincount, however, I need more flexibility in terms of indexing. Specifically, I need to be able to do ``atomicAdd(&a[b[i]], 1)``. Further, I want to create an intermediary 2D device tensor which can hold partial histogram per block, followed by a reduceDim with sum op to get the final histogram. ([reference vanilla CUDA implementation](https://gist.github.com/chintak/239602d79bf425e1a790e35ca3a787fd#file-histogram-cu-L48-L67))\r\n\r\nAre there wrappers for doing similar kernel launch? Any pointers to how I can create such a wrapper? \r\nOr any other approaches to calculating bincount?"}