{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/184095421", "pull_request_review_id": 115220489, "id": 184095421, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NDA5NTQyMQ==", "diff_hunk": "@@ -955,5 +956,199 @@ bool CUDA_tensor_apply4(at::Tensor a,\n   return true;\n }\n \n+/*\n+  Memory types used for the 3 histogram implementations.\n+  See `CUDA_tensor_histogram` below.\n+ */\n+enum class CUDAHistogramMemoryType { MULTI_BLOCK, SHARED, GLOBAL };\n+\n+/*\n+  Kernel for computing the histogram of the input.\n+ */\n+template <\n+    typename scalar1,\n+    typename scalar2,\n+    typename IndexType,\n+    CUDAHistogramMemoryType MemoryType = CUDAHistogramMemoryType::MULTI_BLOCK,\n+    typename Op>\n+__global__ void kernelHistogram1D(\n+    detail::TensorInfo<scalar1, IndexType> a, /* output */\n+    detail::TensorInfo<scalar2, IndexType> b, /* input */\n+    int binsize,\n+    IndexType totalElements,\n+    Op getWeightOp) {\n+  extern __shared__ unsigned char my_smem[];\n+  scalar1* smem = nullptr;\n+  if (MemoryType == CUDAHistogramMemoryType::SHARED) {\n+    smem = reinterpret_cast<scalar1*>(my_smem);\n+    for (size_t i = 0; i < a.sizes[1]; i++) {\n+      smem[i] = 0;\n+    }\n+    __syncthreads();\n+  }\n+\n+  for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+    // Convert `linearIndex` into an offset of `b`\n+    const IndexType bOffset =\n+        detail::IndexToOffset<scalar2, IndexType, 1>::get(linearIndex, b);\n+\n+    if (MemoryType == CUDAHistogramMemoryType::SHARED) {\n+      // use shared memory to accummulate\n+      // Use value at `b` as an offset of `a`\n+      const IndexType aOffset = b.data[bOffset] / binsize;\n+\n+      atomicAdd(&smem[aOffset], getWeightOp(linearIndex));\n+    } else if (MemoryType == CUDAHistogramMemoryType::MULTI_BLOCK) {\n+      // Use value at `b` as an offset of `a`\n+      auto bVal = b.data[bOffset];\n+      const IndexType aIdx = a.sizes[1] * blockIdx.x + bVal / binsize;\n+      const IndexType aOffset =\n+          detail::IndexToOffset<scalar1, IndexType, 2>::get(aIdx, a);\n+\n+      atomicAdd(&a.data[aOffset], getWeightOp(linearIndex));\n+    } else { // MemoryType == CUDAHistogramMemoryType::GLOBAL\n+      // Use value at `b` as an offset of `a`\n+      const IndexType aIdx = b.data[bOffset] / binsize;\n+      const IndexType aOffset =\n+          detail::IndexToOffset<scalar1, IndexType, 2>::get(aIdx, a);\n+\n+      atomicAdd(&a.data[aOffset], getWeightOp(linearIndex));\n+    }\n+  }\n+  if (MemoryType == CUDAHistogramMemoryType::SHARED) {\n+    __syncthreads();\n+    // move the bin values into global device memory d_counts\n+    if (threadIdx.x == 0) {\n+      const IndexType aIdx = a.sizes[1] * blockIdx.x;\n+      const IndexType aRowOffset =\n+          detail::IndexToOffset<scalar1, IndexType, 2>::get(aIdx, a);\n+      for (size_t i = 0; i < a.sizes[1]; i++) {\n+        a.data[aRowOffset + i] = smem[i];\n+      }\n+    }\n+  }\n+}\n+\n+#define MAX_NUMBER_BINS_WITH_SHARED_MEM 5000\n+\n+/*\n+  Calculate the frequesncy of the input values.\n+\n+  `a` contains the final output or the histogram. Input `b` is assumed to\n+  be 1-D non-negative int array. `c` optionally contains the weight vector.\n+  See `help torch.bincount` for details on the math.\n+\n+  3 implementations based of input size and memory usage:\n+    MULTI_BLOCK: `partial_output` temp tensor with (#blocks X #bins) shape.\n+        Each block atomically adds to it's own **global** hist copy:\n+        `partial_output[i, :]`. Once all blocks are done, the partial output is\n+        sum-reduced to give the final output with (#bins) shape.\n+    SHARED: `partial_output` temp tensor with (#blocks X #bins) shape.\n+        Each block atomically adds to it's own **shared** hist copy: `smem[:]`.\n+        Copies it's local histogram to `partial_output[i, :]` followed by\n+        sum-reduction to yield the final output.\n+    GLOBAL: if #bins > MAX_NUMBER_BINS_WITH_SHARED_MEM. In this case, only a\n+        single **global** copy of the output histogram is maintained and all\n+        blocks/threads atomically update bin counts. Hence, this can be slow.\n+        CPU version may perform better.\n+ */\n+template <typename scalar1, typename scalar2>\n+bool CUDA_tensor_histogram(\n+    at::Tensor a, /* output */\n+    at::Tensor b, /* input */\n+    at::Tensor c, /* weights(optional) */\n+    int64_t nbins,\n+    int binsize,\n+    TensorArgType aType = TensorArgType::ReadWrite,\n+    TensorArgType bType = TensorArgType::ReadOnly,\n+    TensorArgType cType = TensorArgType::ReadOnly) {\n+  checkBackend(\"CUDA_tensor_histogram\", {a, b}, Backend::CUDA);\n+  constexpr int has_weights = !std::is_integral<scalar1>::value;\n+  if (has_weights) {\n+    checkBackend(\"CUDA_tensor_histogram\", {c}, Backend::CUDA);\n+  }\n+  int64_t totalElements = b.numel();\n+\n+  const dim3 block = getApplyBlock();\n+  dim3 grid;\n+  if (!getApplyGrid(totalElements, grid)) {\n+    return false;\n+  }\n+#if CUDA_VERSION < 9000\n+  grid.x = std::min(\n+      (unsigned int)at::globalContext()\n+              .getCurrentDeviceProperties()\n+              ->multiProcessorCount *\n+          AT_APPLY_BLOCKS_PER_SM,\n+      grid.x);\n+#endif\n+\n+  Tensor partial_output;\n+  if (nbins < MAX_NUMBER_BINS_WITH_SHARED_MEM) {\n+    partial_output = a.type().zeros({grid.x, nbins});\n+  } else {\n+    partial_output = a.type().zeros({1, nbins});\n+  }\n+  // kernel call to compute partial histogram\n+  using IndexType = uint64_t;\n+  auto pInfo = detail::getTensorInfo<scalar1, IndexType>(partial_output);\n+  auto bInfo = detail::getTensorInfo<scalar2, IndexType>(b);\n+  auto maxSharedMem =\n+      at::globalContext().getCurrentDeviceProperties()->sharedMemPerBlock;\n+  auto sharedMem = nbins * sizeof(scalar1) + 8; // 8 guard bytes\n+\n+#define GET_WEIGHTS_DUMMY_OP [] __device__(IndexType) { return 1L; }\n+#define GET_WEIGHTS_OP                                                    \\", "path": "aten/src/ATen/cuda/CUDAApplyUtils.cuh", "position": null, "original_position": 156, "commit_id": "398cfbc3a790dda0aa46e99b66d0d82b4095a7b1", "original_commit_id": "c70f8328a9868a616dbe939a7635a9dd0ae465b4", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "Are the macros necessary? It would be nice to make them static functions so the code is more maintainable", "created_at": "2018-04-25T15:06:06Z", "updated_at": "2018-11-23T15:43:10Z", "html_url": "https://github.com/pytorch/pytorch/pull/6688#discussion_r184095421", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6688", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/184095421"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6688#discussion_r184095421"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6688"}}, "body_html": "<p>Are the macros necessary? It would be nice to make them static functions so the code is more maintainable</p>", "body_text": "Are the macros necessary? It would be nice to make them static functions so the code is more maintainable"}