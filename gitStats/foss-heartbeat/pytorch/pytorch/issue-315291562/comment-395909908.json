{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/395909908", "html_url": "https://github.com/pytorch/pytorch/pull/6688#issuecomment-395909908", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6688", "id": 395909908, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NTkwOTkwOA==", "user": {"login": "chintak", "id": 3398558, "node_id": "MDQ6VXNlcjMzOTg1NTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3398558?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chintak", "html_url": "https://github.com/chintak", "followers_url": "https://api.github.com/users/chintak/followers", "following_url": "https://api.github.com/users/chintak/following{/other_user}", "gists_url": "https://api.github.com/users/chintak/gists{/gist_id}", "starred_url": "https://api.github.com/users/chintak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chintak/subscriptions", "organizations_url": "https://api.github.com/users/chintak/orgs", "repos_url": "https://api.github.com/users/chintak/repos", "events_url": "https://api.github.com/users/chintak/events{/privacy}", "received_events_url": "https://api.github.com/users/chintak/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-08T22:32:55Z", "updated_at": "2018-06-08T22:32:55Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Benchmark with sync</h2>\n<p>Following offline discussion with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>, there was an error with my earlier benchmarking, in that, I wasn't explicitly synchronizing the GPU after <code>torch.bincount</code> call. This has been now fixed. Here is the <a href=\"https://pastebin.com/pci0nQtv\" rel=\"nofollow\">benchmark.py</a> file used.</p>\n<h3>CPU</h3>\n<table>\n<thead>\n<tr>\n<th>input size</th>\n<th>time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>100K</td>\n<td>0.0055</td>\n</tr>\n<tr>\n<td>1M</td>\n<td>0.052</td>\n</tr>\n<tr>\n<td>10M</td>\n<td>0.52</td>\n</tr>\n</tbody>\n</table>\n<h3>Shared memory usage</h3>\n<table>\n<thead>\n<tr>\n<th>input size</th>\n<th>10</th>\n<th>100</th>\n<th>500</th>\n<th>1000</th>\n<th>2500</th>\n<th>5000</th>\n<th>7500</th>\n<th>10000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>100K</td>\n<td>0.0135</td>\n<td>0.0007</td>\n<td>0.0006</td>\n<td>0.0005</td>\n<td>0.0004</td>\n<td>0.0004</td>\n<td>NA</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>1M</td>\n<td>0.0013</td>\n<td>0.0073</td>\n<td>0.0048</td>\n<td>0.003</td>\n<td>0.0014</td>\n<td>0.0016</td>\n<td>NA</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>10M</td>\n<td>0.0087</td>\n<td>0.0826</td>\n<td>0.0454</td>\n<td>0.0268</td>\n<td>0.0101</td>\n<td>0.0125</td>\n<td>NA</td>\n<td>NA</td>\n</tr>\n</tbody>\n</table>\n<h3>Multi Block memory usage</h3>\n<table>\n<thead>\n<tr>\n<th>input size</th>\n<th>10</th>\n<th>100</th>\n<th>500</th>\n<th>1000</th>\n<th>2500</th>\n<th>5000</th>\n<th>7500</th>\n<th>10000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>100K</td>\n<td>0.0148</td>\n<td>0.0006</td>\n<td>0.0007</td>\n<td>0.0006</td>\n<td>0.0005</td>\n<td>0.0006</td>\n<td>0.0007</td>\n<td>0.0008</td>\n</tr>\n<tr>\n<td>1M</td>\n<td>0.004</td>\n<td>0.004</td>\n<td>0.0049</td>\n<td>0.0028</td>\n<td>0.0018</td>\n<td>0.0023</td>\n<td>0.003</td>\n<td>0.0038</td>\n</tr>\n<tr>\n<td>10M</td>\n<td>0.0315</td>\n<td>0.0308</td>\n<td>0.0421</td>\n<td>0.0246</td>\n<td>0.0134</td>\n<td>0.0183</td>\n<td>0.0248</td>\n<td>0.032</td>\n</tr>\n</tbody>\n</table>\n<h3>Global memory usage</h3>\n<table>\n<thead>\n<tr>\n<th>input size</th>\n<th>10</th>\n<th>100</th>\n<th>500</th>\n<th>1000</th>\n<th>2500</th>\n<th>5000</th>\n<th>7500</th>\n<th>10000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>100K</td>\n<td>0.1677</td>\n<td>0.005</td>\n<td>0.0008</td>\n<td>0.0004</td>\n<td>0.0003</td>\n<td>0.0002</td>\n<td>0.0002</td>\n<td>0.0002</td>\n</tr>\n<tr>\n<td>1M</td>\n<td>1.8555</td>\n<td>0.0625</td>\n<td>0.0071</td>\n<td>0.0022</td>\n<td>0.0011</td>\n<td>0.0007</td>\n<td>0.0006</td>\n<td>0.0006</td>\n</tr>\n<tr>\n<td>10M</td>\n<td>18.5349</td>\n<td>0.6453</td>\n<td>0.0694</td>\n<td>0.019</td>\n<td>0.0086</td>\n<td>0.0048</td>\n<td>0.0038</td>\n<td>0.0038</td>\n</tr>\n</tbody>\n</table>\n<h2>Conclusion</h2>\n<ol>\n<li>For small number of bins, 10-100, shared memory implementation performs the best.</li>\n<li>For the intermediate range of bins, 100-500, multi-block memory implementation wins over the other three.</li>\n<li>For larger number of bins, 1000+, global memory implementation performs the best.</li>\n</ol>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a></p>\n<p>P.S.:</p>\n<ol>\n<li>All time reported in <code>sec</code>.</li>\n<li>Experiments performed on Tesla M40 (12 Gb) (K40 seems to be having a bad day - <code>out of memory</code> errors even though nothing was running).</li>\n</ol>", "body_text": "Benchmark with sync\nFollowing offline discussion with @zou3519, there was an error with my earlier benchmarking, in that, I wasn't explicitly synchronizing the GPU after torch.bincount call. This has been now fixed. Here is the benchmark.py file used.\nCPU\n\n\n\ninput size\ntime\n\n\n\n\n100K\n0.0055\n\n\n1M\n0.052\n\n\n10M\n0.52\n\n\n\nShared memory usage\n\n\n\ninput size\n10\n100\n500\n1000\n2500\n5000\n7500\n10000\n\n\n\n\n100K\n0.0135\n0.0007\n0.0006\n0.0005\n0.0004\n0.0004\nNA\nNA\n\n\n1M\n0.0013\n0.0073\n0.0048\n0.003\n0.0014\n0.0016\nNA\nNA\n\n\n10M\n0.0087\n0.0826\n0.0454\n0.0268\n0.0101\n0.0125\nNA\nNA\n\n\n\nMulti Block memory usage\n\n\n\ninput size\n10\n100\n500\n1000\n2500\n5000\n7500\n10000\n\n\n\n\n100K\n0.0148\n0.0006\n0.0007\n0.0006\n0.0005\n0.0006\n0.0007\n0.0008\n\n\n1M\n0.004\n0.004\n0.0049\n0.0028\n0.0018\n0.0023\n0.003\n0.0038\n\n\n10M\n0.0315\n0.0308\n0.0421\n0.0246\n0.0134\n0.0183\n0.0248\n0.032\n\n\n\nGlobal memory usage\n\n\n\ninput size\n10\n100\n500\n1000\n2500\n5000\n7500\n10000\n\n\n\n\n100K\n0.1677\n0.005\n0.0008\n0.0004\n0.0003\n0.0002\n0.0002\n0.0002\n\n\n1M\n1.8555\n0.0625\n0.0071\n0.0022\n0.0011\n0.0007\n0.0006\n0.0006\n\n\n10M\n18.5349\n0.6453\n0.0694\n0.019\n0.0086\n0.0048\n0.0038\n0.0038\n\n\n\nConclusion\n\nFor small number of bins, 10-100, shared memory implementation performs the best.\nFor the intermediate range of bins, 100-500, multi-block memory implementation wins over the other three.\nFor larger number of bins, 1000+, global memory implementation performs the best.\n\ncc @zou3519, @SsnL\nP.S.:\n\nAll time reported in sec.\nExperiments performed on Tesla M40 (12 Gb) (K40 seems to be having a bad day - out of memory errors even though nothing was running).", "body": "## Benchmark with sync\r\n\r\nFollowing offline discussion with @zou3519, there was an error with my earlier benchmarking, in that, I wasn't explicitly synchronizing the GPU after `torch.bincount` call. This has been now fixed. Here is the [benchmark.py](https://pastebin.com/pci0nQtv) file used.\r\n\r\n### CPU\r\n\r\ninput size | time\r\n-- | --\r\n100K | 0.0055\r\n1M | 0.052\r\n10M | 0.52\r\n\r\n### Shared memory usage\r\n\r\n\r\ninput size | 10 | 100 | 500 | 1000 | 2500 | 5000 | 7500 | 10000\r\n-- | -- | -- | -- | -- | -- | -- | -- | --\r\n100K | 0.0135 | 0.0007 | 0.0006 | 0.0005 | 0.0004 | 0.0004 | NA | NA\r\n1M | 0.0013 | 0.0073 | 0.0048 | 0.003 | 0.0014 | 0.0016 | NA | NA\r\n10M | 0.0087 | 0.0826 | 0.0454 | 0.0268 | 0.0101 | 0.0125 | NA | NA\r\n\r\n\r\n### Multi Block memory usage\r\n\r\n\r\ninput size | 10 | 100 | 500 | 1000 | 2500 | 5000 | 7500 | 10000\r\n-- | -- | -- | -- | -- | -- | -- | -- | --\r\n100K | 0.0148 | 0.0006 | 0.0007 | 0.0006 | 0.0005 | 0.0006 | 0.0007 | 0.0008\r\n1M | 0.004 | 0.004 | 0.0049 | 0.0028 | 0.0018 | 0.0023 | 0.003 | 0.0038\r\n10M | 0.0315 | 0.0308 | 0.0421 | 0.0246 | 0.0134 | 0.0183 | 0.0248 | 0.032\r\n\r\n### Global memory usage\r\n\r\n\r\ninput size | 10 | 100 | 500 | 1000 | 2500 | 5000 | 7500 | 10000\r\n-- | -- | -- | -- | -- | -- | -- | -- | --\r\n100K | 0.1677 | 0.005 | 0.0008 | 0.0004 | 0.0003 | 0.0002 | 0.0002 | 0.0002\r\n1M | 1.8555 | 0.0625 | 0.0071 | 0.0022 | 0.0011 | 0.0007 | 0.0006 | 0.0006\r\n10M | 18.5349 | 0.6453 | 0.0694 | 0.019 | 0.0086 | 0.0048 | 0.0038 | 0.0038\r\n\r\n## Conclusion\r\n\r\n1. For small number of bins, 10-100, shared memory implementation performs the best.\r\n2. For the intermediate range of bins, 100-500, multi-block memory implementation wins over the other three.\r\n3. For larger number of bins, 1000+, global memory implementation performs the best.\r\n\r\ncc @zou3519, @SsnL \r\n\r\nP.S.:\r\n1. All time reported in `sec`.\r\n2. Experiments performed on Tesla M40 (12 Gb) (K40 seems to be having a bad day - `out of memory` errors even though nothing was running).\r\n"}