{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/392975281", "html_url": "https://github.com/pytorch/pytorch/pull/6688#issuecomment-392975281", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6688", "id": 392975281, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Mjk3NTI4MQ==", "user": {"login": "chintak", "id": 3398558, "node_id": "MDQ6VXNlcjMzOTg1NTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3398558?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chintak", "html_url": "https://github.com/chintak", "followers_url": "https://api.github.com/users/chintak/followers", "following_url": "https://api.github.com/users/chintak/following{/other_user}", "gists_url": "https://api.github.com/users/chintak/gists{/gist_id}", "starred_url": "https://api.github.com/users/chintak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chintak/subscriptions", "organizations_url": "https://api.github.com/users/chintak/orgs", "repos_url": "https://api.github.com/users/chintak/repos", "events_url": "https://api.github.com/users/chintak/events{/privacy}", "received_events_url": "https://api.github.com/users/chintak/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-29T23:03:21Z", "updated_at": "2018-05-29T23:16:13Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Benchmark</h2>\n<p><code>bin sizes =   [10, 100, 500, 1000, 2500, 5000, 7500, 10000]</code><br>\n<code>repeat = 10</code></p>\n<h3>CPU</h3>\n<table>\n<thead>\n<tr>\n<th>input size</th>\n<th>time</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>100K</td>\n<td>0.006</td>\n</tr>\n<tr>\n<td>1M</td>\n<td>0.06</td>\n</tr>\n<tr>\n<td>10M</td>\n<td>0.6</td>\n</tr>\n</tbody>\n</table>\n<h3>Shared memory usage</h3>\n<table>\n<thead>\n<tr>\n<th>input size</th>\n<th>10</th>\n<th>100</th>\n<th>500</th>\n<th>1000</th>\n<th>2500</th>\n<th>5000</th>\n<th>7500</th>\n<th>10000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>100K</td>\n<td>0.001</td>\n<td>0.0003</td>\n<td>0.0003</td>\n<td>0.0004</td>\n<td>0.0004</td>\n<td>0.0004</td>\n<td>NA</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>1M</td>\n<td>0.0057</td>\n<td>0.001</td>\n<td>0.0008</td>\n<td>0.0008</td>\n<td>0.0008</td>\n<td>0.001</td>\n<td>NA</td>\n<td>NA</td>\n</tr>\n<tr>\n<td>10M</td>\n<td>0.0533</td>\n<td>0.005</td>\n<td>0.003</td>\n<td>0.003</td>\n<td>0.0033</td>\n<td>0.0048</td>\n<td>NA</td>\n<td>NA</td>\n</tr>\n</tbody>\n</table>\n<h3>Multi Block memory usage</h3>\n<table>\n<thead>\n<tr>\n<th>input size</th>\n<th>10</th>\n<th>100</th>\n<th>500</th>\n<th>1000</th>\n<th>2500</th>\n<th>5000</th>\n<th>7500</th>\n<th>10000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>100K</td>\n<td>0.0007</td>\n<td>0.0004</td>\n<td>0.0004</td>\n<td>0.0004</td>\n<td>0.0004</td>\n<td>0.0005</td>\n<td>0.0005</td>\n<td>0.0006</td>\n</tr>\n<tr>\n<td>1M</td>\n<td>0.003</td>\n<td>0.0012</td>\n<td>0.0012</td>\n<td>0.0012</td>\n<td>0.0012</td>\n<td>0.0013</td>\n<td>0.0017</td>\n<td>0.002</td>\n</tr>\n<tr>\n<td>10M</td>\n<td>0.027</td>\n<td>0.0068</td>\n<td>0.0067</td>\n<td>0.007</td>\n<td>0.0072</td>\n<td>0.0078</td>\n<td>0.0119</td>\n<td>0.014</td>\n</tr>\n</tbody>\n</table>\n<h3>Global memory usage</h3>\n<table>\n<thead>\n<tr>\n<th>input size</th>\n<th>10</th>\n<th>100</th>\n<th>500</th>\n<th>1000</th>\n<th>2500</th>\n<th>5000</th>\n<th>7500</th>\n<th>10000</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>100K</td>\n<td>0.087</td>\n<td>0.002</td>\n<td>0.0008</td>\n<td>0.0006</td>\n<td>0.0004</td>\n<td>0.0004</td>\n<td>0.0004</td>\n<td>0.0004</td>\n</tr>\n<tr>\n<td>1M</td>\n<td>0.9345</td>\n<td>0.03</td>\n<td>0.005</td>\n<td>0.003</td>\n<td>0.002</td>\n<td>0.001</td>\n<td>0.001</td>\n<td>0.001</td>\n</tr>\n<tr>\n<td>10M</td>\n<td>10.42</td>\n<td>0.3967</td>\n<td>0.05</td>\n<td>0.03</td>\n<td>0.016</td>\n<td>0.0119</td>\n<td>0.0105</td>\n<td>0.0098</td>\n</tr>\n</tbody>\n</table>\n<h3>Comments</h3>\n<ol>\n<li>time for shared and multi_block implementation remains almost the same across the bin sizes</li>\n<li>shared implementation fails for bin size &gt; 5000 due to insufficient GPU memory</li>\n<li>for global memory use, high time for small number of bins is expected due to excess collision and underlying atomic addition</li>\n</ol>\n<h2>Conclusion</h2>\n<ol>\n<li>For low bin count, shared memory implementation should be used and for higher bin count, global memory implementation.</li>\n<li>Multi block implementation presents a trade off between memory and speed in comparison to Shared implementation when bin count is ~5k.</li>\n<li>Rather than having 3 implementations, we can simply use shared and global implementations. Defer to global implementation if bin count if greater than a hard threshold of 5000 or shared memory required exceeds the shared memory available in the GPU.</li>\n</ol>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a></p>\n<p>PS:</p>\n<ol>\n<li>Time reported in seconds.</li>\n<li>All experiments done on Nvidia Tesla K40m (12 Gb).</li>\n</ol>", "body_text": "Benchmark\nbin sizes =   [10, 100, 500, 1000, 2500, 5000, 7500, 10000]\nrepeat = 10\nCPU\n\n\n\ninput size\ntime\n\n\n\n\n100K\n0.006\n\n\n1M\n0.06\n\n\n10M\n0.6\n\n\n\nShared memory usage\n\n\n\ninput size\n10\n100\n500\n1000\n2500\n5000\n7500\n10000\n\n\n\n\n100K\n0.001\n0.0003\n0.0003\n0.0004\n0.0004\n0.0004\nNA\nNA\n\n\n1M\n0.0057\n0.001\n0.0008\n0.0008\n0.0008\n0.001\nNA\nNA\n\n\n10M\n0.0533\n0.005\n0.003\n0.003\n0.0033\n0.0048\nNA\nNA\n\n\n\nMulti Block memory usage\n\n\n\ninput size\n10\n100\n500\n1000\n2500\n5000\n7500\n10000\n\n\n\n\n100K\n0.0007\n0.0004\n0.0004\n0.0004\n0.0004\n0.0005\n0.0005\n0.0006\n\n\n1M\n0.003\n0.0012\n0.0012\n0.0012\n0.0012\n0.0013\n0.0017\n0.002\n\n\n10M\n0.027\n0.0068\n0.0067\n0.007\n0.0072\n0.0078\n0.0119\n0.014\n\n\n\nGlobal memory usage\n\n\n\ninput size\n10\n100\n500\n1000\n2500\n5000\n7500\n10000\n\n\n\n\n100K\n0.087\n0.002\n0.0008\n0.0006\n0.0004\n0.0004\n0.0004\n0.0004\n\n\n1M\n0.9345\n0.03\n0.005\n0.003\n0.002\n0.001\n0.001\n0.001\n\n\n10M\n10.42\n0.3967\n0.05\n0.03\n0.016\n0.0119\n0.0105\n0.0098\n\n\n\nComments\n\ntime for shared and multi_block implementation remains almost the same across the bin sizes\nshared implementation fails for bin size > 5000 due to insufficient GPU memory\nfor global memory use, high time for small number of bins is expected due to excess collision and underlying atomic addition\n\nConclusion\n\nFor low bin count, shared memory implementation should be used and for higher bin count, global memory implementation.\nMulti block implementation presents a trade off between memory and speed in comparison to Shared implementation when bin count is ~5k.\nRather than having 3 implementations, we can simply use shared and global implementations. Defer to global implementation if bin count if greater than a hard threshold of 5000 or shared memory required exceeds the shared memory available in the GPU.\n\n@zou3519\nPS:\n\nTime reported in seconds.\nAll experiments done on Nvidia Tesla K40m (12 Gb).", "body": "## Benchmark\r\n\r\n`bin sizes =   [10, 100, 500, 1000, 2500, 5000, 7500, 10000]`\r\n`repeat = 10`\r\n\r\n### CPU\r\n\r\ninput size | time\r\n-- | --\r\n100K | 0.006\r\n1M | 0.06\r\n10M | 0.6\r\n\r\n### Shared memory usage\r\n\r\ninput size | 10 | 100 | 500 | 1000 | 2500 | 5000 | 7500 | 10000\r\n-- | -- | -- | -- | -- | -- | -- | -- | --\r\n100K | 0.001 | 0.0003 | 0.0003 | 0.0004 | 0.0004 | 0.0004 | NA | NA\r\n1M | 0.0057 | 0.001 | 0.0008 | 0.0008 | 0.0008 | 0.001 | NA | NA\r\n10M | 0.0533 | 0.005 | 0.003 | 0.003 | 0.0033 | 0.0048 | NA | NA\r\n\r\n### Multi Block memory usage\r\n\r\ninput size | 10 | 100 | 500 | 1000 | 2500 | 5000 | 7500 | 10000\r\n-- | -- | -- | -- | -- | -- | -- | -- | --\r\n100K | 0.0007 | 0.0004 | 0.0004 | 0.0004 | 0.0004 | 0.0005 | 0.0005 | 0.0006\r\n1M | 0.003 | 0.0012 | 0.0012 | 0.0012 | 0.0012 | 0.0013 | 0.0017 | 0.002\r\n10M | 0.027 | 0.0068 | 0.0067 | 0.007 | 0.0072 | 0.0078 | 0.0119 | 0.014\r\n\r\n### Global memory usage\r\n\r\ninput size | 10 | 100 | 500 | 1000 | 2500 | 5000 | 7500 | 10000\r\n-- | -- | -- | -- | -- | -- | -- | -- | --\r\n100K | 0.087 | 0.002 | 0.0008 | 0.0006 | 0.0004 | 0.0004 | 0.0004 | 0.0004\r\n1M | 0.9345 | 0.03 | 0.005 | 0.003 | 0.002 | 0.001 | 0.001 | 0.001\r\n10M | 10.42 | 0.3967 | 0.05 | 0.03 | 0.016 | 0.0119 | 0.0105 | 0.0098\r\n\r\n### Comments\r\n1. time for shared and multi_block implementation remains almost the same across the bin sizes\r\n2. shared implementation fails for bin size > 5000 due to insufficient GPU memory\r\n3. for global memory use, high time for small number of bins is expected due to excess collision and underlying atomic addition\r\n\r\n## Conclusion\r\n1. For low bin count, shared memory implementation should be used and for higher bin count, global memory implementation.\r\n2. Multi block implementation presents a trade off between memory and speed in comparison to Shared implementation when bin count is ~5k.\r\n3. Rather than having 3 implementations, we can simply use shared and global implementations. Defer to global implementation if bin count if greater than a hard threshold of 5000 or shared memory required exceeds the shared memory available in the GPU.\r\n\r\n@zou3519 \r\n\r\nPS: \r\n1. Time reported in seconds.\r\n2. All experiments done on Nvidia Tesla K40m (12 Gb)."}