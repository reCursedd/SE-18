{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193141038", "pull_request_review_id": 126054853, "id": 193141038, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MzE0MTAzOA==", "diff_hunk": "@@ -0,0 +1,282 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+\n+namespace at {\n+\n+namespace cuda { namespace detail {\n+#define MIN_NUMBER_BINS_FOR_GLOBAL_MEM 5000\n+#define FOR_KERNEL_LOOP(i, lim)                                      \\\n+  for (IndexType i = blockIdx.x * blockDim.x + threadIdx.x; i < lim; \\\n+       i += gridDim.x * blockDim.x)\n+\n+/*\n+  Memory types used for the 3 histogram implementations.\n+  See `CUDA_tensor_histogram` below.\n+ */\n+enum class CUDAHistogramMemoryType { MULTI_BLOCK, SHARED, GLOBAL };\n+\n+/*\n+  Kernel for computing the histogram of the input.\n+ */\n+template <\n+    typename scalar1,\n+    typename scalar2,\n+    typename IndexType,\n+    int ADims,\n+    int PDims,\n+    int BDims,\n+    CUDAHistogramMemoryType MemoryType = CUDAHistogramMemoryType::MULTI_BLOCK,\n+    typename Op>\n+__global__ void kernelHistogram1D(\n+    detail::TensorInfo<scalar1, IndexType> a, /* output */\n+    detail::TensorInfo<scalar1, IndexType> p, /* partial output */\n+    detail::TensorInfo<scalar2, IndexType> b, /* input */\n+    int binsize,\n+    IndexType totalElements,\n+    Op getOp) {\n+  extern __shared__ unsigned char my_smem[];\n+  scalar1* smem = nullptr;\n+\n+  if (MemoryType == CUDAHistogramMemoryType::SHARED) {\n+    ////////////////////////// Shared memory //////////////////////////\n+    // atomically add to block specific shared memory\n+    // then atomically add to the global output tensor\n+    smem = reinterpret_cast<scalar1*>(my_smem);\n+    for (IndexType i = threadIdx.x; i < a.sizes[0]; i += blockDim.x) {\n+      smem[i] = 0;\n+    }\n+    __syncthreads();\n+    FOR_KERNEL_LOOP(linearIndex, totalElements) {\n+      // Convert `linearIndex` into an offset of `b`\n+      const IndexType bOffset =\n+          detail::IndexToOffset<scalar2, IndexType, BDims>::get(linearIndex, b);\n+      // Use value at `b` as an offset of `smem`\n+      const IndexType pOffset = b.data[bOffset] / binsize;\n+      atomicAdd(&smem[pOffset], getOp(linearIndex));\n+    }\n+    __syncthreads();\n+    // NOTE: atomically update output bin count.\n+    //   Atomic update is imp since __syncthread() will only synchronize threads\n+    //   in a given block, not across blocks.\n+    for (IndexType i = threadIdx.x; i < a.sizes[0]; i += blockDim.x) {\n+      const IndexType aOffset =\n+          detail::IndexToOffset<scalar1, IndexType, ADims>::get(i, a);\n+      atomicAdd(&a.data[aOffset], smem[i]);\n+    }\n+\n+  } else if (MemoryType == CUDAHistogramMemoryType::MULTI_BLOCK) {\n+    ////////////////////////// Multi Block memory //////////////////////////\n+    // atomically add to block specific global tensor\n+    // then atomically add to the global output tensor\n+    // compute histogram for the block\n+    FOR_KERNEL_LOOP(linearIndex, totalElements) {\n+      // Convert `linearIndex` into an offset of `b`\n+      const IndexType bOffset =\n+          detail::IndexToOffset<scalar2, IndexType, BDims>::get(linearIndex, b);\n+      const auto bVal = b.data[bOffset];\n+      // Use value at `b` as an offset of `p`\n+      const IndexType pIdx = p.strides[0] * blockIdx.x + bVal / binsize;\n+      const IndexType pOffset =\n+          detail::IndexToOffset<scalar1, IndexType, PDims>::get(pIdx, p);\n+      atomicAdd(&p.data[pOffset], getOp(linearIndex));\n+    }\n+    __syncthreads();\n+    // NOTE: atomically update output bin count.\n+    //   Atomic update is imp since __syncthread() will only synchronize threads\n+    //   in a given block, not across blocks.\n+    const IndexType pIdx = p.strides[0] * blockIdx.x;\n+    const IndexType pOffset =\n+        detail::IndexToOffset<scalar1, IndexType, PDims>::get(pIdx, p);\n+    for (IndexType i = threadIdx.x; i < a.sizes[0]; i += blockDim.x) {\n+      const IndexType aOffset =\n+          detail::IndexToOffset<scalar1, IndexType, ADims>::get(i, a);\n+      atomicAdd(&a.data[aOffset], p.data[pOffset + i]);\n+    }\n+\n+  } else {\n+    ////////////////////////// Global memory //////////////////////////\n+    // atomically add to the output tensor\n+    // compute histogram for the block\n+    FOR_KERNEL_LOOP(linearIndex, totalElements) {\n+      // Convert `linearIndex` into an offset of `b`\n+      const IndexType bOffset =\n+          detail::IndexToOffset<scalar2, IndexType, BDims>::get(linearIndex, b);\n+      const auto bVal = b.data[bOffset];\n+      // Use value at `b` as an offset of `a`\n+      const IndexType aIdx = bVal / binsize;\n+      const IndexType aOffset =\n+          detail::IndexToOffset<scalar1, IndexType, ADims>::get(aIdx, a);\n+      atomicAdd(&a.data[aOffset], getOp(linearIndex));\n+    }\n+  }\n+}\n+\n+#define HANDLE_CASE(MEMORY_TYPE, WEIGHTS_OP)                               \\\n+  kernelHistogram1D<scalar1, scalar2, IndexType, 1, 2, 1, MEMORY_TYPE>     \\\n+      <<<grid,                                                             \\\n+         block,                                                            \\\n+         (MEMORY_TYPE == CUDAHistogramMemoryType::SHARED) ? sharedMem : 0, \\\n+         at::globalContext().getCurrentCUDAStream()>>>(                    \\\n+          aInfo, pInfo, bInfo, binsize, totalElements, WEIGHTS_OP);        \\\n+  AT_ASSERT(cudaGetLastError() == cudaSuccess, \"kernelHistogram1D failed\");\n+\n+#define HANDLE_SWITCH_CASE(mType, getOp)                                      \\\n+  switch (mType) {                                                            \\\n+    case CUDAHistogramMemoryType::SHARED:                                     \\\n+      HANDLE_CASE(CUDAHistogramMemoryType::SHARED, getOp);                    \\\n+      break;                                                                  \\\n+    case CUDAHistogramMemoryType::MULTI_BLOCK:                                \\\n+      HANDLE_CASE(CUDAHistogramMemoryType::MULTI_BLOCK, getOp);               \\\n+      break;                                                                  \\\n+    default:                                                                  \\\n+      std::cerr << \"WARNING: Potentially slow. \"                              \\\n+                   \"CUDA_tensor_histogram with nbins = \"                      \\\n+                << nbins << \" uses global memory with atomics.\" << std::endl; \\\n+      HANDLE_CASE(CUDAHistogramMemoryType::GLOBAL, getOp);                    \\\n+  }\n+\n+/*\n+  Calculate the frequency of the input values.\n+\n+  `a` contains the final output or the histogram.\n+  Input `b` is assumed to be 1-D non-negative int array.\n+  `c` optionally contains the weight vector.\n+  See `help torch.bincount` for details on the math.\n+\n+  3 implementations based of input size and memory usage:\n+    case: #bins < blockDim.x\n+        SHARED: Each block atomically adds to it's own **shared** hist copy,\n+        then atomically updates the global tensor.\n+    case: blockDim.x <= #bins < MIN_NUMBER_BINS_FOR_GLOBAL_MEM\n+        MULTI_BLOCK: Each block atomically adds to it's own **global** hist\n+        copy, then atomically updates the global tensor.\n+    case: MIN_NUMBER_BINS_FOR_GLOBAL_MEM <= #bins\n+        GLOBAL: all threads atomically update to a single **global** hist copy.\n+ */\n+template <typename scalar1, typename scalar2, bool HasWeights>\n+bool CUDA_tensor_histogram(\n+    at::Tensor a, /* output */\n+    at::Tensor b, /* input */\n+    at::Tensor c, /* weights(optional) */\n+    int64_t nbins,\n+    int binsize,\n+    TensorArgType aType = TensorArgType::ReadWrite,\n+    TensorArgType bType = TensorArgType::ReadOnly,\n+    TensorArgType cType = TensorArgType::ReadOnly) {\n+  checkBackend(\"CUDA_tensor_histogram\", {a, b}, Backend::CUDA);\n+  if (HasWeights) {\n+    checkBackend(\"CUDA_tensor_histogram\", {c}, Backend::CUDA);\n+  }\n+  auto totalElements = b.size(0);\n+\n+  const dim3 block = getApplyBlock();\n+  dim3 grid;\n+  if (!getApplyGrid(totalElements, grid)) {\n+    return false;\n+  }\n+#if CUDA_VERSION < 9000\n+  grid.x = std::min(\n+      (unsigned int)at::globalContext()\n+              .getCurrentDeviceProperties()\n+              ->multiProcessorCount *\n+          AT_APPLY_BLOCKS_PER_SM,\n+      grid.x);\n+#endif\n+\n+  CUDAHistogramMemoryType memType = CUDAHistogramMemoryType::SHARED;\n+  auto maxSharedMem =\n+      at::globalContext().getCurrentDeviceProperties()->sharedMemPerBlock;", "path": "aten/src/ATen/native/cuda/SummaryOps.cu", "position": null, "original_position": 188, "commit_id": "398cfbc3a790dda0aa46e99b66d0d82b4095a7b1", "original_commit_id": "5108d22aaba73c223ee0a57e5376cba09be6990b", "user": {"login": "chintak", "id": 3398558, "node_id": "MDQ6VXNlcjMzOTg1NTg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3398558?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chintak", "html_url": "https://github.com/chintak", "followers_url": "https://api.github.com/users/chintak/followers", "following_url": "https://api.github.com/users/chintak/following{/other_user}", "gists_url": "https://api.github.com/users/chintak/gists{/gist_id}", "starred_url": "https://api.github.com/users/chintak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chintak/subscriptions", "organizations_url": "https://api.github.com/users/chintak/orgs", "repos_url": "https://api.github.com/users/chintak/repos", "events_url": "https://api.github.com/users/chintak/events{/privacy}", "received_events_url": "https://api.github.com/users/chintak/received_events", "type": "User", "site_admin": false}, "body": "@zou3519, checking the available shared memory.", "created_at": "2018-06-05T16:36:51Z", "updated_at": "2018-11-23T15:44:58Z", "html_url": "https://github.com/pytorch/pytorch/pull/6688#discussion_r193141038", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6688", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193141038"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6688#discussion_r193141038"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6688"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>, checking the available shared memory.</p>", "body_text": "@zou3519, checking the available shared memory."}