{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9078", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9078/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9078/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9078/events", "html_url": "https://github.com/pytorch/pytorch/pull/9078", "id": 337284109, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk4NTIwMDI5", "number": 9078, "title": "Fix dense Embedding to work with double backward", "user": {"login": "kshitij12345", "id": 19503980, "node_id": "MDQ6VXNlcjE5NTAzOTgw", "avatar_url": "https://avatars3.githubusercontent.com/u/19503980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kshitij12345", "html_url": "https://github.com/kshitij12345", "followers_url": "https://api.github.com/users/kshitij12345/followers", "following_url": "https://api.github.com/users/kshitij12345/following{/other_user}", "gists_url": "https://api.github.com/users/kshitij12345/gists{/gist_id}", "starred_url": "https://api.github.com/users/kshitij12345/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kshitij12345/subscriptions", "organizations_url": "https://api.github.com/users/kshitij12345/orgs", "repos_url": "https://api.github.com/users/kshitij12345/repos", "events_url": "https://api.github.com/users/kshitij12345/events{/privacy}", "received_events_url": "https://api.github.com/users/kshitij12345/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 31, "created_at": "2018-07-01T11:31:45Z", "updated_at": "2018-11-23T15:53:42Z", "closed_at": null, "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9078", "html_url": "https://github.com/pytorch/pytorch/pull/9078", "diff_url": "https://github.com/pytorch/pytorch/pull/9078.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9078.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #6469.\">Fixes</span> : <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"312982596\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6469\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/6469/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/6469\">#6469</a></p>\n<ol>\n<li>\n<p><code>ATen/native/native_functions.yml</code> had <a href=\"https://github.com/pytorch/pytorch/blob/03e7953a98875c0164cb8e2c19b45800e85f4347/aten/src/ATen/native/native_functions.yaml#L451-L455\">dispatch</a> variants for for <code>embedding_dense_backward</code> , however <code>embedding_backward</code> explicitly made <a href=\"https://github.com/pytorch/pytorch/blob/03e7953a98875c0164cb8e2c19b45800e85f4347/aten/src/ATen/native/Embedding.cpp#L35-L45\">call</a> to it, thus leading to error.</p>\n</li>\n<li>\n<p>In case of CUDA type tensor, the function crashed used to crash on dereferencing of indices's data <a href=\"https://github.com/pytorch/pytorch/blob/03e7953a98875c0164cb8e2c19b45800e85f4347/aten/src/ATen/native/Embedding.cpp#L93\">pointer</a>.</p>\n</li>\n</ol>\n<p>Both have been solved and checked against (on CUDA and CPU)</p>\n<ol>\n<li>As mentioned in the issue</li>\n</ol>\n<pre><code>import torch\n\nclass Test(torch.nn.Module):\n    \n    def __init__(self):\n        super(Test,self).__init__()\n        self.embd = torch.nn.Embedding(1000, 100)\n        self.dense = torch.nn.Linear(100, 1)\n    \n    def forward(self, inp):\n        inp = self.embd(inp)\n        return self.dense(inp)\n\ntest = Test()\n#test.cuda()\ninp = torch.tensor([0,1,2,1,1])\nout = test(inp)\nraw_loss = out.mean(dim=0)\n\nloss_grad = torch.autograd.grad(outputs=raw_loss,\n                         inputs=list(test.parameters()),\n                         retain_graph=True, create_graph=True, only_inputs=True)\nnorm = sum([param.norm()**2 for param in loss_grad])\nloss = raw_loss + norm\n\nloss.backward(retain_graph=True)\n\nprint(test.embd.weight.grad)\n\n</code></pre>\n<ol start=\"2\">\n<li>Test Script</li>\n</ol>\n<pre><code>import torch\nimport time\nstart = time.time()\nl = [1,1]*100 \ninput = torch.tensor([[1,0],[1,0]],device='cpu')\nembedding_matrix = torch.tensor([[1.0,3.0],[2.0,4]],requires_grad=True,device='cpu')\n\nsq = embedding_matrix * embedding_matrix\nemb = torch.nn.functional.embedding(input, sq,scale_grad_by_freq=False)\n\nprint('Embedding Matrix')\nprint(embedding_matrix)\nprint('-----------------')\n\n#prod = torch.cumprod(emb,1)\nsum_ = emb.sum()#prod.sum()\n\nloss_grad, = torch.autograd.grad(outputs=sum_,inputs=embedding_matrix,create_graph=True)\n\nprint('Gradient')\nprint(loss_grad)\nprint('-----------------')\n\nsum2_ = sum_ + loss_grad.sum()\nprint(sum2_)\nsum2_.backward()\n\nprint(embedding_matrix.grad)\nprint(time.time() - start)\n</code></pre>", "body_text": "Fixes : #6469\n\n\nATen/native/native_functions.yml had dispatch variants for for embedding_dense_backward , however embedding_backward explicitly made call to it, thus leading to error.\n\n\nIn case of CUDA type tensor, the function crashed used to crash on dereferencing of indices's data pointer.\n\n\nBoth have been solved and checked against (on CUDA and CPU)\n\nAs mentioned in the issue\n\nimport torch\n\nclass Test(torch.nn.Module):\n    \n    def __init__(self):\n        super(Test,self).__init__()\n        self.embd = torch.nn.Embedding(1000, 100)\n        self.dense = torch.nn.Linear(100, 1)\n    \n    def forward(self, inp):\n        inp = self.embd(inp)\n        return self.dense(inp)\n\ntest = Test()\n#test.cuda()\ninp = torch.tensor([0,1,2,1,1])\nout = test(inp)\nraw_loss = out.mean(dim=0)\n\nloss_grad = torch.autograd.grad(outputs=raw_loss,\n                         inputs=list(test.parameters()),\n                         retain_graph=True, create_graph=True, only_inputs=True)\nnorm = sum([param.norm()**2 for param in loss_grad])\nloss = raw_loss + norm\n\nloss.backward(retain_graph=True)\n\nprint(test.embd.weight.grad)\n\n\n\nTest Script\n\nimport torch\nimport time\nstart = time.time()\nl = [1,1]*100 \ninput = torch.tensor([[1,0],[1,0]],device='cpu')\nembedding_matrix = torch.tensor([[1.0,3.0],[2.0,4]],requires_grad=True,device='cpu')\n\nsq = embedding_matrix * embedding_matrix\nemb = torch.nn.functional.embedding(input, sq,scale_grad_by_freq=False)\n\nprint('Embedding Matrix')\nprint(embedding_matrix)\nprint('-----------------')\n\n#prod = torch.cumprod(emb,1)\nsum_ = emb.sum()#prod.sum()\n\nloss_grad, = torch.autograd.grad(outputs=sum_,inputs=embedding_matrix,create_graph=True)\n\nprint('Gradient')\nprint(loss_grad)\nprint('-----------------')\n\nsum2_ = sum_ + loss_grad.sum()\nprint(sum2_)\nsum2_.backward()\n\nprint(embedding_matrix.grad)\nprint(time.time() - start)", "body": "Fixes : #6469 \r\n\r\n1. `ATen/native/native_functions.yml` had [dispatch](https://github.com/pytorch/pytorch/blob/03e7953a98875c0164cb8e2c19b45800e85f4347/aten/src/ATen/native/native_functions.yaml#L451-L455) variants for for `embedding_dense_backward` , however `embedding_backward` explicitly made [call](https://github.com/pytorch/pytorch/blob/03e7953a98875c0164cb8e2c19b45800e85f4347/aten/src/ATen/native/Embedding.cpp#L35-L45) to it, thus leading to error.\r\n\r\n2. In case of CUDA type tensor, the function crashed used to crash on dereferencing of indices's data [pointer](https://github.com/pytorch/pytorch/blob/03e7953a98875c0164cb8e2c19b45800e85f4347/aten/src/ATen/native/Embedding.cpp#L93).\r\n\r\nBoth have been solved and checked against (on CUDA and CPU)\r\n\r\n1.  As mentioned in the issue\r\n```\r\nimport torch\r\n\r\nclass Test(torch.nn.Module):\r\n    \r\n    def __init__(self):\r\n        super(Test,self).__init__()\r\n        self.embd = torch.nn.Embedding(1000, 100)\r\n        self.dense = torch.nn.Linear(100, 1)\r\n    \r\n    def forward(self, inp):\r\n        inp = self.embd(inp)\r\n        return self.dense(inp)\r\n\r\ntest = Test()\r\n#test.cuda()\r\ninp = torch.tensor([0,1,2,1,1])\r\nout = test(inp)\r\nraw_loss = out.mean(dim=0)\r\n\r\nloss_grad = torch.autograd.grad(outputs=raw_loss,\r\n                         inputs=list(test.parameters()),\r\n                         retain_graph=True, create_graph=True, only_inputs=True)\r\nnorm = sum([param.norm()**2 for param in loss_grad])\r\nloss = raw_loss + norm\r\n\r\nloss.backward(retain_graph=True)\r\n\r\nprint(test.embd.weight.grad)\r\n\r\n```\r\n\r\n2. Test Script\r\n```\r\nimport torch\r\nimport time\r\nstart = time.time()\r\nl = [1,1]*100 \r\ninput = torch.tensor([[1,0],[1,0]],device='cpu')\r\nembedding_matrix = torch.tensor([[1.0,3.0],[2.0,4]],requires_grad=True,device='cpu')\r\n\r\nsq = embedding_matrix * embedding_matrix\r\nemb = torch.nn.functional.embedding(input, sq,scale_grad_by_freq=False)\r\n\r\nprint('Embedding Matrix')\r\nprint(embedding_matrix)\r\nprint('-----------------')\r\n\r\n#prod = torch.cumprod(emb,1)\r\nsum_ = emb.sum()#prod.sum()\r\n\r\nloss_grad, = torch.autograd.grad(outputs=sum_,inputs=embedding_matrix,create_graph=True)\r\n\r\nprint('Gradient')\r\nprint(loss_grad)\r\nprint('-----------------')\r\n\r\nsum2_ = sum_ + loss_grad.sum()\r\nprint(sum2_)\r\nsum2_.backward()\r\n\r\nprint(embedding_matrix.grad)\r\nprint(time.time() - start)\r\n```"}