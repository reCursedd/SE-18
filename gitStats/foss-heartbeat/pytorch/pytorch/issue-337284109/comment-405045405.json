{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/405045405", "html_url": "https://github.com/pytorch/pytorch/pull/9078#issuecomment-405045405", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9078", "id": 405045405, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTA0NTQwNQ==", "user": {"login": "kshitij12345", "id": 19503980, "node_id": "MDQ6VXNlcjE5NTAzOTgw", "avatar_url": "https://avatars3.githubusercontent.com/u/19503980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kshitij12345", "html_url": "https://github.com/kshitij12345", "followers_url": "https://api.github.com/users/kshitij12345/followers", "following_url": "https://api.github.com/users/kshitij12345/following{/other_user}", "gists_url": "https://api.github.com/users/kshitij12345/gists{/gist_id}", "starred_url": "https://api.github.com/users/kshitij12345/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kshitij12345/subscriptions", "organizations_url": "https://api.github.com/users/kshitij12345/orgs", "repos_url": "https://api.github.com/users/kshitij12345/repos", "events_url": "https://api.github.com/users/kshitij12345/events{/privacy}", "received_events_url": "https://api.github.com/users/kshitij12345/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-14T19:42:22Z", "updated_at": "2018-07-14T19:42:22Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> I have tried and here is my opinion from what I understand</p>\n<p>In the derivatives.yaml,</p>\n<pre><code>- name: embedding(Tensor weight, Tensor indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse)\n  weight: embedding_backward(grad, indices, weight.size(0), padding_idx, scale_grad_by_freq, sparse)\n\n- name: _embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int64_t mode, bool sparse)\n  weight: _embedding_bag_backward(grad, indices, offsets, result1, result2, result3, weight.size(0), scale_grad_by_freq, mode, sparse)\n\n</code></pre>\n<p>We'll need to pass in <code>weight</code> as an argument to <code>embedding_backward</code> for the <code>embedding_double_backward</code> to update gradient on weight, but that will also require minor changes in <code>embedding_bag_sparse_backward</code> as it <a href=\"https://github.com/pytorch/pytorch/blob/a4f63576b6915a74cba5659d6699ca70e141fe70/aten/src/ATen/native/EmbeddingBag.cpp#L339-L354\">calls</a>  <code>embedding_backward</code>.</p>\n<p>Also <code>embedding_backward</code> <a href=\"https://github.com/pytorch/pytorch/blob/a4f63576b6915a74cba5659d6699ca70e141fe70/aten/src/ATen/native/Embedding.cpp#L34-L44\">calls</a> <code>embedding_dense_backward</code> and <code>embedding_sparse_backward</code>. Thus each will need its own version of double backward.</p>\n<p>So I believe that as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> suggested, we should opt for fused index_add_ + mul.<br>\nPlease let me know what you think.<br>\nThank You.</p>", "body_text": "@SsnL I have tried and here is my opinion from what I understand\nIn the derivatives.yaml,\n- name: embedding(Tensor weight, Tensor indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse)\n  weight: embedding_backward(grad, indices, weight.size(0), padding_idx, scale_grad_by_freq, sparse)\n\n- name: _embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int64_t mode, bool sparse)\n  weight: _embedding_bag_backward(grad, indices, offsets, result1, result2, result3, weight.size(0), scale_grad_by_freq, mode, sparse)\n\n\nWe'll need to pass in weight as an argument to embedding_backward for the embedding_double_backward to update gradient on weight, but that will also require minor changes in embedding_bag_sparse_backward as it calls  embedding_backward.\nAlso embedding_backward calls embedding_dense_backward and embedding_sparse_backward. Thus each will need its own version of double backward.\nSo I believe that as @t-vi suggested, we should opt for fused index_add_ + mul.\nPlease let me know what you think.\nThank You.", "body": "@SsnL I have tried and here is my opinion from what I understand\r\n\r\nIn the derivatives.yaml,\r\n\r\n```\r\n- name: embedding(Tensor weight, Tensor indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse)\r\n  weight: embedding_backward(grad, indices, weight.size(0), padding_idx, scale_grad_by_freq, sparse)\r\n\r\n- name: _embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq, int64_t mode, bool sparse)\r\n  weight: _embedding_bag_backward(grad, indices, offsets, result1, result2, result3, weight.size(0), scale_grad_by_freq, mode, sparse)\r\n\r\n```\r\n\r\nWe'll need to pass in `weight` as an argument to `embedding_backward` for the `embedding_double_backward` to update gradient on weight, but that will also require minor changes in `embedding_bag_sparse_backward` as it [calls](https://github.com/pytorch/pytorch/blob/a4f63576b6915a74cba5659d6699ca70e141fe70/aten/src/ATen/native/EmbeddingBag.cpp#L339-L354)  `embedding_backward`.\r\n\r\nAlso `embedding_backward` [calls](https://github.com/pytorch/pytorch/blob/a4f63576b6915a74cba5659d6699ca70e141fe70/aten/src/ATen/native/Embedding.cpp#L34-L44) `embedding_dense_backward` and `embedding_sparse_backward`. Thus each will need its own version of double backward.\r\n\r\nSo I believe that as @t-vi suggested, we should opt for fused index_add_ + mul. \r\nPlease let me know what you think. \r\nThank You."}