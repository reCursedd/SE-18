{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424557592", "html_url": "https://github.com/pytorch/pytorch/pull/9078#issuecomment-424557592", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9078", "id": 424557592, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDU1NzU5Mg==", "user": {"login": "pooryapzm", "id": 28755567, "node_id": "MDQ6VXNlcjI4NzU1NTY3", "avatar_url": "https://avatars2.githubusercontent.com/u/28755567?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pooryapzm", "html_url": "https://github.com/pooryapzm", "followers_url": "https://api.github.com/users/pooryapzm/followers", "following_url": "https://api.github.com/users/pooryapzm/following{/other_user}", "gists_url": "https://api.github.com/users/pooryapzm/gists{/gist_id}", "starred_url": "https://api.github.com/users/pooryapzm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pooryapzm/subscriptions", "organizations_url": "https://api.github.com/users/pooryapzm/orgs", "repos_url": "https://api.github.com/users/pooryapzm/repos", "events_url": "https://api.github.com/users/pooryapzm/events{/privacy}", "received_events_url": "https://api.github.com/users/pooryapzm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T01:50:01Z", "updated_at": "2018-09-26T01:53:17Z", "author_association": "NONE", "body_html": "<blockquote>\n<blockquote>\n<blockquote>\n<p>I installed pytorch from two source codes:</p>\n<pre><code>Commit 6dcaa47 from your repo (kshitij12345/pytorch:master)\nCommit ae1a972 from original pytorch:master (Which is the last commit your repo has from the pytorch:master)\nSo, there shouldn't be any problem with the single backward call for both. But I get the mentioned error from the former one while the latter one was fine.\n</code></pre>\n<p>It seems that your commit causes this error (btw, I'm not sure). I'll double check and will have a look and let you know if I found anything.</p>\n</blockquote>\n<p>In that case can you please share a minimal code snippet that produces the given error, so even I can check.<br>\nThank You.</p>\n</blockquote>\n<p>I am trying to write a minimal code to reproduce the given error.<br>\nMeanwhile, you can simply reproduce the error by training a simple model with OpenNMT. If you train the very example in their README, you will get the same error (there is no need for double gradients there).<br>\nAs far as I understand, the issue is with the (single) backward of the embedding table in both encoder and decoder.<br>\n<a href=\"https://github.com/OpenNMT/OpenNMT-py\">https://github.com/OpenNMT/OpenNMT-py</a></p>\n</blockquote>\n<p>Here is the minimal code to replicate the error. Sorry, but you need OpenNMT to import Embeddings.</p>\n<pre><code>import torch\n\nfrom onmt.modules.embeddings import Embeddings\n\nclass Test(torch.nn.Module):\n\n    def __init__(self):\n        super(Test, self).__init__()\n        self.dense = torch.nn.Linear(100, 1)\n        self.oembd = Embeddings(word_vec_size=100,\n                   position_encoding=False,\n                   dropout=0.3,\n                   word_padding_idx=1,\n                   word_vocab_size=1000)\n\n    def forward(self, inp):\n        inp = self.oembd(inp)\n        return self.dense(inp)\n\ntest = Test()\n# test.cuda()\ninp = torch.tensor([1, 1, 2, 1, 1])\ninp=inp.unsqueeze(0).unsqueeze(-1)\nout = test(inp)\nraw_loss = out.mean(dim=1)\n\nloss_grad = torch.autograd.grad(outputs=raw_loss,\n                                inputs=list(test.parameters(recurse=False)),\n                                retain_graph=True, create_graph=True, only_inputs=True)\n</code></pre>\n<p>If you run this code with your version of PyTorch (I mean commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/6dcaa475e617f9903afa733c2ea6c0a86f7c7357/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/6dcaa475e617f9903afa733c2ea6c0a86f7c7357\"><tt>6dcaa47</tt></a>) you'll see the error while the original version (<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/ae1a972d78950abc4dab372f496914b5e78b9637/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/ae1a972d78950abc4dab372f496914b5e78b9637\"><tt>ae1a972</tt></a>) doesn't generate error.</p>", "body_text": "I installed pytorch from two source codes:\nCommit 6dcaa47 from your repo (kshitij12345/pytorch:master)\nCommit ae1a972 from original pytorch:master (Which is the last commit your repo has from the pytorch:master)\nSo, there shouldn't be any problem with the single backward call for both. But I get the mentioned error from the former one while the latter one was fine.\n\nIt seems that your commit causes this error (btw, I'm not sure). I'll double check and will have a look and let you know if I found anything.\n\nIn that case can you please share a minimal code snippet that produces the given error, so even I can check.\nThank You.\n\nI am trying to write a minimal code to reproduce the given error.\nMeanwhile, you can simply reproduce the error by training a simple model with OpenNMT. If you train the very example in their README, you will get the same error (there is no need for double gradients there).\nAs far as I understand, the issue is with the (single) backward of the embedding table in both encoder and decoder.\nhttps://github.com/OpenNMT/OpenNMT-py\n\nHere is the minimal code to replicate the error. Sorry, but you need OpenNMT to import Embeddings.\nimport torch\n\nfrom onmt.modules.embeddings import Embeddings\n\nclass Test(torch.nn.Module):\n\n    def __init__(self):\n        super(Test, self).__init__()\n        self.dense = torch.nn.Linear(100, 1)\n        self.oembd = Embeddings(word_vec_size=100,\n                   position_encoding=False,\n                   dropout=0.3,\n                   word_padding_idx=1,\n                   word_vocab_size=1000)\n\n    def forward(self, inp):\n        inp = self.oembd(inp)\n        return self.dense(inp)\n\ntest = Test()\n# test.cuda()\ninp = torch.tensor([1, 1, 2, 1, 1])\ninp=inp.unsqueeze(0).unsqueeze(-1)\nout = test(inp)\nraw_loss = out.mean(dim=1)\n\nloss_grad = torch.autograd.grad(outputs=raw_loss,\n                                inputs=list(test.parameters(recurse=False)),\n                                retain_graph=True, create_graph=True, only_inputs=True)\n\nIf you run this code with your version of PyTorch (I mean commit 6dcaa47) you'll see the error while the original version (ae1a972) doesn't generate error.", "body": "> > > I installed pytorch from two source codes:\r\n> > > ```\r\n> > > Commit 6dcaa47 from your repo (kshitij12345/pytorch:master)\r\n> > > Commit ae1a972 from original pytorch:master (Which is the last commit your repo has from the pytorch:master)\r\n> > > So, there shouldn't be any problem with the single backward call for both. But I get the mentioned error from the former one while the latter one was fine.\r\n> > > ```\r\n> > > It seems that your commit causes this error (btw, I'm not sure). I'll double check and will have a look and let you know if I found anything.\r\n> > \r\n> > \r\n> > In that case can you please share a minimal code snippet that produces the given error, so even I can check.\r\n> > Thank You.\r\n> \r\n> I am trying to write a minimal code to reproduce the given error.\r\n> Meanwhile, you can simply reproduce the error by training a simple model with OpenNMT. If you train the very example in their README, you will get the same error (there is no need for double gradients there).\r\n> As far as I understand, the issue is with the (single) backward of the embedding table in both encoder and decoder.\r\n> https://github.com/OpenNMT/OpenNMT-py\r\n\r\nHere is the minimal code to replicate the error. Sorry, but you need OpenNMT to import Embeddings.\r\n\r\n```\r\nimport torch\r\n\r\nfrom onmt.modules.embeddings import Embeddings\r\n\r\nclass Test(torch.nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Test, self).__init__()\r\n        self.dense = torch.nn.Linear(100, 1)\r\n        self.oembd = Embeddings(word_vec_size=100,\r\n                   position_encoding=False,\r\n                   dropout=0.3,\r\n                   word_padding_idx=1,\r\n                   word_vocab_size=1000)\r\n\r\n    def forward(self, inp):\r\n        inp = self.oembd(inp)\r\n        return self.dense(inp)\r\n\r\ntest = Test()\r\n# test.cuda()\r\ninp = torch.tensor([1, 1, 2, 1, 1])\r\ninp=inp.unsqueeze(0).unsqueeze(-1)\r\nout = test(inp)\r\nraw_loss = out.mean(dim=1)\r\n\r\nloss_grad = torch.autograd.grad(outputs=raw_loss,\r\n                                inputs=list(test.parameters(recurse=False)),\r\n                                retain_graph=True, create_graph=True, only_inputs=True)\r\n```\r\n\r\nIf you run this code with your version of PyTorch (I mean commit 6dcaa47) you'll see the error while the original version (ae1a972) doesn't generate error."}