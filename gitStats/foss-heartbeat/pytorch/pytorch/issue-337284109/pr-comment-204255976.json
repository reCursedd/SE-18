{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204255976", "pull_request_review_id": 139302276, "id": 204255976, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDI1NTk3Ng==", "diff_hunk": "@@ -81,31 +81,52 @@ Tensor embedding_sparse_backward(\n   return sparse_type._sparse_coo_tensor_unsafe(index, values, weight_size);\n }\n \n-Tensor embedding_dense_backward_cpu(\n+Tensor embedding_dense_backward(\n     const Tensor & grad_, const Tensor & indices, int64_t num_weights,\n     int64_t padding_idx, bool scale_grad_by_freq) {\n \n   auto indices_arg = TensorArg(indices, \"indices\", 2);\n   checkScalarType(\"embedding_backward\", indices_arg, kLong);\n \n   auto indices_contig = indices.contiguous();\n-  auto indices_data = indices_contig.data<int64_t>();\n-  int64_t numel = indices.numel();\n+  int64_t numel = indices_contig.numel();\n+  auto flat_indices = indices_contig.view(-1);\n \n   std::unique_ptr<int64_t[]> counts;\n   if (scale_grad_by_freq) {\n     counts.reset(new int64_t[num_weights]);\n     for (int i = 0; i < numel; i++) {\n-      counts[indices_data[i]] = 0;\n+      counts[flat_indices[i].toCLong()] = 0;", "path": "aten/src/ATen/native/Embedding.cpp", "position": null, "original_position": 23, "commit_id": "0727eb87f25e804443d33e47edc8af999eb3803b", "original_commit_id": "3dcb13d967f8f1296d570e35655040bec76c21dc", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "Is this run for Cuda as well? This would look very inefficient.\r\n\r\nFor reference, I would have expected embedding_backward for the scaled by inverse frequency case to look something like backward in the following\r\n```\r\nclass MyEmbeddingWithScaling(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, inp, weight):\r\n        ctx._weight_shape = weight.shape\r\n        ctx.save_for_backward(inp)\r\n        return torch.nn.functional.embedding(inp, weight)\r\n    @staticmethod\r\n    def backward(ctx, grad_out):\r\n        inp, = ctx.saved_tensors\r\n        inp = inp.continuous().view(-1)\r\n        w_num, w_dim = ctx._weight_shape\r\n        counts = weight.new_zeros(w_num)\r\n        counts.index_add_(0, inp, grad_out.new_ones(1).expand_as(inp))\r\n        freq_weight = 1/counts.index_select(0, inp)\r\n        grad_in = grad_out.new_zeros(w_num, w_dim)\r\n        grad_in.index_add_(0, inp, freq_weight[:, None]*grad_out) # replace by fused op\r\n        return None, grad_in\r\n```\r\nbut with a fused op in the index_add_.\r\n(I might have done something wrong, but even the above seems to have hardly any performance difference compared to `F.embedding` over the current implementation in my silly benchmarking with 100000x100 embedding and 10000 indices.)\r\n", "created_at": "2018-07-22T21:43:22Z", "updated_at": "2018-11-23T15:47:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/9078#discussion_r204255976", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9078", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204255976"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9078#discussion_r204255976"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9078"}}, "body_html": "<p>Is this run for Cuda as well? This would look very inefficient.</p>\n<p>For reference, I would have expected embedding_backward for the scaled by inverse frequency case to look something like backward in the following</p>\n<pre><code>class MyEmbeddingWithScaling(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, inp, weight):\n        ctx._weight_shape = weight.shape\n        ctx.save_for_backward(inp)\n        return torch.nn.functional.embedding(inp, weight)\n    @staticmethod\n    def backward(ctx, grad_out):\n        inp, = ctx.saved_tensors\n        inp = inp.continuous().view(-1)\n        w_num, w_dim = ctx._weight_shape\n        counts = weight.new_zeros(w_num)\n        counts.index_add_(0, inp, grad_out.new_ones(1).expand_as(inp))\n        freq_weight = 1/counts.index_select(0, inp)\n        grad_in = grad_out.new_zeros(w_num, w_dim)\n        grad_in.index_add_(0, inp, freq_weight[:, None]*grad_out) # replace by fused op\n        return None, grad_in\n</code></pre>\n<p>but with a fused op in the index_add_.<br>\n(I might have done something wrong, but even the above seems to have hardly any performance difference compared to <code>F.embedding</code> over the current implementation in my silly benchmarking with 100000x100 embedding and 10000 indices.)</p>", "body_text": "Is this run for Cuda as well? This would look very inefficient.\nFor reference, I would have expected embedding_backward for the scaled by inverse frequency case to look something like backward in the following\nclass MyEmbeddingWithScaling(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, inp, weight):\n        ctx._weight_shape = weight.shape\n        ctx.save_for_backward(inp)\n        return torch.nn.functional.embedding(inp, weight)\n    @staticmethod\n    def backward(ctx, grad_out):\n        inp, = ctx.saved_tensors\n        inp = inp.continuous().view(-1)\n        w_num, w_dim = ctx._weight_shape\n        counts = weight.new_zeros(w_num)\n        counts.index_add_(0, inp, grad_out.new_ones(1).expand_as(inp))\n        freq_weight = 1/counts.index_select(0, inp)\n        grad_in = grad_out.new_zeros(w_num, w_dim)\n        grad_in.index_add_(0, inp, freq_weight[:, None]*grad_out) # replace by fused op\n        return None, grad_in\n\nbut with a fused op in the index_add_.\n(I might have done something wrong, but even the above seems to have hardly any performance difference compared to F.embedding over the current implementation in my silly benchmarking with 100000x100 embedding and 10000 indices.)"}