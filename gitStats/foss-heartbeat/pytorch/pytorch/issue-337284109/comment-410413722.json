{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/410413722", "html_url": "https://github.com/pytorch/pytorch/pull/9078#issuecomment-410413722", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9078", "id": 410413722, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMDQxMzcyMg==", "user": {"login": "kshitij12345", "id": 19503980, "node_id": "MDQ6VXNlcjE5NTAzOTgw", "avatar_url": "https://avatars3.githubusercontent.com/u/19503980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kshitij12345", "html_url": "https://github.com/kshitij12345", "followers_url": "https://api.github.com/users/kshitij12345/followers", "following_url": "https://api.github.com/users/kshitij12345/following{/other_user}", "gists_url": "https://api.github.com/users/kshitij12345/gists{/gist_id}", "starred_url": "https://api.github.com/users/kshitij12345/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kshitij12345/subscriptions", "organizations_url": "https://api.github.com/users/kshitij12345/orgs", "repos_url": "https://api.github.com/users/kshitij12345/repos", "events_url": "https://api.github.com/users/kshitij12345/events{/privacy}", "received_events_url": "https://api.github.com/users/kshitij12345/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-04T01:49:47Z", "updated_at": "2018-08-04T18:41:42Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20787943\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/t-vi\">@t-vi</a> Here are the results of a quick benchmark that I ran. Let me know if the method of benchmarking seems valid or not.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> please let me know if you have any benchmarks for this especially CUDA verison, so that this implementation can be verified based on it.</p>\n<p>master is the version till <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/6456b944fd3dfe1b7db830b27afd44b15ba5a6e9/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/6456b944fd3dfe1b7db830b27afd44b15ba5a6e9\"><tt>6456b94</tt></a> and the new code was rebased on this master.</p>\n<p>Both code were compiled with OpenMP and CUDA. The implementation in master uses OpenMp for CPU when number of indices is more than 1000. Also, CUDA implementation resorts to different approach if number of indices is greater than 784. Both cases are checked in the below table.</p>\n<p>For each test, <code>vocabulary_size = 100000</code> and <code>embedding_dimension = 1000</code></p>\n<table>\n<thead>\n<tr>\n<th>Device</th>\n<th>Index Size</th>\n<th>Master</th>\n<th>embedding_with_index_add</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>CPU</td>\n<td>400</td>\n<td>mean = 7.6628<br>std = 0.0594<br></td>\n<td>mean = 7.6094<br>  std = 0.0379</td>\n</tr>\n<tr>\n<td>CPU</td>\n<td>8000</td>\n<td>mean = 9.3580<br>std = 0.0504<br></td>\n<td>mean = 9.4358<br>  std = 0.0396</td>\n</tr>\n<tr>\n<td>CUDA</td>\n<td>400</td>\n<td>mean = 16.5607<br>std = 0.1783<br></td>\n<td>mean = 16.6214<br>  std =0.0960</td>\n</tr>\n<tr>\n<td>CUDA</td>\n<td>8000</td>\n<td>mean = 16.5455<br>std = 0.0509<br></td>\n<td>mean = 16.5029<br>  std =0.0426</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Note: mean and std are calculated for 100 iterations repeated 10 times using timeit</strong></p>\n<p>The above table leads me to believe that the new implementation does not degrade the performance.<br>\nAlso the it passes the <code>short_perf_test_gpu</code>  in the jenkins test ( which the earlier attempted implementation used to fail ).</p>\n<p><a href=\"https://github.com/pytorch/pytorch/files/2259129/embedding_with_index_add.txt\">embedding_with_index_add.txt</a><br>\n<a href=\"https://github.com/pytorch/pytorch/files/2259131/master.txt\">master.txt</a></p>\n<p><strong>Test Code</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> timeit\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\ntorch.manual_seed(<span class=\"pl-c1\">1</span>)\nfilename <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>embedding_with_index_add.txt<span class=\"pl-pds\">'</span></span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> 'master.txt'</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">update_stats</span>(<span class=\"pl-smi\">dev</span>, <span class=\"pl-smi\">timings</span>, <span class=\"pl-smi\">indices_len</span>):\n    stats <span class=\"pl-k\">=</span> {}\n    stats[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dev<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> dev\n    stats[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>timings<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> timings\n    stats[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mean<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> np.mean(timings)\n    stats[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>std<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> np.std(timings)\n    stats[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>indices_len<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> indices_len\n    <span class=\"pl-k\">return</span> stats\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">fmt_to_string</span>(<span class=\"pl-smi\">dict</span>, <span class=\"pl-smi\">key</span>):\n    <span class=\"pl-k\">return</span> key <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>:<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(dict[key]) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">save_stats</span>(<span class=\"pl-smi\">stats</span>):\n    <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(filename,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>a+<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n        f.write(fmt_to_string(stats,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dev<span class=\"pl-pds\">'</span></span>))\n        f.write(fmt_to_string(stats,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>indices_len<span class=\"pl-pds\">'</span></span>))\n        f.write(fmt_to_string(stats,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mean<span class=\"pl-pds\">'</span></span>))\n        f.write(fmt_to_string(stats,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>std<span class=\"pl-pds\">'</span></span>))\n        f.write(fmt_to_string(stats,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>timings<span class=\"pl-pds\">'</span></span>))\n        f.write(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Test</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">weight</span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">=</span> weight\n        <span class=\"pl-c1\">self</span>.embd <span class=\"pl-k\">=</span> torch.nn.functional.embedding\n        <span class=\"pl-c1\">self</span>.dense <span class=\"pl-k\">=</span> torch.nn.Linear(weight.size(<span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inp</span>):\n        inp <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.embd(inp,<span class=\"pl-c1\">self</span>.weight)\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.dense(inp)\n\nstmt <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">out = test(inp)</span>\n<span class=\"pl-s\">raw_loss = out.mean(dim=0)</span>\n<span class=\"pl-s\">raw_loss.backward()<span class=\"pl-pds\">\"\"\"</span></span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_setup</span>(<span class=\"pl-smi\">dev</span>, <span class=\"pl-smi\">index_len</span>, <span class=\"pl-smi\">vocab_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100000</span>, <span class=\"pl-smi\">embedding_dim</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>):\n    inp <span class=\"pl-k\">=</span> torch.randint(<span class=\"pl-c1\">0</span>, vocab_size, (index_len,))\n    inp <span class=\"pl-k\">=</span> inp.type(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.LongTensor<span class=\"pl-pds\">'</span></span>).to(dev)\n    weight <span class=\"pl-k\">=</span> torch.randn(vocab_size, embedding_dim, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>).to(dev)\n    test <span class=\"pl-k\">=</span> Test(weight)\n    test.to(dev)\n    <span class=\"pl-k\">return</span> test,inp\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>##################################################################</span>\n\ndev <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>\ntest,inp <span class=\"pl-k\">=</span> test_setup(<span class=\"pl-v\">dev</span> <span class=\"pl-k\">=</span> dev, <span class=\"pl-v\">index_len</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">400</span>)\n\ntimings <span class=\"pl-k\">=</span> timeit.repeat(<span class=\"pl-v\">stmt</span><span class=\"pl-k\">=</span>stmt,<span class=\"pl-v\">setup</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>from __main__ import torch,test,inp<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">repeat</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\ntime_arr <span class=\"pl-k\">=</span> np.array(timings)\nindices_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(inp.numel())\nstats <span class=\"pl-k\">=</span> update_stats(dev, time_arr, indices_len)\nsave_stats(stats)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>1st Done<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>################################################################</span>\n\ndev <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>\ntest,inp <span class=\"pl-k\">=</span> test_setup(<span class=\"pl-v\">dev</span> <span class=\"pl-k\">=</span> dev, <span class=\"pl-v\">index_len</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8000</span>)\n\ntimings <span class=\"pl-k\">=</span> timeit.repeat(<span class=\"pl-v\">stmt</span><span class=\"pl-k\">=</span>stmt,<span class=\"pl-v\">setup</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>from __main__ import torch,test,inp<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">repeat</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\ntime_arr <span class=\"pl-k\">=</span> np.array(timings)\nindices_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(inp.numel())\nstats <span class=\"pl-k\">=</span> update_stats(dev, time_arr, indices_len)\nsave_stats(stats)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>2nd Done<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>###################################################################</span>\n\ndev <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>\ntest,inp <span class=\"pl-k\">=</span> test_setup(<span class=\"pl-v\">dev</span> <span class=\"pl-k\">=</span> dev, <span class=\"pl-v\">index_len</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">400</span>)\n\ntimings <span class=\"pl-k\">=</span> timeit.repeat(<span class=\"pl-v\">stmt</span><span class=\"pl-k\">=</span>stmt,<span class=\"pl-v\">setup</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>from __main__ import torch,test,inp<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">repeat</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\ntime_arr <span class=\"pl-k\">=</span> np.array(timings)\nindices_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(inp.numel())\nstats <span class=\"pl-k\">=</span> update_stats(dev, time_arr, indices_len)\nsave_stats(stats)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>3rd Done<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>################################################################</span>\n\ndev <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>\ntest,inp <span class=\"pl-k\">=</span> test_setup(<span class=\"pl-v\">dev</span> <span class=\"pl-k\">=</span> dev, <span class=\"pl-v\">index_len</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8000</span>)\n\ntimings <span class=\"pl-k\">=</span> timeit.repeat(<span class=\"pl-v\">stmt</span><span class=\"pl-k\">=</span>stmt,<span class=\"pl-v\">setup</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>from __main__ import torch,test,inp<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>, <span class=\"pl-v\">repeat</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\ntime_arr <span class=\"pl-k\">=</span> np.array(timings)\nindices_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(inp.numel())\nstats <span class=\"pl-k\">=</span> update_stats(dev, time_arr, indices_len)\nsave_stats(stats)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>4th Done<span class=\"pl-pds\">'</span></span>)\n</pre></div>", "body_text": "@colesbury @t-vi Here are the results of a quick benchmark that I ran. Let me know if the method of benchmarking seems valid or not.\n@colesbury please let me know if you have any benchmarks for this especially CUDA verison, so that this implementation can be verified based on it.\nmaster is the version till 6456b94 and the new code was rebased on this master.\nBoth code were compiled with OpenMP and CUDA. The implementation in master uses OpenMp for CPU when number of indices is more than 1000. Also, CUDA implementation resorts to different approach if number of indices is greater than 784. Both cases are checked in the below table.\nFor each test, vocabulary_size = 100000 and embedding_dimension = 1000\n\n\n\nDevice\nIndex Size\nMaster\nembedding_with_index_add\n\n\n\n\nCPU\n400\nmean = 7.6628std = 0.0594\nmean = 7.6094  std = 0.0379\n\n\nCPU\n8000\nmean = 9.3580std = 0.0504\nmean = 9.4358  std = 0.0396\n\n\nCUDA\n400\nmean = 16.5607std = 0.1783\nmean = 16.6214  std =0.0960\n\n\nCUDA\n8000\nmean = 16.5455std = 0.0509\nmean = 16.5029  std =0.0426\n\n\n\nNote: mean and std are calculated for 100 iterations repeated 10 times using timeit\nThe above table leads me to believe that the new implementation does not degrade the performance.\nAlso the it passes the short_perf_test_gpu  in the jenkins test ( which the earlier attempted implementation used to fail ).\nembedding_with_index_add.txt\nmaster.txt\nTest Code\nimport torch\nimport timeit\nimport numpy as np\ntorch.manual_seed(1)\nfilename = 'embedding_with_index_add.txt' # 'master.txt'\n\ndef update_stats(dev, timings, indices_len):\n    stats = {}\n    stats['dev'] = dev\n    stats['timings'] = timings\n    stats['mean'] = np.mean(timings)\n    stats['std'] = np.std(timings)\n    stats['indices_len'] = indices_len\n    return stats\n\ndef fmt_to_string(dict, key):\n    return key + ':' + str(dict[key]) + '\\n'\n\ndef save_stats(stats):\n    with open(filename,'a+') as f:\n        f.write(fmt_to_string(stats,'dev'))\n        f.write(fmt_to_string(stats,'indices_len'))\n        f.write(fmt_to_string(stats,'mean'))\n        f.write(fmt_to_string(stats,'std'))\n        f.write(fmt_to_string(stats,'timings'))\n        f.write('\\n')\n\nclass Test(torch.nn.Module):\n    def __init__(self, weight):\n        super().__init__()\n        self.weight = weight\n        self.embd = torch.nn.functional.embedding\n        self.dense = torch.nn.Linear(weight.size(1), 1)\n    def forward(self, inp):\n        inp = self.embd(inp,self.weight)\n        return self.dense(inp)\n\nstmt = \"\"\"\nout = test(inp)\nraw_loss = out.mean(dim=0)\nraw_loss.backward()\"\"\"\n\ndef test_setup(dev, index_len, vocab_size = 100000, embedding_dim = 1000):\n    inp = torch.randint(0, vocab_size, (index_len,))\n    inp = inp.type('torch.LongTensor').to(dev)\n    weight = torch.randn(vocab_size, embedding_dim, requires_grad=True).to(dev)\n    test = Test(weight)\n    test.to(dev)\n    return test,inp\n\n\n###################################################################\n\ndev = 'cpu'\ntest,inp = test_setup(dev = dev, index_len = 400)\n\ntimings = timeit.repeat(stmt=stmt,setup=\"from __main__ import torch,test,inp\", number=100, repeat=10)\ntime_arr = np.array(timings)\nindices_len = int(inp.numel())\nstats = update_stats(dev, time_arr, indices_len)\nsave_stats(stats)\nprint('1st Done')\n#################################################################\n\ndev = 'cpu'\ntest,inp = test_setup(dev = dev, index_len = 8000)\n\ntimings = timeit.repeat(stmt=stmt,setup=\"from __main__ import torch,test,inp\", number=100, repeat=10)\ntime_arr = np.array(timings)\nindices_len = int(inp.numel())\nstats = update_stats(dev, time_arr, indices_len)\nsave_stats(stats)\nprint('2nd Done')\n####################################################################\n\ndev = 'cuda'\ntest,inp = test_setup(dev = dev, index_len = 400)\n\ntimings = timeit.repeat(stmt=stmt,setup=\"from __main__ import torch,test,inp\", number=100, repeat=10)\ntime_arr = np.array(timings)\nindices_len = int(inp.numel())\nstats = update_stats(dev, time_arr, indices_len)\nsave_stats(stats)\nprint('3rd Done')\n#################################################################\n\ndev = 'cuda'\ntest,inp = test_setup(dev = dev, index_len = 8000)\n\ntimings = timeit.repeat(stmt=stmt,setup=\"from __main__ import torch,test,inp\", number=100, repeat=10)\ntime_arr = np.array(timings)\nindices_len = int(inp.numel())\nstats = update_stats(dev, time_arr, indices_len)\nsave_stats(stats)\nprint('4th Done')", "body": "@colesbury @t-vi Here are the results of a quick benchmark that I ran. Let me know if the method of benchmarking seems valid or not.\r\n\r\n@colesbury please let me know if you have any benchmarks for this especially CUDA verison, so that this implementation can be verified based on it.\r\n\r\nmaster is the version till https://github.com/pytorch/pytorch/commit/6456b944fd3dfe1b7db830b27afd44b15ba5a6e9 and the new code was rebased on this master.\r\n\r\nBoth code were compiled with OpenMP and CUDA. The implementation in master uses OpenMp for CPU when number of indices is more than 1000. Also, CUDA implementation resorts to different approach if number of indices is greater than 784. Both cases are checked in the below table.\r\n\r\nFor each test, `vocabulary_size = 100000` and `embedding_dimension = 1000` \r\n\r\n| Device | Index Size | Master | embedding_with_index_add |\r\n| --- | --- | --- | --- |\r\n|  CPU    |    400 |  mean = 7.6628<br/>std = 0.0594<br/> |mean = 7.6094<br/>  std = 0.0379 |\r\n|  CPU    |    8000 |  mean = 9.3580<br/>std = 0.0504<br/> |mean = 9.4358<br/>  std = 0.0396 |\r\n|  CUDA    |    400 |  mean = 16.5607<br/>std = 0.1783<br/>|mean = 16.6214<br/>  std =0.0960|\r\n|  CUDA    |    8000 |  mean = 16.5455<br/>std = 0.0509<br/> |mean = 16.5029<br/>  std =0.0426|\r\n\r\n**Note: mean and std are calculated for 100 iterations repeated 10 times using timeit**\r\n\r\nThe above table leads me to believe that the new implementation does not degrade the performance.\r\nAlso the it passes the `short_perf_test_gpu`  in the jenkins test ( which the earlier attempted implementation used to fail ).\r\n\r\n\r\n[embedding_with_index_add.txt](https://github.com/pytorch/pytorch/files/2259129/embedding_with_index_add.txt)\r\n[master.txt](https://github.com/pytorch/pytorch/files/2259131/master.txt)\r\n\r\n**Test Code**\r\n``` python\r\n\r\nimport torch\r\nimport timeit\r\nimport numpy as np\r\ntorch.manual_seed(1)\r\nfilename = 'embedding_with_index_add.txt' # 'master.txt'\r\n\r\ndef update_stats(dev, timings, indices_len):\r\n    stats = {}\r\n    stats['dev'] = dev\r\n    stats['timings'] = timings\r\n    stats['mean'] = np.mean(timings)\r\n    stats['std'] = np.std(timings)\r\n    stats['indices_len'] = indices_len\r\n    return stats\r\n\r\ndef fmt_to_string(dict, key):\r\n    return key + ':' + str(dict[key]) + '\\n'\r\n\r\ndef save_stats(stats):\r\n    with open(filename,'a+') as f:\r\n        f.write(fmt_to_string(stats,'dev'))\r\n        f.write(fmt_to_string(stats,'indices_len'))\r\n        f.write(fmt_to_string(stats,'mean'))\r\n        f.write(fmt_to_string(stats,'std'))\r\n        f.write(fmt_to_string(stats,'timings'))\r\n        f.write('\\n')\r\n\r\nclass Test(torch.nn.Module):\r\n    def __init__(self, weight):\r\n        super().__init__()\r\n        self.weight = weight\r\n        self.embd = torch.nn.functional.embedding\r\n        self.dense = torch.nn.Linear(weight.size(1), 1)\r\n    def forward(self, inp):\r\n        inp = self.embd(inp,self.weight)\r\n        return self.dense(inp)\r\n\r\nstmt = \"\"\"\r\nout = test(inp)\r\nraw_loss = out.mean(dim=0)\r\nraw_loss.backward()\"\"\"\r\n\r\ndef test_setup(dev, index_len, vocab_size = 100000, embedding_dim = 1000):\r\n    inp = torch.randint(0, vocab_size, (index_len,))\r\n    inp = inp.type('torch.LongTensor').to(dev)\r\n    weight = torch.randn(vocab_size, embedding_dim, requires_grad=True).to(dev)\r\n    test = Test(weight)\r\n    test.to(dev)\r\n    return test,inp\r\n\r\n\r\n###################################################################\r\n\r\ndev = 'cpu'\r\ntest,inp = test_setup(dev = dev, index_len = 400)\r\n\r\ntimings = timeit.repeat(stmt=stmt,setup=\"from __main__ import torch,test,inp\", number=100, repeat=10)\r\ntime_arr = np.array(timings)\r\nindices_len = int(inp.numel())\r\nstats = update_stats(dev, time_arr, indices_len)\r\nsave_stats(stats)\r\nprint('1st Done')\r\n#################################################################\r\n\r\ndev = 'cpu'\r\ntest,inp = test_setup(dev = dev, index_len = 8000)\r\n\r\ntimings = timeit.repeat(stmt=stmt,setup=\"from __main__ import torch,test,inp\", number=100, repeat=10)\r\ntime_arr = np.array(timings)\r\nindices_len = int(inp.numel())\r\nstats = update_stats(dev, time_arr, indices_len)\r\nsave_stats(stats)\r\nprint('2nd Done')\r\n####################################################################\r\n\r\ndev = 'cuda'\r\ntest,inp = test_setup(dev = dev, index_len = 400)\r\n\r\ntimings = timeit.repeat(stmt=stmt,setup=\"from __main__ import torch,test,inp\", number=100, repeat=10)\r\ntime_arr = np.array(timings)\r\nindices_len = int(inp.numel())\r\nstats = update_stats(dev, time_arr, indices_len)\r\nsave_stats(stats)\r\nprint('3rd Done')\r\n#################################################################\r\n\r\ndev = 'cuda'\r\ntest,inp = test_setup(dev = dev, index_len = 8000)\r\n\r\ntimings = timeit.repeat(stmt=stmt,setup=\"from __main__ import torch,test,inp\", number=100, repeat=10)\r\ntime_arr = np.array(timings)\r\nindices_len = int(inp.numel())\r\nstats = update_stats(dev, time_arr, indices_len)\r\nsave_stats(stats)\r\nprint('4th Done')\r\n\r\n```"}