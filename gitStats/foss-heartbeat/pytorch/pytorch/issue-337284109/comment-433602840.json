{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/433602840", "html_url": "https://github.com/pytorch/pytorch/pull/9078#issuecomment-433602840", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9078", "id": 433602840, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzYwMjg0MA==", "user": {"login": "kshitij12345", "id": 19503980, "node_id": "MDQ6VXNlcjE5NTAzOTgw", "avatar_url": "https://avatars3.githubusercontent.com/u/19503980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kshitij12345", "html_url": "https://github.com/kshitij12345", "followers_url": "https://api.github.com/users/kshitij12345/followers", "following_url": "https://api.github.com/users/kshitij12345/following{/other_user}", "gists_url": "https://api.github.com/users/kshitij12345/gists{/gist_id}", "starred_url": "https://api.github.com/users/kshitij12345/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kshitij12345/subscriptions", "organizations_url": "https://api.github.com/users/kshitij12345/orgs", "repos_url": "https://api.github.com/users/kshitij12345/repos", "events_url": "https://api.github.com/users/kshitij12345/events{/privacy}", "received_events_url": "https://api.github.com/users/kshitij12345/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-27T08:44:08Z", "updated_at": "2018-10-27T09:35:45Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>That looks simple enough for me to be a bit embarrassed to have suggested the weighted <code>index_add_</code> first.<br>\nYou're the better expert for embeddings than me, but:</p>\n<pre><code>* Would having a second derivative also make sense for sparse?\n\n* We are certain that embeddings are always vectors, right? If not it might make sense to pass the weight sizes to the double backward instead deriving the shape from the indices there. (But I don't know if that would be good or not.)\n</code></pre>\n</blockquote>\n<p>Even I am embarrased, for my first attempt of contributing here is taking so long with all these mistakes (also with the fact that in excitement I started work in master branch and sent PR through master )</p>\n<p>As for second point, in the little experience I have,  embeddings have always been vectors in all the courses that I have taken and literature that I have seen. (Would be interesting to see if that is not always the case).</p>", "body_text": "That looks simple enough for me to be a bit embarrassed to have suggested the weighted index_add_ first.\nYou're the better expert for embeddings than me, but:\n* Would having a second derivative also make sense for sparse?\n\n* We are certain that embeddings are always vectors, right? If not it might make sense to pass the weight sizes to the double backward instead deriving the shape from the indices there. (But I don't know if that would be good or not.)\n\n\nEven I am embarrased, for my first attempt of contributing here is taking so long with all these mistakes (also with the fact that in excitement I started work in master branch and sent PR through master )\nAs for second point, in the little experience I have,  embeddings have always been vectors in all the courses that I have taken and literature that I have seen. (Would be interesting to see if that is not always the case).", "body": "> That looks simple enough for me to be a bit embarrassed to have suggested the weighted `index_add_` first.\r\n> You're the better expert for embeddings than me, but:\r\n> \r\n>     * Would having a second derivative also make sense for sparse?\r\n> \r\n>     * We are certain that embeddings are always vectors, right? If not it might make sense to pass the weight sizes to the double backward instead deriving the shape from the indices there. (But I don't know if that would be good or not.)\r\nEven I am embarrased, for my first attempt of contributing here is taking so long with all these mistakes (also with the fact that in excitement I started work in master branch and sent PR through master )\r\n\r\nAs for second point, in the little experience I have,  embeddings have always been vectors in all the courses that I have taken and literature that I have seen. (Would be interesting to see if that is not always the case)."}