{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424832188", "html_url": "https://github.com/pytorch/pytorch/pull/9078#issuecomment-424832188", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9078", "id": 424832188, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDgzMjE4OA==", "user": {"login": "kshitij12345", "id": 19503980, "node_id": "MDQ6VXNlcjE5NTAzOTgw", "avatar_url": "https://avatars3.githubusercontent.com/u/19503980?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kshitij12345", "html_url": "https://github.com/kshitij12345", "followers_url": "https://api.github.com/users/kshitij12345/followers", "following_url": "https://api.github.com/users/kshitij12345/following{/other_user}", "gists_url": "https://api.github.com/users/kshitij12345/gists{/gist_id}", "starred_url": "https://api.github.com/users/kshitij12345/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kshitij12345/subscriptions", "organizations_url": "https://api.github.com/users/kshitij12345/orgs", "repos_url": "https://api.github.com/users/kshitij12345/repos", "events_url": "https://api.github.com/users/kshitij12345/events{/privacy}", "received_events_url": "https://api.github.com/users/kshitij12345/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T19:00:20Z", "updated_at": "2018-09-27T07:15:29Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=28755567\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pooryapzm\">@pooryapzm</a>  Indeed problem was in my part of code. Sorry. Have fixed it. Please let me know if it works for you as well. Have checked the following code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\ntorch.manual_seed(<span class=\"pl-c1\">6</span>)\n\n<span class=\"pl-k\">from</span> onmt.modules.embeddings <span class=\"pl-k\">import</span> Embeddings\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Test</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Test, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.dense <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.oembd <span class=\"pl-k\">=</span> Embeddings(<span class=\"pl-v\">word_vec_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>,\n                   <span class=\"pl-v\">position_encoding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                   <span class=\"pl-v\">dropout</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.3</span>,<span class=\"pl-v\">feat_merge</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                   <span class=\"pl-v\">word_padding_idx</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                   <span class=\"pl-v\">word_vocab_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inp</span>):\n        inp <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.oembd(inp)\n        \n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.dense(inp[<span class=\"pl-c1\">0</span>])\n\ntest <span class=\"pl-k\">=</span> Test()\ntest.cuda()\ninp <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>],<span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>)\ninp<span class=\"pl-k\">=</span>inp.unsqueeze(<span class=\"pl-c1\">0</span>).unsqueeze(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\nout <span class=\"pl-k\">=</span> test(inp)\nraw_loss <span class=\"pl-k\">=</span> out.mean(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\nloss_grad <span class=\"pl-k\">=</span> torch.autograd.grad(<span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>raw_loss,\n                                <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">list</span>(test.parameters()),\n                                <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">only_inputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\nnorm <span class=\"pl-k\">=</span> <span class=\"pl-c1\">sum</span>([param.norm()<span class=\"pl-k\">**</span><span class=\"pl-c1\">2</span> <span class=\"pl-k\">for</span> param <span class=\"pl-k\">in</span> loss_grad])\n\nloss <span class=\"pl-k\">=</span> raw_loss <span class=\"pl-k\">+</span> norm\n\nloss.backward()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Succesful<span class=\"pl-pds\">\"</span></span>)</pre></div>", "body_text": "@pooryapzm  Indeed problem was in my part of code. Sorry. Have fixed it. Please let me know if it works for you as well. Have checked the following code:\nimport torch\ntorch.manual_seed(6)\n\nfrom onmt.modules.embeddings import Embeddings\n\nclass Test(torch.nn.Module):\n\n    def __init__(self):\n        super(Test, self).__init__()\n        self.dense = torch.nn.Linear(100, 1)\n        self.oembd = Embeddings(word_vec_size=100,\n                   position_encoding=False,\n                   dropout=0.3,feat_merge=None,\n                   word_padding_idx=1,\n                   word_vocab_size=1000)\n\n    def forward(self, inp):\n        inp = self.oembd(inp)\n        \n        return self.dense(inp[0])\n\ntest = Test()\ntest.cuda()\ninp = torch.tensor([1, 1, 2, 1, 1, 2],device='cuda')\ninp=inp.unsqueeze(0).unsqueeze(-1)\nout = test(inp)\nraw_loss = out.mean(dim=1)\nloss_grad = torch.autograd.grad(outputs=raw_loss,\n                                inputs=list(test.parameters()),\n                                retain_graph=True, create_graph=True, only_inputs=True)\n\nnorm = sum([param.norm()**2 for param in loss_grad])\n\nloss = raw_loss + norm\n\nloss.backward()\nprint(\"Succesful\")", "body": "@pooryapzm  Indeed problem was in my part of code. Sorry. Have fixed it. Please let me know if it works for you as well. Have checked the following code:\r\n```python\r\nimport torch\r\ntorch.manual_seed(6)\r\n\r\nfrom onmt.modules.embeddings import Embeddings\r\n\r\nclass Test(torch.nn.Module):\r\n\r\n    def __init__(self):\r\n        super(Test, self).__init__()\r\n        self.dense = torch.nn.Linear(100, 1)\r\n        self.oembd = Embeddings(word_vec_size=100,\r\n                   position_encoding=False,\r\n                   dropout=0.3,feat_merge=None,\r\n                   word_padding_idx=1,\r\n                   word_vocab_size=1000)\r\n\r\n    def forward(self, inp):\r\n        inp = self.oembd(inp)\r\n        \r\n        return self.dense(inp[0])\r\n\r\ntest = Test()\r\ntest.cuda()\r\ninp = torch.tensor([1, 1, 2, 1, 1, 2],device='cuda')\r\ninp=inp.unsqueeze(0).unsqueeze(-1)\r\nout = test(inp)\r\nraw_loss = out.mean(dim=1)\r\nloss_grad = torch.autograd.grad(outputs=raw_loss,\r\n                                inputs=list(test.parameters()),\r\n                                retain_graph=True, create_graph=True, only_inputs=True)\r\n\r\nnorm = sum([param.norm()**2 for param in loss_grad])\r\n\r\nloss = raw_loss + norm\r\n\r\nloss.backward()\r\nprint(\"Succesful\")\r\n```"}