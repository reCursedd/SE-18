{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3356", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3356/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3356/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3356/events", "html_url": "https://github.com/pytorch/pytorch/issues/3356", "id": 269420235, "node_id": "MDU6SXNzdWUyNjk0MjAyMzU=", "number": 3356, "title": "Data sampling seems to be more complicated than necessary", "user": {"login": "IssamLaradji", "id": 3382128, "node_id": "MDQ6VXNlcjMzODIxMjg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3382128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IssamLaradji", "html_url": "https://github.com/IssamLaradji", "followers_url": "https://api.github.com/users/IssamLaradji/followers", "following_url": "https://api.github.com/users/IssamLaradji/following{/other_user}", "gists_url": "https://api.github.com/users/IssamLaradji/gists{/gist_id}", "starred_url": "https://api.github.com/users/IssamLaradji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IssamLaradji/subscriptions", "organizations_url": "https://api.github.com/users/IssamLaradji/orgs", "repos_url": "https://api.github.com/users/IssamLaradji/repos", "events_url": "https://api.github.com/users/IssamLaradji/events{/privacy}", "received_events_url": "https://api.github.com/users/IssamLaradji/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-10-29T17:58:53Z", "updated_at": "2018-06-05T01:06:24Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I love Pytorch for its flexibility and debugging friendly environment, and like Andrej Karpathy said after using Pytorch, \"I have more energy. My skin is clearer. My eye sight has improved.\"</p>\n<p>However, I am finding sampling from datasets a bit more convoluted that it needs to be. I was hoping for a way to efficiently extract samples (using multiprocessing) from the dataset by providing a batch index list as input, e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre>batch_indices <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">2</span>]\nloader.get_indices(batch_indices)</pre></div>\n<p>where <code>loader</code>is a <code>DataLoader</code>object or a <code>torch.data.Dataset</code> object. In other words, I am looking for a simple, yet flexible sampling interface.</p>\n<p>Currently, if I want to sample using a non-uniform distribution, first I have to define a sampler class for the <code>loader</code>, then within the class I have to define a generator that returns indices from a pre-defined list. Later, whenever the sampling distribution changes I have to re-create the sampler object that takes input values which are used to compute the new sampling distribution. I didn't find an easier way yet.</p>\n<p>As a result, defining the data loader would be something like,</p>\n<div class=\"highlight highlight-source-python\"><pre>loader <span class=\"pl-k\">=</span> data.DataLoader(train_set, <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>sampler(train_set), <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)<span class=\"pl-bu\">`</span></pre></div>\n<p>for the initialization step; and</p>\n<div class=\"highlight highlight-source-python\"><pre>loader.sampler <span class=\"pl-k\">=</span> sampler(train_set, some_values)</pre></div>\n<p>between epochs.</p>\n<p>This seems to give me certain restrictions (please correct me if I am wrong),</p>\n<ol>\n<li>\n<p>dynamic sampling is not well supported with this approach - consider the case where the sampling distribution or the batch size changes after every iteration (not epoch); and</p>\n</li>\n<li>\n<p>this makes it necessary to have an epoch-based outer loop. Once I tried to avoid having epochs by using <code>itertools.cycle</code> on a <code>DataLoader</code> with <code>RandomSampler</code>, but it gave me a bad memory leak.</p>\n</li>\n</ol>\n<p>Therefore, is it an issue if we have the API allow for a sampling procedure that looks like the code below ? My goal is to have a flexible data sampling interface while harnessing the multiprocessing power of <code>DataLoader</code>.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n_iters):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> 1. Sample according to `probs`</span>\n    indices <span class=\"pl-k\">=</span> np.random.choice(n, batch_size, <span class=\"pl-v\">p</span><span class=\"pl-k\">=</span>probs)\n    batch <span class=\"pl-k\">=</span> loader.get_indices(indices)\n   \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> 2. Optimization step</span>\n    opt.zero_grad()\n    loss <span class=\"pl-k\">=</span> model.compute_loss(batch)\n    loss.backward()\n    opt.step()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> 3. Change probs values according to some criterion</span>\n    probs <span class=\"pl-k\">=</span> get_newProbs(probs, loss)</pre></div>\n<p>If the <code>DataLoader</code> <strong>api</strong> can't be changed, what if we add three extra functions to <code>torch.data.Dataset</code> ?</p>\n<ol>\n<li><code>.get_indices(batch_indices, collate_fn)</code> - to extract a batch from indices using a collate function;</li>\n<li><code>.spawn_workers(num_workers)</code> - to initialize workers for sampling with multiprocessing; and</li>\n<li><code>.terminate_workers()</code> - to terminate the worker threads.</li>\n</ol>\n<p>What do you think ?</p>\n<p>I would be happy to submit a pull request for this feature!</p>", "body_text": "I love Pytorch for its flexibility and debugging friendly environment, and like Andrej Karpathy said after using Pytorch, \"I have more energy. My skin is clearer. My eye sight has improved.\"\nHowever, I am finding sampling from datasets a bit more convoluted that it needs to be. I was hoping for a way to efficiently extract samples (using multiprocessing) from the dataset by providing a batch index list as input, e.g.\nbatch_indices = [5,4,2]\nloader.get_indices(batch_indices)\nwhere loaderis a DataLoaderobject or a torch.data.Dataset object. In other words, I am looking for a simple, yet flexible sampling interface.\nCurrently, if I want to sample using a non-uniform distribution, first I have to define a sampler class for the loader, then within the class I have to define a generator that returns indices from a pre-defined list. Later, whenever the sampling distribution changes I have to re-create the sampler object that takes input values which are used to compute the new sampling distribution. I didn't find an easier way yet.\nAs a result, defining the data loader would be something like,\nloader = data.DataLoader(train_set, sampler=sampler(train_set), num_workers=2)`\nfor the initialization step; and\nloader.sampler = sampler(train_set, some_values)\nbetween epochs.\nThis seems to give me certain restrictions (please correct me if I am wrong),\n\n\ndynamic sampling is not well supported with this approach - consider the case where the sampling distribution or the batch size changes after every iteration (not epoch); and\n\n\nthis makes it necessary to have an epoch-based outer loop. Once I tried to avoid having epochs by using itertools.cycle on a DataLoader with RandomSampler, but it gave me a bad memory leak.\n\n\nTherefore, is it an issue if we have the API allow for a sampling procedure that looks like the code below ? My goal is to have a flexible data sampling interface while harnessing the multiprocessing power of DataLoader.\nfor i in range(n_iters):\n    # 1. Sample according to `probs`\n    indices = np.random.choice(n, batch_size, p=probs)\n    batch = loader.get_indices(indices)\n   \n    # 2. Optimization step\n    opt.zero_grad()\n    loss = model.compute_loss(batch)\n    loss.backward()\n    opt.step()\n\n    # 3. Change probs values according to some criterion\n    probs = get_newProbs(probs, loss)\nIf the DataLoader api can't be changed, what if we add three extra functions to torch.data.Dataset ?\n\n.get_indices(batch_indices, collate_fn) - to extract a batch from indices using a collate function;\n.spawn_workers(num_workers) - to initialize workers for sampling with multiprocessing; and\n.terminate_workers() - to terminate the worker threads.\n\nWhat do you think ?\nI would be happy to submit a pull request for this feature!", "body": "I love Pytorch for its flexibility and debugging friendly environment, and like Andrej Karpathy said after using Pytorch, \"I have more energy. My skin is clearer. My eye sight has improved.\"\r\n\r\nHowever, I am finding sampling from datasets a bit more convoluted that it needs to be. I was hoping for a way to efficiently extract samples (using multiprocessing) from the dataset by providing a batch index list as input, e.g. \r\n```python\r\nbatch_indices = [5,4,2]\r\nloader.get_indices(batch_indices)\r\n```\r\nwhere `loader`is a `DataLoader`object or a `torch.data.Dataset` object. In other words, I am looking for a simple, yet flexible sampling interface.\r\n\r\nCurrently, if I want to sample using a non-uniform distribution, first I have to define a sampler class for the `loader`, then within the class I have to define a generator that returns indices from a pre-defined list. Later, whenever the sampling distribution changes I have to re-create the sampler object that takes input values which are used to compute the new sampling distribution. I didn't find an easier way yet.\r\n\r\nAs a result, defining the data loader would be something like,\r\n```python\r\nloader = data.DataLoader(train_set, sampler=sampler(train_set), num_workers=2)`\r\n```\r\nfor the initialization step; and\r\n```python\r\nloader.sampler = sampler(train_set, some_values)\r\n```\r\nbetween epochs.\r\n\r\nThis seems to give me certain restrictions (please correct me if I am wrong),\r\n1. dynamic sampling is not well supported with this approach - consider the case where the sampling distribution or the batch size changes after every iteration (not epoch); and\r\n\r\n2. this makes it necessary to have an epoch-based outer loop. Once I tried to avoid having epochs by using `itertools.cycle` on a `DataLoader` with `RandomSampler`, but it gave me a bad memory leak.\r\n\r\nTherefore, is it an issue if we have the API allow for a sampling procedure that looks like the code below ? My goal is to have a flexible data sampling interface while harnessing the multiprocessing power of `DataLoader`.\r\n\r\n```python\r\nfor i in range(n_iters):\r\n    # 1. Sample according to `probs`\r\n    indices = np.random.choice(n, batch_size, p=probs)\r\n    batch = loader.get_indices(indices)\r\n   \r\n    # 2. Optimization step\r\n    opt.zero_grad()\r\n    loss = model.compute_loss(batch)\r\n    loss.backward()\r\n    opt.step()\r\n\r\n    # 3. Change probs values according to some criterion\r\n    probs = get_newProbs(probs, loss)\r\n```\r\n\r\nIf the `DataLoader` **api** can't be changed, what if we add three extra functions to `torch.data.Dataset` ?\r\n1. `.get_indices(batch_indices, collate_fn)` - to extract a batch from indices using a collate function;\r\n2. `.spawn_workers(num_workers)` - to initialize workers for sampling with multiprocessing; and \r\n3. `.terminate_workers()` - to terminate the worker threads.\r\n\r\nWhat do you think ?\r\n\r\nI would be happy to submit a pull request for this feature!"}