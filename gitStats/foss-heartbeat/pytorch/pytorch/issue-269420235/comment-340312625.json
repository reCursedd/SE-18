{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340312625", "html_url": "https://github.com/pytorch/pytorch/issues/3356#issuecomment-340312625", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3356", "id": 340312625, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDMxMjYyNQ==", "user": {"login": "IssamLaradji", "id": 3382128, "node_id": "MDQ6VXNlcjMzODIxMjg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3382128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IssamLaradji", "html_url": "https://github.com/IssamLaradji", "followers_url": "https://api.github.com/users/IssamLaradji/followers", "following_url": "https://api.github.com/users/IssamLaradji/following{/other_user}", "gists_url": "https://api.github.com/users/IssamLaradji/gists{/gist_id}", "starred_url": "https://api.github.com/users/IssamLaradji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IssamLaradji/subscriptions", "organizations_url": "https://api.github.com/users/IssamLaradji/orgs", "repos_url": "https://api.github.com/users/IssamLaradji/repos", "events_url": "https://api.github.com/users/IssamLaradji/events{/privacy}", "received_events_url": "https://api.github.com/users/IssamLaradji/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-29T23:38:58Z", "updated_at": "2017-10-30T00:50:51Z", "author_association": "NONE", "body_html": "<p>Thanks a lot for explaining! This sheds light on the causes of the challenges I had with sampling.<br>\nIt seems I am getting much closer to resolving one of my main use cases.</p>\n<p>Consider the use case where we have to carefully select a batch at each iteration. So I compute a score for each sample and find that the best next batch to pick has indices [5,6,7].</p>\n<p>Now, I would like to select this batch in my next iteration, would something like the code below be efficient ?</p>\n<p>The only things I see that might affect performance are the two calls I make with <code>iter</code> but it will probably be negligible when the batches are large enough.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> 1. DATASET</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DummyDataset</span>(<span class=\"pl-e\">Dataset</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">pass</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">index</span>):\n        <span class=\"pl-k\">return</span> torch.ones(<span class=\"pl-c1\">5</span>)<span class=\"pl-k\">*</span>index\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">100</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 2. SAMPLER</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DynamicSampler</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">self</span>.next_batch <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">0</span>]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">next_sample</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">indList</span>):\n        <span class=\"pl-c1\">self</span>.next_batch <span class=\"pl-k\">=</span> indList\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__iter__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">iter</span>(<span class=\"pl-c1\">self</span>.next_batch)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">100</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> EXAMPLE</span>\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    dataset <span class=\"pl-k\">=</span> DummyDataset()\n    sampler <span class=\"pl-k\">=</span> DynamicSampler()\n    loader <span class=\"pl-k\">=</span> DataLoader(dataset, <span class=\"pl-v\">sampler</span><span class=\"pl-k\">=</span>sampler)\n\n    best_batch <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">11</span>]\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(n_iters):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set the next batch indices as the best batch</span>\n        loader.sampler.next_sample(best_batch)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get Batch</span>\n        loader.batch_sampler.batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(best_batch)\n        batch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">iter</span>(loader).next()\n        \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Update weights</span>\n        model.partial_fit(batch)\n       \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get the next best batch</span>\n        best_batch <span class=\"pl-k\">=</span> get_nextBestBatch(model, dataset)\n</pre></div>\n<p>PS: I will try to reproduce the memory leak problem I had and post it as a new issue.</p>", "body_text": "Thanks a lot for explaining! This sheds light on the causes of the challenges I had with sampling.\nIt seems I am getting much closer to resolving one of my main use cases.\nConsider the use case where we have to carefully select a batch at each iteration. So I compute a score for each sample and find that the best next batch to pick has indices [5,6,7].\nNow, I would like to select this batch in my next iteration, would something like the code below be efficient ?\nThe only things I see that might affect performance are the two calls I make with iter but it will probably be negligible when the batches are large enough.\n# 1. DATASET\nclass DummyDataset(Dataset):\n    def __init__(self):\n        pass\n\n    def __getitem__(self, index):\n        return torch.ones(5)*index\n\n    def __len__(self):\n        return 100\n\n# 2. SAMPLER\nclass DynamicSampler(object):\n    def __init__(self):\n        self.next_batch = [0]\n\n    def next_sample(self, indList):\n        self.next_batch = indList\n\n    def __iter__(self):\n        return iter(self.next_batch)\n\n    def __len__(self):\n        return 100\n\n# EXAMPLE\nif __name__ == '__main__':\n    dataset = DummyDataset()\n    sampler = DynamicSampler()\n    loader = DataLoader(dataset, sampler=sampler)\n\n    best_batch = [5, 6, 7, 10, 11]\n    for i in range(n_iters):\n        # Set the next batch indices as the best batch\n        loader.sampler.next_sample(best_batch)\n\n        # Get Batch\n        loader.batch_sampler.batch_size = len(best_batch)\n        batch = iter(loader).next()\n        \n        # Update weights\n        model.partial_fit(batch)\n       \n        # Get the next best batch\n        best_batch = get_nextBestBatch(model, dataset)\n\nPS: I will try to reproduce the memory leak problem I had and post it as a new issue.", "body": "Thanks a lot for explaining! This sheds light on the causes of the challenges I had with sampling.\r\nIt seems I am getting much closer to resolving one of my main use cases. \r\n\r\nConsider the use case where we have to carefully select a batch at each iteration. So I compute a score for each sample and find that the best next batch to pick has indices [5,6,7].\r\n\r\nNow, I would like to select this batch in my next iteration, would something like the code below be efficient ?\r\n\r\nThe only things I see that might affect performance are the two calls I make with `iter` but it will probably be negligible when the batches are large enough.\r\n\r\n```python\r\n# 1. DATASET\r\nclass DummyDataset(Dataset):\r\n    def __init__(self):\r\n        pass\r\n\r\n    def __getitem__(self, index):\r\n        return torch.ones(5)*index\r\n\r\n    def __len__(self):\r\n        return 100\r\n\r\n# 2. SAMPLER\r\nclass DynamicSampler(object):\r\n    def __init__(self):\r\n        self.next_batch = [0]\r\n\r\n    def next_sample(self, indList):\r\n        self.next_batch = indList\r\n\r\n    def __iter__(self):\r\n        return iter(self.next_batch)\r\n\r\n    def __len__(self):\r\n        return 100\r\n\r\n# EXAMPLE\r\nif __name__ == '__main__':\r\n    dataset = DummyDataset()\r\n    sampler = DynamicSampler()\r\n    loader = DataLoader(dataset, sampler=sampler)\r\n\r\n    best_batch = [5, 6, 7, 10, 11]\r\n    for i in range(n_iters):\r\n        # Set the next batch indices as the best batch\r\n        loader.sampler.next_sample(best_batch)\r\n\r\n        # Get Batch\r\n        loader.batch_sampler.batch_size = len(best_batch)\r\n        batch = iter(loader).next()\r\n        \r\n        # Update weights\r\n        model.partial_fit(batch)\r\n       \r\n        # Get the next best batch\r\n        best_batch = get_nextBestBatch(model, dataset)\r\n\r\n```\r\n\r\nPS: I will try to reproduce the memory leak problem I had and post it as a new issue.\r\n"}