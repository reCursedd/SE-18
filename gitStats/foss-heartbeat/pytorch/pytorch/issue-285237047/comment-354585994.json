{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/354585994", "html_url": "https://github.com/pytorch/pytorch/pull/4415#issuecomment-354585994", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4415", "id": 354585994, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDU4NTk5NA==", "user": {"login": "vishwakftw", "id": 23639302, "node_id": "MDQ6VXNlcjIzNjM5MzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/23639302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishwakftw", "html_url": "https://github.com/vishwakftw", "followers_url": "https://api.github.com/users/vishwakftw/followers", "following_url": "https://api.github.com/users/vishwakftw/following{/other_user}", "gists_url": "https://api.github.com/users/vishwakftw/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishwakftw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishwakftw/subscriptions", "organizations_url": "https://api.github.com/users/vishwakftw/orgs", "repos_url": "https://api.github.com/users/vishwakftw/repos", "events_url": "https://api.github.com/users/vishwakftw/events{/privacy}", "received_events_url": "https://api.github.com/users/vishwakftw/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-31T05:21:32Z", "updated_at": "2018-01-01T04:54:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For <code>addcdiv</code>, there is a 4x performance bump but the shapes of the returned values are different:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\nt <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">300</span>)\nt1 <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">600</span>)\nt2 <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">600</span>, <span class=\"pl-c1\">100</span>)\nvalue <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>\nresult <span class=\"pl-k\">=</span> torch.addcdiv(t, value, t1, t2)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_1</span>():\n    <span class=\"pl-k\">return</span> <span class=\"pl-k\">-</span>value <span class=\"pl-k\">*</span> t1 <span class=\"pl-k\">/</span> (t2 <span class=\"pl-k\">*</span> t2)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_2</span>():\n    <span class=\"pl-k\">return</span> (t <span class=\"pl-k\">-</span> result) <span class=\"pl-k\">/</span> t2\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    <span class=\"pl-k\">import</span> timeit\n    <span class=\"pl-c1\">print</span>(timeit.timeit(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test_1()<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">globals</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">globals</span>(), <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>))\n    <span class=\"pl-c1\">print</span>(timeit.timeit(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test_2()<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">globals</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">globals</span>(), <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>))\n    <span class=\"pl-c1\">print</span>(test_1())\n    <span class=\"pl-c1\">print</span>(test_2())\n    <span class=\"pl-c1\">print</span>((test_1() <span class=\"pl-k\">-</span> test_2()).abs().sum())</pre></div>\n<p>Returned values are:</p>\n<div class=\"highlight highlight-source-shell\"><pre>0.03943780899862759\n0.01038944299943978\n\n 7.6768e-01  7.5077e+00 -4.0676e-02  ...  -6.3054e-02 -2.4193e-03  2.0298e+00\n-7.1602e-02 -6.0141e-02 -1.7127e-01  ...   3.0107e+00  8.1869e-02  5.7927e-02\n 8.4864e-02  3.5141e+00  3.5293e-01  ...  -7.5143e-03  6.0525e-01 -7.2473e-03\n                ...                   \u22f1                   ...                \n-2.0216e-01  9.1129e+01 -1.4521e-01  ...  -3.6893e-01 -3.0479e-02 -5.9128e-02\n-3.7437e-02  3.0440e-01  1.7184e-01  ...  -1.4021e+00 -1.9250e-01  2.0726e-02\n-1.5107e+00  1.0939e-02  2.5937e-02  ...   2.0832e-02 -1.7685e-02 -3.9429e-02\n[torch.FloatTensor of size 100x600]\n\n\n 7.6768e-01  7.5077e+00 -4.0676e-02  ...   1.5048e+00  4.5266e-02 -4.7851e-01\n-5.5017e-03  2.2120e+00  1.1448e-01  ...  -6.3054e-02 -2.4193e-03  2.0298e+00\n-7.1602e-02 -6.0141e-02 -1.7127e-01  ...  -2.4567e-01  7.3112e-02 -5.9868e-02\n                ...                   \u22f1                   ...                \n-2.5525e-02  2.8814e-01 -6.7123e-01  ...  -1.4021e+00 -1.9250e-01  2.0726e-02\n-1.5107e+00  1.0939e-02  2.5937e-02  ...   9.1650e-03  1.3605e-02 -4.7351e-01\n-4.9432e-01 -4.3783e-02 -1.1289e+00  ...   2.0832e-02 -1.7685e-02 -3.9429e-02\n[torch.FloatTensor of size 200x300]\n\n4.378303683894671</pre></div>\n<p>Edit: The return values also seem to be different.</p>", "body_text": "For addcdiv, there is a 4x performance bump but the shapes of the returned values are different:\nimport torch\n\nt = torch.randn(200, 300)\nt1 = torch.randn(100, 600)\nt2 = torch.randn(600, 100)\nvalue = 0.1\nresult = torch.addcdiv(t, value, t1, t2)\n\ndef test_1():\n    return -value * t1 / (t2 * t2)\n\ndef test_2():\n    return (t - result) / t2\n\nif __name__ == \"__main__\":\n    import timeit\n    print(timeit.timeit('test_1()', globals=globals(), number=200))\n    print(timeit.timeit('test_2()', globals=globals(), number=200))\n    print(test_1())\n    print(test_2())\n    print((test_1() - test_2()).abs().sum())\nReturned values are:\n0.03943780899862759\n0.01038944299943978\n\n 7.6768e-01  7.5077e+00 -4.0676e-02  ...  -6.3054e-02 -2.4193e-03  2.0298e+00\n-7.1602e-02 -6.0141e-02 -1.7127e-01  ...   3.0107e+00  8.1869e-02  5.7927e-02\n 8.4864e-02  3.5141e+00  3.5293e-01  ...  -7.5143e-03  6.0525e-01 -7.2473e-03\n                ...                   \u22f1                   ...                \n-2.0216e-01  9.1129e+01 -1.4521e-01  ...  -3.6893e-01 -3.0479e-02 -5.9128e-02\n-3.7437e-02  3.0440e-01  1.7184e-01  ...  -1.4021e+00 -1.9250e-01  2.0726e-02\n-1.5107e+00  1.0939e-02  2.5937e-02  ...   2.0832e-02 -1.7685e-02 -3.9429e-02\n[torch.FloatTensor of size 100x600]\n\n\n 7.6768e-01  7.5077e+00 -4.0676e-02  ...   1.5048e+00  4.5266e-02 -4.7851e-01\n-5.5017e-03  2.2120e+00  1.1448e-01  ...  -6.3054e-02 -2.4193e-03  2.0298e+00\n-7.1602e-02 -6.0141e-02 -1.7127e-01  ...  -2.4567e-01  7.3112e-02 -5.9868e-02\n                ...                   \u22f1                   ...                \n-2.5525e-02  2.8814e-01 -6.7123e-01  ...  -1.4021e+00 -1.9250e-01  2.0726e-02\n-1.5107e+00  1.0939e-02  2.5937e-02  ...   9.1650e-03  1.3605e-02 -4.7351e-01\n-4.9432e-01 -4.3783e-02 -1.1289e+00  ...   2.0832e-02 -1.7685e-02 -3.9429e-02\n[torch.FloatTensor of size 200x300]\n\n4.378303683894671\nEdit: The return values also seem to be different.", "body": "For `addcdiv`, there is a 4x performance bump but the shapes of the returned values are different:\r\n```python\r\nimport torch\r\n\r\nt = torch.randn(200, 300)\r\nt1 = torch.randn(100, 600)\r\nt2 = torch.randn(600, 100)\r\nvalue = 0.1\r\nresult = torch.addcdiv(t, value, t1, t2)\r\n\r\ndef test_1():\r\n    return -value * t1 / (t2 * t2)\r\n\r\ndef test_2():\r\n    return (t - result) / t2\r\n\r\nif __name__ == \"__main__\":\r\n    import timeit\r\n    print(timeit.timeit('test_1()', globals=globals(), number=200))\r\n    print(timeit.timeit('test_2()', globals=globals(), number=200))\r\n    print(test_1())\r\n    print(test_2())\r\n    print((test_1() - test_2()).abs().sum())\r\n```\r\nReturned values are:\r\n```bash\r\n0.03943780899862759\r\n0.01038944299943978\r\n\r\n 7.6768e-01  7.5077e+00 -4.0676e-02  ...  -6.3054e-02 -2.4193e-03  2.0298e+00\r\n-7.1602e-02 -6.0141e-02 -1.7127e-01  ...   3.0107e+00  8.1869e-02  5.7927e-02\r\n 8.4864e-02  3.5141e+00  3.5293e-01  ...  -7.5143e-03  6.0525e-01 -7.2473e-03\r\n                ...                   \u22f1                   ...                \r\n-2.0216e-01  9.1129e+01 -1.4521e-01  ...  -3.6893e-01 -3.0479e-02 -5.9128e-02\r\n-3.7437e-02  3.0440e-01  1.7184e-01  ...  -1.4021e+00 -1.9250e-01  2.0726e-02\r\n-1.5107e+00  1.0939e-02  2.5937e-02  ...   2.0832e-02 -1.7685e-02 -3.9429e-02\r\n[torch.FloatTensor of size 100x600]\r\n\r\n\r\n 7.6768e-01  7.5077e+00 -4.0676e-02  ...   1.5048e+00  4.5266e-02 -4.7851e-01\r\n-5.5017e-03  2.2120e+00  1.1448e-01  ...  -6.3054e-02 -2.4193e-03  2.0298e+00\r\n-7.1602e-02 -6.0141e-02 -1.7127e-01  ...  -2.4567e-01  7.3112e-02 -5.9868e-02\r\n                ...                   \u22f1                   ...                \r\n-2.5525e-02  2.8814e-01 -6.7123e-01  ...  -1.4021e+00 -1.9250e-01  2.0726e-02\r\n-1.5107e+00  1.0939e-02  2.5937e-02  ...   9.1650e-03  1.3605e-02 -4.7351e-01\r\n-4.9432e-01 -4.3783e-02 -1.1289e+00  ...   2.0832e-02 -1.7685e-02 -3.9429e-02\r\n[torch.FloatTensor of size 200x300]\r\n\r\n4.378303683894671\r\n```\r\n\r\nEdit: The return values also seem to be different."}