{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12189", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12189/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12189/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12189/events", "html_url": "https://github.com/pytorch/pytorch/issues/12189", "id": 365076870, "node_id": "MDU6SXNzdWUzNjUwNzY4NzA=", "number": 12189, "title": "[Feature] Support Adaptive Max Gradient Norm / Clipping", "user": {"login": "PetrochukM", "id": 7424737, "node_id": "MDQ6VXNlcjc0MjQ3Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7424737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PetrochukM", "html_url": "https://github.com/PetrochukM", "followers_url": "https://api.github.com/users/PetrochukM/followers", "following_url": "https://api.github.com/users/PetrochukM/following{/other_user}", "gists_url": "https://api.github.com/users/PetrochukM/gists{/gist_id}", "starred_url": "https://api.github.com/users/PetrochukM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PetrochukM/subscriptions", "organizations_url": "https://api.github.com/users/PetrochukM/orgs", "repos_url": "https://api.github.com/users/PetrochukM/repos", "events_url": "https://api.github.com/users/PetrochukM/events{/privacy}", "received_events_url": "https://api.github.com/users/PetrochukM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-29T00:11:13Z", "updated_at": "2018-10-01T17:27:18Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>This feature is to use a moving windowed median or exponential moving average for gradient clipping and normalization.</p>\n<p>Additionally, it'd be nice to support batch skipping if the gradient norm is above a threshold which is usually an indication of an imminent gradient explosion.</p>\n<h2>Motivation</h2>\n<p>The threshold for gradient clipping is a hyperparameter that needs to be set uniquely for each model. However, its supported by at least two important and well-written papers that I know of that this parameter can be set automatically:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1212.0901\" rel=\"nofollow\">https://arxiv.org/abs/1212.0901</a> \"The cutoff threshold for gradient clipping is set based on the average norm of the gradient over one pass on the data\"</li>\n<li><a href=\"https://arxiv.org/abs/1804.09849\" rel=\"nofollow\">https://arxiv.org/abs/1804.09849</a> \"To further stabilize training, we also use adaptive gradient clipping. We discard a training step completely if an anomaly in the gradient norm value is detected, which is usually an indication of an imminent gradient explosion. More specifically, we keep track of a moving average and a moving standard deviation of the log of the gradient norm values, and we abort a step if the norm of the gradient exceeds four standard deviations of the moving average.\"</li>\n</ul>\n<p>In addition, I've found in my own experiments in audio synthesis that setting the parameter automatically is helpful.</p>\n<h2>Pitch</h2>\n<p>Implement a gradient clipping module that stores an internal state used for deciding the threshold for clipping.</p>", "body_text": "\ud83d\ude80 Feature\nThis feature is to use a moving windowed median or exponential moving average for gradient clipping and normalization.\nAdditionally, it'd be nice to support batch skipping if the gradient norm is above a threshold which is usually an indication of an imminent gradient explosion.\nMotivation\nThe threshold for gradient clipping is a hyperparameter that needs to be set uniquely for each model. However, its supported by at least two important and well-written papers that I know of that this parameter can be set automatically:\n\nhttps://arxiv.org/abs/1212.0901 \"The cutoff threshold for gradient clipping is set based on the average norm of the gradient over one pass on the data\"\nhttps://arxiv.org/abs/1804.09849 \"To further stabilize training, we also use adaptive gradient clipping. We discard a training step completely if an anomaly in the gradient norm value is detected, which is usually an indication of an imminent gradient explosion. More specifically, we keep track of a moving average and a moving standard deviation of the log of the gradient norm values, and we abort a step if the norm of the gradient exceeds four standard deviations of the moving average.\"\n\nIn addition, I've found in my own experiments in audio synthesis that setting the parameter automatically is helpful.\nPitch\nImplement a gradient clipping module that stores an internal state used for deciding the threshold for clipping.", "body": "## \ud83d\ude80 Feature\r\nThis feature is to use a moving windowed median or exponential moving average for gradient clipping and normalization. \r\n\r\nAdditionally, it'd be nice to support batch skipping if the gradient norm is above a threshold which is usually an indication of an imminent gradient explosion.\r\n\r\n## Motivation\r\n\r\nThe threshold for gradient clipping is a hyperparameter that needs to be set uniquely for each model. However, its supported by at least two important and well-written papers that I know of that this parameter can be set automatically:\r\n- https://arxiv.org/abs/1212.0901 \"The cutoff threshold for gradient clipping is set based on the average norm of the gradient over one pass on the data\"\r\n- https://arxiv.org/abs/1804.09849 \"To further stabilize training, we also use adaptive gradient clipping. We discard a training step completely if an anomaly in the gradient norm value is detected, which is usually an indication of an imminent gradient explosion. More specifically, we keep track of a moving average and a moving standard deviation of the log of the gradient norm values, and we abort a step if the norm of the gradient exceeds four standard deviations of the moving average.\"\r\n\r\nIn addition, I've found in my own experiments in audio synthesis that setting the parameter automatically is helpful.\r\n\r\n## Pitch\r\n\r\nImplement a gradient clipping module that stores an internal state used for deciding the threshold for clipping."}