{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386227026", "html_url": "https://github.com/pytorch/pytorch/pull/7234#issuecomment-386227026", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7234", "id": 386227026, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjIyNzAyNg==", "user": {"login": "gujinghui", "id": 31264804, "node_id": "MDQ6VXNlcjMxMjY0ODA0", "avatar_url": "https://avatars2.githubusercontent.com/u/31264804?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gujinghui", "html_url": "https://github.com/gujinghui", "followers_url": "https://api.github.com/users/gujinghui/followers", "following_url": "https://api.github.com/users/gujinghui/following{/other_user}", "gists_url": "https://api.github.com/users/gujinghui/gists{/gist_id}", "starred_url": "https://api.github.com/users/gujinghui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gujinghui/subscriptions", "organizations_url": "https://api.github.com/users/gujinghui/orgs", "repos_url": "https://api.github.com/users/gujinghui/repos", "events_url": "https://api.github.com/users/gujinghui/events{/privacy}", "received_events_url": "https://api.github.com/users/gujinghui/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-03T08:38:56Z", "updated_at": "2018-05-03T08:38:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1100089\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yinghai\">@yinghai</a><br>\nAbout fallback operators. In current code, we have to copy entire buffer from CPUOp <strong>output</strong> to IDEEP tensor every time. Generally, we can avoid this copy by sharing the tensor buffer between IDEEP and CPU operators, if the life cycle of CPU buffer is longer enough.<br>\nTo avoid the extra copy, we plan to improve the fallback wrapper of IDEEP as following.</p>\n<ol>\n<li>create a output <strong>CPU</strong> blob in IDEEP workspace with the name plused a suffix</li>\n<li>change the OperatorDef base_def_ to introduce this name blob name as the output of CPU operator</li>\n<li><strong>forward</strong> this blob into local_ws_ from IDEEP workspace</li>\n<li>run the CPU operator and share the buffer of this blob to IDEEP tensor after finished</li>\n</ol>\n<p>Any suggestion on this ?</p>", "body_text": "@yinghai\nAbout fallback operators. In current code, we have to copy entire buffer from CPUOp output to IDEEP tensor every time. Generally, we can avoid this copy by sharing the tensor buffer between IDEEP and CPU operators, if the life cycle of CPU buffer is longer enough.\nTo avoid the extra copy, we plan to improve the fallback wrapper of IDEEP as following.\n\ncreate a output CPU blob in IDEEP workspace with the name plused a suffix\nchange the OperatorDef base_def_ to introduce this name blob name as the output of CPU operator\nforward this blob into local_ws_ from IDEEP workspace\nrun the CPU operator and share the buffer of this blob to IDEEP tensor after finished\n\nAny suggestion on this ?", "body": "@yinghai \r\nAbout fallback operators. In current code, we have to copy entire buffer from CPUOp **output** to IDEEP tensor every time. Generally, we can avoid this copy by sharing the tensor buffer between IDEEP and CPU operators, if the life cycle of CPU buffer is longer enough.\r\nTo avoid the extra copy, we plan to improve the fallback wrapper of IDEEP as following.\r\n1. create a output **CPU** blob in IDEEP workspace with the name plused a suffix\r\n2. change the OperatorDef base_def_ to introduce this name blob name as the output of CPU operator\r\n3. **forward** this blob into local_ws_ from IDEEP workspace\r\n4. run the CPU operator and share the buffer of this blob to IDEEP tensor after finished\r\n\r\nAny suggestion on this ?"}