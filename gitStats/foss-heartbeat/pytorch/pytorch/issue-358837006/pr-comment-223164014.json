{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223164014", "pull_request_review_id": 162219742, "id": 223164014, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzE2NDAxNA==", "diff_hunk": "@@ -150,60 +163,91 @@ void THCTensor_(catArray)(THCState *state, THCTensor *result,\n   // 6. All input tensors can use 32-bit indexing\n   // 7. All input tensors are on the same device\n \n-  if (numInputs > 1 &&\n-      !hasSkippedInput &&\n+  if (numInputs > 1 && !hasSkippedInput &&\n       result->dim() <= CAT_ARRAY_MAX_INPUT_DIMS &&\n       THCTensor_canUse32BitIndexMath(state, result) &&\n       THCTensor_allContiguous(state, inputs, numInputs) &&\n       THCTensor_all32BitIndexable(state, inputs, numInputs) &&\n       THCTensor_allSameDevice(state, inputs, numInputs)) {\n-\n-    // First, let's set up our kernel parameters. We start with a raw pointer to the storage\n-    // for the output Tensor.\n-    scalar_t *data = THCTensor_(data)(state, result);\n+    // First, let's set up our kernel parameters. We start with a raw pointer to\n+    // the storage for the output Tensor.\n+    scalar_t* data = THCTensor_(data)(state, result);\n \n     // Kernel Parameter\n-    size_t tensorMetadataSize = sizeof(CatArrInputTensor<scalar_t, unsigned int>) * CAT_ARRAY_BATCH_SIZE;\n-    auto d_inputs = static_cast<CatArrInputTensor<scalar_t, unsigned int> *>(THCudaMalloc(state, tensorMetadataSize));\n+    size_t tensorMetadataSize = sizeof(CatArrInputTensor<\n+                                       scalar_t,\n+                                       unsigned int,\n+                                       CAT_ARRAY_MAX_INPUT_DIMS>) *\n+        CAT_ARRAY_BATCH_SIZE;\n+    auto d_inputs = static_cast<\n+        CatArrInputTensor<scalar_t, unsigned int, CAT_ARRAY_MAX_INPUT_DIMS>*>(\n+        THCudaMalloc(state, tensorMetadataSize));\n \n-    OutputTensorSizeStride<unsigned int, CAT_ARRAY_MAX_INPUT_DIMS> param;\n+    TensorSizeStride<unsigned int, CAT_ARRAY_MAX_INPUT_DIMS> param;\n \n     // Next, let's initialize the size, stride arrays for the output Tensor.\n     for (i = 0; i < nDims; ++i) {\n-      param.outputSize[i] = THCTensor_(size)(state, result, i);\n-      param.outputStride[i] = THCTensor_(stride)(state, result, i);\n+      param.tensorSize[i] = THCTensor_(size)(state, result, i);\n+      param.tensorStride[i] = THCTensor_(stride)(state, result, i);\n     }\n \n     THCStream* stream = THCState_getStream(state);\n \n     // Template Declarations for dim = 1, 2, 3, 4\n-#define HANDLE_CASE(DIMS) \\\n-  CatArrayBatchedCopy<scalar_t, unsigned int, DIMS><<<catGrid, applyBlock, 0, THCStream_stream(stream)>>>(data, d_inputs, param, dimension, param.outputStride[dimension]);\n+#define HANDLE_CASE(DIMS)                                     \\\n+  CatArrayBatchedCopy<scalar_t, unsigned int, DIMS>           \\\n+      <<<catGrid, applyBlock, 0, THCStream_stream(stream)>>>( \\\n+          data, d_inputs, param, dimension, pad, pad_value);\n \n     // Now we loop\n     offset = 0;\n     for (i = 0; i < numInputs; i += CAT_ARRAY_BATCH_SIZE) {\n-      // Re-allocate stackInputs every iteration to avoid read-after-write hazard\n+      // Re-allocate stackInputs every iteration to avoid read-after-write\n+      // hazard\n       {\n         auto stackInputs_owner = THCudaHostAlloc(state, tensorMetadataSize);\n-        CatArrInputTensor<scalar_t, unsigned int>* stackInputs = static_cast<CatArrInputTensor<scalar_t, unsigned int>*>(stackInputs_owner.get());\n+        CatArrInputTensor<scalar_t, unsigned int, CAT_ARRAY_MAX_INPUT_DIMS>*\n+            stackInputs = static_cast<CatArrInputTensor<\n+                scalar_t,\n+                unsigned int,\n+                CAT_ARRAY_MAX_INPUT_DIMS>*>(stackInputs_owner.get());\n         cohortMax = 0;\n-        for (j = 0; j < CAT_ARRAY_BATCH_SIZE && (i+j) < numInputs; ++j) {\n-          int64_t dimSize = THCTensor_(size)(state, inputs[i+j], dimension);\n+        for (j = 0; j < CAT_ARRAY_BATCH_SIZE && (i + j) < numInputs; ++j) {\n+          THCTensor* curtensor = inputs[i + j];\n \n-          stackInputs[j].input = THCTensor_(data)(state, inputs[i+j]);\n-          stackInputs[j].offset = offset;\n-          stackInputs[j].dimSize = dimSize;\n-          stackInputs[j].nElements = THCTensor_(nElement)(state, inputs[i+j]);\n-          cohortMax = cohortMax > (int) stackInputs[j].nElements ? cohortMax : (int) stackInputs[j].nElements;\n+          int64_t dimSize = THCTensor_(size)(state, curtensor, dimension);\n+\n+          TensorSizeStride<unsigned int, CAT_ARRAY_MAX_INPUT_DIMS> inputParam;\n+          for (int k = 0; k < nDims; ++k) {\n+            inputParam.tensorSize[k] = THCTensor_(size)(state, curtensor, k);\n+            inputParam.tensorStride[k] = THCTensor_(stride)(state, curtensor, k);\n+          }\n \n+          THCTensor* nt = THCTensor_(newWithTensor)(state, result);\n+          THCTensor_(narrow)(state, nt, NULL, dimension, offset, dimSize);\n+\n+          stackInputs[j].input = THCTensor_(data)(state, curtensor);\n+          stackInputs[j].offset = offset;\n+          stackInputs[j].inputParam = inputParam;\n+          stackInputs[j].nElements = THCTensor_(nElement)(state, curtensor);", "path": "aten/src/THC/generic/THCTensorMath.cu", "position": null, "original_position": 222, "commit_id": "5b48bc2cd9a61cb81e09f008730bc096561a0e9a", "original_commit_id": "679b3bb28b6479e114147027fbd52369c13d4aee", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "do you ever use nElements?", "created_at": "2018-10-06T00:13:40Z", "updated_at": "2018-11-23T15:52:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/11494#discussion_r223164014", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11494", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223164014"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11494#discussion_r223164014"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11494"}}, "body_html": "<p>do you ever use nElements?</p>", "body_text": "do you ever use nElements?"}