{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220987764", "pull_request_review_id": 159515462, "id": 220987764, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMDk4Nzc2NA==", "diff_hunk": "@@ -1290,47 +1290,79 @@ void THTensor_(triu)(THTensor *r_, THTensor *t, int64_t k)\n   }\n }\n \n-void THTensor_(cat)(THTensor *r_, THTensor *ta, THTensor *tb, int dimension)\n-{\n+void THTensor_(cat)(\n+    THTensor* r_,\n+    THTensor* ta,\n+    THTensor* tb,\n+    int dimension,\n+    int pad,\n+    scalar_t pad_value) {\n   THTensor* inputs[2];\n   inputs[0] = ta;\n   inputs[1] = tb;\n-  THTensor_(catArray)(r_, inputs, 2, dimension);\n-}\n-\n-void THTensor_(check_shape_except_dim)(THTensor *first, THTensor *second, int dimension);\n-inline void THTensor_(check_shape_except_dim)(THTensor *first, THTensor *second, int dimension)\n-{\n+  THTensor_(catArray)(r_, inputs, 2, dimension, pad, pad_value);\n+}\n+\n+void THTensor_(check_shape_except_dim)(\n+    THTensor* first,\n+    THTensor* second,\n+    int dimension,\n+    bool pad);\n+inline void THTensor_(check_shape_except_dim)(\n+    THTensor* first,\n+    THTensor* second,\n+    int dimension,\n+    bool pad) {\n   int first_dims = first->dim();\n   int second_dims = second->dim();\n-  THArgCheck(first_dims == second_dims, 0,\n+  THArgCheck(\n+      first_dims == second_dims,\n+      0,\n       \"Tensors must have same number of dimensions: got %d and %d\",\n-      first_dims, second_dims);\n-  for (int dim = 0; dim < first_dims; dim++) {\n-    if (dim == dimension) {\n-      continue;\n+      first_dims,\n+      second_dims);\n+\n+  // Without padding, sizes of tensors must match except in dimension\n+  if (!pad) {\n+    for (int dim = 0; dim < first_dims; dim++) {\n+      if (dim == dimension) {\n+        continue;\n+      }\n+      int64_t first_dim_size = first->size(dim);\n+      int64_t second_dim_size = second->size(dim);\n+      THArgCheck(\n+          first_dim_size == second_dim_size,\n+          0,\n+          \"Sizes of tensors must match except in dimension %d. Got %lld and %lld in dimension %d\",\n+          dimension,\n+          (long long)first_dim_size,\n+          (long long)second_dim_size,\n+          dim);\n     }\n-    int64_t first_dim_size = first->size(dim);\n-    int64_t second_dim_size = second->size(dim);\n-    THArgCheck(first_dim_size == second_dim_size, 0,\n-        \"Sizes of tensors must match except in dimension %d. Got %lld and %lld in dimension %d\",\n-        dimension, (long long)first_dim_size, (long long)second_dim_size, dim);\n   }\n }\n \n-void THTensor_(catArray)(THTensor *result, THTensor **inputs, int numInputs, int dimension)\n-{\n-  // previously, size [0] tensors were the only possible empty tensors; thus, it wasn't possible\n-  // to cat empty tensors unless all the other tensors were 1-dimensional, so we allowed these tensors\n-  // to be \"skipped\".  We maintain this behavior for backwards compatibility, but only for this specific\n-  // size (i.e. other empty sizes are not skipped).\n+void THTensor_(catArray)(\n+    THTensor* result,\n+    THTensor** inputs,\n+    int numInputs,\n+    int dimension,\n+    int pad,\n+    scalar_t pad_value) {\n+  // previously, size [0] tensors were the only possible empty tensors; thus, it", "path": "aten/src/TH/generic/THTensorMoreMath.cpp", "position": 64, "original_position": 87, "commit_id": "5b48bc2cd9a61cb81e09f008730bc096561a0e9a", "original_commit_id": "66efdc6c11a9705d9069ef91c210b405324487e1", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "this comment needs to be updated, we don't do this if pad is true.", "created_at": "2018-09-27T16:16:29Z", "updated_at": "2018-11-23T15:52:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/11494#discussion_r220987764", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11494", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220987764"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11494#discussion_r220987764"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11494"}}, "body_html": "<p>this comment needs to be updated, we don't do this if pad is true.</p>", "body_text": "this comment needs to be updated, we don't do this if pad is true."}