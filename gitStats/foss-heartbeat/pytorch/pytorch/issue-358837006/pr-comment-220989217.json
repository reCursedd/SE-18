{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220989217", "pull_request_review_id": 159515462, "id": 220989217, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMDk4OTIxNw==", "diff_hunk": "@@ -53,55 +53,83 @@ THCTensor_(numel)(THCState *state, THCTensor *t)\n   return THCTensor_(nElement)(state, t);\n }\n \n-void THCTensor_(cat)(THCState *state, THCTensor *result,\n-\t\t     THCTensor *ta, THCTensor *tb, int dimension)\n-{\n+void THCTensor_(cat)(\n+    THCState* state,\n+    THCTensor* result,\n+    THCTensor* ta,\n+    THCTensor* tb,\n+    int dimension,\n+    int pad,\n+    scalar_t pad_value) {\n   THCTensor* inputs[2];\n   inputs[0] = ta;\n   inputs[1] = tb;\n-  THCTensor_(catArray)(state, result, inputs, 2, dimension);\n+  THCTensor_(catArray)(state, result, inputs, 2, dimension, pad, pad_value);\n }\n \n-void THCTensor_(check_shape_except_dim)(THCState *state, \n-    THCTensor *first, THCTensor *second, int dimension);\n-inline void THCTensor_(check_shape_except_dim)(THCState *state, \n-    THCTensor *first, THCTensor *second, int dimension)\n-{\n+void THCTensor_(check_shape_except_dim)(\n+    THCState* state,\n+    THCTensor* first,\n+    THCTensor* second,\n+    int dimension);\n+inline void THCTensor_(check_shape_except_dim)(\n+    THCState* state,\n+    THCTensor* first,\n+    THCTensor* second,\n+    int dimension,\n+    int pad) {\n   int first_dims = first->dim();\n   int second_dims = second->dim();\n-  THArgCheck(first_dims == second_dims, 0,\n+  THArgCheck(\n+      first_dims == second_dims,\n+      0,\n       \"Tensors must have same number of dimensions: got %d and %d\",\n-      first_dims, second_dims);\n-  for (int dim = 0; dim < first_dims; dim++) {\n-    if (dim == dimension) {\n-      continue;\n+      first_dims,\n+      second_dims);\n+  if (!pad) {\n+    for (int dim = 0; dim < first_dims; dim++) {\n+      if (dim == dimension) {\n+        continue;\n+      }\n+      int64_t first_dim_size = THCTensor_(size)(state, first, dim);\n+      int64_t second_dim_size = THCTensor_(size)(state, second, dim);\n+      THArgCheck(\n+          first_dim_size == second_dim_size,\n+          0,\n+          \"Sizes of tensors must match except in dimension %d. Got %lld and %lld in dimension %d\",\n+          dimension,\n+          (long long)first_dim_size,\n+          (long long)second_dim_size,\n+          dim);\n     }\n-    int64_t first_dim_size = THCTensor_(size)(state, first, dim);\n-    int64_t second_dim_size = THCTensor_(size)(state, second, dim);\n-    THArgCheck(first_dim_size == second_dim_size, 0,\n-        \"Sizes of tensors must match except in dimension %d. Got %lld and %lld in dimension %d\",\n-        dimension, (long long)first_dim_size, (long long)second_dim_size, dim);\n   }\n }\n \n-void THCTensor_(catArray)(THCState *state, THCTensor *result,\n-\t\t\t  THCTensor **inputs, int numInputs, int dimension)\n-{\n-  // previously, size [0] tensors were the only possible empty tensors; thus, it wasn't possible\n-  // to cat empty tensors unless all the other tensors were 1-dimensional, so we allowed these tensors\n-  // to be \"skipped\".  We maintain this behavior for backwards compatibility, but only for this specific\n-  // size (i.e. other empty sizes are not skipped).\n+void THCTensor_(catArray)(\n+    THCState* state,\n+    THCTensor* result,\n+    THCTensor** inputs,\n+    int numInputs,\n+    int dimension,\n+    int pad,\n+    scalar_t pad_value) {\n+  // previously, size [0] tensors were the only possible empty tensors; thus, it\n+  // wasn't possible to cat empty tensors unless all the other tensors were\n+  // 1-dimensional, so we allowed these tensors to be \"skipped\".  We maintain\n+  // this behavior for backwards compatibility, but only for this specific size\n+  // (i.e. other empty sizes are not skipped).", "path": "aten/src/THC/generic/THCTensorMath.cu", "position": null, "original_position": 94, "commit_id": "5b48bc2cd9a61cb81e09f008730bc096561a0e9a", "original_commit_id": "66efdc6c11a9705d9069ef91c210b405324487e1", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "needs updating comment here too.", "created_at": "2018-09-27T16:20:35Z", "updated_at": "2018-11-23T15:52:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/11494#discussion_r220989217", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11494", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220989217"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11494#discussion_r220989217"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11494"}}, "body_html": "<p>needs updating comment here too.</p>", "body_text": "needs updating comment here too."}