{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216850783", "pull_request_review_id": 154426825, "id": 216850783, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjg1MDc4Mw==", "diff_hunk": "@@ -736,11 +736,16 @@ def parse_kwargs(desc):\n \n add_docstr(torch.cat,\n            r\"\"\"\n-cat(seq, dim=0, out=None) -> Tensor\n+cat(seq, dim=0, pad=False, pad_value=0, out=None) -> Tensor\n \n Concatenates the given sequence of :attr:`seq` tensors in the given dimension.\n-All tensors must either have the same shape (except in the concatenating\n-dimension) or be empty.\n+Empty tensors will be ignored.\n+If padding is set to false, all tensors must have the same shape\n+(except in the concatenating dimension).\n+If padding is set to true, all tensors do not need to have the same shape", "path": "torch/_torch_docs.py", "position": null, "original_position": 13, "commit_id": "5b48bc2cd9a61cb81e09f008730bc096561a0e9a", "original_commit_id": "d0c96533f2f3d54d3c8aa3b5286862549d99686c", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "I don't know how to parse this.  There are two cases:\r\n1) In the concatenating dimension: These clearly don't need to match, that's the point of padding\r\n2) Not in the concatenating dimension: These never need to match, that's one of the points of cat.\r\n\r\nSo, should this sentence be here or is it equivalent to leave it off?", "created_at": "2018-09-11T23:01:21Z", "updated_at": "2018-11-23T15:51:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/11494#discussion_r216850783", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11494", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216850783"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11494#discussion_r216850783"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11494"}}, "body_html": "<p>I don't know how to parse this.  There are two cases:</p>\n<ol>\n<li>In the concatenating dimension: These clearly don't need to match, that's the point of padding</li>\n<li>Not in the concatenating dimension: These never need to match, that's one of the points of cat.</li>\n</ol>\n<p>So, should this sentence be here or is it equivalent to leave it off?</p>", "body_text": "I don't know how to parse this.  There are two cases:\n\nIn the concatenating dimension: These clearly don't need to match, that's the point of padding\nNot in the concatenating dimension: These never need to match, that's one of the points of cat.\n\nSo, should this sentence be here or is it equivalent to leave it off?"}