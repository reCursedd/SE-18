{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218907281", "pull_request_review_id": 156913833, "id": 218907281, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxODkwNzI4MQ==", "diff_hunk": "@@ -116,30 +118,42 @@ void THCTensor_(catArray)(THCState *state, THCTensor *result,\n \n   THArgCheck(numInputs > 0, 3, \"invalid number of inputs %d\", numInputs);\n   THArgCheck(dimension >= 0, 4, \"invalid dimension %d\", dimension);\n-  \n-  std::vector<int64_t> size(nDims);\n-  \n+\n   // Compute size of the result in the cat dimension\n   int64_t cat_dim_size = 0;\n   for (int i = 0; i < numInputs; i++) {\n     THCTensor *tensor = inputs[i];\n-    if (should_skip(tensor)) {\n+    if (should_skip(tensor) && !pad) {\n       continue;\n     }\n-    THCTensor_(check_shape_except_dim)(state, notSkippedTensor, tensor, dimension);\n+    THCTensor_(check_shape_except_dim)(state, notSkippedTensor, tensor, dimension, pad);\n     cat_dim_size += THCTensor_(size)(state, tensor, dimension);\n   }\n \n   // Compute the size of the result\n+  std::vector<int64_t> size(nDims);\n   for (int dim = 0; dim < nDims; dim++) {\n     int64_t result_dim_size = THCTensor_(size)(state, notSkippedTensor, dim);\n+    if (pad) {\n+      for (int i = 0; i < numInputs; i++) {\n+        THCTensor* tensor = inputs[i];\n+        if (THCTensor_(size)(state, tensor, dim) > result_dim_size) {\n+          result_dim_size = THCTensor_(size)(state, tensor, dim);\n+        }\n+      }\n+    }\n     if (dim == dimension) {\n       result_dim_size = cat_dim_size;\n     }\n     size[dim] = result_dim_size;\n   }\n   THCTensor_(resize)(state, result, size, {});\n \n+  // filled with pad values if required\n+  if (pad) {\n+    THCTensor_(fill)(state, result, pad_value);", "path": "aten/src/THC/generic/THCTensorMath.cu", "position": null, "original_position": 106, "commit_id": "5b48bc2cd9a61cb81e09f008730bc096561a0e9a", "original_commit_id": "948563f628ed3fbc8d5fb6c9f449030198901baa", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "this is pretty expensive (particularly if you don't even need to pad because the sizes are all proper).  Not a big deal in the CPU, but this seems pretty wasteful in CUDA.  Can this be moved inside the efficient kernel?", "created_at": "2018-09-19T18:06:02Z", "updated_at": "2018-11-23T15:51:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/11494#discussion_r218907281", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11494", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218907281"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11494#discussion_r218907281"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11494"}}, "body_html": "<p>this is pretty expensive (particularly if you don't even need to pad because the sizes are all proper).  Not a big deal in the CPU, but this seems pretty wasteful in CUDA.  Can this be moved inside the efficient kernel?</p>", "body_text": "this is pretty expensive (particularly if you don't even need to pad because the sizes are all proper).  Not a big deal in the CPU, but this seems pretty wasteful in CUDA.  Can this be moved inside the efficient kernel?"}