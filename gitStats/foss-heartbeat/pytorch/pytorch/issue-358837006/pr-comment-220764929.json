{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220764929", "pull_request_review_id": 159249172, "id": 220764929, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMDc2NDkyOQ==", "diff_hunk": "@@ -40,91 +40,92 @@ inline bool getCatGrid(THCState* state, ptrdiff_t nTensors, dim3& grid) {\n   //X dim of grid for cat array cooperates on a single tensor in the cat.\n   //Given half of the GPU, full utilization will always occur.\n   grid = dim3( 2LL * numSM, (long long) nTensors );\n-\t     \n+\n   return true;\n }\n \n-// Similar to any other IndexToOffset calculation for copying along a given dimension.\n-template <typename IndexType, int Dims>\n-struct CatArrIndexToOffset {\n-  static inline __device__ IndexType compute(\n-      const IndexType outputSize[Dims],\n-      const IndexType outputStride[Dims],\n-      const IndexType dimSize,\n-      const unsigned int concatDim,\n-      IndexType linearIndex) {\n-    IndexType offset = 0;\n-\n-#pragma unroll\n-    for (int i = Dims - 1; i >= 1; --i) {\n-      IndexType curDimSize = i == concatDim ? dimSize : outputSize[i];\n-      IndexType nextDimIndex = linearIndex / curDimSize;\n-      IndexType curDimIndex = linearIndex - curDimSize * nextDimIndex;\n-      IndexType curDimOffset = curDimIndex * outputStride[i];\n-      offset += curDimOffset;\n-      linearIndex = nextDimIndex;\n-    }\n-\n-    return offset + linearIndex * outputStride[0];\n-  }\n+template<typename IndexType, unsigned int MaxDims>\n+struct TensorSizeStride {\n+  IndexType tensorSize[MaxDims];\n+  IndexType tensorStride[MaxDims];\n };\n \n-template <typename T, typename IndexType>\n+template <typename T, typename IndexType, unsigned int MaxDims>\n struct CatArrInputTensor {\n   T* input;\n   IndexType offset;\n-  IndexType dimSize;\n+  TensorSizeStride<IndexType, CAT_ARRAY_MAX_INPUT_DIMS> inputParam;\n   IndexType nElements;\n-};\n-\n-template<typename IndexType, unsigned int MaxDims>\n-struct OutputTensorSizeStride {\n-  IndexType outputSize[MaxDims];\n-  IndexType outputStride[MaxDims];\n+  IndexType nElementsOutput;\n };\n \n /**\n-  * Kernel used to concatenated grimDim.y tensors into an output tensor. Uses a grid-stride loop based off of\n-  * the blockIdx.x, threadIdx.x for each input to copy each element from each input tensor into the output.\n-  *\n-  * output: base pointer to the storage associated with the output tensor\n-  * inputs: GPU-allocated array of input metadata for each input to concatenate in the kernel\n-  * os: the size/stride vectors for the output tensor\n-  * concatDim: dimension along which we are concatenating\n-  * dimStride: the stride of the output tensor at the concatDim\n-  *\n-  * The most important assumption made is that the input tensors are contiguous.\n-  */\n-\n-\n-\n+ * Kernel used to concatenated grimDim.y tensors into an output tensor. Uses a\n+ * grid-stride loop based off of the blockIdx.x, threadIdx.x for each input to\n+ * copy each element from each input tensor into the output.\n+ *\n+ * output: base pointer to the storage associated with the output tensor\n+ * inputs: GPU-allocated array of input metadata for each input to concatenate\n+ * in the kernel os: the size/stride vectors for the output tensor concatDim:\n+ * dimension along which we are concatenating dimStride: the stride of the\n+ * output tensor at the concatDim\n+ *\n+ * The most important assumption made is that the input tensors are contiguous.\n+ */\n template <typename T, typename IndexType, int Dims>\n __global__ void CatArrayBatchedCopy(\n     T* output,\n-    CatArrInputTensor<T, IndexType>* inputs,\n-    OutputTensorSizeStride<IndexType, CAT_ARRAY_MAX_INPUT_DIMS> os,\n+    CatArrInputTensor<T, IndexType, CAT_ARRAY_MAX_INPUT_DIMS>* inputs,\n+    TensorSizeStride<IndexType, CAT_ARRAY_MAX_INPUT_DIMS> os,\n     const int concatDim,\n-    IndexType dimStride) {\n-\n-    IndexType tid = blockIdx.x * blockDim.x + threadIdx.x;\n-    IndexType nElements = inputs[blockIdx.y].nElements;\n-\n-    if(tid >= nElements) return;\n-    \n-    T* data = inputs[blockIdx.y].input;\n-    IndexType offset = inputs[blockIdx.y].offset;\n-    IndexType dimSize = inputs[blockIdx.y].dimSize;\n-    IndexType dataOffset = offset * dimStride;\n-\n-    IndexType stride = gridDim.x * blockDim.x;\n-\n-    while( tid < nElements){\n-    IndexType elementOffset = CatArrIndexToOffset<IndexType, Dims>::compute(\n-    \t      os.outputSize, os.outputStride, dimSize, concatDim, tid);\n-    output[dataOffset + elementOffset] = data[tid];\n-\n-    tid += stride;\n+    const int pad,\n+    T pad_value) {\n+  IndexType nElementsOutput = inputs[blockIdx.y].nElementsOutput;\n+  T* data = inputs[blockIdx.y].input;\n+  IndexType dimOffset = inputs[blockIdx.y].offset;\n+  IndexType dataOffset = dimOffset * os.tensorStride[concatDim];\n+\n+  for (IndexType linearIndex = (IndexType)blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < nElementsOutput;\n+       linearIndex += (IndexType)gridDim.x * blockDim.x) {\n+    if (pad) {\n+      IndexType inputOffset = 0;\n+      IndexType outputOffset = 0;\n+      bool inbound = true;\n+      for (int i = Dims - 1; i >= 1; --i) {\n+        IndexType inputDimSize = inputs[blockIdx.y].inputParam.tensorSize[i];\n+        IndexType curDimSize = i == concatDim ? inputDimSize : os.tensorSize[i];\n+        IndexType nextDimIndex = linearIndex / curDimSize;\n+        IndexType curDimIndex = linearIndex - curDimSize * nextDimIndex;\n+        inbound = inbound && curDimIndex < inputDimSize;\n+        inputOffset +=\n+            curDimIndex * inputs[blockIdx.y].inputParam.tensorStride[i];\n+        outputOffset += curDimIndex * os.tensorStride[i];\n+        linearIndex = nextDimIndex;", "path": "aten/src/THC/THCTensorMath.cuh", "position": null, "original_position": 133, "commit_id": "5b48bc2cd9a61cb81e09f008730bc096561a0e9a", "original_commit_id": "59ed88365c35107ffd8ce807ab97989b30bdbaa0", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "you can't assign loop variable within a loop, this leads to infinite looping and hangs. ", "created_at": "2018-09-27T00:42:33Z", "updated_at": "2018-11-23T15:52:03Z", "html_url": "https://github.com/pytorch/pytorch/pull/11494#discussion_r220764929", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11494", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/220764929"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11494#discussion_r220764929"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11494"}}, "body_html": "<p>you can't assign loop variable within a loop, this leads to infinite looping and hangs.</p>", "body_text": "you can't assign loop variable within a loop, this leads to infinite looping and hangs."}