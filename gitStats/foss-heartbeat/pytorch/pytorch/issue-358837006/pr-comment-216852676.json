{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216852676", "pull_request_review_id": 154426825, "id": 216852676, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjg1MjY3Ng==", "diff_hunk": "@@ -736,11 +736,16 @@ def parse_kwargs(desc):\n \n add_docstr(torch.cat,\n            r\"\"\"\n-cat(seq, dim=0, out=None) -> Tensor\n+cat(seq, dim=0, pad=False, pad_value=0, out=None) -> Tensor\n \n Concatenates the given sequence of :attr:`seq` tensors in the given dimension.\n-All tensors must either have the same shape (except in the concatenating\n-dimension) or be empty.\n+Empty tensors will be ignored.\n+If padding is set to false, all tensors must have the same shape\n+(except in the concatenating dimension).\n+If padding is set to true, all tensors do not need to have the same shape\n+(except in the concatenating dimension). Except the concatenating dimension,\n+every dimension of every input tensor will be expanded to the largest size of\n+this dimension owned by attr:`seq` tensors through padding :attr:`pad_value`.", "path": "torch/_torch_docs.py", "position": null, "original_position": 16, "commit_id": "5b48bc2cd9a61cb81e09f008730bc096561a0e9a", "original_commit_id": "d0c96533f2f3d54d3c8aa3b5286862549d99686c", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "tensors don't \"own\" dimensions.  I'd rearrange this a bit to say \"The size of the resulting tensor is the size of the sum of the dimensions in the concatenating dimension, and the maximum of the dimension sizes in all other dimensions.  If an input tensor needs to be logically expanded to fill out its place in the resulting tensor, it is padded with :attr:`pad_value`.\r\n\r\nYou then also want to talk about the case where a tensor doesn't have that dimension (and give an example below).", "created_at": "2018-09-11T23:11:29Z", "updated_at": "2018-11-23T15:51:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/11494#discussion_r216852676", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11494", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216852676"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11494#discussion_r216852676"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11494"}}, "body_html": "<p>tensors don't \"own\" dimensions.  I'd rearrange this a bit to say \"The size of the resulting tensor is the size of the sum of the dimensions in the concatenating dimension, and the maximum of the dimension sizes in all other dimensions.  If an input tensor needs to be logically expanded to fill out its place in the resulting tensor, it is padded with :attr:<code>pad_value</code>.</p>\n<p>You then also want to talk about the case where a tensor doesn't have that dimension (and give an example below).</p>", "body_text": "tensors don't \"own\" dimensions.  I'd rearrange this a bit to say \"The size of the resulting tensor is the size of the sum of the dimensions in the concatenating dimension, and the maximum of the dimension sizes in all other dimensions.  If an input tensor needs to be logically expanded to fill out its place in the resulting tensor, it is padded with :attr:pad_value.\nYou then also want to talk about the case where a tensor doesn't have that dimension (and give an example below)."}