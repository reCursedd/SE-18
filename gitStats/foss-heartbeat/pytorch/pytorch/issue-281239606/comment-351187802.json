{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/351187802", "html_url": "https://github.com/pytorch/pytorch/pull/4127#issuecomment-351187802", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4127", "id": 351187802, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTE4NzgwMg==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-12T20:42:22Z", "updated_at": "2017-12-12T20:42:22Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>In fact, some native functions would greatly benefit from custom derivatives - right now when differentiating split, we treat it as a bunch of narrows, so for each of them we have to allocate a full sized tensor, fill it with zeros, copy the grad into a slice, and then add all of these huge arrays together.</p>\n</blockquote>\n<p>custom derivatives are already possible and in fact split is implemented in this way.  The question is what we do when they aren't implemented explicitly.</p>", "body_text": "In fact, some native functions would greatly benefit from custom derivatives - right now when differentiating split, we treat it as a bunch of narrows, so for each of them we have to allocate a full sized tensor, fill it with zeros, copy the grad into a slice, and then add all of these huge arrays together.\n\ncustom derivatives are already possible and in fact split is implemented in this way.  The question is what we do when they aren't implemented explicitly.", "body": "> In fact, some native functions would greatly benefit from custom derivatives - right now when differentiating split, we treat it as a bunch of narrows, so for each of them we have to allocate a full sized tensor, fill it with zeros, copy the grad into a slice, and then add all of these huge arrays together.\r\n\r\ncustom derivatives are already possible and in fact split is implemented in this way.  The question is what we do when they aren't implemented explicitly.\r\n"}