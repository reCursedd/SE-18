{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4127", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4127/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4127/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4127/events", "html_url": "https://github.com/pytorch/pytorch/pull/4127", "id": 281239606, "node_id": "MDExOlB1bGxSZXF1ZXN0MTU3NzI0NjQ5", "number": 4127, "title": "Trace ATen native functions as themselves, not their implementations.", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2017-12-12T02:52:40Z", "updated_at": "2018-11-23T15:37:16Z", "closed_at": "2017-12-15T18:50:32Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4127", "html_url": "https://github.com/pytorch/pytorch/pull/4127", "diff_url": "https://github.com/pytorch/pytorch/pull/4127.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4127.patch"}, "body_html": "<pre><code>    Trace ATen non-primitive functions as themselves, not their implementations.\n    \n    Previously, if I invoked an ATen non-primitive function foo, which in turn\n    called subfoo, I would always see 'subfoo' in the trace (e.g., tracing\n    'inlines' all of these operations.)  Such inlining is bad for ONNX\n    (and can be bad for optimization) as it prevents high-level\n    optimizations from taking advantage of the structure.  It might\n    be right to inline, but give the optimizer a chance to work before\n    inlining happens!\n    \n    The implementation here is surprisingly simple, because it uses\n    the \"DCE trick\".  Essentially, it doesn't matter if the constituent\n    calls perform tracing, because you can always trace it again, and\n    override the trace nodes associated with the returned variables.\n    The original trace becomes dead and can be DCE'd.\n    \n    While implementing this, I also refactored how 'isTracing' and\n    'trace_outputs' works:\n    \n    - isTracing was previously a single function with overloads for\n      both Tensor and Variable arguments.  Unfortunately, such overloads\n      are not safe, because of how C++ implicit conversions work.  You\n      would think that C++ should never confuse an overload for\n      Variable with ArrayRef&lt;Tensor&gt;, but this is exactly what can\n      happen: Tensor is convertible to both Variable and ArrayRef&lt;Tensor&gt;,\n      thus it's ambiguous and C++ doesn't like it.  The last time I ran\n      into this problem, I applied initializer lists to everything and\n      called it a day.  A more robust fix is to separate out the\n      Variable and Tensor overloads, which I have done in this patch.\n    \n    - trace_outputs was fed as an initializer list, which doesn't work\n      when you have heterogenous inputs.  So instead we first feed\n      everything through 'flatten', which has overloads for each of the\n      argument patterns in ATen, which then goes on to the recordTrace\n      (which takes an ArrayRef).  This is *no less efficient*, because\n      we were allocating a vector anyway (to do the conversion from\n      vector of Tensor to vector of Variable).\n    \n    This fixes mean that 'index' can properly be traced... although the\n    JIT still does not support it.  A failing test case has been added to\n    this effect.\n    \n    Some knock-on effects:\n    \n    - The fuser now knows about chunk as well as split.  They're pretty\n      similar so there is no problem.\n    \n    - There is a new 'canonicalize' pass in the JIT which renumbers a graph\n      so that all structurally equivalent graphs render the same.\n    \n    - We run DCE before the fuser tests, to make sure dead nodes don't\n      block fusion.\n    \n    - There are new ONNX exports for the newly introduced higher level ATen\n      operations.  This includes type_as (no-op case only), chunk, select.\n    \n    Zach didn't like the extra use of 'native' in the new codegen, so\n    we've introduced a new concept, 'primitive'.  A primitive function\n    is one that cannot be differentiated automatically; you need to\n    specify the backward somehow.  ATen Declarations.yaml generation\n    is updated to produce this bit, and gen_variable_type.py makes\n    use of this information to decide which dispatch path to codegen.\n</code></pre>", "body_text": "Trace ATen non-primitive functions as themselves, not their implementations.\n    \n    Previously, if I invoked an ATen non-primitive function foo, which in turn\n    called subfoo, I would always see 'subfoo' in the trace (e.g., tracing\n    'inlines' all of these operations.)  Such inlining is bad for ONNX\n    (and can be bad for optimization) as it prevents high-level\n    optimizations from taking advantage of the structure.  It might\n    be right to inline, but give the optimizer a chance to work before\n    inlining happens!\n    \n    The implementation here is surprisingly simple, because it uses\n    the \"DCE trick\".  Essentially, it doesn't matter if the constituent\n    calls perform tracing, because you can always trace it again, and\n    override the trace nodes associated with the returned variables.\n    The original trace becomes dead and can be DCE'd.\n    \n    While implementing this, I also refactored how 'isTracing' and\n    'trace_outputs' works:\n    \n    - isTracing was previously a single function with overloads for\n      both Tensor and Variable arguments.  Unfortunately, such overloads\n      are not safe, because of how C++ implicit conversions work.  You\n      would think that C++ should never confuse an overload for\n      Variable with ArrayRef<Tensor>, but this is exactly what can\n      happen: Tensor is convertible to both Variable and ArrayRef<Tensor>,\n      thus it's ambiguous and C++ doesn't like it.  The last time I ran\n      into this problem, I applied initializer lists to everything and\n      called it a day.  A more robust fix is to separate out the\n      Variable and Tensor overloads, which I have done in this patch.\n    \n    - trace_outputs was fed as an initializer list, which doesn't work\n      when you have heterogenous inputs.  So instead we first feed\n      everything through 'flatten', which has overloads for each of the\n      argument patterns in ATen, which then goes on to the recordTrace\n      (which takes an ArrayRef).  This is *no less efficient*, because\n      we were allocating a vector anyway (to do the conversion from\n      vector of Tensor to vector of Variable).\n    \n    This fixes mean that 'index' can properly be traced... although the\n    JIT still does not support it.  A failing test case has been added to\n    this effect.\n    \n    Some knock-on effects:\n    \n    - The fuser now knows about chunk as well as split.  They're pretty\n      similar so there is no problem.\n    \n    - There is a new 'canonicalize' pass in the JIT which renumbers a graph\n      so that all structurally equivalent graphs render the same.\n    \n    - We run DCE before the fuser tests, to make sure dead nodes don't\n      block fusion.\n    \n    - There are new ONNX exports for the newly introduced higher level ATen\n      operations.  This includes type_as (no-op case only), chunk, select.\n    \n    Zach didn't like the extra use of 'native' in the new codegen, so\n    we've introduced a new concept, 'primitive'.  A primitive function\n    is one that cannot be differentiated automatically; you need to\n    specify the backward somehow.  ATen Declarations.yaml generation\n    is updated to produce this bit, and gen_variable_type.py makes\n    use of this information to decide which dispatch path to codegen.", "body": "```\r\n    Trace ATen non-primitive functions as themselves, not their implementations.\r\n    \r\n    Previously, if I invoked an ATen non-primitive function foo, which in turn\r\n    called subfoo, I would always see 'subfoo' in the trace (e.g., tracing\r\n    'inlines' all of these operations.)  Such inlining is bad for ONNX\r\n    (and can be bad for optimization) as it prevents high-level\r\n    optimizations from taking advantage of the structure.  It might\r\n    be right to inline, but give the optimizer a chance to work before\r\n    inlining happens!\r\n    \r\n    The implementation here is surprisingly simple, because it uses\r\n    the \"DCE trick\".  Essentially, it doesn't matter if the constituent\r\n    calls perform tracing, because you can always trace it again, and\r\n    override the trace nodes associated with the returned variables.\r\n    The original trace becomes dead and can be DCE'd.\r\n    \r\n    While implementing this, I also refactored how 'isTracing' and\r\n    'trace_outputs' works:\r\n    \r\n    - isTracing was previously a single function with overloads for\r\n      both Tensor and Variable arguments.  Unfortunately, such overloads\r\n      are not safe, because of how C++ implicit conversions work.  You\r\n      would think that C++ should never confuse an overload for\r\n      Variable with ArrayRef<Tensor>, but this is exactly what can\r\n      happen: Tensor is convertible to both Variable and ArrayRef<Tensor>,\r\n      thus it's ambiguous and C++ doesn't like it.  The last time I ran\r\n      into this problem, I applied initializer lists to everything and\r\n      called it a day.  A more robust fix is to separate out the\r\n      Variable and Tensor overloads, which I have done in this patch.\r\n    \r\n    - trace_outputs was fed as an initializer list, which doesn't work\r\n      when you have heterogenous inputs.  So instead we first feed\r\n      everything through 'flatten', which has overloads for each of the\r\n      argument patterns in ATen, which then goes on to the recordTrace\r\n      (which takes an ArrayRef).  This is *no less efficient*, because\r\n      we were allocating a vector anyway (to do the conversion from\r\n      vector of Tensor to vector of Variable).\r\n    \r\n    This fixes mean that 'index' can properly be traced... although the\r\n    JIT still does not support it.  A failing test case has been added to\r\n    this effect.\r\n    \r\n    Some knock-on effects:\r\n    \r\n    - The fuser now knows about chunk as well as split.  They're pretty\r\n      similar so there is no problem.\r\n    \r\n    - There is a new 'canonicalize' pass in the JIT which renumbers a graph\r\n      so that all structurally equivalent graphs render the same.\r\n    \r\n    - We run DCE before the fuser tests, to make sure dead nodes don't\r\n      block fusion.\r\n    \r\n    - There are new ONNX exports for the newly introduced higher level ATen\r\n      operations.  This includes type_as (no-op case only), chunk, select.\r\n    \r\n    Zach didn't like the extra use of 'native' in the new codegen, so\r\n    we've introduced a new concept, 'primitive'.  A primitive function\r\n    is one that cannot be differentiated automatically; you need to\r\n    specify the backward somehow.  ATen Declarations.yaml generation\r\n    is updated to produce this bit, and gen_variable_type.py makes\r\n    use of this information to decide which dispatch path to codegen.\r\n```"}