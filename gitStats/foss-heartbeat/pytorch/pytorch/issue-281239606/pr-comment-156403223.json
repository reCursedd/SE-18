{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156403223", "pull_request_review_id": 82874273, "id": 156403223, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NjQwMzIyMw==", "diff_hunk": "@@ -782,6 +806,42 @@ def emit_check_no_requires_grad(tensor_args, args_with_derivatives):\n             body.append('check_no_requires_grad({}, \"{}\");'.format(name, name))\n         return body\n \n+    def emit_native_body(declaration):", "path": "tools/autograd/gen_variable_type.py", "position": null, "original_position": 73, "commit_id": "2e37b5ac26214da7c445ce2c582bed309dfd26f7", "original_commit_id": "3fddba1c2a5fe568c38f040f1ce926b872f64d52", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "I guess the primary objection here is the \"duplication\" in `return_value` and `trace_outputs` computation? In the case of list return, they are not really duplicates at all in their current form, and this has to do with how I decided to do the codegen. Compare:\r\n\r\n```\r\n// Native dispatch\r\nstd::vector<Tensor> VariableType::chunk(const Tensor & self, int64_t chunks, int64_t dim) const {\r\n    profiler::RecordFunction profiler(\"chunk\");\r\n    auto ret = Type::chunk(self, chunks, dim);\r\n    if (jit::tracer::isTracing( self )) {\r\n      jit::Node *n = jit::tracer::recordTrace( \"chunk\", { self }, cast_tensor_list(ret) );\r\n      setattr(n, jit::stringToSymbol(\"chunks\"), chunks);\r\n      setattr(n, jit::stringToSymbol(\"dim\"), dim);\r\n    }\r\n    return ret;\r\n}\r\n\r\n// Non-native function\r\nstd::vector<Tensor> VariableType::split(const Tensor & self, int64_t split_size, int64_t dim) const {\r\n    profiler::RecordFunction profiler(\"split\");\r\n    auto& self_ = unpack(self, \"self\", 0);\r\n    std::shared_ptr<SplitBackward> grad_fn;\r\n    auto flags = compute_flags({ self });\r\n    if (flags.requires_grad) {\r\n      grad_fn = std::make_shared<SplitBackward>();\r\n      grad_fn->next_functions = compute_next_functions({ self });\r\n      grad_fn->self_sizes = self.sizes();\r\n      grad_fn->self_ = SavedVariable(self, false);\r\n      grad_fn->split_size = split_size;\r\n      grad_fn->dim = dim;\r\n    }\r\n    auto ret = as_variable(baseType->split(self_, split_size, dim));\r\n    set_flags(ret, flags, grad_fn);\r\n    if (jit::tracer::isTracing( self )) {\r\n      jit::Node *n = jit::tracer::recordTrace( \"split\", { self }, ret );\r\n      setattr(n, jit::stringToSymbol(\"split_size\"), split_size);\r\n      setattr(n, jit::stringToSymbol(\"dim\"), dim);\r\n    }\r\n    return as_tensor_list(ret);\r\n}\r\n```\r\n\r\nThe reason why you can't just reuse the computation here is because the types of `ret` codgenned here are different. `ret` in the native case is a `std::vector<Tensor>`, but it is a `std:vector<Variable>` in the non-native case (due to the `as_variable` computation.)\r\n\r\nI suppose we could merge the codepaths by renaming `ret` and then adding an impedence matching assignment:\r\n\r\n```\r\n// Native dispatch\r\nstd::vector<Tensor> VariableType::chunk(const Tensor & self, int64_t chunks, int64_t dim) const {\r\n    profiler::RecordFunction profiler(\"chunk\");\r\n    auto var_ret = Type::chunk(self, chunks, dim); // var_ret is std::vector<Tensor>\r\n    if (jit::tracer::isTracing( self )) {\r\n      auto ret = cast_tensor_list(var_ret); // recordTrace requires ArrayRef<Variable>\r\n      jit::Node *n = jit::tracer::recordTrace( \"chunk\", { self }, ret );\r\n      setattr(n, jit::stringToSymbol(\"chunks\"), chunks);\r\n      setattr(n, jit::stringToSymbol(\"dim\"), dim);\r\n    }\r\n    return var_ret;\r\n}\r\n```\r\n\r\nAnd then, to use the same codepath, I suppose the native template could ignore all of the extra bits (variable flags setting, variable wrapping, etc.) which it doesn't need? Somehow this doesn't seem more clear to me.", "created_at": "2017-12-12T15:40:26Z", "updated_at": "2018-11-23T15:37:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/4127#discussion_r156403223", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4127", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156403223"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4127#discussion_r156403223"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4127"}}, "body_html": "<p>I guess the primary objection here is the \"duplication\" in <code>return_value</code> and <code>trace_outputs</code> computation? In the case of list return, they are not really duplicates at all in their current form, and this has to do with how I decided to do the codegen. Compare:</p>\n<pre><code>// Native dispatch\nstd::vector&lt;Tensor&gt; VariableType::chunk(const Tensor &amp; self, int64_t chunks, int64_t dim) const {\n    profiler::RecordFunction profiler(\"chunk\");\n    auto ret = Type::chunk(self, chunks, dim);\n    if (jit::tracer::isTracing( self )) {\n      jit::Node *n = jit::tracer::recordTrace( \"chunk\", { self }, cast_tensor_list(ret) );\n      setattr(n, jit::stringToSymbol(\"chunks\"), chunks);\n      setattr(n, jit::stringToSymbol(\"dim\"), dim);\n    }\n    return ret;\n}\n\n// Non-native function\nstd::vector&lt;Tensor&gt; VariableType::split(const Tensor &amp; self, int64_t split_size, int64_t dim) const {\n    profiler::RecordFunction profiler(\"split\");\n    auto&amp; self_ = unpack(self, \"self\", 0);\n    std::shared_ptr&lt;SplitBackward&gt; grad_fn;\n    auto flags = compute_flags({ self });\n    if (flags.requires_grad) {\n      grad_fn = std::make_shared&lt;SplitBackward&gt;();\n      grad_fn-&gt;next_functions = compute_next_functions({ self });\n      grad_fn-&gt;self_sizes = self.sizes();\n      grad_fn-&gt;self_ = SavedVariable(self, false);\n      grad_fn-&gt;split_size = split_size;\n      grad_fn-&gt;dim = dim;\n    }\n    auto ret = as_variable(baseType-&gt;split(self_, split_size, dim));\n    set_flags(ret, flags, grad_fn);\n    if (jit::tracer::isTracing( self )) {\n      jit::Node *n = jit::tracer::recordTrace( \"split\", { self }, ret );\n      setattr(n, jit::stringToSymbol(\"split_size\"), split_size);\n      setattr(n, jit::stringToSymbol(\"dim\"), dim);\n    }\n    return as_tensor_list(ret);\n}\n</code></pre>\n<p>The reason why you can't just reuse the computation here is because the types of <code>ret</code> codgenned here are different. <code>ret</code> in the native case is a <code>std::vector&lt;Tensor&gt;</code>, but it is a <code>std:vector&lt;Variable&gt;</code> in the non-native case (due to the <code>as_variable</code> computation.)</p>\n<p>I suppose we could merge the codepaths by renaming <code>ret</code> and then adding an impedence matching assignment:</p>\n<pre><code>// Native dispatch\nstd::vector&lt;Tensor&gt; VariableType::chunk(const Tensor &amp; self, int64_t chunks, int64_t dim) const {\n    profiler::RecordFunction profiler(\"chunk\");\n    auto var_ret = Type::chunk(self, chunks, dim); // var_ret is std::vector&lt;Tensor&gt;\n    if (jit::tracer::isTracing( self )) {\n      auto ret = cast_tensor_list(var_ret); // recordTrace requires ArrayRef&lt;Variable&gt;\n      jit::Node *n = jit::tracer::recordTrace( \"chunk\", { self }, ret );\n      setattr(n, jit::stringToSymbol(\"chunks\"), chunks);\n      setattr(n, jit::stringToSymbol(\"dim\"), dim);\n    }\n    return var_ret;\n}\n</code></pre>\n<p>And then, to use the same codepath, I suppose the native template could ignore all of the extra bits (variable flags setting, variable wrapping, etc.) which it doesn't need? Somehow this doesn't seem more clear to me.</p>", "body_text": "I guess the primary objection here is the \"duplication\" in return_value and trace_outputs computation? In the case of list return, they are not really duplicates at all in their current form, and this has to do with how I decided to do the codegen. Compare:\n// Native dispatch\nstd::vector<Tensor> VariableType::chunk(const Tensor & self, int64_t chunks, int64_t dim) const {\n    profiler::RecordFunction profiler(\"chunk\");\n    auto ret = Type::chunk(self, chunks, dim);\n    if (jit::tracer::isTracing( self )) {\n      jit::Node *n = jit::tracer::recordTrace( \"chunk\", { self }, cast_tensor_list(ret) );\n      setattr(n, jit::stringToSymbol(\"chunks\"), chunks);\n      setattr(n, jit::stringToSymbol(\"dim\"), dim);\n    }\n    return ret;\n}\n\n// Non-native function\nstd::vector<Tensor> VariableType::split(const Tensor & self, int64_t split_size, int64_t dim) const {\n    profiler::RecordFunction profiler(\"split\");\n    auto& self_ = unpack(self, \"self\", 0);\n    std::shared_ptr<SplitBackward> grad_fn;\n    auto flags = compute_flags({ self });\n    if (flags.requires_grad) {\n      grad_fn = std::make_shared<SplitBackward>();\n      grad_fn->next_functions = compute_next_functions({ self });\n      grad_fn->self_sizes = self.sizes();\n      grad_fn->self_ = SavedVariable(self, false);\n      grad_fn->split_size = split_size;\n      grad_fn->dim = dim;\n    }\n    auto ret = as_variable(baseType->split(self_, split_size, dim));\n    set_flags(ret, flags, grad_fn);\n    if (jit::tracer::isTracing( self )) {\n      jit::Node *n = jit::tracer::recordTrace( \"split\", { self }, ret );\n      setattr(n, jit::stringToSymbol(\"split_size\"), split_size);\n      setattr(n, jit::stringToSymbol(\"dim\"), dim);\n    }\n    return as_tensor_list(ret);\n}\n\nThe reason why you can't just reuse the computation here is because the types of ret codgenned here are different. ret in the native case is a std::vector<Tensor>, but it is a std:vector<Variable> in the non-native case (due to the as_variable computation.)\nI suppose we could merge the codepaths by renaming ret and then adding an impedence matching assignment:\n// Native dispatch\nstd::vector<Tensor> VariableType::chunk(const Tensor & self, int64_t chunks, int64_t dim) const {\n    profiler::RecordFunction profiler(\"chunk\");\n    auto var_ret = Type::chunk(self, chunks, dim); // var_ret is std::vector<Tensor>\n    if (jit::tracer::isTracing( self )) {\n      auto ret = cast_tensor_list(var_ret); // recordTrace requires ArrayRef<Variable>\n      jit::Node *n = jit::tracer::recordTrace( \"chunk\", { self }, ret );\n      setattr(n, jit::stringToSymbol(\"chunks\"), chunks);\n      setattr(n, jit::stringToSymbol(\"dim\"), dim);\n    }\n    return var_ret;\n}\n\nAnd then, to use the same codepath, I suppose the native template could ignore all of the extra bits (variable flags setting, variable wrapping, etc.) which it doesn't need? Somehow this doesn't seem more clear to me.", "in_reply_to_id": 156279219}