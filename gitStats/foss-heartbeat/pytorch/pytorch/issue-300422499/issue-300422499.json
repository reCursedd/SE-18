{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5421", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5421/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5421/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5421/events", "html_url": "https://github.com/pytorch/pytorch/issues/5421", "id": 300422499, "node_id": "MDU6SXNzdWUzMDA0MjI0OTk=", "number": 5421, "title": "with nn.DataParallel program runs slower", "user": {"login": "Apogentus", "id": 10587992, "node_id": "MDQ6VXNlcjEwNTg3OTky", "avatar_url": "https://avatars1.githubusercontent.com/u/10587992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Apogentus", "html_url": "https://github.com/Apogentus", "followers_url": "https://api.github.com/users/Apogentus/followers", "following_url": "https://api.github.com/users/Apogentus/following{/other_user}", "gists_url": "https://api.github.com/users/Apogentus/gists{/gist_id}", "starred_url": "https://api.github.com/users/Apogentus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Apogentus/subscriptions", "organizations_url": "https://api.github.com/users/Apogentus/orgs", "repos_url": "https://api.github.com/users/Apogentus/repos", "events_url": "https://api.github.com/users/Apogentus/events{/privacy}", "received_events_url": "https://api.github.com/users/Apogentus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-02-26T22:34:24Z", "updated_at": "2018-02-27T09:44:59Z", "closed_at": "2018-02-27T07:50:45Z", "author_association": "NONE", "body_html": "<p>Hello,<br>\nI ran classification tutorial from pytorch, and I noticed that using nn.DataParallel wrapper around the model forces forward&amp;backward passes to run much slower. How can I take advantage of 2 GPUs?</p>\n<p>I use Ubuntu 16.04 LTS, python 3.6, pytorch 0.3.1.post2, cuDNN 7005, 2 GPUs GeForce 1080.</p>\n<p>Code:</p>\n<pre><code>%matplotlib inline\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=16,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 20, 3)\n        self.conv2 = nn.Conv2d(20, 40, 3)\n        self.conv3 = nn.Conv2d(40, 80, 5)\n        self.pool = nn.MaxPool2d(3, 3)\n        self.fc1 = nn.Linear(5120, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = self.pool(x)\n        #print(x.shape)\n        x = x.view(-1, 5120)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ndataiter = iter(testloader)\ninputs, labels = dataiter.next()\ninputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n\ncriterion = nn.CrossEntropyLoss()\n\nnet = Net().cuda()\n\n%timeit loss=criterion(net(inputs),labels); loss.backward()\n\nnet = nn.DataParallel(Net().cuda())\n\n%timeit loss=criterion(net(inputs),labels); loss.backward()\n</code></pre>\n<p>1st timeit gives 2 ms, second - 6ms, though second utilizes 2 GPUs.</p>", "body_text": "Hello,\nI ran classification tutorial from pytorch, and I noticed that using nn.DataParallel wrapper around the model forces forward&backward passes to run much slower. How can I take advantage of 2 GPUs?\nI use Ubuntu 16.04 LTS, python 3.6, pytorch 0.3.1.post2, cuDNN 7005, 2 GPUs GeForce 1080.\nCode:\n%matplotlib inline\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=16,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 20, 3)\n        self.conv2 = nn.Conv2d(20, 40, 3)\n        self.conv3 = nn.Conv2d(40, 80, 5)\n        self.pool = nn.MaxPool2d(3, 3)\n        self.fc1 = nn.Linear(5120, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = self.pool(x)\n        #print(x.shape)\n        x = x.view(-1, 5120)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ndataiter = iter(testloader)\ninputs, labels = dataiter.next()\ninputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n\ncriterion = nn.CrossEntropyLoss()\n\nnet = Net().cuda()\n\n%timeit loss=criterion(net(inputs),labels); loss.backward()\n\nnet = nn.DataParallel(Net().cuda())\n\n%timeit loss=criterion(net(inputs),labels); loss.backward()\n\n1st timeit gives 2 ms, second - 6ms, though second utilizes 2 GPUs.", "body": "Hello,\r\nI ran classification tutorial from pytorch, and I noticed that using nn.DataParallel wrapper around the model forces forward&backward passes to run much slower. How can I take advantage of 2 GPUs?\r\n\r\nI use Ubuntu 16.04 LTS, python 3.6, pytorch 0.3.1.post2, cuDNN 7005, 2 GPUs GeForce 1080.\r\n\r\nCode:\r\n```\r\n%matplotlib inline\r\n\r\nimport torch\r\nimport torchvision\r\nimport torchvision.transforms as transforms\r\n\r\ntransform = transforms.Compose(\r\n    [transforms.ToTensor(),\r\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n\r\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\r\n                                        download=True, transform=transform)\r\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\r\n                                          shuffle=True, num_workers=2)\r\n\r\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\r\n                                       download=True, transform=transform)\r\ntestloader = torch.utils.data.DataLoader(testset, batch_size=16,\r\n                                         shuffle=False, num_workers=2)\r\n\r\nclasses = ('plane', 'car', 'bird', 'cat',\r\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\r\n\r\n\r\nfrom torch.autograd import Variable\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 20, 3)\r\n        self.conv2 = nn.Conv2d(20, 40, 3)\r\n        self.conv3 = nn.Conv2d(40, 80, 5)\r\n        self.pool = nn.MaxPool2d(3, 3)\r\n        self.fc1 = nn.Linear(5120, 50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.conv1(x))\r\n        x = F.relu(self.conv2(x))\r\n        x = F.relu(self.conv3(x))\r\n        x = self.pool(x)\r\n        #print(x.shape)\r\n        x = x.view(-1, 5120)\r\n        x = F.relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\ndataiter = iter(testloader)\r\ninputs, labels = dataiter.next()\r\ninputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\r\n\r\ncriterion = nn.CrossEntropyLoss()\r\n\r\nnet = Net().cuda()\r\n\r\n%timeit loss=criterion(net(inputs),labels); loss.backward()\r\n\r\nnet = nn.DataParallel(Net().cuda())\r\n\r\n%timeit loss=criterion(net(inputs),labels); loss.backward()\r\n```\r\n1st timeit gives 2 ms, second - 6ms, though second utilizes 2 GPUs."}