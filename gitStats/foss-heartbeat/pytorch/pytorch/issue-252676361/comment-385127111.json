{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/385127111", "html_url": "https://github.com/pytorch/pytorch/issues/2530#issuecomment-385127111", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2530", "id": 385127111, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTEyNzExMQ==", "user": {"login": "EdwardZhang88", "id": 28944236, "node_id": "MDQ6VXNlcjI4OTQ0MjM2", "avatar_url": "https://avatars1.githubusercontent.com/u/28944236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EdwardZhang88", "html_url": "https://github.com/EdwardZhang88", "followers_url": "https://api.github.com/users/EdwardZhang88/followers", "following_url": "https://api.github.com/users/EdwardZhang88/following{/other_user}", "gists_url": "https://api.github.com/users/EdwardZhang88/gists{/gist_id}", "starred_url": "https://api.github.com/users/EdwardZhang88/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EdwardZhang88/subscriptions", "organizations_url": "https://api.github.com/users/EdwardZhang88/orgs", "repos_url": "https://api.github.com/users/EdwardZhang88/repos", "events_url": "https://api.github.com/users/EdwardZhang88/events{/privacy}", "received_events_url": "https://api.github.com/users/EdwardZhang88/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-28T00:53:35Z", "updated_at": "2018-04-28T00:53:35Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>  I have the same error with distributed training using gloo backend. It happened to both the latest v0.4.0 and also the master 0.5.0a0. Using NCCL2 in v0.4.0 is working fine though.<br>\nterminate called after throwing an instance of 'gloo::EnforceNotMet'<br>\nwhat():  [enforce fail at /tmp/pip-6uwp_cfg-build/third_party/gloo/gloo/cuda_private.h:40] error == cudaSuccess. 29 vs 0. Error at: /tmp/pip-6uwp_cfg-build/third_party/gloo/gloo/cuda_private.h:40: driver shutting down<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=736005\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pankajkumar\">@pankajkumar</a> Are you able to get rid of this error?</p>", "body_text": "@apaszke  I have the same error with distributed training using gloo backend. It happened to both the latest v0.4.0 and also the master 0.5.0a0. Using NCCL2 in v0.4.0 is working fine though.\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\nwhat():  [enforce fail at /tmp/pip-6uwp_cfg-build/third_party/gloo/gloo/cuda_private.h:40] error == cudaSuccess. 29 vs 0. Error at: /tmp/pip-6uwp_cfg-build/third_party/gloo/gloo/cuda_private.h:40: driver shutting down\n@pankajkumar Are you able to get rid of this error?", "body": "@apaszke  I have the same error with distributed training using gloo backend. It happened to both the latest v0.4.0 and also the master 0.5.0a0. Using NCCL2 in v0.4.0 is working fine though. \r\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\r\n  what():  [enforce fail at /tmp/pip-6uwp_cfg-build/third_party/gloo/gloo/cuda_private.h:40] error == cudaSuccess. 29 vs 0. Error at: /tmp/pip-6uwp_cfg-build/third_party/gloo/gloo/cuda_private.h:40: driver shutting down\r\n@pankajkumar Are you able to get rid of this error?"}