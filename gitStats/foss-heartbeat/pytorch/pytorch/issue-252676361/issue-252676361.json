{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2530", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2530/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2530/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2530/events", "html_url": "https://github.com/pytorch/pytorch/issues/2530", "id": 252676361, "node_id": "MDU6SXNzdWUyNTI2NzYzNjE=", "number": 2530, "title": "Gloo throwing \"EnforceNotMet\" on distributed shutdown when using CUDA", "user": {"login": "ArEsKay3", "id": 6547143, "node_id": "MDQ6VXNlcjY1NDcxNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6547143?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ArEsKay3", "html_url": "https://github.com/ArEsKay3", "followers_url": "https://api.github.com/users/ArEsKay3/followers", "following_url": "https://api.github.com/users/ArEsKay3/following{/other_user}", "gists_url": "https://api.github.com/users/ArEsKay3/gists{/gist_id}", "starred_url": "https://api.github.com/users/ArEsKay3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ArEsKay3/subscriptions", "organizations_url": "https://api.github.com/users/ArEsKay3/orgs", "repos_url": "https://api.github.com/users/ArEsKay3/repos", "events_url": "https://api.github.com/users/ArEsKay3/events{/privacy}", "received_events_url": "https://api.github.com/users/ArEsKay3/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2017-08-24T17:36:41Z", "updated_at": "2018-06-26T18:45:12Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>This is showing up when tearing down DistributedDataParallel now that <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"252391863\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2520\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2520/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2520\">#2520</a> is fixed. However, it doesn't seem to be related to DistribuedDataParallel code as I can reproduce it with just dist.broadcast(). Simple repro below.</p>\n<p>Error:</p>\n<pre><code>terminate called after throwing an instance of 'gloo::EnforceNotMet'\n  what():  [enforce fail at /tmp/pip-fj8uIV-build/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /tmp/pip-fj8uIV-build/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down\n</code></pre>\n<p>Repro:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> distributed <span class=\"pl-k\">as</span> dist\n<span class=\"pl-k\">import</span> torch\n\ndist.init_process_group(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>gloo<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">init_method</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>file://&lt;FILEPATH&gt;<span class=\"pl-pds\">'</span></span>, \\\n    <span class=\"pl-v\">world_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Finished initializing process group.<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Broadcasting tensor of ones from rank 0.<span class=\"pl-pds\">'</span></span>)\ntensor <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>).cuda()\nrank <span class=\"pl-k\">=</span> dist.get_rank()\n<span class=\"pl-k\">if</span> rank <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n    tensor <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>).cuda()\ndist.broadcast(tensor, <span class=\"pl-c1\">0</span>)\n<span class=\"pl-k\">if</span> rank <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>:\n    t2 <span class=\"pl-k\">=</span> tensor.add(<span class=\"pl-c1\">20</span>)\n    <span class=\"pl-c1\">print</span>(t2)</pre></div>", "body_text": "This is showing up when tearing down DistributedDataParallel now that #2520 is fixed. However, it doesn't seem to be related to DistribuedDataParallel code as I can reproduce it with just dist.broadcast(). Simple repro below.\nError:\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\n  what():  [enforce fail at /tmp/pip-fj8uIV-build/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /tmp/pip-fj8uIV-build/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down\n\nRepro:\nfrom torch import distributed as dist\nimport torch\n\ndist.init_process_group('gloo', init_method='file://<FILEPATH>', \\\n    world_size=2)\nprint('Finished initializing process group.')\n\nprint('Broadcasting tensor of ones from rank 0.')\ntensor = torch.rand(2, 3).cuda()\nrank = dist.get_rank()\nif rank == 0:\n    tensor = torch.ones(2,3).cuda()\ndist.broadcast(tensor, 0)\nif rank == 1:\n    t2 = tensor.add(20)\n    print(t2)", "body": "This is showing up when tearing down DistributedDataParallel now that https://github.com/pytorch/pytorch/issues/2520 is fixed. However, it doesn't seem to be related to DistribuedDataParallel code as I can reproduce it with just dist.broadcast(). Simple repro below.\r\n\r\nError:\r\n```\r\nterminate called after throwing an instance of 'gloo::EnforceNotMet'\r\n  what():  [enforce fail at /tmp/pip-fj8uIV-build/torch/lib/gloo/gloo/cuda.cu:249] error == cudaSuccess. 29 vs 0. Error at: /tmp/pip-fj8uIV-build/torch/lib/gloo/gloo/cuda.cu:249: driver shutting down\r\n```\r\n\r\nRepro:\r\n```python\r\nfrom torch import distributed as dist\r\nimport torch\r\n\r\ndist.init_process_group('gloo', init_method='file://<FILEPATH>', \\\r\n    world_size=2)\r\nprint('Finished initializing process group.')\r\n\r\nprint('Broadcasting tensor of ones from rank 0.')\r\ntensor = torch.rand(2, 3).cuda()\r\nrank = dist.get_rank()\r\nif rank == 0:\r\n    tensor = torch.ones(2,3).cuda()\r\ndist.broadcast(tensor, 0)\r\nif rank == 1:\r\n    t2 = tensor.add(20)\r\n    print(t2)\r\n```\r\n"}