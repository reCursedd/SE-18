{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3336", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3336/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3336/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3336/events", "html_url": "https://github.com/pytorch/pytorch/pull/3336", "id": 269306889, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQ5MzI5ODQ2", "number": 3336, "title": "Prevent numerical issues with poisson_nll_loss when log_input=False", "user": {"login": "gokceneraslan", "id": 1140359, "node_id": "MDQ6VXNlcjExNDAzNTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1140359?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gokceneraslan", "html_url": "https://github.com/gokceneraslan", "followers_url": "https://api.github.com/users/gokceneraslan/followers", "following_url": "https://api.github.com/users/gokceneraslan/following{/other_user}", "gists_url": "https://api.github.com/users/gokceneraslan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gokceneraslan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gokceneraslan/subscriptions", "organizations_url": "https://api.github.com/users/gokceneraslan/orgs", "repos_url": "https://api.github.com/users/gokceneraslan/repos", "events_url": "https://api.github.com/users/gokceneraslan/events{/privacy}", "received_events_url": "https://api.github.com/users/gokceneraslan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-28T10:43:18Z", "updated_at": "2017-11-01T12:47:22Z", "closed_at": "2017-11-01T12:47:20Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/3336", "html_url": "https://github.com/pytorch/pytorch/pull/3336", "diff_url": "https://github.com/pytorch/pytorch/pull/3336.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/3336.patch"}, "body_html": "<p>Evaluation of the logarithm of the input variable in poisson negative log likelihood leads to NaN loss if variable being evaluated is zero. Small epsilon is added to prevent this. See equivalent Keras epsilon here: <a href=\"https://github.com/fchollet/keras/blob/master/keras/losses.py#L68\">https://github.com/fchollet/keras/blob/master/keras/losses.py#L68</a></p>\n<p>Here is the code to reproduce the issue:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\ntorch.manual_seed(<span class=\"pl-c1\">42</span>)\nnp.random.seed(<span class=\"pl-c1\">42</span>)\n\npoisson_mean <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\npoisson_numsample <span class=\"pl-k\">=</span> <span class=\"pl-c1\">200</span>\n\nsamples <span class=\"pl-k\">=</span> np.random.poisson(poisson_mean, poisson_numsample)\n\nparam <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ntorch_samples <span class=\"pl-k\">=</span> Variable(torch.from_numpy(samples).float(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\noptimizer <span class=\"pl-k\">=</span> torch.optim.RMSprop([param], <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\nloss <span class=\"pl-k\">=</span> torch.nn.PoissonNLLLoss(<span class=\"pl-v\">log_input</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">full</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">5000</span>):\n    optimizer.zero_grad()\n    output <span class=\"pl-k\">=</span> loss(param, torch_samples)\n    output.backward()\n\n    <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">1000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loss:<span class=\"pl-pds\">'</span></span>, output.data[<span class=\"pl-c1\">0</span>])\n\n    optimizer.step()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Sample mean: <span class=\"pl-pds\">'</span></span>, torch_samples.mean().data.numpy())\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Mean is : <span class=\"pl-pds\">'</span></span>, param.data.numpy())\n</pre></div>", "body_text": "Evaluation of the logarithm of the input variable in poisson negative log likelihood leads to NaN loss if variable being evaluated is zero. Small epsilon is added to prevent this. See equivalent Keras epsilon here: https://github.com/fchollet/keras/blob/master/keras/losses.py#L68\nHere is the code to reproduce the issue:\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\npoisson_mean = 4\npoisson_numsample = 200\n\nsamples = np.random.poisson(poisson_mean, poisson_numsample)\n\nparam = Variable(torch.zeros(1), requires_grad=True)\ntorch_samples = Variable(torch.from_numpy(samples).float(), requires_grad=False)\n\noptimizer = torch.optim.RMSprop([param], lr=0.1)\nloss = torch.nn.PoissonNLLLoss(log_input=False, full=True)\n\nfor i in range(5000):\n    optimizer.zero_grad()\n    output = loss(param, torch_samples)\n    output.backward()\n\n    if i % 1000 == 0:\n        print('Loss:', output.data[0])\n\n    optimizer.step()\n\nprint('Sample mean: ', torch_samples.mean().data.numpy())\nprint('Mean is : ', param.data.numpy())", "body": "Evaluation of the logarithm of the input variable in poisson negative log likelihood leads to NaN loss if variable being evaluated is zero. Small epsilon is added to prevent this. See equivalent Keras epsilon here: https://github.com/fchollet/keras/blob/master/keras/losses.py#L68\r\n\r\nHere is the code to reproduce the issue:\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ntorch.manual_seed(42)\r\nnp.random.seed(42)\r\n\r\npoisson_mean = 4\r\npoisson_numsample = 200\r\n\r\nsamples = np.random.poisson(poisson_mean, poisson_numsample)\r\n\r\nparam = Variable(torch.zeros(1), requires_grad=True)\r\ntorch_samples = Variable(torch.from_numpy(samples).float(), requires_grad=False)\r\n\r\noptimizer = torch.optim.RMSprop([param], lr=0.1)\r\nloss = torch.nn.PoissonNLLLoss(log_input=False, full=True)\r\n\r\nfor i in range(5000):\r\n    optimizer.zero_grad()\r\n    output = loss(param, torch_samples)\r\n    output.backward()\r\n\r\n    if i % 1000 == 0:\r\n        print('Loss:', output.data[0])\r\n\r\n    optimizer.step()\r\n\r\nprint('Sample mean: ', torch_samples.mean().data.numpy())\r\nprint('Mean is : ', param.data.numpy())\r\n\r\n```"}