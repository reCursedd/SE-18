{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12207", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12207/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12207/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12207/events", "html_url": "https://github.com/pytorch/pytorch/issues/12207", "id": 365246464, "node_id": "MDU6SXNzdWUzNjUyNDY0NjQ=", "number": 12207, "title": "Non Deterministic Behaviour even after cudnn.deterministic = True and cudnn.benchmark=False ", "user": {"login": "Naman-ntc", "id": 23135406, "node_id": "MDQ6VXNlcjIzMTM1NDA2", "avatar_url": "https://avatars2.githubusercontent.com/u/23135406?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Naman-ntc", "html_url": "https://github.com/Naman-ntc", "followers_url": "https://api.github.com/users/Naman-ntc/followers", "following_url": "https://api.github.com/users/Naman-ntc/following{/other_user}", "gists_url": "https://api.github.com/users/Naman-ntc/gists{/gist_id}", "starred_url": "https://api.github.com/users/Naman-ntc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Naman-ntc/subscriptions", "organizations_url": "https://api.github.com/users/Naman-ntc/orgs", "repos_url": "https://api.github.com/users/Naman-ntc/repos", "events_url": "https://api.github.com/users/Naman-ntc/events{/privacy}", "received_events_url": "https://api.github.com/users/Naman-ntc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2018-09-30T18:03:50Z", "updated_at": "2018-11-20T11:55:06Z", "closed_at": "2018-10-17T11:31:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I was performing adversarial attacks with pytorch and I was not able to reproduce my experiments.<br>\nFinally I learned about non deterministic cudnn operations <a href=\"https://discuss.pytorch.org/t/pytorch-doubletensor-operations-are-slow-while-floattensors-autograd-precision-is-low/26187\" rel=\"nofollow\">Discussion Forum-1</a> <a href=\"https://discuss.pytorch.org/t/inconsistent-gradient-values-for-the-same-input/26179\" rel=\"nofollow\">Discussion Forum-2</a> and <a href=\"https://discuss.pytorch.org/t/nondeterminism-even-when-setting-all-seeds-0-workers-and-cudnn-deterministic/26080/4\" rel=\"nofollow\">final post</a><br>\nYou can simply try to reproduce output from the following script (Ubuntu OS, Nvidia 1080 Ti GPU).</p>\n<p>If i change my model to a simple resnet (and appropirately the target and loss function) then it results are deterministic. (So I guess maybe Upsample is the culprit, I don't know!!).<br>\nHere is my complete script :</p>\n<pre><code> import torch\n import torch.nn as nn\n torch.backends.cudnn.deterministic = True\n torch.backends.cudnn.benchmark = False\n  \n class simple(nn.Module):\n         def __init__(self):\n                 super(simple, self).__init__()\n                 self.seq = nn.Sequential(\n                                 nn.Conv2d(3,64,7,2,3),\n                                 nn.Conv2d(64,64,3,1,1),\n                                 nn.MaxPool2d(2,2),\n                                 nn.Conv2d(64,64,3,1,1),\n                                 nn.MaxPool2d(2,2),\n                                 nn.Upsample(scale_factor = 2),\n                                 nn.Conv2d(64,64,3,1,1),\n                                 nn.Upsample(scale_factor = 2),\n                                 nn.Conv2d(64,1,1)\n                         )\n\n         def forward(self, x):\n                 return self.seq(x)\n \n gpuid = 0\n \n ###\n model = simple().to(gpuid)\n lossfn = nn.MSELoss()\n img = torch.randn(1,3,256,256).float().to(gpuid)\n ###\n \n out = model(img)\n target = torch.randn(1,1,128,128).to(gpuid)\n print(out.norm())\n \n def test():\n         adv = torch.zeros_like(img).to(gpuid)\n \n         for i in range(100):\n                 inpcopy = torch.clamp(img + adv, 0, 1)\n                 inpcopy.requires_grad = True\n                 loss = lossfn(model(inpcopy), target)\n                 loss.backward()\n                 adv = torch.clamp(torch.sign(inpcopy.grad) + adv, 0, 1)\n\n         inpcopy = torch.clamp(img + adv, 0, 1)\n         out = model(inpcopy)\n         outNorm = float(out.norm())\n         print(outNorm)\n \n\n while True:\n     test() # Since there is no randomness the output perturbed image norm must be consistent\n</code></pre>\n<p>Can someone please look into it. And if it is problem of Upsample is there some hacky solution out of it??</p>", "body_text": "I was performing adversarial attacks with pytorch and I was not able to reproduce my experiments.\nFinally I learned about non deterministic cudnn operations Discussion Forum-1 Discussion Forum-2 and final post\nYou can simply try to reproduce output from the following script (Ubuntu OS, Nvidia 1080 Ti GPU).\nIf i change my model to a simple resnet (and appropirately the target and loss function) then it results are deterministic. (So I guess maybe Upsample is the culprit, I don't know!!).\nHere is my complete script :\n import torch\n import torch.nn as nn\n torch.backends.cudnn.deterministic = True\n torch.backends.cudnn.benchmark = False\n  \n class simple(nn.Module):\n         def __init__(self):\n                 super(simple, self).__init__()\n                 self.seq = nn.Sequential(\n                                 nn.Conv2d(3,64,7,2,3),\n                                 nn.Conv2d(64,64,3,1,1),\n                                 nn.MaxPool2d(2,2),\n                                 nn.Conv2d(64,64,3,1,1),\n                                 nn.MaxPool2d(2,2),\n                                 nn.Upsample(scale_factor = 2),\n                                 nn.Conv2d(64,64,3,1,1),\n                                 nn.Upsample(scale_factor = 2),\n                                 nn.Conv2d(64,1,1)\n                         )\n\n         def forward(self, x):\n                 return self.seq(x)\n \n gpuid = 0\n \n ###\n model = simple().to(gpuid)\n lossfn = nn.MSELoss()\n img = torch.randn(1,3,256,256).float().to(gpuid)\n ###\n \n out = model(img)\n target = torch.randn(1,1,128,128).to(gpuid)\n print(out.norm())\n \n def test():\n         adv = torch.zeros_like(img).to(gpuid)\n \n         for i in range(100):\n                 inpcopy = torch.clamp(img + adv, 0, 1)\n                 inpcopy.requires_grad = True\n                 loss = lossfn(model(inpcopy), target)\n                 loss.backward()\n                 adv = torch.clamp(torch.sign(inpcopy.grad) + adv, 0, 1)\n\n         inpcopy = torch.clamp(img + adv, 0, 1)\n         out = model(inpcopy)\n         outNorm = float(out.norm())\n         print(outNorm)\n \n\n while True:\n     test() # Since there is no randomness the output perturbed image norm must be consistent\n\nCan someone please look into it. And if it is problem of Upsample is there some hacky solution out of it??", "body": "I was performing adversarial attacks with pytorch and I was not able to reproduce my experiments.\r\nFinally I learned about non deterministic cudnn operations [Discussion Forum-1](https://discuss.pytorch.org/t/pytorch-doubletensor-operations-are-slow-while-floattensors-autograd-precision-is-low/26187) [Discussion Forum-2](https://discuss.pytorch.org/t/inconsistent-gradient-values-for-the-same-input/26179) and [final post](https://discuss.pytorch.org/t/nondeterminism-even-when-setting-all-seeds-0-workers-and-cudnn-deterministic/26080/4)\r\nYou can simply try to reproduce output from the following script (Ubuntu OS, Nvidia 1080 Ti GPU). \r\n\r\nIf i change my model to a simple resnet (and appropirately the target and loss function) then it results are deterministic. (So I guess maybe Upsample is the culprit, I don't know!!). \r\nHere is my complete script : \r\n```                                                                                                                                                                                       \r\n import torch\r\n import torch.nn as nn\r\n torch.backends.cudnn.deterministic = True\r\n torch.backends.cudnn.benchmark = False\r\n  \r\n class simple(nn.Module):\r\n         def __init__(self):\r\n                 super(simple, self).__init__()\r\n                 self.seq = nn.Sequential(\r\n                                 nn.Conv2d(3,64,7,2,3),\r\n                                 nn.Conv2d(64,64,3,1,1),\r\n                                 nn.MaxPool2d(2,2),\r\n                                 nn.Conv2d(64,64,3,1,1),\r\n                                 nn.MaxPool2d(2,2),\r\n                                 nn.Upsample(scale_factor = 2),\r\n                                 nn.Conv2d(64,64,3,1,1),\r\n                                 nn.Upsample(scale_factor = 2),\r\n                                 nn.Conv2d(64,1,1)\r\n                         )\r\n\r\n         def forward(self, x):\r\n                 return self.seq(x)\r\n \r\n gpuid = 0\r\n \r\n ###\r\n model = simple().to(gpuid)\r\n lossfn = nn.MSELoss()\r\n img = torch.randn(1,3,256,256).float().to(gpuid)\r\n ###\r\n \r\n out = model(img)\r\n target = torch.randn(1,1,128,128).to(gpuid)\r\n print(out.norm())\r\n \r\n def test():\r\n         adv = torch.zeros_like(img).to(gpuid)\r\n \r\n         for i in range(100):\r\n                 inpcopy = torch.clamp(img + adv, 0, 1)\r\n                 inpcopy.requires_grad = True\r\n                 loss = lossfn(model(inpcopy), target)\r\n                 loss.backward()\r\n                 adv = torch.clamp(torch.sign(inpcopy.grad) + adv, 0, 1)\r\n\r\n         inpcopy = torch.clamp(img + adv, 0, 1)\r\n         out = model(inpcopy)\r\n         outNorm = float(out.norm())\r\n         print(outNorm)\r\n \r\n\r\n while True:\r\n     test() # Since there is no randomness the output perturbed image norm must be consistent\r\n```\r\n\r\nCan someone please look into it. And if it is problem of Upsample is there some hacky solution out of it??  "}