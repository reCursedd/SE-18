{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/425746601", "html_url": "https://github.com/pytorch/pytorch/issues/12207#issuecomment-425746601", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12207", "id": 425746601, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTc0NjYwMQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-30T19:49:47Z", "updated_at": "2018-09-30T19:49:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I don't think there is much to do, and FGSA is intentionally discontinuous (due to the sign). What you could do is determine the digit where it is unstable empirically and round that away before taking the sign.</p>\n<p>That said, the reason is likely that the backward uses the cuda function <code>atomicAdd</code>, which is non-deterministic. For a factor of two, it would seem that</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">MyUpsample2</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-k\">return</span> x[:, :, :, <span class=\"pl-c1\">None</span>, :, <span class=\"pl-c1\">None</span>].expand(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>).reshape(x.size(<span class=\"pl-c1\">0</span>), x.size(<span class=\"pl-c1\">1</span>), x.size(<span class=\"pl-c1\">2</span>)<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>, x.size(<span class=\"pl-c1\">3</span>)<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>)</pre></div>\n<p>makes this deterministic.</p>\n<p>I'll probably add a note to <a href=\"https://github.com/pytorch/pytorch/blob/master/docs/source/notes/randomness.rst\">https://github.com/pytorch/pytorch/blob/master/docs/source/notes/randomness.rst</a> about atomic add (Users include: index_add, scatter_add, bincount, lossctc in forward, embedding/embedding_bag, all sorts of pooling, padding, sampling in backward, possibly also sparse to dense or coalescing, but I didn't check).</p>", "body_text": "I don't think there is much to do, and FGSA is intentionally discontinuous (due to the sign). What you could do is determine the digit where it is unstable empirically and round that away before taking the sign.\nThat said, the reason is likely that the backward uses the cuda function atomicAdd, which is non-deterministic. For a factor of two, it would seem that\nclass MyUpsample2(nn.Module):\n    def forward(self, x):\n        return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)\nmakes this deterministic.\nI'll probably add a note to https://github.com/pytorch/pytorch/blob/master/docs/source/notes/randomness.rst about atomic add (Users include: index_add, scatter_add, bincount, lossctc in forward, embedding/embedding_bag, all sorts of pooling, padding, sampling in backward, possibly also sparse to dense or coalescing, but I didn't check).", "body": "I don't think there is much to do, and FGSA is intentionally discontinuous (due to the sign). What you could do is determine the digit where it is unstable empirically and round that away before taking the sign.\r\n\r\nThat said, the reason is likely that the backward uses the cuda function `atomicAdd`, which is non-deterministic. For a factor of two, it would seem that\r\n```python\r\nclass MyUpsample2(nn.Module):\r\n    def forward(self, x):\r\n        return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)\r\n```\r\nmakes this deterministic.\r\n\r\nI'll probably add a note to https://github.com/pytorch/pytorch/blob/master/docs/source/notes/randomness.rst about atomic add (Users include: index_add, scatter_add, bincount, lossctc in forward, embedding/embedding_bag, all sorts of pooling, padding, sampling in backward, possibly also sparse to dense or coalescing, but I didn't check).\r\n"}