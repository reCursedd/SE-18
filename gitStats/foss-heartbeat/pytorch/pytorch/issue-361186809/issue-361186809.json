{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11793", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11793/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11793/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11793/events", "html_url": "https://github.com/pytorch/pytorch/issues/11793", "id": 361186809, "node_id": "MDU6SXNzdWUzNjExODY4MDk=", "number": 11793, "title": "DataParallel: Parallel_apply assert len(modules) == len(inputs) AssertionError", "user": {"login": "richielo", "id": 13329406, "node_id": "MDQ6VXNlcjEzMzI5NDA2", "avatar_url": "https://avatars3.githubusercontent.com/u/13329406?v=4", "gravatar_id": "", "url": "https://api.github.com/users/richielo", "html_url": "https://github.com/richielo", "followers_url": "https://api.github.com/users/richielo/followers", "following_url": "https://api.github.com/users/richielo/following{/other_user}", "gists_url": "https://api.github.com/users/richielo/gists{/gist_id}", "starred_url": "https://api.github.com/users/richielo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/richielo/subscriptions", "organizations_url": "https://api.github.com/users/richielo/orgs", "repos_url": "https://api.github.com/users/richielo/repos", "events_url": "https://api.github.com/users/richielo/events{/privacy}", "received_events_url": "https://api.github.com/users/richielo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-18T08:16:58Z", "updated_at": "2018-11-02T03:47:05Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>I am using Pytorch version 0.4.1 with Python 3.6. I am adapting the transformer model for translation from this site (<a href=\"http://nlp.seas.harvard.edu/2018/04/03/attention.html\" rel=\"nofollow\">http://nlp.seas.harvard.edu/2018/04/03/attention.html</a>). The model runs without error with single gpu. However, the AssertionError happens when I use DataParallel for the model. How should I deal with this issue? Thanks for your help in advance. The following are snippet of the code:</p>\n<pre><code>def run_epoch(data_iter, model, loss_compute):\n    \"Standard Training and Logging Function\"\n    start = time.time()\n    total_tokens = 0\n    total_loss = 0\n    tokens = 0\n    for i, batch in enumerate(data_iter):\n        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n        total_loss += loss\n        total_tokens += batch.ntokens.float()\n        tokens += batch.ntokens.float()\n        if i % 50 == 1:\n            elapsed = time.time() - start\n            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" % (i, loss / batch.ntokens.float().item(), tokens / elapsed))\n            start = time.time()\n            tokens = 0\n    return total_loss / total_tokens.float()\n</code></pre>\n<p>loss_compute function</p>\n<pre><code>class MultiGPULossCompute:\n    \"A multi-gpu loss compute and train function.\"\n    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n        # Send out to different gpus.\n        self.generator = generator\n        self.criterion = nn.parallel.replicate(criterion, \n                                               devices=devices)\n        self.opt = opt\n        self.devices = devices\n        self.chunk_size = chunk_size\n        \n    def __call__(self, out, targets, normalize):\n        total = 0.0\n        generator = nn.parallel.replicate(self.generator, \n                                                devices=self.devices)\n        out_scatter = nn.parallel.scatter(out, \n                                          target_gpus=self.devices)\n        out_grad = [[] for _ in out_scatter]\n        targets = nn.parallel.scatter(targets, \n                                      target_gpus=self.devices)\n\n        # Divide generating into chunks.\n        chunk_size = self.chunk_size\n        for i in range(0, out_scatter[0].size(1), chunk_size):\n            # Predict distributions\n            out_column = [[Variable(o[:, i:i+chunk_size].data, \n                                    requires_grad=self.opt is not None)] \n                           for o in out_scatter]\n            gen = nn.parallel.parallel_apply(generator, out_column)\n\n            # Compute loss. \n            y = [(g.contiguous().view(-1, g.size(-1)), \n                  t[:, i:i+chunk_size].contiguous().view(-1)) \n                 for g, t in zip(gen, targets)]\n            loss = nn.parallel.parallel_apply(self.criterion, y)\n\n            # Sum and normalize loss\n            l = nn.parallel.gather(loss, \n                                   target_device=self.devices[0])\n            l = l.sum()[0] / normalize.float()\n            total += l.data[0]\n\n            # Backprop loss to output of transformer\n            if self.opt is not None:\n                l.backward()\n                for j, l in enumerate(loss):\n                    out_grad[j].append(out_column[j][0].grad.data.clone())\n\n        # Backprop all loss through transformer.            \n        if self.opt is not None:\n            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n            o1 = out\n            o2 = nn.parallel.gather(out_grad, \n                                    target_device=self.devices[0])\n            o1.backward(gradient=o2)\n            self.opt.step()\n            self.opt.optimizer.zero_grad()\n        return total * normalize.float()\n\n</code></pre>", "body_text": "Hello,\nI am using Pytorch version 0.4.1 with Python 3.6. I am adapting the transformer model for translation from this site (http://nlp.seas.harvard.edu/2018/04/03/attention.html). The model runs without error with single gpu. However, the AssertionError happens when I use DataParallel for the model. How should I deal with this issue? Thanks for your help in advance. The following are snippet of the code:\ndef run_epoch(data_iter, model, loss_compute):\n    \"Standard Training and Logging Function\"\n    start = time.time()\n    total_tokens = 0\n    total_loss = 0\n    tokens = 0\n    for i, batch in enumerate(data_iter):\n        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n        total_loss += loss\n        total_tokens += batch.ntokens.float()\n        tokens += batch.ntokens.float()\n        if i % 50 == 1:\n            elapsed = time.time() - start\n            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" % (i, loss / batch.ntokens.float().item(), tokens / elapsed))\n            start = time.time()\n            tokens = 0\n    return total_loss / total_tokens.float()\n\nloss_compute function\nclass MultiGPULossCompute:\n    \"A multi-gpu loss compute and train function.\"\n    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n        # Send out to different gpus.\n        self.generator = generator\n        self.criterion = nn.parallel.replicate(criterion, \n                                               devices=devices)\n        self.opt = opt\n        self.devices = devices\n        self.chunk_size = chunk_size\n        \n    def __call__(self, out, targets, normalize):\n        total = 0.0\n        generator = nn.parallel.replicate(self.generator, \n                                                devices=self.devices)\n        out_scatter = nn.parallel.scatter(out, \n                                          target_gpus=self.devices)\n        out_grad = [[] for _ in out_scatter]\n        targets = nn.parallel.scatter(targets, \n                                      target_gpus=self.devices)\n\n        # Divide generating into chunks.\n        chunk_size = self.chunk_size\n        for i in range(0, out_scatter[0].size(1), chunk_size):\n            # Predict distributions\n            out_column = [[Variable(o[:, i:i+chunk_size].data, \n                                    requires_grad=self.opt is not None)] \n                           for o in out_scatter]\n            gen = nn.parallel.parallel_apply(generator, out_column)\n\n            # Compute loss. \n            y = [(g.contiguous().view(-1, g.size(-1)), \n                  t[:, i:i+chunk_size].contiguous().view(-1)) \n                 for g, t in zip(gen, targets)]\n            loss = nn.parallel.parallel_apply(self.criterion, y)\n\n            # Sum and normalize loss\n            l = nn.parallel.gather(loss, \n                                   target_device=self.devices[0])\n            l = l.sum()[0] / normalize.float()\n            total += l.data[0]\n\n            # Backprop loss to output of transformer\n            if self.opt is not None:\n                l.backward()\n                for j, l in enumerate(loss):\n                    out_grad[j].append(out_column[j][0].grad.data.clone())\n\n        # Backprop all loss through transformer.            \n        if self.opt is not None:\n            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n            o1 = out\n            o2 = nn.parallel.gather(out_grad, \n                                    target_device=self.devices[0])\n            o1.backward(gradient=o2)\n            self.opt.step()\n            self.opt.optimizer.zero_grad()\n        return total * normalize.float()", "body": "Hello,\r\n\r\nI am using Pytorch version 0.4.1 with Python 3.6. I am adapting the transformer model for translation from this site (http://nlp.seas.harvard.edu/2018/04/03/attention.html). The model runs without error with single gpu. However, the AssertionError happens when I use DataParallel for the model. How should I deal with this issue? Thanks for your help in advance. The following are snippet of the code:\r\n\r\n```\r\ndef run_epoch(data_iter, model, loss_compute):\r\n    \"Standard Training and Logging Function\"\r\n    start = time.time()\r\n    total_tokens = 0\r\n    total_loss = 0\r\n    tokens = 0\r\n    for i, batch in enumerate(data_iter):\r\n        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\r\n        loss = loss_compute(out, batch.trg_y, batch.ntokens)\r\n        total_loss += loss\r\n        total_tokens += batch.ntokens.float()\r\n        tokens += batch.ntokens.float()\r\n        if i % 50 == 1:\r\n            elapsed = time.time() - start\r\n            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" % (i, loss / batch.ntokens.float().item(), tokens / elapsed))\r\n            start = time.time()\r\n            tokens = 0\r\n    return total_loss / total_tokens.float()\r\n```\r\n\r\nloss_compute function\r\n```\r\nclass MultiGPULossCompute:\r\n    \"A multi-gpu loss compute and train function.\"\r\n    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\r\n        # Send out to different gpus.\r\n        self.generator = generator\r\n        self.criterion = nn.parallel.replicate(criterion, \r\n                                               devices=devices)\r\n        self.opt = opt\r\n        self.devices = devices\r\n        self.chunk_size = chunk_size\r\n        \r\n    def __call__(self, out, targets, normalize):\r\n        total = 0.0\r\n        generator = nn.parallel.replicate(self.generator, \r\n                                                devices=self.devices)\r\n        out_scatter = nn.parallel.scatter(out, \r\n                                          target_gpus=self.devices)\r\n        out_grad = [[] for _ in out_scatter]\r\n        targets = nn.parallel.scatter(targets, \r\n                                      target_gpus=self.devices)\r\n\r\n        # Divide generating into chunks.\r\n        chunk_size = self.chunk_size\r\n        for i in range(0, out_scatter[0].size(1), chunk_size):\r\n            # Predict distributions\r\n            out_column = [[Variable(o[:, i:i+chunk_size].data, \r\n                                    requires_grad=self.opt is not None)] \r\n                           for o in out_scatter]\r\n            gen = nn.parallel.parallel_apply(generator, out_column)\r\n\r\n            # Compute loss. \r\n            y = [(g.contiguous().view(-1, g.size(-1)), \r\n                  t[:, i:i+chunk_size].contiguous().view(-1)) \r\n                 for g, t in zip(gen, targets)]\r\n            loss = nn.parallel.parallel_apply(self.criterion, y)\r\n\r\n            # Sum and normalize loss\r\n            l = nn.parallel.gather(loss, \r\n                                   target_device=self.devices[0])\r\n            l = l.sum()[0] / normalize.float()\r\n            total += l.data[0]\r\n\r\n            # Backprop loss to output of transformer\r\n            if self.opt is not None:\r\n                l.backward()\r\n                for j, l in enumerate(loss):\r\n                    out_grad[j].append(out_column[j][0].grad.data.clone())\r\n\r\n        # Backprop all loss through transformer.            \r\n        if self.opt is not None:\r\n            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\r\n            o1 = out\r\n            o2 = nn.parallel.gather(out_grad, \r\n                                    target_device=self.devices[0])\r\n            o1.backward(gradient=o2)\r\n            self.opt.step()\r\n            self.opt.optimizer.zero_grad()\r\n        return total * normalize.float()\r\n\r\n```"}