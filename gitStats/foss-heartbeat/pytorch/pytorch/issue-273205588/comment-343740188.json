{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/343740188", "html_url": "https://github.com/pytorch/pytorch/issues/3651#issuecomment-343740188", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3651", "id": 343740188, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Mzc0MDE4OA==", "user": {"login": "michaelhuang74", "id": 23154573, "node_id": "MDQ6VXNlcjIzMTU0NTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/23154573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelhuang74", "html_url": "https://github.com/michaelhuang74", "followers_url": "https://api.github.com/users/michaelhuang74/followers", "following_url": "https://api.github.com/users/michaelhuang74/following{/other_user}", "gists_url": "https://api.github.com/users/michaelhuang74/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelhuang74/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelhuang74/subscriptions", "organizations_url": "https://api.github.com/users/michaelhuang74/orgs", "repos_url": "https://api.github.com/users/michaelhuang74/repos", "events_url": "https://api.github.com/users/michaelhuang74/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelhuang74/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-12T14:14:39Z", "updated_at": "2017-11-12T14:14:39Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> The code is at <a href=\"https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90\">https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90</a></p>\n<p>In the code of GramMatrix, I tried to cast FP16 to FP32 at line 82. The modified code is as follows.</p>\n<p>class GramMatrix(nn.Module):<br>\ndef forward(self, input):<br>\ninput_l = input.clone().cuda()<br>\nb,c,h,w = input_l.size()<br>\nF = input_l.view(b, c, h<em>w)<br>\nG = torch.bmm(F, F.transpose(1,2)).cuda()<br>\nG.div_(h</em>w)<br>\nreturn G</p>\n<p>class GramMSELoss(nn.Module):<br>\ndef forward(self, input, target):<br>\nout = nn.MSELoss()(GramMatrix()(input), target)<br>\nreturn(out)</p>", "body_text": "@apaszke The code is at https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90\nIn the code of GramMatrix, I tried to cast FP16 to FP32 at line 82. The modified code is as follows.\nclass GramMatrix(nn.Module):\ndef forward(self, input):\ninput_l = input.clone().cuda()\nb,c,h,w = input_l.size()\nF = input_l.view(b, c, hw)\nG = torch.bmm(F, F.transpose(1,2)).cuda()\nG.div_(hw)\nreturn G\nclass GramMSELoss(nn.Module):\ndef forward(self, input, target):\nout = nn.MSELoss()(GramMatrix()(input), target)\nreturn(out)", "body": "@apaszke The code is at https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90\r\n\r\nIn the code of GramMatrix, I tried to cast FP16 to FP32 at line 82. The modified code is as follows.\r\n\r\nclass GramMatrix(nn.Module):\r\n    def forward(self, input):\r\n        input_l = input.clone().cuda()\r\n        b,c,h,w = input_l.size()\r\n        F = input_l.view(b, c, h*w)\r\n        G = torch.bmm(F, F.transpose(1,2)).cuda() \r\n        G.div_(h*w)\r\n        return G\r\n\r\nclass GramMSELoss(nn.Module):\r\n    def forward(self, input, target):\r\n        out = nn.MSELoss()(GramMatrix()(input), target)\r\n        return(out)"}