{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3651", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3651/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3651/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3651/events", "html_url": "https://github.com/pytorch/pytorch/issues/3651", "id": 273205588, "node_id": "MDU6SXNzdWUyNzMyMDU1ODg=", "number": 3651, "title": "Runtime error when mixing FP32 loss functions and FP16 cnn layers", "user": {"login": "michaelhuang74", "id": 23154573, "node_id": "MDQ6VXNlcjIzMTU0NTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/23154573?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelhuang74", "html_url": "https://github.com/michaelhuang74", "followers_url": "https://api.github.com/users/michaelhuang74/followers", "following_url": "https://api.github.com/users/michaelhuang74/following{/other_user}", "gists_url": "https://api.github.com/users/michaelhuang74/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelhuang74/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelhuang74/subscriptions", "organizations_url": "https://api.github.com/users/michaelhuang74/orgs", "repos_url": "https://api.github.com/users/michaelhuang74/repos", "events_url": "https://api.github.com/users/michaelhuang74/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelhuang74/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-11-12T06:06:53Z", "updated_at": "2018-02-12T22:27:06Z", "closed_at": "2017-11-12T14:18:15Z", "author_association": "NONE", "body_html": "<p>I am trying to speed up the neural style transfer (<a href=\"https://github.com/leongatys/PytorchNeuralStyleTransfer\">https://github.com/leongatys/PytorchNeuralStyleTransfer</a>) on Nvidia Tesla V100 by using FP16.</p>\n<p>I modified the code to move the vgg to cuda().half(). In addition, all three images, style image, content image, and opt_img, are in FP16. I tried to keep the loss functions in FP32 because it easily can generate NaN and infinity in FP16.<br>\nThe code is at <a href=\"https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90\">https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90</a></p>\n<p>When I ran the code, I encountered the following error.<br>\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++<br>\nTraceback (most recent call last):<br>\nFile \"neural-style-Gatys-half.py\", line 167, in<br>\nstyle_targets = [GramMatrix()(A).detach().cuda() for A in vgg(style_image, style_layers)]<br>\nFile \"/home/mqhuang2/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 319, in call<br>\nresult = self.forward(input, **kwargs)<br>\nFile \"neural-style-Gatys-half.py\", line 86, in forward<br>\nG.div_(hw)<br>\nRuntimeError: value cannot be converted to type Half without overflow: 960000<br>\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++</p>\n<p>It seems that although I tried to keep the GramMatrix (see line 82) and loss functions (see line 155) in FP32, somehow, pytorch tried to convert FP32 to FP16 in the GramMatrix forward() method.</p>\n<p>Any idea how to resolve this error?</p>", "body_text": "I am trying to speed up the neural style transfer (https://github.com/leongatys/PytorchNeuralStyleTransfer) on Nvidia Tesla V100 by using FP16.\nI modified the code to move the vgg to cuda().half(). In addition, all three images, style image, content image, and opt_img, are in FP16. I tried to keep the loss functions in FP32 because it easily can generate NaN and infinity in FP16.\nThe code is at https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90\nWhen I ran the code, I encountered the following error.\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++\nTraceback (most recent call last):\nFile \"neural-style-Gatys-half.py\", line 167, in\nstyle_targets = [GramMatrix()(A).detach().cuda() for A in vgg(style_image, style_layers)]\nFile \"/home/mqhuang2/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 319, in call\nresult = self.forward(input, **kwargs)\nFile \"neural-style-Gatys-half.py\", line 86, in forward\nG.div_(hw)\nRuntimeError: value cannot be converted to type Half without overflow: 960000\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++\nIt seems that although I tried to keep the GramMatrix (see line 82) and loss functions (see line 155) in FP32, somehow, pytorch tried to convert FP32 to FP16 in the GramMatrix forward() method.\nAny idea how to resolve this error?", "body": "I am trying to speed up the neural style transfer (https://github.com/leongatys/PytorchNeuralStyleTransfer) on Nvidia Tesla V100 by using FP16.\r\n\r\nI modified the code to move the vgg to cuda().half(). In addition, all three images, style image, content image, and opt_img, are in FP16. I tried to keep the loss functions in FP32 because it easily can generate NaN and infinity in FP16.\r\nThe code is at https://gist.github.com/michaelhuang74/009e149a2002b84696731fb599408c90\r\n\r\nWhen I ran the code, I encountered the following error.\r\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\nTraceback (most recent call last):\r\nFile \"neural-style-Gatys-half.py\", line 167, in \r\nstyle_targets = [GramMatrix()(A).detach().cuda() for A in vgg(style_image, style_layers)]\r\nFile \"/home/mqhuang2/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 319, in call\r\nresult = self.forward(input, **kwargs)\r\nFile \"neural-style-Gatys-half.py\", line 86, in forward\r\nG.div_(hw)\r\nRuntimeError: value cannot be converted to type Half without overflow: 960000\r\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\n\r\nIt seems that although I tried to keep the GramMatrix (see line 82) and loss functions (see line 155) in FP32, somehow, pytorch tried to convert FP32 to FP16 in the GramMatrix forward() method.\r\n\r\nAny idea how to resolve this error?"}