{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/222816671", "pull_request_review_id": 161796902, "id": 222816671, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMjgxNjY3MQ==", "diff_hunk": "@@ -22,12 +21,25 @@ CPU and CUDA)\n     import torch\n     torch.manual_seed(0)\n \n+There are some PyTorch functions that use CUDA functions that can be a source\n+of non-determinism. One class of such CUDA functions are atomic operations,\n+in particular :attr:`atomicAdd`, where the order of parallel additions to the\n+same value is undetermined and, for floating-point variables, a source of\n+variance in the result. PyTorch functions that use :attr:`atomicAdd` in the forward\n+include :meth:`torch.Tensor.index_add_`, :meth:`torch.Tensor.scatter_add_`,\n+:meth:`torch.bincount`.\n+A number of operations have backwards that use :attr:`atomicAdd`, in particular\n+:meth:`torch.nn.functional.embedding`, :meth:`torch.nn.functional.embedding_bag`,", "path": "docs/source/notes/randomness.rst", "position": null, "original_position": 17, "commit_id": "82e781a3b66065ab2cd22983f4b2ad642d269984", "original_commit_id": "9430458281a349e2531b511d1cc2de6cb76d2736", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Embedding doesn't use atomicAdd, although EmbeddingBag does.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Embedding.cu", "created_at": "2018-10-04T20:30:33Z", "updated_at": "2018-11-23T15:52:24Z", "html_url": "https://github.com/pytorch/pytorch/pull/12217#discussion_r222816671", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12217", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/222816671"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12217#discussion_r222816671"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12217"}}, "body_html": "<p>Embedding doesn't use atomicAdd, although EmbeddingBag does.</p>\n<p><a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Embedding.cu\">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Embedding.cu</a></p>", "body_text": "Embedding doesn't use atomicAdd, although EmbeddingBag does.\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Embedding.cu"}