{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/356841753", "html_url": "https://github.com/pytorch/pytorch/issues/973#issuecomment-356841753", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/973", "id": 356841753, "node_id": "MDEyOklzc3VlQ29tbWVudDM1Njg0MTc1Mw==", "user": {"login": "sunhs", "id": 1916614, "node_id": "MDQ6VXNlcjE5MTY2MTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/1916614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sunhs", "html_url": "https://github.com/sunhs", "followers_url": "https://api.github.com/users/sunhs/followers", "following_url": "https://api.github.com/users/sunhs/following{/other_user}", "gists_url": "https://api.github.com/users/sunhs/gists{/gist_id}", "starred_url": "https://api.github.com/users/sunhs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sunhs/subscriptions", "organizations_url": "https://api.github.com/users/sunhs/orgs", "repos_url": "https://api.github.com/users/sunhs/repos", "events_url": "https://api.github.com/users/sunhs/events{/privacy}", "received_events_url": "https://api.github.com/users/sunhs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-11T06:45:22Z", "updated_at": "2018-01-11T06:46:17Z", "author_association": "NONE", "body_html": "<p>I'm faced with this problem too. In my code I did:</p>\n<pre><code>from multiprocessing import pool\np = pool.Pool()\np.map(preprocess, images)\n</code></pre>\n<p>where <code>images</code> are PIL Image objects and <code>preprocess</code> is a function to transform the PIL Image into a FloatTensor. This yields the same exception as above. When I change the ulimit or replace the <code>pool.map</code> with a normal <code>for loop</code> everything is good.</p>\n<p>I'm just curious about why the combination of <code>p.map</code> and <code>torch.FloatTensor</code> would have something to do with the file descriptor, while I'm not explicitly doing anything about shared memory.</p>", "body_text": "I'm faced with this problem too. In my code I did:\nfrom multiprocessing import pool\np = pool.Pool()\np.map(preprocess, images)\n\nwhere images are PIL Image objects and preprocess is a function to transform the PIL Image into a FloatTensor. This yields the same exception as above. When I change the ulimit or replace the pool.map with a normal for loop everything is good.\nI'm just curious about why the combination of p.map and torch.FloatTensor would have something to do with the file descriptor, while I'm not explicitly doing anything about shared memory.", "body": "I'm faced with this problem too. In my code I did:\r\n\r\n```\r\nfrom multiprocessing import pool\r\np = pool.Pool()\r\np.map(preprocess, images)\r\n```\r\n\r\nwhere `images` are PIL Image objects and `preprocess` is a function to transform the PIL Image into a FloatTensor. This yields the same exception as above. When I change the ulimit or replace the `pool.map` with a normal `for loop` everything is good.\r\n\r\nI'm just curious about why the combination of `p.map` and `torch.FloatTensor` would have something to do with the file descriptor, while I'm not explicitly doing anything about shared memory."}