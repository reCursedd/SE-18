{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7806", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7806/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7806/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7806/events", "html_url": "https://github.com/pytorch/pytorch/issues/7806", "id": 326015888, "node_id": "MDU6SXNzdWUzMjYwMTU4ODg=", "number": 7806, "title": "OOM Exception when using torch.nn.grad.conv2d_weight", "user": {"login": "xxtemp", "id": 7406398, "node_id": "MDQ6VXNlcjc0MDYzOTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/7406398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xxtemp", "html_url": "https://github.com/xxtemp", "followers_url": "https://api.github.com/users/xxtemp/followers", "following_url": "https://api.github.com/users/xxtemp/following{/other_user}", "gists_url": "https://api.github.com/users/xxtemp/gists{/gist_id}", "starred_url": "https://api.github.com/users/xxtemp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xxtemp/subscriptions", "organizations_url": "https://api.github.com/users/xxtemp/orgs", "repos_url": "https://api.github.com/users/xxtemp/repos", "events_url": "https://api.github.com/users/xxtemp/events{/privacy}", "received_events_url": "https://api.github.com/users/xxtemp/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-24T08:29:20Z", "updated_at": "2018-05-30T12:45:58Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I encountered an out of memory exception when using torch.nn.grad.conv2d_weight even though using torch.nn.functional.conv2d works (forward and backward, and I assume the same calculations done in torch.nn.grad.conv2d_weight are done at some point during conv2d()'s backward pass).</p>\n<h2>Code example</h2>\n<p>I'm using the following custom convolutional layer:</p>\n<pre><code>class MyConv(Function):\n    @staticmethod\n    def forward(ctx, x, w):\n        ctx.save_for_backward(x, w)\n        return F.conv2d(x, w)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, w = ctx.saved_variables\n        x_grad = w_grad = None\n        if ctx.needs_input_grad[0]:\n            x_grad = torch.nn.grad.conv2d_input(x.shape, w, grad_output)\n        if ctx.needs_input_grad[1]:\n            w_grad = torch.nn.grad.conv2d_weight(x, w.shape, grad_output)\n        return x_grad, w_grad\n</code></pre>\n<p>Using torch.nn.functional.conv2d works, Using MyConv does not. It fails during torch.nn.grad.conv2d_weight and gives an out of memory exception. As I said before, I don't understand how that can happen since, I assume, at some point during conv2d back-prop, it performs the same calculations that are done in torch.nn.grad.conv2d_weight.<br>\nI can give a more detailed code example if needed.</p>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: Pytorch</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: 0.4</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 9.0/7.0.5</li>\n<li>GPU models and configuration: Tesla V100-SXM2-16GB</li>\n<li>GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609</li>\n<li>CMake version: 3.5.1</li>\n<li>Versions of any other relevant libraries:<br>\n[pip3] numpy (1.14.2)<br>\n[conda] cuda90                    1.0                  h6433d27_0    pytorch<br>\n[conda] pytorch                   0.4.0           py36_cuda9.0.176_cudnn7.1.2_1  [cuda90]  pytorch<br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</li>\n</ul>", "body_text": "Issue description\nI encountered an out of memory exception when using torch.nn.grad.conv2d_weight even though using torch.nn.functional.conv2d works (forward and backward, and I assume the same calculations done in torch.nn.grad.conv2d_weight are done at some point during conv2d()'s backward pass).\nCode example\nI'm using the following custom convolutional layer:\nclass MyConv(Function):\n    @staticmethod\n    def forward(ctx, x, w):\n        ctx.save_for_backward(x, w)\n        return F.conv2d(x, w)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x, w = ctx.saved_variables\n        x_grad = w_grad = None\n        if ctx.needs_input_grad[0]:\n            x_grad = torch.nn.grad.conv2d_input(x.shape, w, grad_output)\n        if ctx.needs_input_grad[1]:\n            w_grad = torch.nn.grad.conv2d_weight(x, w.shape, grad_output)\n        return x_grad, w_grad\n\nUsing torch.nn.functional.conv2d works, Using MyConv does not. It fails during torch.nn.grad.conv2d_weight and gives an out of memory exception. As I said before, I don't understand how that can happen since, I assume, at some point during conv2d back-prop, it performs the same calculations that are done in torch.nn.grad.conv2d_weight.\nI can give a more detailed code example if needed.\nSystem Info\n\nPyTorch or Caffe2: Pytorch\nHow you installed PyTorch (conda, pip, source): conda\nBuild command you used (if compiling from source):\nOS: Ubuntu 16.04\nPyTorch version: 0.4\nPython version: 3.6\nCUDA/cuDNN version: 9.0/7.0.5\nGPU models and configuration: Tesla V100-SXM2-16GB\nGCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: 3.5.1\nVersions of any other relevant libraries:\n[pip3] numpy (1.14.2)\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\n[conda] pytorch                   0.4.0           py36_cuda9.0.176_cudnn7.1.2_1  [cuda90]  pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "## Issue description\r\n\r\nI encountered an out of memory exception when using torch.nn.grad.conv2d_weight even though using torch.nn.functional.conv2d works (forward and backward, and I assume the same calculations done in torch.nn.grad.conv2d_weight are done at some point during conv2d()'s backward pass).\r\n\r\n## Code example\r\n\r\nI'm using the following custom convolutional layer:\r\n```\r\nclass MyConv(Function):\r\n    @staticmethod\r\n    def forward(ctx, x, w):\r\n        ctx.save_for_backward(x, w)\r\n        return F.conv2d(x, w)\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        x, w = ctx.saved_variables\r\n        x_grad = w_grad = None\r\n        if ctx.needs_input_grad[0]:\r\n            x_grad = torch.nn.grad.conv2d_input(x.shape, w, grad_output)\r\n        if ctx.needs_input_grad[1]:\r\n            w_grad = torch.nn.grad.conv2d_weight(x, w.shape, grad_output)\r\n        return x_grad, w_grad\r\n```\r\nUsing torch.nn.functional.conv2d works, Using MyConv does not. It fails during torch.nn.grad.conv2d_weight and gives an out of memory exception. As I said before, I don't understand how that can happen since, I assume, at some point during conv2d back-prop, it performs the same calculations that are done in torch.nn.grad.conv2d_weight.\r\nI can give a more detailed code example if needed.\r\n\r\n## System Info\r\n- PyTorch or Caffe2: Pytorch\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Build command you used (if compiling from source):\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.4\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.0/7.0.5\r\n- GPU models and configuration: Tesla V100-SXM2-16GB\r\n- GCC version (if compiling from source): (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- CMake version: 3.5.1\r\n- Versions of any other relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[conda] cuda90                    1.0                  h6433d27_0    pytorch\r\n[conda] pytorch                   0.4.0           py36_cuda9.0.176_cudnn7.1.2_1  [cuda90]  pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch"}