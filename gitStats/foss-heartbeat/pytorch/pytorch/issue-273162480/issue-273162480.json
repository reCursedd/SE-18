{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3644", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3644/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3644/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3644/events", "html_url": "https://github.com/pytorch/pytorch/issues/3644", "id": 273162480, "node_id": "MDU6SXNzdWUyNzMxNjI0ODA=", "number": 3644, "title": "gradients of index_select wrong on GPU", "user": {"login": "mys007", "id": 5921083, "node_id": "MDQ6VXNlcjU5MjEwODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5921083?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mys007", "html_url": "https://github.com/mys007", "followers_url": "https://api.github.com/users/mys007/followers", "following_url": "https://api.github.com/users/mys007/following{/other_user}", "gists_url": "https://api.github.com/users/mys007/gists{/gist_id}", "starred_url": "https://api.github.com/users/mys007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mys007/subscriptions", "organizations_url": "https://api.github.com/users/mys007/orgs", "repos_url": "https://api.github.com/users/mys007/repos", "events_url": "https://api.github.com/users/mys007/events{/privacy}", "received_events_url": "https://api.github.com/users/mys007/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-11-11T17:44:10Z", "updated_at": "2018-05-01T01:14:45Z", "closed_at": "2017-11-13T14:39:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I've found out that gradient of <code>index_select</code> (i..e <code>index_add</code>) behaves non-deterministically - and wrong - on GPU for some rather random conditions. The following code illustrates the problem:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">set_seed</span>(<span class=\"pl-smi\">seed</span>):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    set_seed(<span class=\"pl-c1\">1</span>)\n    cuda <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n    n, e, c <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">5</span>\n    <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.randn(n,c)\n    idxn <span class=\"pl-k\">=</span> torch.from_numpy(np.random.randint(n,<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>e))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> indices are repeated</span>\n\n    <span class=\"pl-k\">if</span> cuda:\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.cuda(); idxn <span class=\"pl-k\">=</span> idxn.cuda()\n\n    gradsI, gradsS <span class=\"pl-k\">=</span> [], []\n    N <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n        inputv <span class=\"pl-k\">=</span> Variable(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        sel_input <span class=\"pl-k\">=</span> torch.index_select(inputv, <span class=\"pl-c1\">0</span>, Variable(idxn))\n        sel_input.retain_grad()\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> the following computation is one of the \"random conditions\"</span>\n        data <span class=\"pl-k\">=</span> [torch.sum(sel_input.narrow(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>,e<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>), <span class=\"pl-c1\">0</span>),\n                torch.sum(sel_input.narrow(<span class=\"pl-c1\">0</span>,e<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>,e<span class=\"pl-k\">//</span><span class=\"pl-c1\">2</span>), <span class=\"pl-c1\">0</span>) ]\n        out <span class=\"pl-k\">=</span> torch.cat(data,<span class=\"pl-c1\">0</span>)\n        out.exp().sum().backward()\n\n        gradsI.append(inputv.grad.data.cpu().clone())\n        gradsS.append(sel_input.grad.data.cpu().clone())\n\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n        <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n            <span class=\"pl-c1\">print</span>(i,j,(gradsI[i]<span class=\"pl-k\">-</span>gradsI[j]).abs().max(), (gradsS[i]<span class=\"pl-k\">-</span>gradsS[j]).abs().max())</pre></div>\n<p>If <code>cuda=False</code>, everything works and 0s are printed, i.e. all gradients are equal. However, with cuda on many nonzero differences in <code>gradsI</code> but not in <code>gradsS</code> pop up, sometimes of high magnitude, e.g.:</p>\n<pre><code>0 0 0.0 0.0\n0 1 1.4411518807585587e+17 0.0\n0 2 1.4411518807585587e+17 0.0\n0 3 1.4411518807585587e+17 0.0\n1 0 1.4411518807585587e+17 0.0\n1 1 0.0 0.0\n1 2 3.602879701896397e+16 0.0\n1 3 0.015625 0.0\n2 0 1.4411518807585587e+17 0.0\n2 1 3.602879701896397e+16 0.0\n2 2 0.0 0.0\n2 3 3.602879701896397e+16 0.0\n3 0 1.4411518807585587e+17 0.0\n3 1 0.015625 0.0\n3 2 3.602879701896397e+16 0.0\n3 3 0.0 0.0\n</code></pre>\n<p>This indicates that there is something wrong with backward of <code>torch.index_select</code>. Setting <code>torch.set_default_tensor_type('torch.DoubleTensor')</code> doesn't help. Can anybody shed light onto this?</p>", "body_text": "I've found out that gradient of index_select (i..e index_add) behaves non-deterministically - and wrong - on GPU for some rather random conditions. The following code illustrates the problem:\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\ndef main():\n    set_seed(1)\n    cuda = True\n    n, e, c = 5, 100, 5\n    input = torch.randn(n,c)\n    idxn = torch.from_numpy(np.random.randint(n,size=e))  # indices are repeated\n\n    if cuda:\n        input = input.cuda(); idxn = idxn.cuda()\n\n    gradsI, gradsS = [], []\n    N = 4\n    for i in range(N):\n        inputv = Variable(input, requires_grad=True)\n        sel_input = torch.index_select(inputv, 0, Variable(idxn))\n        sel_input.retain_grad()\n\n        # the following computation is one of the \"random conditions\"\n        data = [torch.sum(sel_input.narrow(0,0,e//2), 0),\n                torch.sum(sel_input.narrow(0,e//2,e//2), 0) ]\n        out = torch.cat(data,0)\n        out.exp().sum().backward()\n\n        gradsI.append(inputv.grad.data.cpu().clone())\n        gradsS.append(sel_input.grad.data.cpu().clone())\n\n    for i in range(N):\n        for j in range(N):\n            print(i,j,(gradsI[i]-gradsI[j]).abs().max(), (gradsS[i]-gradsS[j]).abs().max())\nIf cuda=False, everything works and 0s are printed, i.e. all gradients are equal. However, with cuda on many nonzero differences in gradsI but not in gradsS pop up, sometimes of high magnitude, e.g.:\n0 0 0.0 0.0\n0 1 1.4411518807585587e+17 0.0\n0 2 1.4411518807585587e+17 0.0\n0 3 1.4411518807585587e+17 0.0\n1 0 1.4411518807585587e+17 0.0\n1 1 0.0 0.0\n1 2 3.602879701896397e+16 0.0\n1 3 0.015625 0.0\n2 0 1.4411518807585587e+17 0.0\n2 1 3.602879701896397e+16 0.0\n2 2 0.0 0.0\n2 3 3.602879701896397e+16 0.0\n3 0 1.4411518807585587e+17 0.0\n3 1 0.015625 0.0\n3 2 3.602879701896397e+16 0.0\n3 3 0.0 0.0\n\nThis indicates that there is something wrong with backward of torch.index_select. Setting torch.set_default_tensor_type('torch.DoubleTensor') doesn't help. Can anybody shed light onto this?", "body": "I've found out that gradient of `index_select` (i..e `index_add`) behaves non-deterministically - and wrong - on GPU for some rather random conditions. The following code illustrates the problem:\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ndef set_seed(seed):\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n\r\ndef main():\r\n    set_seed(1)\r\n    cuda = True\r\n    n, e, c = 5, 100, 5\r\n    input = torch.randn(n,c)\r\n    idxn = torch.from_numpy(np.random.randint(n,size=e))  # indices are repeated\r\n\r\n    if cuda:\r\n        input = input.cuda(); idxn = idxn.cuda()\r\n\r\n    gradsI, gradsS = [], []\r\n    N = 4\r\n    for i in range(N):\r\n        inputv = Variable(input, requires_grad=True)\r\n        sel_input = torch.index_select(inputv, 0, Variable(idxn))\r\n        sel_input.retain_grad()\r\n\r\n        # the following computation is one of the \"random conditions\"\r\n        data = [torch.sum(sel_input.narrow(0,0,e//2), 0),\r\n                torch.sum(sel_input.narrow(0,e//2,e//2), 0) ]\r\n        out = torch.cat(data,0)\r\n        out.exp().sum().backward()\r\n\r\n        gradsI.append(inputv.grad.data.cpu().clone())\r\n        gradsS.append(sel_input.grad.data.cpu().clone())\r\n\r\n    for i in range(N):\r\n        for j in range(N):\r\n            print(i,j,(gradsI[i]-gradsI[j]).abs().max(), (gradsS[i]-gradsS[j]).abs().max())\r\n```\r\n\r\nIf `cuda=False`, everything works and 0s are printed, i.e. all gradients are equal. However, with cuda on many nonzero differences in `gradsI` but not in `gradsS` pop up, sometimes of high magnitude, e.g.:\r\n```\r\n0 0 0.0 0.0\r\n0 1 1.4411518807585587e+17 0.0\r\n0 2 1.4411518807585587e+17 0.0\r\n0 3 1.4411518807585587e+17 0.0\r\n1 0 1.4411518807585587e+17 0.0\r\n1 1 0.0 0.0\r\n1 2 3.602879701896397e+16 0.0\r\n1 3 0.015625 0.0\r\n2 0 1.4411518807585587e+17 0.0\r\n2 1 3.602879701896397e+16 0.0\r\n2 2 0.0 0.0\r\n2 3 3.602879701896397e+16 0.0\r\n3 0 1.4411518807585587e+17 0.0\r\n3 1 0.015625 0.0\r\n3 2 3.602879701896397e+16 0.0\r\n3 3 0.0 0.0\r\n````\r\n\r\nThis indicates that there is something wrong with backward of `torch.index_select`. Setting `torch.set_default_tensor_type('torch.DoubleTensor')` doesn't help. Can anybody shed light onto this?"}