{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/343940359", "html_url": "https://github.com/pytorch/pytorch/issues/3644#issuecomment-343940359", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3644", "id": 343940359, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Mzk0MDM1OQ==", "user": {"login": "mys007", "id": 5921083, "node_id": "MDQ6VXNlcjU5MjEwODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/5921083?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mys007", "html_url": "https://github.com/mys007", "followers_url": "https://api.github.com/users/mys007/followers", "following_url": "https://api.github.com/users/mys007/following{/other_user}", "gists_url": "https://api.github.com/users/mys007/gists{/gist_id}", "starred_url": "https://api.github.com/users/mys007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mys007/subscriptions", "organizations_url": "https://api.github.com/users/mys007/orgs", "repos_url": "https://api.github.com/users/mys007/repos", "events_url": "https://api.github.com/users/mys007/events{/privacy}", "received_events_url": "https://api.github.com/users/mys007/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-13T14:45:17Z", "updated_at": "2017-11-13T14:45:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I indeed do disagree, because the core issue remains unanswered. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a> showed that the gradient of exp() function is correct and large, given the input, fair enough. But I chose this example on purpose to demonstrate that the scale of non-determinism in <code>index_add</code> depends on the scale of gradients. My issue is valid even if I write <code>input = torch.randn(n,c) * 0.01</code> - the gradients are then small but <strong>still different between runs</strong>.</p>", "body_text": "I indeed do disagree, because the core issue remains unanswered. @zou3519 showed that the gradient of exp() function is correct and large, given the input, fair enough. But I chose this example on purpose to demonstrate that the scale of non-determinism in index_add depends on the scale of gradients. My issue is valid even if I write input = torch.randn(n,c) * 0.01 - the gradients are then small but still different between runs.", "body": "I indeed do disagree, because the core issue remains unanswered. @zou3519 showed that the gradient of exp() function is correct and large, given the input, fair enough. But I chose this example on purpose to demonstrate that the scale of non-determinism in `index_add` depends on the scale of gradients. My issue is valid even if I write `input = torch.randn(n,c) * 0.01` - the gradients are then small but **still different between runs**. "}