{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/343933353", "html_url": "https://github.com/pytorch/pytorch/issues/3644#issuecomment-343933353", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3644", "id": 343933353, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MzkzMzM1Mw==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-13T14:23:18Z", "updated_at": "2017-11-13T14:23:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So let <code>output = out.exp().sum()</code>. I'm going to numerically compute the partial derivative of output with respect to <code>input[0, 2]</code>, which is theoretically equal to <code>gradsI[i][0, 2]</code> for all <code>i</code>.</p>\n<p>Here's the code I used to do that: <a href=\"https://gist.github.com/zou3519/4cb1c3a674e9785bf8121e851d4ded0d\">https://gist.github.com/zou3519/4cb1c3a674e9785bf8121e851d4ded0d</a><br>\nI ran this twice, once with the line <code>input[0, 2] += 0</code>, and once with the <code>input[0, 2] += 0.1</code>.<br>\nThey gave the following output</p>\n<pre><code>output =\n 9.7372e+22\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n  input =  -0.43049895763397217\noutput =\n 2.6468e+23\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n  input =  -0.33049896359443665\n</code></pre>\n<p>This gives two data points (input, output): <code>(-0.43049895763397217, 9.7372e+22)</code> and <code>(-0.33049896359443665, 2.6468e+23)</code>. Estimating the partial derivative with (change in output) / (change in input) gives 1.67308e+24, so I think it's reasonable that given the forward pass, the gradients are very large.</p>", "body_text": "So let output = out.exp().sum(). I'm going to numerically compute the partial derivative of output with respect to input[0, 2], which is theoretically equal to gradsI[i][0, 2] for all i.\nHere's the code I used to do that: https://gist.github.com/zou3519/4cb1c3a674e9785bf8121e851d4ded0d\nI ran this twice, once with the line input[0, 2] += 0, and once with the input[0, 2] += 0.1.\nThey gave the following output\noutput =\n 9.7372e+22\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n  input =  -0.43049895763397217\noutput =\n 2.6468e+23\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n  input =  -0.33049896359443665\n\nThis gives two data points (input, output): (-0.43049895763397217, 9.7372e+22) and (-0.33049896359443665, 2.6468e+23). Estimating the partial derivative with (change in output) / (change in input) gives 1.67308e+24, so I think it's reasonable that given the forward pass, the gradients are very large.", "body": "So let `output = out.exp().sum()`. I'm going to numerically compute the partial derivative of output with respect to `input[0, 2]`, which is theoretically equal to `gradsI[i][0, 2]` for all `i`.\r\n\r\nHere's the code I used to do that: https://gist.github.com/zou3519/4cb1c3a674e9785bf8121e851d4ded0d\r\nI ran this twice, once with the line `input[0, 2] += 0`, and once with the `input[0, 2] += 0.1`.\r\nThey gave the following output\r\n\r\n```\r\noutput =\r\n 9.7372e+22\r\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\r\n  input =  -0.43049895763397217\r\noutput =\r\n 2.6468e+23\r\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\r\n  input =  -0.33049896359443665\r\n```\r\nThis gives two data points (input, output): `(-0.43049895763397217, 9.7372e+22)` and `(-0.33049896359443665, 2.6468e+23)`. Estimating the partial derivative with (change in output) / (change in input) gives 1.67308e+24, so I think it's reasonable that given the forward pass, the gradients are very large."}