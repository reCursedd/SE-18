{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7735", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7735/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7735/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7735/events", "html_url": "https://github.com/pytorch/pytorch/issues/7735", "id": 324993822, "node_id": "MDU6SXNzdWUzMjQ5OTM4MjI=", "number": 7735, "title": "ATen C++ tensor creation places tensors on devices inconsistently from torch.* Python calls", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-05-21T17:19:01Z", "updated_at": "2018-06-16T07:40:37Z", "closed_at": "2018-06-16T07:40:37Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>I'm moving our extension backend to use the ATen interface.  It's very cool and convenient for the most part, but I noticed that when the ATen backend functions create new tensors, the device placement behavior after the current device changes is inconsistent from the behavior of identically named Python torch functions.  Specifically:</p>\n<pre><code>x = torch.cuda.FloatTensor(size) # device='cuda:0'\nwith torch.cuda.device(1):\n    y = torch.empty_like(x)\n    z = myext._C.create_empty_like(x) # small custom extension that returns z = at::empty_like(x)\ny is now on device 0\nz is now on device 1\n</code></pre>\n<p>Is this the intended behavior, or a bug?<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22205833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/csarofeen\">@csarofeen</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a> for interest, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> because I see \"TODO:  zach\" scattered around the backend code :D</p>\n<h2>Code example</h2>\n<p>I've attached a minimal working repro: <a href=\"https://github.com/pytorch/pytorch/files/2023467/repro.tar.gz\">repro.tar.gz</a>.<br>\nExtract, enter the directory, and it should be runnable via</p>\n<pre><code>python setup.py install\npython test_empty_like.py\n</code></pre>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.5.0a0+d4f6c84<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.1.85</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.9.4</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.1.85<br>\nGPU models and configuration:<br>\nGPU 0: TITAN X (Pascal)<br>\nGPU 1: Graphics Device</p>\n<p>Nvidia driver version: 390.48<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5.real<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.1<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.13.3)<br>\n[pip3] numpydoc (0.7.0)<br>\n[pip3] torch (0.5.0a0+d4f6c84)<br>\n[pip3] torch-test-cpp-extension (0.0.0)<br>\n[pip3] torchvision (0.2.0)<br>\n[conda] magma-cuda90              2.2.0                hae16b58_1    pytorch<br>\n[conda] torch                     0.5.0a0+d4f6c84           <br>\n[conda] torch-test-cpp-extension  0.0.0                     <br>\n[conda] torchvision               0.2.0                     </p>", "body_text": "Issue description\nI'm moving our extension backend to use the ATen interface.  It's very cool and convenient for the most part, but I noticed that when the ATen backend functions create new tensors, the device placement behavior after the current device changes is inconsistent from the behavior of identically named Python torch functions.  Specifically:\nx = torch.cuda.FloatTensor(size) # device='cuda:0'\nwith torch.cuda.device(1):\n    y = torch.empty_like(x)\n    z = myext._C.create_empty_like(x) # small custom extension that returns z = at::empty_like(x)\ny is now on device 0\nz is now on device 1\n\nIs this the intended behavior, or a bug?\n@csarofeen @ngimel @mruberry for interest, @zdevito because I see \"TODO:  zach\" scattered around the backend code :D\nCode example\nI've attached a minimal working repro: repro.tar.gz.\nExtract, enter the directory, and it should be runnable via\npython setup.py install\npython test_empty_like.py\n\nSystem Info\nCollecting environment information...\nPyTorch version: 0.5.0a0+d4f6c84\nIs debug build: No\nCUDA used to build PyTorch: 9.1.85\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.9.4\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration:\nGPU 0: TITAN X (Pascal)\nGPU 1: Graphics Device\nNvidia driver version: 390.48\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5.real\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.1\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\nVersions of relevant libraries:\n[pip3] numpy (1.13.3)\n[pip3] numpydoc (0.7.0)\n[pip3] torch (0.5.0a0+d4f6c84)\n[pip3] torch-test-cpp-extension (0.0.0)\n[pip3] torchvision (0.2.0)\n[conda] magma-cuda90              2.2.0                hae16b58_1    pytorch\n[conda] torch                     0.5.0a0+d4f6c84           \n[conda] torch-test-cpp-extension  0.0.0                     \n[conda] torchvision               0.2.0", "body": "## Issue description\r\n\r\nI'm moving our extension backend to use the ATen interface.  It's very cool and convenient for the most part, but I noticed that when the ATen backend functions create new tensors, the device placement behavior after the current device changes is inconsistent from the behavior of identically named Python torch functions.  Specifically: \r\n\r\n```\r\nx = torch.cuda.FloatTensor(size) # device='cuda:0'\r\nwith torch.cuda.device(1):\r\n    y = torch.empty_like(x)\r\n    z = myext._C.create_empty_like(x) # small custom extension that returns z = at::empty_like(x)\r\ny is now on device 0\r\nz is now on device 1\r\n```\r\n\r\nIs this the intended behavior, or a bug?\r\n@csarofeen @ngimel @mruberry for interest, @zdevito because I see \"TODO:  zach\" scattered around the backend code :D\r\n\r\n## Code example\r\n\r\nI've attached a minimal working repro: [repro.tar.gz](https://github.com/pytorch/pytorch/files/2023467/repro.tar.gz).\r\nExtract, enter the directory, and it should be runnable via\r\n```\r\npython setup.py install\r\npython test_empty_like.py\r\n``` \r\n\r\n## System Info\r\nCollecting environment information...\r\nPyTorch version: 0.5.0a0+d4f6c84\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.1.85\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.9.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: Graphics Device\r\n\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5.real\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.1\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.13.3)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.5.0a0+d4f6c84)\r\n[pip3] torch-test-cpp-extension (0.0.0)\r\n[pip3] torchvision (0.2.0)\r\n[conda] magma-cuda90              2.2.0                hae16b58_1    pytorch\r\n[conda] torch                     0.5.0a0+d4f6c84           <pip>\r\n[conda] torch-test-cpp-extension  0.0.0                     <pip>\r\n[conda] torchvision               0.2.0                     <pip>\r\n\r\n"}