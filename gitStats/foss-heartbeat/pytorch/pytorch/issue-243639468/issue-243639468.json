{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2138", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2138/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2138/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2138/events", "html_url": "https://github.com/pytorch/pytorch/issues/2138", "id": 243639468, "node_id": "MDU6SXNzdWUyNDM2Mzk0Njg=", "number": 2138, "title": "RuntimeError: expected Double tensor (got Float tensor)", "user": {"login": "diggerdu", "id": 5636045, "node_id": "MDQ6VXNlcjU2MzYwNDU=", "avatar_url": "https://avatars2.githubusercontent.com/u/5636045?v=4", "gravatar_id": "", "url": "https://api.github.com/users/diggerdu", "html_url": "https://github.com/diggerdu", "followers_url": "https://api.github.com/users/diggerdu/followers", "following_url": "https://api.github.com/users/diggerdu/following{/other_user}", "gists_url": "https://api.github.com/users/diggerdu/gists{/gist_id}", "starred_url": "https://api.github.com/users/diggerdu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/diggerdu/subscriptions", "organizations_url": "https://api.github.com/users/diggerdu/orgs", "repos_url": "https://api.github.com/users/diggerdu/repos", "events_url": "https://api.github.com/users/diggerdu/events{/privacy}", "received_events_url": "https://api.github.com/users/diggerdu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-07-18T08:33:14Z", "updated_at": "2018-10-07T22:44:12Z", "closed_at": "2017-11-03T11:59:06Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> -*- coding: utf-8 -*-</span>\n  <span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import\n  <span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n  <span class=\"pl-k\">import</span> torch\n  <span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n  <span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n    \n    \n  kernel <span class=\"pl-k\">=</span> torch.from_numpy(np.random.random_sample((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>))).double()\n  input_ <span class=\"pl-k\">=</span> torch.from_numpy(np.random.random_sample((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">6</span>))).double()\n                                                                                                           \n  <span class=\"pl-c1\">print</span>(kernel)\n  <span class=\"pl-c1\">print</span>(input_)\n   \n  model <span class=\"pl-k\">=</span> nn.Sequential(nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, (<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>), <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))\n  model[<span class=\"pl-c1\">0</span>].weight.data.copy_(kernel)\n  A <span class=\"pl-k\">=</span> Variable(input_)\n  out <span class=\"pl-k\">=</span> model(A)\n  <span class=\"pl-c1\">print</span>(out.data)</pre></div>\n<p>But the input is actually DoubleTensor</p>\n<pre><code>$ python test.py\n\n 0.0100  0.6528  0.8658\n[torch.DoubleTensor of size 1x3]\n\n\n(0 ,0 ,.,.) = \n  0.9582  0.9593  0.1207  0.5719  0.3984  0.4603\n[torch.DoubleTensor of size 1x1x1x6]\n\nTraceback (most recent call last):\n  File \"test.py\", line 18, in &lt;module&gt;\n    out = model(A)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 64, in forward\n    input = module(input)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 237, in forward\n    self.padding, self.dilation, self.groups)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 40, in conv2d\n    return f(input, weight, bias)\nRuntimeError: expected Double tensor (got Float tensor)\n\n</code></pre>", "body_text": "# -*- coding: utf-8 -*-\n  from __future__ import absolute_import\n  import numpy as np\n  import torch\n  import torch.nn as nn\n  from torch.autograd import Variable\n    \n    \n  kernel = torch.from_numpy(np.random.random_sample((1, 3))).double()\n  input_ = torch.from_numpy(np.random.random_sample((1, 1, 1, 6))).double()\n                                                                                                           \n  print(kernel)\n  print(input_)\n   \n  model = nn.Sequential(nn.Conv2d(1, 1, (1, 3), stride=1, padding=0, bias=False))\n  model[0].weight.data.copy_(kernel)\n  A = Variable(input_)\n  out = model(A)\n  print(out.data)\nBut the input is actually DoubleTensor\n$ python test.py\n\n 0.0100  0.6528  0.8658\n[torch.DoubleTensor of size 1x3]\n\n\n(0 ,0 ,.,.) = \n  0.9582  0.9593  0.1207  0.5719  0.3984  0.4603\n[torch.DoubleTensor of size 1x1x1x6]\n\nTraceback (most recent call last):\n  File \"test.py\", line 18, in <module>\n    out = model(A)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 64, in forward\n    input = module(input)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 237, in forward\n    self.padding, self.dilation, self.groups)\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 40, in conv2d\n    return f(input, weight, bias)\nRuntimeError: expected Double tensor (got Float tensor)", "body": "```python\r\n\r\n  # -*- coding: utf-8 -*-\r\n  from __future__ import absolute_import\r\n  import numpy as np\r\n  import torch\r\n  import torch.nn as nn\r\n  from torch.autograd import Variable\r\n    \r\n    \r\n  kernel = torch.from_numpy(np.random.random_sample((1, 3))).double()\r\n  input_ = torch.from_numpy(np.random.random_sample((1, 1, 1, 6))).double()\r\n                                                                                                           \r\n  print(kernel)\r\n  print(input_)\r\n   \r\n  model = nn.Sequential(nn.Conv2d(1, 1, (1, 3), stride=1, padding=0, bias=False))\r\n  model[0].weight.data.copy_(kernel)\r\n  A = Variable(input_)\r\n  out = model(A)\r\n  print(out.data)\r\n```\r\nBut the input is actually DoubleTensor\r\n```\r\n$ python test.py\r\n\r\n 0.0100  0.6528  0.8658\r\n[torch.DoubleTensor of size 1x3]\r\n\r\n\r\n(0 ,0 ,.,.) = \r\n  0.9582  0.9593  0.1207  0.5719  0.3984  0.4603\r\n[torch.DoubleTensor of size 1x1x1x6]\r\n\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 18, in <module>\r\n    out = model(A)\r\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 64, in forward\r\n    input = module(input)\r\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 237, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File \"/home/diggerdu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 40, in conv2d\r\n    return f(input, weight, bias)\r\nRuntimeError: expected Double tensor (got Float tensor)\r\n\r\n```\r\n"}