{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201874526", "pull_request_review_id": 136462673, "id": 201874526, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTg3NDUyNg==", "diff_hunk": "@@ -340,9 +362,6 @@ std::vector<Tensor> split_with_sizes(const Tensor& self, IntList split_sizes, in\n          << \"entries, but got split_sizes=\" << split_sizes;\n       throw std::runtime_error(ss.str());\n     }\n-    if (start_idx >= dim_size) {\n-      break;\n-    }", "path": "aten/src/ATen/native/TensorShape.cpp", "position": 117, "original_position": 113, "commit_id": "6193143982dbda86bd844b548b0a9e142cf4cd1b", "original_commit_id": "f8468dd729d183130e6b8b3e255538fe18153e79", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "I think you are referring to \"include the index to the end\" discussion, but no, I decided not to do that because IMO we should just have a separate function that behaves like numpy split.\r\n\r\nThis change just prevents us from exiting out of the loop early (the previous assumption was that each size was at least 1, so we knew we'd be past the dim_size the next time through the loop, but that isn't correct either because we had size [0] tensors).\r\n\r\nIn any case, narrow already does the correct things in all these cases (error checking, etc.), so there's no need for this code.", "created_at": "2018-07-11T23:57:46Z", "updated_at": "2018-11-23T15:47:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/9362#discussion_r201874526", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9362", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201874526"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9362#discussion_r201874526"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9362"}}, "body_html": "<p>I think you are referring to \"include the index to the end\" discussion, but no, I decided not to do that because IMO we should just have a separate function that behaves like numpy split.</p>\n<p>This change just prevents us from exiting out of the loop early (the previous assumption was that each size was at least 1, so we knew we'd be past the dim_size the next time through the loop, but that isn't correct either because we had size [0] tensors).</p>\n<p>In any case, narrow already does the correct things in all these cases (error checking, etc.), so there's no need for this code.</p>", "body_text": "I think you are referring to \"include the index to the end\" discussion, but no, I decided not to do that because IMO we should just have a separate function that behaves like numpy split.\nThis change just prevents us from exiting out of the loop early (the previous assumption was that each size was at least 1, so we knew we'd be past the dim_size the next time through the loop, but that isn't correct either because we had size [0] tensors).\nIn any case, narrow already does the correct things in all these cases (error checking, etc.), so there's no need for this code.", "in_reply_to_id": 201873621}