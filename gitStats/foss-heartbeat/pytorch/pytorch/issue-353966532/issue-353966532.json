{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10873", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10873/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10873/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10873/events", "html_url": "https://github.com/pytorch/pytorch/issues/10873", "id": 353966532, "node_id": "MDU6SXNzdWUzNTM5NjY1MzI=", "number": 10873, "title": "[JIT][ONNX] aten::add and aten::sub ST overloads don't have alpha, so they don't match the ONNX symbolic", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-25T00:39:16Z", "updated_at": "2018-09-18T20:56:55Z", "closed_at": "2018-09-18T20:56:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Repro (aten::add):</p>\n<pre><code>import torch\n\nclass SomeModule(torch.jit.ScriptModule):\n\n    @torch.jit.script_method\n    def forward(self, x : torch.Tensor):\n        bs = x.size(0)\n        return bs + 1\n\nexample_outputs = (torch.LongTensor([SomeModule()(torch.rand(3, 4))]),)\n\nimport io\nf = io.BytesIO()\ntorch.onnx._export(SomeModule(), (torch.rand(3, 4),), f, verbose=True, example_outputs=example_outputs)\n</code></pre>\n<pre><code>aten::size(Tensor self, int dim) -&gt; int\naten::add(Tensor self, Tensor other, *, Scalar alpha=&lt;default&gt;) -&gt; Tensor\naten::add(Tensor self, Scalar other, Scalar alpha=&lt;default&gt;) -&gt; Tensor\naten::add(int[] a, int[] b) -&gt; int[]\naten::add(float[] a, float[] b) -&gt; float[]\naten::add(Tensor[] a, Tensor[] b) -&gt; Tensor[]\naten::add(int a, int b) -&gt; int\naten::size(Tensor self, int dim) -&gt; int\naten::add(Tensor self, Tensor other, *, Scalar alpha=&lt;default&gt;) -&gt; Tensor\naten::add(Tensor self, Scalar other, Scalar alpha=&lt;default&gt;) -&gt; Tensor\naten::add(int[] a, int[] b) -&gt; int[]\naten::add(float[] a, float[] b) -&gt; float[]\naten::add(Tensor[] a, Tensor[] b) -&gt; Tensor[]\naten::add(int a, int b) -&gt; int\nTraceback (most recent call last):\n  File \"test_sub.py\", line 14, in &lt;module&gt;\n    torch.onnx._export(SomeModule(), (torch.rand(3, 4),), f, verbose=True, example_outputs=example_outputs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/__init__.py\", line 22, in _export\n    return utils._export(*args, **kwargs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 257, in _export\n    example_outputs, propagate)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 210, in _model_to_graph\n    graph = _optimize_graph(graph, operator_export_type)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 138, in _optimize_graph\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/__init__.py\", line 52, in _run_symbolic_function\n    return utils._run_symbolic_function(*args, **kwargs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 480, in _run_symbolic_function\n    return fn(g, *inputs, **attrs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/symbolic.py\", line 82, in wrapper\n    assert len(arg_descriptors) == len(args)\n</code></pre>\n<p>Underlying issue is that <code>DEFINE_ST_OP</code> does not define a Scalar alpha argument for <code>aten::add</code> and <code>aten::sub</code> (<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/register_prim_ops.cpp#L370\">https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/register_prim_ops.cpp#L370</a>), therefore the schema of these ops does not match the ONNX symbolic (<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic.py#L188\">https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic.py#L188</a>).</p>", "body_text": "Repro (aten::add):\nimport torch\n\nclass SomeModule(torch.jit.ScriptModule):\n\n    @torch.jit.script_method\n    def forward(self, x : torch.Tensor):\n        bs = x.size(0)\n        return bs + 1\n\nexample_outputs = (torch.LongTensor([SomeModule()(torch.rand(3, 4))]),)\n\nimport io\nf = io.BytesIO()\ntorch.onnx._export(SomeModule(), (torch.rand(3, 4),), f, verbose=True, example_outputs=example_outputs)\n\naten::size(Tensor self, int dim) -> int\naten::add(Tensor self, Tensor other, *, Scalar alpha=<default>) -> Tensor\naten::add(Tensor self, Scalar other, Scalar alpha=<default>) -> Tensor\naten::add(int[] a, int[] b) -> int[]\naten::add(float[] a, float[] b) -> float[]\naten::add(Tensor[] a, Tensor[] b) -> Tensor[]\naten::add(int a, int b) -> int\naten::size(Tensor self, int dim) -> int\naten::add(Tensor self, Tensor other, *, Scalar alpha=<default>) -> Tensor\naten::add(Tensor self, Scalar other, Scalar alpha=<default>) -> Tensor\naten::add(int[] a, int[] b) -> int[]\naten::add(float[] a, float[] b) -> float[]\naten::add(Tensor[] a, Tensor[] b) -> Tensor[]\naten::add(int a, int b) -> int\nTraceback (most recent call last):\n  File \"test_sub.py\", line 14, in <module>\n    torch.onnx._export(SomeModule(), (torch.rand(3, 4),), f, verbose=True, example_outputs=example_outputs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/__init__.py\", line 22, in _export\n    return utils._export(*args, **kwargs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 257, in _export\n    example_outputs, propagate)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 210, in _model_to_graph\n    graph = _optimize_graph(graph, operator_export_type)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 138, in _optimize_graph\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/__init__.py\", line 52, in _run_symbolic_function\n    return utils._run_symbolic_function(*args, **kwargs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 480, in _run_symbolic_function\n    return fn(g, *inputs, **attrs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/symbolic.py\", line 82, in wrapper\n    assert len(arg_descriptors) == len(args)\n\nUnderlying issue is that DEFINE_ST_OP does not define a Scalar alpha argument for aten::add and aten::sub (https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/register_prim_ops.cpp#L370), therefore the schema of these ops does not match the ONNX symbolic (https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic.py#L188).", "body": "Repro (aten::add):\r\n\r\n```\r\nimport torch\r\n\r\nclass SomeModule(torch.jit.ScriptModule):\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, x : torch.Tensor):\r\n        bs = x.size(0)\r\n        return bs + 1\r\n\r\nexample_outputs = (torch.LongTensor([SomeModule()(torch.rand(3, 4))]),)\r\n\r\nimport io\r\nf = io.BytesIO()\r\ntorch.onnx._export(SomeModule(), (torch.rand(3, 4),), f, verbose=True, example_outputs=example_outputs)\r\n```\r\n\r\n```\r\naten::size(Tensor self, int dim) -> int\r\naten::add(Tensor self, Tensor other, *, Scalar alpha=<default>) -> Tensor\r\naten::add(Tensor self, Scalar other, Scalar alpha=<default>) -> Tensor\r\naten::add(int[] a, int[] b) -> int[]\r\naten::add(float[] a, float[] b) -> float[]\r\naten::add(Tensor[] a, Tensor[] b) -> Tensor[]\r\naten::add(int a, int b) -> int\r\naten::size(Tensor self, int dim) -> int\r\naten::add(Tensor self, Tensor other, *, Scalar alpha=<default>) -> Tensor\r\naten::add(Tensor self, Scalar other, Scalar alpha=<default>) -> Tensor\r\naten::add(int[] a, int[] b) -> int[]\r\naten::add(float[] a, float[] b) -> float[]\r\naten::add(Tensor[] a, Tensor[] b) -> Tensor[]\r\naten::add(int a, int b) -> int\r\nTraceback (most recent call last):\r\n  File \"test_sub.py\", line 14, in <module>\r\n    torch.onnx._export(SomeModule(), (torch.rand(3, 4),), f, verbose=True, example_outputs=example_outputs)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/__init__.py\", line 22, in _export\r\n    return utils._export(*args, **kwargs)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 257, in _export\r\n    example_outputs, propagate)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 210, in _model_to_graph\r\n    graph = _optimize_graph(graph, operator_export_type)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 138, in _optimize_graph\r\n    graph = torch._C._jit_pass_onnx(graph, operator_export_type)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/__init__.py\", line 52, in _run_symbolic_function\r\n    return utils._run_symbolic_function(*args, **kwargs)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 480, in _run_symbolic_function\r\n    return fn(g, *inputs, **attrs)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/symbolic.py\", line 82, in wrapper\r\n    assert len(arg_descriptors) == len(args)\r\n```\r\n\r\nUnderlying issue is that `DEFINE_ST_OP` does not define a Scalar alpha argument for `aten::add` and `aten::sub` (https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/register_prim_ops.cpp#L370), therefore the schema of these ops does not match the ONNX symbolic (https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic.py#L188).\r\n"}