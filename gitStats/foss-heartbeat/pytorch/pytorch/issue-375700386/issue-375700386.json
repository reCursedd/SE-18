{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13337", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13337/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13337/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13337/events", "html_url": "https://github.com/pytorch/pytorch/pull/13337", "id": 375700386, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI3MDg2OTcy", "number": 13337, "title": "[WIP] Strawman/Tracker for fixing grad copy elision + post hooks (e.g. DDP) interaction", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-30T21:25:10Z", "updated_at": "2018-11-12T18:24:15Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/13337", "html_url": "https://github.com/pytorch/pytorch/pull/13337", "diff_url": "https://github.com/pytorch/pytorch/pull/13337.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/13337.patch"}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17164548\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/FDecaYed\">@FDecaYed</a> wrote some nice logic into <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/accumulate_grad.cpp#L44-L47\">accumulate_grad.cpp</a> to elide unnecessary cloning or accumulation of incoming gradients.  Under certain conditions, the incoming gradient can be stolen and used directly.</p>\n<p>One of these conditions is that the incoming gradient's <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/accumulate_grad.cpp#L47\">refcount must be 1</a>.  Nothing else can be referencing the data (which might happen, for instance, for c = b + a:  the + operator's backward can simply pass c's incoming gradient tensor directly to both a and b's <code>AccumulateGrad</code> functions).</p>\n<p>However, if a given param's <code>AccumulateGrad</code> function has post hooks (like, for example, the allreduce hooks employed by both apex and c10d DDP), the autograd engine <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L361\">stashes a copy</a> of the incoming gradients, which increments the refcount, causing <code>AccumulateGrad</code>'s hard check that the refcount is 1 to fail, and triggering an (in this case) unnecessary deep copy.</p>\n<p>This PR updates the logic to account for the possible presence of post hooks on the <code>AccumulateGrad</code> function.  I discussed it with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> and we both agree it introduces a worrisome silent dependency between <code>engine.cpp</code> and <code>accumulate_grad.cpp</code>.  I've commented the relevant pieces, but I'm open to suggestions as to how this check could be made more agnostic to the implementation of <code>engine.cpp</code>.</p>", "body_text": "@FDecaYed wrote some nice logic into accumulate_grad.cpp to elide unnecessary cloning or accumulation of incoming gradients.  Under certain conditions, the incoming gradient can be stolen and used directly.\nOne of these conditions is that the incoming gradient's refcount must be 1.  Nothing else can be referencing the data (which might happen, for instance, for c = b + a:  the + operator's backward can simply pass c's incoming gradient tensor directly to both a and b's AccumulateGrad functions).\nHowever, if a given param's AccumulateGrad function has post hooks (like, for example, the allreduce hooks employed by both apex and c10d DDP), the autograd engine stashes a copy of the incoming gradients, which increments the refcount, causing AccumulateGrad's hard check that the refcount is 1 to fail, and triggering an (in this case) unnecessary deep copy.\nThis PR updates the logic to account for the possible presence of post hooks on the AccumulateGrad function.  I discussed it with @apaszke and we both agree it introduces a worrisome silent dependency between engine.cpp and accumulate_grad.cpp.  I've commented the relevant pieces, but I'm open to suggestions as to how this check could be made more agnostic to the implementation of engine.cpp.", "body": "@FDecaYed wrote some nice logic into [accumulate_grad.cpp](https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/accumulate_grad.cpp#L44-L47) to elide unnecessary cloning or accumulation of incoming gradients.  Under certain conditions, the incoming gradient can be stolen and used directly.\r\n\r\nOne of these conditions is that the incoming gradient's [refcount must be 1](https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/accumulate_grad.cpp#L47).  Nothing else can be referencing the data (which might happen, for instance, for c = b + a:  the + operator's backward can simply pass c's incoming gradient tensor directly to both a and b's `AccumulateGrad` functions).\r\n\r\nHowever, if a given param's `AccumulateGrad` function has post hooks (like, for example, the allreduce hooks employed by both apex and c10d DDP), the autograd engine [stashes a copy](https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/engine.cpp#L361) of the incoming gradients, which increments the refcount, causing `AccumulateGrad`'s hard check that the refcount is 1 to fail, and triggering an (in this case) unnecessary deep copy.  \r\n\r\nThis PR updates the logic to account for the possible presence of post hooks on the `AccumulateGrad` function.  I discussed it with @apaszke and we both agree it introduces a worrisome silent dependency between `engine.cpp` and `accumulate_grad.cpp`.  I've commented the relevant pieces, but I'm open to suggestions as to how this check could be made more agnostic to the implementation of `engine.cpp`.\r\n"}