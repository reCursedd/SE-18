{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/437979133", "html_url": "https://github.com/pytorch/pytorch/pull/13337#issuecomment-437979133", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13337", "id": 437979133, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzk3OTEzMw==", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-12T18:15:55Z", "updated_at": "2018-11-12T18:24:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>b is not a leaf, so <code>other_hook</code> gets converted to a function pre hook on b's AccumulateGrad function, as you told me in pytorch slack.  In current master (without my PR) if you remove <code>ddp_hook</code> on <code>inp</code>'s AccumulateGrad function, the copy elision optimization takes effect as normal.  The incoming gradient is simply handed down through <code>other_hook</code>, then presumably through <code>view()</code>'s backward, and eventually stashed (without any deep copies along the way) by <code>inp</code>'s AccumulateGrad as <code>inp.grad</code>, and so you observe that the <code>data_ptr()</code>s are the same.</p>\n<p>In your example, the only reason that restoring <code>ddp_hook</code> \"fixes\" your issue is because in current master <code>ddp_hook</code> disables copy elision, which causes a deep copy to take place, so pointers become different.  My PR restores the copy elision, which re-exposes your issue.  In this way, by restoring the copy-eliding behavior, my PR is not \"breaking\" anything:  it's simply re-exposing the default behavior that was already present (which, due to your issue, may have been \"broken\" all along).  This behavior should have been spotted and discussed when grad copy elision was originally merged.</p>\n<p>If you guys are comfortable with the way the grad copy elision currently works in master, then my PR should be acceptable, since it simply restores the existing behavior for cases where a ddp-style hook is registered.  If not, then we can take a harder look at grad copy elision, keeping in mind the big picture that ddp-style hooks in particular are NOT the fundamental issue here.</p>", "body_text": "b is not a leaf, so other_hook gets converted to a function pre hook on b's AccumulateGrad function, as you told me in pytorch slack.  In current master (without my PR) if you remove ddp_hook on inp's AccumulateGrad function, the copy elision optimization takes effect as normal.  The incoming gradient is simply handed down through other_hook, then presumably through view()'s backward, and eventually stashed (without any deep copies along the way) by inp's AccumulateGrad as inp.grad, and so you observe that the data_ptr()s are the same.\nIn your example, the only reason that restoring ddp_hook \"fixes\" your issue is because in current master ddp_hook disables copy elision, which causes a deep copy to take place, so pointers become different.  My PR restores the copy elision, which re-exposes your issue.  In this way, by restoring the copy-eliding behavior, my PR is not \"breaking\" anything:  it's simply re-exposing the default behavior that was already present (which, due to your issue, may have been \"broken\" all along).  This behavior should have been spotted and discussed when grad copy elision was originally merged.\nIf you guys are comfortable with the way the grad copy elision currently works in master, then my PR should be acceptable, since it simply restores the existing behavior for cases where a ddp-style hook is registered.  If not, then we can take a harder look at grad copy elision, keeping in mind the big picture that ddp-style hooks in particular are NOT the fundamental issue here.", "body": "b is not a leaf, so `other_hook` gets converted to a function pre hook on b's AccumulateGrad function, as you told me in pytorch slack.  In current master (without my PR) if you remove `ddp_hook` on `inp`'s AccumulateGrad function, the copy elision optimization takes effect as normal.  The incoming gradient is simply handed down through `other_hook`, then presumably through `view()`'s backward, and eventually stashed (without any deep copies along the way) by `inp`'s AccumulateGrad as `inp.grad`, and so you observe that the `data_ptr()`s are the same.  \r\n\r\nIn your example, the only reason that restoring `ddp_hook` \"fixes\" your issue is because in current master `ddp_hook` disables copy elision, which causes a deep copy to take place, so pointers become different.  My PR restores the copy elision, which re-exposes your issue.  In this way, by restoring the copy-eliding behavior, my PR is not \"breaking\" anything:  it's simply re-exposing the default behavior that was already present (which, due to your issue, may have been \"broken\" all along).  This behavior should have been spotted and discussed when grad copy elision was originally merged.\r\n\r\nIf you guys are comfortable with the way the grad copy elision currently works in master, then my PR should be acceptable, since it simply restores the existing behavior for cases where a ddp-style hook is registered.  If not, then we can take a harder look at grad copy elision, keeping in mind the big picture that ddp-style hooks in particular are NOT the fundamental issue here."}