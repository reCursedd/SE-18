{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/435502572", "html_url": "https://github.com/pytorch/pytorch/pull/13337#issuecomment-435502572", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13337", "id": 435502572, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTUwMjU3Mg==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-02T20:41:12Z", "updated_at": "2018-11-02T20:44:26Z", "author_association": "COLLABORATOR", "body_html": "<p>Actually to be fair, the following code sample fails on master if the DDP-like hook is removed. So this maybe not be a blocker for this PR.<br>\nBut the following code used to work and is not working anymore with this PR.</p>\n<p>Code sample:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\ninp <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ndummy <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Make sure we get contiguous gradients</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> DPP like hook</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">dpp_hook</span>(<span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>):\n    <span class=\"pl-k\">pass</span>\ntmp <span class=\"pl-k\">=</span> inp.expand_as(inp)\ntmp.grad_fn.next_functions[<span class=\"pl-c1\">0</span>][<span class=\"pl-c1\">0</span>].register_hook(dpp_hook)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> View your leaf tensor differently</span>\nb <span class=\"pl-k\">=</span> inp.view(<span class=\"pl-c1\">10</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Hook on the result of this op</span>\nmy_bad_ref <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">other_hook</span>(<span class=\"pl-smi\">grad</span>):\n    <span class=\"pl-k\">global</span> my_bad_ref\n    my_bad_ref <span class=\"pl-k\">=</span> grad\nb.register_hook(other_hook)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This first backward will make all refs the same from this PR</span>\nb.mul(dummy).sum().backward()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.grad data: <span class=\"pl-pds\">\"</span></span>, inp.grad.data_ptr())\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>my_bad_ref data: <span class=\"pl-pds\">\"</span></span>, my_bad_ref.data_ptr())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Subsequent backward modify my awesome tensor</span>\n<span class=\"pl-c1\">print</span>(my_bad_ref)\ninp.sum().backward()\n<span class=\"pl-c1\">print</span>(my_bad_ref) <span class=\"pl-c\"><span class=\"pl-c\">#</span> It changed here</span></pre></div>\n<p>With current master, the data_ptr() will not be the same and both prints at the end will print the same tensor.<br>\nWith this change, all the data_ptr() are the same and the two prints at the end contain two different values.</p>", "body_text": "Actually to be fair, the following code sample fails on master if the DDP-like hook is removed. So this maybe not be a blocker for this PR.\nBut the following code used to work and is not working anymore with this PR.\nCode sample:\nimport torch\n\ninp = torch.rand(1, 10, requires_grad=True)\ndummy = torch.rand(1, 10) # Make sure we get contiguous gradients\n\n# DPP like hook\ndef dpp_hook(*args):\n    pass\ntmp = inp.expand_as(inp)\ntmp.grad_fn.next_functions[0][0].register_hook(dpp_hook)\n\n# View your leaf tensor differently\nb = inp.view(10)\n\n# Hook on the result of this op\nmy_bad_ref = None\ndef other_hook(grad):\n    global my_bad_ref\n    my_bad_ref = grad\nb.register_hook(other_hook)\n\n# This first backward will make all refs the same from this PR\nb.mul(dummy).sum().backward()\nprint(\".grad data: \", inp.grad.data_ptr())\nprint(\"my_bad_ref data: \", my_bad_ref.data_ptr())\n\n# Subsequent backward modify my awesome tensor\nprint(my_bad_ref)\ninp.sum().backward()\nprint(my_bad_ref) # It changed here\nWith current master, the data_ptr() will not be the same and both prints at the end will print the same tensor.\nWith this change, all the data_ptr() are the same and the two prints at the end contain two different values.", "body": "Actually to be fair, the following code sample fails on master if the DDP-like hook is removed. So this maybe not be a blocker for this PR.\r\nBut the following code used to work and is not working anymore with this PR.\r\n\r\nCode sample:\r\n```python\r\nimport torch\r\n\r\ninp = torch.rand(1, 10, requires_grad=True)\r\ndummy = torch.rand(1, 10) # Make sure we get contiguous gradients\r\n\r\n# DPP like hook\r\ndef dpp_hook(*args):\r\n    pass\r\ntmp = inp.expand_as(inp)\r\ntmp.grad_fn.next_functions[0][0].register_hook(dpp_hook)\r\n\r\n# View your leaf tensor differently\r\nb = inp.view(10)\r\n\r\n# Hook on the result of this op\r\nmy_bad_ref = None\r\ndef other_hook(grad):\r\n    global my_bad_ref\r\n    my_bad_ref = grad\r\nb.register_hook(other_hook)\r\n\r\n# This first backward will make all refs the same from this PR\r\nb.mul(dummy).sum().backward()\r\nprint(\".grad data: \", inp.grad.data_ptr())\r\nprint(\"my_bad_ref data: \", my_bad_ref.data_ptr())\r\n\r\n# Subsequent backward modify my awesome tensor\r\nprint(my_bad_ref)\r\ninp.sum().backward()\r\nprint(my_bad_ref) # It changed here\r\n```\r\n\r\nWith current master, the data_ptr() will not be the same and both prints at the end will print the same tensor.\r\nWith this change, all the data_ptr() are the same and the two prints at the end contain two different values.\r\n\r\n"}