{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14187", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14187/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14187/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14187/events", "html_url": "https://github.com/pytorch/pytorch/issues/14187", "id": 382205457, "node_id": "MDU6SXNzdWUzODIyMDU0NTc=", "number": 14187, "title": "nn.parallel.DistributedDataParallel raise CUDA error", "user": {"login": "senhui-guo", "id": 20930758, "node_id": "MDQ6VXNlcjIwOTMwNzU4", "avatar_url": "https://avatars0.githubusercontent.com/u/20930758?v=4", "gravatar_id": "", "url": "https://api.github.com/users/senhui-guo", "html_url": "https://github.com/senhui-guo", "followers_url": "https://api.github.com/users/senhui-guo/followers", "following_url": "https://api.github.com/users/senhui-guo/following{/other_user}", "gists_url": "https://api.github.com/users/senhui-guo/gists{/gist_id}", "starred_url": "https://api.github.com/users/senhui-guo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/senhui-guo/subscriptions", "organizations_url": "https://api.github.com/users/senhui-guo/orgs", "repos_url": "https://api.github.com/users/senhui-guo/repos", "events_url": "https://api.github.com/users/senhui-guo/events{/privacy}", "received_events_url": "https://api.github.com/users/senhui-guo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-11-19T13:04:23Z", "updated_at": "2018-11-20T01:53:56Z", "closed_at": "2018-11-19T16:19:19Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>Here's my code</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n\nparser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--local_rank<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">metavar</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>int<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">dest</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>rank<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>rank<span class=\"pl-pds\">'</span></span>)\nargs <span class=\"pl-k\">=</span> parser.parse_args()\n\n\ntorch.cuda.set_device(args.rank)\ntorch.distributed.init_process_group(<span class=\"pl-v\">backend</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>nccl<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">test</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.l <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">data</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.l(data)\n\nnet <span class=\"pl-k\">=</span> nn.parallel.DistributedDataParallel(test(), <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span>[args.rank], <span class=\"pl-v\">output_device</span><span class=\"pl-k\">=</span>args.rank)</pre></div>\n<p>this raise RuntimeError: Only CUDA dense tensor is supported for NCCL collective operations<br>\nI don't know what I was doing wrong, but I checked the test class, and the buffer &amp; gradient are all not sparse.</p>", "body_text": "\ud83d\udc1b Bug\nHere's my code\nimport argparse\nimport torch\nimport torch.nn as nn\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--local_rank', metavar='int', type=int, dest='rank', default=0, help='rank')\nargs = parser.parse_args()\n\n\ntorch.cuda.set_device(args.rank)\ntorch.distributed.init_process_group(backend='nccl')\nclass test(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l = nn.Linear(10,10)\n\n    def forward(self, data):\n        return self.l(data)\n\nnet = nn.parallel.DistributedDataParallel(test(), device_ids=[args.rank], output_device=args.rank)\nthis raise RuntimeError: Only CUDA dense tensor is supported for NCCL collective operations\nI don't know what I was doing wrong, but I checked the test class, and the buffer & gradient are all not sparse.", "body": "## \ud83d\udc1b Bug\r\n\r\nHere's my code\r\n```python\r\nimport argparse\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--local_rank', metavar='int', type=int, dest='rank', default=0, help='rank')\r\nargs = parser.parse_args()\r\n\r\n\r\ntorch.cuda.set_device(args.rank)\r\ntorch.distributed.init_process_group(backend='nccl')\r\nclass test(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.l = nn.Linear(10,10)\r\n\r\n    def forward(self, data):\r\n        return self.l(data)\r\n\r\nnet = nn.parallel.DistributedDataParallel(test(), device_ids=[args.rank], output_device=args.rank)\r\n```\r\n\r\nthis raise RuntimeError: Only CUDA dense tensor is supported for NCCL collective operations\r\nI don't know what I was doing wrong, but I checked the test class, and the buffer & gradient are all not sparse. "}