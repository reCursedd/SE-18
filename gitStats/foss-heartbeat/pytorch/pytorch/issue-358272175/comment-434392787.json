{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434392787", "html_url": "https://github.com/pytorch/pytorch/issues/11422#issuecomment-434392787", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11422", "id": 434392787, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDM5Mjc4Nw==", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-30T17:20:10Z", "updated_at": "2018-10-30T17:20:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think I figured out the problem.  Suppose we have long and float tensor a &amp; b to put into shared memory, but a &amp; b actually share the same memory handle. From the note we put the largest memory region in shared memory once <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/multiprocessing/reductions.py#L93\">Here</a>.  However this casted the memory region to be the same class which is the same as whatever you put into the queue first. And when we try to rebuild the second tensor, it complains about the wrong type of storage.<br>\nI can't think of a good solution right away, need to discuss with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> for it.</p>", "body_text": "I think I figured out the problem.  Suppose we have long and float tensor a & b to put into shared memory, but a & b actually share the same memory handle. From the note we put the largest memory region in shared memory once Here.  However this casted the memory region to be the same class which is the same as whatever you put into the queue first. And when we try to rebuild the second tensor, it complains about the wrong type of storage.\nI can't think of a good solution right away, need to discuss with @ezyang for it.", "body": "I think I figured out the problem.  Suppose we have long and float tensor a & b to put into shared memory, but a & b actually share the same memory handle. From the note we put the largest memory region in shared memory once [Here](https://github.com/pytorch/pytorch/blob/master/torch/multiprocessing/reductions.py#L93).  However this casted the memory region to be the same class which is the same as whatever you put into the queue first. And when we try to rebuild the second tensor, it complains about the wrong type of storage. \r\nI can't think of a good solution right away, need to discuss with @ezyang for it. "}