{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/311821943", "html_url": "https://github.com/pytorch/pytorch/issues/1930#issuecomment-311821943", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1930", "id": 311821943, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTgyMTk0Mw==", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-28T23:41:37Z", "updated_at": "2017-06-28T23:41:37Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">The cases I had in mind was when you had done some optimization pass and\ncome up with pre-allocated Tensor buffers rather than relying on a caching\nallocator. I know that two Tensors are not live at the same time, and want\nto use the same buffer for two different ops, but I don't want to have to\nmanage their sizes manually because the Op should know what size the output\nwould need to be. In this case, I would make two tensors and back them with\nthe same storage, allowing the op itself to resize each individual tensor\nto the right size for that op.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Jun 28, 2017 at 11:40 AM, gchanan ***@***.***&gt; wrote:\n passing the input as the output for a function that does resizes should be\n an error; these functions usually resize the outputs at the start of the\n call and it's likely they don't even do the calculations correctly since\n that also modifies the input.\n\n Or maybe we just check that output tensors aren't resized? (this is what\n numpy does). I believe <a class=\"user-mention\" href=\"https://github.com/zdevito\">@zdevito</a> &lt;<a href=\"https://github.com/zdevito\">https://github.com/zdevito</a>&gt; had a use\n case where he wanted to pass in a big buffer, or let it grow dynamically,\n and just wanted to use that for the output without worrying about the sizes\n himself. Do I have that right, Zach?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"239252964\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1930\" href=\"https://github.com/pytorch/pytorch/issues/1930#issuecomment-311749984\">#1930 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAWmGnc6QBxt-FUPHD96rdZD0Uke8f9mks5sIp4agaJpZM4OIWFK\">https://github.com/notifications/unsubscribe-auth/AAWmGnc6QBxt-FUPHD96rdZD0Uke8f9mks5sIp4agaJpZM4OIWFK</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "The cases I had in mind was when you had done some optimization pass and\ncome up with pre-allocated Tensor buffers rather than relying on a caching\nallocator. I know that two Tensors are not live at the same time, and want\nto use the same buffer for two different ops, but I don't want to have to\nmanage their sizes manually because the Op should know what size the output\nwould need to be. In this case, I would make two tensors and back them with\nthe same storage, allowing the op itself to resize each individual tensor\nto the right size for that op.\n\u2026\nOn Wed, Jun 28, 2017 at 11:40 AM, gchanan ***@***.***> wrote:\n passing the input as the output for a function that does resizes should be\n an error; these functions usually resize the outputs at the start of the\n call and it's likely they don't even do the calculations correctly since\n that also modifies the input.\n\n Or maybe we just check that output tensors aren't resized? (this is what\n numpy does). I believe @zdevito <https://github.com/zdevito> had a use\n case where he wanted to pass in a big buffer, or let it grow dynamically,\n and just wanted to use that for the output without worrying about the sizes\n himself. Do I have that right, Zach?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#1930 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAWmGnc6QBxt-FUPHD96rdZD0Uke8f9mks5sIp4agaJpZM4OIWFK>\n .", "body": "The cases I had in mind was when you had done some optimization pass and\ncome up with pre-allocated Tensor buffers rather than relying on a caching\nallocator. I know that two Tensors are not live at the same time, and want\nto use the same buffer for two different ops, but I don't want to have to\nmanage their sizes manually because the Op should know what size the output\nwould need to be. In this case, I would make two tensors and back them with\nthe same storage, allowing the op itself to resize each individual tensor\nto the right size for that op.\n\nOn Wed, Jun 28, 2017 at 11:40 AM, gchanan <notifications@github.com> wrote:\n\n> passing the input as the output for a function that does resizes should be\n> an error; these functions usually resize the outputs at the start of the\n> call and it's likely they don't even do the calculations correctly since\n> that also modifies the input.\n>\n> Or maybe we just check that output tensors aren't resized? (this is what\n> numpy does). I believe @zdevito <https://github.com/zdevito> had a use\n> case where he wanted to pass in a big buffer, or let it grow dynamically,\n> and just wanted to use that for the output without worrying about the sizes\n> himself. Do I have that right, Zach?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/1930#issuecomment-311749984>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAWmGnc6QBxt-FUPHD96rdZD0Uke8f9mks5sIp4agaJpZM4OIWFK>\n> .\n>\n"}