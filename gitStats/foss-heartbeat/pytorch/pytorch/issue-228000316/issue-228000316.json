{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1539", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1539/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1539/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1539/events", "html_url": "https://github.com/pytorch/pytorch/issues/1539", "id": 228000316, "node_id": "MDU6SXNzdWUyMjgwMDAzMTY=", "number": 1539, "title": "Slower speeds using FP16 (cuDNN)", "user": {"login": "SeanNaren", "id": 6707363, "node_id": "MDQ6VXNlcjY3MDczNjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6707363?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SeanNaren", "html_url": "https://github.com/SeanNaren", "followers_url": "https://api.github.com/users/SeanNaren/followers", "following_url": "https://api.github.com/users/SeanNaren/following{/other_user}", "gists_url": "https://api.github.com/users/SeanNaren/gists{/gist_id}", "starred_url": "https://api.github.com/users/SeanNaren/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SeanNaren/subscriptions", "organizations_url": "https://api.github.com/users/SeanNaren/orgs", "repos_url": "https://api.github.com/users/SeanNaren/repos", "events_url": "https://api.github.com/users/SeanNaren/events{/privacy}", "received_events_url": "https://api.github.com/users/SeanNaren/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-05-11T13:56:25Z", "updated_at": "2017-10-31T22:09:44Z", "closed_at": "2017-05-12T16:41:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I created a script that benchmarks the speed of 1 LSTM on GPU <a href=\"https://gist.github.com/SeanNaren/2918811074aa59fa1d6a0e114bbe1277\">here</a>. It seems to be that with cuDNN I achieve slower performance using FP16 than FP32 on a Tesla P100 (POWER8, but I've tried a DGX-1 P100 and saw similar behaviour). I've reported results below:</p>\n<p>P100 (POWER8) CUDA 8.0 cuDNN 6.0.5(RC)</p>\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th align=\"center\">FP32 Fwd (s)</th>\n<th align=\"center\">FP32 Bwd (s)</th>\n<th align=\"center\">FP16 Fwd (s)</th>\n<th align=\"center\">FP16 Bwd (s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32</td>\n<td align=\"center\">0.10</td>\n<td align=\"center\">0.28</td>\n<td align=\"center\">0.16</td>\n<td align=\"center\">0.27</td>\n</tr>\n<tr>\n<td>64</td>\n<td align=\"center\">0.10</td>\n<td align=\"center\">0.14</td>\n<td align=\"center\">0.11</td>\n<td align=\"center\">0.28</td>\n</tr>\n<tr>\n<td>128</td>\n<td align=\"center\">0.13</td>\n<td align=\"center\">0.20</td>\n<td align=\"center\">0.13</td>\n<td align=\"center\">0.33</td>\n</tr>\n</tbody>\n</table>\n<p>P100 (POWER8) CUDA 8.0 cuDNN 6.0.21(GA)</p>\n<table>\n<thead>\n<tr>\n<th>Batch Size</th>\n<th align=\"center\">FP32 Fwd (s)</th>\n<th align=\"center\">FP32 Bwd (s)</th>\n<th align=\"center\">FP16 Fwd (s)</th>\n<th align=\"center\">FP16 Bwd (s)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>32</td>\n<td align=\"center\">0.09</td>\n<td align=\"center\">0.12</td>\n<td align=\"center\">0.16</td>\n<td align=\"center\">0.27</td>\n</tr>\n<tr>\n<td>64</td>\n<td align=\"center\">0.10</td>\n<td align=\"center\">0.15</td>\n<td align=\"center\">0.11</td>\n<td align=\"center\">0.28</td>\n</tr>\n<tr>\n<td>128</td>\n<td align=\"center\">0.13</td>\n<td align=\"center\">0.25</td>\n<td align=\"center\">0.13</td>\n<td align=\"center\">0.33</td>\n</tr>\n</tbody>\n</table>\n<p>I'd expect true FP16 on the P100s to give a much better performance boost! Not sure if batch size or other parameter tuning is needed to see this performance boost. Going to get extra benchmarks using larger of layers as well</p>", "body_text": "I created a script that benchmarks the speed of 1 LSTM on GPU here. It seems to be that with cuDNN I achieve slower performance using FP16 than FP32 on a Tesla P100 (POWER8, but I've tried a DGX-1 P100 and saw similar behaviour). I've reported results below:\nP100 (POWER8) CUDA 8.0 cuDNN 6.0.5(RC)\n\n\n\nBatch Size\nFP32 Fwd (s)\nFP32 Bwd (s)\nFP16 Fwd (s)\nFP16 Bwd (s)\n\n\n\n\n32\n0.10\n0.28\n0.16\n0.27\n\n\n64\n0.10\n0.14\n0.11\n0.28\n\n\n128\n0.13\n0.20\n0.13\n0.33\n\n\n\nP100 (POWER8) CUDA 8.0 cuDNN 6.0.21(GA)\n\n\n\nBatch Size\nFP32 Fwd (s)\nFP32 Bwd (s)\nFP16 Fwd (s)\nFP16 Bwd (s)\n\n\n\n\n32\n0.09\n0.12\n0.16\n0.27\n\n\n64\n0.10\n0.15\n0.11\n0.28\n\n\n128\n0.13\n0.25\n0.13\n0.33\n\n\n\nI'd expect true FP16 on the P100s to give a much better performance boost! Not sure if batch size or other parameter tuning is needed to see this performance boost. Going to get extra benchmarks using larger of layers as well", "body": "I created a script that benchmarks the speed of 1 LSTM on GPU [here](https://gist.github.com/SeanNaren/2918811074aa59fa1d6a0e114bbe1277). It seems to be that with cuDNN I achieve slower performance using FP16 than FP32 on a Tesla P100 (POWER8, but I've tried a DGX-1 P100 and saw similar behaviour). I've reported results below:\r\n\r\nP100 (POWER8) CUDA 8.0 cuDNN 6.0.5(RC)\r\n\r\n| Batch Size | FP32 Fwd (s)  | FP32 Bwd (s)  | FP16 Fwd (s)  | FP16 Bwd (s) \r\n|-----------------|:--------:|:--------:|:--------:|:--------:|\r\n| 32     | 0.10    | 0.28 | 0.16    | 0.27 |\r\n| 64     | 0.10   | 0.14 | 0.11    | 0.28 |\r\n| 128     | 0.13    | 0.20 | 0.13    | 0.33 |\r\n\r\nP100 (POWER8) CUDA 8.0 cuDNN 6.0.21(GA)\r\n\r\n| Batch Size | FP32 Fwd (s)  | FP32 Bwd (s)  | FP16 Fwd (s)  | FP16 Bwd (s) \r\n|-----------------|:--------:|:--------:|:--------:|:--------:|\r\n| 32     | 0.09    | 0.12 | 0.16    | 0.27 |\r\n| 64     | 0.10   | 0.15 | 0.11    | 0.28 |\r\n| 128     | 0.13    | 0.25 | 0.13    | 0.33 |\r\n\r\nI'd expect true FP16 on the P100s to give a much better performance boost! Not sure if batch size or other parameter tuning is needed to see this performance boost. Going to get extra benchmarks using larger of layers as well"}