{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5010", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5010/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5010/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5010/events", "html_url": "https://github.com/pytorch/pytorch/pull/5010", "id": 293828994, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY2NzU3NDU4", "number": 5010, "title": "add AVX2 implementation for sigmoid function", "user": {"login": "vedanuj", "id": 13946458, "node_id": "MDQ6VXNlcjEzOTQ2NDU4", "avatar_url": "https://avatars2.githubusercontent.com/u/13946458?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vedanuj", "html_url": "https://github.com/vedanuj", "followers_url": "https://api.github.com/users/vedanuj/followers", "following_url": "https://api.github.com/users/vedanuj/following{/other_user}", "gists_url": "https://api.github.com/users/vedanuj/gists{/gist_id}", "starred_url": "https://api.github.com/users/vedanuj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vedanuj/subscriptions", "organizations_url": "https://api.github.com/users/vedanuj/orgs", "repos_url": "https://api.github.com/users/vedanuj/repos", "events_url": "https://api.github.com/users/vedanuj/events{/privacy}", "received_events_url": "https://api.github.com/users/vedanuj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-02-02T09:45:36Z", "updated_at": "2018-11-23T15:39:17Z", "closed_at": "2018-03-23T21:34:35Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5010", "html_url": "https://github.com/pytorch/pytorch/pull/5010", "diff_url": "https://github.com/pytorch/pytorch/pull/5010.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5010.patch"}, "body_html": "<p>PR introduces AVX2 optimization for sigmoid floats. Issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"292687844\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4929\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4929/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4929\">#4929</a>. The internal benchmark shows ~10x speedup.</p>\n<ul>\n<li>\n<p>Added AVX2 vectorized sigmoid using the 8-way vectorized <code>exp</code> (<code>exp256_ps</code>) in <em>avx_mathfun.h</em>.</p>\n</li>\n<li>\n<p>Implemented vector dispatch for sigmoid. Since sigmoid function is defined for floats and doubles only, for now, added preprocessor <code>#ifdef</code> to init sigmoid dispatch only for float and double.</p>\n</li>\n<li>\n<p>Vector functions in <em>THVector.h</em> were not called for all of the basic functions in floating point or double only. Changed the <code>LAB_IMPLEMENT_BASIC_FUNCTION</code> define in <em>THTensorMatch.c</em> to use <code>THVector_(NAME)</code> implementations if the inputs are contiguous. For the functions that do not have vectorized SIMD implementations will use the same default function from <em>THMath.h</em></p>\n</li>\n</ul>\n<p><strong>Benchmark</strong></p>\n<p>Non-vectorized sigmoid :</p>\n<pre><code>In [1]: import torch\nIn [2]: x = torch.randn(10000,10000)\nIn [3]: %time _ = x.sigmoid()\nCPU times: user 2.8 s, sys: 130 ms, total: 2.93 s\nWall time: 737 ms\n</code></pre>\n<pre><code>In [1]: import torch\nIn [2]: x = torch.randn(1000,1000)\nIn [3]: %time _ = x.sigmoid()\nCPU times: user 29.1 ms, sys: 4.16 ms, total: 33.3 ms\nWall time: 8.63 ms\n</code></pre>\n<p>AVX2 Vectorized sigmoid</p>\n<pre><code>In [1]: import torch\nIn [2]: x = torch.randn(10000,10000)\nIn [3]: %time _ = x.sigmoid()\nCPU times: user 206 ms, sys: 106 ms, total: 312 ms\nWall time: 78.2 ms\n</code></pre>\n<pre><code>In [14]: x = torch.randn(1000,1000)\nIn [15]: %time _ = x.sigmoid()\nCPU times: user 179 \u00b5s, sys: 2.95 ms, total: 3.13 ms\nWall time: 858 \u00b5s\n</code></pre>", "body_text": "PR introduces AVX2 optimization for sigmoid floats. Issue #4929. The internal benchmark shows ~10x speedup.\n\n\nAdded AVX2 vectorized sigmoid using the 8-way vectorized exp (exp256_ps) in avx_mathfun.h.\n\n\nImplemented vector dispatch for sigmoid. Since sigmoid function is defined for floats and doubles only, for now, added preprocessor #ifdef to init sigmoid dispatch only for float and double.\n\n\nVector functions in THVector.h were not called for all of the basic functions in floating point or double only. Changed the LAB_IMPLEMENT_BASIC_FUNCTION define in THTensorMatch.c to use THVector_(NAME) implementations if the inputs are contiguous. For the functions that do not have vectorized SIMD implementations will use the same default function from THMath.h\n\n\nBenchmark\nNon-vectorized sigmoid :\nIn [1]: import torch\nIn [2]: x = torch.randn(10000,10000)\nIn [3]: %time _ = x.sigmoid()\nCPU times: user 2.8 s, sys: 130 ms, total: 2.93 s\nWall time: 737 ms\n\nIn [1]: import torch\nIn [2]: x = torch.randn(1000,1000)\nIn [3]: %time _ = x.sigmoid()\nCPU times: user 29.1 ms, sys: 4.16 ms, total: 33.3 ms\nWall time: 8.63 ms\n\nAVX2 Vectorized sigmoid\nIn [1]: import torch\nIn [2]: x = torch.randn(10000,10000)\nIn [3]: %time _ = x.sigmoid()\nCPU times: user 206 ms, sys: 106 ms, total: 312 ms\nWall time: 78.2 ms\n\nIn [14]: x = torch.randn(1000,1000)\nIn [15]: %time _ = x.sigmoid()\nCPU times: user 179 \u00b5s, sys: 2.95 ms, total: 3.13 ms\nWall time: 858 \u00b5s", "body": "PR introduces AVX2 optimization for sigmoid floats. Issue #4929. The internal benchmark shows ~10x speedup.\r\n\r\n- Added AVX2 vectorized sigmoid using the 8-way vectorized `exp` (`exp256_ps`) in _avx_mathfun.h_. \r\n\r\n- Implemented vector dispatch for sigmoid. Since sigmoid function is defined for floats and doubles only, for now, added preprocessor `#ifdef` to init sigmoid dispatch only for float and double.\r\n\r\n- Vector functions in _THVector.h_ were not called for all of the basic functions in floating point or double only. Changed the `LAB_IMPLEMENT_BASIC_FUNCTION` define in _THTensorMatch.c_ to use `THVector_(NAME)` implementations if the inputs are contiguous. For the functions that do not have vectorized SIMD implementations will use the same default function from _THMath.h_\r\n\r\n\r\n**Benchmark**\r\n\r\nNon-vectorized sigmoid : \r\n```\r\nIn [1]: import torch\r\nIn [2]: x = torch.randn(10000,10000)\r\nIn [3]: %time _ = x.sigmoid()\r\nCPU times: user 2.8 s, sys: 130 ms, total: 2.93 s\r\nWall time: 737 ms\r\n```\r\n```\r\nIn [1]: import torch\r\nIn [2]: x = torch.randn(1000,1000)\r\nIn [3]: %time _ = x.sigmoid()\r\nCPU times: user 29.1 ms, sys: 4.16 ms, total: 33.3 ms\r\nWall time: 8.63 ms\r\n```\r\n\r\nAVX2 Vectorized sigmoid\r\n\r\n```\r\nIn [1]: import torch\r\nIn [2]: x = torch.randn(10000,10000)\r\nIn [3]: %time _ = x.sigmoid()\r\nCPU times: user 206 ms, sys: 106 ms, total: 312 ms\r\nWall time: 78.2 ms\r\n```\r\n\r\n```\r\nIn [14]: x = torch.randn(1000,1000)\r\nIn [15]: %time _ = x.sigmoid()\r\nCPU times: user 179 \u00b5s, sys: 2.95 ms, total: 3.13 ms\r\nWall time: 858 \u00b5s\r\n```"}