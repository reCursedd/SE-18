{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166197268", "pull_request_review_id": 94236717, "id": 166197268, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjE5NzI2OA==", "diff_hunk": "@@ -3629,22 +3629,32 @@ TENSOR_IMPLEMENT_LOGICAL(ne,!=)\n     ptrdiff_t r_Size = THTensor_(nElement)(r_);               \\\n     int r_Contig = THTensor_(isContiguous)(r_);               \\\n     int tContig = THTensor_(isContiguous)(t);                 \\\n-    int inOMP = omp_in_parallel();                            \\\n-    if( (r_Size > TH_OMP_OVERHEAD_THRESHOLD) && (!inOMP) ){   \\\n-      TH_TENSOR_APPLY2_OMP(r_Size, r_Contig, tContig, real, r_, real, t, *r__data = CFUNC(*t_data););        \\\n-    }                                                                                                        \\\n-    else {                                                                                                   \\\n-      TH_TENSOR_APPLY2(real, r_, real, t, *r__data = CFUNC(*t_data););                                       \\\n-    }                                                                                                        \\\n+    if (r_Contig && tContig) {                                \\\n+      TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(NAME)(r__data, t_data, r__len););                   \\\n+    } else {                                                                                                   \\\n+      int inOMP = omp_in_parallel();                            \\\n+      if( (r_Size > TH_OMP_OVERHEAD_THRESHOLD) && (!inOMP) ){   \\\n+        TH_TENSOR_APPLY2_OMP(r_Size, r_Contig, tContig, real, r_, real, t, *r__data = CFUNC(*t_data););        \\", "path": "aten/src/TH/generic/THTensorMath.c", "position": 17, "original_position": 16, "commit_id": "4b09649650748a8cb42e86009d8fd0c2f34450d5", "original_commit_id": "9325de7e95e2636a28c72ad657ecd1af4f72b390", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "@zdevito `TH_TENSOR_APPLY2_OMP` was recently introduced and is a superset of `TH_TENSOR_APPLY2_CONTIG`, and handles the case where the inputs are non-contiguous as well.\r\nI believe we can now remove `TH_TENSOR_APPLY2_CONTIG` as this case is covered by the `*_OMP` macros.\r\nAlso, as it fuses the contiguous dimensions, the overhead of using it compared to `*CONTIG` should be minimal I think.\r\n\r\nBut, if we remove the `*CONTIG` macros, I think we would need to pay attention to the fact that \r\n[`TH_TENSOR_APPLY2_OMP` tries to force vectorization at the compiler level for contiguous tensors](https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THTensorApply.h#L372-L388), which might make the binaries that we release not work on cpus without vector instructions, as it would lack the runtime dispatch.", "created_at": "2018-02-06T06:33:28Z", "updated_at": "2018-11-23T15:39:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/5010#discussion_r166197268", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5010", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166197268"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5010#discussion_r166197268"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5010"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> <code>TH_TENSOR_APPLY2_OMP</code> was recently introduced and is a superset of <code>TH_TENSOR_APPLY2_CONTIG</code>, and handles the case where the inputs are non-contiguous as well.<br>\nI believe we can now remove <code>TH_TENSOR_APPLY2_CONTIG</code> as this case is covered by the <code>*_OMP</code> macros.<br>\nAlso, as it fuses the contiguous dimensions, the overhead of using it compared to <code>*CONTIG</code> should be minimal I think.</p>\n<p>But, if we remove the <code>*CONTIG</code> macros, I think we would need to pay attention to the fact that<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THTensorApply.h#L372-L388\"><code>TH_TENSOR_APPLY2_OMP</code> tries to force vectorization at the compiler level for contiguous tensors</a>, which might make the binaries that we release not work on cpus without vector instructions, as it would lack the runtime dispatch.</p>", "body_text": "@zdevito TH_TENSOR_APPLY2_OMP was recently introduced and is a superset of TH_TENSOR_APPLY2_CONTIG, and handles the case where the inputs are non-contiguous as well.\nI believe we can now remove TH_TENSOR_APPLY2_CONTIG as this case is covered by the *_OMP macros.\nAlso, as it fuses the contiguous dimensions, the overhead of using it compared to *CONTIG should be minimal I think.\nBut, if we remove the *CONTIG macros, I think we would need to pay attention to the fact that\nTH_TENSOR_APPLY2_OMP tries to force vectorization at the compiler level for contiguous tensors, which might make the binaries that we release not work on cpus without vector instructions, as it would lack the runtime dispatch.", "in_reply_to_id": 166192714}