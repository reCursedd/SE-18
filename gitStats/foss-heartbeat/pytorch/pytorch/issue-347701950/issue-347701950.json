{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10243", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10243/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10243/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10243/events", "html_url": "https://github.com/pytorch/pytorch/issues/10243", "id": 347701950, "node_id": "MDU6SXNzdWUzNDc3MDE5NTA=", "number": 10243, "title": "Initialization can be surprisingly important, and under-rated", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-05T14:16:53Z", "updated_at": "2018-08-05T16:42:54Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>If you have a question or would like help and support, please ask at our<br>\n<a href=\"https://discuss.pytorch.org/\" rel=\"nofollow\">forums</a>.</p>\n<p>If you are submitting a feature request, please preface the title with [feature request].<br>\nIf you are submitting a bug report, please fill in the following details.</p>\n<h2>Issue description</h2>\n<p>Was trying to reproduce a paper. Flatlined at 50% eval accuracy. On a two-class problem (binary cross-entropy).  Spent days pouring over their Tensorflow code, trying to figure out why their code worked, and mine didnt.</p>\n<p>Eventually, turned to weight initialization. Used their initialization formula in my code. Bingo! 70% accuracy, same as theirs.</p>\n<p>I think it might be good to make it easy/straight-forward to give a high quality initialization to LInear's and Embeddings.  The code for the paper I was reproducing draws weights uniformly from <code>[-r, r]</code>, where <code>r</code> is given by the following formula:</p>\n<pre><code>r = factor * sqrt(3) / sqrt(num_inputs)\n</code></pre>\n<p>Where factor is:</p>\n<ul>\n<li>1 for word embeddings</li>\n<li>1.43 with a ReLU activation</li>\n</ul>\n<p>And num_inputs is, for example for an <code>nn.Embedding(V, E)</code>, <code>num_inputs</code> would be <code>V</code>. For an <code>nn.Linear(V, E)</code>, <code>num_inputs</code> would also be <code>V</code>.</p>\n<p>This formula is described also in Tensorflow, <a href=\"https://www.tensorflow.org/api_docs/python/tf/uniform_unit_scaling_initializer\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/uniform_unit_scaling_initializer</a></p>\n<p>A blog-post on the important of initialization, <a href=\"https://plus.google.com/+SoumithChintala/posts/RZfdrRQWL6u\" rel=\"nofollow\">https://plus.google.com/+SoumithChintala/posts/RZfdrRQWL6u</a></p>\n<p>I guess that it is unlikely that a one-size-fits-all approach will work for initialization (it's one more hyper-parameter to tune....), but might be good to at least make it visible/easy to switch between them? (Many people faced with my issue might have never tracked it down, and might have then thrown up their hands and said, \"Well, some bug in pytorch; I'll just switch to Tensorflow...\".)</p>\n<p>Update (before posting):</p>\n<ul>\n<li>looks like there are init functions, at <a href=\"https://pytorch.org/docs/stable/nn.html#torch-nn-init\" rel=\"nofollow\">https://pytorch.org/docs/stable/nn.html#torch-nn-init</a></li>\n<li>however might be worth somehow making it more obvious/easier to use them in the nn.Linear and nn.Embedding api somehow? (currently, you have to read the source-code to find out default initialization; and you also have to kind of 'hack around' to pull the <code>.weight.data</code> out of the insides, to update them).</li>\n</ul>\n<h2>Code example</h2>\n<p>Please try to provide a minimal example to repro the bug.<br>\nError messages and stack traces are also helpful.</p>\n<h2>System Info</h2>\n<p>Please copy and paste the output from our<br>\n<a href=\"https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\" rel=\"nofollow\">environment collection script</a><br>\n(or fill out the checklist below manually).</p>\n<p>You can get the script and run it with:</p>\n<pre><code>wget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n</code></pre>\n<ul>\n<li>PyTorch or Caffe2:</li>\n<li>How you installed PyTorch (conda, pip, source):</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS:</li>\n<li>PyTorch version:</li>\n<li>Python version:</li>\n<li>CUDA/cuDNN version:</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version:</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "If you have a question or would like help and support, please ask at our\nforums.\nIf you are submitting a feature request, please preface the title with [feature request].\nIf you are submitting a bug report, please fill in the following details.\nIssue description\nWas trying to reproduce a paper. Flatlined at 50% eval accuracy. On a two-class problem (binary cross-entropy).  Spent days pouring over their Tensorflow code, trying to figure out why their code worked, and mine didnt.\nEventually, turned to weight initialization. Used their initialization formula in my code. Bingo! 70% accuracy, same as theirs.\nI think it might be good to make it easy/straight-forward to give a high quality initialization to LInear's and Embeddings.  The code for the paper I was reproducing draws weights uniformly from [-r, r], where r is given by the following formula:\nr = factor * sqrt(3) / sqrt(num_inputs)\n\nWhere factor is:\n\n1 for word embeddings\n1.43 with a ReLU activation\n\nAnd num_inputs is, for example for an nn.Embedding(V, E), num_inputs would be V. For an nn.Linear(V, E), num_inputs would also be V.\nThis formula is described also in Tensorflow, https://www.tensorflow.org/api_docs/python/tf/uniform_unit_scaling_initializer\nA blog-post on the important of initialization, https://plus.google.com/+SoumithChintala/posts/RZfdrRQWL6u\nI guess that it is unlikely that a one-size-fits-all approach will work for initialization (it's one more hyper-parameter to tune....), but might be good to at least make it visible/easy to switch between them? (Many people faced with my issue might have never tracked it down, and might have then thrown up their hands and said, \"Well, some bug in pytorch; I'll just switch to Tensorflow...\".)\nUpdate (before posting):\n\nlooks like there are init functions, at https://pytorch.org/docs/stable/nn.html#torch-nn-init\nhowever might be worth somehow making it more obvious/easier to use them in the nn.Linear and nn.Embedding api somehow? (currently, you have to read the source-code to find out default initialization; and you also have to kind of 'hack around' to pull the .weight.data out of the insides, to update them).\n\nCode example\nPlease try to provide a minimal example to repro the bug.\nError messages and stack traces are also helpful.\nSystem Info\nPlease copy and paste the output from our\nenvironment collection script\n(or fill out the checklist below manually).\nYou can get the script and run it with:\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\n# For security purposes, please check the contents of collect_env.py before running it.\npython collect_env.py\n\n\nPyTorch or Caffe2:\nHow you installed PyTorch (conda, pip, source):\nBuild command you used (if compiling from source):\nOS:\nPyTorch version:\nPython version:\nCUDA/cuDNN version:\nGPU models and configuration:\nGCC version (if compiling from source):\nCMake version:\nVersions of any other relevant libraries:", "body": "If you have a question or would like help and support, please ask at our\r\n[forums](https://discuss.pytorch.org/).\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\nIf you are submitting a bug report, please fill in the following details.\r\n\r\n## Issue description\r\n\r\nWas trying to reproduce a paper. Flatlined at 50% eval accuracy. On a two-class problem (binary cross-entropy).  Spent days pouring over their Tensorflow code, trying to figure out why their code worked, and mine didnt.\r\n\r\nEventually, turned to weight initialization. Used their initialization formula in my code. Bingo! 70% accuracy, same as theirs.\r\n\r\nI think it might be good to make it easy/straight-forward to give a high quality initialization to LInear's and Embeddings.  The code for the paper I was reproducing draws weights uniformly from `[-r, r]`, where `r` is given by the following formula:\r\n\r\n    r = factor * sqrt(3) / sqrt(num_inputs)\r\n\r\nWhere factor is:\r\n- 1 for word embeddings\r\n- 1.43 with a ReLU activation\r\n\r\nAnd num_inputs is, for example for an `nn.Embedding(V, E)`, `num_inputs` would be `V`. For an `nn.Linear(V, E)`, `num_inputs` would also be `V`.\r\n\r\nThis formula is described also in Tensorflow, https://www.tensorflow.org/api_docs/python/tf/uniform_unit_scaling_initializer\r\n\r\nA blog-post on the important of initialization, https://plus.google.com/+SoumithChintala/posts/RZfdrRQWL6u\r\n\r\nI guess that it is unlikely that a one-size-fits-all approach will work for initialization (it's one more hyper-parameter to tune....), but might be good to at least make it visible/easy to switch between them? (Many people faced with my issue might have never tracked it down, and might have then thrown up their hands and said, \"Well, some bug in pytorch; I'll just switch to Tensorflow...\".)\r\n\r\nUpdate (before posting):\r\n- looks like there are init functions, at https://pytorch.org/docs/stable/nn.html#torch-nn-init\r\n- however might be worth somehow making it more obvious/easier to use them in the nn.Linear and nn.Embedding api somehow? (currently, you have to read the source-code to find out default initialization; and you also have to kind of 'hack around' to pull the `.weight.data` out of the insides, to update them).\r\n\r\n## Code example\r\n\r\nPlease try to provide a minimal example to repro the bug.\r\nError messages and stack traces are also helpful.\r\n\r\n## System Info\r\nPlease copy and paste the output from our\r\n[environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)\r\n(or fill out the checklist below manually).\r\n\r\nYou can get the script and run it with:\r\n```\r\nwget https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py\r\n# For security purposes, please check the contents of collect_env.py before running it.\r\npython collect_env.py\r\n```\r\n\r\n- PyTorch or Caffe2:\r\n- How you installed PyTorch (conda, pip, source):\r\n- Build command you used (if compiling from source):\r\n- OS:\r\n- PyTorch version:\r\n- Python version:\r\n- CUDA/cuDNN version:\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n"}