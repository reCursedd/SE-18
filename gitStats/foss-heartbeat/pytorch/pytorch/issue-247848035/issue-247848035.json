{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2287", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2287/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2287/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2287/events", "html_url": "https://github.com/pytorch/pytorch/issues/2287", "id": 247848035, "node_id": "MDU6SXNzdWUyNDc4NDgwMzU=", "number": 2287, "title": "Out of memory error when double backward BatchNorm2d", "user": {"login": "hongyi-zhang", "id": 3592602, "node_id": "MDQ6VXNlcjM1OTI2MDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3592602?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hongyi-zhang", "html_url": "https://github.com/hongyi-zhang", "followers_url": "https://api.github.com/users/hongyi-zhang/followers", "following_url": "https://api.github.com/users/hongyi-zhang/following{/other_user}", "gists_url": "https://api.github.com/users/hongyi-zhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/hongyi-zhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hongyi-zhang/subscriptions", "organizations_url": "https://api.github.com/users/hongyi-zhang/orgs", "repos_url": "https://api.github.com/users/hongyi-zhang/repos", "events_url": "https://api.github.com/users/hongyi-zhang/events{/privacy}", "received_events_url": "https://api.github.com/users/hongyi-zhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-08-03T22:01:07Z", "updated_at": "2017-08-14T18:05:45Z", "closed_at": "2017-08-10T18:09:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is probably related to the previous <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"246937771\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2264\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2264/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2264\">#2264</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a><br>\nRunning many iterations of double backward involving BatchNorm2d may cause out of memory error. There may be a memory leak somewhere? Code to reproduce the error:</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass BatchNormTest(nn.Module):\n    def __init__(self, c, num_classes=2):\n        super(BatchNormTest, self).__init__()\n        self.bn = nn.BatchNorm2d(c)\n\n    def forward(self, x):\n        out = x\n        out = self.bn(out)\n        out = F.relu(out)\n        return out\n\nc = 100\nnet = BatchNormTest(c)\nuse_cuda = True\ninputs = Variable(torch.rand(100,c,100,100), requires_grad=True)\nif use_cuda:\n    net.cuda()\n    inputs = inputs.cuda()\n\n# on my server it fails at iteration 14\nT = 100\nfor i in range(T):\n    output = net(inputs)\n    loss1 = torch.sum(output)\n    grad_params = torch.autograd.grad(loss1, inputs, create_graph=True)\n\n    grad = grad_params[0]\n    loss = torch.sum(grad)\n\n    loss.backward()\n    print(i)\n</code></pre>\n<p>The error message:</p>\n<pre><code>THCudaCheck FAIL file=/private/home/hongyizmit/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\nTraceback (most recent call last):\n  File \"models/testres.py\", line 37, in &lt;module&gt;\n    loss.backward()                                                                               \n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/nn/_functions/thnn/batchnorm_double_backwards.py\", line 80, in batchnorm_double_backwards_fn\n    gG = ggI * first_back_grad_input(gO, 1)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/nn/_functions/thnn/batchnorm_double_backwards.py\", line 73, in first_back_grad_input\n    input_sub_mu.div(sigma2_eps) * sum_exclude_dim1(gO * input_sub_mu))\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/variable.py\", line 829, in __mul__\n    return self.mul(other)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/variable.py\", line 339, in mul\n    return Mul.apply(self, other)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py\", line 48, in forward\n    return a.mul(b)\nRuntimeError: cuda runtime error (2) : out of memory at /private/home/hongyizmit/pytorch/torch/lib/THC/generic/THCStorage.cu:66\n</code></pre>", "body_text": "This is probably related to the previous #2264 @gchanan\nRunning many iterations of double backward involving BatchNorm2d may cause out of memory error. There may be a memory leak somewhere? Code to reproduce the error:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass BatchNormTest(nn.Module):\n    def __init__(self, c, num_classes=2):\n        super(BatchNormTest, self).__init__()\n        self.bn = nn.BatchNorm2d(c)\n\n    def forward(self, x):\n        out = x\n        out = self.bn(out)\n        out = F.relu(out)\n        return out\n\nc = 100\nnet = BatchNormTest(c)\nuse_cuda = True\ninputs = Variable(torch.rand(100,c,100,100), requires_grad=True)\nif use_cuda:\n    net.cuda()\n    inputs = inputs.cuda()\n\n# on my server it fails at iteration 14\nT = 100\nfor i in range(T):\n    output = net(inputs)\n    loss1 = torch.sum(output)\n    grad_params = torch.autograd.grad(loss1, inputs, create_graph=True)\n\n    grad = grad_params[0]\n    loss = torch.sum(grad)\n\n    loss.backward()\n    print(i)\n\nThe error message:\nTHCudaCheck FAIL file=/private/home/hongyizmit/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\nTraceback (most recent call last):\n  File \"models/testres.py\", line 37, in <module>\n    loss.backward()                                                                               \n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/nn/_functions/thnn/batchnorm_double_backwards.py\", line 80, in batchnorm_double_backwards_fn\n    gG = ggI * first_back_grad_input(gO, 1)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/nn/_functions/thnn/batchnorm_double_backwards.py\", line 73, in first_back_grad_input\n    input_sub_mu.div(sigma2_eps) * sum_exclude_dim1(gO * input_sub_mu))\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/variable.py\", line 829, in __mul__\n    return self.mul(other)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/variable.py\", line 339, in mul\n    return Mul.apply(self, other)\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py\", line 48, in forward\n    return a.mul(b)\nRuntimeError: cuda runtime error (2) : out of memory at /private/home/hongyizmit/pytorch/torch/lib/THC/generic/THCStorage.cu:66", "body": "This is probably related to the previous #2264 @gchanan \r\nRunning many iterations of double backward involving BatchNorm2d may cause out of memory error. There may be a memory leak somewhere? Code to reproduce the error:\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass BatchNormTest(nn.Module):\r\n    def __init__(self, c, num_classes=2):\r\n        super(BatchNormTest, self).__init__()\r\n        self.bn = nn.BatchNorm2d(c)\r\n\r\n    def forward(self, x):\r\n        out = x\r\n        out = self.bn(out)\r\n        out = F.relu(out)\r\n        return out\r\n\r\nc = 100\r\nnet = BatchNormTest(c)\r\nuse_cuda = True\r\ninputs = Variable(torch.rand(100,c,100,100), requires_grad=True)\r\nif use_cuda:\r\n    net.cuda()\r\n    inputs = inputs.cuda()\r\n\r\n# on my server it fails at iteration 14\r\nT = 100\r\nfor i in range(T):\r\n    output = net(inputs)\r\n    loss1 = torch.sum(output)\r\n    grad_params = torch.autograd.grad(loss1, inputs, create_graph=True)\r\n\r\n    grad = grad_params[0]\r\n    loss = torch.sum(grad)\r\n\r\n    loss.backward()\r\n    print(i)\r\n```\r\nThe error message:\r\n```\r\nTHCudaCheck FAIL file=/private/home/hongyizmit/pytorch/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\r\nTraceback (most recent call last):\r\n  File \"models/testres.py\", line 37, in <module>\r\n    loss.backward()                                                                               \r\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\r\n    variables, grad_variables, retain_graph)\r\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/nn/_functions/thnn/batchnorm_double_backwards.py\", line 80, in batchnorm_double_backwards_fn\r\n    gG = ggI * first_back_grad_input(gO, 1)\r\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/nn/_functions/thnn/batchnorm_double_backwards.py\", line 73, in first_back_grad_input\r\n    input_sub_mu.div(sigma2_eps) * sum_exclude_dim1(gO * input_sub_mu))\r\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/variable.py\", line 829, in __mul__\r\n    return self.mul(other)\r\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/variable.py\", line 339, in mul\r\n    return Mul.apply(self, other)\r\n  File \"/private/home/hongyizmit/.conda/envs/torchmaster/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py\", line 48, in forward\r\n    return a.mul(b)\r\nRuntimeError: cuda runtime error (2) : out of memory at /private/home/hongyizmit/pytorch/torch/lib/THC/generic/THCStorage.cu:66\r\n```"}