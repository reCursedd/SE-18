{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/320689972", "html_url": "https://github.com/pytorch/pytorch/issues/2287#issuecomment-320689972", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2287", "id": 320689972, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDY4OTk3Mg==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-07T15:06:49Z", "updated_at": "2017-08-07T15:06:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>A couple of notes from early investigations:</p>\n<ol>\n<li>This doesn't seem to be solely a function of batchnorm; e.g. if you replace the relu in your code with a function whose backwards doesn't use the input or output (e.g. sum) it doesn't seem to hit this issue.</li>\n<li>From playing around with the implementation, if we don't save the gradoutput from Backward to  BackwardBackward we free memory correctly.  Of course we need the gradoutput to calculate the BackwardBackward correctly, though.</li>\n</ol>", "body_text": "A couple of notes from early investigations:\n\nThis doesn't seem to be solely a function of batchnorm; e.g. if you replace the relu in your code with a function whose backwards doesn't use the input or output (e.g. sum) it doesn't seem to hit this issue.\nFrom playing around with the implementation, if we don't save the gradoutput from Backward to  BackwardBackward we free memory correctly.  Of course we need the gradoutput to calculate the BackwardBackward correctly, though.", "body": "A couple of notes from early investigations:\r\n1) This doesn't seem to be solely a function of batchnorm; e.g. if you replace the relu in your code with a function whose backwards doesn't use the input or output (e.g. sum) it doesn't seem to hit this issue.\r\n2) From playing around with the implementation, if we don't save the gradoutput from Backward to  BackwardBackward we free memory correctly.  Of course we need the gradoutput to calculate the BackwardBackward correctly, though."}