{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7956", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7956/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7956/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7956/events", "html_url": "https://github.com/pytorch/pytorch/issues/7956", "id": 327843657, "node_id": "MDU6SXNzdWUzMjc4NDM2NTc=", "number": 7956, "title": "[Bug Report] DataParallel can't handle scalar output (PyTorch 0.4.0)", "user": {"login": "Jiaming-Liu", "id": 16099575, "node_id": "MDQ6VXNlcjE2MDk5NTc1", "avatar_url": "https://avatars3.githubusercontent.com/u/16099575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Jiaming-Liu", "html_url": "https://github.com/Jiaming-Liu", "followers_url": "https://api.github.com/users/Jiaming-Liu/followers", "following_url": "https://api.github.com/users/Jiaming-Liu/following{/other_user}", "gists_url": "https://api.github.com/users/Jiaming-Liu/gists{/gist_id}", "starred_url": "https://api.github.com/users/Jiaming-Liu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Jiaming-Liu/subscriptions", "organizations_url": "https://api.github.com/users/Jiaming-Liu/orgs", "repos_url": "https://api.github.com/users/Jiaming-Liu/repos", "events_url": "https://api.github.com/users/Jiaming-Liu/events{/privacy}", "received_events_url": "https://api.github.com/users/Jiaming-Liu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-30T18:16:10Z", "updated_at": "2018-06-01T20:20:40Z", "closed_at": "2018-06-01T20:20:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Seems like <code>torch/nn/parallel/scatter_gather.py &gt; Gather.apply(...)</code> is broken by dim=0 outputs.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.<span class=\"pl-c1\">__version__</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'</span>0.4.0<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">class</span> <span class=\"pl-en\">Foo</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n<span class=\"pl-c1\">...</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n<span class=\"pl-c1\">...</span>         <span class=\"pl-k\">return</span> x.mean() <span class=\"pl-c\"><span class=\"pl-c\">#</span> this gives a scalar output</span>\n<span class=\"pl-c1\">...</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> return x.mean().view(1) # this is a quick fix</span>\n<span class=\"pl-c1\">...</span>     \n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> foo <span class=\"pl-k\">=</span> torch.nn.DataParallel(Foo(),[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>]).cuda()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> foo(x)</pre></div>\n<pre><code>Traceback (most recent call last):\n  File \"&lt;input&gt;\", line 1, in &lt;module&gt;\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 115, in forward\n    return self.gather(outputs, self.output_device)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 127, in gather\n    return gather(outputs, output_device, dim=self.dim)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 68, in gather\n    return gather_map(outputs)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 55, in gather_map\n    return Gather.apply(target_device, dim, *outputs)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in forward\n    ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in &lt;lambda&gt;\n    ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\nRuntimeError: dimension specified as 0 but tensor has no dimensions\n</code></pre>\n<p>Related: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"323106877\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/7568\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/7568/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/7568\">#7568</a></p>", "body_text": "Seems like torch/nn/parallel/scatter_gather.py > Gather.apply(...) is broken by dim=0 outputs.\n>>> import torch\n>>> torch.__version__\n'0.4.0'\n>>> class Foo(torch.nn.Module):\n...     def forward(self, x):\n...         return x.mean() # this gives a scalar output\n...         # return x.mean().view(1) # this is a quick fix\n...     \n>>> foo = torch.nn.DataParallel(Foo(),[0,1]).cuda()\n>>> x = torch.zeros(2,2)\n>>> foo(x)\nTraceback (most recent call last):\n  File \"<input>\", line 1, in <module>\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 115, in forward\n    return self.gather(outputs, self.output_device)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 127, in gather\n    return gather(outputs, output_device, dim=self.dim)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 68, in gather\n    return gather_map(outputs)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 55, in gather_map\n    return Gather.apply(target_device, dim, *outputs)\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in forward\n    ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in <lambda>\n    ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\nRuntimeError: dimension specified as 0 but tensor has no dimensions\n\nRelated: #7568", "body": "Seems like `torch/nn/parallel/scatter_gather.py > Gather.apply(...)` is broken by dim=0 outputs.\r\n``` python\r\n>>> import torch\r\n>>> torch.__version__\r\n'0.4.0'\r\n>>> class Foo(torch.nn.Module):\r\n...     def forward(self, x):\r\n...         return x.mean() # this gives a scalar output\r\n...         # return x.mean().view(1) # this is a quick fix\r\n...     \r\n>>> foo = torch.nn.DataParallel(Foo(),[0,1]).cuda()\r\n>>> x = torch.zeros(2,2)\r\n>>> foo(x)\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 115, in forward\r\n    return self.gather(outputs, self.output_device)\r\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 127, in gather\r\n    return gather(outputs, output_device, dim=self.dim)\r\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 68, in gather\r\n    return gather_map(outputs)\r\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 55, in gather_map\r\n    return Gather.apply(target_device, dim, *outputs)\r\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in forward\r\n    ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\r\n  File \"/home/?????/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 54, in <lambda>\r\n    ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\r\nRuntimeError: dimension specified as 0 but tensor has no dimensions\r\n```\r\n\r\nRelated: https://github.com/pytorch/pytorch/issues/7568"}