{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/321740600", "html_url": "https://github.com/pytorch/pytorch/issues/2384#issuecomment-321740600", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2384", "id": 321740600, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTc0MDYwMA==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-11T06:35:40Z", "updated_at": "2017-08-11T06:35:40Z", "author_association": "COLLABORATOR", "body_html": "<p>Hi,</p>\n<p>By default, only the Variables that you have created with <code>requires_grad=True</code> will have their gradient saved in the <code>.grad</code> field. For all other Variables, for memory efficiency, the gradient is not saved.<br>\nIf you want to save the gradient of your output to be able to inspect it, you can add a <code>output.retain_grad()</code> before calling <code>loss.backward()</code> and the <code>.grad</code> field will be populated as you expect.</p>", "body_text": "Hi,\nBy default, only the Variables that you have created with requires_grad=True will have their gradient saved in the .grad field. For all other Variables, for memory efficiency, the gradient is not saved.\nIf you want to save the gradient of your output to be able to inspect it, you can add a output.retain_grad() before calling loss.backward() and the .grad field will be populated as you expect.", "body": "Hi,\r\n\r\nBy default, only the Variables that you have created with `requires_grad=True` will have their gradient saved in the `.grad` field. For all other Variables, for memory efficiency, the gradient is not saved.\r\nIf you want to save the gradient of your output to be able to inspect it, you can add a `output.retain_grad()` before calling `loss.backward()` and the `.grad` field will be populated as you expect."}