{"url": "https://api.github.com/repos/pytorch/pytorch/issues/943", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/943/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/943/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/943/events", "html_url": "https://github.com/pytorch/pytorch/issues/943", "id": 212303847, "node_id": "MDU6SXNzdWUyMTIzMDM4NDc=", "number": 943, "title": "nn.Embedding with max_norm acting strange?", "user": {"login": "makarandtapaswi", "id": 854232, "node_id": "MDQ6VXNlcjg1NDIzMg==", "avatar_url": "https://avatars1.githubusercontent.com/u/854232?v=4", "gravatar_id": "", "url": "https://api.github.com/users/makarandtapaswi", "html_url": "https://github.com/makarandtapaswi", "followers_url": "https://api.github.com/users/makarandtapaswi/followers", "following_url": "https://api.github.com/users/makarandtapaswi/following{/other_user}", "gists_url": "https://api.github.com/users/makarandtapaswi/gists{/gist_id}", "starred_url": "https://api.github.com/users/makarandtapaswi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/makarandtapaswi/subscriptions", "organizations_url": "https://api.github.com/users/makarandtapaswi/orgs", "repos_url": "https://api.github.com/users/makarandtapaswi/repos", "events_url": "https://api.github.com/users/makarandtapaswi/events{/privacy}", "received_events_url": "https://api.github.com/users/makarandtapaswi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 491934870, "node_id": "MDU6TGFiZWw0OTE5MzQ4NzA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/dependency%20bug", "name": "dependency bug", "color": "b60205", "default": false}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2017-03-07T01:34:07Z", "updated_at": "2017-03-12T17:31:41Z", "closed_at": "2017-03-12T17:31:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi,</p>\n<p>I was trying to use the max-norm feature of  <code>nn.Embedding</code> and noticed that it seems to change the way indexing works.</p>\n<p>So, this is correct:</p>\n<div class=\"highlight highlight-source-python\"><pre>A <span class=\"pl-k\">=</span> nn.Embedding(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">5</span>)\n<span class=\"pl-c1\">print</span> A(Variable(torch.LongTensor([[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]])))</pre></div>\n<pre><code>Variable containing:\n(0 ,.,.) = \n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\n  0.4282  1.8142 -0.3061  0.6822  0.0460\n\n(1 ,.,.) = \n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\n  0.4282  1.8142 -0.3061  0.6822  0.0460\n[torch.FloatTensor of size 2x2x5]\n</code></pre>\n<p>and this happens when using <code>max_norm=1</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>A <span class=\"pl-k\">=</span> nn.Embedding(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-v\">max_norm</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span> A(Variable(torch.LongTensor([[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>], [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>]])))</pre></div>\n<pre><code>Variable containing:\n(0 ,.,.) = \n -0.7178 -0.1059  0.4266 -0.2857  0.4581\n -0.5032  0.1199  0.6023 -0.2505  0.5540\n\n(1 ,.,.) = \n -0.5032  0.1199  0.6023 -0.2505  0.5540\n -0.5032  0.1199  0.6023 -0.2505  0.5540\n[torch.FloatTensor of size 2x2x5]\n</code></pre>\n<p>As seen, the <code>[1, 0]</code>th index seems to load embedding corresponding to 2 instead of 1 as requested.</p>\n<p>Am I misunderstanding something?</p>", "body_text": "Hi,\nI was trying to use the max-norm feature of  nn.Embedding and noticed that it seems to change the way indexing works.\nSo, this is correct:\nA = nn.Embedding(20, 5)\nprint A(Variable(torch.LongTensor([[1, 2], [1, 2]])))\nVariable containing:\n(0 ,.,.) = \n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\n  0.4282  1.8142 -0.3061  0.6822  0.0460\n\n(1 ,.,.) = \n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\n  0.4282  1.8142 -0.3061  0.6822  0.0460\n[torch.FloatTensor of size 2x2x5]\n\nand this happens when using max_norm=1:\nA = nn.Embedding(20, 5, max_norm=1)\nprint A(Variable(torch.LongTensor([[1, 2], [1, 2]])))\nVariable containing:\n(0 ,.,.) = \n -0.7178 -0.1059  0.4266 -0.2857  0.4581\n -0.5032  0.1199  0.6023 -0.2505  0.5540\n\n(1 ,.,.) = \n -0.5032  0.1199  0.6023 -0.2505  0.5540\n -0.5032  0.1199  0.6023 -0.2505  0.5540\n[torch.FloatTensor of size 2x2x5]\n\nAs seen, the [1, 0]th index seems to load embedding corresponding to 2 instead of 1 as requested.\nAm I misunderstanding something?", "body": "Hi,\r\n\r\nI was trying to use the max-norm feature of  `nn.Embedding` and noticed that it seems to change the way indexing works.\r\n\r\nSo, this is correct:\r\n\r\n```python\r\nA = nn.Embedding(20, 5)\r\nprint A(Variable(torch.LongTensor([[1, 2], [1, 2]])))\r\n```\r\n```\r\nVariable containing:\r\n(0 ,.,.) = \r\n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\r\n  0.4282  1.8142 -0.3061  0.6822  0.0460\r\n\r\n(1 ,.,.) = \r\n -0.2591 -0.3065  0.1560 -1.2476 -0.6129\r\n  0.4282  1.8142 -0.3061  0.6822  0.0460\r\n[torch.FloatTensor of size 2x2x5]\r\n```\r\n\r\nand this happens when using `max_norm=1`:\r\n\r\n```python\r\nA = nn.Embedding(20, 5, max_norm=1)\r\nprint A(Variable(torch.LongTensor([[1, 2], [1, 2]])))\r\n```\r\n```\r\nVariable containing:\r\n(0 ,.,.) = \r\n -0.7178 -0.1059  0.4266 -0.2857  0.4581\r\n -0.5032  0.1199  0.6023 -0.2505  0.5540\r\n\r\n(1 ,.,.) = \r\n -0.5032  0.1199  0.6023 -0.2505  0.5540\r\n -0.5032  0.1199  0.6023 -0.2505  0.5540\r\n[torch.FloatTensor of size 2x2x5]\r\n```\r\n\r\nAs seen, the `[1, 0]`th index seems to load embedding corresponding to 2 instead of 1 as requested.\r\n\r\n Am I misunderstanding something?\r\n"}