{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390806026", "html_url": "https://github.com/pytorch/pytorch/issues/7615#issuecomment-390806026", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7615", "id": 390806026, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDgwNjAyNg==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-21T22:49:44Z", "updated_at": "2018-05-21T22:50:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9083746\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/avmgithub\">@avmgithub</a> - Thank you for this! I think, for now, that PR will be a bandaid solution. I suggest we do that now, but in general this means that I'll need to abstract the scheduler away and move this to a more central location. I can do that once we resolve this fire. It also sounds like it doesn't even work with the standard scheduler on your end, which is very bad.</p>\n<p>I'm still in the process of updating tbb within third_party. Did you have any luck using this newer version? Ideally Intel just fixes this on their end, but I think the powerpc component is a community contribution to begin with.</p>\n<p>--</p>\n<p>I just checked and indeed now have 8 available on the VM. I now also see strange behavior for OMP_NUM_THREADS=4, in particular python simply gets stuck unless this variable is stuck to 1 or 2.</p>", "body_text": "@avmgithub - Thank you for this! I think, for now, that PR will be a bandaid solution. I suggest we do that now, but in general this means that I'll need to abstract the scheduler away and move this to a more central location. I can do that once we resolve this fire. It also sounds like it doesn't even work with the standard scheduler on your end, which is very bad.\nI'm still in the process of updating tbb within third_party. Did you have any luck using this newer version? Ideally Intel just fixes this on their end, but I think the powerpc component is a community contribution to begin with.\n--\nI just checked and indeed now have 8 available on the VM. I now also see strange behavior for OMP_NUM_THREADS=4, in particular python simply gets stuck unless this variable is stuck to 1 or 2.", "body": "@avmgithub - Thank you for this! I think, for now, that PR will be a bandaid solution. I suggest we do that now, but in general this means that I'll need to abstract the scheduler away and move this to a more central location. I can do that once we resolve this fire. It also sounds like it doesn't even work with the standard scheduler on your end, which is very bad.\r\n\r\nI'm still in the process of updating tbb within third_party. Did you have any luck using this newer version? Ideally Intel just fixes this on their end, but I think the powerpc component is a community contribution to begin with.\r\n\r\n--\r\n\r\nI just checked and indeed now have 8 available on the VM. I now also see strange behavior for OMP_NUM_THREADS=4, in particular python simply gets stuck unless this variable is stuck to 1 or 2.\r\n\r\n"}