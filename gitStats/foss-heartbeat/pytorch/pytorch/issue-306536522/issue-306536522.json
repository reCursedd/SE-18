{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5878", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5878/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5878/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5878/events", "html_url": "https://github.com/pytorch/pytorch/issues/5878", "id": 306536522, "node_id": "MDU6SXNzdWUzMDY1MzY1MjI=", "number": 5878, "title": "Cannot backward on LSTM in evaluation mode on CUDA", "user": {"login": "daemon", "id": 6188572, "node_id": "MDQ6VXNlcjYxODg1NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6188572?v=4", "gravatar_id": "", "url": "https://api.github.com/users/daemon", "html_url": "https://github.com/daemon", "followers_url": "https://api.github.com/users/daemon/followers", "following_url": "https://api.github.com/users/daemon/following{/other_user}", "gists_url": "https://api.github.com/users/daemon/gists{/gist_id}", "starred_url": "https://api.github.com/users/daemon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/daemon/subscriptions", "organizations_url": "https://api.github.com/users/daemon/orgs", "repos_url": "https://api.github.com/users/daemon/repos", "events_url": "https://api.github.com/users/daemon/events{/privacy}", "received_events_url": "https://api.github.com/users/daemon/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-19T16:29:29Z", "updated_at": "2018-03-19T17:45:22Z", "closed_at": "2018-03-19T17:12:48Z", "author_association": "NONE", "body_html": "<ul>\n<li>use case: dynamic evaluation</li>\n<li>compiled from source 0.4.0, commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/5014adfe2f63c7e30a32f8205a85bced2d59de72/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/5014adfe2f63c7e30a32f8205a85bced2d59de72\"><tt>5014adf</tt></a></li>\n<li>CUDA 8.0.61</li>\n</ul>\n<p>I'm not sure if this is a PyTorch issue or a cuDNN one: you can't call <code>backward</code> on an output produced by LSTMs on CUDA if the model is in evaluation mode. In other words, the following code snippet doesn't run:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.autograd <span class=\"pl-k\">as</span> ag\n\nmodel <span class=\"pl-k\">=</span> nn.LSTM(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\nmodel.cuda()\nx <span class=\"pl-k\">=</span> ag.Variable(torch.ones(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>).cuda())\nlabel <span class=\"pl-k\">=</span> ag.Variable(torch.zeros(<span class=\"pl-c1\">1</span>).long().cuda())\n\nmodel.eval()\nmodel.zero_grad()\ny, _ <span class=\"pl-k\">=</span> model(x)\nloss <span class=\"pl-k\">=</span> torch.nn.functional.nll_loss(y[<span class=\"pl-c1\">0</span>], label)\nloss.backward()</pre></div>\n<pre><code>Traceback (most recent call last):\n  File \"test.py\", line 13, in &lt;module&gt;\n    loss.backward()\n  File \"/home/r33tang/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 120, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/r33tang/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 81, in backward\n    variables, grad_variables, retain_graph, create_graph)\nRuntimeError: backward_input can only be called in training mode\n</code></pre>\n<p>However, it works on my other machine as expected (commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/c65bd6660e1f9da92c331382ab73c1ca87820f3e/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/c65bd6660e1f9da92c331382ab73c1ca87820f3e\"><tt>c65bd66</tt></a>, 0.4.0), so this may suggest that PyTorch is not at fault.</p>", "body_text": "use case: dynamic evaluation\ncompiled from source 0.4.0, commit 5014adf\nCUDA 8.0.61\n\nI'm not sure if this is a PyTorch issue or a cuDNN one: you can't call backward on an output produced by LSTMs on CUDA if the model is in evaluation mode. In other words, the following code snippet doesn't run:\nimport torch\nimport torch.nn as nn\nimport torch.autograd as ag\n\nmodel = nn.LSTM(1, 1, 1)\nmodel.cuda()\nx = ag.Variable(torch.ones(1, 1, 1).cuda())\nlabel = ag.Variable(torch.zeros(1).long().cuda())\n\nmodel.eval()\nmodel.zero_grad()\ny, _ = model(x)\nloss = torch.nn.functional.nll_loss(y[0], label)\nloss.backward()\nTraceback (most recent call last):\n  File \"test.py\", line 13, in <module>\n    loss.backward()\n  File \"/home/r33tang/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 120, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/home/r33tang/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 81, in backward\n    variables, grad_variables, retain_graph, create_graph)\nRuntimeError: backward_input can only be called in training mode\n\nHowever, it works on my other machine as expected (commit c65bd66, 0.4.0), so this may suggest that PyTorch is not at fault.", "body": "- use case: dynamic evaluation\r\n- compiled from source 0.4.0, commit 5014adf\r\n- CUDA 8.0.61\r\n\r\nI'm not sure if this is a PyTorch issue or a cuDNN one: you can't call `backward` on an output produced by LSTMs on CUDA if the model is in evaluation mode. In other words, the following code snippet doesn't run:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.autograd as ag\r\n\r\nmodel = nn.LSTM(1, 1, 1)\r\nmodel.cuda()\r\nx = ag.Variable(torch.ones(1, 1, 1).cuda())\r\nlabel = ag.Variable(torch.zeros(1).long().cuda())\r\n\r\nmodel.eval()\r\nmodel.zero_grad()\r\ny, _ = model(x)\r\nloss = torch.nn.functional.nll_loss(y[0], label)\r\nloss.backward()\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 13, in <module>\r\n    loss.backward()\r\n  File \"/home/r33tang/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 120, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/r33tang/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 81, in backward\r\n    variables, grad_variables, retain_graph, create_graph)\r\nRuntimeError: backward_input can only be called in training mode\r\n```\r\n\r\nHowever, it works on my other machine as expected (commit c65bd66, 0.4.0), so this may suggest that PyTorch is not at fault."}