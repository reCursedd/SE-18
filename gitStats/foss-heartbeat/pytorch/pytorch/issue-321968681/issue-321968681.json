{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7469", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7469/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7469/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7469/events", "html_url": "https://github.com/pytorch/pytorch/issues/7469", "id": 321968681, "node_id": "MDU6SXNzdWUzMjE5Njg2ODE=", "number": 7469, "title": "Better support for adding zero-filled sparse tensors", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-10T15:13:15Z", "updated_at": "2018-05-18T15:50:54Z", "closed_at": "2018-05-18T15:50:54Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Problem</h3>\n<pre><code>import torch\nimport torch.nn as nn\nembed = nn.Embedding(101, 20, padding_idx=0, sparse=True,)\nparam = list(embed.parameters())[0]\n\nx = torch.LongTensor([0, 0])\nx_embed = embed(x)\ny = torch.sum(x_embed)\ny.backward()\n\nx = torch.ones(2).long()\nx_embed = embed(x)\ny = torch.sum(x_embed)\ny.backward()\n</code></pre>\n<p>The first <code>y.backward()</code> results in a gradient (param.grad) of size <code>(101, 20)</code> of all zeros. This is represented in PyTorch as a sparse tensor of size <code>(101, 20)</code>  with empty indices and empty values and <code>(dimI, dimV) = (2, 0)</code>.</p>\n<pre><code>torch.sparse.FloatTensor of size (101,20) with indices:\ntensor([], dtype=torch.int64)\nand values:\ntensor([])\n</code></pre>\n<p>The second <code>y.backward()</code> results in a gradient of size <code>(101, 20)</code> of not all zeros. This is represented in PyTorch as a sparse tensor of size <code>(101, 20)</code> with <code>(dimI, dimV) = (1, 1)</code>:</p>\n<pre><code>torch.sparse.FloatTensor of size (101,20) with indices:\ntensor([[ 1,  1]])\nand values:\ntensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n</code></pre>\n<p>Because both <code>y.backward()</code> update the grad of <code>param</code>, the two gradients are summed and fail because <code>(dimI, dimV)</code> of the two sparse tensors don't match.</p>\n<h3>Possible solutions</h3>\n<p>Ignore <code>(dimI, dimV)</code> when adding a sparse tensor that is all zeros to another sparse tensor as long as the share the same <code>size()</code>. This is the \"each size has a unique zero-filled sparse tensor\" solution.</p>\n<p>Stick with the invariant that <code>(dimI, dimV)</code> of sparse tensors have to be the same for them to be added, and say that the gradient from the first <code>y.backward()</code> should have indices of size (1, 0) and values of size (0, 0), based on the shape of the input to embedding. This is the \"each (size, dimI, dimV) tuple defines a unique zero-filled sparse tensor\" solution.</p>", "body_text": "Problem\nimport torch\nimport torch.nn as nn\nembed = nn.Embedding(101, 20, padding_idx=0, sparse=True,)\nparam = list(embed.parameters())[0]\n\nx = torch.LongTensor([0, 0])\nx_embed = embed(x)\ny = torch.sum(x_embed)\ny.backward()\n\nx = torch.ones(2).long()\nx_embed = embed(x)\ny = torch.sum(x_embed)\ny.backward()\n\nThe first y.backward() results in a gradient (param.grad) of size (101, 20) of all zeros. This is represented in PyTorch as a sparse tensor of size (101, 20)  with empty indices and empty values and (dimI, dimV) = (2, 0).\ntorch.sparse.FloatTensor of size (101,20) with indices:\ntensor([], dtype=torch.int64)\nand values:\ntensor([])\n\nThe second y.backward() results in a gradient of size (101, 20) of not all zeros. This is represented in PyTorch as a sparse tensor of size (101, 20) with (dimI, dimV) = (1, 1):\ntorch.sparse.FloatTensor of size (101,20) with indices:\ntensor([[ 1,  1]])\nand values:\ntensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n\nBecause both y.backward() update the grad of param, the two gradients are summed and fail because (dimI, dimV) of the two sparse tensors don't match.\nPossible solutions\nIgnore (dimI, dimV) when adding a sparse tensor that is all zeros to another sparse tensor as long as the share the same size(). This is the \"each size has a unique zero-filled sparse tensor\" solution.\nStick with the invariant that (dimI, dimV) of sparse tensors have to be the same for them to be added, and say that the gradient from the first y.backward() should have indices of size (1, 0) and values of size (0, 0), based on the shape of the input to embedding. This is the \"each (size, dimI, dimV) tuple defines a unique zero-filled sparse tensor\" solution.", "body": "### Problem\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nembed = nn.Embedding(101, 20, padding_idx=0, sparse=True,)\r\nparam = list(embed.parameters())[0]\r\n\r\nx = torch.LongTensor([0, 0])\r\nx_embed = embed(x)\r\ny = torch.sum(x_embed)\r\ny.backward()\r\n\r\nx = torch.ones(2).long()\r\nx_embed = embed(x)\r\ny = torch.sum(x_embed)\r\ny.backward()\r\n```\r\nThe first `y.backward()` results in a gradient (param.grad) of size `(101, 20)` of all zeros. This is represented in PyTorch as a sparse tensor of size `(101, 20)`  with empty indices and empty values and `(dimI, dimV) = (2, 0)`.\r\n```\r\ntorch.sparse.FloatTensor of size (101,20) with indices:\r\ntensor([], dtype=torch.int64)\r\nand values:\r\ntensor([])\r\n```\r\n\r\nThe second `y.backward()` results in a gradient of size `(101, 20)` of not all zeros. This is represented in PyTorch as a sparse tensor of size `(101, 20)` with `(dimI, dimV) = (1, 1)`:\r\n```\r\ntorch.sparse.FloatTensor of size (101,20) with indices:\r\ntensor([[ 1,  1]])\r\nand values:\r\ntensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\r\n          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\r\n        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\r\n          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\r\n```\r\n\r\nBecause both `y.backward()` update the grad of `param`, the two gradients are summed and fail because `(dimI, dimV)` of the two sparse tensors don't match.\r\n\r\n### Possible solutions\r\n\r\nIgnore `(dimI, dimV)` when adding a sparse tensor that is all zeros to another sparse tensor as long as the share the same `size()`. This is the \"each size has a unique zero-filled sparse tensor\" solution.\r\n\r\nStick with the invariant that `(dimI, dimV)` of sparse tensors have to be the same for them to be added, and say that the gradient from the first `y.backward()` should have indices of size (1, 0) and values of size (0, 0), based on the shape of the input to embedding. This is the \"each (size, dimI, dimV) tuple defines a unique zero-filled sparse tensor\" solution.\r\n"}