{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13222", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13222/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13222/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13222/events", "html_url": "https://github.com/pytorch/pytorch/issues/13222", "id": 374725795, "node_id": "MDU6SXNzdWUzNzQ3MjU3OTU=", "number": 13222, "title": "Memory inefficient in batched matmul when requiring gradients", "user": {"login": "xuanqing94", "id": 8935605, "node_id": "MDQ6VXNlcjg5MzU2MDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/8935605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuanqing94", "html_url": "https://github.com/xuanqing94", "followers_url": "https://api.github.com/users/xuanqing94/followers", "following_url": "https://api.github.com/users/xuanqing94/following{/other_user}", "gists_url": "https://api.github.com/users/xuanqing94/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuanqing94/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuanqing94/subscriptions", "organizations_url": "https://api.github.com/users/xuanqing94/orgs", "repos_url": "https://api.github.com/users/xuanqing94/repos", "events_url": "https://api.github.com/users/xuanqing94/events{/privacy}", "received_events_url": "https://api.github.com/users/xuanqing94/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-28T07:27:00Z", "updated_at": "2018-10-29T17:28:49Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>While implementing the batched matrix multiplication, i noticed that the batched matrix multiplication is not efficient, see the code below</p>\n<pre><code>import torch                                                                                                                                                                             \n                                                                                                                                                                                         \n# Input tensor                                                                                                                                                                           \n## Batch size=8192, dim=512                                                                                                                                                             \nx = torch.FloatTensor(8192, 512).requires_grad_().cuda()                                                                                                                                 \n                                                                                                                                                                                         \nif True:                                                                                                                                                                                 \n    # Batch strategy 1                                                                                                                                                                   \n    x1 = x.view(8192, 8, 1, 64)  # 512 = 8 * 64                                                                                                                                        \n    W1 = torch.FloatTensor(8, 64, 64).cuda()                                                                                                                                             \n    out1 = torch.matmul(x1, W1)  # out: [8192, 8, 1, 64]                                                                                                                                \n                                                                                                                                                                                         \n    print(torch.cuda.memory_allocated())    # 1107427328                                                                                                                                                \n                                                                                                                                                                                         \nif False:                                                                                                                                                                                \n    # Batch strategy 2                                                                                                                                                                   \n    x2 = x.view(8192, 1, 512)  # add one dimension for batch matmul                                                                                                                      \n    W2 = torch.FloatTensor(512, 512).cuda()  # larger than W1                                                                                                                            \n    # out: [8192, 1, 1024] # the same number of elements as out1                                                                                                                                   \n    out2 = torch.matmul(x2, W2)                                                                                                                                                          \n    print(torch.cuda.memory_allocated())  # 34603008\n</code></pre>\n<p>However, it turns out that Batch strategy 2 has less memory cost despite that W2 is larger than Batch strategy 1. And everything else are the same (x1, x2 have same number of elements, also out1, out2).</p>\n<p>I also found that by removing the requires_grad_() the memory costs are similar (~33685504).</p>\n<p>What\u2019s the possible reason for that? (Not sure if this is a real issue, so I also posted on the forum).</p>", "body_text": "While implementing the batched matrix multiplication, i noticed that the batched matrix multiplication is not efficient, see the code below\nimport torch                                                                                                                                                                             \n                                                                                                                                                                                         \n# Input tensor                                                                                                                                                                           \n## Batch size=8192, dim=512                                                                                                                                                             \nx = torch.FloatTensor(8192, 512).requires_grad_().cuda()                                                                                                                                 \n                                                                                                                                                                                         \nif True:                                                                                                                                                                                 \n    # Batch strategy 1                                                                                                                                                                   \n    x1 = x.view(8192, 8, 1, 64)  # 512 = 8 * 64                                                                                                                                        \n    W1 = torch.FloatTensor(8, 64, 64).cuda()                                                                                                                                             \n    out1 = torch.matmul(x1, W1)  # out: [8192, 8, 1, 64]                                                                                                                                \n                                                                                                                                                                                         \n    print(torch.cuda.memory_allocated())    # 1107427328                                                                                                                                                \n                                                                                                                                                                                         \nif False:                                                                                                                                                                                \n    # Batch strategy 2                                                                                                                                                                   \n    x2 = x.view(8192, 1, 512)  # add one dimension for batch matmul                                                                                                                      \n    W2 = torch.FloatTensor(512, 512).cuda()  # larger than W1                                                                                                                            \n    # out: [8192, 1, 1024] # the same number of elements as out1                                                                                                                                   \n    out2 = torch.matmul(x2, W2)                                                                                                                                                          \n    print(torch.cuda.memory_allocated())  # 34603008\n\nHowever, it turns out that Batch strategy 2 has less memory cost despite that W2 is larger than Batch strategy 1. And everything else are the same (x1, x2 have same number of elements, also out1, out2).\nI also found that by removing the requires_grad_() the memory costs are similar (~33685504).\nWhat\u2019s the possible reason for that? (Not sure if this is a real issue, so I also posted on the forum).", "body": "While implementing the batched matrix multiplication, i noticed that the batched matrix multiplication is not efficient, see the code below\r\n\r\n```\r\nimport torch                                                                                                                                                                             \r\n                                                                                                                                                                                         \r\n# Input tensor                                                                                                                                                                           \r\n## Batch size=8192, dim=512                                                                                                                                                             \r\nx = torch.FloatTensor(8192, 512).requires_grad_().cuda()                                                                                                                                 \r\n                                                                                                                                                                                         \r\nif True:                                                                                                                                                                                 \r\n    # Batch strategy 1                                                                                                                                                                   \r\n    x1 = x.view(8192, 8, 1, 64)  # 512 = 8 * 64                                                                                                                                        \r\n    W1 = torch.FloatTensor(8, 64, 64).cuda()                                                                                                                                             \r\n    out1 = torch.matmul(x1, W1)  # out: [8192, 8, 1, 64]                                                                                                                                \r\n                                                                                                                                                                                         \r\n    print(torch.cuda.memory_allocated())    # 1107427328                                                                                                                                                \r\n                                                                                                                                                                                         \r\nif False:                                                                                                                                                                                \r\n    # Batch strategy 2                                                                                                                                                                   \r\n    x2 = x.view(8192, 1, 512)  # add one dimension for batch matmul                                                                                                                      \r\n    W2 = torch.FloatTensor(512, 512).cuda()  # larger than W1                                                                                                                            \r\n    # out: [8192, 1, 1024] # the same number of elements as out1                                                                                                                                   \r\n    out2 = torch.matmul(x2, W2)                                                                                                                                                          \r\n    print(torch.cuda.memory_allocated())  # 34603008\r\n```\r\nHowever, it turns out that Batch strategy 2 has less memory cost despite that W2 is larger than Batch strategy 1. And everything else are the same (x1, x2 have same number of elements, also out1, out2).\r\n\r\nI also found that by removing the requires_grad_() the memory costs are similar (~33685504).\r\n\r\nWhat\u2019s the possible reason for that? (Not sure if this is a real issue, so I also posted on the forum)."}