{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169487933", "pull_request_review_id": 98022124, "id": 169487933, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2OTQ4NzkzMw==", "diff_hunk": "@@ -336,23 +340,69 @@ Tensor & squeeze_(Tensor& self, int64_t dim) {\n   int64_t dims = self.dim();\n   dim = maybe_wrap_dim(dim, self.dim());\n \n+  if (self.is_sparse()) {\n+    throw std::runtime_error(\"NYI: sparse squeeze_\");\n+  }\n+\n   if (dims == 0 || self.sizes()[dim] != 1) {\n     return self.as_strided_(self.sizes().vec(), self.strides().vec());\n   }\n   auto g = inferSqueezeGeometry(self, dim);\n   return self.as_strided_(std::get<0>(g), std::get<1>(g));\n }\n \n+static inline Tensor & sparse_unsqueeze_(Tensor& self, int64_t dim) {\n+  std::vector<int64_t> sizes(self.sizes());\n+  sizes.insert(sizes.begin() + dim, 1);", "path": "aten/src/ATen/native/TensorShape.cpp", "position": 28, "original_position": 28, "commit_id": "47bb5314ade4a38a79788df834b801a3022ff2a2", "original_commit_id": "47bb5314ade4a38a79788df834b801a3022ff2a2", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Never mind. I guess that's done in the wrap dim logic.", "created_at": "2018-02-20T22:56:50Z", "updated_at": "2018-11-23T15:39:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/5236#discussion_r169487933", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5236", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169487933"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5236#discussion_r169487933"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5236"}}, "body_html": "<p>Never mind. I guess that's done in the wrap dim logic.</p>", "body_text": "Never mind. I guess that's done in the wrap dim logic.", "in_reply_to_id": 169487770}