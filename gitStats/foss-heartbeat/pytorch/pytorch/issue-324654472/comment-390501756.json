{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/390501756", "html_url": "https://github.com/pytorch/pytorch/pull/7708#issuecomment-390501756", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7708", "id": 390501756, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDUwMTc1Ng==", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-20T18:33:51Z", "updated_at": "2018-05-20T18:35:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1762463\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/neerajprad\">@neerajprad</a></p>\n<p>It seems a little forceful to always enable grad. One option that seems consistent to me is to inherit <code>grad_enabled</code> from its setting at initialization time</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Distribution</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, ...):\n        <span class=\"pl-c1\">...</span>\n        <span class=\"pl-c1\">self</span>._grad_enabled <span class=\"pl-k\">=</span> torch.autograd.get_grad_enabled()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> what is the actual syntax?</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">lazy_property</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-c1\">...</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__get__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">instance</span>, <span class=\"pl-smi\">obj_type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-k\">if</span> instance <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>\n        <span class=\"pl-k\">with</span> torch.autograd.set_grad_enabled(instance._grad_enabled):\n            value <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.wrapped(instance)\n        <span class=\"pl-c1\">setattr</span>(instance, <span class=\"pl-c1\">self</span>.wrapped.<span class=\"pl-c1\">__name__</span>, value)\n        <span class=\"pl-k\">return</span> value</pre></div>\n<p>However it's unclear whether we should be clever or whether we should simply require strict usage: \"grad_enabled should have a single value throughout the lifetime of a distribution object\". The distributions are intended to be flyweight objects and to be cheap to reconstruct in each grad_enabled context.</p>", "body_text": "cc @neerajprad\nIt seems a little forceful to always enable grad. One option that seems consistent to me is to inherit grad_enabled from its setting at initialization time\nclass Distribution(object):\n    def __init__(self, ...):\n        ...\n        self._grad_enabled = torch.autograd.get_grad_enabled()  # what is the actual syntax?\n\nclass lazy_property(object):\n    ...\n    def __get__(self, instance, obj_type=None):\n        if instance is None:\n            return self\n        with torch.autograd.set_grad_enabled(instance._grad_enabled):\n            value = self.wrapped(instance)\n        setattr(instance, self.wrapped.__name__, value)\n        return value\nHowever it's unclear whether we should be clever or whether we should simply require strict usage: \"grad_enabled should have a single value throughout the lifetime of a distribution object\". The distributions are intended to be flyweight objects and to be cheap to reconstruct in each grad_enabled context.", "body": "cc @neerajprad \r\n\r\nIt seems a little forceful to always enable grad. One option that seems consistent to me is to inherit `grad_enabled` from its setting at initialization time\r\n```py\r\nclass Distribution(object):\r\n    def __init__(self, ...):\r\n        ...\r\n        self._grad_enabled = torch.autograd.get_grad_enabled()  # what is the actual syntax?\r\n\r\nclass lazy_property(object):\r\n    ...\r\n    def __get__(self, instance, obj_type=None):\r\n        if instance is None:\r\n            return self\r\n        with torch.autograd.set_grad_enabled(instance._grad_enabled):\r\n            value = self.wrapped(instance)\r\n        setattr(instance, self.wrapped.__name__, value)\r\n        return value\r\n```\r\nHowever it's unclear whether we should be clever or whether we should simply require strict usage: \"grad_enabled should have a single value throughout the lifetime of a distribution object\". The distributions are intended to be flyweight objects and to be cheap to reconstruct in each grad_enabled context."}