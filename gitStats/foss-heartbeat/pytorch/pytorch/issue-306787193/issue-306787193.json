{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5899", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5899/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5899/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5899/events", "html_url": "https://github.com/pytorch/pytorch/issues/5899", "id": 306787193, "node_id": "MDU6SXNzdWUzMDY3ODcxOTM=", "number": 5899, "title": "How to solve the problem of `RuntimeError: all tensors must be on devices[0]`", "user": {"login": "KaiyuYue", "id": 19852297, "node_id": "MDQ6VXNlcjE5ODUyMjk3", "avatar_url": "https://avatars1.githubusercontent.com/u/19852297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KaiyuYue", "html_url": "https://github.com/KaiyuYue", "followers_url": "https://api.github.com/users/KaiyuYue/followers", "following_url": "https://api.github.com/users/KaiyuYue/following{/other_user}", "gists_url": "https://api.github.com/users/KaiyuYue/gists{/gist_id}", "starred_url": "https://api.github.com/users/KaiyuYue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KaiyuYue/subscriptions", "organizations_url": "https://api.github.com/users/KaiyuYue/orgs", "repos_url": "https://api.github.com/users/KaiyuYue/repos", "events_url": "https://api.github.com/users/KaiyuYue/events{/privacy}", "received_events_url": "https://api.github.com/users/KaiyuYue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-20T09:30:18Z", "updated_at": "2018-03-27T08:27:40Z", "closed_at": "2018-03-27T08:27:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Forgive me, I post this on the <a href=\"https://discuss.pytorch.org/t/how-to-solve-the-problem-of-runtimeerror-all-tensors-must-be-on-devices-0/15198/1\" rel=\"nofollow\">discuss</a> page, but no one help. I encounter the runtime error like:</p>\n<pre><code>Traceback (most recent call last):\n  File \"inference.py\", line 301, in &lt;module&gt;\n    main()\n  File \"inference.py\", line 164, in main\n    inference(test_loader, model)\n  File \"inference.py\", line 195, in inference\n    output = model(input_var)\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 73, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 83, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 67, in parallel_apply\n    raise output\nRuntimeError: all tensors must be on devices[0]\n</code></pre>\n<div class=\"highlight highlight-source-python\"><pre>model <span class=\"pl-k\">=</span> torch.nn.DataParallel(model.cuda(), <span class=\"pl-v\">device_ids</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>])\n\n<span class=\"pl-k\">for</span> i, (<span class=\"pl-c1\">input</span>, target) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(test_loader):\n\ttarget <span class=\"pl-k\">=</span> target.cuda(<span class=\"pl-k\">async</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> in test dataloader, pin_memory = True</span>\n\tinput_var <span class=\"pl-k\">=</span> torch.autograd.Variable(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">volatile</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> (Batch_Size, 10L, 3L, 32L, 224L, 224L)</span>\n\tb, s, c, t, h, w <span class=\"pl-k\">=</span> input_var.size()\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> view in (Batch_Size * 10L, 3L, 32L, 224L, 224L)</span>\n\tinput_var <span class=\"pl-k\">=</span> input_var.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, c, t, h, w)\n\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> forward</span>\n\toutput <span class=\"pl-k\">=</span> model(input_var)</pre></div>\n<p>Thanks.</p>", "body_text": "Forgive me, I post this on the discuss page, but no one help. I encounter the runtime error like:\nTraceback (most recent call last):\n  File \"inference.py\", line 301, in <module>\n    main()\n  File \"inference.py\", line 164, in main\n    inference(test_loader, model)\n  File \"inference.py\", line 195, in inference\n    output = model(input_var)\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 73, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 83, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 67, in parallel_apply\n    raise output\nRuntimeError: all tensors must be on devices[0]\n\nmodel = torch.nn.DataParallel(model.cuda(), device_ids=[0,1,2,3])\n\nfor i, (input, target) in enumerate(test_loader):\n\ttarget = target.cuda(async=True) # in test dataloader, pin_memory = True\n\tinput_var = torch.autograd.Variable(input, volatile=False)\n\n\t# (Batch_Size, 10L, 3L, 32L, 224L, 224L)\n\tb, s, c, t, h, w = input_var.size()\n\t# view in (Batch_Size * 10L, 3L, 32L, 224L, 224L)\n\tinput_var = input_var.view(-1, c, t, h, w)\n\t# forward\n\toutput = model(input_var)\nThanks.", "body": "Forgive me, I post this on the [discuss](https://discuss.pytorch.org/t/how-to-solve-the-problem-of-runtimeerror-all-tensors-must-be-on-devices-0/15198/1) page, but no one help. I encounter the runtime error like:\r\n```\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 301, in <module>\r\n    main()\r\n  File \"inference.py\", line 164, in main\r\n    inference(test_loader, model)\r\n  File \"inference.py\", line 195, in inference\r\n    output = model(input_var)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 73, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 83, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/home/anaconda2/lib/python2.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 67, in parallel_apply\r\n    raise output\r\nRuntimeError: all tensors must be on devices[0]\r\n```\r\n\r\n```python\r\nmodel = torch.nn.DataParallel(model.cuda(), device_ids=[0,1,2,3])\r\n\r\nfor i, (input, target) in enumerate(test_loader):\r\n\ttarget = target.cuda(async=True) # in test dataloader, pin_memory = True\r\n\tinput_var = torch.autograd.Variable(input, volatile=False)\r\n\r\n\t# (Batch_Size, 10L, 3L, 32L, 224L, 224L)\r\n\tb, s, c, t, h, w = input_var.size()\r\n\t# view in (Batch_Size * 10L, 3L, 32L, 224L, 224L)\r\n\tinput_var = input_var.view(-1, c, t, h, w)\r\n\t# forward\r\n\toutput = model(input_var)\r\n```\r\n\r\nThanks."}