{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2878", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2878/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2878/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2878/events", "html_url": "https://github.com/pytorch/pytorch/pull/2878", "id": 261086273, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQzNDc2MTgx", "number": 2878, "title": "Introduce a `reduce` keyword argument for MSELoss", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-09-27T19:12:11Z", "updated_at": "2018-11-23T15:34:54Z", "closed_at": "2017-10-06T14:57:23Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2878", "html_url": "https://github.com/pytorch/pytorch/pull/2878", "diff_url": "https://github.com/pytorch/pytorch/pull/2878.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2878.patch"}, "body_html": "<h3>Summary</h3>\n<p>When <code>reduce</code> is <code>False</code>, MSELoss will return a loss per batch element instead of summing or averaging the losses based on <code>size_average</code>. This was implemented by introducing <code>gradOutput</code> as an argument to the <code>updateGradInput</code> functions of the relevant loss functions. By default, <code>reduce = True</code> to maintain backwards compatibility.</p>\n<p>This is the first step in implementing <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"192143117\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/264\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/264/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/264\">#264</a>, which aims to add a <code>reduce</code> keyword to all loss functions.</p>\n<p>The legacy NN <code>MSELoss</code>es currently ignore the <code>reduce</code> keyword for backwards compatibility (I'm assuming legacy means people don't usually use these).</p>\n<h3>Test Plan</h3>\n<p>Unit tests: <code>test/run_test.sh</code><br>\nI added relevant tests for MSELoss for when <code>reduce = False</code>. I also checked manually using <code>gradcheck</code> for the THNN case and THCUNN case (the unit tests do this as well):</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.autograd import gradcheck\n\nloss = nn.MSELoss(reduce=False)\nx = Variable(torch.randn(2,3,4,5).double(), requires_grad=True)\nt = Variable(torch.randn(2,3,4,5).double(), requires_grad=False)\ngradcheck(loss, (x,t))\n\nx = Variable(torch.randn(2,3,4,5).double().cuda(), requires_grad=True)\nt = Variable(torch.randn(2,3,4,5).double().cuda(), requires_grad=False)\ngradcheck(loss, (x,t))\n</code></pre>", "body_text": "Summary\nWhen reduce is False, MSELoss will return a loss per batch element instead of summing or averaging the losses based on size_average. This was implemented by introducing gradOutput as an argument to the updateGradInput functions of the relevant loss functions. By default, reduce = True to maintain backwards compatibility.\nThis is the first step in implementing #264, which aims to add a reduce keyword to all loss functions.\nThe legacy NN MSELosses currently ignore the reduce keyword for backwards compatibility (I'm assuming legacy means people don't usually use these).\nTest Plan\nUnit tests: test/run_test.sh\nI added relevant tests for MSELoss for when reduce = False. I also checked manually using gradcheck for the THNN case and THCUNN case (the unit tests do this as well):\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.autograd import gradcheck\n\nloss = nn.MSELoss(reduce=False)\nx = Variable(torch.randn(2,3,4,5).double(), requires_grad=True)\nt = Variable(torch.randn(2,3,4,5).double(), requires_grad=False)\ngradcheck(loss, (x,t))\n\nx = Variable(torch.randn(2,3,4,5).double().cuda(), requires_grad=True)\nt = Variable(torch.randn(2,3,4,5).double().cuda(), requires_grad=False)\ngradcheck(loss, (x,t))", "body": "### Summary\r\nWhen `reduce` is `False`, MSELoss will return a loss per batch element instead of summing or averaging the losses based on `size_average`. This was implemented by introducing `gradOutput` as an argument to the `updateGradInput` functions of the relevant loss functions. By default, `reduce = True` to maintain backwards compatibility.\r\n\r\nThis is the first step in implementing #264, which aims to add a `reduce` keyword to all loss functions.\r\n\r\nThe legacy NN `MSELoss`es currently ignore the `reduce` keyword for backwards compatibility (I'm assuming legacy means people don't usually use these).\r\n\r\n### Test Plan\r\nUnit tests: `test/run_test.sh`\r\nI added relevant tests for MSELoss for when `reduce = False`. I also checked manually using `gradcheck` for the THNN case and THCUNN case (the unit tests do this as well):\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nfrom torch.autograd import gradcheck\r\n\r\nloss = nn.MSELoss(reduce=False)\r\nx = Variable(torch.randn(2,3,4,5).double(), requires_grad=True)\r\nt = Variable(torch.randn(2,3,4,5).double(), requires_grad=False)\r\ngradcheck(loss, (x,t))\r\n\r\nx = Variable(torch.randn(2,3,4,5).double().cuda(), requires_grad=True)\r\nt = Variable(torch.randn(2,3,4,5).double().cuda(), requires_grad=False)\r\ngradcheck(loss, (x,t))\r\n```\r\n\r\n\r\n"}