{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3477", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3477/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3477/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3477/events", "html_url": "https://github.com/pytorch/pytorch/issues/3477", "id": 271135218, "node_id": "MDU6SXNzdWUyNzExMzUyMTg=", "number": 3477, "title": "cuda out of memory error when GPU0 memory is fully utilized", "user": {"login": "lucylw", "id": 2721700, "node_id": "MDQ6VXNlcjI3MjE3MDA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2721700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucylw", "html_url": "https://github.com/lucylw", "followers_url": "https://api.github.com/users/lucylw/followers", "following_url": "https://api.github.com/users/lucylw/following{/other_user}", "gists_url": "https://api.github.com/users/lucylw/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucylw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucylw/subscriptions", "organizations_url": "https://api.github.com/users/lucylw/orgs", "repos_url": "https://api.github.com/users/lucylw/repos", "events_url": "https://api.github.com/users/lucylw/events{/privacy}", "received_events_url": "https://api.github.com/users/lucylw/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-11-03T22:27:52Z", "updated_at": "2017-11-06T21:51:03Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I have been experiencing an error on systems with multiple GPUs. When GPU0 is fully utilized by another process, I get <code>RuntimeError: cuda runtime error (2) : out of memory</code>.</p>\n<p>It seems that torch.nn.Module.cuda() transfers data not only to my specified GPU, but also GPU0, whose memory is already being used.</p>\n<p>I can reproduce this error using the below code (memory on GPU0 should be fully utilized by another process):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nseq_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nfeatures <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span>\nhidden_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\n\nmodel <span class=\"pl-k\">=</span> nn.Module()\nmodel.rnn <span class=\"pl-k\">=</span> nn.RNN(<span class=\"pl-v\">input_size</span><span class=\"pl-k\">=</span>features, <span class=\"pl-v\">hidden_size</span><span class=\"pl-k\">=</span>hidden_size, <span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\nmodel.cuda(<span class=\"pl-c1\">5</span>)\n\nX_train <span class=\"pl-k\">=</span> torch.randn(seq_len, batch_size, features)\ny_train <span class=\"pl-k\">=</span> torch.randn(batch_size)\nX_train, y_train <span class=\"pl-k\">=</span> Variable(X_train).cuda(), Variable(y_train).cuda()</pre></div>\n<p>After <code>model.cuda(5)</code>, my nvidia-smi output shows:<br>\n+----------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory  |<br>\n|  GPU       PID   Type   Process name                     Usage       |<br>\n|===========================================|<br>\n|    0     32317      C   python                                      7773MiB |<br>\n|    0     34873      C   python                                       217MiB |<br>\n|    1     41080      C   python                                      7775MiB |<br>\n|    5     34873      C   python                                       289MiB |<br>\n+----------------------------------------------------------+</p>\n<p>I am using GPU5, but the same process 34873 is also using memory on GPU0.</p>\n<p>It looks like in the device class of torch/cuda/<strong>init</strong>.py, the <code>prev_idx</code> is being reset to 0 and then torch._C._cuda_setDevice is setting the device number to 0 upon <strong>exit</strong>.</p>\n<p>torch/cuda/<strong>init</strong>.py:110</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">device</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Context-manager that changes the selected device.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Arguments:</span>\n<span class=\"pl-s\">        idx (int): device index to select. It's a no-op if this argument</span>\n<span class=\"pl-s\">            is negative.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">idx</span>):\n        <span class=\"pl-c1\">self</span>.idx <span class=\"pl-k\">=</span> idx\n        <span class=\"pl-c1\">self</span>.prev_idx <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__enter__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.idx <span class=\"pl-k\">is</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>:\n            <span class=\"pl-k\">return</span>\n        _lazy_init()\n        <span class=\"pl-c1\">self</span>.prev_idx <span class=\"pl-k\">=</span> torch._C._cuda_getDevice()\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.prev_idx <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">self</span>.idx:\n            torch._C._cuda_setDevice(<span class=\"pl-c1\">self</span>.idx)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__exit__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.prev_idx <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">self</span>.idx:\n            torch._C._cuda_setDevice(<span class=\"pl-c1\">self</span>.prev_idx)\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">False</span></pre></div>", "body_text": "I have been experiencing an error on systems with multiple GPUs. When GPU0 is fully utilized by another process, I get RuntimeError: cuda runtime error (2) : out of memory.\nIt seems that torch.nn.Module.cuda() transfers data not only to my specified GPU, but also GPU0, whose memory is already being used.\nI can reproduce this error using the below code (memory on GPU0 should be fully utilized by another process):\nimport torch\n\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nseq_len = 10\nfeatures = 50\nhidden_size = 50\nbatch_size = 32\n\nmodel = nn.Module()\nmodel.rnn = nn.RNN(input_size=features, hidden_size=hidden_size, num_layers=2)\nmodel.cuda(5)\n\nX_train = torch.randn(seq_len, batch_size, features)\ny_train = torch.randn(batch_size)\nX_train, y_train = Variable(X_train).cuda(), Variable(y_train).cuda()\nAfter model.cuda(5), my nvidia-smi output shows:\n+----------------------------------------------------------+\n| Processes:                                                       GPU Memory  |\n|  GPU       PID   Type   Process name                     Usage       |\n|===========================================|\n|    0     32317      C   python                                      7773MiB |\n|    0     34873      C   python                                       217MiB |\n|    1     41080      C   python                                      7775MiB |\n|    5     34873      C   python                                       289MiB |\n+----------------------------------------------------------+\nI am using GPU5, but the same process 34873 is also using memory on GPU0.\nIt looks like in the device class of torch/cuda/init.py, the prev_idx is being reset to 0 and then torch._C._cuda_setDevice is setting the device number to 0 upon exit.\ntorch/cuda/init.py:110\nclass device(object):\n    \"\"\"Context-manager that changes the selected device.\n\n    Arguments:\n        idx (int): device index to select. It's a no-op if this argument\n            is negative.\n    \"\"\"\n\n    def __init__(self, idx):\n        self.idx = idx\n        self.prev_idx = -1\n\n    def __enter__(self):\n        if self.idx is -1:\n            return\n        _lazy_init()\n        self.prev_idx = torch._C._cuda_getDevice()\n        if self.prev_idx != self.idx:\n            torch._C._cuda_setDevice(self.idx)\n\n    def __exit__(self, *args):\n        if self.prev_idx != self.idx:\n            torch._C._cuda_setDevice(self.prev_idx)\n        return False", "body": "I have been experiencing an error on systems with multiple GPUs. When GPU0 is fully utilized by another process, I get `RuntimeError: cuda runtime error (2) : out of memory`. \r\n\r\nIt seems that torch.nn.Module.cuda() transfers data not only to my specified GPU, but also GPU0, whose memory is already being used. \r\n\r\nI can reproduce this error using the below code (memory on GPU0 should be fully utilized by another process):\r\n\r\n```python\r\nimport torch\r\n\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nseq_len = 10\r\nfeatures = 50\r\nhidden_size = 50\r\nbatch_size = 32\r\n\r\nmodel = nn.Module()\r\nmodel.rnn = nn.RNN(input_size=features, hidden_size=hidden_size, num_layers=2)\r\nmodel.cuda(5)\r\n\r\nX_train = torch.randn(seq_len, batch_size, features)\r\ny_train = torch.randn(batch_size)\r\nX_train, y_train = Variable(X_train).cuda(), Variable(y_train).cuda()\r\n```\r\n\r\nAfter `model.cuda(5)`, my nvidia-smi output shows:\r\n+----------------------------------------------------------+\r\n| Processes:                                                       GPU Memory  |\r\n|  GPU       PID   Type   Process name                     Usage       |\r\n|===========================================|\r\n|    0     32317      C   python                                      7773MiB |\r\n|    0     34873      C   python                                       217MiB |\r\n|    1     41080      C   python                                      7775MiB |\r\n|    5     34873      C   python                                       289MiB |\r\n+----------------------------------------------------------+\r\n\r\nI am using GPU5, but the same process 34873 is also using memory on GPU0.\r\n\r\nIt looks like in the device class of torch/cuda/__init__.py, the `prev_idx` is being reset to 0 and then torch._C._cuda_setDevice is setting the device number to 0 upon __exit__.\r\n\r\ntorch/cuda/__init__.py:110\r\n```python\r\nclass device(object):\r\n    \"\"\"Context-manager that changes the selected device.\r\n\r\n    Arguments:\r\n        idx (int): device index to select. It's a no-op if this argument\r\n            is negative.\r\n    \"\"\"\r\n\r\n    def __init__(self, idx):\r\n        self.idx = idx\r\n        self.prev_idx = -1\r\n\r\n    def __enter__(self):\r\n        if self.idx is -1:\r\n            return\r\n        _lazy_init()\r\n        self.prev_idx = torch._C._cuda_getDevice()\r\n        if self.prev_idx != self.idx:\r\n            torch._C._cuda_setDevice(self.idx)\r\n\r\n    def __exit__(self, *args):\r\n        if self.prev_idx != self.idx:\r\n            torch._C._cuda_setDevice(self.prev_idx)\r\n        return False\r\n```"}