{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/341845594", "html_url": "https://github.com/pytorch/pytorch/issues/3477#issuecomment-341845594", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3477", "id": 341845594, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTg0NTU5NA==", "user": {"login": "lucylw", "id": 2721700, "node_id": "MDQ6VXNlcjI3MjE3MDA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2721700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lucylw", "html_url": "https://github.com/lucylw", "followers_url": "https://api.github.com/users/lucylw/followers", "following_url": "https://api.github.com/users/lucylw/following{/other_user}", "gists_url": "https://api.github.com/users/lucylw/gists{/gist_id}", "starred_url": "https://api.github.com/users/lucylw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lucylw/subscriptions", "organizations_url": "https://api.github.com/users/lucylw/orgs", "repos_url": "https://api.github.com/users/lucylw/repos", "events_url": "https://api.github.com/users/lucylw/events{/privacy}", "received_events_url": "https://api.github.com/users/lucylw/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-03T22:46:43Z", "updated_at": "2017-11-03T22:51:15Z", "author_association": "NONE", "body_html": "<p>Even when I specify the cuda device for all my transfers, there is still memory being used on GPU0, e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre>model <span class=\"pl-k\">=</span> nn.Module()\nmodel.rnn <span class=\"pl-k\">=</span> nn.RNN(<span class=\"pl-v\">input_size</span><span class=\"pl-k\">=</span>features, <span class=\"pl-v\">hidden_size</span><span class=\"pl-k\">=</span>hidden_size, <span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\nmodel.cuda(<span class=\"pl-c1\">5</span>)\n\nX_train <span class=\"pl-k\">=</span> torch.randn(seq_len, batch_size, features)\ny_train <span class=\"pl-k\">=</span> torch.randn(batch_size)\nX_train, y_train <span class=\"pl-k\">=</span> Variable(X_train).cuda(<span class=\"pl-c1\">5</span>), Variable(y_train).cuda(<span class=\"pl-c1\">5</span>)</pre></div>\n<p>Is this the right behavior?</p>\n<p>More specifically, <code>model.cuda(5)</code> transfers data onto GPU0 (the default), but the variable transfers do not, the variables seem to only go onto GPU5 when I specify the device number.</p>", "body_text": "Even when I specify the cuda device for all my transfers, there is still memory being used on GPU0, e.g.\nmodel = nn.Module()\nmodel.rnn = nn.RNN(input_size=features, hidden_size=hidden_size, num_layers=2)\nmodel.cuda(5)\n\nX_train = torch.randn(seq_len, batch_size, features)\ny_train = torch.randn(batch_size)\nX_train, y_train = Variable(X_train).cuda(5), Variable(y_train).cuda(5)\nIs this the right behavior?\nMore specifically, model.cuda(5) transfers data onto GPU0 (the default), but the variable transfers do not, the variables seem to only go onto GPU5 when I specify the device number.", "body": "Even when I specify the cuda device for all my transfers, there is still memory being used on GPU0, e.g.\r\n\r\n```python\r\nmodel = nn.Module()\r\nmodel.rnn = nn.RNN(input_size=features, hidden_size=hidden_size, num_layers=2)\r\nmodel.cuda(5)\r\n\r\nX_train = torch.randn(seq_len, batch_size, features)\r\ny_train = torch.randn(batch_size)\r\nX_train, y_train = Variable(X_train).cuda(5), Variable(y_train).cuda(5)\r\n```\r\n\r\nIs this the right behavior?\r\n\r\nMore specifically, `model.cuda(5)` transfers data onto GPU0 (the default), but the variable transfers do not, the variables seem to only go onto GPU5 when I specify the device number."}