{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9621", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9621/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9621/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9621/events", "html_url": "https://github.com/pytorch/pytorch/issues/9621", "id": 343003947, "node_id": "MDU6SXNzdWUzNDMwMDM5NDc=", "number": 9621, "title": "LSTM training crashes Python process for long sequences", "user": {"login": "sairampillai", "id": 11787656, "node_id": "MDQ6VXNlcjExNzg3NjU2", "avatar_url": "https://avatars2.githubusercontent.com/u/11787656?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sairampillai", "html_url": "https://github.com/sairampillai", "followers_url": "https://api.github.com/users/sairampillai/followers", "following_url": "https://api.github.com/users/sairampillai/following{/other_user}", "gists_url": "https://api.github.com/users/sairampillai/gists{/gist_id}", "starred_url": "https://api.github.com/users/sairampillai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sairampillai/subscriptions", "organizations_url": "https://api.github.com/users/sairampillai/orgs", "repos_url": "https://api.github.com/users/sairampillai/repos", "events_url": "https://api.github.com/users/sairampillai/events{/privacy}", "received_events_url": "https://api.github.com/users/sairampillai/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-07-20T07:36:23Z", "updated_at": "2018-08-06T05:10:31Z", "closed_at": "2018-08-06T05:10:31Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Training an LSTM on CPU for long sequences fails with Python process error.<br>\nThe sequence length upto 320 is working fine  but anything above that crashes python process with error:<br>\n<code>Process finished with exit code -1073741571 (0xC00000FD)</code></p>\n<p>The error occurs after the loss function has finished calculating loss.</p>\n<p>Debugging python with the Visual studio JIT debugger shows the following unhandled exception:<br>\n<code>Unhandled exception at 0x00007FFCF104E044 (_C.cp36-win_amd64.pyd) in python3.exe: 0xC00000FD: Stack overflow (parameters: 0x0000000000000001, 0x00000003B1203FE8). occurred</code></p>\n<p>Is this a hardware limitation or can this be solved programatically?</p>\n<h2>Code example</h2>\n<p>The class used: An RNN followed by a linear layer to output class label distributions.<br>\n`class RnnLinearSigmoid(torch.nn.Module):<br>\ndef <strong>init</strong>(self, in_dimension, hidden_dimension, out_dimension, rnn_hidden_layers,<br>\nrnn_activation='relu', batch_size=None, batch_first=False, truncate=5, lstm=False):<br>\nsuper(RnnLinearSigmoid, self).<strong>init</strong>()<br>\nself.in_dimension = in_dimension<br>\nself.hidden_dimension = hidden_dimension<br>\nself.out_dimension = out_dimension<br>\nself.batch_size = 1 if batch_size is None else batch_size<br>\nself.batch_first = batch_first<br>\nself.hidden_layer = rnn_hidden_layers<br>\nself.is_lstm = lstm<br>\nself.truncate = truncate</p>\n<pre><code>    if self.is_lstm:\n        self.rnn = torch.nn.LSTM(input_size=in_dimension, hidden_size=hidden_dimension,\n                                 num_layers=rnn_hidden_layers, batch_first=batch_first)\n    else:\n        self.rnn = torch.nn.RNN(input_size=in_dimension, hidden_size=hidden_dimension,\n                                num_layers=rnn_hidden_layers, nonlinearity=rnn_activation, batch_first=batch_first)\n\n    self.linear_layer = torch.nn.Linear(in_features=hidden_dimension, out_features=out_dimension)\n    self.activation = torch.nn.Softmax()\n\ndef forward(self, input_tensor):\n    if self.is_lstm:\n        rnn_output, hidden = self.rnn(input_tensor)\n    else:\n        hidden = self.init_hidden(self.hidden_layer, self.batch_size, self.hidden_dimension)\n        rnn_output, hidden = self.rnn(input_tensor, hidden)\n\n    linear_output = self.linear_layer(rnn_output[-self.truncate:])\n    activated_output = linear_output\n    return activated_output.view(activated_output.shape[0], self.out_dimension)\n\ndef init_hidden(self, hidden_layers, batch_size, hidden_dimension):\n    return torch.zeros(hidden_layers, batch_size, hidden_dimension, requires_grad=True)`\n</code></pre>\n<p>`def train():</p>\n<pre><code>model = RnnLinearSigmoid(in_dimension=100, hidden_dimension=100, out_dimension=5, rnn_hidden_layers=3, lstm=True)\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nfor epoch in range(10000):\n\n    sequence_length = random.randint(320, 4900)\n    print(sequence_length)\n    input = torch.rand(sequence_length, 1, 100)\n    output = model(input)\n    answer = np.repeat(np.random.randint(low=0, high=5, size=1), 5)\n    answer = torch.from_numpy(answer).long()\n\n    loss = loss_fn(output, answer.detach())\n    loss.backward()\n    optimizer.step()`\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: Could not collect (No)</p>\n<p>OS: Microsoft Windows 10 Enterprise<br>\nGCC version: (MinGW.org GCC-6.3.0-1) 6.3.0<br>\nCMake version: version 3.11.4</p>\n<p>Python version: 3.6<br>\nIs CUDA available: No<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration: GPU 0: GeForce 210<br>\nNvidia driver version: 341.95<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.14.5+mkl)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchvision (0.2.1)<br>\n[conda] Could not collect (NIL)</p>", "body_text": "Issue description\nTraining an LSTM on CPU for long sequences fails with Python process error.\nThe sequence length upto 320 is working fine  but anything above that crashes python process with error:\nProcess finished with exit code -1073741571 (0xC00000FD)\nThe error occurs after the loss function has finished calculating loss.\nDebugging python with the Visual studio JIT debugger shows the following unhandled exception:\nUnhandled exception at 0x00007FFCF104E044 (_C.cp36-win_amd64.pyd) in python3.exe: 0xC00000FD: Stack overflow (parameters: 0x0000000000000001, 0x00000003B1203FE8). occurred\nIs this a hardware limitation or can this be solved programatically?\nCode example\nThe class used: An RNN followed by a linear layer to output class label distributions.\n`class RnnLinearSigmoid(torch.nn.Module):\ndef init(self, in_dimension, hidden_dimension, out_dimension, rnn_hidden_layers,\nrnn_activation='relu', batch_size=None, batch_first=False, truncate=5, lstm=False):\nsuper(RnnLinearSigmoid, self).init()\nself.in_dimension = in_dimension\nself.hidden_dimension = hidden_dimension\nself.out_dimension = out_dimension\nself.batch_size = 1 if batch_size is None else batch_size\nself.batch_first = batch_first\nself.hidden_layer = rnn_hidden_layers\nself.is_lstm = lstm\nself.truncate = truncate\n    if self.is_lstm:\n        self.rnn = torch.nn.LSTM(input_size=in_dimension, hidden_size=hidden_dimension,\n                                 num_layers=rnn_hidden_layers, batch_first=batch_first)\n    else:\n        self.rnn = torch.nn.RNN(input_size=in_dimension, hidden_size=hidden_dimension,\n                                num_layers=rnn_hidden_layers, nonlinearity=rnn_activation, batch_first=batch_first)\n\n    self.linear_layer = torch.nn.Linear(in_features=hidden_dimension, out_features=out_dimension)\n    self.activation = torch.nn.Softmax()\n\ndef forward(self, input_tensor):\n    if self.is_lstm:\n        rnn_output, hidden = self.rnn(input_tensor)\n    else:\n        hidden = self.init_hidden(self.hidden_layer, self.batch_size, self.hidden_dimension)\n        rnn_output, hidden = self.rnn(input_tensor, hidden)\n\n    linear_output = self.linear_layer(rnn_output[-self.truncate:])\n    activated_output = linear_output\n    return activated_output.view(activated_output.shape[0], self.out_dimension)\n\ndef init_hidden(self, hidden_layers, batch_size, hidden_dimension):\n    return torch.zeros(hidden_layers, batch_size, hidden_dimension, requires_grad=True)`\n\n`def train():\nmodel = RnnLinearSigmoid(in_dimension=100, hidden_dimension=100, out_dimension=5, rnn_hidden_layers=3, lstm=True)\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nfor epoch in range(10000):\n\n    sequence_length = random.randint(320, 4900)\n    print(sequence_length)\n    input = torch.rand(sequence_length, 1, 100)\n    output = model(input)\n    answer = np.repeat(np.random.randint(low=0, high=5, size=1), 5)\n    answer = torch.from_numpy(answer).long()\n\n    loss = loss_fn(output, answer.detach())\n    loss.backward()\n    optimizer.step()`\n\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: Could not collect (No)\nOS: Microsoft Windows 10 Enterprise\nGCC version: (MinGW.org GCC-6.3.0-1) 6.3.0\nCMake version: version 3.11.4\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: GeForce 210\nNvidia driver version: 341.95\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip3] numpy (1.14.5+mkl)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.1)\n[conda] Could not collect (NIL)", "body": "## Issue description\r\nTraining an LSTM on CPU for long sequences fails with Python process error. \r\nThe sequence length upto 320 is working fine  but anything above that crashes python process with error:\r\n`Process finished with exit code -1073741571 (0xC00000FD)`\r\n\r\nThe error occurs after the loss function has finished calculating loss.\r\n\r\nDebugging python with the Visual studio JIT debugger shows the following unhandled exception:\r\n`Unhandled exception at 0x00007FFCF104E044 (_C.cp36-win_amd64.pyd) in python3.exe: 0xC00000FD: Stack overflow (parameters: 0x0000000000000001, 0x00000003B1203FE8). occurred`\r\n\r\nIs this a hardware limitation or can this be solved programatically? \r\n\r\n## Code example\r\nThe class used: An RNN followed by a linear layer to output class label distributions.\r\n`class RnnLinearSigmoid(torch.nn.Module):\r\n    def __init__(self, in_dimension, hidden_dimension, out_dimension, rnn_hidden_layers,\r\n                 rnn_activation='relu', batch_size=None, batch_first=False, truncate=5, lstm=False):\r\n        super(RnnLinearSigmoid, self).__init__()\r\n        self.in_dimension = in_dimension\r\n        self.hidden_dimension = hidden_dimension\r\n        self.out_dimension = out_dimension\r\n        self.batch_size = 1 if batch_size is None else batch_size\r\n        self.batch_first = batch_first\r\n        self.hidden_layer = rnn_hidden_layers\r\n        self.is_lstm = lstm\r\n        self.truncate = truncate\r\n\r\n        if self.is_lstm:\r\n            self.rnn = torch.nn.LSTM(input_size=in_dimension, hidden_size=hidden_dimension,\r\n                                     num_layers=rnn_hidden_layers, batch_first=batch_first)\r\n        else:\r\n            self.rnn = torch.nn.RNN(input_size=in_dimension, hidden_size=hidden_dimension,\r\n                                    num_layers=rnn_hidden_layers, nonlinearity=rnn_activation, batch_first=batch_first)\r\n\r\n        self.linear_layer = torch.nn.Linear(in_features=hidden_dimension, out_features=out_dimension)\r\n        self.activation = torch.nn.Softmax()\r\n\r\n    def forward(self, input_tensor):\r\n        if self.is_lstm:\r\n            rnn_output, hidden = self.rnn(input_tensor)\r\n        else:\r\n            hidden = self.init_hidden(self.hidden_layer, self.batch_size, self.hidden_dimension)\r\n            rnn_output, hidden = self.rnn(input_tensor, hidden)\r\n\r\n        linear_output = self.linear_layer(rnn_output[-self.truncate:])\r\n        activated_output = linear_output\r\n        return activated_output.view(activated_output.shape[0], self.out_dimension)\r\n\r\n    def init_hidden(self, hidden_layers, batch_size, hidden_dimension):\r\n        return torch.zeros(hidden_layers, batch_size, hidden_dimension, requires_grad=True)`\r\n\r\n`def train():\r\n\r\n    model = RnnLinearSigmoid(in_dimension=100, hidden_dimension=100, out_dimension=5, rnn_hidden_layers=3, lstm=True)\r\n\r\n    loss_fn = torch.nn.CrossEntropyLoss()\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\r\n    for epoch in range(10000):\r\n\r\n        sequence_length = random.randint(320, 4900)\r\n        print(sequence_length)\r\n        input = torch.rand(sequence_length, 1, 100)\r\n        output = model(input)\r\n        answer = np.repeat(np.random.randint(low=0, high=5, size=1), 5)\r\n        answer = torch.from_numpy(answer).long()\r\n\r\n        loss = loss_fn(output, answer.detach())\r\n        loss.backward()\r\n        optimizer.step()`\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: Could not collect (No)\r\n\r\nOS: Microsoft Windows 10 Enterprise\r\nGCC version: (MinGW.org GCC-6.3.0-1) 6.3.0\r\nCMake version: version 3.11.4\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce 210\r\nNvidia driver version: 341.95\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.5+mkl)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] Could not collect (NIL)\r\n"}