{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5901", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5901/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5901/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5901/events", "html_url": "https://github.com/pytorch/pytorch/issues/5901", "id": 306820786, "node_id": "MDU6SXNzdWUzMDY4MjA3ODY=", "number": 5901, "title": "Train RNN on time series data keeping all time relationships ", "user": {"login": "pancho111203", "id": 9058460, "node_id": "MDQ6VXNlcjkwNTg0NjA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9058460?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pancho111203", "html_url": "https://github.com/pancho111203", "followers_url": "https://api.github.com/users/pancho111203/followers", "following_url": "https://api.github.com/users/pancho111203/following{/other_user}", "gists_url": "https://api.github.com/users/pancho111203/gists{/gist_id}", "starred_url": "https://api.github.com/users/pancho111203/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pancho111203/subscriptions", "organizations_url": "https://api.github.com/users/pancho111203/orgs", "repos_url": "https://api.github.com/users/pancho111203/repos", "events_url": "https://api.github.com/users/pancho111203/events{/privacy}", "received_events_url": "https://api.github.com/users/pancho111203/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-20T11:10:21Z", "updated_at": "2018-03-20T13:23:42Z", "closed_at": "2018-03-20T13:23:42Z", "author_association": "NONE", "body_html": "<p>I want to train a RNN (LSTM) on time series data, doing backpropagation every step and keeping the time relationships from start to end.</p>\n<p>I tried many things without success. If I set seq_len to the number of data points I have, the model trains quickly and keeps temporal relationships, but it only updates weights once per epoch.</p>\n<p>The last thing I tried is setting seq_len and batch_size to 1, and passing the hidden states on every iteration:</p>\n<pre><code>hidden = None\nfor i in range(0, len(X)):\n    single_tick = X[i].view(1, X[i].shape[0], X[i].shape[1])\n    y_pred, hidden = net(single_tick, hidden)\n\n    loss = criterion(y_pred, y[i])\n    loss.backward(retain_graph=True)\n    optimizer.step()\n\n    train_loss_total += loss.data[0]\n</code></pre>\n<p>Note that I set retain_graph to True.</p>\n<p>This kinda works, but it takes an very high amount of time every epoch, to the point where it's almost unusable.</p>\n<p>I want to know what is the common practice for this aparently simple task (keeping time relationships on long datasets). In Keras this works without having to do anything, so I imagine there is no technical limitation, but just a lack of knowledge from my part.</p>\n<p>This is my model:</p>\n<pre><code>class Model(nn.Module):\ndef __init__(self, input_size, num_layers=2, hidden_size=256):\n    super(Model, self).__init__()\n\n    self.input_size = input_size\n    self.num_layers = num_layers\n    self.hidden_size = hidden_size\n\n    self.lstm = nn.LSTM(self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, dropout=0.2, batch_first=True)\n    self.dense = nn.Linear(self.hidden_size, 1)\n    self.activation = nn.Sigmoid()\n\ndef forward(self, x, hidden=None):\n    batch_size = x.shape[0]\n\n    if hidden is None:\n        h0 = Variable(torch.randn(self.num_layers, batch_size, self.hidden_size))\n        c0 = Variable(torch.randn(self.num_layers, batch_size, self.hidden_size))\n    else:\n        (h0, c0) = hidden\n    \n    output, hidden = self.lstm(x, (h0, c0))\n    output = output.view(batch_size, self.hidden_size)\n    output = self.activation(self.dense(output))\n    return output, hidden\n</code></pre>", "body_text": "I want to train a RNN (LSTM) on time series data, doing backpropagation every step and keeping the time relationships from start to end.\nI tried many things without success. If I set seq_len to the number of data points I have, the model trains quickly and keeps temporal relationships, but it only updates weights once per epoch.\nThe last thing I tried is setting seq_len and batch_size to 1, and passing the hidden states on every iteration:\nhidden = None\nfor i in range(0, len(X)):\n    single_tick = X[i].view(1, X[i].shape[0], X[i].shape[1])\n    y_pred, hidden = net(single_tick, hidden)\n\n    loss = criterion(y_pred, y[i])\n    loss.backward(retain_graph=True)\n    optimizer.step()\n\n    train_loss_total += loss.data[0]\n\nNote that I set retain_graph to True.\nThis kinda works, but it takes an very high amount of time every epoch, to the point where it's almost unusable.\nI want to know what is the common practice for this aparently simple task (keeping time relationships on long datasets). In Keras this works without having to do anything, so I imagine there is no technical limitation, but just a lack of knowledge from my part.\nThis is my model:\nclass Model(nn.Module):\ndef __init__(self, input_size, num_layers=2, hidden_size=256):\n    super(Model, self).__init__()\n\n    self.input_size = input_size\n    self.num_layers = num_layers\n    self.hidden_size = hidden_size\n\n    self.lstm = nn.LSTM(self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, dropout=0.2, batch_first=True)\n    self.dense = nn.Linear(self.hidden_size, 1)\n    self.activation = nn.Sigmoid()\n\ndef forward(self, x, hidden=None):\n    batch_size = x.shape[0]\n\n    if hidden is None:\n        h0 = Variable(torch.randn(self.num_layers, batch_size, self.hidden_size))\n        c0 = Variable(torch.randn(self.num_layers, batch_size, self.hidden_size))\n    else:\n        (h0, c0) = hidden\n    \n    output, hidden = self.lstm(x, (h0, c0))\n    output = output.view(batch_size, self.hidden_size)\n    output = self.activation(self.dense(output))\n    return output, hidden", "body": "I want to train a RNN (LSTM) on time series data, doing backpropagation every step and keeping the time relationships from start to end.\r\n\r\nI tried many things without success. If I set seq_len to the number of data points I have, the model trains quickly and keeps temporal relationships, but it only updates weights once per epoch. \r\n\r\nThe last thing I tried is setting seq_len and batch_size to 1, and passing the hidden states on every iteration:\r\n\r\n    hidden = None\r\n    for i in range(0, len(X)):\r\n        single_tick = X[i].view(1, X[i].shape[0], X[i].shape[1])\r\n        y_pred, hidden = net(single_tick, hidden)\r\n\r\n        loss = criterion(y_pred, y[i])\r\n        loss.backward(retain_graph=True)\r\n        optimizer.step()\r\n\r\n        train_loss_total += loss.data[0]\r\n\r\nNote that I set retain_graph to True.\r\n\r\nThis kinda works, but it takes an very high amount of time every epoch, to the point where it's almost unusable.\r\n\r\nI want to know what is the common practice for this aparently simple task (keeping time relationships on long datasets). In Keras this works without having to do anything, so I imagine there is no technical limitation, but just a lack of knowledge from my part.\r\n\r\n\r\nThis is my model:\r\n\r\n    class Model(nn.Module):\r\n    def __init__(self, input_size, num_layers=2, hidden_size=256):\r\n        super(Model, self).__init__()\r\n\r\n        self.input_size = input_size\r\n        self.num_layers = num_layers\r\n        self.hidden_size = hidden_size\r\n\r\n        self.lstm = nn.LSTM(self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, dropout=0.2, batch_first=True)\r\n        self.dense = nn.Linear(self.hidden_size, 1)\r\n        self.activation = nn.Sigmoid()\r\n\r\n    def forward(self, x, hidden=None):\r\n        batch_size = x.shape[0]\r\n\r\n        if hidden is None:\r\n            h0 = Variable(torch.randn(self.num_layers, batch_size, self.hidden_size))\r\n            c0 = Variable(torch.randn(self.num_layers, batch_size, self.hidden_size))\r\n        else:\r\n            (h0, c0) = hidden\r\n        \r\n        output, hidden = self.lstm(x, (h0, c0))\r\n        output = output.view(batch_size, self.hidden_size)\r\n        output = self.activation(self.dense(output))\r\n        return output, hidden\r\n\r\n"}