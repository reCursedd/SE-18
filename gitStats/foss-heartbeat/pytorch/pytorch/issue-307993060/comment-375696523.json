{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/375696523", "html_url": "https://github.com/pytorch/pytorch/issues/5958#issuecomment-375696523", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5958", "id": 375696523, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NTY5NjUyMw==", "user": {"login": "Erotemic", "id": 3186211, "node_id": "MDQ6VXNlcjMxODYyMTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3186211?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Erotemic", "html_url": "https://github.com/Erotemic", "followers_url": "https://api.github.com/users/Erotemic/followers", "following_url": "https://api.github.com/users/Erotemic/following{/other_user}", "gists_url": "https://api.github.com/users/Erotemic/gists{/gist_id}", "starred_url": "https://api.github.com/users/Erotemic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Erotemic/subscriptions", "organizations_url": "https://api.github.com/users/Erotemic/orgs", "repos_url": "https://api.github.com/users/Erotemic/repos", "events_url": "https://api.github.com/users/Erotemic/events{/privacy}", "received_events_url": "https://api.github.com/users/Erotemic/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-23T15:09:17Z", "updated_at": "2018-03-23T15:09:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>My guess is that DataParallel uses multiprocessing under the hood, so the model is copied into worker processes. When you set an attribute in the forward pass it sets it in the local model for that process. Therefore your model in the main thread will never see it (nor could it because the attribute set by the forward passes in multiple processes would clobber each other).</p>\n<p>There might be a way to setup data as shared memory and pass it between processes, but you'll have to be careful of race conditions.</p>\n<p>As an immediate workaround you could use multiprocessing to get the process id and write the data you are interested in to disk tagged with the process id. Perhaps someone who knows how to use <code>torch.Tensor</code>'s shared memory abilities will have a better solution.</p>", "body_text": "My guess is that DataParallel uses multiprocessing under the hood, so the model is copied into worker processes. When you set an attribute in the forward pass it sets it in the local model for that process. Therefore your model in the main thread will never see it (nor could it because the attribute set by the forward passes in multiple processes would clobber each other).\nThere might be a way to setup data as shared memory and pass it between processes, but you'll have to be careful of race conditions.\nAs an immediate workaround you could use multiprocessing to get the process id and write the data you are interested in to disk tagged with the process id. Perhaps someone who knows how to use torch.Tensor's shared memory abilities will have a better solution.", "body": "My guess is that DataParallel uses multiprocessing under the hood, so the model is copied into worker processes. When you set an attribute in the forward pass it sets it in the local model for that process. Therefore your model in the main thread will never see it (nor could it because the attribute set by the forward passes in multiple processes would clobber each other). \r\n\r\nThere might be a way to setup data as shared memory and pass it between processes, but you'll have to be careful of race conditions. \r\n\r\nAs an immediate workaround you could use multiprocessing to get the process id and write the data you are interested in to disk tagged with the process id. Perhaps someone who knows how to use `torch.Tensor`'s shared memory abilities will have a better solution. "}