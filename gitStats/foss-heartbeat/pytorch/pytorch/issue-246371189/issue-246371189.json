{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2230", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2230/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2230/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2230/events", "html_url": "https://github.com/pytorch/pytorch/issues/2230", "id": 246371189, "node_id": "MDU6SXNzdWUyNDYzNzExODk=", "number": 2230, "title": "tensorflow conflicts with nn.DataParallel", "user": {"login": "lanpa", "id": 2005323, "node_id": "MDQ6VXNlcjIwMDUzMjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2005323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lanpa", "html_url": "https://github.com/lanpa", "followers_url": "https://api.github.com/users/lanpa/followers", "following_url": "https://api.github.com/users/lanpa/following{/other_user}", "gists_url": "https://api.github.com/users/lanpa/gists{/gist_id}", "starred_url": "https://api.github.com/users/lanpa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lanpa/subscriptions", "organizations_url": "https://api.github.com/users/lanpa/orgs", "repos_url": "https://api.github.com/users/lanpa/repos", "events_url": "https://api.github.com/users/lanpa/events{/privacy}", "received_events_url": "https://api.github.com/users/lanpa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-07-28T14:53:22Z", "updated_at": "2017-08-17T15:51:59Z", "closed_at": "2017-08-17T15:51:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Environment: 2 GTX1080 GPU<br>\nminimal reproducible code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>):\n    emb <span class=\"pl-k\">=</span> tf.Variable([[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>],[<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">4</span>]], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>embedding<span class=\"pl-pds\">\"</span></span>)\n\nconfig <span class=\"pl-k\">=</span> tf.ConfigProto()\nconfig.gpu_options.allow_growth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n    sess.run(emb.initializer)\n\nmodel <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">1</span>).cuda()\nmodel <span class=\"pl-k\">=</span> torch.nn.DataParallel(model).cuda()\n\ndata <span class=\"pl-k\">=</span> Variable(torch.Tensor(<span class=\"pl-c1\">8</span>,<span class=\"pl-c1\">128</span>)).cuda()\nx <span class=\"pl-k\">=</span> model(data)</pre></div>\n<p>error message:</p>\n<pre><code>  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 225, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 59, in forward\n    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 64, in replicate\n    return replicate(module, device_ids)\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/replicate.py\", line 12, in replicate\n    param_copies = Broadcast(devices)(*params)\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 19, in forward\n    outputs = comm.broadcast_coalesced(inputs, self.target_gpus)\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/cuda/comm.py\", line 49, in broadcast_coalesced\n    raise RuntimeError('all tensors must be on devices[0]')\n</code></pre>\n<p>By removing <code>model = torch.nn.DataParallel(model).cuda()</code> or <code>sess.run</code> the code works fine.</p>", "body_text": "Environment: 2 GTX1080 GPU\nminimal reproducible code:\nimport torch\nfrom torch.autograd import Variable\n\nimport tensorflow as tf\nwith tf.device('/cpu:0'):\n    emb = tf.Variable([[1,2],[3,4]], name=\"embedding\")\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nwith tf.Session(config=config) as sess:\n    sess.run(emb.initializer)\n\nmodel = torch.nn.Linear(128, 1).cuda()\nmodel = torch.nn.DataParallel(model).cuda()\n\ndata = Variable(torch.Tensor(8,128)).cuda()\nx = model(data)\nerror message:\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 225, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 59, in forward\n    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 64, in replicate\n    return replicate(module, device_ids)\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/replicate.py\", line 12, in replicate\n    param_copies = Broadcast(devices)(*params)\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 19, in forward\n    outputs = comm.broadcast_coalesced(inputs, self.target_gpus)\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/cuda/comm.py\", line 49, in broadcast_coalesced\n    raise RuntimeError('all tensors must be on devices[0]')\n\nBy removing model = torch.nn.DataParallel(model).cuda() or sess.run the code works fine.", "body": "Environment: 2 GTX1080 GPU\r\nminimal reproducible code:\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nimport tensorflow as tf\r\nwith tf.device('/cpu:0'):\r\n    emb = tf.Variable([[1,2],[3,4]], name=\"embedding\")\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nwith tf.Session(config=config) as sess:\r\n    sess.run(emb.initializer)\r\n\r\nmodel = torch.nn.Linear(128, 1).cuda()\r\nmodel = torch.nn.DataParallel(model).cuda()\r\n\r\ndata = Variable(torch.Tensor(8,128)).cuda()\r\nx = model(data)\r\n```\r\n\r\nerror message:\r\n```\r\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 225, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 59, in forward\r\n    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\r\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 64, in replicate\r\n    return replicate(module, device_ids)\r\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/replicate.py\", line 12, in replicate\r\n    param_copies = Broadcast(devices)(*params)\r\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 19, in forward\r\n    outputs = comm.broadcast_coalesced(inputs, self.target_gpus)\r\n  File \"/home/dexter/anaconda3/lib/python3.6/site-packages/torch/cuda/comm.py\", line 49, in broadcast_coalesced\r\n    raise RuntimeError('all tensors must be on devices[0]')\r\n```\r\n\r\n\r\nBy removing `model = torch.nn.DataParallel(model).cuda()` or `sess.run` the code works fine."}