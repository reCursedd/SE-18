{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362332128", "html_url": "https://github.com/pytorch/pytorch/issues/4987#issuecomment-362332128", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4987", "id": 362332128, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjMzMjEyOA==", "user": {"login": "malbergo", "id": 6062598, "node_id": "MDQ6VXNlcjYwNjI1OTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6062598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/malbergo", "html_url": "https://github.com/malbergo", "followers_url": "https://api.github.com/users/malbergo/followers", "following_url": "https://api.github.com/users/malbergo/following{/other_user}", "gists_url": "https://api.github.com/users/malbergo/gists{/gist_id}", "starred_url": "https://api.github.com/users/malbergo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/malbergo/subscriptions", "organizations_url": "https://api.github.com/users/malbergo/orgs", "repos_url": "https://api.github.com/users/malbergo/repos", "events_url": "https://api.github.com/users/malbergo/events{/privacy}", "received_events_url": "https://api.github.com/users/malbergo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T17:01:40Z", "updated_at": "2018-02-01T17:02:05Z", "author_association": "NONE", "body_html": "<p>Thanks for clarifying. The only calls to data_parallel in the script are in the definitions of the discriminator and generator in the GAN model being employed. This call is made in \"forward\" like below:</p>\n<p><code>class _netG(nn.Module): </code></p>\n<pre><code>def __init__(self, ngpu):\n    super(_netG, self).__init__()\n    self.ngpu = ngpu\n    self.main = nn.Sequential(\n        # input is Z, going into a convolution\n        nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n        nn.BatchNorm2d(ngf * 8),\n        nn.ReLU(True),\n        # state size. (ngf*8) x 4 x 4\n        nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ngf * 4),\n        nn.ReLU(True),\n        # state size. (ngf*4) x 8 x 8\n        nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ngf * 2),\n        nn.ReLU(True),\n        # state size. (ngf*2) x 16 x 16\n        nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ngf),\n        nn.ReLU(True),\n        # state size. (ngf) x 32 x 32\n        nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n        nn.Tanh()\n        # state size. (nc) x 64 x 64\n    )\n\ndef forward(self, input):\n    if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu &gt; 1:\n        output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n    else:\n        output = self.main(input)\n    return output`\n</code></pre>\n<p><code>netG = _netG(ngpu netG.apply(weight_init) if opt.netG != \": netG.load_state_dict(torch.load(opt.netG)) print(netG)</code></p>\n<p>From my perspective, though I still could be misunderstanding your, it seems that there is only one data_parallel script (or would you consider a second being the one in the discriminator?). The script allows for a flag to specify the number of gpus to use as well,  i.e. \"--ngpu 2\" or \"--ngpu 1\".</p>", "body_text": "Thanks for clarifying. The only calls to data_parallel in the script are in the definitions of the discriminator and generator in the GAN model being employed. This call is made in \"forward\" like below:\nclass _netG(nn.Module): \ndef __init__(self, ngpu):\n    super(_netG, self).__init__()\n    self.ngpu = ngpu\n    self.main = nn.Sequential(\n        # input is Z, going into a convolution\n        nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n        nn.BatchNorm2d(ngf * 8),\n        nn.ReLU(True),\n        # state size. (ngf*8) x 4 x 4\n        nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ngf * 4),\n        nn.ReLU(True),\n        # state size. (ngf*4) x 8 x 8\n        nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ngf * 2),\n        nn.ReLU(True),\n        # state size. (ngf*2) x 16 x 16\n        nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ngf),\n        nn.ReLU(True),\n        # state size. (ngf) x 32 x 32\n        nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n        nn.Tanh()\n        # state size. (nc) x 64 x 64\n    )\n\ndef forward(self, input):\n    if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n        output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n    else:\n        output = self.main(input)\n    return output`\n\nnetG = _netG(ngpu netG.apply(weight_init) if opt.netG != \": netG.load_state_dict(torch.load(opt.netG)) print(netG)\nFrom my perspective, though I still could be misunderstanding your, it seems that there is only one data_parallel script (or would you consider a second being the one in the discriminator?). The script allows for a flag to specify the number of gpus to use as well,  i.e. \"--ngpu 2\" or \"--ngpu 1\".", "body": "Thanks for clarifying. The only calls to data_parallel in the script are in the definitions of the discriminator and generator in the GAN model being employed. This call is made in \"forward\" like below:\r\n\r\n`class _netG(nn.Module): `\r\n\r\n    def __init__(self, ngpu):\r\n        super(_netG, self).__init__()\r\n        self.ngpu = ngpu\r\n        self.main = nn.Sequential(\r\n            # input is Z, going into a convolution\r\n            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\r\n            nn.BatchNorm2d(ngf * 8),\r\n            nn.ReLU(True),\r\n            # state size. (ngf*8) x 4 x 4\r\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\r\n            nn.BatchNorm2d(ngf * 4),\r\n            nn.ReLU(True),\r\n            # state size. (ngf*4) x 8 x 8\r\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\r\n            nn.BatchNorm2d(ngf * 2),\r\n            nn.ReLU(True),\r\n            # state size. (ngf*2) x 16 x 16\r\n            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\r\n            nn.BatchNorm2d(ngf),\r\n            nn.ReLU(True),\r\n            # state size. (ngf) x 32 x 32\r\n            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\r\n            nn.Tanh()\r\n            # state size. (nc) x 64 x 64\r\n        )\r\n\r\n    def forward(self, input):\r\n        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\r\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\r\n        else:\r\n            output = self.main(input)\r\n        return output`\r\n\r\n`netG = _netG(ngpu\r\nnetG.apply(weight_init)\r\nif opt.netG != \":\r\n      netG.load_state_dict(torch.load(opt.netG))\r\nprint(netG)`\r\n\r\n\r\nFrom my perspective, though I still could be misunderstanding your, it seems that there is only one data_parallel script (or would you consider a second being the one in the discriminator?). The script allows for a flag to specify the number of gpus to use as well,  i.e. \"--ngpu 2\" or \"--ngpu 1\"."}