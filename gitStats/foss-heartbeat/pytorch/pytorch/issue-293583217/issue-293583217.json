{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4987", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4987/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4987/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4987/events", "html_url": "https://github.com/pytorch/pytorch/issues/4987", "id": 293583217, "node_id": "MDU6SXNzdWUyOTM1ODMyMTc=", "number": 4987, "title": "MultiGPU hangs Titan Xp in multiprocessing/queue.py", "user": {"login": "malbergo", "id": 6062598, "node_id": "MDQ6VXNlcjYwNjI1OTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/6062598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/malbergo", "html_url": "https://github.com/malbergo", "followers_url": "https://api.github.com/users/malbergo/followers", "following_url": "https://api.github.com/users/malbergo/following{/other_user}", "gists_url": "https://api.github.com/users/malbergo/gists{/gist_id}", "starred_url": "https://api.github.com/users/malbergo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/malbergo/subscriptions", "organizations_url": "https://api.github.com/users/malbergo/orgs", "repos_url": "https://api.github.com/users/malbergo/repos", "events_url": "https://api.github.com/users/malbergo/events{/privacy}", "received_events_url": "https://api.github.com/users/malbergo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2018-02-01T15:57:30Z", "updated_at": "2018-02-08T12:49:23Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>PyTorch GitHub Issues Guidelines</h2>\n<p>We like to limit our issues to bug reports and feature requests. If you have a question or would like help and support, please visit our forums: <a href=\"https://discuss.pytorch.org/\" rel=\"nofollow\">https://discuss.pytorch.org/</a></p>\n<p>If you are submitting a feature request, please preface the title with [feature request].</p>\n<p>When submitting a bug report, please include the following information (where relevant):</p>\n<ul>\n<li>OS:</li>\n<li>PyTorch version:</li>\n<li>How you installed PyTorch (conda, pip, source):</li>\n<li>Python version:</li>\n<li>CUDA/cuDNN version:</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n</ul>\n<p>In addition, including the following information will also be very helpful for us to diagnose the problem:</p>\n<ul>\n<li>A script to reproduce the bug. Please try to provide as minimal of a test case as possible.</li>\n<li>Error messages and/or stack traces of the bug</li>\n<li>Context around what you are trying to do</li>\n</ul>\n<p><strong>Hi there,</strong></p>\n<p>I have been trying for about a week and a half to get all 3 of my NVIDIA Titan Xp, but the process always hangs in the multiprocessing directory in the \"queues.py\" and the \"synchronizing.py\" whenever I use more than 1 of them.</p>\n<p>MY SETUP:</p>\n<ul>\n<li>OS: macOS Sierra 10.12.5</li>\n<li>PyTorch version: 0.3.0.post4</li>\n<li>PyTorch installation via pip<br>\n-Python version: 3.5<br>\n-CUDA/cuDNN version: CUDA 8.0.61  and torch.backends.cudnn.version() = 7003<br>\n-GPU models and configuration: 3 NVIDIA Titan Xps installed on PCI express board</li>\n</ul>\n<p>This problem seems to occur whenever I use the data parallel functionality in PyTorch. It happens on very simple calculations and in things like ConvNets etc. It loads the network architecture and then gets stuck at what seems to be the beginning of training the network.</p>\n<blockquote>\n<p>Traceback (most recent call last):<br>\nFile \"train.py\", line 229, in <br>\noutput = netD(inputv)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 325, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/home/chris/Documents/MPhilProjects/GANs/PyTorchApproach/DCGAN-pytorch/model.py\", line 110, in forward<br>\noutput = nn.parallel.data_parallel(self.main, input, range(self.ngpu))<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/data_parallel.py\", line 113, in data_parallel<br>\noutputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/parallel_apply.py\", line 59, in parallel_apply<br>\nthread.join()<br>\nFile \"/usr/lib/python3.5/threading.py\", line 1054, in join<br>\nTraceback (most recent call last):<br>\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap<br>\nself.run()<br>\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run<br>\nself._target(*self._args, **self._kwargs)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop<br>\nr = index_queue.get()<br>\nFile \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get<br>\nres = self._reader.recv_bytes()<br>\nFile \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes<br>\nbuf = self._recv_bytes(maxlength)<br>\nFile \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes<br>\nbuf = self._recv(4)<br>\nFile \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv<br>\nchunk = read(handle, remaining)<br>\nKeyboardInterrupt<br>\nself._wait_for_tstate_lock()<br>\nFile \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock<br>\nProcess Process-2:<br>\nelif lock.acquire(block, timeout):<br>\nKeyboardInterrupt<br>\nTraceback (most recent call last):<br>\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap<br>\nself.run()<br>\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run<br>\nself._target(*self._args, **self._kwargs)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop<br>\nr = index_queue.get()<br>\nFile \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get<br>\nwith self._rlock:<br>\nFile \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in <strong>enter</strong><br>\nreturn self._semlock.<strong>enter</strong>()<br>\nKeyboardInterrupt<br>\nProcess Process-5:<br>\nTraceback (most recent call last):<br>\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap<br>\nself.run()<br>\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run<br>\nself._target(*self._args, **self._kwargs)<br>\nFile \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop<br>\nr = index_queue.get()<br>\nFile \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get<br>\nwith self._rlock:<br>\nFile \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in <strong>enter</strong><br>\nreturn self._semlock.<strong>enter</strong>()<br>\nKeyboardInterrupt<br>\n^CException ignored in: &lt;module 'threading' from '/usr/lib/python3.5/threading.py'&gt;<br>\nTraceback (most recent call last):<br>\nFile \"/usr/lib/python3.5/threading.py\", line 1288, in _shutdown<br>\nt.join()<br>\nFile \"/usr/lib/python3.5/threading.py\", line 1054, in join<br>\nself._wait_for_tstate_lock()<br>\nFile \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock<br>\nelif lock.acquire(block, timeout):<br>\nKeyboardInterrupt</p>\n</blockquote>\n<p>I tried looking at the connections b/w them and got the following result:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/6062598/35688012-1823ef76-0768-11e8-8fca-c6147df47a80.png\"><img src=\"https://user-images.githubusercontent.com/6062598/35688012-1823ef76-0768-11e8-8fca-c6147df47a80.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>I tried testing the p2pBandwidth example that comes in cuda and got the following result:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/6062598/35688187-786df2aa-0768-11e8-87d0-a6c7462df3bd.png\"><img src=\"https://user-images.githubusercontent.com/6062598/35688187-786df2aa-0768-11e8-87d0-a6c7462df3bd.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>But I am still essentially clueless as to why they won't work together. Any advice / solutions would be <strong>greatly</strong> appreciated!</p>\n<p>Best,<br>\nMichael</p>", "body_text": "PyTorch GitHub Issues Guidelines\nWe like to limit our issues to bug reports and feature requests. If you have a question or would like help and support, please visit our forums: https://discuss.pytorch.org/\nIf you are submitting a feature request, please preface the title with [feature request].\nWhen submitting a bug report, please include the following information (where relevant):\n\nOS:\nPyTorch version:\nHow you installed PyTorch (conda, pip, source):\nPython version:\nCUDA/cuDNN version:\nGPU models and configuration:\nGCC version (if compiling from source):\n\nIn addition, including the following information will also be very helpful for us to diagnose the problem:\n\nA script to reproduce the bug. Please try to provide as minimal of a test case as possible.\nError messages and/or stack traces of the bug\nContext around what you are trying to do\n\nHi there,\nI have been trying for about a week and a half to get all 3 of my NVIDIA Titan Xp, but the process always hangs in the multiprocessing directory in the \"queues.py\" and the \"synchronizing.py\" whenever I use more than 1 of them.\nMY SETUP:\n\nOS: macOS Sierra 10.12.5\nPyTorch version: 0.3.0.post4\nPyTorch installation via pip\n-Python version: 3.5\n-CUDA/cuDNN version: CUDA 8.0.61  and torch.backends.cudnn.version() = 7003\n-GPU models and configuration: 3 NVIDIA Titan Xps installed on PCI express board\n\nThis problem seems to occur whenever I use the data parallel functionality in PyTorch. It happens on very simple calculations and in things like ConvNets etc. It loads the network architecture and then gets stuck at what seems to be the beginning of training the network.\n\nTraceback (most recent call last):\nFile \"train.py\", line 229, in \noutput = netD(inputv)\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 325, in call\nresult = self.forward(*input, **kwargs)\nFile \"/home/chris/Documents/MPhilProjects/GANs/PyTorchApproach/DCGAN-pytorch/model.py\", line 110, in forward\noutput = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/data_parallel.py\", line 113, in data_parallel\noutputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)\nFile \"/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/parallel_apply.py\", line 59, in parallel_apply\nthread.join()\nFile \"/usr/lib/python3.5/threading.py\", line 1054, in join\nTraceback (most recent call last):\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\nself.run()\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\nself._target(*self._args, **self._kwargs)\nFile \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\nr = index_queue.get()\nFile \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\nres = self._reader.recv_bytes()\nFile \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\nbuf = self._recv_bytes(maxlength)\nFile \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\nbuf = self._recv(4)\nFile \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\nchunk = read(handle, remaining)\nKeyboardInterrupt\nself._wait_for_tstate_lock()\nFile \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\nProcess Process-2:\nelif lock.acquire(block, timeout):\nKeyboardInterrupt\nTraceback (most recent call last):\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\nself.run()\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\nself._target(*self._args, **self._kwargs)\nFile \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\nr = index_queue.get()\nFile \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\nwith self._rlock:\nFile \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in enter\nreturn self._semlock.enter()\nKeyboardInterrupt\nProcess Process-5:\nTraceback (most recent call last):\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\nself.run()\nFile \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\nself._target(*self._args, **self._kwargs)\nFile \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\nr = index_queue.get()\nFile \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\nwith self._rlock:\nFile \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in enter\nreturn self._semlock.enter()\nKeyboardInterrupt\n^CException ignored in: <module 'threading' from '/usr/lib/python3.5/threading.py'>\nTraceback (most recent call last):\nFile \"/usr/lib/python3.5/threading.py\", line 1288, in _shutdown\nt.join()\nFile \"/usr/lib/python3.5/threading.py\", line 1054, in join\nself._wait_for_tstate_lock()\nFile \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\nelif lock.acquire(block, timeout):\nKeyboardInterrupt\n\nI tried looking at the connections b/w them and got the following result:\n\nI tried testing the p2pBandwidth example that comes in cuda and got the following result:\n\nBut I am still essentially clueless as to why they won't work together. Any advice / solutions would be greatly appreciated!\nBest,\nMichael", "body": "PyTorch GitHub Issues Guidelines\r\n--------------------------------\r\n\r\nWe like to limit our issues to bug reports and feature requests. If you have a question or would like help and support, please visit our forums: https://discuss.pytorch.org/\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\n\r\nWhen submitting a bug report, please include the following information (where relevant):\r\n- OS:\r\n- PyTorch version:\r\n- How you installed PyTorch (conda, pip, source):\r\n- Python version:\r\n- CUDA/cuDNN version:\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n\r\nIn addition, including the following information will also be very helpful for us to diagnose the problem:\r\n- A script to reproduce the bug. Please try to provide as minimal of a test case as possible.\r\n- Error messages and/or stack traces of the bug\r\n- Context around what you are trying to do\r\n\r\n\r\n**Hi there,**\r\n\r\nI have been trying for about a week and a half to get all 3 of my NVIDIA Titan Xp, but the process always hangs in the multiprocessing directory in the \"queues.py\" and the \"synchronizing.py\" whenever I use more than 1 of them.\r\n\r\nMY SETUP:\r\n- OS: macOS Sierra 10.12.5\r\n- PyTorch version: 0.3.0.post4\r\n- PyTorch installation via pip\r\n-Python version: 3.5\r\n-CUDA/cuDNN version: CUDA 8.0.61  and torch.backends.cudnn.version() = 7003\r\n-GPU models and configuration: 3 NVIDIA Titan Xps installed on PCI express board \r\n\r\n\r\nThis problem seems to occur whenever I use the data parallel functionality in PyTorch. It happens on very simple calculations and in things like ConvNets etc. It loads the network architecture and then gets stuck at what seems to be the beginning of training the network.\r\n\r\n> Traceback (most recent call last):\r\n  File \"train.py\", line 229, in <module>\r\n    output = netD(inputv)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/chris/Documents/MPhilProjects/GANs/PyTorchApproach/DCGAN-pytorch/model.py\", line 110, in forward\r\n    output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/data_parallel.py\", line 113, in data_parallel\r\n    outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/nn/parallel/parallel_apply.py\", line 59, in parallel_apply\r\n    thread.join()\r\n  File \"/usr/lib/python3.5/threading.py\", line 1054, in join\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\r\n    r = index_queue.get()\r\n  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\nKeyboardInterrupt\r\n    self._wait_for_tstate_lock()\r\n  File \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\r\nProcess Process-2:\r\n    elif lock.acquire(block, timeout):\r\nKeyboardInterrupt\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\r\n    r = index_queue.get()\r\n  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\r\n    with self._rlock:\r\n  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\r\n    return self._semlock.__enter__()\r\nKeyboardInterrupt\r\nProcess Process-5:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\r\n    r = index_queue.get()\r\n  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\r\n    with self._rlock:\r\n  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\r\n    return self._semlock.__enter__()\r\nKeyboardInterrupt\r\n^CException ignored in: <module 'threading' from '/usr/lib/python3.5/threading.py'>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/threading.py\", line 1288, in _shutdown\r\n    t.join()\r\n  File \"/usr/lib/python3.5/threading.py\", line 1054, in join\r\n    self._wait_for_tstate_lock()\r\n  File \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\r\n    elif lock.acquire(block, timeout):\r\nKeyboardInterrupt\r\n\r\nI tried looking at the connections b/w them and got the following result:\r\n![image](https://user-images.githubusercontent.com/6062598/35688012-1823ef76-0768-11e8-8fca-c6147df47a80.png)\r\n\r\nI tried testing the p2pBandwidth example that comes in cuda and got the following result:\r\n![image](https://user-images.githubusercontent.com/6062598/35688187-786df2aa-0768-11e8-87d0-a6c7462df3bd.png)\r\n\r\n\r\nBut I am still essentially clueless as to why they won't work together. Any advice / solutions would be **greatly** appreciated!\r\n\r\nBest,\r\nMichael\r\n\r\n"}