{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3002", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3002/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3002/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3002/events", "html_url": "https://github.com/pytorch/pytorch/issues/3002", "id": 263454360, "node_id": "MDU6SXNzdWUyNjM0NTQzNjA=", "number": 3002, "title": "SVD MAGMA gesdd : the updating process of SBDSDC did not converge", "user": {"login": "ntuyt", "id": 14818350, "node_id": "MDQ6VXNlcjE0ODE4MzUw", "avatar_url": "https://avatars1.githubusercontent.com/u/14818350?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ntuyt", "html_url": "https://github.com/ntuyt", "followers_url": "https://api.github.com/users/ntuyt/followers", "following_url": "https://api.github.com/users/ntuyt/following{/other_user}", "gists_url": "https://api.github.com/users/ntuyt/gists{/gist_id}", "starred_url": "https://api.github.com/users/ntuyt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ntuyt/subscriptions", "organizations_url": "https://api.github.com/users/ntuyt/orgs", "repos_url": "https://api.github.com/users/ntuyt/repos", "events_url": "https://api.github.com/users/ntuyt/events{/privacy}", "received_events_url": "https://api.github.com/users/ntuyt/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-10-06T13:55:15Z", "updated_at": "2018-09-05T19:37:25Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi, when I use tensor.svd(), it reports below error.<br>\nAny suggestions to avoid this errors?</p>\n<p>Traceback (most recent call last):<br>\nFile \"maintestlogfast.py\", line 304, in <br>\nmain()<br>\nFile \"maintestlogfast.py\", line 131, in main<br>\nprec1 = validate(val_loader, model, criterion, (epoch + 1) * len(train_loader))<br>\nFile \"maintestlogfast.py\", line 223, in validate<br>\noutput = model(input_var)<br>\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 60, in forward<br>\noutputs = self.parallel_apply(replicas, inputs, kwargs)<br>\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 70, in parallel_apply<br>\nreturn parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])<br>\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 67, in parallel_apply<br>\nraise output<br>\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 42, in _worker<br>\noutput = module(*input, **kwargs)<br>\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/home/ytan/sharedLocal/tsn-pytorch/modelstestlogfixfast.py\", line 229, in forward<br>\nbase_out = self.base_model(input.view((-1, sample_len) + input.size()[-2:]))<br>\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/home/ytan/sharedLocal/tsn-pytorch/tf_model_zoo/bninception/pytorch_load.py\", line 52, in forward<br>\ndata_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))<br>\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/home/ytan/sharedLocal/tsn-pytorch/bilinearsuffast.py\", line 12, in forward<br>\noutput = mylog(input)<br>\nFile \"/home/ytan/sharedLocal/tsn-pytorch/logfunctionfast.py\", line 21, in forward<br>\nu,s,v = m6sub.svd()<br>\nRuntimeError: MAGMA gesdd : the updating process of SBDSDC did not converge (error: 1) at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCTensorMathMagma.cu:325</p>", "body_text": "Hi, when I use tensor.svd(), it reports below error.\nAny suggestions to avoid this errors?\nTraceback (most recent call last):\nFile \"maintestlogfast.py\", line 304, in \nmain()\nFile \"maintestlogfast.py\", line 131, in main\nprec1 = validate(val_loader, model, criterion, (epoch + 1) * len(train_loader))\nFile \"maintestlogfast.py\", line 223, in validate\noutput = model(input_var)\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in call\nresult = self.forward(*input, **kwargs)\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 60, in forward\noutputs = self.parallel_apply(replicas, inputs, kwargs)\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 70, in parallel_apply\nreturn parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 67, in parallel_apply\nraise output\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 42, in _worker\noutput = module(*input, **kwargs)\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in call\nresult = self.forward(*input, **kwargs)\nFile \"/home/ytan/sharedLocal/tsn-pytorch/modelstestlogfixfast.py\", line 229, in forward\nbase_out = self.base_model(input.view((-1, sample_len) + input.size()[-2:]))\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in call\nresult = self.forward(*input, **kwargs)\nFile \"/home/ytan/sharedLocal/tsn-pytorch/tf_model_zoo/bninception/pytorch_load.py\", line 52, in forward\ndata_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))\nFile \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in call\nresult = self.forward(*input, **kwargs)\nFile \"/home/ytan/sharedLocal/tsn-pytorch/bilinearsuffast.py\", line 12, in forward\noutput = mylog(input)\nFile \"/home/ytan/sharedLocal/tsn-pytorch/logfunctionfast.py\", line 21, in forward\nu,s,v = m6sub.svd()\nRuntimeError: MAGMA gesdd : the updating process of SBDSDC did not converge (error: 1) at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCTensorMathMagma.cu:325", "body": "Hi, when I use tensor.svd(), it reports below error.\r\nAny suggestions to avoid this errors?\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"maintestlogfast.py\", line 304, in <module>\r\n    main()\r\n  File \"maintestlogfast.py\", line 131, in main\r\n    prec1 = validate(val_loader, model, criterion, (epoch + 1) * len(train_loader))\r\n  File \"maintestlogfast.py\", line 223, in validate\r\n    output = model(input_var)\r\n  File \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 60, in forward\r\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n  File \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 70, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\r\n  File \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 67, in parallel_apply\r\n    raise output\r\n  File \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 42, in _worker\r\n    output = module(*input, **kwargs)\r\n  File \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ytan/sharedLocal/tsn-pytorch/modelstestlogfixfast.py\", line 229, in forward\r\n    base_out = self.base_model(input.view((-1, sample_len) + input.size()[-2:]))\r\n  File \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ytan/sharedLocal/tsn-pytorch/tf_model_zoo/bninception/pytorch_load.py\", line 52, in forward\r\n    data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))\r\n  File \"/home/ytan/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ytan/sharedLocal/tsn-pytorch/bilinearsuffast.py\", line 12, in forward\r\n    output = mylog(input)\r\n  File \"/home/ytan/sharedLocal/tsn-pytorch/logfunctionfast.py\", line 21, in forward\r\n    u,s,v = m6sub.svd()\r\nRuntimeError: MAGMA gesdd : the updating process of SBDSDC did not converge (error: 1) at /opt/conda/conda-bld/pytorch_1503970438496/work/torch/lib/THC/generic/THCTensorMathMagma.cu:325\r\n"}