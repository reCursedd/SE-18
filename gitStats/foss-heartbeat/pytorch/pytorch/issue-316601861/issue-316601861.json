{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6848", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6848/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6848/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6848/events", "html_url": "https://github.com/pytorch/pytorch/issues/6848", "id": 316601861, "node_id": "MDU6SXNzdWUzMTY2MDE4NjE=", "number": 6848, "title": "[question/pytorch] How to force a part of graph to backward on CPU", "user": {"login": "cdluminate", "id": 5723047, "node_id": "MDQ6VXNlcjU3MjMwNDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5723047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cdluminate", "html_url": "https://github.com/cdluminate", "followers_url": "https://api.github.com/users/cdluminate/followers", "following_url": "https://api.github.com/users/cdluminate/following{/other_user}", "gists_url": "https://api.github.com/users/cdluminate/gists{/gist_id}", "starred_url": "https://api.github.com/users/cdluminate/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cdluminate/subscriptions", "organizations_url": "https://api.github.com/users/cdluminate/orgs", "repos_url": "https://api.github.com/users/cdluminate/repos", "events_url": "https://api.github.com/users/cdluminate/events{/privacy}", "received_events_url": "https://api.github.com/users/cdluminate/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-22T18:31:15Z", "updated_at": "2018-04-22T22:46:16Z", "closed_at": "2018-04-22T22:46:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm trying to write a complex function, where I have to calculate the elements in a big matrix one by one. As far as I know performing an amount of such kind of 'one-element operation' is quite inefficient on GPU due to the hardware interruption latency underneath the cudaMemcpy call. Actual implementation proved that. The function uses torch.autograd, which means there is no manual backward method.</p>\n<p>So, in my function, all the input cuda tensors are converted to cpu tensor first. The result tensors are converted back into cuda tensors before return. This makes the function a bit faster. However, the function is still slow in the backward stage, which happens on GPU.</p>\n<p>My question is, can I force a function, or say a part of the computation graph, to be processed on CPU? Thanks in advamce.</p>", "body_text": "I'm trying to write a complex function, where I have to calculate the elements in a big matrix one by one. As far as I know performing an amount of such kind of 'one-element operation' is quite inefficient on GPU due to the hardware interruption latency underneath the cudaMemcpy call. Actual implementation proved that. The function uses torch.autograd, which means there is no manual backward method.\nSo, in my function, all the input cuda tensors are converted to cpu tensor first. The result tensors are converted back into cuda tensors before return. This makes the function a bit faster. However, the function is still slow in the backward stage, which happens on GPU.\nMy question is, can I force a function, or say a part of the computation graph, to be processed on CPU? Thanks in advamce.", "body": "I'm trying to write a complex function, where I have to calculate the elements in a big matrix one by one. As far as I know performing an amount of such kind of 'one-element operation' is quite inefficient on GPU due to the hardware interruption latency underneath the cudaMemcpy call. Actual implementation proved that. The function uses torch.autograd, which means there is no manual backward method.\r\n\r\nSo, in my function, all the input cuda tensors are converted to cpu tensor first. The result tensors are converted back into cuda tensors before return. This makes the function a bit faster. However, the function is still slow in the backward stage, which happens on GPU.\r\n\r\nMy question is, can I force a function, or say a part of the computation graph, to be processed on CPU? Thanks in advamce."}