{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11691", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11691/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11691/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11691/events", "html_url": "https://github.com/pytorch/pytorch/issues/11691", "id": 360204240, "node_id": "MDU6SXNzdWUzNjAyMDQyNDA=", "number": 11691, "title": "RandomSampler iter methods cause failure with huge dataset size", "user": {"login": "tobyclh", "id": 12501995, "node_id": "MDQ6VXNlcjEyNTAxOTk1", "avatar_url": "https://avatars3.githubusercontent.com/u/12501995?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tobyclh", "html_url": "https://github.com/tobyclh", "followers_url": "https://api.github.com/users/tobyclh/followers", "following_url": "https://api.github.com/users/tobyclh/following{/other_user}", "gists_url": "https://api.github.com/users/tobyclh/gists{/gist_id}", "starred_url": "https://api.github.com/users/tobyclh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tobyclh/subscriptions", "organizations_url": "https://api.github.com/users/tobyclh/orgs", "repos_url": "https://api.github.com/users/tobyclh/repos", "events_url": "https://api.github.com/users/tobyclh/events{/privacy}", "received_events_url": "https://api.github.com/users/tobyclh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-09-14T08:22:56Z", "updated_at": "2018-09-16T04:40:38Z", "closed_at": "2018-09-15T09:47:37Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>I created a video dataset where which each data includes only a few frames,<br>\nincluding all the permutation, this makes the total size of the dataset stupidly huge (higher than 1e9).<br>\nWhenever I try to set shuffle=True in dataloader, the system kills the process for taking too much resource.<br>\nAfter some investigation, it is<br>\n<code>iter(torch.randperm(len(self.data_source)).tolist())</code><br>\nin the iter method of random sampler causing this failure.<br>\nThat leaves me with 2 solutions.</p>\n<ol>\n<li>Use shuffle=False (not ideal)</li>\n<li>Create a custom sampler with numpy (which in my testing, is very slow but gets the job done)</li>\n</ol>\n<h2>Code example</h2>\n<pre><code>dataset = SpeechAnimationDataset(opt)\n\nloader = DataLoader(dataset, batch_size=5, num_workers=2) #works\nloader = DataLoader(dataset, batch_size=5, num_workers=2, shuffle=True) #fails after long process\ndata_iter = iter(loader)\nfor i in tqdm(range(500)):\n    data = next(data_iter)\n</code></pre>\n<p>Working numpy replacement</p>\n<pre><code>class RandomSampler(Sampler):\n    r\"\"\"Samples elements randomly, without replacement.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    \"\"\"\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        arr = np.arange(len(self.data_source))\n        np.random.shuffle(arr)\n        return iter(arr)\n\n    def __len__(self):\n        return len(self.data_source)\n</code></pre>\n<p>I also tried to remove the .tolist() that didn't seem necessary, but that didn't fix anything.<br>\nI don't see pytorch have much numpy code in the repo, so I guess the method I propose is not ideal, but I think this could be useful to some who have the same problem.<br>\nThanks!</p>", "body_text": "Issue description\nI created a video dataset where which each data includes only a few frames,\nincluding all the permutation, this makes the total size of the dataset stupidly huge (higher than 1e9).\nWhenever I try to set shuffle=True in dataloader, the system kills the process for taking too much resource.\nAfter some investigation, it is\niter(torch.randperm(len(self.data_source)).tolist())\nin the iter method of random sampler causing this failure.\nThat leaves me with 2 solutions.\n\nUse shuffle=False (not ideal)\nCreate a custom sampler with numpy (which in my testing, is very slow but gets the job done)\n\nCode example\ndataset = SpeechAnimationDataset(opt)\n\nloader = DataLoader(dataset, batch_size=5, num_workers=2) #works\nloader = DataLoader(dataset, batch_size=5, num_workers=2, shuffle=True) #fails after long process\ndata_iter = iter(loader)\nfor i in tqdm(range(500)):\n    data = next(data_iter)\n\nWorking numpy replacement\nclass RandomSampler(Sampler):\n    r\"\"\"Samples elements randomly, without replacement.\n\n    Arguments:\n        data_source (Dataset): dataset to sample from\n    \"\"\"\n\n    def __init__(self, data_source):\n        self.data_source = data_source\n\n    def __iter__(self):\n        arr = np.arange(len(self.data_source))\n        np.random.shuffle(arr)\n        return iter(arr)\n\n    def __len__(self):\n        return len(self.data_source)\n\nI also tried to remove the .tolist() that didn't seem necessary, but that didn't fix anything.\nI don't see pytorch have much numpy code in the repo, so I guess the method I propose is not ideal, but I think this could be useful to some who have the same problem.\nThanks!", "body": "## Issue description\r\nI created a video dataset where which each data includes only a few frames, \r\nincluding all the permutation, this makes the total size of the dataset stupidly huge (higher than 1e9).\r\nWhenever I try to set shuffle=True in dataloader, the system kills the process for taking too much resource.\r\nAfter some investigation, it is \r\n`iter(torch.randperm(len(self.data_source)).tolist())`\r\nin the iter method of random sampler causing this failure.\r\nThat leaves me with 2 solutions.\r\n1. Use shuffle=False (not ideal)\r\n2. Create a custom sampler with numpy (which in my testing, is very slow but gets the job done)\r\n\r\n## Code example\r\n```\r\ndataset = SpeechAnimationDataset(opt)\r\n\r\nloader = DataLoader(dataset, batch_size=5, num_workers=2) #works\r\nloader = DataLoader(dataset, batch_size=5, num_workers=2, shuffle=True) #fails after long process\r\ndata_iter = iter(loader)\r\nfor i in tqdm(range(500)):\r\n    data = next(data_iter)\r\n```\r\n\r\nWorking numpy replacement \r\n```\r\nclass RandomSampler(Sampler):\r\n    r\"\"\"Samples elements randomly, without replacement.\r\n\r\n    Arguments:\r\n        data_source (Dataset): dataset to sample from\r\n    \"\"\"\r\n\r\n    def __init__(self, data_source):\r\n        self.data_source = data_source\r\n\r\n    def __iter__(self):\r\n        arr = np.arange(len(self.data_source))\r\n        np.random.shuffle(arr)\r\n        return iter(arr)\r\n\r\n    def __len__(self):\r\n        return len(self.data_source)\r\n```\r\nI also tried to remove the .tolist() that didn't seem necessary, but that didn't fix anything. \r\nI don't see pytorch have much numpy code in the repo, so I guess the method I propose is not ideal, but I think this could be useful to some who have the same problem.\r\nThanks!"}