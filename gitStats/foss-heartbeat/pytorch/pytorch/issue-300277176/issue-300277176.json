{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5410", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5410/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5410/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5410/events", "html_url": "https://github.com/pytorch/pytorch/issues/5410", "id": 300277176, "node_id": "MDU6SXNzdWUzMDAyNzcxNzY=", "number": 5410, "title": "Numeric instability in batch_norm for constant inputs", "user": {"login": "wielandbrendel", "id": 6177693, "node_id": "MDQ6VXNlcjYxNzc2OTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6177693?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wielandbrendel", "html_url": "https://github.com/wielandbrendel", "followers_url": "https://api.github.com/users/wielandbrendel/followers", "following_url": "https://api.github.com/users/wielandbrendel/following{/other_user}", "gists_url": "https://api.github.com/users/wielandbrendel/gists{/gist_id}", "starred_url": "https://api.github.com/users/wielandbrendel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wielandbrendel/subscriptions", "organizations_url": "https://api.github.com/users/wielandbrendel/orgs", "repos_url": "https://api.github.com/users/wielandbrendel/repos", "events_url": "https://api.github.com/users/wielandbrendel/events{/privacy}", "received_events_url": "https://api.github.com/users/wielandbrendel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679954866, "node_id": "MDU6TGFiZWw2Nzk5NTQ4NjY=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/numerical-stability", "name": "numerical-stability", "color": "d4c5f9", "default": false}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-02-26T15:16:52Z", "updated_at": "2018-05-14T19:33:29Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>If each channel is constant (along mini-batch and spatial dimensions) then nn.functional.batch_norm should yield zero (with weight = None and bias = None). Instead, however, it often produces non-zero values which saturate to -1 and 1 if the input values to batch_norm are large enough (&gt; 1e7). Here is a minimal working example:</p>\n<pre><code>import torch\nfrom torch.nn.functional import batch_norm\nimport numpy as np\n\nnum_features = 5\n\ndef get_input(spatial_dim=10, scale=1e10):\n    data = np.random.uniform(low=-1, size=(num_features,)) * scale\n    data = data[None, :, None, None].repeat(spatial_dim, axis=2).repeat(spatial_dim, axis=3)\n    data = data.astype(np.float32)\n    data = torch.from_numpy(data)\n    return torch.autograd.Variable(data)\n\ndef run_test(spatial_dim, cuda, scale=1e10):\n    running_mean = torch.zeros(num_features)\n    running_var  = torch.zeros(num_features)\n    input  = get_input(spatial_dim, scale=scale)\n    \n    if cuda:\n        running_mean = running_mean.cuda()\n        running_var  = running_var.cuda()\n        input        = input.cuda()\n        \n    output = batch_norm(input, running_mean, running_var, training=True)\n    output = output.data.cpu().numpy()\n    total = np.prod(output.shape)\n    \n    return np.isclose(output, -1).sum() / total, \\\n           np.isclose(output, 0).sum() / total, \\\n           np.isclose(output, 1).sum() / total\n        \nprint('\\t\\t #-1 \\t #0 \\t #1')\n\n## TEST CPU\nfor spatial_dim in [2, 5, 10, 20, 50, 90]:\n    print('CPU (x dim: {})\\t {} \\t {}\\t {}'.format(spatial_dim, *run_test(spatial_dim, False)))\n\nprint()\n\n## TEST GPU\nfor spatial_dim in [2, 5, 10, 20, 50, 90]:\n    print('GPU (x dim: {})\\t {} \\t {}\\t {}'.format(spatial_dim, *run_test(spatial_dim, True)))\n</code></pre>\n<p>An example output is (each row shows the relative number of output values close to -1, 0 and 1 depending on CPU/GPU and spatial dimension):</p>\n<pre><code>\t\t #-1 \t #0 \t #1\nCPU (x dim: 2)\t 0.0 \t 1.0\t 0.0\nCPU (x dim: 5)\t 0.2 \t 0.8\t 0.0\nCPU (x dim: 10)\t 0.2 \t 0.8\t 0.0\nCPU (x dim: 20)\t 0.2 \t 0.8\t 0.0\nCPU (x dim: 50)\t 0.0 \t 1.0\t 0.0\nCPU (x dim: 90)\t 0.0 \t 1.0\t 0.0\n\nGPU (x dim: 2)\t 0.0 \t 1.0\t 0.0\nGPU (x dim: 5)\t 0.2 \t 0.6\t 0.2\nGPU (x dim: 10)\t 0.2 \t 0.6\t 0.2\nGPU (x dim: 20)\t 0.2 \t 0.6\t 0.2\nGPU (x dim: 50)\t 0.2 \t 0.6\t 0.2\nGPU (x dim: 90)\t 0.4 \t 0.4\t 0.2\n</code></pre>\n<p>Generally I observe that the larger the spatial dimension the more instable the output gets. My system setup is as follows:</p>\n<ul>\n<li>OS: Ubuntu Linux</li>\n<li>PyTorch version: 0.3.1</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Python version: 3.5</li>\n<li>CUDA/cuDNN version: cuda9.0</li>\n</ul>", "body_text": "If each channel is constant (along mini-batch and spatial dimensions) then nn.functional.batch_norm should yield zero (with weight = None and bias = None). Instead, however, it often produces non-zero values which saturate to -1 and 1 if the input values to batch_norm are large enough (> 1e7). Here is a minimal working example:\nimport torch\nfrom torch.nn.functional import batch_norm\nimport numpy as np\n\nnum_features = 5\n\ndef get_input(spatial_dim=10, scale=1e10):\n    data = np.random.uniform(low=-1, size=(num_features,)) * scale\n    data = data[None, :, None, None].repeat(spatial_dim, axis=2).repeat(spatial_dim, axis=3)\n    data = data.astype(np.float32)\n    data = torch.from_numpy(data)\n    return torch.autograd.Variable(data)\n\ndef run_test(spatial_dim, cuda, scale=1e10):\n    running_mean = torch.zeros(num_features)\n    running_var  = torch.zeros(num_features)\n    input  = get_input(spatial_dim, scale=scale)\n    \n    if cuda:\n        running_mean = running_mean.cuda()\n        running_var  = running_var.cuda()\n        input        = input.cuda()\n        \n    output = batch_norm(input, running_mean, running_var, training=True)\n    output = output.data.cpu().numpy()\n    total = np.prod(output.shape)\n    \n    return np.isclose(output, -1).sum() / total, \\\n           np.isclose(output, 0).sum() / total, \\\n           np.isclose(output, 1).sum() / total\n        \nprint('\\t\\t #-1 \\t #0 \\t #1')\n\n## TEST CPU\nfor spatial_dim in [2, 5, 10, 20, 50, 90]:\n    print('CPU (x dim: {})\\t {} \\t {}\\t {}'.format(spatial_dim, *run_test(spatial_dim, False)))\n\nprint()\n\n## TEST GPU\nfor spatial_dim in [2, 5, 10, 20, 50, 90]:\n    print('GPU (x dim: {})\\t {} \\t {}\\t {}'.format(spatial_dim, *run_test(spatial_dim, True)))\n\nAn example output is (each row shows the relative number of output values close to -1, 0 and 1 depending on CPU/GPU and spatial dimension):\n\t\t #-1 \t #0 \t #1\nCPU (x dim: 2)\t 0.0 \t 1.0\t 0.0\nCPU (x dim: 5)\t 0.2 \t 0.8\t 0.0\nCPU (x dim: 10)\t 0.2 \t 0.8\t 0.0\nCPU (x dim: 20)\t 0.2 \t 0.8\t 0.0\nCPU (x dim: 50)\t 0.0 \t 1.0\t 0.0\nCPU (x dim: 90)\t 0.0 \t 1.0\t 0.0\n\nGPU (x dim: 2)\t 0.0 \t 1.0\t 0.0\nGPU (x dim: 5)\t 0.2 \t 0.6\t 0.2\nGPU (x dim: 10)\t 0.2 \t 0.6\t 0.2\nGPU (x dim: 20)\t 0.2 \t 0.6\t 0.2\nGPU (x dim: 50)\t 0.2 \t 0.6\t 0.2\nGPU (x dim: 90)\t 0.4 \t 0.4\t 0.2\n\nGenerally I observe that the larger the spatial dimension the more instable the output gets. My system setup is as follows:\n\nOS: Ubuntu Linux\nPyTorch version: 0.3.1\nHow you installed PyTorch (conda, pip, source): pip\nPython version: 3.5\nCUDA/cuDNN version: cuda9.0", "body": "If each channel is constant (along mini-batch and spatial dimensions) then nn.functional.batch_norm should yield zero (with weight = None and bias = None). Instead, however, it often produces non-zero values which saturate to -1 and 1 if the input values to batch_norm are large enough (> 1e7). Here is a minimal working example:\r\n\r\n```\r\nimport torch\r\nfrom torch.nn.functional import batch_norm\r\nimport numpy as np\r\n\r\nnum_features = 5\r\n\r\ndef get_input(spatial_dim=10, scale=1e10):\r\n    data = np.random.uniform(low=-1, size=(num_features,)) * scale\r\n    data = data[None, :, None, None].repeat(spatial_dim, axis=2).repeat(spatial_dim, axis=3)\r\n    data = data.astype(np.float32)\r\n    data = torch.from_numpy(data)\r\n    return torch.autograd.Variable(data)\r\n\r\ndef run_test(spatial_dim, cuda, scale=1e10):\r\n    running_mean = torch.zeros(num_features)\r\n    running_var  = torch.zeros(num_features)\r\n    input  = get_input(spatial_dim, scale=scale)\r\n    \r\n    if cuda:\r\n        running_mean = running_mean.cuda()\r\n        running_var  = running_var.cuda()\r\n        input        = input.cuda()\r\n        \r\n    output = batch_norm(input, running_mean, running_var, training=True)\r\n    output = output.data.cpu().numpy()\r\n    total = np.prod(output.shape)\r\n    \r\n    return np.isclose(output, -1).sum() / total, \\\r\n           np.isclose(output, 0).sum() / total, \\\r\n           np.isclose(output, 1).sum() / total\r\n        \r\nprint('\\t\\t #-1 \\t #0 \\t #1')\r\n\r\n## TEST CPU\r\nfor spatial_dim in [2, 5, 10, 20, 50, 90]:\r\n    print('CPU (x dim: {})\\t {} \\t {}\\t {}'.format(spatial_dim, *run_test(spatial_dim, False)))\r\n\r\nprint()\r\n\r\n## TEST GPU\r\nfor spatial_dim in [2, 5, 10, 20, 50, 90]:\r\n    print('GPU (x dim: {})\\t {} \\t {}\\t {}'.format(spatial_dim, *run_test(spatial_dim, True)))\r\n```\r\n\r\nAn example output is (each row shows the relative number of output values close to -1, 0 and 1 depending on CPU/GPU and spatial dimension):\r\n```\r\n\t\t #-1 \t #0 \t #1\r\nCPU (x dim: 2)\t 0.0 \t 1.0\t 0.0\r\nCPU (x dim: 5)\t 0.2 \t 0.8\t 0.0\r\nCPU (x dim: 10)\t 0.2 \t 0.8\t 0.0\r\nCPU (x dim: 20)\t 0.2 \t 0.8\t 0.0\r\nCPU (x dim: 50)\t 0.0 \t 1.0\t 0.0\r\nCPU (x dim: 90)\t 0.0 \t 1.0\t 0.0\r\n\r\nGPU (x dim: 2)\t 0.0 \t 1.0\t 0.0\r\nGPU (x dim: 5)\t 0.2 \t 0.6\t 0.2\r\nGPU (x dim: 10)\t 0.2 \t 0.6\t 0.2\r\nGPU (x dim: 20)\t 0.2 \t 0.6\t 0.2\r\nGPU (x dim: 50)\t 0.2 \t 0.6\t 0.2\r\nGPU (x dim: 90)\t 0.4 \t 0.4\t 0.2\r\n```\r\nGenerally I observe that the larger the spatial dimension the more instable the output gets. My system setup is as follows:\r\n- OS: Ubuntu Linux\r\n- PyTorch version: 0.3.1\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: cuda9.0"}