{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193868466", "pull_request_review_id": 126930600, "id": 193868466, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5Mzg2ODQ2Ng==", "diff_hunk": "@@ -3189,17 +3423,127 @@ def run_functional_checks(test_case, test_name, name, apply_fn, run_grad_checks,\n         test_case.assertEqual(self_variable.type(), self_variable.grad.type())\n         test_case.assertEqual(self_variable.size(), self_variable.grad.size())\n \n-for test in method_tests:\n-    name, self_size, args = test[:3]\n-    basic_test_name = 'test_' + name\n-    if len(test) >= 4 and test[3] != '':\n-        basic_test_name += '_' + test[3]\n \n-    dim_args_idx = test[4] if len(test) >= 5 else []\n+# make a new function where all non-tensor arguments in 'args' have been partially\n+# applied, and all tensor arguments remain.\n+# used to trace functions when some arguments are not tensors\n+def partial_apply_nontensors(fn, args):\n+    source = ['t' if isinstance(arg, torch.Tensor) else 's' for arg in args]\n+\n+    def new_fn(*tensors_):\n+        tensors = iter(tensors_)\n+        return fn(*(args[i] if s == 's' else next(tensors) for i, s in enumerate(source)))\n+\n+    return new_fn, [arg for arg in args if isinstance(arg, torch.Tensor)]\n \n-    skipTestIf = test[5] if len(test) >= 6 else []\n \n-    output_process_fn = test[6] if len(test) >= 7 else lambda x: x\n+def create_traced_fn(fn):\n+    def traced_fn(*inputs):\n+        fn_tensors, inputs_tensors = partial_apply_nontensors(fn, inputs)\n+        traced = torch.jit.trace(*inputs_tensors)(fn_tensors)\n+        return traced(*inputs_tensors)\n+    return traced_fn\n+\n+script_template = '''\n+def the_method({}):\n+    return {}\n+'''\n+\n+\n+def create_script_fn(method_name, is_functional, output_process_fn):\n+    def script_fn(*args):\n+        formals = []\n+        tensors = []\n+        actuals = []\n+        for arg in args:\n+            if isinstance(arg, torch.Tensor):\n+                name = 'i{}'.format(len(formals))\n+                formals.append(name)\n+                actuals.append(name)\n+                tensors.append(arg)\n+            else:\n+                actuals.append(str(arg))\n+        if is_functional:\n+            call = 'torch.{}({})'.format(method_name, ', '.join(actuals))\n+        else:\n+            call = '{}.{}({})'.format(actuals[0], method_name, ', '.join(actuals[1:]))\n+        script = script_template.format(', '.join(formals), call)\n+        CU = torch.jit.CompilationUnit(script)\n+        return output_process_fn(CU.the_method(*tensors))\n+    return script_fn\n+\n+\n+def check_against_reference(self, func, reference_func, args, allow_unused=True):\n+    def allSum(vs):\n+        if isinstance(vs, torch.Tensor):\n+            vs = (vs,)\n+        return sum([(i + 1) * v.sum()\n+                    for i, v in enumerate(vs)\n+                    if v is not None and v.dtype.is_floating_point])\n+\n+    def clone_inputs(requires_grad):\n+        inputs = [\n+            arg.detach().clone().requires_grad_(requires_grad and arg.requires_grad)\n+            if isinstance(arg, torch.Tensor) else arg for arg in args\n+        ]\n+        return inputs, [input for input in inputs if isinstance(input, torch.Tensor) and input.requires_grad]\n+\n+    nograd_inputs, nograd_tensors = clone_inputs(False)\n+    recording_inputs, recording_tensors = clone_inputs(True)\n+\n+    # test no gradients case\n+    outputs = reference_func(*nograd_inputs)\n+    outputs_test = func(*nograd_inputs)\n+    self.assertEqual(outputs, outputs_test)\n+\n+    # test single grad case\n+    outputs = reference_func(*recording_inputs)\n+    grads = torch.autograd.grad(allSum(outputs), recording_tensors,\n+                                allow_unused=allow_unused)\n+\n+    outputs_test = func(*recording_inputs)\n+    grads_test = torch.autograd.grad(allSum(outputs_test), recording_tensors,\n+                                     allow_unused=allow_unused)\n+    self.assertEqual(outputs, outputs_test)\n+    self.assertEqual(grads, grads_test)\n+\n+    # test the grad grad case\n+\n+    outputs = reference_func(*recording_inputs)\n+    l1 = allSum(outputs)\n+    grads = torch.autograd.grad(l1, recording_tensors, create_graph=True,\n+                                allow_unused=allow_unused)\n+    l2 = (allSum(grads) * l1)\n+    grads2 = torch.autograd.grad(l2, recording_tensors, allow_unused=allow_unused)\n+\n+    recording_inputs, recording_tensors = clone_inputs(True)\n+\n+    outputs_test = func(*recording_inputs)\n+    l1_test = allSum(outputs_test)\n+    grads_test = torch.autograd.grad(\n+        l1_test, recording_tensors, create_graph=True, allow_unused=allow_unused)\n+    l2_test = (allSum(grads_test) * l1_test)\n+    grads2_test = torch.autograd.grad(l2_test, recording_tensors, allow_unused=allow_unused)\n+\n+    self.assertEqual(outputs, outputs_test)\n+    self.assertEqual(grads, grads_test)\n+    for g2, g2_test in zip(grads2, grads2_test):\n+        if g2 is None and g2_ge is None:\n+            continue\n+        self.assertTrue(torch.allclose(g2, g2_test, atol=5e-4, rtol=1e-4))", "path": "test/test_autograd.py", "position": null, "original_position": 367, "commit_id": "51abb5c20b9aa98254108e37afdec2eb76199a03", "original_commit_id": "bf5b6a352be3187849366e3a6322cc135c32a28c", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "I can factor this into a common function", "created_at": "2018-06-07T19:46:59Z", "updated_at": "2018-11-23T15:45:10Z", "html_url": "https://github.com/pytorch/pytorch/pull/8145#discussion_r193868466", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8145", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193868466"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8145#discussion_r193868466"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8145"}}, "body_html": "<p>I can factor this into a common function</p>", "body_text": "I can factor this into a common function", "in_reply_to_id": 193006349}