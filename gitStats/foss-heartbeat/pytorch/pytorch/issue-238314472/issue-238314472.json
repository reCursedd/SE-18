{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1899", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1899/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1899/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1899/events", "html_url": "https://github.com/pytorch/pytorch/issues/1899", "id": 238314472, "node_id": "MDU6SXNzdWUyMzgzMTQ0NzI=", "number": 1899, "title": "How to achieve grad multiply in pytorch", "user": {"login": "wjbianjason", "id": 9250467, "node_id": "MDQ6VXNlcjkyNTA0Njc=", "avatar_url": "https://avatars2.githubusercontent.com/u/9250467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wjbianjason", "html_url": "https://github.com/wjbianjason", "followers_url": "https://api.github.com/users/wjbianjason/followers", "following_url": "https://api.github.com/users/wjbianjason/following{/other_user}", "gists_url": "https://api.github.com/users/wjbianjason/gists{/gist_id}", "starred_url": "https://api.github.com/users/wjbianjason/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wjbianjason/subscriptions", "organizations_url": "https://api.github.com/users/wjbianjason/orgs", "repos_url": "https://api.github.com/users/wjbianjason/repos", "events_url": "https://api.github.com/users/wjbianjason/events{/privacy}", "received_events_url": "https://api.github.com/users/wjbianjason/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-24T12:24:32Z", "updated_at": "2017-06-24T14:20:55Z", "closed_at": "2017-06-24T14:20:52Z", "author_association": "NONE", "body_html": "<p>I want to transform a torch code to pytorch, torch code as following</p>\n<div class=\"highlight highlight-source-lua\"><pre>tensor_name <span class=\"pl-k\">=</span> nn.<span class=\"pl-c1\">GradMultiply</span>(tensor_name, <span class=\"pl-c1\">3</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">--</span> GradMultiply define</span>\n<span class=\"pl-k\">local</span> GradMultiply, parent <span class=\"pl-k\">=</span> torch.<span class=\"pl-c1\">class</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nn.GradMultiply<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>nn.Container<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">function</span> <span class=\"pl-en\">GradMultiply:__init</span>(<span class=\"pl-smi\">module</span>, <span class=\"pl-smi\">factor</span>)\n    parent.<span class=\"pl-c1\">__init</span>(<span class=\"pl-c1\">self</span>)\n    <span class=\"pl-c1\">self</span>.<span class=\"pl-smi\">modules</span>[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">=</span> module\n    <span class=\"pl-c1\">self</span>.<span class=\"pl-smi\">factor</span> <span class=\"pl-k\">=</span> factor\n<span class=\"pl-k\">end</span>\n<span class=\"pl-k\">function</span> <span class=\"pl-en\">GradMultiply:updateOutput</span>(<span class=\"pl-smi\">input</span>)\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.<span class=\"pl-smi\">modules</span>[<span class=\"pl-c1\">1</span>]:<span class=\"pl-c1\">updateOutput</span>(input)\n<span class=\"pl-k\">end</span>\n<span class=\"pl-k\">function</span> <span class=\"pl-en\">GradMultiply:updateGradInput</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">gradOutput</span>)\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.<span class=\"pl-smi\">modules</span>[<span class=\"pl-c1\">1</span>]:<span class=\"pl-c1\">updateGradInput</span>(input, gradOutput)\n<span class=\"pl-k\">end</span>\n<span class=\"pl-k\">function</span> <span class=\"pl-en\">GradMultiply:accGradParameters</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">gradOutput</span>, <span class=\"pl-smi\">scale</span>)\n    scale <span class=\"pl-k\">=</span> scale <span class=\"pl-k\">or</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.<span class=\"pl-smi\">modules</span>[<span class=\"pl-c1\">1</span>]:<span class=\"pl-c1\">accGradParameters</span>(input, gradOutput,\n        scale <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.<span class=\"pl-smi\">factor</span>)\n<span class=\"pl-k\">end</span>\n<span class=\"pl-k\">function</span> <span class=\"pl-en\">GradMultiply:accUpdateGradParameters</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">gradOutput</span>, <span class=\"pl-smi\">lr</span>)\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.<span class=\"pl-smi\">modules</span>[<span class=\"pl-c1\">1</span>]:<span class=\"pl-c1\">accUpdateGradParameters</span>(input, gradOutput,\n        lr <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.<span class=\"pl-smi\">factor</span>)\n<span class=\"pl-k\">end</span></pre></div>\n<p>--<br>\nI write a pytorch code to achieve this ability but failed.</p>\n<div class=\"highlight highlight-source-python\"><pre> hook <span class=\"pl-k\">=</span> tensor_name.register_hook(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">grad</span>: grad <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span>)</pre></div>\n<p>the fail info as below</p>\n<pre><code>hook = encoder_out_t.register_hook(lambda grad: grad * 1. / self.encoder.n_layers / 2)\nFile \"/home/bianwj/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 174, in register_hook\nraise RuntimeError(\"cannot register a hook on a volatile variable\")\n</code></pre>", "body_text": "I want to transform a torch code to pytorch, torch code as following\ntensor_name = nn.GradMultiply(tensor_name, 3)\n-- GradMultiply define\nlocal GradMultiply, parent = torch.class('nn.GradMultiply', 'nn.Container')\nfunction GradMultiply:__init(module, factor)\n    parent.__init(self)\n    self.modules[1] = module\n    self.factor = factor\nend\nfunction GradMultiply:updateOutput(input)\n    return self.modules[1]:updateOutput(input)\nend\nfunction GradMultiply:updateGradInput(input, gradOutput)\n    return self.modules[1]:updateGradInput(input, gradOutput)\nend\nfunction GradMultiply:accGradParameters(input, gradOutput, scale)\n    scale = scale or 1\n    return self.modules[1]:accGradParameters(input, gradOutput,\n        scale * self.factor)\nend\nfunction GradMultiply:accUpdateGradParameters(input, gradOutput, lr)\n    return self.modules[1]:accUpdateGradParameters(input, gradOutput,\n        lr * self.factor)\nend\n--\nI write a pytorch code to achieve this ability but failed.\n hook = tensor_name.register_hook(lambda grad: grad * 3)\nthe fail info as below\nhook = encoder_out_t.register_hook(lambda grad: grad * 1. / self.encoder.n_layers / 2)\nFile \"/home/bianwj/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 174, in register_hook\nraise RuntimeError(\"cannot register a hook on a volatile variable\")", "body": "I want to transform a torch code to pytorch, torch code as following\r\n```lua\r\ntensor_name = nn.GradMultiply(tensor_name, 3)\r\n-- GradMultiply define\r\nlocal GradMultiply, parent = torch.class('nn.GradMultiply', 'nn.Container')\r\nfunction GradMultiply:__init(module, factor)\r\n    parent.__init(self)\r\n    self.modules[1] = module\r\n    self.factor = factor\r\nend\r\nfunction GradMultiply:updateOutput(input)\r\n    return self.modules[1]:updateOutput(input)\r\nend\r\nfunction GradMultiply:updateGradInput(input, gradOutput)\r\n    return self.modules[1]:updateGradInput(input, gradOutput)\r\nend\r\nfunction GradMultiply:accGradParameters(input, gradOutput, scale)\r\n    scale = scale or 1\r\n    return self.modules[1]:accGradParameters(input, gradOutput,\r\n        scale * self.factor)\r\nend\r\nfunction GradMultiply:accUpdateGradParameters(input, gradOutput, lr)\r\n    return self.modules[1]:accUpdateGradParameters(input, gradOutput,\r\n        lr * self.factor)\r\nend\r\n```\r\n--\r\nI write a pytorch code to achieve this ability but failed.\r\n```python\r\n hook = tensor_name.register_hook(lambda grad: grad * 3)\r\n```\r\nthe fail info as below\r\n```\r\nhook = encoder_out_t.register_hook(lambda grad: grad * 1. / self.encoder.n_layers / 2)\r\nFile \"/home/bianwj/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 174, in register_hook\r\nraise RuntimeError(\"cannot register a hook on a volatile variable\")\r\n```"}