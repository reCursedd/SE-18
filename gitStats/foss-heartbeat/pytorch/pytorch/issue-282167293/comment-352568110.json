{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/352568110", "html_url": "https://github.com/pytorch/pytorch/pull/4174#issuecomment-352568110", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4174", "id": 352568110, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MjU2ODExMA==", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-18T21:47:48Z", "updated_at": "2017-12-18T21:48:01Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=31876421\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yongjik\">@yongjik</a> I wanted to check if it does fix the <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"279128190\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4010\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4010/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4010\">#4010</a>. But it still does:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(a.expand(<span class=\"pl-c1\">len</span>(a), <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>).storage().size())\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 5</span>\n<span class=\"pl-c1\">print</span>(a.expand(<span class=\"pl-c1\">len</span>(a), <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>).mul(<span class=\"pl-c1\">5</span>).storage().size())\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> 500</span></pre></div>\n<p>I thought this PR would rearrange, then collapse dims (like the zero strides in this example), and perform the op only on the small tensor, and then introduce the zero strides back: <a href=\"https://github.com/yongjik/pytorch/blob/22796bd9cac51ba64adca240508284cb3d49a5e4/aten/src/THC/THCApply.cuh#L305-L306\">https://github.com/yongjik/pytorch/blob/22796bd9cac51ba64adca240508284cb3d49a5e4/aten/src/THC/THCApply.cuh#L305-L306</a></p>\n<p>But the example results in the old behavior, and a larger tensor is allocated for the result. Is it the expected behavior for the scope of this PR?</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> I remember our discussion, but it seemed that this solves it</p>", "body_text": "@yongjik I wanted to check if it does fix the #4010. But it still does:\na = torch.ones(5, 1, 1)\nprint(a.expand(len(a), 10, 10).storage().size())\n# 5\nprint(a.expand(len(a), 10, 10).mul(5).storage().size())\n# 500\nI thought this PR would rearrange, then collapse dims (like the zero strides in this example), and perform the op only on the small tensor, and then introduce the zero strides back: https://github.com/yongjik/pytorch/blob/22796bd9cac51ba64adca240508284cb3d49a5e4/aten/src/THC/THCApply.cuh#L305-L306\nBut the example results in the old behavior, and a larger tensor is allocated for the result. Is it the expected behavior for the scope of this PR?\n@soumith I remember our discussion, but it seemed that this solves it", "body": "@yongjik I wanted to check if it does fix the https://github.com/pytorch/pytorch/issues/4010. But it still does:\r\n```python\r\na = torch.ones(5, 1, 1)\r\nprint(a.expand(len(a), 10, 10).storage().size())\r\n# 5\r\nprint(a.expand(len(a), 10, 10).mul(5).storage().size())\r\n# 500\r\n```\r\n\r\nI thought this PR would rearrange, then collapse dims (like the zero strides in this example), and perform the op only on the small tensor, and then introduce the zero strides back: https://github.com/yongjik/pytorch/blob/22796bd9cac51ba64adca240508284cb3d49a5e4/aten/src/THC/THCApply.cuh#L305-L306\r\n\r\nBut the example results in the old behavior, and a larger tensor is allocated for the result. Is it the expected behavior for the scope of this PR?\r\n\r\n@soumith I remember our discussion, but it seemed that this solves it"}