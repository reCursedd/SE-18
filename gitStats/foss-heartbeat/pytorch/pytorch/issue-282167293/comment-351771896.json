{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/351771896", "html_url": "https://github.com/pytorch/pytorch/pull/4174#issuecomment-351771896", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4174", "id": 351771896, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTc3MTg5Ng==", "user": {"login": "yongjik", "id": 31876421, "node_id": "MDQ6VXNlcjMxODc2NDIx", "avatar_url": "https://avatars2.githubusercontent.com/u/31876421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yongjik", "html_url": "https://github.com/yongjik", "followers_url": "https://api.github.com/users/yongjik/followers", "following_url": "https://api.github.com/users/yongjik/following{/other_user}", "gists_url": "https://api.github.com/users/yongjik/gists{/gist_id}", "starred_url": "https://api.github.com/users/yongjik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yongjik/subscriptions", "organizations_url": "https://api.github.com/users/yongjik/orgs", "repos_url": "https://api.github.com/users/yongjik/repos", "events_url": "https://api.github.com/users/yongjik/events{/privacy}", "received_events_url": "https://api.github.com/users/yongjik/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-14T16:58:36Z", "updated_at": "2017-12-14T16:58:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For example, in my GTX 1080, doing simple addition <code>A += B</code> on two 2048*2048 float tensors take ~208 us.  However, if I transpose both of them (<code>A = A.t(); B = B.t()</code>), the same addition now takes ~2380(!!) us.</p>\n<p>After this change, both operations take about the same duration (~208 us).</p>\n<p>Unfortunately it does not help the cases when only one of them are transposed.  (If only A (output) is transposed: 776 us; if only B is transposed: 377 us).  In theory I could rearrange dimensions to prefer the output to be in the \"correct\" order (i.e., transform 776 us to 377 us), but I don't know if it would trash performance for other machines/operations, so I decided to tackle only the most obvious (and safe) case.</p>", "body_text": "For example, in my GTX 1080, doing simple addition A += B on two 2048*2048 float tensors take ~208 us.  However, if I transpose both of them (A = A.t(); B = B.t()), the same addition now takes ~2380(!!) us.\nAfter this change, both operations take about the same duration (~208 us).\nUnfortunately it does not help the cases when only one of them are transposed.  (If only A (output) is transposed: 776 us; if only B is transposed: 377 us).  In theory I could rearrange dimensions to prefer the output to be in the \"correct\" order (i.e., transform 776 us to 377 us), but I don't know if it would trash performance for other machines/operations, so I decided to tackle only the most obvious (and safe) case.", "body": "For example, in my GTX 1080, doing simple addition `A += B` on two 2048*2048 float tensors take ~208 us.  However, if I transpose both of them (`A = A.t(); B = B.t()`), the same addition now takes ~2380(!!) us.\r\n\r\nAfter this change, both operations take about the same duration (~208 us).\r\n\r\nUnfortunately it does not help the cases when only one of them are transposed.  (If only A (output) is transposed: 776 us; if only B is transposed: 377 us).  In theory I could rearrange dimensions to prefer the output to be in the \"correct\" order (i.e., transform 776 us to 377 us), but I don't know if it would trash performance for other machines/operations, so I decided to tackle only the most obvious (and safe) case."}