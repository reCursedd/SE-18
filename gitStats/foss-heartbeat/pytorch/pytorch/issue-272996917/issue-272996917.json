{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3627", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3627/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3627/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3627/events", "html_url": "https://github.com/pytorch/pytorch/issues/3627", "id": 272996917, "node_id": "MDU6SXNzdWUyNzI5OTY5MTc=", "number": 3627, "title": "Proposal: replace Variable.volatile with global switch", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-11-10T16:58:34Z", "updated_at": "2017-12-18T20:46:14Z", "closed_at": "2017-12-18T20:46:14Z", "author_association": "MEMBER", "body_html": "<p>Variable.volatile forces outputs to not require gradients if any of the inputs are marked volatile. This works OK in the forward pass, but we're forced to <a href=\"https://github.com/pytorch/pytorch/blob/b06c59e543aa26586087c19fb7b713f8872105bb/torch/csrc/autograd/functions/accumulate_grad.cpp#L38\">change the meaning</a> in the backwards. Gradients are sometimes volatile and sometimes not, which is awkward if you add them back to parameters, such as in optimizers.</p>\n<p>We should replace <code>volatile</code> with a context manager in Python. (Chainer already did this with <a href=\"http://docs.chainer.org/en/stable/reference/core/generated/chainer.no_backprop_mode.html#chainer.no_backprop_mode\" rel=\"nofollow\"><code>no_backprop_mode()</code></a>).</p>\n<p>At the C++ level, we should replace it with a thread-local global switch.</p>\n<p>This will simplify the logic in the backwards: by default <code>backwards()</code> will set \"no-backprop mode\", unless <code>create_graph</code> is True.</p>", "body_text": "Variable.volatile forces outputs to not require gradients if any of the inputs are marked volatile. This works OK in the forward pass, but we're forced to change the meaning in the backwards. Gradients are sometimes volatile and sometimes not, which is awkward if you add them back to parameters, such as in optimizers.\nWe should replace volatile with a context manager in Python. (Chainer already did this with no_backprop_mode()).\nAt the C++ level, we should replace it with a thread-local global switch.\nThis will simplify the logic in the backwards: by default backwards() will set \"no-backprop mode\", unless create_graph is True.", "body": "Variable.volatile forces outputs to not require gradients if any of the inputs are marked volatile. This works OK in the forward pass, but we're forced to [change the meaning](https://github.com/pytorch/pytorch/blob/b06c59e543aa26586087c19fb7b713f8872105bb/torch/csrc/autograd/functions/accumulate_grad.cpp#L38) in the backwards. Gradients are sometimes volatile and sometimes not, which is awkward if you add them back to parameters, such as in optimizers.\r\n\r\nWe should replace `volatile` with a context manager in Python. (Chainer already did this with [`no_backprop_mode()`](http://docs.chainer.org/en/stable/reference/core/generated/chainer.no_backprop_mode.html#chainer.no_backprop_mode)).\r\n\r\nAt the C++ level, we should replace it with a thread-local global switch.\r\n\r\nThis will simplify the logic in the backwards: by default `backwards()` will set \"no-backprop mode\", unless `create_graph` is True."}