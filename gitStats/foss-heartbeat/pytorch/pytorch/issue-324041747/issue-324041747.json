{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7642", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7642/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7642/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7642/events", "html_url": "https://github.com/pytorch/pytorch/issues/7642", "id": 324041747, "node_id": "MDU6SXNzdWUzMjQwNDE3NDc=", "number": 7642, "title": ".backward() not working in a method of a class", "user": {"login": "liligwu", "id": 20309639, "node_id": "MDQ6VXNlcjIwMzA5NjM5", "avatar_url": "https://avatars2.githubusercontent.com/u/20309639?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liligwu", "html_url": "https://github.com/liligwu", "followers_url": "https://api.github.com/users/liligwu/followers", "following_url": "https://api.github.com/users/liligwu/following{/other_user}", "gists_url": "https://api.github.com/users/liligwu/gists{/gist_id}", "starred_url": "https://api.github.com/users/liligwu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liligwu/subscriptions", "organizations_url": "https://api.github.com/users/liligwu/orgs", "repos_url": "https://api.github.com/users/liligwu/repos", "events_url": "https://api.github.com/users/liligwu/events{/privacy}", "received_events_url": "https://api.github.com/users/liligwu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-17T14:12:57Z", "updated_at": "2018-05-17T17:55:38Z", "closed_at": "2018-05-17T17:55:38Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Try to embed a .backward() Function in a .forward() of another Function, but faild.</p>\n<p>The point is that I cannot embed a Function into another Function. . For example, I want to put function B.forward() in function A.forward(), cache the necessary values, and embed function B.backward() in A.backward(). So B is a module in A.</p>\n<p>Actually, this piece of code should work well in Pytorch 0.1.10. But after a while, I want to continue my work in 0.4 it failed.</p>\n<p>I post the question in Pytorch forum, but no one answer it.<br>\n<a href=\"url\">https://discuss.pytorch.org/t/backward-not-working-in-a-method-of-a-class/18043</a></p>\n<h2>Code example</h2>\n<pre><code>x = np.array([[1.,2.], [3.,4.], [5.,6.]])\nx = Variable(torch.from_numpy(x).float(), requires_grad=True)\ny = var(x)\ny.backward(torch.randn(y.size()))     \n# it works fine up to here\nprint(x.grad)       \n\n# but -----------------\nclass Y_fun(Function):\n    def forward(self, x):\n        y = var(x)\n        y.backward(torch.randn(y.size()))\n        print(x.grad)      # exact same thing, but in a forward method, failed\n        return y\n\ndef y_fun(x):\n    return Y_fun()(x)\n\ny = y_fun(x)\n</code></pre>\n<p><strong>var() is also a Function which has .forward() and .backward()</strong></p>\n<h2>System Info</h2>\n<pre><code>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n</code></pre>\n<ul>\n<li>PyTorch or Caffe2: Pytorch 0.4</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: 0.4</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 9.1</li>\n<li>GPU models and configuration:</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version:</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "Issue description\nTry to embed a .backward() Function in a .forward() of another Function, but faild.\nThe point is that I cannot embed a Function into another Function. . For example, I want to put function B.forward() in function A.forward(), cache the necessary values, and embed function B.backward() in A.backward(). So B is a module in A.\nActually, this piece of code should work well in Pytorch 0.1.10. But after a while, I want to continue my work in 0.4 it failed.\nI post the question in Pytorch forum, but no one answer it.\nhttps://discuss.pytorch.org/t/backward-not-working-in-a-method-of-a-class/18043\nCode example\nx = np.array([[1.,2.], [3.,4.], [5.,6.]])\nx = Variable(torch.from_numpy(x).float(), requires_grad=True)\ny = var(x)\ny.backward(torch.randn(y.size()))     \n# it works fine up to here\nprint(x.grad)       \n\n# but -----------------\nclass Y_fun(Function):\n    def forward(self, x):\n        y = var(x)\n        y.backward(torch.randn(y.size()))\n        print(x.grad)      # exact same thing, but in a forward method, failed\n        return y\n\ndef y_fun(x):\n    return Y_fun()(x)\n\ny = y_fun(x)\n\nvar() is also a Function which has .forward() and .backward()\nSystem Info\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n\n\nPyTorch or Caffe2: Pytorch 0.4\nHow you installed PyTorch (conda, pip, source): conda\nBuild command you used (if compiling from source):\nOS: Ubuntu 16.04\nPyTorch version: 0.4\nPython version: 3.6\nCUDA/cuDNN version: 9.1\nGPU models and configuration:\nGCC version (if compiling from source):\nCMake version:\nVersions of any other relevant libraries:", "body": "## Issue description\r\n\r\nTry to embed a .backward() Function in a .forward() of another Function, but faild.\r\n\r\nThe point is that I cannot embed a Function into another Function. . For example, I want to put function B.forward() in function A.forward(), cache the necessary values, and embed function B.backward() in A.backward(). So B is a module in A.\r\n\r\nActually, this piece of code should work well in Pytorch 0.1.10. But after a while, I want to continue my work in 0.4 it failed. \r\n\r\nI post the question in Pytorch forum, but no one answer it.\r\n[https://discuss.pytorch.org/t/backward-not-working-in-a-method-of-a-class/18043](url)\r\n\r\n## Code example\r\n\r\n```\r\nx = np.array([[1.,2.], [3.,4.], [5.,6.]])\r\nx = Variable(torch.from_numpy(x).float(), requires_grad=True)\r\ny = var(x)\r\ny.backward(torch.randn(y.size()))     \r\n# it works fine up to here\r\nprint(x.grad)       \r\n\r\n# but -----------------\r\nclass Y_fun(Function):\r\n    def forward(self, x):\r\n        y = var(x)\r\n        y.backward(torch.randn(y.size()))\r\n        print(x.grad)      # exact same thing, but in a forward method, failed\r\n        return y\r\n\r\ndef y_fun(x):\r\n    return Y_fun()(x)\r\n\r\ny = y_fun(x)\r\n```\r\n**var() is also a Function which has .forward() and .backward()**\r\n## System Info\r\n\r\n```\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```\r\n\r\n- PyTorch or Caffe2: Pytorch 0.4\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Build command you used (if compiling from source):\r\n- OS: Ubuntu 16.04\r\n- PyTorch version: 0.4\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.1\r\n- GPU models and configuration:\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n\r\n"}