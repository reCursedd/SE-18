{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/359943842", "html_url": "https://github.com/pytorch/pytorch/issues/4219#issuecomment-359943842", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4219", "id": 359943842, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTk0Mzg0Mg==", "user": {"login": "justinchiu", "id": 2389353, "node_id": "MDQ6VXNlcjIzODkzNTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2389353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/justinchiu", "html_url": "https://github.com/justinchiu", "followers_url": "https://api.github.com/users/justinchiu/followers", "following_url": "https://api.github.com/users/justinchiu/following{/other_user}", "gists_url": "https://api.github.com/users/justinchiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/justinchiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/justinchiu/subscriptions", "organizations_url": "https://api.github.com/users/justinchiu/orgs", "repos_url": "https://api.github.com/users/justinchiu/repos", "events_url": "https://api.github.com/users/justinchiu/events{/privacy}", "received_events_url": "https://api.github.com/users/justinchiu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-23T21:53:58Z", "updated_at": "2018-01-23T22:06:27Z", "author_association": "NONE", "body_html": "<p>I think this is related...I'm getting:</p>\n<pre><code>&gt;&gt;&gt; x = torch.arange(0, 25).view(5,5); longidx = torch.LongTensor([0,1]); byteidx = torch.ByteTensor([1,1,0,0,0])\n&gt;&gt;&gt; print(x[longidx]); print(x[:, longidx])\n\n 0  1  2  3  4\n 5  6  7  8  9\n[torch.FloatTensor of size 2x5]\n\n\n  0   1\n  5   6\n 10  11\n 15  16\n 20  21\n[torch.FloatTensor of size 5x2]\n\n&gt;&gt;&gt; x[byteidx]\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nRuntimeError: inconsistent tensor size, expected src [5 x 5] and mask [5] to have the same number of elements, but got 25 and 5 elements respectively at /n/home13/jchiu/python/pytorch/aten/src/TH/generic/THTensorMath.c:207\n&gt;&gt;&gt; x[:, byteidx]\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nTypeError: Performing basic indexing on a tensor and encountered an error indexing dim 1 with an object of type torch.ByteTensor. The only supported types are integers, slices, numpy scalars, or if indexing with a torch.LongTensor or torch.ByteTensor only a single Tensor may be passed.\n</code></pre>\n<p>I think byte masking like this would be pretty useful (masking along dimensions should be doable and there should be parity with indexing ops), am I doing something wrong?<br>\nI'm on <code>torch.__version__ = '0.4.0a0+43ab911'</code></p>", "body_text": "I think this is related...I'm getting:\n>>> x = torch.arange(0, 25).view(5,5); longidx = torch.LongTensor([0,1]); byteidx = torch.ByteTensor([1,1,0,0,0])\n>>> print(x[longidx]); print(x[:, longidx])\n\n 0  1  2  3  4\n 5  6  7  8  9\n[torch.FloatTensor of size 2x5]\n\n\n  0   1\n  5   6\n 10  11\n 15  16\n 20  21\n[torch.FloatTensor of size 5x2]\n\n>>> x[byteidx]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: inconsistent tensor size, expected src [5 x 5] and mask [5] to have the same number of elements, but got 25 and 5 elements respectively at /n/home13/jchiu/python/pytorch/aten/src/TH/generic/THTensorMath.c:207\n>>> x[:, byteidx]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: Performing basic indexing on a tensor and encountered an error indexing dim 1 with an object of type torch.ByteTensor. The only supported types are integers, slices, numpy scalars, or if indexing with a torch.LongTensor or torch.ByteTensor only a single Tensor may be passed.\n\nI think byte masking like this would be pretty useful (masking along dimensions should be doable and there should be parity with indexing ops), am I doing something wrong?\nI'm on torch.__version__ = '0.4.0a0+43ab911'", "body": "I think this is related...I'm getting:\r\n```\r\n>>> x = torch.arange(0, 25).view(5,5); longidx = torch.LongTensor([0,1]); byteidx = torch.ByteTensor([1,1,0,0,0])\r\n>>> print(x[longidx]); print(x[:, longidx])\r\n\r\n 0  1  2  3  4\r\n 5  6  7  8  9\r\n[torch.FloatTensor of size 2x5]\r\n\r\n\r\n  0   1\r\n  5   6\r\n 10  11\r\n 15  16\r\n 20  21\r\n[torch.FloatTensor of size 5x2]\r\n\r\n>>> x[byteidx]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: inconsistent tensor size, expected src [5 x 5] and mask [5] to have the same number of elements, but got 25 and 5 elements respectively at /n/home13/jchiu/python/pytorch/aten/src/TH/generic/THTensorMath.c:207\r\n>>> x[:, byteidx]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: Performing basic indexing on a tensor and encountered an error indexing dim 1 with an object of type torch.ByteTensor. The only supported types are integers, slices, numpy scalars, or if indexing with a torch.LongTensor or torch.ByteTensor only a single Tensor may be passed.\r\n```\r\nI think byte masking like this would be pretty useful (masking along dimensions should be doable and there should be parity with indexing ops), am I doing something wrong?\r\nI'm on `torch.__version__ = '0.4.0a0+43ab911'`"}