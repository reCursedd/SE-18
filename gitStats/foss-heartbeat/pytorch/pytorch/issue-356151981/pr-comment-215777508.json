{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215777508", "pull_request_review_id": 153120082, "id": 215777508, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTc3NzUwOA==", "diff_hunk": "@@ -0,0 +1,599 @@\n+syntax = \"proto2\";\n+\n+package torch;\n+\n+// Overview\n+//\n+// ONNX is an open specification that is comprised of the following components:\n+//\n+// 1)  A definition of an extensible computation graph model.\n+// 2)  Definitions of standard data types.\n+// 3)  Definitions of built-in operators.\n+//\n+// This document describes the syntax of models and their computation graphs,\n+// as well as the standard data types. Together, they are referred to as the ONNX\n+// Intermediate Representation, or 'IR' for short. \n+//\n+// The normative semantic specification of the ONNX IR is found in docs/IR.md.\n+// Definitions of the built-in neural network operators may be found in docs/Operators.md.\n+\n+// Notes\n+//\n+// Release\n+//\n+// We are still in the very early stage of defining ONNX. The current\n+// version of ONNX is a starting point. While we are actively working\n+// towards a complete spec, we would like to get the community involved\n+// by sharing our working version of ONNX.\n+//\n+// Protobuf compatibility\n+//\n+// To simplify framework compatibility, ONNX is defined using the subset of\n+// protobuf that is compatible with both protobuf v2 and v3. This means that we\n+// do not use any protobuf features that are only available in one of the two\n+// versions.\n+//\n+// Here are the most notable contortions we have to carry out to work around\n+// these limitations:\n+//\n+//   - No 'map' (added protobuf 3.0). We instead represent mappings as lists\n+//     of key-value pairs, where order does not matter and duplicates\n+//     are not allowed.\n+\n+// Versioning\n+//\n+// ONNX versioning is specified in docs/IR.md and elaborated on in docs/Versioning.md\n+//\n+// To be compatible with both proto2 and proto3, we will use a version number\n+// that is not defined by the default value but an explicit enum number.\n+enum Version {\n+  // proto3 requires the first enum value to be zero.\n+  // We add this just to appease the compiler.\n+  _START_VERSION = 0;\n+  // The version field is always serialized and we will use it to store the\n+  // version that the  graph is generated from. This helps us set up version\n+  // control.\n+  // For the IR, we are using simple numbers starting with with 0x00000001,\n+  // which was the version we published on Oct 10, 2017.\n+  IR_VERSION_2017_10_10 = 0x0000000000000001;\n+\n+  // IR_VERSION 2 published on Oct 30, 2017\n+  // - Added type discriminator to AttributeProto to support proto3 users\n+  IR_VERSION_2017_10_30 = 0x0000000000000002;\n+\n+  // IR VERSION 3 published on Nov 3, 2017\n+  // - For operator versioning:\n+  //    - Added new message OperatorSetIdProto\n+  //    - Added opset_import in ModelProto\n+  // - For vendor extensions, added domain in NodeProto\n+  IR_VERSION_NEWEST_ONNX = 0x0000000000000003;\n+\n+  // PYTORCH IR VERSION\n+  IR_VERSION_NEWEST = 0x0000000000000103;\n+}\n+\n+// Attributes\n+//\n+// A named attribute containing either singular float, integer, string, graph,\n+// and tensor values, or repeated float, integer, string, graph, and tensor values.\n+// An AttributeProto MUST contain the name field, and *only one* of the\n+// following content fields, effectively enforcing a C/C++ union equivalent.\n+message AttributeProto {\n+\n+  // Note: this enum is structurally identical to the OpSchema::AttrType\n+  // enum defined in schema.h.  If you rev one, you likely need to rev the other.\n+  enum AttributeType {\n+    UNDEFINED = 0;\n+    FLOAT = 1;\n+    INT = 2;\n+    STRING = 3;\n+    TENSOR = 4;\n+    GRAPH = 5;\n+\n+    FLOATS = 6;\n+    INTS = 7;\n+    STRINGS = 8;\n+    TENSORS = 9;\n+    GRAPHS = 10;\n+  }\n+\n+  // The name field MUST be present for this version of the IR.\n+  optional string name = 1;           // namespace Attribute\n+ \n+  // if ref_attr_name is not empty, ref_attr_name is the attribute name in parent function.\n+  // In this case, this AttributeProto does not contain data, and it's a reference of attribute\n+  // in parent scope.\n+  // NOTE: This should ONLY be used in function (sub-graph). It's invalid to be used in main graph.\n+  optional string ref_attr_name = 21;\n+\n+  // A human-readable documentation for this attribute. Markdown is allowed.\n+  optional string doc_string = 13;\n+\n+  // The type field MUST be present for this version of the IR.\n+  // For 0.0.1 versions of the IR, this field was not defined, and\n+  // implementations needed to use has_field hueristics to determine\n+  // which value field was in use.  For IR_VERSION 0.0.2 or later, this\n+  // field MUST be set and match the f|i|s|t|... field in use.  This\n+  // change was made to accomodate proto3 implementations.\n+  optional AttributeType type = 20;   // discriminator that indicates which field below is in use\n+\n+  // Exactly ONE of the following fields must be present for this version of the IR\n+  optional float f = 2;               // float\n+  optional int64 i = 3;               // int\n+  optional bytes s = 4;               // UTF-8 string\n+  optional TensorProto t = 5;         // tensor value\n+  optional GraphProto g = 6;          // graph\n+  // Do not use field below, it's deprecated.\n+  // optional ValueProto v = 12;         // value - subsumes everything but graph\n+\n+  repeated float floats = 7;          // list of floats\n+  repeated int64 ints = 8;            // list of ints\n+  repeated bytes strings = 9;         // list of UTF-8 strings\n+  repeated TensorProto tensors = 10;  // list of tensors\n+  repeated GraphProto graphs = 11;    // list of graph\n+}\n+\n+// Defines information on value, including the name, the type, and\n+// the shape of the value.\n+message ValueInfoProto {\n+  // This field MUST be present in this version of the IR.\n+  optional string name = 1;     // namespace Value\n+  // This field MUST be present in this version of the IR.\n+  optional TypeProto type = 2;\n+  // A human-readable documentation for this value. Markdown is allowed.\n+  optional string doc_string = 3;\n+}\n+\n+// Nodes\n+//\n+// Computation graphs are made up of a DAG of nodes, which represent what is\n+// commonly called a \"layer\" or \"pipeline stage\" in machine learning frameworks.\n+//\n+// For example, it can be a node of type \"Conv\" that takes in an image, a filter \n+// tensor and a bias tensor, and produces the convolved output.\n+message NodeProto {\n+  repeated string input = 1;    // namespace Value\n+  repeated string output = 2;   // namespace Value\n+\n+  // An optional identifier for this node in a graph.\n+  // This field MAY be absent in ths version of the IR.\n+  optional string name = 3;     // namespace Node\n+\n+  // The symbolic identifier of the Operator to execute.\n+  optional string op_type = 4;  // namespace Operator\n+  // The domain of the OperatorSet that specifies the operator named by op_type.\n+  optional string domain = 7;   // namespace Domain\n+\n+  // Additional named attributes.\n+  repeated AttributeProto attribute = 5;\n+\n+  // A human-readable documentation for this node. Markdown is allowed.\n+  optional string doc_string = 6;\n+\n+  // Additional annotations, attributes are defined in Schema\n+  // To be added as annotations:\n+  //    string engine\n+  //    string list control_input\n+  //    int64 is_gradient_op\n+  //    string debug_info\n+  repeated AttributeProto annotations = 8;\n+\n+  // Node type, like PythonOp, etc, purely for PyTorch\n+  optional string node_type = 51;\n+}\n+\n+// Models\n+//\n+// ModelProto is a top-level file/container format for bundling a ML model and\n+// associating its computation graph with metadata.\n+//\n+// The semantics of the model are described by the associated GraphProto.\n+//\n+// Model ==> Caffe2 MetaNetDef\n+//       ==> PyTorch Module\n+message ModelProto {\n+  // The version of the IR this model targets. See Version enum above.\n+  // This field MUST be present.\n+  optional int64 ir_version = 1;\n+\n+  // The OperatorSets this model relies on.\n+  // All ModelProtos MUST have at least one entry that\n+  // specifies which version of the ONNX OperatorSet is\n+  // being imported.\n+  //\n+  // All nodes in the ModelProto's graph will bind against the operator\n+  // with the same-domain/same-op_type operator with the HIGHEST version\n+  // in the referenced operator sets.\n+  repeated OperatorSetIdProto opset_import = 8;\n+\n+  // The name of the framework or tool used to generate this model.\n+  // This field SHOULD be present to indicate which implementation/tool/framework\n+  // emitted the model.\n+  optional string producer_name = 2;\n+\n+  // The version of the framework or tool used to generate this model.\n+  // This field SHOULD be present to indicate which implementation/tool/framework\n+  // emitted the model.\n+  optional string producer_version = 3;\n+\n+  // Domain name of the model.\n+  // We use reverse domain names as name space indicators. For example:\n+  // `com.facebook.fair` or `com.microsoft.cognitiveservices`\n+  //\n+  // Together with `model_version` and GraphProto.name, this forms the unique identity of\n+  // the graph.\n+  optional string domain = 4;\n+\n+  // The version of the graph encoded. See Version enum below.\n+  optional int64 model_version = 5;\n+\n+  // A human-readable documentation for this model. Markdown is allowed.\n+  optional string doc_string = 6;\n+\n+  // The parameterized graph that is evaluated to execute the model.\n+  // The main graph, in single graph case, it is ONNX compatible.\n+  optional GraphProto graph = 7;\n+\n+  // The remaining nets in MetaNetDef.\n+  // Submodules and methods in PyTorch.\n+  repeated GraphProto methods = 15;\n+\n+  // Named metadata values; keys should be distinct.\n+  // Many meta data in MetaNetDef and preditor are piggy backed here.\n+  // 1) project\n+  // 2) model_class\n+  // 3) internal_version\n+  // 4) predictor_type\n+  // 5) predictor_id\n+  // 6) execute_plan\n+  // 7) applicationSpecificInfo (another string map, need to verify it has no duplicate.)\n+  // 8) engine\n+  // 9) publish time\n+  repeated StringStringEntryProto metadata_props = 14;\n+\n+  // Model name\n+  optional string name = 16;\n+\n+  // Model name\n+  repeated AttributeProto annotations = 17;\n+\n+  // Mapping from list name to blob name list, must be string list type.\n+  // Equivalent to blobs in MetaNetDef.\n+  repeated AttributeProto blob_lists = 51;\n+\n+  // Mapping from plan name to serialized plan, must be string list type.\n+  // Equivalent to plans in MetaNetDef.\n+  repeated AttributeProto plans = 52;\n+};\n+\n+// StringStringEntryProto follows the pattern for cross-proto-version maps.\n+// See https://developers.google.com/protocol-buffers/docs/proto3#maps\n+message StringStringEntryProto {\n+  optional string key = 1;\n+  optional string value= 2;\n+};\n+\n+// Graphs\n+//\n+// A graph defines the computational logic of a model and is comprised of a parameterized \n+// list of nodes that form a directed acyclic graph based on their inputs and outputs.\n+// This is the equivalent of the \"network\" or \"graph\" in many deep learning\n+// frameworks.\n+// Graph ==> NetDef in Caffe2\n+//       ==> Submodule/Method in PyTorch\n+message GraphProto {\n+  // The nodes in the graph, sorted topologically.\n+  repeated NodeProto node = 1;\n+\n+  // The name of the graph.\n+  optional string name = 2;   // namespace Graph\n+\n+  // A list of named tensor values, used to specify constant inputs of the graph.\n+  // Each TensorProto entry must have a distinct name (within the list) that\n+  // also appears in the input list.\n+  repeated TensorProto initializer = 5;\n+\n+  // A human-readable documentation for this graph. Markdown is allowed.\n+  optional string doc_string = 10;\n+\n+  // The inputs and outputs of the graph.\n+  repeated ValueInfoProto input = 11;\n+  repeated ValueInfoProto output = 12;\n+\n+  // Information for the values in the graph. The ValueInfoProto.name's\n+  // must be distinct. It is optional for a value to appear in value_info list.\n+  repeated ValueInfoProto value_info = 13;\n+\n+  // Additional annotations.\n+  repeated AttributeProto annotations = 14;\n+\n+  // DO NOT USE the following fields, they were deprecated from earlier versions.\n+  // repeated string input = 3;\n+  // repeated string output = 4;\n+  // optional int64 ir_version = 6;\n+  // optional int64 producer_version = 7;\n+  // optional string producer_tag = 8;\n+  // optional string domain = 9;\n+\n+  // Graph type, equivalent to Caffe2's NetDef.type\n+  optional string type = 51;\n+}\n+\n+// Tensors\n+//\n+// A serialized tensor value.\n+message TensorProto {\n+  enum DataType {\n+    UNDEFINED = 0;\n+    // Basic types.\n+    FLOAT = 1;   // float\n+    UINT8 = 2;   // uint8_t\n+    INT8 = 3;    // int8_t\n+    UINT16 = 4;  // uint16_t\n+    INT16 = 5;   // int16_t\n+    INT32 = 6;   // int32_t\n+    INT64 = 7;   // int64_t\n+    STRING = 8;  // string\n+    BOOL = 9;    // bool\n+\n+    // Advanced types\n+    FLOAT16 = 10;\n+    DOUBLE = 11;\n+    UINT32 = 12;\n+    UINT64 = 13;\n+    COMPLEX64 = 14;     // complex with float32 real and imaginary components\n+    COMPLEX128 = 15;    // complex with float64 real and imaginary components\n+    // Future extensions go here.\n+\n+    // Special data type, real type information is stored in ValueInfoProto.\n+    // If data_type is SPECIAL, raw_data should be used.\n+    SPECIAL = 51;\n+  }\n+\n+  // The shape of the tensor.\n+  repeated int64 dims = 1;\n+  repeated int64 strides = 14;\n+\n+  // The data type of the tensor.\n+  optional DataType data_type = 2;\n+\n+  // For very large tensors, we may want to store them in chunks, in which\n+  // case the following fields will specify the segment that is stored in\n+  // the current TensorProto.\n+  message Segment {\n+    optional int64 begin = 1;\n+    optional int64 end = 2;\n+    optional int64 chuck_num = 51;\n+    optional int64 chuck_id = 52;\n+  }\n+  // Used as offset in the external shared data.\n+  optional Segment segment = 3;\n+\n+  // Tensor content must be organized in row-major order.\n+  //\n+  // Depending on the data_type field, exactly one of the fields below with\n+  // name ending in _data is used to store the elements of the tensor.\n+\n+  // For float and complex64 values\n+  // Complex64 tensors are encoded as a single array of floats,\n+  // with the real components appearing in odd numbered positions,\n+  // and the corresponding imaginary component apparing in the\n+  // subsequent even numbered position. (e.g., [1.0 + 2.0i, 3.0 + 4.0i]\n+  // is encoded as [1.0, 2.0 ,3.0 ,4.0]\n+  // When this field is present, the data_type field MUST be FLOAT or COMPLEX64.\n+  repeated float float_data = 4 [packed = true];\n+\n+  // For int32, uint8, int8, uint16, int16, bool, and float16 values\n+  // float16 values must be bit-wise converted to an uint16_t prior\n+  // to writing to the buffer.\n+  // When this field is present, the data_type field MUST be\n+  // INT32, INT16, INT8, UINT16, INT8, BOOL, or FLOAT16\n+  repeated int32 int32_data = 5 [packed = true];\n+\n+  // For strings.\n+  // Each element of string_data is a UTF-8 encoded Unicode\n+  // string. No trailing null, no leading BOM. The protobuf \"string\"\n+  // scalar type is not used to match ML community conventions.\n+  // When this field is present, the data_type field MUST be STRING\n+  repeated bytes string_data = 6;\n+\n+  // For int64.\n+  // When this field is present, the data_type field MUST be INT64\n+  repeated int64 int64_data = 7 [packed = true];\n+\n+  // Optionally, a name for the tensor.\n+  optional string name = 8; // namespace Value\n+\n+  // A human-readable documentation for this tensor. Markdown is allowed.\n+  optional string doc_string = 12;\n+\n+  // Serializations can either use one of the fields above, or use this\n+  // raw bytes field. The only exception is the string case, where one is\n+  // required to store the content in the repeated bytes string_data field.\n+  //\n+  // When this raw_data field is used to store tensor value, elements MUST\n+  // be stored in as fixed-width, little-endian order.\n+  // Floating-point data types MUST be stored in IEEE 754 format.\n+  // Complex64 elements must be written as two consecutive FLOAT values, real component first.\n+  // Complex128 elements must be written as two consecutive DOUBLE values, real component first.\n+  // Boolean type MUST be written one byte per tensor element (00000001 for true, 00000000 for false).\n+  //\n+  // Note: the advantage of specific field rather than the raw_data field is\n+  // that in some cases (e.g. int data), protobuf does a better packing via\n+  // variable length storage, and may lead to smaller binary footprint.\n+  // When this field is present, the data_type field MUST NOT be STRING or UNDEFINED\n+  optional bytes raw_data = 9;\n+\n+  // For double\n+  // Complex64 tensors are encoded as a single array of doubles,\n+  // with the real components appearing in odd numbered positions,\n+  // and the corresponding imaginary component apparing in the\n+  // subsequent even numbered position. (e.g., [1.0 + 2.0i, 3.0 + 4.0i]\n+  // is encoded as [1.0, 2.0 ,3.0 ,4.0]\n+  // When this field is present, the data_type field MUST be DOUBLE or COMPLEX128\n+  repeated double double_data = 10 [packed = true];\n+\n+  // For uint64 and uint32 values\n+  // When this field is present, the data_type field MUST be\n+  // UINT32 or UINT64\n+  repeated uint64 uint64_data = 11 [packed = true];\n+\n+  // External data by file name\n+  optional string external_data = 13;\n+\n+  // If two tensors represent the same weights/content, use alias.\n+  // Must exist a TensorProto named alias in the initializer list.\n+  // To avoid the duplicate tensor in attribute, such as value in Constant node.\n+  // This is useful, if everything is stored just in the proto.\n+  optional string alias = 16;\n+\n+  // Additional annotations.\n+  repeated AttributeProto annotations = 17;\n+\n+  // Device info\n+  optional DeviceOption device_detail = 51;\n+\n+  // For PyTorch serialized tensor.\n+  optional int64 require_gradient = 52;\n+  optional int64 is_buffer = 53;\n+}\n+\n+// Defines a tensor shape. A dimension can be either an integer value\n+// or a symbolic variable. A symbolic variable represents an unknown\n+// dimension.\n+message TensorShapeProto {\n+  message Dimension {\n+    oneof value {\n+      int64 dim_value = 1;\n+      string dim_param = 2;   // namespace Shape\n+    };\n+    // Standard denotation can optionally be used to denote tensor\n+    // dimensions with standard semantic descriptions to ensure\n+    // that operations are applied to the correct axis of a tensor.\n+    // Refer to https://github.com/onnx/onnx/blob/master/docs/DimensionDenotation.md#denotation-definition\n+    // for pre-defined dimension denotations.\n+    optional string denotation = 3;\n+  };\n+  // To represent a scalar, using no dim to represent 0-d tensor.\n+  repeated Dimension dim = 1;\n+\n+  repeated Dimension stride = 51;\n+}\n+\n+// Types\n+//\n+// The standard ONNX data types.\n+message TypeProto {\n+\n+  message Tensor {\n+    // This field MUST NOT have the value of UNDEFINED\n+    // This field MUST be present for this version of the IR.\n+    optional TensorProto.DataType elem_type = 1;\n+    optional TensorShapeProto shape = 2;\n+  }\n+\n+  // Sequence type: List, Tuple\n+  message Sequence {\n+    // elem_type and elem_type_list cannot appear together.\n+    // If all the element types are the same, we use elem_type,\n+    // otherwise, we specify the type of each element in elem_type_list.\n+    optional TypeProto elem_type = 1;\n+    repeated TypeProto elem_type_list = 51;\n+    enum SequenceType {\n+      UNDEFINED = 0;\n+      LIST = 1;\n+      TUPLE = 2;\n+    }\n+    optional SequenceType sequence_type = 52;\n+  }\n+\n+  // Map<K, V>, (not necessary at this moment)\n+  message Map {\n+    optional TensorProto.DataType key_type = 1;\n+    optional TypeProto value_type = 2;\n+  }\n+\n+  // Special type of blobs, based on the type_name, we can choose the right\n+  // serializer and deserialzier.\n+  message SpecialBlob {\n+    optional string type_name = 1;\n+  }\n+\n+  oneof value {\n+    // The type of a tensor.\n+    Tensor tensor_type = 1;\n+    Sequence sequence_type = 4;\n+    Map map_type = 5;\n+    SpecialBlob special_type = 51;\n+  }\n+\n+  // An optional denotation can be used to denote the whole \n+  // type with a standard semantic description as to what is \n+  // stored inside. Refer to https://github.com/onnx/onnx/blob/master/docs/TypeDenotation.md#type-denotation-definition\n+  // for pre-defined type denotations.\n+  optional string denotation = 6;\n+}\n+\n+// TensorProtos stores multiple TensorProto objects in one single proto. This\n+// is useful for small tensors; For anything big, consider using a DB for\n+// storage.\n+message TensorProtos {\n+  repeated TensorProto protos = 1;", "path": "caffe2/proto/torch.proto", "position": null, "original_position": 541, "commit_id": "31b0cb981c1b664df13bc133df123d7c407a3ca4", "original_commit_id": "925886631fac508dd2032be21532785172cbdc0a", "user": {"login": "dzhulgakov", "id": 17890620, "node_id": "MDQ6VXNlcjE3ODkwNjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/17890620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dzhulgakov", "html_url": "https://github.com/dzhulgakov", "followers_url": "https://api.github.com/users/dzhulgakov/followers", "following_url": "https://api.github.com/users/dzhulgakov/following{/other_user}", "gists_url": "https://api.github.com/users/dzhulgakov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dzhulgakov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dzhulgakov/subscriptions", "organizations_url": "https://api.github.com/users/dzhulgakov/orgs", "repos_url": "https://api.github.com/users/dzhulgakov/repos", "events_url": "https://api.github.com/users/dzhulgakov/events{/privacy}", "received_events_url": "https://api.github.com/users/dzhulgakov/received_events", "type": "User", "site_admin": false}, "body": "it's probably minor and we can  drop it for now as data reading is somewhat different", "created_at": "2018-09-06T21:02:50Z", "updated_at": "2018-11-23T15:50:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/11166#discussion_r215777508", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11166", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215777508"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11166#discussion_r215777508"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11166"}}, "body_html": "<p>it's probably minor and we can  drop it for now as data reading is somewhat different</p>", "body_text": "it's probably minor and we can  drop it for now as data reading is somewhat different", "in_reply_to_id": 215479487}