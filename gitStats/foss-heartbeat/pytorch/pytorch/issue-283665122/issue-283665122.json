{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4281", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4281/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4281/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4281/events", "html_url": "https://github.com/pytorch/pytorch/issues/4281", "id": 283665122, "node_id": "MDU6SXNzdWUyODM2NjUxMjI=", "number": 4281, "title": "Implementation of Bipolar Activation Functions", "user": {"login": "larspars", "id": 664369, "node_id": "MDQ6VXNlcjY2NDM2OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/664369?v=4", "gravatar_id": "", "url": "https://api.github.com/users/larspars", "html_url": "https://github.com/larspars", "followers_url": "https://api.github.com/users/larspars/followers", "following_url": "https://api.github.com/users/larspars/following{/other_user}", "gists_url": "https://api.github.com/users/larspars/gists{/gist_id}", "starred_url": "https://api.github.com/users/larspars/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/larspars/subscriptions", "organizations_url": "https://api.github.com/users/larspars/orgs", "repos_url": "https://api.github.com/users/larspars/repos", "events_url": "https://api.github.com/users/larspars/events{/privacy}", "received_events_url": "https://api.github.com/users/larspars/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-12-20T19:07:21Z", "updated_at": "2017-12-21T13:28:23Z", "closed_at": "2017-12-21T10:37:33Z", "author_association": "NONE", "body_html": "<p>We're looking to make a pull request for <a href=\"https://arxiv.org/abs/1709.04054\" rel=\"nofollow\">bipolar activation functions</a>. Hope to get some feedback.</p>\n<p>This is a trick to make activation functions self centering. In short: Because the ReLU keeps only positive numbers, it shifts the post-activation mean in a positive direction. However, if we for every other neuron we instead keep the negative numbers, we cancel this effect.</p>\n<p>So for half the neurons we do <code>ReLU(x)</code>, and other half we do <code>-ReLU(-x)</code>. For an i.i.d. input vector, the effect of this is to halve the mean of the vector, post-activation, <code>E[BReLU(x)] = 0.5E[x]</code>.</p>\n<p>In our paper we find empirically that this can help learning in RNNs and ConvNets. Our empirical results are with ReLUs and ELUs, but I would expect it to hold for anything ReLU like.</p>\n<p>One way to implement it would be to put something like the following in nn/functional.py:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_make_bipolar</span>(<span class=\"pl-smi\">fn</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_fn</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n        dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">if</span> x.dim() <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">1</span>\n        x0, x1 <span class=\"pl-k\">=</span> torch.chunk(x, <span class=\"pl-v\">chunks</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span>dim)\n        y0 <span class=\"pl-k\">=</span> fn(x0, <span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n        y1 <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>fn(<span class=\"pl-k\">-</span>x1, <span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n        <span class=\"pl-k\">return</span> torch.cat((y0, y1), <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span>dim)\n\n    <span class=\"pl-k\">return</span> _fn\n    \nbrelu <span class=\"pl-k\">=</span> _make_bipolar(relu)\nbelu <span class=\"pl-k\">=</span> _make_bipolar(elu)\nbselu <span class=\"pl-k\">=</span> _make_bipolar(selu)\nleaky_brelu <span class=\"pl-k\">=</span> _make_bipolar(leaky_relu)\nbprelu <span class=\"pl-k\">=</span> _make_bipolar(prelu)\nbrrelu <span class=\"pl-k\">=</span> _make_bipolar(rrelu)\nbsoftplus <span class=\"pl-k\">=</span> _make_bipolar(softplus)\nbsigmoid <span class=\"pl-k\">=</span> _make_bipolar(sigmoid)\nbipolar_max_pool1d <span class=\"pl-k\">=</span> _make_bipolar(max_pool1d)\nbipolar_max_pool2d <span class=\"pl-k\">=</span> _make_bipolar(max_pool2d)\nbipolar_max_pool3d <span class=\"pl-k\">=</span> _make_bipolar(max_pool3d)</pre></div>\n<p>This also includes bipolar max-pooling functions (i.e. min-pooling for half the inputs).</p>\n<ol>\n<li>Would there be interest in a pull request for this?</li>\n<li>Any comments on the implementation? Of course, we'll have to add doc strings, and also put something similar into nn/modules/activation.py</li>\n</ol>", "body_text": "We're looking to make a pull request for bipolar activation functions. Hope to get some feedback.\nThis is a trick to make activation functions self centering. In short: Because the ReLU keeps only positive numbers, it shifts the post-activation mean in a positive direction. However, if we for every other neuron we instead keep the negative numbers, we cancel this effect.\nSo for half the neurons we do ReLU(x), and other half we do -ReLU(-x). For an i.i.d. input vector, the effect of this is to halve the mean of the vector, post-activation, E[BReLU(x)] = 0.5E[x].\nIn our paper we find empirically that this can help learning in RNNs and ConvNets. Our empirical results are with ReLUs and ELUs, but I would expect it to hold for anything ReLU like.\nOne way to implement it would be to put something like the following in nn/functional.py:\ndef _make_bipolar(fn):\n    def _fn(x, *args, **kwargs):\n        dim = 0 if x.dim() == 1 else 1\n        x0, x1 = torch.chunk(x, chunks=2, dim=dim)\n        y0 = fn(x0, *args, **kwargs)\n        y1 = -fn(-x1, *args, **kwargs)\n        return torch.cat((y0, y1), dim=dim)\n\n    return _fn\n    \nbrelu = _make_bipolar(relu)\nbelu = _make_bipolar(elu)\nbselu = _make_bipolar(selu)\nleaky_brelu = _make_bipolar(leaky_relu)\nbprelu = _make_bipolar(prelu)\nbrrelu = _make_bipolar(rrelu)\nbsoftplus = _make_bipolar(softplus)\nbsigmoid = _make_bipolar(sigmoid)\nbipolar_max_pool1d = _make_bipolar(max_pool1d)\nbipolar_max_pool2d = _make_bipolar(max_pool2d)\nbipolar_max_pool3d = _make_bipolar(max_pool3d)\nThis also includes bipolar max-pooling functions (i.e. min-pooling for half the inputs).\n\nWould there be interest in a pull request for this?\nAny comments on the implementation? Of course, we'll have to add doc strings, and also put something similar into nn/modules/activation.py", "body": "We're looking to make a pull request for [bipolar activation functions](https://arxiv.org/abs/1709.04054). Hope to get some feedback.\r\n\r\nThis is a trick to make activation functions self centering. In short: Because the ReLU keeps only positive numbers, it shifts the post-activation mean in a positive direction. However, if we for every other neuron we instead keep the negative numbers, we cancel this effect. \r\n\r\nSo for half the neurons we do `ReLU(x)`, and other half we do `-ReLU(-x)`. For an i.i.d. input vector, the effect of this is to halve the mean of the vector, post-activation, `E[BReLU(x)] = 0.5E[x]`.\r\n\r\nIn our paper we find empirically that this can help learning in RNNs and ConvNets. Our empirical results are with ReLUs and ELUs, but I would expect it to hold for anything ReLU like.\r\n\r\nOne way to implement it would be to put something like the following in nn/functional.py:\r\n\r\n```python\r\ndef _make_bipolar(fn):\r\n    def _fn(x, *args, **kwargs):\r\n        dim = 0 if x.dim() == 1 else 1\r\n        x0, x1 = torch.chunk(x, chunks=2, dim=dim)\r\n        y0 = fn(x0, *args, **kwargs)\r\n        y1 = -fn(-x1, *args, **kwargs)\r\n        return torch.cat((y0, y1), dim=dim)\r\n\r\n    return _fn\r\n    \r\nbrelu = _make_bipolar(relu)\r\nbelu = _make_bipolar(elu)\r\nbselu = _make_bipolar(selu)\r\nleaky_brelu = _make_bipolar(leaky_relu)\r\nbprelu = _make_bipolar(prelu)\r\nbrrelu = _make_bipolar(rrelu)\r\nbsoftplus = _make_bipolar(softplus)\r\nbsigmoid = _make_bipolar(sigmoid)\r\nbipolar_max_pool1d = _make_bipolar(max_pool1d)\r\nbipolar_max_pool2d = _make_bipolar(max_pool2d)\r\nbipolar_max_pool3d = _make_bipolar(max_pool3d)\r\n```\r\n\r\nThis also includes bipolar max-pooling functions (i.e. min-pooling for half the inputs).\r\n\r\n\r\n1) Would there be interest in a pull request for this?\r\n2) Any comments on the implementation? Of course, we'll have to add doc strings, and also put something similar into nn/modules/activation.py"}