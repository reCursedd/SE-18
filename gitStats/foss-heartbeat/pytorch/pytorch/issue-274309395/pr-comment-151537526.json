{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/151537526", "pull_request_review_id": 77257769, "id": 151537526, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MTUzNzUyNg==", "diff_hunk": "@@ -0,0 +1,220 @@\n+// Indexing tensors by by tensors\n+//\n+// This corresponds to \"advanced indexing\" in NumPy. The two operations are:\n+//\n+//  index(Tensor self, indices) -> Tensor\n+//  index_put_(Tensor self, indices, value)\n+//\n+// The index is a TensorList containg kLong or kByte tensors or nulls. Byte\n+// tensors (boolean masks) are expanded to long tensors via nonzero(). Null\n+// tensors signify that the dimension is not indexed.\n+//\n+// All indexes are broadcast together and iterated as *one*. From NumPy:\n+//\n+// result[i_1, ..., i_M] == x[ind_1[i_1, ..., i_M], ind_2[i_1, ..., i_M],\n+//                           ..., ind_N[i_1, ..., i_M]]\n+//\n+// Note 1: ByteTensors expand to index as many dimensions as there are in the\n+// mask.\n+//\n+// Note 2: The behavior is more complicated when the index tensors are not all\n+// adjacent (e.g. x[[0, 1], :, [2, 3]]). In this case, self and the index\n+// tensors are transposed to the front: x.transpose(1, 2)[[0, 1], [2, 3]]\n+\n+\n+#include \"ATen/ATen.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/ExpandUtils.h\"\n+\n+#include <algorithm>\n+#include <functional>\n+#include <numeric>\n+#include <vector>\n+\n+namespace at { namespace native {\n+\n+[[noreturn]]\n+static void invalid_mask(const Tensor & self, int64_t idx, const Tensor & mask, int64_t maskIdx) {\n+  std::stringstream ss;\n+  ss << \"The shape of the mask \" << mask.sizes() << \" at index \" << maskIdx;\n+  ss << \" does not match the shape of the indexed tensor \" << self.sizes();\n+  ss << \" at index \" << idx;\n+  throw std::runtime_error(ss.str());\n+}\n+\n+static std::vector<Tensor> expandByteTensors(const Tensor & self, TensorList indices) {\n+  // Expands byte tensors (masks) into the equivalent indexing by LongTensors\n+  std::vector<Tensor> result;\n+  for (auto & index : indices) {\n+    if (index.type().scalarType() == kByte) {\n+      // The sizes of the ByteTensor mask must match the sizes of the\n+      // corresponding dimensions in self\n+      for (int64_t j = 0; j < index.dim(); j++) {\n+        int64_t srcIdx = result.size() + j;\n+        if (index.size(j) != self.size(srcIdx)) {\n+          invalid_mask(self, srcIdx, index, j);\n+        }\n+      }\n+      // Replace with nonzeros\n+      auto nonzero = index.nonzero();\n+      for (int64_t j = 0; j < nonzero.size(1); j++) {\n+        result.emplace_back(nonzero.select(1, j));\n+      }\n+    } else {\n+      result.emplace_back(index);\n+    }\n+  }\n+  return result;\n+}\n+\n+static bool hasContiguousSubspace(TensorList tl) {\n+  // true if all the non-null tensors are adjacent\n+  auto isDefined = [](const Tensor & tensor){ return tensor.defined(); };\n+  auto isNull = [](const Tensor & tensor){ return !tensor.defined(); };\n+  auto start = std::find_if(tl.begin(), tl.end(), isDefined);\n+  auto stop = std::find_if(tl.rbegin(), tl.rend(), isDefined);\n+  auto it = std::find_if(start, stop.base(), isNull);\n+  return it == stop.base();\n+}\n+\n+// Transposes the tensor and indices together so that all the non-null indices\n+// index the first k dimensions of the tensor. Returns the transposed tensor\n+// and the reordered indices. For example:\n+//  transposeToFront(tensor, {nullptr, a, nullptr, b})\n+// returns\n+//  tensor.permute([1, 3, 0, 2]), {a, b, nullptr, nullptr}\n+static std::tuple<Tensor, std::vector<Tensor>>\n+transposeToFront(Tensor self, TensorList indices) {\n+  std::vector<int64_t> dims;\n+  std::vector<Tensor> transposedIndices;\n+  dims.reserve(self.dim());\n+  for (int64_t i = 0; i < self.dim(); i++) {\n+    if (indices[i].defined()) {\n+      dims.push_back(i);\n+      transposedIndices.emplace_back(indices[i]);\n+    }\n+  }\n+  for (int64_t i = 0; i < self.dim(); i++) {\n+    if (!indices[i].defined()) {\n+      dims.push_back(i);\n+      transposedIndices.emplace_back();\n+    }\n+  }\n+  return std::make_tuple<>(self.permute(dims), std::move(transposedIndices));\n+}\n+\n+static std::vector<int64_t> computeLinearStride(const Tensor & tensor) {\n+  // computes the stride as if tensor were contigous\n+  auto sizes = tensor.sizes();\n+  std::vector<int64_t> stride(tensor.dim());\n+  stride[tensor.dim() - 1] = 1;\n+  std::partial_sum(sizes.rbegin(), sizes.rend() - 1, stride.rbegin() + 1, std::multiplies<int64_t>());\n+  return stride;\n+}\n+\n+// Unsqueezes src `before` times at the front and `after` times at the end\n+static Tensor unsqueezeN(const Tensor & src, int64_t before, int64_t after) {\n+  auto srcSizes = src.sizes();\n+  auto nDim = src.dim();\n+  std::vector<int64_t> sizes(nDim + before + after, 1);\n+  for (int64_t i = 0; i < nDim; i++) {\n+    sizes[i + before] = srcSizes[i];\n+  }\n+  return src.view(sizes);\n+}\n+\n+static Tensor computeLinearIndex(const Tensor & src, TensorList indices) {\n+  auto strides = computeLinearStride(src);\n+\n+  // Compute the linear index by multiplying the indexing tensors by the\n+  // stride and summing them. All the indexing tensors have the same shape at\n+  // this point. We also compute the number of dimensions before and after that\n+  // are not being index.\n+  Tensor linearIndex;\n+  int64_t emptyBefore = 0, emptyAfter = 0, nElemBefore = 1, nElemAfter = 1;\n+  for (int64_t i = 0; i < src.dim(); i++) {\n+    if (indices[i].defined()) {\n+      Tensor index = indices[i] * strides[i];", "path": "aten/src/ATen/native/Indexing.cpp", "position": 137, "original_position": 137, "commit_id": "3c591710eb478eb94498f6333c8a1428a1557933", "original_commit_id": "9435b2238f641ff3f9e7ae3bb56a41b9e91acba4", "user": {"login": "killeent", "id": 4529377, "node_id": "MDQ6VXNlcjQ1MjkzNzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/4529377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/killeent", "html_url": "https://github.com/killeent", "followers_url": "https://api.github.com/users/killeent/followers", "following_url": "https://api.github.com/users/killeent/following{/other_user}", "gists_url": "https://api.github.com/users/killeent/gists{/gist_id}", "starred_url": "https://api.github.com/users/killeent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/killeent/subscriptions", "organizations_url": "https://api.github.com/users/killeent/orgs", "repos_url": "https://api.github.com/users/killeent/repos", "events_url": "https://api.github.com/users/killeent/events{/privacy}", "received_events_url": "https://api.github.com/users/killeent/received_events", "type": "User", "site_admin": false}, "body": "How does this operator overload work? From my interpretation, if `indices` is on the GPU this will invoke a `cudaMemcpy` and `mm` kernel launch for each dimension correct?", "created_at": "2017-11-16T21:09:52Z", "updated_at": "2018-11-23T15:36:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/3725#discussion_r151537526", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3725", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/151537526"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3725#discussion_r151537526"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3725"}}, "body_html": "<p>How does this operator overload work? From my interpretation, if <code>indices</code> is on the GPU this will invoke a <code>cudaMemcpy</code> and <code>mm</code> kernel launch for each dimension correct?</p>", "body_text": "How does this operator overload work? From my interpretation, if indices is on the GPU this will invoke a cudaMemcpy and mm kernel launch for each dimension correct?"}