{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/151474037", "pull_request_review_id": 77185204, "id": 151474037, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MTQ3NDAzNw==", "diff_hunk": "@@ -0,0 +1,219 @@\n+// Indexing tensors by by tensors\n+//\n+// This corresponds to \"advanced indexing\" in NumPy. The two operations are:\n+//\n+//  index(Tensor self, indices) -> Tensor\n+//  index_put_(Tensor self, indices, value)\n+//\n+// The index is a TensorList containg kLong or kByte tensors or nulls. Byte\n+// tensors (boolean masks) are expanded to long tensors via nonzero(). Null\n+// tensors signify that the dimension is not indexed.\n+//\n+// All indexes are broadcast together and iterated as *one*. From NumPy:\n+//\n+// result[i_1, ..., i_M] == x[ind_1[i_1, ..., i_M], ind_2[i_1, ..., i_M],\n+//                           ..., ind_N[i_1, ..., i_M]]\n+//\n+// Note 1: ByteTensors expand to index as many dimensions as there are in the\n+// mask.\n+//\n+// Note 2: The behavior is more complicated when the index tensors are not all\n+// adjacent (e.g. x[[0, 1], :, [2, 3]]). In this case, self and the index\n+// tensors are transposed to the front: x.transpose(1, 2)[[0, 1], [2, 3]]\n+\n+\n+#include \"ATen/ATen.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/ExpandUtils.h\"\n+\n+#include <algorithm>\n+#include <functional>\n+#include <numeric>\n+#include <vector>\n+\n+namespace at { namespace native {\n+\n+static bool isByteTensor(const Tensor & t) {\n+  return t.defined() && t.type().scalarType() == kByte;\n+}\n+\n+[[noreturn]]\n+static void invalid_mask(const Tensor & self, int64_t idx, const Tensor & mask, int64_t maskIdx) {\n+  std::stringstream ss;\n+  ss << \"The shape of the mask \" << mask.sizes() << \" at index \" << maskIdx;\n+  ss << \" does not match the indexed tensor \" << self.sizes() << \" at index \";\n+  ss << idx;\n+  throw std::runtime_error(ss.str());\n+}\n+\n+static std::vector<Tensor> expandByteTensors(const Tensor & self, TensorList indices) {\n+  // Expands byte tensors (masks) into the equivalent indexing by LongTensors\n+  std::vector<Tensor> result;\n+  for (auto & index : indices) {\n+    if (isByteTensor(index)) {\n+      // The sizes of the ByteTensor mask must match the sizes of the\n+      // corresponding dimensions in self\n+      for (int64_t j = 0; j < index.dim(); j++) {\n+        int64_t srcIdx = result.size() + j;\n+        if (index.size(j) != self.size(srcIdx)) {\n+          invalid_mask(self, srcIdx, index, j);\n+        }\n+      }\n+      // Replace with nonzeros\n+      auto nonzero = index.nonzero();\n+      for (int64_t j = 0; j < nonzero.size(1); j++) {\n+        result.emplace_back(nonzero.select(1, j));\n+      }\n+    } else {\n+      result.emplace_back(index);\n+    }\n+  }\n+  return result;\n+}\n+\n+static bool hasContiguousSubspace(TensorList tl) {\n+  // true if all the non-null tensors are adjacent\n+  auto isDefined = [](const Tensor & tensor){ return tensor.defined(); };\n+  auto isNull = [](const Tensor & tensor){ return !tensor.defined(); };\n+  auto start = std::find_if(tl.begin(), tl.end(), isDefined);\n+  auto stop = std::find_if(tl.rbegin(), tl.rend(), isDefined);\n+  auto it = std::find_if(start, stop.base(), isNull);\n+  return it == stop.base();\n+}\n+\n+static std::tuple<Tensor, std::vector<Tensor>>\n+transposeToFront(Tensor self, TensorList indices) {\n+  std::vector<int64_t> dims;\n+  std::vector<Tensor> transposedIndices;\n+  dims.reserve(self.dim());\n+  for (int64_t i = 0; i < self.dim(); i++) {\n+    if (indices[i].defined()) {\n+      dims.push_back(i);\n+      transposedIndices.emplace_back(indices[i]);\n+    }\n+  }\n+  for (int64_t i = 0; i < self.dim(); i++) {\n+    if (!indices[i].defined()) {\n+      dims.push_back(i);\n+      transposedIndices.emplace_back();\n+    }\n+  }\n+  return std::make_tuple<>(self.permute(dims), std::move(transposedIndices));\n+}\n+\n+static std::vector<int64_t> computeLinearStride(const Tensor & tensor) {\n+  // computes the stride as if tensor were contigous\n+  auto sizes = tensor.sizes();\n+  std::vector<int64_t> stride(tensor.dim());\n+  stride[tensor.dim() - 1] = 1;\n+  std::partial_sum(sizes.rbegin(), sizes.rend() - 1, stride.rbegin() + 1, std::multiplies<int64_t>());\n+  return stride;\n+}\n+\n+static Tensor unsqueezeN(const Tensor & src, int64_t before, int64_t after) {", "path": "aten/src/ATen/native/Indexing.cpp", "position": null, "original_position": 113, "commit_id": "3c591710eb478eb94498f6333c8a1428a1557933", "original_commit_id": "0c0dc790c67cecccff28cbe732f763fe9e886fd5", "user": {"login": "killeent", "id": 4529377, "node_id": "MDQ6VXNlcjQ1MjkzNzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/4529377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/killeent", "html_url": "https://github.com/killeent", "followers_url": "https://api.github.com/users/killeent/followers", "following_url": "https://api.github.com/users/killeent/following{/other_user}", "gists_url": "https://api.github.com/users/killeent/gists{/gist_id}", "starred_url": "https://api.github.com/users/killeent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/killeent/subscriptions", "organizations_url": "https://api.github.com/users/killeent/orgs", "repos_url": "https://api.github.com/users/killeent/repos", "events_url": "https://api.github.com/users/killeent/events{/privacy}", "received_events_url": "https://api.github.com/users/killeent/received_events", "type": "User", "site_admin": false}, "body": "Documentation here also.", "created_at": "2017-11-16T16:55:10Z", "updated_at": "2018-11-23T15:36:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/3725#discussion_r151474037", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3725", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/151474037"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3725#discussion_r151474037"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3725"}}, "body_html": "<p>Documentation here also.</p>", "body_text": "Documentation here also."}