{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/151488231", "pull_request_review_id": 77200689, "id": 151488231, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MTQ4ODIzMQ==", "diff_hunk": "@@ -0,0 +1,293 @@\n+#include \"torch/csrc/autograd/python_variable_indexing.h\"\n+\n+#include \"torch/csrc/DynamicTypes.h\"\n+#include \"torch/csrc/Exceptions.h\"\n+#include \"torch/csrc/autograd/python_variable.h\"\n+#include \"torch/csrc/autograd/utils/wrap_outputs.h\"\n+#include \"torch/csrc/utils/python_compat.h\"\n+#include \"torch/csrc/utils/python_numbers.h\"\n+\n+#include <vector>\n+\n+using namespace at;\n+using namespace torch::autograd::utils;\n+\n+extern PyObject* THPLongTensorClass;\n+#ifdef WITH_CUDA\n+extern PyObject* THCPLongTensorClass;\n+#endif\n+\n+extern bool THPModule_isTensor(PyObject *obj);\n+\n+namespace torch { namespace autograd {\n+\n+Py_ssize_t THPVariable_length(PyObject* self) {\n+  HANDLE_TH_ERRORS\n+  auto& self_ = reinterpret_cast<THPVariable*>(self)->cdata;\n+  if (self_.dim() == 0) {\n+    return 0;\n+  }\n+  return (Py_ssize_t)self_.size(0);\n+  END_HANDLE_TH_ERRORS_RET(-1)\n+}\n+\n+\n+// We allow indexing by integers, slices, ellipsis, None, Variables,\n+// and tuples of those types. We also handle bools as if they were a\n+// Variable[ByteTensor].\n+\n+static int64_t count_specified_dimensions(PyObject* index) {\n+  // Count the number of indexed dimensions (everything but ellipsis and None)\n+  int64_t count = 0;\n+  auto size = PyTuple_GET_SIZE(index);\n+  for (Py_ssize_t i = 0; i < size; i++) {\n+    PyObject* obj = PyTuple_GET_ITEM(index, i);\n+    if (THPVariable_Check(obj)) {\n+      auto& var = reinterpret_cast<THPVariable*>(obj)->cdata;\n+      if (var.type().scalarType() == kByte) {\n+        count += var.dim();\n+      }\n+    } else if (obj != Py_None && obj != Py_Ellipsis) {\n+      count++;\n+    }\n+  }\n+  return count;\n+}\n+\n+[[noreturn]]\n+static void invalid_index(PyObject* obj) {\n+  throw IndexError(\n+    \"only integers, slices (`:`), ellipsis (`...`), None and long or byte \"\n+    \"Variables are valid indices (got %s)\", Py_TYPE(obj)->tp_name);\n+}\n+\n+static Variable applySlice(const Variable& self, int64_t dim, PyObject* slice, bool ensure_view=false) {\n+  Py_ssize_t start, stop, step, slicelength;\n+  auto length = self.size(dim);\n+  if (THPUtils_parseSlice(slice, length, &start, &stop, &step, &slicelength) < 0) {\n+    throw python_error();\n+  }\n+  if (start == 0 && stop == length && step == 1) {\n+    if (ensure_view) {\n+      return at::alias(self);\n+    }\n+    return self;\n+  }\n+  // TODO: step\n+  return self.narrow(dim, start, stop - start);\n+}\n+\n+static Variable applySelect(const Variable& self, int64_t dim, int64_t index) {\n+  int64_t size = self.size(dim);\n+  if (index < -size || index >= size) {\n+    throw IndexError(\"index %lld is out of bounds for dimension %lld with size %lld\",\n+      index, dim, size);\n+  }\n+  if (index < 0) {\n+    index += size;\n+  }\n+  return self.select(dim, index);\n+}\n+\n+static Variable sequenceToVariable(const Type& type, PyObject* seq) {\n+  PyObject* ctor = THPLongTensorClass;\n+#ifdef WITH_CUDA\n+  if (type.isCuda()) ctor = THCPLongTensorClass;\n+#endif\n+  auto obj = THPObjectPtr(PyObject_CallFunctionObjArgs(ctor, seq, NULL));\n+  if (!obj) throw python_error();\n+  return make_variable(createTensor(obj.get()));\n+}\n+\n+static Variable valueToTensor(const Type & type, PyObject* value) {\n+  if (THPVariable_Check(value)) {\n+    return reinterpret_cast<THPVariable*>(value)->cdata;\n+  }\n+  if (THPUtils_checkLong(value)) {\n+    return type.scalarTensor(Scalar(THPUtils_unpackLong(value)));\n+  }\n+  if (PyFloat_Check(value)) {\n+    return type.scalarTensor(Scalar(THPUtils_unpackDouble(value)));\n+  }\n+  throw TypeError(\"can't assign a %s to a %s\", Py_TYPE(value)->tp_name, type.toString());\n+}\n+\n+static Variable applySlicing(const Variable& self, PyObject* index, variable_list& outIndices) {\n+  int64_t size = PyTuple_GET_SIZE(index);\n+  int64_t dim = 0;\n+\n+  auto handle_var = [&](const Variable& var) {\n+    // TODO: check scalarType\n+    outIndices.resize(dim + 1);\n+    outIndices[dim] = var;\n+    dim++;\n+  };\n+\n+  Variable result = self;\n+  for (int64_t i = 0; i < size; i++) {\n+    PyObject* obj = PyTuple_GET_ITEM(index, i);\n+    if (THPUtils_checkLong(obj)) {\n+      result = applySelect(result, dim, THPUtils_unpackLong(obj));\n+    } else if (PySlice_Check(obj)) {\n+      result = applySlice(result, dim, obj);\n+      dim++;\n+    } else if (obj == Py_Ellipsis) {\n+      int64_t unspecified_dims = self.dim() - count_specified_dimensions(index);\n+      for(; unspecified_dims > 0; unspecified_dims--) {\n+        dim++;\n+      }\n+    } else if (obj == Py_None) {\n+      result = result.unsqueeze(dim);\n+      dim++;\n+    } else if (THPVariable_Check(obj)) {\n+      handle_var(reinterpret_cast<THPVariable*>(obj)->cdata);\n+    } else if (THPModule_isTensor(obj)) {\n+      handle_var(make_variable(createTensor(obj)));\n+    } else if (PySequence_Check(obj)) {\n+      handle_var(sequenceToVariable(self.type(), obj));\n+    } else {\n+      invalid_index(obj);\n+    }\n+  }\n+  return result;\n+}\n+\n+static std::vector<Tensor> asTensorList(const variable_list& v) {\n+  return std::vector<Tensor>(v.begin(), v.end());\n+}\n+\n+static Variable dispatch_index(const Variable& self, const variable_list& indices) {\n+  AutoNoGIL no_gil;\n+  AutoGPU auto_gpu(self);\n+  return self.index(asTensorList(indices));\n+}\n+\n+static Variable dispatch_index_put_(Variable& self, const variable_list& indices, const Variable& value) {\n+  AutoNoGIL no_gil;\n+  AutoGPU auto_gpu(self);\n+  return self.index_put_(asTensorList(indices), value);\n+}\n+\n+static bool treatSequenceAsTuple(PyObject* index) {\n+  if (PyTuple_Check(index)) {\n+    return true;\n+  }\n+  if (!PySequence_Check(index)) {\n+    return false;\n+  }\n+  // This uses a heuristics from NumPy for determining whether to treat\n+  // non-tuple sequences as if they were a tuple. From the NumPy code comments:\n+  //\n+  // \"At this point, we're left with a non-tuple, non-array, sequence:\n+  //  typically, a list. We use some somewhat-arbitrary heuristics from here\n+  //  onwards to decided whether to treat that list as a single index, or a\n+  //  list of indices. Backwards compatibility only takes effect for short", "path": "torch/csrc/autograd/python_variable_indexing.cpp", "position": 184, "original_position": 184, "commit_id": "3c591710eb478eb94498f6333c8a1428a1557933", "original_commit_id": "ce3fbd9c44cbadb8c535078aa87e37d99f058975", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Treating lists as if they were tuples. The general rule is that if the index is not a tuple, it's wrapped in a tuple. The \"backwards compatibility\" (from NumPy's point of view) is that some lists are treated as if they were tuples instead of wrapping them in tuples.\r\n\r\nAt one point, NumPy was going to deprecate this behavior. I think they abandoned that idea for now.", "created_at": "2017-11-16T17:47:32Z", "updated_at": "2018-11-23T15:36:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/3725#discussion_r151488231", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3725", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/151488231"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3725#discussion_r151488231"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3725"}}, "body_html": "<p>Treating lists as if they were tuples. The general rule is that if the index is not a tuple, it's wrapped in a tuple. The \"backwards compatibility\" (from NumPy's point of view) is that some lists are treated as if they were tuples instead of wrapping them in tuples.</p>\n<p>At one point, NumPy was going to deprecate this behavior. I think they abandoned that idea for now.</p>", "body_text": "Treating lists as if they were tuples. The general rule is that if the index is not a tuple, it's wrapped in a tuple. The \"backwards compatibility\" (from NumPy's point of view) is that some lists are treated as if they were tuples instead of wrapping them in tuples.\nAt one point, NumPy was going to deprecate this behavior. I think they abandoned that idea for now.", "in_reply_to_id": 151478816}