{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/345057510", "html_url": "https://github.com/pytorch/pytorch/pull/3725#issuecomment-345057510", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3725", "id": 345057510, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTA1NzUxMA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-16T20:47:46Z", "updated_at": "2017-11-16T20:47:46Z", "author_association": "MEMBER", "body_html": "<p>This looks very nice!</p>\n<p>Super clean implementation that extends the previous behavior, and I believe we have numpy parity on advanced indexing now!<br>\nI found particularly interesting that you fused the boolean indexing together with list indexing (extending thus boolean indexing to work as in numpy without much effort). Also, it seems that we get autograd (and unlimited backprop?) !</p>\n<p>One question:<br>\nI have the impression that the old <code>masked_*</code> kernels are not used anymore. So for the case where we use a mask of the same size of the input (of dimension <code>d</code>), we perform a <code>nonzero</code> operation, <code>d</code> sums (for linearizing the indices) and then <code>take</code>/<code>put</code>, is that right?<br>\nOut of curiosity, do you have an idea on the performance impact of this change (specially for cuda tensors)? I personally think that having a performance impact here is worth the benefits, because of a super clean and easy to understand implementation with autograd support.</p>\n<p>PS: It might be worth considering making <code>nonzero</code> return a tuple of 1d tensors, instead of a 2d tensor as discussed in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"236684583\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1834\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1834/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1834\">#1834</a> . This behavior is used in this PR in <a href=\"https://github.com/pytorch/pytorch/pull/3725/files#diff-3e60b06052f270a7f3b76038f133a10cR65\">https://github.com/pytorch/pytorch/pull/3725/files#diff-3e60b06052f270a7f3b76038f133a10cR65</a> , and might be convenient for users to have it like that. This is a breaking change though.</p>", "body_text": "This looks very nice!\nSuper clean implementation that extends the previous behavior, and I believe we have numpy parity on advanced indexing now!\nI found particularly interesting that you fused the boolean indexing together with list indexing (extending thus boolean indexing to work as in numpy without much effort). Also, it seems that we get autograd (and unlimited backprop?) !\nOne question:\nI have the impression that the old masked_* kernels are not used anymore. So for the case where we use a mask of the same size of the input (of dimension d), we perform a nonzero operation, d sums (for linearizing the indices) and then take/put, is that right?\nOut of curiosity, do you have an idea on the performance impact of this change (specially for cuda tensors)? I personally think that having a performance impact here is worth the benefits, because of a super clean and easy to understand implementation with autograd support.\nPS: It might be worth considering making nonzero return a tuple of 1d tensors, instead of a 2d tensor as discussed in #1834 . This behavior is used in this PR in https://github.com/pytorch/pytorch/pull/3725/files#diff-3e60b06052f270a7f3b76038f133a10cR65 , and might be convenient for users to have it like that. This is a breaking change though.", "body": "This looks very nice!\r\n\r\nSuper clean implementation that extends the previous behavior, and I believe we have numpy parity on advanced indexing now!\r\nI found particularly interesting that you fused the boolean indexing together with list indexing (extending thus boolean indexing to work as in numpy without much effort). Also, it seems that we get autograd (and unlimited backprop?) !\r\n\r\nOne question:\r\nI have the impression that the old `masked_*` kernels are not used anymore. So for the case where we use a mask of the same size of the input (of dimension `d`), we perform a `nonzero` operation, `d` sums (for linearizing the indices) and then `take`/`put`, is that right?\r\nOut of curiosity, do you have an idea on the performance impact of this change (specially for cuda tensors)? I personally think that having a performance impact here is worth the benefits, because of a super clean and easy to understand implementation with autograd support.\r\n\r\nPS: It might be worth considering making `nonzero` return a tuple of 1d tensors, instead of a 2d tensor as discussed in https://github.com/pytorch/pytorch/issues/1834 . This behavior is used in this PR in https://github.com/pytorch/pytorch/pull/3725/files#diff-3e60b06052f270a7f3b76038f133a10cR65 , and might be convenient for users to have it like that. This is a breaking change though.\r\n"}