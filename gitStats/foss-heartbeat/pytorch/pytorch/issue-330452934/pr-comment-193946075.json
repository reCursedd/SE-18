{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193946075", "pull_request_review_id": 127024229, "id": 193946075, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5Mzk0NjA3NQ==", "diff_hunk": "@@ -0,0 +1,1209 @@\n+#include <ATen/ATen.h>\n+#include <ATen/NativeFunctions.h>\n+#include <ATen/Config.h>\n+\n+#if !AT_MIOPEN_ENABLED\n+\n+namespace at { namespace native {\n+\n+// See Note [ATen preprocessor philosophy]\n+\n+at::Tensor cudnn_convolution(\n+    const at::Tensor& input, const at::Tensor& weight, const at::Tensor& bias /* optional */,\n+    IntList padding, IntList stride, IntList dilation,\n+    int64_t groups, bool benchmark, bool deterministic) {\n+  throw std::runtime_error(\"cudnn_convolution: ATen not compiled with MIOpen support\");\n+}\n+\n+at::Tensor cudnn_convolution_backward_input(\n+    IntList input_size, const at::Tensor& grad_output, const at::Tensor& weight,\n+    IntList padding, IntList stride, IntList dilation, int64_t groups,\n+    bool benchmark, bool deterministic) {\n+  throw std::runtime_error(\"cudnn_convolution_backward_input: ATen not compiled with MIOpen support\");\n+}\n+\n+at::Tensor cudnn_convolution_backward_weight(\n+    IntList weight_size, const at::Tensor& grad_output, const at::Tensor& input,\n+    IntList padding, IntList stride, IntList dilation, int64_t groups,\n+    bool benchmark, bool deterministic) {\n+  throw std::runtime_error(\"cudnn_convolution_backward_weight: ATen not compiled with MIOpen support\");\n+}\n+\n+at::Tensor cudnn_convolution_backward_bias(\n+    const at::Tensor& grad_output) {\n+  throw std::runtime_error(\"cudnn_convolution_backward_bias: ATen not compiled with MIOpen support\");\n+}\n+\n+std::tuple<at::Tensor,at::Tensor,at::Tensor> cudnn_convolution_backward(\n+    const at::Tensor& input, const at::Tensor& grad_output, const at::Tensor& weight,\n+    IntList padding, IntList stride, IntList dilation, int64_t groups,\n+    bool benchmark, bool deterministic, std::array<bool,3> output_mask) {\n+  throw std::runtime_error(\"cudnn_convolution_backward: ATen not compiled with MIOpen support\");\n+}\n+\n+at::Tensor cudnn_convolution_transpose(\n+    const at::Tensor& input, const at::Tensor& weight, const at::Tensor& bias /* optional */,\n+    IntList padding, IntList output_padding, IntList stride, IntList dilation,\n+    int64_t groups, bool benchmark, bool deterministic) {\n+  throw std::runtime_error(\"cudnn_convolution_transpose: ATen not compiled with MIOpen support\");\n+}\n+\n+at::Tensor cudnn_convolution_transpose_backward_input(\n+    const at::Tensor& grad_output, const at::Tensor& weight,\n+    IntList padding, IntList stride, IntList dilation,\n+    int64_t groups, bool benchmark, bool deterministic) {\n+  throw std::runtime_error(\"cudnn_convolution_transpose_backward: ATen not compiled with MIOpen support\");\n+}\n+\n+at::Tensor cudnn_convolution_transpose_backward_weight(\n+    IntList weight_size, const at::Tensor& grad_output, const at::Tensor& input,\n+    IntList padding, IntList stride, IntList dilation, int64_t groups,\n+    bool benchmark, bool deterministic) {\n+  throw std::runtime_error(\"cudnn_convolution_transpose_backward_weight: ATen not compiled with MIOpen support\");\n+}\n+\n+std::tuple<at::Tensor,at::Tensor,at::Tensor> cudnn_convolution_transpose_backward(\n+    const at::Tensor& input, const at::Tensor& grad_output, const at::Tensor& weight,\n+    IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups,\n+    bool benchmark, bool deterministic, std::array<bool,3> output_mask) {\n+  throw std::runtime_error(\"cudnn_convolution_transpose_backward: ATen not compiled with MIOpen support\");\n+}\n+\n+}}\n+\n+#else  // AT_MIOPEN_ENABLED\n+\n+#include \"THC/THC.h\"\n+\n+#include <ATen/miopen/miopen-wrapper.h>\n+#include <ATen/miopen/Descriptors.h>\n+#include <ATen/miopen/Types.h>\n+#include <ATen/miopen/Utils.h>\n+\n+#include <ATen/TensorUtils.h>\n+\n+#include <functional>\n+#include <iterator>\n+#include <sstream>\n+#include <algorithm>\n+#include <memory>\n+#include <mutex>\n+#include <stdint.h>\n+#include <unordered_map>\n+\n+namespace at { namespace native {\n+\n+// TODO: Go through all the checking code again and make sure\n+// we haven't missed anything.\n+\n+// ---------------------------------------------------------------------\n+//\n+// Math\n+//\n+// ---------------------------------------------------------------------\n+\n+constexpr int input_batch_size_dim = 0;  // also grad_input\n+constexpr int input_channels_dim = 1;\n+constexpr int output_batch_size_dim = 0;  // also grad_output\n+constexpr int output_channels_dim = 1;\n+constexpr int weight_output_channels_dim = 0;\n+constexpr int weight_input_channels_dim = 1;\n+\n+// Often written as 2 + max_dim (extra dims for batch size and channels)\n+constexpr int max_dim = 3;\n+\n+// NB: conv_output_size and conv_input_size are not bijections,\n+// as conv_output_size loses information; this is why conv_input_size\n+// takes an extra output_padding argument to resolve the ambiguity.\n+\n+std::vector<int64_t> conv_output_size(\n+    IntList input_size, IntList weight_size,\n+    IntList padding, IntList stride, IntList dilation, int64_t groups\n+) {\n+  // ASSERT(input_size.size() > 2)\n+  // ASSERT(input_size.size() == weight_size.size())\n+  auto dim = input_size.size();\n+  std::vector<int64_t> output_size(dim);\n+  output_size[0] = input_size[input_batch_size_dim];\n+  output_size[1] = weight_size[weight_output_channels_dim];\n+  for (size_t d = 2; d < dim; ++d) {\n+    auto kernel = dilation[d - 2] * (weight_size[d] - 1) + 1;\n+    output_size[d] = (input_size[d] + (2 * padding[d - 2])\n+                        - kernel) / stride[d - 2] + 1;\n+  }\n+  return output_size;\n+}\n+\n+std::vector<int64_t> conv_input_size(\n+    IntList output_size, IntList weight_size,\n+    IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups\n+) {\n+  // ASSERT(output_size.size() > 2)\n+  // ASSERT(output_size.size() == weight_size.size())\n+  auto dim = output_size.size();\n+  std::vector<int64_t> input_size(dim);\n+  input_size[0] = output_size[output_batch_size_dim];\n+  input_size[1] = weight_size[weight_input_channels_dim] * groups;\n+  for (size_t d = 2; d < dim; ++d) {\n+    int kernel = dilation[d - 2] * (weight_size[d] - 1) + 1;\n+    input_size[d] = (output_size[d] - 1) * stride[d - 2] - (2 * padding[d - 2]) +\n+                     kernel + output_padding[d - 2];\n+  }\n+  return input_size;\n+}\n+\n+std::vector<int64_t> conv_weight_size(\n+    IntList input_size, IntList output_size,\n+    IntList padding, IntList output_padding, IntList stride, IntList dilation, int64_t groups\n+) {\n+  auto dim = input_size.size();\n+  std::vector<int64_t> weight_size(dim);\n+  weight_size[0] = output_size[1];\n+  weight_size[1] = input_size[1] / groups;\n+  for (size_t d = 2; d < dim; ++d) {\n+    int kernel = input_size[d] - (output_size[d] - 1) * stride[d - 2]\n+               + 2 * padding[d - 2] - output_padding[d - 2];\n+    weight_size[d] = (kernel - 1) / dilation[d - 2] + 1;\n+  }\n+  return weight_size;\n+}\n+\n+// TODO: Move this into the standard library, with a better name?\n+Tensor narrowGroup(const Tensor& t, int dim, int group_idx, int64_t groups) {\n+  auto group_size = t.size(dim) / groups;\n+  return t.narrow(dim, group_idx * group_size, group_size);\n+}\n+\n+// ---------------------------------------------------------------------\n+//\n+// Checking\n+//\n+// ---------------------------------------------------------------------\n+\n+// Note [Legacy CuDNN grouped convolution support]\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+// CuDNN earlier than CuDNN 7 does not directly support group\n+// convolution, so we provide support for it by sequentially\n+// running a convolution per group  with appropriately\n+// adjusted sizes.  https://blog.yani.io/filter-group-tutorial/\n+// has a fairly good diagram explaining how it works.\n+\n+// Used on pad, stride and dilation\n+static void check_args(CheckedFrom c, IntList args, size_t expected_size, const char* arg_name)\n+{\n+  if (args.size() > expected_size){\n+    std::stringstream ss;\n+    ss << \"Too many \" << arg_name << \" values (\" << args.size() << \") supplied, expecting \" << expected_size << \" (while checking arguments for \" << c << \")\";\n+    throw std::runtime_error(ss.str());\n+  }\n+  else if (args.size() < expected_size){\n+    std::stringstream ss;\n+    ss << \"Not enough \" << arg_name << \" values (\" << args.size() << \") supplied, expecting \" << expected_size << \" (while checking arguments for \" << c << \")\";\n+    throw std::runtime_error(ss.str());\n+  }\n+\n+  auto num_negative_values = std::count_if(args.begin(), args.end(), [](int x){return x < 0;});\n+  if (num_negative_values > 0){\n+    std::stringstream ss;\n+    ss << arg_name << \" should be greater than zero but got (\";\n+    std::copy(args.begin(), args.end() - 1, std::ostream_iterator<int>(ss,\", \"));\n+    ss << args.back() <<  \")\" << \" (while checking arguments for \" << c << \")\";\n+    throw std::runtime_error(ss.str());\n+  }\n+}\n+\n+\n+// NB: For many call sites, it is not strictly necessary to check all of\n+// these relationships (for example, for forward convolution, we compute\n+// the size of output ourselves, so we don't actually need to check\n+// output.  However, writing a single function that does everything\n+// means we get to reuse it for both forwards and all backwards\n+// variants, even when the set of \"real\" inputs varies.  The magic of\n+// relational computing!\n+//\n+// (There is one downside, which is that it is slightly harder to write\n+// error messages which are able to distinguish between real inputs\n+// (which the user can change) and computed inputs (which the user can\n+// only indirectly affect).  It would be an interesting exercise to\n+// come up with a general framework to handle such situations.)\n+static void convolution_shape_check(\n+    CheckedFrom c,\n+    const TensorGeometryArg& input, const TensorGeometryArg& weight, const TensorGeometryArg& output,\n+    IntList padding, IntList stride, IntList dilation, int64_t groups)\n+{\n+  check_args(c, padding, input->dim() - 2, \"padding\");\n+  check_args(c, stride, padding.size(), \"stride\");\n+  check_args(c, dilation, padding.size(), \"dilation\");\n+\n+  // Input\n+  checkDimRange(c, input, 3, 6 /* exclusive */);\n+  checkSize(c, input, input_channels_dim, weight->size(1) * groups);\n+\n+  // Weight\n+  checkSameDim(c, input, weight);\n+\n+  // TODO: check that output->size() matches output_sizes\n+  // TODO: check that weight matches output->sizes()\n+  checkSameDim(c, input, output);\n+}\n+\n+// This POD struct is used to let us easily compute hashes of the\n+// parameters\n+struct ConvolutionParams\n+{\n+  miopenDataType_t dataType;\n+  int input_size[2 + max_dim];\n+  int input_stride[2 + max_dim];\n+  int weight_size[2 + max_dim];\n+  int padding[max_dim];\n+  int stride[max_dim];\n+  int dilation[max_dim];\n+  int64_t groups;\n+  bool deterministic;\n+  // NB: transposed purposely omitted: transposed just swaps\n+  // forward and backward, so you can reuse the benchmark entry,\n+};\n+// ConvolutionParams must be a POD because we read out its memory\n+// contenst as char* when hashing\n+static_assert(std::is_pod<ConvolutionParams>::value, \"ConvolutionParams not POD\");\n+\n+// NB: This can't be a constructor, because then ConvolutionParams\n+// would not be a POD anymore.\n+// TODO: Use TensorGeometry here instead of the entire Tensor, which we\n+// don't actually need.  (OTOH: We can always pass in\n+// grad_input/grad_output, so this is not very pressing)\n+void setConvolutionParams(\n+    ConvolutionParams* params,\n+    const at::Tensor& input, const at::Tensor& weight,\n+    IntList padding, IntList stride, IntList dilation,\n+    int64_t groups, bool deterministic) {\n+\n+  miopenDataType_t dataType = getMiopenDataType(input);\n+  memset(params, 0, sizeof(ConvolutionParams));\n+  params->dataType = dataType;\n+  // ASSERT(weight.dim() == input.dim())\n+  for (int i = 0; i != input.dim(); ++i) {\n+    params->input_size[i] = (int) input.size(i);\n+    params->input_stride[i] = (int) input.stride(i);\n+    params->weight_size[i] = (int) weight.size(i);\n+  }\n+  // ASSERT(padding.size() == stride.size())\n+  // ASSERT(padding.size() == dilation.size())\n+  for (size_t i = 0; i != padding.size(); ++i) {\n+    params->padding[i] = padding[i];\n+    params->stride[i] = stride[i];\n+    params->dilation[i] = dilation[i];\n+  }\n+  // In principle, we shouldn't parametrize by groups for legacy\n+  // CuDNN, but it doesn't seem worth the effort to actually do this.\n+  params->groups = groups;\n+  params->deterministic = deterministic;\n+}\n+\n+// Convenience struct for passing around descriptors and data\n+// pointers\n+struct ConvolutionArgs {\n+  miopenHandle_t handle;\n+  ConvolutionParams params;\n+  TensorDescriptor idesc, odesc;\n+  FilterDescriptor wdesc;\n+  const Tensor& input, output, weight;\n+  ConvolutionDescriptor cdesc;\n+\n+  ConvolutionArgs(const Tensor& input, const Tensor& output, const Tensor& weight) : input(input), output(output), weight(weight) {\n+  }\n+};\n+\n+// ---------------------------------------------------------------------\n+//\n+// Benchmarking\n+//\n+// ---------------------------------------------------------------------\n+\n+// Hashing machinery for ConvolutionParams\n+struct ParamsHash {\n+  std::size_t operator()(const ConvolutionParams& params) const {\n+    auto ptr = reinterpret_cast<const uint8_t*>(&params);\n+    uint32_t value = 0x811C9DC5;\n+    for (int i = 0; i < (int)sizeof(ConvolutionParams); ++i) {\n+      value ^= ptr[i];\n+      value *= 0x01000193;\n+    }\n+    return (size_t)value;\n+  }\n+};\n+\n+struct ParamsEqual {\n+  bool operator()(const ConvolutionParams& a, const ConvolutionParams& b) const {\n+    auto ptr1 = reinterpret_cast<const uint8_t*>(&a);\n+    auto ptr2 = reinterpret_cast<const uint8_t*>(&b);\n+    return memcmp(ptr1, ptr2, sizeof(ConvolutionParams)) == 0;\n+  }\n+};\n+\n+// TODO: Use something less heavy duty than a big honking mutex\n+template <typename T>\n+struct BenchmarkCache {\n+  std::mutex mutex;\n+  std::unordered_map<ConvolutionParams, T, ParamsHash, ParamsEqual> map;\n+\n+  bool find(const ConvolutionParams& params, T* results) {\n+    std::lock_guard<std::mutex> guard(mutex);\n+    auto it = map.find(params);\n+    if (it == map.end()) {\n+      return false;\n+    }\n+    *results = it->second;\n+    return true;\n+  }\n+\n+  void insert(const ConvolutionParams& params, const T& results) {\n+    std::lock_guard<std::mutex> guard(mutex);\n+    map[params] = results;\n+  }\n+};\n+\n+BenchmarkCache<miopenConvFwdAlgorithm_t> fwd_algos;\n+BenchmarkCache<miopenConvBwdDataAlgorithm_t> bwd_data_algos;\n+BenchmarkCache<miopenConvBwdWeightsAlgorithm_t> bwd_filter_algos;\n+\n+// TODO: Stop manually allocating CUDA memory; allocate an ATen byte\n+// tensor instead.\n+struct Workspace {\n+  Workspace(size_t size) : size(size), data(NULL) {\n+    CUDA_CHECK(THCudaMalloc(globalContext().lazyInitCUDA(), &data, size));\n+  }\n+  Workspace(const Workspace&) = delete;\n+  Workspace(Workspace&&) = default;\n+  Workspace& operator=(Workspace&&) = default;\n+  ~Workspace() {\n+    if (data) {\n+      THCudaFree(globalContext().lazyInitCUDA(), data);\n+    }\n+  }\n+\n+  size_t size;\n+  void* data;\n+};\n+\n+template<typename algo_t>\n+struct algorithm_search {\n+};\n+\n+miopenStatus_t getWorkspaceSize(\n+    const ConvolutionArgs& args,\n+    miopenConvFwdAlgorithm_t algo, size_t* sz)\n+{\n+    return miopenConvolutionForwardGetWorkSpaceSize(\n+        args.handle,\n+        args.wdesc.desc(),\n+        args.idesc.desc(),\n+        args.cdesc.desc(),\n+        args.odesc.desc(),\n+        sz);\n+}\n+miopenStatus_t getWorkspaceSize(\n+    const ConvolutionArgs& args, miopenConvBwdDataAlgorithm_t algo, size_t* sz)\n+{\n+    return miopenConvolutionBackwardDataGetWorkSpaceSize(\n+        args.handle,\n+        args.odesc.desc(),\n+        args.wdesc.desc(),\n+        args.cdesc.desc(),\n+        args.idesc.desc(),\n+        sz);\n+}\n+miopenStatus_t getWorkspaceSize(\n+    const ConvolutionArgs& args, miopenConvBwdWeightsAlgorithm_t algo, size_t* sz)\n+{\n+    return miopenConvolutionBackwardWeightsGetWorkSpaceSize(\n+        args.handle,\n+        args.odesc.desc(),\n+        args.idesc.desc(),\n+        args.cdesc.desc(),\n+        args.wdesc.desc(),\n+        sz);\n+}\n+\n+template<typename algo_t>\n+size_t getMaxWorkspaceSize(\n+    const ConvolutionArgs& args,\n+    const algo_t *algo, int n_algo)\n+{\n+    THCState *state = globalContext().lazyInitCUDA();\n+\n+    size_t max_ws_size = 0;\n+    size_t max_block_size = 0;\n+    size_t total_gpu_mem = 0;\n+    size_t free_gpu_mem = 0;\n+\n+    THCudaCheck(THCudaMemGetInfoCached(state, &free_gpu_mem, &total_gpu_mem, &max_block_size));\n+\n+    for (int i = 0; i < n_algo; i++) {\n+        miopenStatus_t err;\n+        size_t sz;\n+        err = getWorkspaceSize(args, algo[i], &sz);\n+        if (miopenStatusSuccess != err || sz == 0\n+            || sz < max_ws_size || sz > max_block_size) continue;\n+        max_ws_size = sz;\n+    }\n+    return max_ws_size;\n+}\n+\n+template<typename perf_t>\n+perf_t getBestAlgorithm(perf_t *perfResults, bool deterministic, int n_algo) {\n+  if (deterministic) {\n+    throw std::runtime_error(\"no deterministic convolution algorithms available in MIOpen\");\n+  } else {\n+    return perfResults[0];\n+  }\n+}\n+\n+template<>\n+struct algorithm_search<miopenConvFwdAlgorithm_t> {\n+  using perf_t = miopenConvAlgoPerf_t;\n+  using algo_t = miopenConvFwdAlgorithm_t;\n+\n+  static constexpr auto DEFAULT_ALGO = miopenConvolutionFwdAlgoGEMM;\n+  static BenchmarkCache<algo_t>& cache() { return fwd_algos; }\n+\n+  static perf_t findAlgorithm(const ConvolutionArgs& args) {\n+    static const algo_t algos[] = {\n+         miopenConvolutionFwdAlgoGEMM,\n+         miopenConvolutionFwdAlgoFFT,\n+         miopenConvolutionFwdAlgoDirect,\n+         miopenConvolutionFwdAlgoWinograd,\n+    };\n+    static constexpr int num_algos = 4;\n+    static_assert(sizeof(algos) / sizeof(algos[0]) == num_algos,\n+                  \"Missing MIOpen convolution forward algorithms\");\n+    int perf_count;\n+    std::unique_ptr<perf_t[]> perf_results(new perf_t[num_algos]);\n+    size_t max_ws_size = getMaxWorkspaceSize(args, algos, num_algos);\n+    Workspace ws(max_ws_size);\n+    MIOPEN_CHECK(miopenFindConvolutionForwardAlgorithm(\n+        args.handle,\n+        args.idesc.desc(), args.input.data_ptr(),\n+        args.wdesc.desc(), args.weight.data_ptr(),\n+        args.cdesc.desc(),\n+        args.odesc.desc(), args.output.data_ptr(),\n+        num_algos,\n+        &perf_count,\n+        &perf_results,\n+        ws.data,\n+        ws.size,\n+        false));\n+    return getBestAlgorithm(perf_results.get(), args.params.deterministic, perf_count)\n+  }\n+\n+  static void getAlgorithm(\n+    const ConvolutionArgs& args,\n+    algo_t* algo)\n+  {\n+    throw std::runtime_error(\"miopenGetConvolutionForwardAlgorithm is not supported.\");\n+    /*miopenConvolutionFwdPreference_t pref = cudnn_convolution_FWD_PREFER_FASTEST;\n+    MIOPEN_CHECK(miopenGetConvolutionForwardAlgorithm(\n+        args.handle,\n+        args.idesc.desc(),\n+        args.wdesc.desc(),\n+        args.cdesc.desc(),\n+        args.odesc.desc(),\n+        pref,\n+        0,\n+        algo));*/\n+  }\n+\n+  static void getWorkspaceSize(\n+    const ConvolutionArgs& args,\n+    algo_t algo, size_t* workspaceSize)\n+  {\n+    throw std::runtime_error(\"miopenGetConvolutionForwardWorkspaceSize is not supported.\");\n+    /*MIOPEN_CHECK(miopenGetConvolutionForwardWorkspaceSize(\n+        args.handle,\n+        args.idesc.desc(),\n+        args.wdesc.desc(),\n+        args.cdesc.desc(),\n+        args.odesc.desc(),\n+        algo,\n+        workspaceSize));*/\n+  }\n+};\n+\n+template<>\n+struct algorithm_search<miopenConvBwdDataAlgorithm_t> {\n+  using perf_t = miopenConvAlgoPerf_t;\n+  using algo_t = miopenConvBwdDataAlgorithm_t;\n+\n+  static constexpr auto DEFAULT_ALGO = miopenConvolutionBwdDataAlgoGEMM;\n+  static BenchmarkCache<algo_t>& cache() { return bwd_data_algos; }\n+\n+  static perf_t findAlgorithm(const ConvolutionArgs& args) {\n+    static const algo_t algos[] = {\n+        miopenConvolutionBwdDataAlgoGEMM,\n+        miopenConvolutionBwdDataAlgoDirect,\n+        miopenConvolutionBwdDataAlgoFFT,\n+        miopenConvolutionBwdDataAlgoWinograd,\n+    };\n+    static constexpr int num_algos = 4;\n+    static_assert(sizeof(algos) / sizeof(algos[0]) == num_algos,\n+                  \"Missing MIOpen convolution backward data algorithms.\");\n+    int perf_count;\n+    std::unique_ptr<perf_t[]> perf_results(new perf_t[num_algos]);\n+    size_t max_ws_size = getMaxWorkspaceSize(args, algos, num_algos);\n+    Workspace ws(max_ws_size);\n+    MIOPEN_CHECK(miopenFindConvolutionBackwardDataAlgorithm(\n+        args.handle,\n+        args.odesc.desc(), args.output.data_ptr(),\n+        args.wdesc.desc(), args.weight.data_ptr(),\n+        args.cdesc.desc(),\n+        args.idesc.desc(), args.input.data_ptr(),\n+        num_algos,\n+        &perf_count,\n+        &perf_results,\n+        ws.data,\n+        ws.size,\n+        false));\n+    return getBestAlgorithm(perf_results.get(), args.params.deterministic, perf_count);\n+  }\n+\n+  static void getAlgorithm(const ConvolutionArgs& args, algo_t* algo) {\n+    throw std::runtime_error(\"miopenGetConvolutionBackwardDataAlgorithm is not supported.\");\n+\n+    /*MIOPEN_CHECK(miopenGetConvolutionBackwardDataAlgorithm(\n+        args.handle,\n+        args.wdesc.desc(),\n+        args.odesc.desc(),\n+        args.cdesc.desc(),\n+        args.idesc.desc(),\n+        cudnn_convolution_BWD_DATA_PREFER_FASTEST,\n+        0,\n+        algo));*/\n+  }\n+\n+  static void getWorkspaceSize(\n+    const ConvolutionArgs& args,\n+    miopenConvBwdDataAlgorithm_t algo, size_t* workspaceSize)\n+  {\n+    throw std::runtime_error(\"miopenGetConvolutionBackwardDataWorkspaceSize is not supported.\");", "path": "aten/src/ATen/native/miopen/Conv.cpp", "position": 588, "original_position": 587, "commit_id": "f483f7f5be6b4c1f978136eda606996dc29bfe12", "original_commit_id": "a59acd623ced715e840278fae8e856250aa9fe71", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "These workspace functions are all internal ones, so if miopen doesn't need them, get rid of them.", "created_at": "2018-06-08T03:42:56Z", "updated_at": "2018-11-23T15:45:13Z", "html_url": "https://github.com/pytorch/pytorch/pull/8257#discussion_r193946075", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8257", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193946075"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8257#discussion_r193946075"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8257"}}, "body_html": "<p>These workspace functions are all internal ones, so if miopen doesn't need them, get rid of them.</p>", "body_text": "These workspace functions are all internal ones, so if miopen doesn't need them, get rid of them."}