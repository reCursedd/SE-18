{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1521", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1521/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1521/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1521/events", "html_url": "https://github.com/pytorch/pytorch/issues/1521", "id": 227475544, "node_id": "MDU6SXNzdWUyMjc0NzU1NDQ=", "number": 1521, "title": "CUDA inplace spcadd doesn't work with noncontiguous dense tensor", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-09T19:41:28Z", "updated_at": "2017-05-09T20:41:50Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Here is code comparing the CUDA and non-CUDA behavior:</p>\n<pre><code>        m1 = torch.randn(2,5)\n        m2 = m1[:, 4]\n        x = torch.sparse.DoubleTensor(torch.LongTensor([[0,1]]), torch.DoubleTensor([33, 44]))\n        m2[0] = 0\n        m2[1] = 0\n        m2.add_(x)\n        print(x)\n        print(m1)\n        print(m2)\n        print(\"----\")\n        \n        m1 = torch.randn(2,5).cuda()\n        m2 = m1[:, 4]\n        x = torch.sparse.DoubleTensor(torch.LongTensor([[0,1]]), torch.DoubleTensor([33, 44])).cuda()    \n        m2[0] = 0\n        m2[1] = 0\n        m2.add_(x)\n        print(x)\n        print(m1)\n        print(m2)\n</code></pre>\n<pre><code>DoubleTensor of size 2 with indices:\n\n 0  1\n[torch.LongTensor of size 1x2]\nand values:\n        \n 33     \n 44     \n[torch.DoubleTensor of size 2]\n\n\n -0.5214  -1.4914  -0.2381   1.0306  33.0000\n  1.5162  -1.5116  -0.5050  -0.2216  44.0000\n[torch.DoubleTensor of size 2x5]\n\n\n 33\n 44     \n[torch.DoubleTensor of size 2]\n \n----\nDoubleTensor of size 2 with indices:\n\n 0  1\n[torch.cuda.LongTensor of size 1x2 (GPU 0)]\nand values:\n\n 33\n 44\n[torch.cuda.DoubleTensor of size 2 (GPU 0)]\n\n\n-1.5411  0.8085  1.0213 -0.1240  0.0000\n-1.2078 -0.5452  0.0656 -0.7886  0.0000\n[torch.cuda.DoubleTensor of size 2x5 (GPU 0)]\n\n\n 0\n 0\n[torch.cuda.DoubleTensor of size 2 (GPU 0)]\n</code></pre>\n<p>In the CUDA case no update actually occurred.</p>\n<p>Looking at the CUDA code it's pretty clear what the problem is:</p>\n<pre><code>  THCTensor *r = r_;\n  if (r != dense) {\n    THCTensor_(retain)(state, r);\n    THCTensor_(resizeAs)(state, r, dense);\n    THCTensor_(copy)(state, r, dense);\n  } else {\n    r = THCTensor_(newContiguous)(state, r_);\n  }\n</code></pre>\n<p>The call to <code>newContiguous</code> will create a clone of <code>r_</code> if it was not contiguous. This means that updates to <code>r</code> will not be reflected in <code>r_</code>.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2560662\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinraison\">@martinraison</a> who originally added <code>spcadd</code>, and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a> who sped it up.</p>", "body_text": "Here is code comparing the CUDA and non-CUDA behavior:\n        m1 = torch.randn(2,5)\n        m2 = m1[:, 4]\n        x = torch.sparse.DoubleTensor(torch.LongTensor([[0,1]]), torch.DoubleTensor([33, 44]))\n        m2[0] = 0\n        m2[1] = 0\n        m2.add_(x)\n        print(x)\n        print(m1)\n        print(m2)\n        print(\"----\")\n        \n        m1 = torch.randn(2,5).cuda()\n        m2 = m1[:, 4]\n        x = torch.sparse.DoubleTensor(torch.LongTensor([[0,1]]), torch.DoubleTensor([33, 44])).cuda()    \n        m2[0] = 0\n        m2[1] = 0\n        m2.add_(x)\n        print(x)\n        print(m1)\n        print(m2)\n\nDoubleTensor of size 2 with indices:\n\n 0  1\n[torch.LongTensor of size 1x2]\nand values:\n        \n 33     \n 44     \n[torch.DoubleTensor of size 2]\n\n\n -0.5214  -1.4914  -0.2381   1.0306  33.0000\n  1.5162  -1.5116  -0.5050  -0.2216  44.0000\n[torch.DoubleTensor of size 2x5]\n\n\n 33\n 44     \n[torch.DoubleTensor of size 2]\n \n----\nDoubleTensor of size 2 with indices:\n\n 0  1\n[torch.cuda.LongTensor of size 1x2 (GPU 0)]\nand values:\n\n 33\n 44\n[torch.cuda.DoubleTensor of size 2 (GPU 0)]\n\n\n-1.5411  0.8085  1.0213 -0.1240  0.0000\n-1.2078 -0.5452  0.0656 -0.7886  0.0000\n[torch.cuda.DoubleTensor of size 2x5 (GPU 0)]\n\n\n 0\n 0\n[torch.cuda.DoubleTensor of size 2 (GPU 0)]\n\nIn the CUDA case no update actually occurred.\nLooking at the CUDA code it's pretty clear what the problem is:\n  THCTensor *r = r_;\n  if (r != dense) {\n    THCTensor_(retain)(state, r);\n    THCTensor_(resizeAs)(state, r, dense);\n    THCTensor_(copy)(state, r, dense);\n  } else {\n    r = THCTensor_(newContiguous)(state, r_);\n  }\n\nThe call to newContiguous will create a clone of r_ if it was not contiguous. This means that updates to r will not be reflected in r_.\nCC @martinraison who originally added spcadd, and @adamlerer who sped it up.", "body": "Here is code comparing the CUDA and non-CUDA behavior:\r\n\r\n```\r\n        m1 = torch.randn(2,5)\r\n        m2 = m1[:, 4]\r\n        x = torch.sparse.DoubleTensor(torch.LongTensor([[0,1]]), torch.DoubleTensor([33, 44]))\r\n        m2[0] = 0\r\n        m2[1] = 0\r\n        m2.add_(x)\r\n        print(x)\r\n        print(m1)\r\n        print(m2)\r\n        print(\"----\")\r\n        \r\n        m1 = torch.randn(2,5).cuda()\r\n        m2 = m1[:, 4]\r\n        x = torch.sparse.DoubleTensor(torch.LongTensor([[0,1]]), torch.DoubleTensor([33, 44])).cuda()    \r\n        m2[0] = 0\r\n        m2[1] = 0\r\n        m2.add_(x)\r\n        print(x)\r\n        print(m1)\r\n        print(m2)\r\n```\r\n\r\n```\r\nDoubleTensor of size 2 with indices:\r\n\r\n 0  1\r\n[torch.LongTensor of size 1x2]\r\nand values:\r\n        \r\n 33     \r\n 44     \r\n[torch.DoubleTensor of size 2]\r\n\r\n\r\n -0.5214  -1.4914  -0.2381   1.0306  33.0000\r\n  1.5162  -1.5116  -0.5050  -0.2216  44.0000\r\n[torch.DoubleTensor of size 2x5]\r\n\r\n\r\n 33\r\n 44     \r\n[torch.DoubleTensor of size 2]\r\n \r\n----\r\nDoubleTensor of size 2 with indices:\r\n\r\n 0  1\r\n[torch.cuda.LongTensor of size 1x2 (GPU 0)]\r\nand values:\r\n\r\n 33\r\n 44\r\n[torch.cuda.DoubleTensor of size 2 (GPU 0)]\r\n\r\n\r\n-1.5411  0.8085  1.0213 -0.1240  0.0000\r\n-1.2078 -0.5452  0.0656 -0.7886  0.0000\r\n[torch.cuda.DoubleTensor of size 2x5 (GPU 0)]\r\n\r\n\r\n 0\r\n 0\r\n[torch.cuda.DoubleTensor of size 2 (GPU 0)]\r\n```\r\n\r\nIn the CUDA case no update actually occurred.\r\n\r\nLooking at the CUDA code it's pretty clear what the problem is:\r\n\r\n```\r\n  THCTensor *r = r_;\r\n  if (r != dense) {\r\n    THCTensor_(retain)(state, r);\r\n    THCTensor_(resizeAs)(state, r, dense);\r\n    THCTensor_(copy)(state, r, dense);\r\n  } else {\r\n    r = THCTensor_(newContiguous)(state, r_);\r\n  }\r\n```\r\n\r\nThe call to `newContiguous` will create a clone of `r_` if it was not contiguous. This means that updates to `r` will not be reflected in `r_`.\r\n\r\nCC @martinraison who originally added `spcadd`, and @adamlerer who sped it up."}