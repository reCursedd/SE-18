{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/369099036", "html_url": "https://github.com/pytorch/pytorch/issues/5430#issuecomment-369099036", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5430", "id": 369099036, "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTA5OTAzNg==", "user": {"login": "jevenzh", "id": 17583521, "node_id": "MDQ6VXNlcjE3NTgzNTIx", "avatar_url": "https://avatars3.githubusercontent.com/u/17583521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jevenzh", "html_url": "https://github.com/jevenzh", "followers_url": "https://api.github.com/users/jevenzh/followers", "following_url": "https://api.github.com/users/jevenzh/following{/other_user}", "gists_url": "https://api.github.com/users/jevenzh/gists{/gist_id}", "starred_url": "https://api.github.com/users/jevenzh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jevenzh/subscriptions", "organizations_url": "https://api.github.com/users/jevenzh/orgs", "repos_url": "https://api.github.com/users/jevenzh/repos", "events_url": "https://api.github.com/users/jevenzh/events{/privacy}", "received_events_url": "https://api.github.com/users/jevenzh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-28T02:24:17Z", "updated_at": "2018-02-28T02:30:22Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> yes, id of iterator,  this error is raised from the C code, right? I tried printing out the id of iter, it's the same.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/17583521/36766787-5c9b30b4-1c72-11e8-862d-34f06c8beed1.png\"><img src=\"https://user-images.githubusercontent.com/17583521/36766787-5c9b30b4-1c72-11e8-862d-34f06c8beed1.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>Sure, I will paste my full script for training. Note that I used a customized dataloader for eval, but it is not this customized one that caused the error.</p>\n<pre><code># encoding: utf-8\n\nimport time\nimport numpy as np\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom tensorboardX import SummaryWriter\nfrom utils import *\n\n\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\nlogdir = './logs/exp5-densenet201'\nwriter = SummaryWriter(log_dir=logdir)\nprint('training log is writing to: ', logdir)\ntrain_data = datasets.ImageFolder('./plants/train/',\n                                  transform=transforms.Compose([\n                                      transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n                                      transforms.RandomHorizontalFlip(),\n                                      transforms.RandomRotation(90),\n                                      transforms.RandomVerticalFlip(),\n                                      # transforms.ColorJitter(brightness=0.5),\n                                      transforms.ToTensor(),\n                                      normalize])\n                                  )\n\neval_data = datasets.ImageFolder('./plants/eval/',\n                                 transform=transforms.Compose([\n                                     transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n                                     transforms.ToTensor(),\n                                     normalize])\n                                 )\n\ntrain_loader = torch.utils.data.DataLoader(train_data,\n                                           batch_size=10,\n                                           shuffle=True,\n                                           num_workers=4)\n\neval_loader = CustomDataLoader(eval_data,\n                               batch_size=10,\n                               shuffle=False,\n                               num_workers=4)\n\nprint('Num of images in the dataset: ' + \\\n      'train {}, eval {}'.format(len(train_loader.dataset.imgs),\n                                 len(eval_loader.dataset.imgs)))\n\nclass_names = train_data.classes\nprint('Label info: ')\nfor key, val in train_data.class_to_idx.items():\n    print(key, val)\n\nprint('=' * 50)\nprint('train batches: ', len(train_loader))\nprint('test batches: ', len(eval_loader))\n\n# Load pretrained model.\nmodel = models.densenet201(pretrained=True)\n# model = models.resnet34(pretrained=True)\n# model.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n\n# Setup optimizer and loss function\nmodel_path = './models/densenet201_newdata_params.pkl'\n# model_path = '/output/model/latest_params.pkl'\nepochs = 600\neval_freq = 1\nprint_freq = 10\ntotal_steps = 0\nlr = 0.001\nhidden_dim = 800\ncontinue_train = False\nbest_test_acc = 0\n\n# If you want to finetune only top layer of the model.\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# Replace top layer for finetuning.\n# model.fc = torch.nn.Linear(model.fc.in_features, len(class_names))\nmodel.classifier = nn.Linear(model.classifier.in_features, len(class_names))\n# model.classifier = nn.Sequential(nn.Linear(model.classifier.in_features, hidden_dim),\n#                                 nn.ReLU(),\n#                                 nn.Linear(hidden_dim, len(class_names))\n#                                 )\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, nesterov=True)\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n\nif torch.cuda.is_available():\n    model.cuda()\n\nif continue_train:\n    model.load_state_dict(torch.load(model_path))\n    print('loaded saved model params from: ', model_path)\n\n\n# Training and testing\nfor epoch in range(epochs):\n    writer.add_text('Epoch: ', str(epoch + 1))\n    iteration = 0\n    start = time.time()\n    bloss = 0\n    model.train()  # train mode\n    for batch, labels in train_loader:\n        iteration += 1\n        total_steps += 1\n        images = Variable(batch)\n        targets = Variable(labels)\n        if torch.cuda.is_available():\n            images = images.cuda()\n            targets = targets.cuda()\n        outputs = model(images)\n        loss = loss_fn(outputs, targets)\n        bloss += loss\n        # backprop\n        # scheduler.step()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # print loss\n        if total_steps % print_freq == 0:\n            print('Epoch: {0}, Iter: {1}, Loss: {2:8.6f}, {3:6.5f} sec/batch'.format(\n                epoch, iteration, bloss.data[0] / print_freq, (time.time() - start) / print_freq))\n            start = time.time()\n            bloss = 0\n\n    # Test the Model\n    if (epoch + 1) % eval_freq == 0:\n        model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n        correct = 0\n        total = 0\n        for inputs, labels in eval_loader:\n            ncorps = 5\n            predicts = []\n            # iterator = eval_loader.iter\n            for i in range(ncorps):\n                images = eval_loader.get_cur_batch()\n                images = Variable(images)\n                if torch.cuda.is_available():\n                    images = images.cuda()\n                    labels = labels.cuda()\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                predicts.append(predicted)\n            eval_loader.iter.reset_indices()\n            predicts = torch.stack(predicts, dim=1)\n            predicted = most_common(predicts)\n            total += labels.size(0)\n            correct += (predicted == labels).sum()\n        print('Correct pred: ', correct)\n        test_acc = 100 * correct / total\n        print('Test Accuracy of the model on test images: %d%%' % test_acc)\n        writer.add_scalar('eval_acc', test_acc, total_steps)\n        # Save the Trained Model\n        if test_acc &gt; best_test_acc:\n            best_test_acc = test_acc\n            print('Saving best model params to {}'.format(model_path))\n            torch.save(model.state_dict(), model_path)\n\nwriter.close()\n</code></pre>", "body_text": "@SsnL yes, id of iterator,  this error is raised from the C code, right? I tried printing out the id of iter, it's the same.\n\nSure, I will paste my full script for training. Note that I used a customized dataloader for eval, but it is not this customized one that caused the error.\n# encoding: utf-8\n\nimport time\nimport numpy as np\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom tensorboardX import SummaryWriter\nfrom utils import *\n\n\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\nlogdir = './logs/exp5-densenet201'\nwriter = SummaryWriter(log_dir=logdir)\nprint('training log is writing to: ', logdir)\ntrain_data = datasets.ImageFolder('./plants/train/',\n                                  transform=transforms.Compose([\n                                      transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n                                      transforms.RandomHorizontalFlip(),\n                                      transforms.RandomRotation(90),\n                                      transforms.RandomVerticalFlip(),\n                                      # transforms.ColorJitter(brightness=0.5),\n                                      transforms.ToTensor(),\n                                      normalize])\n                                  )\n\neval_data = datasets.ImageFolder('./plants/eval/',\n                                 transform=transforms.Compose([\n                                     transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\n                                     transforms.ToTensor(),\n                                     normalize])\n                                 )\n\ntrain_loader = torch.utils.data.DataLoader(train_data,\n                                           batch_size=10,\n                                           shuffle=True,\n                                           num_workers=4)\n\neval_loader = CustomDataLoader(eval_data,\n                               batch_size=10,\n                               shuffle=False,\n                               num_workers=4)\n\nprint('Num of images in the dataset: ' + \\\n      'train {}, eval {}'.format(len(train_loader.dataset.imgs),\n                                 len(eval_loader.dataset.imgs)))\n\nclass_names = train_data.classes\nprint('Label info: ')\nfor key, val in train_data.class_to_idx.items():\n    print(key, val)\n\nprint('=' * 50)\nprint('train batches: ', len(train_loader))\nprint('test batches: ', len(eval_loader))\n\n# Load pretrained model.\nmodel = models.densenet201(pretrained=True)\n# model = models.resnet34(pretrained=True)\n# model.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n\n# Setup optimizer and loss function\nmodel_path = './models/densenet201_newdata_params.pkl'\n# model_path = '/output/model/latest_params.pkl'\nepochs = 600\neval_freq = 1\nprint_freq = 10\ntotal_steps = 0\nlr = 0.001\nhidden_dim = 800\ncontinue_train = False\nbest_test_acc = 0\n\n# If you want to finetune only top layer of the model.\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# Replace top layer for finetuning.\n# model.fc = torch.nn.Linear(model.fc.in_features, len(class_names))\nmodel.classifier = nn.Linear(model.classifier.in_features, len(class_names))\n# model.classifier = nn.Sequential(nn.Linear(model.classifier.in_features, hidden_dim),\n#                                 nn.ReLU(),\n#                                 nn.Linear(hidden_dim, len(class_names))\n#                                 )\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, nesterov=True)\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n\nif torch.cuda.is_available():\n    model.cuda()\n\nif continue_train:\n    model.load_state_dict(torch.load(model_path))\n    print('loaded saved model params from: ', model_path)\n\n\n# Training and testing\nfor epoch in range(epochs):\n    writer.add_text('Epoch: ', str(epoch + 1))\n    iteration = 0\n    start = time.time()\n    bloss = 0\n    model.train()  # train mode\n    for batch, labels in train_loader:\n        iteration += 1\n        total_steps += 1\n        images = Variable(batch)\n        targets = Variable(labels)\n        if torch.cuda.is_available():\n            images = images.cuda()\n            targets = targets.cuda()\n        outputs = model(images)\n        loss = loss_fn(outputs, targets)\n        bloss += loss\n        # backprop\n        # scheduler.step()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # print loss\n        if total_steps % print_freq == 0:\n            print('Epoch: {0}, Iter: {1}, Loss: {2:8.6f}, {3:6.5f} sec/batch'.format(\n                epoch, iteration, bloss.data[0] / print_freq, (time.time() - start) / print_freq))\n            start = time.time()\n            bloss = 0\n\n    # Test the Model\n    if (epoch + 1) % eval_freq == 0:\n        model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n        correct = 0\n        total = 0\n        for inputs, labels in eval_loader:\n            ncorps = 5\n            predicts = []\n            # iterator = eval_loader.iter\n            for i in range(ncorps):\n                images = eval_loader.get_cur_batch()\n                images = Variable(images)\n                if torch.cuda.is_available():\n                    images = images.cuda()\n                    labels = labels.cuda()\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                predicts.append(predicted)\n            eval_loader.iter.reset_indices()\n            predicts = torch.stack(predicts, dim=1)\n            predicted = most_common(predicts)\n            total += labels.size(0)\n            correct += (predicted == labels).sum()\n        print('Correct pred: ', correct)\n        test_acc = 100 * correct / total\n        print('Test Accuracy of the model on test images: %d%%' % test_acc)\n        writer.add_scalar('eval_acc', test_acc, total_steps)\n        # Save the Trained Model\n        if test_acc > best_test_acc:\n            best_test_acc = test_acc\n            print('Saving best model params to {}'.format(model_path))\n            torch.save(model.state_dict(), model_path)\n\nwriter.close()", "body": "@SsnL yes, id of iterator,  this error is raised from the C code, right? I tried printing out the id of iter, it's the same. \r\n![image](https://user-images.githubusercontent.com/17583521/36766787-5c9b30b4-1c72-11e8-862d-34f06c8beed1.png)\r\n\r\n\r\nSure, I will paste my full script for training. Note that I used a customized dataloader for eval, but it is not this customized one that caused the error.\r\n\r\n```\r\n# encoding: utf-8\r\n\r\nimport time\r\nimport numpy as np\r\nimport torch.nn as nn\r\nimport torch.utils.data\r\nimport torch.utils.data.distributed\r\nimport torchvision.datasets as datasets\r\nimport torchvision.models as models\r\nimport torchvision.transforms as transforms\r\nfrom torch.autograd import Variable\r\nfrom tensorboardX import SummaryWriter\r\nfrom utils import *\r\n\r\n\r\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                                 std=[0.229, 0.224, 0.225])\r\n\r\nlogdir = './logs/exp5-densenet201'\r\nwriter = SummaryWriter(log_dir=logdir)\r\nprint('training log is writing to: ', logdir)\r\ntrain_data = datasets.ImageFolder('./plants/train/',\r\n                                  transform=transforms.Compose([\r\n                                      transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\r\n                                      transforms.RandomHorizontalFlip(),\r\n                                      transforms.RandomRotation(90),\r\n                                      transforms.RandomVerticalFlip(),\r\n                                      # transforms.ColorJitter(brightness=0.5),\r\n                                      transforms.ToTensor(),\r\n                                      normalize])\r\n                                  )\r\n\r\neval_data = datasets.ImageFolder('./plants/eval/',\r\n                                 transform=transforms.Compose([\r\n                                     transforms.RandomResizedCrop(224, scale=(0.8, 1.2)),\r\n                                     transforms.ToTensor(),\r\n                                     normalize])\r\n                                 )\r\n\r\ntrain_loader = torch.utils.data.DataLoader(train_data,\r\n                                           batch_size=10,\r\n                                           shuffle=True,\r\n                                           num_workers=4)\r\n\r\neval_loader = CustomDataLoader(eval_data,\r\n                               batch_size=10,\r\n                               shuffle=False,\r\n                               num_workers=4)\r\n\r\nprint('Num of images in the dataset: ' + \\\r\n      'train {}, eval {}'.format(len(train_loader.dataset.imgs),\r\n                                 len(eval_loader.dataset.imgs)))\r\n\r\nclass_names = train_data.classes\r\nprint('Label info: ')\r\nfor key, val in train_data.class_to_idx.items():\r\n    print(key, val)\r\n\r\nprint('=' * 50)\r\nprint('train batches: ', len(train_loader))\r\nprint('test batches: ', len(eval_loader))\r\n\r\n# Load pretrained model.\r\nmodel = models.densenet201(pretrained=True)\r\n# model = models.resnet34(pretrained=True)\r\n# model.avgpool = torch.nn.AdaptiveAvgPool2d(1)\r\n\r\n# Setup optimizer and loss function\r\nmodel_path = './models/densenet201_newdata_params.pkl'\r\n# model_path = '/output/model/latest_params.pkl'\r\nepochs = 600\r\neval_freq = 1\r\nprint_freq = 10\r\ntotal_steps = 0\r\nlr = 0.001\r\nhidden_dim = 800\r\ncontinue_train = False\r\nbest_test_acc = 0\r\n\r\n# If you want to finetune only top layer of the model.\r\n# for param in model.parameters():\r\n#     param.requires_grad = False\r\n\r\n# Replace top layer for finetuning.\r\n# model.fc = torch.nn.Linear(model.fc.in_features, len(class_names))\r\nmodel.classifier = nn.Linear(model.classifier.in_features, len(class_names))\r\n# model.classifier = nn.Sequential(nn.Linear(model.classifier.in_features, hidden_dim),\r\n#                                 nn.ReLU(),\r\n#                                 nn.Linear(hidden_dim, len(class_names))\r\n#                                 )\r\n\r\nloss_fn = torch.nn.CrossEntropyLoss()\r\noptimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9, nesterov=True)\r\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\r\n\r\nif torch.cuda.is_available():\r\n    model.cuda()\r\n\r\nif continue_train:\r\n    model.load_state_dict(torch.load(model_path))\r\n    print('loaded saved model params from: ', model_path)\r\n\r\n\r\n# Training and testing\r\nfor epoch in range(epochs):\r\n    writer.add_text('Epoch: ', str(epoch + 1))\r\n    iteration = 0\r\n    start = time.time()\r\n    bloss = 0\r\n    model.train()  # train mode\r\n    for batch, labels in train_loader:\r\n        iteration += 1\r\n        total_steps += 1\r\n        images = Variable(batch)\r\n        targets = Variable(labels)\r\n        if torch.cuda.is_available():\r\n            images = images.cuda()\r\n            targets = targets.cuda()\r\n        outputs = model(images)\r\n        loss = loss_fn(outputs, targets)\r\n        bloss += loss\r\n        # backprop\r\n        # scheduler.step()\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        # print loss\r\n        if total_steps % print_freq == 0:\r\n            print('Epoch: {0}, Iter: {1}, Loss: {2:8.6f}, {3:6.5f} sec/batch'.format(\r\n                epoch, iteration, bloss.data[0] / print_freq, (time.time() - start) / print_freq))\r\n            start = time.time()\r\n            bloss = 0\r\n\r\n    # Test the Model\r\n    if (epoch + 1) % eval_freq == 0:\r\n        model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\r\n        correct = 0\r\n        total = 0\r\n        for inputs, labels in eval_loader:\r\n            ncorps = 5\r\n            predicts = []\r\n            # iterator = eval_loader.iter\r\n            for i in range(ncorps):\r\n                images = eval_loader.get_cur_batch()\r\n                images = Variable(images)\r\n                if torch.cuda.is_available():\r\n                    images = images.cuda()\r\n                    labels = labels.cuda()\r\n                outputs = model(images)\r\n                _, predicted = torch.max(outputs.data, 1)\r\n                predicts.append(predicted)\r\n            eval_loader.iter.reset_indices()\r\n            predicts = torch.stack(predicts, dim=1)\r\n            predicted = most_common(predicts)\r\n            total += labels.size(0)\r\n            correct += (predicted == labels).sum()\r\n        print('Correct pred: ', correct)\r\n        test_acc = 100 * correct / total\r\n        print('Test Accuracy of the model on test images: %d%%' % test_acc)\r\n        writer.add_scalar('eval_acc', test_acc, total_steps)\r\n        # Save the Trained Model\r\n        if test_acc > best_test_acc:\r\n            best_test_acc = test_acc\r\n            print('Saving best model params to {}'.format(model_path))\r\n            torch.save(model.state_dict(), model_path)\r\n\r\nwriter.close()\r\n```\r\n"}