{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/370299111", "html_url": "https://github.com/pytorch/pytorch/issues/5430#issuecomment-370299111", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5430", "id": 370299111, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDI5OTExMQ==", "user": {"login": "jevenzh", "id": 17583521, "node_id": "MDQ6VXNlcjE3NTgzNTIx", "avatar_url": "https://avatars3.githubusercontent.com/u/17583521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jevenzh", "html_url": "https://github.com/jevenzh", "followers_url": "https://api.github.com/users/jevenzh/followers", "following_url": "https://api.github.com/users/jevenzh/following{/other_user}", "gists_url": "https://api.github.com/users/jevenzh/gists{/gist_id}", "starred_url": "https://api.github.com/users/jevenzh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jevenzh/subscriptions", "organizations_url": "https://api.github.com/users/jevenzh/orgs", "repos_url": "https://api.github.com/users/jevenzh/repos", "events_url": "https://api.github.com/users/jevenzh/events{/privacy}", "received_events_url": "https://api.github.com/users/jevenzh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-05T03:20:35Z", "updated_at": "2018-03-05T10:02:02Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> I tried to print id(self) in _shutdown_workers, then I found my customized eval dataloader does not shutdown properly, so that at next training epoch the train dataloader will get the same object used in previous epoch for evaluation. The id printed right before \"Correct pred: 189\" is the id of eval dataloader, which does not shutdown.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/17583521/36955693-ed64aaf8-2064-11e8-95d2-a4efe9853f4a.png\"><img src=\"https://user-images.githubusercontent.com/17583521/36955693-ed64aaf8-2064-11e8-95d2-a4efe9853f4a.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>My customized dataloader is as follow, the reason for this is I want to use current batch multiple times with different transforms at test time, so I made current indices an attribute, and in each iteration of eval loader, I manually reset the indices. For now, I can't figure out why this makes it not shutting down.</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">CustomDataLoaderIter</span>(DataLoaderIter):\n    def <span class=\"pl-en\">__init__</span>(self, loader):\n        super(CustomDataLoaderIter, self).__init__(loader)\n        self.num_workers = 0\n        self.indices = None\n\n    def __next__(self):\n        if self.num_workers == 0:  # same-process loading\n            <span class=\"pl-k\">if</span> self.indices is None:\n                self.indices = next(self.sample_iter)  # may raise StopIteration\n            batch = self.collate_fn([self.dataset[i] <span class=\"pl-k\">for</span> i in self.indices])\n            <span class=\"pl-k\">if</span> self.pin_memory:\n                batch = pin_memory_batch(batch)\n\n            <span class=\"pl-k\">return</span> batch\n\n        # check <span class=\"pl-k\">if</span> the next sample has already been generated\n        <span class=\"pl-k\">if</span> self.rcvd_idx in self.reorder_dict:\n            batch = self.reorder_dict.pop(self.rcvd_idx)\n            <span class=\"pl-k\">return</span> self._process_next_batch(batch)\n\n        <span class=\"pl-k\">if</span> self.batches_outstanding == <span class=\"pl-c1\">0</span>:\n            self._shutdown_workers()\n            raise StopIteration\n\n        <span class=\"pl-k\">while</span> True:\n            <span class=\"pl-en\">assert</span> (<span class=\"pl-k\">not</span> self.shutdown <span class=\"pl-k\">and</span> self.batches_outstanding &gt; <span class=\"pl-c1\">0</span>)\n            idx, batch = self.data_queue.get()\n            self.batches_outstanding -= 1\n            if idx != self.rcvd_idx:\n                # store out-of-order samples\n                self.reorder_dict[idx] = batch\n                <span class=\"pl-k\">continue</span>\n            <span class=\"pl-k\">return</span> self._process_next_batch(batch)\n\n    def <span class=\"pl-en\">cur_batch</span>(self):\n        return self.__next__()\n\n    def reset_indices(self):\n        self.indices = None\n\n# the customized DataLoader\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CustomDataLoader</span>(DataLoader):\n    def <span class=\"pl-en\">__init__</span>(self, dataset, batch_size=<span class=\"pl-c1\">1</span>, shuffle=False, sampler=None,\n                 batch_sampler=None, num_workers=<span class=\"pl-c1\">0</span>, collate_fn=default_collate,\n                 pin_memory=False, drop_last=False):\n        super(CustomDataLoader, self).__init__(dataset, batch_size, shuffle,\n                                               sampler, batch_sampler, num_workers,\n                                               collate_fn, pin_memory,\n                                               drop_last)\n\n    def __iter__(self):\n        self.iter = CustomDataLoaderIter(self)\n        return self.iter\n\n    def get_cur_batch(self):\n        return self.iter.cur_batch()[0]\n</pre></div>\n<p>In evaluation:</p>\n<pre><code>for inputs, labels in eval_loader:\n            ncorps = 5\n            predicts = []\n            # iterator = eval_loader.iter\n            for i in range(ncorps):\n                images = eval_loader.get_cur_batch()\n                images = Variable(images)\n                if torch.cuda.is_available():\n                    images = images.cuda()\n                    labels = labels.cuda()\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                predicts.append(predicted)\n            eval_loader.iter.reset_indices()   # reset indices to move on\n            predicts = torch.stack(predicts, dim=1)\n            predicted = most_common(predicts)\n            total += labels.size(0)\n            correct += (predicted == labels).sum()\n</code></pre>", "body_text": "@SsnL I tried to print id(self) in _shutdown_workers, then I found my customized eval dataloader does not shutdown properly, so that at next training epoch the train dataloader will get the same object used in previous epoch for evaluation. The id printed right before \"Correct pred: 189\" is the id of eval dataloader, which does not shutdown.\n\nMy customized dataloader is as follow, the reason for this is I want to use current batch multiple times with different transforms at test time, so I made current indices an attribute, and in each iteration of eval loader, I manually reset the indices. For now, I can't figure out why this makes it not shutting down.\nclass CustomDataLoaderIter(DataLoaderIter):\n    def __init__(self, loader):\n        super(CustomDataLoaderIter, self).__init__(loader)\n        self.num_workers = 0\n        self.indices = None\n\n    def __next__(self):\n        if self.num_workers == 0:  # same-process loading\n            if self.indices is None:\n                self.indices = next(self.sample_iter)  # may raise StopIteration\n            batch = self.collate_fn([self.dataset[i] for i in self.indices])\n            if self.pin_memory:\n                batch = pin_memory_batch(batch)\n\n            return batch\n\n        # check if the next sample has already been generated\n        if self.rcvd_idx in self.reorder_dict:\n            batch = self.reorder_dict.pop(self.rcvd_idx)\n            return self._process_next_batch(batch)\n\n        if self.batches_outstanding == 0:\n            self._shutdown_workers()\n            raise StopIteration\n\n        while True:\n            assert (not self.shutdown and self.batches_outstanding > 0)\n            idx, batch = self.data_queue.get()\n            self.batches_outstanding -= 1\n            if idx != self.rcvd_idx:\n                # store out-of-order samples\n                self.reorder_dict[idx] = batch\n                continue\n            return self._process_next_batch(batch)\n\n    def cur_batch(self):\n        return self.__next__()\n\n    def reset_indices(self):\n        self.indices = None\n\n# the customized DataLoader\nclass CustomDataLoader(DataLoader):\n    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,\n                 batch_sampler=None, num_workers=0, collate_fn=default_collate,\n                 pin_memory=False, drop_last=False):\n        super(CustomDataLoader, self).__init__(dataset, batch_size, shuffle,\n                                               sampler, batch_sampler, num_workers,\n                                               collate_fn, pin_memory,\n                                               drop_last)\n\n    def __iter__(self):\n        self.iter = CustomDataLoaderIter(self)\n        return self.iter\n\n    def get_cur_batch(self):\n        return self.iter.cur_batch()[0]\n\nIn evaluation:\nfor inputs, labels in eval_loader:\n            ncorps = 5\n            predicts = []\n            # iterator = eval_loader.iter\n            for i in range(ncorps):\n                images = eval_loader.get_cur_batch()\n                images = Variable(images)\n                if torch.cuda.is_available():\n                    images = images.cuda()\n                    labels = labels.cuda()\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                predicts.append(predicted)\n            eval_loader.iter.reset_indices()   # reset indices to move on\n            predicts = torch.stack(predicts, dim=1)\n            predicted = most_common(predicts)\n            total += labels.size(0)\n            correct += (predicted == labels).sum()", "body": "@SsnL I tried to print id(self) in _shutdown_workers, then I found my customized eval dataloader does not shutdown properly, so that at next training epoch the train dataloader will get the same object used in previous epoch for evaluation. The id printed right before \"Correct pred: 189\" is the id of eval dataloader, which does not shutdown. \r\n![image](https://user-images.githubusercontent.com/17583521/36955693-ed64aaf8-2064-11e8-95d2-a4efe9853f4a.png)\r\n\r\nMy customized dataloader is as follow, the reason for this is I want to use current batch multiple times with different transforms at test time, so I made current indices an attribute, and in each iteration of eval loader, I manually reset the indices. For now, I can't figure out why this makes it not shutting down.\r\n```cpp\r\nclass CustomDataLoaderIter(DataLoaderIter):\r\n    def __init__(self, loader):\r\n        super(CustomDataLoaderIter, self).__init__(loader)\r\n        self.num_workers = 0\r\n        self.indices = None\r\n\r\n    def __next__(self):\r\n        if self.num_workers == 0:  # same-process loading\r\n            if self.indices is None:\r\n                self.indices = next(self.sample_iter)  # may raise StopIteration\r\n            batch = self.collate_fn([self.dataset[i] for i in self.indices])\r\n            if self.pin_memory:\r\n                batch = pin_memory_batch(batch)\r\n\r\n            return batch\r\n\r\n        # check if the next sample has already been generated\r\n        if self.rcvd_idx in self.reorder_dict:\r\n            batch = self.reorder_dict.pop(self.rcvd_idx)\r\n            return self._process_next_batch(batch)\r\n\r\n        if self.batches_outstanding == 0:\r\n            self._shutdown_workers()\r\n            raise StopIteration\r\n\r\n        while True:\r\n            assert (not self.shutdown and self.batches_outstanding > 0)\r\n            idx, batch = self.data_queue.get()\r\n            self.batches_outstanding -= 1\r\n            if idx != self.rcvd_idx:\r\n                # store out-of-order samples\r\n                self.reorder_dict[idx] = batch\r\n                continue\r\n            return self._process_next_batch(batch)\r\n\r\n    def cur_batch(self):\r\n        return self.__next__()\r\n\r\n    def reset_indices(self):\r\n        self.indices = None\r\n\r\n# the customized DataLoader\r\nclass CustomDataLoader(DataLoader):\r\n    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,\r\n                 batch_sampler=None, num_workers=0, collate_fn=default_collate,\r\n                 pin_memory=False, drop_last=False):\r\n        super(CustomDataLoader, self).__init__(dataset, batch_size, shuffle,\r\n                                               sampler, batch_sampler, num_workers,\r\n                                               collate_fn, pin_memory,\r\n                                               drop_last)\r\n\r\n    def __iter__(self):\r\n        self.iter = CustomDataLoaderIter(self)\r\n        return self.iter\r\n\r\n    def get_cur_batch(self):\r\n        return self.iter.cur_batch()[0]\r\n\r\n```\r\n\r\nIn evaluation:\r\n```\r\nfor inputs, labels in eval_loader:\r\n            ncorps = 5\r\n            predicts = []\r\n            # iterator = eval_loader.iter\r\n            for i in range(ncorps):\r\n                images = eval_loader.get_cur_batch()\r\n                images = Variable(images)\r\n                if torch.cuda.is_available():\r\n                    images = images.cuda()\r\n                    labels = labels.cuda()\r\n                outputs = model(images)\r\n                _, predicted = torch.max(outputs.data, 1)\r\n                predicts.append(predicted)\r\n            eval_loader.iter.reset_indices()   # reset indices to move on\r\n            predicts = torch.stack(predicts, dim=1)\r\n            predicted = most_common(predicts)\r\n            total += labels.size(0)\r\n            correct += (predicted == labels).sum()\r\n```\r\n"}