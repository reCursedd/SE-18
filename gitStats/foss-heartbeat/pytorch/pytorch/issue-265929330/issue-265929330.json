{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3142", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3142/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3142/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3142/events", "html_url": "https://github.com/pytorch/pytorch/issues/3142", "id": 265929330, "node_id": "MDU6SXNzdWUyNjU5MjkzMzA=", "number": 3142, "title": "Tracing for new ATen Variable C++ codepath", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-10-16T21:57:05Z", "updated_at": "2017-11-06T23:27:54Z", "closed_at": "2017-11-06T23:27:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here is what I plan to augment the generated code with:</p>\n<div class=\"highlight highlight-source-c++\"><pre>Tensor <span class=\"pl-en\">VariableType::transpose</span>(<span class=\"pl-k\">const</span> Tensor &amp; self, <span class=\"pl-c1\">int64_t</span> dim0, <span class=\"pl-c1\">int64_t</span> dim1) <span class=\"pl-k\">const</span> {\n    <span class=\"pl-k\">auto</span>&amp; self_ = <span class=\"pl-c1\">unpack</span>(self, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>self<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">0</span>);\n    <span class=\"pl-k\">auto</span> flags = <span class=\"pl-c1\">_flags</span>({ self });\n    <span class=\"pl-k\">auto</span> grad_fn = std::make_shared&lt;TransposeBackward&gt;();\n    <span class=\"pl-k\">if</span> (flags.<span class=\"pl-smi\">is_executable</span>) {\n      grad_fn-&gt;<span class=\"pl-smi\">dim0</span> = dim0;\n      grad_fn-&gt;<span class=\"pl-smi\">dim1</span> = dim1;\n    }\n    <span class=\"pl-k\">auto</span> ret = <span class=\"pl-c1\">as_variable</span>(baseType-&gt;<span class=\"pl-c1\">transpose</span>(self_, dim0, dim1));\n    <span class=\"pl-c1\">take_version_counter</span>(ret, self);\n    <span class=\"pl-c1\">wrap_output</span>(ret, <span class=\"pl-c1\">std::move</span>(flags), grad_fn);\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> --- BEGIN NEW CODE HERE ---</span>\n    <span class=\"pl-k\">if</span> (<span class=\"pl-c1\">jit::tracer::isTracing</span>({self})) {\n      jit::Node * n = <span class=\"pl-c1\">jit::tracer::recordTrace</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>transpose<span class=\"pl-pds\">\"</span></span>, {self}, {ret});\n      n-&gt;<span class=\"pl-c1\">i_</span>(<span class=\"pl-c1\">jit::stringToSymbol</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dim0<span class=\"pl-pds\">\"</span></span>), dim0);\n      n-&gt;<span class=\"pl-c1\">i_</span>(<span class=\"pl-c1\">jit::stringToSymbol</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dim1<span class=\"pl-pds\">\"</span></span>), dim1);\n    } \n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> --- END NEW CODE HERE ---</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">Tensor</span>(<span class=\"pl-c1\">std::move</span>(ret));\n}</pre></div>\n<p>For the JIT, we'll also have to code generate things that can translate back to ATen method invocations from this (I considered the design where we also record a lambda when we create a Node, but it's annoying to statically type this correctly, and it doesn't work if you have an optimization pass that creates fresh nodes.)</p>\n<hr>\n<p>Old proposals:</p>\n<div class=\"highlight highlight-source-c++\"><pre>Tensor <span class=\"pl-en\">VariableType::s_add</span>(<span class=\"pl-k\">const</span> Tensor &amp; self, <span class=\"pl-k\">const</span> Tensor &amp; other, Scalar alpha) <span class=\"pl-k\">const</span> {\n    <span class=\"pl-k\">auto</span>&amp; self_ = <span class=\"pl-c1\">checked_unpack</span>(self, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>self<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">0</span>);\n    <span class=\"pl-k\">auto</span>&amp; other_ = <span class=\"pl-c1\">checked_unpack</span>(other, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>other<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">1</span>);\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> ---- BEGIN ----</span>\n    <span class=\"pl-k\">auto</span> tracer = <span class=\"pl-c1\">tracer::maybeState</span>({ self, other });\n    Node* n;\n    <span class=\"pl-k\">if</span> (tracer) {\n      WithLock <span class=\"pl-smi\">l</span>(tracer);\n      n = tracer-&gt;<span class=\"pl-c1\">createNode</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>s_add<span class=\"pl-pds\">\"</span></span>);\n      n-&gt;<span class=\"pl-c1\">addInput</span>(n, tracer-&gt;<span class=\"pl-c1\">getNode</span>(self_));\n      n-&gt;<span class=\"pl-c1\">addInput</span>(n, tracer-&gt;<span class=\"pl-c1\">getNode</span>(other_));\n      n-&gt;<span class=\"pl-c1\">z_</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>alpha<span class=\"pl-pds\">\"</span></span>, alpha); <span class=\"pl-c\"><span class=\"pl-c\">//</span> TODO: Intern all of these symbols?</span>\n      tracer-&gt;<span class=\"pl-c1\">appendNode</span>(n);\n    }\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> ---- END ----</span>\n    <span class=\"pl-k\">auto</span> flags = <span class=\"pl-c1\">Function::flags</span>({ self, other });\n    <span class=\"pl-k\">auto</span> grad_fn = std::make_shared&lt;AddBackward1&gt;();\n    <span class=\"pl-k\">if</span> (flags.<span class=\"pl-smi\">is_executable</span>) {\n      grad_fn-&gt;<span class=\"pl-smi\">alpha</span> = alpha;\n    } \n    <span class=\"pl-k\">auto</span> output = <span class=\"pl-c1\">as_variable</span>(baseType-&gt;<span class=\"pl-c1\">s_add</span>(self_, other_, alpha));\n    <span class=\"pl-c1\">wrap_output</span>(*output.<span class=\"pl-c1\">get</span>(), <span class=\"pl-c1\">std::move</span>(flags), <span class=\"pl-c1\">std::move</span>(grad_fn));\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> ---- BEGIN ----</span>\n    <span class=\"pl-k\">if</span> (tracer) {\n      WithLock <span class=\"pl-smi\">l</span>(tracer);\n      tracer-&gt;<span class=\"pl-c1\">addOutput</span>(output);\n      <span class=\"pl-k\">if</span> (!passes_state_transparently) { <span class=\"pl-c\"><span class=\"pl-c\">//</span> filled in from cwrap</span>\n        <span class=\"pl-c1\">tracer::nontraceableBackwards</span>(n, {self, other}, {output});\n      }\n    }\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> ---- END ----</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">Tensor</span>(<span class=\"pl-c1\">std::move</span>(output));\n}     </pre></div>", "body_text": "Here is what I plan to augment the generated code with:\nTensor VariableType::transpose(const Tensor & self, int64_t dim0, int64_t dim1) const {\n    auto& self_ = unpack(self, \"self\", 0);\n    auto flags = _flags({ self });\n    auto grad_fn = std::make_shared<TransposeBackward>();\n    if (flags.is_executable) {\n      grad_fn->dim0 = dim0;\n      grad_fn->dim1 = dim1;\n    }\n    auto ret = as_variable(baseType->transpose(self_, dim0, dim1));\n    take_version_counter(ret, self);\n    wrap_output(ret, std::move(flags), grad_fn);\n    // --- BEGIN NEW CODE HERE ---\n    if (jit::tracer::isTracing({self})) {\n      jit::Node * n = jit::tracer::recordTrace(\"transpose\", {self}, {ret});\n      n->i_(jit::stringToSymbol(\"dim0\"), dim0);\n      n->i_(jit::stringToSymbol(\"dim1\"), dim1);\n    } \n    // --- END NEW CODE HERE ---\n    return Tensor(std::move(ret));\n}\nFor the JIT, we'll also have to code generate things that can translate back to ATen method invocations from this (I considered the design where we also record a lambda when we create a Node, but it's annoying to statically type this correctly, and it doesn't work if you have an optimization pass that creates fresh nodes.)\n\nOld proposals:\nTensor VariableType::s_add(const Tensor & self, const Tensor & other, Scalar alpha) const {\n    auto& self_ = checked_unpack(self, \"self\", 0);\n    auto& other_ = checked_unpack(other, \"other\", 1);\n    // ---- BEGIN ----\n    auto tracer = tracer::maybeState({ self, other });\n    Node* n;\n    if (tracer) {\n      WithLock l(tracer);\n      n = tracer->createNode(\"s_add\");\n      n->addInput(n, tracer->getNode(self_));\n      n->addInput(n, tracer->getNode(other_));\n      n->z_(\"alpha\", alpha); // TODO: Intern all of these symbols?\n      tracer->appendNode(n);\n    }\n    // ---- END ----\n    auto flags = Function::flags({ self, other });\n    auto grad_fn = std::make_shared<AddBackward1>();\n    if (flags.is_executable) {\n      grad_fn->alpha = alpha;\n    } \n    auto output = as_variable(baseType->s_add(self_, other_, alpha));\n    wrap_output(*output.get(), std::move(flags), std::move(grad_fn));\n    // ---- BEGIN ----\n    if (tracer) {\n      WithLock l(tracer);\n      tracer->addOutput(output);\n      if (!passes_state_transparently) { // filled in from cwrap\n        tracer::nontraceableBackwards(n, {self, other}, {output});\n      }\n    }\n    // ---- END ----\n    return Tensor(std::move(output));\n}", "body": "Here is what I plan to augment the generated code with:\r\n\r\n```cpp\r\nTensor VariableType::transpose(const Tensor & self, int64_t dim0, int64_t dim1) const {\r\n    auto& self_ = unpack(self, \"self\", 0);\r\n    auto flags = _flags({ self });\r\n    auto grad_fn = std::make_shared<TransposeBackward>();\r\n    if (flags.is_executable) {\r\n      grad_fn->dim0 = dim0;\r\n      grad_fn->dim1 = dim1;\r\n    }\r\n    auto ret = as_variable(baseType->transpose(self_, dim0, dim1));\r\n    take_version_counter(ret, self);\r\n    wrap_output(ret, std::move(flags), grad_fn);\r\n    // --- BEGIN NEW CODE HERE ---\r\n    if (jit::tracer::isTracing({self})) {\r\n      jit::Node * n = jit::tracer::recordTrace(\"transpose\", {self}, {ret});\r\n      n->i_(jit::stringToSymbol(\"dim0\"), dim0);\r\n      n->i_(jit::stringToSymbol(\"dim1\"), dim1);\r\n    } \r\n    // --- END NEW CODE HERE ---\r\n    return Tensor(std::move(ret));\r\n}\r\n```\r\n\r\nFor the JIT, we'll also have to code generate things that can translate back to ATen method invocations from this (I considered the design where we also record a lambda when we create a Node, but it's annoying to statically type this correctly, and it doesn't work if you have an optimization pass that creates fresh nodes.)\r\n\r\n----\r\n\r\nOld proposals:\r\n\r\n```cpp\r\nTensor VariableType::s_add(const Tensor & self, const Tensor & other, Scalar alpha) const {\r\n    auto& self_ = checked_unpack(self, \"self\", 0);\r\n    auto& other_ = checked_unpack(other, \"other\", 1);\r\n    // ---- BEGIN ----\r\n    auto tracer = tracer::maybeState({ self, other });\r\n    Node* n;\r\n    if (tracer) {\r\n      WithLock l(tracer);\r\n      n = tracer->createNode(\"s_add\");\r\n      n->addInput(n, tracer->getNode(self_));\r\n      n->addInput(n, tracer->getNode(other_));\r\n      n->z_(\"alpha\", alpha); // TODO: Intern all of these symbols?\r\n      tracer->appendNode(n);\r\n    }\r\n    // ---- END ----\r\n    auto flags = Function::flags({ self, other });\r\n    auto grad_fn = std::make_shared<AddBackward1>();\r\n    if (flags.is_executable) {\r\n      grad_fn->alpha = alpha;\r\n    } \r\n    auto output = as_variable(baseType->s_add(self_, other_, alpha));\r\n    wrap_output(*output.get(), std::move(flags), std::move(grad_fn));\r\n    // ---- BEGIN ----\r\n    if (tracer) {\r\n      WithLock l(tracer);\r\n      tracer->addOutput(output);\r\n      if (!passes_state_transparently) { // filled in from cwrap\r\n        tracer::nontraceableBackwards(n, {self, other}, {output});\r\n      }\r\n    }\r\n    // ---- END ----\r\n    return Tensor(std::move(output));\r\n}     \r\n```"}