{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11829", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11829/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11829/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11829/events", "html_url": "https://github.com/pytorch/pytorch/issues/11829", "id": 361491935, "node_id": "MDU6SXNzdWUzNjE0OTE5MzU=", "number": 11829, "title": "[JIT] Shape inference broken for torch.arange under JIT tracer", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-18T21:30:24Z", "updated_at": "2018-09-19T16:29:18Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>It seems like <code>torch.arange</code> fails to generalize to other tensor shapes when the <code>end</code> argument provided is dependent on the input size. cc. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>. Since this is being used by a few distribution methods, they may provide incorrect results when JITed.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre>The following fails to generalize to other tensor sizes:\n\nIn [<span class=\"pl-c1\">16</span>]: <span class=\"pl-k\">def</span> <span class=\"pl-en\">fn</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-c1\">...</span>:     <span class=\"pl-k\">return</span> torch.arange(x.shape[<span class=\"pl-c1\">0</span>])\n    <span class=\"pl-c1\">...</span>:\n    <span class=\"pl-c1\">...</span>:\n\nIn [<span class=\"pl-c1\">17</span>]: compiled <span class=\"pl-k\">=</span> torch.jit.trace(fn, torch.ones(<span class=\"pl-c1\">3</span>))\n\nIn [<span class=\"pl-c1\">18</span>]: compiled(torch.ones(<span class=\"pl-c1\">5</span>))\nOut[<span class=\"pl-c1\">18</span>]: tensor([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> wrong result</span></pre></div>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=648532\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fritzo\">@fritzo</a> suggested the following work-around in the meantime:</p>\n<div class=\"highlight highlight-source-python\"><pre>In [<span class=\"pl-c1\">22</span>]: <span class=\"pl-k\">def</span> <span class=\"pl-en\">fn_alt</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-c1\">...</span>:     <span class=\"pl-k\">return</span> torch.cumsum(torch.ones(x.shape[<span class=\"pl-c1\">0</span>]), <span class=\"pl-c1\">0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.long) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-c1\">...</span>:\n    <span class=\"pl-c1\">...</span>:\nIn [<span class=\"pl-c1\">24</span>]: compiled <span class=\"pl-k\">=</span> torch.jit.trace(fn_alt, torch.ones(<span class=\"pl-c1\">3</span>))\n\nIn [<span class=\"pl-c1\">25</span>]: compiled(torch.ones(<span class=\"pl-c1\">5</span>))\nOut[<span class=\"pl-c1\">25</span>]: tensor([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])</pre></div>\n<h2>System Info</h2>\n<pre><code>Collecting environment information...\nPyTorch version: 0.5.0a0+6660a12\nIs debug build: Yes\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.13.3\nGCC version: Could not collect\nCMake version: version 3.12.0\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] numpy (1.15.0)\n[pip] torch (0.5.0a0+6c3792b, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\n[pip] torchfile (0.1.0)\n[pip] torchvision (0.2.1)\n[conda] torch                     0.5.0a0+2431eac           &lt;pip&gt;\n[conda] torch                     0.5.0a0+6c3792b           &lt;pip&gt;\n[conda] torch                     0.5.0a0+6660a12           &lt;pip&gt;\n[conda] torch                     0.5.0a0+35d52db           &lt;pip&gt;\n[conda] torchfile                 0.1.0                     &lt;pip&gt;\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n</code></pre>", "body_text": "Issue description\nIt seems like torch.arange fails to generalize to other tensor shapes when the end argument provided is dependent on the input size. cc. @apaszke, @zou3519, @soumith. Since this is being used by a few distribution methods, they may provide incorrect results when JITed.\nCode example\nThe following fails to generalize to other tensor sizes:\n\nIn [16]: def fn(x):\n    ...:     return torch.arange(x.shape[0])\n    ...:\n    ...:\n\nIn [17]: compiled = torch.jit.trace(fn, torch.ones(3))\n\nIn [18]: compiled(torch.ones(5))\nOut[18]: tensor([0, 1, 2])  # wrong result\n@fritzo suggested the following work-around in the meantime:\nIn [22]: def fn_alt(x):\n    ...:     return torch.cumsum(torch.ones(x.shape[0]), 0, dtype=torch.long) - 1\n    ...:\n    ...:\nIn [24]: compiled = torch.jit.trace(fn_alt, torch.ones(3))\n\nIn [25]: compiled(torch.ones(5))\nOut[25]: tensor([0, 1, 2, 3, 4])\nSystem Info\nCollecting environment information...\nPyTorch version: 0.5.0a0+6660a12\nIs debug build: Yes\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.13.3\nGCC version: Could not collect\nCMake version: version 3.12.0\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] numpy (1.15.0)\n[pip] torch (0.5.0a0+6c3792b, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\n[pip] torchfile (0.1.0)\n[pip] torchvision (0.2.1)\n[conda] torch                     0.5.0a0+2431eac           <pip>\n[conda] torch                     0.5.0a0+6c3792b           <pip>\n[conda] torch                     0.5.0a0+6660a12           <pip>\n[conda] torch                     0.5.0a0+35d52db           <pip>\n[conda] torchfile                 0.1.0                     <pip>\n[conda] torchvision               0.2.1                     <pip>", "body": "## Issue description\r\n\r\nIt seems like `torch.arange` fails to generalize to other tensor shapes when the `end` argument provided is dependent on the input size. cc. @apaszke, @zou3519, @soumith. Since this is being used by a few distribution methods, they may provide incorrect results when JITed.\r\n\r\n## Code example\r\n\r\n```python\r\nThe following fails to generalize to other tensor sizes:\r\n\r\nIn [16]: def fn(x):\r\n    ...:     return torch.arange(x.shape[0])\r\n    ...:\r\n    ...:\r\n\r\nIn [17]: compiled = torch.jit.trace(fn, torch.ones(3))\r\n\r\nIn [18]: compiled(torch.ones(5))\r\nOut[18]: tensor([0, 1, 2])  # wrong result\r\n```\r\n\r\n@fritzo suggested the following work-around in the meantime:\r\n```python\r\nIn [22]: def fn_alt(x):\r\n    ...:     return torch.cumsum(torch.ones(x.shape[0]), 0, dtype=torch.long) - 1\r\n    ...:\r\n    ...:\r\nIn [24]: compiled = torch.jit.trace(fn_alt, torch.ones(3))\r\n\r\nIn [25]: compiled(torch.ones(5))\r\nOut[25]: tensor([0, 1, 2, 3, 4])\r\n```\r\n## System Info\r\n\r\n```\r\nCollecting environment information...\r\nPyTorch version: 0.5.0a0+6660a12\r\nIs debug build: Yes\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.12.0\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.0)\r\n[pip] torch (0.5.0a0+6c3792b, /Users/npradhan/miniconda2/envs/pytorch-master/lib/python3.6/site-packages)\r\n[pip] torchfile (0.1.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] torch                     0.5.0a0+2431eac           <pip>\r\n[conda] torch                     0.5.0a0+6c3792b           <pip>\r\n[conda] torch                     0.5.0a0+6660a12           <pip>\r\n[conda] torch                     0.5.0a0+35d52db           <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```\r\n"}