{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/339623177", "html_url": "https://github.com/pytorch/pytorch/issues/3303#issuecomment-339623177", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3303", "id": 339623177, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTYyMzE3Nw==", "user": {"login": "albanD", "id": 6359743, "node_id": "MDQ6VXNlcjYzNTk3NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6359743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/albanD", "html_url": "https://github.com/albanD", "followers_url": "https://api.github.com/users/albanD/followers", "following_url": "https://api.github.com/users/albanD/following{/other_user}", "gists_url": "https://api.github.com/users/albanD/gists{/gist_id}", "starred_url": "https://api.github.com/users/albanD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/albanD/subscriptions", "organizations_url": "https://api.github.com/users/albanD/orgs", "repos_url": "https://api.github.com/users/albanD/repos", "events_url": "https://api.github.com/users/albanD/events{/privacy}", "received_events_url": "https://api.github.com/users/albanD/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-26T10:30:24Z", "updated_at": "2017-10-26T10:30:24Z", "author_association": "COLLABORATOR", "body_html": "<p>Hi,</p>\n<p>As a temporary fix, you can use <code>(A.mv(x) * 1).sum().backward()</code>.</p>\n<p>The problem is the behavior between the <code>sum</code> backward and the <code>mv</code> backward.<br>\nThe sum returns a <code>0</code> strided gradient which is not supported by MKL/BLAS implementations of <code>ger</code> (used in the <code>mv</code> backward).<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> not sure where this was introduced and how to fix this properly. Should the blas wrapper clone the input if it is <code>0</code> strided? Or should it just raise an error (meaning that we need to add some <code>clone</code>s in some places in the code like the sum backward)?</p>", "body_text": "Hi,\nAs a temporary fix, you can use (A.mv(x) * 1).sum().backward().\nThe problem is the behavior between the sum backward and the mv backward.\nThe sum returns a 0 strided gradient which is not supported by MKL/BLAS implementations of ger (used in the mv backward).\n@apaszke not sure where this was introduced and how to fix this properly. Should the blas wrapper clone the input if it is 0 strided? Or should it just raise an error (meaning that we need to add some clones in some places in the code like the sum backward)?", "body": "Hi,\r\n\r\nAs a temporary fix, you can use `(A.mv(x) * 1).sum().backward()`.\r\n\r\nThe problem is the behavior between the `sum` backward and the `mv` backward.\r\nThe sum returns a `0` strided gradient which is not supported by MKL/BLAS implementations of `ger` (used in the `mv` backward).\r\n@apaszke not sure where this was introduced and how to fix this properly. Should the blas wrapper clone the input if it is `0` strided? Or should it just raise an error (meaning that we need to add some `clone`s in some places in the code like the sum backward)?\r\n"}