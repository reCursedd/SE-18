{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8608", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8608/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8608/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8608/events", "html_url": "https://github.com/pytorch/pytorch/pull/8608", "id": 333358245, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk1NTkwMjU3", "number": 8608, "title": "Correct flop computation of FCGradient", "user": {"login": "jspark1105", "id": 5545022, "node_id": "MDQ6VXNlcjU1NDUwMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5545022?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jspark1105", "html_url": "https://github.com/jspark1105", "followers_url": "https://api.github.com/users/jspark1105/followers", "following_url": "https://api.github.com/users/jspark1105/following{/other_user}", "gists_url": "https://api.github.com/users/jspark1105/gists{/gist_id}", "starred_url": "https://api.github.com/users/jspark1105/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jspark1105/subscriptions", "organizations_url": "https://api.github.com/users/jspark1105/orgs", "repos_url": "https://api.github.com/users/jspark1105/repos", "events_url": "https://api.github.com/users/jspark1105/events{/privacy}", "received_events_url": "https://api.github.com/users/jspark1105/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-18T17:06:24Z", "updated_at": "2018-06-18T21:06:29Z", "closed_at": "2018-06-18T21:06:03Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8608", "html_url": "https://github.com/pytorch/pytorch/pull/8608", "diff_url": "https://github.com/pytorch/pytorch/pull/8608.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8608.patch"}, "body_html": "<p>When out.size() == 3, FLOPs for computing dX is 2 * M * N * K not M * N * K because we do multiplications and additions.<br>\nTake out 2 in front of M * N for flops for dB computation to be consistent with FC. Algorithmically, bias term is just addition, and we happen to implement with an outer-product with an all-one vector using a GEMM for convenience.<br>\nCC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20307328\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/salexspb\">@salexspb</a></p>", "body_text": "When out.size() == 3, FLOPs for computing dX is 2 * M * N * K not M * N * K because we do multiplications and additions.\nTake out 2 in front of M * N for flops for dB computation to be consistent with FC. Algorithmically, bias term is just addition, and we happen to implement with an outer-product with an all-one vector using a GEMM for convenience.\nCC @salexspb", "body": "When out.size() == 3, FLOPs for computing dX is 2 * M * N * K not M * N * K because we do multiplications and additions.\r\nTake out 2 in front of M * N for flops for dB computation to be consistent with FC. Algorithmically, bias term is just addition, and we happen to implement with an outer-product with an all-one vector using a GEMM for convenience.\r\nCC @salexspb \r\n\r\n"}