{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5455", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5455/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5455/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5455/events", "html_url": "https://github.com/pytorch/pytorch/issues/5455", "id": 300943441, "node_id": "MDU6SXNzdWUzMDA5NDM0NDE=", "number": 5455, "title": "Segfault in _int_free () from /lib64/libc.so.6 when returning", "user": {"login": "gturri", "id": 308601, "node_id": "MDQ6VXNlcjMwODYwMQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/308601?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gturri", "html_url": "https://github.com/gturri", "followers_url": "https://api.github.com/users/gturri/followers", "following_url": "https://api.github.com/users/gturri/following{/other_user}", "gists_url": "https://api.github.com/users/gturri/gists{/gist_id}", "starred_url": "https://api.github.com/users/gturri/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gturri/subscriptions", "organizations_url": "https://api.github.com/users/gturri/orgs", "repos_url": "https://api.github.com/users/gturri/repos", "events_url": "https://api.github.com/users/gturri/events{/privacy}", "received_events_url": "https://api.github.com/users/gturri/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-28T09:08:55Z", "updated_at": "2018-07-28T14:14:00Z", "closed_at": "2018-07-28T14:14:00Z", "author_association": "NONE", "body_html": "<p>OS: CentOS Linux release 7.3.1611 (also reproduced on Ubuntu 16.04)<br>\nPyTorch version: 0.3.1<br>\nHow you installed PyTorch (conda, pip, source): conda<br>\nPython version: Python 3.6.4 :: Anaconda, Inc.<br>\nCUDA/cuDNN version: I'm not using Cuda<br>\nGPU models and configuration: I'm not using GPU</p>\n<p>Hi,<br>\nI am trying to implement a seq2seq model, and when I run it, it consistently leads me to this segfault:</p>\n<pre><code>Program received signal SIGSEGV, Segmentation fault.\n0x00007ffff787be1f in _int_free () from /lib64/libc.so.6\nMissing separate debuginfos, use: debuginfo-install glibc-2.17-157.el7_3.1.x86_64 libX11-1.6.3-3.el7.x86_64 libXau-1.0.8-2.1.el7.x86_64 libXext-1.3.3-3.el7.x86_64\n(gdb) bt\n#0  0x00007ffff787be1f in _int_free () from /lib64/libc.so.6\n#1  0x00007fffe75aad13 in deallocate (this=&lt;optimized out&gt;, __p=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/ext/new_allocator.h:110\n#2  _M_deallocate (this=&lt;optimized out&gt;, __n=&lt;optimized out&gt;, __p=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:174\n#3  ~_Vector_base (this=0x55567c8e2428, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:160\n#4  ~vector (this=0x55567c8e2428, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:416\n#5  torch::autograd::generated::ExpandBackward::~ExpandBackward (this=0x55567c8e23a8, __in_chrg=&lt;optimized out&gt;) at torch/csrc/autograd/generated/Functions.h:396\n#6  0x00007fffe74de768 in _M_release (this=0x55567c8e2390) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:144\n#7  ~__shared_count (this=0x5556800f2c98, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:546\n#8  ~__shared_ptr (this=0x5556800f2c90, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:781\n#9  ~shared_ptr (this=0x5556800f2c90, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr.h:93\n#10 ~pair (this=0x5556800f2c90, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_pair.h:96\n#11 _Destroy&lt;std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt; &gt; (__pointer=0x5556800f2c90) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:93\n#12 __destroy&lt;std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt;*&gt; (__last=&lt;optimized out&gt;, __first=0x5556800f2c90)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:103\n#13 _Destroy&lt;std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt;*&gt; (__last=&lt;optimized out&gt;, __first=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:126\n#14 _Destroy&lt;std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt;*, std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt; &gt; (__last=0x5556800f2cd8, __first=0x5556800f2c90)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:151\n#15 ~vector (this=0x5556800f2b08, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:415\n#16 torch::autograd::Function::~Function (this=0x5556800f2ae8, __in_chrg=&lt;optimized out&gt;) at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/function.h:77\n#17 0x00007fffe74de768 in _M_release (this=0x5556800f2ad0) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:144\n\n[... the same 10 frames are repeated over and over, with just the memory adresses of \"this\" changing...]\n\n#1439786 0x00007fffe74de768 in _M_release (this=0x55555e581fd0) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:144\n#1439787 ~__shared_count (this=0x555635d01c30, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:546\n#1439788 ~__shared_ptr (this=0x555635d01c28, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:781\n#1439789 ~shared_ptr (this=0x555635d01c28, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr.h:93\n#1439790 ~pair (this=0x555635d01c28, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_pair.h:96\n#1439791 _Destroy&lt;std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt; &gt; (__pointer=0x555635d01c28) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:93\n#1439792 __destroy&lt;std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt;*&gt; (__last=&lt;optimized out&gt;, __first=0x555635d01c28)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:103\n#1439793 _Destroy&lt;std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt;*&gt; (__last=&lt;optimized out&gt;, __first=&lt;optimized out&gt;)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:126\n#1439794 _Destroy&lt;std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt;*, std::pair&lt;std::shared_ptr&lt;torch::autograd::Function&gt;, int&gt; &gt; (__last=0x555635d01c40, \n    __first=0x555635d01c10) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:151\n#1439795 ~vector (this=0x7fff578ceab8, __in_chrg=&lt;optimized out&gt;) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:415\n#1439796 torch::autograd::Function::~Function (this=0x7fff578cea98, __in_chrg=&lt;optimized out&gt;) at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/function.h:77\n#1439797 0x00007fffe74f3086 in ~PyFunction (this=0x7fff578cea98, __in_chrg=&lt;optimized out&gt;)\n    at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/python_function.h:31\n#1439798 THPFunction_dealloc (self=0x7fff578ce9e8) at torch/csrc/autograd/python_function.cpp:283\n#1439799 0x00005555556f1ba6 in subtype_dealloc ()\n#1439800 0x0000555555642700 in _PyTrash_thread_destroy_chain ()\n#1439801 0x00005555556eac3e in fast_function ()\n#1439802 0x00005555556f0cc5 in call_function ()\n#1439803 0x000055555571519a in _PyEval_EvalFrameDefault ()\n#1439804 0x00005555556e9dfe in _PyEval_EvalCodeWithName ()\n#1439805 0x00005555556eaa11 in fast_function ()\n#1439806 0x00005555556f0cc5 in call_function ()\n#1439807 0x0000555555715eb1 in _PyEval_EvalFrameDefault ()\n#1439808 0x0000555555692b6b in _PyFunction_FastCall ()\n#1439809 0x00005555556f0cc5 in call_function ()\n#1439810 0x000055555571519a in _PyEval_EvalFrameDefault ()\n#1439811 0x00005555556eb529 in PyEval_EvalCodeEx ()\n#1439812 0x00005555556ec2cc in PyEval_EvalCode ()\n#1439813 0x0000555555768af4 in run_mod ()\n#1439814 0x0000555555768ef1 in PyRun_FileExFlags ()\n#1439815 0x00005555557690f4 in PyRun_SimpleFileExFlags ()\n#1439816 0x000055555576cc28 in Py_Main ()\n#1439817 0x000055555563471e in main ()\n</code></pre>\n<p>This can be reproduced consistently with the following code: it fails at the end of the 7th iteration. (I'm not able to trim this code down any further. Sorry about that). It needs data from the file downloadable at <a href=\"https://criteois-my.sharepoint.com/:u:/g/personal/g_turri_criteo_com/EXw2X66h3NhNlKBYi8n-7JMB2dj276VuXqi_J6ZkyWcb7A?e=dnqegF\" rel=\"nofollow\">https://criteois-my.sharepoint.com/:u:/g/personal/g_turri_criteo_com/EXw2X66h3NhNlKBYi8n-7JMB2dj276VuXqi_J6ZkyWcb7A?e=dnqegF</a></p>\n<pre><code>import io\nimport random\nimport pickle\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as f\nfrom torch import optim\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=1)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n\n    def forward(self, input, input_lengths, hidden):\n        embedded = self.embedding(input)\n        pack = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)\n        output, hiddens = self.gru(pack)\n        return hiddens\n\n    def init_hidden(self):\n        return Variable(torch.zeros(1, self.hidden_size))\n\n\nclass Decoder(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=1)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax()\n\n    def forward(self, input, hidden):\n        output = self.embedding(input)\n        output = f.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output))\n        return output, hidden\n\n    def init_hidden(self):\n        result = Variable(torch.zeros(1, 1, self.hidden_size))\n        return result.cuda() if self.use_cuda else result\n\ndef train(input_variable, input_lengths, target_variable, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n    batch_size = len(input_variable)\n    encoder_hidden = encoder.init_hidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = max(input_lengths)\n    target_length = max(target_lengths)\n\n\n    hidden = encoder(input_variable, input_lengths, encoder_hidden)\n\n    decoder_input = Variable(torch.LongTensor([[0] for i in range(batch_size)]))\n\n    loss = 0\n    for di in range(max(target_lengths)):\n        decoder_output, decoder_hidden = decoder(decoder_input, hidden)\n        loss += criterion(decoder_output.view(batch_size, -1), target_variable[:,di])\n        decoder_input = target_variable[:,di].contiguous().view(-1, 1)\n\n    loss.backward()\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    ret = loss.data[0] / target_length\n    print(\"returning from train: \" + str(ret));\n    return ret\n\n\ndef main():\n    random.seed(4385)\n    torch.random.manual_seed(4385)\n    # This weird line ensures the random generator state is the expected one\n    dummy = [random.choice([0] * 7353) for i in range(19968)]\n\n    encoder = Encoder(input_size=73, hidden_size=256)\n    decoder = Decoder(hidden_size=256, output_size=73)\n\n    # This data file is downloadable from https://criteois-my.sharepoint.com/:u:/g/personal/g_turri_criteo_com/EXw2X66h3NhNlKBYi8n-7JMB2dj276VuXqi_J6ZkyWcb7A?e=cqBgya\n    with open(\"training_data.small.pickle\", \"rb\") as f:\n        training_pairs = pickle.load(f)\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n    criterion = nn.NLLLoss()\n\n    for iter in range(1, len(training_pairs)):\n        print(\"Starting iteration \" + str(iter))\n        training_pair = training_pairs[iter]\n\n        input_variable, input_lengths = training_pair[0]\n        target_variable, target_lengths = training_pair[1]\n\n        loss = train(input_variable, input_lengths, target_variable, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print(\"loss: \" + str(loss))\n        print(\"Returned in main loop at the end of iteration \" + str(iter))\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n<p>Thanks to the <code>print</code> statement we see that it fails at the end of the <code>train</code> function. ie: it prints the log just before the <code>return</code> but it doesn't print the one just after returning. It seems that upon getting out of this function it releases memory, which ends up on the segfault. However, I'm not able to understand how it should be fixed.</p>\n<p>(It turns out it's quite long to run until the segfault (several hours). I can probably try to serialize the states at the beginning of the last iteration to save time. I didn't do it because I'm afraid it might lead to a harder to understand snippet. Please let me know if you'd rather I try to do it nevertheless).</p>", "body_text": "OS: CentOS Linux release 7.3.1611 (also reproduced on Ubuntu 16.04)\nPyTorch version: 0.3.1\nHow you installed PyTorch (conda, pip, source): conda\nPython version: Python 3.6.4 :: Anaconda, Inc.\nCUDA/cuDNN version: I'm not using Cuda\nGPU models and configuration: I'm not using GPU\nHi,\nI am trying to implement a seq2seq model, and when I run it, it consistently leads me to this segfault:\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007ffff787be1f in _int_free () from /lib64/libc.so.6\nMissing separate debuginfos, use: debuginfo-install glibc-2.17-157.el7_3.1.x86_64 libX11-1.6.3-3.el7.x86_64 libXau-1.0.8-2.1.el7.x86_64 libXext-1.3.3-3.el7.x86_64\n(gdb) bt\n#0  0x00007ffff787be1f in _int_free () from /lib64/libc.so.6\n#1  0x00007fffe75aad13 in deallocate (this=<optimized out>, __p=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/ext/new_allocator.h:110\n#2  _M_deallocate (this=<optimized out>, __n=<optimized out>, __p=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:174\n#3  ~_Vector_base (this=0x55567c8e2428, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:160\n#4  ~vector (this=0x55567c8e2428, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:416\n#5  torch::autograd::generated::ExpandBackward::~ExpandBackward (this=0x55567c8e23a8, __in_chrg=<optimized out>) at torch/csrc/autograd/generated/Functions.h:396\n#6  0x00007fffe74de768 in _M_release (this=0x55567c8e2390) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:144\n#7  ~__shared_count (this=0x5556800f2c98, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:546\n#8  ~__shared_ptr (this=0x5556800f2c90, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:781\n#9  ~shared_ptr (this=0x5556800f2c90, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr.h:93\n#10 ~pair (this=0x5556800f2c90, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_pair.h:96\n#11 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int> > (__pointer=0x5556800f2c90) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:93\n#12 __destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*> (__last=<optimized out>, __first=0x5556800f2c90)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:103\n#13 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*> (__last=<optimized out>, __first=<optimized out>)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:126\n#14 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*, std::pair<std::shared_ptr<torch::autograd::Function>, int> > (__last=0x5556800f2cd8, __first=0x5556800f2c90)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:151\n#15 ~vector (this=0x5556800f2b08, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:415\n#16 torch::autograd::Function::~Function (this=0x5556800f2ae8, __in_chrg=<optimized out>) at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/function.h:77\n#17 0x00007fffe74de768 in _M_release (this=0x5556800f2ad0) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:144\n\n[... the same 10 frames are repeated over and over, with just the memory adresses of \"this\" changing...]\n\n#1439786 0x00007fffe74de768 in _M_release (this=0x55555e581fd0) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:144\n#1439787 ~__shared_count (this=0x555635d01c30, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:546\n#1439788 ~__shared_ptr (this=0x555635d01c28, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:781\n#1439789 ~shared_ptr (this=0x555635d01c28, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr.h:93\n#1439790 ~pair (this=0x555635d01c28, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_pair.h:96\n#1439791 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int> > (__pointer=0x555635d01c28) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:93\n#1439792 __destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*> (__last=<optimized out>, __first=0x555635d01c28)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:103\n#1439793 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*> (__last=<optimized out>, __first=<optimized out>)\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:126\n#1439794 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*, std::pair<std::shared_ptr<torch::autograd::Function>, int> > (__last=0x555635d01c40, \n    __first=0x555635d01c10) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:151\n#1439795 ~vector (this=0x7fff578ceab8, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:415\n#1439796 torch::autograd::Function::~Function (this=0x7fff578cea98, __in_chrg=<optimized out>) at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/function.h:77\n#1439797 0x00007fffe74f3086 in ~PyFunction (this=0x7fff578cea98, __in_chrg=<optimized out>)\n    at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/python_function.h:31\n#1439798 THPFunction_dealloc (self=0x7fff578ce9e8) at torch/csrc/autograd/python_function.cpp:283\n#1439799 0x00005555556f1ba6 in subtype_dealloc ()\n#1439800 0x0000555555642700 in _PyTrash_thread_destroy_chain ()\n#1439801 0x00005555556eac3e in fast_function ()\n#1439802 0x00005555556f0cc5 in call_function ()\n#1439803 0x000055555571519a in _PyEval_EvalFrameDefault ()\n#1439804 0x00005555556e9dfe in _PyEval_EvalCodeWithName ()\n#1439805 0x00005555556eaa11 in fast_function ()\n#1439806 0x00005555556f0cc5 in call_function ()\n#1439807 0x0000555555715eb1 in _PyEval_EvalFrameDefault ()\n#1439808 0x0000555555692b6b in _PyFunction_FastCall ()\n#1439809 0x00005555556f0cc5 in call_function ()\n#1439810 0x000055555571519a in _PyEval_EvalFrameDefault ()\n#1439811 0x00005555556eb529 in PyEval_EvalCodeEx ()\n#1439812 0x00005555556ec2cc in PyEval_EvalCode ()\n#1439813 0x0000555555768af4 in run_mod ()\n#1439814 0x0000555555768ef1 in PyRun_FileExFlags ()\n#1439815 0x00005555557690f4 in PyRun_SimpleFileExFlags ()\n#1439816 0x000055555576cc28 in Py_Main ()\n#1439817 0x000055555563471e in main ()\n\nThis can be reproduced consistently with the following code: it fails at the end of the 7th iteration. (I'm not able to trim this code down any further. Sorry about that). It needs data from the file downloadable at https://criteois-my.sharepoint.com/:u:/g/personal/g_turri_criteo_com/EXw2X66h3NhNlKBYi8n-7JMB2dj276VuXqi_J6ZkyWcb7A?e=dnqegF\nimport io\nimport random\nimport pickle\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as f\nfrom torch import optim\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(Encoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=1)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n\n    def forward(self, input, input_lengths, hidden):\n        embedded = self.embedding(input)\n        pack = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)\n        output, hiddens = self.gru(pack)\n        return hiddens\n\n    def init_hidden(self):\n        return Variable(torch.zeros(1, self.hidden_size))\n\n\nclass Decoder(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(Decoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=1)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax()\n\n    def forward(self, input, hidden):\n        output = self.embedding(input)\n        output = f.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output))\n        return output, hidden\n\n    def init_hidden(self):\n        result = Variable(torch.zeros(1, 1, self.hidden_size))\n        return result.cuda() if self.use_cuda else result\n\ndef train(input_variable, input_lengths, target_variable, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n    batch_size = len(input_variable)\n    encoder_hidden = encoder.init_hidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = max(input_lengths)\n    target_length = max(target_lengths)\n\n\n    hidden = encoder(input_variable, input_lengths, encoder_hidden)\n\n    decoder_input = Variable(torch.LongTensor([[0] for i in range(batch_size)]))\n\n    loss = 0\n    for di in range(max(target_lengths)):\n        decoder_output, decoder_hidden = decoder(decoder_input, hidden)\n        loss += criterion(decoder_output.view(batch_size, -1), target_variable[:,di])\n        decoder_input = target_variable[:,di].contiguous().view(-1, 1)\n\n    loss.backward()\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    ret = loss.data[0] / target_length\n    print(\"returning from train: \" + str(ret));\n    return ret\n\n\ndef main():\n    random.seed(4385)\n    torch.random.manual_seed(4385)\n    # This weird line ensures the random generator state is the expected one\n    dummy = [random.choice([0] * 7353) for i in range(19968)]\n\n    encoder = Encoder(input_size=73, hidden_size=256)\n    decoder = Decoder(hidden_size=256, output_size=73)\n\n    # This data file is downloadable from https://criteois-my.sharepoint.com/:u:/g/personal/g_turri_criteo_com/EXw2X66h3NhNlKBYi8n-7JMB2dj276VuXqi_J6ZkyWcb7A?e=cqBgya\n    with open(\"training_data.small.pickle\", \"rb\") as f:\n        training_pairs = pickle.load(f)\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\n    criterion = nn.NLLLoss()\n\n    for iter in range(1, len(training_pairs)):\n        print(\"Starting iteration \" + str(iter))\n        training_pair = training_pairs[iter]\n\n        input_variable, input_lengths = training_pair[0]\n        target_variable, target_lengths = training_pair[1]\n\n        loss = train(input_variable, input_lengths, target_variable, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print(\"loss: \" + str(loss))\n        print(\"Returned in main loop at the end of iteration \" + str(iter))\n\n\nif __name__ == \"__main__\":\n    main()\n\nThanks to the print statement we see that it fails at the end of the train function. ie: it prints the log just before the return but it doesn't print the one just after returning. It seems that upon getting out of this function it releases memory, which ends up on the segfault. However, I'm not able to understand how it should be fixed.\n(It turns out it's quite long to run until the segfault (several hours). I can probably try to serialize the states at the beginning of the last iteration to save time. I didn't do it because I'm afraid it might lead to a harder to understand snippet. Please let me know if you'd rather I try to do it nevertheless).", "body": "OS: CentOS Linux release 7.3.1611 (also reproduced on Ubuntu 16.04)\r\nPyTorch version: 0.3.1\r\nHow you installed PyTorch (conda, pip, source): conda\r\nPython version: Python 3.6.4 :: Anaconda, Inc.\r\nCUDA/cuDNN version: I'm not using Cuda\r\nGPU models and configuration: I'm not using GPU\r\n\r\nHi,\r\nI am trying to implement a seq2seq model, and when I run it, it consistently leads me to this segfault:\r\n\r\n```\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00007ffff787be1f in _int_free () from /lib64/libc.so.6\r\nMissing separate debuginfos, use: debuginfo-install glibc-2.17-157.el7_3.1.x86_64 libX11-1.6.3-3.el7.x86_64 libXau-1.0.8-2.1.el7.x86_64 libXext-1.3.3-3.el7.x86_64\r\n(gdb) bt\r\n#0  0x00007ffff787be1f in _int_free () from /lib64/libc.so.6\r\n#1  0x00007fffe75aad13 in deallocate (this=<optimized out>, __p=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/ext/new_allocator.h:110\r\n#2  _M_deallocate (this=<optimized out>, __n=<optimized out>, __p=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:174\r\n#3  ~_Vector_base (this=0x55567c8e2428, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:160\r\n#4  ~vector (this=0x55567c8e2428, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:416\r\n#5  torch::autograd::generated::ExpandBackward::~ExpandBackward (this=0x55567c8e23a8, __in_chrg=<optimized out>) at torch/csrc/autograd/generated/Functions.h:396\r\n#6  0x00007fffe74de768 in _M_release (this=0x55567c8e2390) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:144\r\n#7  ~__shared_count (this=0x5556800f2c98, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:546\r\n#8  ~__shared_ptr (this=0x5556800f2c90, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:781\r\n#9  ~shared_ptr (this=0x5556800f2c90, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr.h:93\r\n#10 ~pair (this=0x5556800f2c90, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_pair.h:96\r\n#11 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int> > (__pointer=0x5556800f2c90) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:93\r\n#12 __destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*> (__last=<optimized out>, __first=0x5556800f2c90)\r\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:103\r\n#13 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*> (__last=<optimized out>, __first=<optimized out>)\r\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:126\r\n#14 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*, std::pair<std::shared_ptr<torch::autograd::Function>, int> > (__last=0x5556800f2cd8, __first=0x5556800f2c90)\r\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:151\r\n#15 ~vector (this=0x5556800f2b08, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:415\r\n#16 torch::autograd::Function::~Function (this=0x5556800f2ae8, __in_chrg=<optimized out>) at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/function.h:77\r\n#17 0x00007fffe74de768 in _M_release (this=0x5556800f2ad0) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:144\r\n\r\n[... the same 10 frames are repeated over and over, with just the memory adresses of \"this\" changing...]\r\n\r\n#1439786 0x00007fffe74de768 in _M_release (this=0x55555e581fd0) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:144\r\n#1439787 ~__shared_count (this=0x555635d01c30, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:546\r\n#1439788 ~__shared_ptr (this=0x555635d01c28, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr_base.h:781\r\n#1439789 ~shared_ptr (this=0x555635d01c28, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/shared_ptr.h:93\r\n#1439790 ~pair (this=0x555635d01c28, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_pair.h:96\r\n#1439791 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int> > (__pointer=0x555635d01c28) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:93\r\n#1439792 __destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*> (__last=<optimized out>, __first=0x555635d01c28)\r\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:103\r\n#1439793 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*> (__last=<optimized out>, __first=<optimized out>)\r\n    at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:126\r\n#1439794 _Destroy<std::pair<std::shared_ptr<torch::autograd::Function>, int>*, std::pair<std::shared_ptr<torch::autograd::Function>, int> > (__last=0x555635d01c40, \r\n    __first=0x555635d01c10) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_construct.h:151\r\n#1439795 ~vector (this=0x7fff578ceab8, __in_chrg=<optimized out>) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/stl_vector.h:415\r\n#1439796 torch::autograd::Function::~Function (this=0x7fff578cea98, __in_chrg=<optimized out>) at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/function.h:77\r\n#1439797 0x00007fffe74f3086 in ~PyFunction (this=0x7fff578cea98, __in_chrg=<optimized out>)\r\n    at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/autograd/python_function.h:31\r\n#1439798 THPFunction_dealloc (self=0x7fff578ce9e8) at torch/csrc/autograd/python_function.cpp:283\r\n#1439799 0x00005555556f1ba6 in subtype_dealloc ()\r\n#1439800 0x0000555555642700 in _PyTrash_thread_destroy_chain ()\r\n#1439801 0x00005555556eac3e in fast_function ()\r\n#1439802 0x00005555556f0cc5 in call_function ()\r\n#1439803 0x000055555571519a in _PyEval_EvalFrameDefault ()\r\n#1439804 0x00005555556e9dfe in _PyEval_EvalCodeWithName ()\r\n#1439805 0x00005555556eaa11 in fast_function ()\r\n#1439806 0x00005555556f0cc5 in call_function ()\r\n#1439807 0x0000555555715eb1 in _PyEval_EvalFrameDefault ()\r\n#1439808 0x0000555555692b6b in _PyFunction_FastCall ()\r\n#1439809 0x00005555556f0cc5 in call_function ()\r\n#1439810 0x000055555571519a in _PyEval_EvalFrameDefault ()\r\n#1439811 0x00005555556eb529 in PyEval_EvalCodeEx ()\r\n#1439812 0x00005555556ec2cc in PyEval_EvalCode ()\r\n#1439813 0x0000555555768af4 in run_mod ()\r\n#1439814 0x0000555555768ef1 in PyRun_FileExFlags ()\r\n#1439815 0x00005555557690f4 in PyRun_SimpleFileExFlags ()\r\n#1439816 0x000055555576cc28 in Py_Main ()\r\n#1439817 0x000055555563471e in main ()\r\n```\r\n\r\nThis can be reproduced consistently with the following code: it fails at the end of the 7th iteration. (I'm not able to trim this code down any further. Sorry about that). It needs data from the file downloadable at https://criteois-my.sharepoint.com/:u:/g/personal/g_turri_criteo_com/EXw2X66h3NhNlKBYi8n-7JMB2dj276VuXqi_J6ZkyWcb7A?e=dnqegF\r\n\r\n```\r\nimport io\r\nimport random\r\nimport pickle\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport torch.nn.functional as f\r\nfrom torch import optim\r\n\r\n\r\nclass Encoder(nn.Module):\r\n    def __init__(self, input_size, hidden_size):\r\n        super(Encoder, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=1)\r\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\r\n\r\n    def forward(self, input, input_lengths, hidden):\r\n        embedded = self.embedding(input)\r\n        pack = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)\r\n        output, hiddens = self.gru(pack)\r\n        return hiddens\r\n\r\n    def init_hidden(self):\r\n        return Variable(torch.zeros(1, self.hidden_size))\r\n\r\n\r\nclass Decoder(nn.Module):\r\n    def __init__(self, hidden_size, output_size):\r\n        super(Decoder, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=1)\r\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\r\n        self.out = nn.Linear(hidden_size, output_size)\r\n        self.softmax = nn.LogSoftmax()\r\n\r\n    def forward(self, input, hidden):\r\n        output = self.embedding(input)\r\n        output = f.relu(output)\r\n        output, hidden = self.gru(output, hidden)\r\n        output = self.softmax(self.out(output))\r\n        return output, hidden\r\n\r\n    def init_hidden(self):\r\n        result = Variable(torch.zeros(1, 1, self.hidden_size))\r\n        return result.cuda() if self.use_cuda else result\r\n\r\ndef train(input_variable, input_lengths, target_variable, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\r\n    batch_size = len(input_variable)\r\n    encoder_hidden = encoder.init_hidden()\r\n\r\n    encoder_optimizer.zero_grad()\r\n    decoder_optimizer.zero_grad()\r\n\r\n    input_length = max(input_lengths)\r\n    target_length = max(target_lengths)\r\n\r\n\r\n    hidden = encoder(input_variable, input_lengths, encoder_hidden)\r\n\r\n    decoder_input = Variable(torch.LongTensor([[0] for i in range(batch_size)]))\r\n\r\n    loss = 0\r\n    for di in range(max(target_lengths)):\r\n        decoder_output, decoder_hidden = decoder(decoder_input, hidden)\r\n        loss += criterion(decoder_output.view(batch_size, -1), target_variable[:,di])\r\n        decoder_input = target_variable[:,di].contiguous().view(-1, 1)\r\n\r\n    loss.backward()\r\n    encoder_optimizer.step()\r\n    decoder_optimizer.step()\r\n\r\n    ret = loss.data[0] / target_length\r\n    print(\"returning from train: \" + str(ret));\r\n    return ret\r\n\r\n\r\ndef main():\r\n    random.seed(4385)\r\n    torch.random.manual_seed(4385)\r\n    # This weird line ensures the random generator state is the expected one\r\n    dummy = [random.choice([0] * 7353) for i in range(19968)]\r\n\r\n    encoder = Encoder(input_size=73, hidden_size=256)\r\n    decoder = Decoder(hidden_size=256, output_size=73)\r\n\r\n    # This data file is downloadable from https://criteois-my.sharepoint.com/:u:/g/personal/g_turri_criteo_com/EXw2X66h3NhNlKBYi8n-7JMB2dj276VuXqi_J6ZkyWcb7A?e=cqBgya\r\n    with open(\"training_data.small.pickle\", \"rb\") as f:\r\n        training_pairs = pickle.load(f)\r\n\r\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)\r\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)\r\n    criterion = nn.NLLLoss()\r\n\r\n    for iter in range(1, len(training_pairs)):\r\n        print(\"Starting iteration \" + str(iter))\r\n        training_pair = training_pairs[iter]\r\n\r\n        input_variable, input_lengths = training_pair[0]\r\n        target_variable, target_lengths = training_pair[1]\r\n\r\n        loss = train(input_variable, input_lengths, target_variable, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\r\n        print(\"loss: \" + str(loss))\r\n        print(\"Returned in main loop at the end of iteration \" + str(iter))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nThanks to the `print` statement we see that it fails at the end of the `train` function. ie: it prints the log just before the `return` but it doesn't print the one just after returning. It seems that upon getting out of this function it releases memory, which ends up on the segfault. However, I'm not able to understand how it should be fixed.\r\n\r\n(It turns out it's quite long to run until the segfault (several hours). I can probably try to serialize the states at the beginning of the last iteration to save time. I didn't do it because I'm afraid it might lead to a harder to understand snippet. Please let me know if you'd rather I try to do it nevertheless)."}