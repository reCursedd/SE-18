{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/356911188", "html_url": "https://github.com/pytorch/pytorch/pull/3853#issuecomment-356911188", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3853", "id": 356911188, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjkxMTE4OA==", "user": {"login": "eriche2016", "id": 11784910, "node_id": "MDQ6VXNlcjExNzg0OTEw", "avatar_url": "https://avatars2.githubusercontent.com/u/11784910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eriche2016", "html_url": "https://github.com/eriche2016", "followers_url": "https://api.github.com/users/eriche2016/followers", "following_url": "https://api.github.com/users/eriche2016/following{/other_user}", "gists_url": "https://api.github.com/users/eriche2016/gists{/gist_id}", "starred_url": "https://api.github.com/users/eriche2016/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eriche2016/subscriptions", "organizations_url": "https://api.github.com/users/eriche2016/orgs", "repos_url": "https://api.github.com/users/eriche2016/repos", "events_url": "https://api.github.com/users/eriche2016/events{/privacy}", "received_events_url": "https://api.github.com/users/eriche2016/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-11T11:46:13Z", "updated_at": "2018-01-11T11:48:32Z", "author_association": "NONE", "body_html": "<p>hi, i run into the problem when using LSTM and the error message is listed below when i feed forward data into LSTM:</p>\n<pre><code>UserWarning: RNN module weights are not part of single contiguous chunk\n of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call fla\ntten_parameters()\n</code></pre>\n<p>so i called flatten_parameters method, howver, there is still problem. And i trace back and find that the problem is caused by<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py#L73\">line</a>, and find that p.data_ptr() returns True. so if it is merged, the problem can be solved somehow.</p>", "body_text": "hi, i run into the problem when using LSTM and the error message is listed below when i feed forward data into LSTM:\nUserWarning: RNN module weights are not part of single contiguous chunk\n of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call fla\ntten_parameters()\n\nso i called flatten_parameters method, howver, there is still problem. And i trace back and find that the problem is caused by\nline, and find that p.data_ptr() returns True. so if it is merged, the problem can be solved somehow.", "body": "hi, i run into the problem when using LSTM and the error message is listed below when i feed forward data into LSTM: \r\n```\r\nUserWarning: RNN module weights are not part of single contiguous chunk\r\n of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call fla\r\ntten_parameters()\r\n```\r\nso i called flatten_parameters method, howver, there is still problem. And i trace back and find that the problem is caused by \r\n[line](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py#L73), and find that p.data_ptr() returns True. so if it is merged, the problem can be solved somehow. "}