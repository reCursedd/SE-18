{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/412574947", "html_url": "https://github.com/pytorch/pytorch/issues/10375#issuecomment-412574947", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10375", "id": 412574947, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjU3NDk0Nw==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-13T16:15:07Z", "updated_at": "2018-08-13T16:15:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When I run your script, I get:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/home/ezyang/Dev/pytorch-tmp-env/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n    obj = _ForkingPickler.dumps(obj)\n  File \"/home/ezyang/Dev/pytorch-tmp-env/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\n  File \"/data/users/ezyang/pytorch-tmp/torch/multiprocessing/reductions.py\", line 125, in reduce_tensor\n    (device, handle, storage_size, storage_offset) = storage._share_cuda_()\nRuntimeError: invalid device pointer: 0x2049e0000 at ../aten/src/THC/THCCachingAllocator.cpp:262\n</code></pre>\n<p>This is actually an unrelated bug, where if you send a CUDA tensor to another process, and then send that <em>same</em> CUDA tensor back, it does not work, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> tells me. If I send something else back, nothing blocks.</p>\n<pre><code>import torch\n\n\ndef _process(queue):\n    input_ = queue.get()\n    print('get')\n    queue.put(0)\n    print('put')\n\n\nif __name__ == '__main__':\n    torch.multiprocessing.set_start_method('spawn')\n    input_ = torch.ones(1).cuda()\n    queue = torch.multiprocessing.Queue()\n    process = torch.multiprocessing.Process(target=_process, args=(queue,))\n    process.start()\n    queue.put(input_)\n    process.join()\n    result = queue.get()\n    print('end')\n    print(result)\n</code></pre>", "body_text": "When I run your script, I get:\nTraceback (most recent call last):\n  File \"/home/ezyang/Dev/pytorch-tmp-env/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\n    obj = _ForkingPickler.dumps(obj)\n  File \"/home/ezyang/Dev/pytorch-tmp-env/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\n  File \"/data/users/ezyang/pytorch-tmp/torch/multiprocessing/reductions.py\", line 125, in reduce_tensor\n    (device, handle, storage_size, storage_offset) = storage._share_cuda_()\nRuntimeError: invalid device pointer: 0x2049e0000 at ../aten/src/THC/THCCachingAllocator.cpp:262\n\nThis is actually an unrelated bug, where if you send a CUDA tensor to another process, and then send that same CUDA tensor back, it does not work, @colesbury tells me. If I send something else back, nothing blocks.\nimport torch\n\n\ndef _process(queue):\n    input_ = queue.get()\n    print('get')\n    queue.put(0)\n    print('put')\n\n\nif __name__ == '__main__':\n    torch.multiprocessing.set_start_method('spawn')\n    input_ = torch.ones(1).cuda()\n    queue = torch.multiprocessing.Queue()\n    process = torch.multiprocessing.Process(target=_process, args=(queue,))\n    process.start()\n    queue.put(input_)\n    process.join()\n    result = queue.get()\n    print('end')\n    print(result)", "body": "When I run your script, I get:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ezyang/Dev/pytorch-tmp-env/lib/python3.6/multiprocessing/queues.py\", line 234, in _feed\r\n    obj = _ForkingPickler.dumps(obj)\r\n  File \"/home/ezyang/Dev/pytorch-tmp-env/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\r\n    cls(buf, protocol).dump(obj)\r\n  File \"/data/users/ezyang/pytorch-tmp/torch/multiprocessing/reductions.py\", line 125, in reduce_tensor\r\n    (device, handle, storage_size, storage_offset) = storage._share_cuda_()\r\nRuntimeError: invalid device pointer: 0x2049e0000 at ../aten/src/THC/THCCachingAllocator.cpp:262\r\n```\r\n\r\nThis is actually an unrelated bug, where if you send a CUDA tensor to another process, and then send that *same* CUDA tensor back, it does not work, @colesbury tells me. If I send something else back, nothing blocks.\r\n\r\n```\r\nimport torch\r\n\r\n\r\ndef _process(queue):\r\n    input_ = queue.get()\r\n    print('get')\r\n    queue.put(0)\r\n    print('put')\r\n\r\n\r\nif __name__ == '__main__':\r\n    torch.multiprocessing.set_start_method('spawn')\r\n    input_ = torch.ones(1).cuda()\r\n    queue = torch.multiprocessing.Queue()\r\n    process = torch.multiprocessing.Process(target=_process, args=(queue,))\r\n    process.start()\r\n    queue.put(input_)\r\n    process.join()\r\n    result = queue.get()\r\n    print('end')\r\n    print(result)\r\n```"}