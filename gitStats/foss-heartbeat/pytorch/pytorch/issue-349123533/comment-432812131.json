{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/432812131", "html_url": "https://github.com/pytorch/pytorch/issues/10375#issuecomment-432812131", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10375", "id": 432812131, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMjgxMjEzMQ==", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-24T20:16:25Z", "updated_at": "2018-10-25T21:41:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p><strong>EDIT:</strong> Disregard the simplified example below (which is due to some other issue, and can be seen with a single child process), I think it is unrelated to the issue that we are facing. I have described the issue in this <a href=\"https://discuss.pytorch.org/t/invalid-device-pointer-using-multiprocessing-with-cuda/28023\" rel=\"nofollow\">forum post</a>.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> : I bumped into this issue, in a slightly different context - we have a pool of workers which put tensors into a shared queue that is read by the main process.</p>\n<p>It works great for CPU tensors, but with CUDA tensors it works only with a single worker, and fails when we have 2 workers with the same <code>invalid device pointer</code> exception.</p>\n<p>While the actual example is a bit more complicated, it is (modifying the example above) along the lines of:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_process1</span>(<span class=\"pl-smi\">queue</span>):\n    queue.put(torch.tensor(<span class=\"pl-c1\">1</span>).cuda())\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>put1<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_process2</span>(<span class=\"pl-smi\">queue</span>):\n    queue.put(torch.tensor(<span class=\"pl-c1\">2</span>).cuda())\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>put2<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    torch.multiprocessing.set_start_method(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>spawn<span class=\"pl-pds\">'</span></span>)\n    queue <span class=\"pl-k\">=</span> torch.multiprocessing.Queue()\n    process1 <span class=\"pl-k\">=</span> torch.multiprocessing.Process(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>_process1, <span class=\"pl-v\">args</span><span class=\"pl-k\">=</span>(queue,))\n    process2 <span class=\"pl-k\">=</span> torch.multiprocessing.Process(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>_process2, <span class=\"pl-v\">args</span><span class=\"pl-k\">=</span>(queue,))\n    process1.start()\n    process2.start()\n    r1 <span class=\"pl-k\">=</span> queue.get()\n    r2 <span class=\"pl-k\">=</span> queue.get()\n    process1.join()\n    process2.join()    \n    <span class=\"pl-c1\">print</span>(r1, r2)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>end<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>which throws an exception:</p>\n<pre><code>put1\nTHCudaCheck FAIL file=/home/npradhan/workspace/pyro_dev/pytorch/torch/csrc/generic/StorageSharing.cpp line=276 error=30 : unknown error\nTraceback (most recent call last):\n  File \"examples/ex1.py\", line 19, in &lt;module&gt;\n    r1 = queue.get()\n  File \"/home/npradhan/miniconda3/envs/pytorch-master/lib/python3.6/multiprocessing/queues.py\", line 113, in get\n    return _ForkingPickler.loads(res)\n  File \"/home/npradhan/miniconda3/envs/pytorch-master/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 93, in rebuild_cuda_tensor\n    storage = storage_cls._new_shared_cuda(storage_device, storage_handle, storage_size)\nRuntimeError: cuda runtime error (30) : unknown error at /home/npradhan/workspace/pyro_dev/pytorch/torch/csrc/generic/StorageSharing.cpp:276\nput2\n</code></pre>\n<p><del>I think the underlying reason is probably the same as the more complicated <a href=\"https://github.com/uber/pyro/blob/dev/pyro/infer/mcmc/mcmc.py#L89\">example</a> that I am trying to get to work with CUDA.</del> (see comment above)</p>\n<p>In such cases, what is the recommended way (currently) to read data from multiple workers:</p>\n<ul>\n<li>Should each worker communicate with the main process via its own <code>mp.Queue</code> instance?</li>\n<li>Can we use the lighter-weight <code>mp.Pipe</code> between each worker and the main process instead?</li>\n</ul>", "body_text": "EDIT: Disregard the simplified example below (which is due to some other issue, and can be seen with a single child process), I think it is unrelated to the issue that we are facing. I have described the issue in this forum post.\n@zou3519, @ezyang : I bumped into this issue, in a slightly different context - we have a pool of workers which put tensors into a shared queue that is read by the main process.\nIt works great for CPU tensors, but with CUDA tensors it works only with a single worker, and fails when we have 2 workers with the same invalid device pointer exception.\nWhile the actual example is a bit more complicated, it is (modifying the example above) along the lines of:\nimport torch\n\ndef _process1(queue):\n    queue.put(torch.tensor(1).cuda())\n    print('put1')\n\ndef _process2(queue):\n    queue.put(torch.tensor(2).cuda())\n    print('put2')\n\nif __name__ == '__main__':\n    torch.multiprocessing.set_start_method('spawn')\n    queue = torch.multiprocessing.Queue()\n    process1 = torch.multiprocessing.Process(target=_process1, args=(queue,))\n    process2 = torch.multiprocessing.Process(target=_process2, args=(queue,))\n    process1.start()\n    process2.start()\n    r1 = queue.get()\n    r2 = queue.get()\n    process1.join()\n    process2.join()    \n    print(r1, r2)\n    print('end')\nwhich throws an exception:\nput1\nTHCudaCheck FAIL file=/home/npradhan/workspace/pyro_dev/pytorch/torch/csrc/generic/StorageSharing.cpp line=276 error=30 : unknown error\nTraceback (most recent call last):\n  File \"examples/ex1.py\", line 19, in <module>\n    r1 = queue.get()\n  File \"/home/npradhan/miniconda3/envs/pytorch-master/lib/python3.6/multiprocessing/queues.py\", line 113, in get\n    return _ForkingPickler.loads(res)\n  File \"/home/npradhan/miniconda3/envs/pytorch-master/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 93, in rebuild_cuda_tensor\n    storage = storage_cls._new_shared_cuda(storage_device, storage_handle, storage_size)\nRuntimeError: cuda runtime error (30) : unknown error at /home/npradhan/workspace/pyro_dev/pytorch/torch/csrc/generic/StorageSharing.cpp:276\nput2\n\nI think the underlying reason is probably the same as the more complicated example that I am trying to get to work with CUDA. (see comment above)\nIn such cases, what is the recommended way (currently) to read data from multiple workers:\n\nShould each worker communicate with the main process via its own mp.Queue instance?\nCan we use the lighter-weight mp.Pipe between each worker and the main process instead?", "body": "**EDIT:** Disregard the simplified example below (which is due to some other issue, and can be seen with a single child process), I think it is unrelated to the issue that we are facing. I have described the issue in this [forum post](https://discuss.pytorch.org/t/invalid-device-pointer-using-multiprocessing-with-cuda/28023).\r\n\r\n@zou3519, @ezyang : I bumped into this issue, in a slightly different context - we have a pool of workers which put tensors into a shared queue that is read by the main process. \r\n\r\nIt works great for CPU tensors, but with CUDA tensors it works only with a single worker, and fails when we have 2 workers with the same `invalid device pointer` exception. \r\n\r\nWhile the actual example is a bit more complicated, it is (modifying the example above) along the lines of:\r\n\r\n```python\r\nimport torch\r\n\r\ndef _process1(queue):\r\n    queue.put(torch.tensor(1).cuda())\r\n    print('put1')\r\n\r\ndef _process2(queue):\r\n    queue.put(torch.tensor(2).cuda())\r\n    print('put2')\r\n\r\nif __name__ == '__main__':\r\n    torch.multiprocessing.set_start_method('spawn')\r\n    queue = torch.multiprocessing.Queue()\r\n    process1 = torch.multiprocessing.Process(target=_process1, args=(queue,))\r\n    process2 = torch.multiprocessing.Process(target=_process2, args=(queue,))\r\n    process1.start()\r\n    process2.start()\r\n    r1 = queue.get()\r\n    r2 = queue.get()\r\n    process1.join()\r\n    process2.join()    \r\n    print(r1, r2)\r\n    print('end')\r\n```\r\n\r\nwhich throws an exception:\r\n```\r\nput1\r\nTHCudaCheck FAIL file=/home/npradhan/workspace/pyro_dev/pytorch/torch/csrc/generic/StorageSharing.cpp line=276 error=30 : unknown error\r\nTraceback (most recent call last):\r\n  File \"examples/ex1.py\", line 19, in <module>\r\n    r1 = queue.get()\r\n  File \"/home/npradhan/miniconda3/envs/pytorch-master/lib/python3.6/multiprocessing/queues.py\", line 113, in get\r\n    return _ForkingPickler.loads(res)\r\n  File \"/home/npradhan/miniconda3/envs/pytorch-master/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 93, in rebuild_cuda_tensor\r\n    storage = storage_cls._new_shared_cuda(storage_device, storage_handle, storage_size)\r\nRuntimeError: cuda runtime error (30) : unknown error at /home/npradhan/workspace/pyro_dev/pytorch/torch/csrc/generic/StorageSharing.cpp:276\r\nput2\r\n```\r\n\r\n~I think the underlying reason is probably the same as the more complicated [example](https://github.com/uber/pyro/blob/dev/pyro/infer/mcmc/mcmc.py#L89) that I am trying to get to work with CUDA.~ (see comment above)\r\n\r\nIn such cases, what is the recommended way (currently) to read data from multiple workers:\r\n - Should each worker communicate with the main process via its own `mp.Queue` instance?\r\n - Can we use the lighter-weight `mp.Pipe` between each worker and the main process instead? "}