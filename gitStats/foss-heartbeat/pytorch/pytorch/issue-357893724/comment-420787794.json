{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/420787794", "html_url": "https://github.com/pytorch/pytorch/issues/11360#issuecomment-420787794", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11360", "id": 420787794, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDc4Nzc5NA==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-12T20:33:54Z", "updated_at": "2018-09-13T15:38:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Update with some numbers so I don't forget about them:</p>\n<p>On an AWS image used by the CI, with a docker image that has 8g of memory:</p>\n<ul>\n<li>test_jit.py's python process achieves &gt; 4g of rss usage. This is probably not because of a memory leak (valgrind didn't detect anything) but probably because the malloc allocator didn't feel like freeing its memory.</li>\n<li>the <code>system()</code> call, most notably used by the CPU Fuser (and probably the cuda fuser as well), does a <code>fork()</code> syscall</li>\n<li>The docker image supports copy-on-write. Despite this, fork() fails when the process uses more than half of available RAM before forking (I tested a few small c++ programs that allocate memory and attempt to fork).</li>\n<li>Because test_jit.py uses more than 4g of ram, fork() fails and the CPU fuser just fails to run.</li>\n</ul>\n<p>Other notes:</p>\n<ul>\n<li>This is only reproducible on the CI. On a local machine with a docker image limited to 8g of memory, <code>fork()</code> succeeds in a process that uses &gt; 4g of memory. <code>fork</code> behavior is OS dependent (and I guess the aws instances are vms so it depends on what the vms are running on)</li>\n</ul>\n<p>A few possible action items:</p>\n<ul>\n<li>The fusers should have better error checking and reporting when they fail to run because that information is available.</li>\n<li>test_jit.py should not be using so much peak memory, that needs to be investigated.</li>\n<li>Investigate fork() semantics: is this different on other machines? Is there an easier way to fork() and then immediately exec(), maybe with a small separate process?</li>\n</ul>", "body_text": "Update with some numbers so I don't forget about them:\nOn an AWS image used by the CI, with a docker image that has 8g of memory:\n\ntest_jit.py's python process achieves > 4g of rss usage. This is probably not because of a memory leak (valgrind didn't detect anything) but probably because the malloc allocator didn't feel like freeing its memory.\nthe system() call, most notably used by the CPU Fuser (and probably the cuda fuser as well), does a fork() syscall\nThe docker image supports copy-on-write. Despite this, fork() fails when the process uses more than half of available RAM before forking (I tested a few small c++ programs that allocate memory and attempt to fork).\nBecause test_jit.py uses more than 4g of ram, fork() fails and the CPU fuser just fails to run.\n\nOther notes:\n\nThis is only reproducible on the CI. On a local machine with a docker image limited to 8g of memory, fork() succeeds in a process that uses > 4g of memory. fork behavior is OS dependent (and I guess the aws instances are vms so it depends on what the vms are running on)\n\nA few possible action items:\n\nThe fusers should have better error checking and reporting when they fail to run because that information is available.\ntest_jit.py should not be using so much peak memory, that needs to be investigated.\nInvestigate fork() semantics: is this different on other machines? Is there an easier way to fork() and then immediately exec(), maybe with a small separate process?", "body": "Update with some numbers so I don't forget about them:\r\n\r\nOn an AWS image used by the CI, with a docker image that has 8g of memory:\r\n- test_jit.py's python process achieves > 4g of rss usage. This is probably not because of a memory leak (valgrind didn't detect anything) but probably because the malloc allocator didn't feel like freeing its memory.\r\n- the `system()` call, most notably used by the CPU Fuser (and probably the cuda fuser as well), does a `fork()` syscall\r\n- The docker image supports copy-on-write. Despite this, fork() fails when the process uses more than half of available RAM before forking (I tested a few small c++ programs that allocate memory and attempt to fork). \r\n- Because test_jit.py uses more than 4g of ram, fork() fails and the CPU fuser just fails to run.\r\n\r\nOther notes:\r\n- This is only reproducible on the CI. On a local machine with a docker image limited to 8g of memory, `fork()` succeeds in a process that uses > 4g of memory. `fork` behavior is OS dependent (and I guess the aws instances are vms so it depends on what the vms are running on)\r\n\r\nA few possible action items:\r\n- The fusers should have better error checking and reporting when they fail to run because that information is available.\r\n- test_jit.py should not be using so much peak memory, that needs to be investigated.\r\n- Investigate fork() semantics: is this different on other machines? Is there an easier way to fork() and then immediately exec(), maybe with a small separate process?"}