{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8687", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8687/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8687/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8687/events", "html_url": "https://github.com/pytorch/pytorch/pull/8687", "id": 334107992, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk2MTYzMDQ4", "number": 8687, "title": "Better support for literals in jit script", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-06-20T14:33:18Z", "updated_at": "2018-11-23T15:46:00Z", "closed_at": "2018-06-21T19:43:39Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8687", "html_url": "https://github.com/pytorch/pytorch/pull/8687", "diff_url": "https://github.com/pytorch/pytorch/pull/8687.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8687.patch"}, "body_html": "<p>Addresses <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"329654438\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8177\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8177/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8177\">#8177</a></p>\n<p>A design doc can be found here: <a href=\"https://gist.github.com/zou3519/4b7f13f03cc9f3612bd9363e6405fa0a\">gist</a> version or <a href=\"https://fb.quip.com/azL1AqUckBdo\" rel=\"nofollow\">quip</a> version</p>\n<p>General approach:</p>\n<ul>\n<li>Add NumberType, FloatType, IntType to represent Python numbers, floats and ints.</li>\n<li>Emit these types for python literals</li>\n<li>Change aten_schema such that Scalars are NumberType, int64_t and bool are IntType.</li>\n<li>Emit aten::type_as, prim::NumToTensor, and prim::TensorToNum nodes for tensor-number math. (see examples below)</li>\n<li>Erase NumberType,  prim::NumToTensor, and prim::TensorToNum for ONNX export. Let ONNX export the aten::type_as nodes.</li>\n</ul>\n<h3>Tensor/number math</h3>\n<pre><code>import torch\n@torch.jit.script\ndef fn(x):\n    return x + 1\n</code></pre>\n<pre><code>graph(%x : Dynamic) {\n  %1 : int = prim::Constant[value={1}]()\n  %2 : Dynamic = prim::NumToTensor(%1)\n  %3 : Dynamic = aten::type_as(%2, %x)\n  %4 : Dynamic = aten::add[alpha={1}](%x, %4)\n  return (%5);\n}\n</code></pre>\n<h3>Number/Number Math</h3>\n<pre><code>import torch\n@torch.jit.script\ndef fn(zero):\n    c = 1 + 1\n    return zero + c\n</code></pre>\n<pre><code>graph(%zero : Dynamic) {\n  %1 : int = prim::Constant[value={1}]()\n  %2 : int = prim::Constant[value={1}]()\n  %3 : Dynamic = prim::num_to_tensor(%1)\n  %4 : Dynamic = prim::num_to_tensor(%2)\n  %5 : Dynamic = aten::add[alpha={1}](%3, %4)\n  %c : int = prim::TensorToNum(%6)  # this is the result of the addition\n  ...\n  return (%13);\n}\n</code></pre>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4685384\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jamesr66a\">@jamesr66a</a></p>\n<p>There are a list of follow up tasks that I will file issues for later. If any of these seem important right now I can fold them into this PR.</p>\n<ul>\n<li>Let's say I called prim::NumToTensor on an int. Should the output be of type Long()? (It is currently DynamicType)</li>\n<li>Allow scalar tensors for Scalar args</li>\n<li>Add bool type to the schema to differentiate between int and bool\n<ul>\n<li>fix var/std</li>\n</ul>\n</li>\n<li>Test logical operators and make them follow python semantics</li>\n<li>Support passing in non-tensor arguments to script functions/support returning non-tensor values from script functions</li>\n<li>Change the schema for IntList to List[int] or something similar</li>\n<li>Better support for List[int] or Tuple[int] args</li>\n<li>Add peephole to delete unnecessary type_as. This peephole should be probably be run on specialized graphs (in an unspecialized graph, it is hard to tell if a type_as is unnecessary).</li>\n<li>we should clean up the math handling code. Instead of using if statements on types, we can just register additional builtin ops like <strong>add</strong> and <strong>le</strong>. Including <strong>rdiv</strong> style ops to handle the reverse cases. This will make it easier to see how math is getting emitted and ensures the math functions always go through the same type-conversions as built in functions.</li>\n</ul>", "body_text": "Addresses #8177\nA design doc can be found here: gist version or quip version\nGeneral approach:\n\nAdd NumberType, FloatType, IntType to represent Python numbers, floats and ints.\nEmit these types for python literals\nChange aten_schema such that Scalars are NumberType, int64_t and bool are IntType.\nEmit aten::type_as, prim::NumToTensor, and prim::TensorToNum nodes for tensor-number math. (see examples below)\nErase NumberType,  prim::NumToTensor, and prim::TensorToNum for ONNX export. Let ONNX export the aten::type_as nodes.\n\nTensor/number math\nimport torch\n@torch.jit.script\ndef fn(x):\n    return x + 1\n\ngraph(%x : Dynamic) {\n  %1 : int = prim::Constant[value={1}]()\n  %2 : Dynamic = prim::NumToTensor(%1)\n  %3 : Dynamic = aten::type_as(%2, %x)\n  %4 : Dynamic = aten::add[alpha={1}](%x, %4)\n  return (%5);\n}\n\nNumber/Number Math\nimport torch\n@torch.jit.script\ndef fn(zero):\n    c = 1 + 1\n    return zero + c\n\ngraph(%zero : Dynamic) {\n  %1 : int = prim::Constant[value={1}]()\n  %2 : int = prim::Constant[value={1}]()\n  %3 : Dynamic = prim::num_to_tensor(%1)\n  %4 : Dynamic = prim::num_to_tensor(%2)\n  %5 : Dynamic = aten::add[alpha={1}](%3, %4)\n  %c : int = prim::TensorToNum(%6)  # this is the result of the addition\n  ...\n  return (%13);\n}\n\ncc @zdevito @jamesr66a\nThere are a list of follow up tasks that I will file issues for later. If any of these seem important right now I can fold them into this PR.\n\nLet's say I called prim::NumToTensor on an int. Should the output be of type Long()? (It is currently DynamicType)\nAllow scalar tensors for Scalar args\nAdd bool type to the schema to differentiate between int and bool\n\nfix var/std\n\n\nTest logical operators and make them follow python semantics\nSupport passing in non-tensor arguments to script functions/support returning non-tensor values from script functions\nChange the schema for IntList to List[int] or something similar\nBetter support for List[int] or Tuple[int] args\nAdd peephole to delete unnecessary type_as. This peephole should be probably be run on specialized graphs (in an unspecialized graph, it is hard to tell if a type_as is unnecessary).\nwe should clean up the math handling code. Instead of using if statements on types, we can just register additional builtin ops like add and le. Including rdiv style ops to handle the reverse cases. This will make it easier to see how math is getting emitted and ensures the math functions always go through the same type-conversions as built in functions.", "body": "Addresses #8177\r\n\r\nA design doc can be found here: [gist](https://gist.github.com/zou3519/4b7f13f03cc9f3612bd9363e6405fa0a) version or [quip](https://fb.quip.com/azL1AqUckBdo) version\r\n\r\nGeneral approach:\r\n- Add NumberType, FloatType, IntType to represent Python numbers, floats and ints.\r\n- Emit these types for python literals\r\n- Change aten_schema such that Scalars are NumberType, int64_t and bool are IntType.\r\n- Emit aten::type_as, prim::NumToTensor, and prim::TensorToNum nodes for tensor-number math. (see examples below)\r\n- Erase NumberType,  prim::NumToTensor, and prim::TensorToNum for ONNX export. Let ONNX export the aten::type_as nodes.\r\n\r\n### Tensor/number math\r\n```\r\nimport torch\r\n@torch.jit.script\r\ndef fn(x):\r\n    return x + 1\r\n```\r\n```\r\ngraph(%x : Dynamic) {\r\n  %1 : int = prim::Constant[value={1}]()\r\n  %2 : Dynamic = prim::NumToTensor(%1)\r\n  %3 : Dynamic = aten::type_as(%2, %x)\r\n  %4 : Dynamic = aten::add[alpha={1}](%x, %4)\r\n  return (%5);\r\n}\r\n```\r\n\r\n### Number/Number Math\r\n```\r\nimport torch\r\n@torch.jit.script\r\ndef fn(zero):\r\n    c = 1 + 1\r\n    return zero + c\r\n```\r\n```\r\ngraph(%zero : Dynamic) {\r\n  %1 : int = prim::Constant[value={1}]()\r\n  %2 : int = prim::Constant[value={1}]()\r\n  %3 : Dynamic = prim::num_to_tensor(%1)\r\n  %4 : Dynamic = prim::num_to_tensor(%2)\r\n  %5 : Dynamic = aten::add[alpha={1}](%3, %4)\r\n  %c : int = prim::TensorToNum(%6)  # this is the result of the addition\r\n  ...\r\n  return (%13);\r\n}\r\n```\r\n\r\ncc @zdevito @jamesr66a \r\n\r\nThere are a list of follow up tasks that I will file issues for later. If any of these seem important right now I can fold them into this PR.\r\n- Let's say I called prim::NumToTensor on an int. Should the output be of type Long()? (It is currently DynamicType)\r\n- Allow scalar tensors for Scalar args\r\n- Add bool type to the schema to differentiate between int and bool\r\n  - fix var/std\r\n- Test logical operators and make them follow python semantics\r\n- Support passing in non-tensor arguments to script functions/support returning non-tensor values from script functions\r\n- Change the schema for IntList to List[int] or something similar\r\n- Better support for List[int] or Tuple[int] args\r\n- Add peephole to delete unnecessary type_as. This peephole should be probably be run on specialized graphs (in an unspecialized graph, it is hard to tell if a type_as is unnecessary).\r\n- we should clean up the math handling code. Instead of using if statements on types, we can just register additional builtin ops like __add__ and __le__. Including __rdiv__ style ops to handle the reverse cases. This will make it easier to see how math is getting emitted and ensures the math functions always go through the same type-conversions as built in functions.\r\n\r\n"}