{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7495", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7495/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7495/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7495/events", "html_url": "https://github.com/pytorch/pytorch/pull/7495", "id": 322189707, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg3MzgyOTcx", "number": 7495, "title": "Vectorize tanh", "user": {"login": "vedanuj", "id": 13946458, "node_id": "MDQ6VXNlcjEzOTQ2NDU4", "avatar_url": "https://avatars2.githubusercontent.com/u/13946458?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vedanuj", "html_url": "https://github.com/vedanuj", "followers_url": "https://api.github.com/users/vedanuj/followers", "following_url": "https://api.github.com/users/vedanuj/following{/other_user}", "gists_url": "https://api.github.com/users/vedanuj/gists{/gist_id}", "starred_url": "https://api.github.com/users/vedanuj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vedanuj/subscriptions", "organizations_url": "https://api.github.com/users/vedanuj/orgs", "repos_url": "https://api.github.com/users/vedanuj/repos", "events_url": "https://api.github.com/users/vedanuj/events{/privacy}", "received_events_url": "https://api.github.com/users/vedanuj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-11T07:02:26Z", "updated_at": "2018-11-23T15:43:58Z", "closed_at": "2018-05-29T19:05:41Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7495", "html_url": "https://github.com/pytorch/pytorch/pull/7495", "diff_url": "https://github.com/pytorch/pytorch/pull/7495.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7495.patch"}, "body_html": "<p>This PR adds an AVX vectorized <code>tanh</code> implementation and addresses <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"293403624\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4984\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4984/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4984\">#4984</a>. This is a vectorized version of <a href=\"http://www.netlib.org/cephes/\" rel=\"nofollow\">cephes math library's single precision</a> <code>tanh</code> function. We see <strong>speedups of upto ~9x</strong>. Benchmark results below.</p>\n<p><em>Details :</em> This revives the old <a href=\"https://github.com/pytorch/pytorch/pull/5987\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/5987/hovercard\">PR 5987</a> which was using an <code>exp256_ps</code> vectorized exponentiation method from  <code>avx_mathfun.h</code>. But <code>avx_mathfun</code> was <a href=\"https://github.com/pytorch/pytorch/pull/6192\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6192/hovercard\">imprecise in many cases</a>. After the addition of <a href=\"https://github.com/pytorch/pytorch/pull/7341\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/7341/hovercard\">Sleef</a> we can now use the <code>expf</code> methods from Sleef to replace <code>exp256_ps</code> used in the earlier PR. This implementation uses the <code>Sleef_expf8_u10</code> function for single precision float.</p>\n<p><strong>Benchmark results with vectorized <code>tanh</code> for float::</strong></p>\n<p>master @ <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/947155c69ddc114c8f8a69a0f971386fb365bf09/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/947155c69ddc114c8f8a69a0f971386fb365bf09\"><tt>947155c</tt></a></p>\n<pre><code>['tanh']\ntanh:           size: 10^2      count: 10000    elapsed: 8.143314838409424      type: torch.FloatTensor\ntanh:           size: 10^3      count: 1000     elapsed: 1.8030304908752441     type: torch.FloatTensor\ntanh:           size: 10^4      count: 100      elapsed: 1.4339056015014648     type: torch.FloatTensor\ntanh:           size: 10^5      count: 10       elapsed: 1.2880470752716064     type: torch.FloatTensor\n</code></pre>\n<p><a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/fa02f2a7b07693d436a327ac6ac5b5bc8051fb19/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/fa02f2a7b07693d436a327ac6ac5b5bc8051fb19\"><tt>fa02f2a</tt></a></p>\n<pre><code>tanh:           size: 10^2      count: 10000    elapsed: 1.0960958003997803     type: torch.FloatTensor\ntanh:           size: 10^3      count: 1000     elapsed: 0.19293761253356934    type: torch.FloatTensor\ntanh:           size: 10^4      count: 100      elapsed: 0.5942552089691162     type: torch.FloatTensor\ntanh:           size: 10^5      count: 10       elapsed: 0.5254971981048584     type: torch.FloatTensor\n</code></pre>\n<p><strong>Doesn't affect the performance for doubles ::</strong></p>\n<p>master @ <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/947155c69ddc114c8f8a69a0f971386fb365bf09/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/947155c69ddc114c8f8a69a0f971386fb365bf09\"><tt>947155c</tt></a></p>\n<pre><code>['tanh']\ntanh:           size: 10^2      count: 10000    elapsed: 9.550215005874634      type: torch.DoubleTensor\ntanh:           size: 10^3      count: 1000     elapsed: 2.3597443103790283     type: torch.DoubleTensor\ntanh:           size: 10^4      count: 100      elapsed: 1.9869627952575684     type: torch.DoubleTensor\ntanh:           size: 10^5      count: 10       elapsed: 1.657977819442749      type: torch.DoubleTensor\n</code></pre>\n<p><a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/fa02f2a7b07693d436a327ac6ac5b5bc8051fb19/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/fa02f2a7b07693d436a327ac6ac5b5bc8051fb19\"><tt>fa02f2a</tt></a></p>\n<pre><code>['tanh']\ntanh:           size: 10^2      count: 10000    elapsed: 9.589819431304932      type: torch.DoubleTensor\ntanh:           size: 10^3      count: 1000     elapsed: 2.237194299697876      type: torch.DoubleTensor\ntanh:           size: 10^4      count: 100      elapsed: 1.8959002494812012     type: torch.DoubleTensor\ntanh:           size: 10^5      count: 10       elapsed: 1.7132713794708252     type: torch.DoubleTensor\n</code></pre>\n<p><strong>Correctness Test::</strong></p>\n<pre><code>python test_torch.py TestTorch.test_tanh\n\n.\n----------------------------------------------------------------------\nRan 1 test in 0.205s\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> Please review</p>", "body_text": "This PR adds an AVX vectorized tanh implementation and addresses #4984. This is a vectorized version of cephes math library's single precision tanh function. We see speedups of upto ~9x. Benchmark results below.\nDetails : This revives the old PR 5987 which was using an exp256_ps vectorized exponentiation method from  avx_mathfun.h. But avx_mathfun was imprecise in many cases. After the addition of Sleef we can now use the expf methods from Sleef to replace exp256_ps used in the earlier PR. This implementation uses the Sleef_expf8_u10 function for single precision float.\nBenchmark results with vectorized tanh for float::\nmaster @ 947155c\n['tanh']\ntanh:           size: 10^2      count: 10000    elapsed: 8.143314838409424      type: torch.FloatTensor\ntanh:           size: 10^3      count: 1000     elapsed: 1.8030304908752441     type: torch.FloatTensor\ntanh:           size: 10^4      count: 100      elapsed: 1.4339056015014648     type: torch.FloatTensor\ntanh:           size: 10^5      count: 10       elapsed: 1.2880470752716064     type: torch.FloatTensor\n\nfa02f2a\ntanh:           size: 10^2      count: 10000    elapsed: 1.0960958003997803     type: torch.FloatTensor\ntanh:           size: 10^3      count: 1000     elapsed: 0.19293761253356934    type: torch.FloatTensor\ntanh:           size: 10^4      count: 100      elapsed: 0.5942552089691162     type: torch.FloatTensor\ntanh:           size: 10^5      count: 10       elapsed: 0.5254971981048584     type: torch.FloatTensor\n\nDoesn't affect the performance for doubles ::\nmaster @ 947155c\n['tanh']\ntanh:           size: 10^2      count: 10000    elapsed: 9.550215005874634      type: torch.DoubleTensor\ntanh:           size: 10^3      count: 1000     elapsed: 2.3597443103790283     type: torch.DoubleTensor\ntanh:           size: 10^4      count: 100      elapsed: 1.9869627952575684     type: torch.DoubleTensor\ntanh:           size: 10^5      count: 10       elapsed: 1.657977819442749      type: torch.DoubleTensor\n\nfa02f2a\n['tanh']\ntanh:           size: 10^2      count: 10000    elapsed: 9.589819431304932      type: torch.DoubleTensor\ntanh:           size: 10^3      count: 1000     elapsed: 2.237194299697876      type: torch.DoubleTensor\ntanh:           size: 10^4      count: 100      elapsed: 1.8959002494812012     type: torch.DoubleTensor\ntanh:           size: 10^5      count: 10       elapsed: 1.7132713794708252     type: torch.DoubleTensor\n\nCorrectness Test::\npython test_torch.py TestTorch.test_tanh\n\n.\n----------------------------------------------------------------------\nRan 1 test in 0.205s\n\n@cpuhrsch @apaszke @colesbury Please review", "body": "This PR adds an AVX vectorized `tanh` implementation and addresses #4984. This is a vectorized version of [cephes math library's single precision](http://www.netlib.org/cephes/) `tanh` function. We see **speedups of upto ~9x**. Benchmark results below.\r\n\r\n_Details :_ This revives the old [PR 5987](https://github.com/pytorch/pytorch/pull/5987) which was using an `exp256_ps` vectorized exponentiation method from  `avx_mathfun.h`. But `avx_mathfun` was [imprecise in many cases](https://github.com/pytorch/pytorch/pull/6192). After the addition of [Sleef](https://github.com/pytorch/pytorch/pull/7341) we can now use the `expf` methods from Sleef to replace `exp256_ps` used in the earlier PR. This implementation uses the `Sleef_expf8_u10` function for single precision float. \r\n\r\n**Benchmark results with vectorized `tanh` for float::**\r\n\r\nmaster @ 947155c69ddc114c8f8a69a0f971386fb365bf09\r\n\r\n```\r\n['tanh']\r\ntanh:           size: 10^2      count: 10000    elapsed: 8.143314838409424      type: torch.FloatTensor\r\ntanh:           size: 10^3      count: 1000     elapsed: 1.8030304908752441     type: torch.FloatTensor\r\ntanh:           size: 10^4      count: 100      elapsed: 1.4339056015014648     type: torch.FloatTensor\r\ntanh:           size: 10^5      count: 10       elapsed: 1.2880470752716064     type: torch.FloatTensor\r\n```\r\n\r\nfa02f2a\r\n\r\n```\r\ntanh:           size: 10^2      count: 10000    elapsed: 1.0960958003997803     type: torch.FloatTensor\r\ntanh:           size: 10^3      count: 1000     elapsed: 0.19293761253356934    type: torch.FloatTensor\r\ntanh:           size: 10^4      count: 100      elapsed: 0.5942552089691162     type: torch.FloatTensor\r\ntanh:           size: 10^5      count: 10       elapsed: 0.5254971981048584     type: torch.FloatTensor\r\n```\r\n\r\n**Doesn't affect the performance for doubles ::** \r\n\r\nmaster @ 947155c69ddc114c8f8a69a0f971386fb365bf09\r\n\r\n```\r\n['tanh']\r\ntanh:           size: 10^2      count: 10000    elapsed: 9.550215005874634      type: torch.DoubleTensor\r\ntanh:           size: 10^3      count: 1000     elapsed: 2.3597443103790283     type: torch.DoubleTensor\r\ntanh:           size: 10^4      count: 100      elapsed: 1.9869627952575684     type: torch.DoubleTensor\r\ntanh:           size: 10^5      count: 10       elapsed: 1.657977819442749      type: torch.DoubleTensor\r\n```\r\n\r\nfa02f2a\r\n\r\n```\r\n['tanh']\r\ntanh:           size: 10^2      count: 10000    elapsed: 9.589819431304932      type: torch.DoubleTensor\r\ntanh:           size: 10^3      count: 1000     elapsed: 2.237194299697876      type: torch.DoubleTensor\r\ntanh:           size: 10^4      count: 100      elapsed: 1.8959002494812012     type: torch.DoubleTensor\r\ntanh:           size: 10^5      count: 10       elapsed: 1.7132713794708252     type: torch.DoubleTensor\r\n```\r\n\r\n**Correctness Test::**\r\n```\r\npython test_torch.py TestTorch.test_tanh\r\n\r\n.\r\n----------------------------------------------------------------------\r\nRan 1 test in 0.205s\r\n```\r\n\r\n@cpuhrsch @apaszke @colesbury Please review"}