{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/403636709", "html_url": "https://github.com/pytorch/pytorch/pull/8808#issuecomment-403636709", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8808", "id": 403636709, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMzYzNjcwOQ==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-09T22:11:19Z", "updated_at": "2018-07-09T22:11:19Z", "author_association": "MEMBER", "body_html": "<p>in Convnet models, it's common for the final layer to have a <code>view(batch_size, -1)</code>, like <a href=\"https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L148-L150\">this resnet</a>.</p>\n<p>This shows up in the trace like this:</p>\n<pre><code> %1871 : Float(4, 2048, 1, 1) = aten::avg_pool2d[kernel_size=[7, 7], stride=[1, 1], padding=[0, 0], ceil_mode=0, count_include_pad=1](%1869)\n  %1872 : Long() = aten::size[dim=0](%1871)\n  %1873 : Long() = prim::Constant[value={-1}]()\n  %1874 : Dynamic = aten::stack[dim=0](%1872, %1873)\n  %1875 : Float(4, 2048) = aten::view(%1871, %1874)\n  %1876 : Float(2048!, 1000!) = aten::t(%319)\n  %1877 : Float(4!, 1000) = aten::expand[size=[4, 1000], implicit=1](%320)\n</code></pre>\n<p>Since traces are already specialized on sizes (i.e. we know the size of %1871), it'll be good to fold these instructions in the Constant Folding pass.</p>\n<p>There's a \"just enough done\" pass on doing this by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=246815\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asuhan\">@asuhan</a> at <a href=\"https://github.com/asuhan/pytorch/blob/master/torch/csrc/jit/passes/constant_folding.cpp\">https://github.com/asuhan/pytorch/blob/master/torch/csrc/jit/passes/constant_folding.cpp</a></p>", "body_text": "in Convnet models, it's common for the final layer to have a view(batch_size, -1), like this resnet.\nThis shows up in the trace like this:\n %1871 : Float(4, 2048, 1, 1) = aten::avg_pool2d[kernel_size=[7, 7], stride=[1, 1], padding=[0, 0], ceil_mode=0, count_include_pad=1](%1869)\n  %1872 : Long() = aten::size[dim=0](%1871)\n  %1873 : Long() = prim::Constant[value={-1}]()\n  %1874 : Dynamic = aten::stack[dim=0](%1872, %1873)\n  %1875 : Float(4, 2048) = aten::view(%1871, %1874)\n  %1876 : Float(2048!, 1000!) = aten::t(%319)\n  %1877 : Float(4!, 1000) = aten::expand[size=[4, 1000], implicit=1](%320)\n\nSince traces are already specialized on sizes (i.e. we know the size of %1871), it'll be good to fold these instructions in the Constant Folding pass.\nThere's a \"just enough done\" pass on doing this by @asuhan at https://github.com/asuhan/pytorch/blob/master/torch/csrc/jit/passes/constant_folding.cpp", "body": "in Convnet models, it's common for the final layer to have a `view(batch_size, -1)`, like [this resnet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py#L148-L150).\r\n\r\nThis shows up in the trace like this:\r\n\r\n```\r\n %1871 : Float(4, 2048, 1, 1) = aten::avg_pool2d[kernel_size=[7, 7], stride=[1, 1], padding=[0, 0], ceil_mode=0, count_include_pad=1](%1869)\r\n  %1872 : Long() = aten::size[dim=0](%1871)\r\n  %1873 : Long() = prim::Constant[value={-1}]()\r\n  %1874 : Dynamic = aten::stack[dim=0](%1872, %1873)\r\n  %1875 : Float(4, 2048) = aten::view(%1871, %1874)\r\n  %1876 : Float(2048!, 1000!) = aten::t(%319)\r\n  %1877 : Float(4!, 1000) = aten::expand[size=[4, 1000], implicit=1](%320)\r\n```\r\n\r\nSince traces are already specialized on sizes (i.e. we know the size of %1871), it'll be good to fold these instructions in the Constant Folding pass.\r\n\r\nThere's a \"just enough done\" pass on doing this by @asuhan at https://github.com/asuhan/pytorch/blob/master/torch/csrc/jit/passes/constant_folding.cpp"}