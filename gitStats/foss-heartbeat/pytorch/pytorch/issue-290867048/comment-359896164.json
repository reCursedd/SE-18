{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/359896164", "html_url": "https://github.com/pytorch/pytorch/issues/4801#issuecomment-359896164", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4801", "id": 359896164, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTg5NjE2NA==", "user": {"login": "tomekkorbak", "id": 9259131, "node_id": "MDQ6VXNlcjkyNTkxMzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/9259131?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomekkorbak", "html_url": "https://github.com/tomekkorbak", "followers_url": "https://api.github.com/users/tomekkorbak/followers", "following_url": "https://api.github.com/users/tomekkorbak/following{/other_user}", "gists_url": "https://api.github.com/users/tomekkorbak/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomekkorbak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomekkorbak/subscriptions", "organizations_url": "https://api.github.com/users/tomekkorbak/orgs", "repos_url": "https://api.github.com/users/tomekkorbak/repos", "events_url": "https://api.github.com/users/tomekkorbak/events{/privacy}", "received_events_url": "https://api.github.com/users/tomekkorbak/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-23T19:06:46Z", "updated_at": "2018-01-23T19:06:46Z", "author_association": "NONE", "body_html": "<p>That's totally much needed!</p>\n<p>Some random thoughts:</p>\n<ol>\n<li>One frequently wants to freeze pre-trained word embeddings during training, i.e. does not optimize them (as it may sometimes harm dev set performance if training set covers only a small fraction of word embedding layer vocabulary). A flag or method for setting the weight's <code>requires_grad=False</code> would be handy. Maybe something like <code>is_trainable</code>?</li>\n<li>When using the pre-trained embeddings, arguments <code>num_embedding</code> and <code>embedding_dim</code> can be inferred from the proposed <code>pretrained</code> argument value. Why not mark them as optional then? Or, to keep the keep the interface clean, we may handle the pre-trained embedding case with a classmethod constructor <code>Embedding.load_pretrained(pretrained_embeddings)</code> or a subclass <code>class PretrainedEmbedding(nn.Embedding)</code>.</li>\n<li>torchext provides a super handy wrapper for using pretrained embeddings, namely, <a href=\"https://github.com/pytorch/text/blob/master/torchtext/vocab.py#L98\"><code>torchtext.vocab.Vocab.load_vectors</code></a>. Why not provide a way of populating the embedding layer directly from a torchtext.vocab.Vocab` object?</li>\n</ol>", "body_text": "That's totally much needed!\nSome random thoughts:\n\nOne frequently wants to freeze pre-trained word embeddings during training, i.e. does not optimize them (as it may sometimes harm dev set performance if training set covers only a small fraction of word embedding layer vocabulary). A flag or method for setting the weight's requires_grad=False would be handy. Maybe something like is_trainable?\nWhen using the pre-trained embeddings, arguments num_embedding and embedding_dim can be inferred from the proposed pretrained argument value. Why not mark them as optional then? Or, to keep the keep the interface clean, we may handle the pre-trained embedding case with a classmethod constructor Embedding.load_pretrained(pretrained_embeddings) or a subclass class PretrainedEmbedding(nn.Embedding).\ntorchext provides a super handy wrapper for using pretrained embeddings, namely, torchtext.vocab.Vocab.load_vectors. Why not provide a way of populating the embedding layer directly from a torchtext.vocab.Vocab` object?", "body": "That's totally much needed!\r\n\r\nSome random thoughts:\r\n1. One frequently wants to freeze pre-trained word embeddings during training, i.e. does not optimize them (as it may sometimes harm dev set performance if training set covers only a small fraction of word embedding layer vocabulary). A flag or method for setting the weight's `requires_grad=False` would be handy. Maybe something like `is_trainable`?\r\n2. When using the pre-trained embeddings, arguments `num_embedding` and `embedding_dim` can be inferred from the proposed `pretrained` argument value. Why not mark them as optional then? Or, to keep the keep the interface clean, we may handle the pre-trained embedding case with a classmethod constructor `Embedding.load_pretrained(pretrained_embeddings)` or a subclass `class PretrainedEmbedding(nn.Embedding)`.\r\n2. torchext provides a super handy wrapper for using pretrained embeddings, namely, [`torchtext.vocab.Vocab.load_vectors`](https://github.com/pytorch/text/blob/master/torchtext/vocab.py#L98). Why not provide a way of populating the embedding layer directly from a torchtext.vocab.Vocab` object?"}