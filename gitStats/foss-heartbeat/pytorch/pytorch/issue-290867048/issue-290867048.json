{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4801", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4801/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4801/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4801/events", "html_url": "https://github.com/pytorch/pytorch/issues/4801", "id": 290867048, "node_id": "MDU6SXNzdWUyOTA4NjcwNDg=", "number": 4801, "title": "[feature proposal] An effortless way of loading pre-trained embeddings", "user": {"login": "miguelvr", "id": 7456627, "node_id": "MDQ6VXNlcjc0NTY2Mjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/7456627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/miguelvr", "html_url": "https://github.com/miguelvr", "followers_url": "https://api.github.com/users/miguelvr/followers", "following_url": "https://api.github.com/users/miguelvr/following{/other_user}", "gists_url": "https://api.github.com/users/miguelvr/gists{/gist_id}", "starred_url": "https://api.github.com/users/miguelvr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/miguelvr/subscriptions", "organizations_url": "https://api.github.com/users/miguelvr/orgs", "repos_url": "https://api.github.com/users/miguelvr/repos", "events_url": "https://api.github.com/users/miguelvr/events{/privacy}", "received_events_url": "https://api.github.com/users/miguelvr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-01-23T14:54:58Z", "updated_at": "2018-09-21T08:33:15Z", "closed_at": "2018-09-21T08:33:15Z", "author_association": "NONE", "body_html": "<h3>Summary</h3>\n<p>Loading pre-trained embeddings is common practice with today's Embeddings. Most often than not, people use large pre-trained Embeddings such as Word2Vec, Glove or FastText with their models. As such there should be an easy and simple way of doing this so common operation.</p>\n<h3>Current Behaviour</h3>\n<p>To load a pre-trained embedding one has to do the following:</p>\n<pre><code>e = nn.Embedding(num_embedding, embedding_dim)\ne.weight = nn.Parameter(pretrained_embedding)\n</code></pre>\n<p>This involves converting a <code>Tensor</code> to <code>Variable</code> and access the class' arguments to override them.</p>\n<h3>Suggested Behaviour</h3>\n<p>I propose doing this either with an explicit method, or an optional argument in the <code>__init__</code> function.</p>\n<p>This way you could either initialize an embedding as:</p>\n<pre><code>e = nn.Embedding(num_embedding, embedding_dim, pretrained=pretrained_embedding)\n</code></pre>\n<p>or</p>\n<pre><code>e = nn.Embedding(num_embedding, embedding_dim)\ne.load_pretrained(pretrained_embedding)\n</code></pre>\n<hr>\n<p>Let me know what you think. I'll do a PR as soon I have some feedback from you guys.</p>\n<p>Thank you</p>", "body_text": "Summary\nLoading pre-trained embeddings is common practice with today's Embeddings. Most often than not, people use large pre-trained Embeddings such as Word2Vec, Glove or FastText with their models. As such there should be an easy and simple way of doing this so common operation.\nCurrent Behaviour\nTo load a pre-trained embedding one has to do the following:\ne = nn.Embedding(num_embedding, embedding_dim)\ne.weight = nn.Parameter(pretrained_embedding)\n\nThis involves converting a Tensor to Variable and access the class' arguments to override them.\nSuggested Behaviour\nI propose doing this either with an explicit method, or an optional argument in the __init__ function.\nThis way you could either initialize an embedding as:\ne = nn.Embedding(num_embedding, embedding_dim, pretrained=pretrained_embedding)\n\nor\ne = nn.Embedding(num_embedding, embedding_dim)\ne.load_pretrained(pretrained_embedding)\n\n\nLet me know what you think. I'll do a PR as soon I have some feedback from you guys.\nThank you", "body": "### Summary\r\n\r\nLoading pre-trained embeddings is common practice with today's Embeddings. Most often than not, people use large pre-trained Embeddings such as Word2Vec, Glove or FastText with their models. As such there should be an easy and simple way of doing this so common operation.\r\n\r\n### Current Behaviour\r\n\r\nTo load a pre-trained embedding one has to do the following:\r\n\r\n    e = nn.Embedding(num_embedding, embedding_dim)\r\n    e.weight = nn.Parameter(pretrained_embedding)\r\n\r\nThis involves converting a `Tensor` to `Variable` and access the class' arguments to override them.\r\n\r\n### Suggested Behaviour\r\n\r\nI propose doing this either with an explicit method, or an optional argument in the `__init__` function.\r\n\r\nThis way you could either initialize an embedding as:\r\n\r\n    e = nn.Embedding(num_embedding, embedding_dim, pretrained=pretrained_embedding)\r\n\r\nor\r\n\r\n    e = nn.Embedding(num_embedding, embedding_dim)\r\n    e.load_pretrained(pretrained_embedding)\r\n\r\n--------------\r\n\r\nLet me know what you think. I'll do a PR as soon I have some feedback from you guys.\r\n\r\nThank you\r\n\r\n\r\n"}