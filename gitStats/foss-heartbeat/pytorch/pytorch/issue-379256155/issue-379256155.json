{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13778", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13778/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13778/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13778/events", "html_url": "https://github.com/pytorch/pytorch/issues/13778", "id": 379256155, "node_id": "MDU6SXNzdWUzNzkyNTYxNTU=", "number": 13778, "title": "on new install got out of memory or access violations torch.cuda.empty_cache() and others", "user": {"login": "tyoc213", "id": 506234, "node_id": "MDQ6VXNlcjUwNjIzNA==", "avatar_url": "https://avatars3.githubusercontent.com/u/506234?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tyoc213", "html_url": "https://github.com/tyoc213", "followers_url": "https://api.github.com/users/tyoc213/followers", "following_url": "https://api.github.com/users/tyoc213/following{/other_user}", "gists_url": "https://api.github.com/users/tyoc213/gists{/gist_id}", "starred_url": "https://api.github.com/users/tyoc213/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tyoc213/subscriptions", "organizations_url": "https://api.github.com/users/tyoc213/orgs", "repos_url": "https://api.github.com/users/tyoc213/repos", "events_url": "https://api.github.com/users/tyoc213/events{/privacy}", "received_events_url": "https://api.github.com/users/tyoc213/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-11-09T17:37:11Z", "updated_at": "2018-11-16T04:07:14Z", "closed_at": "2018-11-16T04:07:14Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>On a new install with this spec</p>\n<pre lang=\"text\"><code>=== Software === \npython version  : 3.7.0\nfastai version  : 1.0.20.dev0\ntorch version   : 1.0.0.dev20181105\nnvidia driver   : 410.73\ntorch cuda ver  : 9.2.148\ntorch cuda is   : available\ntorch cudnn ver : 7104\ntorch cudnn is  : enabled\n\n=== Hardware === \nnvidia gpus     : 1\ntorch available : 1\n  - gpu0        : 7949MB | GeForce RTX 2080\n\n=== Environment === \nplatform        : Linux-4.18.0-10-generic-x86_64-with-debian-buster-sid\ndistro          : Ubuntu 18.10 Cosmic Cuttlefish\nconda env       : base\npython          : /home/tyoc213/anaconda3/bin/python\nsys.path        : \n/home/tyoc213/fastai/examples\n/home/tyoc213/anaconda3/lib/python37.zip\n/home/tyoc213/anaconda3/lib/python3.7\n/home/tyoc213/anaconda3/lib/python3.7/lib-dynload\n/home/tyoc213/anaconda3/lib/python3.7/site-packages\n/home/tyoc213/fastai\n/home/tyoc213/anaconda3/lib/python3.7/site-packages/IPython/extensions\n/home/tyoc213/.ipython\n</code></pre>\n<p>I get memory errors almost all times.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<p>Install fastai library as  <a href=\"https://github.com/fastai/fastai/blob/master/README.md#installation\">https://github.com/fastai/fastai/blob/master/README.md#installation</a> and on a new jupyter nootebook get the cuda available = True and then try to call empty  (to see if it frees something because the out of memory).</p>\n<p>I have some problems running the examples provided in fastai lib so I posted on their forum. But after searching here for a solution , I found <code>torch.cuda.empty_cache()</code> but still I get the memory error... so that is why Im comming here</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-8-cc9249814530&gt; in &lt;module&gt;()\n      1 torch.cuda.is_available()\n----&gt; 2 torch.cuda.empty_cache()\n\n~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py in empty_cache()\n    372     \"\"\"\n    373     if _initialized:\n--&gt; 374         torch._C._cuda_emptyCache()\n    375 \n    376 \n\nRuntimeError: CUDA error: an illegal memory access was encountered\n</code></pre>\n<p>Or try to run the different examples provided there <code>collab.ipynb</code> works OK but stepping on cyfar on <code>fastai/examples</code> I an error executing this line</p>\n<pre><code>learn = Learner(data, wrn_22(), metrics=accuracy).to_fp16()\nlearn.fit_one_cycle(30, 3e-3, wd=0.4, div_factor=10, pct_start=0.5)\n</code></pre>\n<p>I get this output</p>\n<pre lang=\"log\"><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-14-72f1e2b0093b&gt; in &lt;module&gt;()\n----&gt; 1 learn = Learner(data, wrn_22(), metrics=accuracy).to_fp16()\n      2 learn.fit_one_cycle(30, 3e-3, wd=0.4, div_factor=10, pct_start=0.5)\n\n&lt;string&gt; in __init__(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups)\n\n~/fastai/fastai/basic_train.py in __post_init__(self)\n    136         self.path = Path(ifnone(self.path, self.data.path))\n    137         (self.path/self.model_dir).mkdir(parents=True, exist_ok=True)\n--&gt; 138         self.model = self.model.to(self.data.device)\n    139         self.loss_func = ifnone(self.loss_func, self.data.loss_func)\n    140         self.metrics=listify(self.metrics)\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)\n    377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    378 \n--&gt; 379         return self._apply(convert)\n    380 \n    381     def register_backward_hook(self, hook):\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--&gt; 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--&gt; 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    189                 # Tensors stored in modules are graph leaves, and we don't\n    190                 # want to create copy nodes, so we have to unpack the data.\n--&gt; 191                 param.data = fn(param.data)\n    192                 if param._grad is not None:\n    193                     param._grad.data = fn(param._grad.data)\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in convert(t)\n    375 \n    376         def convert(t):\n--&gt; 377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    378 \n    379         return self._apply(convert)\n\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/aten/src/THC/generic/THCTensorCopy.cpp:20\n</code></pre>\n<p><code> torch.cuda.is_available()</code> return <code>True</code>.</p>\n<p>Im also running out of memory in <code>dogs_cats.ipynb</code>.</p>\n<div class=\"highlight highlight-source-python\"><pre>learn <span class=\"pl-k\">=</span> create_cnn(data, models.resnet34, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>accuracy)\n\nlearn.fit_one_cycle(<span class=\"pl-c1\">1</span>)</pre></div>\n<pre lang=\"log\"><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-9-6ec085df1eed&gt; in &lt;module&gt;()\n----&gt; 1 learn = create_cnn(data, models.resnet34, metrics=accuracy)\n      2 learn.fit_one_cycle(1)\n\n~/fastai/fastai/vision/learner.py in create_cnn(data, arch, cut, pretrained, lin_ftrs, ps, custom_head, split_on, classification, **kwargs)\n     67     learn.split(ifnone(split_on,meta['split']))\n     68     if pretrained: learn.freeze()\n---&gt; 69     apply_init(model[1], nn.init.kaiming_normal_)\n     70     return learn\n     71 \n\n~/fastai/fastai/torch_core.py in apply_init(m, init_func)\n    193 def apply_init(m, init_func:LayerFunc):\n    194     \"Initialize all non-batchnorm layers of `m` with `init_func`.\"\n--&gt; 195     apply_leaf(m, partial(cond_init, init_func=init_func))\n    196 \n    197 def in_channels(m:nn.Module) -&gt; List[int]:\n\n~/fastai/fastai/torch_core.py in apply_leaf(m, f)\n    189     c = children(m)\n    190     if isinstance(m, nn.Module): f(m)\n--&gt; 191     for l in c: apply_leaf(l,f)\n    192 \n    193 def apply_init(m, init_func:LayerFunc):\n\n~/fastai/fastai/torch_core.py in apply_leaf(m, f)\n    188     \"Apply `f` to children of `m`.\"\n    189     c = children(m)\n--&gt; 190     if isinstance(m, nn.Module): f(m)\n    191     for l in c: apply_leaf(l,f)\n    192 \n\n~/fastai/fastai/torch_core.py in cond_init(m, init_func)\n    183     if (not isinstance(m, bn_types)) and requires_grad(m):\n    184         if hasattr(m, 'weight'): init_func(m.weight)\n--&gt; 185         if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\n    186 \n    187 def apply_leaf(m:nn.Module, f:LayerFunc):\n\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/aten/src/THC/generic/THCTensorMath.cu:14\n</code></pre>\n<p>I get the cuda memory error also in tabular</p>\n<div class=\"highlight highlight-source-python\"><pre>learn <span class=\"pl-k\">=</span> get_tabular_learner(data, <span class=\"pl-v\">layers</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">200</span>,<span class=\"pl-c1\">100</span>], <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>accuracy)\nlearn.fit(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1e-2</span>)</pre></div>\n<p>output</p>\n<pre lang=\"log\"><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-5-480eb9caae1a&gt; in &lt;module&gt;()\n----&gt; 1 learn = get_tabular_learner(data, layers=[200,100], metrics=accuracy)\n      2 learn.fit(1, 1e-2)\n\n~/fastai/fastai/tabular/data.py in get_tabular_learner(data, layers, emb_szs, metrics, ps, emb_drop, y_range, use_bn, **kwargs)\n     93     model = TabularModel(emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\n     94                          y_range=y_range, use_bn=use_bn)\n---&gt; 95     return Learner(data, model, metrics=metrics, **kwargs)\n     96 \n\n&lt;string&gt; in __init__(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups)\n\n~/fastai/fastai/basic_train.py in __post_init__(self)\n    136         self.path = Path(ifnone(self.path, self.data.path))\n    137         (self.path/self.model_dir).mkdir(parents=True, exist_ok=True)\n--&gt; 138         self.model = self.model.to(self.data.device)\n    139         self.loss_func = ifnone(self.loss_func, self.data.loss_func)\n    140         self.metrics=listify(self.metrics)\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)\n    377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    378 \n--&gt; 379         return self._apply(convert)\n    380 \n    381     def register_backward_hook(self, hook):\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--&gt; 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--&gt; 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    189                 # Tensors stored in modules are graph leaves, and we don't\n    190                 # want to create copy nodes, so we have to unpack the data.\n--&gt; 191                 param.data = fn(param.data)\n    192                 if param._grad is not None:\n    193                     param._grad.data = fn(param._grad.data)\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in convert(t)\n    375 \n    376         def convert(t):\n--&gt; 377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    378 \n    379         return self._apply(convert)\n\nRuntimeError: CUDA error: out of memory\n</code></pre>\n<h2>Expected behavior</h2>\n<p>Examples to work</p>\n<h2>Environment</h2>\n<p>********* UPDATE *********</p>\n<p>python collect_env.py<br>\nCollecting environment information...<br>\nPyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.2.148</p>\n<p>OS: Ubuntu 18.10<br>\nGCC version: (Ubuntu 8.2.0-7ubuntu1) 8.2.0<br>\nCMake version: version 3.12.2</p>\n<p>Python version: 3.7<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: Could not collect<br>\nGPU models and configuration: GPU 0: GeForce RTX 2080<br>\nNvidia driver version: 410.73<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip] Could not collect<br>\n[conda] cuda92                    1.0                           0    pytorch<br>\n[conda] pytorch                   0.4.1           py37_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch<br>\n[conda] torchvision               0.2.1                    py37_1    pytorch<br>\n[conda] torchvision-nightly       0.2.1                     </p>", "body_text": "\ud83d\udc1b Bug\nOn a new install with this spec\n=== Software === \npython version  : 3.7.0\nfastai version  : 1.0.20.dev0\ntorch version   : 1.0.0.dev20181105\nnvidia driver   : 410.73\ntorch cuda ver  : 9.2.148\ntorch cuda is   : available\ntorch cudnn ver : 7104\ntorch cudnn is  : enabled\n\n=== Hardware === \nnvidia gpus     : 1\ntorch available : 1\n  - gpu0        : 7949MB | GeForce RTX 2080\n\n=== Environment === \nplatform        : Linux-4.18.0-10-generic-x86_64-with-debian-buster-sid\ndistro          : Ubuntu 18.10 Cosmic Cuttlefish\nconda env       : base\npython          : /home/tyoc213/anaconda3/bin/python\nsys.path        : \n/home/tyoc213/fastai/examples\n/home/tyoc213/anaconda3/lib/python37.zip\n/home/tyoc213/anaconda3/lib/python3.7\n/home/tyoc213/anaconda3/lib/python3.7/lib-dynload\n/home/tyoc213/anaconda3/lib/python3.7/site-packages\n/home/tyoc213/fastai\n/home/tyoc213/anaconda3/lib/python3.7/site-packages/IPython/extensions\n/home/tyoc213/.ipython\n\nI get memory errors almost all times.\nTo Reproduce\nSteps to reproduce the behavior:\nInstall fastai library as  https://github.com/fastai/fastai/blob/master/README.md#installation and on a new jupyter nootebook get the cuda available = True and then try to call empty  (to see if it frees something because the out of memory).\nI have some problems running the examples provided in fastai lib so I posted on their forum. But after searching here for a solution , I found torch.cuda.empty_cache() but still I get the memory error... so that is why Im comming here\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-8-cc9249814530> in <module>()\n      1 torch.cuda.is_available()\n----> 2 torch.cuda.empty_cache()\n\n~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py in empty_cache()\n    372     \"\"\"\n    373     if _initialized:\n--> 374         torch._C._cuda_emptyCache()\n    375 \n    376 \n\nRuntimeError: CUDA error: an illegal memory access was encountered\n\nOr try to run the different examples provided there collab.ipynb works OK but stepping on cyfar on fastai/examples I an error executing this line\nlearn = Learner(data, wrn_22(), metrics=accuracy).to_fp16()\nlearn.fit_one_cycle(30, 3e-3, wd=0.4, div_factor=10, pct_start=0.5)\n\nI get this output\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-14-72f1e2b0093b> in <module>()\n----> 1 learn = Learner(data, wrn_22(), metrics=accuracy).to_fp16()\n      2 learn.fit_one_cycle(30, 3e-3, wd=0.4, div_factor=10, pct_start=0.5)\n\n<string> in __init__(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups)\n\n~/fastai/fastai/basic_train.py in __post_init__(self)\n    136         self.path = Path(ifnone(self.path, self.data.path))\n    137         (self.path/self.model_dir).mkdir(parents=True, exist_ok=True)\n--> 138         self.model = self.model.to(self.data.device)\n    139         self.loss_func = ifnone(self.loss_func, self.data.loss_func)\n    140         self.metrics=listify(self.metrics)\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)\n    377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    378 \n--> 379         return self._apply(convert)\n    380 \n    381     def register_backward_hook(self, hook):\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--> 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--> 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    189                 # Tensors stored in modules are graph leaves, and we don't\n    190                 # want to create copy nodes, so we have to unpack the data.\n--> 191                 param.data = fn(param.data)\n    192                 if param._grad is not None:\n    193                     param._grad.data = fn(param._grad.data)\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in convert(t)\n    375 \n    376         def convert(t):\n--> 377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    378 \n    379         return self._apply(convert)\n\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/aten/src/THC/generic/THCTensorCopy.cpp:20\n\n torch.cuda.is_available() return True.\nIm also running out of memory in dogs_cats.ipynb.\nlearn = create_cnn(data, models.resnet34, metrics=accuracy)\n\nlearn.fit_one_cycle(1)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-9-6ec085df1eed> in <module>()\n----> 1 learn = create_cnn(data, models.resnet34, metrics=accuracy)\n      2 learn.fit_one_cycle(1)\n\n~/fastai/fastai/vision/learner.py in create_cnn(data, arch, cut, pretrained, lin_ftrs, ps, custom_head, split_on, classification, **kwargs)\n     67     learn.split(ifnone(split_on,meta['split']))\n     68     if pretrained: learn.freeze()\n---> 69     apply_init(model[1], nn.init.kaiming_normal_)\n     70     return learn\n     71 \n\n~/fastai/fastai/torch_core.py in apply_init(m, init_func)\n    193 def apply_init(m, init_func:LayerFunc):\n    194     \"Initialize all non-batchnorm layers of `m` with `init_func`.\"\n--> 195     apply_leaf(m, partial(cond_init, init_func=init_func))\n    196 \n    197 def in_channels(m:nn.Module) -> List[int]:\n\n~/fastai/fastai/torch_core.py in apply_leaf(m, f)\n    189     c = children(m)\n    190     if isinstance(m, nn.Module): f(m)\n--> 191     for l in c: apply_leaf(l,f)\n    192 \n    193 def apply_init(m, init_func:LayerFunc):\n\n~/fastai/fastai/torch_core.py in apply_leaf(m, f)\n    188     \"Apply `f` to children of `m`.\"\n    189     c = children(m)\n--> 190     if isinstance(m, nn.Module): f(m)\n    191     for l in c: apply_leaf(l,f)\n    192 \n\n~/fastai/fastai/torch_core.py in cond_init(m, init_func)\n    183     if (not isinstance(m, bn_types)) and requires_grad(m):\n    184         if hasattr(m, 'weight'): init_func(m.weight)\n--> 185         if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\n    186 \n    187 def apply_leaf(m:nn.Module, f:LayerFunc):\n\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/aten/src/THC/generic/THCTensorMath.cu:14\n\nI get the cuda memory error also in tabular\nlearn = get_tabular_learner(data, layers=[200,100], metrics=accuracy)\nlearn.fit(1, 1e-2)\noutput\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-480eb9caae1a> in <module>()\n----> 1 learn = get_tabular_learner(data, layers=[200,100], metrics=accuracy)\n      2 learn.fit(1, 1e-2)\n\n~/fastai/fastai/tabular/data.py in get_tabular_learner(data, layers, emb_szs, metrics, ps, emb_drop, y_range, use_bn, **kwargs)\n     93     model = TabularModel(emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\n     94                          y_range=y_range, use_bn=use_bn)\n---> 95     return Learner(data, model, metrics=metrics, **kwargs)\n     96 \n\n<string> in __init__(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups)\n\n~/fastai/fastai/basic_train.py in __post_init__(self)\n    136         self.path = Path(ifnone(self.path, self.data.path))\n    137         (self.path/self.model_dir).mkdir(parents=True, exist_ok=True)\n--> 138         self.model = self.model.to(self.data.device)\n    139         self.loss_func = ifnone(self.loss_func, self.data.loss_func)\n    140         self.metrics=listify(self.metrics)\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)\n    377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    378 \n--> 379         return self._apply(convert)\n    380 \n    381     def register_backward_hook(self, hook):\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--> 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    183     def _apply(self, fn):\n    184         for module in self.children():\n--> 185             module._apply(fn)\n    186 \n    187         for param in self._parameters.values():\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\n    189                 # Tensors stored in modules are graph leaves, and we don't\n    190                 # want to create copy nodes, so we have to unpack the data.\n--> 191                 param.data = fn(param.data)\n    192                 if param._grad is not None:\n    193                     param._grad.data = fn(param._grad.data)\n\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in convert(t)\n    375 \n    376         def convert(t):\n--> 377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    378 \n    379         return self._apply(convert)\n\nRuntimeError: CUDA error: out of memory\n\nExpected behavior\nExamples to work\nEnvironment\n********* UPDATE *********\npython collect_env.py\nCollecting environment information...\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\nOS: Ubuntu 18.10\nGCC version: (Ubuntu 8.2.0-7ubuntu1) 8.2.0\nCMake version: version 3.12.2\nPython version: 3.7\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: GPU 0: GeForce RTX 2080\nNvidia driver version: 410.73\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] cuda92                    1.0                           0    pytorch\n[conda] pytorch                   0.4.1           py37_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch\n[conda] torchvision               0.2.1                    py37_1    pytorch\n[conda] torchvision-nightly       0.2.1", "body": "## \ud83d\udc1b Bug\r\n\r\nOn a new install with this spec\r\n\r\n```text\r\n=== Software === \r\npython version  : 3.7.0\r\nfastai version  : 1.0.20.dev0\r\ntorch version   : 1.0.0.dev20181105\r\nnvidia driver   : 410.73\r\ntorch cuda ver  : 9.2.148\r\ntorch cuda is   : available\r\ntorch cudnn ver : 7104\r\ntorch cudnn is  : enabled\r\n\r\n=== Hardware === \r\nnvidia gpus     : 1\r\ntorch available : 1\r\n  - gpu0        : 7949MB | GeForce RTX 2080\r\n\r\n=== Environment === \r\nplatform        : Linux-4.18.0-10-generic-x86_64-with-debian-buster-sid\r\ndistro          : Ubuntu 18.10 Cosmic Cuttlefish\r\nconda env       : base\r\npython          : /home/tyoc213/anaconda3/bin/python\r\nsys.path        : \r\n/home/tyoc213/fastai/examples\r\n/home/tyoc213/anaconda3/lib/python37.zip\r\n/home/tyoc213/anaconda3/lib/python3.7\r\n/home/tyoc213/anaconda3/lib/python3.7/lib-dynload\r\n/home/tyoc213/anaconda3/lib/python3.7/site-packages\r\n/home/tyoc213/fastai\r\n/home/tyoc213/anaconda3/lib/python3.7/site-packages/IPython/extensions\r\n/home/tyoc213/.ipython\r\n```\r\n\r\nI get memory errors almost all times.\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\nInstall fastai library as  https://github.com/fastai/fastai/blob/master/README.md#installation and on a new jupyter nootebook get the cuda available = True and then try to call empty  (to see if it frees something because the out of memory).\r\n\r\nI have some problems running the examples provided in fastai lib so I posted on their forum. But after searching here for a solution , I found `torch.cuda.empty_cache()` but still I get the memory error... so that is why Im comming here\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-8-cc9249814530> in <module>()\r\n      1 torch.cuda.is_available()\r\n----> 2 torch.cuda.empty_cache()\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py in empty_cache()\r\n    372     \"\"\"\r\n    373     if _initialized:\r\n--> 374         torch._C._cuda_emptyCache()\r\n    375 \r\n    376 \r\n\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\n```\r\n\r\nOr try to run the different examples provided there `collab.ipynb` works OK but stepping on cyfar on `fastai/examples` I an error executing this line\r\n\r\n```\r\nlearn = Learner(data, wrn_22(), metrics=accuracy).to_fp16()\r\nlearn.fit_one_cycle(30, 3e-3, wd=0.4, div_factor=10, pct_start=0.5)\r\n```\r\n\r\nI get this output\r\n\r\n```log\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-14-72f1e2b0093b> in <module>()\r\n----> 1 learn = Learner(data, wrn_22(), metrics=accuracy).to_fp16()\r\n      2 learn.fit_one_cycle(30, 3e-3, wd=0.4, div_factor=10, pct_start=0.5)\r\n\r\n<string> in __init__(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups)\r\n\r\n~/fastai/fastai/basic_train.py in __post_init__(self)\r\n    136         self.path = Path(ifnone(self.path, self.data.path))\r\n    137         (self.path/self.model_dir).mkdir(parents=True, exist_ok=True)\r\n--> 138         self.model = self.model.to(self.data.device)\r\n    139         self.loss_func = ifnone(self.loss_func, self.data.loss_func)\r\n    140         self.metrics=listify(self.metrics)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)\r\n    377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\r\n    378 \r\n--> 379         return self._apply(convert)\r\n    380 \r\n    381     def register_backward_hook(self, hook):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    183     def _apply(self, fn):\r\n    184         for module in self.children():\r\n--> 185             module._apply(fn)\r\n    186 \r\n    187         for param in self._parameters.values():\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    183     def _apply(self, fn):\r\n    184         for module in self.children():\r\n--> 185             module._apply(fn)\r\n    186 \r\n    187         for param in self._parameters.values():\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    189                 # Tensors stored in modules are graph leaves, and we don't\r\n    190                 # want to create copy nodes, so we have to unpack the data.\r\n--> 191                 param.data = fn(param.data)\r\n    192                 if param._grad is not None:\r\n    193                     param._grad.data = fn(param._grad.data)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in convert(t)\r\n    375 \r\n    376         def convert(t):\r\n--> 377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\r\n    378 \r\n    379         return self._apply(convert)\r\n\r\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/aten/src/THC/generic/THCTensorCopy.cpp:20\r\n```\r\n\r\n` torch.cuda.is_available()` return `True`.\r\n\r\n\r\nIm also running out of memory in `dogs_cats.ipynb`.\r\n\r\n\r\n\r\n```python\r\nlearn = create_cnn(data, models.resnet34, metrics=accuracy)\r\n\r\nlearn.fit_one_cycle(1)\r\n```\r\n\r\n```log\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-9-6ec085df1eed> in <module>()\r\n----> 1 learn = create_cnn(data, models.resnet34, metrics=accuracy)\r\n      2 learn.fit_one_cycle(1)\r\n\r\n~/fastai/fastai/vision/learner.py in create_cnn(data, arch, cut, pretrained, lin_ftrs, ps, custom_head, split_on, classification, **kwargs)\r\n     67     learn.split(ifnone(split_on,meta['split']))\r\n     68     if pretrained: learn.freeze()\r\n---> 69     apply_init(model[1], nn.init.kaiming_normal_)\r\n     70     return learn\r\n     71 \r\n\r\n~/fastai/fastai/torch_core.py in apply_init(m, init_func)\r\n    193 def apply_init(m, init_func:LayerFunc):\r\n    194     \"Initialize all non-batchnorm layers of `m` with `init_func`.\"\r\n--> 195     apply_leaf(m, partial(cond_init, init_func=init_func))\r\n    196 \r\n    197 def in_channels(m:nn.Module) -> List[int]:\r\n\r\n~/fastai/fastai/torch_core.py in apply_leaf(m, f)\r\n    189     c = children(m)\r\n    190     if isinstance(m, nn.Module): f(m)\r\n--> 191     for l in c: apply_leaf(l,f)\r\n    192 \r\n    193 def apply_init(m, init_func:LayerFunc):\r\n\r\n~/fastai/fastai/torch_core.py in apply_leaf(m, f)\r\n    188     \"Apply `f` to children of `m`.\"\r\n    189     c = children(m)\r\n--> 190     if isinstance(m, nn.Module): f(m)\r\n    191     for l in c: apply_leaf(l,f)\r\n    192 \r\n\r\n~/fastai/fastai/torch_core.py in cond_init(m, init_func)\r\n    183     if (not isinstance(m, bn_types)) and requires_grad(m):\r\n    184         if hasattr(m, 'weight'): init_func(m.weight)\r\n--> 185         if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\r\n    186 \r\n    187 def apply_leaf(m:nn.Module, f:LayerFunc):\r\n\r\nRuntimeError: cuda runtime error (2) : out of memory at /opt/conda/conda-bld/pytorch-nightly_1541411195070/work/aten/src/THC/generic/THCTensorMath.cu:14\r\n```\r\n\r\nI get the cuda memory error also in tabular\r\n\r\n```python\r\nlearn = get_tabular_learner(data, layers=[200,100], metrics=accuracy)\r\nlearn.fit(1, 1e-2)\r\n```\r\n\r\noutput\r\n\r\n\r\n```log\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-5-480eb9caae1a> in <module>()\r\n----> 1 learn = get_tabular_learner(data, layers=[200,100], metrics=accuracy)\r\n      2 learn.fit(1, 1e-2)\r\n\r\n~/fastai/fastai/tabular/data.py in get_tabular_learner(data, layers, emb_szs, metrics, ps, emb_drop, y_range, use_bn, **kwargs)\r\n     93     model = TabularModel(emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\r\n     94                          y_range=y_range, use_bn=use_bn)\r\n---> 95     return Learner(data, model, metrics=metrics, **kwargs)\r\n     96 \r\n\r\n<string> in __init__(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups)\r\n\r\n~/fastai/fastai/basic_train.py in __post_init__(self)\r\n    136         self.path = Path(ifnone(self.path, self.data.path))\r\n    137         (self.path/self.model_dir).mkdir(parents=True, exist_ok=True)\r\n--> 138         self.model = self.model.to(self.data.device)\r\n    139         self.loss_func = ifnone(self.loss_func, self.data.loss_func)\r\n    140         self.metrics=listify(self.metrics)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in to(self, *args, **kwargs)\r\n    377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\r\n    378 \r\n--> 379         return self._apply(convert)\r\n    380 \r\n    381     def register_backward_hook(self, hook):\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    183     def _apply(self, fn):\r\n    184         for module in self.children():\r\n--> 185             module._apply(fn)\r\n    186 \r\n    187         for param in self._parameters.values():\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    183     def _apply(self, fn):\r\n    184         for module in self.children():\r\n--> 185             module._apply(fn)\r\n    186 \r\n    187         for param in self._parameters.values():\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)\r\n    189                 # Tensors stored in modules are graph leaves, and we don't\r\n    190                 # want to create copy nodes, so we have to unpack the data.\r\n--> 191                 param.data = fn(param.data)\r\n    192                 if param._grad is not None:\r\n    193                     param._grad.data = fn(param._grad.data)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in convert(t)\r\n    375 \r\n    376         def convert(t):\r\n--> 377             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\r\n    378 \r\n    379         return self._apply(convert)\r\n\r\nRuntimeError: CUDA error: out of memory\r\n```\r\n\r\n## Expected behavior\r\n\r\nExamples to work\r\n\r\n## Environment\r\n\r\n********* UPDATE *********\r\n\r\n python collect_env.py\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Ubuntu 18.10\r\nGCC version: (Ubuntu 8.2.0-7ubuntu1) 8.2.0\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: GPU 0: GeForce RTX 2080\r\nNvidia driver version: 410.73\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] pytorch                   0.4.1           py37_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch\r\n[conda] torchvision               0.2.1                    py37_1    pytorch\r\n[conda] torchvision-nightly       0.2.1                     <pip>\r\n"}