{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/341291968", "html_url": "https://github.com/pytorch/pytorch/issues/1355#issuecomment-341291968", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1355", "id": 341291968, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTI5MTk2OA==", "user": {"login": "jph00", "id": 346999, "node_id": "MDQ6VXNlcjM0Njk5OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/346999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jph00", "html_url": "https://github.com/jph00", "followers_url": "https://api.github.com/users/jph00/followers", "following_url": "https://api.github.com/users/jph00/following{/other_user}", "gists_url": "https://api.github.com/users/jph00/gists{/gist_id}", "starred_url": "https://api.github.com/users/jph00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jph00/subscriptions", "organizations_url": "https://api.github.com/users/jph00/orgs", "repos_url": "https://api.github.com/users/jph00/repos", "events_url": "https://api.github.com/users/jph00/events{/privacy}", "received_events_url": "https://api.github.com/users/jph00/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-02T01:32:36Z", "updated_at": "2017-11-02T01:33:29Z", "author_association": "NONE", "body_html": "<p>OK so I've figured out the basic issue, which is that opencv and Pytorch multiprocessing don't play well together, sometimes. No problems on our box at university, but lots of problems on AWS (on the new deep learning CUDA 9 AMI with P2 instance). Adding locking around all cv2 calls doesn't fix it, and adding <code>cv2.setNumThreads(0)</code> doesn't fix. This seems to fix it:</p>\n<pre><code>from multiprocessing import set_start_method\nset_start_method('spawn')\n</code></pre>\n<p>However that impacts performance by about 15%. The recommendation in the opencv github issue is to use <a href=\"https://github.com/tomMoral/loky\">https://github.com/tomMoral/loky</a> . I've used that module before and found it rock-solid. Not urgent, since we've got a solution that works well enough for now - but might be worth considering using Loky for Dataloader?</p>\n<p>Perhaps more importantly, it would be nice if at least there was some kind of timeout in pytorch's queue so that these infinite hangs would get caught.</p>", "body_text": "OK so I've figured out the basic issue, which is that opencv and Pytorch multiprocessing don't play well together, sometimes. No problems on our box at university, but lots of problems on AWS (on the new deep learning CUDA 9 AMI with P2 instance). Adding locking around all cv2 calls doesn't fix it, and adding cv2.setNumThreads(0) doesn't fix. This seems to fix it:\nfrom multiprocessing import set_start_method\nset_start_method('spawn')\n\nHowever that impacts performance by about 15%. The recommendation in the opencv github issue is to use https://github.com/tomMoral/loky . I've used that module before and found it rock-solid. Not urgent, since we've got a solution that works well enough for now - but might be worth considering using Loky for Dataloader?\nPerhaps more importantly, it would be nice if at least there was some kind of timeout in pytorch's queue so that these infinite hangs would get caught.", "body": "OK so I've figured out the basic issue, which is that opencv and Pytorch multiprocessing don't play well together, sometimes. No problems on our box at university, but lots of problems on AWS (on the new deep learning CUDA 9 AMI with P2 instance). Adding locking around all cv2 calls doesn't fix it, and adding `cv2.setNumThreads(0)` doesn't fix. This seems to fix it:\r\n\r\n```\r\nfrom multiprocessing import set_start_method\r\nset_start_method('spawn')\r\n```\r\n\r\nHowever that impacts performance by about 15%. The recommendation in the opencv github issue is to use https://github.com/tomMoral/loky . I've used that module before and found it rock-solid. Not urgent, since we've got a solution that works well enough for now - but might be worth considering using Loky for Dataloader?\r\n\r\nPerhaps more importantly, it would be nice if at least there was some kind of timeout in pytorch's queue so that these infinite hangs would get caught."}