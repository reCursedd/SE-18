{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/307401097", "html_url": "https://github.com/pytorch/pytorch/issues/1355#issuecomment-307401097", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1355", "id": 307401097, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzQwMTA5Nw==", "user": {"login": "jsainio", "id": 28704589, "node_id": "MDQ6VXNlcjI4NzA0NTg5", "avatar_url": "https://avatars2.githubusercontent.com/u/28704589?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsainio", "html_url": "https://github.com/jsainio", "followers_url": "https://api.github.com/users/jsainio/followers", "following_url": "https://api.github.com/users/jsainio/following{/other_user}", "gists_url": "https://api.github.com/users/jsainio/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsainio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsainio/subscriptions", "organizations_url": "https://api.github.com/users/jsainio/orgs", "repos_url": "https://api.github.com/users/jsainio/repos", "events_url": "https://api.github.com/users/jsainio/events{/privacy}", "received_events_url": "https://api.github.com/users/jsainio/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-09T14:17:06Z", "updated_at": "2017-06-09T14:18:02Z", "author_association": "NONE", "body_html": "<p>In my setup, the dataset lies on a networked disk that is read over NFS. With <code>pin_memory = False</code> and <code>num_workers = 4</code> I can get the system fail fairly fast.</p>\n<pre><code>=&gt; creating model 'resnet18'\n- training epoch 0\nEpoch: [0][0/5005]\tTime 10.713 (10.713)\tData 4.619 (4.619)\tLoss 6.9555 (6.9555)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\nTraceback (most recent call last):\n--snip--\nimagenet_pytorch.main.main([data_dir, \"--transient_dir\", context.transient_dir])\n  File \"/home/user/mnt/imagenet_pytorch/main.py\", line 140, in main\n\ntrain(train_loader, model, criterion, optimizer, epoch, args)\n  File \"/home/user/mnt/imagenet_pytorch/main.py\", line 168, in train\n\nfor i, (input, target) in enumerate(train_loader):\n  File \"/home/user/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 206, in __next__\n\nidx, batch = self.data_queue.get()\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n\nreturn ForkingPickler.loads(res)\n  File \"/home/user/anaconda/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n\nfd = df.detach()\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n\nwith _resource_sharer.get_connection(self._id) as conn:\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n\nc = Client(address, authkey=process.current_process().authkey)\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 493, in Client\n\nanswer_challenge(c, authkey)\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 732, in answer_challenge\n\nmessage = connection.recv_bytes(256)         # reject large message\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n\nbuf = self._recv_bytes(maxlength)\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n\nbuf = self._recv(4)\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n\nchunk = read(handle, remaining)\nConnectionResetError\n: \n[Errno 104] Connection reset by peer\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4273204\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zym1010\">@zym1010</a> do you happen to have a networked disk or a traditional spinning disk as well which might be slower in latency/etc.?</p>", "body_text": "In my setup, the dataset lies on a networked disk that is read over NFS. With pin_memory = False and num_workers = 4 I can get the system fail fairly fast.\n=> creating model 'resnet18'\n- training epoch 0\nEpoch: [0][0/5005]\tTime 10.713 (10.713)\tData 4.619 (4.619)\tLoss 6.9555 (6.9555)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\nTraceback (most recent call last):\n--snip--\nimagenet_pytorch.main.main([data_dir, \"--transient_dir\", context.transient_dir])\n  File \"/home/user/mnt/imagenet_pytorch/main.py\", line 140, in main\n\ntrain(train_loader, model, criterion, optimizer, epoch, args)\n  File \"/home/user/mnt/imagenet_pytorch/main.py\", line 168, in train\n\nfor i, (input, target) in enumerate(train_loader):\n  File \"/home/user/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 206, in __next__\n\nidx, batch = self.data_queue.get()\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n\nreturn ForkingPickler.loads(res)\n  File \"/home/user/anaconda/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n\nfd = df.detach()\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\n\nwith _resource_sharer.get_connection(self._id) as conn:\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\n\nc = Client(address, authkey=process.current_process().authkey)\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 493, in Client\n\nanswer_challenge(c, authkey)\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 732, in answer_challenge\n\nmessage = connection.recv_bytes(256)         # reject large message\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n\nbuf = self._recv_bytes(maxlength)\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n\nbuf = self._recv(4)\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n\nchunk = read(handle, remaining)\nConnectionResetError\n: \n[Errno 104] Connection reset by peer\n\n@zym1010 do you happen to have a networked disk or a traditional spinning disk as well which might be slower in latency/etc.?", "body": "In my setup, the dataset lies on a networked disk that is read over NFS. With `pin_memory = False` and `num_workers = 4` I can get the system fail fairly fast.\r\n\r\n```\r\n=> creating model 'resnet18'\r\n- training epoch 0\r\nEpoch: [0][0/5005]\tTime 10.713 (10.713)\tData 4.619 (4.619)\tLoss 6.9555 (6.9555)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\r\nTraceback (most recent call last):\r\n--snip--\r\nimagenet_pytorch.main.main([data_dir, \"--transient_dir\", context.transient_dir])\r\n  File \"/home/user/mnt/imagenet_pytorch/main.py\", line 140, in main\r\n\r\ntrain(train_loader, model, criterion, optimizer, epoch, args)\r\n  File \"/home/user/mnt/imagenet_pytorch/main.py\", line 168, in train\r\n\r\nfor i, (input, target) in enumerate(train_loader):\r\n  File \"/home/user/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 206, in __next__\r\n\r\nidx, batch = self.data_queue.get()\r\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/queues.py\", line 345, in get\r\n\r\nreturn ForkingPickler.loads(res)\r\n  File \"/home/user/anaconda/lib/python3.5/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\r\n\r\nfd = df.detach()\r\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/resource_sharer.py\", line 57, in detach\r\n\r\nwith _resource_sharer.get_connection(self._id) as conn:\r\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/resource_sharer.py\", line 87, in get_connection\r\n\r\nc = Client(address, authkey=process.current_process().authkey)\r\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 493, in Client\r\n\r\nanswer_challenge(c, authkey)\r\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 732, in answer_challenge\r\n\r\nmessage = connection.recv_bytes(256)         # reject large message\r\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\r\n\r\nbuf = self._recv_bytes(maxlength)\r\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n\r\nbuf = self._recv(4)\r\n  File \"/home/user/anaconda/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\r\n\r\nchunk = read(handle, remaining)\r\nConnectionResetError\r\n: \r\n[Errno 104] Connection reset by peer\r\n```\r\n\r\n@zym1010 do you happen to have a networked disk or a traditional spinning disk as well which might be slower in latency/etc.?"}