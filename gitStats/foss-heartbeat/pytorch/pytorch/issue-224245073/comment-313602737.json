{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/313602737", "html_url": "https://github.com/pytorch/pytorch/issues/1355#issuecomment-313602737", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1355", "id": 313602737, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzYwMjczNw==", "user": {"login": "pratikac", "id": 249328, "node_id": "MDQ6VXNlcjI0OTMyOA==", "avatar_url": "https://avatars2.githubusercontent.com/u/249328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pratikac", "html_url": "https://github.com/pratikac", "followers_url": "https://api.github.com/users/pratikac/followers", "following_url": "https://api.github.com/users/pratikac/following{/other_user}", "gists_url": "https://api.github.com/users/pratikac/gists{/gist_id}", "starred_url": "https://api.github.com/users/pratikac/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pratikac/subscriptions", "organizations_url": "https://api.github.com/users/pratikac/orgs", "repos_url": "https://api.github.com/users/pratikac/repos", "events_url": "https://api.github.com/users/pratikac/events{/privacy}", "received_events_url": "https://api.github.com/users/pratikac/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T06:55:05Z", "updated_at": "2017-07-07T07:01:25Z", "author_association": "NONE", "body_html": "<p>I had the same issue where the dataloader crashes after complaining that it could not allocate memory at the beginning of a new training or validation epoch. The solutions above did not work for me (i) my <code>/dev/shm</code> is 32GB and it was never used more than 2.5GB, and (ii) setting pin_memory=False did not work.</p>\n<p>This is perhaps something to do with garbage collection? My code looks roughly like the following. I need an infinite iterator and hence I do a try / except around the <code>next()</code> below :-)</p>\n<pre><code>def train():\n    train_iter = train_loader.__iter__()\n    for i in xrange(max_batches):\n        try:\n            x, y = next(train_iter)\n        except StopIteration:\n            train_iter = train_loader.__iter__()\n        ...\n    del train_iter\n</code></pre>\n<p><code>train_loader</code> is a <code>DataLoader</code> object. Without the explicit <code>del train_iter</code> line at the end of the function, the process always crashes after 2-3 epochs (<code>/dev/shm</code> still shows 2.5 GB). Hope this helps!</p>\n<p>I am using <code>4</code> workers (version <code>0.1.12_2</code> with CUDA 8.0 on Ubuntu 16.04).</p>", "body_text": "I had the same issue where the dataloader crashes after complaining that it could not allocate memory at the beginning of a new training or validation epoch. The solutions above did not work for me (i) my /dev/shm is 32GB and it was never used more than 2.5GB, and (ii) setting pin_memory=False did not work.\nThis is perhaps something to do with garbage collection? My code looks roughly like the following. I need an infinite iterator and hence I do a try / except around the next() below :-)\ndef train():\n    train_iter = train_loader.__iter__()\n    for i in xrange(max_batches):\n        try:\n            x, y = next(train_iter)\n        except StopIteration:\n            train_iter = train_loader.__iter__()\n        ...\n    del train_iter\n\ntrain_loader is a DataLoader object. Without the explicit del train_iter line at the end of the function, the process always crashes after 2-3 epochs (/dev/shm still shows 2.5 GB). Hope this helps!\nI am using 4 workers (version 0.1.12_2 with CUDA 8.0 on Ubuntu 16.04).", "body": "I had the same issue where the dataloader crashes after complaining that it could not allocate memory at the beginning of a new training or validation epoch. The solutions above did not work for me (i) my `/dev/shm` is 32GB and it was never used more than 2.5GB, and (ii) setting pin_memory=False did not work.\r\n\r\nThis is perhaps something to do with garbage collection? My code looks roughly like the following. I need an infinite iterator and hence I do a try / except around the `next()` below :-)\r\n\r\n```\r\ndef train():\r\n    train_iter = train_loader.__iter__()\r\n    for i in xrange(max_batches):\r\n        try:\r\n            x, y = next(train_iter)\r\n        except StopIteration:\r\n            train_iter = train_loader.__iter__()\r\n        ...\r\n    del train_iter\r\n```\r\n\r\n`train_loader` is a `DataLoader` object. Without the explicit `del train_iter` line at the end of the function, the process always crashes after 2-3 epochs (`/dev/shm` still shows 2.5 GB). Hope this helps!\r\n\r\nI am using `4` workers (version `0.1.12_2` with CUDA 8.0 on Ubuntu 16.04)."}