{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/297394953", "html_url": "https://github.com/pytorch/pytorch/issues/1355#issuecomment-297394953", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1355", "id": 297394953, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NzM5NDk1Mw==", "user": {"login": "zym1010", "id": 4273204, "node_id": "MDQ6VXNlcjQyNzMyMDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/4273204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zym1010", "html_url": "https://github.com/zym1010", "followers_url": "https://api.github.com/users/zym1010/followers", "following_url": "https://api.github.com/users/zym1010/following{/other_user}", "gists_url": "https://api.github.com/users/zym1010/gists{/gist_id}", "starred_url": "https://api.github.com/users/zym1010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zym1010/subscriptions", "organizations_url": "https://api.github.com/users/zym1010/orgs", "repos_url": "https://api.github.com/users/zym1010/repos", "events_url": "https://api.github.com/users/zym1010/events{/privacy}", "received_events_url": "https://api.github.com/users/zym1010/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-26T12:43:05Z", "updated_at": "2017-04-26T12:50:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Thanks. I understand your analysis much better now.</p>\n<p>All other results shown to you up to how are performed on a Ubuntu 14.04 machine with 64GB RAM, dual Xeon, and Titan Black (there's also a K40, but I didn't use it).</p>\n<p>The command to generate the problem is <code>CUDA_VISIBLE_DEVICES=0 PYTHONUNBUFFERED=1 python main.py -a alexnet --print-freq 20 --lr 0.01 --workers 22 --batch-size 256 /mnt/temp_drive_3/cv_datasets/ILSVRC2015/Data/CLS-LOC</code>. I didn't modify code at all.</p>\n<p>I installed pytorch through pip, on Python 3.5. pytorch version is <code>0.1.11_5</code>. Not running in Docker.</p>\n<p>BTW, I also tried using 1 worker. But I did it on another machine (128GB RAM, dual Xeon, 4 Pascal Titan X, CentOS 6). I ran it using <code>CUDA_VISIBLE_DEVICES=0 PYTHONUNBUFFERED=1 python main.py -a alexnet --print-freq 1 --lr 0.01 --workers 1 --batch-size 256 /ssd/cv_datasets/ILSVRC2015/Data/CLS-LOC</code>, and the error log is as follows.</p>\n<pre><code>Epoch: [0][5003/5005]   Time 2.463 (2.955)      Data 2.414 (2.903)      Loss 5.9677 (6.6311)    Prec@1 3.516 (0.545)    Prec@5 8.594 (2.262)\nEpoch: [0][5004/5005]   Time 1.977 (2.955)      Data 1.303 (2.903)      Loss 5.9529 (6.6310)    Prec@1 1.399 (0.545)    Prec@5 7.692 (2.262)\n^CTraceback (most recent call last):\n  File \"main.py\", line 292, in &lt;module&gt;\n    main()\n  File \"main.py\", line 137, in main\n    prec1 = validate(val_loader, model, criterion)\n  File \"main.py\", line 210, in validate\n    for i, (input, target) in enumerate(val_loader):\n  File \"/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 168, in __next__\n    idx, batch = self.data_queue.get()\n  File \"/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/queue.py\", line 164, in get\n    self.not_empty.wait()\n  File \"/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/threading.py\", line 293, in wait\n    waiter.acquire()\n</code></pre>\n<p>the <code>top</code> showed the following when stuck with 1 worker.</p>\n<pre><code>top - 08:34:33 up 15 days, 20:03,  0 users,  load average: 0.37, 0.39, 0.36\nTasks: 894 total,   1 running, 892 sleeping,   0 stopped,   1 zombie\nCpu(s):  7.2%us,  2.8%sy,  0.0%ni, 89.7%id,  0.3%wa,  0.0%hi,  0.0%si,  0.0%st\nMem:  132196824k total, 131461528k used,   735296k free,   347448k buffers\nSwap:  2047996k total,    22656k used,  2025340k free, 125226796k cached\n</code></pre>", "body_text": "@apaszke Thanks. I understand your analysis much better now.\nAll other results shown to you up to how are performed on a Ubuntu 14.04 machine with 64GB RAM, dual Xeon, and Titan Black (there's also a K40, but I didn't use it).\nThe command to generate the problem is CUDA_VISIBLE_DEVICES=0 PYTHONUNBUFFERED=1 python main.py -a alexnet --print-freq 20 --lr 0.01 --workers 22 --batch-size 256 /mnt/temp_drive_3/cv_datasets/ILSVRC2015/Data/CLS-LOC. I didn't modify code at all.\nI installed pytorch through pip, on Python 3.5. pytorch version is 0.1.11_5. Not running in Docker.\nBTW, I also tried using 1 worker. But I did it on another machine (128GB RAM, dual Xeon, 4 Pascal Titan X, CentOS 6). I ran it using CUDA_VISIBLE_DEVICES=0 PYTHONUNBUFFERED=1 python main.py -a alexnet --print-freq 1 --lr 0.01 --workers 1 --batch-size 256 /ssd/cv_datasets/ILSVRC2015/Data/CLS-LOC, and the error log is as follows.\nEpoch: [0][5003/5005]   Time 2.463 (2.955)      Data 2.414 (2.903)      Loss 5.9677 (6.6311)    Prec@1 3.516 (0.545)    Prec@5 8.594 (2.262)\nEpoch: [0][5004/5005]   Time 1.977 (2.955)      Data 1.303 (2.903)      Loss 5.9529 (6.6310)    Prec@1 1.399 (0.545)    Prec@5 7.692 (2.262)\n^CTraceback (most recent call last):\n  File \"main.py\", line 292, in <module>\n    main()\n  File \"main.py\", line 137, in main\n    prec1 = validate(val_loader, model, criterion)\n  File \"main.py\", line 210, in validate\n    for i, (input, target) in enumerate(val_loader):\n  File \"/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 168, in __next__\n    idx, batch = self.data_queue.get()\n  File \"/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/queue.py\", line 164, in get\n    self.not_empty.wait()\n  File \"/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/threading.py\", line 293, in wait\n    waiter.acquire()\n\nthe top showed the following when stuck with 1 worker.\ntop - 08:34:33 up 15 days, 20:03,  0 users,  load average: 0.37, 0.39, 0.36\nTasks: 894 total,   1 running, 892 sleeping,   0 stopped,   1 zombie\nCpu(s):  7.2%us,  2.8%sy,  0.0%ni, 89.7%id,  0.3%wa,  0.0%hi,  0.0%si,  0.0%st\nMem:  132196824k total, 131461528k used,   735296k free,   347448k buffers\nSwap:  2047996k total,    22656k used,  2025340k free, 125226796k cached", "body": "@apaszke Thanks. I understand your analysis much better now.\r\n\r\nAll other results shown to you up to how are performed on a Ubuntu 14.04 machine with 64GB RAM, dual Xeon, and Titan Black (there's also a K40, but I didn't use it).\r\n\r\nThe command to generate the problem is `CUDA_VISIBLE_DEVICES=0 PYTHONUNBUFFERED=1 python main.py -a alexnet --print-freq 20 --lr 0.01 --workers 22 --batch-size 256 /mnt/temp_drive_3/cv_datasets/ILSVRC2015/Data/CLS-LOC`. I didn't modify code at all.\r\n\r\nI installed pytorch through pip, on Python 3.5. pytorch version is `0.1.11_5`. Not running in Docker.\r\n\r\n\r\nBTW, I also tried using 1 worker. But I did it on another machine (128GB RAM, dual Xeon, 4 Pascal Titan X, CentOS 6). I ran it using `CUDA_VISIBLE_DEVICES=0 PYTHONUNBUFFERED=1 python main.py -a alexnet --print-freq 1 --lr 0.01 --workers 1 --batch-size 256 /ssd/cv_datasets/ILSVRC2015/Data/CLS-LOC`, and the error log is as follows.\r\n\r\n```\r\nEpoch: [0][5003/5005]   Time 2.463 (2.955)      Data 2.414 (2.903)      Loss 5.9677 (6.6311)    Prec@1 3.516 (0.545)    Prec@5 8.594 (2.262)\r\nEpoch: [0][5004/5005]   Time 1.977 (2.955)      Data 1.303 (2.903)      Loss 5.9529 (6.6310)    Prec@1 1.399 (0.545)    Prec@5 7.692 (2.262)\r\n^CTraceback (most recent call last):\r\n  File \"main.py\", line 292, in <module>\r\n    main()\r\n  File \"main.py\", line 137, in main\r\n    prec1 = validate(val_loader, model, criterion)\r\n  File \"main.py\", line 210, in validate\r\n    for i, (input, target) in enumerate(val_loader):\r\n  File \"/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 168, in __next__\r\n    idx, batch = self.data_queue.get()\r\n  File \"/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/queue.py\", line 164, in get\r\n    self.not_empty.wait()\r\n  File \"/home/yimengzh/miniconda2/envs/pytorch/lib/python3.5/threading.py\", line 293, in wait\r\n    waiter.acquire()\r\n```\r\n\r\nthe `top` showed the following when stuck with 1 worker.\r\n\r\n~~~\r\ntop - 08:34:33 up 15 days, 20:03,  0 users,  load average: 0.37, 0.39, 0.36\r\nTasks: 894 total,   1 running, 892 sleeping,   0 stopped,   1 zombie\r\nCpu(s):  7.2%us,  2.8%sy,  0.0%ni, 89.7%id,  0.3%wa,  0.0%hi,  0.0%si,  0.0%st\r\nMem:  132196824k total, 131461528k used,   735296k free,   347448k buffers\r\nSwap:  2047996k total,    22656k used,  2025340k free, 125226796k cached\r\n~~~"}