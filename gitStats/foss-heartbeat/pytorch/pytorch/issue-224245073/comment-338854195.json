{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/338854195", "html_url": "https://github.com/pytorch/pytorch/issues/1355#issuecomment-338854195", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1355", "id": 338854195, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODg1NDE5NQ==", "user": {"login": "berzjackson", "id": 1171221, "node_id": "MDQ6VXNlcjExNzEyMjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/1171221?v=4", "gravatar_id": "", "url": "https://api.github.com/users/berzjackson", "html_url": "https://github.com/berzjackson", "followers_url": "https://api.github.com/users/berzjackson/followers", "following_url": "https://api.github.com/users/berzjackson/following{/other_user}", "gists_url": "https://api.github.com/users/berzjackson/gists{/gist_id}", "starred_url": "https://api.github.com/users/berzjackson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/berzjackson/subscriptions", "organizations_url": "https://api.github.com/users/berzjackson/orgs", "repos_url": "https://api.github.com/users/berzjackson/repos", "events_url": "https://api.github.com/users/berzjackson/events{/privacy}", "received_events_url": "https://api.github.com/users/berzjackson/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-24T02:34:45Z", "updated_at": "2017-10-24T02:34:45Z", "author_association": "NONE", "body_html": "<p>I am being troubled by the same problem, yet I'am under a different environment from most others in this thread, so maybe my inputs can help locating the underlying cause. My pytorch is installed using the excellent conda package built by peterjc123 under Windows10.</p>\n<p>I am running some cnn on the cifar10 dataset. For the dataloaders, num_workers is set to 1. Although having num_workers &gt; 0  is known to cause BrokenPipeError and advised against in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"201800912\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/494\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/494/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/494\">#494</a>, what I am experiencing is not BrokenPipeError but some memory allocation error. The error always occurred at around 50 epochs, right after the validation of the last epoch and before the start of training for the next epoch. 90% of the time it's precisely 50 epochs, other times it will be off by 1 or 2 epochs. Other than that everything else is pretty consistent. Setting num_workers=0 will eliminate this problem.</p>", "body_text": "I am being troubled by the same problem, yet I'am under a different environment from most others in this thread, so maybe my inputs can help locating the underlying cause. My pytorch is installed using the excellent conda package built by peterjc123 under Windows10.\nI am running some cnn on the cifar10 dataset. For the dataloaders, num_workers is set to 1. Although having num_workers > 0  is known to cause BrokenPipeError and advised against in #494, what I am experiencing is not BrokenPipeError but some memory allocation error. The error always occurred at around 50 epochs, right after the validation of the last epoch and before the start of training for the next epoch. 90% of the time it's precisely 50 epochs, other times it will be off by 1 or 2 epochs. Other than that everything else is pretty consistent. Setting num_workers=0 will eliminate this problem.", "body": "I am being troubled by the same problem, yet I'am under a different environment from most others in this thread, so maybe my inputs can help locating the underlying cause. My pytorch is installed using the excellent conda package built by peterjc123 under Windows10. \r\n\r\nI am running some cnn on the cifar10 dataset. For the dataloaders, num_workers is set to 1. Although having num_workers > 0  is known to cause BrokenPipeError and advised against in #494, what I am experiencing is not BrokenPipeError but some memory allocation error. The error always occurred at around 50 epochs, right after the validation of the last epoch and before the start of training for the next epoch. 90% of the time it's precisely 50 epochs, other times it will be off by 1 or 2 epochs. Other than that everything else is pretty consistent. Setting num_workers=0 will eliminate this problem."}