{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/341177898", "html_url": "https://github.com/pytorch/pytorch/issues/1355#issuecomment-341177898", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1355", "id": 341177898, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTE3Nzg5OA==", "user": {"login": "jph00", "id": 346999, "node_id": "MDQ6VXNlcjM0Njk5OQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/346999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jph00", "html_url": "https://github.com/jph00", "followers_url": "https://api.github.com/users/jph00/followers", "following_url": "https://api.github.com/users/jph00/following{/other_user}", "gists_url": "https://api.github.com/users/jph00/gists{/gist_id}", "starred_url": "https://api.github.com/users/jph00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jph00/subscriptions", "organizations_url": "https://api.github.com/users/jph00/orgs", "repos_url": "https://api.github.com/users/jph00/repos", "events_url": "https://api.github.com/users/jph00/events{/privacy}", "received_events_url": "https://api.github.com/users/jph00/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-01T17:27:44Z", "updated_at": "2017-11-01T17:27:44Z", "author_association": "NONE", "body_html": "<p>We have ~600 people that started a new course that uses Pytorch on Monday. A lot of folks on our forum are reporting this problem. Some on AWS P2, some on their own systems (mainly GTX 1070, some Titan X).</p>\n<p>When they interrupt training the end of the stack trace shows:</p>\n<pre><code>~/anaconda2/envs/fastai/lib/python3.6/multiprocessing/connection.py in _recv_bytes(self, maxsize)\n    405 \n    406     def _recv_bytes(self, maxsize=None):\n--&gt; 407         buf = self._recv(4)\n    408         size, = struct.unpack(\"!i\", buf.getvalue())\n    409         if maxsize is not None and size &gt; maxsize:\n\n~/anaconda2/envs/fastai/lib/python3.6/multiprocessing/connection.py in _recv(self, size, read)\n    377         remaining = size\n    378         while remaining &gt; 0:\n--&gt; 379             chunk = read(handle, remaining)\n    380             n = len(chunk)\n    381             if n == 0:\n</code></pre>\n<p>We have num_workers=4, pin_memory=False. I've asked them to check their shared memory settings - but is there anything I can do (or we could do in Pytorch) to make this problem go away? (Other than reducing num_workers, since that would slow things down quite a bit.)</p>", "body_text": "We have ~600 people that started a new course that uses Pytorch on Monday. A lot of folks on our forum are reporting this problem. Some on AWS P2, some on their own systems (mainly GTX 1070, some Titan X).\nWhen they interrupt training the end of the stack trace shows:\n~/anaconda2/envs/fastai/lib/python3.6/multiprocessing/connection.py in _recv_bytes(self, maxsize)\n    405 \n    406     def _recv_bytes(self, maxsize=None):\n--> 407         buf = self._recv(4)\n    408         size, = struct.unpack(\"!i\", buf.getvalue())\n    409         if maxsize is not None and size > maxsize:\n\n~/anaconda2/envs/fastai/lib/python3.6/multiprocessing/connection.py in _recv(self, size, read)\n    377         remaining = size\n    378         while remaining > 0:\n--> 379             chunk = read(handle, remaining)\n    380             n = len(chunk)\n    381             if n == 0:\n\nWe have num_workers=4, pin_memory=False. I've asked them to check their shared memory settings - but is there anything I can do (or we could do in Pytorch) to make this problem go away? (Other than reducing num_workers, since that would slow things down quite a bit.)", "body": "We have ~600 people that started a new course that uses Pytorch on Monday. A lot of folks on our forum are reporting this problem. Some on AWS P2, some on their own systems (mainly GTX 1070, some Titan X). \r\n\r\nWhen they interrupt training the end of the stack trace shows:\r\n\r\n```\r\n~/anaconda2/envs/fastai/lib/python3.6/multiprocessing/connection.py in _recv_bytes(self, maxsize)\r\n    405 \r\n    406     def _recv_bytes(self, maxsize=None):\r\n--> 407         buf = self._recv(4)\r\n    408         size, = struct.unpack(\"!i\", buf.getvalue())\r\n    409         if maxsize is not None and size > maxsize:\r\n\r\n~/anaconda2/envs/fastai/lib/python3.6/multiprocessing/connection.py in _recv(self, size, read)\r\n    377         remaining = size\r\n    378         while remaining > 0:\r\n--> 379             chunk = read(handle, remaining)\r\n    380             n = len(chunk)\r\n    381             if n == 0:\r\n```\r\n\r\nWe have num_workers=4, pin_memory=False. I've asked them to check their shared memory settings - but is there anything I can do (or we could do in Pytorch) to make this problem go away? (Other than reducing num_workers, since that would slow things down quite a bit.)"}