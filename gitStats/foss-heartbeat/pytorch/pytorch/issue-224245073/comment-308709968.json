{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/308709968", "html_url": "https://github.com/pytorch/pytorch/issues/1355#issuecomment-308709968", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1355", "id": 308709968, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODcwOTk2OA==", "user": {"login": "ClementPinard", "id": 4380424, "node_id": "MDQ6VXNlcjQzODA0MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4380424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ClementPinard", "html_url": "https://github.com/ClementPinard", "followers_url": "https://api.github.com/users/ClementPinard/followers", "following_url": "https://api.github.com/users/ClementPinard/following{/other_user}", "gists_url": "https://api.github.com/users/ClementPinard/gists{/gist_id}", "starred_url": "https://api.github.com/users/ClementPinard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ClementPinard/subscriptions", "organizations_url": "https://api.github.com/users/ClementPinard/orgs", "repos_url": "https://api.github.com/users/ClementPinard/repos", "events_url": "https://api.github.com/users/ClementPinard/events{/privacy}", "received_events_url": "https://api.github.com/users/ClementPinard/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-15T11:59:44Z", "updated_at": "2017-06-15T12:52:23Z", "author_association": "NONE", "body_html": "<p>Testing now with 12G and half the workers I had, and it failed :(<br>\nIt was working like a charm in lua torch version (same speed, same number of workers) , which makes me wonder if the problem is only <code>/dev/shm</code>related and not closer to python multiprocessing...</p>\n<p>The odd thing about it (as you mentionned) is that <code>/dev/shm</code>is never close to be full. During first training epoch, it never went above 500Mo. And It also never locks during first epoch, and if I shut down testing trainloader never fails across all the epochs. The deadlock seems to only appear when beginning test epoch. I should keep track of <code>/dev/shm</code> when going from train to test, maybe there is a peak usage during dataloaders changing.</p>", "body_text": "Testing now with 12G and half the workers I had, and it failed :(\nIt was working like a charm in lua torch version (same speed, same number of workers) , which makes me wonder if the problem is only /dev/shmrelated and not closer to python multiprocessing...\nThe odd thing about it (as you mentionned) is that /dev/shmis never close to be full. During first training epoch, it never went above 500Mo. And It also never locks during first epoch, and if I shut down testing trainloader never fails across all the epochs. The deadlock seems to only appear when beginning test epoch. I should keep track of /dev/shm when going from train to test, maybe there is a peak usage during dataloaders changing.", "body": "Testing now with 12G and half the workers I had, and it failed :(\r\nIt was working like a charm in lua torch version (same speed, same number of workers) , which makes me wonder if the problem is only `/dev/shm`related and not closer to python multiprocessing...\r\n\r\nThe odd thing about it (as you mentionned) is that `/dev/shm`is never close to be full. During first training epoch, it never went above 500Mo. And It also never locks during first epoch, and if I shut down testing trainloader never fails across all the epochs. The deadlock seems to only appear when beginning test epoch. I should keep track of `/dev/shm` when going from train to test, maybe there is a peak usage during dataloaders changing."}