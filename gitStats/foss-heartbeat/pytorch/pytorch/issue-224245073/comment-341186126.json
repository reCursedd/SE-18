{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/341186126", "html_url": "https://github.com/pytorch/pytorch/issues/1355#issuecomment-341186126", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1355", "id": 341186126, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTE4NjEyNg==", "user": {"login": "apiltamang", "id": 6851604, "node_id": "MDQ6VXNlcjY4NTE2MDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/6851604?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apiltamang", "html_url": "https://github.com/apiltamang", "followers_url": "https://api.github.com/users/apiltamang/followers", "following_url": "https://api.github.com/users/apiltamang/following{/other_user}", "gists_url": "https://api.github.com/users/apiltamang/gists{/gist_id}", "starred_url": "https://api.github.com/users/apiltamang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apiltamang/subscriptions", "organizations_url": "https://api.github.com/users/apiltamang/orgs", "repos_url": "https://api.github.com/users/apiltamang/repos", "events_url": "https://api.github.com/users/apiltamang/events{/privacy}", "received_events_url": "https://api.github.com/users/apiltamang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-01T17:52:49Z", "updated_at": "2017-11-01T17:57:33Z", "author_association": "NONE", "body_html": "<p>I'm in the  class <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=346999\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jph00\">@jph00</a> (thanks Jeremy! :) ) referred to. I tried using \"num_workers=0\" as well. Still get the same error where resnet34 loads very slowly. The fitting is also very slow. But weird thing: this only happens <strong>once</strong> in the lifetime of a notebook session.</p>\n<p>In other words, once the data is loaded, and the fitting is run once, I can move around and keep repeating the steps... even with 4 num_workers, and everything seems to work fast as expected in a GPU.</p>\n<p>I'm on PyTorch 0.2.0_4, Python 3.6.2, Torchvision 0.1.9, Ubuntu 16.04 LTS. Doing \"df -h\" on my terminal says that I've 16GBs on /dev/shm, although the utilization was very low.</p>\n<p>Here's a screenshot of where the loading fails (note I've used num_workers=0 for the data)<br>\n(sorry about the small letters. I had to zoom out to capture everything...)</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/6851604/32289191-6ce36db0-bf0c-11e7-8eae-e4b8b82f2409.png\"><img src=\"https://user-images.githubusercontent.com/6851604/32289191-6ce36db0-bf0c-11e7-8eae-e4b8b82f2409.png\" alt=\"screenshot 2017-11-01 13 55 46\" style=\"max-width:100%;\"></a></p>", "body_text": "I'm in the  class @jph00 (thanks Jeremy! :) ) referred to. I tried using \"num_workers=0\" as well. Still get the same error where resnet34 loads very slowly. The fitting is also very slow. But weird thing: this only happens once in the lifetime of a notebook session.\nIn other words, once the data is loaded, and the fitting is run once, I can move around and keep repeating the steps... even with 4 num_workers, and everything seems to work fast as expected in a GPU.\nI'm on PyTorch 0.2.0_4, Python 3.6.2, Torchvision 0.1.9, Ubuntu 16.04 LTS. Doing \"df -h\" on my terminal says that I've 16GBs on /dev/shm, although the utilization was very low.\nHere's a screenshot of where the loading fails (note I've used num_workers=0 for the data)\n(sorry about the small letters. I had to zoom out to capture everything...)", "body": "I'm in the  class @jph00 (thanks Jeremy! :) ) referred to. I tried using \"num_workers=0\" as well. Still get the same error where resnet34 loads very slowly. The fitting is also very slow. But weird thing: this only happens **once** in the lifetime of a notebook session. \r\n\r\nIn other words, once the data is loaded, and the fitting is run once, I can move around and keep repeating the steps... even with 4 num_workers, and everything seems to work fast as expected in a GPU.\r\n\r\nI'm on PyTorch 0.2.0_4, Python 3.6.2, Torchvision 0.1.9, Ubuntu 16.04 LTS. Doing \"df -h\" on my terminal says that I've 16GBs on /dev/shm, although the utilization was very low.\r\n\r\nHere's a screenshot of where the loading fails (note I've used num_workers=0 for the data)\r\n(sorry about the small letters. I had to zoom out to capture everything...)\r\n\r\n![screenshot 2017-11-01 13 55 46](https://user-images.githubusercontent.com/6851604/32289191-6ce36db0-bf0c-11e7-8eae-e4b8b82f2409.png)\r\n"}