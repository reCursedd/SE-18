{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/425451856", "html_url": "https://github.com/pytorch/pytorch/issues/11683#issuecomment-425451856", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11683", "id": 425451856, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTQ1MTg1Ng==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-28T14:23:01Z", "updated_at": "2018-09-28T14:23:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Apologies for the false alarms; when working on this bug report I got deceived by the fact that this bug was filed for  0.4.1, but actually, on master, distributed has been swapped for the new c10d implementation, which doesn't use hooks on variables.</p>\n<p>You can reproduce this on master, even with my patch, using:</p>\n<pre><code>import argparse\nimport torch\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--local_rank\", type=int)\n    args = parser.parse_args()\n\n    device = torch.device('cuda', args.local_rank)\n\n    torch.distributed.deprecated.init_process_group(backend='nccl')\n    model = torch.nn.LSTM(10, 10).to(device)\n    model = torch.nn.parallel.deprecated.DistributedDataParallel(\n        model, device_ids=[args.local_rank], output_device=args.local_rank, dim=1)\n        \n    torch.save(model, 'model.pt')\n</code></pre>\n<p>Because of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"360117364\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11681\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/11681/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/11681\">#11681</a> we need to specify nproc: <code>python -m torch.distributed.launch --nproc_per_node=2 test.py</code></p>", "body_text": "Apologies for the false alarms; when working on this bug report I got deceived by the fact that this bug was filed for  0.4.1, but actually, on master, distributed has been swapped for the new c10d implementation, which doesn't use hooks on variables.\nYou can reproduce this on master, even with my patch, using:\nimport argparse\nimport torch\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--local_rank\", type=int)\n    args = parser.parse_args()\n\n    device = torch.device('cuda', args.local_rank)\n\n    torch.distributed.deprecated.init_process_group(backend='nccl')\n    model = torch.nn.LSTM(10, 10).to(device)\n    model = torch.nn.parallel.deprecated.DistributedDataParallel(\n        model, device_ids=[args.local_rank], output_device=args.local_rank, dim=1)\n        \n    torch.save(model, 'model.pt')\n\nBecause of #11681 we need to specify nproc: python -m torch.distributed.launch --nproc_per_node=2 test.py", "body": "Apologies for the false alarms; when working on this bug report I got deceived by the fact that this bug was filed for  0.4.1, but actually, on master, distributed has been swapped for the new c10d implementation, which doesn't use hooks on variables.\r\n\r\nYou can reproduce this on master, even with my patch, using:\r\n\r\n```\r\nimport argparse\r\nimport torch\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--local_rank\", type=int)\r\n    args = parser.parse_args()\r\n\r\n    device = torch.device('cuda', args.local_rank)\r\n\r\n    torch.distributed.deprecated.init_process_group(backend='nccl')\r\n    model = torch.nn.LSTM(10, 10).to(device)\r\n    model = torch.nn.parallel.deprecated.DistributedDataParallel(\r\n        model, device_ids=[args.local_rank], output_device=args.local_rank, dim=1)\r\n        \r\n    torch.save(model, 'model.pt')\r\n```\r\n\r\nBecause of #11681 we need to specify nproc: `python -m torch.distributed.launch --nproc_per_node=2 test.py`"}