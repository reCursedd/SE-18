{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/342749007", "html_url": "https://github.com/pytorch/pytorch/pull/3562#issuecomment-342749007", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3562", "id": 342749007, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Mjc0OTAwNw==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-08T08:43:20Z", "updated_at": "2017-11-08T13:21:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As you can see from CI, this patch isn't good enough; I blithely assumed that every Tensor return of any multi-return method was differentiable, but <code>gesv</code> breaks the pattern.  I don't have a good sense for where the best place to put differentiability information is. My current plan is to use the occurrence of <code>grad</code> in the formula to determine if single return is assumed or not. (Update: There's now a hack to check this case, which seems to be good enough for everything we have today.)</p>", "body_text": "As you can see from CI, this patch isn't good enough; I blithely assumed that every Tensor return of any multi-return method was differentiable, but gesv breaks the pattern.  I don't have a good sense for where the best place to put differentiability information is. My current plan is to use the occurrence of grad in the formula to determine if single return is assumed or not. (Update: There's now a hack to check this case, which seems to be good enough for everything we have today.)", "body": "As you can see from CI, this patch isn't good enough; I blithely assumed that every Tensor return of any multi-return method was differentiable, but `gesv` breaks the pattern.  I don't have a good sense for where the best place to put differentiability information is. My current plan is to use the occurrence of `grad` in the formula to determine if single return is assumed or not. (Update: There's now a hack to check this case, which seems to be good enough for everything we have today.)"}