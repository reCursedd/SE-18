{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/343329831", "html_url": "https://github.com/pytorch/pytorch/pull/3562#issuecomment-343329831", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3562", "id": 343329831, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MzMyOTgzMQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-09T23:49:41Z", "updated_at": "2017-11-10T11:50:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>VariableType didn't change, but Functions.cpp did:</p>\n<div class=\"highlight highlight-source-c++\"><pre>variable_list <span class=\"pl-en\">PreluBackwardBackward::apply</span>(<span class=\"pl-k\">const</span> variable_list&amp; grads) {\n  variable_list grad_inputs{<span class=\"pl-c1\">3</span>};\n  <span class=\"pl-k\">auto</span> grad_output = grad_output_.<span class=\"pl-c1\">unpack</span>();\n  <span class=\"pl-k\">auto</span> input = input_.<span class=\"pl-c1\">unpack</span>();\n  <span class=\"pl-k\">auto</span> weight = weight_.<span class=\"pl-c1\">unpack</span>();\n  <span class=\"pl-k\">if</span> (<span class=\"pl-c1\">should_compute_output</span>({ <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span> })) {\n    <span class=\"pl-k\">auto</span> grad_input_mask = std::array&lt;<span class=\"pl-k\">bool</span>, <span class=\"pl-c1\">3</span>&gt;{\n      <span class=\"pl-c1\">should_compute_output</span>(<span class=\"pl-c1\">0</span>),\n      <span class=\"pl-c1\">should_compute_output</span>(<span class=\"pl-c1\">1</span>),\n      <span class=\"pl-c1\">should_compute_output</span>(<span class=\"pl-c1\">2</span>),\n    };\n    <span class=\"pl-c1\">std::tie</span>(grad_inputs[<span class=\"pl-c1\">0</span>], grad_inputs[<span class=\"pl-c1\">1</span>], grad_inputs[<span class=\"pl-c1\">2</span>]) = <span class=\"pl-c1\">prelu_double_backward</span>(grads[<span class=\"pl-c1\">0</span>], grads[<span class=\"pl-c1\">1</span>], grad_output, input, weight, grad_input_mask);\n  }\n  <span class=\"pl-c1\">ensure_no_aten_scalars</span>(grad_inputs);\n  <span class=\"pl-k\">return</span> grad_inputs;\n}</pre></div>\n<p>With the renaming of mask to <code>grad_input_mask</code>, the local no longer shadows the member variable named <code>output_mask</code>.</p>", "body_text": "VariableType didn't change, but Functions.cpp did:\nvariable_list PreluBackwardBackward::apply(const variable_list& grads) {\n  variable_list grad_inputs{3};\n  auto grad_output = grad_output_.unpack();\n  auto input = input_.unpack();\n  auto weight = weight_.unpack();\n  if (should_compute_output({ 0, 1, 2 })) {\n    auto grad_input_mask = std::array<bool, 3>{\n      should_compute_output(0),\n      should_compute_output(1),\n      should_compute_output(2),\n    };\n    std::tie(grad_inputs[0], grad_inputs[1], grad_inputs[2]) = prelu_double_backward(grads[0], grads[1], grad_output, input, weight, grad_input_mask);\n  }\n  ensure_no_aten_scalars(grad_inputs);\n  return grad_inputs;\n}\nWith the renaming of mask to grad_input_mask, the local no longer shadows the member variable named output_mask.", "body": "VariableType didn't change, but Functions.cpp did:\r\n\r\n```cpp\r\nvariable_list PreluBackwardBackward::apply(const variable_list& grads) {\r\n  variable_list grad_inputs{3};\r\n  auto grad_output = grad_output_.unpack();\r\n  auto input = input_.unpack();\r\n  auto weight = weight_.unpack();\r\n  if (should_compute_output({ 0, 1, 2 })) {\r\n    auto grad_input_mask = std::array<bool, 3>{\r\n      should_compute_output(0),\r\n      should_compute_output(1),\r\n      should_compute_output(2),\r\n    };\r\n    std::tie(grad_inputs[0], grad_inputs[1], grad_inputs[2]) = prelu_double_backward(grads[0], grads[1], grad_output, input, weight, grad_input_mask);\r\n  }\r\n  ensure_no_aten_scalars(grad_inputs);\r\n  return grad_inputs;\r\n}\r\n```\r\n\r\nWith the renaming of mask to `grad_input_mask`, the local no longer shadows the member variable named `output_mask`."}