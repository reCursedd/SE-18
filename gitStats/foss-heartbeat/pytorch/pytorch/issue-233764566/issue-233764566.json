{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1734", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1734/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1734/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1734/events", "html_url": "https://github.com/pytorch/pytorch/issues/1734", "id": 233764566, "node_id": "MDU6SXNzdWUyMzM3NjQ1NjY=", "number": 1734, "title": "[feature request] Variable.attach() method to set requires_grad = True", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-06-06T02:47:37Z", "updated_at": "2018-04-16T23:35:03Z", "closed_at": "2018-04-16T23:35:03Z", "author_association": "NONE", "body_html": "<p><code>autograd.grad(...)</code> requires Variables attached to the graph (this is a tiny bit counter-intuitive, since gradients are explicitly asked for).</p>\n<p>Given that <code>volatile</code> would propagate from the beginning of the network, maybe an <code>attach()</code> method would be convenient to have (conversely from <code>detach</code>, it would enable <code>requires_grad</code>).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn\n\ni <span class=\"pl-k\">=</span> torch.autograd.Variable(torch.Tensor(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">8</span>))\nm <span class=\"pl-k\">=</span> torch.nn.Conv2d(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">1</span>)\no <span class=\"pl-k\">=</span> m(i)\ng <span class=\"pl-k\">=</span> torch.autograd.grad(o.sum(), i)[<span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-c1\">print</span>(g.sum())</pre></div>\n<pre><code>Traceback (most recent call last):\n  File \"test_.py\", line 7, in &lt;module&gt;\n    g = torch.autograd.grad(o.sum(), i)[0]\n  File \".../torch/autograd/__init__.py\", line 153, in grad\n    inputs, only_inputs)\nRuntimeError: One of the differentiated Variables appears to not have been used in the graph\n</code></pre>", "body_text": "autograd.grad(...) requires Variables attached to the graph (this is a tiny bit counter-intuitive, since gradients are explicitly asked for).\nGiven that volatile would propagate from the beginning of the network, maybe an attach() method would be convenient to have (conversely from detach, it would enable requires_grad).\nimport torch\nimport torch.nn\n\ni = torch.autograd.Variable(torch.Tensor(1, 4, 8, 8))\nm = torch.nn.Conv2d(4, 4, 1)\no = m(i)\ng = torch.autograd.grad(o.sum(), i)[0]\n\nprint(g.sum())\nTraceback (most recent call last):\n  File \"test_.py\", line 7, in <module>\n    g = torch.autograd.grad(o.sum(), i)[0]\n  File \".../torch/autograd/__init__.py\", line 153, in grad\n    inputs, only_inputs)\nRuntimeError: One of the differentiated Variables appears to not have been used in the graph", "body": "`autograd.grad(...)` requires Variables attached to the graph (this is a tiny bit counter-intuitive, since gradients are explicitly asked for).\r\n\r\nGiven that `volatile` would propagate from the beginning of the network, maybe an `attach()` method would be convenient to have (conversely from `detach`, it would enable `requires_grad`).\r\n\r\n```python\r\nimport torch\r\nimport torch.nn\r\n\r\ni = torch.autograd.Variable(torch.Tensor(1, 4, 8, 8))\r\nm = torch.nn.Conv2d(4, 4, 1)\r\no = m(i)\r\ng = torch.autograd.grad(o.sum(), i)[0]\r\n\r\nprint(g.sum())\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_.py\", line 7, in <module>\r\n    g = torch.autograd.grad(o.sum(), i)[0]\r\n  File \".../torch/autograd/__init__.py\", line 153, in grad\r\n    inputs, only_inputs)\r\nRuntimeError: One of the differentiated Variables appears to not have been used in the graph\r\n```"}