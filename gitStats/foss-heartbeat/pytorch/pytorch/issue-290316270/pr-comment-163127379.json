{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163127379", "pull_request_review_id": 90691005, "id": 163127379, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MzEyNzM3OQ==", "diff_hunk": "@@ -0,0 +1,231 @@\n+import torch\r\n+from torch.distributions import constraints\r\n+from torch.distributions.utils import broadcast_all\r\n+from torch.nn.functional import sigmoid\r\n+\r\n+__all__ = [\r\n+    'AbsTransform',\r\n+    'AffineTransform',\r\n+    'ExpTransform',\r\n+    'InverseTransform',\r\n+    'LogprobTransform',\r\n+    'SigmoidTransform',\r\n+    'StickBreakingTransform',\r\n+    'Transform',\r\n+]\r\n+\r\n+\r\n+class Transform(object):\r\n+    \"\"\"\r\n+    Abstract class for transformations with computable inverse log\r\n+    det jacobians. They are primarily used in\r\n+    :class:`torch.distributions.TransformedDistribution`.\r\n+\r\n+    Transforms are intended to be short-lived objects. They memoize the forward\r\n+    and inverse computations to avoid work; therefore :meth:`inverse` is\r\n+    nearly free after calling :meth:`forward`. To clear the memoization cache,\r\n+    delete the object and create a new object.\r\n+\r\n+    Derived classes should implement one or both of :meth:`_forward` or\r\n+    :meth:`_inverse` and should implement :meth:`log_abs_det_jacobian`.\r\n+    Derived classes may store intermediate results in the `._cache` dict.\r\n+    \"\"\"\r\n+\r\n+    def __init__(self):\r\n+        self._cache = {}\r", "path": "torch/distributions/transforms.py", "position": null, "original_position": 35, "commit_id": "244932097058605559f6baa57da01c50b32a4131", "original_commit_id": "f24d3a3e5bae1838f56a7a2b389ec795d7b1de6b", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "I'm open to ideas here. This seemed like the least weird implementation I could find.\r\n\r\nThe purposes are (1) to speed up inverses, (2) to compute inverses in cases where they have not been implemented, and (3) cache intermediate computations for computing `log_abs_det_jacobian()` to avoid gradients being ping-ponged back and forth through `.forward()` and `.inverse()` (e.g. if `.log_deg_jacobian_inverse(x, y)` depends on x, we want to avoid `x` being defined in terms of `dist.inverse(dist.forward(y))`. The Pyro implementation automatically ejects an item from cache the first time it is read, but that behavior seemed to make it too easy to introduce a leak.\r\n\r\nWe could implement cache size of 1, but that could lead to silent degradation of performance and gradient stability. Alican originally had something like a `.memoize` flag to configure this, but I removed it at some point to simplify the interface.", "created_at": "2018-01-23T02:13:47Z", "updated_at": "2018-11-23T15:38:25Z", "html_url": "https://github.com/pytorch/pytorch/pull/4771#discussion_r163127379", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4771", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/163127379"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4771#discussion_r163127379"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4771"}}, "body_html": "<p>I'm open to ideas here. This seemed like the least weird implementation I could find.</p>\n<p>The purposes are (1) to speed up inverses, (2) to compute inverses in cases where they have not been implemented, and (3) cache intermediate computations for computing <code>log_abs_det_jacobian()</code> to avoid gradients being ping-ponged back and forth through <code>.forward()</code> and <code>.inverse()</code> (e.g. if <code>.log_deg_jacobian_inverse(x, y)</code> depends on x, we want to avoid <code>x</code> being defined in terms of <code>dist.inverse(dist.forward(y))</code>. The Pyro implementation automatically ejects an item from cache the first time it is read, but that behavior seemed to make it too easy to introduce a leak.</p>\n<p>We could implement cache size of 1, but that could lead to silent degradation of performance and gradient stability. Alican originally had something like a <code>.memoize</code> flag to configure this, but I removed it at some point to simplify the interface.</p>", "body_text": "I'm open to ideas here. This seemed like the least weird implementation I could find.\nThe purposes are (1) to speed up inverses, (2) to compute inverses in cases where they have not been implemented, and (3) cache intermediate computations for computing log_abs_det_jacobian() to avoid gradients being ping-ponged back and forth through .forward() and .inverse() (e.g. if .log_deg_jacobian_inverse(x, y) depends on x, we want to avoid x being defined in terms of dist.inverse(dist.forward(y)). The Pyro implementation automatically ejects an item from cache the first time it is read, but that behavior seemed to make it too easy to introduce a leak.\nWe could implement cache size of 1, but that could lead to silent degradation of performance and gradient stability. Alican originally had something like a .memoize flag to configure this, but I removed it at some point to simplify the interface.", "in_reply_to_id": 162830660}