{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164284703", "pull_request_review_id": 92039105, "id": 164284703, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDI4NDcwMw==", "diff_hunk": "@@ -0,0 +1,342 @@\n+import torch\r\n+from torch.distributions import constraints\r\n+from torch.distributions.utils import broadcast_all, lazy_property\r\n+from torch.nn.functional import sigmoid\r\n+\r\n+__all__ = [\r\n+    'AbsTransform',\r\n+    'AffineTransform',\r\n+    'BoltzmannTransform',\r\n+    'ExpTransform',\r\n+    'InverseTransform',\r\n+    'LowerCholeskyTransform',\r\n+    'SigmoidTransform',\r\n+    'StickBreakingTransform',\r\n+    'Transform',\r\n+]\r\n+\r\n+\r\n+class Transform(object):\r\n+    \"\"\"\r\n+    Abstract class for invertable transformations with computable log\r\n+    det jacobians. They are primarily used in\r\n+    :class:`torch.distributions.TransformedDistribution`.\r\n+\r\n+    Caching is useful for tranforms whose inverses are either expensive or\r\n+    numerically unstable. Note that care must be taken with memoized values\r\n+    since the autograd graph may be reversed. For example while the following\r\n+    works with or without caching::\r\n+\r\n+        y = t(x)\r\n+        t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\r\n+\r\n+    However the following will error when caching due to dependency reversal::\r\n+\r\n+        y = t(x)\r\n+        z = t.inv(y)\r\n+        grad(z.sum(), [y])  # error because z is x\r\n+\r\n+    Derived classes should implement one or both of :meth:`_call` or\r\n+    :meth:`_inverse`. Derived classes that set `bijective=True` should also\r\n+    implement :meth:`log_abs_det_jacobian`.\r\n+    \"\"\"\r\n+    bijective = False\r\n+\r\n+    def __init__(self, cache_size=0):\r\n+        if cache_size != 0:\r\n+            if cache_size == 1:\r\n+                self._cached_x_y = None, None\r\n+                self.__call__ = self._cached_call\r\n+                self.inverse = self._cached_inverse\r\n+            else:\r\n+                raise NotImplementedError('cache_size must be 0 or 1')\r\n+\r\n+    @lazy_property\r\n+    def inv(self):\r\n+        \"\"\"\r\n+        Returns the inverse :class:`Transform` of this transform.\r\n+        \"\"\"\r\n+        return InverseTransform(self)\r\n+\r\n+    def __eq__(self, other):\r\n+        return self is other\r\n+\r\n+    def __ne__(self, other):\r\n+        # Necessary for Python2\r\n+        return not self.__eq__(other)\r\n+\r\n+    def __call__(self, x):\r\n+        \"\"\"\r\n+        Computes the transform `x => y`.\r\n+        \"\"\"\r\n+        return self._call(x)\r\n+\r\n+    def inverse(self, y):\r\n+        \"\"\"\r\n+        Inverts the transform `y => x`.\r\n+        \"\"\"\r\n+        return self._inverse(y)\r\n+\r\n+    def _cached_call(self, x):\r\n+        \"\"\"\r\n+        Computes the memoized transform `x => y`.\r\n+        \"\"\"\r\n+        x_old, y_old = self._cached_x_y\r\n+        if x is x_old:\r\n+            return y_old\r\n+        y = self._call(x)\r\n+        self._cached_x_y = x, y\r\n+        return y\r\n+\r\n+    def _cached_inverse(self, y):\r\n+        \"\"\"\r\n+        Inverts the memoized transform `y => x`.\r\n+        \"\"\"\r\n+        x_old, y_old = self._cached_x_y\r\n+        if y is y_old:\r\n+            return x_old\r\n+        x = self._inverse(y)\r\n+        self._cached_x_y = x, y\r\n+        return x\r\n+\r\n+    def _call(self, x):\r\n+        \"\"\"\r\n+        Abstract method to compute forward transformation.\r\n+        \"\"\"\r\n+        raise NotImplementedError\r\n+\r\n+    def _inverse(self, y):\r\n+        \"\"\"\r\n+        Abstract method to compute inverse transformation.\r\n+        \"\"\"\r\n+        raise NotImplementedError\r\n+\r\n+    def log_abs_det_jacobian(self, x, y):\r\n+        \"\"\"\r\n+        Computes the log det jacobian `log |dy/dx|` given input and output.\r\n+        \"\"\"\r\n+        raise NotImplementedError\r\n+\r\n+\r\n+class InverseTransform(Transform):\r\n+    \"\"\"\r\n+    Inverts a single :class:`Transform`.\r\n+    \"\"\"\r\n+    __slots__ = ['inv']\r\n+\r\n+    def __init__(self, transform, cache_size=0):\r\n+        self.inv = transform\r\n+        super(InverseTransform, self).__init__(cache_size=cache_size)\r\n+\r\n+    @constraints.dependent_property\r\n+    def domain(self):\r\n+        return self.inv.codomain\r\n+\r\n+    @constraints.dependent_property\r\n+    def codomain(self):\r\n+        return self.inv.domain\r\n+\r\n+    @property\r\n+    def bijective(self):\r\n+        return self.inv.bijective\r\n+\r\n+    def __eq__(self, other):\r\n+        if not isinstance(other, InverseTransform):\r\n+            return False\r\n+        return self.inv == other.inv\r\n+\r\n+    def _call(self, x):\r\n+        return self.inv.inverse(x)\r\n+\r\n+    def _inverse(self, y):\r\n+        return self.inv.__call__(y)\r", "path": "torch/distributions/transforms.py", "position": null, "original_position": 152, "commit_id": "244932097058605559f6baa57da01c50b32a4131", "original_commit_id": "f40c9fe6c20514bb302e4bdd9a395d00e3b53f54", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Shouldn't this call `_call` (since `__call__` might do some extra things like caching)?", "created_at": "2018-01-27T23:45:33Z", "updated_at": "2018-11-23T15:38:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164284703", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4771", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164284703"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164284703"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4771"}}, "body_html": "<p>Shouldn't this call <code>_call</code> (since <code>__call__</code> might do some extra things like caching)?</p>", "body_text": "Shouldn't this call _call (since __call__ might do some extra things like caching)?"}