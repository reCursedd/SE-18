{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164284658", "pull_request_review_id": 92039105, "id": 164284658, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDI4NDY1OA==", "diff_hunk": "@@ -0,0 +1,342 @@\n+import torch\r\n+from torch.distributions import constraints\r\n+from torch.distributions.utils import broadcast_all, lazy_property\r\n+from torch.nn.functional import sigmoid\r\n+\r\n+__all__ = [\r\n+    'AbsTransform',\r\n+    'AffineTransform',\r\n+    'BoltzmannTransform',\r\n+    'ExpTransform',\r\n+    'InverseTransform',\r\n+    'LowerCholeskyTransform',\r\n+    'SigmoidTransform',\r\n+    'StickBreakingTransform',\r\n+    'Transform',\r\n+]\r\n+\r\n+\r\n+class Transform(object):\r\n+    \"\"\"\r\n+    Abstract class for invertable transformations with computable log\r\n+    det jacobians. They are primarily used in\r\n+    :class:`torch.distributions.TransformedDistribution`.\r\n+\r\n+    Caching is useful for tranforms whose inverses are either expensive or\r\n+    numerically unstable. Note that care must be taken with memoized values\r\n+    since the autograd graph may be reversed. For example while the following\r\n+    works with or without caching::\r\n+\r\n+        y = t(x)\r\n+        t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\r\n+\r\n+    However the following will error when caching due to dependency reversal::\r\n+\r\n+        y = t(x)\r\n+        z = t.inv(y)\r\n+        grad(z.sum(), [y])  # error because z is x\r\n+\r\n+    Derived classes should implement one or both of :meth:`_call` or\r\n+    :meth:`_inverse`. Derived classes that set `bijective=True` should also\r\n+    implement :meth:`log_abs_det_jacobian`.\r\n+    \"\"\"\r\n+    bijective = False\r\n+\r\n+    def __init__(self, cache_size=0):\r\n+        if cache_size != 0:\r\n+            if cache_size == 1:\r\n+                self._cached_x_y = None, None\r\n+                self.__call__ = self._cached_call\r\n+                self.inverse = self._cached_inverse\r\n+            else:\r\n+                raise NotImplementedError('cache_size must be 0 or 1')\r\n+\r\n+    @lazy_property\r\n+    def inv(self):\r\n+        \"\"\"\r\n+        Returns the inverse :class:`Transform` of this transform.\r\n+        \"\"\"\r\n+        return InverseTransform(self)\r\n+\r\n+    def __eq__(self, other):\r\n+        return self is other\r\n+\r\n+    def __ne__(self, other):\r\n+        # Necessary for Python2\r\n+        return not self.__eq__(other)\r\n+\r\n+    def __call__(self, x):\r\n+        \"\"\"\r\n+        Computes the transform `x => y`.\r\n+        \"\"\"\r\n+        return self._call(x)\r\n+\r\n+    def inverse(self, y):\r", "path": "torch/distributions/transforms.py", "position": null, "original_position": 74, "commit_id": "244932097058605559f6baa57da01c50b32a4131", "original_commit_id": "f40c9fe6c20514bb302e4bdd9a395d00e3b53f54", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "is there any reason to keep this method when you can just do `dist.inv(x)`? They should be equivalent", "created_at": "2018-01-27T23:42:42Z", "updated_at": "2018-11-23T15:38:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164284658", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4771", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164284658"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164284658"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4771"}}, "body_html": "<p>is there any reason to keep this method when you can just do <code>dist.inv(x)</code>? They should be equivalent</p>", "body_text": "is there any reason to keep this method when you can just do dist.inv(x)? They should be equivalent"}