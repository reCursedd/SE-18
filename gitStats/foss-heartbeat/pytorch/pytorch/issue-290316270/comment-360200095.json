{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/360200095", "html_url": "https://github.com/pytorch/pytorch/pull/4771#issuecomment-360200095", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4771", "id": 360200095, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDIwMDA5NQ==", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-24T16:55:15Z", "updated_at": "2018-01-24T20:34:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1093846\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alicanb\">@alicanb</a> Here's some mathematical justification for <code>__call__</code> operating on data rather than distributions. Mathematically transforms operate pointwise as differentiable functions. When we apply these to distributions, we're really applying these to a (<code>.sample()</code>, <code>.log_prob()</code>) pair of functions bundled in a Python class. When we apply a transform to <code>.sample()</code> we're really applying a push-forward, a lifted version of the transform, to <code>.sample()</code>. When we apply a transform to <code>.log_prob()</code>, we're really applying a preimage map (or a pullback?) to the density function. Even in mathematical notation, these operations are written as <code>t(x)</code> for pointwise operation, <code>t_&gt;(s)</code> or <code>t o s</code> for operation on a sampler, and <code>t^&lt;(p)</code> or <code>p o t</code> for operation on a density.</p>\n<p>That is, a pointwise operation <code>t</code> <em>is</em> an operation on data, but it <em>induces</em> operations on the space of samplers and densities and therefore distributions. Moreover it is out of scope of torch.distributions.transforms to handle transforms that do not act pointwise, e.g. convolution transforms or Fourier transforms that naturally operate on distributions.</p>\n<p>That said, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> suggests that we could support both notations via type overloading.</p>", "body_text": "@alicanb Here's some mathematical justification for __call__ operating on data rather than distributions. Mathematically transforms operate pointwise as differentiable functions. When we apply these to distributions, we're really applying these to a (.sample(), .log_prob()) pair of functions bundled in a Python class. When we apply a transform to .sample() we're really applying a push-forward, a lifted version of the transform, to .sample(). When we apply a transform to .log_prob(), we're really applying a preimage map (or a pullback?) to the density function. Even in mathematical notation, these operations are written as t(x) for pointwise operation, t_>(s) or t o s for operation on a sampler, and t^<(p) or p o t for operation on a density.\nThat is, a pointwise operation t is an operation on data, but it induces operations on the space of samplers and densities and therefore distributions. Moreover it is out of scope of torch.distributions.transforms to handle transforms that do not act pointwise, e.g. convolution transforms or Fourier transforms that naturally operate on distributions.\nThat said, @apaszke suggests that we could support both notations via type overloading.", "body": "@alicanb Here's some mathematical justification for `__call__` operating on data rather than distributions. Mathematically transforms operate pointwise as differentiable functions. When we apply these to distributions, we're really applying these to a (`.sample()`, `.log_prob()`) pair of functions bundled in a Python class. When we apply a transform to `.sample()` we're really applying a push-forward, a lifted version of the transform, to `.sample()`. When we apply a transform to `.log_prob()`, we're really applying a preimage map (or a pullback?) to the density function. Even in mathematical notation, these operations are written as `t(x)` for pointwise operation, `t_>(s)` or `t o s` for operation on a sampler, and `t^<(p)` or `p o t` for operation on a density.\r\n\r\nThat is, a pointwise operation `t` *is* an operation on data, but it *induces* operations on the space of samplers and densities and therefore distributions. Moreover it is out of scope of torch.distributions.transforms to handle transforms that do not act pointwise, e.g. convolution transforms or Fourier transforms that naturally operate on distributions.\r\n\r\nThat said, @apaszke suggests that we could support both notations via type overloading."}