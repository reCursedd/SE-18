{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164293107", "pull_request_review_id": 92046673, "id": 164293107, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDI5MzEwNw==", "diff_hunk": "@@ -0,0 +1,342 @@\n+import torch\r\n+from torch.distributions import constraints\r\n+from torch.distributions.utils import broadcast_all, lazy_property\r\n+from torch.nn.functional import sigmoid\r\n+\r\n+__all__ = [\r\n+    'AbsTransform',\r\n+    'AffineTransform',\r\n+    'BoltzmannTransform',\r\n+    'ExpTransform',\r\n+    'InverseTransform',\r\n+    'LowerCholeskyTransform',\r\n+    'SigmoidTransform',\r\n+    'StickBreakingTransform',\r\n+    'Transform',\r\n+]\r\n+\r\n+\r\n+class Transform(object):\r\n+    \"\"\"\r\n+    Abstract class for invertable transformations with computable log\r\n+    det jacobians. They are primarily used in\r\n+    :class:`torch.distributions.TransformedDistribution`.\r\n+\r\n+    Caching is useful for tranforms whose inverses are either expensive or\r\n+    numerically unstable. Note that care must be taken with memoized values\r\n+    since the autograd graph may be reversed. For example while the following\r\n+    works with or without caching::\r\n+\r\n+        y = t(x)\r\n+        t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\r\n+\r\n+    However the following will error when caching due to dependency reversal::\r\n+\r\n+        y = t(x)\r\n+        z = t.inv(y)\r\n+        grad(z.sum(), [y])  # error because z is x\r\n+\r\n+    Derived classes should implement one or both of :meth:`_call` or\r\n+    :meth:`_inverse`. Derived classes that set `bijective=True` should also\r\n+    implement :meth:`log_abs_det_jacobian`.\r\n+    \"\"\"\r\n+    bijective = False\r\n+\r\n+    def __init__(self, cache_size=0):\r\n+        if cache_size != 0:\r\n+            if cache_size == 1:\r\n+                self._cached_x_y = None, None\r\n+                self.__call__ = self._cached_call\r\n+                self.inverse = self._cached_inverse\r\n+            else:\r\n+                raise NotImplementedError('cache_size must be 0 or 1')\r\n+\r\n+    @lazy_property\r\n+    def inv(self):\r\n+        \"\"\"\r\n+        Returns the inverse :class:`Transform` of this transform.\r\n+        \"\"\"\r\n+        return InverseTransform(self)\r\n+\r\n+    def __eq__(self, other):\r\n+        return self is other\r\n+\r\n+    def __ne__(self, other):\r\n+        # Necessary for Python2\r\n+        return not self.__eq__(other)\r\n+\r\n+    def __call__(self, x):\r\n+        \"\"\"\r\n+        Computes the transform `x => y`.\r\n+        \"\"\"\r\n+        return self._call(x)\r\n+\r\n+    def inverse(self, y):\r", "path": "torch/distributions/transforms.py", "position": null, "original_position": 74, "commit_id": "244932097058605559f6baa57da01c50b32a4131", "original_commit_id": "f40c9fe6c20514bb302e4bdd9a395d00e3b53f54", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "Good point. I was able to make this private as `._inv_call()`, but it is still needed to implement `.inv.__call__()`. We want a single object to manage both the `.__call__()` cache and the `.inv.__call__()` cache so that calling one populates the other. Thus `_Inverse` delegates caching to its `.inv` transform.", "created_at": "2018-01-28T08:37:26Z", "updated_at": "2018-11-23T15:38:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164293107", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4771", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164293107"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164293107"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4771"}}, "body_html": "<p>Good point. I was able to make this private as <code>._inv_call()</code>, but it is still needed to implement <code>.inv.__call__()</code>. We want a single object to manage both the <code>.__call__()</code> cache and the <code>.inv.__call__()</code> cache so that calling one populates the other. Thus <code>_Inverse</code> delegates caching to its <code>.inv</code> transform.</p>", "body_text": "Good point. I was able to make this private as ._inv_call(), but it is still needed to implement .inv.__call__(). We want a single object to manage both the .__call__() cache and the .inv.__call__() cache so that calling one populates the other. Thus _Inverse delegates caching to its .inv transform.", "in_reply_to_id": 164284658}