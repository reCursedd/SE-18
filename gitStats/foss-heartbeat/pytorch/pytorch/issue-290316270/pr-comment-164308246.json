{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164308246", "pull_request_review_id": 92062126, "id": 164308246, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDMwODI0Ng==", "diff_hunk": "@@ -0,0 +1,435 @@\n+import weakref\n+\n+import torch\n+from torch.autograd import Variable\n+from torch.distributions import constraints\n+from torch.distributions.utils import broadcast_all, lazy_property\n+from torch.nn.functional import sigmoid\n+\n+__all__ = [\n+    'AbsTransform',\n+    'AffineTransform',\n+    'BoltzmannTransform',\n+    'ComposeTransform',\n+    'ExpTransform',\n+    'LowerCholeskyTransform',\n+    'SigmoidTransform',\n+    'StickBreakingTransform',\n+    'Transform',\n+    'identity_transform',\n+]\n+\n+\n+class Transform(object):\n+    \"\"\"\n+    Abstract class for invertable transformations with computable log\n+    det jacobians. They are primarily used in\n+    :class:`torch.distributions.TransformedDistribution`.\n+\n+    Caching is useful for tranforms whose inverses are either expensive or\n+    numerically unstable. Note that care must be taken with memoized values\n+    since the autograd graph may be reversed. For example while the following\n+    works with or without caching::\n+\n+        y = t(x)\n+        t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n+\n+    However the following will error when caching due to dependency reversal::\n+\n+        y = t(x)\n+        z = t.inv(y)\n+        grad(z.sum(), [y])  # error because z is x\n+\n+    Derived classes should implement one or both of :meth:`_call` or\n+    :meth:`_inverse`. Derived classes that set `bijective=True` should also\n+    implement :meth:`log_abs_det_jacobian`.\n+\n+    Args:\n+        cache_size (int): Size of cache. If zero, no caching is done. If one,\n+            the latest single value is cached. Only 0 and 1 are supported.\n+\n+    Attributes:\n+        domain (:class:`~torch.distributions.constraints.Constraint`):\n+            The constraint representing valid inputs to this transform.\n+        codomain (:class:`~torch.distributions.constraints.Constraint`):\n+            The constraint representing valid outputs to this transform\n+            which are inputs to the inverse transform.\n+        bijective (bool): Whether this transform is bijective. A transform\n+            ``t`` is bijective iff ``t.inv(t(x)) == x`` and\n+            ``t(t.inv(y)) == y`` for every ``x`` in the domain and ``y`` in\n+            the codomain. Transforms that are not bijective should at least\n+            maintain the weaker pseudoinverse properties\n+            ``t(t.inv(t(x)) == t(x)`` and ``t.inv(t(t.inv(y))) == t.inv(y)``.\n+    \"\"\"\n+    bijective = False\n+\n+    def __init__(self, cache_size=0):\n+        if cache_size == 0:\n+            pass  # default behavior\n+        elif cache_size == 1:\n+            self._cached_x_y = None, None\n+        else:\n+            raise ValueError('cache_size must be 0 or 1')\n+\n+    @property\n+    def inv(self):\n+        \"\"\"\n+        Returns the inverse :class:`Transform` of this transform.\n+        This should satisfy ``t.inv.inv is t``.\n+        \"\"\"\n+        inv = None\n+        try:\n+            inv = self._inv()\n+        except AttributeError:\n+            pass\n+        if inv is None:\n+            inv = _InverseTransform(self)\n+            self._inv = weakref.ref(inv)\n+        return inv\n+\n+    def __eq__(self, other):\n+        return self is other\n+\n+    def __ne__(self, other):\n+        # Necessary for Python2\n+        return not self.__eq__(other)\n+\n+    def __call__(self, x):\n+        \"\"\"\n+        Computes the transform `x => y`.\n+        \"\"\"\n+        try:\n+            x_old, y_old = self._cached_x_y\n+        except AttributeError:\n+            return self._call(x)\n+        if x is x_old:\n+            return y_old\n+        y = self._call(x)\n+        self._cached_x_y = x, y\n+        return y", "path": "torch/distributions/transforms.py", "position": 108, "original_position": 109, "commit_id": "244932097058605559f6baa57da01c50b32a4131", "original_commit_id": "43908738e32904ee0643f691c6b123dcde0e286c", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "What's your preferred pattern here? `if hastattr`? I try to avoid `hasattr`.", "created_at": "2018-01-28T18:54:27Z", "updated_at": "2018-11-23T15:38:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164308246", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4771", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164308246"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164308246"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4771"}}, "body_html": "<p>What's your preferred pattern here? <code>if hastattr</code>? I try to avoid <code>hasattr</code>.</p>", "body_text": "What's your preferred pattern here? if hastattr? I try to avoid hasattr.", "in_reply_to_id": 164307944}