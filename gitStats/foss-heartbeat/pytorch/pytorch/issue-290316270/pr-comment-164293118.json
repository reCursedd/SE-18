{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164293118", "pull_request_review_id": 92046673, "id": 164293118, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NDI5MzExOA==", "diff_hunk": "@@ -0,0 +1,342 @@\n+import torch\r\n+from torch.distributions import constraints\r\n+from torch.distributions.utils import broadcast_all, lazy_property\r\n+from torch.nn.functional import sigmoid\r\n+\r\n+__all__ = [\r\n+    'AbsTransform',\r\n+    'AffineTransform',\r\n+    'BoltzmannTransform',\r\n+    'ExpTransform',\r\n+    'InverseTransform',\r\n+    'LowerCholeskyTransform',\r\n+    'SigmoidTransform',\r\n+    'StickBreakingTransform',\r\n+    'Transform',\r\n+]\r\n+\r\n+\r\n+class Transform(object):\r\n+    \"\"\"\r\n+    Abstract class for invertable transformations with computable log\r\n+    det jacobians. They are primarily used in\r\n+    :class:`torch.distributions.TransformedDistribution`.\r\n+\r\n+    Caching is useful for tranforms whose inverses are either expensive or\r\n+    numerically unstable. Note that care must be taken with memoized values\r\n+    since the autograd graph may be reversed. For example while the following\r\n+    works with or without caching::\r\n+\r\n+        y = t(x)\r\n+        t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\r\n+\r\n+    However the following will error when caching due to dependency reversal::\r\n+\r\n+        y = t(x)\r\n+        z = t.inv(y)\r\n+        grad(z.sum(), [y])  # error because z is x\r\n+\r\n+    Derived classes should implement one or both of :meth:`_call` or\r\n+    :meth:`_inverse`. Derived classes that set `bijective=True` should also\r\n+    implement :meth:`log_abs_det_jacobian`.\r\n+    \"\"\"\r\n+    bijective = False\r\n+\r\n+    def __init__(self, cache_size=0):\r\n+        if cache_size != 0:\r\n+            if cache_size == 1:\r\n+                self._cached_x_y = None, None\r\n+                self.__call__ = self._cached_call\r\n+                self.inverse = self._cached_inverse\r\n+            else:\r\n+                raise NotImplementedError('cache_size must be 0 or 1')\r\n+\r\n+    @lazy_property\r\n+    def inv(self):\r\n+        \"\"\"\r\n+        Returns the inverse :class:`Transform` of this transform.\r\n+        \"\"\"\r\n+        return InverseTransform(self)\r\n+\r\n+    def __eq__(self, other):\r\n+        return self is other\r\n+\r\n+    def __ne__(self, other):\r\n+        # Necessary for Python2\r\n+        return not self.__eq__(other)\r\n+\r\n+    def __call__(self, x):\r\n+        \"\"\"\r\n+        Computes the transform `x => y`.\r\n+        \"\"\"\r\n+        return self._call(x)\r\n+\r\n+    def inverse(self, y):\r\n+        \"\"\"\r\n+        Inverts the transform `y => x`.\r\n+        \"\"\"\r\n+        return self._inverse(y)\r\n+\r\n+    def _cached_call(self, x):\r\n+        \"\"\"\r\n+        Computes the memoized transform `x => y`.\r\n+        \"\"\"\r\n+        x_old, y_old = self._cached_x_y\r\n+        if x is x_old:\r\n+            return y_old\r\n+        y = self._call(x)\r\n+        self._cached_x_y = x, y\r\n+        return y\r\n+\r\n+    def _cached_inverse(self, y):\r\n+        \"\"\"\r\n+        Inverts the memoized transform `y => x`.\r\n+        \"\"\"\r\n+        x_old, y_old = self._cached_x_y\r\n+        if y is y_old:\r\n+            return x_old\r\n+        x = self._inverse(y)\r\n+        self._cached_x_y = x, y\r\n+        return x\r\n+\r\n+    def _call(self, x):\r\n+        \"\"\"\r\n+        Abstract method to compute forward transformation.\r\n+        \"\"\"\r\n+        raise NotImplementedError\r\n+\r\n+    def _inverse(self, y):\r\n+        \"\"\"\r\n+        Abstract method to compute inverse transformation.\r\n+        \"\"\"\r\n+        raise NotImplementedError\r\n+\r\n+    def log_abs_det_jacobian(self, x, y):\r\n+        \"\"\"\r\n+        Computes the log det jacobian `log |dy/dx|` given input and output.\r\n+        \"\"\"\r\n+        raise NotImplementedError\r\n+\r\n+\r\n+class InverseTransform(Transform):\r\n+    \"\"\"\r\n+    Inverts a single :class:`Transform`.\r\n+    \"\"\"\r\n+    __slots__ = ['inv']\r", "path": "torch/distributions/transforms.py", "position": null, "original_position": 125, "commit_id": "244932097058605559f6baa57da01c50b32a4131", "original_commit_id": "f40c9fe6c20514bb302e4bdd9a395d00e3b53f54", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "Thanks, I didn't know about these disadvantages. Removed.", "created_at": "2018-01-28T08:37:51Z", "updated_at": "2018-11-23T15:38:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164293118", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4771", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/164293118"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4771#discussion_r164293118"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4771"}}, "body_html": "<p>Thanks, I didn't know about these disadvantages. Removed.</p>", "body_text": "Thanks, I didn't know about these disadvantages. Removed.", "in_reply_to_id": 164284699}