{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150649950", "pull_request_review_id": 76231447, "id": 150649950, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MDY0OTk1MA==", "diff_hunk": "@@ -0,0 +1,288 @@\n+#include \"interpreter.h\"\n+#include \"torch/csrc/jit/ir.h\"\n+#include \"torch/csrc/jit/generated/aten_dispatch.h\"\n+#ifdef WITH_CUDA\n+#include \"torch/csrc/jit/fusion_compiler.h\"\n+#endif\n+\n+namespace torch { namespace jit {", "path": "torch/csrc/jit/interpreter.cpp", "position": 8, "original_position": 8, "commit_id": "8421d51b4f4545fc569a2522ca31ebdb0796a7ac", "original_commit_id": "71f42be9e2351f659845bfda434e4d5b303f49e5", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "I didn't add optimizations after the fact. It's easier to start with good perf than try to optimize later. Instead there were two goals:\r\n\r\n1. Don't allocate memory when you running (there is a single malloc when the Interpreter is created)\r\n2. Scan linearly through memory as much as possible to be friendly to cache.\r\n\r\nIt is not clear if either of these were necessary but in some sense it is easier writing it this way, then writing it another way and then trying all optimizations.", "created_at": "2017-11-13T20:04:18Z", "updated_at": "2018-11-23T15:36:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/3634#discussion_r150649950", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3634", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/150649950"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3634#discussion_r150649950"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3634"}}, "body_html": "<p>I didn't add optimizations after the fact. It's easier to start with good perf than try to optimize later. Instead there were two goals:</p>\n<ol>\n<li>Don't allocate memory when you running (there is a single malloc when the Interpreter is created)</li>\n<li>Scan linearly through memory as much as possible to be friendly to cache.</li>\n</ol>\n<p>It is not clear if either of these were necessary but in some sense it is easier writing it this way, then writing it another way and then trying all optimizations.</p>", "body_text": "I didn't add optimizations after the fact. It's easier to start with good perf than try to optimize later. Instead there were two goals:\n\nDon't allocate memory when you running (there is a single malloc when the Interpreter is created)\nScan linearly through memory as much as possible to be friendly to cache.\n\nIt is not clear if either of these were necessary but in some sense it is easier writing it this way, then writing it another way and then trying all optimizations.", "in_reply_to_id": 150382775}