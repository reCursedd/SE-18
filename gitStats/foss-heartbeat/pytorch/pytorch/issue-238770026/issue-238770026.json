{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1919", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1919/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1919/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1919/events", "html_url": "https://github.com/pytorch/pytorch/issues/1919", "id": 238770026, "node_id": "MDU6SXNzdWUyMzg3NzAwMjY=", "number": 1919, "title": "Errors when num_workers is set to be value bigger than 0 in torch.utils.data.DataLoader", "user": {"login": "eriche2016", "id": 11784910, "node_id": "MDQ6VXNlcjExNzg0OTEw", "avatar_url": "https://avatars2.githubusercontent.com/u/11784910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eriche2016", "html_url": "https://github.com/eriche2016", "followers_url": "https://api.github.com/users/eriche2016/followers", "following_url": "https://api.github.com/users/eriche2016/following{/other_user}", "gists_url": "https://api.github.com/users/eriche2016/gists{/gist_id}", "starred_url": "https://api.github.com/users/eriche2016/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eriche2016/subscriptions", "organizations_url": "https://api.github.com/users/eriche2016/orgs", "repos_url": "https://api.github.com/users/eriche2016/repos", "events_url": "https://api.github.com/users/eriche2016/events{/privacy}", "received_events_url": "https://api.github.com/users/eriche2016/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-27T08:12:41Z", "updated_at": "2017-06-27T21:01:25Z", "closed_at": "2017-06-27T21:01:25Z", "author_association": "NONE", "body_html": "<p>My dataset is in <code>.h5</code> format. When i wrap it using torch.utils.data.DataLoader, and if i set num_workers to be 0, everything works fine, However if it is set to be 2, then the labels batch (a 1-D byte tensor)it loads at some epoch always seems to be bigger than the numer of classes(=40), which is 255. what causes this problem? any help?<br>\nmy code is below:</p>\n<pre><code>from __future__ import print_function\nimport torch.utils.data as data\nimport os\nimport os.path\nimport errno\nimport torch\nimport json\nimport h5py\n\nfrom IPython.core.debugger import Tracer \ndebug_here = Tracer() \n\nimport numpy as np\nimport sys\n\nimport json\n\n\nclass Modelnet40_V12_Dataset(data.Dataset):\n    def __init__(self, data_dir, image_size = 224, train=True):\n        self.image_size = image_size\n        self.data_dir = data_dir\n        self.train = train \n\n        file_path = os.path.join(self.data_dir, 'modelnet40.h5')\n        self.modelnet40_data = h5py.File(file_path)\n        \n        if self.train: \n            self.train_data = self.modelnet40_data['train']['data']\n            self.train_labels = self.modelnet40_data['train']['label']\n        else:\n            self.test_data = self.modelnet40_data['test']['data']\n            self.test_labels = self.modelnet40_data['test']['label']\n\n    def __getitem__(self, index):\n        if self.train:\n            shape_12v, label = self.train_data[index], self.train_labels[index] \n        else:\n            shape_12v, label = self.test_data[index], self.test_labels[index]\n        return shape_12v, label \n\n\n    def __len__(self):\n        if self.train:\n            return self.train_data.shape[0]\n        else:\n            return self.test_data.shape[0]\n\nif __name__ == '__main__':\n    print('test')\n    train_dataset = Modelnet40_V12_Dataset(data_dir='path/data', train=True)\n    print(len(train_dataset))\n\n    test_dataset = Modelnet40_V12_Dataset(data_dir='path/data', train=False)\n    print(len(test_dataset))\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8,\n                                     shuffle=True, num_workers=2)\n\n\n    total = 0 \n    # debug_here() \n    # check when to cause labels error \n    for epoch in range(200):\n        print('epoch', epoch)\n        for i, (input_v, labels) in enumerate(train_loader):\n            total = total + labels.size(0)\n\n            # labels can be 255, what is the problem??\n            if labels.max() &gt; 40: \n                debug_here() \n                print('error')\n\n            if labels.min() &lt; 1:\n                debug_here()  \n                print('error')\n            \n            labels.sub_(1) # minus 1 in place \n            \n            if labels.max() &gt;= 40: \n                debug_here() \n                print('error')\n\n            if labels.min() &lt; 0:\n                debug_here()  \n                print('error')\n        print(total)\n</code></pre>", "body_text": "My dataset is in .h5 format. When i wrap it using torch.utils.data.DataLoader, and if i set num_workers to be 0, everything works fine, However if it is set to be 2, then the labels batch (a 1-D byte tensor)it loads at some epoch always seems to be bigger than the numer of classes(=40), which is 255. what causes this problem? any help?\nmy code is below:\nfrom __future__ import print_function\nimport torch.utils.data as data\nimport os\nimport os.path\nimport errno\nimport torch\nimport json\nimport h5py\n\nfrom IPython.core.debugger import Tracer \ndebug_here = Tracer() \n\nimport numpy as np\nimport sys\n\nimport json\n\n\nclass Modelnet40_V12_Dataset(data.Dataset):\n    def __init__(self, data_dir, image_size = 224, train=True):\n        self.image_size = image_size\n        self.data_dir = data_dir\n        self.train = train \n\n        file_path = os.path.join(self.data_dir, 'modelnet40.h5')\n        self.modelnet40_data = h5py.File(file_path)\n        \n        if self.train: \n            self.train_data = self.modelnet40_data['train']['data']\n            self.train_labels = self.modelnet40_data['train']['label']\n        else:\n            self.test_data = self.modelnet40_data['test']['data']\n            self.test_labels = self.modelnet40_data['test']['label']\n\n    def __getitem__(self, index):\n        if self.train:\n            shape_12v, label = self.train_data[index], self.train_labels[index] \n        else:\n            shape_12v, label = self.test_data[index], self.test_labels[index]\n        return shape_12v, label \n\n\n    def __len__(self):\n        if self.train:\n            return self.train_data.shape[0]\n        else:\n            return self.test_data.shape[0]\n\nif __name__ == '__main__':\n    print('test')\n    train_dataset = Modelnet40_V12_Dataset(data_dir='path/data', train=True)\n    print(len(train_dataset))\n\n    test_dataset = Modelnet40_V12_Dataset(data_dir='path/data', train=False)\n    print(len(test_dataset))\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8,\n                                     shuffle=True, num_workers=2)\n\n\n    total = 0 \n    # debug_here() \n    # check when to cause labels error \n    for epoch in range(200):\n        print('epoch', epoch)\n        for i, (input_v, labels) in enumerate(train_loader):\n            total = total + labels.size(0)\n\n            # labels can be 255, what is the problem??\n            if labels.max() > 40: \n                debug_here() \n                print('error')\n\n            if labels.min() < 1:\n                debug_here()  \n                print('error')\n            \n            labels.sub_(1) # minus 1 in place \n            \n            if labels.max() >= 40: \n                debug_here() \n                print('error')\n\n            if labels.min() < 0:\n                debug_here()  \n                print('error')\n        print(total)", "body": "My dataset is in ```.h5``` format. When i wrap it using torch.utils.data.DataLoader, and if i set num_workers to be 0, everything works fine, However if it is set to be 2, then the labels batch (a 1-D byte tensor)it loads at some epoch always seems to be bigger than the numer of classes(=40), which is 255. what causes this problem? any help? \r\nmy code is below:\r\n```\r\nfrom __future__ import print_function\r\nimport torch.utils.data as data\r\nimport os\r\nimport os.path\r\nimport errno\r\nimport torch\r\nimport json\r\nimport h5py\r\n\r\nfrom IPython.core.debugger import Tracer \r\ndebug_here = Tracer() \r\n\r\nimport numpy as np\r\nimport sys\r\n\r\nimport json\r\n\r\n\r\nclass Modelnet40_V12_Dataset(data.Dataset):\r\n    def __init__(self, data_dir, image_size = 224, train=True):\r\n        self.image_size = image_size\r\n        self.data_dir = data_dir\r\n        self.train = train \r\n\r\n        file_path = os.path.join(self.data_dir, 'modelnet40.h5')\r\n        self.modelnet40_data = h5py.File(file_path)\r\n        \r\n        if self.train: \r\n            self.train_data = self.modelnet40_data['train']['data']\r\n            self.train_labels = self.modelnet40_data['train']['label']\r\n        else:\r\n            self.test_data = self.modelnet40_data['test']['data']\r\n            self.test_labels = self.modelnet40_data['test']['label']\r\n\r\n    def __getitem__(self, index):\r\n        if self.train:\r\n            shape_12v, label = self.train_data[index], self.train_labels[index] \r\n        else:\r\n            shape_12v, label = self.test_data[index], self.test_labels[index]\r\n        return shape_12v, label \r\n\r\n\r\n    def __len__(self):\r\n        if self.train:\r\n            return self.train_data.shape[0]\r\n        else:\r\n            return self.test_data.shape[0]\r\n\r\nif __name__ == '__main__':\r\n    print('test')\r\n    train_dataset = Modelnet40_V12_Dataset(data_dir='path/data', train=True)\r\n    print(len(train_dataset))\r\n\r\n    test_dataset = Modelnet40_V12_Dataset(data_dir='path/data', train=False)\r\n    print(len(test_dataset))\r\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8,\r\n                                     shuffle=True, num_workers=2)\r\n\r\n\r\n    total = 0 \r\n    # debug_here() \r\n    # check when to cause labels error \r\n    for epoch in range(200):\r\n        print('epoch', epoch)\r\n        for i, (input_v, labels) in enumerate(train_loader):\r\n            total = total + labels.size(0)\r\n\r\n            # labels can be 255, what is the problem??\r\n            if labels.max() > 40: \r\n                debug_here() \r\n                print('error')\r\n\r\n            if labels.min() < 1:\r\n                debug_here()  \r\n                print('error')\r\n            \r\n            labels.sub_(1) # minus 1 in place \r\n            \r\n            if labels.max() >= 40: \r\n                debug_here() \r\n                print('error')\r\n\r\n            if labels.min() < 0:\r\n                debug_here()  \r\n                print('error')\r\n        print(total)\r\n```"}