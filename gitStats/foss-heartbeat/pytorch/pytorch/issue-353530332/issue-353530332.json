{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10830", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10830/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10830/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10830/events", "html_url": "https://github.com/pytorch/pytorch/issues/10830", "id": 353530332, "node_id": "MDU6SXNzdWUzNTM1MzAzMzI=", "number": 10830, "title": "[Bug] not clearing memory after slicing tensor", "user": {"login": "aldopareja", "id": 7622817, "node_id": "MDQ6VXNlcjc2MjI4MTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7622817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aldopareja", "html_url": "https://github.com/aldopareja", "followers_url": "https://api.github.com/users/aldopareja/followers", "following_url": "https://api.github.com/users/aldopareja/following{/other_user}", "gists_url": "https://api.github.com/users/aldopareja/gists{/gist_id}", "starred_url": "https://api.github.com/users/aldopareja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aldopareja/subscriptions", "organizations_url": "https://api.github.com/users/aldopareja/orgs", "repos_url": "https://api.github.com/users/aldopareja/repos", "events_url": "https://api.github.com/users/aldopareja/events{/privacy}", "received_events_url": "https://api.github.com/users/aldopareja/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-23T19:56:15Z", "updated_at": "2018-08-27T17:30:11Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>It seems that pytorch has a memory leak when doing in-place slicing with tensors that require gradients.</p>\n<h2>Code example</h2>\n<p>This code runs out of memory on a K80 in GCP</p>\n<pre><code>import torch as tr\nimport os\n\ndevice = tr.device(\"cuda:0\" if tr.cuda.is_available() else \"cpu\")\nprint ('device:', device)\n\n#parameters\npars = tr.tensor([0.5],requires_grad=True,device=device)\n\n#data\ndata = tr.arange(1e9,device=device)\nprint(data)\n\n#sample\ndef sample(x,sample_size=int(1e5)):\n\tsample_idx = tr.randint(high=x.size()[0],size=(sample_size,),dtype=tr.long)\n\treturn data[sample_idx]\n\ndef leaveTopK(x,k):\n\t_,idx = tr.sort(x,descending=True)\n\tx = x[idx]\n\treturn x[:k]\n\n#memory leak loop\nout = tr.tensor([],device=device)\nfor i in range(int(1e4)):\n\tout = tr.cat([out,pars*sample(data)])\n\tout = leaveTopK(out,int(1e5))\n\t\n\tif i%1e3 == 0:\n\t\tos.system('nvidia-smi -q --display=MEMORY')\n</code></pre>\n<p>It should not run out of memory since the size of <em>out</em> is kept under 1e5 in the example, but it seems that auto-grad keeps reference to the sliced-out part of the <em>out</em> tensor. When setting requires_grad=False it does not run out of memory.</p>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Debian GNU/Linux 9.5 (stretch)<br>\nGCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.7<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.1.85<br>\nGPU models and configuration: GPU 0: Tesla K80<br>\nNvidia driver version: 390.46<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.3<br>\n/usr/local/cuda-9.1/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.12.1)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchvision (0.2.1)<br>\n[conda] pytorch                   0.4.1           py37_cuda9.0.176_cudnn7.1.2_1    pytorch</p>\n<p>I installed pytorch by installing anaconda.</p>", "body_text": "Issue description\nIt seems that pytorch has a memory leak when doing in-place slicing with tensors that require gradients.\nCode example\nThis code runs out of memory on a K80 in GCP\nimport torch as tr\nimport os\n\ndevice = tr.device(\"cuda:0\" if tr.cuda.is_available() else \"cpu\")\nprint ('device:', device)\n\n#parameters\npars = tr.tensor([0.5],requires_grad=True,device=device)\n\n#data\ndata = tr.arange(1e9,device=device)\nprint(data)\n\n#sample\ndef sample(x,sample_size=int(1e5)):\n\tsample_idx = tr.randint(high=x.size()[0],size=(sample_size,),dtype=tr.long)\n\treturn data[sample_idx]\n\ndef leaveTopK(x,k):\n\t_,idx = tr.sort(x,descending=True)\n\tx = x[idx]\n\treturn x[:k]\n\n#memory leak loop\nout = tr.tensor([],device=device)\nfor i in range(int(1e4)):\n\tout = tr.cat([out,pars*sample(data)])\n\tout = leaveTopK(out,int(1e5))\n\t\n\tif i%1e3 == 0:\n\t\tos.system('nvidia-smi -q --display=MEMORY')\n\nIt should not run out of memory since the size of out is kept under 1e5 in the example, but it seems that auto-grad keeps reference to the sliced-out part of the out tensor. When setting requires_grad=False it does not run out of memory.\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Debian GNU/Linux 9.5 (stretch)\nGCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\nCMake version: Could not collect\nPython version: 3.7\nIs CUDA available: Yes\nCUDA runtime version: 9.1.85\nGPU models and configuration: GPU 0: Tesla K80\nNvidia driver version: 390.46\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.3\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip3] numpy (1.12.1)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.1)\n[conda] pytorch                   0.4.1           py37_cuda9.0.176_cudnn7.1.2_1    pytorch\nI installed pytorch by installing anaconda.", "body": "## Issue description\r\n\r\nIt seems that pytorch has a memory leak when doing in-place slicing with tensors that require gradients.\r\n\r\n## Code example\r\nThis code runs out of memory on a K80 in GCP\r\n\r\n```\r\nimport torch as tr\r\nimport os\r\n\r\ndevice = tr.device(\"cuda:0\" if tr.cuda.is_available() else \"cpu\")\r\nprint ('device:', device)\r\n\r\n#parameters\r\npars = tr.tensor([0.5],requires_grad=True,device=device)\r\n\r\n#data\r\ndata = tr.arange(1e9,device=device)\r\nprint(data)\r\n\r\n#sample\r\ndef sample(x,sample_size=int(1e5)):\r\n\tsample_idx = tr.randint(high=x.size()[0],size=(sample_size,),dtype=tr.long)\r\n\treturn data[sample_idx]\r\n\r\ndef leaveTopK(x,k):\r\n\t_,idx = tr.sort(x,descending=True)\r\n\tx = x[idx]\r\n\treturn x[:k]\r\n\r\n#memory leak loop\r\nout = tr.tensor([],device=device)\r\nfor i in range(int(1e4)):\r\n\tout = tr.cat([out,pars*sample(data)])\r\n\tout = leaveTopK(out,int(1e5))\r\n\t\r\n\tif i%1e3 == 0:\r\n\t\tos.system('nvidia-smi -q --display=MEMORY')\r\n```\r\nIt should not run out of memory since the size of _out_ is kept under 1e5 in the example, but it seems that auto-grad keeps reference to the sliced-out part of the _out_ tensor. When setting requires_grad=False it does not run out of memory. \r\n\r\n## System Info\r\nCollecting environment information...\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Debian GNU/Linux 9.5 (stretch)\r\nGCC version: (Debian 6.3.0-18+deb9u1) 6.3.0 20170516\r\nCMake version: Could not collect\r\n\r\nPython version: 3.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.1.85\r\nGPU models and configuration: GPU 0: Tesla K80\r\nNvidia driver version: 390.46\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.1/lib64/libcudnn.so.7.1.3\r\n/usr/local/cuda-9.1/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.12.1)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.1           py37_cuda9.0.176_cudnn7.1.2_1    pytorch\r\n\r\nI installed pytorch by installing anaconda. \r\n"}