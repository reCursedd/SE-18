{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/203895765", "pull_request_review_id": 138879565, "id": 203895765, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMzg5NTc2NQ==", "diff_hunk": "@@ -93,35 +93,74 @@ def _enforce_cudnn(input):\n     @staticmethod\n     def forward(ctx, theta, size):\n         assert type(size) == torch.Size\n-        N, C, H, W = size\n-        ctx.size = size\n-        if theta.is_cuda:\n-            AffineGridGenerator._enforce_cudnn(theta)\n-            assert False\n-        ctx.is_cuda = False\n-        base_grid = theta.new(N, H, W, 3)\n-        linear_points = torch.linspace(-1, 1, W) if W > 1 else torch.Tensor([-1])\n-        base_grid[:, :, :, 0] = torch.ger(torch.ones(H), linear_points).expand_as(base_grid[:, :, :, 0])\n-        linear_points = torch.linspace(-1, 1, H) if H > 1 else torch.Tensor([-1])\n-        base_grid[:, :, :, 1] = torch.ger(linear_points, torch.ones(W)).expand_as(base_grid[:, :, :, 1])\n-        base_grid[:, :, :, 2] = 1\n-        ctx.base_grid = base_grid\n-        grid = torch.bmm(base_grid.view(N, H * W, 3), theta.transpose(1, 2))\n-        grid = grid.view(N, H, W, 2)\n+\n+        if len(size) == 5:\n+            N, C, D, H, W = size\n+            ctx.size = size\n+            ctx.is_cuda = theta.is_cuda\n+            base_grid = theta.new(N, D, H, W, 4)\n+\n+            w_points = (torch.linspace(-1, 1, W) if W > 1 else torch.Tensor([-1]))\n+            h_points = (torch.linspace(-1, 1, H) if H > 1 else torch.Tensor([-1])).unsqueeze(-1)\n+            d_points = (torch.linspace(-1, 1, D) if D > 1 else torch.Tensor([-1])).unsqueeze(-1).unsqueeze(-1)\n+\n+            base_grid[:, :, :, :, 0] = w_points\n+            base_grid[:, :, :, :, 1] = h_points\n+            base_grid[:, :, :, :, 2] = d_points\n+            base_grid[:, :, :, :, 3] = 1\n+            ctx.base_grid = base_grid\n+            grid = torch.bmm(base_grid.view(N, D * H * W, 4), theta.transpose(1, 2))\n+            grid = grid.view(N, D, H, W, 3)\n+\n+        elif len(size) == 4:\n+            N, C, H, W = size\n+            ctx.size = size\n+            if theta.is_cuda:\n+                AffineGridGenerator._enforce_cudnn(theta)\n+                assert False\n+            ctx.is_cuda = False\n+            base_grid = theta.new(N, H, W, 3)\n+            linear_points = torch.linspace(-1, 1, W) if W > 1 else torch.Tensor([-1])\n+            base_grid[:, :, :, 0] = torch.ger(torch.ones(H), linear_points).expand_as(base_grid[:, :, :, 0])\n+            linear_points = torch.linspace(-1, 1, H) if H > 1 else torch.Tensor([-1])\n+            base_grid[:, :, :, 1] = torch.ger(linear_points, torch.ones(W)).expand_as(base_grid[:, :, :, 1])\n+            base_grid[:, :, :, 2] = 1\n+            ctx.base_grid = base_grid\n+            grid = torch.bmm(base_grid.view(N, H * W, 3), theta.transpose(1, 2))\n+            grid = grid.view(N, H, W, 2)\n+        else:\n+            raise RuntimeError(\"AffineGridGenerator needs 4d (spatial) or 5d (volumetric) inputs.\")\n+\n         return grid\n \n     @staticmethod\n     @once_differentiable\n     def backward(ctx, grad_grid):\n-        N, C, H, W = ctx.size\n-        assert grad_grid.size() == torch.Size([N, H, W, 2])\n-        assert ctx.is_cuda == grad_grid.is_cuda\n-        if grad_grid.is_cuda:\n-            AffineGridGenerator._enforce_cudnn(grad_grid)\n+        if len(ctx.size) == 5:\n+            N, C, D, H, W = ctx.size", "path": "torch/nn/_functions/vision.py", "position": null, "original_position": 77, "commit_id": "b74a18beaf09cc711516acc6e9b88ca97b0bb2cf", "original_commit_id": "dd64ad7838614b7e9a91d049eae333a326ae19c0", "user": {"login": "elistevens", "id": 138016, "node_id": "MDQ6VXNlcjEzODAxNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/138016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elistevens", "html_url": "https://github.com/elistevens", "followers_url": "https://api.github.com/users/elistevens/followers", "following_url": "https://api.github.com/users/elistevens/following{/other_user}", "gists_url": "https://api.github.com/users/elistevens/gists{/gist_id}", "starred_url": "https://api.github.com/users/elistevens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elistevens/subscriptions", "organizations_url": "https://api.github.com/users/elistevens/orgs", "repos_url": "https://api.github.com/users/elistevens/repos", "events_url": "https://api.github.com/users/elistevens/events{/privacy}", "received_events_url": "https://api.github.com/users/elistevens/received_events", "type": "User", "site_admin": false}, "body": "I'm happy to consolidate. I originally didn't to make it clear that the 2D case hadn't meaningfully changed.", "created_at": "2018-07-19T22:49:44Z", "updated_at": "2018-11-23T15:47:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/8322#discussion_r203895765", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8322", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/203895765"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8322#discussion_r203895765"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8322"}}, "body_html": "<p>I'm happy to consolidate. I originally didn't to make it clear that the 2D case hadn't meaningfully changed.</p>", "body_text": "I'm happy to consolidate. I originally didn't to make it clear that the 2D case hadn't meaningfully changed.", "in_reply_to_id": 198215328}