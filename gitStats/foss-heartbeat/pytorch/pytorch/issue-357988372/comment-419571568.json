{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419571568", "html_url": "https://github.com/pytorch/pytorch/pull/11373#issuecomment-419571568", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11373", "id": 419571568, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTU3MTU2OA==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-07T21:32:55Z", "updated_at": "2018-09-07T21:32:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> Makes sense. So there is one thing that doesn't work: device code doesn't have IntList functions/operators, so I would need to make size(i) and stride(i) in TensorAccessor bypass IntList's indexing operator. I'm not sure what the reason is to wrap those into IntList rather than accessing <code>strides_[i]</code> directly<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/576807ce1a2291895cd1301ec6a982df21beb77f/aten/src/ATen/core/TensorAccessor.h#L20-L21\">pytorch/aten/src/ATen/core/TensorAccessor.h</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 20 to 21\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/576807ce1a2291895cd1301ec6a982df21beb77f\">576807c</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L20\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"20\"></td>\n          <td id=\"LC20\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">int64_t</span> <span class=\"pl-en\">stride</span>(<span class=\"pl-c1\">int64_t</span> i) { <span class=\"pl-k\">return</span> <span class=\"pl-c1\">strides</span>()[i]; } </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L21\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"21\"></td>\n          <td id=\"LC21\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">int64_t</span> <span class=\"pl-en\">size</span>(<span class=\"pl-c1\">int64_t</span> i) { <span class=\"pl-k\">return</span> <span class=\"pl-c1\">sizes</span>()[i]; } </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n<br>\nAfter making that change, it superficially (i.e. for my immediate use case) seems to work.</p>", "body_text": "@ezyang Makes sense. So there is one thing that doesn't work: device code doesn't have IntList functions/operators, so I would need to make size(i) and stride(i) in TensorAccessor bypass IntList's indexing operator. I'm not sure what the reason is to wrap those into IntList rather than accessing strides_[i] directly\n\n  \n    \n      pytorch/aten/src/ATen/core/TensorAccessor.h\n    \n    \n        Lines 20 to 21\n      in\n      576807c\n    \n    \n    \n    \n\n        \n          \n           int64_t stride(int64_t i) { return strides()[i]; } \n        \n\n        \n          \n           int64_t size(int64_t i) { return sizes()[i]; } \n        \n    \n  \n\n\nAfter making that change, it superficially (i.e. for my immediate use case) seems to work.", "body": "@ezyang Makes sense. So there is one thing that doesn't work: device code doesn't have IntList functions/operators, so I would need to make size(i) and stride(i) in TensorAccessor bypass IntList's indexing operator. I'm not sure what the reason is to wrap those into IntList rather than accessing `strides_[i]` directly\r\nhttps://github.com/pytorch/pytorch/blob/576807ce1a2291895cd1301ec6a982df21beb77f/aten/src/ATen/core/TensorAccessor.h#L20-L21\r\nAfter making that change, it superficially (i.e. for my immediate use case) seems to work.\r\n"}