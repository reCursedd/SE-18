{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419390019", "html_url": "https://github.com/pytorch/pytorch/pull/11373#issuecomment-419390019", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11373", "id": 419390019, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTM5MDAxOQ==", "user": {"login": "ClementPinard", "id": 4380424, "node_id": "MDQ6VXNlcjQzODA0MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4380424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ClementPinard", "html_url": "https://github.com/ClementPinard", "followers_url": "https://api.github.com/users/ClementPinard/followers", "following_url": "https://api.github.com/users/ClementPinard/following{/other_user}", "gists_url": "https://api.github.com/users/ClementPinard/gists{/gist_id}", "starred_url": "https://api.github.com/users/ClementPinard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ClementPinard/subscriptions", "organizations_url": "https://api.github.com/users/ClementPinard/orgs", "repos_url": "https://api.github.com/users/ClementPinard/repos", "events_url": "https://api.github.com/users/ClementPinard/events{/privacy}", "received_events_url": "https://api.github.com/users/ClementPinard/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-07T10:02:43Z", "updated_at": "2018-09-07T10:02:43Z", "author_association": "NONE", "body_html": "<p>Thanks for the PR !</p>\n<ul>\n<li>\n<p>The <code>cuda::CUDATensorAccessor&lt;scalar_t, 4&gt; image_a(image.accessor&lt;scalar_t, 4&gt;())</code><br>\nseems very tedious and error prone due to having to specify twice the <code>scalar_t</code> and <code>N</code> parameters. Couldn't we make a constructor directly from a ATen tensor ?</p>\n</li>\n<li>\n<p>Ideally I guess that, as you suggested, a <code>.cuda_accessor&lt;&gt;()</code> would be very nice, but maybe it would mean messing up the tensor class with cuda code, which is not necessarily what we want.</p>\n</li>\n<li>\n<p>And last, I'd say that knowing how to use the <code>RestrictPtrTraits</code> is not very easy. Maybe have two different accessors ? something like <code>cuda::CUDATensorRestrictedAccessor&lt;scalar_t, 4&gt;</code> along with the default one.</p>\n</li>\n</ul>\n<p>Note : these are comment from an end-user point of view (That writes ATen torch Extensions), so I focus on the interface more than the code cleanliness of ease of implementation, so take it for what it's worth, I might be actually clueless about what CUDA practice should be enforced or not <g-emoji class=\"g-emoji\" alias=\"monkey\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f412.png\">\ud83d\udc12</g-emoji></p>", "body_text": "Thanks for the PR !\n\n\nThe cuda::CUDATensorAccessor<scalar_t, 4> image_a(image.accessor<scalar_t, 4>())\nseems very tedious and error prone due to having to specify twice the scalar_t and N parameters. Couldn't we make a constructor directly from a ATen tensor ?\n\n\nIdeally I guess that, as you suggested, a .cuda_accessor<>() would be very nice, but maybe it would mean messing up the tensor class with cuda code, which is not necessarily what we want.\n\n\nAnd last, I'd say that knowing how to use the RestrictPtrTraits is not very easy. Maybe have two different accessors ? something like cuda::CUDATensorRestrictedAccessor<scalar_t, 4> along with the default one.\n\n\nNote : these are comment from an end-user point of view (That writes ATen torch Extensions), so I focus on the interface more than the code cleanliness of ease of implementation, so take it for what it's worth, I might be actually clueless about what CUDA practice should be enforced or not \ud83d\udc12", "body": "Thanks for the PR !\r\n\r\n * The `cuda::CUDATensorAccessor<scalar_t, 4> image_a(image.accessor<scalar_t, 4>())`\r\nseems very tedious and error prone due to having to specify twice the `scalar_t` and `N` parameters. Couldn't we make a constructor directly from a ATen tensor ?\r\n\r\n * Ideally I guess that, as you suggested, a `.cuda_accessor<>()` would be very nice, but maybe it would mean messing up the tensor class with cuda code, which is not necessarily what we want.\r\n\r\n * And last, I'd say that knowing how to use the `RestrictPtrTraits` is not very easy. Maybe have two different accessors ? something like `cuda::CUDATensorRestrictedAccessor<scalar_t, 4>` along with the default one.\r\n\r\nNote : these are comment from an end-user point of view (That writes ATen torch Extensions), so I focus on the interface more than the code cleanliness of ease of implementation, so take it for what it's worth, I might be actually clueless about what CUDA practice should be enforced or not :monkey:"}