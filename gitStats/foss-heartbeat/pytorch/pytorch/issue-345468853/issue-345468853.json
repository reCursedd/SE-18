{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9977", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9977/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9977/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9977/events", "html_url": "https://github.com/pytorch/pytorch/issues/9977", "id": 345468853, "node_id": "MDU6SXNzdWUzNDU0Njg4NTM=", "number": 9977, "title": "torch.stack() gradient errors in 0.4.1", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 696160151, "node_id": "MDU6TGFiZWw2OTYxNjAxNTE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/nightly-announce", "name": "nightly-announce", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-07-28T18:22:42Z", "updated_at": "2018-08-14T04:53:58Z", "closed_at": "2018-08-09T15:55:44Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>We're seeing Pyro code breakage due to interaction of <code>torch.stack</code> and <code>binary_cross_entropy_with_logits</code> as used inside <code>torch.Bernoulli.log_prob()</code>.</p>\n<p>Possibly related: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"283600787\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4274\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4274/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4274\">#4274</a></p>\n<h2>Code examples</h2>\n<p>The following examples were distilled from Pyro's <a href=\"https://github.com/uber/pyro/blob/pytorch-0.4.1-failing-tests/tests/contrib/tracking/test_em.py\">tests/contrib/tracking/test_em.py</a>:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>)\nstacked <span class=\"pl-k\">=</span> torch.stack([y, x], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\nloss <span class=\"pl-k\">=</span> binary_cross_entropy_with_logits(stacked[:, <span class=\"pl-c1\">1</span>], y, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>none<span class=\"pl-pds\">'</span></span>)\ng <span class=\"pl-k\">=</span> torch.autograd.grad(loss, [x], <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]\nH <span class=\"pl-k\">=</span> torch.autograd.grad(g.sum(), [x], <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]</pre></div>\n<pre><code>RuntimeError: dim() called on undefined Tensor\n</code></pre>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>)\nstacked <span class=\"pl-k\">=</span> torch.stack([x, y], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\nloss <span class=\"pl-k\">=</span> binary_cross_entropy_with_logits(stacked[:, <span class=\"pl-c1\">0</span>], y, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>none<span class=\"pl-pds\">'</span></span>)\ng <span class=\"pl-k\">=</span> torch.autograd.grad(loss, [x], <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]\nH <span class=\"pl-k\">=</span> torch.autograd.grad(g.sum(), [x], <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]</pre></div>\n<pre><code>RuntimeError: Expected a Tensor of type Variable but found an undefined Tensor at position #1 for iterable argument #0 'tensors'\n</code></pre>\n<p>These are both equivalent to the following unstacked version which works fine:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>)\nloss <span class=\"pl-k\">=</span> binary_cross_entropy_with_logits(x, y, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>none<span class=\"pl-pds\">'</span></span>)\ng <span class=\"pl-k\">=</span> torch.autograd.grad(loss, [x], <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]\nH <span class=\"pl-k\">=</span> torch.autograd.grad(g.sum(), [x], <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]</pre></div>\n<h2>System info</h2>\n<pre><code>PyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.13.3\nGCC version: Could not collect\nCMake version: version 3.9.6\n\nPython version: 2.7\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] torch                     0.4.1                     &lt;pip&gt;\n[conda] torchfile                 0.1.0                     &lt;pip&gt;\n[conda] torchvision               0.2.1                     &lt;pip&gt;\n</code></pre>", "body_text": "Issue description\nWe're seeing Pyro code breakage due to interaction of torch.stack and binary_cross_entropy_with_logits as used inside torch.Bernoulli.log_prob().\nPossibly related: #4274\nCode examples\nThe following examples were distilled from Pyro's tests/contrib/tracking/test_em.py:\nx = torch.zeros(1, requires_grad=True)\ny = torch.zeros(1)\nstacked = torch.stack([y, x], -1)\nloss = binary_cross_entropy_with_logits(stacked[:, 1], y, reduction='none')\ng = torch.autograd.grad(loss, [x], create_graph=True)[0]\nH = torch.autograd.grad(g.sum(), [x], create_graph=True)[0]\nRuntimeError: dim() called on undefined Tensor\n\nx = torch.zeros(1, requires_grad=True)\ny = torch.zeros(1)\nstacked = torch.stack([x, y], -1)\nloss = binary_cross_entropy_with_logits(stacked[:, 0], y, reduction='none')\ng = torch.autograd.grad(loss, [x], create_graph=True)[0]\nH = torch.autograd.grad(g.sum(), [x], create_graph=True)[0]\nRuntimeError: Expected a Tensor of type Variable but found an undefined Tensor at position #1 for iterable argument #0 'tensors'\n\nThese are both equivalent to the following unstacked version which works fine:\nx = torch.zeros(1, requires_grad=True)\ny = torch.zeros(1)\nloss = binary_cross_entropy_with_logits(x, y, reduction='none')\ng = torch.autograd.grad(loss, [x], create_graph=True)[0]\nH = torch.autograd.grad(g.sum(), [x], create_graph=True)[0]\nSystem info\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.13.3\nGCC version: Could not collect\nCMake version: version 3.9.6\n\nPython version: 2.7\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] torch                     0.4.1                     <pip>\n[conda] torchfile                 0.1.0                     <pip>\n[conda] torchvision               0.2.1                     <pip>", "body": "## Issue description\r\n\r\nWe're seeing Pyro code breakage due to interaction of `torch.stack` and `binary_cross_entropy_with_logits` as used inside `torch.Bernoulli.log_prob()`.\r\n\r\nPossibly related: https://github.com/pytorch/pytorch/issues/4274\r\n\r\n## Code examples\r\nThe following examples were distilled from Pyro's [tests/contrib/tracking/test_em.py](https://github.com/uber/pyro/blob/pytorch-0.4.1-failing-tests/tests/contrib/tracking/test_em.py):\r\n```py\r\nx = torch.zeros(1, requires_grad=True)\r\ny = torch.zeros(1)\r\nstacked = torch.stack([y, x], -1)\r\nloss = binary_cross_entropy_with_logits(stacked[:, 1], y, reduction='none')\r\ng = torch.autograd.grad(loss, [x], create_graph=True)[0]\r\nH = torch.autograd.grad(g.sum(), [x], create_graph=True)[0]\r\n```\r\n```\r\nRuntimeError: dim() called on undefined Tensor\r\n```\r\n```py\r\nx = torch.zeros(1, requires_grad=True)\r\ny = torch.zeros(1)\r\nstacked = torch.stack([x, y], -1)\r\nloss = binary_cross_entropy_with_logits(stacked[:, 0], y, reduction='none')\r\ng = torch.autograd.grad(loss, [x], create_graph=True)[0]\r\nH = torch.autograd.grad(g.sum(), [x], create_graph=True)[0]\r\n```\r\n```\r\nRuntimeError: Expected a Tensor of type Variable but found an undefined Tensor at position #1 for iterable argument #0 'tensors'\r\n```\r\nThese are both equivalent to the following unstacked version which works fine:\r\n```py\r\nx = torch.zeros(1, requires_grad=True)\r\ny = torch.zeros(1)\r\nloss = binary_cross_entropy_with_logits(x, y, reduction='none')\r\ng = torch.autograd.grad(loss, [x], create_graph=True)[0]\r\nH = torch.autograd.grad(g.sum(), [x], create_graph=True)[0]\r\n```\r\n\r\n## System info\r\n```\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.9.6\r\n\r\nPython version: 2.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] torch                     0.4.1                     <pip>\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>\r\n```"}