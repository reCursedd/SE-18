{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/408695659", "html_url": "https://github.com/pytorch/pytorch/issues/9977#issuecomment-408695659", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9977", "id": 408695659, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODY5NTY1OQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-29T18:18:33Z", "updated_at": "2018-07-29T18:27:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As a data point: As far as I can see, this isn't about <code>binary_cross_entropy_with_logits</code>, or stack, but about unbind (stack's backward). More minimal repro:</p>\n<pre><code>import torch\ns = torch.zeros(2, requires_grad=True)\nv,w = torch.unbind(s, dim=0)\nv.backward() # or w.backward() for the other error\n</code></pre>\n<p>So I think the problem is that not all grads are there in to_tensor_list. I'll take a stab at defining a backward for unbind based on the saving the shapes in the forward and putting 0s for the missing pieces.</p>", "body_text": "As a data point: As far as I can see, this isn't about binary_cross_entropy_with_logits, or stack, but about unbind (stack's backward). More minimal repro:\nimport torch\ns = torch.zeros(2, requires_grad=True)\nv,w = torch.unbind(s, dim=0)\nv.backward() # or w.backward() for the other error\n\nSo I think the problem is that not all grads are there in to_tensor_list. I'll take a stab at defining a backward for unbind based on the saving the shapes in the forward and putting 0s for the missing pieces.", "body": "As a data point: As far as I can see, this isn't about `binary_cross_entropy_with_logits`, or stack, but about unbind (stack's backward). More minimal repro:\r\n```\r\nimport torch\r\ns = torch.zeros(2, requires_grad=True)\r\nv,w = torch.unbind(s, dim=0)\r\nv.backward() # or w.backward() for the other error\r\n```\r\nSo I think the problem is that not all grads are there in to_tensor_list. I'll take a stab at defining a backward for unbind based on the saving the shapes in the forward and putting 0s for the missing pieces."}