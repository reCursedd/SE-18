{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153594102", "pull_request_review_id": 79622750, "id": 153594102, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzU5NDEwMg==", "diff_hunk": "@@ -390,9 +390,6 @@ int THPVariable_set_requires_grad(THPVariable *self, PyObject *obj)\n     return -1;\n   }\n   var.requires_grad() = (obj == Py_True);\n-  if (auto grad_accumulator = var.get()->grad_accumulator.lock()) {\n-    grad_accumulator->is_executable = var.requires_grad();\n-  }", "path": "torch/csrc/autograd/python_variable.cpp", "position": 6, "original_position": 6, "commit_id": "0d421360aa603343ee371f5a8213983398ac1a56", "original_commit_id": "a96c4aa1e39eaa5e8c5f753a75613191555ed025", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "`AccumulateGrad` is a no-op if the target Variable doesn't require grad. I think that should be sufficient:\r\n\r\nhttps://github.com/pytorch/pytorch/blob/157f949cefb9b15d01160d2becb16c6572023536/torch/csrc/autograd/functions/accumulate_grad.cpp#L30-L31", "created_at": "2017-11-28T19:14:46Z", "updated_at": "2018-11-23T15:36:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/3907#discussion_r153594102", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3907", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153594102"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3907#discussion_r153594102"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3907"}}, "body_html": "<p><code>AccumulateGrad</code> is a no-op if the target Variable doesn't require grad. I think that should be sufficient:</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/157f949cefb9b15d01160d2becb16c6572023536/torch/csrc/autograd/functions/accumulate_grad.cpp#L30-L31\">pytorch/torch/csrc/autograd/functions/accumulate_grad.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 30 to 31\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/157f949cefb9b15d01160d2becb16c6572023536\">157f949</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L30\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"30\"></td>\n          <td id=\"LC30\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> (!variable.<span class=\"pl-c1\">requires_grad</span>()) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L31\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"31\"></td>\n          <td id=\"LC31\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">return</span> {}; </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>", "body_text": "AccumulateGrad is a no-op if the target Variable doesn't require grad. I think that should be sufficient:\n\n  \n    \n      pytorch/torch/csrc/autograd/functions/accumulate_grad.cpp\n    \n    \n        Lines 30 to 31\n      in\n      157f949\n    \n    \n    \n    \n\n        \n          \n           if (!variable.requires_grad()) \n        \n\n        \n          \n             return {};", "in_reply_to_id": 153423820}