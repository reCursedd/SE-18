{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11133", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11133/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11133/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11133/events", "html_url": "https://github.com/pytorch/pytorch/issues/11133", "id": 355857967, "node_id": "MDU6SXNzdWUzNTU4NTc5Njc=", "number": 11133, "title": "pytorch-0.4.1 build fails on autograd test", "user": {"login": "akamaus", "id": 58955, "node_id": "MDQ6VXNlcjU4OTU1", "avatar_url": "https://avatars0.githubusercontent.com/u/58955?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akamaus", "html_url": "https://github.com/akamaus", "followers_url": "https://api.github.com/users/akamaus/followers", "following_url": "https://api.github.com/users/akamaus/following{/other_user}", "gists_url": "https://api.github.com/users/akamaus/gists{/gist_id}", "starred_url": "https://api.github.com/users/akamaus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akamaus/subscriptions", "organizations_url": "https://api.github.com/users/akamaus/orgs", "repos_url": "https://api.github.com/users/akamaus/repos", "events_url": "https://api.github.com/users/akamaus/events{/privacy}", "received_events_url": "https://api.github.com/users/akamaus/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-08-31T06:40:52Z", "updated_at": "2018-09-07T02:47:20Z", "closed_at": "2018-09-07T02:47:20Z", "author_association": "NONE", "body_html": "<p>I'm working on packaging pytorch-0.4.1 for NixOS and I stumbled upon the following test error.</p>\n<pre><code>Running test_autograd ...\n............................................................................................s.........s............................................................................................................................s............................................EEEEF.s........................................................s................................................................................................................................................................ss..................................................s.E........................................................................................s...........................s......................................................................................................................................s......................s.....\n======================================================================\nERROR: test_gesv (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3409, in check\n    output_variable, (self_variable,) + args_variable)\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 1,\nnumerical:tensor([[-0.3519,  0.7853,  0.1334, -0.8869, -0.4107, -0.1127,  0.2516,  0.0427,\n         -0.2841, -0.1315,  0.0555, -0.1238, -0.0210,  0.1398,  0.0648, -0.1272,\n          0.2838,  0.0482, -0.3205, -0.1484,  0.1304, -0.2910, -0.0494,  0.3286,\n          0.1522],\n        [-0.1972,  0.0193,  0.3803,  0.0496, -0.6060, -0.0632,  0.0062,  0.1218,\n          0.0159, -0.1941,  0.0311, -0.0030, -0.0600, -0.0078,  0.0956, -0.0713,\n          0.0070,  0.1374,  0.0179, -0.2190,  0.0731, -0.0072, -0.1409, -0.0184,\n          0.2245],\n        [-0.1190,  0.8624,  0.3051, -0.2650, -0.4737, -0.0381,  0.2763,  0.0977,\n         -0.0849, -0.1517,  0.0188, -0.1360, -0.0481,  0.0418,  0.0747, -0.0430,\n          0.3117,  0.1102, -0.0958, -0.1712,  0.0441, -0.3195, -0.1130,  0.0982,\n          0.1755],\n        [ 0.3640, -0.1793, -0.4726, -0.1183,  0.7092,  0.1166, -0.0574, -0.1514,\n         -0.0379,  0.2272, -0.0574,  0.0283,  0.0745,  0.0186, -0.1118,  0.1315,\n         -0.0648, -0.1708, -0.0427,  0.2563, -0.1349,  0.0664,  0.1751,  0.0438,\n         -0.2628],\n        [ 0.0017, -0.3339,  0.0982,  0.6557, -0.1890,  0.0005, -0.1070,  0.0314,\n          0.2100, -0.0605, -0.0003,  0.0526, -0.0155, -0.1034,  0.0298,  0.0006,\n         -0.1207,  0.0355,  0.2370, -0.0683, -0.0006,  0.1237, -0.0364, -0.2429,\n          0.0700],\n        [ 0.0955, -0.2132, -0.0362,  0.2407,  0.1115, -0.0274,  0.0612,  0.0104,\n         -0.0691, -0.0320,  0.0300, -0.0670, -0.0114,  0.0757,  0.0350, -0.0648,\n          0.1446,  0.0246, -0.1633, -0.0756,  0.1864, -0.4161, -0.0707,  0.4698,\n          0.2176],\n        [ 0.0535, -0.0053, -0.1032, -0.0135,  0.1645, -0.0154,  0.0015,  0.0296,\n          0.0039, -0.0472,  0.0168, -0.0017, -0.0324, -0.0042,  0.0517, -0.0363,\n          0.0036,  0.0700,  0.0091, -0.1116,  0.1045, -0.0102, -0.2015, -0.0263,\n          0.3211],\n        [ 0.0323, -0.2341, -0.0828,  0.0719,  0.1286, -0.0093,  0.0672,  0.0238,\n         -0.0206, -0.0369,  0.0101, -0.0736, -0.0260,  0.0226,  0.0404, -0.0219,\n          0.1588,  0.0562, -0.0488, -0.0872,  0.0630, -0.4569, -0.1616,  0.1404,\n          0.2509],\n        [-0.0988,  0.0487,  0.1283,  0.0321, -0.1925,  0.0284, -0.0140, -0.0368,\n         -0.0092,  0.0552, -0.0311,  0.0153,  0.0403,  0.0101, -0.0605,  0.0670,\n         -0.0330, -0.0870, -0.0218,  0.1306, -0.1928,  0.0950,  0.2504,  0.0627,\n         -0.3757],\n        [-0.0005,  0.0906, -0.0266, -0.1780,  0.0513,  0.0001, -0.0260,  0.0076,\n          0.0511, -0.0147, -0.0001,  0.0285, -0.0084, -0.0559,  0.0161,  0.0003,\n         -0.0615,  0.0181,  0.1207, -0.0348, -0.0009,  0.1769, -0.0520, -0.3474,\n          0.1001],\n        [ 0.2218, -0.4949, -0.0841,  0.5589,  0.2588,  0.3033, -0.6769, -0.1150,\n          0.7644,  0.3540,  0.2494, -0.5565, -0.0945,  0.6284,  0.2910, -0.3366,\n          0.7513,  0.1276, -0.8484, -0.3928,  0.0710, -0.1584, -0.0269,  0.1789,\n          0.0829],\n        [ 0.1243, -0.0122, -0.2397, -0.0313,  0.3819,  0.1700, -0.0167, -0.3278,\n         -0.0428,  0.5223,  0.1397, -0.0137, -0.2695, -0.0352,  0.4294, -0.1887,\n          0.0185,  0.3638,  0.0475, -0.5797,  0.0398, -0.0039, -0.0767, -0.0100,\n          0.1223],\n        [ 0.0750, -0.5435, -0.1923,  0.1670,  0.2985,  0.1025, -0.7433, -0.2630,\n          0.2284,  0.4083,  0.0843, -0.6111, -0.2162,  0.1877,  0.3356, -0.1138,\n          0.8250,  0.2918, -0.2535, -0.4531,  0.0240, -0.1740, -0.0615,  0.0535,\n          0.0956],\n        [-0.2294,  0.1130,  0.2979,  0.0745, -0.4469, -0.3137,  0.1546,  0.4074,\n          0.1020, -0.6113, -0.2579,  0.1271,  0.3349,  0.0838, -0.5025,  0.3482,\n         -0.1716, -0.4521, -0.1131,  0.6784, -0.0734,  0.0362,  0.0954,  0.0239,\n         -0.1431],\n        [-0.0011,  0.2104, -0.0619, -0.4132,  0.1191, -0.0015,  0.2878, -0.0846,\n         -0.5652,  0.1629, -0.0012,  0.2366, -0.0696, -0.4646,  0.1339,  0.0016,\n         -0.3194,  0.0939,  0.6272, -0.1808, -0.0003,  0.0674, -0.0198, -0.1323,\n          0.0381],\n        [-0.7464,  1.6657,  0.2829, -1.8810, -0.8710,  0.0600, -0.1339, -0.0227,\n          0.1512,  0.0700, -0.5586,  1.2466,  0.2117, -1.4077, -0.6518,  0.0677,\n         -0.1510, -0.0256,  0.1705,  0.0790,  0.3959, -0.8835, -0.1501,  0.9977,\n          0.4620],\n        [-0.4183,  0.0410,  0.8067,  0.1053, -1.2853,  0.0336, -0.0033, -0.0648,\n         -0.0085,  0.1033, -0.3130,  0.0307,  0.6037,  0.0788, -0.9619,  0.0379,\n         -0.0037, -0.0731, -0.0095,  0.1165,  0.2219, -0.0218, -0.4279, -0.0558,\n          0.6818],\n        [-0.2523,  1.8291,  0.6471, -0.5620, -1.0046,  0.0203, -0.1470, -0.0520,\n          0.0452,  0.0807, -0.1888,  1.3689,  0.4842, -0.4206, -0.7518,  0.0229,\n         -0.1658, -0.0587,  0.0509,  0.0911,  0.1338, -0.9702, -0.3432,  0.2981,\n          0.5329],\n        [ 0.7720, -0.3804, -1.0024, -0.2509,  1.5041, -0.0620,  0.0306,  0.0806,\n          0.0202, -0.1209,  0.5778, -0.2847, -0.7502, -0.1877,  1.1256, -0.0700,\n          0.0345,  0.0909,  0.0227, -0.1363, -0.4095,  0.2018,  0.5317,  0.1331,\n         -0.7978],\n        [ 0.0036, -0.7082,  0.2082,  1.3907, -0.4008, -0.0003,  0.0569, -0.0167,\n         -0.1118,  0.0322,  0.0027, -0.5300,  0.1558,  1.0408, -0.3000, -0.0003,\n          0.0642, -0.0189, -0.1261,  0.0363, -0.0019,  0.3756, -0.1104, -0.7377,\n          0.2126],\n        [-0.4169,  0.9304,  0.1580, -1.0507, -0.4865, -0.1176,  0.2625,  0.0446,\n         -0.2964, -0.1373, -0.0849,  0.1895,  0.0322, -0.2140, -0.0991,  0.2586,\n         -0.5772, -0.0980,  0.6518,  0.3018,  0.2081, -0.4645, -0.0789,  0.5245,\n          0.2429],\n        [-0.2336,  0.0229,  0.4506,  0.0588, -0.7180, -0.0659,  0.0065,  0.1271,\n          0.0166, -0.2026, -0.0476,  0.0047,  0.0918,  0.0120, -0.1462,  0.1449,\n         -0.0142, -0.2795, -0.0365,  0.4454,  0.1166, -0.0114, -0.2249, -0.0294,\n          0.3584],\n        [-0.1409,  1.0217,  0.3614, -0.3139, -0.5611, -0.0398,  0.2883,  0.1020,\n         -0.0886, -0.1583, -0.0287,  0.2081,  0.0736, -0.0639, -0.1143,  0.0874,\n         -0.6338, -0.2242,  0.1947,  0.3481,  0.0704, -0.5101, -0.1804,  0.1567,\n          0.2801],\n        [ 0.4312, -0.2125, -0.5599, -0.1401,  0.8402,  0.1217, -0.0599, -0.1580,\n         -0.0395,  0.2371,  0.0878, -0.0433, -0.1140, -0.0285,  0.1711, -0.2675,\n          0.1318,  0.3474,  0.0869, -0.5212, -0.2153,  0.1061,  0.2795,  0.0700,\n         -0.4194],\n        [ 0.0020, -0.3956,  0.1163,  0.7768, -0.2239,  0.0006, -0.1116,  0.0328,\n          0.2192, -0.0632,  0.0004, -0.0806,  0.0237,  0.1582, -0.0456, -0.0013,\n          0.2454, -0.0721, -0.4819,  0.1389, -0.0010,  0.1975, -0.0581, -0.3878,\n          0.1118]])\nanalytical:tensor([[-0.3519,  0.7853,  0.1334, -0.8869, -0.4107, -0.1127,  0.2516,  0.0427,\n         -0.2841, -0.1315,  0.0555, -0.1238, -0.0210,  0.1398,  0.0648, -0.1272,\n          0.2838,  0.0482, -0.3205, -0.1484,  0.1304, -0.2910, -0.0494,  0.3286,\n         -0.1436],\n        [-0.1972,  0.0193,  0.3803,  0.0496, -0.6060, -0.0632,  0.0062,  0.1218,\n          0.0159, -0.1941,  0.0311, -0.0030, -0.0600, -0.0078,  0.0956, -0.0713,\n          0.0070,  0.1374,  0.0179, -0.2190,  0.0731, -0.0072, -0.1409, -0.0184,\n         -0.3407],\n        [-0.1190,  0.8624,  0.3051, -0.2650, -0.4737, -0.0381,  0.2763,  0.0977,\n         -0.0849, -0.1517,  0.0188, -0.1360, -0.0481,  0.0418,  0.0747, -0.0430,\n          0.3117,  0.1102, -0.0958, -0.1712,  0.0441, -0.3195, -0.1130,  0.0982,\n         -0.2718],\n        [ 0.3640, -0.1793, -0.4726, -0.1183,  0.7092,  0.1166, -0.0574, -0.1514,\n         -0.0379,  0.2272, -0.0574,  0.0283,  0.0745,  0.0186, -0.1118,  0.1315,\n         -0.0648, -0.1708, -0.0427,  0.2563, -0.1349,  0.0664,  0.1751,  0.0438,\n          0.4181],\n        [ 0.0017, -0.3339,  0.0982,  0.6557, -0.1890,  0.0005, -0.1070,  0.0314,\n          0.2100, -0.0605, -0.0003,  0.0526, -0.0155, -0.1034,  0.0298,  0.0006,\n         -0.1207,  0.0355,  0.2370, -0.0683, -0.0006,  0.1237, -0.0364, -0.2429,\n         -0.0919],\n        [ 0.0955, -0.2132, -0.0362,  0.2407,  0.1115, -0.0274,  0.0612,  0.0104,\n         -0.0691, -0.0320,  0.0300, -0.0670, -0.0114,  0.0757,  0.0350, -0.0648,\n          0.1446,  0.0246, -0.1633, -0.0756,  0.1864, -0.4161, -0.0707,  0.4698,\n          0.2978],\n        [ 0.0535, -0.0053, -0.1032, -0.0135,  0.1645, -0.0154,  0.0015,  0.0296,\n          0.0039, -0.0472,  0.0168, -0.0017, -0.0324, -0.0042,  0.0517, -0.0363,\n          0.0036,  0.0700,  0.0091, -0.1116,  0.1045, -0.0102, -0.2015, -0.0263,\n          0.4745],\n        [ 0.0323, -0.2341, -0.0828,  0.0719,  0.1286, -0.0093,  0.0672,  0.0238,\n         -0.0206, -0.0369,  0.0101, -0.0736, -0.0260,  0.0226,  0.0404, -0.0219,\n          0.1588,  0.0562, -0.0488, -0.0872,  0.0630, -0.4569, -0.1616,  0.1404,\n          0.3724],\n        [-0.0988,  0.0487,  0.1283,  0.0321, -0.1925,  0.0284, -0.0140, -0.0368,\n         -0.0092,  0.0552, -0.0311,  0.0153,  0.0403,  0.0101, -0.0605,  0.0670,\n         -0.0330, -0.0870, -0.0218,  0.1306, -0.1928,  0.0950,  0.2504,  0.0627,\n         -0.5605],\n        [-0.0005,  0.0906, -0.0266, -0.1780,  0.0513,  0.0001, -0.0260,  0.0076,\n          0.0511, -0.0147, -0.0001,  0.0285, -0.0084, -0.0559,  0.0161,  0.0003,\n         -0.0615,  0.0181,  0.1207, -0.0348, -0.0009,  0.1769, -0.0520, -0.3474,\n          0.1441],\n        [ 0.2218, -0.4949, -0.0841,  0.5589,  0.2588,  0.3033, -0.6769, -0.1150,\n          0.7644,  0.3540,  0.2494, -0.5565, -0.0945,  0.6284,  0.2910, -0.3366,\n          0.7513,  0.1276, -0.8484, -0.3928,  0.0710, -0.1584, -0.0269,  0.1789,\n          0.2692],\n        [ 0.1243, -0.0122, -0.2397, -0.0313,  0.3819,  0.1700, -0.0167, -0.3278,\n         -0.0428,  0.5223,  0.1397, -0.0137, -0.2695, -0.0352,  0.4294, -0.1887,\n          0.0185,  0.3638,  0.0475, -0.5797,  0.0398, -0.0039, -0.0767, -0.0100,\n          0.4785],\n        [ 0.0750, -0.5435, -0.1923,  0.1670,  0.2985,  0.1025, -0.7433, -0.2630,\n          0.2284,  0.4083,  0.0843, -0.6111, -0.2162,  0.1877,  0.3356, -0.1138,\n          0.8250,  0.2918, -0.2535, -0.4531,  0.0240, -0.1740, -0.0615,  0.0535,\n          0.3774],\n        [-0.2294,  0.1130,  0.2979,  0.0745, -0.4469, -0.3137,  0.1546,  0.4074,\n          0.1020, -0.6113, -0.2579,  0.1271,  0.3349,  0.0838, -0.5025,  0.3482,\n         -0.1716, -0.4521, -0.1131,  0.6784, -0.0734,  0.0362,  0.0954,  0.0239,\n         -0.5721],\n        [-0.0011,  0.2104, -0.0619, -0.4132,  0.1191, -0.0015,  0.2878, -0.0846,\n         -0.5652,  0.1629, -0.0012,  0.2366, -0.0696, -0.4646,  0.1339,  0.0016,\n         -0.3194,  0.0939,  0.6272, -0.1808, -0.0003,  0.0674, -0.0198, -0.1323,\n          0.1402],\n        [-0.7464,  1.6657,  0.2829, -1.8810, -0.8710,  0.0600, -0.1339, -0.0227,\n          0.1512,  0.0700, -0.5586,  1.2466,  0.2117, -1.4077, -0.6518,  0.0677,\n         -0.1510, -0.0256,  0.1705,  0.0790,  0.3959, -0.8835, -0.1501,  0.9977,\n         -0.1653],\n        [-0.4183,  0.0410,  0.8067,  0.1053, -1.2853,  0.0336, -0.0033, -0.0648,\n         -0.0085,  0.1033, -0.3130,  0.0307,  0.6037,  0.0788, -0.9619,  0.0379,\n         -0.0037, -0.0731, -0.0095,  0.1165,  0.2219, -0.0218, -0.4279, -0.0558,\n         -0.5171],\n        [-0.2523,  1.8291,  0.6471, -0.5620, -1.0046,  0.0203, -0.1470, -0.0520,\n          0.0452,  0.0807, -0.1888,  1.3689,  0.4842, -0.4206, -0.7518,  0.0229,\n         -0.1658, -0.0587,  0.0509,  0.0911,  0.1338, -0.9702, -0.3432,  0.2981,\n         -0.4158],\n        [ 0.7720, -0.3804, -1.0024, -0.2509,  1.5041, -0.0620,  0.0306,  0.0806,\n          0.0202, -0.1209,  0.5778, -0.2847, -0.7502, -0.1877,  1.1256, -0.0700,\n          0.0345,  0.0909,  0.0227, -0.1363, -0.4095,  0.2018,  0.5317,  0.1331,\n          0.6462],\n        [ 0.0036, -0.7082,  0.2082,  1.3907, -0.4008, -0.0003,  0.0569, -0.0167,\n         -0.1118,  0.0322,  0.0027, -0.5300,  0.1558,  1.0408, -0.3000, -0.0003,\n          0.0642, -0.0189, -0.1261,  0.0363, -0.0019,  0.3756, -0.1104, -0.7377,\n         -0.1308],\n        [-0.4169,  0.9304,  0.1580, -1.0507, -0.4865, -0.1176,  0.2625,  0.0446,\n         -0.2964, -0.1373, -0.0849,  0.1895,  0.0322, -0.2140, -0.0991,  0.2586,\n         -0.5772, -0.0980,  0.6518,  0.3018,  0.2081, -0.4645, -0.0789,  0.5245,\n         -0.1075],\n        [-0.2336,  0.0229,  0.4506,  0.0588, -0.7180, -0.0659,  0.0065,  0.1271,\n          0.0166, -0.2026, -0.0476,  0.0047,  0.0918,  0.0120, -0.1462,  0.1449,\n         -0.0142, -0.2795, -0.0365,  0.4454,  0.1166, -0.0114, -0.2249, -0.0294,\n         -0.3113],\n        [-0.1409,  1.0217,  0.3614, -0.3139, -0.5611, -0.0398,  0.2883,  0.1020,\n         -0.0886, -0.1583, -0.0287,  0.2081,  0.0736, -0.0639, -0.1143,  0.0874,\n         -0.6338, -0.2242,  0.1947,  0.3481,  0.0704, -0.5101, -0.1804,  0.1567,\n         -0.2498],\n        [ 0.4312, -0.2125, -0.5599, -0.1401,  0.8402,  0.1217, -0.0599, -0.1580,\n         -0.0395,  0.2371,  0.0878, -0.0433, -0.1140, -0.0285,  0.1711, -0.2675,\n          0.1318,  0.3474,  0.0869, -0.5212, -0.2153,  0.1061,  0.2795,  0.0700,\n          0.3872],\n        [ 0.0020, -0.3956,  0.1163,  0.7768, -0.2239,  0.0006, -0.1116,  0.0328,\n          0.2192, -0.0632,  0.0004, -0.0806,  0.0237,  0.1582, -0.0456, -0.0013,\n          0.2454, -0.0721, -0.4819,  0.1389, -0.0010,  0.1975, -0.0581, -0.3878,\n         -0.0801]])\n\n\n======================================================================\nERROR: test_gesv_batched (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3409, in check\n    output_variable, (self_variable,) + args_variable)\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-5.2063e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00, -5.2063e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00, -5.2063e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        ...,\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0869e-01,\n          0.0000e+00,  4.1437e+05],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n         -3.0869e-01,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00, -3.0869e-01]])\nanalytical:tensor([[-5.2063,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -5.2063,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -5.2063,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [-0.0000, -0.0000, -0.0000,  ..., -0.3087, -0.0000, -0.0000],\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.3087, -0.0000],\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.3087]])\n\n\n======================================================================\nERROR: test_gesv_batched_broadcast_A (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3409, in check\n    output_variable, (self_variable,) + args_variable)\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-0.0583,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.0583,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.0583,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.3674,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.3674,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.3674]])\nanalytical:tensor([[-0.0583,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.0583,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.0583,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [-0.0000, -0.0000, -0.0000,  ...,  0.3674, -0.0000, -0.0000],\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.3674, -0.0000],\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.3674]])\n\n\n======================================================================\nERROR: test_gesv_batched_broadcast_b (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3409, in check\n    output_variable, (self_variable,) + args_variable)\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-0.2197,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.2197,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.2197,  ...,  0.4906,  0.0000,  0.0000],\n        ...,\n        [ 0.0000,  0.0000, -0.5873,  ..., -0.1934,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.1934,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1934]])\nanalytical:tensor([[-0.2197,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.2197,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.2197,  ...,  0.4906,  0.0000,  0.0000],\n        ...,\n        [ 0.0000,  0.0000, -0.5873,  ..., -0.1934,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.1934,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1934]])\n\n\n======================================================================\nERROR: test_potrf (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 1914, in test_potrf\n    run_test(upper=True)\n  File \"test_autograd.py\", line 1911, in run_test\n    gradcheck(func, [root])\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 217, in gradcheck\n    return fail_test('Backward is not reentrant, i.e., running backward with same '\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient\n\n======================================================================\nFAIL: test_gesv_batched_dims (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3400, in check\n    self.assertEqual(unpack_variables(output_variable), output_tensor)\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 352, in assertEqual\n    self.assertEqual(x_, y_, prec, message)\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 344, in assertEqual\n    assertTensorsEqual(x, y)\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 336, in assertTensorsEqual\n    self.assertLessEqual(max_err, prec, message)\nAssertionError: tensor(0.7946, grad_fn=&lt;MaxBackward1&gt;) not less than or equal to 1e-05 :\n\n----------------------------------------------------------------------\nRan 831 tests in 69.093s\n\nFAILED (failures=1, errors=5, skipped=12)\nTraceback (most recent call last):\n  File \"test/run_test.py\", line 345, in &lt;module&gt;\n    main()\n  File \"test/run_test.py\", line 337, in main\n    raise RuntimeError(message)\nRuntimeError: test_autograd failed!\n</code></pre>\n<ul>\n<li>OS: nixos-unstable</li>\n<li>PyTorch version: 0.4.1 (rev <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/a24163a95edb193ff7b06e98cd69bf7cfd4c0d2f/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/a24163a95edb193ff7b06e98cd69bf7cfd4c0d2f\"><tt>a24163a</tt></a>)</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: CPU-only</li>\n<li>GCC version (if compiling from source): 7.3.0</li>\n<li>CMake version: 3.11.2</li>\n</ul>\n<p>Any ideas what might went wrong?</p>", "body_text": "I'm working on packaging pytorch-0.4.1 for NixOS and I stumbled upon the following test error.\nRunning test_autograd ...\n............................................................................................s.........s............................................................................................................................s............................................EEEEF.s........................................................s................................................................................................................................................................ss..................................................s.E........................................................................................s...........................s......................................................................................................................................s......................s.....\n======================================================================\nERROR: test_gesv (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3409, in check\n    output_variable, (self_variable,) + args_variable)\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 1,\nnumerical:tensor([[-0.3519,  0.7853,  0.1334, -0.8869, -0.4107, -0.1127,  0.2516,  0.0427,\n         -0.2841, -0.1315,  0.0555, -0.1238, -0.0210,  0.1398,  0.0648, -0.1272,\n          0.2838,  0.0482, -0.3205, -0.1484,  0.1304, -0.2910, -0.0494,  0.3286,\n          0.1522],\n        [-0.1972,  0.0193,  0.3803,  0.0496, -0.6060, -0.0632,  0.0062,  0.1218,\n          0.0159, -0.1941,  0.0311, -0.0030, -0.0600, -0.0078,  0.0956, -0.0713,\n          0.0070,  0.1374,  0.0179, -0.2190,  0.0731, -0.0072, -0.1409, -0.0184,\n          0.2245],\n        [-0.1190,  0.8624,  0.3051, -0.2650, -0.4737, -0.0381,  0.2763,  0.0977,\n         -0.0849, -0.1517,  0.0188, -0.1360, -0.0481,  0.0418,  0.0747, -0.0430,\n          0.3117,  0.1102, -0.0958, -0.1712,  0.0441, -0.3195, -0.1130,  0.0982,\n          0.1755],\n        [ 0.3640, -0.1793, -0.4726, -0.1183,  0.7092,  0.1166, -0.0574, -0.1514,\n         -0.0379,  0.2272, -0.0574,  0.0283,  0.0745,  0.0186, -0.1118,  0.1315,\n         -0.0648, -0.1708, -0.0427,  0.2563, -0.1349,  0.0664,  0.1751,  0.0438,\n         -0.2628],\n        [ 0.0017, -0.3339,  0.0982,  0.6557, -0.1890,  0.0005, -0.1070,  0.0314,\n          0.2100, -0.0605, -0.0003,  0.0526, -0.0155, -0.1034,  0.0298,  0.0006,\n         -0.1207,  0.0355,  0.2370, -0.0683, -0.0006,  0.1237, -0.0364, -0.2429,\n          0.0700],\n        [ 0.0955, -0.2132, -0.0362,  0.2407,  0.1115, -0.0274,  0.0612,  0.0104,\n         -0.0691, -0.0320,  0.0300, -0.0670, -0.0114,  0.0757,  0.0350, -0.0648,\n          0.1446,  0.0246, -0.1633, -0.0756,  0.1864, -0.4161, -0.0707,  0.4698,\n          0.2176],\n        [ 0.0535, -0.0053, -0.1032, -0.0135,  0.1645, -0.0154,  0.0015,  0.0296,\n          0.0039, -0.0472,  0.0168, -0.0017, -0.0324, -0.0042,  0.0517, -0.0363,\n          0.0036,  0.0700,  0.0091, -0.1116,  0.1045, -0.0102, -0.2015, -0.0263,\n          0.3211],\n        [ 0.0323, -0.2341, -0.0828,  0.0719,  0.1286, -0.0093,  0.0672,  0.0238,\n         -0.0206, -0.0369,  0.0101, -0.0736, -0.0260,  0.0226,  0.0404, -0.0219,\n          0.1588,  0.0562, -0.0488, -0.0872,  0.0630, -0.4569, -0.1616,  0.1404,\n          0.2509],\n        [-0.0988,  0.0487,  0.1283,  0.0321, -0.1925,  0.0284, -0.0140, -0.0368,\n         -0.0092,  0.0552, -0.0311,  0.0153,  0.0403,  0.0101, -0.0605,  0.0670,\n         -0.0330, -0.0870, -0.0218,  0.1306, -0.1928,  0.0950,  0.2504,  0.0627,\n         -0.3757],\n        [-0.0005,  0.0906, -0.0266, -0.1780,  0.0513,  0.0001, -0.0260,  0.0076,\n          0.0511, -0.0147, -0.0001,  0.0285, -0.0084, -0.0559,  0.0161,  0.0003,\n         -0.0615,  0.0181,  0.1207, -0.0348, -0.0009,  0.1769, -0.0520, -0.3474,\n          0.1001],\n        [ 0.2218, -0.4949, -0.0841,  0.5589,  0.2588,  0.3033, -0.6769, -0.1150,\n          0.7644,  0.3540,  0.2494, -0.5565, -0.0945,  0.6284,  0.2910, -0.3366,\n          0.7513,  0.1276, -0.8484, -0.3928,  0.0710, -0.1584, -0.0269,  0.1789,\n          0.0829],\n        [ 0.1243, -0.0122, -0.2397, -0.0313,  0.3819,  0.1700, -0.0167, -0.3278,\n         -0.0428,  0.5223,  0.1397, -0.0137, -0.2695, -0.0352,  0.4294, -0.1887,\n          0.0185,  0.3638,  0.0475, -0.5797,  0.0398, -0.0039, -0.0767, -0.0100,\n          0.1223],\n        [ 0.0750, -0.5435, -0.1923,  0.1670,  0.2985,  0.1025, -0.7433, -0.2630,\n          0.2284,  0.4083,  0.0843, -0.6111, -0.2162,  0.1877,  0.3356, -0.1138,\n          0.8250,  0.2918, -0.2535, -0.4531,  0.0240, -0.1740, -0.0615,  0.0535,\n          0.0956],\n        [-0.2294,  0.1130,  0.2979,  0.0745, -0.4469, -0.3137,  0.1546,  0.4074,\n          0.1020, -0.6113, -0.2579,  0.1271,  0.3349,  0.0838, -0.5025,  0.3482,\n         -0.1716, -0.4521, -0.1131,  0.6784, -0.0734,  0.0362,  0.0954,  0.0239,\n         -0.1431],\n        [-0.0011,  0.2104, -0.0619, -0.4132,  0.1191, -0.0015,  0.2878, -0.0846,\n         -0.5652,  0.1629, -0.0012,  0.2366, -0.0696, -0.4646,  0.1339,  0.0016,\n         -0.3194,  0.0939,  0.6272, -0.1808, -0.0003,  0.0674, -0.0198, -0.1323,\n          0.0381],\n        [-0.7464,  1.6657,  0.2829, -1.8810, -0.8710,  0.0600, -0.1339, -0.0227,\n          0.1512,  0.0700, -0.5586,  1.2466,  0.2117, -1.4077, -0.6518,  0.0677,\n         -0.1510, -0.0256,  0.1705,  0.0790,  0.3959, -0.8835, -0.1501,  0.9977,\n          0.4620],\n        [-0.4183,  0.0410,  0.8067,  0.1053, -1.2853,  0.0336, -0.0033, -0.0648,\n         -0.0085,  0.1033, -0.3130,  0.0307,  0.6037,  0.0788, -0.9619,  0.0379,\n         -0.0037, -0.0731, -0.0095,  0.1165,  0.2219, -0.0218, -0.4279, -0.0558,\n          0.6818],\n        [-0.2523,  1.8291,  0.6471, -0.5620, -1.0046,  0.0203, -0.1470, -0.0520,\n          0.0452,  0.0807, -0.1888,  1.3689,  0.4842, -0.4206, -0.7518,  0.0229,\n         -0.1658, -0.0587,  0.0509,  0.0911,  0.1338, -0.9702, -0.3432,  0.2981,\n          0.5329],\n        [ 0.7720, -0.3804, -1.0024, -0.2509,  1.5041, -0.0620,  0.0306,  0.0806,\n          0.0202, -0.1209,  0.5778, -0.2847, -0.7502, -0.1877,  1.1256, -0.0700,\n          0.0345,  0.0909,  0.0227, -0.1363, -0.4095,  0.2018,  0.5317,  0.1331,\n         -0.7978],\n        [ 0.0036, -0.7082,  0.2082,  1.3907, -0.4008, -0.0003,  0.0569, -0.0167,\n         -0.1118,  0.0322,  0.0027, -0.5300,  0.1558,  1.0408, -0.3000, -0.0003,\n          0.0642, -0.0189, -0.1261,  0.0363, -0.0019,  0.3756, -0.1104, -0.7377,\n          0.2126],\n        [-0.4169,  0.9304,  0.1580, -1.0507, -0.4865, -0.1176,  0.2625,  0.0446,\n         -0.2964, -0.1373, -0.0849,  0.1895,  0.0322, -0.2140, -0.0991,  0.2586,\n         -0.5772, -0.0980,  0.6518,  0.3018,  0.2081, -0.4645, -0.0789,  0.5245,\n          0.2429],\n        [-0.2336,  0.0229,  0.4506,  0.0588, -0.7180, -0.0659,  0.0065,  0.1271,\n          0.0166, -0.2026, -0.0476,  0.0047,  0.0918,  0.0120, -0.1462,  0.1449,\n         -0.0142, -0.2795, -0.0365,  0.4454,  0.1166, -0.0114, -0.2249, -0.0294,\n          0.3584],\n        [-0.1409,  1.0217,  0.3614, -0.3139, -0.5611, -0.0398,  0.2883,  0.1020,\n         -0.0886, -0.1583, -0.0287,  0.2081,  0.0736, -0.0639, -0.1143,  0.0874,\n         -0.6338, -0.2242,  0.1947,  0.3481,  0.0704, -0.5101, -0.1804,  0.1567,\n          0.2801],\n        [ 0.4312, -0.2125, -0.5599, -0.1401,  0.8402,  0.1217, -0.0599, -0.1580,\n         -0.0395,  0.2371,  0.0878, -0.0433, -0.1140, -0.0285,  0.1711, -0.2675,\n          0.1318,  0.3474,  0.0869, -0.5212, -0.2153,  0.1061,  0.2795,  0.0700,\n         -0.4194],\n        [ 0.0020, -0.3956,  0.1163,  0.7768, -0.2239,  0.0006, -0.1116,  0.0328,\n          0.2192, -0.0632,  0.0004, -0.0806,  0.0237,  0.1582, -0.0456, -0.0013,\n          0.2454, -0.0721, -0.4819,  0.1389, -0.0010,  0.1975, -0.0581, -0.3878,\n          0.1118]])\nanalytical:tensor([[-0.3519,  0.7853,  0.1334, -0.8869, -0.4107, -0.1127,  0.2516,  0.0427,\n         -0.2841, -0.1315,  0.0555, -0.1238, -0.0210,  0.1398,  0.0648, -0.1272,\n          0.2838,  0.0482, -0.3205, -0.1484,  0.1304, -0.2910, -0.0494,  0.3286,\n         -0.1436],\n        [-0.1972,  0.0193,  0.3803,  0.0496, -0.6060, -0.0632,  0.0062,  0.1218,\n          0.0159, -0.1941,  0.0311, -0.0030, -0.0600, -0.0078,  0.0956, -0.0713,\n          0.0070,  0.1374,  0.0179, -0.2190,  0.0731, -0.0072, -0.1409, -0.0184,\n         -0.3407],\n        [-0.1190,  0.8624,  0.3051, -0.2650, -0.4737, -0.0381,  0.2763,  0.0977,\n         -0.0849, -0.1517,  0.0188, -0.1360, -0.0481,  0.0418,  0.0747, -0.0430,\n          0.3117,  0.1102, -0.0958, -0.1712,  0.0441, -0.3195, -0.1130,  0.0982,\n         -0.2718],\n        [ 0.3640, -0.1793, -0.4726, -0.1183,  0.7092,  0.1166, -0.0574, -0.1514,\n         -0.0379,  0.2272, -0.0574,  0.0283,  0.0745,  0.0186, -0.1118,  0.1315,\n         -0.0648, -0.1708, -0.0427,  0.2563, -0.1349,  0.0664,  0.1751,  0.0438,\n          0.4181],\n        [ 0.0017, -0.3339,  0.0982,  0.6557, -0.1890,  0.0005, -0.1070,  0.0314,\n          0.2100, -0.0605, -0.0003,  0.0526, -0.0155, -0.1034,  0.0298,  0.0006,\n         -0.1207,  0.0355,  0.2370, -0.0683, -0.0006,  0.1237, -0.0364, -0.2429,\n         -0.0919],\n        [ 0.0955, -0.2132, -0.0362,  0.2407,  0.1115, -0.0274,  0.0612,  0.0104,\n         -0.0691, -0.0320,  0.0300, -0.0670, -0.0114,  0.0757,  0.0350, -0.0648,\n          0.1446,  0.0246, -0.1633, -0.0756,  0.1864, -0.4161, -0.0707,  0.4698,\n          0.2978],\n        [ 0.0535, -0.0053, -0.1032, -0.0135,  0.1645, -0.0154,  0.0015,  0.0296,\n          0.0039, -0.0472,  0.0168, -0.0017, -0.0324, -0.0042,  0.0517, -0.0363,\n          0.0036,  0.0700,  0.0091, -0.1116,  0.1045, -0.0102, -0.2015, -0.0263,\n          0.4745],\n        [ 0.0323, -0.2341, -0.0828,  0.0719,  0.1286, -0.0093,  0.0672,  0.0238,\n         -0.0206, -0.0369,  0.0101, -0.0736, -0.0260,  0.0226,  0.0404, -0.0219,\n          0.1588,  0.0562, -0.0488, -0.0872,  0.0630, -0.4569, -0.1616,  0.1404,\n          0.3724],\n        [-0.0988,  0.0487,  0.1283,  0.0321, -0.1925,  0.0284, -0.0140, -0.0368,\n         -0.0092,  0.0552, -0.0311,  0.0153,  0.0403,  0.0101, -0.0605,  0.0670,\n         -0.0330, -0.0870, -0.0218,  0.1306, -0.1928,  0.0950,  0.2504,  0.0627,\n         -0.5605],\n        [-0.0005,  0.0906, -0.0266, -0.1780,  0.0513,  0.0001, -0.0260,  0.0076,\n          0.0511, -0.0147, -0.0001,  0.0285, -0.0084, -0.0559,  0.0161,  0.0003,\n         -0.0615,  0.0181,  0.1207, -0.0348, -0.0009,  0.1769, -0.0520, -0.3474,\n          0.1441],\n        [ 0.2218, -0.4949, -0.0841,  0.5589,  0.2588,  0.3033, -0.6769, -0.1150,\n          0.7644,  0.3540,  0.2494, -0.5565, -0.0945,  0.6284,  0.2910, -0.3366,\n          0.7513,  0.1276, -0.8484, -0.3928,  0.0710, -0.1584, -0.0269,  0.1789,\n          0.2692],\n        [ 0.1243, -0.0122, -0.2397, -0.0313,  0.3819,  0.1700, -0.0167, -0.3278,\n         -0.0428,  0.5223,  0.1397, -0.0137, -0.2695, -0.0352,  0.4294, -0.1887,\n          0.0185,  0.3638,  0.0475, -0.5797,  0.0398, -0.0039, -0.0767, -0.0100,\n          0.4785],\n        [ 0.0750, -0.5435, -0.1923,  0.1670,  0.2985,  0.1025, -0.7433, -0.2630,\n          0.2284,  0.4083,  0.0843, -0.6111, -0.2162,  0.1877,  0.3356, -0.1138,\n          0.8250,  0.2918, -0.2535, -0.4531,  0.0240, -0.1740, -0.0615,  0.0535,\n          0.3774],\n        [-0.2294,  0.1130,  0.2979,  0.0745, -0.4469, -0.3137,  0.1546,  0.4074,\n          0.1020, -0.6113, -0.2579,  0.1271,  0.3349,  0.0838, -0.5025,  0.3482,\n         -0.1716, -0.4521, -0.1131,  0.6784, -0.0734,  0.0362,  0.0954,  0.0239,\n         -0.5721],\n        [-0.0011,  0.2104, -0.0619, -0.4132,  0.1191, -0.0015,  0.2878, -0.0846,\n         -0.5652,  0.1629, -0.0012,  0.2366, -0.0696, -0.4646,  0.1339,  0.0016,\n         -0.3194,  0.0939,  0.6272, -0.1808, -0.0003,  0.0674, -0.0198, -0.1323,\n          0.1402],\n        [-0.7464,  1.6657,  0.2829, -1.8810, -0.8710,  0.0600, -0.1339, -0.0227,\n          0.1512,  0.0700, -0.5586,  1.2466,  0.2117, -1.4077, -0.6518,  0.0677,\n         -0.1510, -0.0256,  0.1705,  0.0790,  0.3959, -0.8835, -0.1501,  0.9977,\n         -0.1653],\n        [-0.4183,  0.0410,  0.8067,  0.1053, -1.2853,  0.0336, -0.0033, -0.0648,\n         -0.0085,  0.1033, -0.3130,  0.0307,  0.6037,  0.0788, -0.9619,  0.0379,\n         -0.0037, -0.0731, -0.0095,  0.1165,  0.2219, -0.0218, -0.4279, -0.0558,\n         -0.5171],\n        [-0.2523,  1.8291,  0.6471, -0.5620, -1.0046,  0.0203, -0.1470, -0.0520,\n          0.0452,  0.0807, -0.1888,  1.3689,  0.4842, -0.4206, -0.7518,  0.0229,\n         -0.1658, -0.0587,  0.0509,  0.0911,  0.1338, -0.9702, -0.3432,  0.2981,\n         -0.4158],\n        [ 0.7720, -0.3804, -1.0024, -0.2509,  1.5041, -0.0620,  0.0306,  0.0806,\n          0.0202, -0.1209,  0.5778, -0.2847, -0.7502, -0.1877,  1.1256, -0.0700,\n          0.0345,  0.0909,  0.0227, -0.1363, -0.4095,  0.2018,  0.5317,  0.1331,\n          0.6462],\n        [ 0.0036, -0.7082,  0.2082,  1.3907, -0.4008, -0.0003,  0.0569, -0.0167,\n         -0.1118,  0.0322,  0.0027, -0.5300,  0.1558,  1.0408, -0.3000, -0.0003,\n          0.0642, -0.0189, -0.1261,  0.0363, -0.0019,  0.3756, -0.1104, -0.7377,\n         -0.1308],\n        [-0.4169,  0.9304,  0.1580, -1.0507, -0.4865, -0.1176,  0.2625,  0.0446,\n         -0.2964, -0.1373, -0.0849,  0.1895,  0.0322, -0.2140, -0.0991,  0.2586,\n         -0.5772, -0.0980,  0.6518,  0.3018,  0.2081, -0.4645, -0.0789,  0.5245,\n         -0.1075],\n        [-0.2336,  0.0229,  0.4506,  0.0588, -0.7180, -0.0659,  0.0065,  0.1271,\n          0.0166, -0.2026, -0.0476,  0.0047,  0.0918,  0.0120, -0.1462,  0.1449,\n         -0.0142, -0.2795, -0.0365,  0.4454,  0.1166, -0.0114, -0.2249, -0.0294,\n         -0.3113],\n        [-0.1409,  1.0217,  0.3614, -0.3139, -0.5611, -0.0398,  0.2883,  0.1020,\n         -0.0886, -0.1583, -0.0287,  0.2081,  0.0736, -0.0639, -0.1143,  0.0874,\n         -0.6338, -0.2242,  0.1947,  0.3481,  0.0704, -0.5101, -0.1804,  0.1567,\n         -0.2498],\n        [ 0.4312, -0.2125, -0.5599, -0.1401,  0.8402,  0.1217, -0.0599, -0.1580,\n         -0.0395,  0.2371,  0.0878, -0.0433, -0.1140, -0.0285,  0.1711, -0.2675,\n          0.1318,  0.3474,  0.0869, -0.5212, -0.2153,  0.1061,  0.2795,  0.0700,\n          0.3872],\n        [ 0.0020, -0.3956,  0.1163,  0.7768, -0.2239,  0.0006, -0.1116,  0.0328,\n          0.2192, -0.0632,  0.0004, -0.0806,  0.0237,  0.1582, -0.0456, -0.0013,\n          0.2454, -0.0721, -0.4819,  0.1389, -0.0010,  0.1975, -0.0581, -0.3878,\n         -0.0801]])\n\n\n======================================================================\nERROR: test_gesv_batched (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3409, in check\n    output_variable, (self_variable,) + args_variable)\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-5.2063e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00, -5.2063e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00, -5.2063e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        ...,\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0869e-01,\n          0.0000e+00,  4.1437e+05],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n         -3.0869e-01,  0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00, -3.0869e-01]])\nanalytical:tensor([[-5.2063,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -5.2063,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -5.2063,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [-0.0000, -0.0000, -0.0000,  ..., -0.3087, -0.0000, -0.0000],\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.3087, -0.0000],\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.3087]])\n\n\n======================================================================\nERROR: test_gesv_batched_broadcast_A (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3409, in check\n    output_variable, (self_variable,) + args_variable)\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-0.0583,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.0583,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.0583,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.3674,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.3674,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.3674]])\nanalytical:tensor([[-0.0583,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.0583,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.0583,  ...,  0.0000,  0.0000,  0.0000],\n        ...,\n        [-0.0000, -0.0000, -0.0000,  ...,  0.3674, -0.0000, -0.0000],\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.3674, -0.0000],\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.3674]])\n\n\n======================================================================\nERROR: test_gesv_batched_broadcast_b (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3409, in check\n    output_variable, (self_variable,) + args_variable)\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-0.2197,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.2197,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.2197,  ...,  0.4906,  0.0000,  0.0000],\n        ...,\n        [ 0.0000,  0.0000, -0.5873,  ..., -0.1934,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.1934,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1934]])\nanalytical:tensor([[-0.2197,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.2197,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.2197,  ...,  0.4906,  0.0000,  0.0000],\n        ...,\n        [ 0.0000,  0.0000, -0.5873,  ..., -0.1934,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.1934,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1934]])\n\n\n======================================================================\nERROR: test_potrf (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 1914, in test_potrf\n    run_test(upper=True)\n  File \"test_autograd.py\", line 1911, in run_test\n    gradcheck(func, [root])\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 217, in gradcheck\n    return fail_test('Backward is not reentrant, i.e., running backward with same '\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient\n\n======================================================================\nFAIL: test_gesv_batched_dims (__main__.TestAutograd)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\n    fn(*args, **kwargs)\n  File \"test_autograd.py\", line 3475, in do_test\n    check(name)\n  File \"test_autograd.py\", line 3400, in check\n    self.assertEqual(unpack_variables(output_variable), output_tensor)\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 352, in assertEqual\n    self.assertEqual(x_, y_, prec, message)\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 344, in assertEqual\n    assertTensorsEqual(x, y)\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 336, in assertTensorsEqual\n    self.assertLessEqual(max_err, prec, message)\nAssertionError: tensor(0.7946, grad_fn=<MaxBackward1>) not less than or equal to 1e-05 :\n\n----------------------------------------------------------------------\nRan 831 tests in 69.093s\n\nFAILED (failures=1, errors=5, skipped=12)\nTraceback (most recent call last):\n  File \"test/run_test.py\", line 345, in <module>\n    main()\n  File \"test/run_test.py\", line 337, in main\n    raise RuntimeError(message)\nRuntimeError: test_autograd failed!\n\n\nOS: nixos-unstable\nPyTorch version: 0.4.1 (rev a24163a)\nPython version: 3.6\nCUDA/cuDNN version: CPU-only\nGCC version (if compiling from source): 7.3.0\nCMake version: 3.11.2\n\nAny ideas what might went wrong?", "body": "I'm working on packaging pytorch-0.4.1 for NixOS and I stumbled upon the following test error.\r\n\r\n```\r\nRunning test_autograd ...\r\n............................................................................................s.........s............................................................................................................................s............................................EEEEF.s........................................................s................................................................................................................................................................ss..................................................s.E........................................................................................s...........................s......................................................................................................................................s......................s.....\r\n======================================================================\r\nERROR: test_gesv (__main__.TestAutograd)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\r\n    fn(*args, **kwargs)\r\n  File \"test_autograd.py\", line 3475, in do_test\r\n    check(name)\r\n  File \"test_autograd.py\", line 3409, in check\r\n    output_variable, (self_variable,) + args_variable)\r\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\r\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\r\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\r\n    raise RuntimeError(msg)\r\nRuntimeError: Jacobian mismatch for output 0 with respect to input 1,\r\nnumerical:tensor([[-0.3519,  0.7853,  0.1334, -0.8869, -0.4107, -0.1127,  0.2516,  0.0427,\r\n         -0.2841, -0.1315,  0.0555, -0.1238, -0.0210,  0.1398,  0.0648, -0.1272,\r\n          0.2838,  0.0482, -0.3205, -0.1484,  0.1304, -0.2910, -0.0494,  0.3286,\r\n          0.1522],\r\n        [-0.1972,  0.0193,  0.3803,  0.0496, -0.6060, -0.0632,  0.0062,  0.1218,\r\n          0.0159, -0.1941,  0.0311, -0.0030, -0.0600, -0.0078,  0.0956, -0.0713,\r\n          0.0070,  0.1374,  0.0179, -0.2190,  0.0731, -0.0072, -0.1409, -0.0184,\r\n          0.2245],\r\n        [-0.1190,  0.8624,  0.3051, -0.2650, -0.4737, -0.0381,  0.2763,  0.0977,\r\n         -0.0849, -0.1517,  0.0188, -0.1360, -0.0481,  0.0418,  0.0747, -0.0430,\r\n          0.3117,  0.1102, -0.0958, -0.1712,  0.0441, -0.3195, -0.1130,  0.0982,\r\n          0.1755],\r\n        [ 0.3640, -0.1793, -0.4726, -0.1183,  0.7092,  0.1166, -0.0574, -0.1514,\r\n         -0.0379,  0.2272, -0.0574,  0.0283,  0.0745,  0.0186, -0.1118,  0.1315,\r\n         -0.0648, -0.1708, -0.0427,  0.2563, -0.1349,  0.0664,  0.1751,  0.0438,\r\n         -0.2628],\r\n        [ 0.0017, -0.3339,  0.0982,  0.6557, -0.1890,  0.0005, -0.1070,  0.0314,\r\n          0.2100, -0.0605, -0.0003,  0.0526, -0.0155, -0.1034,  0.0298,  0.0006,\r\n         -0.1207,  0.0355,  0.2370, -0.0683, -0.0006,  0.1237, -0.0364, -0.2429,\r\n          0.0700],\r\n        [ 0.0955, -0.2132, -0.0362,  0.2407,  0.1115, -0.0274,  0.0612,  0.0104,\r\n         -0.0691, -0.0320,  0.0300, -0.0670, -0.0114,  0.0757,  0.0350, -0.0648,\r\n          0.1446,  0.0246, -0.1633, -0.0756,  0.1864, -0.4161, -0.0707,  0.4698,\r\n          0.2176],\r\n        [ 0.0535, -0.0053, -0.1032, -0.0135,  0.1645, -0.0154,  0.0015,  0.0296,\r\n          0.0039, -0.0472,  0.0168, -0.0017, -0.0324, -0.0042,  0.0517, -0.0363,\r\n          0.0036,  0.0700,  0.0091, -0.1116,  0.1045, -0.0102, -0.2015, -0.0263,\r\n          0.3211],\r\n        [ 0.0323, -0.2341, -0.0828,  0.0719,  0.1286, -0.0093,  0.0672,  0.0238,\r\n         -0.0206, -0.0369,  0.0101, -0.0736, -0.0260,  0.0226,  0.0404, -0.0219,\r\n          0.1588,  0.0562, -0.0488, -0.0872,  0.0630, -0.4569, -0.1616,  0.1404,\r\n          0.2509],\r\n        [-0.0988,  0.0487,  0.1283,  0.0321, -0.1925,  0.0284, -0.0140, -0.0368,\r\n         -0.0092,  0.0552, -0.0311,  0.0153,  0.0403,  0.0101, -0.0605,  0.0670,\r\n         -0.0330, -0.0870, -0.0218,  0.1306, -0.1928,  0.0950,  0.2504,  0.0627,\r\n         -0.3757],\r\n        [-0.0005,  0.0906, -0.0266, -0.1780,  0.0513,  0.0001, -0.0260,  0.0076,\r\n          0.0511, -0.0147, -0.0001,  0.0285, -0.0084, -0.0559,  0.0161,  0.0003,\r\n         -0.0615,  0.0181,  0.1207, -0.0348, -0.0009,  0.1769, -0.0520, -0.3474,\r\n          0.1001],\r\n        [ 0.2218, -0.4949, -0.0841,  0.5589,  0.2588,  0.3033, -0.6769, -0.1150,\r\n          0.7644,  0.3540,  0.2494, -0.5565, -0.0945,  0.6284,  0.2910, -0.3366,\r\n          0.7513,  0.1276, -0.8484, -0.3928,  0.0710, -0.1584, -0.0269,  0.1789,\r\n          0.0829],\r\n        [ 0.1243, -0.0122, -0.2397, -0.0313,  0.3819,  0.1700, -0.0167, -0.3278,\r\n         -0.0428,  0.5223,  0.1397, -0.0137, -0.2695, -0.0352,  0.4294, -0.1887,\r\n          0.0185,  0.3638,  0.0475, -0.5797,  0.0398, -0.0039, -0.0767, -0.0100,\r\n          0.1223],\r\n        [ 0.0750, -0.5435, -0.1923,  0.1670,  0.2985,  0.1025, -0.7433, -0.2630,\r\n          0.2284,  0.4083,  0.0843, -0.6111, -0.2162,  0.1877,  0.3356, -0.1138,\r\n          0.8250,  0.2918, -0.2535, -0.4531,  0.0240, -0.1740, -0.0615,  0.0535,\r\n          0.0956],\r\n        [-0.2294,  0.1130,  0.2979,  0.0745, -0.4469, -0.3137,  0.1546,  0.4074,\r\n          0.1020, -0.6113, -0.2579,  0.1271,  0.3349,  0.0838, -0.5025,  0.3482,\r\n         -0.1716, -0.4521, -0.1131,  0.6784, -0.0734,  0.0362,  0.0954,  0.0239,\r\n         -0.1431],\r\n        [-0.0011,  0.2104, -0.0619, -0.4132,  0.1191, -0.0015,  0.2878, -0.0846,\r\n         -0.5652,  0.1629, -0.0012,  0.2366, -0.0696, -0.4646,  0.1339,  0.0016,\r\n         -0.3194,  0.0939,  0.6272, -0.1808, -0.0003,  0.0674, -0.0198, -0.1323,\r\n          0.0381],\r\n        [-0.7464,  1.6657,  0.2829, -1.8810, -0.8710,  0.0600, -0.1339, -0.0227,\r\n          0.1512,  0.0700, -0.5586,  1.2466,  0.2117, -1.4077, -0.6518,  0.0677,\r\n         -0.1510, -0.0256,  0.1705,  0.0790,  0.3959, -0.8835, -0.1501,  0.9977,\r\n          0.4620],\r\n        [-0.4183,  0.0410,  0.8067,  0.1053, -1.2853,  0.0336, -0.0033, -0.0648,\r\n         -0.0085,  0.1033, -0.3130,  0.0307,  0.6037,  0.0788, -0.9619,  0.0379,\r\n         -0.0037, -0.0731, -0.0095,  0.1165,  0.2219, -0.0218, -0.4279, -0.0558,\r\n          0.6818],\r\n        [-0.2523,  1.8291,  0.6471, -0.5620, -1.0046,  0.0203, -0.1470, -0.0520,\r\n          0.0452,  0.0807, -0.1888,  1.3689,  0.4842, -0.4206, -0.7518,  0.0229,\r\n         -0.1658, -0.0587,  0.0509,  0.0911,  0.1338, -0.9702, -0.3432,  0.2981,\r\n          0.5329],\r\n        [ 0.7720, -0.3804, -1.0024, -0.2509,  1.5041, -0.0620,  0.0306,  0.0806,\r\n          0.0202, -0.1209,  0.5778, -0.2847, -0.7502, -0.1877,  1.1256, -0.0700,\r\n          0.0345,  0.0909,  0.0227, -0.1363, -0.4095,  0.2018,  0.5317,  0.1331,\r\n         -0.7978],\r\n        [ 0.0036, -0.7082,  0.2082,  1.3907, -0.4008, -0.0003,  0.0569, -0.0167,\r\n         -0.1118,  0.0322,  0.0027, -0.5300,  0.1558,  1.0408, -0.3000, -0.0003,\r\n          0.0642, -0.0189, -0.1261,  0.0363, -0.0019,  0.3756, -0.1104, -0.7377,\r\n          0.2126],\r\n        [-0.4169,  0.9304,  0.1580, -1.0507, -0.4865, -0.1176,  0.2625,  0.0446,\r\n         -0.2964, -0.1373, -0.0849,  0.1895,  0.0322, -0.2140, -0.0991,  0.2586,\r\n         -0.5772, -0.0980,  0.6518,  0.3018,  0.2081, -0.4645, -0.0789,  0.5245,\r\n          0.2429],\r\n        [-0.2336,  0.0229,  0.4506,  0.0588, -0.7180, -0.0659,  0.0065,  0.1271,\r\n          0.0166, -0.2026, -0.0476,  0.0047,  0.0918,  0.0120, -0.1462,  0.1449,\r\n         -0.0142, -0.2795, -0.0365,  0.4454,  0.1166, -0.0114, -0.2249, -0.0294,\r\n          0.3584],\r\n        [-0.1409,  1.0217,  0.3614, -0.3139, -0.5611, -0.0398,  0.2883,  0.1020,\r\n         -0.0886, -0.1583, -0.0287,  0.2081,  0.0736, -0.0639, -0.1143,  0.0874,\r\n         -0.6338, -0.2242,  0.1947,  0.3481,  0.0704, -0.5101, -0.1804,  0.1567,\r\n          0.2801],\r\n        [ 0.4312, -0.2125, -0.5599, -0.1401,  0.8402,  0.1217, -0.0599, -0.1580,\r\n         -0.0395,  0.2371,  0.0878, -0.0433, -0.1140, -0.0285,  0.1711, -0.2675,\r\n          0.1318,  0.3474,  0.0869, -0.5212, -0.2153,  0.1061,  0.2795,  0.0700,\r\n         -0.4194],\r\n        [ 0.0020, -0.3956,  0.1163,  0.7768, -0.2239,  0.0006, -0.1116,  0.0328,\r\n          0.2192, -0.0632,  0.0004, -0.0806,  0.0237,  0.1582, -0.0456, -0.0013,\r\n          0.2454, -0.0721, -0.4819,  0.1389, -0.0010,  0.1975, -0.0581, -0.3878,\r\n          0.1118]])\r\nanalytical:tensor([[-0.3519,  0.7853,  0.1334, -0.8869, -0.4107, -0.1127,  0.2516,  0.0427,\r\n         -0.2841, -0.1315,  0.0555, -0.1238, -0.0210,  0.1398,  0.0648, -0.1272,\r\n          0.2838,  0.0482, -0.3205, -0.1484,  0.1304, -0.2910, -0.0494,  0.3286,\r\n         -0.1436],\r\n        [-0.1972,  0.0193,  0.3803,  0.0496, -0.6060, -0.0632,  0.0062,  0.1218,\r\n          0.0159, -0.1941,  0.0311, -0.0030, -0.0600, -0.0078,  0.0956, -0.0713,\r\n          0.0070,  0.1374,  0.0179, -0.2190,  0.0731, -0.0072, -0.1409, -0.0184,\r\n         -0.3407],\r\n        [-0.1190,  0.8624,  0.3051, -0.2650, -0.4737, -0.0381,  0.2763,  0.0977,\r\n         -0.0849, -0.1517,  0.0188, -0.1360, -0.0481,  0.0418,  0.0747, -0.0430,\r\n          0.3117,  0.1102, -0.0958, -0.1712,  0.0441, -0.3195, -0.1130,  0.0982,\r\n         -0.2718],\r\n        [ 0.3640, -0.1793, -0.4726, -0.1183,  0.7092,  0.1166, -0.0574, -0.1514,\r\n         -0.0379,  0.2272, -0.0574,  0.0283,  0.0745,  0.0186, -0.1118,  0.1315,\r\n         -0.0648, -0.1708, -0.0427,  0.2563, -0.1349,  0.0664,  0.1751,  0.0438,\r\n          0.4181],\r\n        [ 0.0017, -0.3339,  0.0982,  0.6557, -0.1890,  0.0005, -0.1070,  0.0314,\r\n          0.2100, -0.0605, -0.0003,  0.0526, -0.0155, -0.1034,  0.0298,  0.0006,\r\n         -0.1207,  0.0355,  0.2370, -0.0683, -0.0006,  0.1237, -0.0364, -0.2429,\r\n         -0.0919],\r\n        [ 0.0955, -0.2132, -0.0362,  0.2407,  0.1115, -0.0274,  0.0612,  0.0104,\r\n         -0.0691, -0.0320,  0.0300, -0.0670, -0.0114,  0.0757,  0.0350, -0.0648,\r\n          0.1446,  0.0246, -0.1633, -0.0756,  0.1864, -0.4161, -0.0707,  0.4698,\r\n          0.2978],\r\n        [ 0.0535, -0.0053, -0.1032, -0.0135,  0.1645, -0.0154,  0.0015,  0.0296,\r\n          0.0039, -0.0472,  0.0168, -0.0017, -0.0324, -0.0042,  0.0517, -0.0363,\r\n          0.0036,  0.0700,  0.0091, -0.1116,  0.1045, -0.0102, -0.2015, -0.0263,\r\n          0.4745],\r\n        [ 0.0323, -0.2341, -0.0828,  0.0719,  0.1286, -0.0093,  0.0672,  0.0238,\r\n         -0.0206, -0.0369,  0.0101, -0.0736, -0.0260,  0.0226,  0.0404, -0.0219,\r\n          0.1588,  0.0562, -0.0488, -0.0872,  0.0630, -0.4569, -0.1616,  0.1404,\r\n          0.3724],\r\n        [-0.0988,  0.0487,  0.1283,  0.0321, -0.1925,  0.0284, -0.0140, -0.0368,\r\n         -0.0092,  0.0552, -0.0311,  0.0153,  0.0403,  0.0101, -0.0605,  0.0670,\r\n         -0.0330, -0.0870, -0.0218,  0.1306, -0.1928,  0.0950,  0.2504,  0.0627,\r\n         -0.5605],\r\n        [-0.0005,  0.0906, -0.0266, -0.1780,  0.0513,  0.0001, -0.0260,  0.0076,\r\n          0.0511, -0.0147, -0.0001,  0.0285, -0.0084, -0.0559,  0.0161,  0.0003,\r\n         -0.0615,  0.0181,  0.1207, -0.0348, -0.0009,  0.1769, -0.0520, -0.3474,\r\n          0.1441],\r\n        [ 0.2218, -0.4949, -0.0841,  0.5589,  0.2588,  0.3033, -0.6769, -0.1150,\r\n          0.7644,  0.3540,  0.2494, -0.5565, -0.0945,  0.6284,  0.2910, -0.3366,\r\n          0.7513,  0.1276, -0.8484, -0.3928,  0.0710, -0.1584, -0.0269,  0.1789,\r\n          0.2692],\r\n        [ 0.1243, -0.0122, -0.2397, -0.0313,  0.3819,  0.1700, -0.0167, -0.3278,\r\n         -0.0428,  0.5223,  0.1397, -0.0137, -0.2695, -0.0352,  0.4294, -0.1887,\r\n          0.0185,  0.3638,  0.0475, -0.5797,  0.0398, -0.0039, -0.0767, -0.0100,\r\n          0.4785],\r\n        [ 0.0750, -0.5435, -0.1923,  0.1670,  0.2985,  0.1025, -0.7433, -0.2630,\r\n          0.2284,  0.4083,  0.0843, -0.6111, -0.2162,  0.1877,  0.3356, -0.1138,\r\n          0.8250,  0.2918, -0.2535, -0.4531,  0.0240, -0.1740, -0.0615,  0.0535,\r\n          0.3774],\r\n        [-0.2294,  0.1130,  0.2979,  0.0745, -0.4469, -0.3137,  0.1546,  0.4074,\r\n          0.1020, -0.6113, -0.2579,  0.1271,  0.3349,  0.0838, -0.5025,  0.3482,\r\n         -0.1716, -0.4521, -0.1131,  0.6784, -0.0734,  0.0362,  0.0954,  0.0239,\r\n         -0.5721],\r\n        [-0.0011,  0.2104, -0.0619, -0.4132,  0.1191, -0.0015,  0.2878, -0.0846,\r\n         -0.5652,  0.1629, -0.0012,  0.2366, -0.0696, -0.4646,  0.1339,  0.0016,\r\n         -0.3194,  0.0939,  0.6272, -0.1808, -0.0003,  0.0674, -0.0198, -0.1323,\r\n          0.1402],\r\n        [-0.7464,  1.6657,  0.2829, -1.8810, -0.8710,  0.0600, -0.1339, -0.0227,\r\n          0.1512,  0.0700, -0.5586,  1.2466,  0.2117, -1.4077, -0.6518,  0.0677,\r\n         -0.1510, -0.0256,  0.1705,  0.0790,  0.3959, -0.8835, -0.1501,  0.9977,\r\n         -0.1653],\r\n        [-0.4183,  0.0410,  0.8067,  0.1053, -1.2853,  0.0336, -0.0033, -0.0648,\r\n         -0.0085,  0.1033, -0.3130,  0.0307,  0.6037,  0.0788, -0.9619,  0.0379,\r\n         -0.0037, -0.0731, -0.0095,  0.1165,  0.2219, -0.0218, -0.4279, -0.0558,\r\n         -0.5171],\r\n        [-0.2523,  1.8291,  0.6471, -0.5620, -1.0046,  0.0203, -0.1470, -0.0520,\r\n          0.0452,  0.0807, -0.1888,  1.3689,  0.4842, -0.4206, -0.7518,  0.0229,\r\n         -0.1658, -0.0587,  0.0509,  0.0911,  0.1338, -0.9702, -0.3432,  0.2981,\r\n         -0.4158],\r\n        [ 0.7720, -0.3804, -1.0024, -0.2509,  1.5041, -0.0620,  0.0306,  0.0806,\r\n          0.0202, -0.1209,  0.5778, -0.2847, -0.7502, -0.1877,  1.1256, -0.0700,\r\n          0.0345,  0.0909,  0.0227, -0.1363, -0.4095,  0.2018,  0.5317,  0.1331,\r\n          0.6462],\r\n        [ 0.0036, -0.7082,  0.2082,  1.3907, -0.4008, -0.0003,  0.0569, -0.0167,\r\n         -0.1118,  0.0322,  0.0027, -0.5300,  0.1558,  1.0408, -0.3000, -0.0003,\r\n          0.0642, -0.0189, -0.1261,  0.0363, -0.0019,  0.3756, -0.1104, -0.7377,\r\n         -0.1308],\r\n        [-0.4169,  0.9304,  0.1580, -1.0507, -0.4865, -0.1176,  0.2625,  0.0446,\r\n         -0.2964, -0.1373, -0.0849,  0.1895,  0.0322, -0.2140, -0.0991,  0.2586,\r\n         -0.5772, -0.0980,  0.6518,  0.3018,  0.2081, -0.4645, -0.0789,  0.5245,\r\n         -0.1075],\r\n        [-0.2336,  0.0229,  0.4506,  0.0588, -0.7180, -0.0659,  0.0065,  0.1271,\r\n          0.0166, -0.2026, -0.0476,  0.0047,  0.0918,  0.0120, -0.1462,  0.1449,\r\n         -0.0142, -0.2795, -0.0365,  0.4454,  0.1166, -0.0114, -0.2249, -0.0294,\r\n         -0.3113],\r\n        [-0.1409,  1.0217,  0.3614, -0.3139, -0.5611, -0.0398,  0.2883,  0.1020,\r\n         -0.0886, -0.1583, -0.0287,  0.2081,  0.0736, -0.0639, -0.1143,  0.0874,\r\n         -0.6338, -0.2242,  0.1947,  0.3481,  0.0704, -0.5101, -0.1804,  0.1567,\r\n         -0.2498],\r\n        [ 0.4312, -0.2125, -0.5599, -0.1401,  0.8402,  0.1217, -0.0599, -0.1580,\r\n         -0.0395,  0.2371,  0.0878, -0.0433, -0.1140, -0.0285,  0.1711, -0.2675,\r\n          0.1318,  0.3474,  0.0869, -0.5212, -0.2153,  0.1061,  0.2795,  0.0700,\r\n          0.3872],\r\n        [ 0.0020, -0.3956,  0.1163,  0.7768, -0.2239,  0.0006, -0.1116,  0.0328,\r\n          0.2192, -0.0632,  0.0004, -0.0806,  0.0237,  0.1582, -0.0456, -0.0013,\r\n          0.2454, -0.0721, -0.4819,  0.1389, -0.0010,  0.1975, -0.0581, -0.3878,\r\n         -0.0801]])\r\n\r\n\r\n======================================================================\r\nERROR: test_gesv_batched (__main__.TestAutograd)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\r\n    fn(*args, **kwargs)\r\n  File \"test_autograd.py\", line 3475, in do_test\r\n    check(name)\r\n  File \"test_autograd.py\", line 3409, in check\r\n    output_variable, (self_variable,) + args_variable)\r\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\r\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\r\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\r\n    raise RuntimeError(msg)\r\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\r\nnumerical:tensor([[-5.2063e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\r\n          0.0000e+00,  0.0000e+00],\r\n        [ 0.0000e+00, -5.2063e+00,  0.0000e+00,  ...,  0.0000e+00,\r\n          0.0000e+00,  0.0000e+00],\r\n        [ 0.0000e+00,  0.0000e+00, -5.2063e+00,  ...,  0.0000e+00,\r\n          0.0000e+00,  0.0000e+00],\r\n        ...,\r\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.0869e-01,\r\n          0.0000e+00,  4.1437e+05],\r\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\r\n         -3.0869e-01,  0.0000e+00],\r\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\r\n          0.0000e+00, -3.0869e-01]])\r\nanalytical:tensor([[-5.2063,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000, -5.2063,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000, -5.2063,  ...,  0.0000,  0.0000,  0.0000],\r\n        ...,\r\n        [-0.0000, -0.0000, -0.0000,  ..., -0.3087, -0.0000, -0.0000],\r\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.3087, -0.0000],\r\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.3087]])\r\n\r\n\r\n======================================================================\r\nERROR: test_gesv_batched_broadcast_A (__main__.TestAutograd)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\r\n    fn(*args, **kwargs)\r\n  File \"test_autograd.py\", line 3475, in do_test\r\n    check(name)\r\n  File \"test_autograd.py\", line 3409, in check\r\n    output_variable, (self_variable,) + args_variable)\r\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\r\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\r\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\r\n    raise RuntimeError(msg)\r\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\r\nnumerical:tensor([[-0.0583,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000, -0.0583,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000, -0.0583,  ...,  0.0000,  0.0000,  0.0000],\r\n        ...,\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.3674,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.3674,  0.0000],\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.3674]])\r\nanalytical:tensor([[-0.0583,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000, -0.0583,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000, -0.0583,  ...,  0.0000,  0.0000,  0.0000],\r\n        ...,\r\n        [-0.0000, -0.0000, -0.0000,  ...,  0.3674, -0.0000, -0.0000],\r\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.3674, -0.0000],\r\n        [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.3674]])\r\n\r\n\r\n======================================================================\r\nERROR: test_gesv_batched_broadcast_b (__main__.TestAutograd)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\r\n    fn(*args, **kwargs)\r\n  File \"test_autograd.py\", line 3475, in do_test\r\n    check(name)\r\n  File \"test_autograd.py\", line 3409, in check\r\n    output_variable, (self_variable,) + args_variable)\r\n  File \"test_autograd.py\", line 3332, in run_grad_and_gradgrad_checks\r\n    test_case.assertTrue(gradcheck(apply_method, input_variables, eps=1e-6, atol=PRECISION))\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 214, in gradcheck\r\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\r\n    raise RuntimeError(msg)\r\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\r\nnumerical:tensor([[-0.2197,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000, -0.2197,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000, -0.2197,  ...,  0.4906,  0.0000,  0.0000],\r\n        ...,\r\n        [ 0.0000,  0.0000, -0.5873,  ..., -0.1934,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.1934,  0.0000],\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1934]])\r\nanalytical:tensor([[-0.2197,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000, -0.2197,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000, -0.2197,  ...,  0.4906,  0.0000,  0.0000],\r\n        ...,\r\n        [ 0.0000,  0.0000, -0.5873,  ..., -0.1934,  0.0000,  0.0000],\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -0.1934,  0.0000],\r\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.1934]])\r\n\r\n\r\n======================================================================\r\nERROR: test_potrf (__main__.TestAutograd)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\r\n    fn(*args, **kwargs)\r\n  File \"test_autograd.py\", line 1914, in test_potrf\r\n    run_test(upper=True)\r\n  File \"test_autograd.py\", line 1911, in run_test\r\n    gradcheck(func, [root])\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 217, in gradcheck\r\n    return fail_test('Backward is not reentrant, i.e., running backward with same '\r\n  File \"/nix/store/l8kg9xdmyc51qji5ywh5ad9hakmlc9fn-python3.6-pytorch-0.4.1/lib/python3.6/site-packages/torch/autograd/gradcheck.py\", line 194, in fail_test\r\n    raise RuntimeError(msg)\r\nRuntimeError: Backward is not reentrant, i.e., running backward with same input and grad_output multiple times gives different values, although analytical gradient matches numerical gradient\r\n\r\n======================================================================\r\nFAIL: test_gesv_batched_dims (__main__.TestAutograd)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 104, in wrapper\r\n    fn(*args, **kwargs)\r\n  File \"test_autograd.py\", line 3475, in do_test\r\n    check(name)\r\n  File \"test_autograd.py\", line 3400, in check\r\n    self.assertEqual(unpack_variables(output_variable), output_tensor)\r\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 352, in assertEqual\r\n    self.assertEqual(x_, y_, prec, message)\r\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 344, in assertEqual\r\n    assertTensorsEqual(x, y)\r\n  File \"/tmp/nix-build-python3.6-pytorch-0.4.1.drv-0/source/test/common.py\", line 336, in assertTensorsEqual\r\n    self.assertLessEqual(max_err, prec, message)\r\nAssertionError: tensor(0.7946, grad_fn=<MaxBackward1>) not less than or equal to 1e-05 :\r\n\r\n----------------------------------------------------------------------\r\nRan 831 tests in 69.093s\r\n\r\nFAILED (failures=1, errors=5, skipped=12)\r\nTraceback (most recent call last):\r\n  File \"test/run_test.py\", line 345, in <module>\r\n    main()\r\n  File \"test/run_test.py\", line 337, in main\r\n    raise RuntimeError(message)\r\nRuntimeError: test_autograd failed!\r\n```\r\n\r\n- OS: nixos-unstable\r\n- PyTorch version: 0.4.1 (rev a24163a95edb)\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: CPU-only\r\n- GCC version (if compiling from source): 7.3.0\r\n- CMake version: 3.11.2\r\n\r\nAny ideas what might went wrong?"}