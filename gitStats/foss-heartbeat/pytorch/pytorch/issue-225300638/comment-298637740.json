{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/298637740", "html_url": "https://github.com/pytorch/pytorch/issues/1407#issuecomment-298637740", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1407", "id": 298637740, "node_id": "MDEyOklzc3VlQ29tbWVudDI5ODYzNzc0MA==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-02T13:37:24Z", "updated_at": "2017-05-02T13:37:24Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>a closure for each separate variable that adds the gradient values to a list?<br>\nYes, what you've proposed sounds about right.</p>\n</blockquote>\n<blockquote>\n<p>I'm wondering for long term usability if there's any reason not to have an option to, say, add a keyword option accumulate_gradients=False to backward?</p>\n</blockquote>\n<p>It's not obvious what one wants to do there. Do we just overwrite gradients? or keep a list of gradients?<br>\nThe semantics are hard and non-obvious, and the overall API becomes much harder to use.<br>\nHowever, in your benefit, we introduced an API on <code>master</code> (not yet on any binary releases) that will get you gradient wrt an input, independent of using <code>.grad</code>, i.e. it doesn't accumulate gradients: <a href=\"http://pytorch.org/docs/autograd.html#torch.autograd.grad\" rel=\"nofollow\">http://pytorch.org/docs/autograd.html#torch.autograd.grad</a><br>\nIt's more powerful, but also much more verbose to use.</p>", "body_text": "a closure for each separate variable that adds the gradient values to a list?\nYes, what you've proposed sounds about right.\n\n\nI'm wondering for long term usability if there's any reason not to have an option to, say, add a keyword option accumulate_gradients=False to backward?\n\nIt's not obvious what one wants to do there. Do we just overwrite gradients? or keep a list of gradients?\nThe semantics are hard and non-obvious, and the overall API becomes much harder to use.\nHowever, in your benefit, we introduced an API on master (not yet on any binary releases) that will get you gradient wrt an input, independent of using .grad, i.e. it doesn't accumulate gradients: http://pytorch.org/docs/autograd.html#torch.autograd.grad\nIt's more powerful, but also much more verbose to use.", "body": "> a closure for each separate variable that adds the gradient values to a list?\r\nYes, what you've proposed sounds about right.\r\n\r\n>  I'm wondering for long term usability if there's any reason not to have an option to, say, add a keyword option accumulate_gradients=False to backward?\r\n\r\nIt's not obvious what one wants to do there. Do we just overwrite gradients? or keep a list of gradients?\r\nThe semantics are hard and non-obvious, and the overall API becomes much harder to use.\r\nHowever, in your benefit, we introduced an API on `master` (not yet on any binary releases) that will get you gradient wrt an input, independent of using `.grad`, i.e. it doesn't accumulate gradients: http://pytorch.org/docs/autograd.html#torch.autograd.grad\r\nIt's more powerful, but also much more verbose to use.\r\n"}