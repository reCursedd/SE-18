{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2498", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2498/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2498/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2498/events", "html_url": "https://github.com/pytorch/pytorch/issues/2498", "id": 251579386, "node_id": "MDU6SXNzdWUyNTE1NzkzODY=", "number": 2498, "title": "Memory leak in high order derivative? (pytorch 0.2)", "user": {"login": "xuanqing94", "id": 8935605, "node_id": "MDQ6VXNlcjg5MzU2MDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/8935605?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xuanqing94", "html_url": "https://github.com/xuanqing94", "followers_url": "https://api.github.com/users/xuanqing94/followers", "following_url": "https://api.github.com/users/xuanqing94/following{/other_user}", "gists_url": "https://api.github.com/users/xuanqing94/gists{/gist_id}", "starred_url": "https://api.github.com/users/xuanqing94/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xuanqing94/subscriptions", "organizations_url": "https://api.github.com/users/xuanqing94/orgs", "repos_url": "https://api.github.com/users/xuanqing94/repos", "events_url": "https://api.github.com/users/xuanqing94/events{/privacy}", "received_events_url": "https://api.github.com/users/xuanqing94/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-08-21T08:05:07Z", "updated_at": "2017-09-18T13:38:15Z", "closed_at": "2017-08-26T23:05:47Z", "author_association": "NONE", "body_html": "<p>Not sure if this implementation is efficient, but when calculating Hessian-vector product in a loop, it terminates because of out of memory after ~30 iterations (Titan X), here is my implementation:</p>\n<div class=\"highlight highlight-source-python\"><pre>     net <span class=\"pl-k\">=</span> VGG() <span class=\"pl-c\"><span class=\"pl-c\">#</span> could be other model</span>\n     loss_f <span class=\"pl-k\">=</span> nn.NLLLoss()\n     net.cuda()\n     loss_f.cuda()\n     data <span class=\"pl-k\">=</span> dst.CIFAR10(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~/cifar10-py<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">train</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>tfs.Compose(\n                                                                   [ tfs.ToTensor(),\n                                                                     tfs.Normalize((<span class=\"pl-c1\">.5</span>, <span class=\"pl-c1\">.5</span>, <span class=\"pl-c1\">.5</span>), (<span class=\"pl-c1\">.5</span>, <span class=\"pl-c1\">.5</span>, <span class=\"pl-c1\">.5</span>)) ]))\n     dataloader <span class=\"pl-k\">=</span> DataLoader(data, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>opt.batchSize, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n     v <span class=\"pl-k\">=</span> []\n     <span class=\"pl-k\">for</span> i, p <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(net.parameters()):\n         v.append(Variable(torch.FloatTensor(p.data.size()).normal_(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>).cuda(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))\n     <span class=\"pl-k\">for</span> img, label <span class=\"pl-k\">in</span> dataloader:\n         img <span class=\"pl-k\">=</span> img.cuda()\n         label <span class=\"pl-k\">=</span> label.cuda()\n         <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(img, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n         target <span class=\"pl-k\">=</span> Variable(label)\n         output <span class=\"pl-k\">=</span> net(<span class=\"pl-c1\">input</span>)\n         loss <span class=\"pl-k\">=</span> loss_f(output, target)\n         grad_params <span class=\"pl-k\">=</span> torch.autograd.grad(loss, net.parameters(), <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n         inner_prod <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n         <span class=\"pl-k\">for</span> vi, grad <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(v, grad_params):\n             inner_prod <span class=\"pl-k\">+=</span> torch.sum(grad <span class=\"pl-k\">*</span> vi)\n         Hv <span class=\"pl-k\">=</span> torch.autograd.grad(inner_prod, net.parameters(), <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p>Basically, it first calculates loss, then gradient of weight. Do an inner product with v, then takes gradient of weight again. Similar code works good on Theano.</p>\n<p>Here is the traceback:</p>\n<p>THCudaCheck FAIL file=xxx/pytorch-0.2.0/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory<br>\nTraceback (most recent call last):<br>\nFile \"./main.py\", line 50, in <br>\nHv = torch.autograd.grad(inner_prod, grad_params, create_graph=True)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/<strong>init</strong>.py\", line 153, in grad<br>\ninputs, only_inputs)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/nn/<em>functions/thnn/batchnorm_double_backwards.py\", line 70, in batchnorm_double_backwards_fn<br>\ngI_2t = (gOinmu_sum * sigma2_eps_neg_3_2).div</em>(M) * (ggI_sum.div(M) - ggI)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py\", line 820, in <strong>sub</strong><br>\nreturn self.sub(other)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py\", line 332, in sub<br>\nreturn self._sub(other, False)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py\", line 326, in _sub<br>\nreturn Sub.apply(self, other, inplace)<br>\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/basic_ops.py\", line 34, in forward<br>\nreturn a.sub(b)<br>\nRuntimeError: cuda runtime error (2) : out of memory at xxx/pytorch-0.2.0/torch/lib/THC/generic/THCStorage.cu:66</p>", "body_text": "Not sure if this implementation is efficient, but when calculating Hessian-vector product in a loop, it terminates because of out of memory after ~30 iterations (Titan X), here is my implementation:\n     net = VGG() # could be other model\n     loss_f = nn.NLLLoss()\n     net.cuda()\n     loss_f.cuda()\n     data = dst.CIFAR10(\"~/cifar10-py\", download=True, train=True, transform=tfs.Compose(\n                                                                   [ tfs.ToTensor(),\n                                                                     tfs.Normalize((.5, .5, .5), (.5, .5, .5)) ]))\n     dataloader = DataLoader(data, batch_size=opt.batchSize, shuffle=True, num_workers=2)\n     v = []\n     for i, p in enumerate(net.parameters()):\n         v.append(Variable(torch.FloatTensor(p.data.size()).normal_(0, 1).cuda(), requires_grad=False))\n     for img, label in dataloader:\n         img = img.cuda()\n         label = label.cuda()\n         input = Variable(img, requires_grad=True)\n         target = Variable(label)\n         output = net(input)\n         loss = loss_f(output, target)\n         grad_params = torch.autograd.grad(loss, net.parameters(), create_graph=True)\n         inner_prod = 0.0\n         for vi, grad in zip(v, grad_params):\n             inner_prod += torch.sum(grad * vi)\n         Hv = torch.autograd.grad(inner_prod, net.parameters(), create_graph=True)\nBasically, it first calculates loss, then gradient of weight. Do an inner product with v, then takes gradient of weight again. Similar code works good on Theano.\nHere is the traceback:\nTHCudaCheck FAIL file=xxx/pytorch-0.2.0/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\nTraceback (most recent call last):\nFile \"./main.py\", line 50, in \nHv = torch.autograd.grad(inner_prod, grad_params, create_graph=True)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/init.py\", line 153, in grad\ninputs, only_inputs)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/nn/functions/thnn/batchnorm_double_backwards.py\", line 70, in batchnorm_double_backwards_fn\ngI_2t = (gOinmu_sum * sigma2_eps_neg_3_2).div(M) * (ggI_sum.div(M) - ggI)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py\", line 820, in sub\nreturn self.sub(other)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py\", line 332, in sub\nreturn self._sub(other, False)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py\", line 326, in _sub\nreturn Sub.apply(self, other, inplace)\nFile \"/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/basic_ops.py\", line 34, in forward\nreturn a.sub(b)\nRuntimeError: cuda runtime error (2) : out of memory at xxx/pytorch-0.2.0/torch/lib/THC/generic/THCStorage.cu:66", "body": "Not sure if this implementation is efficient, but when calculating Hessian-vector product in a loop, it terminates because of out of memory after ~30 iterations (Titan X), here is my implementation:\r\n\r\n```Python\r\n     net = VGG() # could be other model\r\n     loss_f = nn.NLLLoss()\r\n     net.cuda()\r\n     loss_f.cuda()\r\n     data = dst.CIFAR10(\"~/cifar10-py\", download=True, train=True, transform=tfs.Compose(\r\n                                                                   [ tfs.ToTensor(),\r\n                                                                     tfs.Normalize((.5, .5, .5), (.5, .5, .5)) ]))\r\n     dataloader = DataLoader(data, batch_size=opt.batchSize, shuffle=True, num_workers=2)\r\n     v = []\r\n     for i, p in enumerate(net.parameters()):\r\n         v.append(Variable(torch.FloatTensor(p.data.size()).normal_(0, 1).cuda(), requires_grad=False))\r\n     for img, label in dataloader:\r\n         img = img.cuda()\r\n         label = label.cuda()\r\n         input = Variable(img, requires_grad=True)\r\n         target = Variable(label)\r\n         output = net(input)\r\n         loss = loss_f(output, target)\r\n         grad_params = torch.autograd.grad(loss, net.parameters(), create_graph=True)\r\n         inner_prod = 0.0\r\n         for vi, grad in zip(v, grad_params):\r\n             inner_prod += torch.sum(grad * vi)\r\n         Hv = torch.autograd.grad(inner_prod, net.parameters(), create_graph=True)\r\n```\r\n\r\nBasically, it first calculates loss, then gradient of weight. Do an inner product with v, then takes gradient of weight again. Similar code works good on Theano.\r\n\r\nHere is the traceback:\r\n\r\nTHCudaCheck FAIL file=xxx/pytorch-0.2.0/torch/lib/THC/generic/THCStorage.cu line=66 error=2 : out of memory\r\nTraceback (most recent call last):\r\n  File \"./main.py\", line 50, in <module>\r\n    Hv = torch.autograd.grad(inner_prod, grad_params, create_graph=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.py\", line 153, in grad\r\n    inputs, only_inputs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/nn/_functions/thnn/batchnorm_double_backwards.py\", line 70, in batchnorm_double_backwards_fn\r\n    gI_2t = (gOinmu_sum * sigma2_eps_neg_3_2).div_(M) * (ggI_sum.div(M) - ggI)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py\", line 820, in __sub__\r\n    return self.sub(other)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py\", line 332, in sub\r\n    return self._sub(other, False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.py\", line 326, in _sub\r\n    return Sub.apply(self, other, inplace)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/autograd/_functions/basic_ops.py\", line 34, in forward\r\n    return a.sub(b)\r\nRuntimeError: cuda runtime error (2) : out of memory at xxx/pytorch-0.2.0/torch/lib/THC/generic/THCStorage.cu:66"}