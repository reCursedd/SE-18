{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/328719631", "html_url": "https://github.com/pytorch/pytorch/issues/2498#issuecomment-328719631", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2498", "id": 328719631, "node_id": "MDEyOklzc3VlQ29tbWVudDMyODcxOTYzMQ==", "user": {"login": "chao1224", "id": 14155181, "node_id": "MDQ6VXNlcjE0MTU1MTgx", "avatar_url": "https://avatars1.githubusercontent.com/u/14155181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chao1224", "html_url": "https://github.com/chao1224", "followers_url": "https://api.github.com/users/chao1224/followers", "following_url": "https://api.github.com/users/chao1224/following{/other_user}", "gists_url": "https://api.github.com/users/chao1224/gists{/gist_id}", "starred_url": "https://api.github.com/users/chao1224/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chao1224/subscriptions", "organizations_url": "https://api.github.com/users/chao1224/orgs", "repos_url": "https://api.github.com/users/chao1224/repos", "events_url": "https://api.github.com/users/chao1224/events{/privacy}", "received_events_url": "https://api.github.com/users/chao1224/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-12T03:00:49Z", "updated_at": "2017-09-12T03:00:49Z", "author_association": "NONE", "body_html": "<p>Hi Xuanqing, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8935605\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/xuanqing94\">@xuanqing94</a></p>\n<p>I'm also working on second-order derivative. Can you help with two questions here?</p>\n<ol>\n<li>It seems like the hessian output has the same dimension as weight parameters right? Since all of the gradients are accumulated w.r.t. the target tensor.</li>\n<li>When calculating the inner product, <code>inner_prod += torch.sum(grad * vi)</code> , the <code>vi</code> is under the normal distribution, curious why the normal distribution? I was wondering if we can try <code>inner_prod += torch.sum(grad)</code></li>\n</ol>\n<p>Thank you in advance.</p>", "body_text": "Hi Xuanqing, @xuanqing94\nI'm also working on second-order derivative. Can you help with two questions here?\n\nIt seems like the hessian output has the same dimension as weight parameters right? Since all of the gradients are accumulated w.r.t. the target tensor.\nWhen calculating the inner product, inner_prod += torch.sum(grad * vi) , the vi is under the normal distribution, curious why the normal distribution? I was wondering if we can try inner_prod += torch.sum(grad)\n\nThank you in advance.", "body": "Hi Xuanqing, @xuanqing94 \r\n\r\nI'm also working on second-order derivative. Can you help with two questions here?\r\n1. It seems like the hessian output has the same dimension as weight parameters right? Since all of the gradients are accumulated w.r.t. the target tensor.\r\n2. When calculating the inner product, `inner_prod += torch.sum(grad * vi)` , the `vi` is under the normal distribution, curious why the normal distribution? I was wondering if we can try `inner_prod += torch.sum(grad)`\r\n\r\nThank you in advance."}