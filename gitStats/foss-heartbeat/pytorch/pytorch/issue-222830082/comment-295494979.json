{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/295494979", "html_url": "https://github.com/pytorch/pytorch/pull/1302#issuecomment-295494979", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1302", "id": 295494979, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTQ5NDk3OQ==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-19T23:39:41Z", "updated_at": "2017-04-19T23:39:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Btw I'm still unsure about whether we should make <code>coalesce</code> an in-place op or not. It's an important decision and has some trade-offs</p>\n<h3>Pro in-place:</h3>\n<ul>\n<li>A coalesced tensor is always faster to operate on than an un-coalesced tensor.</li>\n<li>Suppose you have an uncoalesced sparse tensor <code>S</code> and you repeatedly call coalescing ops on it:</li>\n</ul>\n<pre><code>S = S + S # now uncoalesced\nfor i in range(N):\n  T = S * S # coalesces each time\n</code></pre>\n<p>This is going to be extremely inefficient.</p>\n<h3>Pro out-of-place:</h3>\n<ul>\n<li>const operands should not be mutable; therefore, if a const operand (e.g. input to <code>cadd</code>) can get coalesced in place, it must be invisible to the Python user. That means that <code>t.indexes()</code>, <code>t.values()</code>, and <code>t.__str__</code> should all call coalesce. That hides opportunities for optimizations by munging <code>indices</code> and <code>values</code> directly from Python. Then if there are C bugs, you can get nasty behavior like a Heisenbug where when you call <code>print</code> on the tensor, the bug goes away.</li>\n</ul>\n<h3>Always coalesce</h3>\n<p>There's a third option which is to always keep tensors in coalesced form. This is probably hard to make efficient. But it's possible that if we move <code>nnz</code> to be stored on the device, we can avoid host syncs and make it reasonably fast.</p>", "body_text": "Btw I'm still unsure about whether we should make coalesce an in-place op or not. It's an important decision and has some trade-offs\nPro in-place:\n\nA coalesced tensor is always faster to operate on than an un-coalesced tensor.\nSuppose you have an uncoalesced sparse tensor S and you repeatedly call coalescing ops on it:\n\nS = S + S # now uncoalesced\nfor i in range(N):\n  T = S * S # coalesces each time\n\nThis is going to be extremely inefficient.\nPro out-of-place:\n\nconst operands should not be mutable; therefore, if a const operand (e.g. input to cadd) can get coalesced in place, it must be invisible to the Python user. That means that t.indexes(), t.values(), and t.__str__ should all call coalesce. That hides opportunities for optimizations by munging indices and values directly from Python. Then if there are C bugs, you can get nasty behavior like a Heisenbug where when you call print on the tensor, the bug goes away.\n\nAlways coalesce\nThere's a third option which is to always keep tensors in coalesced form. This is probably hard to make efficient. But it's possible that if we move nnz to be stored on the device, we can avoid host syncs and make it reasonably fast.", "body": "Btw I'm still unsure about whether we should make `coalesce` an in-place op or not. It's an important decision and has some trade-offs\r\n\r\n### Pro in-place:\r\n* A coalesced tensor is always faster to operate on than an un-coalesced tensor.\r\n* Suppose you have an uncoalesced sparse tensor `S` and you repeatedly call coalescing ops on it:\r\n```\r\nS = S + S # now uncoalesced\r\nfor i in range(N):\r\n  T = S * S # coalesces each time\r\n```\r\nThis is going to be extremely inefficient.\r\n\r\n### Pro out-of-place:\r\n* const operands should not be mutable; therefore, if a const operand (e.g. input to `cadd`) can get coalesced in place, it must be invisible to the Python user. That means that `t.indexes()`, `t.values()`, and `t.__str__` should all call coalesce. That hides opportunities for optimizations by munging `indices` and `values` directly from Python. Then if there are C bugs, you can get nasty behavior like a Heisenbug where when you call `print` on the tensor, the bug goes away.\r\n\r\n### Always coalesce\r\nThere's a third option which is to always keep tensors in coalesced form. This is probably hard to make efficient. But it's possible that if we move `nnz` to be stored on the device, we can avoid host syncs and make it reasonably fast."}