{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1302", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1302/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1302/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1302/events", "html_url": "https://github.com/pytorch/pytorch/pull/1302", "id": 222830082, "node_id": "MDExOlB1bGxSZXF1ZXN0MTE2NjEyOTQ3", "number": 1302, "title": "Rename sparse contiguous() to coalesce(); make out of place; speed up THC Embedding 10x", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-04-19T18:33:22Z", "updated_at": "2017-04-28T21:11:06Z", "closed_at": "2017-04-28T21:11:06Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/1302", "html_url": "https://github.com/pytorch/pytorch/pull/1302", "diff_url": "https://github.com/pytorch/pytorch/pull/1302.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/1302.patch"}, "body_html": "<p>Rename sparse tensor <code>contiguous</code> to <code>coalesce</code>, and be more sane about in-place vs out-of-place (i.e. always do it in place).</p>\n<p>One thing I'm wondering: should the python methods <code>tensor.indices()</code>, <code>tensor.values()</code>, and <code>tensor.__repr__()</code> automatically coalesce first? This would have the advantage of hiding the implementation detail of coalescing from the Python user, guaranteeing expected invariants such as <code>t.indices() == t.coalesce_().indices()</code>. On the other hand, it limits what you can do in python (i.e. if you want to do something that uses the indices without coalescing for efficiency).</p>\n<p>Opinions welcome!</p>", "body_text": "Rename sparse tensor contiguous to coalesce, and be more sane about in-place vs out-of-place (i.e. always do it in place).\nOne thing I'm wondering: should the python methods tensor.indices(), tensor.values(), and tensor.__repr__() automatically coalesce first? This would have the advantage of hiding the implementation detail of coalescing from the Python user, guaranteeing expected invariants such as t.indices() == t.coalesce_().indices(). On the other hand, it limits what you can do in python (i.e. if you want to do something that uses the indices without coalescing for efficiency).\nOpinions welcome!", "body": "Rename sparse tensor `contiguous` to `coalesce`, and be more sane about in-place vs out-of-place (i.e. always do it in place).\r\n\r\nOne thing I'm wondering: should the python methods `tensor.indices()`, `tensor.values()`, and `tensor.__repr__()` automatically coalesce first? This would have the advantage of hiding the implementation detail of coalescing from the Python user, guaranteeing expected invariants such as `t.indices() == t.coalesce_().indices()`. On the other hand, it limits what you can do in python (i.e. if you want to do something that uses the indices without coalescing for efficiency).\r\n\r\nOpinions welcome!"}