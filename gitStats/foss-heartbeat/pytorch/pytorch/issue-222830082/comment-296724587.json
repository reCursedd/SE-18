{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/296724587", "html_url": "https://github.com/pytorch/pytorch/pull/1302#issuecomment-296724587", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1302", "id": 296724587, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjcyNDU4Nw==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-24T16:15:46Z", "updated_at": "2017-04-24T16:15:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm not too qualified to review this diff, but some thoughts did occur to me:</p>\n<ol>\n<li>Does it make sense to still offer an in-place coalesce, even though operators won't call this directly? The main use case would be a user is performing a bunch of sparse ops in place, and then wants to finally coalesce the tensor before passing it off.</li>\n<li>It would be great if we had some docs, because there is quite a bit of diversity in how duplicate indexes are handled. SciPy, for example, sums duplicate indexes together (like us, if I understand these diffs correctly), but TensorFlow drops duplicates (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"119303223\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/371\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/371/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/371\">tensorflow/tensorflow#371</a>). I know the API is in flux, but it would still be great to have a paragraph or two stating what the basic assumptions in the implementation are.</li>\n</ol>", "body_text": "I'm not too qualified to review this diff, but some thoughts did occur to me:\n\nDoes it make sense to still offer an in-place coalesce, even though operators won't call this directly? The main use case would be a user is performing a bunch of sparse ops in place, and then wants to finally coalesce the tensor before passing it off.\nIt would be great if we had some docs, because there is quite a bit of diversity in how duplicate indexes are handled. SciPy, for example, sums duplicate indexes together (like us, if I understand these diffs correctly), but TensorFlow drops duplicates (tensorflow/tensorflow#371). I know the API is in flux, but it would still be great to have a paragraph or two stating what the basic assumptions in the implementation are.", "body": "I'm not too qualified to review this diff, but some thoughts did occur to me:\r\n\r\n1. Does it make sense to still offer an in-place coalesce, even though operators won't call this directly? The main use case would be a user is performing a bunch of sparse ops in place, and then wants to finally coalesce the tensor before passing it off.\r\n2. It would be great if we had some docs, because there is quite a bit of diversity in how duplicate indexes are handled. SciPy, for example, sums duplicate indexes together (like us, if I understand these diffs correctly), but TensorFlow drops duplicates (https://github.com/tensorflow/tensorflow/issues/371). I know the API is in flux, but it would still be great to have a paragraph or two stating what the basic assumptions in the implementation are."}