{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/296730597", "html_url": "https://github.com/pytorch/pytorch/pull/1302#issuecomment-296730597", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1302", "id": 296730597, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NjczMDU5Nw==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-24T16:29:29Z", "updated_at": "2017-04-24T16:29:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a></p>\n<ol>\n<li>\n<p>I don't see what value an in-place op provides... the in-place coalesce is no more efficient than out-of place, because a sparse tensor is just a tiny wrapper for {THLongTensor indices, THTensor values}, so all you're creating is a new wrapper. The difference for the in-place one is that if anyone else was holding a reference to the tensor, it would be changed as well. Is there a reason we want that?</p>\n</li>\n<li>\n<p>Yeah, we definitely need docs. I will at least put a comment in <code>coalesce</code> but we do need to write the full docs for this library soon. I won't be able to get to it until at least late-May, so if you'd like to pick this up, it would be a huge help.</p>\n</li>\n</ol>\n<p>Re: duplicates, the current implementation must sum duplicates because we take advantage of this to do some ops efficiently. <code>cadd</code> just \"adds\" two sparse tensors by concatenating their indices and values. This way you can add together a bunch of tensors and then accumulate them into a dense tensor, without ever doing a (slow) <code>coalesce</code>.<br>\nP.S. We may be able to make <code>coalesce</code> much faster and then wouldn't need this, but that would require some substantial and tricky rewriting (esp. holding <code>nnz</code> on the GPU to avoid a host sync after <code>coalesce</code>).</p>", "body_text": "Thanks @ezyang\n\n\nI don't see what value an in-place op provides... the in-place coalesce is no more efficient than out-of place, because a sparse tensor is just a tiny wrapper for {THLongTensor indices, THTensor values}, so all you're creating is a new wrapper. The difference for the in-place one is that if anyone else was holding a reference to the tensor, it would be changed as well. Is there a reason we want that?\n\n\nYeah, we definitely need docs. I will at least put a comment in coalesce but we do need to write the full docs for this library soon. I won't be able to get to it until at least late-May, so if you'd like to pick this up, it would be a huge help.\n\n\nRe: duplicates, the current implementation must sum duplicates because we take advantage of this to do some ops efficiently. cadd just \"adds\" two sparse tensors by concatenating their indices and values. This way you can add together a bunch of tensors and then accumulate them into a dense tensor, without ever doing a (slow) coalesce.\nP.S. We may be able to make coalesce much faster and then wouldn't need this, but that would require some substantial and tricky rewriting (esp. holding nnz on the GPU to avoid a host sync after coalesce).", "body": "Thanks @ezyang\r\n \r\n1. I don't see what value an in-place op provides... the in-place coalesce is no more efficient than out-of place, because a sparse tensor is just a tiny wrapper for {THLongTensor indices, THTensor values}, so all you're creating is a new wrapper. The difference for the in-place one is that if anyone else was holding a reference to the tensor, it would be changed as well. Is there a reason we want that?\r\n\r\n2. Yeah, we definitely need docs. I will at least put a comment in `coalesce` but we do need to write the full docs for this library soon. I won't be able to get to it until at least late-May, so if you'd like to pick this up, it would be a huge help.\r\n\r\nRe: duplicates, the current implementation must sum duplicates because we take advantage of this to do some ops efficiently. `cadd` just \"adds\" two sparse tensors by concatenating their indices and values. This way you can add together a bunch of tensors and then accumulate them into a dense tensor, without ever doing a (slow) `coalesce`. \r\nP.S. We may be able to make `coalesce` much faster and then wouldn't need this, but that would require some substantial and tricky rewriting (esp. holding `nnz` on the GPU to avoid a host sync after `coalesce`)."}