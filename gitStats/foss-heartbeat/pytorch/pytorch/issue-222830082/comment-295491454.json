{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/295491454", "html_url": "https://github.com/pytorch/pytorch/pull/1302#issuecomment-295491454", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1302", "id": 295491454, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTQ5MTQ1NA==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-19T23:28:50Z", "updated_at": "2017-04-19T23:28:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I made some modifications to make sparse accumulation into dense gradients on GPU a lot faster (~10x) by avoiding coalescing and calling <code>indexAdd</code>. I also sped up <code>coalesce</code> by using something closer to what <code>LookupTable</code> does. I still need to go back and unify everything under <code>indexAdd</code>, but that will be trickier.</p>\n<p>Here are my previous notes from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"218344414\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1147\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/1147/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/1147\">#1147</a> :</p>\n<hr>\n<p>I dug into speeding up sparse for nn.Embedding based on my benchmark on GPU (<a href=\"https://gist.github.com/adamlerer/865c1a09000c7dc8208e1456255209c2\">https://gist.github.com/adamlerer/865c1a09000c7dc8208e1456255209c2</a>). I think my findings apply more broadly though.</p>\n<ul>\n<li>Most importantly, the <code>contiguous</code> (or <code>reorder</code>) operation, which merges rows with the same index, is inevitably slow on GPU because it demands a host synchronization. The reason is that the number of unique rows (calculated on GPU) becomes the new nnz, i.e. indices.size[0] / values.size[0] (which is a CPU field). So, it ends up being much faster to compute operations like spcadd (the main op for Embedding update) directly on the non-'contiguous' tensor, using indexAdd or LookupTable_accGradParameters.<br>\nThe potential problem with never compacting sparse tensors is that they can grow to unbounded size if you repeatedly call <code>cadd</code> without ever calling <code>contiguous</code>. Maybe make it the user's responsibility to call <code>contiguous</code> when necessary? This can be problematic when the <code>cadd</code> is buried in a library somewhere e.g. in autograd backward. I don't have a good answer here... We could have heuristics for when to compact a tensor. Something like this would be kinda clever (too clever?):</li>\n</ul>\n<pre><code>void THCSTensor_(cadd)(r, a, b) {\n...\nr-&gt;minUnique = max(a-&gt;minUnique + b-&gt;minUnique);\n...\nif (r-&gt;nnz / r-&gt;minUnique &gt; COMPACTION_THRESHOLD) {\n  THCSTensor_(contiguous)(r);\n  r-&gt;minUnique = r-&gt;nnz;\n}\n</code></pre>\n<ul>\n<li>\n<p>LookupTable_accGradParameters is just computing indexAdd, yet it's implemented completely differently. The different implementation was due to a desire a couple years ago for LookupTable to be deterministic therefore not use atomicAdd. But then why does indexAdd still use atomicAdd? Either we care about determinism or we don't... my guess is that we don't any more since even cudnn isn't deterministic.<br>\nI benchmarked then and the non-deterministic one is several times faster, regardless of how many index collision there are. We should pick if we care about determinism, use that to pick an indexAdd kernel in THC, and then delegate LookupTable, spcadd, etc. all to that.</p>\n</li>\n<li>\n<p>Autograd backwards on nn.Embedding spends about 1ms in Python autograd code, which after speeding things up takes &gt;90% of the time for batch sizes below 1e4 (it was taking ~50% of the time before my kernel changes). So batch=1e3 isn't interesting, we should look at batch=1e4.</p>\n</li>\n</ul>", "body_text": "I made some modifications to make sparse accumulation into dense gradients on GPU a lot faster (~10x) by avoiding coalescing and calling indexAdd. I also sped up coalesce by using something closer to what LookupTable does. I still need to go back and unify everything under indexAdd, but that will be trickier.\nHere are my previous notes from #1147 :\n\nI dug into speeding up sparse for nn.Embedding based on my benchmark on GPU (https://gist.github.com/adamlerer/865c1a09000c7dc8208e1456255209c2). I think my findings apply more broadly though.\n\nMost importantly, the contiguous (or reorder) operation, which merges rows with the same index, is inevitably slow on GPU because it demands a host synchronization. The reason is that the number of unique rows (calculated on GPU) becomes the new nnz, i.e. indices.size[0] / values.size[0] (which is a CPU field). So, it ends up being much faster to compute operations like spcadd (the main op for Embedding update) directly on the non-'contiguous' tensor, using indexAdd or LookupTable_accGradParameters.\nThe potential problem with never compacting sparse tensors is that they can grow to unbounded size if you repeatedly call cadd without ever calling contiguous. Maybe make it the user's responsibility to call contiguous when necessary? This can be problematic when the cadd is buried in a library somewhere e.g. in autograd backward. I don't have a good answer here... We could have heuristics for when to compact a tensor. Something like this would be kinda clever (too clever?):\n\nvoid THCSTensor_(cadd)(r, a, b) {\n...\nr->minUnique = max(a->minUnique + b->minUnique);\n...\nif (r->nnz / r->minUnique > COMPACTION_THRESHOLD) {\n  THCSTensor_(contiguous)(r);\n  r->minUnique = r->nnz;\n}\n\n\n\nLookupTable_accGradParameters is just computing indexAdd, yet it's implemented completely differently. The different implementation was due to a desire a couple years ago for LookupTable to be deterministic therefore not use atomicAdd. But then why does indexAdd still use atomicAdd? Either we care about determinism or we don't... my guess is that we don't any more since even cudnn isn't deterministic.\nI benchmarked then and the non-deterministic one is several times faster, regardless of how many index collision there are. We should pick if we care about determinism, use that to pick an indexAdd kernel in THC, and then delegate LookupTable, spcadd, etc. all to that.\n\n\nAutograd backwards on nn.Embedding spends about 1ms in Python autograd code, which after speeding things up takes >90% of the time for batch sizes below 1e4 (it was taking ~50% of the time before my kernel changes). So batch=1e3 isn't interesting, we should look at batch=1e4.", "body": "I made some modifications to make sparse accumulation into dense gradients on GPU a lot faster (~10x) by avoiding coalescing and calling `indexAdd`. I also sped up `coalesce` by using something closer to what `LookupTable` does. I still need to go back and unify everything under `indexAdd`, but that will be trickier.\r\n\r\nHere are my previous notes from https://github.com/pytorch/pytorch/pull/1147 :\r\n\r\n-----------------------------\r\n\r\nI dug into speeding up sparse for nn.Embedding based on my benchmark on GPU (https://gist.github.com/adamlerer/865c1a09000c7dc8208e1456255209c2). I think my findings apply more broadly though.\r\n\r\n- Most importantly, the `contiguous` (or `reorder`) operation, which merges rows with the same index, is inevitably slow on GPU because it demands a host synchronization. The reason is that the number of unique rows (calculated on GPU) becomes the new nnz, i.e. indices.size[0] / values.size[0] (which is a CPU field). So, it ends up being much faster to compute operations like spcadd (the main op for Embedding update) directly on the non-'contiguous' tensor, using indexAdd or LookupTable_accGradParameters.\r\nThe potential problem with never compacting sparse tensors is that they can grow to unbounded size if you repeatedly call `cadd` without ever calling `contiguous`. Maybe make it the user's responsibility to call `contiguous` when necessary? This can be problematic when the `cadd` is buried in a library somewhere e.g. in autograd backward. I don't have a good answer here... We could have heuristics for when to compact a tensor. Something like this would be kinda clever (too clever?):\r\n\r\n```\r\nvoid THCSTensor_(cadd)(r, a, b) {\r\n...\r\nr->minUnique = max(a->minUnique + b->minUnique);\r\n...\r\nif (r->nnz / r->minUnique > COMPACTION_THRESHOLD) {\r\n  THCSTensor_(contiguous)(r);\r\n  r->minUnique = r->nnz;\r\n}\r\n```\r\n\r\n- LookupTable_accGradParameters is just computing indexAdd, yet it's implemented completely differently. The different implementation was due to a desire a couple years ago for LookupTable to be deterministic therefore not use atomicAdd. But then why does indexAdd still use atomicAdd? Either we care about determinism or we don't... my guess is that we don't any more since even cudnn isn't deterministic.\r\nI benchmarked then and the non-deterministic one is several times faster, regardless of how many index collision there are. We should pick if we care about determinism, use that to pick an indexAdd kernel in THC, and then delegate LookupTable, spcadd, etc. all to that.\r\n\r\n- Autograd backwards on nn.Embedding spends about 1ms in Python autograd code, which after speeding things up takes >90% of the time for batch sizes below 1e4 (it was taking ~50% of the time before my kernel changes). So batch=1e3 isn't interesting, we should look at batch=1e4."}