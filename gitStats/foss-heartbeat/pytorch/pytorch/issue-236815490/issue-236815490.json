{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1842", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1842/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1842/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1842/events", "html_url": "https://github.com/pytorch/pytorch/issues/1842", "id": 236815490, "node_id": "MDU6SXNzdWUyMzY4MTU0OTA=", "number": 1842, "title": "Some functions only accept or return LongTensor variable, is it drawback?", "user": {"login": "acgtyrant", "id": 3921062, "node_id": "MDQ6VXNlcjM5MjEwNjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3921062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/acgtyrant", "html_url": "https://github.com/acgtyrant", "followers_url": "https://api.github.com/users/acgtyrant/followers", "following_url": "https://api.github.com/users/acgtyrant/following{/other_user}", "gists_url": "https://api.github.com/users/acgtyrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/acgtyrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/acgtyrant/subscriptions", "organizations_url": "https://api.github.com/users/acgtyrant/orgs", "repos_url": "https://api.github.com/users/acgtyrant/repos", "events_url": "https://api.github.com/users/acgtyrant/events{/privacy}", "received_events_url": "https://api.github.com/users/acgtyrant/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-19T08:29:25Z", "updated_at": "2017-07-13T07:08:22Z", "closed_at": "2017-07-13T07:08:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I met this error:</p>\n<pre><code>  File \"./train.py\", line 162, in rcnn_build_loss\n    cross_entropy = F.cross_entropy(score, label)\n  File \"/home/acgtyrant/Projects/faster_rcnn_pytorch/.env/lib/python3.5/site-packages/torch/nn/functional.py\", line 533, in cross_entropy\n    return nll_loss(log_softmax(input), target, weight, size_average)\n  File \"/home/acgtyrant/Projects/faster_rcnn_pytorch/.env/lib/python3.5/site-packages/torch/nn/functional.py\", line 501, in nll_loss\n    return f(input, target)\n  File \"/home/acgtyrant/Projects/faster_rcnn_pytorch/.env/lib/python3.5/site-packages/torch/nn/_functions/thnn/auto.py\", line 41, in forward\n    output, *self.additional_args)\nTypeError: CudaClassNLLCriterion_updateOutput received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, torch.cuda.IntTensor, torch.cuda.FloatTensor, bool, NoneType, torch.cuda.FloatTensor),\n but expected (int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor output, bool sizeAverage, [torch.cuda.FloatTensor weights or None], torch.cuda.FloatTensor total_weight)\n</code></pre>\n<p>It note that the <code>cross_entropy</code> function only accept a <code>LongTensor</code> variable. By the way, <code>torch.max(input, dim, keepdim=True, max=None, max_indices=None)</code> returns <code>(Tensor, LongTensor)</code> too, so my <code>prediction</code> is LongTensor which produced in my program. I can not compare it with the <code>labels</code> which is <code>IntTensor type</code>. In other words, <code>labels = IntTensor(1); prediction = score.data.max(1); prediction.eq(labels)</code> reports errors.</p>\n<p>It is not coordinating with the dynamic Python. I do not know if this is a drawback, in other words, is there any plan to improve these functions so dynamic/flexible?</p>", "body_text": "I met this error:\n  File \"./train.py\", line 162, in rcnn_build_loss\n    cross_entropy = F.cross_entropy(score, label)\n  File \"/home/acgtyrant/Projects/faster_rcnn_pytorch/.env/lib/python3.5/site-packages/torch/nn/functional.py\", line 533, in cross_entropy\n    return nll_loss(log_softmax(input), target, weight, size_average)\n  File \"/home/acgtyrant/Projects/faster_rcnn_pytorch/.env/lib/python3.5/site-packages/torch/nn/functional.py\", line 501, in nll_loss\n    return f(input, target)\n  File \"/home/acgtyrant/Projects/faster_rcnn_pytorch/.env/lib/python3.5/site-packages/torch/nn/_functions/thnn/auto.py\", line 41, in forward\n    output, *self.additional_args)\nTypeError: CudaClassNLLCriterion_updateOutput received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, torch.cuda.IntTensor, torch.cuda.FloatTensor, bool, NoneType, torch.cuda.FloatTensor),\n but expected (int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor output, bool sizeAverage, [torch.cuda.FloatTensor weights or None], torch.cuda.FloatTensor total_weight)\n\nIt note that the cross_entropy function only accept a LongTensor variable. By the way, torch.max(input, dim, keepdim=True, max=None, max_indices=None) returns (Tensor, LongTensor) too, so my prediction is LongTensor which produced in my program. I can not compare it with the labels which is IntTensor type. In other words, labels = IntTensor(1); prediction = score.data.max(1); prediction.eq(labels) reports errors.\nIt is not coordinating with the dynamic Python. I do not know if this is a drawback, in other words, is there any plan to improve these functions so dynamic/flexible?", "body": "I met this error:\r\n\r\n      File \"./train.py\", line 162, in rcnn_build_loss\r\n        cross_entropy = F.cross_entropy(score, label)\r\n      File \"/home/acgtyrant/Projects/faster_rcnn_pytorch/.env/lib/python3.5/site-packages/torch/nn/functional.py\", line 533, in cross_entropy\r\n        return nll_loss(log_softmax(input), target, weight, size_average)\r\n      File \"/home/acgtyrant/Projects/faster_rcnn_pytorch/.env/lib/python3.5/site-packages/torch/nn/functional.py\", line 501, in nll_loss\r\n        return f(input, target)\r\n      File \"/home/acgtyrant/Projects/faster_rcnn_pytorch/.env/lib/python3.5/site-packages/torch/nn/_functions/thnn/auto.py\", line 41, in forward\r\n        output, *self.additional_args)\r\n    TypeError: CudaClassNLLCriterion_updateOutput received an invalid combination of arguments - got (int, torch.cuda.FloatTensor, torch.cuda.IntTensor, torch.cuda.FloatTensor, bool, NoneType, torch.cuda.FloatTensor),\r\n     but expected (int state, torch.cuda.FloatTensor input, torch.cuda.LongTensor target, torch.cuda.FloatTensor output, bool sizeAverage, [torch.cuda.FloatTensor weights or None], torch.cuda.FloatTensor total_weight)\r\n\r\nIt note that the `cross_entropy` function only accept a `LongTensor` variable. By the way, `torch.max(input, dim, keepdim=True, max=None, max_indices=None)` returns `(Tensor, LongTensor)` too, so my `prediction` is LongTensor which produced in my program. I can not compare it with the `labels` which is `IntTensor type`. In other words, `labels = IntTensor(1); prediction = score.data.max(1); prediction.eq(labels)` reports errors.\r\n\r\nIt is not coordinating with the dynamic Python. I do not know if this is a drawback, in other words, is there any plan to improve these functions so dynamic/flexible?"}