{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2474", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2474/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2474/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2474/events", "html_url": "https://github.com/pytorch/pytorch/issues/2474", "id": 250953181, "node_id": "MDU6SXNzdWUyNTA5NTMxODE=", "number": 2474, "title": "Timeout option for parallel DataLoader", "user": {"login": "vadimkantorov", "id": 1041752, "node_id": "MDQ6VXNlcjEwNDE3NTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1041752?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vadimkantorov", "html_url": "https://github.com/vadimkantorov", "followers_url": "https://api.github.com/users/vadimkantorov/followers", "following_url": "https://api.github.com/users/vadimkantorov/following{/other_user}", "gists_url": "https://api.github.com/users/vadimkantorov/gists{/gist_id}", "starred_url": "https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vadimkantorov/subscriptions", "organizations_url": "https://api.github.com/users/vadimkantorov/orgs", "repos_url": "https://api.github.com/users/vadimkantorov/repos", "events_url": "https://api.github.com/users/vadimkantorov/events{/privacy}", "received_events_url": "https://api.github.com/users/vadimkantorov/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-08-17T13:58:06Z", "updated_at": "2018-10-09T22:42:00Z", "closed_at": "2018-10-09T17:08:29Z", "author_association": "NONE", "body_html": "<p>At least with <code>pin_memory = False</code>, torch uses Python's <code>SimpleQueue</code> to implement producer-consumer pattern to sample example indices and then load/preprocess them on worker threads. A problem with <code>SimpleQueue</code> is that they use native OS pipes which have small buffer sizes (64Kb on Linux), lock when this buffer size is exceeded and don't provide a way to fail on lock timeout.</p>\n<p>When for some reason worker threads are locked or die (e.g. due to failed unpickling) this leads to a permanent hang in <code>_put_indices { indices_queue.put(...) }</code> in <code>DataLoaderIter</code> (for me it happens even in constructor during batch pre-fetching).</p>\n<p>If we could have a way to specify a timeout for <code>SimpleQueue.put</code>, this may eliminate a class of uncertain hangs and at least turn them into exceptions. The <code>put</code> calls are in <code>_put_indices</code> and in <code>_worker_loop</code>.</p>\n<p>A particularly nasty deadlock may theoretically happen if all worker threads are blocked because <code>data_queue</code> exceeded its buffer size, and the main thread instead of removing the results from <code>data_queue</code>, fills up <code>indices_queue</code> and gets blocked because <code>indices_queue</code> is filled up. Given large enough batches (say with large lists in them transferred by value), this might happen during initial batch pre-fetching / priming.</p>", "body_text": "At least with pin_memory = False, torch uses Python's SimpleQueue to implement producer-consumer pattern to sample example indices and then load/preprocess them on worker threads. A problem with SimpleQueue is that they use native OS pipes which have small buffer sizes (64Kb on Linux), lock when this buffer size is exceeded and don't provide a way to fail on lock timeout.\nWhen for some reason worker threads are locked or die (e.g. due to failed unpickling) this leads to a permanent hang in _put_indices { indices_queue.put(...) } in DataLoaderIter (for me it happens even in constructor during batch pre-fetching).\nIf we could have a way to specify a timeout for SimpleQueue.put, this may eliminate a class of uncertain hangs and at least turn them into exceptions. The put calls are in _put_indices and in _worker_loop.\nA particularly nasty deadlock may theoretically happen if all worker threads are blocked because data_queue exceeded its buffer size, and the main thread instead of removing the results from data_queue, fills up indices_queue and gets blocked because indices_queue is filled up. Given large enough batches (say with large lists in them transferred by value), this might happen during initial batch pre-fetching / priming.", "body": "At least with `pin_memory = False`, torch uses Python's `SimpleQueue` to implement producer-consumer pattern to sample example indices and then load/preprocess them on worker threads. A problem with `SimpleQueue` is that they use native OS pipes which have small buffer sizes (64Kb on Linux), lock when this buffer size is exceeded and don't provide a way to fail on lock timeout.\r\n\r\nWhen for some reason worker threads are locked or die (e.g. due to failed unpickling) this leads to a permanent hang in `_put_indices { indices_queue.put(...) }` in `DataLoaderIter` (for me it happens even in constructor during batch pre-fetching).\r\n\r\nIf we could have a way to specify a timeout for `SimpleQueue.put`, this may eliminate a class of uncertain hangs and at least turn them into exceptions. The `put` calls are in `_put_indices` and in `_worker_loop`.\r\n\r\nA particularly nasty deadlock may theoretically happen if all worker threads are blocked because `data_queue` exceeded its buffer size, and the main thread instead of removing the results from `data_queue`, fills up `indices_queue` and gets blocked because `indices_queue` is filled up. Given large enough batches (say with large lists in them transferred by value), this might happen during initial batch pre-fetching / priming."}