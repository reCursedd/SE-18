{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/346561805", "html_url": "https://github.com/pytorch/pytorch/issues/3790#issuecomment-346561805", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3790", "id": 346561805, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NjU2MTgwNQ==", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-23T09:03:06Z", "updated_at": "2017-11-23T09:03:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6382130\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/anandsaha\">@anandsaha</a> if you could have a look at my PR (linked above) and check that it is fine then that would be useful. In particular I've also written tests for it, so once the PyTorch devs are free to check it then it can hopefully be merged (but there's lots of higher priority work going on with the internals of PyTorch atm).</p>\n<p>Since we have a learning rate scheduler module, it would be good to be modular and use that for (warm) restarts. I haven't checked in detail but I imagine that a <code>torch.optim.lr_scheduler.WarmRestartLR</code> could take another scheduler and a restart schedule as arguments, and then it could be applied to any optimiser, and potentially and learning rate decay scheme (the docs can have the info and example that the usage of cosine annealing is what would be needed to replicate that paper).</p>", "body_text": "@anandsaha if you could have a look at my PR (linked above) and check that it is fine then that would be useful. In particular I've also written tests for it, so once the PyTorch devs are free to check it then it can hopefully be merged (but there's lots of higher priority work going on with the internals of PyTorch atm).\nSince we have a learning rate scheduler module, it would be good to be modular and use that for (warm) restarts. I haven't checked in detail but I imagine that a torch.optim.lr_scheduler.WarmRestartLR could take another scheduler and a restart schedule as arguments, and then it could be applied to any optimiser, and potentially and learning rate decay scheme (the docs can have the info and example that the usage of cosine annealing is what would be needed to replicate that paper).", "body": "@anandsaha if you could have a look at my PR (linked above) and check that it is fine then that would be useful. In particular I've also written tests for it, so once the PyTorch devs are free to check it then it can hopefully be merged (but there's lots of higher priority work going on with the internals of PyTorch atm).\r\n\r\nSince we have a learning rate scheduler module, it would be good to be modular and use that for (warm) restarts. I haven't checked in detail but I imagine that a `torch.optim.lr_scheduler.WarmRestartLR` could take another scheduler and a restart schedule as arguments, and then it could be applied to any optimiser, and potentially and learning rate decay scheme (the docs can have the info and example that the usage of cosine annealing is what would be needed to replicate that paper)."}