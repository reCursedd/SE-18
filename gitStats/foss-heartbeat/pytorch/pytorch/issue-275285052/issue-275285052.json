{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3790", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3790/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3790/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3790/events", "html_url": "https://github.com/pytorch/pytorch/issues/3790", "id": 275285052, "node_id": "MDU6SXNzdWUyNzUyODUwNTI=", "number": 3790, "title": "Add SGDR, SGDW, AdamW and AdamWR", "user": {"login": "onlytailei", "id": 7247925, "node_id": "MDQ6VXNlcjcyNDc5MjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/7247925?v=4", "gravatar_id": "", "url": "https://api.github.com/users/onlytailei", "html_url": "https://github.com/onlytailei", "followers_url": "https://api.github.com/users/onlytailei/followers", "following_url": "https://api.github.com/users/onlytailei/following{/other_user}", "gists_url": "https://api.github.com/users/onlytailei/gists{/gist_id}", "starred_url": "https://api.github.com/users/onlytailei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/onlytailei/subscriptions", "organizations_url": "https://api.github.com/users/onlytailei/orgs", "repos_url": "https://api.github.com/users/onlytailei/repos", "events_url": "https://api.github.com/users/onlytailei/events{/privacy}", "received_events_url": "https://api.github.com/users/onlytailei/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-11-20T09:20:23Z", "updated_at": "2018-08-14T21:41:58Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Recently, there are two papers from <a href=\"http://ml.informatik.uni-freiburg.de/people/loshchilov/index.html\" rel=\"nofollow\">Ilya Loshchilov</a> and <a href=\"http://www2.informatik.uni-freiburg.de/~hutter/\" rel=\"nofollow\">Frank Hutter</a>.<br>\n<a href=\"https://arxiv.org/abs/1608.03983\" rel=\"nofollow\">SGDR: Stochastic Gradient Descent with Warm Restarts</a>, introduces a learning rate decay method according to different training periods. Several hyperparameters improve the-state-of-the art result a lot. It has been added to tensorflow as <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/train/cosine_decay\" rel=\"nofollow\">tf.train.cosine_decay</a><br>\nTheir recent work <a href=\"https://arxiv.org/abs/1711.05101\" rel=\"nofollow\">Fixing Weight Decay Regularization in Adam</a> fixes a wide misunderstanding when using Adam and weight decay synchronously. It has been highly admitted by the author of <a href=\"https://twitter.com/dpkingma/status/930849593767608320\" rel=\"nofollow\">Adam</a>.</p>\n<p>I think pytorch should add these features as well.</p>", "body_text": "Recently, there are two papers from Ilya Loshchilov and Frank Hutter.\nSGDR: Stochastic Gradient Descent with Warm Restarts, introduces a learning rate decay method according to different training periods. Several hyperparameters improve the-state-of-the art result a lot. It has been added to tensorflow as tf.train.cosine_decay\nTheir recent work Fixing Weight Decay Regularization in Adam fixes a wide misunderstanding when using Adam and weight decay synchronously. It has been highly admitted by the author of Adam.\nI think pytorch should add these features as well.", "body": "Recently, there are two papers from [Ilya Loshchilov](http://ml.informatik.uni-freiburg.de/people/loshchilov/index.html) and [Frank Hutter](http://www2.informatik.uni-freiburg.de/~hutter/).\r\n[SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983), introduces a learning rate decay method according to different training periods. Several hyperparameters improve the-state-of-the art result a lot. It has been added to tensorflow as [tf.train.cosine_decay](https://www.tensorflow.org/versions/master/api_docs/python/tf/train/cosine_decay)\r\nTheir recent work [Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101) fixes a wide misunderstanding when using Adam and weight decay synchronously. It has been highly admitted by the author of [Adam](https://twitter.com/dpkingma/status/930849593767608320).\r\n \r\nI think pytorch should add these features as well."}