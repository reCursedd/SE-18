{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383584026", "html_url": "https://github.com/pytorch/pytorch/issues/6857#issuecomment-383584026", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6857", "id": 383584026, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzU4NDAyNg==", "user": {"login": "jimmyoic", "id": 7804036, "node_id": "MDQ6VXNlcjc4MDQwMzY=", "avatar_url": "https://avatars0.githubusercontent.com/u/7804036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jimmyoic", "html_url": "https://github.com/jimmyoic", "followers_url": "https://api.github.com/users/jimmyoic/followers", "following_url": "https://api.github.com/users/jimmyoic/following{/other_user}", "gists_url": "https://api.github.com/users/jimmyoic/gists{/gist_id}", "starred_url": "https://api.github.com/users/jimmyoic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jimmyoic/subscriptions", "organizations_url": "https://api.github.com/users/jimmyoic/orgs", "repos_url": "https://api.github.com/users/jimmyoic/repos", "events_url": "https://api.github.com/users/jimmyoic/events{/privacy}", "received_events_url": "https://api.github.com/users/jimmyoic/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-23T13:55:47Z", "updated_at": "2018-04-23T13:55:47Z", "author_association": "NONE", "body_html": "<ol>\n<li>You can load the image to your data structure such as array of float, and copy it into the input of your training net</li>\n<li>like 1.</li>\n<li>&amp; 4. Back-propagation works as how you construct your network, which may consist of operators used to calculate the gradient and update the weight. Given the loss function (e.g. softmaxWithLossLayer), there are APIs to construct a training net automatically in Python. If you want to know the detail you can check data_parallel_model.py.</li>\n</ol>", "body_text": "You can load the image to your data structure such as array of float, and copy it into the input of your training net\nlike 1.\n& 4. Back-propagation works as how you construct your network, which may consist of operators used to calculate the gradient and update the weight. Given the loss function (e.g. softmaxWithLossLayer), there are APIs to construct a training net automatically in Python. If you want to know the detail you can check data_parallel_model.py.", "body": "1. You can load the image to your data structure such as array of float, and copy it into the input of your training net\r\n2. like 1.\r\n3. & 4. Back-propagation works as how you construct your network, which may consist of operators used to calculate the gradient and update the weight. Given the loss function (e.g. softmaxWithLossLayer), there are APIs to construct a training net automatically in Python. If you want to know the detail you can check data_parallel_model.py.\r\n\r\n"}