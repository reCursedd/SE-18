{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/356959098", "html_url": "https://github.com/pytorch/pytorch/issues/4606#issuecomment-356959098", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4606", "id": 356959098, "node_id": "MDEyOklzc3VlQ29tbWVudDM1Njk1OTA5OA==", "user": {"login": "Zrachel", "id": 4532062, "node_id": "MDQ6VXNlcjQ1MzIwNjI=", "avatar_url": "https://avatars0.githubusercontent.com/u/4532062?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zrachel", "html_url": "https://github.com/Zrachel", "followers_url": "https://api.github.com/users/Zrachel/followers", "following_url": "https://api.github.com/users/Zrachel/following{/other_user}", "gists_url": "https://api.github.com/users/Zrachel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zrachel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zrachel/subscriptions", "organizations_url": "https://api.github.com/users/Zrachel/orgs", "repos_url": "https://api.github.com/users/Zrachel/repos", "events_url": "https://api.github.com/users/Zrachel/events{/privacy}", "received_events_url": "https://api.github.com/users/Zrachel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-11T14:59:01Z", "updated_at": "2018-01-11T14:59:01Z", "author_association": "NONE", "body_html": "<p>Thank you for your quick reply. However, without specify distributed flag, I get:</p>\n<pre><code>In [1]: from torch.multiprocessing import Process\n\nIn [2]: import torch.distributed as dist\n\nIn [3]: import torch\n\nIn [4]: dist.init_process_group(\"tcp\", rank=0, world_size=2)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-4-1b6d1d87a098&gt; in &lt;module&gt;()\n----&gt; 1 dist.init_process_group(\"tcp\", rank=0, world_size=2)\n\n/home/zhangruiqing01/tools/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py in init_process_group(backend, init_method, **kwargs)\n     38 \n     39     if not is_available():\n---&gt; 40         raise RuntimeError(\"PyTorch built without distributed support\")\n     41 \n     42     global _initialized\n\nRuntimeError: PyTorch built without distributed support\n</code></pre>", "body_text": "Thank you for your quick reply. However, without specify distributed flag, I get:\nIn [1]: from torch.multiprocessing import Process\n\nIn [2]: import torch.distributed as dist\n\nIn [3]: import torch\n\nIn [4]: dist.init_process_group(\"tcp\", rank=0, world_size=2)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-4-1b6d1d87a098> in <module>()\n----> 1 dist.init_process_group(\"tcp\", rank=0, world_size=2)\n\n/home/zhangruiqing01/tools/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py in init_process_group(backend, init_method, **kwargs)\n     38 \n     39     if not is_available():\n---> 40         raise RuntimeError(\"PyTorch built without distributed support\")\n     41 \n     42     global _initialized\n\nRuntimeError: PyTorch built without distributed support", "body": "Thank you for your quick reply. However, without specify distributed flag, I get:\r\n```\r\nIn [1]: from torch.multiprocessing import Process\r\n\r\nIn [2]: import torch.distributed as dist\r\n\r\nIn [3]: import torch\r\n\r\nIn [4]: dist.init_process_group(\"tcp\", rank=0, world_size=2)\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-4-1b6d1d87a098> in <module>()\r\n----> 1 dist.init_process_group(\"tcp\", rank=0, world_size=2)\r\n\r\n/home/zhangruiqing01/tools/anaconda3/lib/python3.6/site-packages/torch/distributed/__init__.py in init_process_group(backend, init_method, **kwargs)\r\n     38 \r\n     39     if not is_available():\r\n---> 40         raise RuntimeError(\"PyTorch built without distributed support\")\r\n     41 \r\n     42     global _initialized\r\n\r\nRuntimeError: PyTorch built without distributed support\r\n```"}