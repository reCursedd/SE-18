{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/304205418", "html_url": "https://github.com/pytorch/pytorch/issues/1620#issuecomment-304205418", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1620", "id": 304205418, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDIwNTQxOA==", "user": {"login": "rishida0141", "id": 28887149, "node_id": "MDQ6VXNlcjI4ODg3MTQ5", "avatar_url": "https://avatars0.githubusercontent.com/u/28887149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rishida0141", "html_url": "https://github.com/rishida0141", "followers_url": "https://api.github.com/users/rishida0141/followers", "following_url": "https://api.github.com/users/rishida0141/following{/other_user}", "gists_url": "https://api.github.com/users/rishida0141/gists{/gist_id}", "starred_url": "https://api.github.com/users/rishida0141/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rishida0141/subscriptions", "organizations_url": "https://api.github.com/users/rishida0141/orgs", "repos_url": "https://api.github.com/users/rishida0141/repos", "events_url": "https://api.github.com/users/rishida0141/events{/privacy}", "received_events_url": "https://api.github.com/users/rishida0141/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-26T06:44:11Z", "updated_at": "2017-05-26T06:44:11Z", "author_association": "NONE", "body_html": "<p>Thank you for your comment.</p>\n<p>I understood the reason why this problem happen.<br>\nThe \"nan\" of the gradient is caused by the backward function of torch.log.</p>\n<p>The gradient is dividied by the input of torch.log in the backward function.<br>\nIf one of the input is quite small, the gradient explode.<br>\n(When the number of the class is large, this phenomenon sometimes happen.)</p>\n<p>I use torch.masked_select to avoid this problem.</p>\n<p>If torch.masked_select is used before torch.log,<br>\nthe gradient is divided by the specific value of the input of torch.log.<br>\nIf the specific value is large enough ( &gt; 10 ** (-30)), the gradient never explode.</p>\n<p>This is a sample code</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch import autograd\nfrom torch import optim\nimport numpy\n\nsm = nn.Softmax()\n\ncrit = nn.NLLLoss()\n\ndata = torch.FloatTensor([[0, 50, -50], [100, 0, -100]])\ndata = torch.autograd.Variable(data, requires_grad=True, volatile=False)\n\nnum_class = 5\n\ntarget = torch.LongTensor([3, 1])\ntarget = torch.autograd.Variable(target)\n\nmask = [[1 if k == j else 0 for k in range(num_class)] for j in target.data]\n\nnum_loop = 1000\n\nfor i in xrange(num_loop):\n\n    # This model does not have enough layer to learn correctly.\n    model = nn.Linear(3, num_class)\n\n    #model = nn.Sequential(\n    #    nn.Linear(3, 2 *num_class),\n    #    nn.Tanh(),\n    #    nn.Linear(2 * num_class, num_class)\n    #)\n\n    optimizer = optim.SGD(model.parameters(), lr = 0.01)\n    optimizer.zero_grad()\n    \n    output = model(data)\n    output = output - float(torch.max(output).data.cpu().numpy())\n    p = sm(output)\n\n    tmp_mask = autograd.Variable(torch.ByteTensor(mask))\n    p_masked = torch.masked_select(p, tmp_mask)\n    p_masked += (p_masked &lt; 10 ** (-30)).float() * (10 ** (-30))\n    ll = torch.log(p_masked).unsqueeze(1)\n    \n    loss_sm = crit(ll, autograd.Variable(torch.LongTensor([0, 0])))\n    loss_sm.backward()\n        \n    optimizer.step()\n    tmp_grad = data.grad.data.cpu().numpy()\n    if numpy.isnan(tmp_grad).any():\n        print \"gradients contain 'nan'\"\n        print \"gradient\\n\", tmp_grad\n        break\n        \nif i == (num_loop - 1):\n    print \"All gradients were calculated correctly\"\n\nprint \"parameter\\n\", model._parameters\nprint \"prob\\n\", p\n\n</code></pre>", "body_text": "Thank you for your comment.\nI understood the reason why this problem happen.\nThe \"nan\" of the gradient is caused by the backward function of torch.log.\nThe gradient is dividied by the input of torch.log in the backward function.\nIf one of the input is quite small, the gradient explode.\n(When the number of the class is large, this phenomenon sometimes happen.)\nI use torch.masked_select to avoid this problem.\nIf torch.masked_select is used before torch.log,\nthe gradient is divided by the specific value of the input of torch.log.\nIf the specific value is large enough ( > 10 ** (-30)), the gradient never explode.\nThis is a sample code\nimport torch\nfrom torch import nn\nfrom torch import autograd\nfrom torch import optim\nimport numpy\n\nsm = nn.Softmax()\n\ncrit = nn.NLLLoss()\n\ndata = torch.FloatTensor([[0, 50, -50], [100, 0, -100]])\ndata = torch.autograd.Variable(data, requires_grad=True, volatile=False)\n\nnum_class = 5\n\ntarget = torch.LongTensor([3, 1])\ntarget = torch.autograd.Variable(target)\n\nmask = [[1 if k == j else 0 for k in range(num_class)] for j in target.data]\n\nnum_loop = 1000\n\nfor i in xrange(num_loop):\n\n    # This model does not have enough layer to learn correctly.\n    model = nn.Linear(3, num_class)\n\n    #model = nn.Sequential(\n    #    nn.Linear(3, 2 *num_class),\n    #    nn.Tanh(),\n    #    nn.Linear(2 * num_class, num_class)\n    #)\n\n    optimizer = optim.SGD(model.parameters(), lr = 0.01)\n    optimizer.zero_grad()\n    \n    output = model(data)\n    output = output - float(torch.max(output).data.cpu().numpy())\n    p = sm(output)\n\n    tmp_mask = autograd.Variable(torch.ByteTensor(mask))\n    p_masked = torch.masked_select(p, tmp_mask)\n    p_masked += (p_masked < 10 ** (-30)).float() * (10 ** (-30))\n    ll = torch.log(p_masked).unsqueeze(1)\n    \n    loss_sm = crit(ll, autograd.Variable(torch.LongTensor([0, 0])))\n    loss_sm.backward()\n        \n    optimizer.step()\n    tmp_grad = data.grad.data.cpu().numpy()\n    if numpy.isnan(tmp_grad).any():\n        print \"gradients contain 'nan'\"\n        print \"gradient\\n\", tmp_grad\n        break\n        \nif i == (num_loop - 1):\n    print \"All gradients were calculated correctly\"\n\nprint \"parameter\\n\", model._parameters\nprint \"prob\\n\", p", "body": "Thank you for your comment.\r\n\r\nI understood the reason why this problem happen.\r\nThe \"nan\" of the gradient is caused by the backward function of torch.log.\r\n\r\nThe gradient is dividied by the input of torch.log in the backward function.\r\nIf one of the input is quite small, the gradient explode.\r\n(When the number of the class is large, this phenomenon sometimes happen.)\r\n\r\nI use torch.masked_select to avoid this problem.\r\n\r\nIf torch.masked_select is used before torch.log, \r\nthe gradient is divided by the specific value of the input of torch.log.\r\nIf the specific value is large enough ( > 10 ** (-30)), the gradient never explode.\r\n\r\nThis is a sample code\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom torch import autograd\r\nfrom torch import optim\r\nimport numpy\r\n\r\nsm = nn.Softmax()\r\n\r\ncrit = nn.NLLLoss()\r\n\r\ndata = torch.FloatTensor([[0, 50, -50], [100, 0, -100]])\r\ndata = torch.autograd.Variable(data, requires_grad=True, volatile=False)\r\n\r\nnum_class = 5\r\n\r\ntarget = torch.LongTensor([3, 1])\r\ntarget = torch.autograd.Variable(target)\r\n\r\nmask = [[1 if k == j else 0 for k in range(num_class)] for j in target.data]\r\n\r\nnum_loop = 1000\r\n\r\nfor i in xrange(num_loop):\r\n\r\n    # This model does not have enough layer to learn correctly.\r\n    model = nn.Linear(3, num_class)\r\n\r\n    #model = nn.Sequential(\r\n    #    nn.Linear(3, 2 *num_class),\r\n    #    nn.Tanh(),\r\n    #    nn.Linear(2 * num_class, num_class)\r\n    #)\r\n\r\n    optimizer = optim.SGD(model.parameters(), lr = 0.01)\r\n    optimizer.zero_grad()\r\n    \r\n    output = model(data)\r\n    output = output - float(torch.max(output).data.cpu().numpy())\r\n    p = sm(output)\r\n\r\n    tmp_mask = autograd.Variable(torch.ByteTensor(mask))\r\n    p_masked = torch.masked_select(p, tmp_mask)\r\n    p_masked += (p_masked < 10 ** (-30)).float() * (10 ** (-30))\r\n    ll = torch.log(p_masked).unsqueeze(1)\r\n    \r\n    loss_sm = crit(ll, autograd.Variable(torch.LongTensor([0, 0])))\r\n    loss_sm.backward()\r\n        \r\n    optimizer.step()\r\n    tmp_grad = data.grad.data.cpu().numpy()\r\n    if numpy.isnan(tmp_grad).any():\r\n        print \"gradients contain 'nan'\"\r\n        print \"gradient\\n\", tmp_grad\r\n        break\r\n        \r\nif i == (num_loop - 1):\r\n    print \"All gradients were calculated correctly\"\r\n\r\nprint \"parameter\\n\", model._parameters\r\nprint \"prob\\n\", p\r\n\r\n```\r\n\r\n"}