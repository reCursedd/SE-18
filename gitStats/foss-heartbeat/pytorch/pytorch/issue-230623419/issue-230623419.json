{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1620", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1620/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1620/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1620/events", "html_url": "https://github.com/pytorch/pytorch/issues/1620", "id": 230623419, "node_id": "MDU6SXNzdWUyMzA2MjM0MTk=", "number": 1620, "title": "[Feature Request] nn.Log (or improvement of torch.log)", "user": {"login": "rishida0141", "id": 28887149, "node_id": "MDQ6VXNlcjI4ODg3MTQ5", "avatar_url": "https://avatars0.githubusercontent.com/u/28887149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rishida0141", "html_url": "https://github.com/rishida0141", "followers_url": "https://api.github.com/users/rishida0141/followers", "following_url": "https://api.github.com/users/rishida0141/following{/other_user}", "gists_url": "https://api.github.com/users/rishida0141/gists{/gist_id}", "starred_url": "https://api.github.com/users/rishida0141/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rishida0141/subscriptions", "organizations_url": "https://api.github.com/users/rishida0141/orgs", "repos_url": "https://api.github.com/users/rishida0141/repos", "events_url": "https://api.github.com/users/rishida0141/events{/privacy}", "received_events_url": "https://api.github.com/users/rishida0141/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-23T08:13:26Z", "updated_at": "2017-05-29T02:04:30Z", "closed_at": "2017-05-26T06:44:11Z", "author_association": "NONE", "body_html": "<p>I want to add the output of attention layer to the output of decoding layer like Equation(9) of this paper(<a href=\"https://arxiv.org/abs/1704.04368\" rel=\"nofollow\">https://arxiv.org/abs/1704.04368</a>).</p>\n<p>This model is called as Pointer-Networks.<br>\nThe loss function of this model is '- torch.log(p * nn.Softmax(x1) + (1 - p) * nn.Softmax(x2))'.<br>\n(nn.LogSoftmax class is not available for this calculation.)</p>\n<p>However, there is a difference between the behavior of nn.LogSoftmax(x) and that of torch.log(nn.Softmax(x)).<br>\nBecause of that, the gradient sometimes get \"nan\".</p>\n<p>Someone please implement nn.Log class to avoid this problem.</p>\n<p>This is a sample code to confirm 'nan' gradient.</p>\n<pre><code>import torch\nfrom torch import nn\nfrom torch import autograd\nfrom torch import optim\nimport numpy\n\nsm = nn.Softmax()\nlsm = nn.LogSoftmax()\n\ncrit = nn.NLLLoss()\ntarget = data = torch.LongTensor([1])\ntarget = torch.autograd.Variable(target)\n\ndata = torch.FloatTensor([[0, 50, -50]])\ndata = torch.autograd.Variable(data, requires_grad=True, volatile=False)\n\nnum_loop = 1000\n\nfor i in xrange(num_loop):\n    model = nn.Linear(3, 3, bias=False)\n    optimizer = optim.SGD(model.parameters(), lr = 0.01)\n    optimizer.zero_grad()\n\n    loss_sm = crit(torch.log(sm(model(data))), target)\n    loss_lsm = crit(lsm(model(data)), target)\n\n    # When I use nn.LogSoftmax() class, all gradients are calculated correctly.\n    #loss_lsm.backward()\n    loss_sm.backward()\n\n    optimizer.step()\n    tmp_grad = data.grad.data.cpu().numpy()\n    if numpy.isnan(tmp_grad).any():\n        print \"gradient\\n\", tmp_grad\n        print \"parameter\\n\", model._parameters\n        print \"gradients contain 'nan'\"\n        break\nif i == (num_loop - 1):\n    print \"All gradients were calculated correctly\"\n    print \"parameter\\n\", model._parameters\n</code></pre>\n<p>The output of above code</p>\n<pre><code>gradient\n[[ nan  nan  nan]]\nparameter\nOrderedDict([('weight', Parameter containing:\nnan -inf inf\nnan nan nan\nnan -inf inf\n[torch.FloatTensor of size 3x3]\n), ('bias', None)])\ngradients contain 'nan'\n</code></pre>", "body_text": "I want to add the output of attention layer to the output of decoding layer like Equation(9) of this paper(https://arxiv.org/abs/1704.04368).\nThis model is called as Pointer-Networks.\nThe loss function of this model is '- torch.log(p * nn.Softmax(x1) + (1 - p) * nn.Softmax(x2))'.\n(nn.LogSoftmax class is not available for this calculation.)\nHowever, there is a difference between the behavior of nn.LogSoftmax(x) and that of torch.log(nn.Softmax(x)).\nBecause of that, the gradient sometimes get \"nan\".\nSomeone please implement nn.Log class to avoid this problem.\nThis is a sample code to confirm 'nan' gradient.\nimport torch\nfrom torch import nn\nfrom torch import autograd\nfrom torch import optim\nimport numpy\n\nsm = nn.Softmax()\nlsm = nn.LogSoftmax()\n\ncrit = nn.NLLLoss()\ntarget = data = torch.LongTensor([1])\ntarget = torch.autograd.Variable(target)\n\ndata = torch.FloatTensor([[0, 50, -50]])\ndata = torch.autograd.Variable(data, requires_grad=True, volatile=False)\n\nnum_loop = 1000\n\nfor i in xrange(num_loop):\n    model = nn.Linear(3, 3, bias=False)\n    optimizer = optim.SGD(model.parameters(), lr = 0.01)\n    optimizer.zero_grad()\n\n    loss_sm = crit(torch.log(sm(model(data))), target)\n    loss_lsm = crit(lsm(model(data)), target)\n\n    # When I use nn.LogSoftmax() class, all gradients are calculated correctly.\n    #loss_lsm.backward()\n    loss_sm.backward()\n\n    optimizer.step()\n    tmp_grad = data.grad.data.cpu().numpy()\n    if numpy.isnan(tmp_grad).any():\n        print \"gradient\\n\", tmp_grad\n        print \"parameter\\n\", model._parameters\n        print \"gradients contain 'nan'\"\n        break\nif i == (num_loop - 1):\n    print \"All gradients were calculated correctly\"\n    print \"parameter\\n\", model._parameters\n\nThe output of above code\ngradient\n[[ nan  nan  nan]]\nparameter\nOrderedDict([('weight', Parameter containing:\nnan -inf inf\nnan nan nan\nnan -inf inf\n[torch.FloatTensor of size 3x3]\n), ('bias', None)])\ngradients contain 'nan'", "body": "I want to add the output of attention layer to the output of decoding layer like Equation(9) of this paper(https://arxiv.org/abs/1704.04368).\r\n\r\nThis model is called as Pointer-Networks.\r\nThe loss function of this model is '- torch.log(p * nn.Softmax(x1) + (1 - p) * nn.Softmax(x2))'.\r\n(nn.LogSoftmax class is not available for this calculation.)\r\n\r\nHowever, there is a difference between the behavior of nn.LogSoftmax(x) and that of torch.log(nn.Softmax(x)).\r\nBecause of that, the gradient sometimes get \"nan\".\r\n\r\nSomeone please implement nn.Log class to avoid this problem.\r\n\r\n\r\nThis is a sample code to confirm 'nan' gradient.\r\n```\r\nimport torch\r\nfrom torch import nn\r\nfrom torch import autograd\r\nfrom torch import optim\r\nimport numpy\r\n\r\nsm = nn.Softmax()\r\nlsm = nn.LogSoftmax()\r\n\r\ncrit = nn.NLLLoss()\r\ntarget = data = torch.LongTensor([1])\r\ntarget = torch.autograd.Variable(target)\r\n\r\ndata = torch.FloatTensor([[0, 50, -50]])\r\ndata = torch.autograd.Variable(data, requires_grad=True, volatile=False)\r\n\r\nnum_loop = 1000\r\n\r\nfor i in xrange(num_loop):\r\n    model = nn.Linear(3, 3, bias=False)\r\n    optimizer = optim.SGD(model.parameters(), lr = 0.01)\r\n    optimizer.zero_grad()\r\n\r\n    loss_sm = crit(torch.log(sm(model(data))), target)\r\n    loss_lsm = crit(lsm(model(data)), target)\r\n\r\n    # When I use nn.LogSoftmax() class, all gradients are calculated correctly.\r\n    #loss_lsm.backward()\r\n    loss_sm.backward()\r\n\r\n    optimizer.step()\r\n    tmp_grad = data.grad.data.cpu().numpy()\r\n    if numpy.isnan(tmp_grad).any():\r\n        print \"gradient\\n\", tmp_grad\r\n        print \"parameter\\n\", model._parameters\r\n        print \"gradients contain 'nan'\"\r\n        break\r\nif i == (num_loop - 1):\r\n    print \"All gradients were calculated correctly\"\r\n    print \"parameter\\n\", model._parameters\r\n```\r\n\r\nThe output of above code\r\n```\r\ngradient\r\n[[ nan  nan  nan]]\r\nparameter\r\nOrderedDict([('weight', Parameter containing:\r\nnan -inf inf\r\nnan nan nan\r\nnan -inf inf\r\n[torch.FloatTensor of size 3x3]\r\n), ('bias', None)])\r\ngradients contain 'nan'\r\n```\r\n"}