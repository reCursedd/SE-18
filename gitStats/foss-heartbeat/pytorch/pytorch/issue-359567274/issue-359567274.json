{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11578", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11578/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11578/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11578/events", "html_url": "https://github.com/pytorch/pytorch/issues/11578", "id": 359567274, "node_id": "MDU6SXNzdWUzNTk1NjcyNzQ=", "number": 11578, "title": "Request to import pytest in test/*.py", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-09-12T16:50:40Z", "updated_at": "2018-09-17T18:01:05Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Currently PyTorch uses the builtin <code>unittest</code> module for testing. Would it be possible to add a dependency on <a href=\"https://docs.pytest.org/en/latest/\" rel=\"nofollow\">pytest</a> so developers can more easily write parametrized tests?</p>\n<p>While working on <a href=\"https://github.com/pytorch/pytorch/blob/master/test/test_distributions.py\">test/test_distributions.py</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1762463\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/neerajprad\">@neerajprad</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1093846\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alicanb\">@alicanb</a>, and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=648532\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fritzo\">@fritzo</a> have found it challenging to write heavily parametrized tests. As a result, test coverage suffers. By contrast in Pyro we use heavily parametrized tests <a href=\"https://docs.pytest.org/en/latest/parametrize.html\" rel=\"nofollow\">using pytest</a>, and our Pyro tests seem to catch many bugs that aren't caught in PyTorch's own tests (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"345066733\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9917\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9917/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9917\">#9917</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"342167455\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9521\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9521/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9521\">#9521</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"345468853\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9977\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9977/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9977\">#9977</a>, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"347696545\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10241\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/10241/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/10241\">#10241</a>).</p>\n<p>In particular, I'd like to be able to gather <a href=\"https://docs.pytest.org/en/documentation-restructure/how-to/skipping.html\" rel=\"nofollow\">xfailing tests</a> in batch</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">DISTRIBUTIONS</span> <span class=\"pl-k\">=</span> [Bernoulli, Beta, Cauchy, <span class=\"pl-c1\">...</span>, Weibull]\n\n<span class=\"pl-en\">@pytest.mark.parametrize</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Dist<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">DISTRIBUTIONS</span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_that_sometime_fails</span>(<span class=\"pl-smi\">Dist</span>):\n    dist <span class=\"pl-k\">=</span> Dist(<span class=\"pl-c1\">...</span>)\n    <span class=\"pl-k\">assert</span> something(dist)</pre></div>\n<p>and then mark xfailing parameters</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">DISTRIBUTIONS</span> <span class=\"pl-k\">=</span> [\n    Bernoulli,\n    Beta,\n    pytest.param(Cauchy, <span class=\"pl-v\">marks</span><span class=\"pl-k\">=</span>[pytest.mark.xfail(<span class=\"pl-v\">reason</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>schema not found for node<span class=\"pl-pds\">'</span></span>)]),\n    <span class=\"pl-c1\">...</span>\n    Weibull,\n]</pre></div>\n<p>pytest makes it easy to collect xfailing tests in batch (<code>pytest -v --tb=no</code>) and to see the entire list of xfailing tests, and makes it easy to run xfailing tests to see which have started passing since the last time tests were run (<code>pytest -v</code>, or <code>pytest --runxfail</code>). In contrast, <code>unittest</code> fixtures typically parametrize via for loops and can report only a single failure on each run. (Apologies if I'm unaware of convenient functionality in <code>unittest</code>!)</p>\n<p>I think allowing usage of pytest in test/*.py could help improve PyTorch test coverage.</p>", "body_text": "Currently PyTorch uses the builtin unittest module for testing. Would it be possible to add a dependency on pytest so developers can more easily write parametrized tests?\nWhile working on test/test_distributions.py, @neerajprad, @alicanb, and @fritzo have found it challenging to write heavily parametrized tests. As a result, test coverage suffers. By contrast in Pyro we use heavily parametrized tests using pytest, and our Pyro tests seem to catch many bugs that aren't caught in PyTorch's own tests (#9917, #9521, #9977, #10241).\nIn particular, I'd like to be able to gather xfailing tests in batch\nDISTRIBUTIONS = [Bernoulli, Beta, Cauchy, ..., Weibull]\n\n@pytest.mark.parametrize('Dist', DISTRIBUTIONS)\ndef test_that_sometime_fails(Dist):\n    dist = Dist(...)\n    assert something(dist)\nand then mark xfailing parameters\nDISTRIBUTIONS = [\n    Bernoulli,\n    Beta,\n    pytest.param(Cauchy, marks=[pytest.mark.xfail(reason='schema not found for node')]),\n    ...\n    Weibull,\n]\npytest makes it easy to collect xfailing tests in batch (pytest -v --tb=no) and to see the entire list of xfailing tests, and makes it easy to run xfailing tests to see which have started passing since the last time tests were run (pytest -v, or pytest --runxfail). In contrast, unittest fixtures typically parametrize via for loops and can report only a single failure on each run. (Apologies if I'm unaware of convenient functionality in unittest!)\nI think allowing usage of pytest in test/*.py could help improve PyTorch test coverage.", "body": "Currently PyTorch uses the builtin `unittest` module for testing. Would it be possible to add a dependency on [pytest](https://docs.pytest.org/en/latest/) so developers can more easily write parametrized tests?\r\n\r\nWhile working on [test/test_distributions.py](https://github.com/pytorch/pytorch/blob/master/test/test_distributions.py), @neerajprad, @alicanb, and @fritzo have found it challenging to write heavily parametrized tests. As a result, test coverage suffers. By contrast in Pyro we use heavily parametrized tests [using pytest](https://docs.pytest.org/en/latest/parametrize.html), and our Pyro tests seem to catch many bugs that aren't caught in PyTorch's own tests (#9917, #9521, #9977, #10241).\r\n\r\nIn particular, I'd like to be able to gather [xfailing tests](https://docs.pytest.org/en/documentation-restructure/how-to/skipping.html) in batch\r\n```py\r\nDISTRIBUTIONS = [Bernoulli, Beta, Cauchy, ..., Weibull]\r\n\r\n@pytest.mark.parametrize('Dist', DISTRIBUTIONS)\r\ndef test_that_sometime_fails(Dist):\r\n    dist = Dist(...)\r\n    assert something(dist)\r\n```\r\nand then mark xfailing parameters\r\n```py\r\nDISTRIBUTIONS = [\r\n    Bernoulli,\r\n    Beta,\r\n    pytest.param(Cauchy, marks=[pytest.mark.xfail(reason='schema not found for node')]),\r\n    ...\r\n    Weibull,\r\n]\r\n```\r\npytest makes it easy to collect xfailing tests in batch (`pytest -v --tb=no`) and to see the entire list of xfailing tests, and makes it easy to run xfailing tests to see which have started passing since the last time tests were run (`pytest -v`, or `pytest --runxfail`). In contrast, `unittest` fixtures typically parametrize via for loops and can report only a single failure on each run. (Apologies if I'm unaware of convenient functionality in `unittest`!)\r\n\r\nI think allowing usage of pytest in test/*.py could help improve PyTorch test coverage."}