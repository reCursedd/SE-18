{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/431139058", "html_url": "https://github.com/pytorch/pytorch/issues/4741#issuecomment-431139058", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4741", "id": 431139058, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTEzOTA1OA==", "user": {"login": "penguinshin", "id": 10840426, "node_id": "MDQ6VXNlcjEwODQwNDI2", "avatar_url": "https://avatars3.githubusercontent.com/u/10840426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/penguinshin", "html_url": "https://github.com/penguinshin", "followers_url": "https://api.github.com/users/penguinshin/followers", "following_url": "https://api.github.com/users/penguinshin/following{/other_user}", "gists_url": "https://api.github.com/users/penguinshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/penguinshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/penguinshin/subscriptions", "organizations_url": "https://api.github.com/users/penguinshin/orgs", "repos_url": "https://api.github.com/users/penguinshin/repos", "events_url": "https://api.github.com/users/penguinshin/events{/privacy}", "received_events_url": "https://api.github.com/users/penguinshin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-18T19:49:48Z", "updated_at": "2018-10-18T19:49:48Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Right, so I fixed that at made it all one single batch- i find that setting momentum for BN to .9999 helps, but doesnt completely solve the problem. (theres still a difference between average train performance using train mode and average train performance using eval). Do you have any suggestions here?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\"> On Oct 17, 2018, at 11:42 AM, Tongzhou Wang ***@***.***&gt; wrote:\n\n I run each 180x1x180 window (of the 128) through the net and sum the losses over all 128, before calling a backward pass and running optimizer.step().\n\n <a class=\"user-mention\" href=\"https://github.com/penguinshin\">@penguinshin</a> &lt;<a href=\"https://github.com/penguinshin\">https://github.com/penguinshin</a>&gt; Here is your problem. Batch norm (and its backward) is not linear in batch size. I.e., f(x) + f(y) != f([x, y]).sum(). Therefore, instead, you should use tensor.unfold <a href=\"https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold\">https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold</a> &lt;<a href=\"https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold\">https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold</a>&gt; to create a batch of 128 windows and activate them through the network once.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"289940493\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4741\" href=\"https://github.com/pytorch/pytorch/issues/4741#issuecomment-430680335\">#4741 (comment)</a>&gt;, or mute the thread &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AKVpaovEyENALI1CvyETjUD3199UFNRkks5ul0_4gaJpZM4RkUtW\">https://github.com/notifications/unsubscribe-auth/AKVpaovEyENALI1CvyETjUD3199UFNRkks5ul0_4gaJpZM4RkUtW</a>&gt;.\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Right, so I fixed that at made it all one single batch- i find that setting momentum for BN to .9999 helps, but doesnt completely solve the problem. (theres still a difference between average train performance using train mode and average train performance using eval). Do you have any suggestions here?\n\u2026\n On Oct 17, 2018, at 11:42 AM, Tongzhou Wang ***@***.***> wrote:\n\n I run each 180x1x180 window (of the 128) through the net and sum the losses over all 128, before calling a backward pass and running optimizer.step().\n\n @penguinshin <https://github.com/penguinshin> Here is your problem. Batch norm (and its backward) is not linear in batch size. I.e., f(x) + f(y) != f([x, y]).sum(). Therefore, instead, you should use tensor.unfold https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold <https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold> to create a batch of 128 windows and activate them through the network once.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub <#4741 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKVpaovEyENALI1CvyETjUD3199UFNRkks5ul0_4gaJpZM4RkUtW>.", "body": "Right, so I fixed that at made it all one single batch- i find that setting momentum for BN to .9999 helps, but doesnt completely solve the problem. (theres still a difference between average train performance using train mode and average train performance using eval). Do you have any suggestions here?\n\n> On Oct 17, 2018, at 11:42 AM, Tongzhou Wang <notifications@github.com> wrote:\n> \n> I run each 180x1x180 window (of the 128) through the net and sum the losses over all 128, before calling a backward pass and running optimizer.step().\n> \n> @penguinshin <https://github.com/penguinshin> Here is your problem. Batch norm (and its backward) is not linear in batch size. I.e., f(x) + f(y) != f([x, y]).sum(). Therefore, instead, you should use tensor.unfold https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold <https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold> to create a batch of 128 windows and activate them through the network once.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/pytorch/pytorch/issues/4741#issuecomment-430680335>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AKVpaovEyENALI1CvyETjUD3199UFNRkks5ul0_4gaJpZM4RkUtW>.\n> \n\n"}