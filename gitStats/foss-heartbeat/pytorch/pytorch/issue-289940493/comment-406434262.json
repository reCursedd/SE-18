{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/406434262", "html_url": "https://github.com/pytorch/pytorch/issues/4741#issuecomment-406434262", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4741", "id": 406434262, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjQzNDI2Mg==", "user": {"login": "RLisfun", "id": 39865636, "node_id": "MDQ6VXNlcjM5ODY1NjM2", "avatar_url": "https://avatars3.githubusercontent.com/u/39865636?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RLisfun", "html_url": "https://github.com/RLisfun", "followers_url": "https://api.github.com/users/RLisfun/followers", "following_url": "https://api.github.com/users/RLisfun/following{/other_user}", "gists_url": "https://api.github.com/users/RLisfun/gists{/gist_id}", "starred_url": "https://api.github.com/users/RLisfun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RLisfun/subscriptions", "organizations_url": "https://api.github.com/users/RLisfun/orgs", "repos_url": "https://api.github.com/users/RLisfun/repos", "events_url": "https://api.github.com/users/RLisfun/events{/privacy}", "received_events_url": "https://api.github.com/users/RLisfun/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-19T22:31:15Z", "updated_at": "2018-07-20T17:18:08Z", "author_association": "NONE", "body_html": "<p>Always setting the training parameter to True and manually setting momentum to 0 on eval is a workarund that solves this bug in the software.</p>\n<p>just add:</p>\n<pre><code>    if self.training:\n        momentum = self.momentum\n    else:\n        momentum = 0.\n</code></pre>\n<p>in the forward of <code>_BatchNorm</code> found here:<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py\">https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py</a></p>\n<p>and remove the  <code>not</code> in <code>self.training or not self.track_running_stats</code>.</p>\n<p>Of course you also need to replace <code>self.momentum</code> by  <code>momentum</code> in the return.</p>", "body_text": "Always setting the training parameter to True and manually setting momentum to 0 on eval is a workarund that solves this bug in the software.\njust add:\n    if self.training:\n        momentum = self.momentum\n    else:\n        momentum = 0.\n\nin the forward of _BatchNorm found here:\nhttps://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py\nand remove the  not in self.training or not self.track_running_stats.\nOf course you also need to replace self.momentum by  momentum in the return.", "body": "Always setting the training parameter to True and manually setting momentum to 0 on eval is a workarund that solves this bug in the software.\r\n\r\njust add:\r\n\r\n        if self.training:\r\n            momentum = self.momentum\r\n        else:\r\n            momentum = 0.\r\n\r\nin the forward of `_BatchNorm` found here:\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py\r\n\r\nand remove the  `not` in `self.training or not self.track_running_stats`.\r\n\r\nOf course you also need to replace `self.momentum` by  `momentum` in the return."}