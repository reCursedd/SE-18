{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430680335", "html_url": "https://github.com/pytorch/pytorch/issues/4741#issuecomment-430680335", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4741", "id": 430680335, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDY4MDMzNQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T15:41:50Z", "updated_at": "2018-10-17T15:41:50Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>I run each 180x1x180 window (of the 128) through the net and sum the losses over all 128, before calling a backward pass and running optimizer.step().</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10840426\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/penguinshin\">@penguinshin</a> Here is your problem. Batch norm (and its backward) is <strong>not</strong> linear in batch size. I.e., <code>f(x) + f(y) != f([x, y]).sum()</code>. Therefore, instead, you should use <code>tensor.unfold</code> <a href=\"https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold\" rel=\"nofollow\">https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold</a> to create a <strong>batch</strong> of 128 windows and activate them through the network <strong>once</strong>.</p>", "body_text": "I run each 180x1x180 window (of the 128) through the net and sum the losses over all 128, before calling a backward pass and running optimizer.step().\n\n@penguinshin Here is your problem. Batch norm (and its backward) is not linear in batch size. I.e., f(x) + f(y) != f([x, y]).sum(). Therefore, instead, you should use tensor.unfold https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold to create a batch of 128 windows and activate them through the network once.", "body": "> I run each 180x1x180 window (of the 128) through the net and sum the losses over all 128, before calling a backward pass and running optimizer.step().\r\n\r\n@penguinshin Here is your problem. Batch norm (and its backward) is **not** linear in batch size. I.e., `f(x) + f(y) != f([x, y]).sum()`. Therefore, instead, you should use `tensor.unfold` https://pytorch.org/docs/master/tensors.html#torch.Tensor.unfold to create a **batch** of 128 windows and activate them through the network **once**."}