{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430471447", "html_url": "https://github.com/pytorch/pytorch/issues/4741#issuecomment-430471447", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4741", "id": 430471447, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDQ3MTQ0Nw==", "user": {"login": "penguinshin", "id": 10840426, "node_id": "MDQ6VXNlcjEwODQwNDI2", "avatar_url": "https://avatars3.githubusercontent.com/u/10840426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/penguinshin", "html_url": "https://github.com/penguinshin", "followers_url": "https://api.github.com/users/penguinshin/followers", "following_url": "https://api.github.com/users/penguinshin/following{/other_user}", "gists_url": "https://api.github.com/users/penguinshin/gists{/gist_id}", "starred_url": "https://api.github.com/users/penguinshin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/penguinshin/subscriptions", "organizations_url": "https://api.github.com/users/penguinshin/orgs", "repos_url": "https://api.github.com/users/penguinshin/repos", "events_url": "https://api.github.com/users/penguinshin/events{/privacy}", "received_events_url": "https://api.github.com/users/penguinshin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T02:53:31Z", "updated_at": "2018-10-17T02:53:31Z", "author_association": "NONE", "body_html": "<p>Thank you. That does fix the minimum working example, although for some reason it doesn't fix the problem for my training of the model. So pytorch applies bessel's correction by default? Is there a setting for batchnorm such that the inputs are not normalized by their own batch statistics, but rather the updated version of the running mean/var? Maybe that could be the issue- because I'm still finding that when creating a simple model based on a single batch norm layer + 1 dimensional convolution of a single filter, trained and applied on a single batch of data, the error goes up drastically when switching from train to eval mode.</p>\n<p>I will try to provide the full training loop but it will take longer to create.</p>", "body_text": "Thank you. That does fix the minimum working example, although for some reason it doesn't fix the problem for my training of the model. So pytorch applies bessel's correction by default? Is there a setting for batchnorm such that the inputs are not normalized by their own batch statistics, but rather the updated version of the running mean/var? Maybe that could be the issue- because I'm still finding that when creating a simple model based on a single batch norm layer + 1 dimensional convolution of a single filter, trained and applied on a single batch of data, the error goes up drastically when switching from train to eval mode.\nI will try to provide the full training loop but it will take longer to create.", "body": "Thank you. That does fix the minimum working example, although for some reason it doesn't fix the problem for my training of the model. So pytorch applies bessel's correction by default? Is there a setting for batchnorm such that the inputs are not normalized by their own batch statistics, but rather the updated version of the running mean/var? Maybe that could be the issue- because I'm still finding that when creating a simple model based on a single batch norm layer + 1 dimensional convolution of a single filter, trained and applied on a single batch of data, the error goes up drastically when switching from train to eval mode.\r\n\r\nI will try to provide the full training loop but it will take longer to create."}