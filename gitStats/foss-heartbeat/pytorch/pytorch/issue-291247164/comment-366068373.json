{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/366068373", "html_url": "https://github.com/pytorch/pytorch/issues/4831#issuecomment-366068373", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4831", "id": 366068373, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjA2ODM3Mw==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-15T21:32:01Z", "updated_at": "2018-02-15T21:32:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for reporting this, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6707363\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SeanNaren\">@SeanNaren</a>. A smaller reproducible script would be very helpful.</p>\n<p>One possibility I can think of when seeing your output is that your batch size isn't a multiple of the number of GPUs you're using. The <a href=\"http://pytorch.org/docs/master/nn.html?highlight=dataparallel#torch.nn.DataParallel\" rel=\"nofollow\">docs for DataParallel</a> say that the batch size should be a multiple of the number of gpus. If this is really the problem, I do agree the error should be clearer.</p>", "body_text": "Thanks for reporting this, @SeanNaren. A smaller reproducible script would be very helpful.\nOne possibility I can think of when seeing your output is that your batch size isn't a multiple of the number of GPUs you're using. The docs for DataParallel say that the batch size should be a multiple of the number of gpus. If this is really the problem, I do agree the error should be clearer.", "body": "Thanks for reporting this, @SeanNaren. A smaller reproducible script would be very helpful.\r\n\r\nOne possibility I can think of when seeing your output is that your batch size isn't a multiple of the number of GPUs you're using. The [docs for DataParallel](http://pytorch.org/docs/master/nn.html?highlight=dataparallel#torch.nn.DataParallel) say that the batch size should be a multiple of the number of gpus. If this is really the problem, I do agree the error should be clearer."}