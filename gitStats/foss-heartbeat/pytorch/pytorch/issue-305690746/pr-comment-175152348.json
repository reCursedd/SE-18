{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175152348", "pull_request_review_id": 104640601, "id": 175152348, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTE1MjM0OA==", "diff_hunk": "@@ -364,17 +364,23 @@ Tensor unsqueeze_to(const Tensor & self, int64_t dim, IntList sizes) {\n   return self;\n }\n \n-std::vector<Tensor> cat_tensors_backward(const Tensor & grad, const std::vector<int64_t> &sizes, int64_t dim) {\n+std::vector<Tensor> cat_tensors_backward(const Tensor & grad, const std::vector<std::vector<int64_t>> &sizes, int64_t dim) {\n+  if (sizes.size() > 0) {\n+    // cat wraps dim to the first tensor's shape \n+    dim = at::maybe_wrap_dim(dim, sizes[0].size());", "path": "tools/autograd/templates/Functions.cpp", "position": null, "original_position": 8, "commit_id": "fee486bd3bd78309203926f4af5734d6bbfc1839", "original_commit_id": "9ed3ebafdd29902b54193635a644c09e3a358bd5", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "can we move this logic into wrap_dim_utils?  In another PR you change how TensorList wrap_dim works, but it will be hard to remember to update this.", "created_at": "2018-03-16T16:53:23Z", "updated_at": "2018-11-23T15:40:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/5819#discussion_r175152348", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5819", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175152348"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5819#discussion_r175152348"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5819"}}, "body_html": "<p>can we move this logic into wrap_dim_utils?  In another PR you change how TensorList wrap_dim works, but it will be hard to remember to update this.</p>", "body_text": "can we move this logic into wrap_dim_utils?  In another PR you change how TensorList wrap_dim works, but it will be hard to remember to update this."}