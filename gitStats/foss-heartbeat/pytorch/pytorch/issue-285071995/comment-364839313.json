{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/364839313", "html_url": "https://github.com/pytorch/pytorch/issues/4406#issuecomment-364839313", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4406", "id": 364839313, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDgzOTMxMw==", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-12T06:41:51Z", "updated_at": "2018-02-12T06:41:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Should gradients be accumulated across ranks or averaged?<br>\nFrom my experience, averaging is likely to slow the converging since gradient is smaller than normal.</p>\n<p><a href=\"https://github.com/facebookresearch/TorchMPI/blob/master/torchmpi/nn.lua#L49\">TorchMPI</a> allreduces the gradients:</p>\n<div class=\"highlight highlight-source-lua\"><pre><span class=\"pl-c\"><span class=\"pl-c\">--</span> Synchronize gradients</span>\nM.<span class=\"pl-smi\">synchronizeGradients</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">function</span>(<span class=\"pl-smi\">net</span>)\n   <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> net.<span class=\"pl-smi\">parameters</span> <span class=\"pl-k\">then</span> <span class=\"pl-k\">return</span> <span class=\"pl-k\">end</span>\n   <span class=\"pl-k\">local</span> p, g <span class=\"pl-k\">=</span> net:<span class=\"pl-c1\">parameters</span>()\n   <span class=\"pl-k\">for</span> i, gw <span class=\"pl-k\">in</span> <span class=\"pl-c1\">ipairs</span>(g) <span class=\"pl-k\">do</span>\n      <span class=\"pl-k\">local</span> allreduceTensor <span class=\"pl-k\">=</span> <span class=\"pl-c1\">selectCollective</span>(gw, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>sync<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>allreduceTensor<span class=\"pl-pds\">'</span></span>)\n      <span class=\"pl-c1\">allreduceTensor</span>(gw)\n   <span class=\"pl-k\">end</span>\n<span class=\"pl-k\">end</span></pre></div>\n<p><a href=\"https://github.com/twitter/torch-distlearn/blob/master/lua/AllReduceSGD.lua#L18\">distlearn</a> averages the gradients:</p>\n<div class=\"highlight highlight-source-lua\"><pre>   <span class=\"pl-c\"><span class=\"pl-c\">--</span> Sum and normalize the gradients of all nodes</span>\n   <span class=\"pl-k\">local</span> <span class=\"pl-k\">function</span> <span class=\"pl-en\">sumAndNormalizeGradients</span>(<span class=\"pl-smi\">grads</span>)\n      <span class=\"pl-c\"><span class=\"pl-c\">--</span> All reduce and sum the gradients</span>\n      <span class=\"pl-k\">local</span> _,n <span class=\"pl-k\">=</span> tree.<span class=\"pl-c1\">allReduce</span>(grads, <span class=\"pl-k\">function</span>(<span class=\"pl-smi\">a</span>, <span class=\"pl-smi\">b</span>) <span class=\"pl-k\">return</span> a:<span class=\"pl-c1\">add</span>(b) <span class=\"pl-k\">end</span>)\n      <span class=\"pl-c\"><span class=\"pl-c\">--</span> Normalize them by the # of nodes that contributed</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">--</span> Not all nodes contribute to every step due to uneven partitioning of data</span>\n      <span class=\"pl-k\">if</span> n <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">then</span>\n         tree.<span class=\"pl-c1\">walkTable</span>(grads, <span class=\"pl-k\">function</span>(<span class=\"pl-smi\">grad</span>)\n            grad:<span class=\"pl-c1\">mul</span>(<span class=\"pl-c1\">1</span><span class=\"pl-k\">/</span>n)\n         <span class=\"pl-k\">end</span>)\n      <span class=\"pl-k\">end</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">--</span> This node contributed to this step</span>\n      stepsPerNode[tree.<span class=\"pl-smi\">nodeIndex</span>] <span class=\"pl-k\">=</span> stepsPerNode[tree.<span class=\"pl-smi\">nodeIndex</span>] <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n   <span class=\"pl-k\">end</span></pre></div>", "body_text": "Should gradients be accumulated across ranks or averaged?\nFrom my experience, averaging is likely to slow the converging since gradient is smaller than normal.\nTorchMPI allreduces the gradients:\n-- Synchronize gradients\nM.synchronizeGradients = function(net)\n   if not net.parameters then return end\n   local p, g = net:parameters()\n   for i, gw in ipairs(g) do\n      local allreduceTensor = selectCollective(gw, 'sync', 'allreduceTensor')\n      allreduceTensor(gw)\n   end\nend\ndistlearn averages the gradients:\n   -- Sum and normalize the gradients of all nodes\n   local function sumAndNormalizeGradients(grads)\n      -- All reduce and sum the gradients\n      local _,n = tree.allReduce(grads, function(a, b) return a:add(b) end)\n      -- Normalize them by the # of nodes that contributed\n      -- Not all nodes contribute to every step due to uneven partitioning of data\n      if n > 1 then\n         tree.walkTable(grads, function(grad)\n            grad:mul(1/n)\n         end)\n      end\n      -- This node contributed to this step\n      stepsPerNode[tree.nodeIndex] = stepsPerNode[tree.nodeIndex] + 1\n   end", "body": "Should gradients be accumulated across ranks or averaged?\r\nFrom my experience, averaging is likely to slow the converging since gradient is smaller than normal.\r\n \r\n[TorchMPI](https://github.com/facebookresearch/TorchMPI/blob/master/torchmpi/nn.lua#L49) allreduces the gradients:\r\n```lua\r\n-- Synchronize gradients\r\nM.synchronizeGradients = function(net)\r\n   if not net.parameters then return end\r\n   local p, g = net:parameters()\r\n   for i, gw in ipairs(g) do\r\n      local allreduceTensor = selectCollective(gw, 'sync', 'allreduceTensor')\r\n      allreduceTensor(gw)\r\n   end\r\nend\r\n```\r\n[distlearn](https://github.com/twitter/torch-distlearn/blob/master/lua/AllReduceSGD.lua#L18) averages the gradients:\r\n```lua\r\n   -- Sum and normalize the gradients of all nodes\r\n   local function sumAndNormalizeGradients(grads)\r\n      -- All reduce and sum the gradients\r\n      local _,n = tree.allReduce(grads, function(a, b) return a:add(b) end)\r\n      -- Normalize them by the # of nodes that contributed\r\n      -- Not all nodes contribute to every step due to uneven partitioning of data\r\n      if n > 1 then\r\n         tree.walkTable(grads, function(grad)\r\n            grad:mul(1/n)\r\n         end)\r\n      end\r\n      -- This node contributed to this step\r\n      stepsPerNode[tree.nodeIndex] = stepsPerNode[tree.nodeIndex] + 1\r\n   end\r\n```"}