{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/382952191", "html_url": "https://github.com/pytorch/pytorch/issues/4406#issuecomment-382952191", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4406", "id": 382952191, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Mjk1MjE5MQ==", "user": {"login": "pengjiang030", "id": 34865773, "node_id": "MDQ6VXNlcjM0ODY1Nzcz", "avatar_url": "https://avatars2.githubusercontent.com/u/34865773?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pengjiang030", "html_url": "https://github.com/pengjiang030", "followers_url": "https://api.github.com/users/pengjiang030/followers", "following_url": "https://api.github.com/users/pengjiang030/following{/other_user}", "gists_url": "https://api.github.com/users/pengjiang030/gists{/gist_id}", "starred_url": "https://api.github.com/users/pengjiang030/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pengjiang030/subscriptions", "organizations_url": "https://api.github.com/users/pengjiang030/orgs", "repos_url": "https://api.github.com/users/pengjiang030/repos", "events_url": "https://api.github.com/users/pengjiang030/events{/privacy}", "received_events_url": "https://api.github.com/users/pengjiang030/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-20T02:45:37Z", "updated_at": "2018-04-20T02:53:20Z", "author_association": "NONE", "body_html": "<p>I have met the same problem. It seems distributed training with gradient averaging is exactly the same as minibatch on a single node; however, the algorithm converges slower than single node training. I looked at the code in DistributedDataParallel, it actually synchronizes the parameters at the beginning of forward propagation.</p>\n<p>In Source code for torch.nn.parallel.distributed:</p>\n<pre><code>def forward(self, *inputs, **kwargs):\n        self.need_reduction = True\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        self._sync_params()\n        if len(self.device_ids) == 1:\n            return self.module(*inputs[0], **kwargs[0])\n        outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)\n        return self.gather(outputs, self.output_device)\n</code></pre>\n<p>The gradient averaging is registered:</p>\n<pre><code># Now register the reduction hook on the parameters\n       for p in self.module.parameters():\n           if not p.requires_grad:\n               continue\n\n           def allreduce_hook(*unused):\n               Variable._execution_engine.queue_callback(reduction_fn_nccl)\n\n           p.register_hook(allreduce_hook)\n\n</code></pre>\n<p>and executed later at the end of backward propagation.</p>\n<p>I wonder if it is some internal state on different nodes that is not communicated with gradient averaging causes the slow down of convergence. Is the parameter synchronization at the beginning of each iteration really necessary?</p>", "body_text": "I have met the same problem. It seems distributed training with gradient averaging is exactly the same as minibatch on a single node; however, the algorithm converges slower than single node training. I looked at the code in DistributedDataParallel, it actually synchronizes the parameters at the beginning of forward propagation.\nIn Source code for torch.nn.parallel.distributed:\ndef forward(self, *inputs, **kwargs):\n        self.need_reduction = True\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        self._sync_params()\n        if len(self.device_ids) == 1:\n            return self.module(*inputs[0], **kwargs[0])\n        outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)\n        return self.gather(outputs, self.output_device)\n\nThe gradient averaging is registered:\n# Now register the reduction hook on the parameters\n       for p in self.module.parameters():\n           if not p.requires_grad:\n               continue\n\n           def allreduce_hook(*unused):\n               Variable._execution_engine.queue_callback(reduction_fn_nccl)\n\n           p.register_hook(allreduce_hook)\n\n\nand executed later at the end of backward propagation.\nI wonder if it is some internal state on different nodes that is not communicated with gradient averaging causes the slow down of convergence. Is the parameter synchronization at the beginning of each iteration really necessary?", "body": "I have met the same problem. It seems distributed training with gradient averaging is exactly the same as minibatch on a single node; however, the algorithm converges slower than single node training. I looked at the code in DistributedDataParallel, it actually synchronizes the parameters at the beginning of forward propagation.\r\n\r\nIn Source code for torch.nn.parallel.distributed:\r\n```\r\ndef forward(self, *inputs, **kwargs):\r\n        self.need_reduction = True\r\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\r\n        self._sync_params()\r\n        if len(self.device_ids) == 1:\r\n            return self.module(*inputs[0], **kwargs[0])\r\n        outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)\r\n        return self.gather(outputs, self.output_device)\r\n```\r\n\r\nThe gradient averaging is registered:\r\n ```\r\n # Now register the reduction hook on the parameters\r\n        for p in self.module.parameters():\r\n            if not p.requires_grad:\r\n                continue\r\n\r\n            def allreduce_hook(*unused):\r\n                Variable._execution_engine.queue_callback(reduction_fn_nccl)\r\n\r\n            p.register_hook(allreduce_hook)\r\n\r\n```\r\nand executed later at the end of backward propagation.\r\n\r\nI wonder if it is some internal state on different nodes that is not communicated with gradient averaging causes the slow down of convergence. Is the parameter synchronization at the beginning of each iteration really necessary?"}