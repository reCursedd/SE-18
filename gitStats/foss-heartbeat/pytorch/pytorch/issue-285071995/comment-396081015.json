{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396081015", "html_url": "https://github.com/pytorch/pytorch/issues/4406#issuecomment-396081015", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4406", "id": 396081015, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjA4MTAxNQ==", "user": {"login": "dashesy", "id": 873905, "node_id": "MDQ6VXNlcjg3MzkwNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/873905?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dashesy", "html_url": "https://github.com/dashesy", "followers_url": "https://api.github.com/users/dashesy/followers", "following_url": "https://api.github.com/users/dashesy/following{/other_user}", "gists_url": "https://api.github.com/users/dashesy/gists{/gist_id}", "starred_url": "https://api.github.com/users/dashesy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dashesy/subscriptions", "organizations_url": "https://api.github.com/users/dashesy/orgs", "repos_url": "https://api.github.com/users/dashesy/repos", "events_url": "https://api.github.com/users/dashesy/events{/privacy}", "received_events_url": "https://api.github.com/users/dashesy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-10T20:51:48Z", "updated_at": "2018-07-21T02:32:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=34865773\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pengjiang030\">@pengjiang030</a> did you find an answer, and if <code>_sync_params</code> is always necessary? It may be of interest to people interested in the distributed trainings <a href=\"https://discuss.pytorch.org\" rel=\"nofollow\">to discuss</a> the findings in performance improvements. One point I think to consider: there is a single process per GPU, or single process per node when you create the <code>world_size</code> and DDP works in both cases. This line:</p>\n<pre><code>inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n</code></pre>\n<p>only should matter if there are multiple GPUs in each node, DDP only parallelizes among nodes/processes. I think it is worthwhile to always spawn a single process per GPU (e.g. using <code>CUDA_VISIBLE_DEVICES</code> or some other method).</p>\n<hr>\n<p>For nccl backend, it seems like gradients are averaged so it should perform like simple gradient averaging: <code>average_gradients</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre>                <span class=\"pl-c\"><span class=\"pl-c\">#</span> We will only use device 0's results, but this single op should be</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> faster than doing the following two operation sequentially:</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> (1) intra-node reduce to lead GPU, followed by</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> (2) inter-node allreduce for all the first lead GPUs in all nodes</span>\n                dist.all_reduce_multigpu(grads_batch_coalesced,\n                                         <span class=\"pl-v\">group</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.nccl_reduction_group_id)\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Now only work on the first device of self.device_ids, uncoalesce</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> the gradients for each bucket</span>\n                grads_batch_coalesced[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">/=</span> dist.get_world_size()</pre></div>", "body_text": "@pengjiang030 did you find an answer, and if _sync_params is always necessary? It may be of interest to people interested in the distributed trainings to discuss the findings in performance improvements. One point I think to consider: there is a single process per GPU, or single process per node when you create the world_size and DDP works in both cases. This line:\ninputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n\nonly should matter if there are multiple GPUs in each node, DDP only parallelizes among nodes/processes. I think it is worthwhile to always spawn a single process per GPU (e.g. using CUDA_VISIBLE_DEVICES or some other method).\n\nFor nccl backend, it seems like gradients are averaged so it should perform like simple gradient averaging: average_gradients:\n                # We will only use device 0's results, but this single op should be\n                # faster than doing the following two operation sequentially:\n                # (1) intra-node reduce to lead GPU, followed by\n                # (2) inter-node allreduce for all the first lead GPUs in all nodes\n                dist.all_reduce_multigpu(grads_batch_coalesced,\n                                         group=self.nccl_reduction_group_id)\n\n                # Now only work on the first device of self.device_ids, uncoalesce\n                # the gradients for each bucket\n                grads_batch_coalesced[0] /= dist.get_world_size()", "body": "@pengjiang030 did you find an answer, and if `_sync_params` is always necessary? It may be of interest to people interested in the distributed trainings [to discuss](https://discuss.pytorch.org) the findings in performance improvements. One point I think to consider: there is a single process per GPU, or single process per node when you create the `world_size` and DDP works in both cases. This line:\r\n\r\n    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\r\n\r\nonly should matter if there are multiple GPUs in each node, DDP only parallelizes among nodes/processes. I think it is worthwhile to always spawn a single process per GPU (e.g. using `CUDA_VISIBLE_DEVICES` or some other method).\r\n\r\n-----\r\nFor nccl backend, it seems like gradients are averaged so it should perform like simple gradient averaging: `average_gradients`:\r\n\r\n```python\r\n                # We will only use device 0's results, but this single op should be\r\n                # faster than doing the following two operation sequentially:\r\n                # (1) intra-node reduce to lead GPU, followed by\r\n                # (2) inter-node allreduce for all the first lead GPUs in all nodes\r\n                dist.all_reduce_multigpu(grads_batch_coalesced,\r\n                                         group=self.nccl_reduction_group_id)\r\n\r\n                # Now only work on the first device of self.device_ids, uncoalesce\r\n                # the gradients for each bucket\r\n                grads_batch_coalesced[0] /= dist.get_world_size()\r\n```"}