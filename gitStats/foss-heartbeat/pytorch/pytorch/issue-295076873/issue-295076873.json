{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5105", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5105/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5105/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5105/events", "html_url": "https://github.com/pytorch/pytorch/issues/5105", "id": 295076873, "node_id": "MDU6SXNzdWUyOTUwNzY4NzM=", "number": 5105, "title": "UNKNOWN_TYPE = Constant while converting model to onnx", "user": {"login": "engine303", "id": 11837626, "node_id": "MDQ6VXNlcjExODM3NjI2", "avatar_url": "https://avatars2.githubusercontent.com/u/11837626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/engine303", "html_url": "https://github.com/engine303", "followers_url": "https://api.github.com/users/engine303/followers", "following_url": "https://api.github.com/users/engine303/following{/other_user}", "gists_url": "https://api.github.com/users/engine303/gists{/gist_id}", "starred_url": "https://api.github.com/users/engine303/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/engine303/subscriptions", "organizations_url": "https://api.github.com/users/engine303/orgs", "repos_url": "https://api.github.com/users/engine303/repos", "events_url": "https://api.github.com/users/engine303/events{/privacy}", "received_events_url": "https://api.github.com/users/engine303/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-02-07T10:08:43Z", "updated_at": "2018-02-07T10:08:43Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>hi! i try to convert some simple feedforward model with one-hot encoding input with fixed dimensions</p>\n<p>here is code for reproducing</p>\n<pre><code>from torch import nn, zeros, LongTensor, FloatTensor, onnx\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self, input_size):\n        super(SimpleNet, self).__init__()\n        \n        self.embedd = nn.Embedding(input_size, 100)\n        self.fc1 = nn.Linear(100, 200)\n        self.fc2 = nn.Linear(200, 200)\n        self.fc3 = nn.Linear(200, 300)\n                \n    def forward(self, input):\n        r_0 = input.mm(self.embedd.weight)  # in real case i want to load weight matrix from trained net\n        r_1 = F.tanh(self.fc1(r_0).div(10.))\n        r_2 = F.tanh(self.fc2(r_1).div(10.)) + r_1\n        r_3 = F.tanh(self.fc3(r_2).div(10.))\n        return r_3\n\nindices = [1,2,3,4,5]\ninput = zeros(1000).type(FloatTensor)\n\n# create one-hot encoded input\nfor i in indices:\n    input[i] = 1\n    \ninput = Variable(input)\ninput = input.view(1, -1)\n\nsimple_net = SimpleNet(1000)\nout = simple_net(input)\n\ntorch_out = onnx._export(simple_net,\n                         input,\n                         \"test_simple_net.onnx\",\n                         export_params=True,\n                         verbose=True\n                         )\n</code></pre>\n<p>this is the output</p>\n<pre><code>graph(%0 : Float(1, 1000)\n      %1 : Float(1000, 100)\n      %2 : Float(200, 100)\n      %3 : Float(200)\n      %4 : Float(200, 200)\n      %5 : Float(200)\n      %6 : Float(300, 200)\n      %7 : Float(300)) {\n  %8 : UNKNOWN_TYPE = Constant[value={0}](), scope: SimpleNet\n  %9 : Float(1, 100) = Gemm[alpha=1, beta=0, broadcast=1](%0, %1, %8), scope: SimpleNet\n  %10 : Float(1, 200) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%9, %2, %3), scope: SimpleNet/Linear[fc1]\n  %11 : UNKNOWN_TYPE = Constant[value={10}](), scope: SimpleNet\n  %12 : Float(1, 200) = Div[broadcast=1](%10, %11), scope: SimpleNet\n  %13 : Float(1, 200) = Tanh(%12), scope: SimpleNet\n  %14 : Float(1, 200) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%13, %4, %5), scope: SimpleNet/Linear[fc2]\n  %15 : UNKNOWN_TYPE = Constant[value={10}](), scope: SimpleNet\n  %16 : Float(1, 200) = Div[broadcast=1](%14, %15), scope: SimpleNet\n  %17 : Float(1, 200) = Tanh(%16), scope: SimpleNet\n  %18 : Float(1, 200) = Add(%17, %13), scope: SimpleNet\n  %19 : Float(1, 300) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%18, %6, %7), scope: SimpleNet/Linear[fc3]\n  %20 : UNKNOWN_TYPE = Constant[value={10}](), scope: SimpleNet\n  %21 : Float(1, 300) = Div[broadcast=1](%19, %20), scope: SimpleNet\n  %22 : Float(1, 300) = Tanh(%21), scope: SimpleNet\n  return (%22);\n}\n</code></pre>\n<p>So,</p>\n<ol>\n<li>why all Constant nodes have UNKNOWN_TYPE?</li>\n<li>why i got another Constant node (%8 on onnx graph). i found that this node appears because this code row</li>\n</ol>\n<pre><code> r_0 = input.mm(self.embedd.weight)\n</code></pre>\n<p>thanks for answers</p>", "body_text": "hi! i try to convert some simple feedforward model with one-hot encoding input with fixed dimensions\nhere is code for reproducing\nfrom torch import nn, zeros, LongTensor, FloatTensor, onnx\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\nclass SimpleNet(nn.Module):\n    def __init__(self, input_size):\n        super(SimpleNet, self).__init__()\n        \n        self.embedd = nn.Embedding(input_size, 100)\n        self.fc1 = nn.Linear(100, 200)\n        self.fc2 = nn.Linear(200, 200)\n        self.fc3 = nn.Linear(200, 300)\n                \n    def forward(self, input):\n        r_0 = input.mm(self.embedd.weight)  # in real case i want to load weight matrix from trained net\n        r_1 = F.tanh(self.fc1(r_0).div(10.))\n        r_2 = F.tanh(self.fc2(r_1).div(10.)) + r_1\n        r_3 = F.tanh(self.fc3(r_2).div(10.))\n        return r_3\n\nindices = [1,2,3,4,5]\ninput = zeros(1000).type(FloatTensor)\n\n# create one-hot encoded input\nfor i in indices:\n    input[i] = 1\n    \ninput = Variable(input)\ninput = input.view(1, -1)\n\nsimple_net = SimpleNet(1000)\nout = simple_net(input)\n\ntorch_out = onnx._export(simple_net,\n                         input,\n                         \"test_simple_net.onnx\",\n                         export_params=True,\n                         verbose=True\n                         )\n\nthis is the output\ngraph(%0 : Float(1, 1000)\n      %1 : Float(1000, 100)\n      %2 : Float(200, 100)\n      %3 : Float(200)\n      %4 : Float(200, 200)\n      %5 : Float(200)\n      %6 : Float(300, 200)\n      %7 : Float(300)) {\n  %8 : UNKNOWN_TYPE = Constant[value={0}](), scope: SimpleNet\n  %9 : Float(1, 100) = Gemm[alpha=1, beta=0, broadcast=1](%0, %1, %8), scope: SimpleNet\n  %10 : Float(1, 200) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%9, %2, %3), scope: SimpleNet/Linear[fc1]\n  %11 : UNKNOWN_TYPE = Constant[value={10}](), scope: SimpleNet\n  %12 : Float(1, 200) = Div[broadcast=1](%10, %11), scope: SimpleNet\n  %13 : Float(1, 200) = Tanh(%12), scope: SimpleNet\n  %14 : Float(1, 200) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%13, %4, %5), scope: SimpleNet/Linear[fc2]\n  %15 : UNKNOWN_TYPE = Constant[value={10}](), scope: SimpleNet\n  %16 : Float(1, 200) = Div[broadcast=1](%14, %15), scope: SimpleNet\n  %17 : Float(1, 200) = Tanh(%16), scope: SimpleNet\n  %18 : Float(1, 200) = Add(%17, %13), scope: SimpleNet\n  %19 : Float(1, 300) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%18, %6, %7), scope: SimpleNet/Linear[fc3]\n  %20 : UNKNOWN_TYPE = Constant[value={10}](), scope: SimpleNet\n  %21 : Float(1, 300) = Div[broadcast=1](%19, %20), scope: SimpleNet\n  %22 : Float(1, 300) = Tanh(%21), scope: SimpleNet\n  return (%22);\n}\n\nSo,\n\nwhy all Constant nodes have UNKNOWN_TYPE?\nwhy i got another Constant node (%8 on onnx graph). i found that this node appears because this code row\n\n r_0 = input.mm(self.embedd.weight)\n\nthanks for answers", "body": "hi! i try to convert some simple feedforward model with one-hot encoding input with fixed dimensions\r\n\r\nhere is code for reproducing\r\n```\r\nfrom torch import nn, zeros, LongTensor, FloatTensor, onnx\r\nfrom torch.autograd import Variable\r\nimport torch.nn.functional as F\r\n\r\n\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self, input_size):\r\n        super(SimpleNet, self).__init__()\r\n        \r\n        self.embedd = nn.Embedding(input_size, 100)\r\n        self.fc1 = nn.Linear(100, 200)\r\n        self.fc2 = nn.Linear(200, 200)\r\n        self.fc3 = nn.Linear(200, 300)\r\n                \r\n    def forward(self, input):\r\n        r_0 = input.mm(self.embedd.weight)  # in real case i want to load weight matrix from trained net\r\n        r_1 = F.tanh(self.fc1(r_0).div(10.))\r\n        r_2 = F.tanh(self.fc2(r_1).div(10.)) + r_1\r\n        r_3 = F.tanh(self.fc3(r_2).div(10.))\r\n        return r_3\r\n\r\nindices = [1,2,3,4,5]\r\ninput = zeros(1000).type(FloatTensor)\r\n\r\n# create one-hot encoded input\r\nfor i in indices:\r\n    input[i] = 1\r\n    \r\ninput = Variable(input)\r\ninput = input.view(1, -1)\r\n\r\nsimple_net = SimpleNet(1000)\r\nout = simple_net(input)\r\n\r\ntorch_out = onnx._export(simple_net,\r\n                         input,\r\n                         \"test_simple_net.onnx\",\r\n                         export_params=True,\r\n                         verbose=True\r\n                         )\r\n```\r\n\r\nthis is the output\r\n```\r\ngraph(%0 : Float(1, 1000)\r\n      %1 : Float(1000, 100)\r\n      %2 : Float(200, 100)\r\n      %3 : Float(200)\r\n      %4 : Float(200, 200)\r\n      %5 : Float(200)\r\n      %6 : Float(300, 200)\r\n      %7 : Float(300)) {\r\n  %8 : UNKNOWN_TYPE = Constant[value={0}](), scope: SimpleNet\r\n  %9 : Float(1, 100) = Gemm[alpha=1, beta=0, broadcast=1](%0, %1, %8), scope: SimpleNet\r\n  %10 : Float(1, 200) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%9, %2, %3), scope: SimpleNet/Linear[fc1]\r\n  %11 : UNKNOWN_TYPE = Constant[value={10}](), scope: SimpleNet\r\n  %12 : Float(1, 200) = Div[broadcast=1](%10, %11), scope: SimpleNet\r\n  %13 : Float(1, 200) = Tanh(%12), scope: SimpleNet\r\n  %14 : Float(1, 200) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%13, %4, %5), scope: SimpleNet/Linear[fc2]\r\n  %15 : UNKNOWN_TYPE = Constant[value={10}](), scope: SimpleNet\r\n  %16 : Float(1, 200) = Div[broadcast=1](%14, %15), scope: SimpleNet\r\n  %17 : Float(1, 200) = Tanh(%16), scope: SimpleNet\r\n  %18 : Float(1, 200) = Add(%17, %13), scope: SimpleNet\r\n  %19 : Float(1, 300) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%18, %6, %7), scope: SimpleNet/Linear[fc3]\r\n  %20 : UNKNOWN_TYPE = Constant[value={10}](), scope: SimpleNet\r\n  %21 : Float(1, 300) = Div[broadcast=1](%19, %20), scope: SimpleNet\r\n  %22 : Float(1, 300) = Tanh(%21), scope: SimpleNet\r\n  return (%22);\r\n}\r\n```\r\n\r\nSo,\r\n1) why all Constant nodes have UNKNOWN_TYPE?\r\n2) why i got another Constant node (%8 on onnx graph). i found that this node appears because this code row \r\n```\r\n r_0 = input.mm(self.embedd.weight)\r\n```\r\n\r\nthanks for answers"}