{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/380140651", "html_url": "https://github.com/pytorch/pytorch/issues/6021#issuecomment-380140651", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6021", "id": 380140651, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDE0MDY1MQ==", "user": {"login": "mttk", "id": 3007947, "node_id": "MDQ6VXNlcjMwMDc5NDc=", "avatar_url": "https://avatars2.githubusercontent.com/u/3007947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mttk", "html_url": "https://github.com/mttk", "followers_url": "https://api.github.com/users/mttk/followers", "following_url": "https://api.github.com/users/mttk/following{/other_user}", "gists_url": "https://api.github.com/users/mttk/gists{/gist_id}", "starred_url": "https://api.github.com/users/mttk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mttk/subscriptions", "organizations_url": "https://api.github.com/users/mttk/orgs", "repos_url": "https://api.github.com/users/mttk/repos", "events_url": "https://api.github.com/users/mttk/events{/privacy}", "received_events_url": "https://api.github.com/users/mttk/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-10T15:22:01Z", "updated_at": "2018-04-10T15:22:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I ran your implementation on CPU, and confirm that the speed-up is around 10x. Ran it on another server and it goes up to 20x.</p>\n<p>The implementation of <code>sparse_</code> as per your template:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">sparse_</span>(<span class=\"pl-smi\">tensor</span>, <span class=\"pl-smi\">sparsity</span>, <span class=\"pl-smi\">std</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>):\n    <span class=\"pl-k\">if</span> tensor.ndimension() <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">2</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Only tensors with 2 dimensions are supported<span class=\"pl-pds\">\"</span></span>)\n\n    rows, cols <span class=\"pl-k\">=</span> tensor.shape\n    <span class=\"pl-k\">with</span> torch.no_grad():\n        tensor <span class=\"pl-k\">=</span> tensor.view(tensor.numel())\n        tensor.normal_(<span class=\"pl-c1\">0</span>, std)\n        <span class=\"pl-k\">if</span> sparsity <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n            zero_weights <span class=\"pl-k\">=</span> torch.randperm(<span class=\"pl-c1\">int</span>(rows <span class=\"pl-k\">*</span> cols))\n            zero_weights <span class=\"pl-k\">=</span> zero_weights[:<span class=\"pl-c1\">round</span>(rows <span class=\"pl-k\">*</span> cols <span class=\"pl-k\">*</span> sparsity)]\n            tensor[zero_weights] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        tensor <span class=\"pl-k\">=</span> tensor.view(rows, cols)\n        <span class=\"pl-k\">return</span> tensor</pre></div>\n<p>Essentially - this would mean that the 2-dim limitation could be lifted as well. I left it here for comparison with the previous method.<br>\nI also coded up something I thought was neat as an alternative (and works at approx the same speed):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">sparse_</span>(<span class=\"pl-smi\">tensor</span>, <span class=\"pl-smi\">sparsity</span>, <span class=\"pl-smi\">std</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>):\n    <span class=\"pl-k\">if</span> tensor.ndimension() <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">2</span>:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Only tensors with 2 dimensions are supported<span class=\"pl-pds\">\"</span></span>)\n\n    p <span class=\"pl-k\">=</span> torch.Tensor([<span class=\"pl-c1\">1</span>. <span class=\"pl-k\">-</span> sparsity])\n    <span class=\"pl-k\">with</span> torch.no_grad():\n        tensor.normal_(<span class=\"pl-c1\">0</span>, std)\n        mask <span class=\"pl-k\">=</span> torch.bernoulli(p.expand_as(tensor))\n        tensor <span class=\"pl-k\">=</span> tensor<span class=\"pl-k\">*</span>mask\n\n    <span class=\"pl-k\">return</span> tensor</pre></div>\n<p>One of these solutions can be made into a PR, whichever preferred.</p>", "body_text": "I ran your implementation on CPU, and confirm that the speed-up is around 10x. Ran it on another server and it goes up to 20x.\nThe implementation of sparse_ as per your template:\ndef sparse_(tensor, sparsity, std=0.01):\n    if tensor.ndimension() != 2:\n        raise ValueError(\"Only tensors with 2 dimensions are supported\")\n\n    rows, cols = tensor.shape\n    with torch.no_grad():\n        tensor = tensor.view(tensor.numel())\n        tensor.normal_(0, std)\n        if sparsity > 0:\n            zero_weights = torch.randperm(int(rows * cols))\n            zero_weights = zero_weights[:round(rows * cols * sparsity)]\n            tensor[zero_weights] = 0\n        tensor = tensor.view(rows, cols)\n        return tensor\nEssentially - this would mean that the 2-dim limitation could be lifted as well. I left it here for comparison with the previous method.\nI also coded up something I thought was neat as an alternative (and works at approx the same speed):\ndef sparse_(tensor, sparsity, std=0.01):\n    if tensor.ndimension() != 2:\n        raise ValueError(\"Only tensors with 2 dimensions are supported\")\n\n    p = torch.Tensor([1. - sparsity])\n    with torch.no_grad():\n        tensor.normal_(0, std)\n        mask = torch.bernoulli(p.expand_as(tensor))\n        tensor = tensor*mask\n\n    return tensor\nOne of these solutions can be made into a PR, whichever preferred.", "body": "I ran your implementation on CPU, and confirm that the speed-up is around 10x. Ran it on another server and it goes up to 20x.\r\n\r\nThe implementation of `sparse_` as per your template:\r\n\r\n```python\r\ndef sparse_(tensor, sparsity, std=0.01):\r\n    if tensor.ndimension() != 2:\r\n        raise ValueError(\"Only tensors with 2 dimensions are supported\")\r\n\r\n    rows, cols = tensor.shape\r\n    with torch.no_grad():\r\n        tensor = tensor.view(tensor.numel())\r\n        tensor.normal_(0, std)\r\n        if sparsity > 0:\r\n            zero_weights = torch.randperm(int(rows * cols))\r\n            zero_weights = zero_weights[:round(rows * cols * sparsity)]\r\n            tensor[zero_weights] = 0\r\n        tensor = tensor.view(rows, cols)\r\n        return tensor\r\n```\r\n\r\nEssentially - this would mean that the 2-dim limitation could be lifted as well. I left it here for comparison with the previous method.\r\nI also coded up something I thought was neat as an alternative (and works at approx the same speed):\r\n\r\n```python\r\ndef sparse_(tensor, sparsity, std=0.01):\r\n    if tensor.ndimension() != 2:\r\n        raise ValueError(\"Only tensors with 2 dimensions are supported\")\r\n\r\n    p = torch.Tensor([1. - sparsity])\r\n    with torch.no_grad():\r\n        tensor.normal_(0, std)\r\n        mask = torch.bernoulli(p.expand_as(tensor))\r\n        tensor = tensor*mask\r\n\r\n    return tensor\r\n```\r\n\r\nOne of these solutions can be made into a PR, whichever preferred."}