{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10881", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10881/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10881/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10881/events", "html_url": "https://github.com/pytorch/pytorch/pull/10881", "id": 354053243, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEwOTI3MjI0", "number": 10881, "title": "Augment emit_nvtx to help connect backward-pass Function apply calls with their corresponding forward pass ops", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 559719279, "node_id": "MDU6TGFiZWw1NTk3MTkyNzk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ready%20for%20review", "name": "ready for review", "color": "b60205", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-08-26T00:16:38Z", "updated_at": "2018-11-23T15:51:16Z", "closed_at": "2018-09-14T18:58:15Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10881", "html_url": "https://github.com/pytorch/pytorch/pull/10881", "diff_url": "https://github.com/pytorch/pytorch/pull/10881.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10881.patch"}, "body_html": "<p>Often, we find ourselves looking at some long-running kernel or emit_nvtx range on an nvvp profile and trying to connect it to the offending line in a training script.  If the op is in the forward pass that's easy:  ops are enqueued explicitly from the Python side, so tracking it down with manual nvtx ranges supplemented by the built-in emit_nvtx ranges is straightforward.  If the op is in the backward pass, it's much more difficult.  From the Python side, all you can do is wrap loss.backward() in an nvtx range, and if you also use emit_nvtx, the automatic ranges provide only local information.  Right now, the only consistent way to connect backward-pass kernels to their associated forward-pass lines of Python is to understand your script line by line, and know exactly where in the backward pass you are.</p>\n<p>This PR augments the existing nvtx machinery to bridge the gap between forward and backward, allowing connection of backward-pass Function apply calls to the forward-pass operations that required/created those Functions.</p>\n<p>The method is simple and surgical.  During the forward pass, when running with emit_nvtx, the nvtx range for each function in VariableType is tagged with the current sequence number.  During the backward pass, the nvtx range associated with each Function's operator() is tagged with that Function's stashed sequence number, which can be compared to \"current sequence numbers\" from the forward pass to locate the associated op.</p>\n<p>Double-backward is not a problem.  If a backward pass with create_graph = True is underway, the relationship between backward and double-backward is conceptually the same as the relationship between forward and backward:  The functions in VariableType still spit out current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and in the eventual double-backward execution, their operator() ranges are still tagged with the stashed numbers, which can be compared to \"current sequence numbers\" from the backward pass.</p>\n<p>Minor caveats:</p>\n<ul>\n<li>The sequence number is thread-local, and many VariableType functions (specifically, those without a derivative explicitly defined in derivatives.yaml) don't create an associated function object (instead delegating that to sub-functions further down the call chain, perhaps called from within at::native functions that route back through VariableType by calling at::function_name).  So the correspondence of stashed sequence numbers in Function operator() ranges with numbers in forward-pass ranges is not guaranteed to be 1 to 1.  However, it's still a vast improvement over the current situation, and I don't think this issue should be a blocker.</li>\n<li>Feel free to litigate my use of stringstream in profiler.cpp.  I did it because it was easy and clean.  If that's too big a hammer, let's figure out something more lightweight.</li>\n</ul>", "body_text": "Often, we find ourselves looking at some long-running kernel or emit_nvtx range on an nvvp profile and trying to connect it to the offending line in a training script.  If the op is in the forward pass that's easy:  ops are enqueued explicitly from the Python side, so tracking it down with manual nvtx ranges supplemented by the built-in emit_nvtx ranges is straightforward.  If the op is in the backward pass, it's much more difficult.  From the Python side, all you can do is wrap loss.backward() in an nvtx range, and if you also use emit_nvtx, the automatic ranges provide only local information.  Right now, the only consistent way to connect backward-pass kernels to their associated forward-pass lines of Python is to understand your script line by line, and know exactly where in the backward pass you are.\nThis PR augments the existing nvtx machinery to bridge the gap between forward and backward, allowing connection of backward-pass Function apply calls to the forward-pass operations that required/created those Functions.\nThe method is simple and surgical.  During the forward pass, when running with emit_nvtx, the nvtx range for each function in VariableType is tagged with the current sequence number.  During the backward pass, the nvtx range associated with each Function's operator() is tagged with that Function's stashed sequence number, which can be compared to \"current sequence numbers\" from the forward pass to locate the associated op.\nDouble-backward is not a problem.  If a backward pass with create_graph = True is underway, the relationship between backward and double-backward is conceptually the same as the relationship between forward and backward:  The functions in VariableType still spit out current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and in the eventual double-backward execution, their operator() ranges are still tagged with the stashed numbers, which can be compared to \"current sequence numbers\" from the backward pass.\nMinor caveats:\n\nThe sequence number is thread-local, and many VariableType functions (specifically, those without a derivative explicitly defined in derivatives.yaml) don't create an associated function object (instead delegating that to sub-functions further down the call chain, perhaps called from within at::native functions that route back through VariableType by calling at::function_name).  So the correspondence of stashed sequence numbers in Function operator() ranges with numbers in forward-pass ranges is not guaranteed to be 1 to 1.  However, it's still a vast improvement over the current situation, and I don't think this issue should be a blocker.\nFeel free to litigate my use of stringstream in profiler.cpp.  I did it because it was easy and clean.  If that's too big a hammer, let's figure out something more lightweight.", "body": "Often, we find ourselves looking at some long-running kernel or emit_nvtx range on an nvvp profile and trying to connect it to the offending line in a training script.  If the op is in the forward pass that's easy:  ops are enqueued explicitly from the Python side, so tracking it down with manual nvtx ranges supplemented by the built-in emit_nvtx ranges is straightforward.  If the op is in the backward pass, it's much more difficult.  From the Python side, all you can do is wrap loss.backward() in an nvtx range, and if you also use emit_nvtx, the automatic ranges provide only local information.  Right now, the only consistent way to connect backward-pass kernels to their associated forward-pass lines of Python is to understand your script line by line, and know exactly where in the backward pass you are.\r\n\r\nThis PR augments the existing nvtx machinery to bridge the gap between forward and backward, allowing connection of backward-pass Function apply calls to the forward-pass operations that required/created those Functions.\r\n\r\nThe method is simple and surgical.  During the forward pass, when running with emit_nvtx, the nvtx range for each function in VariableType is tagged with the current sequence number.  During the backward pass, the nvtx range associated with each Function's operator() is tagged with that Function's stashed sequence number, which can be compared to \"current sequence numbers\" from the forward pass to locate the associated op.\r\n\r\nDouble-backward is not a problem.  If a backward pass with create_graph = True is underway, the relationship between backward and double-backward is conceptually the same as the relationship between forward and backward:  The functions in VariableType still spit out current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and in the eventual double-backward execution, their operator() ranges are still tagged with the stashed numbers, which can be compared to \"current sequence numbers\" from the backward pass.\r\n\r\nMinor caveats:  \r\n\r\n- The sequence number is thread-local, and many VariableType functions (specifically, those without a derivative explicitly defined in derivatives.yaml) don't create an associated function object (instead delegating that to sub-functions further down the call chain, perhaps called from within at::native functions that route back through VariableType by calling at::function_name).  So the correspondence of stashed sequence numbers in Function operator() ranges with numbers in forward-pass ranges is not guaranteed to be 1 to 1.  However, it's still a vast improvement over the current situation, and I don't think this issue should be a blocker.\r\n- Feel free to litigate my use of stringstream in profiler.cpp.  I did it because it was easy and clean.  If that's too big a hammer, let's figure out something more lightweight.\r\n\r\n"}