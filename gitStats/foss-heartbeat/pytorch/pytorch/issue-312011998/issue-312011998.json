{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6350", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6350/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6350/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6350/events", "html_url": "https://github.com/pytorch/pytorch/issues/6350", "id": 312011998, "node_id": "MDU6SXNzdWUzMTIwMTE5OTg=", "number": 6350, "title": "worker assignments in torch.utils.dataloader.py", "user": {"login": "stefan-schroedl", "id": 9063521, "node_id": "MDQ6VXNlcjkwNjM1MjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/9063521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefan-schroedl", "html_url": "https://github.com/stefan-schroedl", "followers_url": "https://api.github.com/users/stefan-schroedl/followers", "following_url": "https://api.github.com/users/stefan-schroedl/following{/other_user}", "gists_url": "https://api.github.com/users/stefan-schroedl/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefan-schroedl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefan-schroedl/subscriptions", "organizations_url": "https://api.github.com/users/stefan-schroedl/orgs", "repos_url": "https://api.github.com/users/stefan-schroedl/repos", "events_url": "https://api.github.com/users/stefan-schroedl/events{/privacy}", "received_events_url": "https://api.github.com/users/stefan-schroedl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-06T15:07:39Z", "updated_at": "2018-04-07T09:27:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>In <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/60a16e5663f35f2df17ae1d5b060d0e1accdbefd/torch/utils/data/dataloader.py#L297\">pytorch/torch/utils/data/dataloader.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 297\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/60a16e5663f35f2df17ae1d5b060d0e1accdbefd\">60a16e5</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L297\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"297\"></td>\n          <td id=\"LC297\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.index_queues[<span class=\"pl-c1\">self</span>.worker_queue_idx].put((<span class=\"pl-c1\">self</span>.send_idx, indices)) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n, all items from the dataset for a given minibatch are assigned to the same worker process. This means that the first minibatch always takes the full sequential processing time, and it can similarly lead to wait times later if the consumer ever races ahead to consume all prefeched minibatches quickly. Wouldn't it be better to distribute individual items (rather than entire minibatches) in a round robin fashion?<br>\nThank you!</p>", "body_text": "In \n  \n    \n      pytorch/torch/utils/data/dataloader.py\n    \n    \n         Line 297\n      in\n      60a16e5\n    \n    \n    \n    \n\n        \n          \n           self.index_queues[self.worker_queue_idx].put((self.send_idx, indices)) \n        \n    \n  \n\n, all items from the dataset for a given minibatch are assigned to the same worker process. This means that the first minibatch always takes the full sequential processing time, and it can similarly lead to wait times later if the consumer ever races ahead to consume all prefeched minibatches quickly. Wouldn't it be better to distribute individual items (rather than entire minibatches) in a round robin fashion?\nThank you!", "body": "\r\nIn https://github.com/pytorch/pytorch/blob/60a16e5663f35f2df17ae1d5b060d0e1accdbefd/torch/utils/data/dataloader.py#L297, all items from the dataset for a given minibatch are assigned to the same worker process. This means that the first minibatch always takes the full sequential processing time, and it can similarly lead to wait times later if the consumer ever races ahead to consume all prefeched minibatches quickly. Wouldn't it be better to distribute individual items (rather than entire minibatches) in a round robin fashion?\r\nThank you!\r\n\r\n"}