{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379342556", "html_url": "https://github.com/pytorch/pytorch/issues/6350#issuecomment-379342556", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6350", "id": 379342556, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTM0MjU1Ng==", "user": {"login": "stefan-schroedl", "id": 9063521, "node_id": "MDQ6VXNlcjkwNjM1MjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/9063521?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefan-schroedl", "html_url": "https://github.com/stefan-schroedl", "followers_url": "https://api.github.com/users/stefan-schroedl/followers", "following_url": "https://api.github.com/users/stefan-schroedl/following{/other_user}", "gists_url": "https://api.github.com/users/stefan-schroedl/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefan-schroedl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefan-schroedl/subscriptions", "organizations_url": "https://api.github.com/users/stefan-schroedl/orgs", "repos_url": "https://api.github.com/users/stefan-schroedl/repos", "events_url": "https://api.github.com/users/stefan-schroedl/events{/privacy}", "received_events_url": "https://api.github.com/users/stefan-schroedl/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-06T18:45:56Z", "updated_at": "2018-04-06T18:45:56Z", "author_association": "NONE", "body_html": "<p>Interesting - thank you for getting back to me so quickly!<br>\nYes, I can see that it can imply more scheduling overhead. But maybe would it be possible doing something simple, e.g., have n threads processing each n-th item each, join the threads (wait until all are finished on the current minibatch), then move on to the next minibatch (in parallel to a dedicated collation thread)? At least in my use case, most of the latency stems from image augmentation triggered by dataset.<strong>getitem</strong>().<br>\nThanks!</p>", "body_text": "Interesting - thank you for getting back to me so quickly!\nYes, I can see that it can imply more scheduling overhead. But maybe would it be possible doing something simple, e.g., have n threads processing each n-th item each, join the threads (wait until all are finished on the current minibatch), then move on to the next minibatch (in parallel to a dedicated collation thread)? At least in my use case, most of the latency stems from image augmentation triggered by dataset.getitem().\nThanks!", "body": "Interesting - thank you for getting back to me so quickly!\r\nYes, I can see that it can imply more scheduling overhead. But maybe would it be possible doing something simple, e.g., have n threads processing each n-th item each, join the threads (wait until all are finished on the current minibatch), then move on to the next minibatch (in parallel to a dedicated collation thread)? At least in my use case, most of the latency stems from image augmentation triggered by dataset.__getitem__(). \r\nThanks!  "}