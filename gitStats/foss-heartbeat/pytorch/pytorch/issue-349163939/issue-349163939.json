{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10378", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10378/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10378/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10378/events", "html_url": "https://github.com/pytorch/pytorch/pull/10378", "id": 349163939, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA3MzQ0Njkw", "number": 10378, "title": "Advanced Indexing: Allow skipping wrapping indices if returned from nonzero", "user": {"login": "zasdfgbnm", "id": 1032377, "node_id": "MDQ6VXNlcjEwMzIzNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1032377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zasdfgbnm", "html_url": "https://github.com/zasdfgbnm", "followers_url": "https://api.github.com/users/zasdfgbnm/followers", "following_url": "https://api.github.com/users/zasdfgbnm/following{/other_user}", "gists_url": "https://api.github.com/users/zasdfgbnm/gists{/gist_id}", "starred_url": "https://api.github.com/users/zasdfgbnm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zasdfgbnm/subscriptions", "organizations_url": "https://api.github.com/users/zasdfgbnm/orgs", "repos_url": "https://api.github.com/users/zasdfgbnm/repos", "events_url": "https://api.github.com/users/zasdfgbnm/events{/privacy}", "received_events_url": "https://api.github.com/users/zasdfgbnm/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-08-09T14:53:37Z", "updated_at": "2018-11-23T15:49:02Z", "closed_at": "2018-08-19T19:09:50Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10378", "html_url": "https://github.com/pytorch/pytorch/pull/10378", "diff_url": "https://github.com/pytorch/pytorch/pull/10378.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10378.patch"}, "body_html": "<p>In <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"349123454\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10374\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/10374/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/10374\">#10374</a>, I made the helper function that wrap indices a method of tensor to allow profiling how much overhead this feature adds to advanced indexing. But unfortunately, the indices wrapping is very expensive.</p>\n<p>A simple demo is:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nd <span class=\"pl-k\">=</span> torch.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>)\n\na <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>d)\ni <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.uint8, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>d).triu()\n\n<span class=\"pl-k\">with</span> torch.autograd.profiler.profile(<span class=\"pl-v\">use_cuda</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>) <span class=\"pl-k\">as</span> prof:\n\ta[i]\n\t\nprof.export_chrome_trace(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>triu<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(prof)</pre></div>\n<p>This gives:</p>\n<pre><code>-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                            CPU time        CUDA time            Calls        CPU total       CUDA total\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n_th_get_device                   5.958us          5.216us                1          5.958us          5.216us\n_th_get_device                   2.896us          2.272us                1          2.896us          2.272us\n_th_get_device                   3.140us          3.072us                1          3.140us          3.072us\n_cast_Byte                      15.384us         15.360us                1         15.384us         15.360us\n_th_get_device                   2.801us          2.848us                1          2.801us          2.848us\n_th_get_device                   2.805us          2.784us                1          2.805us          2.784us\nindex                         1832.829us       1862.496us                1       1832.829us       1862.496us\n_th_get_device                   2.832us          3.040us                1          2.832us          3.040us\nnonzero                        328.179us        605.728us                1        328.179us        605.728us\nselect                          30.519us          1.024us                1         30.519us          1.024us\nselect                           5.983us          1.024us                1          5.983us          1.024us\n_wrap_index_once               628.163us        442.368us                1        628.163us        442.368us\nmax                            295.205us         78.048us                1        295.205us         78.048us\n_local_scalar                   41.098us         41.088us                1         41.098us         41.088us\n_th_get_device                   2.954us          3.072us                1          2.954us          3.072us\n_local_scalar_dense             19.456us         19.456us                1         19.456us         19.456us\nmin                             81.825us         82.240us                1         81.825us         82.240us\n_local_scalar                   26.363us         26.240us                1         26.363us         26.240us\n_th_get_device                   4.620us          4.864us                1          4.620us          4.864us\n_local_scalar_dense             12.135us         12.192us                1         12.135us         12.192us\nremainder                      154.303us        195.424us                1        154.303us        195.424us\nmul                            234.862us        229.376us                1        234.862us        229.376us\n_wrap_index_once               255.512us        277.504us                1        255.512us        277.504us\nmax                             94.118us         76.192us                1         94.118us         76.192us\n_local_scalar                   25.571us         26.016us                1         25.571us         26.016us\n_th_get_device                   2.874us          2.432us                1          2.874us          2.432us\n_local_scalar_dense             12.734us         12.768us                1         12.734us         12.768us\nmin                             75.452us         76.576us                1         75.452us         76.576us\n_local_scalar                   26.015us         25.952us                1         26.015us         25.952us\n_th_get_device                   2.980us          3.008us                1          2.980us          3.008us\n_local_scalar_dense             11.987us         12.032us                1         11.987us         12.032us\nremainder                       12.788us         55.776us                1         12.788us         55.776us\nmul                            151.378us        140.288us                1        151.378us        140.288us\nadd_                            20.204us         53.248us                1         20.204us         53.248us\nview                             9.550us          1.024us                1          9.550us          1.024us\ntake                            82.686us         80.896us                1         82.686us         80.896us\n</code></pre>\n<p>We can see that <code>_wrap_index_once</code> is a major cost of computation.</p>\n<p>However, wrapping indices is not always necessary. For example, if an index tensor is constructed by <code>expandByteTensors</code>, then there is no need to wrap indices because <code>nonzero</code> will not return indices out of range.</p>\n<p>This PR adds a <code>bool</code> to store if wrap index can be skipped. And if yes, then skip this expensive operation. As a result of this change, we get the following profiling:</p>\n<pre><code>------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                       CPU time        CUDA time            Calls        CPU total       CUDA total\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n_th_get_device              7.707us          7.328us                1          7.707us          7.328us\n_th_get_device              3.011us          2.784us                1          3.011us          2.784us\n_th_get_device              3.460us          3.072us                1          3.460us          3.072us\n_cast_Byte                 17.001us         17.408us                1         17.001us         17.408us\n_th_get_device              4.109us          4.064us                1          4.109us          4.064us\n_th_get_device              2.808us          2.752us                1          2.808us          2.752us\nindex                     975.918us       1005.408us                1        975.918us       1005.408us\n_th_get_device              3.208us          3.072us                1          3.208us          3.072us\nnonzero                   339.928us        615.424us                1        339.928us        615.424us\nselect                     30.331us          1.024us                1         30.331us          1.024us\nselect                      5.766us          1.024us                1          5.766us          1.024us\nmul                       257.175us         81.920us                1        257.175us         81.920us\nmul                       146.566us        147.456us                1        146.566us        147.456us\nadd_                       19.227us         53.248us                1         19.227us         53.248us\nview                        9.385us          2.048us                1          9.385us          2.048us\ntake                       86.470us         69.632us                1         86.470us         69.632us\n</code></pre>\n<p>We can see that advanced indexing is now much faster.</p>\n<p>One more point is, this pull request is based on <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"349123454\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10374\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/10374/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/10374\">#10374</a>, so it include changes there. I suggest not reverting these changes. Since wrap indices is expensive, it worth to add a custom cuda kernel to do that job to reduce cost. Adding it to <code>native_functions.yaml</code> allows automatic dispatching CPU and GPU. I'm planning this in a future PR to see how much a custom kernel can improve.</p>", "body_text": "In #10374, I made the helper function that wrap indices a method of tensor to allow profiling how much overhead this feature adds to advanced indexing. But unfortunately, the indices wrapping is very expensive.\nA simple demo is:\nimport torch\nd = torch.device('cuda')\n\na = torch.rand(1000, 1000, device=d)\ni = torch.ones(1000, 1000, dtype=torch.uint8, device=d).triu()\n\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n\ta[i]\n\t\nprof.export_chrome_trace('triu')\nprint(prof)\nThis gives:\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                            CPU time        CUDA time            Calls        CPU total       CUDA total\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n_th_get_device                   5.958us          5.216us                1          5.958us          5.216us\n_th_get_device                   2.896us          2.272us                1          2.896us          2.272us\n_th_get_device                   3.140us          3.072us                1          3.140us          3.072us\n_cast_Byte                      15.384us         15.360us                1         15.384us         15.360us\n_th_get_device                   2.801us          2.848us                1          2.801us          2.848us\n_th_get_device                   2.805us          2.784us                1          2.805us          2.784us\nindex                         1832.829us       1862.496us                1       1832.829us       1862.496us\n_th_get_device                   2.832us          3.040us                1          2.832us          3.040us\nnonzero                        328.179us        605.728us                1        328.179us        605.728us\nselect                          30.519us          1.024us                1         30.519us          1.024us\nselect                           5.983us          1.024us                1          5.983us          1.024us\n_wrap_index_once               628.163us        442.368us                1        628.163us        442.368us\nmax                            295.205us         78.048us                1        295.205us         78.048us\n_local_scalar                   41.098us         41.088us                1         41.098us         41.088us\n_th_get_device                   2.954us          3.072us                1          2.954us          3.072us\n_local_scalar_dense             19.456us         19.456us                1         19.456us         19.456us\nmin                             81.825us         82.240us                1         81.825us         82.240us\n_local_scalar                   26.363us         26.240us                1         26.363us         26.240us\n_th_get_device                   4.620us          4.864us                1          4.620us          4.864us\n_local_scalar_dense             12.135us         12.192us                1         12.135us         12.192us\nremainder                      154.303us        195.424us                1        154.303us        195.424us\nmul                            234.862us        229.376us                1        234.862us        229.376us\n_wrap_index_once               255.512us        277.504us                1        255.512us        277.504us\nmax                             94.118us         76.192us                1         94.118us         76.192us\n_local_scalar                   25.571us         26.016us                1         25.571us         26.016us\n_th_get_device                   2.874us          2.432us                1          2.874us          2.432us\n_local_scalar_dense             12.734us         12.768us                1         12.734us         12.768us\nmin                             75.452us         76.576us                1         75.452us         76.576us\n_local_scalar                   26.015us         25.952us                1         26.015us         25.952us\n_th_get_device                   2.980us          3.008us                1          2.980us          3.008us\n_local_scalar_dense             11.987us         12.032us                1         11.987us         12.032us\nremainder                       12.788us         55.776us                1         12.788us         55.776us\nmul                            151.378us        140.288us                1        151.378us        140.288us\nadd_                            20.204us         53.248us                1         20.204us         53.248us\nview                             9.550us          1.024us                1          9.550us          1.024us\ntake                            82.686us         80.896us                1         82.686us         80.896us\n\nWe can see that _wrap_index_once is a major cost of computation.\nHowever, wrapping indices is not always necessary. For example, if an index tensor is constructed by expandByteTensors, then there is no need to wrap indices because nonzero will not return indices out of range.\nThis PR adds a bool to store if wrap index can be skipped. And if yes, then skip this expensive operation. As a result of this change, we get the following profiling:\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------\nName                       CPU time        CUDA time            Calls        CPU total       CUDA total\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------\n_th_get_device              7.707us          7.328us                1          7.707us          7.328us\n_th_get_device              3.011us          2.784us                1          3.011us          2.784us\n_th_get_device              3.460us          3.072us                1          3.460us          3.072us\n_cast_Byte                 17.001us         17.408us                1         17.001us         17.408us\n_th_get_device              4.109us          4.064us                1          4.109us          4.064us\n_th_get_device              2.808us          2.752us                1          2.808us          2.752us\nindex                     975.918us       1005.408us                1        975.918us       1005.408us\n_th_get_device              3.208us          3.072us                1          3.208us          3.072us\nnonzero                   339.928us        615.424us                1        339.928us        615.424us\nselect                     30.331us          1.024us                1         30.331us          1.024us\nselect                      5.766us          1.024us                1          5.766us          1.024us\nmul                       257.175us         81.920us                1        257.175us         81.920us\nmul                       146.566us        147.456us                1        146.566us        147.456us\nadd_                       19.227us         53.248us                1         19.227us         53.248us\nview                        9.385us          2.048us                1          9.385us          2.048us\ntake                       86.470us         69.632us                1         86.470us         69.632us\n\nWe can see that advanced indexing is now much faster.\nOne more point is, this pull request is based on #10374, so it include changes there. I suggest not reverting these changes. Since wrap indices is expensive, it worth to add a custom cuda kernel to do that job to reduce cost. Adding it to native_functions.yaml allows automatic dispatching CPU and GPU. I'm planning this in a future PR to see how much a custom kernel can improve.", "body": "In https://github.com/pytorch/pytorch/pull/10374, I made the helper function that wrap indices a method of tensor to allow profiling how much overhead this feature adds to advanced indexing. But unfortunately, the indices wrapping is very expensive.\r\n\r\nA simple demo is:\r\n```python\r\nimport torch\r\nd = torch.device('cuda')\r\n\r\na = torch.rand(1000, 1000, device=d)\r\ni = torch.ones(1000, 1000, dtype=torch.uint8, device=d).triu()\r\n\r\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\r\n\ta[i]\r\n\t\r\nprof.export_chrome_trace('triu')\r\nprint(prof)\r\n```\r\n\r\nThis gives:\r\n```\r\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nName                            CPU time        CUDA time            Calls        CPU total       CUDA total\r\n-----------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\n_th_get_device                   5.958us          5.216us                1          5.958us          5.216us\r\n_th_get_device                   2.896us          2.272us                1          2.896us          2.272us\r\n_th_get_device                   3.140us          3.072us                1          3.140us          3.072us\r\n_cast_Byte                      15.384us         15.360us                1         15.384us         15.360us\r\n_th_get_device                   2.801us          2.848us                1          2.801us          2.848us\r\n_th_get_device                   2.805us          2.784us                1          2.805us          2.784us\r\nindex                         1832.829us       1862.496us                1       1832.829us       1862.496us\r\n_th_get_device                   2.832us          3.040us                1          2.832us          3.040us\r\nnonzero                        328.179us        605.728us                1        328.179us        605.728us\r\nselect                          30.519us          1.024us                1         30.519us          1.024us\r\nselect                           5.983us          1.024us                1          5.983us          1.024us\r\n_wrap_index_once               628.163us        442.368us                1        628.163us        442.368us\r\nmax                            295.205us         78.048us                1        295.205us         78.048us\r\n_local_scalar                   41.098us         41.088us                1         41.098us         41.088us\r\n_th_get_device                   2.954us          3.072us                1          2.954us          3.072us\r\n_local_scalar_dense             19.456us         19.456us                1         19.456us         19.456us\r\nmin                             81.825us         82.240us                1         81.825us         82.240us\r\n_local_scalar                   26.363us         26.240us                1         26.363us         26.240us\r\n_th_get_device                   4.620us          4.864us                1          4.620us          4.864us\r\n_local_scalar_dense             12.135us         12.192us                1         12.135us         12.192us\r\nremainder                      154.303us        195.424us                1        154.303us        195.424us\r\nmul                            234.862us        229.376us                1        234.862us        229.376us\r\n_wrap_index_once               255.512us        277.504us                1        255.512us        277.504us\r\nmax                             94.118us         76.192us                1         94.118us         76.192us\r\n_local_scalar                   25.571us         26.016us                1         25.571us         26.016us\r\n_th_get_device                   2.874us          2.432us                1          2.874us          2.432us\r\n_local_scalar_dense             12.734us         12.768us                1         12.734us         12.768us\r\nmin                             75.452us         76.576us                1         75.452us         76.576us\r\n_local_scalar                   26.015us         25.952us                1         26.015us         25.952us\r\n_th_get_device                   2.980us          3.008us                1          2.980us          3.008us\r\n_local_scalar_dense             11.987us         12.032us                1         11.987us         12.032us\r\nremainder                       12.788us         55.776us                1         12.788us         55.776us\r\nmul                            151.378us        140.288us                1        151.378us        140.288us\r\nadd_                            20.204us         53.248us                1         20.204us         53.248us\r\nview                             9.550us          1.024us                1          9.550us          1.024us\r\ntake                            82.686us         80.896us                1         82.686us         80.896us\r\n```\r\nWe can see that `_wrap_index_once` is a major cost of computation.\r\n\r\nHowever, wrapping indices is not always necessary. For example, if an index tensor is constructed by `expandByteTensors`, then there is no need to wrap indices because `nonzero` will not return indices out of range.\r\n\r\nThis PR adds a `bool` to store if wrap index can be skipped. And if yes, then skip this expensive operation. As a result of this change, we get the following profiling:\r\n\r\n```\r\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\nName                       CPU time        CUDA time            Calls        CPU total       CUDA total\r\n------------------  ---------------  ---------------  ---------------  ---------------  ---------------\r\n_th_get_device              7.707us          7.328us                1          7.707us          7.328us\r\n_th_get_device              3.011us          2.784us                1          3.011us          2.784us\r\n_th_get_device              3.460us          3.072us                1          3.460us          3.072us\r\n_cast_Byte                 17.001us         17.408us                1         17.001us         17.408us\r\n_th_get_device              4.109us          4.064us                1          4.109us          4.064us\r\n_th_get_device              2.808us          2.752us                1          2.808us          2.752us\r\nindex                     975.918us       1005.408us                1        975.918us       1005.408us\r\n_th_get_device              3.208us          3.072us                1          3.208us          3.072us\r\nnonzero                   339.928us        615.424us                1        339.928us        615.424us\r\nselect                     30.331us          1.024us                1         30.331us          1.024us\r\nselect                      5.766us          1.024us                1          5.766us          1.024us\r\nmul                       257.175us         81.920us                1        257.175us         81.920us\r\nmul                       146.566us        147.456us                1        146.566us        147.456us\r\nadd_                       19.227us         53.248us                1         19.227us         53.248us\r\nview                        9.385us          2.048us                1          9.385us          2.048us\r\ntake                       86.470us         69.632us                1         86.470us         69.632us\r\n```\r\n\r\nWe can see that advanced indexing is now much faster.\r\n\r\nOne more point is, this pull request is based on https://github.com/pytorch/pytorch/pull/10374, so it include changes there. I suggest not reverting these changes. Since wrap indices is expensive, it worth to add a custom cuda kernel to do that job to reduce cost. Adding it to `native_functions.yaml` allows automatic dispatching CPU and GPU. I'm planning this in a future PR to see how much a custom kernel can improve."}