{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/295979224", "html_url": "https://github.com/pytorch/pytorch/issues/1311#issuecomment-295979224", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1311", "id": 295979224, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTk3OTIyNA==", "user": {"login": "khanhptnk", "id": 1854828, "node_id": "MDQ6VXNlcjE4NTQ4Mjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1854828?v=4", "gravatar_id": "", "url": "https://api.github.com/users/khanhptnk", "html_url": "https://github.com/khanhptnk", "followers_url": "https://api.github.com/users/khanhptnk/followers", "following_url": "https://api.github.com/users/khanhptnk/following{/other_user}", "gists_url": "https://api.github.com/users/khanhptnk/gists{/gist_id}", "starred_url": "https://api.github.com/users/khanhptnk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/khanhptnk/subscriptions", "organizations_url": "https://api.github.com/users/khanhptnk/orgs", "repos_url": "https://api.github.com/users/khanhptnk/repos", "events_url": "https://api.github.com/users/khanhptnk/events{/privacy}", "received_events_url": "https://api.github.com/users/khanhptnk/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-21T00:07:58Z", "updated_at": "2017-04-21T00:07:58Z", "author_association": "NONE", "body_html": "<p>Right, but there are two problems if your model is an RNN:</p>\n<ol>\n<li>Because the decisions are computed by shared RNN parameters. You would need to use retain_variables so that you can backward() multiple times through a parameter. And I guess for the last backward(), you have to switch retain_variables off so that those buffers are destroyed.</li>\n<li>If all backward() are conducted independently, then each node in the computation graph is visited more than once, which is inefficient. If you can concat and backward() at once, then each node is only visited once.</li>\n</ol>\n<p>I think those are the reasons I feel inconvenient. Right now I am not using reinforce(), but just specifying a loss that incurs the same gradients as doing reinforce(), which allows me to concat and backward() all steps at once.</p>", "body_text": "Right, but there are two problems if your model is an RNN:\n\nBecause the decisions are computed by shared RNN parameters. You would need to use retain_variables so that you can backward() multiple times through a parameter. And I guess for the last backward(), you have to switch retain_variables off so that those buffers are destroyed.\nIf all backward() are conducted independently, then each node in the computation graph is visited more than once, which is inefficient. If you can concat and backward() at once, then each node is only visited once.\n\nI think those are the reasons I feel inconvenient. Right now I am not using reinforce(), but just specifying a loss that incurs the same gradients as doing reinforce(), which allows me to concat and backward() all steps at once.", "body": "Right, but there are two problems if your model is an RNN:\r\n1. Because the decisions are computed by shared RNN parameters. You would need to use retain_variables so that you can backward() multiple times through a parameter. And I guess for the last backward(), you have to switch retain_variables off so that those buffers are destroyed. \r\n2. If all backward() are conducted independently, then each node in the computation graph is visited more than once, which is inefficient. If you can concat and backward() at once, then each node is only visited once. \r\n\r\nI think those are the reasons I feel inconvenient. Right now I am not using reinforce(), but just specifying a loss that incurs the same gradients as doing reinforce(), which allows me to concat and backward() all steps at once. \r\n"}