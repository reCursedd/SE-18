{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355463984", "html_url": "https://github.com/pytorch/pytorch/issues/4492#issuecomment-355463984", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4492", "id": 355463984, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTQ2Mzk4NA==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-05T03:26:58Z", "updated_at": "2018-01-05T03:26:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Minimal fix:</p>\n<pre><code>diff --git a/test/test_nn.py b/test/test_nn.py\nindex 46acdca..0cbd6a4 100644\n--- a/test/test_nn.py\n+++ b/test/test_nn.py\n@@ -1951,24 +1951,25 @@ class TestNN(NNTestCase):\n     def test_Conv2d_inconsistent_types_on_GPU_without_cudnn(self):\n         inputs = Variable(torch.randn(4, 1, 7, 7).float().cuda())\n         weights = Variable(torch.randn(1, 1, 3, 3).double().cuda())\n         bias = Variable(torch.randn(1).double().cuda())\n \n         torch.backends.cudnn.enabled = False\n         # inconsistent types should raise an exception\n         self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights))\n         self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights.float(), bias))\n \n         # but it should work with the same type\n         nn.functional.conv2d(inputs.float(), weights.float(), bias.float())\n+        torch.backends.cudnn.enabled = True\n</code></pre>\n<p>More robust fix coming tomorrow.</p>", "body_text": "Minimal fix:\ndiff --git a/test/test_nn.py b/test/test_nn.py\nindex 46acdca..0cbd6a4 100644\n--- a/test/test_nn.py\n+++ b/test/test_nn.py\n@@ -1951,24 +1951,25 @@ class TestNN(NNTestCase):\n     def test_Conv2d_inconsistent_types_on_GPU_without_cudnn(self):\n         inputs = Variable(torch.randn(4, 1, 7, 7).float().cuda())\n         weights = Variable(torch.randn(1, 1, 3, 3).double().cuda())\n         bias = Variable(torch.randn(1).double().cuda())\n \n         torch.backends.cudnn.enabled = False\n         # inconsistent types should raise an exception\n         self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights))\n         self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights.float(), bias))\n \n         # but it should work with the same type\n         nn.functional.conv2d(inputs.float(), weights.float(), bias.float())\n+        torch.backends.cudnn.enabled = True\n\nMore robust fix coming tomorrow.", "body": "Minimal fix:\r\n\r\n```\r\ndiff --git a/test/test_nn.py b/test/test_nn.py\r\nindex 46acdca..0cbd6a4 100644\r\n--- a/test/test_nn.py\r\n+++ b/test/test_nn.py\r\n@@ -1951,24 +1951,25 @@ class TestNN(NNTestCase):\r\n     def test_Conv2d_inconsistent_types_on_GPU_without_cudnn(self):\r\n         inputs = Variable(torch.randn(4, 1, 7, 7).float().cuda())\r\n         weights = Variable(torch.randn(1, 1, 3, 3).double().cuda())\r\n         bias = Variable(torch.randn(1).double().cuda())\r\n \r\n         torch.backends.cudnn.enabled = False\r\n         # inconsistent types should raise an exception\r\n         self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights))\r\n         self.assertRaises(RuntimeError, lambda: nn.functional.conv2d(inputs, weights.float(), bias))\r\n \r\n         # but it should work with the same type\r\n         nn.functional.conv2d(inputs.float(), weights.float(), bias.float())\r\n+        torch.backends.cudnn.enabled = True\r\n```\r\n\r\nMore robust fix coming tomorrow."}