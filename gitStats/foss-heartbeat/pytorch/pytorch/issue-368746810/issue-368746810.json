{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12536", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12536/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12536/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12536/events", "html_url": "https://github.com/pytorch/pytorch/issues/12536", "id": 368746810, "node_id": "MDU6SXNzdWUzNjg3NDY4MTA=", "number": 12536, "title": "Changing the state of the Dataset without re-instantiating the DataLoader", "user": {"login": "DavideA", "id": 7113894, "node_id": "MDQ6VXNlcjcxMTM4OTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/7113894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavideA", "html_url": "https://github.com/DavideA", "followers_url": "https://api.github.com/users/DavideA/followers", "following_url": "https://api.github.com/users/DavideA/following{/other_user}", "gists_url": "https://api.github.com/users/DavideA/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavideA/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavideA/subscriptions", "organizations_url": "https://api.github.com/users/DavideA/orgs", "repos_url": "https://api.github.com/users/DavideA/repos", "events_url": "https://api.github.com/users/DavideA/events{/privacy}", "received_events_url": "https://api.github.com/users/DavideA/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-10T16:22:22Z", "updated_at": "2018-10-10T18:10:13Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,<br>\nI'd like an enlightment on a problem I'm facing with DataLoaders.<br>\nThe issue is that - everything else fixed (random seeds included) - I get different training behaviors if I use nb_workers=0 or nb_workers&gt;0.</p>\n<p>After a few trials, I suspect the reason could be the way I change the state of a Dataset. Basically, instead of using different DataLoaders for training and validation, I use one instance only and I change the state of the underlying Dataset. Here's an example on MNIST:</p>\n<pre><code>class MNIST(Dataset):\n\n    def __init__(self):\n\n        super(MNIST, self).__init__()\n\n        self.train_dataset = datasets.MNIST(self.path, train=True, download=True, transform=None)\n        self.val_dataset = datasets.MNIST(self.path, train=False, download=True, transform=None)\n        self.cur_dataset = None\n\n        # Initialize currents\n        self.cur_len = None\n\n    def train(self):\n        self.cur_dataset = self.train_dataset\n        self.cur_len = len(self.train_dataset)\n\n    def val(self):\n        self.cur_dataset = self.val_dataset\n        self.cur_len = len(self.val_dataset)\n\n    def __len__(self):\n        return self.cur_len\n\n    def __getitem__(self, i):\n       return self.cur_dataset[i]\n</code></pre>\n<p>and then use <code>dataloader.dataset.train()</code> and <code>dataloader.dataset.val()</code> to switch between the two modes. Is this a bad choice?<br>\n(The code above is not the actual code I'm using, it's just an example)</p>\n<p>I'm using pytorch 0.4.0</p>\n<p>Thanks,<br>\nD</p>", "body_text": "Hi,\nI'd like an enlightment on a problem I'm facing with DataLoaders.\nThe issue is that - everything else fixed (random seeds included) - I get different training behaviors if I use nb_workers=0 or nb_workers>0.\nAfter a few trials, I suspect the reason could be the way I change the state of a Dataset. Basically, instead of using different DataLoaders for training and validation, I use one instance only and I change the state of the underlying Dataset. Here's an example on MNIST:\nclass MNIST(Dataset):\n\n    def __init__(self):\n\n        super(MNIST, self).__init__()\n\n        self.train_dataset = datasets.MNIST(self.path, train=True, download=True, transform=None)\n        self.val_dataset = datasets.MNIST(self.path, train=False, download=True, transform=None)\n        self.cur_dataset = None\n\n        # Initialize currents\n        self.cur_len = None\n\n    def train(self):\n        self.cur_dataset = self.train_dataset\n        self.cur_len = len(self.train_dataset)\n\n    def val(self):\n        self.cur_dataset = self.val_dataset\n        self.cur_len = len(self.val_dataset)\n\n    def __len__(self):\n        return self.cur_len\n\n    def __getitem__(self, i):\n       return self.cur_dataset[i]\n\nand then use dataloader.dataset.train() and dataloader.dataset.val() to switch between the two modes. Is this a bad choice?\n(The code above is not the actual code I'm using, it's just an example)\nI'm using pytorch 0.4.0\nThanks,\nD", "body": "Hi,\r\nI'd like an enlightment on a problem I'm facing with DataLoaders.\r\nThe issue is that - everything else fixed (random seeds included) - I get different training behaviors if I use nb_workers=0 or nb_workers>0.\r\n\r\nAfter a few trials, I suspect the reason could be the way I change the state of a Dataset. Basically, instead of using different DataLoaders for training and validation, I use one instance only and I change the state of the underlying Dataset. Here's an example on MNIST:\r\n\r\n```\r\nclass MNIST(Dataset):\r\n\r\n    def __init__(self):\r\n\r\n        super(MNIST, self).__init__()\r\n\r\n        self.train_dataset = datasets.MNIST(self.path, train=True, download=True, transform=None)\r\n        self.val_dataset = datasets.MNIST(self.path, train=False, download=True, transform=None)\r\n        self.cur_dataset = None\r\n\r\n        # Initialize currents\r\n        self.cur_len = None\r\n\r\n    def train(self):\r\n        self.cur_dataset = self.train_dataset\r\n        self.cur_len = len(self.train_dataset)\r\n\r\n    def val(self):\r\n        self.cur_dataset = self.val_dataset\r\n        self.cur_len = len(self.val_dataset)\r\n\r\n    def __len__(self):\r\n        return self.cur_len\r\n\r\n    def __getitem__(self, i):\r\n       return self.cur_dataset[i]\r\n```\r\n\r\nand then use `dataloader.dataset.train()` and `dataloader.dataset.val()` to switch between the two modes. Is this a bad choice?\r\n(The code above is not the actual code I'm using, it's just an example)\r\n\r\nI'm using pytorch 0.4.0\r\n\r\nThanks,\r\nD"}