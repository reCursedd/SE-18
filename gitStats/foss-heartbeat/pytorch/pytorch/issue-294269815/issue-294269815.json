{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5039", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5039/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5039/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5039/events", "html_url": "https://github.com/pytorch/pytorch/issues/5039", "id": 294269815, "node_id": "MDU6SXNzdWUyOTQyNjk4MTU=", "number": 5039, "title": "torch.nn.DataParallel supporting unequal sizes", "user": {"login": "wenwei202", "id": 12142066, "node_id": "MDQ6VXNlcjEyMTQyMDY2", "avatar_url": "https://avatars0.githubusercontent.com/u/12142066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wenwei202", "html_url": "https://github.com/wenwei202", "followers_url": "https://api.github.com/users/wenwei202/followers", "following_url": "https://api.github.com/users/wenwei202/following{/other_user}", "gists_url": "https://api.github.com/users/wenwei202/gists{/gist_id}", "starred_url": "https://api.github.com/users/wenwei202/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wenwei202/subscriptions", "organizations_url": "https://api.github.com/users/wenwei202/orgs", "repos_url": "https://api.github.com/users/wenwei202/repos", "events_url": "https://api.github.com/users/wenwei202/events{/privacy}", "received_events_url": "https://api.github.com/users/wenwei202/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-05T04:21:03Z", "updated_at": "2018-02-26T05:55:08Z", "closed_at": "2018-02-26T05:55:08Z", "author_association": "MEMBER", "body_html": "<p>As documented <a href=\"http://pytorch.org/docs/master/_modules/torch/nn/parallel/data_parallel.html\" rel=\"nofollow\">here</a>:</p>\n<pre><code>The batch size should be larger than the number of GPUs used. It should\n    also be an integer multiple of the number of GPUs so that each chunk is the\n    same size (so that each GPU processes the same number of samples).\n</code></pre>\n<p>To use <code>torch.nn.DataParallel</code>, people should carefully set the batch size according to the number of gpus they plan to use, otherwise it will pop up errors.</p>\n<p>This issue becomes more subtle when using <code>torch.utils.data.DataLoader</code> with <code>drop_last=False</code> by default. As the total number of training/validation samples varies with the dataset, the size of the last batch of data loaded by <code>torch.utils.data.DataLoader</code> is easy to become indivisible by the number of GPUs (e.g., 2,3,4,8,...).</p>\n<p>A feature request would be:<br>\nsupporting <code>torch.nn.DataParallel</code> with batch size indivisible by the number of GPUs used.</p>", "body_text": "As documented here:\nThe batch size should be larger than the number of GPUs used. It should\n    also be an integer multiple of the number of GPUs so that each chunk is the\n    same size (so that each GPU processes the same number of samples).\n\nTo use torch.nn.DataParallel, people should carefully set the batch size according to the number of gpus they plan to use, otherwise it will pop up errors.\nThis issue becomes more subtle when using torch.utils.data.DataLoader with drop_last=False by default. As the total number of training/validation samples varies with the dataset, the size of the last batch of data loaded by torch.utils.data.DataLoader is easy to become indivisible by the number of GPUs (e.g., 2,3,4,8,...).\nA feature request would be:\nsupporting torch.nn.DataParallel with batch size indivisible by the number of GPUs used.", "body": "As documented [here](http://pytorch.org/docs/master/_modules/torch/nn/parallel/data_parallel.html):\r\n```\r\nThe batch size should be larger than the number of GPUs used. It should\r\n    also be an integer multiple of the number of GPUs so that each chunk is the\r\n    same size (so that each GPU processes the same number of samples).\r\n```\r\nTo use `torch.nn.DataParallel`, people should carefully set the batch size according to the number of gpus they plan to use, otherwise it will pop up errors. \r\n\r\nThis issue becomes more subtle when using `torch.utils.data.DataLoader` with `drop_last=False` by default. As the total number of training/validation samples varies with the dataset, the size of the last batch of data loaded by `torch.utils.data.DataLoader` is easy to become indivisible by the number of GPUs (e.g., 2,3,4,8,...).\r\n\r\nA feature request would be:\r\nsupporting `torch.nn.DataParallel` with batch size indivisible by the number of GPUs used."}