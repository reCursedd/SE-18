{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169934964", "pull_request_review_id": 98515584, "id": 169934964, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2OTkzNDk2NA==", "diff_hunk": "@@ -19,6 +19,270 @@ namespace py = pybind11;\n \n namespace torch { namespace jit {\n \n+\n+// Before we translate to intepreter instructions, we do\n+// some preprocessing of the graph to turn it into a form that is closer\n+// to what the instructions will look like.\n+// In particular we:\n+// * (TODO) desugar Loop trip counts into c = 0, c += 1 instructions in the loop\n+// * flatten stages so that each stage starts with a load from the stack\n+//   and ends with a store to the stack\n+// *. computes move_flags (see Outputs), and inserts\n+//    'Drop' nodes where necessary to release Tensors after a If/Loop body exits\n+//    or if a value is never used.\n+// Outputs are:\n+// * graph - the post processed copy of g\n+// * move_flags[n] - a list of booleans, one for each input,\n+//   indicating whether this is the last use of the value. The interpreter\n+//   should generate a move rather than a copy in this case.\n+// * stage_input_types: the type annotations on the inputs to each stage\n+//   these can be removed once the the backward tracer is no longer used\n+\n+struct PreprocessGraph {\n+  PreprocessGraph(Graph & g)\n+  : graph(g.copy()) {\n+    desugarTripCounts(graph->block());\n+    flattenStages();\n+    dropUnused(graph->block());\n+\n+    // fill in move_flags by scanning blocks;\n+    findLastUses(graph->block());\n+    //TODO: desugar Loop trip counts, for now we drop trip counts\n+  }\n+  std::shared_ptr<Graph> graph;\n+  // for each input, should we move rather than copy the inputs\n+  std::unordered_map<Node*, std::vector<uint8_t>> move_flags;\n+\n+  // because JIT classic needs this to fix up gradients, remove when possible\n+  std::vector<std::vector<TypePtr>> stage_input_types;\n+private:\n+\n+  // this currently just _removes_ the trip count inputs and checks they are\n+  // unused. In the future they will be desugared into normal arithmetic to\n+  // provide a loop counter\n+  void desugarTripCounts(Block * b) {\n+    for(auto n : b->nodes()) {\n+      if(n->kind() == kLoop) {\n+\n+        // remove the trip count from Loop inputs, we don't support it yet\n+        n->removeInput(0);\n+        JIT_ASSERT(n->blocks()[0]->inputs()[0]->uses().size() == 0 &&\n+          \"NYI - use of trip count variable\");\n+        JIT_ASSERT(n->blocks()[0]->inputs()[1]->uses().size() == 0 &&\n+          \"NYI - use of cond variable in loop\");\n+\n+        // TODO: remove cond as input to loop carries, it just complicates this\n+        // implementation\n+        n->blocks()[0]->eraseInput(1);\n+        n->blocks()[0]->eraseInput(0);\n+      }\n+      for(auto sb : n->blocks()) {\n+        desugarTripCounts(sb);\n+      }\n+    }\n+  }\n+  Node * prependLocation() {\n+    //handle corner case where there are no nodes,\n+    if(graph->nodes().begin() == graph->nodes().end()) {\n+      return graph->return_node();\n+    }\n+    return *graph->nodes().begin();\n+  }\n+  // removes all inputs and outputs to a graph, replacing them with nodes before of after each insertStage\n+  void flattenStages() {\n+    WithInsertPoint guard(*graph, prependLocation());\n+    size_t input_pos = 0;\n+    size_t output_pos = 0;\n+    auto it = graph->nodes().begin();\n+    for(size_t i = 0; i <= graph->stage(); i++) {\n+      stage_input_types.emplace_back();", "path": "torch/csrc/jit/interpreter.cpp", "position": null, "original_position": 80, "commit_id": "f5f7d01b5b3c2859ffdb99316771529d4a624c22", "original_commit_id": "be682ec37e185e4b57c4b125d715d2d9e86485de", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "But why?", "created_at": "2018-02-22T12:01:17Z", "updated_at": "2018-11-23T15:39:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/5293#discussion_r169934964", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5293", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169934964"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5293#discussion_r169934964"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5293"}}, "body_html": "<p>But why?</p>", "body_text": "But why?"}