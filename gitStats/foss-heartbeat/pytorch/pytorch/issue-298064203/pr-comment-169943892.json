{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169943892", "pull_request_review_id": 98515584, "id": 169943892, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2OTk0Mzg5Mg==", "diff_hunk": "@@ -283,97 +592,189 @@ struct Instruction {\n   Operation callback;\n   UseList inputs;\n   ListHandle<int> outputs;\n+  Symbol debug_name; // used in dump to understand the generated code\n };\n \n \n-struct Stage {\n-  ListHandle<int> inputs; // inputs to define for the stage\n-  UseList outputs; // values consumed by the return\n-  std::vector<Instruction> instructions;\n-};\n+int relativeJump(int from_inst, int to_inst) {\n+  return to_inst - (from_inst + 1);\n+}\n \n // pre-processing that happens once per graph\n struct CodeImpl {\n-  CodeImpl(std::shared_ptr<Graph> & graph)\n-  : graph(graph) {\n-    int64_t cur_stage = -1;\n-    size_t input_pos = 0;\n-    size_t output_pos = 0;\n-    // step 1: encode all operators and stages into registers and fill in\n-    // input/output lists\n-    for(auto node : graph->nodes()) {\n-      insertStagesTo(cur_stage, node->stage(), input_pos, output_pos);\n-      cur_stage = node->stage();\n-      stages.back().instructions.emplace_back();\n-      auto & inst = stages.back().instructions.back();\n-      listBegin(inst.inputs.values);\n-      for(auto input : node->inputs()) {\n-        listInsert(inst.inputs.values, getOrAllocateRegister(input, true));\n-      }\n-      listBegin(inst.outputs);\n-      for(auto output : node->outputs()) {\n-        listInsert(inst.outputs, getOrAllocateRegister(output));\n-      }\n-      inst.callback = getOperation(node);\n-    }\n-    // it is possible that the final stages have no instructions in them\n-    // and are just identity functions. We call insertStagesTo here\n-    // to force all these empty stages to be generated if they exist\n-    insertStagesTo(cur_stage, graph->stage(), input_pos, output_pos);\n-\n-    // step 2: the last time we use a register  we want to mark its free_flag\n-    // so we clean it up\n-    // this is done with a backward scan where we mark the first time we see it\n-    std::unordered_set<int> seen_registers;\n-    auto scanUses = [&](UseList & u) {\n-      // scan backwards because the same value may appear > once in a use list\n-      // and it is the last use that should free it\n-      std::vector<bool> free_flags(u.values.size);\n-      for(int i = u.values.size - 1; i >= 0; i--) {\n-        int reg = get(u.values,i);\n-        free_flags[i] = seen_registers.count(reg) == 0;\n-        seen_registers.insert(reg);\n-      }\n-      listBegin(u.free_flags);\n-      for(auto b : free_flags)\n-        listInsert(u.free_flags, b);\n+  CodeImpl(std::shared_ptr<Graph> & graph_, bool computes_on_variables)\n+  : computes_on_variables(computes_on_variables)\n+  , preprocess(*graph_) {\n+    graph = preprocess.graph;\n+    //std::cout << \"into code graph:\\n\" << *graph << \"\\n\";\n+    insertNodesFromBlock(graph->block());\n+  }\n+\n+  // jump when input is 0\n+  void createJumpZ(int from_inst, int to_inst) {\n+    auto & inst = instructions[from_inst];\n+    JIT_ASSERT(inst.debug_name == kPlaceholder);\n+    auto offset = relativeJump(from_inst, to_inst);\n+    inst.callback = [offset](Stack & stack) {\n+      auto t = tensor_as<int64_t>(pop(stack));\n+      return (t == 0) ? offset : 0;\n     };\n-    for(auto sit = stages.rbegin(); sit != stages.rend(); sit++) {\n-      scanUses(sit->outputs);\n-      for(auto iit = sit->instructions.rbegin(); iit != sit->instructions.rend(); iit++) {\n-        scanUses(iit->inputs);\n-      }\n-    }\n+    inst.debug_name = kJumpZ;\n   }\n-  void insertStagesTo(int64_t cur_stage, int64_t goal_stage, size_t & input_pos, size_t & output_pos) {\n-    while(cur_stage < goal_stage) {\n-      cur_stage++;\n-      stages.emplace_back();\n-      auto & stage = stages.back();\n-      listBegin(stage.inputs);\n-      for(;input_pos < graph->inputs().size(); input_pos++) {\n-        auto input = graph->inputs()[input_pos];\n-        if((int64_t)input->stage() > cur_stage)\n-          break;\n-        // unused inputs are given a false register -1 so that we never hold a\n-        // reference to the tensor data, otherwise we would fail to clean them\n-        // up since they do not have a last use at which to free them\n-        int reg = input->uses().size() > 0 ? getOrAllocateRegister(input) : -1;\n-        listInsert(stage.inputs, reg);\n+\n+  // jump when input is not 0\n+  void createJumpNZ(int from_inst, int to_inst) {\n+    auto & inst = instructions[from_inst];\n+    JIT_ASSERT(inst.debug_name == kPlaceholder);\n+    auto offset = relativeJump(from_inst, to_inst);\n+    inst.callback = [offset](Stack & stack) {\n+      auto t = tensor_as<int64_t>(pop(stack));\n+      return (t != 0) ? offset : 0;\n+    };\n+    inst.debug_name = kJumpNZ;\n+  }\n+\n+  void createJump(int from_inst, int to_inst) {\n+    auto & inst = instructions[from_inst];\n+    JIT_ASSERT(inst.debug_name == kPlaceholder);\n+    auto offset = relativeJump(from_inst, to_inst);\n+    inst.callback = [=](Stack & stack) {\n+      return offset;\n+    };\n+    inst.debug_name = kJump;\n+  }\n+\n+  void insertNodesFromBlock(Block* block) {\n+    for(auto node : block->nodes()) {\n+      switch(node->kind()) {\n+        case kIf: {\n+          // x = if c:\n+          //   <then_block>\n+          //   -> (vt)\n+          // else:\n+          //    <else_block>\n+          //   -> (vf)\n+\n+          // turns into:\n+          //   JumpNZ c, then\n+          //   <else_block>\n+          //   x = vf\n+          //   Jump end\n+          // then:\n+          //   <then_block>\n+          //   x = vt\n+          // end:\n+\n+          // kPlaceholder instructions are replaced with branch instructions\n+          // when the branch target locations are known\n+          auto cond_branch = insertInstruction(kPlaceholder, node->inputs(), moveFlags(node), {});\n+          auto then_block = node->blocks()[0];\n+          auto else_block = node->blocks()[1];\n+          insertNodesFromBlock(else_block);\n+          insertAssign(else_block->outputs(), moveFlags(else_block), node->outputs());\n+          auto jump = insertInstruction(kPlaceholder, {}, {}, {});\n+          auto then_block_start = instructions.size();\n+          insertNodesFromBlock(then_block);\n+          insertAssign(then_block->outputs(), moveFlags(then_block), node->outputs());\n+          createJump(jump, instructions.size());\n+          createJumpNZ(cond_branch, then_block_start);\n+        } break;\n+        case kLoop: {\n+          // o0 = while c i0\n+          //        block 0: l0\n+          //          <body>\n+          //          -> (v0, v1)\n+\n+          // turns into:\n+          // l0 = i0\n+          // JumpZ c, end\n+          // begin:\n+          //   <body>\n+          //   c, l0 = v0, v1\n+          //   JumpNZ c, begin\n+          // end:\n+\n+          auto body_block = node->blocks()[0];\n+\n+          // before assign op: stack: ... <cond> <loop-carried-depdencies>\n+          insertAssign(node->inputs(), moveFlags(node), body_block->inputs());\n+          // after assign op: stack: ... <cond>\n+          // cond_branch consumes <cond> from top of the stack\n+          auto cond_branch = insertInstruction(kPlaceholder, {}, {}, {});\n+          // after branch: stack: ...\n+\n+          auto entry = instructions.size();\n+          insertNodesFromBlock(body_block);\n+          // before assign op: stack: ... <cond> <loop-carried-depdencies>\n+          insertAssign(body_block->outputs(), moveFlags(body_block), body_block->inputs());\n+          // after assign op: stack: ... <cond>\n+          auto cond_branch_end = insertInstruction(kPlaceholder, {}, {}, {});\n+          // after branch: stack: ...\n+\n+          aliasRegistersTo(node->outputs(), body_block->inputs());\n+          createJumpZ(cond_branch, instructions.size());\n+          createJumpNZ(cond_branch_end, entry);\n+        } break;\n+        default: {\n+          insertInstruction(node);\n+        } break;\n       }\n-      listBegin(stage.outputs.values);\n-      for(;output_pos < graph->outputs().size(); output_pos++) {\n-        auto output = graph->outputs()[output_pos];\n-        if((int64_t)output->stage() > cur_stage)\n-          break;\n-        listInsert(stage.outputs.values, getOrAllocateRegister(output));\n+      // each stage ends with a load instruction\n+      // we record where these instructions occur, and use them to\n+      // exit the interpreter\n+      if(node->kind() == kLoad) {\n+        stage_end.push_back(instructions.size());\n       }\n     }\n   }\n+\n+  size_t insertInstruction(Node * n) {\n+    auto inst = insertInstruction(n->kind(), n->inputs(), moveFlags(n) , n->outputs());\n+    instructions[inst].callback = getOperation(n, computes_on_variables);\n+    return inst;\n+  }\n+  size_t insertInstruction(Symbol sym,\n+                                 ArrayRef<Value*> inputs,\n+                                 ArrayRef<uint8_t> move_flags,\n+                                 ArrayRef<Value*> outputs) {\n+    instructions.emplace_back();\n+    auto & inst = instructions.back();\n+    inst.debug_name = sym;\n+    listBegin(inst.inputs.values);\n+    listBegin(inst.inputs.free_flags);\n+    auto free_it = move_flags.begin();\n+    for(auto input : inputs) {\n+      listInsert(inst.inputs.values, getOrAllocateRegister(input, true));\n+      listInsert(inst.inputs.free_flags, *free_it++);\n+    }\n+    listBegin(inst.outputs);\n+    for(auto output : outputs) {\n+      listInsert(inst.outputs, getOrAllocateRegister(output));\n+    }\n+    return instructions.size() - 1;\n+  }\n+  ArrayRef<uint8_t> moveFlags(Node * n) {\n+    return preprocess.move_flags.at(n);\n+  }\n+  ArrayRef<uint8_t> moveFlags(Block *b) {\n+    return moveFlags(b->return_node());\n+  }\n+\n+  size_t insertAssign(ArrayRef<Value*> inputs, ArrayRef<uint8_t> move_flags, ArrayRef<Value*> outputs) {\n+    auto inst = insertInstruction(kAssign, inputs, move_flags, outputs);\n+    // like Load/Store Assign does nothing since all actions are encoded\n+    // in inputs/outputs\n+    instructions[inst].callback = [](Stack& stack) { return 0; };\n+    return inst;", "path": "torch/csrc/jit/interpreter.cpp", "position": null, "original_position": 823, "commit_id": "f5f7d01b5b3c2859ffdb99316771529d4a624c22", "original_commit_id": "be682ec37e185e4b57c4b125d715d2d9e86485de", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Why do we even need this instruction then? Couldn't this function be a no-op?", "created_at": "2018-02-22T12:44:56Z", "updated_at": "2018-11-23T15:39:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/5293#discussion_r169943892", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5293", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/169943892"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5293#discussion_r169943892"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5293"}}, "body_html": "<p>Why do we even need this instruction then? Couldn't this function be a no-op?</p>", "body_text": "Why do we even need this instruction then? Couldn't this function be a no-op?"}