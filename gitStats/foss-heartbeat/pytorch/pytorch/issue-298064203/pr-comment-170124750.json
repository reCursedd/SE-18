{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170124750", "pull_request_review_id": 98765476, "id": 170124750, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDEyNDc1MA==", "diff_hunk": "@@ -213,46 +485,83 @@ Operation getOperation(jit::Node *node) {\n     }\n   IR_ELSEIF(FusionGroup)\n     auto fusion_fn = sharedFusionCompiler().getOrCompile(value);\n-    return [fusion_fn](const list_of_retainable & inputs, list_of_retainable & outputs) {\n+    auto num_inputs = value->inputs().size();\n+    return [fusion_fn, num_inputs](Stack & stack) {\n       autograd::profiler::RecordFunction record(\"FusionGroup\");\n-      tensor_list tinputs = unsafeToTensorListShare(inputs);\n-      tensor_list toutputs;\n-      fusion_fn->launch(tinputs, toutputs);\n-      moveToListOfRetainables(std::move(toutputs), outputs);\n+      Stack toutputs;\n+      // TODO: have fusion_fn work off of a stack as well\n+      fusion_fn->launch(last(stack, num_inputs), toutputs);\n+      drop(stack, num_inputs);\n+      stack.insert(stack.end(), toutputs.begin(), toutputs.end());\n+      return 0;\n     };\n   IR_ELSEIF(Constant)\n-    auto t = value->t(kvalue);\n-    return [t](const list_of_retainable & inputs, list_of_retainable & outputs) {\n-      outputs.push_back(toRetainableShare(t));\n-    };\n+    if(computes_on_variables) {\n+      auto t = torch::autograd::make_variable(value->t(kvalue), false);\n+      return [t](Stack & stack) {\n+        stack.push_back(t);\n+        return 0;\n+      };\n+    } else {\n+      auto t = value->t(kvalue);\n+      return [t](Stack & stack) {\n+        stack.push_back(t);\n+        return 0;\n+      };\n+    }\n   IR_ELSEIF(Undefined)\n-    return [](const list_of_retainable & inputs, list_of_retainable & outputs) {\n-      outputs.push_back(toRetainableSteal(at::Tensor()));\n+    return [](Stack & stack) {\n+      stack.push_back(at::Tensor());\n+      return 0;\n     };\n   IR_ELSEIF(ReplaceIfUndef)\n-    return [](const list_of_retainable & inputs, list_of_retainable & outputs) {\n-      auto result = inputs[0];\n-      //TODO: refcounting stuff here is ugly but TensorTemporary is not\n-      //present. Consider whether we\n-      // 1. expose tensor temporary here\n-      // 2. keep as is\n-      // 3. remove all of this retainable stuff anyway since the new\n-      // execution paths do not need handle types.\n-      // Note that list_of_retainable is painful because it is yet another\n-      // list of pointers that require needless copies.\n-      if(result == at::UndefinedTensor::singleton()) {\n-        result = inputs[1];\n+    return [](Stack & stack) {\n+      auto alternate = pop(stack);\n+      auto result = pop(stack);\n+      if(result.defined()) {\n+        stack.push_back(std::move(result));\n+      } else {\n+        stack.push_back(std::move(alternate));\n       }\n-      result->retain();\n-      outputs.push_back(result);\n+      return 0;\n     };\n   IR_ELSEIF(GraphExecutor)\n     GraphExecutor executor(value->g(kSubgraph));\n-    return [=](const list_of_retainable & inputs, list_of_retainable & outputs) mutable {\n+    auto num_inputs = value->inputs().size();\n+    return [=](Stack& stack) mutable {\n       autograd::profiler::RecordFunction record(\"GraphExecutor\");\n-      tensor_list tinputs = unsafeToTensorListShare(inputs);\n+      auto inputs = last(stack, num_inputs);\n+      variable_tensor_list tinputs(Stack(inputs.begin(), inputs.end()));\n+      drop(stack, num_inputs);\n+      //TODO: has graph executor work from a stack as well\n       variable_tensor_list toutputs = executor.run(variable_tensor_list(std::move(tinputs)));\n-      moveToListOfRetainables(std::move(toutputs), outputs);\n+      stack.insert(stack.end(), toutputs.begin(), toutputs.end());\n+      return 0;\n+    };\n+\n+\n+  // Load x, y\n+  // loads values from registers onto the stack, the actual callback does\n+  // nothing since the stack manipulation is already encoded in inst.inputs\n+  // and inst.outputs\n+  IR_ELSEIF(Load)\n+    return [=](Stack& stack) {\n+      return 0;\n+    };\n+\n+  // x, y = Store\n+  // stores values from stack into registers, the actual callback does\n+  // nothing since the stack manipulation is already encoded in inst.inputs\n+  // and inst.outputs\n+  IR_ELSEIF(Store)\n+    return [=](Stack& stack) {\n+      return 0;\n+    };\n+  IR_ELSEIF(Drop)\n+    auto N = value->inputs().size();\n+    return [=](Stack& stack) {\n+      drop(stack, N);\n+      return 0;", "path": "torch/csrc/jit/interpreter.cpp", "position": 579, "original_position": 567, "commit_id": "f5f7d01b5b3c2859ffdb99316771529d4a624c22", "original_commit_id": "be682ec37e185e4b57c4b125d715d2d9e86485de", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "Yes, I've clarified their use in the comment describing preprocessing.", "created_at": "2018-02-22T23:21:36Z", "updated_at": "2018-11-23T15:39:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/5293#discussion_r170124750", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5293", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170124750"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5293#discussion_r170124750"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5293"}}, "body_html": "<p>Yes, I've clarified their use in the comment describing preprocessing.</p>", "body_text": "Yes, I've clarified their use in the comment describing preprocessing.", "in_reply_to_id": 169939935}