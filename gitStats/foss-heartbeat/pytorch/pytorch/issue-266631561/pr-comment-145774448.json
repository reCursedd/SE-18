{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145774448", "pull_request_review_id": 70625338, "id": 145774448, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NTc3NDQ0OA==", "diff_hunk": "@@ -0,0 +1,150 @@\n+\"\"\"\n+The ``distributions`` package contains parameterizable probability distributions\n+and sampling functions.\n+\n+The :meth:`log_prob` method is useful for policy gradient based methods. If the\n+parameters of the distribution are differentiable, then the result of ``log_prob``\n+is also differentiable.\n+\n+Example::\n+\n+    probs = network(input)\n+    m = Multinomial(probs)\n+    action = m.sample()\n+    loss = -m.log_prob(action) * get_reward(env, action)\n+    loss.backward()\n+\"\"\"\n+import math\n+import torch\n+from torch.autograd import Variable\n+\n+\n+__all__ = ['Distribution', 'Bernoulli', 'Multinomial', 'Normal']\n+\n+\n+class Distribution(object):\n+    r\"\"\"\n+    Distribution is the abstract base class for probability distributions.\n+    \"\"\"\n+\n+    def sample(self):\n+        \"\"\"\n+        Generates a single sample or single batch of samples if the distribution\n+        parameters are batched.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def log_prob(self, value):\n+        \"\"\"\n+        Returns the log of the probability density/mass function evaluated at\n+        `value`.\n+\n+        Args:\n+            value (Tensor or Variable):\n+        \"\"\"\n+        raise NotImplementedError\n+\n+\n+class Bernoulli(Distribution):\n+    r\"\"\"\n+    Creates a Bernoulli distribution parameterized by `probs`.\n+\n+    Samples are binary (0 or 1). They take the value `1` with probability `p`\n+    and `0` with probability `1 - p`.\n+\n+    Example::\n+\n+        >>> m = Bernoulli(torch.Tensor([0.3]))\n+        >>> m.sample()  # 30% chance 1; 70% chance 0\n+         0.0\n+        [torch.FloatTensor of size 1]\n+\n+    Args:\n+        probs (Tensor or Variable): the probabilty of sampling `1`\n+    \"\"\"\n+    def __init__(self, probs):\n+        self.probs = probs\n+\n+    def sample(self):\n+        return torch.bernoulli(self.probs)\n+\n+    def log_prob(self, value):\n+        # compute the log probabilities for 0 and 1\n+        log_pmf = (torch.stack([1 - self.probs, self.probs])).log()\n+\n+        # evaluate using the values\n+        return log_pmf.gather(0, value.unsqueeze(0).long()).squeeze(0)\n+\n+\n+class Multinomial(Distribution):\n+    r\"\"\"\n+    Creates a multinomial distribution parameterized by `probs`.\n+\n+    Samples are integers from `0 ... K-1` where `K` is probs.size(-1).\n+\n+    If `probs` is 1D with length-`K`, each element is the relative probability\n+    of sampling the class at that index.\n+\n+    If `probs` is 2D, it is treated as a batch of probability vectors.\n+\n+    See also: :func:`torch.multinomial`\n+\n+    Example::\n+\n+        >>> m = Multinomial(torch.Tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n+        >>> m.sample()  # equal probability of 0, 1, 2, 3\n+         3\n+        [torch.LongTensor of size 1]\n+\n+    Args:\n+        probs (Tensor or Variable): event probabilities\n+    \"\"\"\n+    def __init__(self, probs):\n+        if probs.dim() != 1 and probs.dim() != 2:\n+            # TODO: treat higher dimensions as part of the batch\n+            raise ValueError(\"probs must be 1D or 2D\")\n+        self.probs = probs\n+\n+    def sample(self):\n+        return torch.multinomial(self.probs, 1, True).squeeze(-1)\n+\n+    def log_prob(self, value):\n+        p = self.probs / self.probs.sum(-1, keepdim=True)\n+        if value.dim() == 1 and self.probs.dim() == 1:\n+            # special handling until we have 0-dim tensor support\n+            return p.gather(-1, value).log()\n+\n+        return p.gather(-1, value.unsqueeze(-1)).squeeze(-1).log()\n+\n+\n+class Normal(Distribution):\n+    r\"\"\"\n+    Creates a normal (also called Gaussian) distribution parameterized by\n+    `mean` and `std`.\n+\n+    Example::\n+\n+        >>> m = Normal(torch.Tensor([0.0]), torch.Tensor([1.0]))\n+        >>> m.sample()  # normally distributed with mean=0 and stddev=1\n+         0.1046\n+        [torch.FloatTensor of size 1]\n+\n+    Args:\n+        mean (Tensor or Variable): mean of the distribution\n+        std (Tensor or Variable): standard deviation of the distribution\n+    \"\"\"\n+    def __init__(self, mean, std):\n+        if not torch.is_tensor(mean) and not isinstance(mean, Variable):\n+            raise TypeError('mean must be a Tensor or Variable')\n+        if not torch.is_tensor(std) and not isinstance(std, Variable):\n+            raise TypeError('std must be a Tensor or Variable')", "path": "torch/distributions.py", "position": null, "original_position": 140, "commit_id": "0cc156374bca28adc210ce0f73798bad6c73503b", "original_commit_id": "6cf6a5bf2732e2b243ce6807dd51b5009f56fdc3", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think it makes sense to allow one of them to be a number, just forbid both. E.g. you want your model to generate means and you want to sample with a fixed variance", "created_at": "2017-10-19T17:50:16Z", "updated_at": "2018-11-23T15:35:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/3165#discussion_r145774448", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3165", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/145774448"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3165#discussion_r145774448"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3165"}}, "body_html": "<p>I think it makes sense to allow one of them to be a number, just forbid both. E.g. you want your model to generate means and you want to sample with a fixed variance</p>", "body_text": "I think it makes sense to allow one of them to be a number, just forbid both. E.g. you want your model to generate means and you want to sample with a fixed variance"}