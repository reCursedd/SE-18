{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/341857670", "html_url": "https://github.com/pytorch/pytorch/pull/3165#issuecomment-341857670", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3165", "id": 341857670, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTg1NzY3MA==", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-04T00:27:04Z", "updated_at": "2017-11-04T00:27:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>well, typically, I would think that the policy where we generate the action and the partial loss term could be well separated from where we actually act on the action, and apply the loss/reward?</p>\n<p>For example, policy could be way up here:</p>\n<p><a href=\"https://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L148-L154\">https://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L148-L154</a></p>\n<pre><code>    def forward(self, x, eps=1e-8):\n        x = self.h1(x)\n        x = F.sigmoid(x)\n        m = torch.distributions.Bernoulli(x)\n        a = m.sample()\n        eligibility = m.log_prob(a)\n        a = a.data\n        x = x + eps\n        entropy = - (x * x.log()).sum(1).sum()\n        return eligibility, a, entropy\n</code></pre>\n<p>but action isnt acted on till much later, eg in this case, the action is to finish the game, and calculate the reward, if the terminate action was true:<br>\n<a href=\"https://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L358\">https://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L358</a></p>\n<pre><code>        # calcualate rewards for any that just finished\n        reward_eligible_mask = term_a.view(batch_size).clone().byte()\n</code></pre>\n<p>... which is in a totally different part of the code</p>\n<p>... and then later on we calculate hte loss and do backprop, again in a totally different part of the code, eg<br>\n<a href=\"https://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L519\">https://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L519</a></p>\n<pre><code>loss_by_agent[agent] -= (action * Variable(alive_rewards[:, agent].contiguous().view(_batch_size, 1))).sum()\n</code></pre>\n<p>So... if we multiply out the <code>m.log_prop(a)</code> in the policy, we can pass that bit around, without needing to take the <code>a</code> with it. Similarly, since we're never going to backprop through <code>a</code>, we may as well strip the <code>Variable</code> wrapper, and pass <code>a</code> around as a tensor?</p>", "body_text": "well, typically, I would think that the policy where we generate the action and the partial loss term could be well separated from where we actually act on the action, and apply the loss/reward?\nFor example, policy could be way up here:\nhttps://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L148-L154\n    def forward(self, x, eps=1e-8):\n        x = self.h1(x)\n        x = F.sigmoid(x)\n        m = torch.distributions.Bernoulli(x)\n        a = m.sample()\n        eligibility = m.log_prob(a)\n        a = a.data\n        x = x + eps\n        entropy = - (x * x.log()).sum(1).sum()\n        return eligibility, a, entropy\n\nbut action isnt acted on till much later, eg in this case, the action is to finish the game, and calculate the reward, if the terminate action was true:\nhttps://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L358\n        # calcualate rewards for any that just finished\n        reward_eligible_mask = term_a.view(batch_size).clone().byte()\n\n... which is in a totally different part of the code\n... and then later on we calculate hte loss and do backprop, again in a totally different part of the code, eg\nhttps://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L519\nloss_by_agent[agent] -= (action * Variable(alive_rewards[:, agent].contiguous().view(_batch_size, 1))).sum()\n\nSo... if we multiply out the m.log_prop(a) in the policy, we can pass that bit around, without needing to take the a with it. Similarly, since we're never going to backprop through a, we may as well strip the Variable wrapper, and pass a around as a tensor?", "body": "well, typically, I would think that the policy where we generate the action and the partial loss term could be well separated from where we actually act on the action, and apply the loss/reward?\r\n\r\nFor example, policy could be way up here:\r\n\r\nhttps://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L148-L154\r\n```\r\n    def forward(self, x, eps=1e-8):\r\n        x = self.h1(x)\r\n        x = F.sigmoid(x)\r\n        m = torch.distributions.Bernoulli(x)\r\n        a = m.sample()\r\n        eligibility = m.log_prob(a)\r\n        a = a.data\r\n        x = x + eps\r\n        entropy = - (x * x.log()).sum(1).sum()\r\n        return eligibility, a, entropy\r\n```\r\n\r\nbut action isnt acted on till much later, eg in this case, the action is to finish the game, and calculate the reward, if the terminate action was true:\r\nhttps://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L358\r\n```\r\n        # calcualate rewards for any that just finished\r\n        reward_eligible_mask = term_a.view(batch_size).clone().byte()\r\n```\r\n... which is in a totally different part of the code\r\n\r\n... and then later on we calculate hte loss and do backprop, again in a totally different part of the code, eg\r\nhttps://github.com/ASAPPinc/emergent_comms_negotiation/blob/fb50c3979f240d8eff07db5aef08b4f4f59a3309/ecn.py#L519\r\n```\r\nloss_by_agent[agent] -= (action * Variable(alive_rewards[:, agent].contiguous().view(_batch_size, 1))).sum()\r\n```\r\n\r\nSo... if we multiply out the `m.log_prop(a)` in the policy, we can pass that bit around, without needing to take the `a` with it. Similarly, since we're never going to backprop through `a`, we may as well strip the `Variable` wrapper, and pass `a` around as a tensor?\r\n"}