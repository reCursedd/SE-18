{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6813", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6813/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6813/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6813/events", "html_url": "https://github.com/pytorch/pytorch/issues/6813", "id": 316387532, "node_id": "MDU6SXNzdWUzMTYzODc1MzI=", "number": 6813, "title": "Instance of Device 2 Device transfer non-differentiable", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-20T19:38:07Z", "updated_at": "2018-04-21T00:15:52Z", "closed_at": "2018-04-21T00:15:52Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>).cuda(<span class=\"pl-c1\">0</span>), <span class=\"pl-v\">requires_grad</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\noutput <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.cuda(<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">input</span>.cuda(<span class=\"pl-c1\">1</span>)\noutput.sum().backward()</pre></div>\n<p>is producing the following error:</p>\n<pre lang=\"Traceback\" data-meta=\"(most recent call last):\"><code>  File \"tmp.py\", line 6, in &lt;module&gt;\n    output.sum().backward()\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 89, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: arguments are located on different GPUs at /opt/pytorch/pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:233\n</code></pre>\n<p>however, the following runs fine</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>).cuda(<span class=\"pl-c1\">0</span>), <span class=\"pl-v\">requires_grad</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.cuda(<span class=\"pl-c1\">1</span>)\noutput <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">input</span>\noutput.sum().backward()</pre></div>", "body_text": "import torch\nfrom torch.autograd import Variable\n\ninput = Variable(torch.randn(1).cuda(0), requires_grad = True)\noutput = input.cuda(1) + input.cuda(1)\noutput.sum().backward()\nis producing the following error:\n  File \"tmp.py\", line 6, in <module>\n    output.sum().backward()\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 89, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: arguments are located on different GPUs at /opt/pytorch/pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:233\n\nhowever, the following runs fine\nimport torch\nfrom torch.autograd import Variable\n\ninput = Variable(torch.randn(1).cuda(0), requires_grad = True)\ninput = input.cuda(1)\noutput = input + input\noutput.sum().backward()", "body": "```.py\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ninput = Variable(torch.randn(1).cuda(0), requires_grad = True)\r\noutput = input.cuda(1) + input.cuda(1)\r\noutput.sum().backward()\r\n```\r\n\r\nis producing the following error:\r\n\r\n```Traceback (most recent call last):\r\n  File \"tmp.py\", line 6, in <module>\r\n    output.sum().backward()\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 89, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: arguments are located on different GPUs at /opt/pytorch/pytorch/aten/src/THC/generated/../generic/THCTensorMathPointwise.cu:233\r\n```\r\n\r\nhowever, the following runs fine\r\n\r\n```.py\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ninput = Variable(torch.randn(1).cuda(0), requires_grad = True)\r\ninput = input.cuda(1)\r\noutput = input + input\r\noutput.sum().backward()\r\n```\r\n"}