{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10980", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10980/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10980/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10980/events", "html_url": "https://github.com/pytorch/pytorch/pull/10980", "id": 354927929, "node_id": "MDExOlB1bGxSZXF1ZXN0MjExNTc3MjA1", "number": 10980, "title": "[Ready] Vectorize grid sample 2d CPU kernels", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 559719279, "node_id": "MDU6TGFiZWw1NTk3MTkyNzk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/ready%20for%20review", "name": "ready for review", "color": "b60205", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-08-28T22:29:39Z", "updated_at": "2018-11-23T15:51:27Z", "closed_at": "2018-09-17T03:42:15Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10980", "html_url": "https://github.com/pytorch/pytorch/pull/10980", "diff_url": "https://github.com/pytorch/pytorch/pull/10980.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10980.patch"}, "body_html": "<p>This PR vectorizes the CPU grid sample 2d forward and backward kernels. Specifically,</p>\n<ol>\n<li>\n<p>add <code>.data()</code> in <code>TensorAccessor</code></p>\n</li>\n<li>\n<p>support non-void return value for declaring CPU kernel stub</p>\n</li>\n<li>\n<p>add <code>bool at:: geometry_is_contiguous(IntList sizes, IntList strides)</code></p>\n</li>\n<li>\n<p>The following vectorized CPU primitives are added:</p>\n<ul>\n<li><code>gather&lt;scale&gt;(baseaddr, vindex)</code>: <code>result[i] = baseaddr[vindex[i] * scale]</code></li>\n<li><code>mask_gather&lt;scale&gt;(src, baseaddr, vindex, mask)</code>: <code>result[i] = mask[i] ? baseaddr[vindex[i] * scale] : src[i]</code>.</li>\n<li>comparison ops</li>\n<li>binary logical ops</li>\n<li><code>min(a, b)</code></li>\n<li><code>cast&lt;dst_t, src_t&gt;(src_vec)</code>: changing dtype but keeping the bit representation</li>\n<li><code>blendv(a, b, mask)</code>: <code>result[i] = mask[i] ? b[i] : a[i]</code>.</li>\n<li>ctor with multiple values (i.e., <code>setr</code>)</li>\n<li><code>arange(start = 0, step = 1)</code>: constructs a vector with values specified by the arange parameters</li>\n<li><code>convert_to_int_of_same_size(vec)</code>: convert floating point vector to corresponding integral type of same size</li>\n<li><code>interleave2(a, b)</code> &amp; <code>deinterleave2(x, y)</code>: interleave or deinterleaves two vectors. E.g., for <code>interleave</code>:\n<pre><code>inputs:\n  {a0, a1, a2, a3, a4, a5, a6, a7}\n  {b0, b1, b2, b3, b4, b5, b6, b7}\noutputs:\n  {a0, b0, a1, b1, a2, b2, a3, b3}\n  {a4, b4, a5, b5, a6, b6, a7, b7}\n</code></pre>\n</li>\n</ul>\n</li>\n<li>\n<p>Grid sample CPU kernel implementations are described in the following note (also in <code>GridSampleKernel.cpp</code>:</p>\n</li>\n</ol>\n<pre><code> NOTE [ Grid Sample CPU Kernels ]\n\n Implementation of vectorized grid sample CPU kernels is divided into three\n parts:\n\n 1. `ComputeLocation` struct\n    Transforms grid values into interpolation locations of the input tensor\n    for a particular spatial dimension, basing on the size of that dimension\n    in input tensor, and the padding mode.\n</code></pre>\n<div class=\"highlight highlight-source-c++\"><pre>      <span class=\"pl-k\">template</span>&lt;<span class=\"pl-k\">typename</span> <span class=\"pl-c1\">scalar_t</span>, GridSamplerPadding padding&gt;\n      <span class=\"pl-k\">struct</span> <span class=\"pl-en\">ComputeLocation</span> {\n        <span class=\"pl-k\">using</span> Vec = Vec256&lt;<span class=\"pl-c1\">scalar_t</span>&gt;;\n  \n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> ctor</span>\n        <span class=\"pl-en\">ComputeLocation</span>(<span class=\"pl-c1\">int64_t</span> size);\n  \n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> Given grid values `in`, return the interpolation locations after</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> un-normalization and padding mechanism (elementwise).</span>\n        Vec <span class=\"pl-en\">apply</span>(<span class=\"pl-k\">const</span> Vec &amp;in) <span class=\"pl-k\">const</span>;\n  \n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> Similar to `apply`, but also returns `d apply(in) / d in`</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> (elementwise).</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> this is often used in gradient computation.</span>\n        std::pair&lt;Vec, Vec&gt; <span class=\"pl-en\">apply_get_grad</span>(<span class=\"pl-k\">const</span> Vec &amp;in) <span class=\"pl-k\">const</span>;\n      };</pre></div>\n<pre><code>   2. `ApplyGridSample` struct\n      Owns N `ComputeLocation` structs, where N is the number of spatial\n      dimensions. Given N input grid vectors (one for each spatial dimension)\n      and spatial offset, it gets the interpolation locations from\n      `ComputeLocation`s, applies interpolation procedure, and then writes to\n      the output (or grad_input &amp; grad_grid in backward).\n</code></pre>\n<div class=\"highlight highlight-source-c++\"><pre>      <span class=\"pl-k\">template</span>&lt;<span class=\"pl-k\">typename</span> <span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-k\">int</span> spatial_dim,\n               GridSamplerInterpolation interp,\n               GridSamplerPadding padding&gt;\n      <span class=\"pl-k\">struct</span> <span class=\"pl-en\">ApplyGridSample</span> {\n  \n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> ctor</span>\n        <span class=\"pl-en\">ApplyGridSample</span>(<span class=\"pl-k\">const</span> TensorAccessor&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">4</span>&gt;&amp; input);\n  \n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> Applies grid sampling (forward) procedure:</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span>   1. computes interpolation locations from grid values `grid_x` and</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span>      `grid_y`,</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span>   2. interpolates output values using the locations and input data</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span>      in `inp_slice`, and</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span>   3. writes the first `len` values in the interpolated vector to</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span>      `out_slice` with spatial offset being `offset`.</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span></span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> This assimes that `grid_x` and `grid_y` all contain valid grid</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> values \\in [-1, 1], even at indices greater than `len`.</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span></span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> The `*_slice` argument namess mean samples within a batch (i.e.,</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> with the batch dimension sliced out).</span>\n        <span class=\"pl-k\">void</span> <span class=\"pl-en\">forward</span>(TensorAccessor&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">3</span>&gt;&amp; out_slice,\n                     <span class=\"pl-k\">const</span> TensorAccessor&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">3</span>&gt;&amp; inp_slice,\n                     <span class=\"pl-c1\">int64_t</span> offset, <span class=\"pl-k\">const</span> Vec&amp; grid_x, <span class=\"pl-k\">const</span> Vec&amp; grid_y,\n                     <span class=\"pl-c1\">int64_t</span> len) <span class=\"pl-k\">const</span>;\n  \n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> Applies grid sampling (backward) procedure. Arguments semantics</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">//</span> and strategy are similar to those of `forward`.</span>\n        <span class=\"pl-k\">void</span> <span class=\"pl-en\">backward</span>(TensorAccessor&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">3</span>&gt;&amp; <span class=\"pl-smi\">gInp_slice</span>,\n                      TensorAccessor&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">3</span>&gt;&amp; <span class=\"pl-smi\">gGrid_slice</span>,\n                      <span class=\"pl-k\">const</span> TensorAccessor&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">3</span>&gt;&amp; <span class=\"pl-smi\">gOut_slice</span>,\n                      <span class=\"pl-k\">const</span> TensorAccessor&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">3</span>&gt;&amp; inp_slice,\n                      <span class=\"pl-c1\">int64_t</span> offset, <span class=\"pl-k\">const</span> Vec&amp; grid_x, <span class=\"pl-k\">const</span> Vec&amp; grid_y,\n                      <span class=\"pl-c1\">int64_t</span> len) <span class=\"pl-k\">const</span>;\n      }<span class=\"pl-ii\"></span></pre></div>\n<pre><code>   3. `grid_sample_2d_grid_slice_iterator` function\n      Among the tensors we work with, we know that the output tensors are\n      contiguous (i.e., `output` in forward, and `grad_input` &amp; `grad_grid` in\n      backward), we need to randomly read `input` anyways, and `grad_output`\n      usually comes from autograd and is often contiguous. So we base our\n      iterating strategy on the geometry of grid.\n      `grid_sample_2d_grid_slice_iterator` function provides an abstract to\n      efficiently iterates through a `grid` slice (without batch dimension).\n      See comments of that function on the specific cases and strategies used.\n</code></pre>\n<div class=\"highlight highlight-source-c++\"><pre>      <span class=\"pl-k\">template</span>&lt;<span class=\"pl-k\">typename</span> <span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-k\">typename</span> ApplyFn&gt;\n      <span class=\"pl-k\">void</span> <span class=\"pl-en\">grid_sample_2d_grid_slice_iterator</span>(\n        <span class=\"pl-k\">const</span> TensorAccessor&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">3</span>&gt;&amp; grid_slice,\n        <span class=\"pl-k\">const</span> ApplyFn &amp;apply_fn);\n\n      <span class=\"pl-c\"><span class=\"pl-c\">//</span> `apply_fn` is a function/lambda that can be called as if it has</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">//</span> declaration:</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">//</span>   void apply_fn(const Vec256&lt;scalar_t&gt;&amp; grid_x,</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">//</span>                 const Vec256&lt;scalar_t&gt;&amp; grid_y,</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">//</span>                 int64_t spatial_offset, int64_t len);</span></pre></div>\n<pre><code>      `apply_fn` will be called multiple times, and together cover the entire\n      output spatial space. Therefore, e.g., to implement forward 2d grid\n      sample, we can do\n</code></pre>\n<div class=\"highlight highlight-source-c++\"><pre>      ApplyGridSample&lt;<span class=\"pl-c1\">scalar_t</span>, <span class=\"pl-c1\">2</span>, interp, padding&gt; <span class=\"pl-en\">grid_sample</span>(input_accessor);\n  \n      <span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> n = <span class=\"pl-c1\">0</span>; n &lt; input_accessor.size(<span class=\"pl-c1\">0</span>); n++) {\n        <span class=\"pl-c1\">grid_sample_2d_grid_slice_iterator</span>(\n          grid_accessor[n],\n          [&amp;](<span class=\"pl-k\">const</span> Vec256&lt;<span class=\"pl-c1\">scalar_t</span>&gt;&amp; grid_x, <span class=\"pl-k\">const</span> Vec256&lt;<span class=\"pl-c1\">scalar_t</span>&gt;&amp; grid_y,\n              <span class=\"pl-c1\">int64_t</span> spatial_offset, <span class=\"pl-c1\">int64_t</span> len) {\n            grid_sample.<span class=\"pl-c1\">forward</span>(out_accessor[n], input_accessor[n],\n                                spatial_offset, grid_x, grid_y, len);\n          });\n      }</pre></div>", "body_text": "This PR vectorizes the CPU grid sample 2d forward and backward kernels. Specifically,\n\n\nadd .data() in TensorAccessor\n\n\nsupport non-void return value for declaring CPU kernel stub\n\n\nadd bool at:: geometry_is_contiguous(IntList sizes, IntList strides)\n\n\nThe following vectorized CPU primitives are added:\n\ngather<scale>(baseaddr, vindex): result[i] = baseaddr[vindex[i] * scale]\nmask_gather<scale>(src, baseaddr, vindex, mask): result[i] = mask[i] ? baseaddr[vindex[i] * scale] : src[i].\ncomparison ops\nbinary logical ops\nmin(a, b)\ncast<dst_t, src_t>(src_vec): changing dtype but keeping the bit representation\nblendv(a, b, mask): result[i] = mask[i] ? b[i] : a[i].\nctor with multiple values (i.e., setr)\narange(start = 0, step = 1): constructs a vector with values specified by the arange parameters\nconvert_to_int_of_same_size(vec): convert floating point vector to corresponding integral type of same size\ninterleave2(a, b) & deinterleave2(x, y): interleave or deinterleaves two vectors. E.g., for interleave:\ninputs:\n  {a0, a1, a2, a3, a4, a5, a6, a7}\n  {b0, b1, b2, b3, b4, b5, b6, b7}\noutputs:\n  {a0, b0, a1, b1, a2, b2, a3, b3}\n  {a4, b4, a5, b5, a6, b6, a7, b7}\n\n\n\n\n\nGrid sample CPU kernel implementations are described in the following note (also in GridSampleKernel.cpp:\n\n\n NOTE [ Grid Sample CPU Kernels ]\n\n Implementation of vectorized grid sample CPU kernels is divided into three\n parts:\n\n 1. `ComputeLocation` struct\n    Transforms grid values into interpolation locations of the input tensor\n    for a particular spatial dimension, basing on the size of that dimension\n    in input tensor, and the padding mode.\n\n      template<typename scalar_t, GridSamplerPadding padding>\n      struct ComputeLocation {\n        using Vec = Vec256<scalar_t>;\n  \n        // ctor\n        ComputeLocation(int64_t size);\n  \n        // Given grid values `in`, return the interpolation locations after\n        // un-normalization and padding mechanism (elementwise).\n        Vec apply(const Vec &in) const;\n  \n        // Similar to `apply`, but also returns `d apply(in) / d in`\n        // (elementwise).\n        // this is often used in gradient computation.\n        std::pair<Vec, Vec> apply_get_grad(const Vec &in) const;\n      };\n   2. `ApplyGridSample` struct\n      Owns N `ComputeLocation` structs, where N is the number of spatial\n      dimensions. Given N input grid vectors (one for each spatial dimension)\n      and spatial offset, it gets the interpolation locations from\n      `ComputeLocation`s, applies interpolation procedure, and then writes to\n      the output (or grad_input & grad_grid in backward).\n\n      template<typename scalar_t, int spatial_dim,\n               GridSamplerInterpolation interp,\n               GridSamplerPadding padding>\n      struct ApplyGridSample {\n  \n        // ctor\n        ApplyGridSample(const TensorAccessor<scalar_t, 4>& input);\n  \n        // Applies grid sampling (forward) procedure:\n        //   1. computes interpolation locations from grid values `grid_x` and\n        //      `grid_y`,\n        //   2. interpolates output values using the locations and input data\n        //      in `inp_slice`, and\n        //   3. writes the first `len` values in the interpolated vector to\n        //      `out_slice` with spatial offset being `offset`.\n        //\n        // This assimes that `grid_x` and `grid_y` all contain valid grid\n        // values \\in [-1, 1], even at indices greater than `len`.\n        //\n        // The `*_slice` argument namess mean samples within a batch (i.e.,\n        // with the batch dimension sliced out).\n        void forward(TensorAccessor<scalar_t, 3>& out_slice,\n                     const TensorAccessor<scalar_t, 3>& inp_slice,\n                     int64_t offset, const Vec& grid_x, const Vec& grid_y,\n                     int64_t len) const;\n  \n        // Applies grid sampling (backward) procedure. Arguments semantics\n        // and strategy are similar to those of `forward`.\n        void backward(TensorAccessor<scalar_t, 3>& gInp_slice,\n                      TensorAccessor<scalar_t, 3>& gGrid_slice,\n                      const TensorAccessor<scalar_t, 3>& gOut_slice,\n                      const TensorAccessor<scalar_t, 3>& inp_slice,\n                      int64_t offset, const Vec& grid_x, const Vec& grid_y,\n                      int64_t len) const;\n      }\n   3. `grid_sample_2d_grid_slice_iterator` function\n      Among the tensors we work with, we know that the output tensors are\n      contiguous (i.e., `output` in forward, and `grad_input` & `grad_grid` in\n      backward), we need to randomly read `input` anyways, and `grad_output`\n      usually comes from autograd and is often contiguous. So we base our\n      iterating strategy on the geometry of grid.\n      `grid_sample_2d_grid_slice_iterator` function provides an abstract to\n      efficiently iterates through a `grid` slice (without batch dimension).\n      See comments of that function on the specific cases and strategies used.\n\n      template<typename scalar_t, typename ApplyFn>\n      void grid_sample_2d_grid_slice_iterator(\n        const TensorAccessor<scalar_t, 3>& grid_slice,\n        const ApplyFn &apply_fn);\n\n      // `apply_fn` is a function/lambda that can be called as if it has\n      // declaration:\n      //   void apply_fn(const Vec256<scalar_t>& grid_x,\n      //                 const Vec256<scalar_t>& grid_y,\n      //                 int64_t spatial_offset, int64_t len);\n      `apply_fn` will be called multiple times, and together cover the entire\n      output spatial space. Therefore, e.g., to implement forward 2d grid\n      sample, we can do\n\n      ApplyGridSample<scalar_t, 2, interp, padding> grid_sample(input_accessor);\n  \n      for (int n = 0; n < input_accessor.size(0); n++) {\n        grid_sample_2d_grid_slice_iterator(\n          grid_accessor[n],\n          [&](const Vec256<scalar_t>& grid_x, const Vec256<scalar_t>& grid_y,\n              int64_t spatial_offset, int64_t len) {\n            grid_sample.forward(out_accessor[n], input_accessor[n],\n                                spatial_offset, grid_x, grid_y, len);\n          });\n      }", "body": "This PR vectorizes the CPU grid sample 2d forward and backward kernels. Specifically, \r\n\r\n\r\n\r\n 1. add `.data()` in `TensorAccessor`\r\n 2. support non-void return value for declaring CPU kernel stub\r\n 2. add `bool at:: geometry_is_contiguous(IntList sizes, IntList strides)`\r\n1. The following vectorized CPU primitives are added: \r\n\r\n    + `gather<scale>(baseaddr, vindex)`: `result[i] = baseaddr[vindex[i] * scale]`\r\n    + `mask_gather<scale>(src, baseaddr, vindex, mask)`: `result[i] = mask[i] ? baseaddr[vindex[i] * scale] : src[i]`. \r\n    + comparison ops\r\n    + binary logical ops\r\n    + `min(a, b)`\r\n    + `cast<dst_t, src_t>(src_vec)`: changing dtype but keeping the bit representation\r\n    + `blendv(a, b, mask)`: `result[i] = mask[i] ? b[i] : a[i]`.\r\n    + ctor with multiple values (i.e., `setr`)\r\n    + `arange(start = 0, step = 1)`: constructs a vector with values specified by the arange parameters\r\n    + `convert_to_int_of_same_size(vec)`: convert floating point vector to corresponding integral type of same size\r\n    + `interleave2(a, b)` & `deinterleave2(x, y)`: interleave or deinterleaves two vectors. E.g., for `interleave`:\r\n        ```\r\n        inputs:\r\n          {a0, a1, a2, a3, a4, a5, a6, a7}\r\n          {b0, b1, b2, b3, b4, b5, b6, b7}\r\n        outputs:\r\n          {a0, b0, a1, b1, a2, b2, a3, b3}\r\n          {a4, b4, a5, b5, a6, b6, a7, b7}\r\n        ```\r\n\r\n  2. Grid sample CPU kernel implementations are described in the following note (also in `GridSampleKernel.cpp`:\r\n\r\n  ```\r\n   NOTE [ Grid Sample CPU Kernels ]\r\n  \r\n   Implementation of vectorized grid sample CPU kernels is divided into three\r\n   parts:\r\n  \r\n   1. `ComputeLocation` struct\r\n      Transforms grid values into interpolation locations of the input tensor\r\n      for a particular spatial dimension, basing on the size of that dimension\r\n      in input tensor, and the padding mode.\r\n```\r\n```cpp\r\n      template<typename scalar_t, GridSamplerPadding padding>\r\n      struct ComputeLocation {\r\n        using Vec = Vec256<scalar_t>;\r\n  \r\n        // ctor\r\n        ComputeLocation(int64_t size);\r\n  \r\n        // Given grid values `in`, return the interpolation locations after\r\n        // un-normalization and padding mechanism (elementwise).\r\n        Vec apply(const Vec &in) const;\r\n  \r\n        // Similar to `apply`, but also returns `d apply(in) / d in`\r\n        // (elementwise).\r\n        // this is often used in gradient computation.\r\n        std::pair<Vec, Vec> apply_get_grad(const Vec &in) const;\r\n      };\r\n```\r\n```\r\n   2. `ApplyGridSample` struct\r\n      Owns N `ComputeLocation` structs, where N is the number of spatial\r\n      dimensions. Given N input grid vectors (one for each spatial dimension)\r\n      and spatial offset, it gets the interpolation locations from\r\n      `ComputeLocation`s, applies interpolation procedure, and then writes to\r\n      the output (or grad_input & grad_grid in backward).\r\n```\r\n```cpp\r\n      template<typename scalar_t, int spatial_dim,\r\n               GridSamplerInterpolation interp,\r\n               GridSamplerPadding padding>\r\n      struct ApplyGridSample {\r\n  \r\n        // ctor\r\n        ApplyGridSample(const TensorAccessor<scalar_t, 4>& input);\r\n  \r\n        // Applies grid sampling (forward) procedure:\r\n        //   1. computes interpolation locations from grid values `grid_x` and\r\n        //      `grid_y`,\r\n        //   2. interpolates output values using the locations and input data\r\n        //      in `inp_slice`, and\r\n        //   3. writes the first `len` values in the interpolated vector to\r\n        //      `out_slice` with spatial offset being `offset`.\r\n        //\r\n        // This assimes that `grid_x` and `grid_y` all contain valid grid\r\n        // values \\in [-1, 1], even at indices greater than `len`.\r\n        //\r\n        // The `*_slice` argument namess mean samples within a batch (i.e.,\r\n        // with the batch dimension sliced out).\r\n        void forward(TensorAccessor<scalar_t, 3>& out_slice,\r\n                     const TensorAccessor<scalar_t, 3>& inp_slice,\r\n                     int64_t offset, const Vec& grid_x, const Vec& grid_y,\r\n                     int64_t len) const;\r\n  \r\n        // Applies grid sampling (backward) procedure. Arguments semantics\r\n        // and strategy are similar to those of `forward`.\r\n        void backward(TensorAccessor<scalar_t, 3>& gInp_slice,\r\n                      TensorAccessor<scalar_t, 3>& gGrid_slice,\r\n                      const TensorAccessor<scalar_t, 3>& gOut_slice,\r\n                      const TensorAccessor<scalar_t, 3>& inp_slice,\r\n                      int64_t offset, const Vec& grid_x, const Vec& grid_y,\r\n                      int64_t len) const;\r\n      }\r\n```\r\n```\r\n   3. `grid_sample_2d_grid_slice_iterator` function\r\n      Among the tensors we work with, we know that the output tensors are\r\n      contiguous (i.e., `output` in forward, and `grad_input` & `grad_grid` in\r\n      backward), we need to randomly read `input` anyways, and `grad_output`\r\n      usually comes from autograd and is often contiguous. So we base our\r\n      iterating strategy on the geometry of grid.\r\n      `grid_sample_2d_grid_slice_iterator` function provides an abstract to\r\n      efficiently iterates through a `grid` slice (without batch dimension).\r\n      See comments of that function on the specific cases and strategies used.\r\n```\r\n```cpp\r\n      template<typename scalar_t, typename ApplyFn>\r\n      void grid_sample_2d_grid_slice_iterator(\r\n        const TensorAccessor<scalar_t, 3>& grid_slice,\r\n        const ApplyFn &apply_fn);\r\n\r\n      // `apply_fn` is a function/lambda that can be called as if it has\r\n      // declaration:\r\n      //   void apply_fn(const Vec256<scalar_t>& grid_x,\r\n      //                 const Vec256<scalar_t>& grid_y,\r\n      //                 int64_t spatial_offset, int64_t len);\r\n```\r\n```\r\n      `apply_fn` will be called multiple times, and together cover the entire\r\n      output spatial space. Therefore, e.g., to implement forward 2d grid\r\n      sample, we can do\r\n```\r\n```cpp\r\n      ApplyGridSample<scalar_t, 2, interp, padding> grid_sample(input_accessor);\r\n  \r\n      for (int n = 0; n < input_accessor.size(0); n++) {\r\n        grid_sample_2d_grid_slice_iterator(\r\n          grid_accessor[n],\r\n          [&](const Vec256<scalar_t>& grid_x, const Vec256<scalar_t>& grid_y,\r\n              int64_t spatial_offset, int64_t len) {\r\n            grid_sample.forward(out_accessor[n], input_accessor[n],\r\n                                spatial_offset, grid_x, grid_y, len);\r\n          });\r\n      }\r\n   ```"}