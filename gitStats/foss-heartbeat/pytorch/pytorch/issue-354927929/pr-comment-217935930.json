{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217935930", "pull_request_review_id": 155766376, "id": 217935930, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzkzNTkzMA==", "diff_hunk": "@@ -0,0 +1,890 @@\n+#include <ATen/ATen.h>\n+#include <ATen/Dispatch.h>\n+#include <ATen/Parallel.h>\n+#include <ATen/core/C++17.h>\n+#include <ATen/TensorUtils.h>\n+#include <ATen/NativeFunctions.h>\n+#include <ATen/native/GridSampler.h>\n+#include <ATen/native/cpu/GridSamplerKernel.h>\n+#include <ATen/cpu/vml.h>\n+\n+#ifdef _OPENMP\n+#include <omp.h>\n+#endif\n+\n+#include <algorithm>\n+#include <cstring>\n+#include <type_traits>\n+\n+namespace at { namespace native { namespace {\n+\n+/**  NOTE [ Grid Sample CPU Kernels ]\n+ *\n+ *   Implementation of vectorized grid sample CPU kernels is divided into three\n+ *   parts. More detailed description exist after this paragraph, but on a high\n+ *   level, they are\n+ *   1. `ComputeLocation` struct\n+ *      + Computes the interpolation location basing on padding mode.\n+ *   2. `ApplyGridSample` struct\n+ *      + Owns N (# spatial dims) `ComputeLocation` structs, and uses them to\n+ *        compute the interpolation locations.\n+ *      + Interpolates the values and writes to output.\n+ *   3. `grid_sample_2d_grid_slice_iterator` function\n+ *      + Iterates over a slice of the grid tensor based on the geometry by the\n+ *        spatial ordering, i.e., the first iteration will process grid values\n+ *           grid[n, 0, 0, :], grid[n, 0, 1, :], grid[n, 0, 2, :], ...\n+ *        (Recall that, e.g., 2D grid has shape [N x H x W x 2], so grid[n, ...]\n+ *         is a slice, and grid[n, h, w, :] contains the values for a single\n+ *         output spatial location.)\n+ *      + Applies a given operator at each iteration, so we can use the same\n+ *        pattern for forward and backward.\n+ *\n+ *   Putting everything together, we have, e.g., the forward kernel implemented\n+ *   as\n+ *\n+ *      // `ApplyGridSample` struct that processes grid values, extracts and\n+ *      // interpolates input values, and write to output.\n+ *      ApplyGridSample<scalar_t, 2, interp, padding> grid_sample(input_accessor);\n+ *\n+ *      // For each slice, we call `grid_sample_2d_grid_slice_iterator` with\n+ *      //   1. the grid slice, and\n+ *      //   2. a lambda that takes in\n+ *      //      i.   location vectors (x and y for 2D) extracted from grid\n+ *      //      ii.  `spatial_offset` as the spatial offset of these vectors\n+ *      //           from the beginning of this slice.\n+ *      //      iii. `len` as the number of valid locations in the vectors.\n+ *      //           (There might not be enough near boundary.)\n+ *      for (int n = 0; n < input_accessor.size(0); n++) {\n+ *        grid_sample_2d_grid_slice_iterator(\n+ *          grid_accessor[n],\n+ *          [&](const Vec256<scalar_t>& grid_x,\n+ *              const Vec256<scalar_t>& grid_y,\n+ *              int64_t spatial_offset, int64_t len) {\n+ *            grid_sample.forward(out_accessor[n], input_accessor[n],\n+ *                                spatial_offset, grid_x, grid_y, len);\n+ *          });\n+ *      }\n+ *\n+ *   Now we talk about details of each of these three parts:\n+ *\n+ *   1. `ComputeLocation` struct\n+ *      Transforms grid values into interpolation locations of the input tensor\n+ *      for a particular spatial dimension, based on the size of that dimension\n+ *      in input tensor, and the padding mode.\n+ *\n+ *        template<typename scalar_t, GridSamplerPadding padding>\n+ *        struct ComputeLocation {\n+ *          using Vec = Vec256<scalar_t>;\n+ *\n+ *          // ctor\n+ *          ComputeLocation(int64_t size);\n+ *\n+ *          // Given grid values `in`, return the interpolation locations after\n+ *          // un-normalization and padding mechanism (elementwise).\n+ *          Vec apply(const Vec &in) const;\n+ *\n+ *          // Similar to `apply`, but also returns `d apply(in) / d in`\n+ *          // (elementwise).\n+ *          // this is often used in gradient computation.\n+ *          std::pair<Vec, Vec> apply_get_grad(const Vec &in) const;\n+ *        };\n+ *\n+ *   2. `ApplyGridSample` struct\n+ *      Owns N `ComputeLocation` structs, where N is the number of spatial\n+ *      dimensions. Given N input grid vectors (one for each spatial dimension)\n+ *      and spatial offset, it gets the interpolation locations from\n+ *      `ComputeLocation`s, applies interpolation procedure, and then writes to\n+ *      the output (or grad_input & grad_grid in backward).\n+ *\n+ *        template<typename scalar_t, int spatial_dim,\n+ *                 GridSamplerInterpolation interp,\n+ *                 GridSamplerPadding padding>\n+ *        struct ApplyGridSample {\n+ *\n+ *          // ctor\n+ *          ApplyGridSample(const TensorAccessor<scalar_t, 4>& input);\n+ *\n+ *          // Applies grid sampling (forward) procedure:\n+ *          //   1. computes interpolation locations from grid values `grid_x`\n+ *          //      and `grid_y`,\n+ *          //   2. interpolates output values using the locations and input\n+ *          //      data in `inp_slice`, and\n+ *          //   3. writes the first `len` values in the interpolated vector to\n+ *          //      `out_slice` with spatial offset being `offset`.\n+ *          //\n+ *          // This assimes that `grid_x` and `grid_y` all contain valid grid\n+ *          // values \\in [-1, 1], even at indices greater than `len`.\n+ *          //\n+ *          // The `*_slice` argument namess mean samples within a batch (i.e.,\n+ *          // with the batch dimension sliced out).\n+ *          void forward(TensorAccessor<scalar_t, 3>& out_slice,\n+ *                       const TensorAccessor<scalar_t, 3>& inp_slice,\n+ *                       int64_t offset, const Vec& grid_x, const Vec& grid_y,\n+ *                       int64_t len) const;\n+ *\n+ *          // Applies grid sampling (backward) procedure. Arguments semantics\n+ *          // and strategy are similar to those of `forward`.\n+ *          void backward(TensorAccessor<scalar_t, 3>& gInp_slice,\n+ *                        TensorAccessor<scalar_t, 3>& gGrid_slice,\n+ *                        const TensorAccessor<scalar_t, 3>& gOut_slice,\n+ *                        const TensorAccessor<scalar_t, 3>& inp_slice,\n+ *                        int64_t offset, const Vec& grid_x, const Vec& grid_y,\n+ *                        int64_t len) const;\n+ *        };\n+ *\n+ *   3. `grid_sample_2d_grid_slice_iterator` function\n+ *      Among the tensors we work with, we know that the output tensors are\n+ *      contiguous (i.e., `output` in forward, and `grad_input` & `grad_grid` in\n+ *      backward), we need to randomly read `input` anyways, and `grad_output`\n+ *      usually comes from autograd and is often contiguous. So we base our\n+ *      iterating strategy on the geometry of grid.\n+ *      `grid_sample_2d_grid_slice_iterator` function provides an abstraction to\n+ *      efficiently iterates through a `grid` slice (without batch dimension).\n+ *      See comments of that function on the specific cases and strategies used.\n+ *\n+ *        template<typename scalar_t, typename ApplyFn>\n+ *        void grid_sample_2d_grid_slice_iterator(\n+ *          const TensorAccessor<scalar_t, 3>& grid_slice,\n+ *          const ApplyFn &apply_fn);\n+ *\n+ *      `apply_fn` is a function/lambda that takes in\n+ *           i.   location vectors (x and y for 2D) extracted from grid\n+ *           ii.  `spatial_offset` as the spatial offset of these vectors\n+ *                from the beginning of this slice.\n+ *           iii. `len` as the number of valid locations in the vectors.\n+ *                (There might not be enough near boundary.)\n+\n+ *       It should be callable as if it has declaration:\n+ *          void apply_fn(const Vec256<scalar_t>& grid_x,\n+ *                        const Vec256<scalar_t>& grid_y,\n+ *                        int64_t spatial_offset, int64_t len);\n+ *\n+ *      `apply_fn` will be called multiple times, and together cover the entire\n+ *      output spatial space.\n+ *\n+ *  Now you should be able tp understand everything about the implementaion of\n+ *  2D forward kernel shown at the beginning of this note.\n+ *\n+ **/\n+\n+\n+using at::native::detail::GridSamplerInterpolation;\n+using at::native::detail::GridSamplerPadding;\n+using namespace at::vec256;\n+\n+\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ComputeLocation ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+// Struct to compute interpolation location from grid values, and to apply\n+// padding mechanism (e.g., reflection).\n+// See NOTE [ Grid Sample CPU Kernels ] for details.\n+\n+template<typename scalar_t>\n+struct ComputeLocationBase {\n+  using Vec = Vec256<scalar_t>;\n+\n+  const scalar_t half_max_val;\n+\n+  ComputeLocationBase(int64_t size)\n+    : half_max_val(static_cast<scalar_t>(size - 1) / 2) {}\n+\n+  inline Vec unnormalize(const Vec &in) const {\n+    return (in + Vec(1)) * Vec(half_max_val);\n+  }\n+};\n+\n+template<typename scalar_t, GridSamplerPadding padding>\n+struct ComputeLocation;\n+\n+template<typename scalar_t>\n+struct ComputeLocation<scalar_t, GridSamplerPadding::Zeros>\n+  : ComputeLocationBase<scalar_t> {\n+  using Vec = Vec256<scalar_t>;\n+  using ComputeLocationBase<scalar_t>::unnormalize;\n+  using ComputeLocationBase<scalar_t>::half_max_val;\n+\n+  using ComputeLocationBase<scalar_t>::ComputeLocationBase;\n+\n+  inline Vec apply(const Vec &in) const {\n+    return unnormalize(in);\n+  }\n+\n+  inline std::pair<Vec, Vec> apply_get_grad(const Vec &in) const {\n+    return std::make_pair(unnormalize(in), Vec(half_max_val));\n+  }\n+};\n+\n+template<typename scalar_t>\n+struct ComputeLocation<scalar_t, GridSamplerPadding::Border>\n+  : ComputeLocationBase<scalar_t> {\n+  using Vec = Vec256<scalar_t>;\n+  using ComputeLocationBase<scalar_t>::unnormalize;\n+  using ComputeLocationBase<scalar_t>::half_max_val;\n+\n+  const scalar_t max_val;\n+\n+  ComputeLocation(int64_t size)\n+    : ComputeLocationBase<scalar_t>(size)\n+    , max_val(static_cast<scalar_t>(size - 1)) {}\n+\n+  inline Vec apply(const Vec &in) const {\n+    return min(Vec(max_val), max(unnormalize(in), Vec(0)));\n+  }\n+  inline std::pair<Vec, Vec> apply_get_grad(const Vec &in) const {\n+    using int_t = int_same_size_t<scalar_t>;\n+    Vec max_val_vec(max_val), zeros(0);\n+    auto indices = unnormalize(in);\n+    auto bounded_lo = max(indices, zeros);\n+    // Integral type equality comparison is very very fast because it just looks\n+    // at the bits. Casting is free too. So we use the following pattern instead\n+    // of comparison + blendv.\n+    auto in_bound_lo = cast<scalar_t>(cast<int_t>(bounded_lo) == cast<int_t>(indices));\n+    auto res = min(bounded_lo, max_val_vec);\n+    auto in_bound_hi = cast<scalar_t>(cast<int_t>(res) == cast<int_t>(indices));\n+    return std::make_pair(res, (in_bound_lo & in_bound_hi) & Vec(half_max_val));\n+  }\n+};\n+\n+template<typename scalar_t>\n+struct ComputeLocation<scalar_t, GridSamplerPadding::Reflection>\n+  : ComputeLocationBase<scalar_t> {\n+  using Vec = Vec256<scalar_t>;\n+  using ComputeLocationBase<scalar_t>::unnormalize;\n+  using ComputeLocationBase<scalar_t>::half_max_val;\n+\n+  bool unit_size;  // whether size == 1, just return 0 in this case\n+  const scalar_t double_max_val;\n+  const scalar_t neg_half_max_val;\n+\n+  ComputeLocation(int64_t size)\n+    : ComputeLocationBase<scalar_t>(size)\n+    , unit_size(size == 1)\n+    , double_max_val(static_cast<scalar_t>((size - 1) * 2))\n+    , neg_half_max_val(-0.5 * static_cast<scalar_t>(size - 1)) {}\n+\n+  inline Vec apply(const Vec &in) const {\n+    if (unit_size) {\n+      return Vec(0);\n+    }\n+    Vec double_max_val_vec(double_max_val);\n+    auto abs_in = unnormalize(in).abs();\n+    auto fdouble_flips = abs_in / double_max_val_vec;\n+    auto double_flips = fdouble_flips.trunc();\n+    auto extra = abs_in - double_flips * double_max_val_vec;\n+    // Now we need to test if extra > max_val to find out if another flip is\n+    // needed. The following comparison does that and returns the correct\n+    // flipped value.\n+    return min(extra, double_max_val_vec - extra);\n+  }\n+\n+  inline std::pair<Vec, Vec> apply_get_grad(const Vec &in) const {\n+    if (unit_size) {\n+      return std::make_pair(Vec(0), Vec(0));\n+    }\n+    Vec double_max_val_vec(double_max_val);\n+    auto unnorm_in = unnormalize(in);\n+    auto neg_in = unnorm_in < Vec(0);\n+    auto abs_in = unnorm_in.abs();\n+    auto fdouble_flips = abs_in / double_max_val_vec;\n+    auto double_flips = fdouble_flips.trunc();\n+\n+    auto extra = abs_in - double_flips * double_max_val_vec;\n+    auto reflected_extra = double_max_val_vec - extra;\n+    auto one_more_flip = extra > reflected_extra;\n+\n+    return std::make_pair(\n+      Vec::blendv(extra, reflected_extra, one_more_flip),\n+      Vec::blendv(Vec(half_max_val), Vec(neg_half_max_val), one_more_flip ^ neg_in)\n+    );\n+  }\n+};\n+\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ApplyGridSample ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+// Struct to apply grid sample (reading from input, interpolate, and write to\n+// output).\n+// See NOTE [ Grid Sample CPU Kernels ] for details.\n+\n+template<typename scalar_t>\n+static inline void\n+mask_scatter_add(const scalar_t *src, scalar_t* base_addr,\n+                 const int_same_size_t<scalar_t> *offsets,\n+                 const int_same_size_t<scalar_t> *mask, int64_t len) {\n+  #pragma unroll\n+  for (int64_t i = 0; i < len; i++) {\n+    if (mask[i] & 0x01) {\n+      base_addr[offsets[i]] += src[i];\n+    }\n+  }\n+}\n+\n+template<typename scalar_t, int spatial_dim,\n+         GridSamplerInterpolation interp,\n+         GridSamplerPadding padding>\n+struct ApplyGridSample;\n+\n+template<typename scalar_t, GridSamplerPadding padding>\n+struct ApplyGridSample<scalar_t, 2, GridSamplerInterpolation::Bilinear, padding> {\n+  using Vec = Vec256<scalar_t>;\n+  using integer_t = int_same_size_t<scalar_t>;\n+  using iVec = Vec256<integer_t>;\n+\n+  const int64_t inp_H;\n+  const int64_t inp_W;\n+  const int64_t inp_sH;\n+  const int64_t inp_sW;\n+  const int64_t C;\n+  const int64_t inp_sC;\n+  const ComputeLocation<scalar_t, padding> compute_H;\n+  const ComputeLocation<scalar_t, padding> compute_W;\n+  const bool must_in_bound = padding != GridSamplerPadding::Zeros;\n+\n+  ApplyGridSample(const TensorAccessor<scalar_t, 4>& input)\n+    : inp_H(input.size(2))\n+    , inp_W(input.size(3))\n+    , inp_sH(input.stride(2))\n+    , inp_sW(input.stride(3))\n+    , C(input.size(1))\n+    , inp_sC(input.stride(1))\n+    , compute_H(input.size(2))\n+    , compute_W(input.size(3)) {}\n+\n+  inline std::tuple<\n+    Vec, Vec, Vec, Vec,       // distances to 4 sides\n+    Vec, Vec, Vec, Vec,       // interpolation weights wrt 4 corners\n+    Vec, Vec, Vec, Vec,       // in_bound masks\n+    iVec, iVec                // y_n and x_w\n+  >\n+  compute_interp_params(const Vec& x, const Vec& y) const {\n+    // get NE, NW, SE, SW pixel values from (x, y)\n+    // assuming we get exact integer representation and just use scalar_t\n+    // if we don't, the weights will be garbage anyways.\n+    auto x_w = x.floor();\n+    auto y_n = y.floor();\n+\n+    // get distances to each side\n+    auto w = x - x_w;\n+    auto e = Vec(1) - w;\n+    auto n = y - y_n;\n+    auto s = Vec(1) - n;\n+\n+    // get interpolation weights for each neighbor\n+    // e.g., for the nw corder, the weight is `dist_to_south * dist_to_east`.\n+    auto nw = s * e;\n+    auto ne = s * w;\n+    auto sw = n * e;\n+    auto se = n * w;\n+\n+    auto i_x_w = convert_to_int_of_same_size(x_w);\n+    auto i_y_n = convert_to_int_of_same_size(y_n);\n+    auto i_x_e = i_x_w + iVec(1);\n+    auto i_y_s = i_y_n + iVec(1);\n+\n+    // Use int comparison because it is much faster than float comp with AVX2\n+    // (latency 1 cyc vs. 4 cyc on skylake)\n+    // Avoid using the le and ge because those are not implemented in AVX2 and", "path": "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "position": 383, "original_position": 383, "commit_id": "78a5398f873de8d9c2a46ea7cb3a9eff1613fae6", "original_commit_id": "78a5398f873de8d9c2a46ea7cb3a9eff1613fae6", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "(We can still provide a reference implementation of it, but under a name that makes it clear that it's simulated)", "created_at": "2018-09-17T01:17:05Z", "updated_at": "2018-11-23T15:51:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/10980#discussion_r217935930", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10980", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217935930"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10980#discussion_r217935930"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10980"}}, "body_html": "<p>(We can still provide a reference implementation of it, but under a name that makes it clear that it's simulated)</p>", "body_text": "(We can still provide a reference implementation of it, but under a name that makes it clear that it's simulated)", "in_reply_to_id": 217935882}