{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216015627", "pull_request_review_id": 153414349, "id": 216015627, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjAxNTYyNw==", "diff_hunk": "@@ -0,0 +1,859 @@\n+#include <ATen/ATen.h>\n+#include <ATen/Dispatch.h>\n+#include <ATen/Parallel.h>\n+#include <ATen/core/C++17.h>\n+#include <ATen/TensorUtils.h>\n+#include <ATen/NativeFunctions.h>\n+#include <ATen/native/GridSampler.h>\n+#include <ATen/native/cpu/GridSamplerKernel.h>\n+#include <ATen/cpu/vml.h>\n+\n+#ifdef _OPENMP\n+#include <omp.h>\n+#endif\n+\n+#include <algorithm>\n+#include <cstring>\n+#include <type_traits>\n+\n+namespace at { namespace native { namespace {\n+\n+/**  NOTE [ Grid Sample CPU Kernels ]\n+ *\n+ *   Implementation of vectorized grid sample CPU kernels is divided into three\n+ *   parts:\n+ *\n+ *   1. `ComputeLocation` struct\n+ *      Transforms grid values into interpolation locations of the input tensor\n+ *      for a particular spatial dimension, basing on the size of that dimension\n+ *      in input tensor, and the padding mode.\n+ *\n+ *      template<typename scalar_t, GridSamplerPadding padding>\n+ *      struct ComputeLocation {\n+ *        using Vec = Vec256<scalar_t>;\n+ *\n+ *        // ctor\n+ *        ComputeLocation(int64_t size);\n+ *\n+ *        // Given grid values `in`, return the interpolation locations after\n+ *        // un-normalization and padding mechanism (elementwise).\n+ *        Vec apply(const Vec &in) const;\n+ *\n+ *        // Similar to `apply`, but also returns `d apply(in) / d in`\n+ *        // (elementwise).\n+ *        // this is often used in gradient computation.\n+ *        std::pair<Vec, Vec> apply_get_grad(const Vec &in) const;\n+ *      };\n+ *\n+ *   2. `ApplyGridSample` struct\n+ *      Owns N `ComputeLocation` structs, where N is the number of spatial\n+ *      dimensions. Given N input grid vectors (one for each spatial dimension)\n+ *      and spatial offset, it gets the interpolation locations from\n+ *      `ComputeLocation`s, applies interpolation procedure, and then writes to\n+ *      the output (or grad_input & grad_grid in backward).\n+ *\n+ *      template<typename scalar_t, int spatial_dim,\n+ *               GridSamplerInterpolation interp,\n+ *               GridSamplerPadding padding>\n+ *      struct ApplyGridSample {\n+ *\n+ *        // ctor\n+ *        ApplyGridSample(const TensorAccessor<scalar_t, 4>& input);\n+ *\n+ *        // Applies grid sampling (forward) procedure:\n+ *        //   1. computes interpolation locations from grid values `grid_x` and\n+ *        //      `grid_y`,\n+ *        //   2. interpolates output values using the locations and input data\n+ *        //      in `inp_slice`, and\n+ *        //   3. writes the first `len` values in the interpolated vector to\n+ *        //      `out_slice` with spatial offset being `offset`.\n+ *        //\n+ *        // This assimes that `grid_x` and `grid_y` all contain valid grid\n+ *        // values \\in [-1, 1], even at indices greater than `len`.\n+ *        //\n+ *        // The `*_slice` argument namess mean samples within a batch (i.e.,\n+ *        // with the batch dimension sliced out).\n+ *        void forward(TensorAccessor<scalar_t, 3>& out_slice,\n+ *                     const TensorAccessor<scalar_t, 3>& inp_slice,\n+ *                     int64_t offset, const Vec& grid_x, const Vec& grid_y,\n+ *                     int64_t len) const;\n+ *\n+ *        // Applies grid sampling (backward) procedure. Arguments semantics\n+ *        // and strategy are similar to those of `forward`.\n+ *        void backward(TensorAccessor<scalar_t, 3>& gInp_slice,\n+ *                      TensorAccessor<scalar_t, 3>& gGrid_slice,\n+ *                      const TensorAccessor<scalar_t, 3>& gOut_slice,\n+ *                      const TensorAccessor<scalar_t, 3>& inp_slice,\n+ *                      int64_t offset, const Vec& grid_x, const Vec& grid_y,\n+ *                      int64_t len) const;\n+ *      }\n+ *\n+ *   3. `grid_sample_2d_grid_slice_iterator` function\n+ *      Among the tensors we work with, we know that the output tensors are\n+ *      contiguous (i.e., `output` in forward, and `grad_input` & `grad_grid` in\n+ *      backward), we need to randomly read `input` anyways, and `grad_output`\n+ *      usually comes from autograd and is often contiguous. So we base our\n+ *      iterating strategy on the geometry of grid.\n+ *      `grid_sample_2d_grid_slice_iterator` function provides an abstract to\n+ *      efficiently iterates through a `grid` slice (without batch dimension).\n+ *      See comments of that function on the specific cases and strategies used.\n+ *\n+ *      template<typename scalar_t, typename ApplyFn>\n+ *      void grid_sample_2d_grid_slice_iterator(\n+ *        const TensorAccessor<scalar_t, 3>& grid_slice,\n+ *        const ApplyFn &apply_fn);\n+ *\n+ *      // `apply_fn` is a function/lambda that can be called as if it has\n+ *      // declaration:\n+ *      //   void apply_fn(const Vec256<scalar_t>& grid_x,\n+ *      //                 const Vec256<scalar_t>& grid_y,\n+ *      //                 int64_t spatial_offset, int64_t len);\n+ *\n+ *      `apply_fn` will be called multiple times, and together cover the entire\n+ *      output spatial space. Therefore, e.g., to implement forward 2d grid\n+ *      sample, we can do\n+ *\n+ *      ApplyGridSample<scalar_t, 2, interp, padding> grid_sample(input_accessor);\n+ *\n+ *      for (int n = 0; n < input_accessor.size(0); n++) {\n+ *        grid_sample_2d_grid_slice_iterator(\n+ *          grid_accessor[n],\n+ *          [&](const Vec256<scalar_t>& grid_x, const Vec256<scalar_t>& grid_y,\n+ *              int64_t spatial_offset, int64_t len) {\n+ *            grid_sample.forward(out_accessor[n], input_accessor[n],\n+ *                                spatial_offset, grid_x, grid_y, len);\n+ *          });\n+ *      }\n+ *\n+ **/\n+\n+\n+using at::native::detail::GridSamplerInterpolation;\n+using at::native::detail::GridSamplerPadding;\n+using namespace at::vec256;\n+\n+\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ComputeLocation ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+// Struct to compute interpolation location from grid values, and to apply\n+// padding mechanism (e.g., reflection).\n+// See NOTE [ Grid Sample CPU Kernels ] for details.\n+\n+template<typename scalar_t>\n+struct ComputeLocationBase {\n+  using Vec = Vec256<scalar_t>;\n+\n+  const Vec half_max_val;\n+  const Vec zeros = Vec(0);\n+  const Vec ones = Vec(1);\n+\n+  ComputeLocationBase(int64_t size)\n+    : half_max_val(static_cast<scalar_t>(size - 1) / 2) {}\n+\n+  inline Vec unnormalize(const Vec &in) const {\n+    return (in + ones) * half_max_val;\n+  }\n+};\n+\n+template<typename scalar_t, GridSamplerPadding padding>\n+struct ComputeLocation;\n+\n+template<typename scalar_t>\n+struct ComputeLocation<scalar_t, GridSamplerPadding::Zeros>\n+  : ComputeLocationBase<scalar_t> {\n+  using Vec = Vec256<scalar_t>;\n+  using ComputeLocationBase<scalar_t>::unnormalize;\n+  using ComputeLocationBase<scalar_t>::half_max_val;\n+\n+  using ComputeLocationBase<scalar_t>::ComputeLocationBase;\n+\n+  inline Vec apply(const Vec &in) const {\n+    return unnormalize(in);\n+  }\n+\n+  inline std::pair<Vec, Vec> apply_get_grad(const Vec &in) const {\n+    return std::make_pair(unnormalize(in), half_max_val);\n+  }\n+};\n+\n+template<typename scalar_t>\n+struct ComputeLocation<scalar_t, GridSamplerPadding::Border>\n+  : ComputeLocationBase<scalar_t> {\n+  using Vec = Vec256<scalar_t>;\n+  using ComputeLocationBase<scalar_t>::zeros;\n+  using ComputeLocationBase<scalar_t>::unnormalize;\n+  using ComputeLocationBase<scalar_t>::half_max_val;\n+\n+  const Vec max_val;\n+\n+  ComputeLocation(int64_t size)\n+    : ComputeLocationBase<scalar_t>(size)\n+    , max_val(static_cast<scalar_t>(size - 1)) {}\n+\n+  inline Vec apply(const Vec &in) const {\n+    return min(max_val, max(unnormalize(in), zeros));\n+  }\n+  inline std::pair<Vec, Vec> apply_get_grad(const Vec &in) const {\n+    auto indices = unnormalize(in);\n+    auto in_bound_hi = indices <= max_val;\n+    auto in_bound_lo = indices >= zeros;\n+    auto res = Vec::blendv(zeros,\n+                           Vec::blendv(max_val, indices, in_bound_hi),\n+                           in_bound_lo);\n+    return std::make_pair(res, (in_bound_hi & in_bound_lo) & half_max_val);\n+  }\n+};\n+\n+template<typename scalar_t>\n+struct ComputeLocation<scalar_t, GridSamplerPadding::Reflection>\n+  : ComputeLocationBase<scalar_t> {\n+  using Vec = Vec256<scalar_t>;\n+  using ComputeLocationBase<scalar_t>::zeros;\n+  using ComputeLocationBase<scalar_t>::unnormalize;\n+  using ComputeLocationBase<scalar_t>::half_max_val;\n+\n+  bool unit_size;  // whether size == 1, just return 0 in this case\n+  const Vec double_max_val;\n+  const Vec neg_half_max_val;\n+\n+  ComputeLocation(int64_t size)\n+    : ComputeLocationBase<scalar_t>(size)\n+    , unit_size(size == 1)\n+    , double_max_val(static_cast<scalar_t>((size - 1) * 2))\n+    , neg_half_max_val(-0.5 * static_cast<scalar_t>(size - 1)) {}\n+\n+  inline Vec apply(const Vec &in) const {\n+    if (unit_size) {\n+      return zeros;\n+    }\n+    auto abs_in = unnormalize(in).abs();\n+    auto fdouble_flips = abs_in / double_max_val;\n+    auto double_flips = fdouble_flips.trunc();\n+    auto extra = abs_in - double_flips * double_max_val;\n+    // Now we need to test if extra > max_val to find out if another flip is\n+    // needed. The following comparison does that and returns the correct\n+    // flipped value.\n+    return min(extra, double_max_val - extra);\n+  }\n+\n+  inline std::pair<Vec, Vec> apply_get_grad(const Vec &in) const {\n+    if (unit_size) {\n+      return std::make_pair(zeros, zeros);\n+    }\n+    auto unnorm_in = unnormalize(in);\n+    auto neg_in = unnorm_in < zeros;\n+    auto abs_in = unnorm_in.abs();\n+    auto fdouble_flips = abs_in / double_max_val;\n+    auto double_flips = fdouble_flips.trunc();\n+\n+    auto extra = abs_in - double_flips * double_max_val;\n+    auto reflected_extra = double_max_val - extra;\n+    auto one_more_flip = extra > reflected_extra;\n+\n+    return std::make_pair(\n+      Vec::blendv(extra, reflected_extra, one_more_flip),\n+      Vec::blendv(half_max_val, neg_half_max_val, one_more_flip ^ neg_in)\n+    );\n+  }\n+};\n+\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ApplyGridSample ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+// Struct to apply grid sample (reading from input, interpolate, and write to\n+// output).\n+// See NOTE [ Grid Sample CPU Kernels ] for details.\n+\n+template<typename scalar_t>\n+static inline void\n+mask_scatter_add(const scalar_t *src, scalar_t* base_addr,\n+                 const int_same_size_t<scalar_t> *offsets,\n+                 const int_same_size_t<scalar_t> *mask, int64_t len) {\n+  #pragma unroll", "path": "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "position": 311, "original_position": 269, "commit_id": "78a5398f873de8d9c2a46ea7cb3a9eff1613fae6", "original_commit_id": "a0b3dad8a060ca2e8944f94ce635cf144b9a98b2", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I think it works with clang. GCC has `#pragma GCC unroll n` but it's not usually any better than the code generated by `-O3`", "created_at": "2018-09-07T16:27:06Z", "updated_at": "2018-11-23T15:50:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/10980#discussion_r216015627", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10980", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216015627"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10980#discussion_r216015627"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10980"}}, "body_html": "<p>I think it works with clang. GCC has <code>#pragma GCC unroll n</code> but it's not usually any better than the code generated by <code>-O3</code></p>", "body_text": "I think it works with clang. GCC has #pragma GCC unroll n but it's not usually any better than the code generated by -O3", "in_reply_to_id": 215711353}