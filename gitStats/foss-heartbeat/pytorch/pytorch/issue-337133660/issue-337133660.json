{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9046", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9046/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9046/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9046/events", "html_url": "https://github.com/pytorch/pytorch/issues/9046", "id": 337133660, "node_id": "MDU6SXNzdWUzMzcxMzM2NjA=", "number": 9046, "title": "[feature request] freeze() for nn.Module", "user": {"login": "samuelbroscheit", "id": 22645035, "node_id": "MDQ6VXNlcjIyNjQ1MDM1", "avatar_url": "https://avatars3.githubusercontent.com/u/22645035?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuelbroscheit", "html_url": "https://github.com/samuelbroscheit", "followers_url": "https://api.github.com/users/samuelbroscheit/followers", "following_url": "https://api.github.com/users/samuelbroscheit/following{/other_user}", "gists_url": "https://api.github.com/users/samuelbroscheit/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuelbroscheit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuelbroscheit/subscriptions", "organizations_url": "https://api.github.com/users/samuelbroscheit/orgs", "repos_url": "https://api.github.com/users/samuelbroscheit/repos", "events_url": "https://api.github.com/users/samuelbroscheit/events{/privacy}", "received_events_url": "https://api.github.com/users/samuelbroscheit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-29T20:51:14Z", "updated_at": "2018-07-02T17:44:32Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I want to suggest a freeze() method to freeze an already trained module, and its parameters. However, for modules like BatchNorm it is not sufficient to set requires_grad = False, because the running_mean and running_var are still updated unless the the module is set to .eval() .</p>\n<p>Unless I am missing something, ATM, I only can solve this with</p>\n<pre><code>            self.bn.train(self.bn.training and self.bn.weight.requires_grad)\n            x = self.bn(x)\n</code></pre>\n<p>which feels quite hacky.</p>\n<p>The idea would be that freeze() sets all the parameters of a module to requires_grad = False, sets self.training = False  and sets a flag self.is_frozen =  True which blocks all future .train() calls to the module.</p>", "body_text": "I want to suggest a freeze() method to freeze an already trained module, and its parameters. However, for modules like BatchNorm it is not sufficient to set requires_grad = False, because the running_mean and running_var are still updated unless the the module is set to .eval() .\nUnless I am missing something, ATM, I only can solve this with\n            self.bn.train(self.bn.training and self.bn.weight.requires_grad)\n            x = self.bn(x)\n\nwhich feels quite hacky.\nThe idea would be that freeze() sets all the parameters of a module to requires_grad = False, sets self.training = False  and sets a flag self.is_frozen =  True which blocks all future .train() calls to the module.", "body": "I want to suggest a freeze() method to freeze an already trained module, and its parameters. However, for modules like BatchNorm it is not sufficient to set requires_grad = False, because the running_mean and running_var are still updated unless the the module is set to .eval() . \r\n\r\nUnless I am missing something, ATM, I only can solve this with\r\n\r\n```\r\n            self.bn.train(self.bn.training and self.bn.weight.requires_grad)\r\n            x = self.bn(x)\r\n```\r\nwhich feels quite hacky.  \r\n\r\nThe idea would be that freeze() sets all the parameters of a module to requires_grad = False, sets self.training = False  and sets a flag self.is_frozen =  True which blocks all future .train() calls to the module. "}