{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5852", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5852/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5852/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5852/events", "html_url": "https://github.com/pytorch/pytorch/issues/5852", "id": 306165632, "node_id": "MDU6SXNzdWUzMDYxNjU2MzI=", "number": 5852, "title": "Loading huge text files for neural machine translation", "user": {"login": "saitarslanboun", "id": 9799395, "node_id": "MDQ6VXNlcjk3OTkzOTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9799395?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saitarslanboun", "html_url": "https://github.com/saitarslanboun", "followers_url": "https://api.github.com/users/saitarslanboun/followers", "following_url": "https://api.github.com/users/saitarslanboun/following{/other_user}", "gists_url": "https://api.github.com/users/saitarslanboun/gists{/gist_id}", "starred_url": "https://api.github.com/users/saitarslanboun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saitarslanboun/subscriptions", "organizations_url": "https://api.github.com/users/saitarslanboun/orgs", "repos_url": "https://api.github.com/users/saitarslanboun/repos", "events_url": "https://api.github.com/users/saitarslanboun/events{/privacy}", "received_events_url": "https://api.github.com/users/saitarslanboun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-17T15:35:08Z", "updated_at": "2018-03-17T17:17:52Z", "closed_at": "2018-03-17T17:17:51Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I have written a dataset loading class for loading 2 text files as source and targets, for neural machine translation purpose. Each file has 93577946 lines, and each of them allocates 8GB memory on Hard Disc.</p>\n<p>The class is as the following:</p>\n<pre><code>class LoadUniModal(Dataset):\n\tsources = []\n\ttargets = []\n\tmaxlen = 0\n\tlengths = []\n\n\tdef __init__(self, src, trg, src_vocab, trg_vocab):\n\t\tself.src_vocab = src_vocab\n\t\tself.trg_vocab = trg_vocab\n\n\t\twith codecs.open(src, encoding=\"utf-8\") as f:\n\t\t\tfor line in f:\n\t\t\t\ttokens = line.replace(\"\\n\", \"\").split()\n\t\t\t\tself.maxlen = max(self.maxlen, len(tokens))\n\t\t\t\tself.sources.append(tokens)\n\t\twith codecs.open(trg, encoding=\"utf-8\") as f:\n\t\t\tfor line in f:\n\t\t\t\ttokens = line.replace(\"\\n\", \"\").split()\n\t\t\t\tself.maxlen = max(self.maxlen, len(tokens))\n\t\t\t\tself.targets.append(tokens)\n\t\t\t\tself.lengths.append(len(tokens)+2)\n\n\t# Overrride to give PyTorch access to any image on the dataset\n\tdef __getitem__(self, index):\n\n\t\t# Source sentence processing\n\t\ttokens = self.sources[index]\n\t\tntokens = [self.src_vocab['&lt;START&gt;']]\n\t\tfor a in range(self.maxlen):\n\t\t\tif a &lt;= (len(tokens) - 1):\n\t\t\t\tif tokens[a] in self.src_vocab.keys():\n\t\t\t\t\tntokens.append(self.src_vocab[tokens[a]])\n\t\t\t\telse:\n\t\t\t\t\tntokens.append(self.src_vocab['&lt;UNK&gt;'])\n\t\t\telif a == len(tokens):\n\t\t\t\tntokens.append(self.src_vocab['&lt;END&gt;'])\n\t\t\telif a &gt; len(tokens):\n\t\t\t\tntokens.append(self.src_vocab['&lt;PAD&gt;'])\n\n\t\tsource = torch.from_numpy(np.asarray(ntokens)).long()\n\n\t\t# Target sentence processing\n\t\ttokens = self.targets[index]\n                ntokens = [self.trg_vocab['&lt;START&gt;']]\n                for a in range(self.maxlen):\n                        if a &lt;= (len(tokens) - 1):\n                                if tokens[a] in self.trg_vocab.keys():\n                                        ntokens.append(self.trg_vocab[tokens[a]])\n                                else:\n                                        ntokens.append(self.trg_vocab['&lt;UNK&gt;'])\n                        elif a == len(tokens):\n                                ntokens.append(self.trg_vocab['&lt;END&gt;'])\n                        elif a &gt; len(tokens):\n                                ntokens.append(self.trg_vocab['&lt;PAD&gt;'])\n\n                target = torch.from_numpy(np.asarray(ntokens)).long()\n\n\t\tlength = self.lengths[index]\n\n\t\treturn [0], source, target, length\n\n\tdef __len__(self):\n\t\treturn len(self.sources)\n</code></pre>\n<p>I use the class in order to load dataset as follows:</p>\n<pre><code>def load_text_train_data(train_dir, src_vocab, trg_vocab, lang_pair, batch_size):\n\n        tpl = ast.literal_eval(lang_pair)\n        slang = tpl[1]\n        tlang = tpl[2]\n\n        strain_file = os.path.join(train_dir, \"train\"+slang)\n        ttrain_file = os.path.join(train_dir, \"train\"+tlang)\n\n        data_iter = LoadUniModal(strain_file, ttrain_file, src_vocab, trg_vocab)\n        data_iter = DataLoader(data_iter, batch_size=batch_size)\n\n        return data_iter\n</code></pre>\n<p>When I am trying to load the data, I get memory error.</p>\n<p>How would it be possible to load the data without memory problem?</p>\n<p>Thanks,</p>", "body_text": "Hi,\nI have written a dataset loading class for loading 2 text files as source and targets, for neural machine translation purpose. Each file has 93577946 lines, and each of them allocates 8GB memory on Hard Disc.\nThe class is as the following:\nclass LoadUniModal(Dataset):\n\tsources = []\n\ttargets = []\n\tmaxlen = 0\n\tlengths = []\n\n\tdef __init__(self, src, trg, src_vocab, trg_vocab):\n\t\tself.src_vocab = src_vocab\n\t\tself.trg_vocab = trg_vocab\n\n\t\twith codecs.open(src, encoding=\"utf-8\") as f:\n\t\t\tfor line in f:\n\t\t\t\ttokens = line.replace(\"\\n\", \"\").split()\n\t\t\t\tself.maxlen = max(self.maxlen, len(tokens))\n\t\t\t\tself.sources.append(tokens)\n\t\twith codecs.open(trg, encoding=\"utf-8\") as f:\n\t\t\tfor line in f:\n\t\t\t\ttokens = line.replace(\"\\n\", \"\").split()\n\t\t\t\tself.maxlen = max(self.maxlen, len(tokens))\n\t\t\t\tself.targets.append(tokens)\n\t\t\t\tself.lengths.append(len(tokens)+2)\n\n\t# Overrride to give PyTorch access to any image on the dataset\n\tdef __getitem__(self, index):\n\n\t\t# Source sentence processing\n\t\ttokens = self.sources[index]\n\t\tntokens = [self.src_vocab['<START>']]\n\t\tfor a in range(self.maxlen):\n\t\t\tif a <= (len(tokens) - 1):\n\t\t\t\tif tokens[a] in self.src_vocab.keys():\n\t\t\t\t\tntokens.append(self.src_vocab[tokens[a]])\n\t\t\t\telse:\n\t\t\t\t\tntokens.append(self.src_vocab['<UNK>'])\n\t\t\telif a == len(tokens):\n\t\t\t\tntokens.append(self.src_vocab['<END>'])\n\t\t\telif a > len(tokens):\n\t\t\t\tntokens.append(self.src_vocab['<PAD>'])\n\n\t\tsource = torch.from_numpy(np.asarray(ntokens)).long()\n\n\t\t# Target sentence processing\n\t\ttokens = self.targets[index]\n                ntokens = [self.trg_vocab['<START>']]\n                for a in range(self.maxlen):\n                        if a <= (len(tokens) - 1):\n                                if tokens[a] in self.trg_vocab.keys():\n                                        ntokens.append(self.trg_vocab[tokens[a]])\n                                else:\n                                        ntokens.append(self.trg_vocab['<UNK>'])\n                        elif a == len(tokens):\n                                ntokens.append(self.trg_vocab['<END>'])\n                        elif a > len(tokens):\n                                ntokens.append(self.trg_vocab['<PAD>'])\n\n                target = torch.from_numpy(np.asarray(ntokens)).long()\n\n\t\tlength = self.lengths[index]\n\n\t\treturn [0], source, target, length\n\n\tdef __len__(self):\n\t\treturn len(self.sources)\n\nI use the class in order to load dataset as follows:\ndef load_text_train_data(train_dir, src_vocab, trg_vocab, lang_pair, batch_size):\n\n        tpl = ast.literal_eval(lang_pair)\n        slang = tpl[1]\n        tlang = tpl[2]\n\n        strain_file = os.path.join(train_dir, \"train\"+slang)\n        ttrain_file = os.path.join(train_dir, \"train\"+tlang)\n\n        data_iter = LoadUniModal(strain_file, ttrain_file, src_vocab, trg_vocab)\n        data_iter = DataLoader(data_iter, batch_size=batch_size)\n\n        return data_iter\n\nWhen I am trying to load the data, I get memory error.\nHow would it be possible to load the data without memory problem?\nThanks,", "body": "Hi,\r\n\r\nI have written a dataset loading class for loading 2 text files as source and targets, for neural machine translation purpose. Each file has 93577946 lines, and each of them allocates 8GB memory on Hard Disc.\r\n\r\nThe class is as the following:\r\n\r\n```\r\nclass LoadUniModal(Dataset):\r\n\tsources = []\r\n\ttargets = []\r\n\tmaxlen = 0\r\n\tlengths = []\r\n\r\n\tdef __init__(self, src, trg, src_vocab, trg_vocab):\r\n\t\tself.src_vocab = src_vocab\r\n\t\tself.trg_vocab = trg_vocab\r\n\r\n\t\twith codecs.open(src, encoding=\"utf-8\") as f:\r\n\t\t\tfor line in f:\r\n\t\t\t\ttokens = line.replace(\"\\n\", \"\").split()\r\n\t\t\t\tself.maxlen = max(self.maxlen, len(tokens))\r\n\t\t\t\tself.sources.append(tokens)\r\n\t\twith codecs.open(trg, encoding=\"utf-8\") as f:\r\n\t\t\tfor line in f:\r\n\t\t\t\ttokens = line.replace(\"\\n\", \"\").split()\r\n\t\t\t\tself.maxlen = max(self.maxlen, len(tokens))\r\n\t\t\t\tself.targets.append(tokens)\r\n\t\t\t\tself.lengths.append(len(tokens)+2)\r\n\r\n\t# Overrride to give PyTorch access to any image on the dataset\r\n\tdef __getitem__(self, index):\r\n\r\n\t\t# Source sentence processing\r\n\t\ttokens = self.sources[index]\r\n\t\tntokens = [self.src_vocab['<START>']]\r\n\t\tfor a in range(self.maxlen):\r\n\t\t\tif a <= (len(tokens) - 1):\r\n\t\t\t\tif tokens[a] in self.src_vocab.keys():\r\n\t\t\t\t\tntokens.append(self.src_vocab[tokens[a]])\r\n\t\t\t\telse:\r\n\t\t\t\t\tntokens.append(self.src_vocab['<UNK>'])\r\n\t\t\telif a == len(tokens):\r\n\t\t\t\tntokens.append(self.src_vocab['<END>'])\r\n\t\t\telif a > len(tokens):\r\n\t\t\t\tntokens.append(self.src_vocab['<PAD>'])\r\n\r\n\t\tsource = torch.from_numpy(np.asarray(ntokens)).long()\r\n\r\n\t\t# Target sentence processing\r\n\t\ttokens = self.targets[index]\r\n                ntokens = [self.trg_vocab['<START>']]\r\n                for a in range(self.maxlen):\r\n                        if a <= (len(tokens) - 1):\r\n                                if tokens[a] in self.trg_vocab.keys():\r\n                                        ntokens.append(self.trg_vocab[tokens[a]])\r\n                                else:\r\n                                        ntokens.append(self.trg_vocab['<UNK>'])\r\n                        elif a == len(tokens):\r\n                                ntokens.append(self.trg_vocab['<END>'])\r\n                        elif a > len(tokens):\r\n                                ntokens.append(self.trg_vocab['<PAD>'])\r\n\r\n                target = torch.from_numpy(np.asarray(ntokens)).long()\r\n\r\n\t\tlength = self.lengths[index]\r\n\r\n\t\treturn [0], source, target, length\r\n\r\n\tdef __len__(self):\r\n\t\treturn len(self.sources)\r\n```\r\n\r\nI use the class in order to load dataset as follows:\r\n\r\n```\r\ndef load_text_train_data(train_dir, src_vocab, trg_vocab, lang_pair, batch_size):\r\n\r\n        tpl = ast.literal_eval(lang_pair)\r\n        slang = tpl[1]\r\n        tlang = tpl[2]\r\n\r\n        strain_file = os.path.join(train_dir, \"train\"+slang)\r\n        ttrain_file = os.path.join(train_dir, \"train\"+tlang)\r\n\r\n        data_iter = LoadUniModal(strain_file, ttrain_file, src_vocab, trg_vocab)\r\n        data_iter = DataLoader(data_iter, batch_size=batch_size)\r\n\r\n        return data_iter\r\n```\r\n\r\nWhen I am trying to load the data, I get memory error.\r\n\r\nHow would it be possible to load the data without memory problem?\r\n\r\nThanks,"}