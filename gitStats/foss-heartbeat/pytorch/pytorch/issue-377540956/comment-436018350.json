{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/436018350", "html_url": "https://github.com/pytorch/pytorch/issues/13583#issuecomment-436018350", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13583", "id": 436018350, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjAxODM1MA==", "user": {"login": "pietern", "id": 9845, "node_id": "MDQ6VXNlcjk4NDU=", "avatar_url": "https://avatars3.githubusercontent.com/u/9845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pietern", "html_url": "https://github.com/pietern", "followers_url": "https://api.github.com/users/pietern/followers", "following_url": "https://api.github.com/users/pietern/following{/other_user}", "gists_url": "https://api.github.com/users/pietern/gists{/gist_id}", "starred_url": "https://api.github.com/users/pietern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pietern/subscriptions", "organizations_url": "https://api.github.com/users/pietern/orgs", "repos_url": "https://api.github.com/users/pietern/repos", "events_url": "https://api.github.com/users/pietern/events{/privacy}", "received_events_url": "https://api.github.com/users/pietern/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-05T20:11:43Z", "updated_at": "2018-11-05T20:11:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Yes, correct. Specifying <code>world_size=2</code> means that it expects 2 processes are started with ranks 0 and 1.</p>\n<p>You can either use <code>torch.distributed.launch</code> or start the processes manually if you want. This doesn't make a difference for how <code>init_process_group</code> works.</p>", "body_text": "Yes, correct. Specifying world_size=2 means that it expects 2 processes are started with ranks 0 and 1.\nYou can either use torch.distributed.launch or start the processes manually if you want. This doesn't make a difference for how init_process_group works.", "body": "Yes, correct. Specifying `world_size=2` means that it expects 2 processes are started with ranks 0 and 1.\r\n\r\nYou can either use `torch.distributed.launch` or start the processes manually if you want. This doesn't make a difference for how `init_process_group` works."}