{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/436106995", "html_url": "https://github.com/pytorch/pytorch/issues/13583#issuecomment-436106995", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13583", "id": 436106995, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjEwNjk5NQ==", "user": {"login": "mosjoker", "id": 40882852, "node_id": "MDQ6VXNlcjQwODgyODUy", "avatar_url": "https://avatars2.githubusercontent.com/u/40882852?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mosjoker", "html_url": "https://github.com/mosjoker", "followers_url": "https://api.github.com/users/mosjoker/followers", "following_url": "https://api.github.com/users/mosjoker/following{/other_user}", "gists_url": "https://api.github.com/users/mosjoker/gists{/gist_id}", "starred_url": "https://api.github.com/users/mosjoker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mosjoker/subscriptions", "organizations_url": "https://api.github.com/users/mosjoker/orgs", "repos_url": "https://api.github.com/users/mosjoker/repos", "events_url": "https://api.github.com/users/mosjoker/events{/privacy}", "received_events_url": "https://api.github.com/users/mosjoker/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-06T02:12:32Z", "updated_at": "2018-11-06T02:12:32Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Hi pietern, sorry for the cross post, I will keep in mind after.<br>\nThank you for your reply.<br>\nI tested the above only on one machine.<br>\nDo you mean that I must run pytorch.distributed.init_process_group simultaneously(or within limited time)?<br>\nFor example, time=0, machine=1, run:<br>\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py<br>\ntime=0, machine=2, run:<br>\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py</p>\n</blockquote>\n<p>Did you make a mistake when running the example? like unhandled error?</p>", "body_text": "Hi pietern, sorry for the cross post, I will keep in mind after.\nThank you for your reply.\nI tested the above only on one machine.\nDo you mean that I must run pytorch.distributed.init_process_group simultaneously(or within limited time)?\nFor example, time=0, machine=1, run:\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py\ntime=0, machine=2, run:\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py\n\nDid you make a mistake when running the example? like unhandled error?", "body": "> Hi pietern, sorry for the cross post, I will keep in mind after.\r\n> Thank you for your reply.\r\n> I tested the above only on one machine.\r\n> Do you mean that I must run pytorch.distributed.init_process_group simultaneously(or within limited time)?\r\n> For example, time=0, machine=1, run:\r\n> python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py\r\n> time=0, machine=2, run:\r\n> python -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py\r\n\r\nDid you make a mistake when running the example? like unhandled error?"}