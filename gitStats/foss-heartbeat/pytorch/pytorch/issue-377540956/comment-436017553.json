{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/436017553", "html_url": "https://github.com/pytorch/pytorch/issues/13583#issuecomment-436017553", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13583", "id": 436017553, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjAxNzU1Mw==", "user": {"login": "forhonourlx", "id": 16576219, "node_id": "MDQ6VXNlcjE2NTc2MjE5", "avatar_url": "https://avatars3.githubusercontent.com/u/16576219?v=4", "gravatar_id": "", "url": "https://api.github.com/users/forhonourlx", "html_url": "https://github.com/forhonourlx", "followers_url": "https://api.github.com/users/forhonourlx/followers", "following_url": "https://api.github.com/users/forhonourlx/following{/other_user}", "gists_url": "https://api.github.com/users/forhonourlx/gists{/gist_id}", "starred_url": "https://api.github.com/users/forhonourlx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/forhonourlx/subscriptions", "organizations_url": "https://api.github.com/users/forhonourlx/orgs", "repos_url": "https://api.github.com/users/forhonourlx/repos", "events_url": "https://api.github.com/users/forhonourlx/events{/privacy}", "received_events_url": "https://api.github.com/users/forhonourlx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-05T20:09:04Z", "updated_at": "2018-11-05T20:09:04Z", "author_association": "NONE", "body_html": "<p>Hi pietern, sorry for the cross post, I will keep in mind after.<br>\nThank you for your reply.<br>\nI tested the above only on one machine.<br>\nDo you mean that I must run pytorch.distributed.init_process_group simultaneously(or within limited time)?<br>\nFor example, time=0, machine=1, run:<br>\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py<br>\ntime=0, machine=2, run:<br>\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py</p>", "body_text": "Hi pietern, sorry for the cross post, I will keep in mind after.\nThank you for your reply.\nI tested the above only on one machine.\nDo you mean that I must run pytorch.distributed.init_process_group simultaneously(or within limited time)?\nFor example, time=0, machine=1, run:\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py\ntime=0, machine=2, run:\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py", "body": "Hi pietern, sorry for the cross post, I will keep in mind after.\r\nThank you for your reply. \r\nI tested the above only on one machine.\r\nDo you mean that I must run pytorch.distributed.init_process_group simultaneously(or within limited time)?\r\nFor example, time=0, machine=1, run:\r\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py\r\ntime=0, machine=2, run:\r\npython -m torch.distributed.launch --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\" --master_port=1234 OUR_TRAINING_SCRIPT.py\r\n\r\n"}