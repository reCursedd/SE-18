{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396454908", "html_url": "https://github.com/pytorch/pytorch/pull/7889#issuecomment-396454908", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7889", "id": 396454908, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjQ1NDkwOA==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-12T03:33:16Z", "updated_at": "2018-06-12T03:33:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26222919\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bado-lee\">@bado-lee</a> I don't think the change you suggested here is right. In the example code in the docs, we clearly step the scheduler before training:</p>\n<pre><code>&gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups\n&gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30\n&gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 80\n&gt;&gt;&gt; # lr = 0.0005   if epoch &gt;= 80\n&gt;&gt;&gt; scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n&gt;&gt;&gt; for epoch in range(100):\n&gt;&gt;&gt;     scheduler.step()\n&gt;&gt;&gt;     train(...)\n&gt;&gt;&gt;     validate(...)\n</code></pre>\n<p>Perhaps a more logical design would have been to step the scheduler at the same time you step the optimizer, but that's not how the API works today, and you'd be changing the behavior of anyone's code who had been abiding by the API previously.</p>\n<p>What do you think about this reasoning?</p>", "body_text": "@bado-lee I don't think the change you suggested here is right. In the example code in the docs, we clearly step the scheduler before training:\n>>> # Assuming optimizer uses lr = 0.05 for all groups\n>>> # lr = 0.05     if epoch < 30\n>>> # lr = 0.005    if 30 <= epoch < 80\n>>> # lr = 0.0005   if epoch >= 80\n>>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n>>> for epoch in range(100):\n>>>     scheduler.step()\n>>>     train(...)\n>>>     validate(...)\n\nPerhaps a more logical design would have been to step the scheduler at the same time you step the optimizer, but that's not how the API works today, and you'd be changing the behavior of anyone's code who had been abiding by the API previously.\nWhat do you think about this reasoning?", "body": "@bado-lee I don't think the change you suggested here is right. In the example code in the docs, we clearly step the scheduler before training:\r\n\r\n```\r\n>>> # Assuming optimizer uses lr = 0.05 for all groups\r\n>>> # lr = 0.05     if epoch < 30\r\n>>> # lr = 0.005    if 30 <= epoch < 80\r\n>>> # lr = 0.0005   if epoch >= 80\r\n>>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\r\n>>> for epoch in range(100):\r\n>>>     scheduler.step()\r\n>>>     train(...)\r\n>>>     validate(...)\r\n```\r\n\r\nPerhaps a more logical design would have been to step the scheduler at the same time you step the optimizer, but that's not how the API works today, and you'd be changing the behavior of anyone's code who had been abiding by the API previously.\r\n\r\nWhat do you think about this reasoning?"}