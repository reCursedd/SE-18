{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7889", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7889/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7889/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7889/events", "html_url": "https://github.com/pytorch/pytorch/pull/7889", "id": 326879825, "node_id": "MDExOlB1bGxSZXF1ZXN0MTkwODI0MTM3", "number": 7889, "title": "Fix lr_scheduler's last_epoch value at the time of initialization", "user": {"login": "bado-lee", "id": 26222919, "node_id": "MDQ6VXNlcjI2MjIyOTE5", "avatar_url": "https://avatars1.githubusercontent.com/u/26222919?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bado-lee", "html_url": "https://github.com/bado-lee", "followers_url": "https://api.github.com/users/bado-lee/followers", "following_url": "https://api.github.com/users/bado-lee/following{/other_user}", "gists_url": "https://api.github.com/users/bado-lee/gists{/gist_id}", "starred_url": "https://api.github.com/users/bado-lee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bado-lee/subscriptions", "organizations_url": "https://api.github.com/users/bado-lee/orgs", "repos_url": "https://api.github.com/users/bado-lee/repos", "events_url": "https://api.github.com/users/bado-lee/events{/privacy}", "received_events_url": "https://api.github.com/users/bado-lee/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 846310180, "node_id": "MDU6TGFiZWw4NDYzMTAxODA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bc-breaking", "name": "bc-breaking", "color": "ed7d8a", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2018-05-28T02:49:51Z", "updated_at": "2018-07-10T17:03:41Z", "closed_at": null, "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7889", "html_url": "https://github.com/pytorch/pytorch/pull/7889", "diff_url": "https://github.com/pytorch/pytorch/pull/7889.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7889.patch"}, "body_html": "<p>Hello everyone :) !!</p>\n<p>I've found that lr_scheduler was initialized with last_epoch as -1.<br>\nThis causes that even after the first step (not the one in init but explicit step of scheduler),<br>\nlearning rate of scheduler's optimizer remains as the previous.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> cc <span class=\"pl-k\">=</span> torch.nn.Conv2d(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">3</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myinitial_lr <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.1</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myoptimizer <span class=\"pl-k\">=</span> torch.optim.Adam(cc.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>myinitial_lr)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> mylrdecay <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.5</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler <span class=\"pl-k\">=</span> torch.optim.lr_scheduler.ExponentialLR(myoptimizer,mylrdecay)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> get_lr value and optimizer's lr value is not consistent, last_epoch should be 0 instead of -1</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.get_lr()\n[<span class=\"pl-c1\">0.2</span>]    <span class=\"pl-c\"><span class=\"pl-c\">#</span> this is because of  get_lr calculates lr by 0.1 * 0.5^-1</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.optimizer.param_groups[<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lr<span class=\"pl-pds\">\"</span></span>]\n<span class=\"pl-c1\">0.1</span>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> this is not consistent with get_lr value</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.last_epoch\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> values seem to be in sync but should have been decayed value of 0.05 after first step(decay)</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.step()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.get_lr()\n[<span class=\"pl-c1\">0.1</span>]    <span class=\"pl-c\"><span class=\"pl-c\">#</span> this should be the value right after the init, not after first step</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.optimizer.param_groups[<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lr<span class=\"pl-pds\">\"</span></span>]\n<span class=\"pl-c1\">0.1</span>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> since this is after first step, it should have been decayed as 0.05</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.last_epoch\n<span class=\"pl-c1\">0</span>\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.step()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.last_epoch\n<span class=\"pl-c1\">1</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.get_lr()\n[<span class=\"pl-c1\">0.05</span>]\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.optimizer.param_groups[<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lr<span class=\"pl-pds\">\"</span></span>]\n<span class=\"pl-c1\">0.05</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> myscheduler.last_epoch\n<span class=\"pl-c1\">1</span></pre></div>\n<p>First problem is, even after the init of lr_scheduler, you get the inconsistent parameter values.</p>\n<p>The second problem is, you are stuck with same learning rate in the first 2 epochs if the step function of lr_scheduler is not called in the beginning of the epoch loop.<br>\nOf course, you can avoid this by calling lr_scheduler's step in the beginning,<br>\nbut I don't think this is proper use since, incase of optimizer, step is called in the end of the iteration loop.</p>\n<p>I've simply avoided all above issues by setting last_epoch as 0 after the initialization.</p>\n<p>This also makes sense when you init with some value of last_epoch which is not -1.<br>\nFor example, if you want to init with last epoch 10,<br>\nlr should not be set with decayed 1 step further. Which is<br>\nlast_epoch gets +1 in the previous code.<br>\nbase_lr * self.gamma ** self.last_epoch</p>\n<p>Instead, it should be set with step 10 exact value.</p>\n<p>I hope this fix find it's way with all your help :)<br>\nI'm really looking forward &amp; excited to become a contributor for pytorch!<br>\nPytorch Rocks!!</p>", "body_text": "Hello everyone :) !!\nI've found that lr_scheduler was initialized with last_epoch as -1.\nThis causes that even after the first step (not the one in init but explicit step of scheduler),\nlearning rate of scheduler's optimizer remains as the previous.\n>>> import torch\n>>> cc = torch.nn.Conv2d(10,10,3)\n>>> myinitial_lr = 0.1\n>>> myoptimizer = torch.optim.Adam(cc.parameters(), lr=myinitial_lr)\n>>> mylrdecay = 0.5\n>>> myscheduler = torch.optim.lr_scheduler.ExponentialLR(myoptimizer,mylrdecay)\n\n# get_lr value and optimizer's lr value is not consistent, last_epoch should be 0 instead of -1\n>>> myscheduler.get_lr()\n[0.2]    # this is because of  get_lr calculates lr by 0.1 * 0.5^-1\n>>> myscheduler.optimizer.param_groups[0][\"lr\"]\n0.1    # this is not consistent with get_lr value\n>>> myscheduler.last_epoch\n-1\n\n# values seem to be in sync but should have been decayed value of 0.05 after first step(decay)\n>>> myscheduler.step()\n>>> myscheduler.get_lr()\n[0.1]    # this should be the value right after the init, not after first step\n>>> myscheduler.optimizer.param_groups[0][\"lr\"]\n0.1    # since this is after first step, it should have been decayed as 0.05\n>>> myscheduler.last_epoch\n0\n\n>>> myscheduler.step()\n>>> myscheduler.last_epoch\n1\n>>> myscheduler.get_lr()\n[0.05]\n>>> myscheduler.optimizer.param_groups[0][\"lr\"]\n0.05\n>>> myscheduler.last_epoch\n1\nFirst problem is, even after the init of lr_scheduler, you get the inconsistent parameter values.\nThe second problem is, you are stuck with same learning rate in the first 2 epochs if the step function of lr_scheduler is not called in the beginning of the epoch loop.\nOf course, you can avoid this by calling lr_scheduler's step in the beginning,\nbut I don't think this is proper use since, incase of optimizer, step is called in the end of the iteration loop.\nI've simply avoided all above issues by setting last_epoch as 0 after the initialization.\nThis also makes sense when you init with some value of last_epoch which is not -1.\nFor example, if you want to init with last epoch 10,\nlr should not be set with decayed 1 step further. Which is\nlast_epoch gets +1 in the previous code.\nbase_lr * self.gamma ** self.last_epoch\nInstead, it should be set with step 10 exact value.\nI hope this fix find it's way with all your help :)\nI'm really looking forward & excited to become a contributor for pytorch!\nPytorch Rocks!!", "body": "Hello everyone :) !!\r\n\r\nI've found that lr_scheduler was initialized with last_epoch as -1.\r\nThis causes that even after the first step (not the one in init but explicit step of scheduler),\r\nlearning rate of scheduler's optimizer remains as the previous.\r\n```python\r\n>>> import torch\r\n>>> cc = torch.nn.Conv2d(10,10,3)\r\n>>> myinitial_lr = 0.1\r\n>>> myoptimizer = torch.optim.Adam(cc.parameters(), lr=myinitial_lr)\r\n>>> mylrdecay = 0.5\r\n>>> myscheduler = torch.optim.lr_scheduler.ExponentialLR(myoptimizer,mylrdecay)\r\n\r\n# get_lr value and optimizer's lr value is not consistent, last_epoch should be 0 instead of -1\r\n>>> myscheduler.get_lr()\r\n[0.2]    # this is because of  get_lr calculates lr by 0.1 * 0.5^-1\r\n>>> myscheduler.optimizer.param_groups[0][\"lr\"]\r\n0.1    # this is not consistent with get_lr value\r\n>>> myscheduler.last_epoch\r\n-1\r\n\r\n# values seem to be in sync but should have been decayed value of 0.05 after first step(decay)\r\n>>> myscheduler.step()\r\n>>> myscheduler.get_lr()\r\n[0.1]    # this should be the value right after the init, not after first step\r\n>>> myscheduler.optimizer.param_groups[0][\"lr\"]\r\n0.1    # since this is after first step, it should have been decayed as 0.05\r\n>>> myscheduler.last_epoch\r\n0\r\n\r\n>>> myscheduler.step()\r\n>>> myscheduler.last_epoch\r\n1\r\n>>> myscheduler.get_lr()\r\n[0.05]\r\n>>> myscheduler.optimizer.param_groups[0][\"lr\"]\r\n0.05\r\n>>> myscheduler.last_epoch\r\n1\r\n```\r\n\r\nFirst problem is, even after the init of lr_scheduler, you get the inconsistent parameter values.\r\n\r\nThe second problem is, you are stuck with same learning rate in the first 2 epochs if the step function of lr_scheduler is not called in the beginning of the epoch loop.\r\nOf course, you can avoid this by calling lr_scheduler's step in the beginning,\r\nbut I don't think this is proper use since, incase of optimizer, step is called in the end of the iteration loop.\r\n\r\nI've simply avoided all above issues by setting last_epoch as 0 after the initialization.\r\n\r\nThis also makes sense when you init with some value of last_epoch which is not -1.\r\nFor example, if you want to init with last epoch 10,\r\nlr should not be set with decayed 1 step further. Which is \r\nlast_epoch gets +1 in the previous code.\r\nbase_lr * self.gamma ** self.last_epoch\r\n\r\nInstead, it should be set with step 10 exact value.\r\n\r\nI hope this fix find it's way with all your help :)\r\nI'm really looking forward & excited to become a contributor for pytorch!\r\nPytorch Rocks!!"}