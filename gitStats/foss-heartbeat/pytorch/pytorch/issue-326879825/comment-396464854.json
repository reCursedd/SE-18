{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396464854", "html_url": "https://github.com/pytorch/pytorch/pull/7889#issuecomment-396464854", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7889", "id": 396464854, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjQ2NDg1NA==", "user": {"login": "bado-lee", "id": 26222919, "node_id": "MDQ6VXNlcjI2MjIyOTE5", "avatar_url": "https://avatars1.githubusercontent.com/u/26222919?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bado-lee", "html_url": "https://github.com/bado-lee", "followers_url": "https://api.github.com/users/bado-lee/followers", "following_url": "https://api.github.com/users/bado-lee/following{/other_user}", "gists_url": "https://api.github.com/users/bado-lee/gists{/gist_id}", "starred_url": "https://api.github.com/users/bado-lee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bado-lee/subscriptions", "organizations_url": "https://api.github.com/users/bado-lee/orgs", "repos_url": "https://api.github.com/users/bado-lee/repos", "events_url": "https://api.github.com/users/bado-lee/events{/privacy}", "received_events_url": "https://api.github.com/users/bado-lee/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-12T04:50:21Z", "updated_at": "2018-06-12T04:50:21Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a><br>\nThank you for your comment.<br>\nYes you are are right on the docs. So I fixed them also.</p>\n<p>There are several points about this.</p>\n<ol>\n<li>\n<p>Meaning of \"Step\" should be stepping forward from the initial status.<br>\nPrevious implementation requires step to be part of the initialization. Which is ambiguous.<br>\nWhich means, the very first step in the loop is actually an initialization rather than actual \"Step\".<br>\nIt will be clear if step can be called only when needed.</p>\n</li>\n<li>\n<p>This code fixes inconsistency of results between the following before calling the very first step. (stated above)</p>\n</li>\n</ol>\n<pre><code>&gt;&gt;&gt; myscheduler.get_lr()\n&gt;&gt;&gt; myscheduler.optimizer.param_groups[0][\"lr\"]\n</code></pre>\n<ol start=\"3\">\n<li>There is already a case where scheduler step should be called in the end of the loop</li>\n</ol>\n<pre><code>class ReduceLROnPlateau(object):\n...\nExample:\n    &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    &gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, 'min')\n    &gt;&gt;&gt; for epoch in range(10):\n    &gt;&gt;&gt;     train(...)\n    &gt;&gt;&gt;     val_loss = validate(...)\n    &gt;&gt;&gt;     # Note that step should be called after validate()\n    &gt;&gt;&gt;     scheduler.step(val_loss)\n\"\"\"\n</code></pre>\n<p>I'll update if further fixes are required.</p>", "body_text": "@ezyang\nThank you for your comment.\nYes you are are right on the docs. So I fixed them also.\nThere are several points about this.\n\n\nMeaning of \"Step\" should be stepping forward from the initial status.\nPrevious implementation requires step to be part of the initialization. Which is ambiguous.\nWhich means, the very first step in the loop is actually an initialization rather than actual \"Step\".\nIt will be clear if step can be called only when needed.\n\n\nThis code fixes inconsistency of results between the following before calling the very first step. (stated above)\n\n\n>>> myscheduler.get_lr()\n>>> myscheduler.optimizer.param_groups[0][\"lr\"]\n\n\nThere is already a case where scheduler step should be called in the end of the loop\n\nclass ReduceLROnPlateau(object):\n...\nExample:\n    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n    >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n    >>> for epoch in range(10):\n    >>>     train(...)\n    >>>     val_loss = validate(...)\n    >>>     # Note that step should be called after validate()\n    >>>     scheduler.step(val_loss)\n\"\"\"\n\nI'll update if further fixes are required.", "body": "@ezyang \r\nThank you for your comment.\r\nYes you are are right on the docs. So I fixed them also.\r\n\r\nThere are several points about this.\r\n\r\n1. Meaning of \"Step\" should be stepping forward from the initial status.\r\n    Previous implementation requires step to be part of the initialization. Which is ambiguous.\r\n    Which means, the very first step in the loop is actually an initialization rather than actual \"Step\".\r\n    It will be clear if step can be called only when needed.\r\n\r\n2. This code fixes inconsistency of results between the following before calling the very first step. (stated above)\r\n```\r\n>>> myscheduler.get_lr()\r\n>>> myscheduler.optimizer.param_groups[0][\"lr\"]\r\n```\r\n \r\n3. There is already a case where scheduler step should be called in the end of the loop\r\n```\r\nclass ReduceLROnPlateau(object):\r\n...\r\nExample:\r\n    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\n    >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\r\n    >>> for epoch in range(10):\r\n    >>>     train(...)\r\n    >>>     val_loss = validate(...)\r\n    >>>     # Note that step should be called after validate()\r\n    >>>     scheduler.step(val_loss)\r\n\"\"\"\r\n```\r\n\r\nI'll update if further fixes are required."}