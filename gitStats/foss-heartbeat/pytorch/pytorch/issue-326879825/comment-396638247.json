{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396638247", "html_url": "https://github.com/pytorch/pytorch/pull/7889#issuecomment-396638247", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7889", "id": 396638247, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjYzODI0Nw==", "user": {"login": "bado-lee", "id": 26222919, "node_id": "MDQ6VXNlcjI2MjIyOTE5", "avatar_url": "https://avatars1.githubusercontent.com/u/26222919?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bado-lee", "html_url": "https://github.com/bado-lee", "followers_url": "https://api.github.com/users/bado-lee/followers", "following_url": "https://api.github.com/users/bado-lee/following{/other_user}", "gists_url": "https://api.github.com/users/bado-lee/gists{/gist_id}", "starred_url": "https://api.github.com/users/bado-lee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bado-lee/subscriptions", "organizations_url": "https://api.github.com/users/bado-lee/orgs", "repos_url": "https://api.github.com/users/bado-lee/repos", "events_url": "https://api.github.com/users/bado-lee/events{/privacy}", "received_events_url": "https://api.github.com/users/bado-lee/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-12T15:45:50Z", "updated_at": "2018-06-12T15:45:50Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Thanks for the comment.<br>\nI've looked for what you have mentioned in <a href=\"https://github.com/keras-team/keras/blob/0237542f48566c9115b3dde4cce920b00e25507a/keras/callbacks.py#L609\">keras</a></p>\n<pre><code>class LearningRateScheduler(Callback):\n    \"\"\"Learning rate scheduler.\n    # Arguments\n        schedule: a function that takes an epoch index as input\n            (integer, indexed from 0) and current learning rate\n            and returns a new learning rate as output (float).\n        verbose: int. 0: quiet, 1: update messages.\n    \"\"\"\n\n    def __init__(self, schedule, verbose=0):\n        super(LearningRateScheduler, self).__init__()\n        self.schedule = schedule\n        self.verbose = verbose\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, 'lr'):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        lr = float(K.get_value(self.model.optimizer.lr))\n        try:  # new API\n            lr = self.schedule(epoch, lr)\n        except TypeError:  # old API for backward compatibility\n            lr = self.schedule(epoch)\n        if not isinstance(lr, (float, np.float32, np.float64)):\n            raise ValueError('The output of the \"schedule\" function '\n                             'should be float.')\n        K.set_value(self.model.optimizer.lr, lr)\n        if self.verbose &gt; 0:\n            print('\\nEpoch %05d: LearningRateScheduler reducing learning '\n                  'rate to %s.' % (epoch + 1, lr))\n</code></pre>\n<p>Well as you said, this one in Keras specifies the function name as \"on_epoch_begin\" specifically which is counterpart of \"step\" in pytorch. Yes, this one is required to be call in the beginning of the loop.<br>\nBut I think it's behavior and intention is different from what is implemented in pytorch.<br>\nThe naming is not \"step\", it's \"on_epoch_begin\". Which are clearly different.<br>\nFurthermore, \"on_epoch_begin\" is required to have epoch number as an input variable where \"step\" is not.<br>\nSo, IMHO, it's not valid that this implementation matches that of Keras.<br>\nAnd, to me it's more reasonable that naming \"step\" is to be matched with the behavior of the optimizer step in pytorch which is called in the end in the loop.</p>", "body_text": "@apaszke Thanks for the comment.\nI've looked for what you have mentioned in keras\nclass LearningRateScheduler(Callback):\n    \"\"\"Learning rate scheduler.\n    # Arguments\n        schedule: a function that takes an epoch index as input\n            (integer, indexed from 0) and current learning rate\n            and returns a new learning rate as output (float).\n        verbose: int. 0: quiet, 1: update messages.\n    \"\"\"\n\n    def __init__(self, schedule, verbose=0):\n        super(LearningRateScheduler, self).__init__()\n        self.schedule = schedule\n        self.verbose = verbose\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if not hasattr(self.model.optimizer, 'lr'):\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\n        lr = float(K.get_value(self.model.optimizer.lr))\n        try:  # new API\n            lr = self.schedule(epoch, lr)\n        except TypeError:  # old API for backward compatibility\n            lr = self.schedule(epoch)\n        if not isinstance(lr, (float, np.float32, np.float64)):\n            raise ValueError('The output of the \"schedule\" function '\n                             'should be float.')\n        K.set_value(self.model.optimizer.lr, lr)\n        if self.verbose > 0:\n            print('\\nEpoch %05d: LearningRateScheduler reducing learning '\n                  'rate to %s.' % (epoch + 1, lr))\n\nWell as you said, this one in Keras specifies the function name as \"on_epoch_begin\" specifically which is counterpart of \"step\" in pytorch. Yes, this one is required to be call in the beginning of the loop.\nBut I think it's behavior and intention is different from what is implemented in pytorch.\nThe naming is not \"step\", it's \"on_epoch_begin\". Which are clearly different.\nFurthermore, \"on_epoch_begin\" is required to have epoch number as an input variable where \"step\" is not.\nSo, IMHO, it's not valid that this implementation matches that of Keras.\nAnd, to me it's more reasonable that naming \"step\" is to be matched with the behavior of the optimizer step in pytorch which is called in the end in the loop.", "body": "@apaszke Thanks for the comment.\r\nI've looked for what you have mentioned in [keras](https://github.com/keras-team/keras/blob/0237542f48566c9115b3dde4cce920b00e25507a/keras/callbacks.py#L609)\r\n```\r\nclass LearningRateScheduler(Callback):\r\n    \"\"\"Learning rate scheduler.\r\n    # Arguments\r\n        schedule: a function that takes an epoch index as input\r\n            (integer, indexed from 0) and current learning rate\r\n            and returns a new learning rate as output (float).\r\n        verbose: int. 0: quiet, 1: update messages.\r\n    \"\"\"\r\n\r\n    def __init__(self, schedule, verbose=0):\r\n        super(LearningRateScheduler, self).__init__()\r\n        self.schedule = schedule\r\n        self.verbose = verbose\r\n\r\n    def on_epoch_begin(self, epoch, logs=None):\r\n        if not hasattr(self.model.optimizer, 'lr'):\r\n            raise ValueError('Optimizer must have a \"lr\" attribute.')\r\n        lr = float(K.get_value(self.model.optimizer.lr))\r\n        try:  # new API\r\n            lr = self.schedule(epoch, lr)\r\n        except TypeError:  # old API for backward compatibility\r\n            lr = self.schedule(epoch)\r\n        if not isinstance(lr, (float, np.float32, np.float64)):\r\n            raise ValueError('The output of the \"schedule\" function '\r\n                             'should be float.')\r\n        K.set_value(self.model.optimizer.lr, lr)\r\n        if self.verbose > 0:\r\n            print('\\nEpoch %05d: LearningRateScheduler reducing learning '\r\n                  'rate to %s.' % (epoch + 1, lr))\r\n```\r\nWell as you said, this one in Keras specifies the function name as \"on_epoch_begin\" specifically which is counterpart of \"step\" in pytorch. Yes, this one is required to be call in the beginning of the loop.\r\nBut I think it's behavior and intention is different from what is implemented in pytorch.\r\nThe naming is not \"step\", it's \"on_epoch_begin\". Which are clearly different.\r\nFurthermore, \"on_epoch_begin\" is required to have epoch number as an input variable where \"step\" is not.\r\nSo, IMHO, it's not valid that this implementation matches that of Keras.\r\nAnd, to me it's more reasonable that naming \"step\" is to be matched with the behavior of the optimizer step in pytorch which is called in the end in the loop.\r\n\r\n\r\n"}