{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/420262643", "html_url": "https://github.com/pytorch/pytorch/issues/4582#issuecomment-420262643", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4582", "id": 420262643, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDI2MjY0Mw==", "user": {"login": "yucoian", "id": 12205308, "node_id": "MDQ6VXNlcjEyMjA1MzA4", "avatar_url": "https://avatars3.githubusercontent.com/u/12205308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yucoian", "html_url": "https://github.com/yucoian", "followers_url": "https://api.github.com/users/yucoian/followers", "following_url": "https://api.github.com/users/yucoian/following{/other_user}", "gists_url": "https://api.github.com/users/yucoian/gists{/gist_id}", "starred_url": "https://api.github.com/users/yucoian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yucoian/subscriptions", "organizations_url": "https://api.github.com/users/yucoian/orgs", "repos_url": "https://api.github.com/users/yucoian/repos", "events_url": "https://api.github.com/users/yucoian/events{/privacy}", "received_events_url": "https://api.github.com/users/yucoian/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-11T12:50:53Z", "updated_at": "2018-09-11T12:59:01Z", "author_association": "NONE", "body_html": "<p>uha.... I found this problem too. In order to learn the BiRNN char level embedding ,  the char of the pad token obviously is pad again. Thus,  the length of the pad token is zero. That's to say, when we learn the char level embedding of the pad token, the batch_size is zero. So, in this way, it' not very convenient. However I put forward a nice method to solve it. Maybe it helps:</p>\n<p>class CharEncoder(nn.Module):<br>\ndef <strong>init</strong>(self, input_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True):<br>\nsuper(CharEncoder, self).<strong>init</strong>()<br>\nself.rnn = nn.GRU(input_size=input_size,<br>\nhidden_size=hidden_size,<br>\nnum_layers=num_layers,<br>\nbatch_first=batch_first,<br>\nbidirectional=bidirectional)</p>\n<pre><code>def forward(self, x, mask):\n    \"\"\"\n    :param x:            [batch*seq_len,  max_word_len, char_emb_size]\n    :param mask:         [batch*seq_len, max_word_len]\n    :return result:      [batch, output_dim]\n    \"\"\"\n    lengths = mask.eq(0).long().sum(1)         # NOTE: 1 for mask.\n    lengths_sort, idx_sort = torch.sort(lengths, dim=0, descending=True)\n    _, idx_unsort = torch.sort(idx_sort, dim=0)\n\n    x_sort = x.index_select(0, idx_sort)\n\n    # NOTE : in case pad token.\n    for i, _ in enumerate(lengths_sort):\n        if lengths_sort[i] == 0:\n            lengths_sort[i] = 1\n    print(\"lengths_sort:\", lengths_sort)\n    print(\"x_sort.shape\", x_sort.size())\n    x_pack = torch.nn.utils.rnn.pack_padded_sequence(x_sort, lengths_sort, batch_first=True)\n    o_pack, hn = self.rnn(x_pack)\n    o, _ = torch.nn.utils.rnn.pad_packed_sequence(o_pack, batch_first=True)\n\n    # unsorted o\n    o_unsort = o.index_select(0, idx_unsort)  # Note that here first dim is batch\n    hn = hn.transpose(0,1).index_select(0, idx_unsort)\n\n    o_unsort = o_unsort.transpose(0, 1)       # Note that here first dim is seq_len\uff0cfor gather purpose.\n\n    # get the last time state\n    for i, _ in enumerate(lengths):  # NOTE : in case pad token.\n        if lengths[i] == 0:\n            lengths[i] = 1\n    len_idx = (lengths - 1).view(-1, 1).expand(-1, o_unsort.size(2)).unsqueeze(0)\n    # bidirectional bugs\n    len_idx_reversed = torch.zeros_like(len_idx)\n    o_last = torch.cat([o_unsort.gather(0, len_idx)[:, :, :int(o_unsort.size(2) / 2)],\n                        o_unsort.gather(0, len_idx_reversed)[:, :, int(-o_unsort.size(2) / 2):]], -1)\n    o_last = o_last.squeeze(0)\n\n    print(o_unsort.transpose(0, 1))             #[batch, seq_len, output_dim]\n    #print(o_last)\n    print(\"hn.shape:\", hn.shape)\n    print(\"hn\",hn)\n    print(\"self.rnn(x)\", self.rnn(x))\n    return o_last\n</code></pre>", "body_text": "uha.... I found this problem too. In order to learn the BiRNN char level embedding ,  the char of the pad token obviously is pad again. Thus,  the length of the pad token is zero. That's to say, when we learn the char level embedding of the pad token, the batch_size is zero. So, in this way, it' not very convenient. However I put forward a nice method to solve it. Maybe it helps:\nclass CharEncoder(nn.Module):\ndef init(self, input_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True):\nsuper(CharEncoder, self).init()\nself.rnn = nn.GRU(input_size=input_size,\nhidden_size=hidden_size,\nnum_layers=num_layers,\nbatch_first=batch_first,\nbidirectional=bidirectional)\ndef forward(self, x, mask):\n    \"\"\"\n    :param x:            [batch*seq_len,  max_word_len, char_emb_size]\n    :param mask:         [batch*seq_len, max_word_len]\n    :return result:      [batch, output_dim]\n    \"\"\"\n    lengths = mask.eq(0).long().sum(1)         # NOTE: 1 for mask.\n    lengths_sort, idx_sort = torch.sort(lengths, dim=0, descending=True)\n    _, idx_unsort = torch.sort(idx_sort, dim=0)\n\n    x_sort = x.index_select(0, idx_sort)\n\n    # NOTE : in case pad token.\n    for i, _ in enumerate(lengths_sort):\n        if lengths_sort[i] == 0:\n            lengths_sort[i] = 1\n    print(\"lengths_sort:\", lengths_sort)\n    print(\"x_sort.shape\", x_sort.size())\n    x_pack = torch.nn.utils.rnn.pack_padded_sequence(x_sort, lengths_sort, batch_first=True)\n    o_pack, hn = self.rnn(x_pack)\n    o, _ = torch.nn.utils.rnn.pad_packed_sequence(o_pack, batch_first=True)\n\n    # unsorted o\n    o_unsort = o.index_select(0, idx_unsort)  # Note that here first dim is batch\n    hn = hn.transpose(0,1).index_select(0, idx_unsort)\n\n    o_unsort = o_unsort.transpose(0, 1)       # Note that here first dim is seq_len\uff0cfor gather purpose.\n\n    # get the last time state\n    for i, _ in enumerate(lengths):  # NOTE : in case pad token.\n        if lengths[i] == 0:\n            lengths[i] = 1\n    len_idx = (lengths - 1).view(-1, 1).expand(-1, o_unsort.size(2)).unsqueeze(0)\n    # bidirectional bugs\n    len_idx_reversed = torch.zeros_like(len_idx)\n    o_last = torch.cat([o_unsort.gather(0, len_idx)[:, :, :int(o_unsort.size(2) / 2)],\n                        o_unsort.gather(0, len_idx_reversed)[:, :, int(-o_unsort.size(2) / 2):]], -1)\n    o_last = o_last.squeeze(0)\n\n    print(o_unsort.transpose(0, 1))             #[batch, seq_len, output_dim]\n    #print(o_last)\n    print(\"hn.shape:\", hn.shape)\n    print(\"hn\",hn)\n    print(\"self.rnn(x)\", self.rnn(x))\n    return o_last", "body": "uha.... I found this problem too. In order to learn the BiRNN char level embedding ,  the char of the pad token obviously is pad again. Thus,  the length of the pad token is zero. That's to say, when we learn the char level embedding of the pad token, the batch_size is zero. So, in this way, it' not very convenient. However I put forward a nice method to solve it. Maybe it helps:\r\n\r\n\r\nclass CharEncoder(nn.Module):\r\n    def __init__(self, input_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True):\r\n        super(CharEncoder, self).__init__()\r\n        self.rnn = nn.GRU(input_size=input_size,\r\n                          hidden_size=hidden_size,\r\n                          num_layers=num_layers,\r\n                          batch_first=batch_first,\r\n                          bidirectional=bidirectional)\r\n\r\n    def forward(self, x, mask):\r\n        \"\"\"\r\n        :param x:            [batch*seq_len,  max_word_len, char_emb_size]\r\n        :param mask:         [batch*seq_len, max_word_len]\r\n        :return result:      [batch, output_dim]\r\n        \"\"\"\r\n        lengths = mask.eq(0).long().sum(1)         # NOTE: 1 for mask.\r\n        lengths_sort, idx_sort = torch.sort(lengths, dim=0, descending=True)\r\n        _, idx_unsort = torch.sort(idx_sort, dim=0)\r\n\r\n        x_sort = x.index_select(0, idx_sort)\r\n\r\n        # NOTE : in case pad token.\r\n        for i, _ in enumerate(lengths_sort):\r\n            if lengths_sort[i] == 0:\r\n                lengths_sort[i] = 1\r\n        print(\"lengths_sort:\", lengths_sort)\r\n        print(\"x_sort.shape\", x_sort.size())\r\n        x_pack = torch.nn.utils.rnn.pack_padded_sequence(x_sort, lengths_sort, batch_first=True)\r\n        o_pack, hn = self.rnn(x_pack)\r\n        o, _ = torch.nn.utils.rnn.pad_packed_sequence(o_pack, batch_first=True)\r\n\r\n        # unsorted o\r\n        o_unsort = o.index_select(0, idx_unsort)  # Note that here first dim is batch\r\n        hn = hn.transpose(0,1).index_select(0, idx_unsort)\r\n\r\n        o_unsort = o_unsort.transpose(0, 1)       # Note that here first dim is seq_len\uff0cfor gather purpose.\r\n\r\n        # get the last time state\r\n        for i, _ in enumerate(lengths):  # NOTE : in case pad token.\r\n            if lengths[i] == 0:\r\n                lengths[i] = 1\r\n        len_idx = (lengths - 1).view(-1, 1).expand(-1, o_unsort.size(2)).unsqueeze(0)\r\n        # bidirectional bugs\r\n        len_idx_reversed = torch.zeros_like(len_idx)\r\n        o_last = torch.cat([o_unsort.gather(0, len_idx)[:, :, :int(o_unsort.size(2) / 2)],\r\n                            o_unsort.gather(0, len_idx_reversed)[:, :, int(-o_unsort.size(2) / 2):]], -1)\r\n        o_last = o_last.squeeze(0)\r\n\r\n        print(o_unsort.transpose(0, 1))             #[batch, seq_len, output_dim]\r\n        #print(o_last)\r\n        print(\"hn.shape:\", hn.shape)\r\n        print(\"hn\",hn)\r\n        print(\"self.rnn(x)\", self.rnn(x))\r\n        return o_last\r\n\r\n"}