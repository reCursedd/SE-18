{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/431808675", "html_url": "https://github.com/pytorch/pytorch/issues/4582#issuecomment-431808675", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4582", "id": 431808675, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTgwODY3NQ==", "user": {"login": "turangojayev", "id": 4106961, "node_id": "MDQ6VXNlcjQxMDY5NjE=", "avatar_url": "https://avatars2.githubusercontent.com/u/4106961?v=4", "gravatar_id": "", "url": "https://api.github.com/users/turangojayev", "html_url": "https://github.com/turangojayev", "followers_url": "https://api.github.com/users/turangojayev/followers", "following_url": "https://api.github.com/users/turangojayev/following{/other_user}", "gists_url": "https://api.github.com/users/turangojayev/gists{/gist_id}", "starred_url": "https://api.github.com/users/turangojayev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/turangojayev/subscriptions", "organizations_url": "https://api.github.com/users/turangojayev/orgs", "repos_url": "https://api.github.com/users/turangojayev/repos", "events_url": "https://api.github.com/users/turangojayev/events{/privacy}", "received_events_url": "https://api.github.com/users/turangojayev/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-22T11:31:19Z", "updated_at": "2018-10-22T11:31:19Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12205308\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yucoian\">@yucoian</a> , I also want to use char embeddings with LSTM now and stumbled upon this post and was trying to understand your code, however having difficulty with understanding it completely. I am a newbie to pytorch and have some questions:</p>\n<ol>\n<li><code>:return result:      [batch, output_dim]</code> - is the return shape [batch, output_dim] or [batch * seq_len, output_dim]?</li>\n<li><code>o_last = torch.cat([o_unsort.gather(0, len_idx)[:, :, :int(o_unsort.size(2) / 2)], o_unsort.gather(0, len_idx_reversed)[:, :, int(-o_unsort.size(2) / 2):]], -1)</code><br>\nWhy do you gather it this way? Doesn't pytorch take care of bidirectional rnns itself and you have to gather as if it were only one direction from the output?</li>\n</ol>", "body_text": "Hi @yucoian , I also want to use char embeddings with LSTM now and stumbled upon this post and was trying to understand your code, however having difficulty with understanding it completely. I am a newbie to pytorch and have some questions:\n\n:return result:      [batch, output_dim] - is the return shape [batch, output_dim] or [batch * seq_len, output_dim]?\no_last = torch.cat([o_unsort.gather(0, len_idx)[:, :, :int(o_unsort.size(2) / 2)], o_unsort.gather(0, len_idx_reversed)[:, :, int(-o_unsort.size(2) / 2):]], -1)\nWhy do you gather it this way? Doesn't pytorch take care of bidirectional rnns itself and you have to gather as if it were only one direction from the output?", "body": "Hi @yucoian , I also want to use char embeddings with LSTM now and stumbled upon this post and was trying to understand your code, however having difficulty with understanding it completely. I am a newbie to pytorch and have some questions:\r\n1. `:return result:      [batch, output_dim]` - is the return shape [batch, output_dim] or [batch * seq_len, output_dim]?\r\n2. `o_last = torch.cat([o_unsort.gather(0, len_idx)[:, :, :int(o_unsort.size(2) / 2)],\r\n                        o_unsort.gather(0, len_idx_reversed)[:, :, int(-o_unsort.size(2) / 2):]], -1)`\r\nWhy do you gather it this way? Doesn't pytorch take care of bidirectional rnns itself and you have to gather as if it were only one direction from the output?\r\n"}