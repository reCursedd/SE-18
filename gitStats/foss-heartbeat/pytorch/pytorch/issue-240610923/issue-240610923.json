{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1984", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1984/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1984/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1984/events", "html_url": "https://github.com/pytorch/pytorch/issues/1984", "id": 240610923, "node_id": "MDU6SXNzdWUyNDA2MTA5MjM=", "number": 1984, "title": "Different results for batch size 1 Variables", "user": {"login": "ethanluoyc", "id": 6040760, "node_id": "MDQ6VXNlcjYwNDA3NjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/6040760?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ethanluoyc", "html_url": "https://github.com/ethanluoyc", "followers_url": "https://api.github.com/users/ethanluoyc/followers", "following_url": "https://api.github.com/users/ethanluoyc/following{/other_user}", "gists_url": "https://api.github.com/users/ethanluoyc/gists{/gist_id}", "starred_url": "https://api.github.com/users/ethanluoyc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ethanluoyc/subscriptions", "organizations_url": "https://api.github.com/users/ethanluoyc/orgs", "repos_url": "https://api.github.com/users/ethanluoyc/repos", "events_url": "https://api.github.com/users/ethanluoyc/events{/privacy}", "received_events_url": "https://api.github.com/users/ethanluoyc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-05T10:45:15Z", "updated_at": "2017-07-05T10:49:04Z", "closed_at": "2017-07-05T10:49:04Z", "author_association": "CONTRIBUTOR", "body_html": "<p>pytorch version: '0.1.12_2'</p>\n<p>Run this snippet (The required serialised tensor has been included in the attachment.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n\ntorch.manual_seed(<span class=\"pl-c1\">0</span>)\n\nx_restored <span class=\"pl-k\">=</span> Variable(torch.load(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x.pth<span class=\"pl-pds\">'</span></span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nx_res_repeated <span class=\"pl-k\">=</span> x_restored.repeat(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Network</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">dim_in</span>, <span class=\"pl-smi\">dim_out</span>):\n        <span class=\"pl-c1\">super</span>(Network, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.m <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.Linear(dim_in, <span class=\"pl-c1\">100</span>),\n            nn.BatchNorm1d(<span class=\"pl-c1\">100</span>),\n            nn.ReLU(),\n            nn.Linear(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>),\n            nn.BatchNorm1d(<span class=\"pl-c1\">100</span>),\n            nn.ReLU(),\n            nn.Linear(<span class=\"pl-c1\">100</span>, dim_out<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>)\n        )\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">X</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.m(X)\n\nnet <span class=\"pl-k\">=</span> Network(x_restored.size()[<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">2</span>)\n<span class=\"pl-c1\">print</span>(net(x_restored))\n<span class=\"pl-c1\">print</span>(net(x_res_repeated))</pre></div>\n<p>The output</p>\n<pre><code>Variable containing:\n1.00000e-02 *\n -0.5704 -3.7342 -3.6541  7.4486\n[torch.FloatTensor of size 1x4]\n\nVariable containing:\n1.00000e-02 *\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.4867 -3.7301 -3.6103  7.4878\n -0.4867 -3.7301 -3.6103  7.4878\n[torch.FloatTensor of size 10x4]\n</code></pre>\n<p><a href=\"https://github.com/pytorch/pytorch/files/1124350/issue.zip\">issue.zip</a></p>", "body_text": "pytorch version: '0.1.12_2'\nRun this snippet (The required serialised tensor has been included in the attachment.\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\n\ntorch.manual_seed(0)\n\nx_restored = Variable(torch.load('x.pth'), requires_grad=False)\nx_res_repeated = x_restored.repeat(10, 1)\n\nclass Network(nn.Module):\n    def __init__(self, dim_in, dim_out):\n        super(Network, self).__init__()\n        self.m = nn.Sequential(\n            nn.Linear(dim_in, 100),\n            nn.BatchNorm1d(100),\n            nn.ReLU(),\n            nn.Linear(100, 100),\n            nn.BatchNorm1d(100),\n            nn.ReLU(),\n            nn.Linear(100, dim_out*2)\n        )\n\n    def forward(self, X):\n        return self.m(X)\n\nnet = Network(x_restored.size()[1], 2)\nprint(net(x_restored))\nprint(net(x_res_repeated))\nThe output\nVariable containing:\n1.00000e-02 *\n -0.5704 -3.7342 -3.6541  7.4486\n[torch.FloatTensor of size 1x4]\n\nVariable containing:\n1.00000e-02 *\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.5703 -3.7480 -3.6610  7.4439\n -0.4867 -3.7301 -3.6103  7.4878\n -0.4867 -3.7301 -3.6103  7.4878\n[torch.FloatTensor of size 10x4]\n\nissue.zip", "body": "pytorch version: '0.1.12_2'\r\n\r\nRun this snippet (The required serialised tensor has been included in the attachment.\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nfrom torch import nn\r\n\r\ntorch.manual_seed(0)\r\n\r\nx_restored = Variable(torch.load('x.pth'), requires_grad=False)\r\nx_res_repeated = x_restored.repeat(10, 1)\r\n\r\nclass Network(nn.Module):\r\n    def __init__(self, dim_in, dim_out):\r\n        super(Network, self).__init__()\r\n        self.m = nn.Sequential(\r\n            nn.Linear(dim_in, 100),\r\n            nn.BatchNorm1d(100),\r\n            nn.ReLU(),\r\n            nn.Linear(100, 100),\r\n            nn.BatchNorm1d(100),\r\n            nn.ReLU(),\r\n            nn.Linear(100, dim_out*2)\r\n        )\r\n\r\n    def forward(self, X):\r\n        return self.m(X)\r\n\r\nnet = Network(x_restored.size()[1], 2)\r\nprint(net(x_restored))\r\nprint(net(x_res_repeated))\r\n```\r\n\r\nThe output\r\n```\r\nVariable containing:\r\n1.00000e-02 *\r\n -0.5704 -3.7342 -3.6541  7.4486\r\n[torch.FloatTensor of size 1x4]\r\n\r\nVariable containing:\r\n1.00000e-02 *\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.5703 -3.7480 -3.6610  7.4439\r\n -0.4867 -3.7301 -3.6103  7.4878\r\n -0.4867 -3.7301 -3.6103  7.4878\r\n[torch.FloatTensor of size 10x4]\r\n```\r\n[issue.zip](https://github.com/pytorch/pytorch/files/1124350/issue.zip)\r\n"}