{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1192", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1192/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1192/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1192/events", "html_url": "https://github.com/pytorch/pytorch/issues/1192", "id": 219594218, "node_id": "MDU6SXNzdWUyMTk1OTQyMTg=", "number": 1192, "title": "Problem with CosineEmbeddingLoss()", "user": {"login": "linxd5", "id": 9481955, "node_id": "MDQ6VXNlcjk0ODE5NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9481955?v=4", "gravatar_id": "", "url": "https://api.github.com/users/linxd5", "html_url": "https://github.com/linxd5", "followers_url": "https://api.github.com/users/linxd5/followers", "following_url": "https://api.github.com/users/linxd5/following{/other_user}", "gists_url": "https://api.github.com/users/linxd5/gists{/gist_id}", "starred_url": "https://api.github.com/users/linxd5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/linxd5/subscriptions", "organizations_url": "https://api.github.com/users/linxd5/orgs", "repos_url": "https://api.github.com/users/linxd5/repos", "events_url": "https://api.github.com/users/linxd5/events{/privacy}", "received_events_url": "https://api.github.com/users/linxd5/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-04-05T14:05:15Z", "updated_at": "2017-04-06T07:50:30Z", "closed_at": "2017-04-05T15:12:57Z", "author_association": "NONE", "body_html": "<p>The Following is the test code for CosineEmbeddingLoss()</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\n\ncuda_num = 2\nbatch_size = 50\n\nloss_fn = torch.nn.CosineEmbeddingLoss().cuda(cuda_num)\n\n\nfor i in range(500):\n    sentences = Variable(torch.randn(batch_size, 300)).cuda(cuda_num)\n    images = Variable(torch.randn(batch_size, 400)).cuda(cuda_num)\n    flags = Variable(torch.ones(batch_size)).cuda(cuda_num)\n\n    fc = torch.nn.Linear(400, 300).cuda(cuda_num)\n    optimizer = torch.optim.SGD(fc.parameters(), lr=1e-4)\n    img_emb = fc(images)\n    sen_emb = sentences\n\n    loss = loss_fn(img_emb, sen_emb, flags)\n    print 'Batch_id %d \\t  loss %.2f' %(i, loss.data.mean())\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n</code></pre>\n<p>In my compute, it raise the following traceback:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test2.py\", line 19, in &lt;module&gt;\n    loss = loss_fn(img_emb, sen_emb, flags)\n  File \"/home/lindayong/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 202, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/lindayong/anaconda2/lib/python2.7/site-packages/torch/nn/modules/loss.py\", line 365, in forward\n    self.size_average)(input1, input2, target)\n  File \"/home/lindayong/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/loss.py\", line 47, in forward\n    torch.eq(y, -1, out=_idx)\nRuntimeError: arguments are located on different GPUs at /data/users/soumith/builder/wheel/pytorch-src/torch/lib/THC/generated/../generic/THCTensorMathCompare.cu:39\n</code></pre>\n<p>There are three GPUs in my computer, the bug arises when I set cuda_num to 1 or 2, but no bug when I set cuda_num to 0.</p>", "body_text": "The Following is the test code for CosineEmbeddingLoss()\nimport torch\nfrom torch.autograd import Variable\n\ncuda_num = 2\nbatch_size = 50\n\nloss_fn = torch.nn.CosineEmbeddingLoss().cuda(cuda_num)\n\n\nfor i in range(500):\n    sentences = Variable(torch.randn(batch_size, 300)).cuda(cuda_num)\n    images = Variable(torch.randn(batch_size, 400)).cuda(cuda_num)\n    flags = Variable(torch.ones(batch_size)).cuda(cuda_num)\n\n    fc = torch.nn.Linear(400, 300).cuda(cuda_num)\n    optimizer = torch.optim.SGD(fc.parameters(), lr=1e-4)\n    img_emb = fc(images)\n    sen_emb = sentences\n\n    loss = loss_fn(img_emb, sen_emb, flags)\n    print 'Batch_id %d \\t  loss %.2f' %(i, loss.data.mean())\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n\nIn my compute, it raise the following traceback:\nTraceback (most recent call last):\n  File \"test2.py\", line 19, in <module>\n    loss = loss_fn(img_emb, sen_emb, flags)\n  File \"/home/lindayong/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 202, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/lindayong/anaconda2/lib/python2.7/site-packages/torch/nn/modules/loss.py\", line 365, in forward\n    self.size_average)(input1, input2, target)\n  File \"/home/lindayong/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/loss.py\", line 47, in forward\n    torch.eq(y, -1, out=_idx)\nRuntimeError: arguments are located on different GPUs at /data/users/soumith/builder/wheel/pytorch-src/torch/lib/THC/generated/../generic/THCTensorMathCompare.cu:39\n\nThere are three GPUs in my computer, the bug arises when I set cuda_num to 1 or 2, but no bug when I set cuda_num to 0.", "body": "The Following is the test code for CosineEmbeddingLoss()\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ncuda_num = 2\r\nbatch_size = 50\r\n\r\nloss_fn = torch.nn.CosineEmbeddingLoss().cuda(cuda_num)\r\n\r\n\r\nfor i in range(500):\r\n    sentences = Variable(torch.randn(batch_size, 300)).cuda(cuda_num)\r\n    images = Variable(torch.randn(batch_size, 400)).cuda(cuda_num)\r\n    flags = Variable(torch.ones(batch_size)).cuda(cuda_num)\r\n\r\n    fc = torch.nn.Linear(400, 300).cuda(cuda_num)\r\n    optimizer = torch.optim.SGD(fc.parameters(), lr=1e-4)\r\n    img_emb = fc(images)\r\n    sen_emb = sentences\r\n\r\n    loss = loss_fn(img_emb, sen_emb, flags)\r\n    print 'Batch_id %d \\t  loss %.2f' %(i, loss.data.mean())\r\n\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n```\r\n\r\nIn my compute, it raise the following traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test2.py\", line 19, in <module>\r\n    loss = loss_fn(img_emb, sen_emb, flags)\r\n  File \"/home/lindayong/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 202, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/lindayong/anaconda2/lib/python2.7/site-packages/torch/nn/modules/loss.py\", line 365, in forward\r\n    self.size_average)(input1, input2, target)\r\n  File \"/home/lindayong/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/loss.py\", line 47, in forward\r\n    torch.eq(y, -1, out=_idx)\r\nRuntimeError: arguments are located on different GPUs at /data/users/soumith/builder/wheel/pytorch-src/torch/lib/THC/generated/../generic/THCTensorMathCompare.cu:39\r\n```\r\n\r\nThere are three GPUs in my computer, the bug arises when I set cuda_num to 1 or 2, but no bug when I set cuda_num to 0. \r\n\r\n"}