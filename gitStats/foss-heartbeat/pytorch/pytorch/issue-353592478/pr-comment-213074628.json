{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213074628", "pull_request_review_id": 149834101, "id": 213074628, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMzA3NDYyOA==", "diff_hunk": "@@ -0,0 +1,512 @@\n+#include \"caffe2/operators/rnn/hip/recurrent_op_miopen.h\"\n+#include \"caffe2/utils/math.h\"\n+\n+#include <map>\n+\n+namespace caffe2 {\n+\n+namespace detail {\n+\n+template <typename T>\n+TensorDescriptors<T>::TensorDescriptors(\n+    size_t n,\n+    std::vector<int>& dim,\n+    std::vector<int>& stride) {\n+  descs_.resize(n);\n+  CAFFE_ENFORCE_EQ(dim.size(), stride.size());\n+  for (auto i = 0; i < n; ++i) {\n+    MIOPEN_ENFORCE(miopenCreateTensorDescriptor(&descs_[i]));\n+    MIOPEN_ENFORCE(miopenSetTensorDescriptor(\n+        descs_[i],\n+        miopenTypeWrapper<T>::type,\n+        dim.size(),\n+        dim.data(),\n+        stride.data()));\n+  }\n+}\n+\n+template <typename T>\n+TensorDescriptors<T>::~TensorDescriptors() {\n+  for (auto desc : descs_) {\n+    miopenDestroyTensorDescriptor(desc);\n+  }\n+}\n+}\n+\n+template <typename T>\n+RecurrentBaseOp<T>::RecurrentBaseOp(\n+    const OperatorDef& operator_def,\n+    Workspace* ws)\n+    : Operator<HIPContext>(operator_def, ws), miopen_wrapper_(&context_) {\n+  MIOPEN_ENFORCE(miopenCreateRNNDescriptor(&rnnDesc_));\n+  MIOPEN_ENFORCE(miopenCreateTensorDescriptor(&wDesc_));\n+  MIOPEN_ENFORCE(miopenCreateTensorDescriptor(&hxDesc_));\n+  MIOPEN_ENFORCE(miopenCreateTensorDescriptor(&cxDesc_));\n+  MIOPEN_ENFORCE(miopenCreateTensorDescriptor(&hyDesc_));\n+  MIOPEN_ENFORCE(miopenCreateTensorDescriptor(&cyDesc_));\n+}\n+\n+template <typename T>\n+RecurrentBaseOp<T>::~RecurrentBaseOp() {\n+  MIOPEN_ENFORCE(miopenDestroyRNNDescriptor(rnnDesc_));\n+  MIOPEN_ENFORCE(miopenDestroyTensorDescriptor(wDesc_));\n+  MIOPEN_ENFORCE(miopenDestroyTensorDescriptor(hxDesc_));\n+  MIOPEN_ENFORCE(miopenDestroyTensorDescriptor(cxDesc_));\n+  MIOPEN_ENFORCE(miopenDestroyTensorDescriptor(hyDesc_));\n+  MIOPEN_ENFORCE(miopenDestroyTensorDescriptor(cyDesc_));\n+}\n+\n+template <typename T>\n+void RecurrentBaseOp<T>::initialize(\n+    const Tensor& input,\n+    Tensor* output,\n+    Tensor* hiddenOutput,\n+    Tensor* cellOutput) {\n+  static_assert(sizeof(T) == 4, \"\"); // workaround clang bug\n+  CAFFE_ENFORCE_GE(input.ndim(), 3);\n+  const int seqLength = input.dim(0);\n+  const int batchSize = input.dim(1);\n+  const int inputDim = input.dim(2);\n+  const int hiddenSize = OperatorBase::GetSingleArgument<int>(\"hidden_size\", 0);\n+  CAFFE_ENFORCE_GT(hiddenSize, 0);\n+  const auto bidirectional =\n+      OperatorBase::GetSingleArgument<int>(\"bidirectional\", 0);\n+  CAFFE_ENFORCE(bidirectional == 0 || bidirectional == 1);\n+  const auto numDirections = bidirectional == 1 ? 2 : 1;\n+  const auto outputDim = hiddenSize * numDirections;\n+  const auto rnnDirection =\n+      bidirectional == 1 ? miopenRNNbidirection : miopenRNNunidirection;\n+  const auto numLayers = OperatorBase::GetSingleArgument<int>(\"num_layers\", 0);\n+  CAFFE_ENFORCE_GT(numLayers, 0);\n+  const auto& rnnModeStr =\n+      OperatorBase::GetSingleArgument<string>(\"rnn_mode\", \"\");\n+  CAFFE_ENFORCE(rnnModeStr == \"lstm\" || rnnModeStr == \"gru\");\n+  const auto rnnMode = rnnModeStr == \"lstm\" ? miopenLSTM : miopenGRU;\n+  const auto& rnnInputStr =\n+      OperatorBase::GetSingleArgument<string>(\"input_mode\", \"\");\n+  CAFFE_ENFORCE(rnnInputStr == \"linear\" || rnnInputStr == \"skip\");\n+  const auto rnnInput =\n+      rnnInputStr == \"linear\" ? miopenRNNlinear : miopenRNNskip;\n+\n+  // RNN setup\n+  {\n+    MIOPEN_ENFORCE(miopenSetRNNDescriptor(\n+        rnnDesc_,\n+        hiddenSize,\n+        numLayers,\n+        rnnInput,\n+        rnnDirection,\n+        rnnMode,\n+        miopenRNNwithBias, \n+        miopenRNNdefault,\n+        miopenTypeWrapper<T>::type));\n+  }\n+  // X setup\n+  {\n+    std::vector<int> xDesc_dim = {batchSize, inputDim, 1};\n+    std::vector<int> xDesc_stride = {inputDim, 1, 1};\n+    xDesc_.reset(new detail::TensorDescriptors<T>(\n+        seqLength,\n+        xDesc_dim,\n+        xDesc_stride));\n+  }\n+  // Y setup\n+  { std::vector<int> yDesc_dim = {batchSize, hiddenSize * numDirections, 1};\n+    std::vector<int> yDesc_stride = {numDirections * hiddenSize, 1, 1};\n+    yDesc_.reset(new detail::TensorDescriptors<T>(\n+        seqLength,\n+        yDesc_dim,\n+        yDesc_stride));\n+\n+    if (output) {\n+      output->Resize(std::vector<int>{seqLength, batchSize, outputDim});\n+    }\n+  }\n+\n+  // Hidden/Cell setup\n+  {\n+    std::array<int, 3> dim{\n+        numLayers * numDirections, batchSize, hiddenSize};\n+    std::array<int, 3> stride{batchSize * hiddenSize, hiddenSize, 1};\n+    MIOPEN_ENFORCE(miopenSetTensorDescriptor(\n+        hxDesc_, miopenTypeWrapper<T>::type, 3, dim.data(), stride.data()));\n+    MIOPEN_ENFORCE(miopenSetTensorDescriptor(\n+        cxDesc_, miopenTypeWrapper<T>::type, 3, dim.data(), stride.data()));\n+    MIOPEN_ENFORCE(miopenSetTensorDescriptor(\n+        hyDesc_, miopenTypeWrapper<T>::type, 3, dim.data(), stride.data()));\n+    MIOPEN_ENFORCE(miopenSetTensorDescriptor(\n+        cyDesc_, miopenTypeWrapper<T>::type, 3, dim.data(), stride.data()));\n+\n+    if (hiddenOutput) {\n+      hiddenOutput->Resize(\n+          std::vector<int>{numLayers * numDirections, batchSize, hiddenSize});\n+    }\n+\n+    if (cellOutput) {\n+      cellOutput->Resize(\n+          std::vector<int>{numLayers * numDirections, batchSize, hiddenSize});\n+    }\n+  }\n+\n+  // Weights setup\n+  {\n+    MIOPEN_ENFORCE(miopenGetRNNParamsDescriptor(\n+        miopen_wrapper_.inline_miopen_handle(),\n+        rnnDesc_,\n+        xDesc_->descs()[0],\n+        wDesc_, \n+        miopenTypeWrapper<T>::type));\n+  }\n+\n+  // RNN workspace size\n+  {\n+    MIOPEN_ENFORCE(miopenGetRNNWorkspaceSize(\n+        miopen_wrapper_.inline_miopen_handle(),\n+        rnnDesc_,\n+        seqLength,\n+        xDesc_->descs(),\n+        &miopenWsNbytes_));\n+  }\n+}\n+\n+template <typename T>\n+bool RecurrentOp<T>::RunOnDevice() {\n+  const int seqLength = Input(INPUT).dim32(0);\n+  if (Input(INPUT).dims() != cachedInputDims_) {\n+    initialize(\n+        Input(INPUT),\n+        Output(OUTPUT),\n+        Output(HIDDEN_OUTPUT),\n+        Output(CELL_OUTPUT));\n+    cachedInputDims_ = Input(INPUT).dims();\n+  }\n+\n+  // Validation checks\n+  size_t weightsSize;\n+  MIOPEN_ENFORCE(miopenGetRNNParamsSize(\n+      miopen_wrapper_.inline_miopen_handle(),\n+      rnnDesc_,\n+      xDesc_->descs()[0],\n+      &weightsSize,\n+      miopenTypeWrapper<T>::type));\n+  \n+  CAFFE_ENFORCE_EQ(Input(WEIGHT).nbytes(), weightsSize);\n+\n+  // Training reserve size\n+  MIOPEN_ENFORCE(miopenGetRNNTrainingReserveSize(\n+      miopen_wrapper_.inline_miopen_handle(),\n+      rnnDesc_,\n+      seqLength,\n+      xDesc_->descs(),\n+      &reserveNbytes_));\n+  Output(RNN_SCRATCH)\n+      ->Resize(std::vector<int>{static_cast<int>(\n+          reserveNbytes_ / 4)}); // sizeof(T) - workaround clang bug\n+  Output(RNN_SCRATCH)->template mutable_data<T>();\n+\n+  auto InputData = [this](int i) { return this->Input(i).template data<T>(); };\n+  auto OutputData = [this](int i) {\n+    return this->Output(i)->template mutable_data<T>();\n+  };\n+\n+  if (OperatorBase::GetSingleArgument<int>(OpSchema::Arg_IsTest, 0)) {\n+    miopen_wrapper_.with_miopen_state(0, [&](MIOPENState* state) {\n+      MIOPEN_ENFORCE(miopenRNNForwardInference(\n+          state->miopen_handle(),\n+          rnnDesc_,\n+          seqLength,\n+          xDesc_->descs(),\n+          InputData(INPUT), //.template data<T>(),\n+          hxDesc_,\n+          InputData(HIDDEN_INPUT), //.template data<T>(),\n+          cxDesc_,\n+          InputData(CELL_INPUT), //.template data<T>(),\n+          wDesc_,\n+          InputData(WEIGHT), //.template data<T>(),\n+          yDesc_->descs(),\n+          OutputData(OUTPUT), //->template mutable_data<T>(),\n+          hyDesc_,\n+          OutputData(HIDDEN_OUTPUT), //->template mutable_data<T>(),\n+          cyDesc_,\n+          OutputData(CELL_OUTPUT), //->template mutable_data<T>(),\n+          state->workspace().get(miopenWsNbytes_),\n+          miopenWsNbytes_));\n+    });\n+  } else {\n+    miopen_wrapper_.with_miopen_state(0, [&](MIOPENState* state) {\n+      MIOPEN_ENFORCE(miopenRNNForwardTraining(\n+          state->miopen_handle(),\n+          rnnDesc_,\n+          seqLength,\n+          xDesc_->descs(),\n+          InputData(INPUT), //.template data<T>(),./install.sh -icd", "path": "caffe2/operators/rnn/hip/recurrent_op_miopen.cc", "position": null, "original_position": 242, "commit_id": "833ff68d9f95390740fe7fa2f2587ee87b168744", "original_commit_id": "04cd01e964cd1219ac9ef34d4ad89c745304ee55", "user": {"login": "bddppq", "id": 9300575, "node_id": "MDQ6VXNlcjkzMDA1NzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9300575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bddppq", "html_url": "https://github.com/bddppq", "followers_url": "https://api.github.com/users/bddppq/followers", "following_url": "https://api.github.com/users/bddppq/following{/other_user}", "gists_url": "https://api.github.com/users/bddppq/gists{/gist_id}", "starred_url": "https://api.github.com/users/bddppq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bddppq/subscriptions", "organizations_url": "https://api.github.com/users/bddppq/orgs", "repos_url": "https://api.github.com/users/bddppq/repos", "events_url": "https://api.github.com/users/bddppq/events{/privacy}", "received_events_url": "https://api.github.com/users/bddppq/received_events", "type": "User", "site_admin": false}, "body": "lol what is the \"install.sh\" here", "created_at": "2018-08-27T18:44:37Z", "updated_at": "2018-11-23T15:50:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/10840#discussion_r213074628", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10840", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213074628"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10840#discussion_r213074628"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10840"}}, "body_html": "<p>lol what is the \"install.sh\" here</p>", "body_text": "lol what is the \"install.sh\" here"}