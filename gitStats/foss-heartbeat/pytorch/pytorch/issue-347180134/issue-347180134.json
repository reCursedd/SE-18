{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10181", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10181/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10181/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10181/events", "html_url": "https://github.com/pytorch/pytorch/pull/10181", "id": 347180134, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA1ODY1Nzc2", "number": 10181, "title": "Sparse tensor printing; add NotImplemented autograd fn", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-08-02T21:24:47Z", "updated_at": "2018-11-23T15:50:39Z", "closed_at": "2018-09-06T02:42:20Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10181", "html_url": "https://github.com/pytorch/pytorch/pull/10181", "diff_url": "https://github.com/pytorch/pytorch/pull/10181.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10181.patch"}, "body_html": "<p>Commits:</p>\n<ol>\n<li>\n<p>Add autograd function <code>NotImplemented</code> (subclass of <code>Error</code>) so python <code>grad_fn</code> prints nicer. Since <code>Error</code> is used in <code>DelayedError</code> to implement <code>@oncedifferentiable</code>, I can't just change its name. cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a></p>\n</li>\n<li>\n<p>Add printing for sparse tensors. <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #9412.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"340848643\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/9412\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/9412/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/9412\">#9412</a> . cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38509346\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/weiyangfb\">@weiyangfb</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8813817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/li-roy\">@li-roy</a> .</p>\n</li>\n<li>\n<p>Add tests for sparse printing</p>\n</li>\n</ol>\n<p>Examples:</p>\n<div class=\"highlight highlight-source-diff\"><pre>  In [2]: x = torch.sparse.FloatTensor(torch.arange(4).view(2,2), torch.randn(2, 2), [10, 10, 2])\n  \n  In [3]: x\n  Out[3]:\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> torch.sparse.FloatTensor of size (10,10,2) with indices:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([[0, 1],</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>         [2, 3]])</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> and values:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([[-1.1832, -0.5927],</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>         [ 0.0831,  0.2511]])</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> tensor(indices=tensor([[0, 1],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                        [2, 3]]),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        values=tensor([[ 1.5081,  0.3451],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                       [-0.0392,  0.4776]]),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo)</span>\n  \n  In [4]: x.requires_grad_()\n  Out[4]:\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> torch.sparse.FloatTensor of size (10,10,2) with indices:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([[0, 1],</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>         [2, 3]], grad_fn=&lt;Error&gt;)</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> and values:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([[-1.1832, -0.5927],</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>         [ 0.0831,  0.2511]], grad_fn=&lt;Error&gt;)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> tensor(indices=tensor([[0, 1],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                        [2, 3]]),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        values=tensor([[ 1.5081,  0.3451],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                       [-0.0392,  0.4776]]),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo, requires_grad=True)</span>\n  \n  In [5]: x + x\n  Out[5]:\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> torch.sparse.FloatTensor of size (10,10,2) with indices:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([[0, 1],</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>         [2, 3]], grad_fn=&lt;Error&gt;)</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> and values:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([[-2.3664, -1.1855],</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>         [ 0.1662,  0.5021]], grad_fn=&lt;Error&gt;)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> tensor(indices=tensor([[0, 1],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                        [2, 3]]),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        values=tensor([[ 3.0162,  0.6902],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                       [-0.0785,  0.9553]]),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo, grad_fn=&lt;AddBackward0&gt;)</span>\n  \n  In [6]: x.double()\n  Out[6]:\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> torch.sparse.DoubleTensor of size (10,10,2) with indices:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([[0, 1],</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>         [2, 3]], grad_fn=&lt;Error&gt;)</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> and values:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([[-1.1832, -0.5927],</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>         [ 0.0831,  0.2511]], dtype=torch.float64, grad_fn=&lt;Error&gt;)</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> tensor(indices=tensor([[0, 1],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                        [2, 3]]),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        values=tensor([[ 1.5081,  0.3451],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>                       [-0.0392,  0.4776]]),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        size=(10, 10, 2), nnz=2, dtype=torch.float64, layout=torch.sparse_coo,</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        grad_fn=&lt;NotImplemented&gt;)</span>\n  \n  In [7]: x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2, 0), [0])\n  \n  In [8]: x\n  Out[8]:\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> torch.sparse.FloatTensor of size (0,) with indices:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([], size=(0, 2), dtype=torch.int64)</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> and values:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([], size=(2, 0))</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> tensor(indices=tensor([], size=(0, 2)),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        values=tensor([], size=(2, 0)),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        size=(0,), nnz=2, layout=torch.sparse_coo)</span>\n  \n  In [9]: x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2), [])\n  \n  In [10]: x\n  Out[10]:\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> torch.sparse.FloatTensor of size () with indices:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([], size=(0, 2), dtype=torch.int64)</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> and values:</span>\n<span class=\"pl-md\"><span class=\"pl-md\">-</span> tensor([-0.0064,  0.8518])</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> tensor(indices=tensor([], size=(0, 2)),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        values=tensor([ 0.9800, -0.5978]),</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>        size=(), nnz=2, layout=torch.sparse_coo)</span></pre></div>", "body_text": "Commits:\n\n\nAdd autograd function NotImplemented (subclass of Error) so python grad_fn prints nicer. Since Error is used in DelayedError to implement @oncedifferentiable, I can't just change its name. cc @colesbury\n\n\nAdd printing for sparse tensors. Fixes #9412 . cc @weiyangfb @li-roy .\n\n\nAdd tests for sparse printing\n\n\nExamples:\n  In [2]: x = torch.sparse.FloatTensor(torch.arange(4).view(2,2), torch.randn(2, 2), [10, 10, 2])\n  \n  In [3]: x\n  Out[3]:\n- torch.sparse.FloatTensor of size (10,10,2) with indices:\n- tensor([[0, 1],\n-         [2, 3]])\n- and values:\n- tensor([[-1.1832, -0.5927],\n-         [ 0.0831,  0.2511]])\n+ tensor(indices=tensor([[0, 1],\n+                        [2, 3]]),\n+        values=tensor([[ 1.5081,  0.3451],\n+                       [-0.0392,  0.4776]]),\n+        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo)\n  \n  In [4]: x.requires_grad_()\n  Out[4]:\n- torch.sparse.FloatTensor of size (10,10,2) with indices:\n- tensor([[0, 1],\n-         [2, 3]], grad_fn=<Error>)\n- and values:\n- tensor([[-1.1832, -0.5927],\n-         [ 0.0831,  0.2511]], grad_fn=<Error>)\n+ tensor(indices=tensor([[0, 1],\n+                        [2, 3]]),\n+        values=tensor([[ 1.5081,  0.3451],\n+                       [-0.0392,  0.4776]]),\n+        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo, requires_grad=True)\n  \n  In [5]: x + x\n  Out[5]:\n- torch.sparse.FloatTensor of size (10,10,2) with indices:\n- tensor([[0, 1],\n-         [2, 3]], grad_fn=<Error>)\n- and values:\n- tensor([[-2.3664, -1.1855],\n-         [ 0.1662,  0.5021]], grad_fn=<Error>)\n+ tensor(indices=tensor([[0, 1],\n+                        [2, 3]]),\n+        values=tensor([[ 3.0162,  0.6902],\n+                       [-0.0785,  0.9553]]),\n+        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo, grad_fn=<AddBackward0>)\n  \n  In [6]: x.double()\n  Out[6]:\n- torch.sparse.DoubleTensor of size (10,10,2) with indices:\n- tensor([[0, 1],\n-         [2, 3]], grad_fn=<Error>)\n- and values:\n- tensor([[-1.1832, -0.5927],\n-         [ 0.0831,  0.2511]], dtype=torch.float64, grad_fn=<Error>)\n+ tensor(indices=tensor([[0, 1],\n+                        [2, 3]]),\n+        values=tensor([[ 1.5081,  0.3451],\n+                       [-0.0392,  0.4776]]),\n+        size=(10, 10, 2), nnz=2, dtype=torch.float64, layout=torch.sparse_coo,\n+        grad_fn=<NotImplemented>)\n  \n  In [7]: x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2, 0), [0])\n  \n  In [8]: x\n  Out[8]:\n- torch.sparse.FloatTensor of size (0,) with indices:\n- tensor([], size=(0, 2), dtype=torch.int64)\n- and values:\n- tensor([], size=(2, 0))\n+ tensor(indices=tensor([], size=(0, 2)),\n+        values=tensor([], size=(2, 0)),\n+        size=(0,), nnz=2, layout=torch.sparse_coo)\n  \n  In [9]: x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2), [])\n  \n  In [10]: x\n  Out[10]:\n- torch.sparse.FloatTensor of size () with indices:\n- tensor([], size=(0, 2), dtype=torch.int64)\n- and values:\n- tensor([-0.0064,  0.8518])\n+ tensor(indices=tensor([], size=(0, 2)),\n+        values=tensor([ 0.9800, -0.5978]),\n+        size=(), nnz=2, layout=torch.sparse_coo)", "body": "Commits:\r\n\r\n1. Add autograd function `NotImplemented` (subclass of `Error`) so python `grad_fn` prints nicer. Since `Error` is used in `DelayedError` to implement `@oncedifferentiable`, I can't just change its name. cc @colesbury \r\n\r\n2. Add printing for sparse tensors. Fixes https://github.com/pytorch/pytorch/issues/9412 . cc @weiyangfb @li-roy . \r\n\r\n3. Add tests for sparse printing\r\n\r\nExamples:\r\n```diff\r\n  In [2]: x = torch.sparse.FloatTensor(torch.arange(4).view(2,2), torch.randn(2, 2), [10, 10, 2])\r\n  \r\n  In [3]: x\r\n  Out[3]:\r\n- torch.sparse.FloatTensor of size (10,10,2) with indices:\r\n- tensor([[0, 1],\r\n-         [2, 3]])\r\n- and values:\r\n- tensor([[-1.1832, -0.5927],\r\n-         [ 0.0831,  0.2511]])\r\n+ tensor(indices=tensor([[0, 1],\r\n+                        [2, 3]]),\r\n+        values=tensor([[ 1.5081,  0.3451],\r\n+                       [-0.0392,  0.4776]]),\r\n+        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo)\r\n  \r\n  In [4]: x.requires_grad_()\r\n  Out[4]:\r\n- torch.sparse.FloatTensor of size (10,10,2) with indices:\r\n- tensor([[0, 1],\r\n-         [2, 3]], grad_fn=<Error>)\r\n- and values:\r\n- tensor([[-1.1832, -0.5927],\r\n-         [ 0.0831,  0.2511]], grad_fn=<Error>)\r\n+ tensor(indices=tensor([[0, 1],\r\n+                        [2, 3]]),\r\n+        values=tensor([[ 1.5081,  0.3451],\r\n+                       [-0.0392,  0.4776]]),\r\n+        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo, requires_grad=True)\r\n  \r\n  In [5]: x + x\r\n  Out[5]:\r\n- torch.sparse.FloatTensor of size (10,10,2) with indices:\r\n- tensor([[0, 1],\r\n-         [2, 3]], grad_fn=<Error>)\r\n- and values:\r\n- tensor([[-2.3664, -1.1855],\r\n-         [ 0.1662,  0.5021]], grad_fn=<Error>)\r\n+ tensor(indices=tensor([[0, 1],\r\n+                        [2, 3]]),\r\n+        values=tensor([[ 3.0162,  0.6902],\r\n+                       [-0.0785,  0.9553]]),\r\n+        size=(10, 10, 2), nnz=2, layout=torch.sparse_coo, grad_fn=<AddBackward0>)\r\n  \r\n  In [6]: x.double()\r\n  Out[6]:\r\n- torch.sparse.DoubleTensor of size (10,10,2) with indices:\r\n- tensor([[0, 1],\r\n-         [2, 3]], grad_fn=<Error>)\r\n- and values:\r\n- tensor([[-1.1832, -0.5927],\r\n-         [ 0.0831,  0.2511]], dtype=torch.float64, grad_fn=<Error>)\r\n+ tensor(indices=tensor([[0, 1],\r\n+                        [2, 3]]),\r\n+        values=tensor([[ 1.5081,  0.3451],\r\n+                       [-0.0392,  0.4776]]),\r\n+        size=(10, 10, 2), nnz=2, dtype=torch.float64, layout=torch.sparse_coo,\r\n+        grad_fn=<NotImplemented>)\r\n  \r\n  In [7]: x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2, 0), [0])\r\n  \r\n  In [8]: x\r\n  Out[8]:\r\n- torch.sparse.FloatTensor of size (0,) with indices:\r\n- tensor([], size=(0, 2), dtype=torch.int64)\r\n- and values:\r\n- tensor([], size=(2, 0))\r\n+ tensor(indices=tensor([], size=(0, 2)),\r\n+        values=tensor([], size=(2, 0)),\r\n+        size=(0,), nnz=2, layout=torch.sparse_coo)\r\n  \r\n  In [9]: x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2), [])\r\n  \r\n  In [10]: x\r\n  Out[10]:\r\n- torch.sparse.FloatTensor of size () with indices:\r\n- tensor([], size=(0, 2), dtype=torch.int64)\r\n- and values:\r\n- tensor([-0.0064,  0.8518])\r\n+ tensor(indices=tensor([], size=(0, 2)),\r\n+        values=tensor([ 0.9800, -0.5978]),\r\n+        size=(), nnz=2, layout=torch.sparse_coo)\r\n```"}