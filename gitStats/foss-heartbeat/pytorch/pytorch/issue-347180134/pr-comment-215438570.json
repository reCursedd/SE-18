{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215438570", "pull_request_review_id": 152699342, "id": 215438570, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTQzODU3MA==", "diff_hunk": "@@ -222,50 +240,60 @@ def get_summarized_data(self):\n \n \n def _str(self):\n-    if self.is_sparse:\n-        size_str = str(tuple(self.shape)).replace(' ', '')\n-        return '{} of size {} with indices:\\n{}\\nand values:\\n{}'.format(\n-            self.type(), size_str, self._indices(), self._values())\n-\n     prefix = 'tensor('\n     indent = len(prefix)\n-    summarize = self.numel() > PRINT_OPTS.threshold\n \n-    suffix = ''\n+    suffixes = []\n     if not torch._C._is_default_type_cuda():\n         if self.device.type == 'cuda':\n-            suffix += ', device=\\'' + str(self.device) + '\\''\n+            suffixes.append('device=\\'' + str(self.device) + '\\'')\n     else:\n         if self.device.type == 'cpu' or torch.cuda.current_device() != self.device.index:\n-            suffix += ', device=\\'' + str(self.device) + '\\''\n+            suffixes.append('device=\\'' + str(self.device) + '\\'')\n \n-    if self.numel() == 0:\n-        # Explicitly print the shape if it is not (0,), to match NumPy behavior\n-        if self.dim() != 1:\n-            suffix += ', size=' + str(tuple(self.shape))\n-\n-        # In an empty tensor, there are no elements to infer if the dtype should be int64,\n-        # so it must be shown explicitly.\n-        if self.dtype != torch.get_default_dtype():\n-            suffix += ', dtype=' + str(self.dtype)\n-        tensor_str = '[]'\n+    has_default_dtype = self.dtype == torch.get_default_dtype() or self.dtype == torch.int64\n+\n+    if self.is_sparse:\n+        suffixes.append('size=' + str(tuple(self.shape)))\n+        suffixes.append('nnz=' + str(self._nnz()))\n+        if not has_default_dtype:\n+            suffixes.append('dtype=' + str(self.dtype))\n+        indices_prefix = 'indices=tensor('\n+        indices = self._indices().detach()", "path": "torch/_tensor_str.py", "position": 102, "original_position": 102, "commit_id": "72ca0478da31a4cd50ad0bdfca1d493dceafa397", "original_commit_id": "d8969e5e5872e4b893285021a448a1242d23fade", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "does this detach matter?  Since it's int64_t it shouldn't have requires_grad, right?", "created_at": "2018-09-05T21:57:38Z", "updated_at": "2018-11-23T15:50:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/10181#discussion_r215438570", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10181", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215438570"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10181#discussion_r215438570"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10181"}}, "body_html": "<p>does this detach matter?  Since it's int64_t it shouldn't have requires_grad, right?</p>", "body_text": "does this detach matter?  Since it's int64_t it shouldn't have requires_grad, right?"}