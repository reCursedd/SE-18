{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215455846", "pull_request_review_id": 152725420, "id": 215455846, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTQ1NTg0Ng==", "diff_hunk": "@@ -222,50 +240,60 @@ def get_summarized_data(self):\n \n \n def _str(self):\n-    if self.is_sparse:\n-        size_str = str(tuple(self.shape)).replace(' ', '')\n-        return '{} of size {} with indices:\\n{}\\nand values:\\n{}'.format(\n-            self.type(), size_str, self._indices(), self._values())\n-\n     prefix = 'tensor('\n     indent = len(prefix)\n-    summarize = self.numel() > PRINT_OPTS.threshold\n \n-    suffix = ''\n+    suffixes = []\n     if not torch._C._is_default_type_cuda():\n         if self.device.type == 'cuda':\n-            suffix += ', device=\\'' + str(self.device) + '\\''\n+            suffixes.append('device=\\'' + str(self.device) + '\\'')\n     else:\n         if self.device.type == 'cpu' or torch.cuda.current_device() != self.device.index:\n-            suffix += ', device=\\'' + str(self.device) + '\\''\n+            suffixes.append('device=\\'' + str(self.device) + '\\'')\n \n-    if self.numel() == 0:\n-        # Explicitly print the shape if it is not (0,), to match NumPy behavior\n-        if self.dim() != 1:\n-            suffix += ', size=' + str(tuple(self.shape))\n-\n-        # In an empty tensor, there are no elements to infer if the dtype should be int64,\n-        # so it must be shown explicitly.\n-        if self.dtype != torch.get_default_dtype():\n-            suffix += ', dtype=' + str(self.dtype)\n-        tensor_str = '[]'\n+    has_default_dtype = self.dtype == torch.get_default_dtype() or self.dtype == torch.int64\n+\n+    if self.is_sparse:\n+        suffixes.append('size=' + str(tuple(self.shape)))\n+        suffixes.append('nnz=' + str(self._nnz()))\n+        if not has_default_dtype:\n+            suffixes.append('dtype=' + str(self.dtype))\n+        indices_prefix = 'indices=tensor('\n+        indices = self._indices().detach()", "path": "torch/_tensor_str.py", "position": 102, "original_position": 102, "commit_id": "72ca0478da31a4cd50ad0bdfca1d493dceafa397", "original_commit_id": "d8969e5e5872e4b893285021a448a1242d23fade", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Apparently we don't check returned dtype for setting that flag. See\r\n```py\r\n>>> x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2, 0\r\n, [0])\r\n>>> x.requires_grad_()\r\ntensor(indices=tensor([], size=(0, 2)),\r\n       values=tensor([], size=(2, 0)),\r\n       size=(0,), nnz=2, layout=torch.sparse_coo, requires_grad=True)\r\n>>> x._indices()\r\ntensor([], size=(0, 2), dtype=torch.int64, grad_fn=<NotImplemented>)\r\n```\r\n\r\nI'll fixing this in a new PR.", "created_at": "2018-09-05T23:27:54Z", "updated_at": "2018-11-23T15:50:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/10181#discussion_r215455846", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10181", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215455846"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10181#discussion_r215455846"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10181"}}, "body_html": "<p>Apparently we don't check returned dtype for setting that flag. See</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.sparse.FloatTensor(torch.ones(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.long), torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>\n, [<span class=\"pl-c1\">0</span>])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x.requires_grad_()\ntensor(<span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>tensor([], <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>)),\n       <span class=\"pl-v\">values</span><span class=\"pl-k\">=</span>tensor([], <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>)),\n       <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">0</span>,), <span class=\"pl-v\">nnz</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">layout</span><span class=\"pl-k\">=</span>torch.sparse_coo, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x._indices()\ntensor([], <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.int64, <span class=\"pl-v\">grad_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">&lt;</span><span class=\"pl-c1\">NotImplemented</span><span class=\"pl-k\">&gt;</span>)</pre></div>\n<p>I'll fixing this in a new PR.</p>", "body_text": "Apparently we don't check returned dtype for setting that flag. See\n>>> x = torch.sparse.FloatTensor(torch.ones(0, 2, dtype=torch.long), torch.randn(2, 0\n, [0])\n>>> x.requires_grad_()\ntensor(indices=tensor([], size=(0, 2)),\n       values=tensor([], size=(2, 0)),\n       size=(0,), nnz=2, layout=torch.sparse_coo, requires_grad=True)\n>>> x._indices()\ntensor([], size=(0, 2), dtype=torch.int64, grad_fn=<NotImplemented>)\nI'll fixing this in a new PR.", "in_reply_to_id": 215438570}