{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11959", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11959/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11959/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11959/events", "html_url": "https://github.com/pytorch/pytorch/issues/11959", "id": 362805079, "node_id": "MDU6SXNzdWUzNjI4MDUwNzk=", "number": 11959, "title": "[feature request] Support soft target distribution in cross entropy loss", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-09-21T23:45:38Z", "updated_at": "2018-11-19T12:27:13Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Currently our cross entropy loss (i.e., <a href=\"https://pytorch.org/docs/stable/nn.html#crossentropyloss\" rel=\"nofollow\">nn.CrossEntropyLoss</a>) only supports a hard target class, i.e., wanting to maximize the output (log) probability of a particular class. But in many times training w.r.t. a soft target distribution (i.e., wanting the output to match a particular distribution) is quite useful too, e.g., preventing overfitting.</p>\n<h2>Math</h2>\n<p>Cross entropy loss operates on logits after softmax.</p>\n<p>Denote the input vector as <code>x</code>. Log softmax computes a vector <code>y</code> of same length as <code>x</code>, where <code>y_i = x_i - log( \\sum_j exp(x_j) )</code>, representing the log likelihood of each class.</p>\n<ul>\n<li>\n<p>In the hard target case, if the target clss is <code>c</code>, the loss is simply negative log likelihood loss <code>-y_c</code>.</p>\n</li>\n<li>\n<p>In the soft target case, let the target distribution vector be <code>p</code> (i.e., <code>p_i</code> is the target probability for predicting class <code>i</code>). The loss is the KL divergence</p>\n<pre><code>D( softmax(x) || p) = \\sum_i p_i (log p_i  / softmax(x)_i) = -\\sum_i p_i y_i + constant\n</code></pre>\n<p>The constant is independent of <code>x</code> and thus discarded. Our loss formula is just <code>-\\sum_i p_i y_i</code>.</p>\n<p>When <code>p_c = 1</code> for some class <code>c</code>, this simplifies to the hard target class.</p>\n<p>The formula for gradient computation can be easily derived from this:</p>\n<pre><code>\nd l / d y_i = -p_i\n\nd y_i / d x_i = 1 - exp(x_i) / \\sum_j exp(x_j) = 1 - exp(y_i)\n\n# suppose k != i\nd y_k / d_x_i = -exp(x_i) / \\sum_j exp(x_j) = - exp(y_i)\n\n# so\nd l / d x_i = exp(y_i) (\\sum p) - p_i = exp (y_i) - p_i (= softmax(x) - p_i).\n\n</code></pre>\n</li>\n</ul>\n<h2>Possible Implementation</h2>\n<p>Currently our cross entropy loss implementation takes in batched <code>x</code> of shape <code>(N, C)</code> and floating point dtype (<code>N</code> is the batch size and <code>C</code> is the number of classes), and a batched target class indices vector <code>target</code> of shape <code>(N)</code>, where <code>target[i]</code> is the index of the desired output class, and dtype <code>long</code> (an integral type).</p>\n<p>Since we want it to also take in soft target distribution as target, we can allow it to also take in <code>target</code> as a target batched distribution of shape <code>(N, C)</code>, and detect whether we want soft target or hard target basing on shape and dtype.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a></p>", "body_text": "Currently our cross entropy loss (i.e., nn.CrossEntropyLoss) only supports a hard target class, i.e., wanting to maximize the output (log) probability of a particular class. But in many times training w.r.t. a soft target distribution (i.e., wanting the output to match a particular distribution) is quite useful too, e.g., preventing overfitting.\nMath\nCross entropy loss operates on logits after softmax.\nDenote the input vector as x. Log softmax computes a vector y of same length as x, where y_i = x_i - log( \\sum_j exp(x_j) ), representing the log likelihood of each class.\n\n\nIn the hard target case, if the target clss is c, the loss is simply negative log likelihood loss -y_c.\n\n\nIn the soft target case, let the target distribution vector be p (i.e., p_i is the target probability for predicting class i). The loss is the KL divergence\nD( softmax(x) || p) = \\sum_i p_i (log p_i  / softmax(x)_i) = -\\sum_i p_i y_i + constant\n\nThe constant is independent of x and thus discarded. Our loss formula is just -\\sum_i p_i y_i.\nWhen p_c = 1 for some class c, this simplifies to the hard target class.\nThe formula for gradient computation can be easily derived from this:\n\nd l / d y_i = -p_i\n\nd y_i / d x_i = 1 - exp(x_i) / \\sum_j exp(x_j) = 1 - exp(y_i)\n\n# suppose k != i\nd y_k / d_x_i = -exp(x_i) / \\sum_j exp(x_j) = - exp(y_i)\n\n# so\nd l / d x_i = exp(y_i) (\\sum p) - p_i = exp (y_i) - p_i (= softmax(x) - p_i).\n\n\n\n\nPossible Implementation\nCurrently our cross entropy loss implementation takes in batched x of shape (N, C) and floating point dtype (N is the batch size and C is the number of classes), and a batched target class indices vector target of shape (N), where target[i] is the index of the desired output class, and dtype long (an integral type).\nSince we want it to also take in soft target distribution as target, we can allow it to also take in target as a target batched distribution of shape (N, C), and detect whether we want soft target or hard target basing on shape and dtype.\ncc @gchanan", "body": "Currently our cross entropy loss (i.e., [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/nn.html#crossentropyloss)) only supports a hard target class, i.e., wanting to maximize the output (log) probability of a particular class. But in many times training w.r.t. a soft target distribution (i.e., wanting the output to match a particular distribution) is quite useful too, e.g., preventing overfitting.\r\n\r\n## Math\r\nCross entropy loss operates on logits after softmax. \r\n\r\nDenote the input vector as `x`. Log softmax computes a vector `y` of same length as `x`, where `y_i = x_i - log( \\sum_j exp(x_j) )`, representing the log likelihood of each class.\r\n\r\n+ In the hard target case, if the target clss is `c`, the loss is simply negative log likelihood loss `-y_c`.\r\n\r\n+ In the soft target case, let the target distribution vector be `p` (i.e., `p_i` is the target probability for predicting class `i`). The loss is the KL divergence \r\n\r\n  ```\r\n  D( softmax(x) || p) = \\sum_i p_i (log p_i  / softmax(x)_i) = -\\sum_i p_i y_i + constant\r\n  ```\r\n  \r\n  The constant is independent of `x` and thus discarded. Our loss formula is just `-\\sum_i p_i y_i`. \r\n  \r\n  When `p_c = 1` for some class `c`, this simplifies to the hard target class.\r\n  \r\n\r\n  The formula for gradient computation can be easily derived from this:\r\n  \r\n  ```\r\n  \r\n  d l / d y_i = -p_i\r\n  \r\n  d y_i / d x_i = 1 - exp(x_i) / \\sum_j exp(x_j) = 1 - exp(y_i)\r\n  \r\n  # suppose k != i\r\n  d y_k / d_x_i = -exp(x_i) / \\sum_j exp(x_j) = - exp(y_i)\r\n  \r\n  # so\r\n  d l / d x_i = exp(y_i) (\\sum p) - p_i = exp (y_i) - p_i (= softmax(x) - p_i).\r\n  \r\n  ```\r\n\r\n## Possible Implementation\r\n\r\nCurrently our cross entropy loss implementation takes in batched `x` of shape `(N, C)` and floating point dtype (`N` is the batch size and `C` is the number of classes), and a batched target class indices vector `target` of shape `(N)`, where `target[i]` is the index of the desired output class, and dtype `long` (an integral type).\r\n\r\nSince we want it to also take in soft target distribution as target, we can allow it to also take in `target` as a target batched distribution of shape `(N, C)`, and detect whether we want soft target or hard target basing on shape and dtype.\r\n\r\ncc @gchanan "}