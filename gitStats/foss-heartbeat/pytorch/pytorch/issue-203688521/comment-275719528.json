{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/275719528", "html_url": "https://github.com/pytorch/pytorch/issues/610#issuecomment-275719528", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/610", "id": 275719528, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTcxOTUyOA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-27T17:14:10Z", "updated_at": "2017-01-27T17:14:10Z", "author_association": "MEMBER", "body_html": "<p>To bypass you problem, you can add a <code>.squeeze()</code> after computing the norm. This will make the backward computation work without a problem.</p>\n<p>But the underlying issue as you mentioned is that many torch operations don't care about the number of dimensions, and only the number of elements matter. This is not the case for their equivalent backward operations. Should we leave it as is, or fix it?</p>", "body_text": "To bypass you problem, you can add a .squeeze() after computing the norm. This will make the backward computation work without a problem.\nBut the underlying issue as you mentioned is that many torch operations don't care about the number of dimensions, and only the number of elements matter. This is not the case for their equivalent backward operations. Should we leave it as is, or fix it?", "body": "To bypass you problem, you can add a `.squeeze()` after computing the norm. This will make the backward computation work without a problem.\r\n\r\nBut the underlying issue as you mentioned is that many torch operations don't care about the number of dimensions, and only the number of elements matter. This is not the case for their equivalent backward operations. Should we leave it as is, or fix it?"}