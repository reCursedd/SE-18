{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208937610", "pull_request_review_id": 144850720, "id": 208937610, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwODkzNzYxMA==", "diff_hunk": "@@ -221,4 +221,6 @@ def _validate_sample(self, value):\n             raise ValueError('The value argument must be within the support')\n \n     def __repr__(self):\n-        return self.__class__.__name__ + '()'\n+        param_names = [k for k,_ in self.arg_constraints.items()]\n+        args_string = ', '.join(['{}: {}'.format(p, self.__dict__[p]) for p in param_names])\n+        return self.__class__.__name__ + '(' + args_string + ')'", "path": "torch/distributions/distribution.py", "position": null, "original_position": 7, "commit_id": "83712a78ab1e1e571952328ffd8950cc5b330869", "original_commit_id": "5114a2bbbd56772292b2a7e4fbd90c04d7ba7f45", "user": {"login": "vishwakftw", "id": 23639302, "node_id": "MDQ6VXNlcjIzNjM5MzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/23639302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishwakftw", "html_url": "https://github.com/vishwakftw", "followers_url": "https://api.github.com/users/vishwakftw/followers", "following_url": "https://api.github.com/users/vishwakftw/following{/other_user}", "gists_url": "https://api.github.com/users/vishwakftw/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishwakftw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishwakftw/subscriptions", "organizations_url": "https://api.github.com/users/vishwakftw/orgs", "repos_url": "https://api.github.com/users/vishwakftw/repos", "events_url": "https://api.github.com/users/vishwakftw/events{/privacy}", "received_events_url": "https://api.github.com/users/vishwakftw/received_events", "type": "User", "site_admin": false}, "body": "Will this not create a lot of clutter? If we had a large tensor parameterizing a distribution, the `__repr__` would fill up a lot of space. For example:\r\n```\r\n>>> import torch\r\n>>> torch.distributions.Normal(torch.randn(10, 10), torch.rand(10, 10))\r\nNormal(loc: tensor([[ 0.2669, -2.5083, -0.9248, -2.5580,  0.0581, -0.8053, -1.3089,  0.5454,\r\n         -1.1018, -2.9030],\r\n        [-0.7943,  1.1466, -0.4999,  0.4367,  0.1974,  0.8404,  0.1555, -0.0307,\r\n         -0.3918,  1.6991],\r\n        [ 0.4674, -0.3881,  0.2157,  0.2178, -0.8992,  0.6417, -1.0599, -1.2741,\r\n          1.4784, -1.6111],\r\n        [ 0.9133, -2.4415, -0.3185,  0.2513,  0.7126, -0.0511,  0.0191, -0.6843,\r\n          0.5016, -0.7255],\r\n        [-0.2401, -0.5157,  1.7942, -0.2079,  1.1840,  0.9210,  0.1123, -0.8723,\r\n         -0.2457, -1.0446],\r\n        [ 1.3834,  2.0407,  0.6839,  0.1849,  0.5977, -0.2929,  1.7955,  1.0106,\r\n         -0.5795, -0.8875],\r\n        [-0.6298, -0.2499,  1.3777, -0.0384, -0.4921,  0.7832,  1.3252, -0.8033,\r\n          0.0562,  1.0407],\r\n        [ 0.6389, -1.3752, -0.5781,  0.4153, -0.3506,  0.1727,  1.0859, -1.2366,\r\n         -0.4017, -0.9681],\r\n        [-0.5646,  0.4137, -1.7426, -0.1854, -0.1042, -1.4406, -0.2368, -2.9316,\r\n          1.8967, -1.1922],\r\n        [-0.9363, -0.3481, -0.5971,  2.0882, -0.8389, -0.7388,  0.4768,  0.7671,\r\n         -0.2073, -1.3037]]), scale: tensor([[0.7092, 0.2489, 0.7965, 0.8367, 0.0876, 0.9370, 0.9043, 0.8322, 0.7560,\r\n         0.5924],\r\n        [0.7195, 0.3544, 0.7644, 0.8478, 0.0301, 0.5853, 0.5366, 0.7020, 0.0556,\r\n         0.6009],\r\n        [0.5069, 0.3518, 0.5553, 0.7847, 0.4687, 0.3424, 0.2620, 0.9680, 0.8361,\r\n         0.2880],\r\n        [0.2476, 0.0632, 0.9116, 0.2349, 0.7765, 0.4884, 0.2115, 0.4961, 0.8478,\r\n         0.0358],\r\n        [0.1228, 0.2393, 0.8527, 0.8694, 0.0185, 0.3833, 0.8530, 0.3855, 0.5278,\r\n         0.4227],\r\n        [0.6401, 0.1710, 0.0031, 0.7733, 0.3369, 0.1228, 0.6203, 0.0220, 0.9387,\r\n         0.0853],\r\n        [0.1819, 0.6805, 0.3538, 0.1556, 0.3126, 0.9230, 0.3187, 0.8747, 0.0509,\r\n         0.3309],\r\n        [0.5708, 0.9662, 0.4152, 0.7872, 0.7201, 0.8699, 0.2644, 0.4142, 0.7762,\r\n         0.7114],\r\n        [0.2799, 0.2036, 0.3563, 0.9897, 0.7181, 0.7042, 0.5987, 0.1086, 0.4707,\r\n         0.1144],\r\n        [0.8662, 0.6451, 0.2284, 0.9216, 0.5946, 0.8736, 0.0990, 0.5090, 0.7335,\r\n         0.1160]]))\r\n```\r\nInstead, I think it would be nice to have the shapes of the parameters instead, like so:\r\n```\r\n>>> import torch\r\n>>> torch.distributions.Normal(torch.randn(10, 10), torch.rand(10, 10))\r\nNormal(loc: torch.Size([10, 10]), scale: torch.Size([10, 10]))\r\n```", "created_at": "2018-08-09T13:54:40Z", "updated_at": "2018-11-23T15:49:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/10373#discussion_r208937610", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10373", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208937610"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10373#discussion_r208937610"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10373"}}, "body_html": "<p>Will this not create a lot of clutter? If we had a large tensor parameterizing a distribution, the <code>__repr__</code> would fill up a lot of space. For example:</p>\n<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.distributions.Normal(torch.randn(10, 10), torch.rand(10, 10))\nNormal(loc: tensor([[ 0.2669, -2.5083, -0.9248, -2.5580,  0.0581, -0.8053, -1.3089,  0.5454,\n         -1.1018, -2.9030],\n        [-0.7943,  1.1466, -0.4999,  0.4367,  0.1974,  0.8404,  0.1555, -0.0307,\n         -0.3918,  1.6991],\n        [ 0.4674, -0.3881,  0.2157,  0.2178, -0.8992,  0.6417, -1.0599, -1.2741,\n          1.4784, -1.6111],\n        [ 0.9133, -2.4415, -0.3185,  0.2513,  0.7126, -0.0511,  0.0191, -0.6843,\n          0.5016, -0.7255],\n        [-0.2401, -0.5157,  1.7942, -0.2079,  1.1840,  0.9210,  0.1123, -0.8723,\n         -0.2457, -1.0446],\n        [ 1.3834,  2.0407,  0.6839,  0.1849,  0.5977, -0.2929,  1.7955,  1.0106,\n         -0.5795, -0.8875],\n        [-0.6298, -0.2499,  1.3777, -0.0384, -0.4921,  0.7832,  1.3252, -0.8033,\n          0.0562,  1.0407],\n        [ 0.6389, -1.3752, -0.5781,  0.4153, -0.3506,  0.1727,  1.0859, -1.2366,\n         -0.4017, -0.9681],\n        [-0.5646,  0.4137, -1.7426, -0.1854, -0.1042, -1.4406, -0.2368, -2.9316,\n          1.8967, -1.1922],\n        [-0.9363, -0.3481, -0.5971,  2.0882, -0.8389, -0.7388,  0.4768,  0.7671,\n         -0.2073, -1.3037]]), scale: tensor([[0.7092, 0.2489, 0.7965, 0.8367, 0.0876, 0.9370, 0.9043, 0.8322, 0.7560,\n         0.5924],\n        [0.7195, 0.3544, 0.7644, 0.8478, 0.0301, 0.5853, 0.5366, 0.7020, 0.0556,\n         0.6009],\n        [0.5069, 0.3518, 0.5553, 0.7847, 0.4687, 0.3424, 0.2620, 0.9680, 0.8361,\n         0.2880],\n        [0.2476, 0.0632, 0.9116, 0.2349, 0.7765, 0.4884, 0.2115, 0.4961, 0.8478,\n         0.0358],\n        [0.1228, 0.2393, 0.8527, 0.8694, 0.0185, 0.3833, 0.8530, 0.3855, 0.5278,\n         0.4227],\n        [0.6401, 0.1710, 0.0031, 0.7733, 0.3369, 0.1228, 0.6203, 0.0220, 0.9387,\n         0.0853],\n        [0.1819, 0.6805, 0.3538, 0.1556, 0.3126, 0.9230, 0.3187, 0.8747, 0.0509,\n         0.3309],\n        [0.5708, 0.9662, 0.4152, 0.7872, 0.7201, 0.8699, 0.2644, 0.4142, 0.7762,\n         0.7114],\n        [0.2799, 0.2036, 0.3563, 0.9897, 0.7181, 0.7042, 0.5987, 0.1086, 0.4707,\n         0.1144],\n        [0.8662, 0.6451, 0.2284, 0.9216, 0.5946, 0.8736, 0.0990, 0.5090, 0.7335,\n         0.1160]]))\n</code></pre>\n<p>Instead, I think it would be nice to have the shapes of the parameters instead, like so:</p>\n<pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.distributions.Normal(torch.randn(10, 10), torch.rand(10, 10))\nNormal(loc: torch.Size([10, 10]), scale: torch.Size([10, 10]))\n</code></pre>", "body_text": "Will this not create a lot of clutter? If we had a large tensor parameterizing a distribution, the __repr__ would fill up a lot of space. For example:\n>>> import torch\n>>> torch.distributions.Normal(torch.randn(10, 10), torch.rand(10, 10))\nNormal(loc: tensor([[ 0.2669, -2.5083, -0.9248, -2.5580,  0.0581, -0.8053, -1.3089,  0.5454,\n         -1.1018, -2.9030],\n        [-0.7943,  1.1466, -0.4999,  0.4367,  0.1974,  0.8404,  0.1555, -0.0307,\n         -0.3918,  1.6991],\n        [ 0.4674, -0.3881,  0.2157,  0.2178, -0.8992,  0.6417, -1.0599, -1.2741,\n          1.4784, -1.6111],\n        [ 0.9133, -2.4415, -0.3185,  0.2513,  0.7126, -0.0511,  0.0191, -0.6843,\n          0.5016, -0.7255],\n        [-0.2401, -0.5157,  1.7942, -0.2079,  1.1840,  0.9210,  0.1123, -0.8723,\n         -0.2457, -1.0446],\n        [ 1.3834,  2.0407,  0.6839,  0.1849,  0.5977, -0.2929,  1.7955,  1.0106,\n         -0.5795, -0.8875],\n        [-0.6298, -0.2499,  1.3777, -0.0384, -0.4921,  0.7832,  1.3252, -0.8033,\n          0.0562,  1.0407],\n        [ 0.6389, -1.3752, -0.5781,  0.4153, -0.3506,  0.1727,  1.0859, -1.2366,\n         -0.4017, -0.9681],\n        [-0.5646,  0.4137, -1.7426, -0.1854, -0.1042, -1.4406, -0.2368, -2.9316,\n          1.8967, -1.1922],\n        [-0.9363, -0.3481, -0.5971,  2.0882, -0.8389, -0.7388,  0.4768,  0.7671,\n         -0.2073, -1.3037]]), scale: tensor([[0.7092, 0.2489, 0.7965, 0.8367, 0.0876, 0.9370, 0.9043, 0.8322, 0.7560,\n         0.5924],\n        [0.7195, 0.3544, 0.7644, 0.8478, 0.0301, 0.5853, 0.5366, 0.7020, 0.0556,\n         0.6009],\n        [0.5069, 0.3518, 0.5553, 0.7847, 0.4687, 0.3424, 0.2620, 0.9680, 0.8361,\n         0.2880],\n        [0.2476, 0.0632, 0.9116, 0.2349, 0.7765, 0.4884, 0.2115, 0.4961, 0.8478,\n         0.0358],\n        [0.1228, 0.2393, 0.8527, 0.8694, 0.0185, 0.3833, 0.8530, 0.3855, 0.5278,\n         0.4227],\n        [0.6401, 0.1710, 0.0031, 0.7733, 0.3369, 0.1228, 0.6203, 0.0220, 0.9387,\n         0.0853],\n        [0.1819, 0.6805, 0.3538, 0.1556, 0.3126, 0.9230, 0.3187, 0.8747, 0.0509,\n         0.3309],\n        [0.5708, 0.9662, 0.4152, 0.7872, 0.7201, 0.8699, 0.2644, 0.4142, 0.7762,\n         0.7114],\n        [0.2799, 0.2036, 0.3563, 0.9897, 0.7181, 0.7042, 0.5987, 0.1086, 0.4707,\n         0.1144],\n        [0.8662, 0.6451, 0.2284, 0.9216, 0.5946, 0.8736, 0.0990, 0.5090, 0.7335,\n         0.1160]]))\n\nInstead, I think it would be nice to have the shapes of the parameters instead, like so:\n>>> import torch\n>>> torch.distributions.Normal(torch.randn(10, 10), torch.rand(10, 10))\nNormal(loc: torch.Size([10, 10]), scale: torch.Size([10, 10]))"}