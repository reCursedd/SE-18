{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215442445", "pull_request_review_id": 152709737, "id": 215442445, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTQ0MjQ0NQ==", "diff_hunk": "@@ -0,0 +1,71 @@\n+#if USE_CPU_FUSER || USE_CUDA_FUSER\n+#pragma once\n+\n+#include \"torch/csrc/jit/fusers/fusion_interface.h\"\n+#include \"torch/csrc/jit/fusers/common/fusion_arg_spec.h\"\n+#include \"torch/csrc/jit/fusers/common/common_fusion_function.h\"\n+\n+#include \"torch/csrc/jit/stack.h\"\n+#include \"torch/csrc/jit/interpreter.h\"\n+#include \"torch/csrc/jit/ir.h\"\n+\n+#include \"ATen/ATen.h\"\n+\n+#include <memory>\n+#include <cstdint>\n+#include <vector>\n+#include <unordered_map>\n+\n+namespace torch { namespace jit {\n+\n+// FusionCompiler has very limited shape information available at the time getOrCompile\n+// is called, and this is why it can't really prepare the kernels at that time. Instead,\n+// it returns this object, which will take care of matching the run-time shapes to whatever\n+// kernels we have compiled already.\n+//\n+// Two configurations are considered eligible for the same fused kernel if:\n+//   - the shapes satisfy graph invariants for our fused code (e.g. that all intermediate shapes\n+//     are the same - see fusion_compiler.cpp for more details).\n+//   - their FusionArgSpecs compare equal\n+struct CommonFusionHandle : public FusionHandle {", "path": "torch/csrc/jit/fusers/common/common_fusion_handle.h", "position": null, "original_position": 30, "commit_id": "b8793a0e4816739a48d503f6388d92e61b8c83a1", "original_commit_id": "6fdaaa1f85d6e9ed7189419328ada48e773c6dec", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "body": "\"Handle\" was the term we've been using, and I think FusedKernelCache is more reflective of an implementation than interface concept. \r\n\r\nWhat about FusionHandle and FusionHandleImpl? ", "created_at": "2018-09-05T22:15:18Z", "updated_at": "2018-11-23T15:50:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/10981#discussion_r215442445", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10981", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215442445"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10981#discussion_r215442445"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10981"}}, "body_html": "<p>\"Handle\" was the term we've been using, and I think FusedKernelCache is more reflective of an implementation than interface concept.</p>\n<p>What about FusionHandle and FusionHandleImpl?</p>", "body_text": "\"Handle\" was the term we've been using, and I think FusedKernelCache is more reflective of an implementation than interface concept.\nWhat about FusionHandle and FusionHandleImpl?", "in_reply_to_id": 213699745}