{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215676340", "pull_request_review_id": 152995435, "id": 215676340, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTY3NjM0MA==", "diff_hunk": "@@ -0,0 +1,71 @@\n+#if USE_CPU_FUSER || USE_CUDA_FUSER\n+#pragma once\n+\n+#include \"torch/csrc/jit/fusers/fusion_interface.h\"\n+#include \"torch/csrc/jit/fusers/common/fusion_arg_spec.h\"\n+#include \"torch/csrc/jit/fusers/common/common_fusion_function.h\"\n+\n+#include \"torch/csrc/jit/stack.h\"\n+#include \"torch/csrc/jit/interpreter.h\"\n+#include \"torch/csrc/jit/ir.h\"\n+\n+#include \"ATen/ATen.h\"\n+\n+#include <memory>\n+#include <cstdint>\n+#include <vector>\n+#include <unordered_map>\n+\n+namespace torch { namespace jit {\n+\n+// FusionCompiler has very limited shape information available at the time getOrCompile\n+// is called, and this is why it can't really prepare the kernels at that time. Instead,\n+// it returns this object, which will take care of matching the run-time shapes to whatever\n+// kernels we have compiled already.\n+//\n+// Two configurations are considered eligible for the same fused kernel if:\n+//   - the shapes satisfy graph invariants for our fused code (e.g. that all intermediate shapes\n+//     are the same - see fusion_compiler.cpp for more details).\n+//   - their FusionArgSpecs compare equal\n+struct CommonFusionHandle : public FusionHandle {", "path": "torch/csrc/jit/fusers/common/common_fusion_handle.h", "position": null, "original_position": 30, "commit_id": "b8793a0e4816739a48d503f6388d92e61b8c83a1", "original_commit_id": "6fdaaa1f85d6e9ed7189419328ada48e773c6dec", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Would that mean that `FusionHandle == FusedKernelCache` and `FusionHandleImpl == FusedKernel`? I think that's ok, but I'd like us to move away from the \"fusion\" nomenclature, and I think kernel/function is clearer than handle which is a more generic term. What we have here is a callable.", "created_at": "2018-09-06T15:42:02Z", "updated_at": "2018-11-23T15:50:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/10981#discussion_r215676340", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10981", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215676340"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10981#discussion_r215676340"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10981"}}, "body_html": "<p>Would that mean that <code>FusionHandle == FusedKernelCache</code> and <code>FusionHandleImpl == FusedKernel</code>? I think that's ok, but I'd like us to move away from the \"fusion\" nomenclature, and I think kernel/function is clearer than handle which is a more generic term. What we have here is a callable.</p>", "body_text": "Would that mean that FusionHandle == FusedKernelCache and FusionHandleImpl == FusedKernel? I think that's ok, but I'd like us to move away from the \"fusion\" nomenclature, and I think kernel/function is clearer than handle which is a more generic term. What we have here is a callable.", "in_reply_to_id": 213699745}