{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/368308264", "html_url": "https://github.com/pytorch/pytorch/issues/5311#issuecomment-368308264", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5311", "id": 368308264, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODMwODI2NA==", "user": {"login": "tunz", "id": 7830853, "node_id": "MDQ6VXNlcjc4MzA4NTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7830853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tunz", "html_url": "https://github.com/tunz", "followers_url": "https://api.github.com/users/tunz/followers", "following_url": "https://api.github.com/users/tunz/following{/other_user}", "gists_url": "https://api.github.com/users/tunz/gists{/gist_id}", "starred_url": "https://api.github.com/users/tunz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tunz/subscriptions", "organizations_url": "https://api.github.com/users/tunz/orgs", "repos_url": "https://api.github.com/users/tunz/repos", "events_url": "https://api.github.com/users/tunz/events{/privacy}", "received_events_url": "https://api.github.com/users/tunz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-25T13:12:45Z", "updated_at": "2018-02-25T13:12:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>repro:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.multiprocessing <span class=\"pl-k\">as</span> mp\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> mp.set_sharing_strategy('file_system')</span>\n\nx <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">5</span>)\ndata <span class=\"pl-k\">=</span> [x.storage(), x.storage()[<span class=\"pl-c1\">1</span>:]]\nq <span class=\"pl-k\">=</span> mp.Queue()\nq.put(data)</pre></div>\n<p>This issue came from the sliced data of Storage. The problem is that multiple storage objects can share the same data pointer without reference counting or maybe listing of the other storage objects sharing the same data.</p>\n<p>data ptr is freed at the following code. <code>storage</code> is swapped with the <code>new_storage</code>, and the old <code>storage</code> gets freed when it goes to out of block scope. But, still, we have other storage objects having the pointer of the free data, so it's use-after-free.<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/d2f71cbdebe48b64888f27b0c97ffb346dacecc2/torch/csrc/generic/StorageSharing.cpp#L184-L189\">pytorch/torch/csrc/generic/StorageSharing.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 184 to 189\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/d2f71cbdebe48b64888f27b0c97ffb346dacecc2\">d2f71cb</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L184\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"184\"></td>\n          <td id=\"LC184\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> } <span class=\"pl-k\">else</span> { </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L185\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"185\"></td>\n          <td id=\"LC185\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   THStoragePtr <span class=\"pl-smi\">new_storage</span>(<span class=\"pl-c1\">THPStorage_</span>(newFdStorage)(storage-&gt;<span class=\"pl-smi\">size</span>)); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L186\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"186\"></td>\n          <td id=\"LC186\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">THStorage_</span>(copy)(new_storage, storage); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L187\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"187\"></td>\n          <td id=\"LC187\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">THStorage_</span>(swap)(storage, new_storage); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L188\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"188\"></td>\n          <td id=\"LC188\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   ctx = (THMapAllocatorContext*)storage-&gt;<span class=\"pl-smi\">allocatorContext</span>; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L189\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"189\"></td>\n          <td id=\"LC189\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> } </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>I think we should not allow to slice Storage, or we should allocate a new data buffer for sliced Storages so that each Storage has a separate data buffer. Storage should be a minimal object that manages a buffer, and Tensor should manage the storage offset things. In JavaScript, for example, they have TypedArray and ArrayBuffer. Multiple TypedArrays can share the same array buffer, and each TypedArray manages array buffer offset. ArrayBuffer.slice returns a new ArrayBuffer that has a separate buffer with the original one. But, It might be critical since it's changing a behavior, so I'm a bit worried. WDYT?</p>", "body_text": "repro:\nimport torch\nimport torch.multiprocessing as mp\n\n# mp.set_sharing_strategy('file_system')\n\nx = torch.randn(5)\ndata = [x.storage(), x.storage()[1:]]\nq = mp.Queue()\nq.put(data)\nThis issue came from the sliced data of Storage. The problem is that multiple storage objects can share the same data pointer without reference counting or maybe listing of the other storage objects sharing the same data.\ndata ptr is freed at the following code. storage is swapped with the new_storage, and the old storage gets freed when it goes to out of block scope. But, still, we have other storage objects having the pointer of the free data, so it's use-after-free.\n\n  \n    \n      pytorch/torch/csrc/generic/StorageSharing.cpp\n    \n    \n        Lines 184 to 189\n      in\n      d2f71cb\n    \n    \n    \n    \n\n        \n          \n           } else { \n        \n\n        \n          \n             THStoragePtr new_storage(THPStorage_(newFdStorage)(storage->size)); \n        \n\n        \n          \n             THStorage_(copy)(new_storage, storage); \n        \n\n        \n          \n             THStorage_(swap)(storage, new_storage); \n        \n\n        \n          \n             ctx = (THMapAllocatorContext*)storage->allocatorContext; \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nI think we should not allow to slice Storage, or we should allocate a new data buffer for sliced Storages so that each Storage has a separate data buffer. Storage should be a minimal object that manages a buffer, and Tensor should manage the storage offset things. In JavaScript, for example, they have TypedArray and ArrayBuffer. Multiple TypedArrays can share the same array buffer, and each TypedArray manages array buffer offset. ArrayBuffer.slice returns a new ArrayBuffer that has a separate buffer with the original one. But, It might be critical since it's changing a behavior, so I'm a bit worried. WDYT?", "body": "repro:\r\n```python\r\nimport torch\r\nimport torch.multiprocessing as mp\r\n\r\n# mp.set_sharing_strategy('file_system')\r\n\r\nx = torch.randn(5)\r\ndata = [x.storage(), x.storage()[1:]]\r\nq = mp.Queue()\r\nq.put(data)\r\n```\r\n\r\nThis issue came from the sliced data of Storage. The problem is that multiple storage objects can share the same data pointer without reference counting or maybe listing of the other storage objects sharing the same data.\r\n\r\ndata ptr is freed at the following code. `storage` is swapped with the `new_storage`, and the old `storage` gets freed when it goes to out of block scope. But, still, we have other storage objects having the pointer of the free data, so it's use-after-free.\r\nhttps://github.com/pytorch/pytorch/blob/d2f71cbdebe48b64888f27b0c97ffb346dacecc2/torch/csrc/generic/StorageSharing.cpp#L184-L189\r\n\r\nI think we should not allow to slice Storage, or we should allocate a new data buffer for sliced Storages so that each Storage has a separate data buffer. Storage should be a minimal object that manages a buffer, and Tensor should manage the storage offset things. In JavaScript, for example, they have TypedArray and ArrayBuffer. Multiple TypedArrays can share the same array buffer, and each TypedArray manages array buffer offset. ArrayBuffer.slice returns a new ArrayBuffer that has a separate buffer with the original one. But, It might be critical since it's changing a behavior, so I'm a bit worried. WDYT?\r\n"}