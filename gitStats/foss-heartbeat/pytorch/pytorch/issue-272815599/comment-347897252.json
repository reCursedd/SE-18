{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/347897252", "html_url": "https://github.com/pytorch/pytorch/issues/3619#issuecomment-347897252", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3619", "id": 347897252, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Nzg5NzI1Mg==", "user": {"login": "ChangYong-Oh", "id": 5415216, "node_id": "MDQ6VXNlcjU0MTUyMTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/5415216?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChangYong-Oh", "html_url": "https://github.com/ChangYong-Oh", "followers_url": "https://api.github.com/users/ChangYong-Oh/followers", "following_url": "https://api.github.com/users/ChangYong-Oh/following{/other_user}", "gists_url": "https://api.github.com/users/ChangYong-Oh/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChangYong-Oh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChangYong-Oh/subscriptions", "organizations_url": "https://api.github.com/users/ChangYong-Oh/orgs", "repos_url": "https://api.github.com/users/ChangYong-Oh/repos", "events_url": "https://api.github.com/users/ChangYong-Oh/events{/privacy}", "received_events_url": "https://api.github.com/users/ChangYong-Oh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T15:33:40Z", "updated_at": "2017-11-29T15:33:40Z", "author_association": "NONE", "body_html": "<pre><code>import torch\nfrom torch.autograd import Variable, grad\nimport torch.optim as optim\nimport time\n\n\ndef optimize(max_step, x0, b, A):\n\tx = Variable(x0.clone(), requires_grad=True)\n\toptimizer = optim.Adam([x], lr=0.01)\n\tfor s in range(max_step):\n\t\toptimizer.zero_grad()\n\t\tloss = torch.mean((A.mm(x) - b) ** 2)\n\t\tx.grad = grad([loss], [x], retain_graph=True)[0]\n\t\toptimizer.step()\n\toptimum_loc = x.clone()\n\toptimum_value = torch.sum((A.mm(x) - b) ** 2).data.squeeze()[0]\n\treturn optimum_loc, optimum_value\n\n\ndef error_reproduce_pool():\n\tndim = 1000\n\tn_init = 20\n\n\t# Calling pool BEFORE any pytorch linear algebra operation does not cause any problem\n\t# pool = torch.multiprocessing.Pool(n_init)\n\n\tb = Variable(torch.randn(ndim, 1))\n\tA = Variable(torch.randn(ndim, ndim))\n\tA = A.mm(A.t()) # pytorch Linear Algebra operation\n\tx0 = torch.randn(ndim, n_init)\n\n\t# Calling pool AFTER any pytorch linear algebra operation causes hanging\n\tpool = torch.multiprocessing.Pool(n_init)\n\n\tresult_list = []\n\tfor i in range(n_init):\n\t\tresult_list.append(pool.apply_async(optimize, args=(100, x0[:, i:i+1], b, A)))\n\twhile [not p.ready() for p in result_list].count(True) &gt; 0:\n\t\ttime.sleep(1)\n\tpool.close()\n\treturn [r.get() for r in result_list]\n</code></pre>\n<p>Depending on the order of pool and linear algebra operation this error happens,<br>\nthis can be avoided by putting pool in front of all linear algebra operation.</p>\n<p>But still quite confusing why this happens, it seems that this should be modifed.</p>", "body_text": "import torch\nfrom torch.autograd import Variable, grad\nimport torch.optim as optim\nimport time\n\n\ndef optimize(max_step, x0, b, A):\n\tx = Variable(x0.clone(), requires_grad=True)\n\toptimizer = optim.Adam([x], lr=0.01)\n\tfor s in range(max_step):\n\t\toptimizer.zero_grad()\n\t\tloss = torch.mean((A.mm(x) - b) ** 2)\n\t\tx.grad = grad([loss], [x], retain_graph=True)[0]\n\t\toptimizer.step()\n\toptimum_loc = x.clone()\n\toptimum_value = torch.sum((A.mm(x) - b) ** 2).data.squeeze()[0]\n\treturn optimum_loc, optimum_value\n\n\ndef error_reproduce_pool():\n\tndim = 1000\n\tn_init = 20\n\n\t# Calling pool BEFORE any pytorch linear algebra operation does not cause any problem\n\t# pool = torch.multiprocessing.Pool(n_init)\n\n\tb = Variable(torch.randn(ndim, 1))\n\tA = Variable(torch.randn(ndim, ndim))\n\tA = A.mm(A.t()) # pytorch Linear Algebra operation\n\tx0 = torch.randn(ndim, n_init)\n\n\t# Calling pool AFTER any pytorch linear algebra operation causes hanging\n\tpool = torch.multiprocessing.Pool(n_init)\n\n\tresult_list = []\n\tfor i in range(n_init):\n\t\tresult_list.append(pool.apply_async(optimize, args=(100, x0[:, i:i+1], b, A)))\n\twhile [not p.ready() for p in result_list].count(True) > 0:\n\t\ttime.sleep(1)\n\tpool.close()\n\treturn [r.get() for r in result_list]\n\nDepending on the order of pool and linear algebra operation this error happens,\nthis can be avoided by putting pool in front of all linear algebra operation.\nBut still quite confusing why this happens, it seems that this should be modifed.", "body": "```\r\nimport torch\r\nfrom torch.autograd import Variable, grad\r\nimport torch.optim as optim\r\nimport time\r\n\r\n\r\ndef optimize(max_step, x0, b, A):\r\n\tx = Variable(x0.clone(), requires_grad=True)\r\n\toptimizer = optim.Adam([x], lr=0.01)\r\n\tfor s in range(max_step):\r\n\t\toptimizer.zero_grad()\r\n\t\tloss = torch.mean((A.mm(x) - b) ** 2)\r\n\t\tx.grad = grad([loss], [x], retain_graph=True)[0]\r\n\t\toptimizer.step()\r\n\toptimum_loc = x.clone()\r\n\toptimum_value = torch.sum((A.mm(x) - b) ** 2).data.squeeze()[0]\r\n\treturn optimum_loc, optimum_value\r\n\r\n\r\ndef error_reproduce_pool():\r\n\tndim = 1000\r\n\tn_init = 20\r\n\r\n\t# Calling pool BEFORE any pytorch linear algebra operation does not cause any problem\r\n\t# pool = torch.multiprocessing.Pool(n_init)\r\n\r\n\tb = Variable(torch.randn(ndim, 1))\r\n\tA = Variable(torch.randn(ndim, ndim))\r\n\tA = A.mm(A.t()) # pytorch Linear Algebra operation\r\n\tx0 = torch.randn(ndim, n_init)\r\n\r\n\t# Calling pool AFTER any pytorch linear algebra operation causes hanging\r\n\tpool = torch.multiprocessing.Pool(n_init)\r\n\r\n\tresult_list = []\r\n\tfor i in range(n_init):\r\n\t\tresult_list.append(pool.apply_async(optimize, args=(100, x0[:, i:i+1], b, A)))\r\n\twhile [not p.ready() for p in result_list].count(True) > 0:\r\n\t\ttime.sleep(1)\r\n\tpool.close()\r\n\treturn [r.get() for r in result_list]\r\n```\r\n\r\nDepending on the order of pool and linear algebra operation this error happens,\r\nthis can be avoided by putting pool in front of all linear algebra operation.\r\n\r\nBut still quite confusing why this happens, it seems that this should be modifed."}