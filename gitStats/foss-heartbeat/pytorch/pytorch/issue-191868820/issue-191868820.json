{"url": "https://api.github.com/repos/pytorch/pytorch/issues/262", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/262/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/262/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/262/events", "html_url": "https://github.com/pytorch/pytorch/issues/262", "id": 191868820, "node_id": "MDU6SXNzdWUxOTE4Njg4MjA=", "number": 262, "title": "allow forward / backward hooks to rewrite outputs and gradients", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2016-11-27T16:00:36Z", "updated_at": "2018-10-05T15:06:35Z", "closed_at": "2016-12-16T13:31:19Z", "author_association": "MEMBER", "body_html": "<p>Right now, hooks registered need to be read-only, and cannot be used to modify the grad_input / output and still get correct results in the whole graph.<br>\nThis is because we make some read-only assumptions and optimize buffer reuse.</p>\n<p>Allow writeable hooks with an interface like this:</p>\n<div class=\"highlight highlight-source-python\"><pre>register_backward_hook(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>name<span class=\"pl-pds\">'</span></span>, hook, <span class=\"pl-v\">write</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p>that would allow arbitrary changing of grad_input and output with some user-defined stuff. This for example will be useful for metalearning and in RL.</p>\n<p>Here's an example put together by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> showcasing the bugs if we try to modify the gradients in the current implementation of hooks:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nx <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ny <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\na <span class=\"pl-k\">=</span> x <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>\nb <span class=\"pl-k\">=</span> y <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">hook_a</span>(<span class=\"pl-smi\">grad_output</span>):\n    grad_output.mul_(<span class=\"pl-c1\">2</span>)\n\na.register_hook(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test<span class=\"pl-pds\">'</span></span>, hook_a)\n\nc <span class=\"pl-k\">=</span> a <span class=\"pl-k\">+</span> b\nc.sum().backward()\n\n<span class=\"pl-c1\">print</span>(x.grad) <span class=\"pl-c\"><span class=\"pl-c\">#</span> should be 2, is 2</span>\n<span class=\"pl-c1\">print</span>(y.grad) <span class=\"pl-c\"><span class=\"pl-c\">#</span> should be 3, is 6</span></pre></div>\n<p>cc: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5995229\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ludc\">@ludc</a></p>", "body_text": "Right now, hooks registered need to be read-only, and cannot be used to modify the grad_input / output and still get correct results in the whole graph.\nThis is because we make some read-only assumptions and optimize buffer reuse.\nAllow writeable hooks with an interface like this:\nregister_backward_hook('name', hook, write=True)\nthat would allow arbitrary changing of grad_input and output with some user-defined stuff. This for example will be useful for metalearning and in RL.\nHere's an example put together by @apaszke showcasing the bugs if we try to modify the gradients in the current implementation of hooks:\nimport torch\nfrom torch.autograd import Variable\n\nx = Variable(torch.randn(5, 5), requires_grad=True)\ny = Variable(torch.randn(5, 5), requires_grad=True)\n\na = x * 2\nb = y * 3\n\ndef hook_a(grad_output):\n    grad_output.mul_(2)\n\na.register_hook('test', hook_a)\n\nc = a + b\nc.sum().backward()\n\nprint(x.grad) # should be 2, is 2\nprint(y.grad) # should be 3, is 6\ncc: @ludc", "body": "Right now, hooks registered need to be read-only, and cannot be used to modify the grad_input / output and still get correct results in the whole graph.\r\nThis is because we make some read-only assumptions and optimize buffer reuse.\r\n\r\nAllow writeable hooks with an interface like this:\r\n\r\n```python\r\nregister_backward_hook('name', hook, write=True)\r\n```\r\n\r\nthat would allow arbitrary changing of grad_input and output with some user-defined stuff. This for example will be useful for metalearning and in RL.\r\n\r\nHere's an example put together by @apaszke showcasing the bugs if we try to modify the gradients in the current implementation of hooks:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\nx = Variable(torch.randn(5, 5), requires_grad=True)\r\ny = Variable(torch.randn(5, 5), requires_grad=True)\r\n\r\na = x * 2\r\nb = y * 3\r\n\r\ndef hook_a(grad_output):\r\n    grad_output.mul_(2)\r\n\r\na.register_hook('test', hook_a)\r\n\r\nc = a + b\r\nc.sum().backward()\r\n\r\nprint(x.grad) # should be 2, is 2\r\nprint(y.grad) # should be 3, is 6\r\n```\r\n\r\ncc: @ludc "}