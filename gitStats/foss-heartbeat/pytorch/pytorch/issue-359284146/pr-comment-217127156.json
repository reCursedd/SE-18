{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217127156", "pull_request_review_id": 154767397, "id": 217127156, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNzEyNzE1Ng==", "diff_hunk": "@@ -4092,5 +4092,191 @@ def tearDown(self):\n         super(TestCase, self).tearDown()\n         Distribution.set_default_validate_args(False)\n \n+\n+class TestJit(TestCase):\n+    def _examples(self):\n+        for Dist, params in EXAMPLES:\n+            for param in params:\n+                print('testing {}'.format(Dist.__name__))\n+                keys = param.keys()\n+                values = tuple(param[key] for key in keys)\n+                if not all(isinstance(x, torch.Tensor) for x in values):\n+                    continue\n+                sample = Dist(**param).sample()\n+                yield Dist, keys, values, sample\n+\n+    def test_sample(self):\n+        for Dist, keys, values, sample in self._examples():\n+\n+            def f(*values):\n+                param = dict(zip(keys, values))\n+                dist = Dist(**param)\n+                return dist.sample()\n+\n+            traced_f = torch.jit.trace(f, values, check_trace=False)\n+\n+            # FIXME Schema not found for node\n+            xfail = [\n+                Cauchy,  # aten::cauchy(Double(2,1), float, float, Generator)\n+                Exponential,  # aten::exponential(Double(5, 5), float, Generator)\n+                Geometric,  # aten::uniform(Double(3), float, float, Generator)\n+                Gumbel,  # aten::uniform(Double(5, 5), float, float, Generator)\n+                HalfCauchy,  # aten::cauchy(Double(2, 1), float, float, Generator)\n+                Laplace,  # aten::uniform(Double(5, 5), float, float, Generator)\n+                LowRankMultivariateNormal,  # aten::normal(Dynamic, float, float, Generator)\n+                MultivariateNormal,  # aten::normal(Dynamic, float, float, Generator)\n+                OneHotCategorical,  # aten::scatter(Double(2, 3), int, Dynamic, int)\n+                Pareto,  # aten::exponential(Double(5, 5), float, Generator)\n+                RelaxedBernoulli,  # aten::uniform(Double(3), float, float, Generator)\n+                RelaxedOneHotCategorical,  # aten::uniform(Double(2, 3), float, float, Generator)\n+                StudentT,  # aten::normal(Double(2, 3), float, float, Generator)\n+                Uniform,  # aten::uniform(Double(5, 5), float, float, Generator)\n+                Weibull,  # aten::exponential(Double(5, 5), float, Generator)\n+            ]\n+            if Dist in xfail:\n+                print('        xfail reproducibility')\n+                continue\n+\n+            with torch.random.fork_rng():\n+                sample_0 = traced_f(*values)\n+            sample_1 = traced_f(*values)\n+            self.assertEqual(sample_0, sample_1)\n+\n+            # FIXME no nondeterministic nodes found in trace\n+            xfail = [Beta, Dirichlet]\n+            if Dist in xfail:\n+                print('        xfail nondeterministic nodes')\n+                continue\n+            self.assertTrue(any(n.isNondeterministic() for n in traced_f.graph.nodes()))\n+\n+    def test_rsample(self):\n+        for Dist, keys, values, sample in self._examples():\n+            if not Dist.has_rsample:\n+                continue\n+\n+            def f(*values):\n+                param = dict(zip(keys, values))\n+                dist = Dist(**param)\n+                return dist.rsample()\n+\n+            traced_f = torch.jit.trace(f, values, check_trace=False)\n+\n+            # FIXME Schema not found for node\n+            xfail = [\n+                Cauchy,  # aten::cauchy(Double(2,1), float, float, Generator)\n+                Exponential,  # aten::exponential(Double(5, 5), float, Generator)\n+                Geometric,  # aten::uniform(Double(3), float, float, Generator)\n+                Gumbel,  # aten::uniform(Double(5, 5), float, float, Generator)\n+                HalfCauchy,  # aten::cauchy(Double(2, 1), float, float, Generator)\n+                HalfNormal,  # aten::normal(Double(5, 5), float, float, Generator)\n+                Laplace,  # aten::uniform(Double(5, 5), float, float, Generator)\n+                LogNormal,  # aten::normal(Double(5, 5), float, float, Generator)\n+                LogisticNormal,  # aten::normal(Double(5, 5), float, float, Generator)\n+                LowRankMultivariateNormal,  # aten::normal(Dynamic, float, float, Generator)\n+                MultivariateNormal,  # aten::normal(Dynamic, float, float, Generator)\n+                Normal,  # aten::normal(Double(5, 5), float, float, Generator)\n+                OneHotCategorical,  # aten::scatter(Double(2, 3), int, Dynamic, int)\n+                Pareto,  # aten::exponential(Double(5, 5), float, Generator)\n+                RelaxedBernoulli,  # aten::uniform(Double(3), float, float, Generator)\n+                RelaxedOneHotCategorical,  # aten::uniform(Double(2, 3), float, float, Generator)\n+                StudentT,  # aten::normal(Double(2, 3), float, float, Generator)\n+                Uniform,  # aten::uniform(Double(5, 5), float, float, Generator)\n+                Weibull,  # aten::exponential(Double(5, 5), float, Generator)\n+            ]\n+            if Dist in xfail:\n+                print('        xfail reproducibility')\n+                continue\n+\n+            with torch.random.fork_rng():\n+                sample_0 = traced_f(*values)\n+            sample_1 = traced_f(*values)\n+            self.assertEqual(sample_0, sample_1)\n+\n+            # FIXME no nondeterministic nodes found in trace\n+            xfail = [Beta, Dirichlet]\n+            if Dist in xfail:\n+                print('        xfail nondeterministic nodes')\n+                continue\n+            self.assertTrue(any(n.isNondeterministic() for n in traced_f.graph.nodes()))\n+\n+    def test_log_prob(self):\n+        for Dist, keys, values, sample in self._examples():\n+\n+            def f(sample, *values):\n+                param = dict(zip(keys, values))\n+                dist = Dist(**param)\n+                return dist.log_prob(sample)\n+\n+            torch.jit.trace(f, (sample,) + values)\n+\n+    def test_enumerate_support(self):\n+        for Dist, keys, values, sample in self._examples():\n+            if not Dist.has_enumerate_support:\n+                continue\n+\n+            def f(*values):\n+                param = dict(zip(keys, values))\n+                dist = Dist(**param)\n+                return dist.enumerate_support()\n+\n+            try:\n+                torch.jit.trace(f, values)\n+            except NotImplementedError:\n+                continue\n+\n+    def test_mean(self):\n+        for Dist, keys, values, sample in self._examples():\n+\n+            def f(*values):\n+                param = dict(zip(keys, values))\n+                dist = Dist(**param)\n+                return dist.mean\n+\n+            try:\n+                torch.jit.trace(f, values)\n+            except NotImplementedError:\n+                continue\n+\n+    def test_variance(self):\n+        for Dist, keys, values, sample in self._examples():", "path": "test/test_distributions.py", "position": null, "original_position": 150, "commit_id": "cf132fce4d6bf01d9033ac82c8de6d62c168f150", "original_commit_id": "3c098f47b066255a335ce6c4735132c19e3f406e", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "body": "Is it possible to add a small test for distribution transforms, i.e. transform a sample to unconstrained space and back again to constrained space? I noticed that some of our transforms are using in-place operations like `copy_`, so it will be good to know the failure cases.", "created_at": "2018-09-12T17:45:45Z", "updated_at": "2018-11-23T15:51:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/11560#discussion_r217127156", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11560", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/217127156"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11560#discussion_r217127156"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11560"}}, "body_html": "<p>Is it possible to add a small test for distribution transforms, i.e. transform a sample to unconstrained space and back again to constrained space? I noticed that some of our transforms are using in-place operations like <code>copy_</code>, so it will be good to know the failure cases.</p>", "body_text": "Is it possible to add a small test for distribution transforms, i.e. transform a sample to unconstrained space and back again to constrained space? I noticed that some of our transforms are using in-place operations like copy_, so it will be good to know the failure cases."}