{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1223", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1223/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1223/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1223/events", "html_url": "https://github.com/pytorch/pytorch/issues/1223", "id": 220609998, "node_id": "MDU6SXNzdWUyMjA2MDk5OTg=", "number": 1223, "title": "How To Correctly Kill MultiProcesses During Multi-GPU Training", "user": {"login": "catalystfrank", "id": 5650433, "node_id": "MDQ6VXNlcjU2NTA0MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5650433?v=4", "gravatar_id": "", "url": "https://api.github.com/users/catalystfrank", "html_url": "https://github.com/catalystfrank", "followers_url": "https://api.github.com/users/catalystfrank/followers", "following_url": "https://api.github.com/users/catalystfrank/following{/other_user}", "gists_url": "https://api.github.com/users/catalystfrank/gists{/gist_id}", "starred_url": "https://api.github.com/users/catalystfrank/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/catalystfrank/subscriptions", "organizations_url": "https://api.github.com/users/catalystfrank/orgs", "repos_url": "https://api.github.com/users/catalystfrank/repos", "events_url": "https://api.github.com/users/catalystfrank/events{/privacy}", "received_events_url": "https://api.github.com/users/catalystfrank/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-04-10T11:04:48Z", "updated_at": "2018-10-19T05:59:07Z", "closed_at": "2017-05-24T13:52:46Z", "author_association": "NONE", "body_html": "<p>During the training of using github.com/pytorch/examples/imagenet/main.py, I used the following command:</p>\n<pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 nohup python main.py [options] path/to/imagenetdir 1&gt;a.log 2&gt;a.err &amp;\n</code></pre>\n<p>Then it starts 5 processes in the system, 1 main process appears in nvidia-smi.</p>\n<p>Most of the Time (90% of the time) after I first kill the main process, GPU usage down to 0% so I can kill the other 4 to release GPU Mem to start a new training task. Sometimes (10% of the time), after I killed these 5 processes, the main process remained to be \"python [defunct]\" that cannot be killed even by sudo kill -s 9. The usage of GPU AND the GPU mem are not released.</p>\n<p>Multi-gpu training happened at where I use the following line in my code:</p>\n<pre><code>model = torch.nn.DataParallel(model).cuda()\n</code></pre>\n<p>Please give some hint on \"how to correctly kill multi-gpu training pytorch process[es].\"</p>\n<p>Thanks.</p>", "body_text": "During the training of using github.com/pytorch/examples/imagenet/main.py, I used the following command:\nCUDA_VISIBLE_DEVICES=0,1,2,3 nohup python main.py [options] path/to/imagenetdir 1>a.log 2>a.err &\n\nThen it starts 5 processes in the system, 1 main process appears in nvidia-smi.\nMost of the Time (90% of the time) after I first kill the main process, GPU usage down to 0% so I can kill the other 4 to release GPU Mem to start a new training task. Sometimes (10% of the time), after I killed these 5 processes, the main process remained to be \"python [defunct]\" that cannot be killed even by sudo kill -s 9. The usage of GPU AND the GPU mem are not released.\nMulti-gpu training happened at where I use the following line in my code:\nmodel = torch.nn.DataParallel(model).cuda()\n\nPlease give some hint on \"how to correctly kill multi-gpu training pytorch process[es].\"\nThanks.", "body": "During the training of using github.com/pytorch/examples/imagenet/main.py, I used the following command:\r\n\r\n    CUDA_VISIBLE_DEVICES=0,1,2,3 nohup python main.py [options] path/to/imagenetdir 1>a.log 2>a.err &\r\n\r\nThen it starts 5 processes in the system, 1 main process appears in nvidia-smi.\r\n\r\nMost of the Time (90% of the time) after I first kill the main process, GPU usage down to 0% so I can kill the other 4 to release GPU Mem to start a new training task. Sometimes (10% of the time), after I killed these 5 processes, the main process remained to be \"python [defunct]\" that cannot be killed even by sudo kill -s 9. The usage of GPU AND the GPU mem are not released.\r\n\r\nMulti-gpu training happened at where I use the following line in my code:\r\n\r\n    model = torch.nn.DataParallel(model).cuda()\r\n\r\nPlease give some hint on \"how to correctly kill multi-gpu training pytorch process[es].\"\r\n\r\nThanks."}