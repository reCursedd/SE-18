{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11054", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11054/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11054/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11054/events", "html_url": "https://github.com/pytorch/pytorch/pull/11054", "id": 355389965, "node_id": "MDExOlB1bGxSZXF1ZXN0MjExOTMwOTQ3", "number": 11054, "title": "move bceWithLogits from python to Aten", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-08-30T01:30:19Z", "updated_at": "2018-10-12T18:14:46Z", "closed_at": "2018-10-12T18:14:46Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11054", "html_url": "https://github.com/pytorch/pytorch/pull/11054", "diff_url": "https://github.com/pytorch/pytorch/pull/11054.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11054.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #10648.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"351769019\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10648\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/10648/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/10648\">#10648</a> .<br>\nPerf comparison:</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport time\n\ndef bm(testsize, repeat=100, cuda=False):\n    total_time = 0.0\n    pos_weight= torch.ones(testsize[1], device='cuda' if cuda else 'cpu') / testsize[1]\n    # loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    loss = nn.BCEWithLogitsLoss()\n    input = torch.randn(testsize, device='cuda' if cuda else 'cpu').clamp_(2.8e-2, 1 - 2.8e-2)\n    target = torch.randn(testsize, device='cuda' if cuda else 'cpu').gt(0).float()\n    input.requires_grad = True\n    target.requires_grad = True\n    for _ in range(repeat):\n        start = time.time()\n        l = loss(input, target)\n        l.backward()\n        # print(target.grad)\n        end = time.time()\n        total_time += end - start\n    return total_time\n\nfor cuda in [False, True]:\n    for testsize in [(100, 100), (1000, 1000), (2000, 2000)]:\n        # print(testsize, cuda)\n        print('{:.5f}'.format(bm(testsize, cuda=cuda)))\n</code></pre>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>Python CPU</th>\n<th>Aten CPU</th>\n<th>Python GPU</th>\n<th>Aten GPU</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>(100, 100)</td>\n<td>0.15813s</td>\n<td>0.10890s</td>\n<td>0.14601s</td>\n<td>0.07070s</td>\n</tr>\n<tr>\n<td>(1000, 1000)</td>\n<td>1.74051s</td>\n<td>0.95038s</td>\n<td>0.15158s</td>\n<td>0.10153s</td>\n</tr>\n<tr>\n<td>(2000, 2000)</td>\n<td>5.36515s</td>\n<td>2.46996s</td>\n<td>0.31322s</td>\n<td>0.200941s</td>\n</tr>\n</tbody>\n</table>", "body_text": "Fixes #10648 .\nPerf comparison:\nimport torch\nimport torch.nn as nn\nimport time\n\ndef bm(testsize, repeat=100, cuda=False):\n    total_time = 0.0\n    pos_weight= torch.ones(testsize[1], device='cuda' if cuda else 'cpu') / testsize[1]\n    # loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n    loss = nn.BCEWithLogitsLoss()\n    input = torch.randn(testsize, device='cuda' if cuda else 'cpu').clamp_(2.8e-2, 1 - 2.8e-2)\n    target = torch.randn(testsize, device='cuda' if cuda else 'cpu').gt(0).float()\n    input.requires_grad = True\n    target.requires_grad = True\n    for _ in range(repeat):\n        start = time.time()\n        l = loss(input, target)\n        l.backward()\n        # print(target.grad)\n        end = time.time()\n        total_time += end - start\n    return total_time\n\nfor cuda in [False, True]:\n    for testsize in [(100, 100), (1000, 1000), (2000, 2000)]:\n        # print(testsize, cuda)\n        print('{:.5f}'.format(bm(testsize, cuda=cuda)))\n\n\n\n\n\nPython CPU\nAten CPU\nPython GPU\nAten GPU\n\n\n\n\n(100, 100)\n0.15813s\n0.10890s\n0.14601s\n0.07070s\n\n\n(1000, 1000)\n1.74051s\n0.95038s\n0.15158s\n0.10153s\n\n\n(2000, 2000)\n5.36515s\n2.46996s\n0.31322s\n0.200941s", "body": "Fixes #10648 . \r\nPerf comparison:\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport time\r\n\r\ndef bm(testsize, repeat=100, cuda=False):\r\n    total_time = 0.0\r\n    pos_weight= torch.ones(testsize[1], device='cuda' if cuda else 'cpu') / testsize[1]\r\n    # loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\r\n    loss = nn.BCEWithLogitsLoss()\r\n    input = torch.randn(testsize, device='cuda' if cuda else 'cpu').clamp_(2.8e-2, 1 - 2.8e-2)\r\n    target = torch.randn(testsize, device='cuda' if cuda else 'cpu').gt(0).float()\r\n    input.requires_grad = True\r\n    target.requires_grad = True\r\n    for _ in range(repeat):\r\n        start = time.time()\r\n        l = loss(input, target)\r\n        l.backward()\r\n        # print(target.grad)\r\n        end = time.time()\r\n        total_time += end - start\r\n    return total_time\r\n\r\nfor cuda in [False, True]:\r\n    for testsize in [(100, 100), (1000, 1000), (2000, 2000)]:\r\n        # print(testsize, cuda)\r\n        print('{:.5f}'.format(bm(testsize, cuda=cuda)))\r\n```\r\n|    | Python CPU | Aten CPU | Python GPU | Aten GPU\r\n| ------------- | ------------- | ------------- | ------------- | ------------- |\r\n| (100, 100)  | 0.15813s | 0.10890s | 0.14601s | 0.07070s |\r\n| (1000, 1000)  | 1.74051s | 0.95038s | 0.15158s | 0.10153s | \r\n| (2000, 2000) | 5.36515s | 2.46996s | 0.31322s | 0.200941s |\r\n\r\n"}