{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3751", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3751/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3751/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3751/events", "html_url": "https://github.com/pytorch/pytorch/issues/3751", "id": 274720612, "node_id": "MDU6SXNzdWUyNzQ3MjA2MTI=", "number": 3751, "title": "cuDNN RNNs silently break weight tying in __init__", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-11-17T01:20:04Z", "updated_at": "2017-11-18T03:32:39Z", "closed_at": "2017-11-18T03:32:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>While the fast path for cuDNN RNNs introduced in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"244681283\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2179\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/2179/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/2179\">#2179</a> includes a check intended to fall back to copying weights if weights are moved or aliased, this only works if the aliasing happens after the weights are first compacted. But the most natural place to tie weights is inside a module's <code>__init__</code> method, which runs before <code>.cuda()</code>, the typical time when <code>flatten_parameters</code> is first invoked.</p>\n<p>Here's an MWE. The three runs (CPU, weight tying before <code>.cuda()</code>, and weight tying after <code>.cuda()</code>) should all give the same results, and they do before <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"244681283\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2179\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/2179/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/2179\">#2179</a>, but they have not since:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">for</span> cuda <span class=\"pl-k\">in</span> (<span class=\"pl-c1\">False</span>, <span class=\"pl-c1\">True</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>CUDA:<span class=\"pl-pds\">'</span></span>, cuda)\n    <span class=\"pl-k\">for</span> when <span class=\"pl-k\">in</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>before<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>after<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">if</span> cuda <span class=\"pl-k\">else</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>before<span class=\"pl-pds\">'</span></span>,):\n        <span class=\"pl-k\">if</span> cuda: <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tying<span class=\"pl-pds\">'</span></span>, when, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>.cuda()<span class=\"pl-pds\">'</span></span>)\n        torch.manual_seed(<span class=\"pl-c1\">1</span>)\n        rnn <span class=\"pl-k\">=</span> torch.nn.LSTM(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">bidirectional</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">dropout</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        <span class=\"pl-k\">if</span> when <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>before<span class=\"pl-pds\">'</span></span>: rnn.bias_ih_l0_reverse <span class=\"pl-k\">=</span> rnn.bias_ih_l0\n        inputs <span class=\"pl-k\">=</span> torch.autograd.Variable(torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n        <span class=\"pl-k\">if</span> cuda:\n            inputs <span class=\"pl-k\">=</span> inputs.cuda(<span class=\"pl-c1\">0</span>)\n            rnn.cuda(<span class=\"pl-c1\">0</span>)\n        <span class=\"pl-k\">if</span> when <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>after<span class=\"pl-pds\">'</span></span>: rnn.bias_ih_l0_reverse <span class=\"pl-k\">=</span> rnn.bias_ih_l0\n        opt <span class=\"pl-k\">=</span> torch.optim.SGD(rnn.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\n\n        opt.zero_grad()\n        output <span class=\"pl-k\">=</span> rnn(inputs)[<span class=\"pl-c1\">0</span>]\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>iter 1 output:<span class=\"pl-pds\">'</span></span>, output.data[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>])\n        output.sum().backward()\n\n        opt.step()\n        output <span class=\"pl-k\">=</span> rnn(inputs)[<span class=\"pl-c1\">0</span>]\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>iter 2 output:<span class=\"pl-pds\">'</span></span>, output.data[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>])</pre></div>", "body_text": "While the fast path for cuDNN RNNs introduced in #2179 includes a check intended to fall back to copying weights if weights are moved or aliased, this only works if the aliasing happens after the weights are first compacted. But the most natural place to tie weights is inside a module's __init__ method, which runs before .cuda(), the typical time when flatten_parameters is first invoked.\nHere's an MWE. The three runs (CPU, weight tying before .cuda(), and weight tying after .cuda()) should all give the same results, and they do before #2179, but they have not since:\nimport torch\n\nfor cuda in (False, True):\n    print('CUDA:', cuda)\n    for when in ('before', 'after') if cuda else ('before',):\n        if cuda: print('tying', when, '.cuda()')\n        torch.manual_seed(1)\n        rnn = torch.nn.LSTM(1, 1, num_layers=1, bidirectional=True, dropout=0)\n        if when == 'before': rnn.bias_ih_l0_reverse = rnn.bias_ih_l0\n        inputs = torch.autograd.Variable(torch.rand(1, 1, 1))\n        if cuda:\n            inputs = inputs.cuda(0)\n            rnn.cuda(0)\n        if when == 'after': rnn.bias_ih_l0_reverse = rnn.bias_ih_l0\n        opt = torch.optim.SGD(rnn.parameters(), lr=0.1)\n\n        opt.zero_grad()\n        output = rnn(inputs)[0]\n        print('iter 1 output:', output.data[0, 0, 0])\n        output.sum().backward()\n\n        opt.step()\n        output = rnn(inputs)[0]\n        print('iter 2 output:', output.data[0, 0, 0])", "body": "While the fast path for cuDNN RNNs introduced in https://github.com/pytorch/pytorch/pull/2179 includes a check intended to fall back to copying weights if weights are moved or aliased, this only works if the aliasing happens after the weights are first compacted. But the most natural place to tie weights is inside a module's `__init__` method, which runs before `.cuda()`, the typical time when `flatten_parameters` is first invoked.\r\n\r\nHere's an MWE. The three runs (CPU, weight tying before `.cuda()`, and weight tying after `.cuda()`) should all give the same results, and they do before #2179, but they have not since:\r\n\r\n```python\r\nimport torch\r\n\r\nfor cuda in (False, True):\r\n    print('CUDA:', cuda)\r\n    for when in ('before', 'after') if cuda else ('before',):\r\n        if cuda: print('tying', when, '.cuda()')\r\n        torch.manual_seed(1)\r\n        rnn = torch.nn.LSTM(1, 1, num_layers=1, bidirectional=True, dropout=0)\r\n        if when == 'before': rnn.bias_ih_l0_reverse = rnn.bias_ih_l0\r\n        inputs = torch.autograd.Variable(torch.rand(1, 1, 1))\r\n        if cuda:\r\n            inputs = inputs.cuda(0)\r\n            rnn.cuda(0)\r\n        if when == 'after': rnn.bias_ih_l0_reverse = rnn.bias_ih_l0\r\n        opt = torch.optim.SGD(rnn.parameters(), lr=0.1)\r\n\r\n        opt.zero_grad()\r\n        output = rnn(inputs)[0]\r\n        print('iter 1 output:', output.data[0, 0, 0])\r\n        output.sum().backward()\r\n\r\n        opt.step()\r\n        output = rnn(inputs)[0]\r\n        print('iter 2 output:', output.data[0, 0, 0])\r\n```"}