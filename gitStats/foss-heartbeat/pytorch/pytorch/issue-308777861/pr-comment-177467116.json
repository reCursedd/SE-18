{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/177467116", "pull_request_review_id": 107341567, "id": 177467116, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NzQ2NzExNg==", "diff_hunk": "@@ -0,0 +1,132 @@\n+#include \"ATen/native/cpu/UnaryOpsKernel.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/Parallel.h\"\n+#include <iostream>\n+#include <cmath>\n+#include \"ATen/native/cpu/Vec256.h\"\n+\n+namespace at {\n+namespace native {\n+\n+using namespace vec256;\n+\n+// This modifies arr in place with given OP\n+template <class scalar_t, template <class> class VOP, template <class> class SOP, CPUCapability C>\n+inline void kernel_(scalar_t *arr_out, const scalar_t *arr_in, size_t start, size_t end) {\n+  // Use all 16 registers.\n+  Vec256<scalar_t> a[8];\n+  size_t width =\n+      256 / sizeof(scalar_t); // primitives per 256 bytes (two cache lines)\n+  size_t epr = 32 / sizeof(scalar_t); // primitives per Vec256\n+  size_t k = 0;\n+  for (; k < (end - start) / width; k++) {\n+    for (size_t i = 0; i < 8; i++) {\n+      a[i].load(arr_in + (k * width) + i * epr + start);\n+    }\n+    for (size_t i = 0; i < 8; i++) {\n+      VOP<scalar_t>()(a[i]);\n+    }\n+    for (size_t i = 0; i < 8; i++) {\n+      a[i].store(arr_out + (k * width) + i * epr + start);\n+    }\n+  }\n+  k = k * width + start;\n+  for (; k < end; k++) {\n+    arr_out[k] = SOP<scalar_t>()(arr_in[k]);\n+  }\n+}\n+\n+template <template <class> class VOP, template <class> class SOP, CPUCapability C>\n+inline void allImpl(Tensor & result, const Tensor &self, const char* name) {\n+  AT_DISPATCH_FLOATING_TYPES(self.type(), name, [&] {\n+    at::parallel_for_1d<scalar_t>(&kernel_<scalar_t, VOP, SOP, CURRENT_CAPABILITY>, result, self);\n+  });\n+}\n+\n+namespace {\n+\n+template <typename T> struct ceilSOP {\n+   T operator()(const T &x) const { return std::ceil(x); }\n+};\n+\n+template <typename T> struct ceilVOP {\n+   void operator()(Vec256<T> &x) const { x.ceil(); }\n+};\n+\n+}\n+\n+template <>\n+void ceilImplC<CURRENT_CAPABILITY>::function(Tensor &result,\n+                                             const Tensor &self) {\n+  allImpl<ceilVOP, ceilSOP, CURRENT_CAPABILITY>(result, self, \"ceil\");\n+}\n+\n+namespace {\n+\n+template <typename T> struct floorSOP {\n+   T operator()(const T &x) const { return std::floor(x); }\n+};\n+\n+template <typename T> struct floorVOP {\n+   void operator()(Vec256<T> &x) const { x.floor(); }\n+};\n+\n+}\n+\n+template <>\n+void floorImplC<CURRENT_CAPABILITY>::function(Tensor &result, const Tensor &self) {\n+  allImpl<floorVOP, floorSOP, CURRENT_CAPABILITY>(result, self, \"floor\");\n+}\n+\n+namespace {\n+\n+template <typename T> struct roundSOP {\n+   T operator()(const T &x) const { return std::round(x); }\n+};\n+\n+template <typename T> struct roundVOP {\n+   void operator()(Vec256<T> &x) const { x.round(); }\n+};\n+\n+}\n+\n+template <>\n+void roundImplC<CURRENT_CAPABILITY>::function(Tensor &result, const Tensor &self) {\n+  allImpl<roundVOP, roundSOP, CURRENT_CAPABILITY>(result, self, \"round\");\n+}\n+\n+namespace {\n+\n+template <typename T> struct truncSOP {\n+   T operator()(const T &x) const { return std::trunc(x); }\n+};\n+\n+template <typename T> struct truncVOP {\n+   void operator()(Vec256<T> &x) const { x.trunc(); }\n+};\n+\n+}\n+\n+template <>\n+void truncImplC<CURRENT_CAPABILITY>::function(Tensor &result, const Tensor &self) {\n+  allImpl<truncVOP, truncSOP, CURRENT_CAPABILITY>(result, self, \"trunc\");\n+}\n+\n+namespace {\n+\n+template <typename T> struct sqrtSOP {\n+   T operator()(const T &x) const { return std::sqrt(x); }\n+};\n+\n+template <typename T> struct sqrtVOP {\n+   void operator()(Vec256<T> &x) const { x.sqrt(); }\n+};\n+\n+}\n+\n+template <>\n+void sqrtImplC<CURRENT_CAPABILITY>::function(Tensor &result, const Tensor &self) {\n+  allImpl<sqrtVOP, sqrtSOP, CURRENT_CAPABILITY>(result, self, \"sqrt\");\n+}", "path": "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "position": null, "original_position": 130, "commit_id": "1a6993fb9b94a8bd24ece404e3d5d235e82ef6f4", "original_commit_id": "f40e0a8cbe19c19d83a86fe9d94313170347c800", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This looks like a good candidate for a macro too", "created_at": "2018-03-27T15:25:09Z", "updated_at": "2018-11-23T15:41:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/6030#discussion_r177467116", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6030", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/177467116"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6030#discussion_r177467116"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6030"}}, "body_html": "<p>This looks like a good candidate for a macro too</p>", "body_text": "This looks like a good candidate for a macro too"}