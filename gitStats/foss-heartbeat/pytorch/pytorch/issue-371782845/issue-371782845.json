{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12855", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12855/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12855/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12855/events", "html_url": "https://github.com/pytorch/pytorch/issues/12855", "id": 371782845, "node_id": "MDU6SXNzdWUzNzE3ODI4NDU=", "number": 12855, "title": "Model with Caffe2 runs much slower than it with pytorch in GPU mode !!!!", "user": {"login": "perrywu1989", "id": 29011669, "node_id": "MDQ6VXNlcjI5MDExNjY5", "avatar_url": "https://avatars1.githubusercontent.com/u/29011669?v=4", "gravatar_id": "", "url": "https://api.github.com/users/perrywu1989", "html_url": "https://github.com/perrywu1989", "followers_url": "https://api.github.com/users/perrywu1989/followers", "following_url": "https://api.github.com/users/perrywu1989/following{/other_user}", "gists_url": "https://api.github.com/users/perrywu1989/gists{/gist_id}", "starred_url": "https://api.github.com/users/perrywu1989/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/perrywu1989/subscriptions", "organizations_url": "https://api.github.com/users/perrywu1989/orgs", "repos_url": "https://api.github.com/users/perrywu1989/repos", "events_url": "https://api.github.com/users/perrywu1989/events{/privacy}", "received_events_url": "https://api.github.com/users/perrywu1989/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-19T02:28:13Z", "updated_at": "2018-10-22T17:44:14Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Firstly, I run my model(CRNN model) on Pytorch GPU. With code:</p>\n<pre><code>directory = os.path.dirname(os.path.realpath(__file__))\nmodel_file = os.path.join(directory, './pth/netCRNN_epoch17_idx78105_step2733675.pth')\nkeys = new_key.KEY_4498\nmodel = Resnet18(False, num_classes=len(keys) + 2)\n\nstate_dict = torch.load(model_file)\nmodel.load_state_dict(state_dict)\nmodel.cuda()\nmodel.train(False)\n\nbegin = time.time()\ntol_time = 0\nfor i in range(5000):\n    x = torch.randn(1, 3, 32, 512, requires_grad=False).cuda()\n    begin = time.time()\n    out = model(x)\n    end = time.time()\n    tol_time += (end - begin)\n\nprint('cost time:', tol_time / 5000)\n</code></pre>\n<p><strong>Result:  cost time: 0.002339872884750366</strong></p>\n<p>Secondly, I convert my pytorch model to onnx and then convert onnx to pb in caffe2. With code:</p>\n<pre><code>directory = os.path.dirname(os.path.realpath(__file__))\nmodel_file = os.path.join(directory, './pth/netCRNN_epoch17_idx78105_step2733675.pth')\nkeys = new_key.KEY_4498\nmodel = Resnet18(False, num_classes=len(keys) + 2)\n\nstate_dict = torch.load(model_file)\nmodel.load_state_dict(state_dict)\nmodel.cuda()\nmodel.train(False)\n\nx = torch.randn(1, 3, 32, 512, requires_grad=False).cuda()\n\n# Export the model\ntorch_out = torch.onnx._export(model,  # model being run\n                               x,  # model input (or a tuple for multiple inputs)\n                               \"ocr_api.onnx\",  # where to save the model (can be a file or file-like object)\n                               export_params=True)\n</code></pre>\n<p>and</p>\n<pre><code>convert-onnx-to-caffe2 ocr_api.onnx --output predict_net.pb --init-net-output init_net.pb\n</code></pre>\n<p>Thirdly,I run model in Caffe2 on GPU.  With code:</p>\n<pre><code>workspace.ResetWorkspace()\ndevice_opts = core.DeviceOption(caffe2_pb2.CUDA, 0)\n\ninput_data = np.random.rand(1, 3, 32, 128).astype(np.float32)  # NCHW\n\ninit_def = caffe2_pb2.NetDef()\nwith open('init_net.pb', 'rb') as f:\n    init_def.ParseFromString(f.read())\n    init_def.device_option.CopyFrom(device_opts)\n    workspace.RunNetOnce(init_def)\n    # workspace.RunNetOnce(init_def.SerializeToString())\n\nnet_def = caffe2_pb2.NetDef()\nwith open('predict_net.pb', 'rb') as f:\n    net_def.ParseFromString(f.read())\n    net_def.device_option.CopyFrom(device_opts)\n\n    input_name = net_def.external_input[0]\n    workspace.FeedBlob(input_name, input_data,device_opts)\n    workspace.CreateNet(net_def)\n    # workspace.CreateNet(net_def.SerializeToString())\n\nname = net_def.name\noutput_name = net_def.external_output[-1]\ninput_name = net_def.external_input[0]\nprint(name, input_name, output_name)\n\nnum_iters = 5000\n\ntol_time = 0\nfor i in range(num_iters):\n    input_data = np.random.rand(1, 3, 32, 512).astype(np.float32)  # NCHW\n    start = time.time()\n    workspace.FeedBlob(input_name, input_data,device_opts)\n    workspace.RunNet(name, 1)\n    results = workspace.FetchBlob(output_name)\n    end = time.time()\n    tol_time += (end - start)\n\nprint('Run time per RunNet: {}'.format(tol_time / num_iters))\n</code></pre>\n<p><strong>Run time per RunNet: 0.005980247449874878</strong></p>\n<p><strong>But, the speed is slower than that in Pytorch !  How weird!</strong></p>\n<p>Any one konws what happens!</p>", "body_text": "Firstly, I run my model(CRNN model) on Pytorch GPU. With code:\ndirectory = os.path.dirname(os.path.realpath(__file__))\nmodel_file = os.path.join(directory, './pth/netCRNN_epoch17_idx78105_step2733675.pth')\nkeys = new_key.KEY_4498\nmodel = Resnet18(False, num_classes=len(keys) + 2)\n\nstate_dict = torch.load(model_file)\nmodel.load_state_dict(state_dict)\nmodel.cuda()\nmodel.train(False)\n\nbegin = time.time()\ntol_time = 0\nfor i in range(5000):\n    x = torch.randn(1, 3, 32, 512, requires_grad=False).cuda()\n    begin = time.time()\n    out = model(x)\n    end = time.time()\n    tol_time += (end - begin)\n\nprint('cost time:', tol_time / 5000)\n\nResult:  cost time: 0.002339872884750366\nSecondly, I convert my pytorch model to onnx and then convert onnx to pb in caffe2. With code:\ndirectory = os.path.dirname(os.path.realpath(__file__))\nmodel_file = os.path.join(directory, './pth/netCRNN_epoch17_idx78105_step2733675.pth')\nkeys = new_key.KEY_4498\nmodel = Resnet18(False, num_classes=len(keys) + 2)\n\nstate_dict = torch.load(model_file)\nmodel.load_state_dict(state_dict)\nmodel.cuda()\nmodel.train(False)\n\nx = torch.randn(1, 3, 32, 512, requires_grad=False).cuda()\n\n# Export the model\ntorch_out = torch.onnx._export(model,  # model being run\n                               x,  # model input (or a tuple for multiple inputs)\n                               \"ocr_api.onnx\",  # where to save the model (can be a file or file-like object)\n                               export_params=True)\n\nand\nconvert-onnx-to-caffe2 ocr_api.onnx --output predict_net.pb --init-net-output init_net.pb\n\nThirdly,I run model in Caffe2 on GPU.  With code:\nworkspace.ResetWorkspace()\ndevice_opts = core.DeviceOption(caffe2_pb2.CUDA, 0)\n\ninput_data = np.random.rand(1, 3, 32, 128).astype(np.float32)  # NCHW\n\ninit_def = caffe2_pb2.NetDef()\nwith open('init_net.pb', 'rb') as f:\n    init_def.ParseFromString(f.read())\n    init_def.device_option.CopyFrom(device_opts)\n    workspace.RunNetOnce(init_def)\n    # workspace.RunNetOnce(init_def.SerializeToString())\n\nnet_def = caffe2_pb2.NetDef()\nwith open('predict_net.pb', 'rb') as f:\n    net_def.ParseFromString(f.read())\n    net_def.device_option.CopyFrom(device_opts)\n\n    input_name = net_def.external_input[0]\n    workspace.FeedBlob(input_name, input_data,device_opts)\n    workspace.CreateNet(net_def)\n    # workspace.CreateNet(net_def.SerializeToString())\n\nname = net_def.name\noutput_name = net_def.external_output[-1]\ninput_name = net_def.external_input[0]\nprint(name, input_name, output_name)\n\nnum_iters = 5000\n\ntol_time = 0\nfor i in range(num_iters):\n    input_data = np.random.rand(1, 3, 32, 512).astype(np.float32)  # NCHW\n    start = time.time()\n    workspace.FeedBlob(input_name, input_data,device_opts)\n    workspace.RunNet(name, 1)\n    results = workspace.FetchBlob(output_name)\n    end = time.time()\n    tol_time += (end - start)\n\nprint('Run time per RunNet: {}'.format(tol_time / num_iters))\n\nRun time per RunNet: 0.005980247449874878\nBut, the speed is slower than that in Pytorch !  How weird!\nAny one konws what happens!", "body": "Firstly, I run my model(CRNN model) on Pytorch GPU. With code:\r\n```\r\ndirectory = os.path.dirname(os.path.realpath(__file__))\r\nmodel_file = os.path.join(directory, './pth/netCRNN_epoch17_idx78105_step2733675.pth')\r\nkeys = new_key.KEY_4498\r\nmodel = Resnet18(False, num_classes=len(keys) + 2)\r\n\r\nstate_dict = torch.load(model_file)\r\nmodel.load_state_dict(state_dict)\r\nmodel.cuda()\r\nmodel.train(False)\r\n\r\nbegin = time.time()\r\ntol_time = 0\r\nfor i in range(5000):\r\n    x = torch.randn(1, 3, 32, 512, requires_grad=False).cuda()\r\n    begin = time.time()\r\n    out = model(x)\r\n    end = time.time()\r\n    tol_time += (end - begin)\r\n\r\nprint('cost time:', tol_time / 5000)\r\n```\r\n**Result:  cost time: 0.002339872884750366**\r\n\r\nSecondly, I convert my pytorch model to onnx and then convert onnx to pb in caffe2. With code:\r\n\r\n```\r\ndirectory = os.path.dirname(os.path.realpath(__file__))\r\nmodel_file = os.path.join(directory, './pth/netCRNN_epoch17_idx78105_step2733675.pth')\r\nkeys = new_key.KEY_4498\r\nmodel = Resnet18(False, num_classes=len(keys) + 2)\r\n\r\nstate_dict = torch.load(model_file)\r\nmodel.load_state_dict(state_dict)\r\nmodel.cuda()\r\nmodel.train(False)\r\n\r\nx = torch.randn(1, 3, 32, 512, requires_grad=False).cuda()\r\n\r\n# Export the model\r\ntorch_out = torch.onnx._export(model,  # model being run\r\n                               x,  # model input (or a tuple for multiple inputs)\r\n                               \"ocr_api.onnx\",  # where to save the model (can be a file or file-like object)\r\n                               export_params=True)\r\n```\r\nand \r\n```\r\nconvert-onnx-to-caffe2 ocr_api.onnx --output predict_net.pb --init-net-output init_net.pb\r\n```\r\n\r\nThirdly,I run model in Caffe2 on GPU.  With code:\r\n```\r\nworkspace.ResetWorkspace()\r\ndevice_opts = core.DeviceOption(caffe2_pb2.CUDA, 0)\r\n\r\ninput_data = np.random.rand(1, 3, 32, 128).astype(np.float32)  # NCHW\r\n\r\ninit_def = caffe2_pb2.NetDef()\r\nwith open('init_net.pb', 'rb') as f:\r\n    init_def.ParseFromString(f.read())\r\n    init_def.device_option.CopyFrom(device_opts)\r\n    workspace.RunNetOnce(init_def)\r\n    # workspace.RunNetOnce(init_def.SerializeToString())\r\n\r\nnet_def = caffe2_pb2.NetDef()\r\nwith open('predict_net.pb', 'rb') as f:\r\n    net_def.ParseFromString(f.read())\r\n    net_def.device_option.CopyFrom(device_opts)\r\n\r\n    input_name = net_def.external_input[0]\r\n    workspace.FeedBlob(input_name, input_data,device_opts)\r\n    workspace.CreateNet(net_def)\r\n    # workspace.CreateNet(net_def.SerializeToString())\r\n\r\nname = net_def.name\r\noutput_name = net_def.external_output[-1]\r\ninput_name = net_def.external_input[0]\r\nprint(name, input_name, output_name)\r\n\r\nnum_iters = 5000\r\n\r\ntol_time = 0\r\nfor i in range(num_iters):\r\n    input_data = np.random.rand(1, 3, 32, 512).astype(np.float32)  # NCHW\r\n    start = time.time()\r\n    workspace.FeedBlob(input_name, input_data,device_opts)\r\n    workspace.RunNet(name, 1)\r\n    results = workspace.FetchBlob(output_name)\r\n    end = time.time()\r\n    tol_time += (end - start)\r\n\r\nprint('Run time per RunNet: {}'.format(tol_time / num_iters))\r\n```\r\n**Run time per RunNet: 0.005980247449874878**\r\n\r\n**But, the speed is slower than that in Pytorch !  How weird!** \r\n\r\nAny one konws what happens!\r\n"}