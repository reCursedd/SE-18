{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/297796756", "html_url": "https://github.com/pytorch/pytorch/issues/1369#issuecomment-297796756", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1369", "id": 297796756, "node_id": "MDEyOklzc3VlQ29tbWVudDI5Nzc5Njc1Ng==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-27T18:17:55Z", "updated_at": "2017-04-27T18:17:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here are my thoughts:</p>\n<ol>\n<li>torch.sparse.XXXTensor should accept empty index/value tensors, so <code>make_sparse</code> should not be necessary.</li>\n<li>We should add sparse versions of unary pointwise ops, that delegate to <code>values()</code>.</li>\n<li>Performing * or / directly on the values is just an optimization to avoid comparing indices; it would work even if you didn't do that. Maybe we need an unsafe direct divide (assumes that indices are equal), but I don't think so.</li>\n<li>The <code>add_</code> call here directly on the values is a special case; it only makes sense here because we're then going to multiply through by grad so it's okay to only add to the non-zero entries. I think we should leave this to the user to sort out.<br>\n4a. I think the <code>add_</code> is only here to prevent divide by zero, so I don't think we need it in the sparse case.<br>\nHere's how the code would look with these minor changes (1 and 2). Still a bit longer, but mostly just because of the \"unusual\" parts (optimized <code>add_</code> and <code>div</code>) that should be done by hand.</li>\n<li>I'm not sure this kernel is safe unless you call <code>p.grad.coalesce</code> first (yikes!). Does <code>sparse_mask</code> have a contract about whether it will coalesce its indices?</li>\n</ol>\n<pre><code>if p.grad.data.is_sparse:\n    grad = p.grad\n    state['sum'].add_(grad.pow(2))\n    std_values = state['sum'].sparse_mask(grad).values().sqrt_()\n    # warning! this is not safe because `sparse_mask` or `sqrt` may have coalesced the gradient!\n    p.data.add_(-clr, grad.data.new(grad.indices(), grad.values() / std_values))\nelse:\n    state['sum'].addcmul_(1, grad, grad)\n    std = state['sum'].sqrt().add_(1e-10)\n    p.data.addcdiv_(-clr, grad, std)\n</code></pre>\n<p>P.S. You might want to measure the performance of adagrad with my benchmark script (<a href=\"https://gist.github.com/adamlerer/4159ddb4bd19e87de1b4bb3215a1db6f\">https://gist.github.com/adamlerer/4159ddb4bd19e87de1b4bb3215a1db6f</a>) ; if it does coalescing under the hood it could be much slower.</p>", "body_text": "Here are my thoughts:\n\ntorch.sparse.XXXTensor should accept empty index/value tensors, so make_sparse should not be necessary.\nWe should add sparse versions of unary pointwise ops, that delegate to values().\nPerforming * or / directly on the values is just an optimization to avoid comparing indices; it would work even if you didn't do that. Maybe we need an unsafe direct divide (assumes that indices are equal), but I don't think so.\nThe add_ call here directly on the values is a special case; it only makes sense here because we're then going to multiply through by grad so it's okay to only add to the non-zero entries. I think we should leave this to the user to sort out.\n4a. I think the add_ is only here to prevent divide by zero, so I don't think we need it in the sparse case.\nHere's how the code would look with these minor changes (1 and 2). Still a bit longer, but mostly just because of the \"unusual\" parts (optimized add_ and div) that should be done by hand.\nI'm not sure this kernel is safe unless you call p.grad.coalesce first (yikes!). Does sparse_mask have a contract about whether it will coalesce its indices?\n\nif p.grad.data.is_sparse:\n    grad = p.grad\n    state['sum'].add_(grad.pow(2))\n    std_values = state['sum'].sparse_mask(grad).values().sqrt_()\n    # warning! this is not safe because `sparse_mask` or `sqrt` may have coalesced the gradient!\n    p.data.add_(-clr, grad.data.new(grad.indices(), grad.values() / std_values))\nelse:\n    state['sum'].addcmul_(1, grad, grad)\n    std = state['sum'].sqrt().add_(1e-10)\n    p.data.addcdiv_(-clr, grad, std)\n\nP.S. You might want to measure the performance of adagrad with my benchmark script (https://gist.github.com/adamlerer/4159ddb4bd19e87de1b4bb3215a1db6f) ; if it does coalescing under the hood it could be much slower.", "body": "Here are my thoughts:\r\n\r\n1.  torch.sparse.XXXTensor should accept empty index/value tensors, so `make_sparse` should not be necessary.\r\n2. We should add sparse versions of unary pointwise ops, that delegate to `values()`.\r\n3. Performing * or / directly on the values is just an optimization to avoid comparing indices; it would work even if you didn't do that. Maybe we need an unsafe direct divide (assumes that indices are equal), but I don't think so.\r\n4. The `add_` call here directly on the values is a special case; it only makes sense here because we're then going to multiply through by grad so it's okay to only add to the non-zero entries. I think we should leave this to the user to sort out.\r\n4a. I think the `add_` is only here to prevent divide by zero, so I don't think we need it in the sparse case.\r\nHere's how the code would look with these minor changes (1 and 2). Still a bit longer, but mostly just because of the \"unusual\" parts (optimized `add_` and `div`) that should be done by hand.\r\n5. I'm not sure this kernel is safe unless you call `p.grad.coalesce` first (yikes!). Does `sparse_mask` have a contract about whether it will coalesce its indices?\r\n\r\n```\r\nif p.grad.data.is_sparse:\r\n    grad = p.grad\r\n    state['sum'].add_(grad.pow(2))\r\n    std_values = state['sum'].sparse_mask(grad).values().sqrt_()\r\n    # warning! this is not safe because `sparse_mask` or `sqrt` may have coalesced the gradient!\r\n    p.data.add_(-clr, grad.data.new(grad.indices(), grad.values() / std_values))\r\nelse:\r\n    state['sum'].addcmul_(1, grad, grad)\r\n    std = state['sum'].sqrt().add_(1e-10)\r\n    p.data.addcdiv_(-clr, grad, std)\r\n```\r\n\r\nP.S. You might want to measure the performance of adagrad with my benchmark script (https://gist.github.com/adamlerer/4159ddb4bd19e87de1b4bb3215a1db6f) ; if it does coalescing under the hood it could be much slower."}