{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/297730346", "html_url": "https://github.com/pytorch/pytorch/issues/1369#issuecomment-297730346", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1369", "id": 297730346, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NzczMDM0Ng==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-27T14:29:44Z", "updated_at": "2017-04-27T14:29:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2560662\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinraison\">@martinraison</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> Are you suggesting an in-place coalesce? <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a> considered this in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"222830082\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1302\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/1302/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/1302\">#1302</a> but he came down on the side of not doing in-place coalesce. The key situation he was worried about was coalescing related bugs which went away when you called print on a tensor, since print calls indices/values (which would induce a coalesce.) I suppose we could add a \"raw_indices/raw_values\" which don't call coalesce and make sure debugging operators call those instead.</p>\n<p>However, even if indices/values always coalesce, a user can still end up in a situation where they have a pointer to a tensor which is not coalesced. The simplest way is to grab <code>t.values()</code>, and then do an in-place cadd on <code>t</code>. Because <code>t.values()</code> refers to the same underlying tensor as <code>t</code>, when <code>t</code> becomes uncoalesced, the change becomes visible. I suppose we could solve this problem by changing the semantics of values/indices to clone (or deciding we don't care) but I think it's worth thinking about.</p>\n<blockquote>\n<p>Proposal 3: how would you solve the <code>grad_values / std_values</code> problem? If we wanted to do <code>std.lift_sparse2(lambda x, y: x / y)</code> we'd need to \"upgrade\" the values tensors of x and y so that they have the same support.</p>\n</blockquote>\n<p>Yeah, it's the same situation as in Proposal 2. I think we don't actually want to upgrade the values tensors so they have the same support; we just want to check that they have the same support and bug out if they don't, which hopefully is cheaper. Having copy-on-write index arrays would go a long way towards this. Perhaps we should consider adding those?</p>", "body_text": "@martinraison @soumith Are you suggesting an in-place coalesce? @adamlerer considered this in #1302 but he came down on the side of not doing in-place coalesce. The key situation he was worried about was coalescing related bugs which went away when you called print on a tensor, since print calls indices/values (which would induce a coalesce.) I suppose we could add a \"raw_indices/raw_values\" which don't call coalesce and make sure debugging operators call those instead.\nHowever, even if indices/values always coalesce, a user can still end up in a situation where they have a pointer to a tensor which is not coalesced. The simplest way is to grab t.values(), and then do an in-place cadd on t. Because t.values() refers to the same underlying tensor as t, when t becomes uncoalesced, the change becomes visible. I suppose we could solve this problem by changing the semantics of values/indices to clone (or deciding we don't care) but I think it's worth thinking about.\n\nProposal 3: how would you solve the grad_values / std_values problem? If we wanted to do std.lift_sparse2(lambda x, y: x / y) we'd need to \"upgrade\" the values tensors of x and y so that they have the same support.\n\nYeah, it's the same situation as in Proposal 2. I think we don't actually want to upgrade the values tensors so they have the same support; we just want to check that they have the same support and bug out if they don't, which hopefully is cheaper. Having copy-on-write index arrays would go a long way towards this. Perhaps we should consider adding those?", "body": "@martinraison @soumith Are you suggesting an in-place coalesce? @adamlerer considered this in #1302 but he came down on the side of not doing in-place coalesce. The key situation he was worried about was coalescing related bugs which went away when you called print on a tensor, since print calls indices/values (which would induce a coalesce.) I suppose we could add a \"raw_indices/raw_values\" which don't call coalesce and make sure debugging operators call those instead.\r\n\r\nHowever, even if indices/values always coalesce, a user can still end up in a situation where they have a pointer to a tensor which is not coalesced. The simplest way is to grab `t.values()`, and then do an in-place cadd on `t`. Because `t.values()` refers to the same underlying tensor as `t`, when `t` becomes uncoalesced, the change becomes visible. I suppose we could solve this problem by changing the semantics of values/indices to clone (or deciding we don't care) but I think it's worth thinking about.\r\n\r\n> Proposal 3: how would you solve the `grad_values / std_values` problem? If we wanted to do `std.lift_sparse2(lambda x, y: x / y)` we'd need to \"upgrade\" the values tensors of x and y so that they have the same support.\r\n\r\nYeah, it's the same situation as in Proposal 2. I think we don't actually want to upgrade the values tensors so they have the same support; we just want to check that they have the same support and bug out if they don't, which hopefully is cheaper. Having copy-on-write index arrays would go a long way towards this. Perhaps we should consider adding those?"}