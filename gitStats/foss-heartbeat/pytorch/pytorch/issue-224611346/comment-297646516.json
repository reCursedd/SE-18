{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/297646516", "html_url": "https://github.com/pytorch/pytorch/issues/1369#issuecomment-297646516", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1369", "id": 297646516, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NzY0NjUxNg==", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-27T08:20:25Z", "updated_at": "2017-04-27T08:20:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> for opening the discussion.</p>\n<p>Proposal 1: I'm wondering if it would be ok to automatically coalesce sparse tensors when calling <code>indices()</code> or <code>values()</code> in Python. This way the user always has a consistent view of the tensor, and doing <code>s.values().pow_()</code> is not that much of an issue anymore (note: <code>__repr__</code> also coalesces sparse tensors for that reason). As long as those functions are not called, tensors would still remain uncoalesced for efficiency reasons: for example in <code>cadd</code> when aggregating gradients. Oh and you're right that my adagrad code can be simplified, not sure why I did it this way in the end :)</p>\n<p>Proposal 2: you're right that we have to be careful with sparse operations, because many of them don't really make sense (like addition with a scalar), or are ill-defined. This is why I chose to rely on the existing well-defined operations as much as possible (Proposal 1), but if some patterns come up often we can add them to the library.</p>\n<p>Proposal 3: how would you solve the <code>grad_values / std_values</code> problem? If we wanted to do <code>std.lift_sparse2(lambda x, y: x / y)</code> we'd need to \"upgrade\" the values tensors of <code>x</code> and <code>y</code> so that they have the same support.</p>\n<p>It looks like <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"222528997\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1285\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1285/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1285\">#1285</a> can be used as a good reference for our needs in the immediate future.</p>", "body_text": "Thanks @ezyang for opening the discussion.\nProposal 1: I'm wondering if it would be ok to automatically coalesce sparse tensors when calling indices() or values() in Python. This way the user always has a consistent view of the tensor, and doing s.values().pow_() is not that much of an issue anymore (note: __repr__ also coalesces sparse tensors for that reason). As long as those functions are not called, tensors would still remain uncoalesced for efficiency reasons: for example in cadd when aggregating gradients. Oh and you're right that my adagrad code can be simplified, not sure why I did it this way in the end :)\nProposal 2: you're right that we have to be careful with sparse operations, because many of them don't really make sense (like addition with a scalar), or are ill-defined. This is why I chose to rely on the existing well-defined operations as much as possible (Proposal 1), but if some patterns come up often we can add them to the library.\nProposal 3: how would you solve the grad_values / std_values problem? If we wanted to do std.lift_sparse2(lambda x, y: x / y) we'd need to \"upgrade\" the values tensors of x and y so that they have the same support.\nIt looks like #1285 can be used as a good reference for our needs in the immediate future.", "body": "Thanks @ezyang for opening the discussion.\r\n\r\nProposal 1: I'm wondering if it would be ok to automatically coalesce sparse tensors when calling `indices()` or `values()` in Python. This way the user always has a consistent view of the tensor, and doing `s.values().pow_()` is not that much of an issue anymore (note: `__repr__` also coalesces sparse tensors for that reason). As long as those functions are not called, tensors would still remain uncoalesced for efficiency reasons: for example in `cadd` when aggregating gradients. Oh and you're right that my adagrad code can be simplified, not sure why I did it this way in the end :)\r\n\r\nProposal 2: you're right that we have to be careful with sparse operations, because many of them don't really make sense (like addition with a scalar), or are ill-defined. This is why I chose to rely on the existing well-defined operations as much as possible (Proposal 1), but if some patterns come up often we can add them to the library.\r\n\r\nProposal 3: how would you solve the `grad_values / std_values` problem? If we wanted to do `std.lift_sparse2(lambda x, y: x / y)` we'd need to \"upgrade\" the values tensors of `x` and `y` so that they have the same support.\r\n\r\nIt looks like #1285 can be used as a good reference for our needs in the immediate future."}