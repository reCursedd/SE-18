{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/297801564", "html_url": "https://github.com/pytorch/pytorch/issues/1369#issuecomment-297801564", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1369", "id": 297801564, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NzgwMTU2NA==", "user": {"login": "martinraison", "id": 2560662, "node_id": "MDQ6VXNlcjI1NjA2NjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2560662?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinraison", "html_url": "https://github.com/martinraison", "followers_url": "https://api.github.com/users/martinraison/followers", "following_url": "https://api.github.com/users/martinraison/following{/other_user}", "gists_url": "https://api.github.com/users/martinraison/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinraison/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinraison/subscriptions", "organizations_url": "https://api.github.com/users/martinraison/orgs", "repos_url": "https://api.github.com/users/martinraison/repos", "events_url": "https://api.github.com/users/martinraison/events{/privacy}", "received_events_url": "https://api.github.com/users/martinraison/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-27T18:35:27Z", "updated_at": "2017-04-27T18:35:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5702157\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/adamlerer\">@adamlerer</a> the idea of storing <code>nnz</code> on the GPU crossed my mind but it seems like most operations would need to synchronously fetch it anyway. For example if you do a <code>cadd</code>, you'll want to allocate a new values tensor with first dimension <code>nnz1 + nnz2</code>, which only the GPU knows about. So either we can allocate this tensor asynchronously from the device (does that even make sense? I have no idea), either we have to synchronously fetch <code>nnz1</code> and <code>nnz2</code> first before doing anything else.</p>", "body_text": "@adamlerer the idea of storing nnz on the GPU crossed my mind but it seems like most operations would need to synchronously fetch it anyway. For example if you do a cadd, you'll want to allocate a new values tensor with first dimension nnz1 + nnz2, which only the GPU knows about. So either we can allocate this tensor asynchronously from the device (does that even make sense? I have no idea), either we have to synchronously fetch nnz1 and nnz2 first before doing anything else.", "body": "@adamlerer the idea of storing `nnz` on the GPU crossed my mind but it seems like most operations would need to synchronously fetch it anyway. For example if you do a `cadd`, you'll want to allocate a new values tensor with first dimension `nnz1 + nnz2`, which only the GPU knows about. So either we can allocate this tensor asynchronously from the device (does that even make sense? I have no idea), either we have to synchronously fetch `nnz1` and `nnz2` first before doing anything else."}