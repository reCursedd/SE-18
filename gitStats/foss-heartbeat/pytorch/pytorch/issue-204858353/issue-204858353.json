{"url": "https://api.github.com/repos/pytorch/pytorch/issues/679", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/679/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/679/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/679/events", "html_url": "https://github.com/pytorch/pytorch/issues/679", "id": 204858353, "node_id": "MDU6SXNzdWUyMDQ4NTgzNTM=", "number": 679, "title": "Allow optimizers to skip nn.Parameters that have requires_grad=False", "user": {"login": "alykhantejani", "id": 687194, "node_id": "MDQ6VXNlcjY4NzE5NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/687194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alykhantejani", "html_url": "https://github.com/alykhantejani", "followers_url": "https://api.github.com/users/alykhantejani/followers", "following_url": "https://api.github.com/users/alykhantejani/following{/other_user}", "gists_url": "https://api.github.com/users/alykhantejani/gists{/gist_id}", "starred_url": "https://api.github.com/users/alykhantejani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alykhantejani/subscriptions", "organizations_url": "https://api.github.com/users/alykhantejani/orgs", "repos_url": "https://api.github.com/users/alykhantejani/repos", "events_url": "https://api.github.com/users/alykhantejani/events{/privacy}", "received_events_url": "https://api.github.com/users/alykhantejani/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-02-02T12:22:06Z", "updated_at": "2018-11-05T23:02:52Z", "closed_at": "2017-02-03T15:12:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am trying to implement a gaussian blur as a convolution layer in a network, where the weights do not change. I currently have this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">GaussianBlur</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">kernelSize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-smi\">sigma</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n        <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">=</span> nn.Parameter(<span class=\"pl-c1\">self</span>._calculate_weights(kernelSize, sigma),\n                                   <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-k\">return</span> F.conv2d(x, <span class=\"pl-c1\">self</span>.weight)</pre></div>\n<p>This then gives the error <code>optimizing a parameter that doesn't require gradients</code> from the optimizer.</p>\n<p>It would be a nice feature to be able to exclude some parameters from the optimizer.</p>\n<p>I also tried changing <code>self.weight</code> to a Variable, but then when <code>.cuda()</code> is called, the weights are not transferred, resulting in other errors</p>", "body_text": "I am trying to implement a gaussian blur as a convolution layer in a network, where the weights do not change. I currently have this:\nclass GaussianBlur(nn.Module):\n    def __init__(self, kernelSize=5, sigma=1):\n        self.weight = nn.Parameter(self._calculate_weights(kernelSize, sigma),\n                                   requires_grad=False)\n\n    def forward(self, x):\n        return F.conv2d(x, self.weight)\nThis then gives the error optimizing a parameter that doesn't require gradients from the optimizer.\nIt would be a nice feature to be able to exclude some parameters from the optimizer.\nI also tried changing self.weight to a Variable, but then when .cuda() is called, the weights are not transferred, resulting in other errors", "body": "I am trying to implement a gaussian blur as a convolution layer in a network, where the weights do not change. I currently have this:\r\n\r\n```Python\r\nclass GaussianBlur(nn.Module):\r\n    def __init__(self, kernelSize=5, sigma=1):\r\n        self.weight = nn.Parameter(self._calculate_weights(kernelSize, sigma),\r\n                                   requires_grad=False)\r\n\r\n    def forward(self, x):\r\n        return F.conv2d(x, self.weight)\r\n```\r\n\r\nThis then gives the error `optimizing a parameter that doesn't require gradients` from the optimizer. \r\n\r\nIt would be a nice feature to be able to exclude some parameters from the optimizer. \r\n\r\nI also tried changing `self.weight` to a Variable, but then when `.cuda()` is called, the weights are not transferred, resulting in other errors\r\n\r\n"}