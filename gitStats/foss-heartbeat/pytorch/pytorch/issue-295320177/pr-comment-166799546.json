{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166799546", "pull_request_review_id": 94920222, "id": 166799546, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2Njc5OTU0Ng==", "diff_hunk": "@@ -29,8 +29,8 @@ class Pareto(TransformedDistribution):\n \n     def __init__(self, scale, alpha):\n         self.scale, self.alpha = broadcast_all(scale, alpha)\n-        base_dist = Exponential(alpha)\n-        transforms = [ExpTransform(), AffineTransform(loc=0, scale=scale)]\n+        base_dist = Exponential(self.alpha)\n+        transforms = [ExpTransform(), AffineTransform(loc=0, scale=self.scale)]", "path": "torch/distributions/pareto.py", "position": 7, "original_position": 7, "commit_id": "74f106f254028148717d552f2a794d4e7c483909", "original_commit_id": "84673f50dd310ef4593729c81a47de1c9a6ec71d", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "body": "@alicanb pointed out that if `scale` and `alpha` are both scalars, then the 0 here will broadcast everything up to dimension-1. Does it seem reasonable to use `loc=variable(0.0)` here instead, at least until Python numbers become 0-dimensional?", "created_at": "2018-02-08T00:21:07Z", "updated_at": "2018-11-23T15:39:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/5121#discussion_r166799546", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5121", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166799546"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5121#discussion_r166799546"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5121"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1093846\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alicanb\">@alicanb</a> pointed out that if <code>scale</code> and <code>alpha</code> are both scalars, then the 0 here will broadcast everything up to dimension-1. Does it seem reasonable to use <code>loc=variable(0.0)</code> here instead, at least until Python numbers become 0-dimensional?</p>", "body_text": "@alicanb pointed out that if scale and alpha are both scalars, then the 0 here will broadcast everything up to dimension-1. Does it seem reasonable to use loc=variable(0.0) here instead, at least until Python numbers become 0-dimensional?"}