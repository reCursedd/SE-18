{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207115554", "pull_request_review_id": 142660689, "id": 207115554, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNzExNTU1NA==", "diff_hunk": "@@ -53,54 +53,57 @@ class IDEEPFallbackOp final : public IDEEPOperator {\n     // then forward output blobs to local workspace.\n     std::unordered_map<string, string> forwarded_output_blobs;\n     for (int i = 0; i < base_def_.output_size(); i++) {\n+      // For in-place case, the in/output tensor for local_ws must be\n+      // re-created, instead of forwarding from current workspace.\n       string parent_name(base_def_.output(i));\n-      if (!SkipOutputCopy::Contains(i)) {\n-        parent_name += \"_cpu_output_blob_\" + base_def_.type();\n-      }\n+      parent_name += \"_cpu_output_blob_\" + base_def_.type();\n       local_output_blobs_.push_back(ws->CreateBlob(parent_name));\n       CHECK_NOTNULL(local_output_blobs_.back());\n       forwarded_output_blobs[base_def_.output(i)] = parent_name;\n+      output_inplace_.push_back(false);\n+      for (const string &input_name : base_def_.input()) {\n+        if (input_name == base_def_.output(i)) {\n+          output_inplace_[i] = true;\n+          break;\n+        }\n+      }\n     }\n     local_ws_.reset(new Workspace(ws, forwarded_output_blobs));\n     // Set up the symbols for the local workspace.\n     for (const string& name : base_def_.input()) {\n       local_input_blobs_.push_back(local_ws_->CreateBlob(name));\n       CHECK_NOTNULL(local_input_blobs_.back());\n     }\n+    input_share_.resize(local_input_blobs_.size(), false);\n     base_op_.reset(new CPUOp(base_def_, local_ws_.get()));\n   }\n \n   bool RunOnDevice() override {\n     for (int i = 0; i < InputSize(); ++i) {\n-      if (InputIsType<itensor>(i) && Input(i).get_data_type() == itensor::data_type::f32) {\n+      if (InputIsType<itensor>(i) &&\n+          Input(i).get_data_type() == itensor::data_type::f32) {\n         auto& input = Input(i);\n-        auto dtensor = local_input_blobs_[i]->GetMutableTensor(CPU);\n-        dtensor->Resize(input.get_dims());\n-        if (input.is_public_format()) {\n-          dtensor->ShareExternalPointer(static_cast<float*>(input.get_data_handle()));\n-        } else {\n-          input.reorder_to(dtensor->template mutable_data<float>());\n+        if (input_share_[i]) {\n+          local_input_blobs_[i]->Reset();\n         }\n-      } else if (\n-          InputIsType<itensor>(i) &&\n-          Input(i).get_data_type() == itensor::data_type::s32) {\n-        auto& input = Input(i);", "path": "caffe2/ideep/operators/operator_fallback_ideep.h", "position": 49, "original_position": 50, "commit_id": "406f1bafc343417b64ffbe7fb5ea93570e645b06", "original_commit_id": "df74febfbf2a898d6df8ea8a605cfac7a18f7788", "user": {"login": "jgong5", "id": 8359223, "node_id": "MDQ6VXNlcjgzNTkyMjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/8359223?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgong5", "html_url": "https://github.com/jgong5", "followers_url": "https://api.github.com/users/jgong5/followers", "following_url": "https://api.github.com/users/jgong5/following{/other_user}", "gists_url": "https://api.github.com/users/jgong5/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgong5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgong5/subscriptions", "organizations_url": "https://api.github.com/users/jgong5/orgs", "repos_url": "https://api.github.com/users/jgong5/repos", "events_url": "https://api.github.com/users/jgong5/events{/privacy}", "received_events_url": "https://api.github.com/users/jgong5/received_events", "type": "User", "site_admin": false}, "body": "Special handling of non-float tensors is not needed since all non-float tensors are held with CPU device now.", "created_at": "2018-08-02T06:41:49Z", "updated_at": "2018-11-23T15:48:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/10157#discussion_r207115554", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10157", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/207115554"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10157#discussion_r207115554"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10157"}}, "body_html": "<p>Special handling of non-float tensors is not needed since all non-float tensors are held with CPU device now.</p>", "body_text": "Special handling of non-float tensors is not needed since all non-float tensors are held with CPU device now."}