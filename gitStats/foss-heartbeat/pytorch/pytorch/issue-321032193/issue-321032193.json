{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7365", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7365/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7365/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7365/events", "html_url": "https://github.com/pytorch/pytorch/issues/7365", "id": 321032193, "node_id": "MDU6SXNzdWUzMjEwMzIxOTM=", "number": 7365, "title": "nn.DataParallel fills None grads with 0", "user": {"login": "karandwivedi42", "id": 9624554, "node_id": "MDQ6VXNlcjk2MjQ1NTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/9624554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karandwivedi42", "html_url": "https://github.com/karandwivedi42", "followers_url": "https://api.github.com/users/karandwivedi42/followers", "following_url": "https://api.github.com/users/karandwivedi42/following{/other_user}", "gists_url": "https://api.github.com/users/karandwivedi42/gists{/gist_id}", "starred_url": "https://api.github.com/users/karandwivedi42/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karandwivedi42/subscriptions", "organizations_url": "https://api.github.com/users/karandwivedi42/orgs", "repos_url": "https://api.github.com/users/karandwivedi42/repos", "events_url": "https://api.github.com/users/karandwivedi42/events{/privacy}", "received_events_url": "https://api.github.com/users/karandwivedi42/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-05-08T03:42:16Z", "updated_at": "2018-08-18T00:52:29Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>On pytorch 0.4.0, using nn.DataParallel leads to grads of parameters not used being filled with 0. This is a problem when the model has many parameters but only small part of it is used in forward.</p>\n<p>Test:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Test</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Test, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.l1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>)\n        <span class=\"pl-c1\">self</span>.l2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.l1(<span class=\"pl-c1\">input</span>)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>CPU:<span class=\"pl-pds\">'</span></span>)\nnet <span class=\"pl-k\">=</span> Test()\ninputs <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">10</span>)\noutputs <span class=\"pl-k\">=</span> net(inputs)\nloss <span class=\"pl-k\">=</span> torch.norm(outputs)\nloss.backward()\n<span class=\"pl-k\">for</span> j,p <span class=\"pl-k\">in</span> net.named_parameters():\n    <span class=\"pl-k\">if</span> p.grad <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-c1\">print</span>(j, p.shape, torch.norm(p.grad))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c1\">print</span>(j)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>GPU:<span class=\"pl-pds\">'</span></span>)\nnet <span class=\"pl-k\">=</span> Test().to(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>)\ninputs <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">10</span>).to(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>)\noutputs <span class=\"pl-k\">=</span> net(inputs)\nloss <span class=\"pl-k\">=</span> torch.norm(outputs)\nloss.backward()\n<span class=\"pl-k\">for</span> j,p <span class=\"pl-k\">in</span> net.named_parameters():\n    <span class=\"pl-k\">if</span> p.grad <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-c1\">print</span>(j, p.shape, torch.norm(p.grad))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c1\">print</span>(j)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>MultiGPU:<span class=\"pl-pds\">'</span></span>)\nnet <span class=\"pl-k\">=</span> nn.DataParallel(Test().to(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>))\ninputs <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">10</span>).to(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda<span class=\"pl-pds\">'</span></span>)\noutputs <span class=\"pl-k\">=</span> net(inputs)\nloss <span class=\"pl-k\">=</span> torch.norm(outputs)\nloss.backward()\n<span class=\"pl-k\">for</span> j,p <span class=\"pl-k\">in</span> net.module.named_parameters():\n    <span class=\"pl-k\">if</span> p.grad <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-c1\">print</span>(j, p.shape, torch.norm(p.grad))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c1\">print</span>(j)</pre></div>\n<p>For multi GPU we see all grads are filled with zeros. For CPU and GPU the grads are None.</p>", "body_text": "On pytorch 0.4.0, using nn.DataParallel leads to grads of parameters not used being filled with 0. This is a problem when the model has many parameters but only small part of it is used in forward.\nTest:\nimport torch\nfrom torch import nn\nclass Test(nn.Module):\n    def __init__(self):\n        super(Test, self).__init__()\n        self.l1 = nn.Linear(10,10)\n        self.l2 = nn.Linear(10,10)\n\n    def forward(self, input):\n        return self.l1(input)\n\nprint('CPU:')\nnet = Test()\ninputs = torch.randn(1,10)\noutputs = net(inputs)\nloss = torch.norm(outputs)\nloss.backward()\nfor j,p in net.named_parameters():\n    if p.grad is not None:\n        print(j, p.shape, torch.norm(p.grad))\n    else:\n        print(j)\nprint('GPU:')\nnet = Test().to('cuda')\ninputs = torch.randn(1,10).to('cuda')\noutputs = net(inputs)\nloss = torch.norm(outputs)\nloss.backward()\nfor j,p in net.named_parameters():\n    if p.grad is not None:\n        print(j, p.shape, torch.norm(p.grad))\n    else:\n        print(j)\nprint('MultiGPU:')\nnet = nn.DataParallel(Test().to('cuda'))\ninputs = torch.randn(1,10).to('cuda')\noutputs = net(inputs)\nloss = torch.norm(outputs)\nloss.backward()\nfor j,p in net.module.named_parameters():\n    if p.grad is not None:\n        print(j, p.shape, torch.norm(p.grad))\n    else:\n        print(j)\nFor multi GPU we see all grads are filled with zeros. For CPU and GPU the grads are None.", "body": "On pytorch 0.4.0, using nn.DataParallel leads to grads of parameters not used being filled with 0. This is a problem when the model has many parameters but only small part of it is used in forward.\r\n\r\nTest:\r\n```python\r\nimport torch\r\nfrom torch import nn\r\nclass Test(nn.Module):\r\n    def __init__(self):\r\n        super(Test, self).__init__()\r\n        self.l1 = nn.Linear(10,10)\r\n        self.l2 = nn.Linear(10,10)\r\n\r\n    def forward(self, input):\r\n        return self.l1(input)\r\n\r\nprint('CPU:')\r\nnet = Test()\r\ninputs = torch.randn(1,10)\r\noutputs = net(inputs)\r\nloss = torch.norm(outputs)\r\nloss.backward()\r\nfor j,p in net.named_parameters():\r\n    if p.grad is not None:\r\n        print(j, p.shape, torch.norm(p.grad))\r\n    else:\r\n        print(j)\r\nprint('GPU:')\r\nnet = Test().to('cuda')\r\ninputs = torch.randn(1,10).to('cuda')\r\noutputs = net(inputs)\r\nloss = torch.norm(outputs)\r\nloss.backward()\r\nfor j,p in net.named_parameters():\r\n    if p.grad is not None:\r\n        print(j, p.shape, torch.norm(p.grad))\r\n    else:\r\n        print(j)\r\nprint('MultiGPU:')\r\nnet = nn.DataParallel(Test().to('cuda'))\r\ninputs = torch.randn(1,10).to('cuda')\r\noutputs = net(inputs)\r\nloss = torch.norm(outputs)\r\nloss.backward()\r\nfor j,p in net.module.named_parameters():\r\n    if p.grad is not None:\r\n        print(j, p.shape, torch.norm(p.grad))\r\n    else:\r\n        print(j)\r\n```\r\nFor multi GPU we see all grads are filled with zeros. For CPU and GPU the grads are None."}