{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/414012634", "html_url": "https://github.com/pytorch/pytorch/issues/7365#issuecomment-414012634", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7365", "id": 414012634, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNDAxMjYzNA==", "user": {"login": "ZhengRui", "id": 4105014, "node_id": "MDQ6VXNlcjQxMDUwMTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4105014?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhengRui", "html_url": "https://github.com/ZhengRui", "followers_url": "https://api.github.com/users/ZhengRui/followers", "following_url": "https://api.github.com/users/ZhengRui/following{/other_user}", "gists_url": "https://api.github.com/users/ZhengRui/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhengRui/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhengRui/subscriptions", "organizations_url": "https://api.github.com/users/ZhengRui/orgs", "repos_url": "https://api.github.com/users/ZhengRui/repos", "events_url": "https://api.github.com/users/ZhengRui/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhengRui/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-17T23:34:37Z", "updated_at": "2018-08-18T00:52:29Z", "author_association": "NONE", "body_html": "<p>I am having the same issue, my use case is when only fine-tuning a small head attached to a much bigger backbone network</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">with</span> torch.no_grad():\n        o <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.backbone(x)\n\n    o <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.head(o)\n    <span class=\"pl-k\">return</span> o</pre></div>\n<p>in single gpu case, after backward, backbone parameter's gradients are None, while in multi gpu cases, their gradients are all 0s, I am not sure if it does backward operations on the backbone or not, even if not  it wastes some gpu memory, so I would like it to behave as in single gpu case.</p>\n<p>Update: It can support a way bigger train batch size so I think the backward does not go through the base model, which is good. It only wastes some gpu memory for the 0s gradients.</p>", "body_text": "I am having the same issue, my use case is when only fine-tuning a small head attached to a much bigger backbone network\ndef forward(self, x):\n    with torch.no_grad():\n        o = self.backbone(x)\n\n    o = self.head(o)\n    return o\nin single gpu case, after backward, backbone parameter's gradients are None, while in multi gpu cases, their gradients are all 0s, I am not sure if it does backward operations on the backbone or not, even if not  it wastes some gpu memory, so I would like it to behave as in single gpu case.\nUpdate: It can support a way bigger train batch size so I think the backward does not go through the base model, which is good. It only wastes some gpu memory for the 0s gradients.", "body": "I am having the same issue, my use case is when only fine-tuning a small head attached to a much bigger backbone network \r\n\r\n```python\r\ndef forward(self, x):\r\n    with torch.no_grad():\r\n        o = self.backbone(x)\r\n\r\n    o = self.head(o)\r\n    return o\r\n```\r\n\r\nin single gpu case, after backward, backbone parameter's gradients are None, while in multi gpu cases, their gradients are all 0s, I am not sure if it does backward operations on the backbone or not, even if not  it wastes some gpu memory, so I would like it to behave as in single gpu case.\r\n\r\nUpdate: It can support a way bigger train batch size so I think the backward does not go through the base model, which is good. It only wastes some gpu memory for the 0s gradients."}