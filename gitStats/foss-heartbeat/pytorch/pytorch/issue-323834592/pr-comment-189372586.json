{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189372586", "pull_request_review_id": 121546111, "id": 189372586, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTM3MjU4Ng==", "diff_hunk": "@@ -65,132 +65,128 @@ def set_printoptions(\n         PRINT_OPTS.linewidth = linewidth\n \n \n-def _get_min_log_scale():\n-    min_positive = float_info.min * float_info.epsilon  # get smallest denormal\n-    if min_positive == 0:  # use smallest normal if DAZ/FTZ is set\n-        min_positive = float_info.min\n-    return math.ceil(math.log(min_positive, 10))\n+class _Formatter(object):\n+    def __init__(self, tensor):\n+        tensor = tensor.view(tensor.nelement())\n+        self.floating_dtype = tensor.dtype.is_floating_point\n+        self.int_mode = True\n+        self.sci_mode = False\n+        self.max_width = 1\n+\n+        if self.floating_dtype:\n+            for value in tensor.tolist():\n+                value_str = '{}'.format(value)\n+                self.max_width = max(self.max_width, len(value_str))\n \n+        else:\n+            try:\n+                for value in tensor.tolist():\n+                    if value != math.ceil(value):\n+                        self.int_mode = False\n+                        break\n+            except OverflowError:\n+                self.int_mode = False\n+\n+            if self.int_mode:\n+                for value in tensor.tolist():\n+                    value_str = '{:.0f}'.format(value)\n+                    if math.isnan(value) or math.isinf(value):\n+                        self.max_width = max(self.max_width, len(value_str))\n+                    else:\n+                        self.max_width = max(self.max_width, len(value_str) + 1)\n \n-def _get_format_fn(format, nonfinite_format):\n-    return lambda x: format.format(x) if math.isinf(x) or math.isnan(x) else nonfinite_format.format(x)\n-\n+            else:\n+                copy = torch.DoubleTensor(\n+                    tensor.size()).copy_(tensor).abs_().view(tensor.nelement())\n+                pos_inf_mask = copy.eq(float('inf'))\n+                neg_inf_mask = copy.eq(float('-inf'))\n+                nan_mask = copy.ne(copy)\n+                invalid_value_mask = pos_inf_mask + neg_inf_mask + nan_mask\n+                if invalid_value_mask.all():\n+                    example_value = 0\n+                else:\n+                    example_value = copy[invalid_value_mask.eq(0)][0]\n+                copy[invalid_value_mask] = example_value\n \n-def _number_format(tensor, min_sz=-1):\n-    floating_dtype = tensor.dtype.is_floating_point  # save this because we cast later\n-    _min_log_scale = _get_min_log_scale()\n-    min_sz = max(min_sz, 2)\n-    tensor = torch.DoubleTensor(tensor.size()).copy_(tensor).abs_().view(tensor.nelement())\n+                exp_min = copy.min()\n+                if exp_min != 0:\n+                    exp_min = math.floor(math.log10(exp_min)) + 1\n+                else:\n+                    exp_min = 1\n+                exp_max = copy.max()\n+                if exp_max != 0:\n+                    exp_max = math.floor(math.log10(exp_max)) + 1\n+                else:\n+                    exp_max = 1\n \n-    pos_inf_mask = tensor.eq(float('inf'))\n-    neg_inf_mask = tensor.eq(float('-inf'))\n-    nan_mask = tensor.ne(tensor)\n-    invalid_value_mask = pos_inf_mask + neg_inf_mask + nan_mask\n-    if invalid_value_mask.all():\n-        example_value = 0\n-    else:\n-        example_value = tensor[invalid_value_mask.eq(0)][0]\n-    tensor[invalid_value_mask] = example_value\n-    if invalid_value_mask.any():\n-        min_sz = max(min_sz, 3)\n-\n-    int_mode = True\n-    # TODO: use fmod?\n-    for value in tensor.tolist():\n-        if value != math.ceil(value):\n-            int_mode = False\n-            break\n-\n-    exp_min = tensor.min()\n-    if exp_min != 0:\n-        exp_min = math.floor(math.log10(exp_min)) + 1\n-    else:\n-        exp_min = 1\n-    exp_max = tensor.max()\n-    if exp_max != 0:\n-        exp_max = math.floor(math.log10(exp_max)) + 1\n-    else:\n-        exp_max = 1\n-    include_decimal_int_mode = floating_dtype and int_mode\n-\n-    scale = 1\n-    exp_max = int(exp_max)\n-    prec = PRINT_OPTS.precision\n-    if int_mode:\n-        if exp_max > prec + 1:\n-            format = '{{:11.{}e}}'.format(prec)\n-            fmt_fn = format.format\n-            sz = max(min_sz, 7 + prec)\n-        else:\n-            sz = max(min_sz, exp_max + 1 + include_decimal_int_mode)\n-            format = '{:' + str(sz) + '.0f}'\n-            fmt_fn = format.format\n-            if include_decimal_int_mode:\n-                format = '{:' + str(sz - 1) + '.0f}'\n-                nonfinite_format = format + '.'\n-                fmt_fn = _get_format_fn(format, nonfinite_format)\n-    else:\n-        if exp_max - exp_min > prec:\n-            sz = 7 + prec\n-            if abs(exp_max) > 99 or abs(exp_min) > 99:\n-                sz = sz + 1\n-            sz = max(min_sz, sz)\n-            format = '{{:{}.{}e}}'.format(sz, prec)\n-            fmt_fn = format.format\n-        else:\n-            if exp_max > prec + 1 or exp_max < 0:\n-                sz = max(min_sz, 7)\n-                scale = math.pow(10, max(exp_max - 1, _min_log_scale))\n-            else:\n-                if exp_max == 0:\n-                    sz = 7\n+                if exp_max - exp_min > PRINT_OPTS.precision or exp_max > 8 or exp_min < -4:\n+                    value_str = ('{:' + str(self.max_width) +\n+                                 '.' + str(PRINT_OPTS.precision) + 'e}').format(value)\n+                    self.max_width = max(self.max_width, len(value_str))\n+                    self.sci_mode = True\n                 else:\n-                    sz = exp_max + 6\n-                sz = max(min_sz, sz)\n-            format = '{{:{}.{}f}}'.format(sz, prec)\n-            fmt_fn = format.format\n-    return fmt_fn, scale, sz\n+                    for value in tensor.tolist():\n+                        value_str = ('{:.' +\n+                                     str(PRINT_OPTS.precision) +\n+                                     'f}').format(value)\n+                        self.max_width = max(self.max_width, len(value_str))\n+\n+    def width(self):\n+        return self.max_width\n+\n+    def format(self, value):\n+        if self.floating_dtype:\n+            if self.int_mode:\n+                ret = '{:.0f}'.format(value)\n+                if not (math.isinf(value) or math.isnan(value)):\n+                    ret += '.'\n+            elif self.sci_mode:\n+                ret = ('{:' + str(self.max_width) +\n+                       '.' + str(PRINT_OPTS.precision) + 'e}').format(value)\n+            else:\n+                ret = ('{:.' + str(PRINT_OPTS.precision) + 'f}').format(value)\n+        else:\n+            ret = '{}'.format(value)\n+        return (self.max_width - len(ret)) * ' ' + ret\n \n \n-def _scalar_str(self, fmt, scale):\n-    scalar_str = fmt(self.item() / scale)\n-    # The leading space for positives is ugly on scalars, so we strip it\n-    return scalar_str.lstrip()\n+def _scalar_str(self, formatter):\n+    return formatter.format(self.item())\n \n \n-def _vector_str(self, indent, fmt, scale, sz, summarize):\n-    element_length = sz + 3\n+def _vector_str(self, indent, formatter, summarize):\n+    element_length = formatter.width() + 2", "path": "torch/_tensor_str.py", "position": null, "original_position": 180, "commit_id": "10f7365bedb1b558eb54600e42c9300eda3c28f3", "original_commit_id": "106c33422ad109fcc5743bc11da68ac435f840ab", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "can you comment what the magic number here is supposed to be for (I realize it was there before, but I don't remember what is was for before and I don't know what it's for now).", "created_at": "2018-05-18T19:33:16Z", "updated_at": "2018-11-23T15:44:17Z", "html_url": "https://github.com/pytorch/pytorch/pull/7632#discussion_r189372586", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7632", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189372586"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7632#discussion_r189372586"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7632"}}, "body_html": "<p>can you comment what the magic number here is supposed to be for (I realize it was there before, but I don't remember what is was for before and I don't know what it's for now).</p>", "body_text": "can you comment what the magic number here is supposed to be for (I realize it was there before, but I don't remember what is was for before and I don't know what it's for now)."}