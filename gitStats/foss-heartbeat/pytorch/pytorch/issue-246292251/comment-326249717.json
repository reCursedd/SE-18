{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/326249717", "html_url": "https://github.com/pytorch/pytorch/issues/2228#issuecomment-326249717", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2228", "id": 326249717, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjI0OTcxNw==", "user": {"login": "petered", "id": 1148799, "node_id": "MDQ6VXNlcjExNDg3OTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1148799?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petered", "html_url": "https://github.com/petered", "followers_url": "https://api.github.com/users/petered/followers", "following_url": "https://api.github.com/users/petered/following{/other_user}", "gists_url": "https://api.github.com/users/petered/gists{/gist_id}", "starred_url": "https://api.github.com/users/petered/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petered/subscriptions", "organizations_url": "https://api.github.com/users/petered/orgs", "repos_url": "https://api.github.com/users/petered/repos", "events_url": "https://api.github.com/users/petered/events{/privacy}", "received_events_url": "https://api.github.com/users/petered/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-31T10:00:34Z", "updated_at": "2017-08-31T10:00:55Z", "author_association": "NONE", "body_html": "<p>So I'm a bit new to PyTorch and haven't fully wrapped my head around why Variable and Tensor are two different things (from the user's perspective anyway), though I see why the concepts should be separated in code.</p>\n<blockquote>\n<p>do you see yourself using it just for Tensor, or also for Variable?</p>\n</blockquote>\n<p>I would want this to work with autograd, if that's what you mean, so Variable.</p>\n<p>More concretely, it would be great to work towards a <code>torch.np</code> package, where:</p>\n<ul>\n<li>The Tensor API matches numpy's exactly (eg. <code>.shape</code> instead of <code>.size()</code> etc).  There is only one kind of Tensor (instead of <code>torch.FloatTensor</code>, <code>torch.cuda.FloatTensor</code>, etc.), which has a property dtype, and a flag indicating which device it should live on.  eg <code>arr = torch.np.array([1,2,3], dtype='float32', device='gpu0', requires_grad=False)</code></li>\n<li>Users do not need to think about or distinguish between Variables and Tensors.  Everything is a Variable.  If desired, autograd could be disabled with a context manager that sets a global flag <code>with enable_autograd(False): ...</code>, which would effectively make functions return Tensors, not Variables.</li>\n<li>Every function in numpy is implemented.  Every function can accept a list/tuple/np.ndarray/Tensor/Variable (if Tensor and Variable are still distinct).  Internally, inputs are promoted to Variables before use.</li>\n<li>Calls to var.expand are made automatically (for array broadcasting)</li>\n</ul>", "body_text": "So I'm a bit new to PyTorch and haven't fully wrapped my head around why Variable and Tensor are two different things (from the user's perspective anyway), though I see why the concepts should be separated in code.\n\ndo you see yourself using it just for Tensor, or also for Variable?\n\nI would want this to work with autograd, if that's what you mean, so Variable.\nMore concretely, it would be great to work towards a torch.np package, where:\n\nThe Tensor API matches numpy's exactly (eg. .shape instead of .size() etc).  There is only one kind of Tensor (instead of torch.FloatTensor, torch.cuda.FloatTensor, etc.), which has a property dtype, and a flag indicating which device it should live on.  eg arr = torch.np.array([1,2,3], dtype='float32', device='gpu0', requires_grad=False)\nUsers do not need to think about or distinguish between Variables and Tensors.  Everything is a Variable.  If desired, autograd could be disabled with a context manager that sets a global flag with enable_autograd(False): ..., which would effectively make functions return Tensors, not Variables.\nEvery function in numpy is implemented.  Every function can accept a list/tuple/np.ndarray/Tensor/Variable (if Tensor and Variable are still distinct).  Internally, inputs are promoted to Variables before use.\nCalls to var.expand are made automatically (for array broadcasting)", "body": "So I'm a bit new to PyTorch and haven't fully wrapped my head around why Variable and Tensor are two different things (from the user's perspective anyway), though I see why the concepts should be separated in code. \r\n\r\n> do you see yourself using it just for Tensor, or also for Variable?\r\n\r\nI would want this to work with autograd, if that's what you mean, so Variable.\r\n\r\nMore concretely, it would be great to work towards a `torch.np` package, where:\r\n\r\n- The Tensor API matches numpy's exactly (eg. `.shape` instead of `.size()` etc).  There is only one kind of Tensor (instead of `torch.FloatTensor`, `torch.cuda.FloatTensor`, etc.), which has a property dtype, and a flag indicating which device it should live on.  eg `arr = torch.np.array([1,2,3], dtype='float32', device='gpu0', requires_grad=False)`\r\n- Users do not need to think about or distinguish between Variables and Tensors.  Everything is a Variable.  If desired, autograd could be disabled with a context manager that sets a global flag `with enable_autograd(False): ...`, which would effectively make functions return Tensors, not Variables.  \r\n- Every function in numpy is implemented.  Every function can accept a list/tuple/np.ndarray/Tensor/Variable (if Tensor and Variable are still distinct).  Internally, inputs are promoted to Variables before use.  \r\n- Calls to var.expand are made automatically (for array broadcasting)\r\n\r\n\r\n"}