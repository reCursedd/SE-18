{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/298096572", "html_url": "https://github.com/pytorch/pytorch/pull/1348#issuecomment-298096572", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1348", "id": 298096572, "node_id": "MDEyOklzc3VlQ29tbWVudDI5ODA5NjU3Mg==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-28T20:17:29Z", "updated_at": "2017-04-28T20:24:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm sorry I'm late to the party, but isn't VolumetricUpsamplingNearest the same as VolumetricAveragePooling inverse (possibly with a multiplication coefficient, cudnn has that as a parameter). I mean, in forward upsampling you are putting a same input value into a window of an output tensor, which is what updateGradInput of average pooling does, and in updateGradInput of upsampling you are calculating an average (or a sum) of your gradOutput over some window and put it into gradInput, which is what average pooling forward does, so you could just reuse average pooling kernels probably?<br>\nAnd from your description subsampling sounds like average pooling + per-channel learnable scale and shift, which also looks like it does not require special kernels - average pooling is there already, and scale and shift can be implemented as autograd tensor operations.</p>", "body_text": "I'm sorry I'm late to the party, but isn't VolumetricUpsamplingNearest the same as VolumetricAveragePooling inverse (possibly with a multiplication coefficient, cudnn has that as a parameter). I mean, in forward upsampling you are putting a same input value into a window of an output tensor, which is what updateGradInput of average pooling does, and in updateGradInput of upsampling you are calculating an average (or a sum) of your gradOutput over some window and put it into gradInput, which is what average pooling forward does, so you could just reuse average pooling kernels probably?\nAnd from your description subsampling sounds like average pooling + per-channel learnable scale and shift, which also looks like it does not require special kernels - average pooling is there already, and scale and shift can be implemented as autograd tensor operations.", "body": "I'm sorry I'm late to the party, but isn't VolumetricUpsamplingNearest the same as VolumetricAveragePooling inverse (possibly with a multiplication coefficient, cudnn has that as a parameter). I mean, in forward upsampling you are putting a same input value into a window of an output tensor, which is what updateGradInput of average pooling does, and in updateGradInput of upsampling you are calculating an average (or a sum) of your gradOutput over some window and put it into gradInput, which is what average pooling forward does, so you could just reuse average pooling kernels probably?\r\nAnd from your description subsampling sounds like average pooling + per-channel learnable scale and shift, which also looks like it does not require special kernels - average pooling is there already, and scale and shift can be implemented as autograd tensor operations. "}