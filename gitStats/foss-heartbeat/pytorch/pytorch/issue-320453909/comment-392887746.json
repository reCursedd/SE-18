{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/392887746", "html_url": "https://github.com/pytorch/pytorch/issues/7313#issuecomment-392887746", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7313", "id": 392887746, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Mjg4Nzc0Ng==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-29T18:32:09Z", "updated_at": "2018-05-29T18:32:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So to give an update from my side: I implemented inplace hooks in Variables and am updating CalculatedParameter to use that. This would enable the idiom</p>\n<pre><code>with torch.no_grad():\n    p.add_(-group['lr'], d_p)\n</code></pre>\n<p>(in SGD) to correctly invalidate the CalculatedParameter.<br>\nIt would then be a bug to use <code>.data</code> or <code>.detach()</code> in such cases.</p>\n<p>From my POV this removes the last major obstacle to using CalculatedParameter to cleanly implement cached calculated parameters.</p>", "body_text": "So to give an update from my side: I implemented inplace hooks in Variables and am updating CalculatedParameter to use that. This would enable the idiom\nwith torch.no_grad():\n    p.add_(-group['lr'], d_p)\n\n(in SGD) to correctly invalidate the CalculatedParameter.\nIt would then be a bug to use .data or .detach() in such cases.\nFrom my POV this removes the last major obstacle to using CalculatedParameter to cleanly implement cached calculated parameters.", "body": " So to give an update from my side: I implemented inplace hooks in Variables and am updating CalculatedParameter to use that. This would enable the idiom\r\n```\r\nwith torch.no_grad():\r\n    p.add_(-group['lr'], d_p)\r\n```\r\n(in SGD) to correctly invalidate the CalculatedParameter.\r\nIt would then be a bug to use `.data` or `.detach()` in such cases.\r\n\r\nFrom my POV this removes the last major obstacle to using CalculatedParameter to cleanly implement cached calculated parameters.\r\n"}