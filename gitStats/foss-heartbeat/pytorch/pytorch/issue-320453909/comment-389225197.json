{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/389225197", "html_url": "https://github.com/pytorch/pytorch/issues/7313#issuecomment-389225197", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7313", "id": 389225197, "node_id": "MDEyOklzc3VlQ29tbWVudDM4OTIyNTE5Nw==", "user": {"login": "raulpuric", "id": 9101033, "node_id": "MDQ6VXNlcjkxMDEwMzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/9101033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raulpuric", "html_url": "https://github.com/raulpuric", "followers_url": "https://api.github.com/users/raulpuric/followers", "following_url": "https://api.github.com/users/raulpuric/following{/other_user}", "gists_url": "https://api.github.com/users/raulpuric/gists{/gist_id}", "starred_url": "https://api.github.com/users/raulpuric/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raulpuric/subscriptions", "organizations_url": "https://api.github.com/users/raulpuric/orgs", "repos_url": "https://api.github.com/users/raulpuric/repos", "events_url": "https://api.github.com/users/raulpuric/events{/privacy}", "received_events_url": "https://api.github.com/users/raulpuric/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-15T16:13:54Z", "updated_at": "2018-05-15T16:13:54Z", "author_association": "NONE", "body_html": "<p>Chiming in on this discussion. I'm the author of the reparameterization <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> shared earlier.<br>\nI've actually thought of 3 of these solutions in the past, and I'll be providing my thoughts on them in addition to the thoughts on my implementation.</p>\n<p>My/hooking implementation:<br>\nPros:</p>\n<ul>\n<li>Abstract interface for reparameterization</li>\n<li>wrapper to add hooks to all parameters in a module recursively</li>\n<li>handles some degree of parameter sharing.</li>\n<li>Works behind the scenes. No modifications to module aside from adding new parameters. The module can be used in the exact same way as before.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>can be hard/hacky to use with parameter sharing.</li>\n<li>saving/reloading model is not user-friendly. One can't load the weight_norm (g/v) parameters into a model without weight norm.</li>\n</ul>\n<p>Optimizer handles reparameterization:<br>\nPros:</p>\n<ul>\n<li>Saving/reloading model is user friendly as it does not modify anything in the model\n<ul>\n<li>works behind the scenes disentangled from the module</li>\n</ul>\n</li>\n<li>Offloading responsibility for doing reparameterizations makes sense in the context of weight norm, as one of the goals is to change how the parameters are optimized.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>Unusual optimizer usage is hard to allow (skipping optimization if gradients/loss blowup)</li>\n</ul>\n<p>Calculated/Lazy Parameter:<br>\nPersonally I'm a fan of this option, and had thought about it in the past, but didn't have time to implement it in a good fashion. If someone can implement a general class for this in a very safe PyTorch-y way, that also makes saving/reloading parameters easy, I would be in support of this option.<br>\nPros:</p>\n<ul>\n<li>Works at the parameter level and does not add new parameters to the module\n<ul>\n<li>Easy to swap param with a non-reparameterized version of the parameter (helps with save/load)</li>\n<li>should be able to handle weird optimization strategies so long as .data and .grad are managed well</li>\n</ul>\n</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>Works at the parameter level so managing these might be hard.\n<ul>\n<li>Dictionary is a good first step, but needs something more. A hook might be a good solution here, since no new parameters/functionality are getting added to the module itself</li>\n</ul>\n</li>\n</ul>\n<p>Module wrapper:<br>\nPros:</p>\n<ul>\n<li>It can handle everything that all the other options can (fprop, bprop, param updates, etc)</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>Module wrappers can complicate code. In order to access attributes/functionality of the wrapped module you have to add an additional <code>.module</code> everywhere</li>\n</ul>\n<p>The ideal sort of setup I envision is either one module wrapper class that handles everything for the user, or a combination of CalculatedParameters and hooks/module wrapper. (Preferably the latter)</p>\n<p>I think though in general PyTorch needs some sort of module wrapper nn.Module subclass that absorbs most if not all the functionality/attributes of the wrapped module instance. This would alleviate a lot of code considerations that have to be done when using things like (Distributed)DataParallel which forces us to add <code>model.module</code> all over our code.</p>", "body_text": "Chiming in on this discussion. I'm the author of the reparameterization @ngimel shared earlier.\nI've actually thought of 3 of these solutions in the past, and I'll be providing my thoughts on them in addition to the thoughts on my implementation.\nMy/hooking implementation:\nPros:\n\nAbstract interface for reparameterization\nwrapper to add hooks to all parameters in a module recursively\nhandles some degree of parameter sharing.\nWorks behind the scenes. No modifications to module aside from adding new parameters. The module can be used in the exact same way as before.\n\nCons:\n\ncan be hard/hacky to use with parameter sharing.\nsaving/reloading model is not user-friendly. One can't load the weight_norm (g/v) parameters into a model without weight norm.\n\nOptimizer handles reparameterization:\nPros:\n\nSaving/reloading model is user friendly as it does not modify anything in the model\n\nworks behind the scenes disentangled from the module\n\n\nOffloading responsibility for doing reparameterizations makes sense in the context of weight norm, as one of the goals is to change how the parameters are optimized.\n\nCons:\n\nUnusual optimizer usage is hard to allow (skipping optimization if gradients/loss blowup)\n\nCalculated/Lazy Parameter:\nPersonally I'm a fan of this option, and had thought about it in the past, but didn't have time to implement it in a good fashion. If someone can implement a general class for this in a very safe PyTorch-y way, that also makes saving/reloading parameters easy, I would be in support of this option.\nPros:\n\nWorks at the parameter level and does not add new parameters to the module\n\nEasy to swap param with a non-reparameterized version of the parameter (helps with save/load)\nshould be able to handle weird optimization strategies so long as .data and .grad are managed well\n\n\n\nCons:\n\nWorks at the parameter level so managing these might be hard.\n\nDictionary is a good first step, but needs something more. A hook might be a good solution here, since no new parameters/functionality are getting added to the module itself\n\n\n\nModule wrapper:\nPros:\n\nIt can handle everything that all the other options can (fprop, bprop, param updates, etc)\n\nCons:\n\nModule wrappers can complicate code. In order to access attributes/functionality of the wrapped module you have to add an additional .module everywhere\n\nThe ideal sort of setup I envision is either one module wrapper class that handles everything for the user, or a combination of CalculatedParameters and hooks/module wrapper. (Preferably the latter)\nI think though in general PyTorch needs some sort of module wrapper nn.Module subclass that absorbs most if not all the functionality/attributes of the wrapped module instance. This would alleviate a lot of code considerations that have to be done when using things like (Distributed)DataParallel which forces us to add model.module all over our code.", "body": "Chiming in on this discussion. I'm the author of the reparameterization @ngimel shared earlier.\r\nI've actually thought of 3 of these solutions in the past, and I'll be providing my thoughts on them in addition to the thoughts on my implementation.\r\n\r\nMy/hooking implementation:\r\nPros:\r\n* Abstract interface for reparameterization\r\n* wrapper to add hooks to all parameters in a module recursively\r\n* handles some degree of parameter sharing.\r\n* Works behind the scenes. No modifications to module aside from adding new parameters. The module can be used in the exact same way as before.\r\n\r\nCons:\r\n* can be hard/hacky to use with parameter sharing. \r\n* saving/reloading model is not user-friendly. One can't load the weight_norm (g/v) parameters into a model without weight norm.\r\n\r\nOptimizer handles reparameterization:\r\nPros:\r\n* Saving/reloading model is user friendly as it does not modify anything in the model\r\n  * works behind the scenes disentangled from the module\r\n* Offloading responsibility for doing reparameterizations makes sense in the context of weight norm, as one of the goals is to change how the parameters are optimized.\r\n\r\nCons:\r\n* Unusual optimizer usage is hard to allow (skipping optimization if gradients/loss blowup)\r\n\r\nCalculated/Lazy Parameter:\r\nPersonally I'm a fan of this option, and had thought about it in the past, but didn't have time to implement it in a good fashion. If someone can implement a general class for this in a very safe PyTorch-y way, that also makes saving/reloading parameters easy, I would be in support of this option.\r\nPros:\r\n * Works at the parameter level and does not add new parameters to the module\r\n   * Easy to swap param with a non-reparameterized version of the parameter (helps with save/load)\r\n   * should be able to handle weird optimization strategies so long as .data and .grad are managed well\r\n\r\nCons:\r\n* Works at the parameter level so managing these might be hard. \r\n  * Dictionary is a good first step, but needs something more. A hook might be a good solution here, since no new parameters/functionality are getting added to the module itself\r\n\r\nModule wrapper:\r\nPros:\r\n * It can handle everything that all the other options can (fprop, bprop, param updates, etc)\r\n\r\nCons:\r\n* Module wrappers can complicate code. In order to access attributes/functionality of the wrapped module you have to add an additional `.module` everywhere\r\n\r\n\r\nThe ideal sort of setup I envision is either one module wrapper class that handles everything for the user, or a combination of CalculatedParameters and hooks/module wrapper. (Preferably the latter)\r\n\r\nI think though in general PyTorch needs some sort of module wrapper nn.Module subclass that absorbs most if not all the functionality/attributes of the wrapped module instance. This would alleviate a lot of code considerations that have to be done when using things like (Distributed)DataParallel which forces us to add `model.module` all over our code.\r\n"}