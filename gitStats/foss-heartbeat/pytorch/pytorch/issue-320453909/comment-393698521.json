{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/393698521", "html_url": "https://github.com/pytorch/pytorch/issues/7313#issuecomment-393698521", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7313", "id": 393698521, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzY5ODUyMQ==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-31T22:16:19Z", "updated_at": "2018-05-31T22:16:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So I pushed an implementation using inplace hooks on tensor base to<br>\n<a href=\"https://github.com/t-vi/pytorch/tree/calculated_param\">https://github.com/t-vi/pytorch/tree/calculated_param</a><br>\nI think this addresses most things.</p>\n<p>For multiple parameters, I the main alternative to passing the parameter to the CalculatedParam call would be to specify the name when assigning. So it would be</p>\n<pre><code>cp = SubClassOfCalculatedParamWithMultipleReturns(somestuff)\nmodel.weight = cp.get_output(\"weight\")\nmodel.bias = cp.get_output(\"bias\")\n</code></pre>\n<p>but you could equally do</p>\n<pre><code>cp = SubClassOfCalculatedParamWithMultipleReturns(somestuff)\nmodel.weight = cp.get_output(\"wouldyouknow\")\nmodel.bias = cp.get_output(\"totallydifferent\")\n</code></pre>\n<p>if your forward returns those in a dict.</p>\n<p>Regarding the functional interface, I think it is a bit of a red herring, but without a Module subclass instance and its getattr method, one would have to do</p>\n<pre><code>F.linear(input, mycalculatedparam())\n</code></pre>\n<p>i.e. add the \"()\" to make it a call. Given that it gives you an error message rather than silently fail, I would think it is OK.</p>\n<p>As discussed above, it would need deprecation of using p.data.do_() instead of with nograd(): p.do_(). In theory, one could try to move the hooks to the storage level, but why would we - it adds complications for not really that much gain.</p>\n<p>I look forward to your comments.</p>", "body_text": "So I pushed an implementation using inplace hooks on tensor base to\nhttps://github.com/t-vi/pytorch/tree/calculated_param\nI think this addresses most things.\nFor multiple parameters, I the main alternative to passing the parameter to the CalculatedParam call would be to specify the name when assigning. So it would be\ncp = SubClassOfCalculatedParamWithMultipleReturns(somestuff)\nmodel.weight = cp.get_output(\"weight\")\nmodel.bias = cp.get_output(\"bias\")\n\nbut you could equally do\ncp = SubClassOfCalculatedParamWithMultipleReturns(somestuff)\nmodel.weight = cp.get_output(\"wouldyouknow\")\nmodel.bias = cp.get_output(\"totallydifferent\")\n\nif your forward returns those in a dict.\nRegarding the functional interface, I think it is a bit of a red herring, but without a Module subclass instance and its getattr method, one would have to do\nF.linear(input, mycalculatedparam())\n\ni.e. add the \"()\" to make it a call. Given that it gives you an error message rather than silently fail, I would think it is OK.\nAs discussed above, it would need deprecation of using p.data.do_() instead of with nograd(): p.do_(). In theory, one could try to move the hooks to the storage level, but why would we - it adds complications for not really that much gain.\nI look forward to your comments.", "body": "So I pushed an implementation using inplace hooks on tensor base to \r\nhttps://github.com/t-vi/pytorch/tree/calculated_param\r\nI think this addresses most things.\r\n\r\nFor multiple parameters, I the main alternative to passing the parameter to the CalculatedParam call would be to specify the name when assigning. So it would be \r\n```\r\ncp = SubClassOfCalculatedParamWithMultipleReturns(somestuff)\r\nmodel.weight = cp.get_output(\"weight\")\r\nmodel.bias = cp.get_output(\"bias\")\r\n```\r\nbut you could equally do\r\n```\r\ncp = SubClassOfCalculatedParamWithMultipleReturns(somestuff)\r\nmodel.weight = cp.get_output(\"wouldyouknow\")\r\nmodel.bias = cp.get_output(\"totallydifferent\")\r\n```\r\nif your forward returns those in a dict.\r\n\r\nRegarding the functional interface, I think it is a bit of a red herring, but without a Module subclass instance and its getattr method, one would have to do\r\n```\r\nF.linear(input, mycalculatedparam())\r\n```\r\ni.e. add the \"()\" to make it a call. Given that it gives you an error message rather than silently fail, I would think it is OK.\r\n\r\nAs discussed above, it would need deprecation of using p.data.do_() instead of with nograd(): p.do_(). In theory, one could try to move the hooks to the storage level, but why would we - it adds complications for not really that much gain.\r\n\r\nI look forward to your comments.\r\n"}