{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/389498108", "html_url": "https://github.com/pytorch/pytorch/issues/7313#issuecomment-389498108", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7313", "id": 389498108, "node_id": "MDEyOklzc3VlQ29tbWVudDM4OTQ5ODEwOA==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-16T12:18:39Z", "updated_at": "2018-05-16T12:18:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>For the calculated parameter, you just have a module that uses <code>other_param</code>, so I think it covers that easily. The multiple outputs require cooperation between the calculated params and the module in naming the parameters, but that seems unavoidable regardless of the solution.</p>\n<p>I'm not sure I quite understand the wrapper vs. patching argument. I see CalculatedParameter to do neither (you just assign the parameter). If you wanted to stack things, you would assign the one CalculatedParameter's parameters with those of another (needs a look at propagating the calling <code>updated</code>, but doesn't seem too hard).</p>\n<p>In a different area: I looked a bit into what to do about calling the Parameter.updated hook and think it can be pushed to TensorBase in C and have it called automatically in inplace operations. That only works when using <code>torch.without_grad(): p.add_(...)</code> or whatever instead of using <code>p.data.add_</code> as is currently done, but I think that would be good enough. When working with a getter/setter for definition of the updated function, the performance impact on other Tensor's should be irrelevant.</p>", "body_text": "For the calculated parameter, you just have a module that uses other_param, so I think it covers that easily. The multiple outputs require cooperation between the calculated params and the module in naming the parameters, but that seems unavoidable regardless of the solution.\nI'm not sure I quite understand the wrapper vs. patching argument. I see CalculatedParameter to do neither (you just assign the parameter). If you wanted to stack things, you would assign the one CalculatedParameter's parameters with those of another (needs a look at propagating the calling updated, but doesn't seem too hard).\nIn a different area: I looked a bit into what to do about calling the Parameter.updated hook and think it can be pushed to TensorBase in C and have it called automatically in inplace operations. That only works when using torch.without_grad(): p.add_(...) or whatever instead of using p.data.add_ as is currently done, but I think that would be good enough. When working with a getter/setter for definition of the updated function, the performance impact on other Tensor's should be irrelevant.", "body": "For the calculated parameter, you just have a module that uses `other_param`, so I think it covers that easily. The multiple outputs require cooperation between the calculated params and the module in naming the parameters, but that seems unavoidable regardless of the solution.\r\n\r\nI'm not sure I quite understand the wrapper vs. patching argument. I see CalculatedParameter to do neither (you just assign the parameter). If you wanted to stack things, you would assign the one CalculatedParameter's parameters with those of another (needs a look at propagating the calling `updated`, but doesn't seem too hard). \r\n\r\nIn a different area: I looked a bit into what to do about calling the Parameter.updated hook and think it can be pushed to TensorBase in C and have it called automatically in inplace operations. That only works when using `torch.without_grad(): p.add_(...)` or whatever instead of using `p.data.add_` as is currently done, but I think that would be good enough. When working with a getter/setter for definition of the updated function, the performance impact on other Tensor's should be irrelevant.\r\n"}