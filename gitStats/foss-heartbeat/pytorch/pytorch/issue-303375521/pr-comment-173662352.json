{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/173662352", "pull_request_review_id": 102606950, "id": 173662352, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MzY2MjM1Mg==", "diff_hunk": "@@ -0,0 +1,256 @@\n+#pragma once\n+#include \"torch/csrc/jit/ir.h\"\n+#include \"torch/csrc/jit/graph_executor.h\"\n+#include \"torch/csrc/autograd/variable.h\"\n+\n+// This file contains classes which assist in desugaring Python style\n+// modules and their methods into flattened graphs which don't have any\n+// function calls.\n+\n+namespace torch { namespace jit { namespace script {\n+\n+// A method in a module, e.g. f in:\n+//\n+// class M(ScriptModule):\n+//   @script_method\n+//   def f(self, x):\n+//     ...\n+struct Method {\n+  Method(std::string name, bool optimize)\n+  : name_(std::move(name))\n+  , graph_(std::make_shared<Graph>())\n+  , optimize(optimize) {}\n+  variable_tensor_list run(variable_tensor_list && inputs) {\n+    std::call_once(executor_init, [&]{\n+      executor = GraphExecutor(graph_, optimize);\n+    });\n+    for(auto tp : member_inputs) {\n+      inputs.push_back(*tp);\n+    }\n+    return executor.run(std::move(inputs));\n+  }\n+  std::shared_ptr<Graph> graph() const {\n+    return graph_;\n+  }\n+\n+  const std::string & name() const {\n+    return name_;\n+  }\n+  // emit a function call by inlining the callees Graph into this one\n+  // adding any extra parameters necessary to do this call\n+\n+  // defined here to keep details of member_input handling confined to this class\n+  std::vector<Value*> emit_call_to(Method & callee, ArrayRef<Value*> inputs) {\n+    JIT_ASSERT(!executor);\n+    auto fn = callee.graph();\n+    JIT_ASSERT(inputs.size() == callee.num_inputs());\n+    std::unordered_map<Value*, Value*> value_map;\n+    auto value_map_func = [&](Value* v) { return value_map.at(v); };\n+    // actual inputs\n+    for (size_t i = 0; i < inputs.size(); ++i) {\n+      value_map[fn->inputs()[i]] = inputs[i];\n+    }\n+    // parameters to callee method (which become parameters to _this_ method\n+    // if they were not already)\n+    auto members_it = callee.member_inputs.begin();\n+    for(size_t i = inputs.size(); i < fn->inputs().size(); i++) {\n+      value_map[fn->inputs()[i]] = get_or_add_parameter(*members_it++);\n+    }\n+    for (auto* node : fn->nodes()) {\n+      auto* new_node =\n+          graph_->insertNode(graph_->createClone(node, value_map_func));\n+      for (size_t i = 0; i < node->outputs().size(); ++i) {\n+        value_map[node->outputs()[i]] = new_node->outputs()[i];\n+      }\n+    }\n+\n+    std::vector<Value*> outputs;\n+    for (auto* output : fn->outputs()) {\n+      outputs.push_back(value_map_func(output));\n+    }\n+    return outputs;\n+  }\n+  size_t num_inputs() const {\n+    return graph_->inputs().size() - member_inputs.size();\n+  }\n+  Value * get_or_add_parameter(at::Tensor* slot) {\n+    size_t first_parameter = graph()->inputs().size() - member_inputs.size();\n+    for(size_t i = 0; i < member_inputs.size(); i++) {\n+      // it is already a parameter\n+      if(member_inputs[i] == slot) {\n+        return graph()->inputs()[first_parameter + i];\n+      }\n+    }\n+    // add it as a new parameter\n+    member_inputs.push_back(slot);\n+    return graph()->addInput();\n+  }\n+private:\n+  std::string name_;\n+  std::shared_ptr<Graph> graph_; // for debugging and for inlining\n+  bool optimize;\n+  GraphExecutor executor; // for execution\n+  // member_inputs are a list of additional arguments appended to graph that are\n+  // inputs that come from the members of the Module or its submodules.\n+  // each is a pointer to a slot in the module that owns this Method or a submethod\n+  // of the module.\n+  // parameters and submodules can only be _added_ to script Modules to ensure\n+  // these pointers always stay valid\n+  std::vector<at::Tensor*> member_inputs;\n+\n+\n+  // TODO: support that case where we allow _writes_ to parameters from\n+  // compiled functions.\n+  // This requires more sophisticated tracking of ssa values in Graphs so that\n+  // stores to all modules can be lifted to the end of a graph execution.\n+  // It also adds more complexity to adding actual module invocations\n+  // to the executor, so currently it is not done.\n+  // std::vector<at::Tensor*> member_outputs;\n+\n+  std::once_flag executor_init;\n+};\n+\n+struct Module;\n+\n+struct NamedModule {\n+  std::string name;\n+  std::shared_ptr<Module> module;\n+};\n+\n+struct NamedParameter {\n+  NamedParameter(std::string name, at::Tensor tensor)\n+  : name(std::move(name)), parameter(new at::Tensor(std::move(tensor))) {}\n+\n+  std::string name;\n+  at::Tensor* slot() const {\n+    return parameter.get();\n+  }\n+private:\n+  // the extra level of indirection allows Methods to safely store pointers\n+  // to the slots where parameters are kept while also allow parameters\n+  // to be reassigned\n+  std::unique_ptr<at::Tensor> parameter;", "path": "torch/csrc/jit/script/module.h", "position": 105, "original_position": 132, "commit_id": "ea6aa3491a3ed3df04c89336181fc62bbae67cd7", "original_commit_id": "1c077075a28c6808727b4d63ae7ac69659ccf869", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I think we could have just used a list to store module parameters, and that would flatten the extra level of indirection into the container, right?", "created_at": "2018-03-11T17:20:42Z", "updated_at": "2018-11-23T15:40:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/5630#discussion_r173662352", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5630", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/173662352"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5630#discussion_r173662352"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5630"}}, "body_html": "<p>I think we could have just used a list to store module parameters, and that would flatten the extra level of indirection into the container, right?</p>", "body_text": "I think we could have just used a list to store module parameters, and that would flatten the extra level of indirection into the container, right?"}