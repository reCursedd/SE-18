{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8160", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8160/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8160/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8160/events", "html_url": "https://github.com/pytorch/pytorch/issues/8160", "id": 329447023, "node_id": "MDU6SXNzdWUzMjk0NDcwMjM=", "number": 8160, "title": "SpectralNorm fails when using DistributedDataParallel", "user": {"login": "adampolyak", "id": 7717301, "node_id": "MDQ6VXNlcjc3MTczMDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/7717301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adampolyak", "html_url": "https://github.com/adampolyak", "followers_url": "https://api.github.com/users/adampolyak/followers", "following_url": "https://api.github.com/users/adampolyak/following{/other_user}", "gists_url": "https://api.github.com/users/adampolyak/gists{/gist_id}", "starred_url": "https://api.github.com/users/adampolyak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adampolyak/subscriptions", "organizations_url": "https://api.github.com/users/adampolyak/orgs", "repos_url": "https://api.github.com/users/adampolyak/repos", "events_url": "https://api.github.com/users/adampolyak/events{/privacy}", "received_events_url": "https://api.github.com/users/adampolyak/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-06-05T13:04:24Z", "updated_at": "2018-06-18T17:53:41Z", "closed_at": "2018-06-18T17:53:41Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>Using spectral_norm in a DistributedDataParallel model in PyTorch raises :<br>\n<code>RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.</code></p>\n<h2>Repro</h2>\n<pre><code>import os\nimport torch\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\nimport torch.nn as nn\nfrom spectral_norm import spectral_norm\n\ndef run(rank, size):\n    \"\"\" Distributed function to be implemented later. \"\"\"\n    print(\"Started\")\n    net = nn.Linear(20, 20)\n    SN_net = spectral_norm(net)\n    dist_net = torch.nn.parallel.DistributedDataParallel(net,\n                                                     device_ids=[rank],\n                                                     output_device=rank)\n    pass\n\ndef init_processes(rank, size, fn, backend='tcp'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    processes = []\n    for rank in range(size):\n        p = Process(target=init_processes, args=(rank, size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()\n</code></pre>", "body_text": "Issue description\nUsing spectral_norm in a DistributedDataParallel model in PyTorch raises :\nRuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\nRepro\nimport os\nimport torch\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\nimport torch.nn as nn\nfrom spectral_norm import spectral_norm\n\ndef run(rank, size):\n    \"\"\" Distributed function to be implemented later. \"\"\"\n    print(\"Started\")\n    net = nn.Linear(20, 20)\n    SN_net = spectral_norm(net)\n    dist_net = torch.nn.parallel.DistributedDataParallel(net,\n                                                     device_ids=[rank],\n                                                     output_device=rank)\n    pass\n\ndef init_processes(rank, size, fn, backend='tcp'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    processes = []\n    for rank in range(size):\n        p = Process(target=init_processes, args=(rank, size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()", "body": "## Issue description\r\n\r\nUsing spectral_norm in a DistributedDataParallel model in PyTorch raises :\r\n``` RuntimeError: a leaf Variable that requires grad has been used in an in-place operation. ```\r\n\r\n## Repro\r\n\r\n```\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\nfrom torch.multiprocessing import Process\r\nimport torch.nn as nn\r\nfrom spectral_norm import spectral_norm\r\n\r\ndef run(rank, size):\r\n    \"\"\" Distributed function to be implemented later. \"\"\"\r\n    print(\"Started\")\r\n    net = nn.Linear(20, 20)\r\n    SN_net = spectral_norm(net)\r\n    dist_net = torch.nn.parallel.DistributedDataParallel(net,\r\n                                                     device_ids=[rank],\r\n                                                     output_device=rank)\r\n    pass\r\n\r\ndef init_processes(rank, size, fn, backend='tcp'):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '29500'\r\n    dist.init_process_group(backend, rank=rank, world_size=size)\r\n    fn(rank, size)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    size = 2\r\n    processes = []\r\n    for rank in range(size):\r\n        p = Process(target=init_processes, args=(rank, size, run))\r\n        p.start()\r\n        processes.append(p)\r\n\r\n    for p in processes:\r\n        p.join()\r\n```\r\n"}