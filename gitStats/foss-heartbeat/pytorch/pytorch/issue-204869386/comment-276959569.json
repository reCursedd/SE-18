{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/276959569", "html_url": "https://github.com/pytorch/pytorch/issues/680#issuecomment-276959569", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/680", "id": 276959569, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Njk1OTU2OQ==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-02T13:40:41Z", "updated_at": "2017-02-02T13:40:41Z", "author_association": "MEMBER", "body_html": "<p>if you are training alexnet and you have a hdd, you are bottlenecked by disk random read speed which is very low for hdds. your best bet is to preshuffle your dataset, resize it down to 256, make an lmdb out of it and read the lmdb image entries sequentially.</p>", "body_text": "if you are training alexnet and you have a hdd, you are bottlenecked by disk random read speed which is very low for hdds. your best bet is to preshuffle your dataset, resize it down to 256, make an lmdb out of it and read the lmdb image entries sequentially.", "body": "if you are training alexnet and you have a hdd, you are bottlenecked by disk random read speed which is very low for hdds. your best bet is to preshuffle your dataset, resize it down to 256, make an lmdb out of it and read the lmdb image entries sequentially. "}