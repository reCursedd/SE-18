{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1085", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1085/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1085/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1085/events", "html_url": "https://github.com/pytorch/pytorch/issues/1085", "id": 216752941, "node_id": "MDU6SXNzdWUyMTY3NTI5NDE=", "number": 1085, "title": "pytorch not returning GPU memory", "user": {"login": "rogertrullo", "id": 8496304, "node_id": "MDQ6VXNlcjg0OTYzMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/8496304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rogertrullo", "html_url": "https://github.com/rogertrullo", "followers_url": "https://api.github.com/users/rogertrullo/followers", "following_url": "https://api.github.com/users/rogertrullo/following{/other_user}", "gists_url": "https://api.github.com/users/rogertrullo/gists{/gist_id}", "starred_url": "https://api.github.com/users/rogertrullo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rogertrullo/subscriptions", "organizations_url": "https://api.github.com/users/rogertrullo/orgs", "repos_url": "https://api.github.com/users/rogertrullo/repos", "events_url": "https://api.github.com/users/rogertrullo/events{/privacy}", "received_events_url": "https://api.github.com/users/rogertrullo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-24T11:23:44Z", "updated_at": "2017-06-02T18:26:49Z", "closed_at": "2017-03-24T22:40:41Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nI have implemented a model which seems to be working. However, when I kill it, with <code>kill PID</code>, the gpu memory is not freed:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/0b3b4b87ccd7ae514bbfbb481b3a71565b74f743/68747470733a2f2f646973637573732e7079746f7263682e6f72672f75706c6f6164732f64656661756c742f6f726967696e616c2f31582f363136343133633762333965643935373434346139343232333461636339343738343365306365632e706e67\"><img src=\"https://camo.githubusercontent.com/0b3b4b87ccd7ae514bbfbb481b3a71565b74f743/68747470733a2f2f646973637573732e7079746f7263682e6f72672f75706c6f6164732f64656661756c742f6f726967696e616c2f31582f363136343133633762333965643935373434346139343232333461636339343738343365306365632e706e67\" alt=\"alt text\" title=\"gpu mem\" data-canonical-src=\"https://discuss.pytorch.org/uploads/default/original/1X/616413c7b39ed957444a942234acc947843e0cec.png\" style=\"max-width:100%;\"></a></p>\n<p>As you can see, there is some memory used and there is no process running.<br>\nHere's the code that I am running:</p>\n<pre><code>class seg_GAN(object):\n    def __init__(self, batch_size=10, height=512,width=512,channels=3, wd=0.0005,nfilters_d=64, checkpoint_dir=None, path_imgs=None, learning_rate=2e-8,lr_step=30000,lam_fcn=1, lam_adv=1,adversarial=False,nclasses=5):\n\n        self.adversarial=adversarial\n        self.channels=channels\n        self.lam_fcn=lam_fcn\n        self.lam_adv=lam_adv\n        self.lr_step=lr_step\n        self.wd=wd\n        self.learning_rate=learning_rate\n        self.batch_size=batch_size       \n        self.height=height\n        self.width=width\n        self.checkpoint_dir = checkpoint_dir\n        self.path_imgs=path_imgs\n        self.nfilters_d=nfilters_d\n        self.organ_target=1#1 eso 2 heart 3 trach 4 aorta\n        self.nclasses=nclasses\n        self.netG=UNet(self.nclasses,self.channels)\n        self.netG.apply(weights_init)\n\tif self.adversarial:\n\t    self.netD=Discriminator(self.nclasses,self.nfilters_d,self.height,self.width)\n            self.netD.apply(weights_init)\n\n        self.dst = stanfordDataSet(self.path_imgs, is_transform=True)\n        self.trainloader = data.DataLoader(self.dst, batch_size=self.batch_size, shuffle=True, num_workers=2)\n\n    def train(self,config):\n        print 'verion ',torch.__version__\n\n        start=0#TODO change this so that it can continue when loading a model\n        print(\"Start from:\", start)\n\n        label_ones=torch.ones(self.batch_size)\n        label_zeros=torch.zeros(self.batch_size)\n        y_onehot = torch.FloatTensor(self.batch_size,self.nclasses,self.height, self.width)            \n\n        #print 'shape y_onehot ',y_onehot.size()\n        if self.adversarial:\n            self.netD.cuda()\n        self.netG.cuda()\n        label_ones,label_zeros,y_onehot=label_ones.cuda(),label_zeros.cuda(),y_onehot.cuda()\n        \n        y_onehot_var= Variable(y_onehot)\n        label_ones_var = Variable(label_ones)\n        label_zeros_var = Variable(label_zeros)\n        if self.adversarial:\n            optimizerD = optim.Adam(self.netD.parameters(), lr = self.learning_rate, betas = (0.5, 0.999))\n        optimizerG = optim.Adam(self.netG.parameters(), lr = self.learning_rate, betas = (0.5, 0.999))\n\n        for it in range(start,config.iterations):#epochs\n            for i, (images,GT) in enumerate(self.trainloader):                    \n                \n                y_onehot.resize_(GT.size(0),self.nclasses,self.height, self.width)\n                y_onehot.zero_()\n                label_ones.resize_(GT.size(0))\n                label_zeros.resize_(GT.size(0))                    \n\n                images = Variable(images.cuda()) \n                #images = Variable(images)\n                #print 'unique ',np.unique(GT.numpy())\n                GT=GT.cuda()\n                \n                #print 'image size ',images.size()\n                #print 'GT size ',GT.size()\n                #print 'shape y_onehot ',y_onehot.size()    \n                y_onehot.scatter_(1,GT.view(GT.size(0),1,GT.size(1),GT.size(2)),1)#we need to add singleton dim so thatnum of dims is equal\n                \n\n                #GT=Variable(GT.cuda())#N,H,W\n                GT=Variable(GT)#N,H,W\n                if self.adversarial:\n\n                    ##########################\n                    #Update Discriminator\n                    ##########################\n                    #train with real samples\n                    self.netD.zero_grad()\n                    #print self.netD\n                    output=self.netD(y_onehot_var)#this must be in one hot\n                    errD_real =F.binary_cross_entropy(output,label_ones_var)#loss_D\n                    errD_real.backward()#update grads of netD\n                    \n\n                    # train with fake\n                    fake = self.netG(images)#this is a prob map which we want to be similar to y_onehot\n                    #print 'fake sz',fake.size()\n                    output = self.netD(fake.detach())#only for speed, so grads of netg are not computed\n                    errD_fake = F.binary_cross_entropy(output, label_zeros_var)\n                    \n                    errD_fake.backward()\n\n                    optimizerD.step()#update the parameters of netD\n\n                ############################\n                # Update G network\n                ###########################\n                self.netG.zero_grad()\n                if self.adversarial:\n                    output_D=self.netD(fake)\n                    output_G, GT,label_ones,output_D\n                    errG = self.loss_G(fake,GT, label_ones_var,output_D)#here we should use ones with the fakes\n                else:\n                    fake = self.netG(images)\n                    errG = self.loss_G(fake,GT)\n\n                \n                errG.backward()#backprop errors\n                optimizerG.step()#optimize only netG params\n\n                if i%10==0:\n                    print 'epoch  ',it\n                    print 'iteration ',i\n                    if self.adversarial:\n                        print 'error real ',errD_real.data                  \n                        print 'error fake ',errD_fake.data\n                    print 'error Generator ',errG.data\n\n            if it%5==0:\n                print 'testing ...'\n                name_img='0000047'\n                meanval=self.dst.mean_rgb\n                img_test_name=os.path.join(self.path_imgs,'iccv09Data',\"images_resized\",name_img+'.png')\n                lab_test_name=os.path.join(self.path_imgs,'iccv09Data',\"labels_resized\",name_img+'_label.png')\n                img = Image.open(img_test_name)\n                img = np.array(img, dtype=np.uint8)\n                label = Image.open(lab_test_name)\n                label = np.array(label, dtype=np.uint8)\n                img = img.astype(np.float32)\n                img -= meanval\n                img = img.transpose(2, 0, 1)\n\n                img_ = torch.from_numpy(img)#.float()\n                img_=img_.cuda()\n                img_var=Variable(img_)\n                prob_map = self.netG(img_var.view(1,img_.size(0),img_.size(1),img_.size(2)))\n                prob_np=prob_map.data.cpu().numpy()\n                prob_np=np.squeeze(prob_np)\n                label_out=np.argmax(prob_np,0)\n                print 'unique labout ',np.unique(label_out)\n                print 'probmap ',prob_map.size()\n                lab_visual=label2color(label_out)\n                imsave('out_color.png',lab_visual)\n                for idlabel in np.unique(label):\n                    diceratio=dice(label, label_out,idlabel)\n                    print 'dice id {} '.format(idlabel),diceratio\n\n\n    def loss_G(self,output_G, GT,label_ones=None,output_D=None):\n        fcnterm=CrossEntropy2d(output_G,GT)\n        if self.adversarial:\n            bceterm=F.binary_cross_entropy(output_D,label_ones)\n            return fcnterm+self.lam_adv*bceterm\n        else:\n            return fcnterm\n</code></pre>\n<p>I have tried it in 2 different machines and the same problem is happening.<br>\nSomebody in the pytorch  <a href=\"https://discuss.pytorch.org/t/gpu-memory-not-returned/1311\" rel=\"nofollow\">forum</a> told me that it was a problem of DataLoader.<br>\nIs there a solution for that?<br>\nThanks!</p>", "body_text": "Hi,\nI have implemented a model which seems to be working. However, when I kill it, with kill PID, the gpu memory is not freed:\n\nAs you can see, there is some memory used and there is no process running.\nHere's the code that I am running:\nclass seg_GAN(object):\n    def __init__(self, batch_size=10, height=512,width=512,channels=3, wd=0.0005,nfilters_d=64, checkpoint_dir=None, path_imgs=None, learning_rate=2e-8,lr_step=30000,lam_fcn=1, lam_adv=1,adversarial=False,nclasses=5):\n\n        self.adversarial=adversarial\n        self.channels=channels\n        self.lam_fcn=lam_fcn\n        self.lam_adv=lam_adv\n        self.lr_step=lr_step\n        self.wd=wd\n        self.learning_rate=learning_rate\n        self.batch_size=batch_size       \n        self.height=height\n        self.width=width\n        self.checkpoint_dir = checkpoint_dir\n        self.path_imgs=path_imgs\n        self.nfilters_d=nfilters_d\n        self.organ_target=1#1 eso 2 heart 3 trach 4 aorta\n        self.nclasses=nclasses\n        self.netG=UNet(self.nclasses,self.channels)\n        self.netG.apply(weights_init)\n\tif self.adversarial:\n\t    self.netD=Discriminator(self.nclasses,self.nfilters_d,self.height,self.width)\n            self.netD.apply(weights_init)\n\n        self.dst = stanfordDataSet(self.path_imgs, is_transform=True)\n        self.trainloader = data.DataLoader(self.dst, batch_size=self.batch_size, shuffle=True, num_workers=2)\n\n    def train(self,config):\n        print 'verion ',torch.__version__\n\n        start=0#TODO change this so that it can continue when loading a model\n        print(\"Start from:\", start)\n\n        label_ones=torch.ones(self.batch_size)\n        label_zeros=torch.zeros(self.batch_size)\n        y_onehot = torch.FloatTensor(self.batch_size,self.nclasses,self.height, self.width)            \n\n        #print 'shape y_onehot ',y_onehot.size()\n        if self.adversarial:\n            self.netD.cuda()\n        self.netG.cuda()\n        label_ones,label_zeros,y_onehot=label_ones.cuda(),label_zeros.cuda(),y_onehot.cuda()\n        \n        y_onehot_var= Variable(y_onehot)\n        label_ones_var = Variable(label_ones)\n        label_zeros_var = Variable(label_zeros)\n        if self.adversarial:\n            optimizerD = optim.Adam(self.netD.parameters(), lr = self.learning_rate, betas = (0.5, 0.999))\n        optimizerG = optim.Adam(self.netG.parameters(), lr = self.learning_rate, betas = (0.5, 0.999))\n\n        for it in range(start,config.iterations):#epochs\n            for i, (images,GT) in enumerate(self.trainloader):                    \n                \n                y_onehot.resize_(GT.size(0),self.nclasses,self.height, self.width)\n                y_onehot.zero_()\n                label_ones.resize_(GT.size(0))\n                label_zeros.resize_(GT.size(0))                    \n\n                images = Variable(images.cuda()) \n                #images = Variable(images)\n                #print 'unique ',np.unique(GT.numpy())\n                GT=GT.cuda()\n                \n                #print 'image size ',images.size()\n                #print 'GT size ',GT.size()\n                #print 'shape y_onehot ',y_onehot.size()    \n                y_onehot.scatter_(1,GT.view(GT.size(0),1,GT.size(1),GT.size(2)),1)#we need to add singleton dim so thatnum of dims is equal\n                \n\n                #GT=Variable(GT.cuda())#N,H,W\n                GT=Variable(GT)#N,H,W\n                if self.adversarial:\n\n                    ##########################\n                    #Update Discriminator\n                    ##########################\n                    #train with real samples\n                    self.netD.zero_grad()\n                    #print self.netD\n                    output=self.netD(y_onehot_var)#this must be in one hot\n                    errD_real =F.binary_cross_entropy(output,label_ones_var)#loss_D\n                    errD_real.backward()#update grads of netD\n                    \n\n                    # train with fake\n                    fake = self.netG(images)#this is a prob map which we want to be similar to y_onehot\n                    #print 'fake sz',fake.size()\n                    output = self.netD(fake.detach())#only for speed, so grads of netg are not computed\n                    errD_fake = F.binary_cross_entropy(output, label_zeros_var)\n                    \n                    errD_fake.backward()\n\n                    optimizerD.step()#update the parameters of netD\n\n                ############################\n                # Update G network\n                ###########################\n                self.netG.zero_grad()\n                if self.adversarial:\n                    output_D=self.netD(fake)\n                    output_G, GT,label_ones,output_D\n                    errG = self.loss_G(fake,GT, label_ones_var,output_D)#here we should use ones with the fakes\n                else:\n                    fake = self.netG(images)\n                    errG = self.loss_G(fake,GT)\n\n                \n                errG.backward()#backprop errors\n                optimizerG.step()#optimize only netG params\n\n                if i%10==0:\n                    print 'epoch  ',it\n                    print 'iteration ',i\n                    if self.adversarial:\n                        print 'error real ',errD_real.data                  \n                        print 'error fake ',errD_fake.data\n                    print 'error Generator ',errG.data\n\n            if it%5==0:\n                print 'testing ...'\n                name_img='0000047'\n                meanval=self.dst.mean_rgb\n                img_test_name=os.path.join(self.path_imgs,'iccv09Data',\"images_resized\",name_img+'.png')\n                lab_test_name=os.path.join(self.path_imgs,'iccv09Data',\"labels_resized\",name_img+'_label.png')\n                img = Image.open(img_test_name)\n                img = np.array(img, dtype=np.uint8)\n                label = Image.open(lab_test_name)\n                label = np.array(label, dtype=np.uint8)\n                img = img.astype(np.float32)\n                img -= meanval\n                img = img.transpose(2, 0, 1)\n\n                img_ = torch.from_numpy(img)#.float()\n                img_=img_.cuda()\n                img_var=Variable(img_)\n                prob_map = self.netG(img_var.view(1,img_.size(0),img_.size(1),img_.size(2)))\n                prob_np=prob_map.data.cpu().numpy()\n                prob_np=np.squeeze(prob_np)\n                label_out=np.argmax(prob_np,0)\n                print 'unique labout ',np.unique(label_out)\n                print 'probmap ',prob_map.size()\n                lab_visual=label2color(label_out)\n                imsave('out_color.png',lab_visual)\n                for idlabel in np.unique(label):\n                    diceratio=dice(label, label_out,idlabel)\n                    print 'dice id {} '.format(idlabel),diceratio\n\n\n    def loss_G(self,output_G, GT,label_ones=None,output_D=None):\n        fcnterm=CrossEntropy2d(output_G,GT)\n        if self.adversarial:\n            bceterm=F.binary_cross_entropy(output_D,label_ones)\n            return fcnterm+self.lam_adv*bceterm\n        else:\n            return fcnterm\n\nI have tried it in 2 different machines and the same problem is happening.\nSomebody in the pytorch  forum told me that it was a problem of DataLoader.\nIs there a solution for that?\nThanks!", "body": "Hi,\r\nI have implemented a model which seems to be working. However, when I kill it, with `kill PID`, the gpu memory is not freed:\r\n\r\n![alt text](https://discuss.pytorch.org/uploads/default/original/1X/616413c7b39ed957444a942234acc947843e0cec.png \"gpu mem\")\r\n\r\nAs you can see, there is some memory used and there is no process running.\r\nHere's the code that I am running:\r\n\r\n```\r\nclass seg_GAN(object):\r\n    def __init__(self, batch_size=10, height=512,width=512,channels=3, wd=0.0005,nfilters_d=64, checkpoint_dir=None, path_imgs=None, learning_rate=2e-8,lr_step=30000,lam_fcn=1, lam_adv=1,adversarial=False,nclasses=5):\r\n\r\n        self.adversarial=adversarial\r\n        self.channels=channels\r\n        self.lam_fcn=lam_fcn\r\n        self.lam_adv=lam_adv\r\n        self.lr_step=lr_step\r\n        self.wd=wd\r\n        self.learning_rate=learning_rate\r\n        self.batch_size=batch_size       \r\n        self.height=height\r\n        self.width=width\r\n        self.checkpoint_dir = checkpoint_dir\r\n        self.path_imgs=path_imgs\r\n        self.nfilters_d=nfilters_d\r\n        self.organ_target=1#1 eso 2 heart 3 trach 4 aorta\r\n        self.nclasses=nclasses\r\n        self.netG=UNet(self.nclasses,self.channels)\r\n        self.netG.apply(weights_init)\r\n\tif self.adversarial:\r\n\t    self.netD=Discriminator(self.nclasses,self.nfilters_d,self.height,self.width)\r\n            self.netD.apply(weights_init)\r\n\r\n        self.dst = stanfordDataSet(self.path_imgs, is_transform=True)\r\n        self.trainloader = data.DataLoader(self.dst, batch_size=self.batch_size, shuffle=True, num_workers=2)\r\n\r\n    def train(self,config):\r\n        print 'verion ',torch.__version__\r\n\r\n        start=0#TODO change this so that it can continue when loading a model\r\n        print(\"Start from:\", start)\r\n\r\n        label_ones=torch.ones(self.batch_size)\r\n        label_zeros=torch.zeros(self.batch_size)\r\n        y_onehot = torch.FloatTensor(self.batch_size,self.nclasses,self.height, self.width)            \r\n\r\n        #print 'shape y_onehot ',y_onehot.size()\r\n        if self.adversarial:\r\n            self.netD.cuda()\r\n        self.netG.cuda()\r\n        label_ones,label_zeros,y_onehot=label_ones.cuda(),label_zeros.cuda(),y_onehot.cuda()\r\n        \r\n        y_onehot_var= Variable(y_onehot)\r\n        label_ones_var = Variable(label_ones)\r\n        label_zeros_var = Variable(label_zeros)\r\n        if self.adversarial:\r\n            optimizerD = optim.Adam(self.netD.parameters(), lr = self.learning_rate, betas = (0.5, 0.999))\r\n        optimizerG = optim.Adam(self.netG.parameters(), lr = self.learning_rate, betas = (0.5, 0.999))\r\n\r\n        for it in range(start,config.iterations):#epochs\r\n            for i, (images,GT) in enumerate(self.trainloader):                    \r\n                \r\n                y_onehot.resize_(GT.size(0),self.nclasses,self.height, self.width)\r\n                y_onehot.zero_()\r\n                label_ones.resize_(GT.size(0))\r\n                label_zeros.resize_(GT.size(0))                    \r\n\r\n                images = Variable(images.cuda()) \r\n                #images = Variable(images)\r\n                #print 'unique ',np.unique(GT.numpy())\r\n                GT=GT.cuda()\r\n                \r\n                #print 'image size ',images.size()\r\n                #print 'GT size ',GT.size()\r\n                #print 'shape y_onehot ',y_onehot.size()    \r\n                y_onehot.scatter_(1,GT.view(GT.size(0),1,GT.size(1),GT.size(2)),1)#we need to add singleton dim so thatnum of dims is equal\r\n                \r\n\r\n                #GT=Variable(GT.cuda())#N,H,W\r\n                GT=Variable(GT)#N,H,W\r\n                if self.adversarial:\r\n\r\n                    ##########################\r\n                    #Update Discriminator\r\n                    ##########################\r\n                    #train with real samples\r\n                    self.netD.zero_grad()\r\n                    #print self.netD\r\n                    output=self.netD(y_onehot_var)#this must be in one hot\r\n                    errD_real =F.binary_cross_entropy(output,label_ones_var)#loss_D\r\n                    errD_real.backward()#update grads of netD\r\n                    \r\n\r\n                    # train with fake\r\n                    fake = self.netG(images)#this is a prob map which we want to be similar to y_onehot\r\n                    #print 'fake sz',fake.size()\r\n                    output = self.netD(fake.detach())#only for speed, so grads of netg are not computed\r\n                    errD_fake = F.binary_cross_entropy(output, label_zeros_var)\r\n                    \r\n                    errD_fake.backward()\r\n\r\n                    optimizerD.step()#update the parameters of netD\r\n\r\n                ############################\r\n                # Update G network\r\n                ###########################\r\n                self.netG.zero_grad()\r\n                if self.adversarial:\r\n                    output_D=self.netD(fake)\r\n                    output_G, GT,label_ones,output_D\r\n                    errG = self.loss_G(fake,GT, label_ones_var,output_D)#here we should use ones with the fakes\r\n                else:\r\n                    fake = self.netG(images)\r\n                    errG = self.loss_G(fake,GT)\r\n\r\n                \r\n                errG.backward()#backprop errors\r\n                optimizerG.step()#optimize only netG params\r\n\r\n                if i%10==0:\r\n                    print 'epoch  ',it\r\n                    print 'iteration ',i\r\n                    if self.adversarial:\r\n                        print 'error real ',errD_real.data                  \r\n                        print 'error fake ',errD_fake.data\r\n                    print 'error Generator ',errG.data\r\n\r\n            if it%5==0:\r\n                print 'testing ...'\r\n                name_img='0000047'\r\n                meanval=self.dst.mean_rgb\r\n                img_test_name=os.path.join(self.path_imgs,'iccv09Data',\"images_resized\",name_img+'.png')\r\n                lab_test_name=os.path.join(self.path_imgs,'iccv09Data',\"labels_resized\",name_img+'_label.png')\r\n                img = Image.open(img_test_name)\r\n                img = np.array(img, dtype=np.uint8)\r\n                label = Image.open(lab_test_name)\r\n                label = np.array(label, dtype=np.uint8)\r\n                img = img.astype(np.float32)\r\n                img -= meanval\r\n                img = img.transpose(2, 0, 1)\r\n\r\n                img_ = torch.from_numpy(img)#.float()\r\n                img_=img_.cuda()\r\n                img_var=Variable(img_)\r\n                prob_map = self.netG(img_var.view(1,img_.size(0),img_.size(1),img_.size(2)))\r\n                prob_np=prob_map.data.cpu().numpy()\r\n                prob_np=np.squeeze(prob_np)\r\n                label_out=np.argmax(prob_np,0)\r\n                print 'unique labout ',np.unique(label_out)\r\n                print 'probmap ',prob_map.size()\r\n                lab_visual=label2color(label_out)\r\n                imsave('out_color.png',lab_visual)\r\n                for idlabel in np.unique(label):\r\n                    diceratio=dice(label, label_out,idlabel)\r\n                    print 'dice id {} '.format(idlabel),diceratio\r\n\r\n\r\n    def loss_G(self,output_G, GT,label_ones=None,output_D=None):\r\n        fcnterm=CrossEntropy2d(output_G,GT)\r\n        if self.adversarial:\r\n            bceterm=F.binary_cross_entropy(output_D,label_ones)\r\n            return fcnterm+self.lam_adv*bceterm\r\n        else:\r\n            return fcnterm\r\n```\r\nI have tried it in 2 different machines and the same problem is happening.\r\nSomebody in the pytorch  [forum](https://discuss.pytorch.org/t/gpu-memory-not-returned/1311) told me that it was a problem of DataLoader.\r\nIs there a solution for that?\r\nThanks!"}