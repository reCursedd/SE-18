{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8820", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8820/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8820/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8820/events", "html_url": "https://github.com/pytorch/pytorch/issues/8820", "id": 335080083, "node_id": "MDU6SXNzdWUzMzUwODAwODM=", "number": 8820, "title": "Pytorch 0.4.0: Model behavior changes heavily after save and load weights", "user": {"login": "saturdays", "id": 5194117, "node_id": "MDQ6VXNlcjUxOTQxMTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5194117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saturdays", "html_url": "https://github.com/saturdays", "followers_url": "https://api.github.com/users/saturdays/followers", "following_url": "https://api.github.com/users/saturdays/following{/other_user}", "gists_url": "https://api.github.com/users/saturdays/gists{/gist_id}", "starred_url": "https://api.github.com/users/saturdays/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saturdays/subscriptions", "organizations_url": "https://api.github.com/users/saturdays/orgs", "repos_url": "https://api.github.com/users/saturdays/repos", "events_url": "https://api.github.com/users/saturdays/events{/privacy}", "received_events_url": "https://api.github.com/users/saturdays/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2018-06-23T08:08:38Z", "updated_at": "2018-06-27T22:08:50Z", "closed_at": "2018-06-27T22:08:50Z", "author_association": "NONE", "body_html": "<h1>Issue description</h1>\n<p>During training I save model by using: torch.save(model.state_dict(), file). And I reach 99% accuracy on both test and train data set.<br>\nThen, I load the model and test again using: model.load_state_dict(checkpoint, strict=True). The accuracy drops to 0.1%. An initial value!!</p>\n<p>My net is quite simple, so I print all the weights in the checkpoint both before and after validation. They are exactly the same.</p>\n<p>I thought the drop_out layer causes this issue, but remove it doesn't work either.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span># Code example</span>\nTo save:\n    model <span class=\"pl-k\">=</span> Net()\n    model <span class=\"pl-k\">=</span> Net().to(device)\n    optimizer <span class=\"pl-k\">=</span> torch.optim.SGD(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>args.lr,\n                                <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span>args.momentum,\n                                <span class=\"pl-v\">weight_decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-5</span>)\n    <span class=\"pl-k\">while</span> acc <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0.98</span>:\n        epoch <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        adjust_learning_rate(optimizer, epoch)\n        acc1 <span class=\"pl-k\">=</span> train(args, model, device, train_loader, optimizer, epoch)\n        acc2 <span class=\"pl-k\">=</span> test(args, model, device, test_loader)\n        acc <span class=\"pl-k\">=</span> (acc1<span class=\"pl-k\">+</span>acc2)<span class=\"pl-k\">*</span><span class=\"pl-c1\">0.5</span>\n        torch.save(model.state_dict(), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>checkpoints/mnist_gpu_<span class=\"pl-c1\">%d</span>.pkl<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">%</span>epoch)\n\nTo load:\n    model <span class=\"pl-k\">=</span> Net()\n    model.eval()\n    model <span class=\"pl-k\">=</span> Net().to(device)\n    checkpoint <span class=\"pl-k\">=</span> torch.load(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>checkpoints/mnist_gpu_20.pkl<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">for</span> k,v <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(checkpoint):\n        np.savetxt(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%s</span>_pre<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">%</span>v, checkpoint[v].cpu().numpy().reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n    model.load_state_dict(checkpoint, <span class=\"pl-v\">strict</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    acc2 <span class=\"pl-k\">=</span> test(args, model, device, test_loader)\n    checkpoint <span class=\"pl-k\">=</span> model.state_dict()\n    <span class=\"pl-k\">for</span> k,v <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(checkpoint):\n        np.savetxt(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%s</span>_after<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">%</span>v, checkpoint[v].cpu().numpy().reshape(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>model is super simple:</span>\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Fire</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,<span class=\"pl-smi\">inchn</span>,<span class=\"pl-smi\">sqzout_chn</span>,<span class=\"pl-smi\">exp1x1out_chn</span>,<span class=\"pl-smi\">exp3x3out_chn</span>):\n        <span class=\"pl-c1\">super</span>(Fire,<span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.inchn <span class=\"pl-k\">=</span> inchn\n        <span class=\"pl-c1\">self</span>.squeeze <span class=\"pl-k\">=</span> torch.nn.Conv2d(inchn,sqzout_chn,<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.squeeze_act <span class=\"pl-k\">=</span> torch.nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.expand1x1 <span class=\"pl-k\">=</span> torch.nn.Conv2d(sqzout_chn,exp1x1out_chn,<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.expand1x1_act <span class=\"pl-k\">=</span> torch.nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n        <span class=\"pl-c1\">self</span>.expand3x3 <span class=\"pl-k\">=</span> torch.nn.Conv2d(sqzout_chn,exp3x3out_chn,<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.expand3x3_act <span class=\"pl-k\">=</span> torch.nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.squeeze_act(<span class=\"pl-c1\">self</span>.squeeze(x))\n        <span class=\"pl-k\">return</span> torch.cat([\n                <span class=\"pl-c1\">self</span>.expand1x1_act(<span class=\"pl-c1\">self</span>.expand1x1(x)),\n                <span class=\"pl-c1\">self</span>.expand3x3_act(<span class=\"pl-c1\">self</span>.expand3x3(x))\n                ], <span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_class</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.num_class <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n        <span class=\"pl-c1\">self</span>.features <span class=\"pl-k\">=</span> torch.nn.Sequential(\n            torch.nn.Conv2d(<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">96</span>,<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>),\n            torch.nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),\n            torch.nn.MaxPool2d(<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">ceil_mode</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n            Fire(<span class=\"pl-c1\">96</span>,<span class=\"pl-c1\">16</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">64</span>),\n            torch.nn.MaxPool2d(<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">ceil_mode</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n            Fire(<span class=\"pl-c1\">128</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">128</span>,<span class=\"pl-c1\">128</span>),\n            torch.nn.MaxPool2d(<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">ceil_mode</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>),\n            Fire(<span class=\"pl-c1\">256</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">256</span>,<span class=\"pl-c1\">256</span>),\n        )\n        final_conv <span class=\"pl-k\">=</span> torch.nn.Conv2d(<span class=\"pl-c1\">512</span>,<span class=\"pl-c1\">self</span>.num_class,<span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.classifier <span class=\"pl-k\">=</span> torch.nn.Sequential(\n            final_conv,\n            torch.nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),\n            torch.nn.AdaptiveAvgPool2d(<span class=\"pl-c1\">1</span>)\n        )\n        <span class=\"pl-k\">for</span> m <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.modules():\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(m, torch.nn.Conv2d):\n                <span class=\"pl-k\">if</span> m <span class=\"pl-k\">is</span> final_conv:\n                    torch.nn.init.normal_(m.weight.data, <span class=\"pl-v\">mean</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>, <span class=\"pl-v\">std</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>)\n                <span class=\"pl-k\">else</span>:\n                    torch.nn.init.kaiming_uniform_(m.weight.data)\n                <span class=\"pl-k\">if</span> m.bias <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n                    m.bias.data.zero_()\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.features(x)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.classifier(x)\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.num_class)\n        <span class=\"pl-k\">return</span> x</pre></div>\n<h2>System Info</h2>\n<p>Please copy and paste the output from our<br>\nCollecting environment information...<br>\nPyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.3 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.44<br>\nGPU models and configuration:<br>\nGPU 0: TITAN X (Pascal)<br>\nGPU 1: TITAN X (Pascal)<br>\nGPU 2: TITAN X (Pascal)<br>\nGPU 3: TITAN X (Pascal)</p>\n<p>Nvidia driver version: 381.09<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.5<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.6<br>\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21<br>\n/usr/local/cuda-8.0/lib64/libcudnn_static.a<br>\n/usr/local/lib/libcudnn.so.7.0.5<br>\n/usr/local/lib/libcudnn_static.a<br>\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-a2b758a6.so.7.0.3<br>\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.13.3)<br>\n[pip3] numpydoc (0.7.0)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchvision (0.2.1)<br>\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch<br>\n[conda] torch                     0.4.0                     <br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>", "body_text": "Issue description\nDuring training I save model by using: torch.save(model.state_dict(), file). And I reach 99% accuracy on both test and train data set.\nThen, I load the model and test again using: model.load_state_dict(checkpoint, strict=True). The accuracy drops to 0.1%. An initial value!!\nMy net is quite simple, so I print all the weights in the checkpoint both before and after validation. They are exactly the same.\nI thought the drop_out layer causes this issue, but remove it doesn't work either.\n## Code example\nTo save:\n    model = Net()\n    model = Net().to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,\n                                momentum=args.momentum,\n                                weight_decay=1e-5)\n    while acc < 0.98:\n        epoch += 1\n        adjust_learning_rate(optimizer, epoch)\n        acc1 = train(args, model, device, train_loader, optimizer, epoch)\n        acc2 = test(args, model, device, test_loader)\n        acc = (acc1+acc2)*0.5\n        torch.save(model.state_dict(), 'checkpoints/mnist_gpu_%d.pkl'%epoch)\n\nTo load:\n    model = Net()\n    model.eval()\n    model = Net().to(device)\n    checkpoint = torch.load('checkpoints/mnist_gpu_20.pkl')\n    for k,v in enumerate(checkpoint):\n        np.savetxt('%s_pre'%v, checkpoint[v].cpu().numpy().reshape(-1))\n    model.load_state_dict(checkpoint, strict=True)\n    acc2 = test(args, model, device, test_loader)\n    checkpoint = model.state_dict()\n    for k,v in enumerate(checkpoint):\n        np.savetxt('%s_after'%v, checkpoint[v].cpu().numpy().reshape(-1))\n\n#model is super simple:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass Fire(torch.nn.Module):\n    def __init__(self,inchn,sqzout_chn,exp1x1out_chn,exp3x3out_chn):\n        super(Fire,self).__init__()\n        self.inchn = inchn\n        self.squeeze = torch.nn.Conv2d(inchn,sqzout_chn,kernel_size=1)\n        self.squeeze_act = torch.nn.ReLU(inplace=True)\n        self.expand1x1 = torch.nn.Conv2d(sqzout_chn,exp1x1out_chn,kernel_size=1)\n        self.expand1x1_act = torch.nn.ReLU(inplace=True)\n        self.expand3x3 = torch.nn.Conv2d(sqzout_chn,exp3x3out_chn,kernel_size=3, padding=1)\n        self.expand3x3_act = torch.nn.ReLU(inplace=True)\n    def forward(self, x):\n        x = self.squeeze_act(self.squeeze(x))\n        return torch.cat([\n                self.expand1x1_act(self.expand1x1(x)),\n                self.expand3x3_act(self.expand3x3(x))\n                ], 1)\nclass Net(nn.Module):\n    def __init__(self, num_class=10):\n        super(Net, self).__init__()\n        self.num_class = 10\n        self.features = torch.nn.Sequential(\n            torch.nn.Conv2d(3,96,kernel_size=3),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\n            Fire(96,16,64,64),\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\n            Fire(128,32,128,128),\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\n            Fire(256,64,256,256),\n        )\n        final_conv = torch.nn.Conv2d(512,self.num_class,kernel_size=1)\n        self.classifier = torch.nn.Sequential(\n            final_conv,\n            torch.nn.ReLU(inplace=True),\n            torch.nn.AdaptiveAvgPool2d(1)\n        )\n        for m in self.modules():\n            if isinstance(m, torch.nn.Conv2d):\n                if m is final_conv:\n                    torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.1)\n                else:\n                    torch.nn.init.kaiming_uniform_(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        x = x.view(-1, self.num_class)\n        return x\nSystem Info\nPlease copy and paste the output from our\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.3 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.44\nGPU models and configuration:\nGPU 0: TITAN X (Pascal)\nGPU 1: TITAN X (Pascal)\nGPU 2: TITAN X (Pascal)\nGPU 3: TITAN X (Pascal)\nNvidia driver version: 381.09\ncuDNN version: Probably one of the following:\n/usr/local/cuda-8.0/lib64/libcudnn.so\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\n/usr/local/lib/libcudnn.so.7.0.5\n/usr/local/lib/libcudnn_static.a\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-a2b758a6.so.7.0.3\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5\nVersions of relevant libraries:\n[pip3] numpy (1.13.3)\n[pip3] numpydoc (0.7.0)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.1)\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\n[conda] torch                     0.4.0                     \n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "# Issue description\r\nDuring training I save model by using: torch.save(model.state_dict(), file). And I reach 99% accuracy on both test and train data set.\r\nThen, I load the model and test again using: model.load_state_dict(checkpoint, strict=True). The accuracy drops to 0.1%. An initial value!!\r\n\r\nMy net is quite simple, so I print all the weights in the checkpoint both before and after validation. They are exactly the same.\r\n\r\nI thought the drop_out layer causes this issue, but remove it doesn't work either.\r\n\r\n```python\r\n## Code example\r\nTo save:\r\n    model = Net()\r\n    model = Net().to(device)\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,\r\n                                momentum=args.momentum,\r\n                                weight_decay=1e-5)\r\n    while acc < 0.98:\r\n        epoch += 1\r\n        adjust_learning_rate(optimizer, epoch)\r\n        acc1 = train(args, model, device, train_loader, optimizer, epoch)\r\n        acc2 = test(args, model, device, test_loader)\r\n        acc = (acc1+acc2)*0.5\r\n        torch.save(model.state_dict(), 'checkpoints/mnist_gpu_%d.pkl'%epoch)\r\n\r\nTo load:\r\n    model = Net()\r\n    model.eval()\r\n    model = Net().to(device)\r\n    checkpoint = torch.load('checkpoints/mnist_gpu_20.pkl')\r\n    for k,v in enumerate(checkpoint):\r\n        np.savetxt('%s_pre'%v, checkpoint[v].cpu().numpy().reshape(-1))\r\n    model.load_state_dict(checkpoint, strict=True)\r\n    acc2 = test(args, model, device, test_loader)\r\n    checkpoint = model.state_dict()\r\n    for k,v in enumerate(checkpoint):\r\n        np.savetxt('%s_after'%v, checkpoint[v].cpu().numpy().reshape(-1))\r\n\r\n#model is super simple:\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nclass Fire(torch.nn.Module):\r\n    def __init__(self,inchn,sqzout_chn,exp1x1out_chn,exp3x3out_chn):\r\n        super(Fire,self).__init__()\r\n        self.inchn = inchn\r\n        self.squeeze = torch.nn.Conv2d(inchn,sqzout_chn,kernel_size=1)\r\n        self.squeeze_act = torch.nn.ReLU(inplace=True)\r\n        self.expand1x1 = torch.nn.Conv2d(sqzout_chn,exp1x1out_chn,kernel_size=1)\r\n        self.expand1x1_act = torch.nn.ReLU(inplace=True)\r\n        self.expand3x3 = torch.nn.Conv2d(sqzout_chn,exp3x3out_chn,kernel_size=3, padding=1)\r\n        self.expand3x3_act = torch.nn.ReLU(inplace=True)\r\n    def forward(self, x):\r\n        x = self.squeeze_act(self.squeeze(x))\r\n        return torch.cat([\r\n                self.expand1x1_act(self.expand1x1(x)),\r\n                self.expand3x3_act(self.expand3x3(x))\r\n                ], 1)\r\nclass Net(nn.Module):\r\n    def __init__(self, num_class=10):\r\n        super(Net, self).__init__()\r\n        self.num_class = 10\r\n        self.features = torch.nn.Sequential(\r\n            torch.nn.Conv2d(3,96,kernel_size=3),\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\r\n            Fire(96,16,64,64),\r\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\r\n            Fire(128,32,128,128),\r\n            torch.nn.MaxPool2d(kernel_size=2, ceil_mode=False),\r\n            Fire(256,64,256,256),\r\n        )\r\n        final_conv = torch.nn.Conv2d(512,self.num_class,kernel_size=1)\r\n        self.classifier = torch.nn.Sequential(\r\n            final_conv,\r\n            torch.nn.ReLU(inplace=True),\r\n            torch.nn.AdaptiveAvgPool2d(1)\r\n        )\r\n        for m in self.modules():\r\n            if isinstance(m, torch.nn.Conv2d):\r\n                if m is final_conv:\r\n                    torch.nn.init.normal_(m.weight.data, mean=0.0, std=0.1)\r\n                else:\r\n                    torch.nn.init.kaiming_uniform_(m.weight.data)\r\n                if m.bias is not None:\r\n                    m.bias.data.zero_()\r\n    def forward(self, x):\r\n        x = self.features(x)\r\n        x = self.classifier(x)\r\n        x = x.view(-1, self.num_class)\r\n        return x\r\n```\r\n\r\n## System Info\r\nPlease copy and paste the output from our\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.44\r\nGPU models and configuration:\r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\nGPU 2: TITAN X (Pascal)\r\nGPU 3: TITAN X (Pascal)\r\n\r\nNvidia driver version: 381.09\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5.1.10\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.6.0.21\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/lib/libcudnn.so.7.0.5\r\n/usr/local/lib/libcudnn_static.a\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn-a2b758a6.so.7.0.3\r\n/usr/local/lib/python3.5/dist-packages/torch/lib/libcudnn-900fef33.so.7.0.5\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.13.3)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py36_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n\r\n"}