{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5147", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5147/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5147/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5147/events", "html_url": "https://github.com/pytorch/pytorch/pull/5147", "id": 295719499, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY4MTI4MzQz", "number": 5147, "title": "Added check and test for betas parameter in Adam optimizer", "user": {"login": "lazypanda1", "id": 35884075, "node_id": "MDQ6VXNlcjM1ODg0MDc1", "avatar_url": "https://avatars0.githubusercontent.com/u/35884075?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lazypanda1", "html_url": "https://github.com/lazypanda1", "followers_url": "https://api.github.com/users/lazypanda1/followers", "following_url": "https://api.github.com/users/lazypanda1/following{/other_user}", "gists_url": "https://api.github.com/users/lazypanda1/gists{/gist_id}", "starred_url": "https://api.github.com/users/lazypanda1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lazypanda1/subscriptions", "organizations_url": "https://api.github.com/users/lazypanda1/orgs", "repos_url": "https://api.github.com/users/lazypanda1/repos", "events_url": "https://api.github.com/users/lazypanda1/events{/privacy}", "received_events_url": "https://api.github.com/users/lazypanda1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-02-09T00:55:58Z", "updated_at": "2018-11-23T15:39:25Z", "closed_at": "2018-02-12T01:24:43Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5147", "html_url": "https://github.com/pytorch/pytorch/pull/5147", "diff_url": "https://github.com/pytorch/pytorch/pull/5147.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5147.patch"}, "body_html": "<p>This PR adds a check to prevent division by zero errors and give users friendlier error messages when using the Adam optimizer.</p>\n<p>Currently, if one specifies the beta value of the Adam optimizer as <code>1.0</code> for the first parameter, the program fails with the error message, <code>ZeroDivisionError: float division by zero</code>. According to the definition of <a href=\"https://arxiv.org/abs/1412.6980\" rel=\"nofollow\">Adam</a>, beta values should be in the range [0, 1).</p>\n<p>Also referring <a href=\"https://github.com/uber/pyro/pull/751\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/uber/pyro/pull/751/hovercard\">#751</a> where I encountered and tried to fix this issue in the first place. I believe this would benefit all clients using pytorch as backend.</p>", "body_text": "This PR adds a check to prevent division by zero errors and give users friendlier error messages when using the Adam optimizer.\nCurrently, if one specifies the beta value of the Adam optimizer as 1.0 for the first parameter, the program fails with the error message, ZeroDivisionError: float division by zero. According to the definition of Adam, beta values should be in the range [0, 1).\nAlso referring #751 where I encountered and tried to fix this issue in the first place. I believe this would benefit all clients using pytorch as backend.", "body": "This PR adds a check to prevent division by zero errors and give users friendlier error messages when using the Adam optimizer.\r\n\r\nCurrently, if one specifies the beta value of the Adam optimizer as `1.0` for the first parameter, the program fails with the error message, `ZeroDivisionError: float division by zero`. According to the definition of [Adam](https://arxiv.org/abs/1412.6980), beta values should be in the range \\[0, 1\\).\r\n\r\nAlso referring [#751](https://github.com/uber/pyro/pull/751) where I encountered and tried to fix this issue in the first place. I believe this would benefit all clients using pytorch as backend."}