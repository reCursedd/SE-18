{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/255617424", "html_url": "https://github.com/pytorch/pytorch/issues/139#issuecomment-255617424", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/139", "id": 255617424, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NTYxNzQyNA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-23T21:56:15Z", "updated_at": "2016-10-23T21:56:31Z", "author_association": "MEMBER", "body_html": "<p>I can't find any solution for the host memory sharing in the thread you liked, they only solved the device memory sharing. We should probably ask how <code>cudaHostRegisterPortable</code> behaves if one of the processes frees the memory, but another one is still using it.</p>\n<p>On the other hand, if you could get a big pinned buffer mapped as shared memory, you could slice it and send the chunks to worker processes that would fill them. Once you move your tensor to shared memory, it will be the same for it and its slices until the program terminates (or it gets freed).</p>", "body_text": "I can't find any solution for the host memory sharing in the thread you liked, they only solved the device memory sharing. We should probably ask how cudaHostRegisterPortable behaves if one of the processes frees the memory, but another one is still using it.\nOn the other hand, if you could get a big pinned buffer mapped as shared memory, you could slice it and send the chunks to worker processes that would fill them. Once you move your tensor to shared memory, it will be the same for it and its slices until the program terminates (or it gets freed).", "body": "I can't find any solution for the host memory sharing in the thread you liked, they only solved the device memory sharing. We should probably ask how `cudaHostRegisterPortable` behaves if one of the processes frees the memory, but another one is still using it.\n\nOn the other hand, if you could get a big pinned buffer mapped as shared memory, you could slice it and send the chunks to worker processes that would fill them. Once you move your tensor to shared memory, it will be the same for it and its slices until the program terminates (or it gets freed).\n"}