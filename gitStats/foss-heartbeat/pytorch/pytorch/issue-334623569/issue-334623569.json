{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8755", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8755/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8755/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8755/events", "html_url": "https://github.com/pytorch/pytorch/issues/8755", "id": 334623569, "node_id": "MDU6SXNzdWUzMzQ2MjM1Njk=", "number": 8755, "title": "[JIT] Weird attribute bug in VariableType::s_th_add", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jramseyer", "id": 33807912, "node_id": "MDQ6VXNlcjMzODA3OTEy", "avatar_url": "https://avatars2.githubusercontent.com/u/33807912?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jramseyer", "html_url": "https://github.com/jramseyer", "followers_url": "https://api.github.com/users/jramseyer/followers", "following_url": "https://api.github.com/users/jramseyer/following{/other_user}", "gists_url": "https://api.github.com/users/jramseyer/gists{/gist_id}", "starred_url": "https://api.github.com/users/jramseyer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jramseyer/subscriptions", "organizations_url": "https://api.github.com/users/jramseyer/orgs", "repos_url": "https://api.github.com/users/jramseyer/repos", "events_url": "https://api.github.com/users/jramseyer/events{/privacy}", "received_events_url": "https://api.github.com/users/jramseyer/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jramseyer", "id": 33807912, "node_id": "MDQ6VXNlcjMzODA3OTEy", "avatar_url": "https://avatars2.githubusercontent.com/u/33807912?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jramseyer", "html_url": "https://github.com/jramseyer", "followers_url": "https://api.github.com/users/jramseyer/followers", "following_url": "https://api.github.com/users/jramseyer/following{/other_user}", "gists_url": "https://api.github.com/users/jramseyer/gists{/gist_id}", "starred_url": "https://api.github.com/users/jramseyer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jramseyer/subscriptions", "organizations_url": "https://api.github.com/users/jramseyer/orgs", "repos_url": "https://api.github.com/users/jramseyer/repos", "events_url": "https://api.github.com/users/jramseyer/events{/privacy}", "received_events_url": "https://api.github.com/users/jramseyer/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-06-21T19:19:31Z", "updated_at": "2018-07-13T17:25:59Z", "closed_at": "2018-07-13T17:25:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Example:</p>\n<pre><code>import torch\n\n# This is how you define a traced function\n# Pass in an example input to this decorator and then apply it to the function\n@torch.jit.trace(torch.rand(()))\ndef traced_fn(x):\n    return torch.abs(2*x)\n\n# This is how you define a script function\n# Apply this decorator directly to the function\n@torch.jit.script\ndef script_fn(x):\n    z = torch.ones([1], dtype=torch.int64)\n    for i in range(x):\n        z = z * (i + 1)\n    return z\n\n# This is a normal module that can be traced.\nclass TracedModule(torch.nn.Module):\n    def forward(self, x):\n        x = x.type(torch.float32)\n        return torch.floor(torch.sqrt(x) / 5.)\n\n# This is how you define a scripted module.\n# The module should inherit from ScriptModule and the forward should have the\n# script_method decorator applied to it.\nclass ScriptModule(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x):\n        r = -x\n        if torch.fmod(x, 2.0) == 0.0:\n            r = x / 2.0\n        return r\n\n# This is a demonstration net that calls all of the different types of\n# methods and functions\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Modules must be attributes on the Module because if you want to trace\n        # or script this Module, we must be able to inherit the submodules'\n        # params.\n        self.traced_module = torch.jit.trace(torch.rand(()))(TracedModule())\n        self.script_module = ScriptModule()\n\n        print('traced_fn graph', traced_fn.graph)\n        print('script_fn graph', script_fn.graph)\n        print('TracedModule graph', self.traced_module.__getattr__('forward').graph)\n        print('ScriptModule graph', self.script_module.__getattr__('forward').graph)\n\n    def forward(self, x):\n        # Call a traced function\n        x = traced_fn(x)\n\n        # Call a script function\n        import pdb; pdb.set_trace()\n        x = script_fn(x)\n\n        # Call a traced submodule\n        x = self.traced_module(x)\n\n        # Call a scripted submodule\n        x = self.script_module(x)\n\n        return x\n\n\n# Instantiate this net and run it\nn = Net()\n# print(n(torch.tensor([5]))) # 190.\n\nn_traced = torch.jit.trace(torch.tensor([5]))(n)\nprint(n_traced(torch.tensor([5])))\n</code></pre>\n<p>The trace of the base module fails with:</p>\n<pre><code>Traceback (most recent call last):\n  File \"frontend_demo.py\", line 71, in &lt;module&gt;\n    n_traced = torch.jit.trace(torch.tensor([5]))(n)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py\", line 305, in wrapper\n    module._create_method_from_trace('forward', func, args)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"frontend_demo.py\", line 56, in forward\n    x = script_fn(x)\nRuntimeError: /Users/jamesreed/onnx-fairseq/pytorch/torch/csrc/jit/attributes.h:180: t_: Assertion `!v.defined() || !v.is_variable()` failed.\n</code></pre>\n<p>Stack trace:</p>\n<pre><code>  * frame #0: 0x00007fff70c3a1f6 libc++abi.dylib`__cxa_throw\n    frame #1: 0x000000010875d4ce _C.cpython-36m-darwin.so`torch::barf(fmt=\"%s:%u: %s: Assertion `%s` failed.\") at assertions.cpp:18\n    frame #2: 0x0000000108839e70 _C.cpython-36m-darwin.so`torch::jit::Attributes&lt;torch::jit::Node&gt;::t_(this=0x000060f00000de78, name=(value = 1627389961), v=torch::jit::ScalarAttributeValue&lt;at::Tensor, torch::jit::AttributeKind::t&gt;::ConstructorType @ 0x00007ffeefbf6858) atattributes.h:180\n    frame #3: 0x00000001098c4b86 _C.cpython-36m-darwin.so`torch::autograd::setattr(n=0x000060f00000de70, name=(value = 1627389961), v=0x00007ffeefbf6d20) at VariableType.cpp:44\n    frame #4: 0x0000000109967cbe _C.cpython-36m-darwin.so`torch::autograd::VariableType::s_th_add(this=0x000060600000abe0, self=0x00007ffeefbf6d70, other=0x00007ffeefbf6d58, alpha=Scalar @ 0x00007ffeefbf6d20) const at VariableType.cpp:7116\n    frame #5: 0x000000010c2fc3e9 libcaffe2.dylib`at::Type::th_add(this=0x000060600000abe0, self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf6f48) const at Type.cpp:1275\n    frame #6: 0x000000010bfed2cf libcaffe2.dylib`at::th_add(self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7020) at Functions.h:1694\n    frame #7: 0x000000010bfecfec libcaffe2.dylib`at::native::add(self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7268) at LegacyBridge.cpp:161\n    frame #8: 0x000000010c343c5d libcaffe2.dylib`at::Type::add(this=0x000060600000abe0, self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf72f0) const at Type.cpp:4679\n    frame #9: 0x0000000109b922e3 _C.cpython-36m-darwin.so`torch::autograd::VariableType::add(this=0x000060600000abe0, self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7398) const at VariableType.cpp:29021\n    frame #10: 0x0000000108d75b5e _C.cpython-36m-darwin.so`at::add(self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7400) at Functions.h:4305\n    frame #11: 0x0000000108d78186 _C.cpython-36m-darwin.so`torch::jit::(anonymous namespace)::$_177::operator(this=0x000061f0000b0248, stack=size=1)(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)::operator()(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;) const at aten_dispatch.cpp:2433\n    frame #12: 0x0000000108d7806d _C.cpython-36m-darwin.so`int std::__1::__invoke_void_return_wrapper&lt;int&gt;::__call&lt;torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)&amp;, std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;&gt;(torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)&amp;&amp;&amp;, std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;&amp;&amp;) [inlined] decltype(__f=0x000061f0000b0248, __args=size=1)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)&amp;&gt;(fp)(std::__1::forward&lt;std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;&gt;(fp0))) std::__1::__invoke&lt;torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)&amp;, std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;&gt;(torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)&amp;&amp;&amp;, std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;&amp;&amp;) at type_traits:4323\n    frame #13: 0x0000000108d78050 _C.cpython-36m-darwin.so`int std::__1::__invoke_void_return_wrapper&lt;int&gt;::__call&lt;torch::jit::(anonymous namespace)::$_177::operator(__args=0x000061f0000b0248, __args=size=1)(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)&amp;, std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;&gt;(torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)&amp;&amp;&amp;, std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;&amp;&amp;) at __functional_base:318\n    frame #14: 0x0000000108d77f64 _C.cpython-36m-darwin.so`std::__1::__function::__func&lt;torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;), std::__1::allocator&lt;torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt;&gt;&amp;)&gt;, int (std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)&gt;::operator(this=0x000061f0000b0240, __arg=size=1)(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;) at functional:1562\n    frame #15: 0x00000001088815fe _C.cpython-36m-darwin.so`std::__1::function&lt;int (std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;)&gt;::operator(this=0x000061f0000b0240, __arg=size=1)(std::__1::vector&lt;at::Tensor, std::__1::allocator&lt;at::Tensor&gt; &gt;&amp;) const at functional:1921\n    frame #16: 0x000000010883289b _C.cpython-36m-darwin.so`torch::jit::InterpreterStateImpl::runOneStage(this=0x0000608000034020, stack=size=1) at interpreter.cpp:877\n    frame #17: 0x0000000108832758 _C.cpython-36m-darwin.so`torch::jit::InterpreterState::runOneStage(this=0x00007ffeefbf7778, stack=size=1) at interpreter.cpp:958\n    frame #18: 0x00000001088fea85 _C.cpython-36m-darwin.so`torch::jit::GraphExecutorImpl::runFallback(this=0x000060f0000072a0, inputs=variable_tensor_list @ 0x00007ffeefbf78e0) at graph_executor.cpp:311\n    frame #19: 0x00000001088fe0dd _C.cpython-36m-darwin.so`torch::jit::GraphExecutorImpl::runTraced(this=0x000060f0000072a0, inputs=variable_tensor_list @ 0x00007ffeefbf7de0) at graph_executor.cpp:280\n    frame #20: 0x00000001088fadce _C.cpython-36m-darwin.so`torch::jit::GraphExecutorImpl::run(this=0x000060f0000072a0, inputs=variable_tensor_list @ 0x00007ffeefbf8148) at graph_executor.cpp:213\n    frame #21: 0x00000001088fa59e _C.cpython-36m-darwin.so`torch::jit::GraphExecutor::run(this=0x0000602000009750, inputs=0x00007ffeefbf8238) at graph_executor.cpp:540\n    frame #22: 0x000000010882dfe4 _C.cpython-36m-darwin.so`torch::jit::initJITBindings(this=0x000060c00000d338, ge=0x0000602000009750, args=&lt;unavailable&gt;)::$_20::operator()(torch::jit::GraphExecutor&amp;, pybind11::args) const at init.cpp:164\n    frame #23: 0x000000010882df13 _C.cpython-36m-darwin.so`pybind11::object pybind11::detail::argument_loader&lt;torch::jit::GraphExecutor&amp;, pybind11::args&gt;::call_impl&lt;pybind11::object, torch::jit::initJITBindings(this=0x00007ffeefbf8440, f=0x000060c00000d338, (null)=index_sequence&lt;0, 1&gt; @ 0x00007ffeefbf8340, (null)=0x00007ffeefbf83a0)::$_20&amp;, 0ul, 1ul, pybind11::detail::void_type&gt;(torch::jit::initJITBindings(_object*)::$_20&amp;&amp;&amp;, pybind11::detail::index_sequence&lt;0ul, 1ul&gt;, pybind11::detail::void_type&amp;&amp;) at cast.h:1866\n    frame #24: 0x000000010882de38 _C.cpython-36m-darwin.so`std::__1::enable_if&lt;!(std::is_void&lt;pybind11::object&gt;::value), pybind11::object&gt;::type pybind11::detail::argument_loader&lt;torch::jit::GraphExecutor&amp;, pybind11::args&gt;::call&lt;pybind11::object, pybind11::detail::void_type, torch::jit::initJITBindings(this=0x00007ffeefbf8440, f=0x000060c00000d338)::$_20&amp;&gt;(torch::jit::initJITBindings(_object*)::$_20&amp;&amp;&amp;) at cast.h:1843\n    frame #25: 0x000000010882dd41 _C.cpython-36m-darwin.so`void pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::$_20, pybind11::object, torch::jit::GraphExecutor&amp;, pybind11::args, pybind11::name, pybind11::is_method, pybind11::sibling&gt;(this=0x000000010882dc30, call=0x00007ffeefbf8da8)::$_20&amp;&amp;, pybind11::object (*)(torch::jit::GraphExecutor&amp;, pybind11::args), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;)::'lambda'(pybind11::detail::function_call&amp;)::operator()(pybind11::detail::function_call&amp;) const at pybind11.h:155\n    frame #26: 0x000000010882dc48 _C.cpython-36m-darwin.so`void pybind11::cpp_function::initialize&lt;torch::jit::initJITBindings(_object*)::$_20, pybind11::object, torch::jit::GraphExecutor&amp;, pybind11::args, pybind11::name, pybind11::is_method, pybind11::sibling&gt;(call=0x00007ffeefbf8da8)::$_20&amp;&amp;, pybind11::object (*)(torch::jit::GraphExecutor&amp;, pybind11::args), pybind11::name const&amp;, pybind11::is_method const&amp;, pybind11::sibling const&amp;)::'lambda'(pybind11::detail::function_call&amp;)::__invoke(pybind11::detail::function_call&amp;) at pybind11.h:132\n    frame #27: 0x00000001085f1697 _C.cpython-36m-darwin.so`pybind11::cpp_function::dispatcher(self=0x000000010dea4990, args_in=0x000000010f328108, kwargs_in=0x0000000000000000) at pybind11.h:619\n</code></pre>", "body_text": "Example:\nimport torch\n\n# This is how you define a traced function\n# Pass in an example input to this decorator and then apply it to the function\n@torch.jit.trace(torch.rand(()))\ndef traced_fn(x):\n    return torch.abs(2*x)\n\n# This is how you define a script function\n# Apply this decorator directly to the function\n@torch.jit.script\ndef script_fn(x):\n    z = torch.ones([1], dtype=torch.int64)\n    for i in range(x):\n        z = z * (i + 1)\n    return z\n\n# This is a normal module that can be traced.\nclass TracedModule(torch.nn.Module):\n    def forward(self, x):\n        x = x.type(torch.float32)\n        return torch.floor(torch.sqrt(x) / 5.)\n\n# This is how you define a scripted module.\n# The module should inherit from ScriptModule and the forward should have the\n# script_method decorator applied to it.\nclass ScriptModule(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x):\n        r = -x\n        if torch.fmod(x, 2.0) == 0.0:\n            r = x / 2.0\n        return r\n\n# This is a demonstration net that calls all of the different types of\n# methods and functions\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Modules must be attributes on the Module because if you want to trace\n        # or script this Module, we must be able to inherit the submodules'\n        # params.\n        self.traced_module = torch.jit.trace(torch.rand(()))(TracedModule())\n        self.script_module = ScriptModule()\n\n        print('traced_fn graph', traced_fn.graph)\n        print('script_fn graph', script_fn.graph)\n        print('TracedModule graph', self.traced_module.__getattr__('forward').graph)\n        print('ScriptModule graph', self.script_module.__getattr__('forward').graph)\n\n    def forward(self, x):\n        # Call a traced function\n        x = traced_fn(x)\n\n        # Call a script function\n        import pdb; pdb.set_trace()\n        x = script_fn(x)\n\n        # Call a traced submodule\n        x = self.traced_module(x)\n\n        # Call a scripted submodule\n        x = self.script_module(x)\n\n        return x\n\n\n# Instantiate this net and run it\nn = Net()\n# print(n(torch.tensor([5]))) # 190.\n\nn_traced = torch.jit.trace(torch.tensor([5]))(n)\nprint(n_traced(torch.tensor([5])))\n\nThe trace of the base module fails with:\nTraceback (most recent call last):\n  File \"frontend_demo.py\", line 71, in <module>\n    n_traced = torch.jit.trace(torch.tensor([5]))(n)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py\", line 305, in wrapper\n    module._create_method_from_trace('forward', func, args)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py\", line 468, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"frontend_demo.py\", line 56, in forward\n    x = script_fn(x)\nRuntimeError: /Users/jamesreed/onnx-fairseq/pytorch/torch/csrc/jit/attributes.h:180: t_: Assertion `!v.defined() || !v.is_variable()` failed.\n\nStack trace:\n  * frame #0: 0x00007fff70c3a1f6 libc++abi.dylib`__cxa_throw\n    frame #1: 0x000000010875d4ce _C.cpython-36m-darwin.so`torch::barf(fmt=\"%s:%u: %s: Assertion `%s` failed.\") at assertions.cpp:18\n    frame #2: 0x0000000108839e70 _C.cpython-36m-darwin.so`torch::jit::Attributes<torch::jit::Node>::t_(this=0x000060f00000de78, name=(value = 1627389961), v=torch::jit::ScalarAttributeValue<at::Tensor, torch::jit::AttributeKind::t>::ConstructorType @ 0x00007ffeefbf6858) atattributes.h:180\n    frame #3: 0x00000001098c4b86 _C.cpython-36m-darwin.so`torch::autograd::setattr(n=0x000060f00000de70, name=(value = 1627389961), v=0x00007ffeefbf6d20) at VariableType.cpp:44\n    frame #4: 0x0000000109967cbe _C.cpython-36m-darwin.so`torch::autograd::VariableType::s_th_add(this=0x000060600000abe0, self=0x00007ffeefbf6d70, other=0x00007ffeefbf6d58, alpha=Scalar @ 0x00007ffeefbf6d20) const at VariableType.cpp:7116\n    frame #5: 0x000000010c2fc3e9 libcaffe2.dylib`at::Type::th_add(this=0x000060600000abe0, self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf6f48) const at Type.cpp:1275\n    frame #6: 0x000000010bfed2cf libcaffe2.dylib`at::th_add(self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7020) at Functions.h:1694\n    frame #7: 0x000000010bfecfec libcaffe2.dylib`at::native::add(self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7268) at LegacyBridge.cpp:161\n    frame #8: 0x000000010c343c5d libcaffe2.dylib`at::Type::add(this=0x000060600000abe0, self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf72f0) const at Type.cpp:4679\n    frame #9: 0x0000000109b922e3 _C.cpython-36m-darwin.so`torch::autograd::VariableType::add(this=0x000060600000abe0, self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7398) const at VariableType.cpp:29021\n    frame #10: 0x0000000108d75b5e _C.cpython-36m-darwin.so`at::add(self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7400) at Functions.h:4305\n    frame #11: 0x0000000108d78186 _C.cpython-36m-darwin.so`torch::jit::(anonymous namespace)::$_177::operator(this=0x000061f0000b0248, stack=size=1)(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)::operator()(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&) const at aten_dispatch.cpp:2433\n    frame #12: 0x0000000108d7806d _C.cpython-36m-darwin.so`int std::__1::__invoke_void_return_wrapper<int>::__call<torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&>(torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&&&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&&) [inlined] decltype(__f=0x000061f0000b0248, __args=size=1)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&>(fp)(std::__1::forward<std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&>(fp0))) std::__1::__invoke<torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&>(torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&&&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&&) at type_traits:4323\n    frame #13: 0x0000000108d78050 _C.cpython-36m-darwin.so`int std::__1::__invoke_void_return_wrapper<int>::__call<torch::jit::(anonymous namespace)::$_177::operator(__args=0x000061f0000b0248, __args=size=1)(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&>(torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&&&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&&) at __functional_base:318\n    frame #14: 0x0000000108d77f64 _C.cpython-36m-darwin.so`std::__1::__function::__func<torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&), std::__1::allocator<torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor>>&)>, int (std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)>::operator(this=0x000061f0000b0240, __arg=size=1)(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&) at functional:1562\n    frame #15: 0x00000001088815fe _C.cpython-36m-darwin.so`std::__1::function<int (std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)>::operator(this=0x000061f0000b0240, __arg=size=1)(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&) const at functional:1921\n    frame #16: 0x000000010883289b _C.cpython-36m-darwin.so`torch::jit::InterpreterStateImpl::runOneStage(this=0x0000608000034020, stack=size=1) at interpreter.cpp:877\n    frame #17: 0x0000000108832758 _C.cpython-36m-darwin.so`torch::jit::InterpreterState::runOneStage(this=0x00007ffeefbf7778, stack=size=1) at interpreter.cpp:958\n    frame #18: 0x00000001088fea85 _C.cpython-36m-darwin.so`torch::jit::GraphExecutorImpl::runFallback(this=0x000060f0000072a0, inputs=variable_tensor_list @ 0x00007ffeefbf78e0) at graph_executor.cpp:311\n    frame #19: 0x00000001088fe0dd _C.cpython-36m-darwin.so`torch::jit::GraphExecutorImpl::runTraced(this=0x000060f0000072a0, inputs=variable_tensor_list @ 0x00007ffeefbf7de0) at graph_executor.cpp:280\n    frame #20: 0x00000001088fadce _C.cpython-36m-darwin.so`torch::jit::GraphExecutorImpl::run(this=0x000060f0000072a0, inputs=variable_tensor_list @ 0x00007ffeefbf8148) at graph_executor.cpp:213\n    frame #21: 0x00000001088fa59e _C.cpython-36m-darwin.so`torch::jit::GraphExecutor::run(this=0x0000602000009750, inputs=0x00007ffeefbf8238) at graph_executor.cpp:540\n    frame #22: 0x000000010882dfe4 _C.cpython-36m-darwin.so`torch::jit::initJITBindings(this=0x000060c00000d338, ge=0x0000602000009750, args=<unavailable>)::$_20::operator()(torch::jit::GraphExecutor&, pybind11::args) const at init.cpp:164\n    frame #23: 0x000000010882df13 _C.cpython-36m-darwin.so`pybind11::object pybind11::detail::argument_loader<torch::jit::GraphExecutor&, pybind11::args>::call_impl<pybind11::object, torch::jit::initJITBindings(this=0x00007ffeefbf8440, f=0x000060c00000d338, (null)=index_sequence<0, 1> @ 0x00007ffeefbf8340, (null)=0x00007ffeefbf83a0)::$_20&, 0ul, 1ul, pybind11::detail::void_type>(torch::jit::initJITBindings(_object*)::$_20&&&, pybind11::detail::index_sequence<0ul, 1ul>, pybind11::detail::void_type&&) at cast.h:1866\n    frame #24: 0x000000010882de38 _C.cpython-36m-darwin.so`std::__1::enable_if<!(std::is_void<pybind11::object>::value), pybind11::object>::type pybind11::detail::argument_loader<torch::jit::GraphExecutor&, pybind11::args>::call<pybind11::object, pybind11::detail::void_type, torch::jit::initJITBindings(this=0x00007ffeefbf8440, f=0x000060c00000d338)::$_20&>(torch::jit::initJITBindings(_object*)::$_20&&&) at cast.h:1843\n    frame #25: 0x000000010882dd41 _C.cpython-36m-darwin.so`void pybind11::cpp_function::initialize<torch::jit::initJITBindings(_object*)::$_20, pybind11::object, torch::jit::GraphExecutor&, pybind11::args, pybind11::name, pybind11::is_method, pybind11::sibling>(this=0x000000010882dc30, call=0x00007ffeefbf8da8)::$_20&&, pybind11::object (*)(torch::jit::GraphExecutor&, pybind11::args), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const at pybind11.h:155\n    frame #26: 0x000000010882dc48 _C.cpython-36m-darwin.so`void pybind11::cpp_function::initialize<torch::jit::initJITBindings(_object*)::$_20, pybind11::object, torch::jit::GraphExecutor&, pybind11::args, pybind11::name, pybind11::is_method, pybind11::sibling>(call=0x00007ffeefbf8da8)::$_20&&, pybind11::object (*)(torch::jit::GraphExecutor&, pybind11::args), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) at pybind11.h:132\n    frame #27: 0x00000001085f1697 _C.cpython-36m-darwin.so`pybind11::cpp_function::dispatcher(self=0x000000010dea4990, args_in=0x000000010f328108, kwargs_in=0x0000000000000000) at pybind11.h:619", "body": "Example:\r\n```\r\nimport torch\r\n\r\n# This is how you define a traced function\r\n# Pass in an example input to this decorator and then apply it to the function\r\n@torch.jit.trace(torch.rand(()))\r\ndef traced_fn(x):\r\n    return torch.abs(2*x)\r\n\r\n# This is how you define a script function\r\n# Apply this decorator directly to the function\r\n@torch.jit.script\r\ndef script_fn(x):\r\n    z = torch.ones([1], dtype=torch.int64)\r\n    for i in range(x):\r\n        z = z * (i + 1)\r\n    return z\r\n\r\n# This is a normal module that can be traced.\r\nclass TracedModule(torch.nn.Module):\r\n    def forward(self, x):\r\n        x = x.type(torch.float32)\r\n        return torch.floor(torch.sqrt(x) / 5.)\r\n\r\n# This is how you define a scripted module.\r\n# The module should inherit from ScriptModule and the forward should have the\r\n# script_method decorator applied to it.\r\nclass ScriptModule(torch.jit.ScriptModule):\r\n    @torch.jit.script_method\r\n    def forward(self, x):\r\n        r = -x\r\n        if torch.fmod(x, 2.0) == 0.0:\r\n            r = x / 2.0\r\n        return r\r\n\r\n# This is a demonstration net that calls all of the different types of\r\n# methods and functions\r\nclass Net(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        # Modules must be attributes on the Module because if you want to trace\r\n        # or script this Module, we must be able to inherit the submodules'\r\n        # params.\r\n        self.traced_module = torch.jit.trace(torch.rand(()))(TracedModule())\r\n        self.script_module = ScriptModule()\r\n\r\n        print('traced_fn graph', traced_fn.graph)\r\n        print('script_fn graph', script_fn.graph)\r\n        print('TracedModule graph', self.traced_module.__getattr__('forward').graph)\r\n        print('ScriptModule graph', self.script_module.__getattr__('forward').graph)\r\n\r\n    def forward(self, x):\r\n        # Call a traced function\r\n        x = traced_fn(x)\r\n\r\n        # Call a script function\r\n        import pdb; pdb.set_trace()\r\n        x = script_fn(x)\r\n\r\n        # Call a traced submodule\r\n        x = self.traced_module(x)\r\n\r\n        # Call a scripted submodule\r\n        x = self.script_module(x)\r\n\r\n        return x\r\n\r\n\r\n# Instantiate this net and run it\r\nn = Net()\r\n# print(n(torch.tensor([5]))) # 190.\r\n\r\nn_traced = torch.jit.trace(torch.tensor([5]))(n)\r\nprint(n_traced(torch.tensor([5])))\r\n```\r\n\r\nThe trace of the base module fails with:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"frontend_demo.py\", line 71, in <module>\r\n    n_traced = torch.jit.trace(torch.tensor([5]))(n)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/jit/__init__.py\", line 305, in wrapper\r\n    module._create_method_from_trace('forward', func, args)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/nn/modules/module.py\", line 468, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"frontend_demo.py\", line 56, in forward\r\n    x = script_fn(x)\r\nRuntimeError: /Users/jamesreed/onnx-fairseq/pytorch/torch/csrc/jit/attributes.h:180: t_: Assertion `!v.defined() || !v.is_variable()` failed.\r\n```\r\n\r\nStack trace:\r\n\r\n```\r\n  * frame #0: 0x00007fff70c3a1f6 libc++abi.dylib`__cxa_throw\r\n    frame #1: 0x000000010875d4ce _C.cpython-36m-darwin.so`torch::barf(fmt=\"%s:%u: %s: Assertion `%s` failed.\") at assertions.cpp:18\r\n    frame #2: 0x0000000108839e70 _C.cpython-36m-darwin.so`torch::jit::Attributes<torch::jit::Node>::t_(this=0x000060f00000de78, name=(value = 1627389961), v=torch::jit::ScalarAttributeValue<at::Tensor, torch::jit::AttributeKind::t>::ConstructorType @ 0x00007ffeefbf6858) atattributes.h:180\r\n    frame #3: 0x00000001098c4b86 _C.cpython-36m-darwin.so`torch::autograd::setattr(n=0x000060f00000de70, name=(value = 1627389961), v=0x00007ffeefbf6d20) at VariableType.cpp:44\r\n    frame #4: 0x0000000109967cbe _C.cpython-36m-darwin.so`torch::autograd::VariableType::s_th_add(this=0x000060600000abe0, self=0x00007ffeefbf6d70, other=0x00007ffeefbf6d58, alpha=Scalar @ 0x00007ffeefbf6d20) const at VariableType.cpp:7116\r\n    frame #5: 0x000000010c2fc3e9 libcaffe2.dylib`at::Type::th_add(this=0x000060600000abe0, self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf6f48) const at Type.cpp:1275\r\n    frame #6: 0x000000010bfed2cf libcaffe2.dylib`at::th_add(self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7020) at Functions.h:1694\r\n    frame #7: 0x000000010bfecfec libcaffe2.dylib`at::native::add(self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7268) at LegacyBridge.cpp:161\r\n    frame #8: 0x000000010c343c5d libcaffe2.dylib`at::Type::add(this=0x000060600000abe0, self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf72f0) const at Type.cpp:4679\r\n    frame #9: 0x0000000109b922e3 _C.cpython-36m-darwin.so`torch::autograd::VariableType::add(this=0x000060600000abe0, self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7398) const at VariableType.cpp:29021\r\n    frame #10: 0x0000000108d75b5e _C.cpython-36m-darwin.so`at::add(self=0x000060300004c810, other=0x000060300004c818, alpha=Scalar @ 0x00007ffeefbf7400) at Functions.h:4305\r\n    frame #11: 0x0000000108d78186 _C.cpython-36m-darwin.so`torch::jit::(anonymous namespace)::$_177::operator(this=0x000061f0000b0248, stack=size=1)(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)::operator()(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&) const at aten_dispatch.cpp:2433\r\n    frame #12: 0x0000000108d7806d _C.cpython-36m-darwin.so`int std::__1::__invoke_void_return_wrapper<int>::__call<torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&>(torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&&&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&&) [inlined] decltype(__f=0x000061f0000b0248, __args=size=1)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&>(fp)(std::__1::forward<std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&>(fp0))) std::__1::__invoke<torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&>(torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&&&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&&) at type_traits:4323\r\n    frame #13: 0x0000000108d78050 _C.cpython-36m-darwin.so`int std::__1::__invoke_void_return_wrapper<int>::__call<torch::jit::(anonymous namespace)::$_177::operator(__args=0x000061f0000b0248, __args=size=1)(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&>(torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)&&&, std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&&&) at __functional_base:318\r\n    frame #14: 0x0000000108d77f64 _C.cpython-36m-darwin.so`std::__1::__function::__func<torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&), std::__1::allocator<torch::jit::(anonymous namespace)::$_177::operator()(torch::jit::Node*) const::'lambda'(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor>>&)>, int (std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)>::operator(this=0x000061f0000b0240, __arg=size=1)(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&) at functional:1562\r\n    frame #15: 0x00000001088815fe _C.cpython-36m-darwin.so`std::__1::function<int (std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&)>::operator(this=0x000061f0000b0240, __arg=size=1)(std::__1::vector<at::Tensor, std::__1::allocator<at::Tensor> >&) const at functional:1921\r\n    frame #16: 0x000000010883289b _C.cpython-36m-darwin.so`torch::jit::InterpreterStateImpl::runOneStage(this=0x0000608000034020, stack=size=1) at interpreter.cpp:877\r\n    frame #17: 0x0000000108832758 _C.cpython-36m-darwin.so`torch::jit::InterpreterState::runOneStage(this=0x00007ffeefbf7778, stack=size=1) at interpreter.cpp:958\r\n    frame #18: 0x00000001088fea85 _C.cpython-36m-darwin.so`torch::jit::GraphExecutorImpl::runFallback(this=0x000060f0000072a0, inputs=variable_tensor_list @ 0x00007ffeefbf78e0) at graph_executor.cpp:311\r\n    frame #19: 0x00000001088fe0dd _C.cpython-36m-darwin.so`torch::jit::GraphExecutorImpl::runTraced(this=0x000060f0000072a0, inputs=variable_tensor_list @ 0x00007ffeefbf7de0) at graph_executor.cpp:280\r\n    frame #20: 0x00000001088fadce _C.cpython-36m-darwin.so`torch::jit::GraphExecutorImpl::run(this=0x000060f0000072a0, inputs=variable_tensor_list @ 0x00007ffeefbf8148) at graph_executor.cpp:213\r\n    frame #21: 0x00000001088fa59e _C.cpython-36m-darwin.so`torch::jit::GraphExecutor::run(this=0x0000602000009750, inputs=0x00007ffeefbf8238) at graph_executor.cpp:540\r\n    frame #22: 0x000000010882dfe4 _C.cpython-36m-darwin.so`torch::jit::initJITBindings(this=0x000060c00000d338, ge=0x0000602000009750, args=<unavailable>)::$_20::operator()(torch::jit::GraphExecutor&, pybind11::args) const at init.cpp:164\r\n    frame #23: 0x000000010882df13 _C.cpython-36m-darwin.so`pybind11::object pybind11::detail::argument_loader<torch::jit::GraphExecutor&, pybind11::args>::call_impl<pybind11::object, torch::jit::initJITBindings(this=0x00007ffeefbf8440, f=0x000060c00000d338, (null)=index_sequence<0, 1> @ 0x00007ffeefbf8340, (null)=0x00007ffeefbf83a0)::$_20&, 0ul, 1ul, pybind11::detail::void_type>(torch::jit::initJITBindings(_object*)::$_20&&&, pybind11::detail::index_sequence<0ul, 1ul>, pybind11::detail::void_type&&) at cast.h:1866\r\n    frame #24: 0x000000010882de38 _C.cpython-36m-darwin.so`std::__1::enable_if<!(std::is_void<pybind11::object>::value), pybind11::object>::type pybind11::detail::argument_loader<torch::jit::GraphExecutor&, pybind11::args>::call<pybind11::object, pybind11::detail::void_type, torch::jit::initJITBindings(this=0x00007ffeefbf8440, f=0x000060c00000d338)::$_20&>(torch::jit::initJITBindings(_object*)::$_20&&&) at cast.h:1843\r\n    frame #25: 0x000000010882dd41 _C.cpython-36m-darwin.so`void pybind11::cpp_function::initialize<torch::jit::initJITBindings(_object*)::$_20, pybind11::object, torch::jit::GraphExecutor&, pybind11::args, pybind11::name, pybind11::is_method, pybind11::sibling>(this=0x000000010882dc30, call=0x00007ffeefbf8da8)::$_20&&, pybind11::object (*)(torch::jit::GraphExecutor&, pybind11::args), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const at pybind11.h:155\r\n    frame #26: 0x000000010882dc48 _C.cpython-36m-darwin.so`void pybind11::cpp_function::initialize<torch::jit::initJITBindings(_object*)::$_20, pybind11::object, torch::jit::GraphExecutor&, pybind11::args, pybind11::name, pybind11::is_method, pybind11::sibling>(call=0x00007ffeefbf8da8)::$_20&&, pybind11::object (*)(torch::jit::GraphExecutor&, pybind11::args), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) at pybind11.h:132\r\n    frame #27: 0x00000001085f1697 _C.cpython-36m-darwin.so`pybind11::cpp_function::dispatcher(self=0x000000010dea4990, args_in=0x000000010f328108, kwargs_in=0x0000000000000000) at pybind11.h:619\r\n```"}