{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/372160315", "html_url": "https://github.com/pytorch/pytorch/pull/5481#issuecomment-372160315", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5481", "id": 372160315, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjE2MDMxNQ==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-11T23:41:01Z", "updated_at": "2018-03-12T05:14:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>After offline discussion I might restrict this PR to float types. float sum/prod would still be accumulated into float instead of double, since this is <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCGenerateFloatType.h#L8\">consistent with CUDA</a>. For operations that accumulate over some dimensions instead of all dimensions integer types we can't accumulate into larger types since the resulting tensor will be of the same type, so don't need to worry about this. All of this could be added in another PR.</p>", "body_text": "After offline discussion I might restrict this PR to float types. float sum/prod would still be accumulated into float instead of double, since this is consistent with CUDA. For operations that accumulate over some dimensions instead of all dimensions integer types we can't accumulate into larger types since the resulting tensor will be of the same type, so don't need to worry about this. All of this could be added in another PR.", "body": "After offline discussion I might restrict this PR to float types. float sum/prod would still be accumulated into float instead of double, since this is [consistent with CUDA](https://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCGenerateFloatType.h#L8). For operations that accumulate over some dimensions instead of all dimensions integer types we can't accumulate into larger types since the resulting tensor will be of the same type, so don't need to worry about this. All of this could be added in another PR."}