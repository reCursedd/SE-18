{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/172461430", "pull_request_review_id": 101475892, "id": 172461430, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MjQ2MTQzMA==", "diff_hunk": "@@ -0,0 +1,29 @@\n+#pragma once\n+#include \"vec256.h\"\n+#include \"parallel.h\"\n+\n+#include \"ATen/ATen.h\"\n+#include \"ATen/ExpandUtils.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/WrapDimUtils.h\"\n+\n+#include <algorithm>\n+#include <functional>\n+#include <numeric>\n+#include <vector>\n+\n+#include <iostream>\n+#include <map>\n+\n+\n+namespace at {\n+namespace native {\n+\n+using sumImplF = void(Tensor &, const Tensor &, size_t, bool);\n+sumImplF sumImplAVX;\n+sumImplF sumImplAVX2;\n+sumImplF sumImplNONE;", "path": "aten/src/ATen/native/cpu/ReduceOpsKernel.h", "position": null, "original_position": 25, "commit_id": "92e4e0671ee0326a1744e3b3b2c3c57694eae39d", "original_commit_id": "cc65e71c0b223ad75cf0d077d825e33ddd4c1cc6", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I'm starting to wonder if this won't cause us missing symbols if you'll be on a machine that doesn't support AVX2 - these functions are only compiled if the current CPU supports their capabilities. We might want to switch them to be always built (even if the current machine can't run them). Also, if we decide to do this, it will probably be clearer to use the specific capability as an enum template argument (both to implementation functions, and as an extra template arg to `vec256`).", "created_at": "2018-03-06T09:50:47Z", "updated_at": "2018-11-23T15:40:23Z", "html_url": "https://github.com/pytorch/pytorch/pull/5481#discussion_r172461430", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5481", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/172461430"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5481#discussion_r172461430"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5481"}}, "body_html": "<p>I'm starting to wonder if this won't cause us missing symbols if you'll be on a machine that doesn't support AVX2 - these functions are only compiled if the current CPU supports their capabilities. We might want to switch them to be always built (even if the current machine can't run them). Also, if we decide to do this, it will probably be clearer to use the specific capability as an enum template argument (both to implementation functions, and as an extra template arg to <code>vec256</code>).</p>", "body_text": "I'm starting to wonder if this won't cause us missing symbols if you'll be on a machine that doesn't support AVX2 - these functions are only compiled if the current CPU supports their capabilities. We might want to switch them to be always built (even if the current machine can't run them). Also, if we decide to do this, it will probably be clearer to use the specific capability as an enum template argument (both to implementation functions, and as an extra template arg to vec256)."}