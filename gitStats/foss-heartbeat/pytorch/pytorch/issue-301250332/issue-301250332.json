{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5481", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5481/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5481/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5481/events", "html_url": "https://github.com/pytorch/pytorch/pull/5481", "id": 301250332, "node_id": "MDExOlB1bGxSZXF1ZXN0MTcyMTIzNzIy", "number": 5481, "title": "ATen ReduceOps", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 24, "created_at": "2018-03-01T01:53:28Z", "updated_at": "2018-11-23T15:40:38Z", "closed_at": "2018-03-12T19:19:12Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/5481", "html_url": "https://github.com/pytorch/pytorch/pull/5481", "diff_url": "https://github.com/pytorch/pytorch/pull/5481.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/5481.patch"}, "body_html": "<p>This diff adds vectorization to ATen. It uses intel intrinsics to build a general vec256 class, that represents types of 256bit width. These can then be treated like regular variables. Using those it implements torch.sum() for the contiguous case. It uses Intel TBB for multithreading, which allows workstealing and chunks the reduction operations based on a experimentally chosen value (_THRESHOLD). It uses cpuinfo to pick the right code depending on the host's capabilities.</p>\n<p>The kernels are implemented under native/cpu. Each .cpp file is compiled with -avx, -avx2 and no additional flags. A macro is used to append AVX, AVX2 or NONE to the function name. The header then needs to define the functions three times, one for each capability. This could be improved by either changing the cmake file a bit or possibly generating source code using a Python script etc.</p>\n<p>For the non-contiguous case this defaults to the current implementation within TH. For CUDA is entirely defaults to the implementation within THC.</p>\n<p>There probably needs to be a bit of a debate around the design decisions here, the additional dependencies, parallelization strategy, clarity, etc. The numerical results also diverge from numpy with larger tensors, which is expected since we're summing, for example, 8 numbers and then adding the result to the running sum, instead of each number one by one. But there might be something to be said about accumulating into a double for floats or the degree of divergence, the behavior with respect to CUDA, etc.</p>\n<p>I wrote a <a href=\"https://github.com/cpuhrsch/benchmark/blob/sumall/benchmarks/sum_bench.py\">small Python script</a> to compare the results with numpy numerically as well as on timing. I ran this script to create timings both on master and this branch.</p>\n<p>Here is the command for 1 core<br>\n<code>OMP_NUM_THREAD=1 taskset -c 0 python sum_bench.py --enable_numpy 200</code></p>\n<p>Here is the command for all cores<br>\n<code>python sum_bench.py --enable_numpy 200</code></p>\n<p>Here are the results of each:</p>\n<p><a href=\"https://paste.fedoraproject.org/paste/Nho9JzHpPVK9av8a6mByjQ\" rel=\"nofollow\">Master, 1 core</a></p>\n<p><a href=\"https://paste.fedoraproject.org/paste/6xLHkYvcVJx9z~5MoHxN4w\" rel=\"nofollow\">This branch, 1 core</a></p>\n<p><a href=\"https://paste.fedoraproject.org/paste/5l3V1d5zGqvJcMXIUteMRw\" rel=\"nofollow\">Master, all cores</a></p>\n<p><a href=\"https://paste.fedoraproject.org/paste/J4RuDU-0Drz0aZwtphQwEA\" rel=\"nofollow\">This branch, all cores</a></p>\n<p>To test the command is<br>\n<code>python sum_bench.py --test 200</code></p>\n<p><a href=\"https://paste.fedoraproject.org/paste/kTEoUC~oWgXA6XWMAfNfNw\" rel=\"nofollow\">This branch, test results</a></p>\n<p>For this test we look at the average absolute value of the differences. This does not take into account the relative magnitude of the numbers. The numbers are sampled from a standard normal distribution.</p>\n<p>In terms of performance this diff should bring PyTorch on par with Numpy and usually exceed it by 1.5 to 2x.</p>", "body_text": "This diff adds vectorization to ATen. It uses intel intrinsics to build a general vec256 class, that represents types of 256bit width. These can then be treated like regular variables. Using those it implements torch.sum() for the contiguous case. It uses Intel TBB for multithreading, which allows workstealing and chunks the reduction operations based on a experimentally chosen value (_THRESHOLD). It uses cpuinfo to pick the right code depending on the host's capabilities.\nThe kernels are implemented under native/cpu. Each .cpp file is compiled with -avx, -avx2 and no additional flags. A macro is used to append AVX, AVX2 or NONE to the function name. The header then needs to define the functions three times, one for each capability. This could be improved by either changing the cmake file a bit or possibly generating source code using a Python script etc.\nFor the non-contiguous case this defaults to the current implementation within TH. For CUDA is entirely defaults to the implementation within THC.\nThere probably needs to be a bit of a debate around the design decisions here, the additional dependencies, parallelization strategy, clarity, etc. The numerical results also diverge from numpy with larger tensors, which is expected since we're summing, for example, 8 numbers and then adding the result to the running sum, instead of each number one by one. But there might be something to be said about accumulating into a double for floats or the degree of divergence, the behavior with respect to CUDA, etc.\nI wrote a small Python script to compare the results with numpy numerically as well as on timing. I ran this script to create timings both on master and this branch.\nHere is the command for 1 core\nOMP_NUM_THREAD=1 taskset -c 0 python sum_bench.py --enable_numpy 200\nHere is the command for all cores\npython sum_bench.py --enable_numpy 200\nHere are the results of each:\nMaster, 1 core\nThis branch, 1 core\nMaster, all cores\nThis branch, all cores\nTo test the command is\npython sum_bench.py --test 200\nThis branch, test results\nFor this test we look at the average absolute value of the differences. This does not take into account the relative magnitude of the numbers. The numbers are sampled from a standard normal distribution.\nIn terms of performance this diff should bring PyTorch on par with Numpy and usually exceed it by 1.5 to 2x.", "body": "This diff adds vectorization to ATen. It uses intel intrinsics to build a general vec256 class, that represents types of 256bit width. These can then be treated like regular variables. Using those it implements torch.sum() for the contiguous case. It uses Intel TBB for multithreading, which allows workstealing and chunks the reduction operations based on a experimentally chosen value (_THRESHOLD). It uses cpuinfo to pick the right code depending on the host's capabilities.\r\n\r\nThe kernels are implemented under native/cpu. Each .cpp file is compiled with -avx, -avx2 and no additional flags. A macro is used to append AVX, AVX2 or NONE to the function name. The header then needs to define the functions three times, one for each capability. This could be improved by either changing the cmake file a bit or possibly generating source code using a Python script etc.\r\n\r\nFor the non-contiguous case this defaults to the current implementation within TH. For CUDA is entirely defaults to the implementation within THC.\r\n\r\nThere probably needs to be a bit of a debate around the design decisions here, the additional dependencies, parallelization strategy, clarity, etc. The numerical results also diverge from numpy with larger tensors, which is expected since we're summing, for example, 8 numbers and then adding the result to the running sum, instead of each number one by one. But there might be something to be said about accumulating into a double for floats or the degree of divergence, the behavior with respect to CUDA, etc.\r\n\r\nI wrote a [small Python script]( https://github.com/cpuhrsch/benchmark/blob/sumall/benchmarks/sum_bench.py) to compare the results with numpy numerically as well as on timing. I ran this script to create timings both on master and this branch.\r\n\r\nHere is the command for 1 core\r\n`OMP_NUM_THREAD=1 taskset -c 0 python sum_bench.py --enable_numpy 200`\r\n\r\nHere is the command for all cores\r\n`python sum_bench.py --enable_numpy 200`\r\n\r\nHere are the results of each:\r\n\r\n[Master, 1 core](https://paste.fedoraproject.org/paste/Nho9JzHpPVK9av8a6mByjQ)\r\n\r\n[This branch, 1 core](https://paste.fedoraproject.org/paste/6xLHkYvcVJx9z~5MoHxN4w)\r\n\r\n[Master, all cores](https://paste.fedoraproject.org/paste/5l3V1d5zGqvJcMXIUteMRw)\r\n\r\n[This branch, all cores](https://paste.fedoraproject.org/paste/J4RuDU-0Drz0aZwtphQwEA)\r\n\r\nTo test the command is\r\n`python sum_bench.py --test 200`\r\n\r\n[This branch, test results](https://paste.fedoraproject.org/paste/kTEoUC~oWgXA6XWMAfNfNw)\r\n\r\nFor this test we look at the average absolute value of the differences. This does not take into account the relative magnitude of the numbers. The numbers are sampled from a standard normal distribution. \r\n\r\nIn terms of performance this diff should bring PyTorch on par with Numpy and usually exceed it by 1.5 to 2x.\r\n\r\n\r\n"}