{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/369666881", "html_url": "https://github.com/pytorch/pytorch/pull/5481#issuecomment-369666881", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5481", "id": 369666881, "node_id": "MDEyOklzc3VlQ29tbWVudDM2OTY2Njg4MQ==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-01T17:29:12Z", "updated_at": "2018-03-01T17:30:04Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> TBB comes with a bunch of extra goodies OpenMP doesn't aside from being able to parallelise for loops.</p>\n<p>It has access to <a href=\"https://software.intel.com/en-us/node/506140\" rel=\"nofollow\">more sophisticated parallelisation strategies</a>, allows scheduling within <a href=\"https://software.intel.com/en-us/node/506359\" rel=\"nofollow\">threadpools of different sizes</a> at runtime with a lot less overhead, allows to schedule with <a href=\"https://software.intel.com/en-us/node/506061\" rel=\"nofollow\">cache affinity</a>, allows <a href=\"https://software.intel.com/en-us/node/506111\" rel=\"nofollow\">work stealing</a> by default (I know OpenMP has <a href=\"https://software.intel.com/en-us/articles/openmp-loop-scheduling\" rel=\"nofollow\">similar strategies</a>), but more importantly it comes with <a href=\"https://software.intel.com/en-us/node/506169\" rel=\"nofollow\">nice containers</a>, interesting algorithms (<a href=\"https://software.intel.com/en-us/node/506167\" rel=\"nofollow\">parallel sort</a> and <a href=\"https://software.intel.com/en-us/node/506156\" rel=\"nofollow\">parallel scan</a>) and is a prerequisite for <a href=\"https://software.intel.com/en-us/get-started-with-pstl\" rel=\"nofollow\">parallel stl</a> which implements a lot of STL functionality in parallel.</p>\n<p>Having said that, I think we should separate out the parallelization into a separate file and provide a general purpose API for our purposes that's independent of any particular backend.</p>\n<p>I also like the better dispatch code. I think in an ideal world the user doesn't worry about the kind of instruction sets her code is compiled with and just uses the template. The sum algorithm etc., in my opinion, should also be generalized to allow for many other operations such as product so we can reduce the code even more. We'll probably spend quite a bit of time on this, or might even decide to do the parallelization in a different PR.</p>", "body_text": "@apaszke TBB comes with a bunch of extra goodies OpenMP doesn't aside from being able to parallelise for loops.\nIt has access to more sophisticated parallelisation strategies, allows scheduling within threadpools of different sizes at runtime with a lot less overhead, allows to schedule with cache affinity, allows work stealing by default (I know OpenMP has similar strategies), but more importantly it comes with nice containers, interesting algorithms (parallel sort and parallel scan) and is a prerequisite for parallel stl which implements a lot of STL functionality in parallel.\nHaving said that, I think we should separate out the parallelization into a separate file and provide a general purpose API for our purposes that's independent of any particular backend.\nI also like the better dispatch code. I think in an ideal world the user doesn't worry about the kind of instruction sets her code is compiled with and just uses the template. The sum algorithm etc., in my opinion, should also be generalized to allow for many other operations such as product so we can reduce the code even more. We'll probably spend quite a bit of time on this, or might even decide to do the parallelization in a different PR.", "body": "@apaszke TBB comes with a bunch of extra goodies OpenMP doesn't aside from being able to parallelise for loops.\r\n\r\nIt has access to [more sophisticated parallelisation strategies](https://software.intel.com/en-us/node/506140), allows scheduling within [threadpools of different sizes](https://software.intel.com/en-us/node/506359) at runtime with a lot less overhead, allows to schedule with [cache affinity](https://software.intel.com/en-us/node/506061), allows [work stealing](https://software.intel.com/en-us/node/506111) by default (I know OpenMP has [similar strategies](https://software.intel.com/en-us/articles/openmp-loop-scheduling)), but more importantly it comes with [nice containers](https://software.intel.com/en-us/node/506169), interesting algorithms ([parallel sort](https://software.intel.com/en-us/node/506167) and [parallel scan](https://software.intel.com/en-us/node/506156)) and is a prerequisite for [parallel stl](https://software.intel.com/en-us/get-started-with-pstl) which implements a lot of STL functionality in parallel.\r\n\r\nHaving said that, I think we should separate out the parallelization into a separate file and provide a general purpose API for our purposes that's independent of any particular backend.\r\n\r\nI also like the better dispatch code. I think in an ideal world the user doesn't worry about the kind of instruction sets her code is compiled with and just uses the template. The sum algorithm etc., in my opinion, should also be generalized to allow for many other operations such as product so we can reduce the code even more. We'll probably spend quite a bit of time on this, or might even decide to do the parallelization in a different PR."}