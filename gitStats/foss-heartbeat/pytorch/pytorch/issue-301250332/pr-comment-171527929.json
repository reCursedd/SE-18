{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/171527929", "pull_request_review_id": 100376540, "id": 171527929, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MTUyNzkyOQ==", "diff_hunk": "@@ -0,0 +1,91 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/ExpandUtils.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/WrapDimUtils.h\"\n+#include \"ATen/cpu/cpuinfo/include/cpuinfo.h\"\n+#include \"cpu/ReduceOpsKernel.h\"\n+\n+#include <algorithm>\n+#include <functional>\n+#include <numeric>\n+#include <vector>\n+\n+#include <map>\n+\n+namespace at {\n+namespace native {\n+\n+sumallF sumallDispatch;\n+sumallF *sumall = &sumallDispatch;\n+void sumallDispatch(Tensor &result, const Tensor &self) {\n+  cpuinfo_initialize();\n+  sumall = &sumallNONE;\n+  if (cpuinfo_has_x86_avx()) {\n+    sumall = &sumallAVX;\n+  }\n+  if (cpuinfo_has_x86_avx2()) {\n+    sumall = &sumallAVX2;\n+  }\n+  return (*sumall)(result, self);\n+}\n+\n+reducesumF reducesumDispatch;\n+reducesumF *reducesum = &reducesumDispatch;\n+void reducesumDispatch(Tensor &result, const Tensor &self, size_t dim) {\n+  cpuinfo_initialize();\n+  reducesum = &reducesumNONE;\n+  if (cpuinfo_has_x86_avx()) {\n+    reducesum = &reducesumAVX;\n+  }\n+  if (cpuinfo_has_x86_avx2()) {\n+    reducesum = &reducesumAVX2;\n+  }\n+  return (*reducesum)(result, self, dim);\n+}", "path": "aten/src/ATen/native/ReduceOps.cpp", "position": null, "original_position": 45, "commit_id": "92e4e0671ee0326a1744e3b3b2c3c57694eae39d", "original_commit_id": "06ba5c4d25a59834104b22aa83f02566f5db7760", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I played a bit with ways of listing the possible implementations and came up with [something like this](https://godbolt.org/g/rqKjtL). The implementation is quite short, and everything that's needed to implement the dispatch of `sumAll` is this:\r\n```cpp\r\nusing sum_type = void(const float *a, float *b);\r\nextern sum_type sumAllAVX;\r\nextern sum_type sumAllNone;\r\nstruct ImplSpec<sum_type> sumAllImpls[] = {\r\n  { CPUCapability::AVX,  &sumAllAVX   },\r\n  { CPUCapability::NONE, &sumAllNone  },\r\n};\r\nFnPtr<sum_type> sumAll = &DispatchStub<sum_type>::init<sumAllImpls, &sumAll>;\r\n```\r\nIt will automatically generate a stub that will check CPU capability when called for the first time, and later replace the function pointer with the best concrete implementation.", "created_at": "2018-03-01T11:15:05Z", "updated_at": "2018-11-23T15:40:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/5481#discussion_r171527929", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5481", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/171527929"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5481#discussion_r171527929"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5481"}}, "body_html": "<p>I played a bit with ways of listing the possible implementations and came up with <a href=\"https://godbolt.org/g/rqKjtL\" rel=\"nofollow\">something like this</a>. The implementation is quite short, and everything that's needed to implement the dispatch of <code>sumAll</code> is this:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">using</span> sum_type = <span class=\"pl-k\">void</span>(<span class=\"pl-k\">const</span> <span class=\"pl-k\">float</span> *a, <span class=\"pl-k\">float</span> *b);\n<span class=\"pl-k\">extern</span> sum_type sumAllAVX;\n<span class=\"pl-k\">extern</span> sum_type sumAllNone;\n<span class=\"pl-k\">struct</span> <span class=\"pl-en\">ImplSpec</span>&lt;sum_type&gt; sumAllImpls[] = {\n  { CPUCapability::AVX,  &amp;sumAllAVX   },\n  { CPUCapability::NONE, &amp;sumAllNone  },\n};\nFnPtr&lt;sum_type&gt; sumAll = &amp;DispatchStub&lt;sum_type&gt;::init&lt;sumAllImpls, &amp;sumAll&gt;;</pre></div>\n<p>It will automatically generate a stub that will check CPU capability when called for the first time, and later replace the function pointer with the best concrete implementation.</p>", "body_text": "I played a bit with ways of listing the possible implementations and came up with something like this. The implementation is quite short, and everything that's needed to implement the dispatch of sumAll is this:\nusing sum_type = void(const float *a, float *b);\nextern sum_type sumAllAVX;\nextern sum_type sumAllNone;\nstruct ImplSpec<sum_type> sumAllImpls[] = {\n  { CPUCapability::AVX,  &sumAllAVX   },\n  { CPUCapability::NONE, &sumAllNone  },\n};\nFnPtr<sum_type> sumAll = &DispatchStub<sum_type>::init<sumAllImpls, &sumAll>;\nIt will automatically generate a stub that will check CPU capability when called for the first time, and later replace the function pointer with the best concrete implementation."}