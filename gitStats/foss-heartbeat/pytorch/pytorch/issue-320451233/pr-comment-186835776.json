{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186835776", "pull_request_review_id": 118443026, "id": 186835776, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4NjgzNTc3Ng==", "diff_hunk": "@@ -260,97 +263,272 @@ at::Tensor getAttributeValue(const NamedValue& nv) {\n   return getConstantValue(nv.loc, v);\n }\n \n-std::shared_ptr<SugaredValue> emitBuiltinCall(\n+Value* createConstant(Graph& g, const SourceRange& loc, const at::Tensor& val) {\n+  auto n = g.createConstant(val);\n+  n->setSourceLocation(std::make_shared<SourceRange>(loc));\n+  return g.insertNode(n)->output();\n+}\n+\n+Value* createStack(Graph& g, const SourceRange& loc, at::ArrayRef<Value*> inputs) {\n+  // bake in constant propagation for the all-constant case because it is\n+  // common to see constant lists like [1, 2] passed to attributes\n+  bool all_constant = std::all_of(inputs.begin(), inputs.end(), [&](Value* v) {\n+    return v->node()->kind() == prim::Constant;\n+  });\n+  if(all_constant) {\n+    auto values = fmap(inputs, [&](Value* v) {\n+      return v->node()->t(attr::value);\n+    });\n+    return createConstant(g, loc, at::stack(values));\n+  }\n+  return g.insertNode(g.create(aten::stack, inputs)\n+                      ->i_(attr::dim, 0)\n+                      ->setSourceLocation(std::make_shared<SourceRange>(loc)))->output();\n+}\n+\n+static bool isTensorSubtype(Value* v) {\n+  return v->type()->isSubtypeOf(*DynamicType::get());\n+}\n+\n+// if a value is a constant then try to turn into type T using the\n+// same rules as the interpreter\n+template<typename T>\n+at::optional<T> constant_as(Value* v) {\n+  if(v->node()->kind() != prim::Constant)\n+    return at::nullopt;\n+  auto tensor = v->node()->t(attr::value);\n+  try {\n+    return tensor_as<T>(std::move(tensor));\n+  } catch (tensor_conversion_error& err) {\n+    return at::nullopt;\n+  }\n+}\n+\n+// try to turn constant inputs into attributes\n+void liftConstantAttributes(const FunctionSchema& schema, Node* node) {\n+  // we shouldn't start with attributes, just inputs\n+  JIT_ASSERT(!node->hasAttributes());\n+  std::vector<Value*> new_inputs;\n+  Attributes<Node> attributes;\n+  for(size_t i = 0; i < node->inputs().size(); ++i) {\n+    const auto& arg = schema.arguments[i];\n+    auto input = node->input(i);\n+    if(arg.attribute_kind) {\n+      switch(*arg.attribute_kind) {\n+        case AttributeKind::i: {\n+          auto r = constant_as<int64_t>(input);\n+          if(!r)\n+            return;\n+          attributes.i_(Symbol::attr(arg.name), *r);\n+        } break;\n+        case AttributeKind::is: {\n+          auto r = constant_as<at::IntList>(input);\n+          if(!r)\n+            return;\n+          attributes.is_(Symbol::attr(arg.name), *r);\n+        } break;\n+        case AttributeKind::f: {\n+          auto r = constant_as<double>(input);\n+          if(!r)\n+            return;\n+          attributes.f_(Symbol::attr(arg.name), *r);\n+        } break;\n+        case AttributeKind::t: {\n+          auto r = constant_as<at::Tensor>(input);\n+          if(!r)\n+            return;\n+          attributes.t_(Symbol::attr(arg.name), *r);\n+        } break;\n+        default:\n+          barf(\"AttributeKind not handled in LiftConstantAttributes file a bug report.\");\n+          return;\n+      }\n+    } else {\n+      new_inputs.push_back(input);\n+    }\n+  }\n+  // nothing changed no need to modify the node\n+  if(!attributes.hasAttributes())\n+    return;\n+\n+  node->removeAllInputs();\n+  for(Value* input : new_inputs) {\n+    node->addInput(input);\n+  }\n+  node->copyAttributes(attributes);\n+}\n+\n+\n+static std::shared_ptr<SugaredValue> tryEmitSchema(\n+  const FunctionSchema& schema,\n+  std::stringstream& failure_messages,\n   const SourceRange& loc,\n   Method& method,\n   const std::string & name,\n-  at::ArrayRef<Value*> inputs,\n-  at::ArrayRef<NamedValue> attributes,\n-  // if true, emitBuiltinCall will throw an exception if this builtin does not exist,\n-  // otherwise it will return nullptr if the builtin is not found.\n-  bool required) {\n+  at::ArrayRef<NamedValue> inputs,\n+  at::ArrayRef<NamedValue> attributes) {\n+\n+  auto err = [&]() -> std::ostream& {\n+    failure_messages << \"\\nfor operator \" << schema << \":\\n\";\n+    return failure_messages;\n+  };\n \n-  NodeKind kind(Symbol::aten(name)); // TODO: this is a guess; could it be jit?\n   auto graph = method.graph();\n-  auto n = graph->insertNode(graph->create(kind, inputs, 0))\n-                ->setSourceLocation(std::make_shared<SourceRange>(loc));\n+  std::vector<at::optional<NamedValue>> positional_inputs(schema.arguments.size(), at::nullopt);\n \n-  for (const auto& attr : attributes) {\n-    const auto& name = Symbol::attr(attr.name);\n-    auto v = getAttributeValue(attr).toBackend(at::kCPU).contiguous();\n-    if(at::isFloatingType(v.type().scalarType())) {\n-      v = v.toType(at::kDouble);\n-      if(v.ndimension() == 0) {\n-        n->f_(name, v.toCDouble());\n-      } else {\n-        n->fs_(name, at::ArrayRef<double>(v.data<double>(), v.size(0)));\n-      }\n-    } else {\n-      v = v.toType(at::kLong);\n-      if(v.ndimension() == 0) {\n-        n->i_(name, v.toCLong());\n-      } else {\n-        n->is_(name, at::ArrayRef<int64_t>(v.data<int64_t>(), v.size(0)));\n-      }\n-    }\n+  size_t total_inputs = attributes.size() + inputs.size();\n+  if(total_inputs > schema.arguments.size()) {\n+    err() << \"expected at most \" << schema.arguments.size() << \" arguments \"\n+    << \" but found \" << total_inputs << \"\\n\" << loc << \"\\n\";\n+    return nullptr;\n   }\n-  auto op = findTensorOp(n);\n-  if(!op) {\n-    n->destroy();\n-    if(!required)\n-      return nullptr;\n-    throw ErrorReport(loc) << \"unknown builtin op\";\n+  // fill in position arguments\n+  for(size_t i = 0; i < inputs.size(); ++i) {\n+    positional_inputs[i] = inputs[i];\n   }\n-  if(op->num_outputs == UNKNOWN_OUTPUTS) {\n-    throw ErrorReport(loc) << \"produces an unknown number of outputs, so it cannot be used directly from script methods\";\n+  // fill in named arguments\n+  for(const NamedValue& nv : attributes) {\n+    auto idx = schema.argumentIndexWithName(nv.name);\n+    if(!idx) {\n+      err() << \"unknown keyword argument '\" << nv.name << \"'\\n\" << nv.loc;\n+      return nullptr;\n+    }\n+    if(positional_inputs[*idx]) {\n+      err() << \"argument '\" <<  nv.name << \"' previously set \\n\" << nv.loc;\n+      return nullptr;\n+    }\n+    positional_inputs[*idx] = nv;\n+  }\n+  // fill in default values\n+  for(size_t i = 0; i < positional_inputs.size(); ++i) {\n+    if(positional_inputs[i])\n+      continue;\n+    auto default_value = schema.arguments[i].default_value;\n+    if(!default_value) {\n+      err() << \"argument '\" << schema.arguments[i].name << \"' not provided.\\n\" << loc;\n+      return nullptr;\n+    }\n+    positional_inputs[i] = NamedValue(loc, i, createConstant(*method.graph(), loc, *default_value));\n   }\n-  for(size_t i = 0; i < op->num_outputs; ++i)\n-    n->addOutput();\n \n-  // special handling for the tuple that cat takes as its first argument\n-  if(name == \"cat\") {\n-    ensureTensors(loc, inputs.slice(1));\n-    auto first = inputs.at(0);\n-    if(first->type()->kind() != TupleType::Kind) {\n-      throw ErrorReport(loc) << \"expected a tuple\";\n+  // check input types\n+  std::vector<Value*> flat_inputs;\n+  for(size_t i = 0; i < schema.arguments.size(); ++i) {\n+    NamedValue v = *positional_inputs[i];\n+    const auto& arg = schema.arguments[i];\n+\n+    // implicit conversion from List[Tensor] -> Tensor for when the argument\n+    // is an IntList in aten, for things like x.expand(sizes=[3,4,5])\n+    if(arg.attribute_kind == AttributeKind::is &&\n+       v.value->type()->isSubtypeOf(*ListType::ofTensors())) {\n+      auto unpacked = graph->insertNode(graph->createTupleUnpack(v.value));\n+      v.value = createStack(*graph, loc, unpacked->outputs());\n     }\n \n-    if(attributes.size() == 1) {\n-      if(inputs.size() > 1) {\n-        throw ErrorReport(loc) << \"expected 1 input\";\n-      }\n+    if(!v.value->type()->isSubtypeOf(*arg.type)) {\n+      err() << \"expected a value of type \" << arg.type->name() << \" for argument '\" << arg.name << \"' but found \"\n+            << v.value->type()->name() << \"\\n\"\n+            << v.loc;\n+      return nullptr;\n+    }\n+\n+    // we only support lists for builtins, where they must be flattened\n+    if(arg.type->kind() == TypeKind::ListType) {\n+      auto unpacked = graph->insertNode(graph->createTupleUnpack(v.value));\n+      const auto & outputs = unpacked->outputs();\n+      flat_inputs.insert(flat_inputs.end(), outputs.begin(), outputs.end());\n     } else {\n-      // findTensorOp already verified we don't have additional attributes\n-      JIT_ASSERT(attributes.size() == 0);\n-      if(inputs.size() != 2) {\n-          throw ErrorReport(loc) << \"expected 2 inputs\";\n-      }\n+      flat_inputs.push_back(v.value);\n     }\n \n-    // flatten the tuple into the argument list\n-    auto unpacked = graph->insertNode(graph->createTupleUnpack(first));\n-    ensureTensors(loc, unpacked->outputs());\n-    n->removeInput(0);\n-    for(size_t i = 0; i < unpacked->outputs().size(); ++i) {\n-      n->insertInput(i, unpacked->outputs().at(i));\n+  }\n+\n+  // we successfully matched this schema, construct the node\n+\n+  // note: we always construct a purely positional nodes here\n+  // the pass liftConstantAttributes replaces the node with with one that\n+  // uses attributes if all the attributes ended up as constants\n+\n+  NodeKind kind(Symbol::aten(name));\n+  auto n = graph->insertNode(graph->create(kind, flat_inputs, 0))\n+                ->setSourceLocation(std::make_shared<SourceRange>(loc));\n+\n+  size_t num_outputs = schema.returns.size();\n+\n+  // special case for chunk when the chunks=<const> is known\n+  // DO NOT ADD MORE SPECIAL CASES HERE, REFACTOR INTO A FUNCTION IF\n+  // NEEDED\n+  if(n->kind() == aten::chunk) {\n+    auto value = constant_as<int64_t>(flat_inputs[1]);\n+    if(!value) {\n+      throw ErrorReport(positional_inputs[1]->loc) << \"argument 'chunks' must be a constant\";\n     }\n-  } else {\n-    ensureTensors(loc, inputs);\n+    num_outputs = *value;", "path": "torch/csrc/jit/script/compiler.cpp", "position": null, "original_position": 277, "commit_id": "ed702bbdd1400a0d0865f59a9e69828337b447d9", "original_commit_id": "548698dbc6c03485edf11f5f07f04b796302d448", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Actually what's `num_outputs` for chunk?", "created_at": "2018-05-08T19:08:26Z", "updated_at": "2018-11-23T15:43:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/7311#discussion_r186835776", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7311", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/186835776"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7311#discussion_r186835776"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7311"}}, "body_html": "<p>Actually what's <code>num_outputs</code> for chunk?</p>", "body_text": "Actually what's num_outputs for chunk?"}