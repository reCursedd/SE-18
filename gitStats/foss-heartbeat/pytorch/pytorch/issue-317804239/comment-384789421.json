{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/384789421", "html_url": "https://github.com/pytorch/pytorch/pull/6970#issuecomment-384789421", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6970", "id": 384789421, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NDc4OTQyMQ==", "user": {"login": "FDecaYed", "id": 17164548, "node_id": "MDQ6VXNlcjE3MTY0NTQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/17164548?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FDecaYed", "html_url": "https://github.com/FDecaYed", "followers_url": "https://api.github.com/users/FDecaYed/followers", "following_url": "https://api.github.com/users/FDecaYed/following{/other_user}", "gists_url": "https://api.github.com/users/FDecaYed/gists{/gist_id}", "starred_url": "https://api.github.com/users/FDecaYed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FDecaYed/subscriptions", "organizations_url": "https://api.github.com/users/FDecaYed/orgs", "repos_url": "https://api.github.com/users/FDecaYed/repos", "events_url": "https://api.github.com/users/FDecaYed/events{/privacy}", "received_events_url": "https://api.github.com/users/FDecaYed/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-26T21:08:03Z", "updated_at": "2018-04-26T21:09:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I agree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> on we should not allow use case that run very slow just to prevent OOM. For reference, the resource NCCL need for each communicator is on scale of 4MB~50MB depends on interconnect speed between GPU in this group. In theory, this can be improved with substantial change to NCCL, by not requiring different communicator for different collectives.<br>\nAlso <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a> if setting a limit is truly desired, it should be per device, not per group. You can easily have 8 pairs when you have 16GPU(8 sets of communicator), but there is only one resource per GPU. It is also pretty rare(in my opinion) that 1 GPU being involved in lots of different communication pattern, so per GPU resource taken won't be too bad in general.</p>", "body_text": "I agree with @apaszke on we should not allow use case that run very slow just to prevent OOM. For reference, the resource NCCL need for each communicator is on scale of 4MB~50MB depends on interconnect speed between GPU in this group. In theory, this can be improved with substantial change to NCCL, by not requiring different communicator for different collectives.\nAlso @teng-li if setting a limit is truly desired, it should be per device, not per group. You can easily have 8 pairs when you have 16GPU(8 sets of communicator), but there is only one resource per GPU. It is also pretty rare(in my opinion) that 1 GPU being involved in lots of different communication pattern, so per GPU resource taken won't be too bad in general.", "body": "I agree with @apaszke on we should not allow use case that run very slow just to prevent OOM. For reference, the resource NCCL need for each communicator is on scale of 4MB~50MB depends on interconnect speed between GPU in this group. In theory, this can be improved with substantial change to NCCL, by not requiring different communicator for different collectives.\r\nAlso @teng-li if setting a limit is truly desired, it should be per device, not per group. You can easily have 8 pairs when you have 16GPU(8 sets of communicator), but there is only one resource per GPU. It is also pretty rare(in my opinion) that 1 GPU being involved in lots of different communication pattern, so per GPU resource taken won't be too bad in general. \r\n"}