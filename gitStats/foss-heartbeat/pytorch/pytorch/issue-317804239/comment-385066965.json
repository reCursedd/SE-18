{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/385066965", "html_url": "https://github.com/pytorch/pytorch/pull/6970#issuecomment-385066965", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6970", "id": 385066965, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTA2Njk2NQ==", "user": {"login": "FDecaYed", "id": 17164548, "node_id": "MDQ6VXNlcjE3MTY0NTQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/17164548?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FDecaYed", "html_url": "https://github.com/FDecaYed", "followers_url": "https://api.github.com/users/FDecaYed/followers", "following_url": "https://api.github.com/users/FDecaYed/following{/other_user}", "gists_url": "https://api.github.com/users/FDecaYed/gists{/gist_id}", "starred_url": "https://api.github.com/users/FDecaYed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FDecaYed/subscriptions", "organizations_url": "https://api.github.com/users/FDecaYed/orgs", "repos_url": "https://api.github.com/users/FDecaYed/repos", "events_url": "https://api.github.com/users/FDecaYed/events{/privacy}", "received_events_url": "https://api.github.com/users/FDecaYed/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-27T19:11:37Z", "updated_at": "2018-04-27T19:11:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a> Just talked to our NCCL team, the concern on multiple communicator other than memory is mostly order of scheduling. Even with single communicator, each GPU need to call collectives in the same order, which I think is true here in pytorch. Otherwise you should already see hangs.<br>\nWith multiple communicator, we need to be more careful about multi-thread and stream to ensure calling orders are same on each GPU. But it shouldn't prevent us from using more than 1 communicator since that's the only way to solve our problem, no matter you store it under different group/device list.</p>\n<p>Also, communicators won't interfere each other if they don't have overlap. This is the main use case we want to enable here(pairs to do allreduce).</p>", "body_text": "@teng-li Just talked to our NCCL team, the concern on multiple communicator other than memory is mostly order of scheduling. Even with single communicator, each GPU need to call collectives in the same order, which I think is true here in pytorch. Otherwise you should already see hangs.\nWith multiple communicator, we need to be more careful about multi-thread and stream to ensure calling orders are same on each GPU. But it shouldn't prevent us from using more than 1 communicator since that's the only way to solve our problem, no matter you store it under different group/device list.\nAlso, communicators won't interfere each other if they don't have overlap. This is the main use case we want to enable here(pairs to do allreduce).", "body": "@teng-li Just talked to our NCCL team, the concern on multiple communicator other than memory is mostly order of scheduling. Even with single communicator, each GPU need to call collectives in the same order, which I think is true here in pytorch. Otherwise you should already see hangs.\r\nWith multiple communicator, we need to be more careful about multi-thread and stream to ensure calling orders are same on each GPU. But it shouldn't prevent us from using more than 1 communicator since that's the only way to solve our problem, no matter you store it under different group/device list.\r\n\r\nAlso, communicators won't interfere each other if they don't have overlap. This is the main use case we want to enable here(pairs to do allreduce).\r\n"}