{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6970", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6970/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6970/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6970/events", "html_url": "https://github.com/pytorch/pytorch/pull/6970", "id": 317804239, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg0MTcxNjI5", "number": 6970, "title": "cache nccl resourse better for efficient hybrid data/model parallel", "user": {"login": "FDecaYed", "id": 17164548, "node_id": "MDQ6VXNlcjE3MTY0NTQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/17164548?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FDecaYed", "html_url": "https://github.com/FDecaYed", "followers_url": "https://api.github.com/users/FDecaYed/followers", "following_url": "https://api.github.com/users/FDecaYed/following{/other_user}", "gists_url": "https://api.github.com/users/FDecaYed/gists{/gist_id}", "starred_url": "https://api.github.com/users/FDecaYed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FDecaYed/subscriptions", "organizations_url": "https://api.github.com/users/FDecaYed/orgs", "repos_url": "https://api.github.com/users/FDecaYed/repos", "events_url": "https://api.github.com/users/FDecaYed/events{/privacy}", "received_events_url": "https://api.github.com/users/FDecaYed/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2018-04-25T21:41:42Z", "updated_at": "2018-05-10T17:42:43Z", "closed_at": "2018-05-10T17:42:37Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6970", "html_url": "https://github.com/pytorch/pytorch/pull/6970", "diff_url": "https://github.com/pytorch/pytorch/pull/6970.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6970.patch"}, "body_html": "<p>This change allow more than 1 device list to be stored under a group.</p>\n<p>Consider the following use case, which is model parallel within process, data parallel between process.</p>\n<p>A 2-layer model, layer0@gpu0 and layer1@gpu1. when you add distributed data parallel on top of it, you have another process with layer0@gpu2 and layer1@gpu3.<br>\nthere are 2 reduction of grad, layer0 on gpu0/2 and layer1 on gpu1/3.</p>\n<p>Every batch when you switch between these 2 reductions, all nccl resources are destroyed and re-allocated.(take seconds) This commit will allow pytorch to cache nccl resource for both device pair.</p>", "body_text": "This change allow more than 1 device list to be stored under a group.\nConsider the following use case, which is model parallel within process, data parallel between process.\nA 2-layer model, layer0@gpu0 and layer1@gpu1. when you add distributed data parallel on top of it, you have another process with layer0@gpu2 and layer1@gpu3.\nthere are 2 reduction of grad, layer0 on gpu0/2 and layer1 on gpu1/3.\nEvery batch when you switch between these 2 reductions, all nccl resources are destroyed and re-allocated.(take seconds) This commit will allow pytorch to cache nccl resource for both device pair.", "body": "This change allow more than 1 device list to be stored under a group.\r\n\r\nConsider the following use case, which is model parallel within process, data parallel between process.\r\n \r\nA 2-layer model, layer0@gpu0 and layer1@gpu1. when you add distributed data parallel on top of it, you have another process with layer0@gpu2 and layer1@gpu3.\r\nthere are 2 reduction of grad, layer0 on gpu0/2 and layer1 on gpu1/3.\r\n\r\nEvery batch when you switch between these 2 reductions, all nccl resources are destroyed and re-allocated.(take seconds) This commit will allow pytorch to cache nccl resource for both device pair.\r\n"}