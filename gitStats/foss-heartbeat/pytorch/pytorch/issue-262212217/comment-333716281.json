{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/333716281", "html_url": "https://github.com/pytorch/pytorch/pull/2933#issuecomment-333716281", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2933", "id": 333716281, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzcxNjI4MQ==", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-03T01:52:54Z", "updated_at": "2017-10-03T01:52:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The problem is deeper than managing the memory of the PyCapsule, because the DLTensor is going to get handed off to another library, which will then hold the view. The PyCapsule will likely die right after the handoff in the common case, so having it manage the lifetime of the tensor ref will not prevent many crashes:</p>\n<pre><code>c2.from_dlpack(torch.to_dlpack(t))\n</code></pre>\n<p>We are working to get a <code>ManagedDLTensor</code> that includes a deleter callback so that memory management can be done correctly. Until then, there really isn't a good way to make this crash free.</p>", "body_text": "The problem is deeper than managing the memory of the PyCapsule, because the DLTensor is going to get handed off to another library, which will then hold the view. The PyCapsule will likely die right after the handoff in the common case, so having it manage the lifetime of the tensor ref will not prevent many crashes:\nc2.from_dlpack(torch.to_dlpack(t))\n\nWe are working to get a ManagedDLTensor that includes a deleter callback so that memory management can be done correctly. Until then, there really isn't a good way to make this crash free.", "body": "The problem is deeper than managing the memory of the PyCapsule, because the DLTensor is going to get handed off to another library, which will then hold the view. The PyCapsule will likely die right after the handoff in the common case, so having it manage the lifetime of the tensor ref will not prevent many crashes:\r\n```\r\nc2.from_dlpack(torch.to_dlpack(t))\r\n```\r\nWe are working to get a `ManagedDLTensor` that includes a deleter callback so that memory management can be done correctly. Until then, there really isn't a good way to make this crash free.\r\n"}