{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4360", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4360/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4360/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4360/events", "html_url": "https://github.com/pytorch/pytorch/issues/4360", "id": 284670981, "node_id": "MDU6SXNzdWUyODQ2NzA5ODE=", "number": 4360, "title": "For multi-GPU computing, the module buffer report error if using torch.autograd.Variable", "user": {"login": "jyzhang-bjtu", "id": 10786236, "node_id": "MDQ6VXNlcjEwNzg2MjM2", "avatar_url": "https://avatars2.githubusercontent.com/u/10786236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jyzhang-bjtu", "html_url": "https://github.com/jyzhang-bjtu", "followers_url": "https://api.github.com/users/jyzhang-bjtu/followers", "following_url": "https://api.github.com/users/jyzhang-bjtu/following{/other_user}", "gists_url": "https://api.github.com/users/jyzhang-bjtu/gists{/gist_id}", "starred_url": "https://api.github.com/users/jyzhang-bjtu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jyzhang-bjtu/subscriptions", "organizations_url": "https://api.github.com/users/jyzhang-bjtu/orgs", "repos_url": "https://api.github.com/users/jyzhang-bjtu/repos", "events_url": "https://api.github.com/users/jyzhang-bjtu/events{/privacy}", "received_events_url": "https://api.github.com/users/jyzhang-bjtu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-12-27T08:05:39Z", "updated_at": "2018-04-07T09:07:38Z", "closed_at": "2017-12-27T20:54:13Z", "author_association": "NONE", "body_html": "<p>This is my own defined module with the buffer ind_upper where inds['upper'] can be a tensor or variable.<br>\nHere is one line of the init function of my module.<br>\nself.register_buffer( 'ind_upper', inds['upper']   )<br>\nMy torch version is 0.3.0.</p>\n<ol>\n<li>When my module uses torch.nn.DataParallel for multi-GPUs, the error msg is as follows if inds['upper'] is a torch.autograd.Variable.</li>\n</ol>\n<p>File \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 325, in <strong>call</strong><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 67, in forward<br>\nreplicas = self.replicate(self.module, self.device_ids[:len(inputs)])<br>\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 72, in replicate<br>\nreturn replicate(module, device_ids)<br>\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/parallel/replicate.py\", line 19, in replicate<br>\nbuffer_copies = comm.broadcast_coalesced(buffers, devices)<br>\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/cuda/comm.py\", line 55, in broadcast_coalesced<br>\nfor chunk in _take_tensors(tensors, buffer_size):<br>\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/_utils.py\", line 232, in _take_tensors<br>\nif tensor.is_sparse:<br>\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/autograd/variable.py\", line 67, in <strong>getattr</strong><br>\nreturn object.<strong>getattribute</strong>(self, name)<br>\nAttributeError: 'Variable' object has no attribute 'is_sparse'</p>\n<ol start=\"2\">\n<li>if inds['upper'] is a torch.tensor, everything is OK.</li>\n</ol>", "body_text": "This is my own defined module with the buffer ind_upper where inds['upper'] can be a tensor or variable.\nHere is one line of the init function of my module.\nself.register_buffer( 'ind_upper', inds['upper']   )\nMy torch version is 0.3.0.\n\nWhen my module uses torch.nn.DataParallel for multi-GPUs, the error msg is as follows if inds['upper'] is a torch.autograd.Variable.\n\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 325, in call\nresult = self.forward(*input, **kwargs)\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 67, in forward\nreplicas = self.replicate(self.module, self.device_ids[:len(inputs)])\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 72, in replicate\nreturn replicate(module, device_ids)\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/parallel/replicate.py\", line 19, in replicate\nbuffer_copies = comm.broadcast_coalesced(buffers, devices)\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/cuda/comm.py\", line 55, in broadcast_coalesced\nfor chunk in _take_tensors(tensors, buffer_size):\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/_utils.py\", line 232, in _take_tensors\nif tensor.is_sparse:\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/autograd/variable.py\", line 67, in getattr\nreturn object.getattribute(self, name)\nAttributeError: 'Variable' object has no attribute 'is_sparse'\n\nif inds['upper'] is a torch.tensor, everything is OK.", "body": "This is my own defined module with the buffer ind_upper where inds['upper'] can be a tensor or variable.\r\nHere is one line of the init function of my module.\r\n  self.register_buffer( 'ind_upper', inds['upper']   )\r\nMy torch version is 0.3.0. \r\n\r\n1. When my module uses torch.nn.DataParallel for multi-GPUs, the error msg is as follows if inds['upper'] is a torch.autograd.Variable.   \r\n\r\nFile \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 325, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 67, in forward\r\n    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\r\n  File \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 72, in replicate\r\n    return replicate(module, device_ids)\r\n  File \"/opt/pytorch/lib/python3.5/site-packages/torch/nn/parallel/replicate.py\", line 19, in replicate\r\n    buffer_copies = comm.broadcast_coalesced(buffers, devices)\r\n  File \"/opt/pytorch/lib/python3.5/site-packages/torch/cuda/comm.py\", line 55, in broadcast_coalesced\r\n    for chunk in _take_tensors(tensors, buffer_size):\r\n  File \"/opt/pytorch/lib/python3.5/site-packages/torch/_utils.py\", line 232, in _take_tensors\r\n    if tensor.is_sparse:\r\n  File \"/opt/pytorch/lib/python3.5/site-packages/torch/autograd/variable.py\", line 67, in __getattr__\r\n    return object.__getattribute__(self, name)\r\nAttributeError: 'Variable' object has no attribute 'is_sparse'\r\n\r\n2. if inds['upper'] is a torch.tensor, everything is OK.\r\n"}