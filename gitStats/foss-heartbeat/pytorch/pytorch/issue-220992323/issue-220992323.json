{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1230", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1230/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1230/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1230/events", "html_url": "https://github.com/pytorch/pytorch/issues/1230", "id": 220992323, "node_id": "MDU6SXNzdWUyMjA5OTIzMjM=", "number": 1230, "title": "CUDA memory leak?", "user": {"login": "SeparateReality", "id": 13707244, "node_id": "MDQ6VXNlcjEzNzA3MjQ0", "avatar_url": "https://avatars1.githubusercontent.com/u/13707244?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SeparateReality", "html_url": "https://github.com/SeparateReality", "followers_url": "https://api.github.com/users/SeparateReality/followers", "following_url": "https://api.github.com/users/SeparateReality/following{/other_user}", "gists_url": "https://api.github.com/users/SeparateReality/gists{/gist_id}", "starred_url": "https://api.github.com/users/SeparateReality/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SeparateReality/subscriptions", "organizations_url": "https://api.github.com/users/SeparateReality/orgs", "repos_url": "https://api.github.com/users/SeparateReality/repos", "events_url": "https://api.github.com/users/SeparateReality/events{/privacy}", "received_events_url": "https://api.github.com/users/SeparateReality/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-04-11T15:33:51Z", "updated_at": "2017-04-11T17:21:34Z", "closed_at": "2017-04-11T17:21:34Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.backends.cudnn <span class=\"pl-k\">as</span> cudnn\ncudnn.benchmark <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>__Python VERSION:<span class=\"pl-pds\">'</span></span>, sys.version)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>__pyTorch VERSION:<span class=\"pl-pds\">'</span></span>, torch.<span class=\"pl-c1\">__version__</span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>__CUDA VERSION<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">from</span> subprocess <span class=\"pl-k\">import</span> call\ncall([<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>nvcc<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--version<span class=\"pl-pds\">\"</span></span>])\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>__CUDNN VERSION:<span class=\"pl-pds\">'</span></span>, torch.backends.cudnn.version())\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>__Number CUDA Devices:<span class=\"pl-pds\">'</span></span>, torch.cuda.device_count())\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>__Devices<span class=\"pl-pds\">'</span></span>)\ncall([<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>nvidia-smi<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--format=csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free<span class=\"pl-pds\">\"</span></span>])\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Active CUDA Device: GPU<span class=\"pl-pds\">'</span></span>, torch.cuda.current_device())\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> print('  Try to change to Device 2 - with \"torch.cuda.device(2)\"')</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> torch.cuda.device(2)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> print('  ! Active CUDA Device is still:', torch.cuda.current_device())</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> print('  Try again with environment vars')</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> import os</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> print('  ! Active CUDA Device is still:', torch.cuda.current_device())</span>\n\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> Conv1d <span class=\"pl-k\">as</span> Conv1d\n\nnum_runs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\ns <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">22050</span>\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">for</span> seqlen <span class=\"pl-k\">in</span> [s]:\n    <span class=\"pl-k\">for</span> batch_size <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>]:\n        <span class=\"pl-k\">for</span> dilation <span class=\"pl-k\">in</span> <span class=\"pl-c1\">reversed</span>([<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">512</span>]):\n            m <span class=\"pl-k\">=</span> nn.Sequential(Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation),\n                              Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation),\n                              Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation),\n                              Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation),\n                              Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation)).cuda()\n            <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.randn(batch_size, <span class=\"pl-c1\">32</span>, seqlen).float().cuda()\n\n            torch.cuda.synchronize()\n            start <span class=\"pl-k\">=</span> time.time()\n            <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_runs):\n                output <span class=\"pl-k\">=</span> m(Variable(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n                output.backward(output.data)\n            torch.cuda.synchronize()\n            mean_time <span class=\"pl-k\">=</span> (time.time() <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">float</span>(num_runs)\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_size: <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span>dilation: <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span>seqlen: <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span> time <span class=\"pl-c1\">%f</span><span class=\"pl-cce\">\\t</span> runs: <span class=\"pl-c1\">%i</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span>(batch_size, dilation, seqlen, mean_time, num_runs))</pre></div>\n<p><strong>Output:</strong></p>\n<pre><code>__Python VERSION: 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 12:22:00) \n__pyTorch VERSION: 0.1.11+8aa1cef\n__CUDA VERSION\nCuda compilation tools, release 8.0, V8.0.61\n__CUDNN VERSION: 6020\n__Number CUDA Devices: 4\n__Devices\nindex, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n0, GeForce GTX 1080 Ti, 381.09, 11158 MiB, 318 MiB, 10840 MiB\n1, GeForce GTX 1080 Ti, 381.09, 11172 MiB, 11 MiB, 11161 MiB\n2, GeForce GTX 1080 Ti, 381.09, 11172 MiB, 11 MiB, 11161 MiB\n3, GeForce GTX 1080 Ti, 381.09, 11172 MiB, 11 MiB, 11161 MiB\nActive CUDA Device: GPU 0\n\nbatch_size: 16\tdilation: 512\tseqlen: 110250\t time 0.204314\t runs: 10\nbatch_size: 16\tdilation: 256\tseqlen: 110250\t time 0.162138\t runs: 10\nbatch_size: 16\tdilation: 128\tseqlen: 110250\t time 0.148690\t runs: 10\nbatch_size: 16\tdilation: 64\tseqlen: 110250\t time 0.141783\t runs: 10\nbatch_size: 32\tdilation: 512\tseqlen: 110250\t time 0.279548\t runs: 10\nTraceback (most recent call last):\n  File \"benchmark_test.py\", line 48, in &lt;module&gt;\n    output = m(Variable(input, requires_grad=True))\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 64, in forward\n    input = module(input)\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 143, in forward\n    self.padding, self.dilation, self.groups)\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 62, in conv1d\n    return f(input, weight, bias)\nRuntimeError: CUDNN_STATUS_ALLOC_FAILED\n</code></pre>\n<p>I was wondering why I got the CUDNN_STATUS_ALLOC_FAILED.<br>\nAfter some experiments I found out that - error or not - depends on the sequence in the dilation list:<br>\nline 37:    <code>for dilation in reversed([64, 128, 256, 512]):</code><br>\nExecution without <code>reversed</code> goes without error.</p>\n<p>I am not yet familiar with the whole thing. I am I missing something?</p>\n<p>--<br>\nI thankfully adapted this code from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"213195830\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/967\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/967/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/967\">#967</a>.<br>\nBy the way: I am also curious why I can\u2019t change the active CUDA device (see comment in the code)\u2026 but I probably just need to get more into it\u2026</p>", "body_text": "from torch.autograd import Variable\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\ncudnn.benchmark = True\n\nimport sys\nprint('__Python VERSION:', sys.version)\nprint('__pyTorch VERSION:', torch.__version__)\nprint('__CUDA VERSION')\nfrom subprocess import call\ncall([\"nvcc\", \"--version\"])\nprint('__CUDNN VERSION:', torch.backends.cudnn.version())\nprint('__Number CUDA Devices:', torch.cuda.device_count())\nprint('__Devices')\ncall([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\nprint('Active CUDA Device: GPU', torch.cuda.current_device())\n# print('  Try to change to Device 2 - with \"torch.cuda.device(2)\"')\n# torch.cuda.device(2)\n# print('  ! Active CUDA Device is still:', torch.cuda.current_device())\n#\n# print('  Try again with environment vars')\n# import os\n# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n# print('  ! Active CUDA Device is still:', torch.cuda.current_device())\n\nimport time\nfrom torch.nn import Conv1d as Conv1d\n\nnum_runs = 10\ns = 5*22050\n\nprint('\\n')\nfor seqlen in [s]:\n    for batch_size in [16, 32]:\n        for dilation in reversed([64, 128, 256, 512]):\n            m = nn.Sequential(Conv1d(32, 32, kernel_size=2, dilation=dilation),\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation)).cuda()\n            input = torch.randn(batch_size, 32, seqlen).float().cuda()\n\n            torch.cuda.synchronize()\n            start = time.time()\n            for j in range(num_runs):\n                output = m(Variable(input, requires_grad=True))\n                output.backward(output.data)\n            torch.cuda.synchronize()\n            mean_time = (time.time() - start) / float(num_runs)\n            print('batch_size: %i\\tdilation: %i\\tseqlen: %i\\t time %f\\t runs: %i' %(batch_size, dilation, seqlen, mean_time, num_runs))\nOutput:\n__Python VERSION: 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 12:22:00) \n__pyTorch VERSION: 0.1.11+8aa1cef\n__CUDA VERSION\nCuda compilation tools, release 8.0, V8.0.61\n__CUDNN VERSION: 6020\n__Number CUDA Devices: 4\n__Devices\nindex, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n0, GeForce GTX 1080 Ti, 381.09, 11158 MiB, 318 MiB, 10840 MiB\n1, GeForce GTX 1080 Ti, 381.09, 11172 MiB, 11 MiB, 11161 MiB\n2, GeForce GTX 1080 Ti, 381.09, 11172 MiB, 11 MiB, 11161 MiB\n3, GeForce GTX 1080 Ti, 381.09, 11172 MiB, 11 MiB, 11161 MiB\nActive CUDA Device: GPU 0\n\nbatch_size: 16\tdilation: 512\tseqlen: 110250\t time 0.204314\t runs: 10\nbatch_size: 16\tdilation: 256\tseqlen: 110250\t time 0.162138\t runs: 10\nbatch_size: 16\tdilation: 128\tseqlen: 110250\t time 0.148690\t runs: 10\nbatch_size: 16\tdilation: 64\tseqlen: 110250\t time 0.141783\t runs: 10\nbatch_size: 32\tdilation: 512\tseqlen: 110250\t time 0.279548\t runs: 10\nTraceback (most recent call last):\n  File \"benchmark_test.py\", line 48, in <module>\n    output = m(Variable(input, requires_grad=True))\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 64, in forward\n    input = module(input)\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 143, in forward\n    self.padding, self.dilation, self.groups)\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 62, in conv1d\n    return f(input, weight, bias)\nRuntimeError: CUDNN_STATUS_ALLOC_FAILED\n\nI was wondering why I got the CUDNN_STATUS_ALLOC_FAILED.\nAfter some experiments I found out that - error or not - depends on the sequence in the dilation list:\nline 37:    for dilation in reversed([64, 128, 256, 512]):\nExecution without reversed goes without error.\nI am not yet familiar with the whole thing. I am I missing something?\n--\nI thankfully adapted this code from #967.\nBy the way: I am also curious why I can\u2019t change the active CUDA device (see comment in the code)\u2026 but I probably just need to get more into it\u2026", "body": "```python\r\nfrom torch.autograd import Variable\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.backends.cudnn as cudnn\r\ncudnn.benchmark = True\r\n\r\nimport sys\r\nprint('__Python VERSION:', sys.version)\r\nprint('__pyTorch VERSION:', torch.__version__)\r\nprint('__CUDA VERSION')\r\nfrom subprocess import call\r\ncall([\"nvcc\", \"--version\"])\r\nprint('__CUDNN VERSION:', torch.backends.cudnn.version())\r\nprint('__Number CUDA Devices:', torch.cuda.device_count())\r\nprint('__Devices')\r\ncall([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\r\nprint('Active CUDA Device: GPU', torch.cuda.current_device())\r\n# print('  Try to change to Device 2 - with \"torch.cuda.device(2)\"')\r\n# torch.cuda.device(2)\r\n# print('  ! Active CUDA Device is still:', torch.cuda.current_device())\r\n#\r\n# print('  Try again with environment vars')\r\n# import os\r\n# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\r\n# print('  ! Active CUDA Device is still:', torch.cuda.current_device())\r\n\r\nimport time\r\nfrom torch.nn import Conv1d as Conv1d\r\n\r\nnum_runs = 10\r\ns = 5*22050\r\n\r\nprint('\\n')\r\nfor seqlen in [s]:\r\n    for batch_size in [16, 32]:\r\n        for dilation in reversed([64, 128, 256, 512]):\r\n            m = nn.Sequential(Conv1d(32, 32, kernel_size=2, dilation=dilation),\r\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\r\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\r\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\r\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation)).cuda()\r\n            input = torch.randn(batch_size, 32, seqlen).float().cuda()\r\n\r\n            torch.cuda.synchronize()\r\n            start = time.time()\r\n            for j in range(num_runs):\r\n                output = m(Variable(input, requires_grad=True))\r\n                output.backward(output.data)\r\n            torch.cuda.synchronize()\r\n            mean_time = (time.time() - start) / float(num_runs)\r\n            print('batch_size: %i\\tdilation: %i\\tseqlen: %i\\t time %f\\t runs: %i' %(batch_size, dilation, seqlen, mean_time, num_runs))\r\n```\r\n\r\n**Output:**\r\n``` \r\n__Python VERSION: 3.6.0 |Anaconda 4.3.1 (64-bit)| (default, Dec 23 2016, 12:22:00) \r\n__pyTorch VERSION: 0.1.11+8aa1cef\r\n__CUDA VERSION\r\nCuda compilation tools, release 8.0, V8.0.61\r\n__CUDNN VERSION: 6020\r\n__Number CUDA Devices: 4\r\n__Devices\r\nindex, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\r\n0, GeForce GTX 1080 Ti, 381.09, 11158 MiB, 318 MiB, 10840 MiB\r\n1, GeForce GTX 1080 Ti, 381.09, 11172 MiB, 11 MiB, 11161 MiB\r\n2, GeForce GTX 1080 Ti, 381.09, 11172 MiB, 11 MiB, 11161 MiB\r\n3, GeForce GTX 1080 Ti, 381.09, 11172 MiB, 11 MiB, 11161 MiB\r\nActive CUDA Device: GPU 0\r\n\r\nbatch_size: 16\tdilation: 512\tseqlen: 110250\t time 0.204314\t runs: 10\r\nbatch_size: 16\tdilation: 256\tseqlen: 110250\t time 0.162138\t runs: 10\r\nbatch_size: 16\tdilation: 128\tseqlen: 110250\t time 0.148690\t runs: 10\r\nbatch_size: 16\tdilation: 64\tseqlen: 110250\t time 0.141783\t runs: 10\r\nbatch_size: 32\tdilation: 512\tseqlen: 110250\t time 0.279548\t runs: 10\r\nTraceback (most recent call last):\r\n  File \"benchmark_test.py\", line 48, in <module>\r\n    output = m(Variable(input, requires_grad=True))\r\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 64, in forward\r\n    input = module(input)\r\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 206, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 143, in forward\r\n    self.padding, self.dilation, self.groups)\r\n  File \"/home/USERNAME/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\", line 62, in conv1d\r\n    return f(input, weight, bias)\r\nRuntimeError: CUDNN_STATUS_ALLOC_FAILED\r\n```\r\n\r\nI was wondering why I got the CUDNN_STATUS_ALLOC_FAILED.\r\nAfter some experiments I found out that - error or not - depends on the sequence in the dilation list:\r\nline 37:    ```for dilation in reversed([64, 128, 256, 512]):```\r\nExecution without ```reversed``` goes without error.\r\n\r\nI am not yet familiar with the whole thing. I am I missing something?\r\n\r\n--\r\nI thankfully adapted this code from #967.\r\nBy the way: I am also curious why I can\u2019t change the active CUDA device (see comment in the code)\u2026 but I probably just need to get more into it\u2026\r\n\r\n\r\n\r\n\r\n"}