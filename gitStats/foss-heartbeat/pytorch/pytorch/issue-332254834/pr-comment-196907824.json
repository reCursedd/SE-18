{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196907824", "pull_request_review_id": 130546831, "id": 196907824, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NjkwNzgyNA==", "diff_hunk": "@@ -593,5 +594,21 @@ Tensor hann_window(\n   return native::hamming_window(\n       window_length, periodic, /*alpha=*/0.5, /*beta=*/0.5, options);\n }\n+\n+template <typename T>\n+Tensor tensor(ArrayRef<T> values, const TensorOptions& options) {\n+  auto result = at::empty(values.size(), options);\n+  for (size_t i = 0; i < values.size(); ++i) {\n+    result[i] = values[i];", "path": "aten/src/ATen/native/TensorFactories.cpp", "position": 17, "original_position": 17, "commit_id": "8fa9e90cb63e3b6209aef2c1a36efa9591cbd539", "original_commit_id": "8fa9e90cb63e3b6209aef2c1a36efa9591cbd539", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "You're right. I pondered about this for a bit. Can you think of a better way of doing this, such that it also works for CUDA types and so on (i.e. I guess not `memcpy` into the `.data`). Happy to change these things in a follow up PR", "created_at": "2018-06-20T19:02:56Z", "updated_at": "2018-11-23T15:45:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/8475#discussion_r196907824", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8475", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196907824"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8475#discussion_r196907824"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8475"}}, "body_html": "<p>You're right. I pondered about this for a bit. Can you think of a better way of doing this, such that it also works for CUDA types and so on (i.e. I guess not <code>memcpy</code> into the <code>.data</code>). Happy to change these things in a follow up PR</p>", "body_text": "You're right. I pondered about this for a bit. Can you think of a better way of doing this, such that it also works for CUDA types and so on (i.e. I guess not memcpy into the .data). Happy to change these things in a follow up PR", "in_reply_to_id": 196903355}