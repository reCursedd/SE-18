{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/403587755", "html_url": "https://github.com/pytorch/pytorch/issues/9193#issuecomment-403587755", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9193", "id": 403587755, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMzU4Nzc1NQ==", "user": {"login": "Novak3", "id": 2293807, "node_id": "MDQ6VXNlcjIyOTM4MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2293807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Novak3", "html_url": "https://github.com/Novak3", "followers_url": "https://api.github.com/users/Novak3/followers", "following_url": "https://api.github.com/users/Novak3/following{/other_user}", "gists_url": "https://api.github.com/users/Novak3/gists{/gist_id}", "starred_url": "https://api.github.com/users/Novak3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Novak3/subscriptions", "organizations_url": "https://api.github.com/users/Novak3/orgs", "repos_url": "https://api.github.com/users/Novak3/repos", "events_url": "https://api.github.com/users/Novak3/events{/privacy}", "received_events_url": "https://api.github.com/users/Novak3/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-09T19:07:50Z", "updated_at": "2018-07-09T19:08:20Z", "author_association": "NONE", "body_html": "<p>I'm not sure what's going on there, either.  I wish I'd had the sense to take a complete code snapshot for posterity.  I do know, after thinking about it for a bit, that the following refactor seems to work.  I.e., it runs without error, reduces over-training, shows better error/loss when I turn the dropout off for evaluation, etc:</p>\n<p><code>        out1, _            = self.lstm1(pack1,  (self.h1, self.c1))</code><br>\n<code>        pad1               = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)[0]</code></p>\n<p>Which, on reflection, is a much more sensible approach.  I'm not sure if the error message I was getting the other day is a bug in the API or the natural result of the dumb thing I was doing.</p>\n<p>That said, if there's a concrete feature request in here, it's something like this:  Enable better handling of packed sequences.  I've been smacked in the face three times, now, trying to get them to work for me:</p>\n<ol>\n<li>\n<p>I naively thought I could pack, pass the packed sequence as an argument to the nn module, and return the packed sequence and unpack it outside the nn module.  Not so!  The packed sequence is silently converted to something else that breaks the gradient.  If I pass the raw data to the module, pack it within the forward method, and unpack it inside the forward method, that works.</p>\n</li>\n<li>\n<p>Above</p>\n</li>\n<li>\n<p>Currently trying to get batch norm to work on an LSTM with uneven length sequences.  This time I at least put my hands in front of my face before getting smacked.</p>\n</li>\n</ol>\n<p>It'd be awfully nice if any of the following were true:</p>\n<ol>\n<li>\n<p>Warnings when you pass a packed sequence somewhere that needs a raw Tensor</p>\n</li>\n<li>\n<p>Clearly documented list of places you can pass a packed sequence and get good results; if, as I now suspect, that list includes all RNN modules <em>and nothing else</em>, it should say \"and nothing else.\"</p>\n</li>\n</ol>\n<p>3a)  Ability to pass packed sequences to some other layers, including especially the common infrastructure layers like dropout, batch normalizers, and probably a few others<br>\n3b)  Or, failing that but a little uglier, packed sequence equivalents for those functions</p>", "body_text": "I'm not sure what's going on there, either.  I wish I'd had the sense to take a complete code snapshot for posterity.  I do know, after thinking about it for a bit, that the following refactor seems to work.  I.e., it runs without error, reduces over-training, shows better error/loss when I turn the dropout off for evaluation, etc:\n        out1, _            = self.lstm1(pack1,  (self.h1, self.c1))\n        pad1               = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)[0]\nWhich, on reflection, is a much more sensible approach.  I'm not sure if the error message I was getting the other day is a bug in the API or the natural result of the dumb thing I was doing.\nThat said, if there's a concrete feature request in here, it's something like this:  Enable better handling of packed sequences.  I've been smacked in the face three times, now, trying to get them to work for me:\n\n\nI naively thought I could pack, pass the packed sequence as an argument to the nn module, and return the packed sequence and unpack it outside the nn module.  Not so!  The packed sequence is silently converted to something else that breaks the gradient.  If I pass the raw data to the module, pack it within the forward method, and unpack it inside the forward method, that works.\n\n\nAbove\n\n\nCurrently trying to get batch norm to work on an LSTM with uneven length sequences.  This time I at least put my hands in front of my face before getting smacked.\n\n\nIt'd be awfully nice if any of the following were true:\n\n\nWarnings when you pass a packed sequence somewhere that needs a raw Tensor\n\n\nClearly documented list of places you can pass a packed sequence and get good results; if, as I now suspect, that list includes all RNN modules and nothing else, it should say \"and nothing else.\"\n\n\n3a)  Ability to pass packed sequences to some other layers, including especially the common infrastructure layers like dropout, batch normalizers, and probably a few others\n3b)  Or, failing that but a little uglier, packed sequence equivalents for those functions", "body": "I'm not sure what's going on there, either.  I wish I'd had the sense to take a complete code snapshot for posterity.  I do know, after thinking about it for a bit, that the following refactor seems to work.  I.e., it runs without error, reduces over-training, shows better error/loss when I turn the dropout off for evaluation, etc:\r\n\r\n`        out1, _            = self.lstm1(pack1,  (self.h1, self.c1))`\r\n`        pad1               = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)[0]`\r\n\r\nWhich, on reflection, is a much more sensible approach.  I'm not sure if the error message I was getting the other day is a bug in the API or the natural result of the dumb thing I was doing.\r\n\r\nThat said, if there's a concrete feature request in here, it's something like this:  Enable better handling of packed sequences.  I've been smacked in the face three times, now, trying to get them to work for me:\r\n\r\n1)  I naively thought I could pack, pass the packed sequence as an argument to the nn module, and return the packed sequence and unpack it outside the nn module.  Not so!  The packed sequence is silently converted to something else that breaks the gradient.  If I pass the raw data to the module, pack it within the forward method, and unpack it inside the forward method, that works. \r\n\r\n2)  Above\r\n\r\n3)  Currently trying to get batch norm to work on an LSTM with uneven length sequences.  This time I at least put my hands in front of my face before getting smacked.\r\n\r\nIt'd be awfully nice if any of the following were true:\r\n\r\n1)  Warnings when you pass a packed sequence somewhere that needs a raw Tensor\r\n\r\n2)  Clearly documented list of places you can pass a packed sequence and get good results; if, as I now suspect, that list includes all RNN modules *and nothing else*, it should say \"and nothing else.\"\r\n\r\n3a)  Ability to pass packed sequences to some other layers, including especially the common infrastructure layers like dropout, batch normalizers, and probably a few others\r\n3b)  Or, failing that but a little uglier, packed sequence equivalents for those functions"}