{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9193", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9193/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9193/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9193/events", "html_url": "https://github.com/pytorch/pytorch/issues/9193", "id": 338729364, "node_id": "MDU6SXNzdWUzMzg3MjkzNjQ=", "number": 9193, "title": "[Feature Request] Additional torch.nn.LSTM functionality", "user": {"login": "Novak3", "id": 2293807, "node_id": "MDQ6VXNlcjIyOTM4MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/2293807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Novak3", "html_url": "https://github.com/Novak3", "followers_url": "https://api.github.com/users/Novak3/followers", "following_url": "https://api.github.com/users/Novak3/following{/other_user}", "gists_url": "https://api.github.com/users/Novak3/gists{/gist_id}", "starred_url": "https://api.github.com/users/Novak3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Novak3/subscriptions", "organizations_url": "https://api.github.com/users/Novak3/orgs", "repos_url": "https://api.github.com/users/Novak3/repos", "events_url": "https://api.github.com/users/Novak3/events{/privacy}", "received_events_url": "https://api.github.com/users/Novak3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-05T21:18:55Z", "updated_at": "2018-07-09T19:08:20Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>If you have a question or would like help and support, please ask at our<br>\n<a href=\"https://discuss.pytorch.org/\" rel=\"nofollow\">forums</a>.</p>\n<p>If you are submitting a feature request, please preface the title with [feature request].<br>\nIf you are submitting a bug report, please fill in the following details.</p>\n<h2>Issue description</h2>\n<p>Ultimately, I would like to create a clean, fully functional, stacked LSTM unit out of existing functions, which allows different widths for each layer, allows dropout, batched and packed sequences with masks, and allows passage of a list of layer sizes to the module for automatic construction.  This specific feature request is motivated by the error message from the code below, telling me to ask for a feature request.</p>\n<h2>Code example</h2>\n<blockquote>\n<p>class Model1(torch.nn.Module):</p>\n<pre><code>[etc, etc]\n\ndef forward(self, inputs, lengths):\n    pack1              = nn.utils.rnn.pack_padded_sequence(inputs, lengths, batch_first=True)\n    out1, self.hidden1 = self.lstm1(pack1, (self.hidden1[0].detach(), self.hidden1[1].detach()))\n    pad1               = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)        \n    drop1              = self.dropout1(pad1)\n    \n    pack2              = nn.utils.rnn.pack_padded_sequence(drop1, lengths, batch_first=True)\n    out2, self.hidden2 = self.lstm2(pack2,   (self.hidden2[0].detach(), self.hidden2[1].detach()))\n    pad2               = nn.utils.rnn.pad_packed_sequence(out2, batch_first=True)\n</code></pre>\n<p>return pad2</p>\n</blockquote>\n<p>This results in the error message: \"RuntimeError: Returning Variables sharing storage with other Variables that require grad is not supported in Python functions. Please submit a feature request if you hit this error.\"</p>\n<p>That is the direct result of trying to unpack and repack the sequences around the application of the dropout layer, since applying the dropout to a packed sequence does not result in a packed sequence.  And trying to apply the dropout manually is necessary because the LSTM layer is hardwired against applying its own dropout to the \"last\" layer of the LSTM, which is every layer in this approach because they are all separate.  They are all separate because the multi-layer stacked LSTM units all assume the same layer width.</p>\n<p>And even if I got all that to work right, I would still be tinkering with the module definition directly, because trying to construct those layers in a list ends up with the layer weights not registered as parameters for the optimizer.</p>\n<p>I must be missing something obvious, because this seems to be very much more difficult than it should be, to construct a pretty standard configuration with modern features.</p>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.5<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration: GPU 0: Quadro M4000<br>\nNvidia driver version: 390.48<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.14.2)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchvision (0.2.1)<br>\n[conda] Could not collect</p>", "body_text": "If you have a question or would like help and support, please ask at our\nforums.\nIf you are submitting a feature request, please preface the title with [feature request].\nIf you are submitting a bug report, please fill in the following details.\nIssue description\nUltimately, I would like to create a clean, fully functional, stacked LSTM unit out of existing functions, which allows different widths for each layer, allows dropout, batched and packed sequences with masks, and allows passage of a list of layer sizes to the module for automatic construction.  This specific feature request is motivated by the error message from the code below, telling me to ask for a feature request.\nCode example\n\nclass Model1(torch.nn.Module):\n[etc, etc]\n\ndef forward(self, inputs, lengths):\n    pack1              = nn.utils.rnn.pack_padded_sequence(inputs, lengths, batch_first=True)\n    out1, self.hidden1 = self.lstm1(pack1, (self.hidden1[0].detach(), self.hidden1[1].detach()))\n    pad1               = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)        \n    drop1              = self.dropout1(pad1)\n    \n    pack2              = nn.utils.rnn.pack_padded_sequence(drop1, lengths, batch_first=True)\n    out2, self.hidden2 = self.lstm2(pack2,   (self.hidden2[0].detach(), self.hidden2[1].detach()))\n    pad2               = nn.utils.rnn.pad_packed_sequence(out2, batch_first=True)\n\nreturn pad2\n\nThis results in the error message: \"RuntimeError: Returning Variables sharing storage with other Variables that require grad is not supported in Python functions. Please submit a feature request if you hit this error.\"\nThat is the direct result of trying to unpack and repack the sequences around the application of the dropout layer, since applying the dropout to a packed sequence does not result in a packed sequence.  And trying to apply the dropout manually is necessary because the LSTM layer is hardwired against applying its own dropout to the \"last\" layer of the LSTM, which is every layer in this approach because they are all separate.  They are all separate because the multi-layer stacked LSTM units all assume the same layer width.\nAnd even if I got all that to work right, I would still be tinkering with the module definition directly, because trying to construct those layers in a list ends up with the layer weights not registered as parameters for the optimizer.\nI must be missing something obvious, because this seems to be very much more difficult than it should be, to construct a pretty standard configuration with modern features.\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: Could not collect\nPython version: 3.5\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration: GPU 0: Quadro M4000\nNvidia driver version: 390.48\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\nVersions of relevant libraries:\n[pip3] numpy (1.14.2)\n[pip3] torch (0.4.0)\n[pip3] torchvision (0.2.1)\n[conda] Could not collect", "body": "If you have a question or would like help and support, please ask at our\r\n[forums](https://discuss.pytorch.org/).\r\n\r\nIf you are submitting a feature request, please preface the title with [feature request].\r\nIf you are submitting a bug report, please fill in the following details.\r\n\r\n## Issue description\r\n\r\nUltimately, I would like to create a clean, fully functional, stacked LSTM unit out of existing functions, which allows different widths for each layer, allows dropout, batched and packed sequences with masks, and allows passage of a list of layer sizes to the module for automatic construction.  This specific feature request is motivated by the error message from the code below, telling me to ask for a feature request.\r\n\r\n## Code example\r\n\r\n> class Model1(torch.nn.Module):\r\n> \r\n>     [etc, etc]\r\n> \r\n>     def forward(self, inputs, lengths):\r\n>         pack1              = nn.utils.rnn.pack_padded_sequence(inputs, lengths, batch_first=True)\r\n>         out1, self.hidden1 = self.lstm1(pack1, (self.hidden1[0].detach(), self.hidden1[1].detach()))\r\n>         pad1               = nn.utils.rnn.pad_packed_sequence(out1, batch_first=True)        \r\n>         drop1              = self.dropout1(pad1)\r\n>         \r\n>         pack2              = nn.utils.rnn.pack_padded_sequence(drop1, lengths, batch_first=True)\r\n>         out2, self.hidden2 = self.lstm2(pack2,   (self.hidden2[0].detach(), self.hidden2[1].detach()))\r\n>         pad2               = nn.utils.rnn.pad_packed_sequence(out2, batch_first=True)\r\n> \r\n> return pad2\r\n\r\nThis results in the error message: \"RuntimeError: Returning Variables sharing storage with other Variables that require grad is not supported in Python functions. Please submit a feature request if you hit this error.\"\r\n\r\nThat is the direct result of trying to unpack and repack the sequences around the application of the dropout layer, since applying the dropout to a packed sequence does not result in a packed sequence.  And trying to apply the dropout manually is necessary because the LSTM layer is hardwired against applying its own dropout to the \"last\" layer of the LSTM, which is every layer in this approach because they are all separate.  They are all separate because the multi-layer stacked LSTM units all assume the same layer width.\r\n\r\nAnd even if I got all that to work right, I would still be tinkering with the module definition directly, because trying to construct those layers in a list ends up with the layer weights not registered as parameters for the optimizer. \r\n\r\nI must be missing something obvious, because this seems to be very much more difficult than it should be, to construct a pretty standard configuration with modern features.  \r\n\r\n## System Info\r\n\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.5\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: GPU 0: Quadro M4000\r\nNvidia driver version: 390.48\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.2)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchvision (0.2.1)\r\n[conda] Could not collect"}