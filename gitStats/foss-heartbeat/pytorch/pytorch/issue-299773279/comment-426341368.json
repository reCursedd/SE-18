{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/426341368", "html_url": "https://github.com/pytorch/pytorch/issues/5374#issuecomment-426341368", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5374", "id": 426341368, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjM0MTM2OA==", "user": {"login": "samuelbroscheit", "id": 22645035, "node_id": "MDQ6VXNlcjIyNjQ1MDM1", "avatar_url": "https://avatars3.githubusercontent.com/u/22645035?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samuelbroscheit", "html_url": "https://github.com/samuelbroscheit", "followers_url": "https://api.github.com/users/samuelbroscheit/followers", "following_url": "https://api.github.com/users/samuelbroscheit/following{/other_user}", "gists_url": "https://api.github.com/users/samuelbroscheit/gists{/gist_id}", "starred_url": "https://api.github.com/users/samuelbroscheit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samuelbroscheit/subscriptions", "organizations_url": "https://api.github.com/users/samuelbroscheit/orgs", "repos_url": "https://api.github.com/users/samuelbroscheit/repos", "events_url": "https://api.github.com/users/samuelbroscheit/events{/privacy}", "received_events_url": "https://api.github.com/users/samuelbroscheit/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-02T16:31:37Z", "updated_at": "2018-10-02T16:31:37Z", "author_association": "NONE", "body_html": "<p>The script below has two networks Net 1 and Net 2. The difference between Net 1 and Net 2 is</p>\n<p>Net 1 (standard Batchnorm1d)</p>\n<pre><code>if self.affine:\n    self.weight = Parameter(torch.Tensor(num_features))\n    self.bias = Parameter(torch.Tensor(num_features))\nelse:\n    self.register_parameter('weight', None)\n    self.register_parameter('bias', None)\n</code></pre>\n<p>Net 2 where the init of Batchnorm is changed to</p>\n<pre><code>if self.affine:\n    self.weight = Parameter(torch.Tensor(num_features))\n    self.bias = Parameter(torch.Tensor(num_features))\nelse:\n    self.weight = Parameter(torch.ones(num_features), requires_grad=False)\n    self.bias = Parameter(torch.zeros(num_features), requires_grad=False)\n</code></pre>\n<p>I ran the script below with pytorch 3.1 and pytorch 4.1:</p>\n<p>pytorch 3.1</p>\n<pre><code>hid_sz\tNet 1\tNet 2\n    32\t1.03\t1.43\n    64\t0.94\t0.87\n   128\t0.91\t0.90\n   256\t0.88\t0.80\n   512\t1.01\t0.94\n  1024\t1.93\t0.96\n  2048\t3.52\t1.54\n  4096\t8.05\t4.72\n</code></pre>\n<p>pytorch 4.1</p>\n<pre><code>hid_sz\tNet 1\tNet 2\n    32\t1.12\t1.21\n    64\t1.14\t1.22\n   128\t1.15\t1.22\n   256\t1.15\t1.23\n   512\t1.29\t1.29\n  1024\t2.04\t1.32\n  2048\t3.65\t1.75\n  4096\t8.42\t4.77\n</code></pre>\n<p>The timings show that from a hidden size of 1024 onwards it is faster to use the Batchnorm from Net 2, i.e. to do a identity transformation in CUDNN instead of doing THNN BN. Also there seems to be a regression from 3.1 to 4.1. In general, for smaller hidden sizes THNN is not significantly faster.</p>\n<p>The simple fix would be to implement the init for _Batchnorm like in the example code, i.e. do a identity transform in CUDNN.</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<pre><code>import time\nimport torch\nfrom torch.nn import Parameter\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nclass BatchNorm1d(_BatchNorm):\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        super(BatchNorm1d, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\n        else:\n            self.weight = Parameter(torch.ones(num_features), requires_grad=False)\n            self.bias = Parameter(torch.zeros(num_features), requires_grad=False)\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n        self.reset_parameters()\n\n\nbatch_size = 500\nepochs = 1000\n\nfor hid_size in [32*2**i for i in range(8)]:\n\n    net_1 = torch.nn.Sequential(\n        torch.nn.Linear(hid_size,hid_size),\n        torch.nn.BatchNorm1d(affine=False, num_features=hid_size),\n        torch.nn.Linear(hid_size,2),\n    )\n    net_1.cuda()\n\n    net_2 = torch.nn.Sequential(\n        torch.nn.Linear(hid_size,hid_size),\n        BatchNorm1d(affine=False, num_features=hid_size),\n        torch.nn.Linear(hid_size,2),\n    )\n    net_2.cuda()\n\n    x = torch.autograd.Variable(torch.randn(batch_size,hid_size))\n    y = torch.autograd.Variable(torch.LongTensor([1, 0] * (batch_size//2)))\n\n    x = x.cuda()\n    y = y.cuda()\n\n    for id, net in enumerate([net_1, net_2]):\n\n        loss = torch.nn.CrossEntropyLoss()\n        optim = torch.optim.SGD([p for p in net.parameters() if p.requires_grad == True], lr=0.1)\n\n        t = 0\n        for i in range(epochs):\n            torch.cuda.synchronize()\n            t0 = time.time()\n            optim.zero_grad()\n            p = net(x)\n            loss(p, y).backward()\n            optim.step()\n            torch.cuda.synchronize()\n            t += time.time() - t0\n\n        print('Net {} with hidden size {}: {}'.format(id+1, hid_size, t))\n</code></pre>", "body_text": "The script below has two networks Net 1 and Net 2. The difference between Net 1 and Net 2 is\nNet 1 (standard Batchnorm1d)\nif self.affine:\n    self.weight = Parameter(torch.Tensor(num_features))\n    self.bias = Parameter(torch.Tensor(num_features))\nelse:\n    self.register_parameter('weight', None)\n    self.register_parameter('bias', None)\n\nNet 2 where the init of Batchnorm is changed to\nif self.affine:\n    self.weight = Parameter(torch.Tensor(num_features))\n    self.bias = Parameter(torch.Tensor(num_features))\nelse:\n    self.weight = Parameter(torch.ones(num_features), requires_grad=False)\n    self.bias = Parameter(torch.zeros(num_features), requires_grad=False)\n\nI ran the script below with pytorch 3.1 and pytorch 4.1:\npytorch 3.1\nhid_sz\tNet 1\tNet 2\n    32\t1.03\t1.43\n    64\t0.94\t0.87\n   128\t0.91\t0.90\n   256\t0.88\t0.80\n   512\t1.01\t0.94\n  1024\t1.93\t0.96\n  2048\t3.52\t1.54\n  4096\t8.05\t4.72\n\npytorch 4.1\nhid_sz\tNet 1\tNet 2\n    32\t1.12\t1.21\n    64\t1.14\t1.22\n   128\t1.15\t1.22\n   256\t1.15\t1.23\n   512\t1.29\t1.29\n  1024\t2.04\t1.32\n  2048\t3.65\t1.75\n  4096\t8.42\t4.77\n\nThe timings show that from a hidden size of 1024 onwards it is faster to use the Batchnorm from Net 2, i.e. to do a identity transformation in CUDNN instead of doing THNN BN. Also there seems to be a regression from 3.1 to 4.1. In general, for smaller hidden sizes THNN is not significantly faster.\nThe simple fix would be to implement the init for _Batchnorm like in the example code, i.e. do a identity transform in CUDNN.\nTo Reproduce\nSteps to reproduce the behavior:\nimport time\nimport torch\nfrom torch.nn import Parameter\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nclass BatchNorm1d(_BatchNorm):\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        super(BatchNorm1d, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\n        else:\n            self.weight = Parameter(torch.ones(num_features), requires_grad=False)\n            self.bias = Parameter(torch.zeros(num_features), requires_grad=False)\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n        self.reset_parameters()\n\n\nbatch_size = 500\nepochs = 1000\n\nfor hid_size in [32*2**i for i in range(8)]:\n\n    net_1 = torch.nn.Sequential(\n        torch.nn.Linear(hid_size,hid_size),\n        torch.nn.BatchNorm1d(affine=False, num_features=hid_size),\n        torch.nn.Linear(hid_size,2),\n    )\n    net_1.cuda()\n\n    net_2 = torch.nn.Sequential(\n        torch.nn.Linear(hid_size,hid_size),\n        BatchNorm1d(affine=False, num_features=hid_size),\n        torch.nn.Linear(hid_size,2),\n    )\n    net_2.cuda()\n\n    x = torch.autograd.Variable(torch.randn(batch_size,hid_size))\n    y = torch.autograd.Variable(torch.LongTensor([1, 0] * (batch_size//2)))\n\n    x = x.cuda()\n    y = y.cuda()\n\n    for id, net in enumerate([net_1, net_2]):\n\n        loss = torch.nn.CrossEntropyLoss()\n        optim = torch.optim.SGD([p for p in net.parameters() if p.requires_grad == True], lr=0.1)\n\n        t = 0\n        for i in range(epochs):\n            torch.cuda.synchronize()\n            t0 = time.time()\n            optim.zero_grad()\n            p = net(x)\n            loss(p, y).backward()\n            optim.step()\n            torch.cuda.synchronize()\n            t += time.time() - t0\n\n        print('Net {} with hidden size {}: {}'.format(id+1, hid_size, t))", "body": "The script below has two networks Net 1 and Net 2. The difference between Net 1 and Net 2 is\r\n\r\nNet 1 (standard Batchnorm1d) \r\n\r\n    if self.affine:\r\n        self.weight = Parameter(torch.Tensor(num_features))\r\n        self.bias = Parameter(torch.Tensor(num_features))\r\n    else:\r\n        self.register_parameter('weight', None)\r\n        self.register_parameter('bias', None)\r\n           \r\n\r\nNet 2 where the init of Batchnorm is changed to\r\n\r\n    if self.affine:\r\n        self.weight = Parameter(torch.Tensor(num_features))\r\n        self.bias = Parameter(torch.Tensor(num_features))\r\n    else:\r\n        self.weight = Parameter(torch.ones(num_features), requires_grad=False)\r\n        self.bias = Parameter(torch.zeros(num_features), requires_grad=False)\r\n\r\nI ran the script below with pytorch 3.1 and pytorch 4.1: \r\n\r\npytorch 3.1\r\n\r\n    hid_sz\tNet 1\tNet 2\r\n        32\t1.03\t1.43\r\n        64\t0.94\t0.87\r\n       128\t0.91\t0.90\r\n       256\t0.88\t0.80\r\n       512\t1.01\t0.94\r\n      1024\t1.93\t0.96\r\n      2048\t3.52\t1.54\r\n      4096\t8.05\t4.72\r\n\r\npytorch 4.1\r\n\r\n    hid_sz\tNet 1\tNet 2\r\n        32\t1.12\t1.21\r\n        64\t1.14\t1.22\r\n       128\t1.15\t1.22\r\n       256\t1.15\t1.23\r\n       512\t1.29\t1.29\r\n      1024\t2.04\t1.32\r\n      2048\t3.65\t1.75\r\n      4096\t8.42\t4.77\r\n\r\nThe timings show that from a hidden size of 1024 onwards it is faster to use the Batchnorm from Net 2, i.e. to do a identity transformation in CUDNN instead of doing THNN BN. Also there seems to be a regression from 3.1 to 4.1. In general, for smaller hidden sizes THNN is not significantly faster.\r\n\r\nThe simple fix would be to implement the init for _Batchnorm like in the example code, i.e. do a identity transform in CUDNN. \r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n\r\n\r\n    import time\r\n    import torch\r\n    from torch.nn import Parameter\r\n    from torch.nn.modules.batchnorm import _BatchNorm\r\n    \r\n    class BatchNorm1d(_BatchNorm):\r\n    \r\n        def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\r\n            super(BatchNorm1d, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\r\n            self.num_features = num_features\r\n            self.affine = affine\r\n            self.eps = eps\r\n            self.momentum = momentum\r\n            if self.affine:\r\n                self.weight = Parameter(torch.Tensor(num_features))\r\n                self.bias = Parameter(torch.Tensor(num_features))\r\n            else:\r\n                self.weight = Parameter(torch.ones(num_features), requires_grad=False)\r\n                self.bias = Parameter(torch.zeros(num_features), requires_grad=False)\r\n            self.register_buffer('running_mean', torch.zeros(num_features))\r\n            self.register_buffer('running_var', torch.ones(num_features))\r\n            self.reset_parameters()\r\n    \r\n    \r\n    batch_size = 500\r\n    epochs = 1000\r\n    \r\n    for hid_size in [32*2**i for i in range(8)]:\r\n    \r\n        net_1 = torch.nn.Sequential(\r\n            torch.nn.Linear(hid_size,hid_size),\r\n            torch.nn.BatchNorm1d(affine=False, num_features=hid_size),\r\n            torch.nn.Linear(hid_size,2),\r\n        )\r\n        net_1.cuda()\r\n    \r\n        net_2 = torch.nn.Sequential(\r\n            torch.nn.Linear(hid_size,hid_size),\r\n            BatchNorm1d(affine=False, num_features=hid_size),\r\n            torch.nn.Linear(hid_size,2),\r\n        )\r\n        net_2.cuda()\r\n    \r\n        x = torch.autograd.Variable(torch.randn(batch_size,hid_size))\r\n        y = torch.autograd.Variable(torch.LongTensor([1, 0] * (batch_size//2)))\r\n    \r\n        x = x.cuda()\r\n        y = y.cuda()\r\n    \r\n        for id, net in enumerate([net_1, net_2]):\r\n    \r\n            loss = torch.nn.CrossEntropyLoss()\r\n            optim = torch.optim.SGD([p for p in net.parameters() if p.requires_grad == True], lr=0.1)\r\n    \r\n            t = 0\r\n            for i in range(epochs):\r\n                torch.cuda.synchronize()\r\n                t0 = time.time()\r\n                optim.zero_grad()\r\n                p = net(x)\r\n                loss(p, y).backward()\r\n                optim.step()\r\n                torch.cuda.synchronize()\r\n                t += time.time() - t0\r\n    \r\n            print('Net {} with hidden size {}: {}'.format(id+1, hid_size, t))\r\n\r\n"}