{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7464", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7464/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7464/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7464/events", "html_url": "https://github.com/pytorch/pytorch/issues/7464", "id": 321915198, "node_id": "MDU6SXNzdWUzMjE5MTUxOTg=", "number": 7464, "title": "kldivloss doc issues", "user": {"login": "hughperkins", "id": 123560, "node_id": "MDQ6VXNlcjEyMzU2MA==", "avatar_url": "https://avatars2.githubusercontent.com/u/123560?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hughperkins", "html_url": "https://github.com/hughperkins", "followers_url": "https://api.github.com/users/hughperkins/followers", "following_url": "https://api.github.com/users/hughperkins/following{/other_user}", "gists_url": "https://api.github.com/users/hughperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/hughperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hughperkins/subscriptions", "organizations_url": "https://api.github.com/users/hughperkins/orgs", "repos_url": "https://api.github.com/users/hughperkins/repos", "events_url": "https://api.github.com/users/hughperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/hughperkins/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-05-10T12:33:18Z", "updated_at": "2018-05-13T19:44:25Z", "closed_at": "2018-05-13T19:44:25Z", "author_association": "CONTRIBUTOR", "body_html": "<ul>\n<li>not clearly stated that <code>y</code> should not be log probabilities</li>\n<li><code>l(x,y)</code> is defined both as <code>L</code> and as <code>mean(L)</code></li>\n<li>not defined that <code>l_n</code>, <code>y_n</code> and <code>x_n</code> are each tensors, associated with a single data example</li>\n</ul>\n<p>(I basically had to search through discuss.pytorch.org for examples, in order to use it). Actually, in fact, what I ended up doing was write my own kl-divergence implementation, and comparing the output with that of kldivloss, until the kldivloss one matched the self-coded one :P</p>", "body_text": "not clearly stated that y should not be log probabilities\nl(x,y) is defined both as L and as mean(L)\nnot defined that l_n, y_n and x_n are each tensors, associated with a single data example\n\n(I basically had to search through discuss.pytorch.org for examples, in order to use it). Actually, in fact, what I ended up doing was write my own kl-divergence implementation, and comparing the output with that of kldivloss, until the kldivloss one matched the self-coded one :P", "body": "- not clearly stated that `y` should not be log probabilities\r\n- `l(x,y)` is defined both as `L` and as `mean(L)`\r\n- not defined that `l_n`, `y_n` and `x_n` are each tensors, associated with a single data example\r\n\r\n(I basically had to search through discuss.pytorch.org for examples, in order to use it). Actually, in fact, what I ended up doing was write my own kl-divergence implementation, and comparing the output with that of kldivloss, until the kldivloss one matched the self-coded one :P"}