{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/253034712", "html_url": "https://github.com/pytorch/pytorch/issues/84#issuecomment-253034712", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/84", "id": 253034712, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzAzNDcxMg==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-11T20:24:40Z", "updated_at": "2016-10-11T20:25:02Z", "author_association": "MEMBER", "body_html": "<p>I actually don't think it's a good idea. All optimization we do makes sense only for scalar valued functions, and implicit initialization of a scalar \"grad output\" to 1 is sensible in this case, because it gives us correct gradient values. We also allow starting backward from an arbitrary location in the graph, as long as you provide gradient w.r.t. each element of the tensor, because it's also perfectly valid mathematically, but I'm not sure why should we interpret \"backward from a tensor\" as \"backward from a sum for tensor elements\".</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1190634\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rbgirshick\">@rbgirshick</a> when do you think it would be useful?</p>", "body_text": "I actually don't think it's a good idea. All optimization we do makes sense only for scalar valued functions, and implicit initialization of a scalar \"grad output\" to 1 is sensible in this case, because it gives us correct gradient values. We also allow starting backward from an arbitrary location in the graph, as long as you provide gradient w.r.t. each element of the tensor, because it's also perfectly valid mathematically, but I'm not sure why should we interpret \"backward from a tensor\" as \"backward from a sum for tensor elements\".\n@rbgirshick when do you think it would be useful?", "body": "I actually don't think it's a good idea. All optimization we do makes sense only for scalar valued functions, and implicit initialization of a scalar \"grad output\" to 1 is sensible in this case, because it gives us correct gradient values. We also allow starting backward from an arbitrary location in the graph, as long as you provide gradient w.r.t. each element of the tensor, because it's also perfectly valid mathematically, but I'm not sure why should we interpret \"backward from a tensor\" as \"backward from a sum for tensor elements\".\n\n@rbgirshick when do you think it would be useful?\n"}