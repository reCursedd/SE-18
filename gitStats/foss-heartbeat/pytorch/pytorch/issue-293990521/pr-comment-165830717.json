{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/165830717", "pull_request_review_id": 93832439, "id": 165830717, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NTgzMDcxNw==", "diff_hunk": "@@ -39,79 +43,50 @@ struct edge_hasher {\n   }\n };\n \n-// TODO: separate is_executable and next_functions\n-// State used to create \"backward\" functions\n-struct FunctionFlags {\n-  // Roughly speaking, is_executable corresponds to requires_grad.\n-  // It's true if any input requires grad and gradient calculation is enabled.\n-  // See http://pytorch.org/docs/notes/autograd.html for more details.\n-  bool is_executable = false;\n-  // What functions take the output of this function as input.\n-  // There is one function per output of this function.\n-  function_list next_functions;\n-};\n-\n namespace detail {\n-\n-// Why can't we just combine the set_variable and set_tensor variants\n-// into one set of overloads?  The problem is Variable is convertible\n-// to both Tensor and ArrayRef<Variable>, making the overload ambiguous.\n-\n-// Invariant: this function unconditionally calls f.next_functions.emplace_back\n-inline void set_function_flags(FunctionFlags& f, const Variable& var) {\n-  if (!var.defined()) {\n-    f.next_functions.emplace_back();\n-    return;\n-  }\n-  f.is_executable |= var.requires_grad();\n-  if (var.grad_fn()) {\n-    f.next_functions.emplace_back(var.grad_fn(), var.output_nr());\n-  } else if (var.requires_grad()) {\n-    f.next_functions.emplace_back(var.grad_accumulator(), 0);\n-  } else {\n-    f.next_functions.emplace_back();\n+inline edge_type make_edge(const Variable &variable) {\n+  if (variable.defined()) {\n+    if (variable.grad_fn() != nullptr) {\n+      return {variable.grad_fn(), variable.output_nr()};\n+    } else if (variable.requires_grad()) {\n+      return {variable.grad_accumulator(), 0};\n+    }\n   }\n+  return {nullptr, 0};\n }\n \n-struct SetFunctionFlags : IterArgs<SetFunctionFlags> {\n-  FunctionFlags& out;\n-  SetFunctionFlags(FunctionFlags& out) : out(out) {}\n-  using IterArgs<SetFunctionFlags>::operator();\n-  void operator()(const Variable& v) { set_function_flags(out, v); }\n-};\n-\n-struct SetTensorFunctionFlags : IterArgs<SetTensorFunctionFlags> {\n-  FunctionFlags& out;\n-  SetTensorFunctionFlags(FunctionFlags& out) : out(out) {}\n-  using IterArgs<SetTensorFunctionFlags>::operator();\n-  void operator()(const Tensor& t) {\n-    set_function_flags(out, static_cast<const Variable&>(t));\n+struct MakeNextFunctionList : IterArgs<MakeNextFunctionList> {\n+  function_list next_functions;\n+  using IterArgs<MakeNextFunctionList>::operator();\n+  void operator()(const Variable& variable) {\n+    next_functions.push_back(make_edge(variable));\n   }\n };\n+} // namespace detail\n \n+// Returns true if any of the variables in the list require a gradient.\n+inline bool any_variable_requires_grad(const variable_list& variables) {\n+  return std::any_of(\n+      variables.begin(), variables.end(), [](const Variable& variable) {\n+        return variable.requires_grad();\n+      });\n+}\n \n-} // namespace detail\n+template <typename... Variables>\n+function_list get_next_functions(Variables&&... variables) {\n+  if (!GradMode::is_enabled()) return {};\n+  detail::MakeNextFunctionList make;\n+  make.apply(std::forward<Variables>(variables)...);\n+  return std::move(make.next_functions);", "path": "torch/csrc/autograd/function.h", "position": 109, "original_position": 109, "commit_id": "e4aeb31decc4c3f77f519318e16e08f86a717fb3", "original_commit_id": "e4aeb31decc4c3f77f519318e16e08f86a717fb3", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "RVO will not take place for members. RVO (Named RVO to be precise) will only take place in very simple cases, basically when the value returned is declared on its own and has the same type as the return type (`X x; ...; return x;`). From [cppreference](http://en.cppreference.com/w/cpp/language/copy_elision):\r\n> If a function returns a class type by value, and the return statement's expression is the name of a non-volatile object with automatic storage duration ... then copy/move (since C++11) is omitted.\r\n\r\nI.e. the return expression literally has to be the name of a value of the type. Otherwise a copy will occur.\r\nhttps://stackoverflow.com/questions/12953127/what-are-copy-elision-and-return-value-optimization/12953129", "created_at": "2018-02-04T01:32:28Z", "updated_at": "2018-11-23T15:39:05Z", "html_url": "https://github.com/pytorch/pytorch/pull/5018#discussion_r165830717", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5018", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/165830717"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5018#discussion_r165830717"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5018"}}, "body_html": "<p>RVO will not take place for members. RVO (Named RVO to be precise) will only take place in very simple cases, basically when the value returned is declared on its own and has the same type as the return type (<code>X x; ...; return x;</code>). From <a href=\"http://en.cppreference.com/w/cpp/language/copy_elision\" rel=\"nofollow\">cppreference</a>:</p>\n<blockquote>\n<p>If a function returns a class type by value, and the return statement's expression is the name of a non-volatile object with automatic storage duration ... then copy/move (since C++11) is omitted.</p>\n</blockquote>\n<p>I.e. the return expression literally has to be the name of a value of the type. Otherwise a copy will occur.<br>\n<a href=\"https://stackoverflow.com/questions/12953127/what-are-copy-elision-and-return-value-optimization/12953129\" rel=\"nofollow\">https://stackoverflow.com/questions/12953127/what-are-copy-elision-and-return-value-optimization/12953129</a></p>", "body_text": "RVO will not take place for members. RVO (Named RVO to be precise) will only take place in very simple cases, basically when the value returned is declared on its own and has the same type as the return type (X x; ...; return x;). From cppreference:\n\nIf a function returns a class type by value, and the return statement's expression is the name of a non-volatile object with automatic storage duration ... then copy/move (since C++11) is omitted.\n\nI.e. the return expression literally has to be the name of a value of the type. Otherwise a copy will occur.\nhttps://stackoverflow.com/questions/12953127/what-are-copy-elision-and-return-value-optimization/12953129", "in_reply_to_id": 165829962}