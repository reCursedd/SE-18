{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229167332", "pull_request_review_id": 169601838, "id": 229167332, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyOTE2NzMzMg==", "diff_hunk": "@@ -457,19 +459,17 @@ struct CAFFE2_API IValue final {\n \n // Future\n struct C10_EXPORT ivalue::Future final : c10::intrusive_ptr_target {\n-  explicit Future(IValue result_) : result(result_), ready(true) {}\n+  explicit Future(std::future<IValue>&& future_) : future(std::move(future_)) {}\n \n-  IValue get() const {\n-    AT_ASSERT(ready);\n-    return result;\n+  IValue get() {", "path": "aten/src/ATen/core/ivalue.h", "position": null, "original_position": 18, "commit_id": "928476a76fc9249e71a8a523919ab91bc49e5b2e", "original_commit_id": "d6a86359da6c1a35792defdfc2482772175fb6a5", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "This patch is a good way of testing the multi-threaded perf before we add a cooperative thread pool. However, this call to `future.get()` is going to cause the caller to block if the future is not ready. We eventually want a cooperative threading model, where when this method is called by the interpreter, if the future is not ready, then the interpreter suspends itself, schedules a task to complete itself, and then returns a Future to caller of the interpreter that can be waited on to get the result of the interpreter. Calling `wait` on the future would pull the current thread into the group of worker threads until that Future is finished.\r\n\r\nI think the next step is to implement this \"interpreter suspend\" behavior, even if we do not have a thread pool that can use it. That way the only remaining step is to integrate the threadpool.", "created_at": "2018-10-30T03:37:34Z", "updated_at": "2018-11-23T15:53:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/13212#discussion_r229167332", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13212", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/229167332"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13212#discussion_r229167332"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13212"}}, "body_html": "<p>This patch is a good way of testing the multi-threaded perf before we add a cooperative thread pool. However, this call to <code>future.get()</code> is going to cause the caller to block if the future is not ready. We eventually want a cooperative threading model, where when this method is called by the interpreter, if the future is not ready, then the interpreter suspends itself, schedules a task to complete itself, and then returns a Future to caller of the interpreter that can be waited on to get the result of the interpreter. Calling <code>wait</code> on the future would pull the current thread into the group of worker threads until that Future is finished.</p>\n<p>I think the next step is to implement this \"interpreter suspend\" behavior, even if we do not have a thread pool that can use it. That way the only remaining step is to integrate the threadpool.</p>", "body_text": "This patch is a good way of testing the multi-threaded perf before we add a cooperative thread pool. However, this call to future.get() is going to cause the caller to block if the future is not ready. We eventually want a cooperative threading model, where when this method is called by the interpreter, if the future is not ready, then the interpreter suspends itself, schedules a task to complete itself, and then returns a Future to caller of the interpreter that can be waited on to get the result of the interpreter. Calling wait on the future would pull the current thread into the group of worker threads until that Future is finished.\nI think the next step is to implement this \"interpreter suspend\" behavior, even if we do not have a thread pool that can use it. That way the only remaining step is to integrate the threadpool."}