{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/133345597", "pull_request_review_id": 56515919, "id": 133345597, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMzM0NTU5Nw==", "diff_hunk": "@@ -198,7 +199,8 @@ def forward(self, input, offsets=None):\n                                  \" fixed length sequences. However, found \"\n                                  \"offsets of type {}\".format(type(offsets)))\n             else:\n-                offsets = input.data.new(input.size(0)).fill_(input.size(1))\n+                offsets = Variable(torch.arange(0, input.numel(), input.size(1)).long())", "path": "torch/nn/modules/sparse.py", "position": null, "original_position": 12, "commit_id": "444c220812986e6cf9b357a66c9a941d7432bf12", "original_commit_id": "723a3e516d6f6a7a76af9c2b6171611f94a89a36", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This won't work with CUDA inputs. This would be better:\r\n```python\r\noffsets = Variable(torch.arange(0, input.numel(), input.size(1), out=input.data.new().long()))\r\n```\r\n\r\nCan you also modify the test so that it checks CUDA implementation too? It'd be the best to refactor this function to take an additional `cast` arg (like [in here](https://github.com/ajfisch/pytorch/blob/master/test/test_torch.py?utf8=%E2%9C%93#L501-L520)), and then add CPU and CUDA tests that call it with different lambdas", "created_at": "2017-08-16T02:12:49Z", "updated_at": "2018-11-23T15:34:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/2429#discussion_r133345597", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2429", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/133345597"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2429#discussion_r133345597"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2429"}}, "body_html": "<p>This won't work with CUDA inputs. This would be better:</p>\n<div class=\"highlight highlight-source-python\"><pre>offsets <span class=\"pl-k\">=</span> Variable(torch.arange(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">input</span>.numel(), <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">out</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">input</span>.data.new().long()))</pre></div>\n<p>Can you also modify the test so that it checks CUDA implementation too? It'd be the best to refactor this function to take an additional <code>cast</code> arg (like <a href=\"https://github.com/ajfisch/pytorch/blob/master/test/test_torch.py?utf8=%E2%9C%93#L501-L520\">in here</a>), and then add CPU and CUDA tests that call it with different lambdas</p>", "body_text": "This won't work with CUDA inputs. This would be better:\noffsets = Variable(torch.arange(0, input.numel(), input.size(1), out=input.data.new().long()))\nCan you also modify the test so that it checks CUDA implementation too? It'd be the best to refactor this function to take an additional cast arg (like in here), and then add CPU and CUDA tests that call it with different lambdas"}