{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2494", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2494/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2494/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2494/events", "html_url": "https://github.com/pytorch/pytorch/issues/2494", "id": 251503749, "node_id": "MDU6SXNzdWUyNTE1MDM3NDk=", "number": 2494, "title": "dimension issue of max function ", "user": {"login": "LiyuanLucasLiu", "id": 15102458, "node_id": "MDQ6VXNlcjE1MTAyNDU4", "avatar_url": "https://avatars3.githubusercontent.com/u/15102458?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LiyuanLucasLiu", "html_url": "https://github.com/LiyuanLucasLiu", "followers_url": "https://api.github.com/users/LiyuanLucasLiu/followers", "following_url": "https://api.github.com/users/LiyuanLucasLiu/following{/other_user}", "gists_url": "https://api.github.com/users/LiyuanLucasLiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/LiyuanLucasLiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LiyuanLucasLiu/subscriptions", "organizations_url": "https://api.github.com/users/LiyuanLucasLiu/orgs", "repos_url": "https://api.github.com/users/LiyuanLucasLiu/repos", "events_url": "https://api.github.com/users/LiyuanLucasLiu/events{/privacy}", "received_events_url": "https://api.github.com/users/LiyuanLucasLiu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-20T17:52:06Z", "updated_at": "2017-08-20T18:16:26Z", "closed_at": "2017-08-20T18:16:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I noticed something weird. Maybe it's a bug.</p>\n<pre><code>import torch as t\ntmp = t.LongTensor([2,3]).view(1,-1)\ntm, _ = tmp.max(0, keepdim=False)\nprint(tm)\n</code></pre>\n<p>would print</p>\n<pre><code>2 3\n[torch.LongTensor of size 1 * 2]\n</code></pre>\n<p>while it seems to me, the output should be <code>of size 2</code> instead of <code>of size 1 * 2</code> (since the keepdim is set to False)</p>", "body_text": "I noticed something weird. Maybe it's a bug.\nimport torch as t\ntmp = t.LongTensor([2,3]).view(1,-1)\ntm, _ = tmp.max(0, keepdim=False)\nprint(tm)\n\nwould print\n2 3\n[torch.LongTensor of size 1 * 2]\n\nwhile it seems to me, the output should be of size 2 instead of of size 1 * 2 (since the keepdim is set to False)", "body": "I noticed something weird. Maybe it's a bug.\r\n\r\n```\r\nimport torch as t\r\ntmp = t.LongTensor([2,3]).view(1,-1)\r\ntm, _ = tmp.max(0, keepdim=False)\r\nprint(tm)\r\n```\r\nwould print\r\n```\r\n2 3\r\n[torch.LongTensor of size 1 * 2]\r\n```\r\n\r\nwhile it seems to me, the output should be ```of size 2``` instead of ```of size 1 * 2``` (since the keepdim is set to False)"}