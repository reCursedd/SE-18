{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12070", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12070/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12070/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12070/events", "html_url": "https://github.com/pytorch/pytorch/pull/12070", "id": 363786982, "node_id": "MDExOlB1bGxSZXF1ZXN0MjE4MTM1Njg3", "number": 12070, "title": "[JIT] Adds support for NaN, +inf, -inf float scalars to CPU and CUDA fusers", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-25T22:15:30Z", "updated_at": "2018-09-28T21:13:15Z", "closed_at": "2018-09-28T21:13:15Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/12070", "html_url": "https://github.com/pytorch/pytorch/pull/12070", "diff_url": "https://github.com/pytorch/pytorch/pull/12070.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/12070.patch"}, "body_html": "<p>In current upstream float scalars are always written into kernels with:</p>\n<p><code>out &lt;&lt; std::scientific &lt;&lt; v &lt;&lt; \"f\";</code></p>\n<p>When the floats are special values like NaN, +inf, or -inf this produces nonsense that causes compilation to fail. This fix updates the conversion of float scalars to device-specific special values. The appropriate macros are added to the CPU and CUDA resource strings. Note that a NAN macro was not necessary on the CPU since math.h defines NAN.</p>\n<p>To verify this fix I updated the test_clamp_fusion test in test_jit.py. I wanted to test -inf, too, but -inf is not currently accepted by the interpreter.</p>\n<p>Edit:</p>\n<p>Forgot to mention, this partially addresses issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"363762142\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/12067\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/12067/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/12067\">#12067</a>.</p>", "body_text": "In current upstream float scalars are always written into kernels with:\nout << std::scientific << v << \"f\";\nWhen the floats are special values like NaN, +inf, or -inf this produces nonsense that causes compilation to fail. This fix updates the conversion of float scalars to device-specific special values. The appropriate macros are added to the CPU and CUDA resource strings. Note that a NAN macro was not necessary on the CPU since math.h defines NAN.\nTo verify this fix I updated the test_clamp_fusion test in test_jit.py. I wanted to test -inf, too, but -inf is not currently accepted by the interpreter.\nEdit:\nForgot to mention, this partially addresses issue #12067.", "body": "In current upstream float scalars are always written into kernels with:\r\n\r\n`out << std::scientific << v << \"f\";`\r\n\r\nWhen the floats are special values like NaN, +inf, or -inf this produces nonsense that causes compilation to fail. This fix updates the conversion of float scalars to device-specific special values. The appropriate macros are added to the CPU and CUDA resource strings. Note that a NAN macro was not necessary on the CPU since math.h defines NAN. \r\n\r\nTo verify this fix I updated the test_clamp_fusion test in test_jit.py. I wanted to test -inf, too, but -inf is not currently accepted by the interpreter. \r\n\r\nEdit:\r\n\r\nForgot to mention, this partially addresses issue #12067. "}