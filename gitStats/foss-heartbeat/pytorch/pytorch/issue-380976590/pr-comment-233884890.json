{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233884890", "pull_request_review_id": 175401785, "id": 233884890, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzg4NDg5MA==", "diff_hunk": "@@ -512,6 +596,141 @@ def test_reduce_lr_on_plateau8(self):\n                                       threshold=0.1, patience=5, cooldown=5)\n         self._test_reduce_lr_on_plateau(scheduler, targets, metrics, epochs)\n \n+    def test_compound_step_and_multistep_lr(self):", "path": "test/test_optim.py", "position": 341, "original_position": 228, "commit_id": "bda21bd32104fe4d2c6a56effa7c5bbc26f37742", "original_commit_id": "7d14f182b7c1a49c424cbd012d0b969a8e54a183", "user": {"login": "chandlerzuo", "id": 6797874, "node_id": "MDQ6VXNlcjY3OTc4NzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/6797874?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chandlerzuo", "html_url": "https://github.com/chandlerzuo", "followers_url": "https://api.github.com/users/chandlerzuo/followers", "following_url": "https://api.github.com/users/chandlerzuo/following{/other_user}", "gists_url": "https://api.github.com/users/chandlerzuo/gists{/gist_id}", "starred_url": "https://api.github.com/users/chandlerzuo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chandlerzuo/subscriptions", "organizations_url": "https://api.github.com/users/chandlerzuo/orgs", "repos_url": "https://api.github.com/users/chandlerzuo/repos", "events_url": "https://api.github.com/users/chandlerzuo/events{/privacy}", "received_events_url": "https://api.github.com/users/chandlerzuo/received_events", "type": "User", "site_admin": false}, "body": "I added 10 tests. First, I tested all combinations between StepLR, MultiStepLR, ExponentialLR and CosineAnnealing (6 combinations). Then I tested ReduceLROnPlateau + 1 of these modified changes (4 in total). The purpose of these tests is to see that combining different schedulers now works as intended.\r\n\r\nFor single implementations, there are already uni-tests for old implementations, and the new implementations already pass those tests, so that gives some validation. But, I can add some tests that implements the old codes explicitly for comparison as you suggested.", "created_at": "2018-11-15T15:20:24Z", "updated_at": "2018-11-23T15:54:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/14010#discussion_r233884890", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/14010", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233884890"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/14010#discussion_r233884890"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/14010"}}, "body_html": "<p>I added 10 tests. First, I tested all combinations between StepLR, MultiStepLR, ExponentialLR and CosineAnnealing (6 combinations). Then I tested ReduceLROnPlateau + 1 of these modified changes (4 in total). The purpose of these tests is to see that combining different schedulers now works as intended.</p>\n<p>For single implementations, there are already uni-tests for old implementations, and the new implementations already pass those tests, so that gives some validation. But, I can add some tests that implements the old codes explicitly for comparison as you suggested.</p>", "body_text": "I added 10 tests. First, I tested all combinations between StepLR, MultiStepLR, ExponentialLR and CosineAnnealing (6 combinations). Then I tested ReduceLROnPlateau + 1 of these modified changes (4 in total). The purpose of these tests is to see that combining different schedulers now works as intended.\nFor single implementations, there are already uni-tests for old implementations, and the new implementations already pass those tests, so that gives some validation. But, I can add some tests that implements the old codes explicitly for comparison as you suggested.", "in_reply_to_id": 233707865}