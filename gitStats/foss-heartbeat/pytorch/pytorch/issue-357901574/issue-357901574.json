{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11365", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11365/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11365/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11365/events", "html_url": "https://github.com/pytorch/pytorch/pull/11365", "id": 357901574, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEzODAzMjc0", "number": 11365, "title": "use batched gemm from mkl on torch.bmm when mkl is available", "user": {"login": "mingfeima", "id": 20233731, "node_id": "MDQ6VXNlcjIwMjMzNzMx", "avatar_url": "https://avatars0.githubusercontent.com/u/20233731?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mingfeima", "html_url": "https://github.com/mingfeima", "followers_url": "https://api.github.com/users/mingfeima/followers", "following_url": "https://api.github.com/users/mingfeima/following{/other_user}", "gists_url": "https://api.github.com/users/mingfeima/gists{/gist_id}", "starred_url": "https://api.github.com/users/mingfeima/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mingfeima/subscriptions", "organizations_url": "https://api.github.com/users/mingfeima/orgs", "repos_url": "https://api.github.com/users/mingfeima/repos", "events_url": "https://api.github.com/users/mingfeima/events{/privacy}", "received_events_url": "https://api.github.com/users/mingfeima/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-09-07T02:59:46Z", "updated_at": "2018-11-23T15:50:45Z", "closed_at": "2018-09-12T04:51:10Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11365", "html_url": "https://github.com/pytorch/pytorch/pull/11365", "diff_url": "https://github.com/pytorch/pytorch/pull/11365.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11365.patch"}, "body_html": "<p>This PR uses mkl batched gemm for <code>torch.bmm</code> when mkl is available. The current <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/TH/generic/THTensorMoreMath.cpp#L7\">logic </a>dealing with <code>torch.bmm</code> is to do <code>batch_size</code> iterations of gemm. From the performance point of view, this should be OK in case the gemm size is large enough. However, in many cases, the gemm size is relatively small and not efficient.</p>\n<p>One scenario it <a href=\"https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/global_attention.py#L194\">globalAttention</a> calculation of NMT, where<br>\n<code>mat1</code>: N * 1 * T<br>\n<code>mat2</code>: N * T * H<br>\nN refers to batch size, T refers to timestep and H is the hidden size.<br>\nthere the gemm size is relatively small, MKL has batched gemm APIs which is beneficial in case dealing with batched small gemms.</p>\n<p>The following script is used for benchmarking and testing the PR. On Xeon skylake 8180 (2 sockets * 28 cores), it runs <code>0.81ms</code> without the PR and <code>0.45ms</code> with the PR.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> time <span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> os\n\nN <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>\nT <span class=\"pl-k\">=</span> <span class=\"pl-c1\">30</span>\nH <span class=\"pl-k\">=</span> <span class=\"pl-c1\">500</span>\ncount <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bench_bmm</span>():\n    mat1 <span class=\"pl-k\">=</span> torch.randn(N, <span class=\"pl-c1\">1</span>, T)\n    mat2 <span class=\"pl-k\">=</span> torch.randn(N, T, H)\n\n    tstart <span class=\"pl-k\">=</span> time()\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(count):\n        res <span class=\"pl-k\">=</span> torch.bmm(mat1, mat2)\n    tend <span class=\"pl-k\">=</span> time()\n    t <span class=\"pl-k\">=</span> (tend<span class=\"pl-k\">-</span>tstart)<span class=\"pl-k\">/</span>count<span class=\"pl-k\">*</span><span class=\"pl-c1\">1000</span>\n    flops <span class=\"pl-k\">=</span> N<span class=\"pl-k\">*</span><span class=\"pl-c1\">1</span><span class=\"pl-k\">*</span>T<span class=\"pl-k\">*</span>H<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span> <span class=\"pl-k\">/</span> t <span class=\"pl-k\">/</span> <span class=\"pl-c1\">1000000</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>run torch.bmm:<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>total time     : <span class=\"pl-c1\">%.2f</span> s<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (tend<span class=\"pl-k\">-</span>tstart))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>each iteration : <span class=\"pl-c1\">%.2f</span> ms <span class=\"pl-c1\">%.2f</span> GFlops<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (t, flops))\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_bmm</span>(<span class=\"pl-smi\">trans_A</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">trans_B</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    I <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>testing torch.bmm mat1: <span class=\"pl-c1\">%s</span>, mat2 <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>T<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">if</span> trans_A <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>N<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>T<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">if</span> trans_B <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>N<span class=\"pl-pds\">\"</span></span>))\n    mat1 <span class=\"pl-k\">=</span> torch.randn(N, T, I).transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">if</span> trans_A <span class=\"pl-k\">else</span> torch.randn(N, I, T)\n    mat2 <span class=\"pl-k\">=</span> torch.randn(N, H, T).transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">if</span> trans_B <span class=\"pl-k\">else</span> torch.randn(N, T, H)\n    mat1_ <span class=\"pl-k\">=</span> mat1.clone()\n    mat2_ <span class=\"pl-k\">=</span> mat2.clone()\n\n    res <span class=\"pl-k\">=</span> torch.bmm(mat1, mat2)\n    res_ <span class=\"pl-k\">=</span> torch.Tensor(N, I, H)\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n        res_[i] <span class=\"pl-k\">=</span> mat1_[i].matmul(mat2_[i])\n\n    <span class=\"pl-k\">for</span> ii <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(N):\n        <span class=\"pl-k\">for</span> jj <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(I):\n            <span class=\"pl-k\">for</span> kk <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(H):\n                val1 <span class=\"pl-k\">=</span> res[ii][jj][kk]\n                val2 <span class=\"pl-k\">=</span> res_[ii][jj][kk]\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">abs</span>(val1<span class=\"pl-k\">-</span>val2) <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1e-5</span>:\n                    <span class=\"pl-k\">continue</span>\n                <span class=\"pl-k\">else</span>:\n                    <span class=\"pl-c1\">print</span>(res[ii][jj][kk], res_[ii][jj][kk], <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>not equal, case FAIL<span class=\"pl-pds\">\"</span></span>)\n                    <span class=\"pl-k\">return</span>\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>PASS<span class=\"pl-pds\">\"</span></span>)\n\nbench_bmm()\ntest_bmm(<span class=\"pl-v\">trans_A</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">trans_B</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\ntest_bmm(<span class=\"pl-v\">trans_A</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">trans_B</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\ntest_bmm(<span class=\"pl-v\">trans_A</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">trans_B</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ntest_bmm(<span class=\"pl-v\">trans_A</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">trans_B</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>", "body_text": "This PR uses mkl batched gemm for torch.bmm when mkl is available. The current logic dealing with torch.bmm is to do batch_size iterations of gemm. From the performance point of view, this should be OK in case the gemm size is large enough. However, in many cases, the gemm size is relatively small and not efficient.\nOne scenario it globalAttention calculation of NMT, where\nmat1: N * 1 * T\nmat2: N * T * H\nN refers to batch size, T refers to timestep and H is the hidden size.\nthere the gemm size is relatively small, MKL has batched gemm APIs which is beneficial in case dealing with batched small gemms.\nThe following script is used for benchmarking and testing the PR. On Xeon skylake 8180 (2 sockets * 28 cores), it runs 0.81ms without the PR and 0.45ms with the PR.\nimport torch\nfrom time import time\nimport os\n\nN = 128\nT = 30\nH = 500\ncount = 1000\n\ndef bench_bmm():\n    mat1 = torch.randn(N, 1, T)\n    mat2 = torch.randn(N, T, H)\n\n    tstart = time()\n    for i in range(count):\n        res = torch.bmm(mat1, mat2)\n    tend = time()\n    t = (tend-tstart)/count*1000\n    flops = N*1*T*H*2 / t / 1000000\n    print(\"run torch.bmm:\")\n    print(\"total time     : %.2f s\" % (tend-tstart))\n    print(\"each iteration : %.2f ms %.2f GFlops\" % (t, flops))\n\ndef test_bmm(trans_A=False, trans_B=False):\n    I = 10\n    print(\"testing torch.bmm mat1: %s, mat2 %s\" % (\"T\" if trans_A else \"N\", \"T\" if trans_B else \"N\"))\n    mat1 = torch.randn(N, T, I).transpose(1, 2) if trans_A else torch.randn(N, I, T)\n    mat2 = torch.randn(N, H, T).transpose(1, 2) if trans_B else torch.randn(N, T, H)\n    mat1_ = mat1.clone()\n    mat2_ = mat2.clone()\n\n    res = torch.bmm(mat1, mat2)\n    res_ = torch.Tensor(N, I, H)\n    for i in range(N):\n        res_[i] = mat1_[i].matmul(mat2_[i])\n\n    for ii in range(N):\n        for jj in range(I):\n            for kk in range(H):\n                val1 = res[ii][jj][kk]\n                val2 = res_[ii][jj][kk]\n                if abs(val1-val2) < 1e-5:\n                    continue\n                else:\n                    print(res[ii][jj][kk], res_[ii][jj][kk], \"not equal, case FAIL\")\n                    return\n\n    print(\"PASS\")\n\nbench_bmm()\ntest_bmm(trans_A=False, trans_B=False)\ntest_bmm(trans_A=True, trans_B=False)\ntest_bmm(trans_A=False, trans_B=True)\ntest_bmm(trans_A=True, trans_B=True)", "body": "This PR uses mkl batched gemm for `torch.bmm` when mkl is available. The current [logic ](https://github.com/pytorch/pytorch/blob/master/aten/src/TH/generic/THTensorMoreMath.cpp#L7)dealing with `torch.bmm` is to do `batch_size` iterations of gemm. From the performance point of view, this should be OK in case the gemm size is large enough. However, in many cases, the gemm size is relatively small and not efficient.\r\n\r\nOne scenario it [globalAttention](https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/global_attention.py#L194) calculation of NMT, where\r\n`mat1`: N * 1 * T\r\n`mat2`: N * T * H\r\nN refers to batch size, T refers to timestep and H is the hidden size.\r\nthere the gemm size is relatively small, MKL has batched gemm APIs which is beneficial in case dealing with batched small gemms.\r\n\r\nThe following script is used for benchmarking and testing the PR. On Xeon skylake 8180 (2 sockets * 28 cores), it runs `0.81ms` without the PR and `0.45ms` with the PR.\r\n```python\r\nimport torch\r\nfrom time import time\r\nimport os\r\n\r\nN = 128\r\nT = 30\r\nH = 500\r\ncount = 1000\r\n\r\ndef bench_bmm():\r\n    mat1 = torch.randn(N, 1, T)\r\n    mat2 = torch.randn(N, T, H)\r\n\r\n    tstart = time()\r\n    for i in range(count):\r\n        res = torch.bmm(mat1, mat2)\r\n    tend = time()\r\n    t = (tend-tstart)/count*1000\r\n    flops = N*1*T*H*2 / t / 1000000\r\n    print(\"run torch.bmm:\")\r\n    print(\"total time     : %.2f s\" % (tend-tstart))\r\n    print(\"each iteration : %.2f ms %.2f GFlops\" % (t, flops))\r\n\r\ndef test_bmm(trans_A=False, trans_B=False):\r\n    I = 10\r\n    print(\"testing torch.bmm mat1: %s, mat2 %s\" % (\"T\" if trans_A else \"N\", \"T\" if trans_B else \"N\"))\r\n    mat1 = torch.randn(N, T, I).transpose(1, 2) if trans_A else torch.randn(N, I, T)\r\n    mat2 = torch.randn(N, H, T).transpose(1, 2) if trans_B else torch.randn(N, T, H)\r\n    mat1_ = mat1.clone()\r\n    mat2_ = mat2.clone()\r\n\r\n    res = torch.bmm(mat1, mat2)\r\n    res_ = torch.Tensor(N, I, H)\r\n    for i in range(N):\r\n        res_[i] = mat1_[i].matmul(mat2_[i])\r\n\r\n    for ii in range(N):\r\n        for jj in range(I):\r\n            for kk in range(H):\r\n                val1 = res[ii][jj][kk]\r\n                val2 = res_[ii][jj][kk]\r\n                if abs(val1-val2) < 1e-5:\r\n                    continue\r\n                else:\r\n                    print(res[ii][jj][kk], res_[ii][jj][kk], \"not equal, case FAIL\")\r\n                    return\r\n\r\n    print(\"PASS\")\r\n\r\nbench_bmm()\r\ntest_bmm(trans_A=False, trans_B=False)\r\ntest_bmm(trans_A=True, trans_B=False)\r\ntest_bmm(trans_A=False, trans_B=True)\r\ntest_bmm(trans_A=True, trans_B=True)\r\n```"}