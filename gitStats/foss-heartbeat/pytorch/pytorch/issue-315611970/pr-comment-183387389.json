{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183387389", "pull_request_review_id": 114374152, "id": 183387389, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MzM4NzM4OQ==", "diff_hunk": "@@ -1909,6 +1909,41 @@ def _test_diagonal(self, dtype, device):\n     def test_diagonal(self):\n         self._test_diagonal(self, dtype=torch.float32, device='cpu')\n \n+    @unittest.skipIf(not TEST_NUMPY, 'Numpy not found')\n+    def test_diagonal_multidim(self):\n+        x = torch.randn(10, 11, 12, 13)\n+        xn = x.numpy()\n+        result = torch.diagonal(x, 2, 2, 3)\n+        expected = xn.diagonal(2, 2, 3)\n+        self.assertEqual(expected.shape, result.shape)\n+        self.assertTrue(np.allclose(expected, result.numpy()))\n+        result = torch.diagonal(x, 2)\n+        expected = torch.diagonal(x, 2, 0, 1)\n+        self.assertEqual(expected, result)\n+        result = torch.diagonal(x, -2, 1, 2)\n+        expected = xn.diagonal(-2, 1, 2)\n+        self.assertEqual(expected.shape, result.shape)\n+        self.assertTrue(np.allclose(expected, result.numpy()))\n+        result = torch.diagonal(x, 0, -2, -1)\n+        expected = xn.diagonal(0, -2, -1)\n+        self.assertEqual(expected.shape, result.shape)\n+        self.assertTrue(np.allclose(expected, result.numpy()))\n+        # test non-continguous\n+        xp = x.permute(1, 2, 3, 0)\n+        result = torch.diagonal(xp, 0, -2, -1)\n+        expected = xp.numpy().diagonal(0, -2, -1)\n+        self.assertEqual(expected.shape, result.shape)\n+        self.assertTrue(np.allclose(expected, result.numpy()))\n+        # test that the backward requires grad\n+        # we do this is because diagonal_backward uses inplace\n+        # operations and gradgradcheck does not catch whether\n+        # they works as expected", "path": "test/test_torch.py", "position": null, "original_position": 32, "commit_id": "71eb0d5b7ce223c63deed433516d7bf6ace9762f", "original_commit_id": "67d2734ecd6f748d7639efeab4270205313a227f", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "Because gradgradcheck doesn't complain when the derivative does not require grad. So when I used the diagonal in the backward, it passed gradgradcheck, but the gradient was marked as not needing a derivative, and double backward was zero.\r\nThe test is a bit strange, but I think it is a good test to have given the subtle backward behaviour below, can I move it to test_autograd.py then?\r\n", "created_at": "2018-04-23T13:08:25Z", "updated_at": "2018-11-23T15:42:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/6718#discussion_r183387389", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6718", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183387389"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6718#discussion_r183387389"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6718"}}, "body_html": "<p>Because gradgradcheck doesn't complain when the derivative does not require grad. So when I used the diagonal in the backward, it passed gradgradcheck, but the gradient was marked as not needing a derivative, and double backward was zero.<br>\nThe test is a bit strange, but I think it is a good test to have given the subtle backward behaviour below, can I move it to test_autograd.py then?</p>", "body_text": "Because gradgradcheck doesn't complain when the derivative does not require grad. So when I used the diagonal in the backward, it passed gradgradcheck, but the gradient was marked as not needing a derivative, and double backward was zero.\nThe test is a bit strange, but I think it is a good test to have given the subtle backward behaviour below, can I move it to test_autograd.py then?", "in_reply_to_id": 183349206}