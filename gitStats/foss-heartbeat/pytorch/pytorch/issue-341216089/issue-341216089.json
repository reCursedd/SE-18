{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9439", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9439/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9439/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9439/events", "html_url": "https://github.com/pytorch/pytorch/issues/9439", "id": 341216089, "node_id": "MDU6SXNzdWUzNDEyMTYwODk=", "number": 9439, "title": "OOM when using Adam optimizer compared to SGD when using same batch size.", "user": {"login": "limwenyao", "id": 23585543, "node_id": "MDQ6VXNlcjIzNTg1NTQz", "avatar_url": "https://avatars3.githubusercontent.com/u/23585543?v=4", "gravatar_id": "", "url": "https://api.github.com/users/limwenyao", "html_url": "https://github.com/limwenyao", "followers_url": "https://api.github.com/users/limwenyao/followers", "following_url": "https://api.github.com/users/limwenyao/following{/other_user}", "gists_url": "https://api.github.com/users/limwenyao/gists{/gist_id}", "starred_url": "https://api.github.com/users/limwenyao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/limwenyao/subscriptions", "organizations_url": "https://api.github.com/users/limwenyao/orgs", "repos_url": "https://api.github.com/users/limwenyao/repos", "events_url": "https://api.github.com/users/limwenyao/events{/privacy}", "received_events_url": "https://api.github.com/users/limwenyao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-07-14T07:14:15Z", "updated_at": "2018-07-14T11:13:53Z", "closed_at": "2018-07-14T11:13:52Z", "author_association": "NONE", "body_html": "<p>I tried training a VGG model with the torch.optim.Adam and torch.optim.SGD. I can train my model on SGD batch_size = 8 for SGD, but Adam throws me OOM error with batch_size 8, but it works for batch_size 4. Model is imported from torchvision.</p>\n<pre><code>model = vgg16(pretrained=True)\nmodel.fc = torch.nn.Linear(4096, len(data_ctr.class_dir))\nmodel.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\n# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n\nfor i in range(EPOCH):\n    model.train()\n    all_prediction = None\n    all_labels = None\n    for batch in tqdm(batch_ctr):\n        images = batch['image'].cuda().float()\n        labels_ = batch['labels'].cuda().long()\n        \n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels_)\n        loss.backward()\n        optimizer.step()\n</code></pre>\n<pre><code>Epoch 0\n  1%|\u258b                                                                                 | 1/112 [00:03&lt;06:38,  3.59s/it]\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-5-a2a7f14f2b5a&gt; in &lt;module&gt;()\n     27         loss = criterion(output, labels_)\n     28         #Backpropagation of the losses to compute gradients of all the weights.\n---&gt; 29         loss.backward()\n     30         #Update the weights, based on parameters defined in your optimizer.\n     31         optimizer.step()\n\nC:\\Users\\limwe\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\tensor.py in backward(self, gradient, retain_graph, create_graph)\n     91                 products. Defaults to ``False``.\n     92         \"\"\"\n---&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n     94 \n     95     def register_hook(self, hook):\n\nC:\\Users\\limwe\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     87     Variable._execution_engine.run_backward(\n     88         tensors, grad_tensors, retain_graph, create_graph,\n---&gt; 89         allow_unreachable=True)  # allow_unreachable flag\n     90 \n     91 \n\nRuntimeError: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524539825236\\work\\aten\\src\\thc\\generic/THCStorage.cu:58\n</code></pre>\n<ul>\n<li>PyTorch installed from conda</li>\n<li>OS: Windows 10</li>\n<li>PyTorch version: 0.4.0</li>\n<li>Python version: 3.5.2</li>\n<li>CUDA/cuDNN version: 8.0/5.1</li>\n<li>GPU models and configuration: NVIDIA 850M</li>\n</ul>", "body_text": "I tried training a VGG model with the torch.optim.Adam and torch.optim.SGD. I can train my model on SGD batch_size = 8 for SGD, but Adam throws me OOM error with batch_size 8, but it works for batch_size 4. Model is imported from torchvision.\nmodel = vgg16(pretrained=True)\nmodel.fc = torch.nn.Linear(4096, len(data_ctr.class_dir))\nmodel.cuda()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\n# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n\nfor i in range(EPOCH):\n    model.train()\n    all_prediction = None\n    all_labels = None\n    for batch in tqdm(batch_ctr):\n        images = batch['image'].cuda().float()\n        labels_ = batch['labels'].cuda().long()\n        \n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels_)\n        loss.backward()\n        optimizer.step()\n\nEpoch 0\n  1%|\u258b                                                                                 | 1/112 [00:03<06:38,  3.59s/it]\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-a2a7f14f2b5a> in <module>()\n     27         loss = criterion(output, labels_)\n     28         #Backpropagation of the losses to compute gradients of all the weights.\n---> 29         loss.backward()\n     30         #Update the weights, based on parameters defined in your optimizer.\n     31         optimizer.step()\n\nC:\\Users\\limwe\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\tensor.py in backward(self, gradient, retain_graph, create_graph)\n     91                 products. Defaults to ``False``.\n     92         \"\"\"\n---> 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n     94 \n     95     def register_hook(self, hook):\n\nC:\\Users\\limwe\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     87     Variable._execution_engine.run_backward(\n     88         tensors, grad_tensors, retain_graph, create_graph,\n---> 89         allow_unreachable=True)  # allow_unreachable flag\n     90 \n     91 \n\nRuntimeError: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524539825236\\work\\aten\\src\\thc\\generic/THCStorage.cu:58\n\n\nPyTorch installed from conda\nOS: Windows 10\nPyTorch version: 0.4.0\nPython version: 3.5.2\nCUDA/cuDNN version: 8.0/5.1\nGPU models and configuration: NVIDIA 850M", "body": "I tried training a VGG model with the torch.optim.Adam and torch.optim.SGD. I can train my model on SGD batch_size = 8 for SGD, but Adam throws me OOM error with batch_size 8, but it works for batch_size 4. Model is imported from torchvision.\r\n\r\n```\r\nmodel = vgg16(pretrained=True)\r\nmodel.fc = torch.nn.Linear(4096, len(data_ctr.class_dir))\r\nmodel.cuda()\r\ncriterion = nn.CrossEntropyLoss()\r\noptimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\r\n# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\r\n\r\nfor i in range(EPOCH):\r\n    model.train()\r\n    all_prediction = None\r\n    all_labels = None\r\n    for batch in tqdm(batch_ctr):\r\n        images = batch['image'].cuda().float()\r\n        labels_ = batch['labels'].cuda().long()\r\n        \r\n        optimizer.zero_grad()\r\n        output = model(images)\r\n        loss = criterion(output, labels_)\r\n        loss.backward()\r\n        optimizer.step()\r\n```\r\n\r\n```\r\nEpoch 0\r\n  1%|\u258b                                                                                 | 1/112 [00:03<06:38,  3.59s/it]\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-5-a2a7f14f2b5a> in <module>()\r\n     27         loss = criterion(output, labels_)\r\n     28         #Backpropagation of the losses to compute gradients of all the weights.\r\n---> 29         loss.backward()\r\n     30         #Update the weights, based on parameters defined in your optimizer.\r\n     31         optimizer.step()\r\n\r\nC:\\Users\\limwe\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n     91                 products. Defaults to ``False``.\r\n     92         \"\"\"\r\n---> 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n     94 \r\n     95     def register_hook(self, hook):\r\n\r\nC:\\Users\\limwe\\Anaconda3\\envs\\tf\\lib\\site-packages\\torch\\autograd\\__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     87     Variable._execution_engine.run_backward(\r\n     88         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 89         allow_unreachable=True)  # allow_unreachable flag\r\n     90 \r\n     91 \r\n\r\nRuntimeError: cuda runtime error (2) : out of memory at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1524539825236\\work\\aten\\src\\thc\\generic/THCStorage.cu:58\r\n```\r\n\r\n\r\n- PyTorch installed from conda\r\n- OS: Windows 10\r\n- PyTorch version: 0.4.0\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 8.0/5.1\r\n- GPU models and configuration: NVIDIA 850M\r\n"}