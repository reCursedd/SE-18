{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/421146936", "html_url": "https://github.com/pytorch/pytorch/issues/11201#issuecomment-421146936", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11201", "id": 421146936, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTE0NjkzNg==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-13T20:44:38Z", "updated_at": "2018-09-13T20:44:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42612199\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/whucdf\">@whucdf</a> Thanks for reporting this issue. It is expected because the default <code>file_descriptor</code> <a href=\"https://pytorch.org/docs/master/multiprocessing.html?highlight=sharing%20strategy#sharing-strategies\" rel=\"nofollow\">share strategy</a> uses file descriptors as shared memory handles, and this will hit the limit when there are too many batches at DataLoader. To get around this, you can switch to <code>file_system</code> strategy by adding this to your script.</p>\n<pre><code>import torch.multiprocessing\ntorch.multiprocessing.set_sharing_strategy('file_system')\n</code></pre>\n<p>Let me know if there is still any issue.</p>", "body_text": "@whucdf Thanks for reporting this issue. It is expected because the default file_descriptor share strategy uses file descriptors as shared memory handles, and this will hit the limit when there are too many batches at DataLoader. To get around this, you can switch to file_system strategy by adding this to your script.\nimport torch.multiprocessing\ntorch.multiprocessing.set_sharing_strategy('file_system')\n\nLet me know if there is still any issue.", "body": "@whucdf Thanks for reporting this issue. It is expected because the default `file_descriptor` [share strategy](https://pytorch.org/docs/master/multiprocessing.html?highlight=sharing%20strategy#sharing-strategies) uses file descriptors as shared memory handles, and this will hit the limit when there are too many batches at DataLoader. To get around this, you can switch to `file_system` strategy by adding this to your script.\r\n```\r\nimport torch.multiprocessing\r\ntorch.multiprocessing.set_sharing_strategy('file_system')\r\n```\r\n\r\nLet me know if there is still any issue."}