{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11201", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11201/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11201/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11201/events", "html_url": "https://github.com/pytorch/pytorch/issues/11201", "id": 356516297, "node_id": "MDU6SXNzdWUzNTY1MTYyOTc=", "number": 11201, "title": "Too many open files error", "user": {"login": "whucdf", "id": 42612199, "node_id": "MDQ6VXNlcjQyNjEyMTk5", "avatar_url": "https://avatars2.githubusercontent.com/u/42612199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whucdf", "html_url": "https://github.com/whucdf", "followers_url": "https://api.github.com/users/whucdf/followers", "following_url": "https://api.github.com/users/whucdf/following{/other_user}", "gists_url": "https://api.github.com/users/whucdf/gists{/gist_id}", "starred_url": "https://api.github.com/users/whucdf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whucdf/subscriptions", "organizations_url": "https://api.github.com/users/whucdf/orgs", "repos_url": "https://api.github.com/users/whucdf/repos", "events_url": "https://api.github.com/users/whucdf/events{/privacy}", "received_events_url": "https://api.github.com/users/whucdf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-09-03T13:56:30Z", "updated_at": "2018-09-14T18:39:40Z", "closed_at": "2018-09-14T18:39:40Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>While using the dataloader from pytorch 0.4.1:<br>\nWith num_workers &gt; 0 the workers store the tensors in shared memory, but do not release the shared memory file handles after they return the tensor to the main process and file handles are no longer needed. The worker will then run out of file handles, if one stores the tensor in a list.</p>\n<h2>Code example</h2>\n<pre><code>\nfrom torch.utils.data import Dataset\nclass testSet(Dataset):\n    def __init__(self):\n        super(testSet,self).__init__()\n    def `__len__(self):`\n        return 1000000\n    def __getitem__(self,index):\n        return {\"index\":index}\n\nimport torch\n\ntest_data = testSet()\ntest_data_loader = torch.utils.data.DataLoader( dataset=test_data, batch_size=1, num_workers=1)\nindex = []\nfor sample in test_data_loader:\n    index.append(sample['index'])\n</code></pre>\n<p>The error:</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-5-cf6ed576bc1c&gt; in &lt;module&gt;()\n----&gt; 1 for sample in test_data_loader:\n      2     #print(sample['index'])\n      3     index.append(sample['index'])\n\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    328         while True:\n    329             assert (not self.shutdown and self.batches_outstanding &gt; 0)\n--&gt; 330             idx, batch = self._get_batch()\n    331             self.batches_outstanding -= 1\n    332             if idx != self.rcvd_idx:\n\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _get_batch(self)\n    307                 raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\n    308         else:\n--&gt; 309             return self.data_queue.get()\n    310 \n    311     def __next__(self):\n\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py in get(self)\n    335             res = self._reader.recv_bytes()\n    336         # unserialize the data after having released the lock\n--&gt; 337         return _ForkingPickler.loads(res)\n    338 \n    339     def put(self, obj):\n\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py in rebuild_storage_fd(cls, df, size)\n    149         fd = multiprocessing.reduction.rebuild_handle(df)\n    150     else:\n--&gt; 151         fd = df.detach()\n    152     try:\n    153         storage = storage_from_cache(cls, fd_id(fd))\n\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py in detach(self)\n     56             '''Get the fd.  This should only be called once.'''\n     57             with _resource_sharer.get_connection(self._id) as conn:\n---&gt; 58                 return reduction.recv_handle(conn)\n     59 \n     60 \n\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py in recv_handle(conn)\n    180         '''Receive a handle over a local connection.'''\n    181         with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:\n--&gt; 182             return recvfds(s, 1)[0]\n    183 \n    184     def DupFd(fd):\n\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py in recvfds(sock, size)\n    159             if len(ancdata) != 1:\n    160                 raise RuntimeError('received %d items of ancdata' %\n--&gt; 161                                    len(ancdata))\n    162             cmsg_level, cmsg_type, cmsg_data = ancdata[0]\n    163             if (cmsg_level == socket.SOL_SOCKET and\n\nRuntimeError: received 0 items of ancdata\n\n</code></pre>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch</li>\n<li>OS: Ubuntu 16.04</li>\n<li>PyTorch version: 0.4.1</li>\n</ul>", "body_text": "Issue description\nWhile using the dataloader from pytorch 0.4.1:\nWith num_workers > 0 the workers store the tensors in shared memory, but do not release the shared memory file handles after they return the tensor to the main process and file handles are no longer needed. The worker will then run out of file handles, if one stores the tensor in a list.\nCode example\n\nfrom torch.utils.data import Dataset\nclass testSet(Dataset):\n    def __init__(self):\n        super(testSet,self).__init__()\n    def `__len__(self):`\n        return 1000000\n    def __getitem__(self,index):\n        return {\"index\":index}\n\nimport torch\n\ntest_data = testSet()\ntest_data_loader = torch.utils.data.DataLoader( dataset=test_data, batch_size=1, num_workers=1)\nindex = []\nfor sample in test_data_loader:\n    index.append(sample['index'])\n\nThe error:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-cf6ed576bc1c> in <module>()\n----> 1 for sample in test_data_loader:\n      2     #print(sample['index'])\n      3     index.append(sample['index'])\n\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\n    328         while True:\n    329             assert (not self.shutdown and self.batches_outstanding > 0)\n--> 330             idx, batch = self._get_batch()\n    331             self.batches_outstanding -= 1\n    332             if idx != self.rcvd_idx:\n\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _get_batch(self)\n    307                 raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\n    308         else:\n--> 309             return self.data_queue.get()\n    310 \n    311     def __next__(self):\n\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py in get(self)\n    335             res = self._reader.recv_bytes()\n    336         # unserialize the data after having released the lock\n--> 337         return _ForkingPickler.loads(res)\n    338 \n    339     def put(self, obj):\n\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py in rebuild_storage_fd(cls, df, size)\n    149         fd = multiprocessing.reduction.rebuild_handle(df)\n    150     else:\n--> 151         fd = df.detach()\n    152     try:\n    153         storage = storage_from_cache(cls, fd_id(fd))\n\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py in detach(self)\n     56             '''Get the fd.  This should only be called once.'''\n     57             with _resource_sharer.get_connection(self._id) as conn:\n---> 58                 return reduction.recv_handle(conn)\n     59 \n     60 \n\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py in recv_handle(conn)\n    180         '''Receive a handle over a local connection.'''\n    181         with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:\n--> 182             return recvfds(s, 1)[0]\n    183 \n    184     def DupFd(fd):\n\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py in recvfds(sock, size)\n    159             if len(ancdata) != 1:\n    160                 raise RuntimeError('received %d items of ancdata' %\n--> 161                                    len(ancdata))\n    162             cmsg_level, cmsg_type, cmsg_data = ancdata[0]\n    163             if (cmsg_level == socket.SOL_SOCKET and\n\nRuntimeError: received 0 items of ancdata\n\n\nSystem Info\n\nPyTorch\nOS: Ubuntu 16.04\nPyTorch version: 0.4.1", "body": "## Issue description\r\n\r\nWhile using the dataloader from pytorch 0.4.1:\r\nWith num_workers > 0 the workers store the tensors in shared memory, but do not release the shared memory file handles after they return the tensor to the main process and file handles are no longer needed. The worker will then run out of file handles, if one stores the tensor in a list.\r\n\r\n## Code example\r\n```\r\n\r\nfrom torch.utils.data import Dataset\r\nclass testSet(Dataset):\r\n    def __init__(self):\r\n        super(testSet,self).__init__()\r\n    def `__len__(self):`\r\n        return 1000000\r\n    def __getitem__(self,index):\r\n        return {\"index\":index}\r\n\r\nimport torch\r\n\r\ntest_data = testSet()\r\ntest_data_loader = torch.utils.data.DataLoader( dataset=test_data, batch_size=1, num_workers=1)\r\nindex = []\r\nfor sample in test_data_loader:\r\n    index.append(sample['index'])\r\n```\r\nThe error:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-5-cf6ed576bc1c> in <module>()\r\n----> 1 for sample in test_data_loader:\r\n      2     #print(sample['index'])\r\n      3     index.append(sample['index'])\r\n\r\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    328         while True:\r\n    329             assert (not self.shutdown and self.batches_outstanding > 0)\r\n--> 330             idx, batch = self._get_batch()\r\n    331             self.batches_outstanding -= 1\r\n    332             if idx != self.rcvd_idx:\r\n\r\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _get_batch(self)\r\n    307                 raise RuntimeError('DataLoader timed out after {} seconds'.format(self.timeout))\r\n    308         else:\r\n--> 309             return self.data_queue.get()\r\n    310 \r\n    311     def __next__(self):\r\n\r\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py in get(self)\r\n    335             res = self._reader.recv_bytes()\r\n    336         # unserialize the data after having released the lock\r\n--> 337         return _ForkingPickler.loads(res)\r\n    338 \r\n    339     def put(self, obj):\r\n\r\n~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py in rebuild_storage_fd(cls, df, size)\r\n    149         fd = multiprocessing.reduction.rebuild_handle(df)\r\n    150     else:\r\n--> 151         fd = df.detach()\r\n    152     try:\r\n    153         storage = storage_from_cache(cls, fd_id(fd))\r\n\r\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/resource_sharer.py in detach(self)\r\n     56             '''Get the fd.  This should only be called once.'''\r\n     57             with _resource_sharer.get_connection(self._id) as conn:\r\n---> 58                 return reduction.recv_handle(conn)\r\n     59 \r\n     60 \r\n\r\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py in recv_handle(conn)\r\n    180         '''Receive a handle over a local connection.'''\r\n    181         with socket.fromfd(conn.fileno(), socket.AF_UNIX, socket.SOCK_STREAM) as s:\r\n--> 182             return recvfds(s, 1)[0]\r\n    183 \r\n    184     def DupFd(fd):\r\n\r\n~/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/reduction.py in recvfds(sock, size)\r\n    159             if len(ancdata) != 1:\r\n    160                 raise RuntimeError('received %d items of ancdata' %\r\n--> 161                                    len(ancdata))\r\n    162             cmsg_level, cmsg_type, cmsg_data = ancdata[0]\r\n    163             if (cmsg_level == socket.SOL_SOCKET and\r\n\r\nRuntimeError: received 0 items of ancdata\r\n\r\n```\r\n\r\n## System Info\r\n- PyTorch\r\n- OS: Ubuntu 16.04 \r\n- PyTorch version: 0.4.1\r\n"}