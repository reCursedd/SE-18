{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204607300", "pull_request_review_id": 139714770, "id": 204607300, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDYwNzMwMA==", "diff_hunk": "@@ -367,5 +430,90 @@ def allreduce(tensors, op):\n             self.assertEqual(torch.Tensor([self.num_gpus]), tensors[i])\n \n \n+class Net(nn.Module):\n+    def __init__(self):\n+        super(Net, self).__init__()\n+        self.fc1 = nn.Linear(2, 10, bias=False)\n+        self.fc2 = nn.Linear(10, 50, bias=False)\n+        self.fc3 = nn.Linear(50, 4, bias=False)\n+        self.relu = nn.ReLU()\n+\n+    def forward(self, x):\n+        x = self.relu(self.fc1(x))\n+        x = self.relu(self.fc2(x))\n+        x = self.fc3(x)\n+        return F.softmax(x, dim=1)\n+\n+\n+class DistributedDataParallelTest(MultiProcessTestCase):\n+\n+    @property\n+    def size(self):\n+        return 2\n+\n+    def _test_ddp_with_process_group(self, process_group):\n+        gpus = gpus_for_rank(self.size)[self.rank]\n+        model = Net()\n+        ddp_model = distributed_c10d._DistributedDataParallelC10d(\n+            copy.deepcopy(model).cuda(gpus[0]),\n+            process_group,\n+            device_ids=gpus)\n+        model.cuda(gpus[0])\n+\n+        local_batch_size = len(gpus)\n+        global_batch_size = self.size * local_batch_size\n+        criterion = nn.MSELoss()\n+        input = torch.randn(global_batch_size, 2).cuda(gpus[0])\n+        target = torch.randn(global_batch_size, 4).cuda(gpus[0])\n+\n+        def step_model(model, input, target, criterion):\n+            model.train()\n+            output = model(input)\n+            loss = criterion(output, target)\n+            loss.backward()\n+\n+        def update_parameters(model):\n+            for param in model.parameters():\n+                param.data -= param.grad\n+                param.grad = None\n+\n+        # check two model parameters over 2 iterations\n+        for _ in range(2):\n+            # single cpu/gpu training\n+            step_model(model, input, target, criterion)\n+\n+            # DDP training, DDP scatters subsets of input_cpu to nodes/GPUs\n+            step_model(ddp_model,\n+                       input[self.rank * local_batch_size: (self.rank + 1) * local_batch_size],\n+                       target[self.rank * local_batch_size: (self.rank + 1) * local_batch_size],\n+                       criterion)\n+\n+            # Update weights and run a second iteration to shake out errors\n+            update_parameters(model)\n+            update_parameters(ddp_model)\n+            assert len(list(model.parameters())) == len(list(ddp_model.parameters()))\n+            for i, j in zip(model.parameters(), ddp_model.parameters()):\n+                self.assertEqual(i, j)\n+\n+            # Shuffle the input so that DDP input is different\n+            input = input[torch.randperm(global_batch_size)]\n+\n+    @skip_if_not_multigpu\n+    def test_gloo_backend(self):\n+        store = c10d.TCPStore('localhost', self.port, self.rank == 0)\n+        options = c10d.ProcessGroupGloo.Options()\n+        options.devices = [c10d.ProcessGroupGloo.create_tcp_device(interface=\"lo\")]\n+        process_group = c10d.ProcessGroupGloo(store, self.rank, self.size, options)\n+        self._test_ddp_with_process_group(process_group)\n+\n+    @skip_if_not_multigpu\n+    @skip_if_not_nccl\n+    def test_nccl_backend(self):\n+        store = c10d.TCPStore('localhost', self.port, self.rank == 0)\n+        process_group = c10d.ProcessGroupNCCL(store, self.rank, self.size)\n+        self._test_ddp_with_process_group(process_group)\n+\n if __name__ == '__main__':\n+    assert not torch.cuda._initialized, \"test_distributed must not have initialized CUDA context on main process\"", "path": "test/test_c10d.py", "position": null, "original_position": 248, "commit_id": "23e8275f3bacf8cf0cc87ff86d6d854d43dc9045", "original_commit_id": "3d92883e2f17514131afd584b98bf1d87a980dac", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I'm curious what issues did you find? Were they fork related or what?", "created_at": "2018-07-24T02:28:10Z", "updated_at": "2018-11-23T15:47:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/9670#discussion_r204607300", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9670", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204607300"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9670#discussion_r204607300"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9670"}}, "body_html": "<p>I'm curious what issues did you find? Were they fork related or what?</p>", "body_text": "I'm curious what issues did you find? Were they fork related or what?"}