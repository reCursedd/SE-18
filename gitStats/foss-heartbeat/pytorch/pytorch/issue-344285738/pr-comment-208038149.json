{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208038149", "pull_request_review_id": 143770894, "id": 208038149, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwODAzODE0OQ==", "diff_hunk": "@@ -509,10 +515,103 @@ def test_gloo_backend(self):\n     @skip_if_not_multigpu\n     @skip_if_not_nccl\n     def test_nccl_backend(self):\n-        store = c10d.TCPStore('localhost', self.port, self.rank == 0)\n+        store = c10d.TCPStore('localhost', self.port, self.is_master)\n         process_group = c10d.ProcessGroupNCCL(store, self.rank, self.world_size)\n         self._test_ddp_with_process_group(process_group)\n \n+    @skip_if_not_multigpu\n+    def test_dist_broadcast_coalesced(self):\n+        # Set up process group.\n+        store = c10d.TCPStore('localhost', self.port, self.is_master)\n+        options = c10d.ProcessGroupGloo.Options()\n+        options.devices = [c10d.ProcessGroupGloo.create_tcp_device(interface=\"lo\")]\n+        process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size, options)\n+\n+        device = torch.device('cuda')\n+\n+        target = torch.arange(10, dtype=torch.float64, device=device).chunk(2)\n+\n+        if self.is_master:\n+            # All processes should have these tensors in the end.\n+            tensors = target\n+        else:\n+            # Non-master processes start with empty tensors and should be\n+            # filled with the tensors from the master.\n+            tensors = torch.zeros(10, device=device).chunk(2)\n+\n+        c10d._dist_broadcast_coalesced(\n+            tensors,\n+            buffer_size=25 * MB,\n+            process_group=process_group)\n+\n+        if not self.is_master:\n+            self.assertEqual(tensors, target)\n+\n+    @skip_if_not_multigpu\n+    def test_sync_params_no_buffers(self):\n+        # Set up process group.\n+        store = c10d.TCPStore('localhost', self.port, self.is_master)\n+        options = c10d.ProcessGroupGloo.Options()\n+        options.devices = [c10d.ProcessGroupGloo.create_tcp_device(interface=\"lo\")]\n+        process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size, options)\n+\n+        # Use all available devices on every process here (data is small, so should be fine).\n+        devices = list(range(torch.cuda.device_count()))", "path": "test/test_c10d.py", "position": null, "original_position": 75, "commit_id": "ec7d38172ea9e1ed54bc0fb1774f54d51fe5e035", "original_commit_id": "e115f43682408e7f3e7354f0807df4fa77690bcc", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "We do this in other tests, but in a lot of cases we only run with two GPUs, so with two processes that would only leave one device per process and make this test less meaningful. Is this reasonable?", "created_at": "2018-08-06T21:40:58Z", "updated_at": "2018-11-23T15:48:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/9805#discussion_r208038149", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9805", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208038149"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9805#discussion_r208038149"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9805"}}, "body_html": "<p>We do this in other tests, but in a lot of cases we only run with two GPUs, so with two processes that would only leave one device per process and make this test less meaningful. Is this reasonable?</p>", "body_text": "We do this in other tests, but in a lot of cases we only run with two GPUs, so with two processes that would only leave one device per process and make this test less meaningful. Is this reasonable?", "in_reply_to_id": 208032412}