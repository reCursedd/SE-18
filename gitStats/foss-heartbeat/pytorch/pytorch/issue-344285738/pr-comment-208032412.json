{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208032412", "pull_request_review_id": 143764193, "id": 208032412, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwODAzMjQxMg==", "diff_hunk": "@@ -509,10 +515,103 @@ def test_gloo_backend(self):\n     @skip_if_not_multigpu\n     @skip_if_not_nccl\n     def test_nccl_backend(self):\n-        store = c10d.TCPStore('localhost', self.port, self.rank == 0)\n+        store = c10d.TCPStore('localhost', self.port, self.is_master)\n         process_group = c10d.ProcessGroupNCCL(store, self.rank, self.world_size)\n         self._test_ddp_with_process_group(process_group)\n \n+    @skip_if_not_multigpu\n+    def test_dist_broadcast_coalesced(self):\n+        # Set up process group.\n+        store = c10d.TCPStore('localhost', self.port, self.is_master)\n+        options = c10d.ProcessGroupGloo.Options()\n+        options.devices = [c10d.ProcessGroupGloo.create_tcp_device(interface=\"lo\")]\n+        process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size, options)\n+\n+        device = torch.device('cuda')\n+\n+        target = torch.arange(10, dtype=torch.float64, device=device).chunk(2)\n+\n+        if self.is_master:\n+            # All processes should have these tensors in the end.\n+            tensors = target\n+        else:\n+            # Non-master processes start with empty tensors and should be\n+            # filled with the tensors from the master.\n+            tensors = torch.zeros(10, device=device).chunk(2)\n+\n+        c10d._dist_broadcast_coalesced(\n+            tensors,\n+            buffer_size=25 * MB,\n+            process_group=process_group)\n+\n+        if not self.is_master:\n+            self.assertEqual(tensors, target)\n+\n+    @skip_if_not_multigpu\n+    def test_sync_params_no_buffers(self):\n+        # Set up process group.\n+        store = c10d.TCPStore('localhost', self.port, self.is_master)\n+        options = c10d.ProcessGroupGloo.Options()\n+        options.devices = [c10d.ProcessGroupGloo.create_tcp_device(interface=\"lo\")]\n+        process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size, options)\n+\n+        # Use all available devices on every process here (data is small, so should be fine).\n+        devices = list(range(torch.cuda.device_count()))", "path": "test/test_c10d.py", "position": null, "original_position": 75, "commit_id": "ec7d38172ea9e1ed54bc0fb1774f54d51fe5e035", "original_commit_id": "e115f43682408e7f3e7354f0807df4fa77690bcc", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "I am thinking of dividing the devices among all processes, wdyt?", "created_at": "2018-08-06T21:17:55Z", "updated_at": "2018-11-23T15:48:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/9805#discussion_r208032412", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9805", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208032412"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9805#discussion_r208032412"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9805"}}, "body_html": "<p>I am thinking of dividing the devices among all processes, wdyt?</p>", "body_text": "I am thinking of dividing the devices among all processes, wdyt?"}