{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216739202", "pull_request_review_id": 154288008, "id": 216739202, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjczOTIwMg==", "diff_hunk": "@@ -543,3 +544,241 @@ def argsort(input, dim=None, descending=False):\n     if dim is None:\n         return torch.sort(input, -1, descending)[1]\n     return torch.sort(input, dim, descending)[1]\n+\n+\n+def norm(input, p=\"fro\", dim=None, keepdim=False, out=None):\n+    r\"\"\"Returns the matrix norm or vector norm of a given tensor.\n+\n+    Args:\n+        input (Tensor): the input tensor\n+        p ({int, float, inf, -inf, 'fro', 'nuc'}): the order of norm\n+            The following norms can be calculated:\n+            =====  ============================  ==========================\n+            ord    matrix norm                   vector norm\n+            =====  ============================  ==========================\n+            None   Frobenius norm                2-norm\n+            'fro'  Frobenius norm                --\n+            'nuc'  nuclear norm                  --\n+            inf    max(sum(abs(x), dim=1))       max(abs(x))\n+            -inf   min(sum(abs(x), dim=1))       min(abs(x))\n+            0      --                            sum(x != 0)\n+            1      max(sum(abs(x), dim=0))       as below\n+            -1     min(sum(abs(x), dim=0))       as below\n+            2      largest singular value        as below\n+            -2     smallest singular value       as below\n+            other  as vec norm when dim is None  sum(abs(x)**ord)**(1./ord)\n+            =====  ============================  ==========================\n+        dim ({int, 2-tuple of ints, 2-list of ints}): If it is a int, vector norm\n+        will be calculated, if it is 2-tuple of ints, matrix norm will be\n+        calculated.\n+        keepdim (bool): whether the output tensors have :attr:`dim`\n+            retained or not. Ignored if ``dim=None``.\n+        out (Tensor, optional) \u2013 the output tensor\n+\n+    Example::\n+        >>> import torch\n+        >>> a = torch.arange(9, dtype= torch.float) - 4\n+        >>> b = a.reshape((3, 3))\n+        >>> torch.norm(a)\n+        tensor(7.7460)\n+        >>> torch.norm(b)\n+        tensor(7.7460)\n+        >>> torch.norm(a, float('inf'))\n+        tensor(4.)\n+        >>> torch.norm(b, float('inf'))\n+        tensor(9.)\n+        >>> torch.norm(a, float('-inf'))\n+        tensor(0.)\n+        >>> torch.norm(b, float('-inf'))\n+        tensor(2.)\n+        >>> torch.norm(a, 1)\n+        tensor(20.)\n+        >>> torch.norm(b, 1)\n+        tensor(20.)\n+        >>> torch.norm(a, -1)\n+        tensor(0.)\n+        >>> torch.norm(b, -1)\n+        tensor(0.)\n+        >>> torch.norm(a, 2)\n+        tensor(7.7460)\n+        >>> torch.norm(b, 2)\n+        tensor(7.7460)\n+        >>> torch.norm(a, -2)\n+        tensor(0.)\n+        >>> torch.norm(b, -2)\n+        tensor(0.)\n+        >>> torch.norm(a, 3)\n+        tensor(5.8480)\n+        >>> torch.norm(a, -3)\n+        tensor(0.)\n+        >>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n+        >>> torch.norm(c, dim=0)\n+        tensor([1.4142, 2.2361, 5.0000])\n+        >>> torch.norm(c, dim=1)\n+        tensor([3.7417, 4.2426])\n+        >>> torch.norm(c, p=1, dim=1)\n+        tensor([6., 6.])\n+        >>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n+        >>> torch.norm(d, dim=(1,2))\n+        tensor([ 3.7417, 11.2250])\n+        >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n+        (tensor(3.7417), tensor(11.2250))\n+    \"\"\"\n+    ndim = input.dim()\n+\n+    # catch default case\n+    if dim is None and out is None:\n+        if p == \"fro\":\n+            return torch._C._VariableFunctions.norm(input, 2)\n+        if isinstance(p, int):\n+            return torch._C._VariableFunctions.norm(input, p)\n+        if isinstance(p, float) and p != inf and p != -inf:\n+            return torch._C._VariableFunctions.norm(input, p)", "path": "torch/functional.py", "position": null, "original_position": 101, "commit_id": "757e6d8d507c44e454fe5f507ca2266b67acab2d", "original_commit_id": "f97623c871fe74e6a9e46947ee829f6304b0aaea", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "@erikbrinkman That is the current way. However, in the long term goal, we want to have enum support in native function, and to allow specifying how to map python objects to enum values. That will make this, nn.functional.pad, and many other things much easier to write!  ", "created_at": "2018-09-11T16:44:31Z", "updated_at": "2018-11-23T15:50:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/11261#discussion_r216739202", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11261", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216739202"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11261#discussion_r216739202"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11261"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=858926\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/erikbrinkman\">@erikbrinkman</a> That is the current way. However, in the long term goal, we want to have enum support in native function, and to allow specifying how to map python objects to enum values. That will make this, nn.functional.pad, and many other things much easier to write!</p>", "body_text": "@erikbrinkman That is the current way. However, in the long term goal, we want to have enum support in native function, and to allow specifying how to map python objects to enum values. That will make this, nn.functional.pad, and many other things much easier to write!", "in_reply_to_id": 216194050}