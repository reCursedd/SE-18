{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388026588", "html_url": "https://github.com/pytorch/pytorch/issues/7460#issuecomment-388026588", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7460", "id": 388026588, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODAyNjU4OA==", "user": {"login": "nehz", "id": 151358, "node_id": "MDQ6VXNlcjE1MTM1OA==", "avatar_url": "https://avatars3.githubusercontent.com/u/151358?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nehz", "html_url": "https://github.com/nehz", "followers_url": "https://api.github.com/users/nehz/followers", "following_url": "https://api.github.com/users/nehz/following{/other_user}", "gists_url": "https://api.github.com/users/nehz/gists{/gist_id}", "starred_url": "https://api.github.com/users/nehz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nehz/subscriptions", "organizations_url": "https://api.github.com/users/nehz/orgs", "repos_url": "https://api.github.com/users/nehz/repos", "events_url": "https://api.github.com/users/nehz/events{/privacy}", "received_events_url": "https://api.github.com/users/nehz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-10T11:22:41Z", "updated_at": "2018-05-10T12:28:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Potentially a new mixin class that requires all parameters to be on same device and move .cuda() and .to() methods to that.<br>\nAs in the case with Modules with parameters on difference devices, doing a .cuda() or .to() can also mess it all up.</p>\n<p>E.G</p>\n<pre><code>class Module:\n    # Remove .cuda(), .to(), or put warning\n    ...\n\nclass SingleDevice:\n    def to():\n        # code from Module\n        ...\n\n    @property\n    def device():\n         ...\n\nclass Linear(Module, SingleDevice):\n    ...\n    pass\n\nclass Container(Module):\n    ...\n    pass\n\nContainer([net1, net2]).cuda() # &lt;--- doesn't make sense if some modules on different devices\nLinear(5, 5).cuda() # OK\n</code></pre>\n<p>Thoughts ?</p>", "body_text": "Potentially a new mixin class that requires all parameters to be on same device and move .cuda() and .to() methods to that.\nAs in the case with Modules with parameters on difference devices, doing a .cuda() or .to() can also mess it all up.\nE.G\nclass Module:\n    # Remove .cuda(), .to(), or put warning\n    ...\n\nclass SingleDevice:\n    def to():\n        # code from Module\n        ...\n\n    @property\n    def device():\n         ...\n\nclass Linear(Module, SingleDevice):\n    ...\n    pass\n\nclass Container(Module):\n    ...\n    pass\n\nContainer([net1, net2]).cuda() # <--- doesn't make sense if some modules on different devices\nLinear(5, 5).cuda() # OK\n\nThoughts ?", "body": "Potentially a new mixin class that requires all parameters to be on same device and move .cuda() and .to() methods to that.\r\nAs in the case with Modules with parameters on difference devices, doing a .cuda() or .to() can also mess it all up.\r\n\r\nE.G\r\n\r\n```\r\nclass Module:\r\n    # Remove .cuda(), .to(), or put warning\r\n    ...\r\n\r\nclass SingleDevice:\r\n    def to():\r\n        # code from Module\r\n        ...\r\n\r\n    @property\r\n    def device():\r\n         ...\r\n\r\nclass Linear(Module, SingleDevice):\r\n    ...\r\n    pass\r\n\r\nclass Container(Module):\r\n    ...\r\n    pass\r\n\r\nContainer([net1, net2]).cuda() # <--- doesn't make sense if some modules on different devices\r\nLinear(5, 5).cuda() # OK\r\n```\r\n\r\n\r\nThoughts ?\r\n"}