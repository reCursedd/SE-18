{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233658923", "pull_request_review_id": 175126784, "id": 233658923, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzY1ODkyMw==", "diff_hunk": "@@ -0,0 +1,245 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/Context.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+#include \"ATen/cuda/CUDAEvent.h\"\n+#include \"ATen/cuda/CUDAStream.h\"\n+#include \"ATen/native/Copy.h\"\n+\n+namespace {\n+\n+using namespace at;\n+using namespace at::cuda;\n+\n+// Copy operator for the pointwise apply kernel\n+template <typename dst_T, typename src_T>\n+struct CopyOp {\n+  static void apply(Tensor& dst, const Tensor& src) {\n+    CUDA_tensor_apply2<dst_T, src_T>(\n+        dst, src, [] __device__(dst_T & dst_val, const src_T& src_val) {\n+#if __CUDA_ARCH__ >= 350\n+          dst_val = static_cast<dst_T>(\n+              static_cast<native::inter_copy_type_t<dst_T>>(__ldg(&src_val)));\n+#else\n+          dst_val = static_cast<dst_T>(static_cast<native::inter_copy_type_t<dst_T>>(src_val));\n+#endif\n+        });\n+  }\n+};\n+\n+// device-to-device copy, does type conversion\n+template <typename dst_T, typename src_T>\n+void copy_device_to_device(Tensor& dst, const Tensor& src) {\n+  auto numel = dst.numel();\n+  if (dst.is_same(src) || numel == 0) {\n+    return;\n+  }\n+\n+  // We can memcpy the memory if:\n+  // -both tensors are contiguous; or,\n+  // -there is only one element to copy; or,\n+  // -FIXME: if both tensors have matching size and stride arrays, and no\n+  // holes within (in other words, there is some permutation that can be applied\n+  // to the size/strides such that the resulting tensor is\n+  // contiguous).\n+  // -AND: both tensors have the same type.\n+  bool same_type = std::is_same<dst_T, src_T>::value;\n+  bool memcpy_eligible =\n+      ((src.is_contiguous() && dst.is_contiguous()) || (numel == 1)) &&", "path": "aten/src/ATen/native/cuda/Copy.cu", "position": 50, "original_position": 50, "commit_id": "4052372d4f8f595ce69417cfd7db9dde6279397c", "original_commit_id": "768d164162b2b688f17f84ac4db6e9998ba00414", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Hehe, numel == 1", "created_at": "2018-11-14T23:18:50Z", "updated_at": "2018-11-23T15:54:52Z", "html_url": "https://github.com/pytorch/pytorch/pull/13348#discussion_r233658923", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13348", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233658923"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13348#discussion_r233658923"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13348"}}, "body_html": "<p>Hehe, numel == 1</p>", "body_text": "Hehe, numel == 1"}