{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233674472", "pull_request_review_id": 175145013, "id": 233674472, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzY3NDQ3Mg==", "diff_hunk": "@@ -0,0 +1,245 @@\n+#include \"ATen/ATen.h\"\n+#include \"ATen/Context.h\"\n+#include \"ATen/Dispatch.h\"\n+#include \"ATen/NativeFunctions.h\"\n+#include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+#include \"ATen/cuda/CUDAContext.h\"\n+#include \"ATen/cuda/CUDAEvent.h\"\n+#include \"ATen/cuda/CUDAStream.h\"\n+#include \"ATen/native/Copy.h\"\n+\n+namespace {\n+\n+using namespace at;\n+using namespace at::cuda;\n+\n+// Copy operator for the pointwise apply kernel\n+template <typename dst_T, typename src_T>\n+struct CopyOp {\n+  static void apply(Tensor& dst, const Tensor& src) {\n+    CUDA_tensor_apply2<dst_T, src_T>(\n+        dst, src, [] __device__(dst_T & dst_val, const src_T& src_val) {\n+#if __CUDA_ARCH__ >= 350\n+          dst_val = static_cast<dst_T>(\n+              static_cast<native::inter_copy_type_t<dst_T>>(__ldg(&src_val)));\n+#else\n+          dst_val = static_cast<dst_T>(static_cast<native::inter_copy_type_t<dst_T>>(src_val));\n+#endif\n+        });\n+  }\n+};\n+\n+// device-to-device copy, does type conversion\n+template <typename dst_T, typename src_T>\n+void copy_device_to_device(Tensor& dst, const Tensor& src) {\n+  auto numel = dst.numel();\n+  if (dst.is_same(src) || numel == 0) {\n+    return;\n+  }\n+\n+  // We can memcpy the memory if:\n+  // -both tensors are contiguous; or,\n+  // -there is only one element to copy; or,\n+  // -FIXME: if both tensors have matching size and stride arrays, and no\n+  // holes within (in other words, there is some permutation that can be applied\n+  // to the size/strides such that the resulting tensor is\n+  // contiguous).\n+  // -AND: both tensors have the same type.\n+  bool same_type = std::is_same<dst_T, src_T>::value;\n+  bool memcpy_eligible =\n+      ((src.is_contiguous() && dst.is_contiguous()) || (numel == 1)) &&\n+      same_type;\n+\n+  Device src_device = src.device();\n+  Device dst_device = dst.device();\n+\n+  // Try to enable p2p access. This also handles the case src_device ==\n+  // dst_device.\n+  bool p2pEnabled = THCState_getPeerToPeerAccess(\n+      globalContext().getTHCState(), src_device.index(), dst_device.index());\n+\n+  // We always perform the copy on the source device, using the\n+  // current stream on the source device.\n+  // If the copy is on the default stream, then we fully synchronize\n+  // both src and dst's default streams for completion of the\n+  // copy. We have to explicitly do this for non-contig copies.\n+  // This mimics the behavior of cross-device cudaMemcpyAsync on\n+  // the default stream.\n+  // If the copy is not on the default stream, then it is up to the\n+  // user to add needed synchronization on the dst device, since the\n+  // stream on the dst device that wishes to synchronize may not be\n+  // the same index as the one on the src device.\n+  CUDAStream copy_stream = getCurrentCUDAStream(src_device.index());\n+  if (src_device != dst_device && copy_stream == NULL) {\n+    // This is a cross-device copy on the default stream. We perform a\n+    // two-way barrier between both devices' default streams before\n+    // the copy. This ensures that any write-after-write and\n+    // write-after-read dependencies on the destination side are\n+    // handled, so that no one is operating on the dst memory when\n+    // we perform the copy.\n+    // src waits on dst barrier (src already waits on src)\n+    CUDAEvent dst_ready;\n+    DeviceGuard device_guard_dst{dst_device};\n+    dst_ready.record(getDefaultCUDAStream(dst_device.index()));\n+\n+    DeviceGuard device_guard_src{src_device};\n+    dst_ready.block(copy_stream);\n+  }\n+\n+  DeviceGuard device_guard{src_device};\n+\n+  if (memcpy_eligible) {\n+    // Perform the copy\n+    AT_CUDA_CHECK(cudaMemcpyAsync(\n+        dst.data<dst_T>(),\n+        src.data<src_T>(),\n+        numel * sizeof(dst_T),\n+        cudaMemcpyDeviceToDevice,\n+        copy_stream));\n+  } else {\n+    // Non-contiguous copy or a type-conversion copy\n+\n+    // We avoid creating temporary memory copies if possible.\n+    // If both src and dst are on the same device, or if they are on\n+    // different devices and p2p access is enabled, perform the copy\n+    // by a pointwise copy kernel.\n+    // Otherwise, we'll have to make contiguous (which will in fact\n+    // invoke copy() again), and then perform the copy.\n+    // FIXME: might want to consider only running the pointwise kernel\n+    // if both src and dst innermost dimensions are contiguous. If\n+    // they are not, then taking the hit of the memory allocation/free\n+    // might be worth it to avoid non-coalesced reads or writes.\n+    if (p2pEnabled) {\n+      CopyOp<dst_T, src_T>::apply(dst, src);\n+    } else {\n+      // GPUs can't access each other directly, but the tensors\n+      // involved are non-contiguous and/or are different types.\n+\n+      // Make sure the src is contiguous and in the same type as dst\n+      Tensor src_contig;\n+      if (same_type) {\n+        src_contig = src.contiguous();\n+      } else {\n+        // Types are different\n+        // Copy into the new format, contiguous, on the source device\n+        src_contig = at::empty_like(dst, src.options().dtype(dst.dtype()));\n+\n+        CopyOp<dst_T, src_T>::apply(dst, src);\n+      }\n+\n+      // Make sure the dst is contiguous\n+      DeviceGuard device_guard_dst{dst_device};\n+      Tensor dst_contig = dst.contiguous();\n+\n+      // Now, we are ready for a cross-device memcpy of contiguous\n+      // data, of the same layout and type\n+      DeviceGuard device_guard_src{src_device};\n+\n+      AT_CUDA_CHECK(cudaMemcpyAsync(\n+          dst_contig.data<dst_T>(),\n+          src_contig.data<dst_T>(),\n+          numel * sizeof(dst_T),\n+          cudaMemcpyDeviceToDevice,\n+          copy_stream));\n+\n+      if (!dst.is_contiguous()) {\n+        copy_device_to_device<dst_T, dst_T>(dst_contig, dst);\n+      }\n+    }\n+  }\n+\n+  if (src_device != dst_device && copy_stream == NULL) {\n+    // dst waits on src barrier (dst already waits on dst). We cannot\n+    // operate on dst's copy until the copy is complete.\n+\n+    // Still on src_device, record default stream event\n+    CUDAEvent src_ready;\n+    src_ready.record(copy_stream);\n+\n+    DeviceGuard device_guard{dst_device};\n+    src_ready.block(getDefaultCUDAStream(dst_device.index()));\n+  }\n+\n+  AT_CUDA_CHECK(cudaGetLastError());\n+}\n+\n+void copy_from_cpu(Tensor& dst, const Tensor& src) {\n+  Tensor dst_contig = dst.contiguous();\n+  Tensor src_contig = src.contiguous();\n+\n+  CUDAStream stream = getCurrentCUDAStream();\n+\n+  AT_DISPATCH_ALL_TYPES_AND_HALF(src.type(), \"copy_from_cpu\", [&]() {\n+    AT_CUDA_CHECK(cudaMemcpyAsync(\n+        dst_contig.data<scalar_t>(),\n+        src_contig.data<scalar_t>(),\n+        src.numel() * sizeof(scalar_t),\n+        cudaMemcpyHostToDevice,\n+        stream));\n+    AT_CUDA_CHECK(cudaStreamSynchronize(stream));\n+    copy_device_to_device<scalar_t, scalar_t>(dst, dst_contig);\n+  });\n+}\n+\n+void copy_to_cpu(Tensor& dst, const Tensor& src) {\n+  Tensor dst_contig = dst.contiguous();\n+  Tensor src_contig = src.contiguous();\n+\n+  DeviceGuard device_guard{src.device()};\n+  CUDAStream stream = getCurrentCUDAStream();\n+\n+  AT_DISPATCH_ALL_TYPES_AND_HALF(src.type(), \"copy_to_cpu\", [&]() {\n+    AT_CUDA_CHECK(cudaMemcpyAsync(\n+        dst_contig.data<scalar_t>(),\n+        src_contig.data<scalar_t>(),\n+        src.numel() * sizeof(scalar_t),\n+        cudaMemcpyDeviceToHost,\n+        stream));\n+    AT_CUDA_CHECK(cudaStreamSynchronize(stream));\n+    _copy_same_type_(dst, dst_contig);\n+  });\n+}\n+\n+template <typename dst_T>\n+void _copy__cuda(Tensor& dst, const Tensor& src) {\n+  AT_DISPATCH_ALL_TYPES_AND_HALF(src.type(), \"_copy__cuda\", [&]() {", "path": "aten/src/ATen/native/cuda/Copy.cu", "position": 206, "original_position": 205, "commit_id": "4052372d4f8f595ce69417cfd7db9dde6279397c", "original_commit_id": "768d164162b2b688f17f84ac4db6e9998ba00414", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Numel check here too?", "created_at": "2018-11-15T00:33:19Z", "updated_at": "2018-11-23T15:54:52Z", "html_url": "https://github.com/pytorch/pytorch/pull/13348#discussion_r233674472", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13348", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233674472"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13348#discussion_r233674472"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13348"}}, "body_html": "<p>Numel check here too?</p>", "body_text": "Numel check here too?"}