{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178599633", "pull_request_review_id": 108680659, "id": 178599633, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3ODU5OTYzMw==", "diff_hunk": "@@ -69,6 +73,47 @@ struct ReadyQueue {\n   FunctionTask pop();\n };\n \n+// Note [Reentrant backwards]\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~\n+// To understand the reentrant backwards problem, we have to notice two\n+// aspects of how the autograd engine is implemented today:\n+//\n+//  1. When you call Engine::execute(), you want to block until autograd\n+//  execution finishes so that you can get the final result variables\n+//  of the backwards pass.\n+//\n+//  2. Autograd execution operates by having a *single* worker assigned\n+//  for any given task (based on the device the task it will run on),\n+//  which is responsible for executing the contents of the task.\n+//\n+// The problem is, suppose that you call backward() inside of a worker\n+// thread.  By property (1), we're supposed to block until the nested task\n+// finishes.  However, by property (2), this worker thread is on the\n+// hook for processing the tasks assigned to it; we better not block,\n+// because then all of our backward executions (including the one we\n+// just started) will deadlock!\n+//\n+// Here's our cunning idea: instead of blocking, just get back to work\n+// on whatever task queue you should have been working on previously\n+// (this is saved via the TLS variable worker_device)!  There are\n+// simply two things you have to arrange for:\n+//\n+//  - We have to promptly kick ourselves out of the thread_main() loop\n+//    when our graph_task complete, because we need to unblock the\n+//    parent function tasks that started the reentrant execution in\n+//    the first place.  This is why thread_main() takes an optional\n+//    graph_task as input.\n+//\n+//  - When we finish a graph, we have to make sure we wake up the worker\n+//    thread so that it actually has a chance to exit the thread_main()\n+//    loop.  Thus the faffing about in thread_main() after\n+//    evaluate_function() completes.\n+\n+\n+// There is one GraphTask per execution of backward(); it holds the\n+// metadata for an overall backwards execution.  There may be multiple", "path": "torch/csrc/autograd/engine.cpp", "position": null, "original_position": 53, "commit_id": "283fdaff28de18b54aee4acd69ca7fe39bbac5e8", "original_commit_id": "ca4b82ec4db63604f63cd5f31add17d0b5f1c8f4", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "First two sentences say pretty much the same thing. Let's limit them to just \"GraphTask holds metadata needed for a single execution of backward()\"", "created_at": "2018-04-02T17:39:35Z", "updated_at": "2018-11-23T15:41:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/6191#discussion_r178599633", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6191", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178599633"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6191#discussion_r178599633"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6191"}}, "body_html": "<p>First two sentences say pretty much the same thing. Let's limit them to just \"GraphTask holds metadata needed for a single execution of backward()\"</p>", "body_text": "First two sentences say pretty much the same thing. Let's limit them to just \"GraphTask holds metadata needed for a single execution of backward()\""}