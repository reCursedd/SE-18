{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2332", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2332/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2332/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2332/events", "html_url": "https://github.com/pytorch/pytorch/issues/2332", "id": 248647185, "node_id": "MDU6SXNzdWUyNDg2NDcxODU=", "number": 2332, "title": "Unhandled CUDA Error (1)", "user": {"login": "ritchieng", "id": 11657655, "node_id": "MDQ6VXNlcjExNjU3NjU1", "avatar_url": "https://avatars3.githubusercontent.com/u/11657655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ritchieng", "html_url": "https://github.com/ritchieng", "followers_url": "https://api.github.com/users/ritchieng/followers", "following_url": "https://api.github.com/users/ritchieng/following{/other_user}", "gists_url": "https://api.github.com/users/ritchieng/gists{/gist_id}", "starred_url": "https://api.github.com/users/ritchieng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ritchieng/subscriptions", "organizations_url": "https://api.github.com/users/ritchieng/orgs", "repos_url": "https://api.github.com/users/ritchieng/repos", "events_url": "https://api.github.com/users/ritchieng/events{/privacy}", "received_events_url": "https://api.github.com/users/ritchieng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2017-08-08T09:02:30Z", "updated_at": "2018-03-04T16:02:27Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>Error</strong></p>\n<pre><code>    Traceback (most recent call last):\n      File \"main.py\", line 201, in &lt;module&gt;\n        loss_list, lr_epoch, mu_epoch = train(epoch)\n      File \"main.py\", line 132, in train\n        outputs = net(inputs)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n        result = self.forward(*input, **kwargs)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 59, in forward\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 64, in replicate\n        return replicate(module, device_ids)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/replicate.py\", line 12, in replicate\n        param_copies = Broadcast(devices)(*params)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/_functions.py\", line 19, in forward\n        outputs = comm.broadcast_coalesced(inputs, self.target_gpus)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 54, in broadcast_coalesced\n        results = broadcast(_flatten_tensors(chunk), devices)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 24, in broadcast\n        nccl.broadcast(tensors)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 190, in broadcast\n        data_type, root, comm[i], cudaStream()))\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 118, in check_error\n        raise NcclError(status)\n    torch.cuda.nccl.NcclError: Unhandled Cuda Error (1)\n</code></pre>\n<p>When I run without GPU, the code is fine. On v0.1.12 it is fine on GPU and CPU.</p>\n<p><strong>Lines with issues I believe</strong></p>\n<pre><code>    if use_cuda:\n        net.cuda()\n        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n        cudnn.benchmark = True\n</code></pre>", "body_text": "Error\n    Traceback (most recent call last):\n      File \"main.py\", line 201, in <module>\n        loss_list, lr_epoch, mu_epoch = train(epoch)\n      File \"main.py\", line 132, in train\n        outputs = net(inputs)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 224, in __call__\n        result = self.forward(*input, **kwargs)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 59, in forward\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 64, in replicate\n        return replicate(module, device_ids)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/replicate.py\", line 12, in replicate\n        param_copies = Broadcast(devices)(*params)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/_functions.py\", line 19, in forward\n        outputs = comm.broadcast_coalesced(inputs, self.target_gpus)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 54, in broadcast_coalesced\n        results = broadcast(_flatten_tensors(chunk), devices)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 24, in broadcast\n        nccl.broadcast(tensors)\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 190, in broadcast\n        data_type, root, comm[i], cudaStream()))\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 118, in check_error\n        raise NcclError(status)\n    torch.cuda.nccl.NcclError: Unhandled Cuda Error (1)\n\nWhen I run without GPU, the code is fine. On v0.1.12 it is fine on GPU and CPU.\nLines with issues I believe\n    if use_cuda:\n        net.cuda()\n        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n        cudnn.benchmark = True", "body": "**Error**\r\n```\r\n    Traceback (most recent call last):\r\n      File \"main.py\", line 201, in <module>\r\n        loss_list, lr_epoch, mu_epoch = train(epoch)\r\n      File \"main.py\", line 132, in train\r\n        outputs = net(inputs)\r\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/modules/module.py\", line 224, in __call__\r\n        result = self.forward(*input, **kwargs)\r\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 59, in forward\r\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\r\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/data_parallel.py\", line 64, in replicate\r\n        return replicate(module, device_ids)\r\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/replicate.py\", line 12, in replicate\r\n        param_copies = Broadcast(devices)(*params)\r\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/nn/parallel/_functions.py\", line 19, in forward\r\n        outputs = comm.broadcast_coalesced(inputs, self.target_gpus)\r\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 54, in broadcast_coalesced\r\n        results = broadcast(_flatten_tensors(chunk), devices)\r\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/comm.py\", line 24, in broadcast\r\n        nccl.broadcast(tensors)\r\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 190, in broadcast\r\n        data_type, root, comm[i], cudaStream()))\r\n      File \"/home/lala/miniconda2/lib/python2.7/site-packages/torch/cuda/nccl.py\", line 118, in check_error\r\n        raise NcclError(status)\r\n    torch.cuda.nccl.NcclError: Unhandled Cuda Error (1)\r\n```\r\n\r\n\r\nWhen I run without GPU, the code is fine. On v0.1.12 it is fine on GPU and CPU.\r\n\r\n**Lines with issues I believe**\r\n\r\n```\r\n    if use_cuda:\r\n        net.cuda()\r\n        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\r\n        cudnn.benchmark = True\r\n```"}