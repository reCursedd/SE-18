{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4179", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4179/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4179/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4179/events", "html_url": "https://github.com/pytorch/pytorch/issues/4179", "id": 282222466, "node_id": "MDU6SXNzdWUyODIyMjI0NjY=", "number": 4179, "title": "Feature Request: Support torch.autograd.grad() in optimizers", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-14T20:03:26Z", "updated_at": "2017-12-14T20:13:14Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>My team is trying to use <code>torch.autograd.grad(...)</code> with torch optimizers, but optimizers only consume <code>.grad</code> properties, not arbitrary lists of gradients. To work around this, we manually set <code>.grad</code> properties from stuff we compute with <code>grad(...)</code>.</p>\n<p>Could optimizers add first-class support for lists of gradients? <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> <a href=\"https://discuss.pytorch.org/t/using-optimizers-with-torch-autograd-grad/11078\" rel=\"nofollow\">suggests</a> that</p>\n<blockquote>\n<p>optimizers should take an optional <code>grads</code> argument, either in step or at construction time</p>\n</blockquote>", "body_text": "My team is trying to use torch.autograd.grad(...) with torch optimizers, but optimizers only consume .grad properties, not arbitrary lists of gradients. To work around this, we manually set .grad properties from stuff we compute with grad(...).\nCould optimizers add first-class support for lists of gradients? @soumith suggests that\n\noptimizers should take an optional grads argument, either in step or at construction time", "body": "My team is trying to use `torch.autograd.grad(...)` with torch optimizers, but optimizers only consume `.grad` properties, not arbitrary lists of gradients. To work around this, we manually set `.grad` properties from stuff we compute with `grad(...)`.\r\n\r\nCould optimizers add first-class support for lists of gradients? @soumith [suggests](https://discuss.pytorch.org/t/using-optimizers-with-torch-autograd-grad/11078) that\r\n>  optimizers should take an optional `grads` argument, either in step or at construction time"}