{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4007", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4007/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4007/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4007/events", "html_url": "https://github.com/pytorch/pytorch/issues/4007", "id": 279122791, "node_id": "MDU6SXNzdWUyNzkxMjI3OTE=", "number": 4007, "title": "Feature request: expm1", "user": {"login": "frkl", "id": 12701816, "node_id": "MDQ6VXNlcjEyNzAxODE2", "avatar_url": "https://avatars3.githubusercontent.com/u/12701816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frkl", "html_url": "https://github.com/frkl", "followers_url": "https://api.github.com/users/frkl/followers", "following_url": "https://api.github.com/users/frkl/following{/other_user}", "gists_url": "https://api.github.com/users/frkl/gists{/gist_id}", "starred_url": "https://api.github.com/users/frkl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frkl/subscriptions", "organizations_url": "https://api.github.com/users/frkl/orgs", "repos_url": "https://api.github.com/users/frkl/repos", "events_url": "https://api.github.com/users/frkl/events{/privacy}", "received_events_url": "https://api.github.com/users/frkl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-12-04T19:41:39Z", "updated_at": "2017-12-28T10:24:10Z", "closed_at": "2017-12-28T10:24:10Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>numpy has expm1 and log1p functions for numerically stable exp(x)-1 and log(1+x) when x is small. For instance, expm1 can be computed using</p>\n<p><code>exp(x)-1=x+x/2!+x/3!+x/4!+...</code></p>\n<p>There should be faster implementations out there somewhere.</p>\n<p>These two functions are useful for numerically accurate conversions between log probability and logit. Below are my current mock up implementations. They are very slow and might use huge speedup with a proper implementation.</p>\n<pre><code>def expm1(x): #only works in ~ [-0.5,0.5]\n\tv=x;\n\tv=(v/8+1)*x\n\tv=(v/7+1)*x\n\tv=(v/6+1)*x\n\tv=(v/5+1)*x\n\tv=(v/4+1)*x\n\tv=(v/3+1)*x\n\tv=(v/2+1)*x\n\treturn v;\n\ndef log1p(x): #only works in ~ [-0.5,0.5]\n\tv=x/10;\n\tv=-v*x+x/9;\n\tv=-v*x+x/8;\n\tv=-v*x+x/7;\n\tv=-v*x+x/6;\n\tv=-v*x+x/5;\n\tv=-v*x+x/4;\n\tv=-v*x+x/3;\n\tv=-v*x+x/2;\n\tv=-v*x+x;\n\treturn v;\n</code></pre>\n<p>Also, it would be nice to add functions that convert log probability to logit and back.<br>\nConverting logit to log probability is logsigmoid. Current implementation of logsigmoid has good precision but does not support second order gradient. Converting log probability to logit is logitexp.<br>\nHere are my mock up implementations.</p>\n<pre><code>\ndef logsigmoid(x):\n\tpos=torch.clamp(x,min=1);\n\tneg=torch.clamp(x,max=1);\n\tout=-log1p(torch.exp(-pos))+neg-torch.log(1+torch.exp(neg))+0.31326168751;\n\treturn out;\n\ndef logitexp(logp):\n\tpos=torch.clamp(logp,min=-0.69314718056);\n\tneg=torch.clamp(logp,max=-0.69314718056);\n\tneg_val=neg-torch.log(1-torch.exp(neg));\n\tpos_val=-torch.log(torch.clamp(expm1(-pos),min=1e-20));\n\treturn pos_val+neg_val;\n</code></pre>", "body_text": "Hi,\nnumpy has expm1 and log1p functions for numerically stable exp(x)-1 and log(1+x) when x is small. For instance, expm1 can be computed using\nexp(x)-1=x+x/2!+x/3!+x/4!+...\nThere should be faster implementations out there somewhere.\nThese two functions are useful for numerically accurate conversions between log probability and logit. Below are my current mock up implementations. They are very slow and might use huge speedup with a proper implementation.\ndef expm1(x): #only works in ~ [-0.5,0.5]\n\tv=x;\n\tv=(v/8+1)*x\n\tv=(v/7+1)*x\n\tv=(v/6+1)*x\n\tv=(v/5+1)*x\n\tv=(v/4+1)*x\n\tv=(v/3+1)*x\n\tv=(v/2+1)*x\n\treturn v;\n\ndef log1p(x): #only works in ~ [-0.5,0.5]\n\tv=x/10;\n\tv=-v*x+x/9;\n\tv=-v*x+x/8;\n\tv=-v*x+x/7;\n\tv=-v*x+x/6;\n\tv=-v*x+x/5;\n\tv=-v*x+x/4;\n\tv=-v*x+x/3;\n\tv=-v*x+x/2;\n\tv=-v*x+x;\n\treturn v;\n\nAlso, it would be nice to add functions that convert log probability to logit and back.\nConverting logit to log probability is logsigmoid. Current implementation of logsigmoid has good precision but does not support second order gradient. Converting log probability to logit is logitexp.\nHere are my mock up implementations.\n\ndef logsigmoid(x):\n\tpos=torch.clamp(x,min=1);\n\tneg=torch.clamp(x,max=1);\n\tout=-log1p(torch.exp(-pos))+neg-torch.log(1+torch.exp(neg))+0.31326168751;\n\treturn out;\n\ndef logitexp(logp):\n\tpos=torch.clamp(logp,min=-0.69314718056);\n\tneg=torch.clamp(logp,max=-0.69314718056);\n\tneg_val=neg-torch.log(1-torch.exp(neg));\n\tpos_val=-torch.log(torch.clamp(expm1(-pos),min=1e-20));\n\treturn pos_val+neg_val;", "body": "Hi,\r\n\r\nnumpy has expm1 and log1p functions for numerically stable exp(x)-1 and log(1+x) when x is small. For instance, expm1 can be computed using\r\n\r\n`exp(x)-1=x+x/2!+x/3!+x/4!+...`\r\n\r\nThere should be faster implementations out there somewhere.\r\n\r\nThese two functions are useful for numerically accurate conversions between log probability and logit. Below are my current mock up implementations. They are very slow and might use huge speedup with a proper implementation.\r\n\r\n```\r\ndef expm1(x): #only works in ~ [-0.5,0.5]\r\n\tv=x;\r\n\tv=(v/8+1)*x\r\n\tv=(v/7+1)*x\r\n\tv=(v/6+1)*x\r\n\tv=(v/5+1)*x\r\n\tv=(v/4+1)*x\r\n\tv=(v/3+1)*x\r\n\tv=(v/2+1)*x\r\n\treturn v;\r\n\r\ndef log1p(x): #only works in ~ [-0.5,0.5]\r\n\tv=x/10;\r\n\tv=-v*x+x/9;\r\n\tv=-v*x+x/8;\r\n\tv=-v*x+x/7;\r\n\tv=-v*x+x/6;\r\n\tv=-v*x+x/5;\r\n\tv=-v*x+x/4;\r\n\tv=-v*x+x/3;\r\n\tv=-v*x+x/2;\r\n\tv=-v*x+x;\r\n\treturn v;\r\n```\r\n\r\nAlso, it would be nice to add functions that convert log probability to logit and back. \r\nConverting logit to log probability is logsigmoid. Current implementation of logsigmoid has good precision but does not support second order gradient. Converting log probability to logit is logitexp. \r\nHere are my mock up implementations.\r\n\r\n```\r\n\r\ndef logsigmoid(x):\r\n\tpos=torch.clamp(x,min=1);\r\n\tneg=torch.clamp(x,max=1);\r\n\tout=-log1p(torch.exp(-pos))+neg-torch.log(1+torch.exp(neg))+0.31326168751;\r\n\treturn out;\r\n\r\ndef logitexp(logp):\r\n\tpos=torch.clamp(logp,min=-0.69314718056);\r\n\tneg=torch.clamp(logp,max=-0.69314718056);\r\n\tneg_val=neg-torch.log(1-torch.exp(neg));\r\n\tpos_val=-torch.log(torch.clamp(expm1(-pos),min=1e-20));\r\n\treturn pos_val+neg_val;\r\n```"}