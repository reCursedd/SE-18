{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13478", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13478/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13478/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13478/events", "html_url": "https://github.com/pytorch/pytorch/issues/13478", "id": 376570367, "node_id": "MDU6SXNzdWUzNzY1NzAzNjc=", "number": 13478, "title": "x.sum(64) now raises a bitset error", "user": {"login": "fritzo", "id": 648532, "node_id": "MDQ6VXNlcjY0ODUzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/648532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fritzo", "html_url": "https://github.com/fritzo", "followers_url": "https://api.github.com/users/fritzo/followers", "following_url": "https://api.github.com/users/fritzo/following{/other_user}", "gists_url": "https://api.github.com/users/fritzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/fritzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fritzo/subscriptions", "organizations_url": "https://api.github.com/users/fritzo/orgs", "repos_url": "https://api.github.com/users/fritzo/repos", "events_url": "https://api.github.com/users/fritzo/events{/privacy}", "received_events_url": "https://api.github.com/users/fritzo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-11-01T21:04:43Z", "updated_at": "2018-11-01T22:04:55Z", "closed_at": "2018-11-01T22:04:54Z", "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p><code>torch.sum(dim)</code> no longer works when dim &gt;= 64.</p>\n<h2>To Reproduce</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.ones((<span class=\"pl-c1\">2</span>,) <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span>,) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">62</span> <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">2</span>,)).sum(<span class=\"pl-c1\">63</span>).dim()\n<span class=\"pl-c1\">63</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.ones((<span class=\"pl-c1\">2</span>,) <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span>,) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">63</span> <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">2</span>,)).sum(<span class=\"pl-c1\">64</span>).dim()\nTraceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;stdin&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-c1\">RuntimeError</span>: bitset <span class=\"pl-c1\">set</span> argument out of <span class=\"pl-c1\">range</span></pre></div>\n<h2>Expected behavior</h2>\n<p>An ugly workaround is to squeeze and reshape.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">workaround_sum</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">dim</span>):\n    result_shape <span class=\"pl-k\">=</span> x.shape[:dim] <span class=\"pl-k\">+</span> x.shape[<span class=\"pl-c1\">1</span><span class=\"pl-k\">+</span>dim:]\n    x <span class=\"pl-k\">=</span> x.reshape((<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,) <span class=\"pl-k\">+</span> x.shape[dim:])\n    x <span class=\"pl-k\">=</span> x.reshape(x.shape[<span class=\"pl-c1\">0</span>], x.shape[<span class=\"pl-c1\">1</span>], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    x <span class=\"pl-k\">=</span> x.sum(<span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">return</span> x.reshape(result_shape)</pre></div>\n<h2>Environment</h2>\n<p>PyTorch version: 1.0.0a0+99ce499<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: None</p>\n<p>OS: Mac OSX 10.13.3<br>\nGCC version: Could not collect<br>\nCMake version: version 3.12.0</p>\n<p>Python version: 2.7<br>\nIs CUDA available: No<br>\nCUDA runtime version: No CUDA<br>\nGPU models and configuration: No CUDA<br>\nNvidia driver version: No CUDA<br>\ncuDNN version: No CUDA</p>\n<p>Versions of relevant libraries:<br>\n[pip] Could not collect<br>\n[conda] torchfile                 0.1.0                     <br>\n[conda] torchvision               0.2.0                     </p>", "body_text": "\ud83d\udc1b Bug\ntorch.sum(dim) no longer works when dim >= 64.\nTo Reproduce\n>>> torch.ones((2,) + (1,) * 62 + (2,)).sum(63).dim()\n63\n>>> torch.ones((2,) + (1,) * 63 + (2,)).sum(64).dim()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: bitset set argument out of range\nExpected behavior\nAn ugly workaround is to squeeze and reshape.\ndef workaround_sum(x, dim):\n    result_shape = x.shape[:dim] + x.shape[1+dim:]\n    x = x.reshape((-1,) + x.shape[dim:])\n    x = x.reshape(x.shape[0], x.shape[1], -1)\n    x = x.sum(1)\n    return x.reshape(result_shape)\nEnvironment\nPyTorch version: 1.0.0a0+99ce499\nIs debug build: No\nCUDA used to build PyTorch: None\nOS: Mac OSX 10.13.3\nGCC version: Could not collect\nCMake version: version 3.12.0\nPython version: 2.7\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] torchfile                 0.1.0                     \n[conda] torchvision               0.2.0", "body": "## \ud83d\udc1b Bug\r\n\r\n`torch.sum(dim)` no longer works when dim >= 64.\r\n\r\n## To Reproduce\r\n\r\n```py\r\n>>> torch.ones((2,) + (1,) * 62 + (2,)).sum(63).dim()\r\n63\r\n>>> torch.ones((2,) + (1,) * 63 + (2,)).sum(64).dim()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: bitset set argument out of range\r\n```\r\n\r\n## Expected behavior\r\n\r\nAn ugly workaround is to squeeze and reshape.\r\n```py\r\ndef workaround_sum(x, dim):\r\n    result_shape = x.shape[:dim] + x.shape[1+dim:]\r\n    x = x.reshape((-1,) + x.shape[dim:])\r\n    x = x.reshape(x.shape[0], x.shape[1], -1)\r\n    x = x.sum(1)\r\n    return x.reshape(result_shape)\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0a0+99ce499\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.3\r\nGCC version: Could not collect\r\nCMake version: version 3.12.0\r\n\r\nPython version: 2.7\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] torchfile                 0.1.0                     <pip>\r\n[conda] torchvision               0.2.0                     <pip>"}