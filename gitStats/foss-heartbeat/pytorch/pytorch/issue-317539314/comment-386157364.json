{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386157364", "html_url": "https://github.com/pytorch/pytorch/issues/6940#issuecomment-386157364", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6940", "id": 386157364, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjE1NzM2NA==", "user": {"login": "MlWoo", "id": 20226293, "node_id": "MDQ6VXNlcjIwMjI2Mjkz", "avatar_url": "https://avatars2.githubusercontent.com/u/20226293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MlWoo", "html_url": "https://github.com/MlWoo", "followers_url": "https://api.github.com/users/MlWoo/followers", "following_url": "https://api.github.com/users/MlWoo/following{/other_user}", "gists_url": "https://api.github.com/users/MlWoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/MlWoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MlWoo/subscriptions", "organizations_url": "https://api.github.com/users/MlWoo/orgs", "repos_url": "https://api.github.com/users/MlWoo/repos", "events_url": "https://api.github.com/users/MlWoo/events{/privacy}", "received_events_url": "https://api.github.com/users/MlWoo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-03T00:02:39Z", "updated_at": "2018-05-03T00:06:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> I do not think the requirement is reasonable. If we have the version compiled with MKL, we can also freeze the RNG to compare the results of our other developments which use the RNG. In other words, it does not affect the consistency of itself. Actually, do we make sure the RNG on CPU and that on GPU are the same if the seeds are the same? Or do we compare the result of RNG from PyTorch with that from caffe2 if the seeds are the same. I think it is OK if the RNG does not destroy the self-integrity and self-consistency in a version with or without MKL.  Further, it is maybe not necessary to make sure the integrity and consistency in two versions compiled in different envs. If a model depends on the implementation of RNG, I think the model is useless.</p>", "body_text": "@soumith I do not think the requirement is reasonable. If we have the version compiled with MKL, we can also freeze the RNG to compare the results of our other developments which use the RNG. In other words, it does not affect the consistency of itself. Actually, do we make sure the RNG on CPU and that on GPU are the same if the seeds are the same? Or do we compare the result of RNG from PyTorch with that from caffe2 if the seeds are the same. I think it is OK if the RNG does not destroy the self-integrity and self-consistency in a version with or without MKL.  Further, it is maybe not necessary to make sure the integrity and consistency in two versions compiled in different envs. If a model depends on the implementation of RNG, I think the model is useless.", "body": "@soumith I do not think the requirement is reasonable. If we have the version compiled with MKL, we can also freeze the RNG to compare the results of our other developments which use the RNG. In other words, it does not affect the consistency of itself. Actually, do we make sure the RNG on CPU and that on GPU are the same if the seeds are the same? Or do we compare the result of RNG from PyTorch with that from caffe2 if the seeds are the same. I think it is OK if the RNG does not destroy the self-integrity and self-consistency in a version with or without MKL.  Further, it is maybe not necessary to make sure the integrity and consistency in two versions compiled in different envs. If a model depends on the implementation of RNG, I think the model is useless."}