{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/387993429", "html_url": "https://github.com/pytorch/pytorch/issues/6940#issuecomment-387993429", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6940", "id": 387993429, "node_id": "MDEyOklzc3VlQ29tbWVudDM4Nzk5MzQyOQ==", "user": {"login": "MlWoo", "id": 20226293, "node_id": "MDQ6VXNlcjIwMjI2Mjkz", "avatar_url": "https://avatars2.githubusercontent.com/u/20226293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MlWoo", "html_url": "https://github.com/MlWoo", "followers_url": "https://api.github.com/users/MlWoo/followers", "following_url": "https://api.github.com/users/MlWoo/following{/other_user}", "gists_url": "https://api.github.com/users/MlWoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/MlWoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MlWoo/subscriptions", "organizations_url": "https://api.github.com/users/MlWoo/orgs", "repos_url": "https://api.github.com/users/MlWoo/repos", "events_url": "https://api.github.com/users/MlWoo/events{/privacy}", "received_events_url": "https://api.github.com/users/MlWoo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-10T08:47:03Z", "updated_at": "2018-05-10T13:37:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a><br>\nThere are some tricky problems should be solved.<br>\n1). <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Distributions.cpp#L111-L119\">These lines</a> shows <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Distributions.cpp#L116\">the <code>bernoulli_</code> operation with a scalar probability</a>  which is exactly called by <code>Dropout</code> is implemented by reusing <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Distributions.cpp#L112\">the <code>bernoulli</code> operation with a tensor probability</a>.   And the <code>bernoulli</code> operation with a tensor is instanced only in float and double type. The view could be drew from <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/Declarations.cwrap#L3615-L3616\">the scripts code</a>. So there are possibly 2 redundant operations(filling a tensr with a scalar AND copying &amp; convcerting floating-point to other type) in term with the <code>bernoulli_</code> operation with a scalar probability.<br>\nHowever, the <code>bernoulli_</code> operation with a scalar probability could be implemtented by calling  <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/TH/generic/THTensorRandom.cpp#L54\"><code>THTensor_(bernoulli)</code> </a> directly.  I can not figure out why we abandon the simple and efficient way and select a complex and time-consuming path.<br>\n2). <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/Declarations.cwrap#L3612-L3630\">The scripts code</a> of  <code>bernoulli</code> operation does not provide a interface for a scalar probability and is in <code>at</code> scope. But <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L147-L149\">the scripts code</a> of  <code>bernoulli_</code> operation does provide that and in <code>at::native</code> scope.  I am also confused by the convention.</p>\n<p>I plan to boost <code>THTensor_(bernoulli)</code> with vsl and opoenmp. There are my plans.<br>\n1). adding a interface for a scalar probability to <code>bernoulli</code>. But bernoulli could be not parsed as a in-place functions. As what I know, the suffix of <code>_</code> just suggests the operation runs in-place. It maybe need to modify the parse code.<br>\n2). recover <code>bernoulli_</code> in same level of <code>bernoulli</code>. In other words, <code>bernoulli_</code> could be visited in the <code>at</code> scope</p>\n<p>I am really looking forward to your professinal advice.</p>", "body_text": "@cpuhrsch @soumith\nThere are some tricky problems should be solved.\n1). These lines shows the bernoulli_ operation with a scalar probability  which is exactly called by Dropout is implemented by reusing the bernoulli operation with a tensor probability.   And the bernoulli operation with a tensor is instanced only in float and double type. The view could be drew from the scripts code. So there are possibly 2 redundant operations(filling a tensr with a scalar AND copying & convcerting floating-point to other type) in term with the bernoulli_ operation with a scalar probability.\nHowever, the bernoulli_ operation with a scalar probability could be implemtented by calling  THTensor_(bernoulli)  directly.  I can not figure out why we abandon the simple and efficient way and select a complex and time-consuming path.\n2). The scripts code of  bernoulli operation does not provide a interface for a scalar probability and is in at scope. But the scripts code of  bernoulli_ operation does provide that and in at::native scope.  I am also confused by the convention.\nI plan to boost THTensor_(bernoulli) with vsl and opoenmp. There are my plans.\n1). adding a interface for a scalar probability to bernoulli. But bernoulli could be not parsed as a in-place functions. As what I know, the suffix of _ just suggests the operation runs in-place. It maybe need to modify the parse code.\n2). recover bernoulli_ in same level of bernoulli. In other words, bernoulli_ could be visited in the at scope\nI am really looking forward to your professinal advice.", "body": "@cpuhrsch @soumith \r\nThere are some tricky problems should be solved. \r\n1). [These lines](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Distributions.cpp#L111-L119) shows [the `bernoulli_` operation with a scalar probability](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Distributions.cpp#L116)  which is exactly called by `Dropout` is implemented by reusing [the `bernoulli` operation with a tensor probability](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Distributions.cpp#L112).   And the `bernoulli` operation with a tensor is instanced only in float and double type. The view could be drew from [the scripts code](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/Declarations.cwrap#L3615-L3616). So there are possibly 2 redundant operations(filling a tensr with a scalar AND copying & convcerting floating-point to other type) in term with the `bernoulli_` operation with a scalar probability. \r\n However, the `bernoulli_` operation with a scalar probability could be implemtented by calling  [`THTensor_(bernoulli)` ](https://github.com/pytorch/pytorch/blob/master/aten/src/TH/generic/THTensorRandom.cpp#L54) directly.  I can not figure out why we abandon the simple and efficient way and select a complex and time-consuming path.\r\n2). [The scripts code](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/Declarations.cwrap#L3612-L3630) of  `bernoulli` operation does not provide a interface for a scalar probability and is in `at` scope. But [the scripts code](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml#L147-L149) of  `bernoulli_` operation does provide that and in `at::native` scope.  I am also confused by the convention.\r\n\r\nI plan to boost `THTensor_(bernoulli)` with vsl and opoenmp. There are my plans.\r\n1). adding a interface for a scalar probability to `bernoulli`. But bernoulli could be not parsed as a in-place functions. As what I know, the suffix of `_` just suggests the operation runs in-place. It maybe need to modify the parse code.\r\n2). recover `bernoulli_` in same level of `bernoulli`. In other words, `bernoulli_` could be visited in the `at` scope\r\n\r\n I am really looking forward to your professinal advice. \r\n"}