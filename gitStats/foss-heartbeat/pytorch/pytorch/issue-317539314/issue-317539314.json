{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6940", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6940/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6940/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6940/events", "html_url": "https://github.com/pytorch/pytorch/issues/6940", "id": 317539314, "node_id": "MDU6SXNzdWUzMTc1MzkzMTQ=", "number": 6940, "title": "[Feature request]: Accelerate bernoulli number generation on CPU ", "user": {"login": "MlWoo", "id": 20226293, "node_id": "MDQ6VXNlcjIwMjI2Mjkz", "avatar_url": "https://avatars2.githubusercontent.com/u/20226293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MlWoo", "html_url": "https://github.com/MlWoo", "followers_url": "https://api.github.com/users/MlWoo/followers", "following_url": "https://api.github.com/users/MlWoo/following{/other_user}", "gists_url": "https://api.github.com/users/MlWoo/gists{/gist_id}", "starred_url": "https://api.github.com/users/MlWoo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MlWoo/subscriptions", "organizations_url": "https://api.github.com/users/MlWoo/orgs", "repos_url": "https://api.github.com/users/MlWoo/repos", "events_url": "https://api.github.com/users/MlWoo/events{/privacy}", "received_events_url": "https://api.github.com/users/MlWoo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-04-25T09:04:41Z", "updated_at": "2018-06-05T15:11:29Z", "closed_at": "2018-06-05T15:11:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The bernoulli numbers in current version are generated in series by a single, naive and serial stream. The performance of OPs like Dropout which call the steam is poor on CPU. The evidence of some case shows that the performance on CPU is 250X slower than that on GPU. But the gap should not be that great in consideration of the peak theoretical performance.<br>\nThe code shows it is clear that <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THC/generic/THCTensorRandom.cu#L414\">bernoulli number generation of GPU</a> is not restricted by the only thread. Furtherly, the code also calls the lib of cuda to create random number(<a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THC/generic/THCTensorRandom.cu#L400\">curand_uniform_double</a>).</p>\n<p>Actually, <a href=\"https://github.com/intel/caffe\">Intel-Caffe</a> takes advantage of <a href=\"https://github.com/intel/caffe/blob/9f0108f5a82d3a180f9b8837b7b09d72d8af8dcc/src/caffe/util/math_functions.cpp#L424-L457\">VSL math lib and openmp</a> to do the same work on CPU as that on GPU in PyTorch. We may borrow the code to help PyTorch to get better performance on CPU. However, I also notice that the seed of random number stream of current version could be set mannually. We maybe change the code in Intel-Caffe slightly if necessary.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1716488\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cpuhrsch\">@cpuhrsch</a> Your advice is important to us because of your effort on CPU. Could you spare some time to look into the part of code? Looking forward to your point.</p>", "body_text": "The bernoulli numbers in current version are generated in series by a single, naive and serial stream. The performance of OPs like Dropout which call the steam is poor on CPU. The evidence of some case shows that the performance on CPU is 250X slower than that on GPU. But the gap should not be that great in consideration of the peak theoretical performance.\nThe code shows it is clear that bernoulli number generation of GPU is not restricted by the only thread. Furtherly, the code also calls the lib of cuda to create random number(curand_uniform_double).\nActually, Intel-Caffe takes advantage of VSL math lib and openmp to do the same work on CPU as that on GPU in PyTorch. We may borrow the code to help PyTorch to get better performance on CPU. However, I also notice that the seed of random number stream of current version could be set mannually. We maybe change the code in Intel-Caffe slightly if necessary.\n@cpuhrsch Your advice is important to us because of your effort on CPU. Could you spare some time to look into the part of code? Looking forward to your point.", "body": "The bernoulli numbers in current version are generated in series by a single, naive and serial stream. The performance of OPs like Dropout which call the steam is poor on CPU. The evidence of some case shows that the performance on CPU is 250X slower than that on GPU. But the gap should not be that great in consideration of the peak theoretical performance.  \r\nThe code shows it is clear that [bernoulli number generation of GPU](https://github.com/pytorch/pytorch/blob/master/aten/src/THC/generic/THCTensorRandom.cu#L414) is not restricted by the only thread. Furtherly, the code also calls the lib of cuda to create random number([curand_uniform_double](https://github.com/pytorch/pytorch/blob/master/aten/src/THC/generic/THCTensorRandom.cu#L400)).\r\n\r\nActually, [Intel-Caffe](https://github.com/intel/caffe) takes advantage of [VSL math lib and openmp](https://github.com/intel/caffe/blob/9f0108f5a82d3a180f9b8837b7b09d72d8af8dcc/src/caffe/util/math_functions.cpp#L424-L457) to do the same work on CPU as that on GPU in PyTorch. We may borrow the code to help PyTorch to get better performance on CPU. However, I also notice that the seed of random number stream of current version could be set mannually. We maybe change the code in Intel-Caffe slightly if necessary.\r\n\r\n@cpuhrsch Your advice is important to us because of your effort on CPU. Could you spare some time to look into the part of code? Looking forward to your point.\r\n"}