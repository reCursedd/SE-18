{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/379076657", "html_url": "https://github.com/pytorch/pytorch/issues/6321#issuecomment-379076657", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6321", "id": 379076657, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTA3NjY1Nw==", "user": {"login": "nikoliazekter", "id": 6832533, "node_id": "MDQ6VXNlcjY4MzI1MzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/6832533?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikoliazekter", "html_url": "https://github.com/nikoliazekter", "followers_url": "https://api.github.com/users/nikoliazekter/followers", "following_url": "https://api.github.com/users/nikoliazekter/following{/other_user}", "gists_url": "https://api.github.com/users/nikoliazekter/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikoliazekter/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikoliazekter/subscriptions", "organizations_url": "https://api.github.com/users/nikoliazekter/orgs", "repos_url": "https://api.github.com/users/nikoliazekter/repos", "events_url": "https://api.github.com/users/nikoliazekter/events{/privacy}", "received_events_url": "https://api.github.com/users/nikoliazekter/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-05T21:05:43Z", "updated_at": "2018-04-05T21:10:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11948233\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sdmonov\">@sdmonov</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> so I tried that but I still get the warning. Looking at the <code>torch.cuda</code> sources it seems like the warning is hardcoded and does not actually care about specified capability:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_check_capability</span>():\n    incorrect_binary_warn <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Found GPU<span class=\"pl-c1\">%d</span> <span class=\"pl-c1\">%s</span> which requires CUDA_VERSION &gt;= <span class=\"pl-c1\">%d</span> for</span>\n<span class=\"pl-s\">     optimal performance and fast startup time, but your PyTorch was compiled</span>\n<span class=\"pl-s\">     with CUDA_VERSION <span class=\"pl-c1\">%d</span>. Please install the correct PyTorch binary</span>\n<span class=\"pl-s\">     using instructions from http://pytorch.org</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    old_gpu_warn <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Found GPU<span class=\"pl-c1\">%d</span> <span class=\"pl-c1\">%s</span> which is of cuda capability <span class=\"pl-c1\">%d</span>.<span class=\"pl-c1\">%d</span>.</span>\n<span class=\"pl-s\">    PyTorch no longer supports this GPU because it is too old.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-c1\">CUDA_VERSION</span> <span class=\"pl-k\">=</span> torch._C._cuda_getCompiledVersion()\n    <span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(device_count()):\n        capability <span class=\"pl-k\">=</span> get_device_capability(d)\n        major <span class=\"pl-k\">=</span> capability[<span class=\"pl-c1\">0</span>]\n        name <span class=\"pl-k\">=</span> get_device_name(d)\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">CUDA_VERSION</span> <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">8000</span> <span class=\"pl-k\">and</span> major <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">6</span>:\n            warnings.warn(incorrect_binary_warn <span class=\"pl-k\">%</span> (d, name, <span class=\"pl-c1\">8000</span>, <span class=\"pl-c1\">CUDA_VERSION</span>))\n        <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">CUDA_VERSION</span> <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">9000</span> <span class=\"pl-k\">and</span> major <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">7</span>:\n            warnings.warn(incorrect_binary_warn <span class=\"pl-k\">%</span> (d, name, <span class=\"pl-c1\">9000</span>, <span class=\"pl-c1\">CUDA_VERSION</span>))\n        <span class=\"pl-k\">elif</span> capability <span class=\"pl-k\">==</span> (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">0</span>) <span class=\"pl-k\">or</span> capability <span class=\"pl-k\">==</span> (<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">0</span>) <span class=\"pl-k\">or</span> major <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">3</span>:\n            warnings.warn(old_gpu_warn <span class=\"pl-k\">%</span> (d, name, major, capability[<span class=\"pl-c1\">1</span>]))</pre></div>", "body_text": "@sdmonov @soumith so I tried that but I still get the warning. Looking at the torch.cuda sources it seems like the warning is hardcoded and does not actually care about specified capability:\ndef _check_capability():\n    incorrect_binary_warn = \"\"\"\n    Found GPU%d %s which requires CUDA_VERSION >= %d for\n     optimal performance and fast startup time, but your PyTorch was compiled\n     with CUDA_VERSION %d. Please install the correct PyTorch binary\n     using instructions from http://pytorch.org\n    \"\"\"\n\n    old_gpu_warn = \"\"\"\n    Found GPU%d %s which is of cuda capability %d.%d.\n    PyTorch no longer supports this GPU because it is too old.\n    \"\"\"\n\n    CUDA_VERSION = torch._C._cuda_getCompiledVersion()\n    for d in range(device_count()):\n        capability = get_device_capability(d)\n        major = capability[0]\n        name = get_device_name(d)\n        if CUDA_VERSION < 8000 and major >= 6:\n            warnings.warn(incorrect_binary_warn % (d, name, 8000, CUDA_VERSION))\n        elif CUDA_VERSION < 9000 and major >= 7:\n            warnings.warn(incorrect_binary_warn % (d, name, 9000, CUDA_VERSION))\n        elif capability == (3, 0) or capability == (5, 0) or major < 3:\n            warnings.warn(old_gpu_warn % (d, name, major, capability[1]))", "body": "@sdmonov @soumith so I tried that but I still get the warning. Looking at the `torch.cuda` sources it seems like the warning is hardcoded and does not actually care about specified capability:\r\n```python\r\ndef _check_capability():\r\n    incorrect_binary_warn = \"\"\"\r\n    Found GPU%d %s which requires CUDA_VERSION >= %d for\r\n     optimal performance and fast startup time, but your PyTorch was compiled\r\n     with CUDA_VERSION %d. Please install the correct PyTorch binary\r\n     using instructions from http://pytorch.org\r\n    \"\"\"\r\n\r\n    old_gpu_warn = \"\"\"\r\n    Found GPU%d %s which is of cuda capability %d.%d.\r\n    PyTorch no longer supports this GPU because it is too old.\r\n    \"\"\"\r\n\r\n    CUDA_VERSION = torch._C._cuda_getCompiledVersion()\r\n    for d in range(device_count()):\r\n        capability = get_device_capability(d)\r\n        major = capability[0]\r\n        name = get_device_name(d)\r\n        if CUDA_VERSION < 8000 and major >= 6:\r\n            warnings.warn(incorrect_binary_warn % (d, name, 8000, CUDA_VERSION))\r\n        elif CUDA_VERSION < 9000 and major >= 7:\r\n            warnings.warn(incorrect_binary_warn % (d, name, 9000, CUDA_VERSION))\r\n        elif capability == (3, 0) or capability == (5, 0) or major < 3:\r\n            warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\r\n```"}