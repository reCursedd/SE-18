{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162809340", "pull_request_review_id": 90329355, "id": 162809340, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MjgwOTM0MA==", "diff_hunk": "@@ -12,4 +12,21 @@ namespace torch { namespace jit {\n // (aka backward) of inputs to the first stage w.r.t. the outputs of the first stage.\n void differentiate(std::shared_ptr<Graph>& graph);\n \n+using value_list = std::vector<Value*>;\n+struct LiftedReverse {", "path": "torch/csrc/jit/autodiff.h", "position": null, "original_position": 5, "commit_id": "a41b11143e2fad09918cf66d0a60216e9ab83f5a", "original_commit_id": "5914745b88ea6b541561f1889baf268a67443a17", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This is not `Gradient` - if you look at the pseudo code, you'll see that half of its fields come from `differentiate`, while the other half come from `lambdaLiftReverse`. This lets us encapsulate the AD code in a fairly independent way, that doesn't need to know anything about `GraphExecutor`s.", "created_at": "2018-01-21T11:51:59Z", "updated_at": "2018-11-23T15:38:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/4759#discussion_r162809340", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4759", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162809340"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4759#discussion_r162809340"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4759"}}, "body_html": "<p>This is not <code>Gradient</code> - if you look at the pseudo code, you'll see that half of its fields come from <code>differentiate</code>, while the other half come from <code>lambdaLiftReverse</code>. This lets us encapsulate the AD code in a fairly independent way, that doesn't need to know anything about <code>GraphExecutor</code>s.</p>", "body_text": "This is not Gradient - if you look at the pseudo code, you'll see that half of its fields come from differentiate, while the other half come from lambdaLiftReverse. This lets us encapsulate the AD code in a fairly independent way, that doesn't need to know anything about GraphExecutors.", "in_reply_to_id": 162801067}