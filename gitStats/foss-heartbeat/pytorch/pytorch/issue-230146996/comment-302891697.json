{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/302891697", "html_url": "https://github.com/pytorch/pytorch/issues/1601#issuecomment-302891697", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1601", "id": 302891697, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjg5MTY5Nw==", "user": {"login": "ajbrock", "id": 7751273, "node_id": "MDQ6VXNlcjc3NTEyNzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7751273?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ajbrock", "html_url": "https://github.com/ajbrock", "followers_url": "https://api.github.com/users/ajbrock/followers", "following_url": "https://api.github.com/users/ajbrock/following{/other_user}", "gists_url": "https://api.github.com/users/ajbrock/gists{/gist_id}", "starred_url": "https://api.github.com/users/ajbrock/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ajbrock/subscriptions", "organizations_url": "https://api.github.com/users/ajbrock/orgs", "repos_url": "https://api.github.com/users/ajbrock/repos", "events_url": "https://api.github.com/users/ajbrock/events{/privacy}", "received_events_url": "https://api.github.com/users/ajbrock/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-20T18:51:39Z", "updated_at": "2017-05-20T20:18:10Z", "author_association": "NONE", "body_html": "<p>It looks like that version includes the complicated data-dependent initialization, which I've also had trouble using even with the original Theano/Lasagne implementation provided by salimans. You can just use regular WeightNorm like this (verbose/naive version with the original formulation):</p>\n<pre><code># 2D-conv with weightnorm\nclass Conv2d_WN(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False):\n        super(Conv2d_WN,self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias)\n        self.g = Parameter(torch.ones(out_channels,1,1,1,1))\n        \n    def forward(self,x):\n        wnorm = torch.sqrt((self.weight**2).sum(1).sum(2).sum(3)).expand_as(self.weight)\n        return F.conv2d(x, self.g.expand_as(self.weight)*self.weight/wnorm, bias=self.bias, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=self.groups) \n</code></pre>\n<p>There's a couple other variants lying around that in my experience work just as well and are even simpler.<br>\nEDIT: fixed a typo</p>", "body_text": "It looks like that version includes the complicated data-dependent initialization, which I've also had trouble using even with the original Theano/Lasagne implementation provided by salimans. You can just use regular WeightNorm like this (verbose/naive version with the original formulation):\n# 2D-conv with weightnorm\nclass Conv2d_WN(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False):\n        super(Conv2d_WN,self).__init__(in_channels, out_channels, kernel_size, stride,\n                 padding, dilation, groups, bias)\n        self.g = Parameter(torch.ones(out_channels,1,1,1,1))\n        \n    def forward(self,x):\n        wnorm = torch.sqrt((self.weight**2).sum(1).sum(2).sum(3)).expand_as(self.weight)\n        return F.conv2d(x, self.g.expand_as(self.weight)*self.weight/wnorm, bias=self.bias, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=self.groups) \n\nThere's a couple other variants lying around that in my experience work just as well and are even simpler.\nEDIT: fixed a typo", "body": "It looks like that version includes the complicated data-dependent initialization, which I've also had trouble using even with the original Theano/Lasagne implementation provided by salimans. You can just use regular WeightNorm like this (verbose/naive version with the original formulation):\r\n\r\n```\r\n# 2D-conv with weightnorm\r\nclass Conv2d_WN(nn.Conv2d):\r\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False):\r\n        super(Conv2d_WN,self).__init__(in_channels, out_channels, kernel_size, stride,\r\n                 padding, dilation, groups, bias)\r\n        self.g = Parameter(torch.ones(out_channels,1,1,1,1))\r\n        \r\n    def forward(self,x):\r\n        wnorm = torch.sqrt((self.weight**2).sum(1).sum(2).sum(3)).expand_as(self.weight)\r\n        return F.conv2d(x, self.g.expand_as(self.weight)*self.weight/wnorm, bias=self.bias, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=self.groups) \r\n```\r\n\r\nThere's a couple other variants lying around that in my experience work just as well and are even simpler.\r\nEDIT: fixed a typo"}