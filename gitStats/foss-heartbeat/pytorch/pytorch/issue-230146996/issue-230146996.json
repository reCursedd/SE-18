{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1601", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1601/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1601/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1601/events", "html_url": "https://github.com/pytorch/pytorch/issues/1601", "id": 230146996, "node_id": "MDU6SXNzdWUyMzAxNDY5OTY=", "number": 1601, "title": "[Feature Request] Weight Normalization", "user": {"login": "Kaixhin", "id": 991891, "node_id": "MDQ6VXNlcjk5MTg5MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/991891?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Kaixhin", "html_url": "https://github.com/Kaixhin", "followers_url": "https://api.github.com/users/Kaixhin/followers", "following_url": "https://api.github.com/users/Kaixhin/following{/other_user}", "gists_url": "https://api.github.com/users/Kaixhin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Kaixhin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Kaixhin/subscriptions", "organizations_url": "https://api.github.com/users/Kaixhin/orgs", "repos_url": "https://api.github.com/users/Kaixhin/repos", "events_url": "https://api.github.com/users/Kaixhin/events{/privacy}", "received_events_url": "https://api.github.com/users/Kaixhin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-05-20T11:11:08Z", "updated_at": "2017-06-30T20:15:20Z", "closed_at": "2017-06-30T20:15:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a href=\"https://arxiv.org/abs/1607.06450\" rel=\"nofollow\">Layer normalization</a> seems to be pretty popular for RNNs nowadays, and it is worth having an implementation available. Several people seem to have already rolled their own (like <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7751273\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ajbrock\">@ajbrock</a>), so this issue also aims to prevent several people working on this at the same time, as they can announce their intentions here.</p>\n<p>The same applies for <a href=\"https://arxiv.org/abs/1602.07868\" rel=\"nofollow\">weight normalization</a>. This issue can be used to discuss both, but implementations could be done in separate PRs.</p>", "body_text": "Layer normalization seems to be pretty popular for RNNs nowadays, and it is worth having an implementation available. Several people seem to have already rolled their own (like @ajbrock), so this issue also aims to prevent several people working on this at the same time, as they can announce their intentions here.\nThe same applies for weight normalization. This issue can be used to discuss both, but implementations could be done in separate PRs.", "body": "[Layer normalization](https://arxiv.org/abs/1607.06450) seems to be pretty popular for RNNs nowadays, and it is worth having an implementation available. Several people seem to have already rolled their own (like @ajbrock), so this issue also aims to prevent several people working on this at the same time, as they can announce their intentions here.\r\n\r\nThe same applies for [weight normalization](https://arxiv.org/abs/1602.07868). This issue can be used to discuss both, but implementations could be done in separate PRs."}