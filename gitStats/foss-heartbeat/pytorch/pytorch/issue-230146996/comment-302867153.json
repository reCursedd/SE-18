{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/302867153", "html_url": "https://github.com/pytorch/pytorch/issues/1601#issuecomment-302867153", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1601", "id": 302867153, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjg2NzE1Mw==", "user": {"login": "ajbrock", "id": 7751273, "node_id": "MDQ6VXNlcjc3NTEyNzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7751273?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ajbrock", "html_url": "https://github.com/ajbrock", "followers_url": "https://api.github.com/users/ajbrock/followers", "following_url": "https://api.github.com/users/ajbrock/following{/other_user}", "gists_url": "https://api.github.com/users/ajbrock/gists{/gist_id}", "starred_url": "https://api.github.com/users/ajbrock/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ajbrock/subscriptions", "organizations_url": "https://api.github.com/users/ajbrock/orgs", "repos_url": "https://api.github.com/users/ajbrock/repos", "events_url": "https://api.github.com/users/ajbrock/events{/privacy}", "received_events_url": "https://api.github.com/users/ajbrock/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-20T11:21:53Z", "updated_at": "2017-05-20T11:21:53Z", "author_association": "NONE", "body_html": "<p>Good idea--does anyone have any ideas as to a faster way to do it then sending (0,1) transposed tensors through BatchNorm?</p>\n<p>Relatedly, I've naively hacked out several weightnorm variants and normprop, which might both be worth having, though weightnorm would probably have to be a decorator of some sort rather than a separate layer.</p>", "body_text": "Good idea--does anyone have any ideas as to a faster way to do it then sending (0,1) transposed tensors through BatchNorm?\nRelatedly, I've naively hacked out several weightnorm variants and normprop, which might both be worth having, though weightnorm would probably have to be a decorator of some sort rather than a separate layer.", "body": "Good idea--does anyone have any ideas as to a faster way to do it then sending (0,1) transposed tensors through BatchNorm?\r\n\r\nRelatedly, I've naively hacked out several weightnorm variants and normprop, which might both be worth having, though weightnorm would probably have to be a decorator of some sort rather than a separate layer."}