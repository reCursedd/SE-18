{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2641", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2641/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2641/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2641/events", "html_url": "https://github.com/pytorch/pytorch/issues/2641", "id": 255537011, "node_id": "MDU6SXNzdWUyNTU1MzcwMTE=", "number": 2641, "title": "How to auto convert Module constant Variable between cpu and cuda?", "user": {"login": "dianyancao", "id": 8494704, "node_id": "MDQ6VXNlcjg0OTQ3MDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/8494704?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dianyancao", "html_url": "https://github.com/dianyancao", "followers_url": "https://api.github.com/users/dianyancao/followers", "following_url": "https://api.github.com/users/dianyancao/following{/other_user}", "gists_url": "https://api.github.com/users/dianyancao/gists{/gist_id}", "starred_url": "https://api.github.com/users/dianyancao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dianyancao/subscriptions", "organizations_url": "https://api.github.com/users/dianyancao/orgs", "repos_url": "https://api.github.com/users/dianyancao/repos", "events_url": "https://api.github.com/users/dianyancao/events{/privacy}", "received_events_url": "https://api.github.com/users/dianyancao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-09-06T09:23:53Z", "updated_at": "2017-09-12T12:00:05Z", "closed_at": "2017-09-11T18:11:05Z", "author_association": "NONE", "body_html": "<pre><code>from __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable, Function\nfrom torch.nn import Parameter\nfrom torch.nn.modules.module import Module\n\n# Training settings\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                    help='input batch size for testing (default: 1000)')\nparser.add_argument('--epochs', type=int, default=1, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                    help='SGD momentum (default: 0.5)')\nparser.add_argument('--cuda', type=bool, default=True,\n                    help='Whether to use CUDA for training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                    help='how many batches to wait before logging training status')\nargs = parser.parse_args()\n\nimport numpy as np\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=False, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=False, **kwargs)\n\nclass ERelu(Module):\n\n    def __init__(self, size):\n        super(ERelu, self).__init__()\n        self.weight = Parameter(torch.zeros(size))\n        self.constant = torch.zeros(1)\n        self.constant = Variable(data=self.constant, requires_grad=False)\n\n    def forward(self, input):\n        if len(input.size()) == 2:\n            input_reshape = input\n            t_reshape = self.weight.view(1,-1)\n        elif len(input.size()) == 4:\n            input_reshape = input\n            t_reshape = self.weight.view(1,-1,1,1)\n        else:\n            raise NotImplementedError()\n        max = torch.max(input_reshape - t_reshape, self.constant)\n        return (max + t_reshape).view(input.size())\n    def cuda(self, device_id=None):\n        \"\"\"Moves all model parameters and buffers to the GPU.\n\n        Arguments:\n            device_id (int, optional): if specified, all parameters will be\n                copied to that device\n        \"\"\"\n        self.constant = self.constant.cuda()\n        return self._apply(lambda t: t.cuda(device_id))\n\n    def cpu(self):\n        \"\"\"Moves all model parameters and buffers to the CPU.\"\"\"\n        self.constant = self.constant.cpu()\n        return self._apply(lambda t: t.cpu())\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.erelu1 = ERelu(10)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.erelu2 = ERelu(20)\n        self.fc1 = nn.Linear(320, 50)\n        self.erelu3 = ERelu(50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = self.erelu1((F.max_pool2d(self.conv1(x), 2)))\n        x = self.erelu2((F.max_pool2d(self.conv2(x), 2)))\n        x = x.view(-1, 320)\n        x = self.erelu3((self.fc1(x)))\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nmodel = Net()\nif args.cuda:\n    model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n        #pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        pred = output.data.max(1)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nfor epoch in range(1, args.epochs + 1):\n    train(epoch)\n    test()\n</code></pre>\n<p>Two questions:<br>\nThe code run with cuda,I write an ERelu Module to replace Relu activation funtion<br>\nbut the ERelu.cuda method is not called when I call model.cuda(),how to get work with it?</p>\n<p>And I want to fixed all input the same when I run the code next time.<br>\nI fix np.random seed, torch cpu cuda random seed, and use shuffle=False for DataLoader,but the test result is different when I run the code next time with cuda,but normal on cpu,is there something to be set?</p>", "body_text": "from __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable, Function\nfrom torch.nn import Parameter\nfrom torch.nn.modules.module import Module\n\n# Training settings\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                    help='input batch size for testing (default: 1000)')\nparser.add_argument('--epochs', type=int, default=1, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                    help='SGD momentum (default: 0.5)')\nparser.add_argument('--cuda', type=bool, default=True,\n                    help='Whether to use CUDA for training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                    help='how many batches to wait before logging training status')\nargs = parser.parse_args()\n\nimport numpy as np\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=False, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=args.batch_size, shuffle=False, **kwargs)\n\nclass ERelu(Module):\n\n    def __init__(self, size):\n        super(ERelu, self).__init__()\n        self.weight = Parameter(torch.zeros(size))\n        self.constant = torch.zeros(1)\n        self.constant = Variable(data=self.constant, requires_grad=False)\n\n    def forward(self, input):\n        if len(input.size()) == 2:\n            input_reshape = input\n            t_reshape = self.weight.view(1,-1)\n        elif len(input.size()) == 4:\n            input_reshape = input\n            t_reshape = self.weight.view(1,-1,1,1)\n        else:\n            raise NotImplementedError()\n        max = torch.max(input_reshape - t_reshape, self.constant)\n        return (max + t_reshape).view(input.size())\n    def cuda(self, device_id=None):\n        \"\"\"Moves all model parameters and buffers to the GPU.\n\n        Arguments:\n            device_id (int, optional): if specified, all parameters will be\n                copied to that device\n        \"\"\"\n        self.constant = self.constant.cuda()\n        return self._apply(lambda t: t.cuda(device_id))\n\n    def cpu(self):\n        \"\"\"Moves all model parameters and buffers to the CPU.\"\"\"\n        self.constant = self.constant.cpu()\n        return self._apply(lambda t: t.cpu())\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.erelu1 = ERelu(10)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.erelu2 = ERelu(20)\n        self.fc1 = nn.Linear(320, 50)\n        self.erelu3 = ERelu(50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = self.erelu1((F.max_pool2d(self.conv1(x), 2)))\n        x = self.erelu2((F.max_pool2d(self.conv2(x), 2)))\n        x = x.view(-1, 320)\n        x = self.erelu3((self.fc1(x)))\n        x = self.fc2(x)\n        return F.log_softmax(x)\n\nmodel = Net()\nif args.cuda:\n    model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n        #pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n        pred = output.data.max(1)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nfor epoch in range(1, args.epochs + 1):\n    train(epoch)\n    test()\n\nTwo questions:\nThe code run with cuda,I write an ERelu Module to replace Relu activation funtion\nbut the ERelu.cuda method is not called when I call model.cuda(),how to get work with it?\nAnd I want to fixed all input the same when I run the code next time.\nI fix np.random seed, torch cpu cuda random seed, and use shuffle=False for DataLoader,but the test result is different when I run the code next time with cuda,but normal on cpu,is there something to be set?", "body": "```\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\nfrom torch.autograd import Variable, Function\r\nfrom torch.nn import Parameter\r\nfrom torch.nn.modules.module import Module\r\n\r\n# Training settings\r\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\r\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\r\n                    help='input batch size for training (default: 64)')\r\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\r\n                    help='input batch size for testing (default: 1000)')\r\nparser.add_argument('--epochs', type=int, default=1, metavar='N',\r\n                    help='number of epochs to train (default: 10)')\r\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\r\n                    help='learning rate (default: 0.01)')\r\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\r\n                    help='SGD momentum (default: 0.5)')\r\nparser.add_argument('--cuda', type=bool, default=True,\r\n                    help='Whether to use CUDA for training')\r\nparser.add_argument('--seed', type=int, default=1, metavar='S',\r\n                    help='random seed (default: 1)')\r\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\r\n                    help='how many batches to wait before logging training status')\r\nargs = parser.parse_args()\r\n\r\nimport numpy as np\r\nnp.random.seed(args.seed)\r\ntorch.manual_seed(args.seed)\r\nif args.cuda:\r\n    torch.cuda.manual_seed(args.seed)\r\n\r\n\r\nkwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=True, download=True,\r\n                   transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=args.batch_size, shuffle=False, **kwargs)\r\ntest_loader = torch.utils.data.DataLoader(\r\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n                       transforms.ToTensor(),\r\n                       transforms.Normalize((0.1307,), (0.3081,))\r\n                   ])),\r\n    batch_size=args.batch_size, shuffle=False, **kwargs)\r\n\r\nclass ERelu(Module):\r\n\r\n    def __init__(self, size):\r\n        super(ERelu, self).__init__()\r\n        self.weight = Parameter(torch.zeros(size))\r\n        self.constant = torch.zeros(1)\r\n        self.constant = Variable(data=self.constant, requires_grad=False)\r\n\r\n    def forward(self, input):\r\n        if len(input.size()) == 2:\r\n            input_reshape = input\r\n            t_reshape = self.weight.view(1,-1)\r\n        elif len(input.size()) == 4:\r\n            input_reshape = input\r\n            t_reshape = self.weight.view(1,-1,1,1)\r\n        else:\r\n            raise NotImplementedError()\r\n        max = torch.max(input_reshape - t_reshape, self.constant)\r\n        return (max + t_reshape).view(input.size())\r\n    def cuda(self, device_id=None):\r\n        \"\"\"Moves all model parameters and buffers to the GPU.\r\n\r\n        Arguments:\r\n            device_id (int, optional): if specified, all parameters will be\r\n                copied to that device\r\n        \"\"\"\r\n        self.constant = self.constant.cuda()\r\n        return self._apply(lambda t: t.cuda(device_id))\r\n\r\n    def cpu(self):\r\n        \"\"\"Moves all model parameters and buffers to the CPU.\"\"\"\r\n        self.constant = self.constant.cpu()\r\n        return self._apply(lambda t: t.cpu())\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n        self.erelu1 = ERelu(10)\r\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n        self.erelu2 = ERelu(20)\r\n        self.fc1 = nn.Linear(320, 50)\r\n        self.erelu3 = ERelu(50)\r\n        self.fc2 = nn.Linear(50, 10)\r\n\r\n    def forward(self, x):\r\n        x = self.erelu1((F.max_pool2d(self.conv1(x), 2)))\r\n        x = self.erelu2((F.max_pool2d(self.conv2(x), 2)))\r\n        x = x.view(-1, 320)\r\n        x = self.erelu3((self.fc1(x)))\r\n        x = self.fc2(x)\r\n        return F.log_softmax(x)\r\n\r\nmodel = Net()\r\nif args.cuda:\r\n    model.cuda()\r\n\r\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\r\n\r\ndef train(epoch):\r\n    model.train()\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        if args.cuda:\r\n            data, target = data.cuda(), target.cuda()\r\n        data, target = Variable(data), Variable(target)\r\n        optimizer.zero_grad()\r\n        output = model(data)\r\n        loss = F.nll_loss(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n        if batch_idx % args.log_interval == 0:\r\n            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'.format(\r\n                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n                100. * batch_idx / len(train_loader), loss.data[0]))\r\n\r\ndef test():\r\n    model.eval()\r\n    test_loss = 0\r\n    correct = 0\r\n    for data, target in test_loader:\r\n        if args.cuda:\r\n            data, target = data.cuda(), target.cuda()\r\n        data, target = Variable(data, volatile=True), Variable(target)\r\n        output = model(data)\r\n        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\r\n        #pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\r\n        pred = output.data.max(1)[1]\r\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\r\n\r\n    test_loss /= len(test_loader.dataset)\r\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\r\n        test_loss, correct, len(test_loader.dataset),\r\n        100. * correct / len(test_loader.dataset)))\r\n\r\n\r\nfor epoch in range(1, args.epochs + 1):\r\n    train(epoch)\r\n    test()\r\n```\r\nTwo questions:\r\nThe code run with cuda,I write an ERelu Module to replace Relu activation funtion\r\nbut the ERelu.cuda method is not called when I call model.cuda(),how to get work with it?\r\n\r\nAnd I want to fixed all input the same when I run the code next time.\r\nI fix np.random seed, torch cpu cuda random seed, and use shuffle=False for DataLoader,but the test result is different when I run the code next time with cuda,but normal on cpu,is there something to be set?"}