{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11756", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11756/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11756/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11756/events", "html_url": "https://github.com/pytorch/pytorch/issues/11756", "id": 360903991, "node_id": "MDU6SXNzdWUzNjA5MDM5OTE=", "number": 11756, "title": "RuntimeError: NCCL Error 1: unhandled cuda error", "user": {"login": "chenwc07", "id": 40051091, "node_id": "MDQ6VXNlcjQwMDUxMDkx", "avatar_url": "https://avatars2.githubusercontent.com/u/40051091?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenwc07", "html_url": "https://github.com/chenwc07", "followers_url": "https://api.github.com/users/chenwc07/followers", "following_url": "https://api.github.com/users/chenwc07/following{/other_user}", "gists_url": "https://api.github.com/users/chenwc07/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenwc07/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenwc07/subscriptions", "organizations_url": "https://api.github.com/users/chenwc07/orgs", "repos_url": "https://api.github.com/users/chenwc07/repos", "events_url": "https://api.github.com/users/chenwc07/events{/privacy}", "received_events_url": "https://api.github.com/users/chenwc07/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 22, "created_at": "2018-09-17T14:45:50Z", "updated_at": "2018-11-06T12:30:33Z", "closed_at": "2018-09-21T16:43:30Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Get <code>NCCL Error 1: unhandled cuda error</code> when using DataParallel<br>\nI wonder  what's wrong with it because it works when using only 1 GPU, and cuda9/cuda8 got the same problem</p>\n<h2>Code example</h2>\n<p>I ran:<br>\n<code>testdata = torch.rand(12,3,112,112)</code><br>\n<code>model = torch.nn.DataParallel(model, device_ids=[0,1,2,3]).cuda()</code><br>\n<code>out = model(testdata)</code></p>\n<p>then i got:<br>\nRuntimeError                              Traceback (most recent call last)<br>\n in ()<br>\n----&gt; 1 out = model(testdata)</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py in <strong>call</strong>(self, *input, **kwargs)<br>\n475             result = self._slow_forward(*input, **kwargs)<br>\n476         else:<br>\n--&gt; 477             result = self.forward(*input, **kwargs)<br>\n478         for hook in self._forward_hooks.values():<br>\n479             hook_result = hook(self, input, result)</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)<br>\n120         if len(self.device_ids) == 1:<br>\n121             return self.module(*inputs[0], **kwargs[0])<br>\n--&gt; 122         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])<br>\n123         outputs = self.parallel_apply(replicas, inputs, kwargs)<br>\n124         return self.gather(outputs, self.output_device)</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in replicate(self, module, device_ids)<br>\n125<br>\n126     def replicate(self, module, device_ids):<br>\n--&gt; 127         return replicate(module, device_ids)<br>\n128<br>\n129     def scatter(self, inputs, kwargs, device_ids):</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/replicate.py in replicate(network, devices, detach)<br>\n10     params = list(network.parameters())<br>\n11     param_indices = {param: idx for idx, param in enumerate(params)}<br>\n---&gt; 12     param_copies = Broadcast.apply(devices, *params)<br>\n13     if len(params) &gt; 0:<br>\n14         param_copies = [param_copies[i:i + len(params)]</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/_functions.py in forward(ctx, target_gpus, *inputs)<br>\n17         ctx.num_inputs = len(inputs)<br>\n18         ctx.input_device = inputs[0].get_device()<br>\n---&gt; 19         outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)<br>\n20         non_differentiables = []<br>\n21         for idx, input_requires_grad in enumerate(ctx.needs_input_grad[1:]):</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/cuda/comm.py in broadcast_coalesced(tensors, devices, buffer_size)<br>\n38         corresponding to indices from <code>devices</code>.<br>\n39     \"\"\"<br>\n---&gt; 40     return torch._C._broadcast_coalesced(tensors, devices, buffer_size)<br>\n41<br>\n42</p>\n<p><strong>RuntimeError: NCCL Error 1: unhandled cuda error</strong></p>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch</li>\n<li>How you installed PyTorch : conda</li>\n<li>OS: ubuntu-server-16.04</li>\n<li>PyTorch version: 0.4.1</li>\n<li>Python version: 3.5</li>\n<li>CUDA/cuDNN version: cuda8, cudnn6 and cuda9,cudnn7 got the same problem</li>\n<li>GPU models and configuration: Nvidia Titan Xp * 4</li>\n<li>GCC version (if compiling from source): 5.4</li>\n<li>CMake version:</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "Issue description\nGet NCCL Error 1: unhandled cuda error when using DataParallel\nI wonder  what's wrong with it because it works when using only 1 GPU, and cuda9/cuda8 got the same problem\nCode example\nI ran:\ntestdata = torch.rand(12,3,112,112)\nmodel = torch.nn.DataParallel(model, device_ids=[0,1,2,3]).cuda()\nout = model(testdata)\nthen i got:\nRuntimeError                              Traceback (most recent call last)\n in ()\n----> 1 out = model(testdata)\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)\n475             result = self._slow_forward(*input, **kwargs)\n476         else:\n--> 477             result = self.forward(*input, **kwargs)\n478         for hook in self._forward_hooks.values():\n479             hook_result = hook(self, input, result)\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)\n120         if len(self.device_ids) == 1:\n121             return self.module(*inputs[0], **kwargs[0])\n--> 122         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n123         outputs = self.parallel_apply(replicas, inputs, kwargs)\n124         return self.gather(outputs, self.output_device)\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in replicate(self, module, device_ids)\n125\n126     def replicate(self, module, device_ids):\n--> 127         return replicate(module, device_ids)\n128\n129     def scatter(self, inputs, kwargs, device_ids):\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/replicate.py in replicate(network, devices, detach)\n10     params = list(network.parameters())\n11     param_indices = {param: idx for idx, param in enumerate(params)}\n---> 12     param_copies = Broadcast.apply(devices, *params)\n13     if len(params) > 0:\n14         param_copies = [param_copies[i:i + len(params)]\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/_functions.py in forward(ctx, target_gpus, *inputs)\n17         ctx.num_inputs = len(inputs)\n18         ctx.input_device = inputs[0].get_device()\n---> 19         outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)\n20         non_differentiables = []\n21         for idx, input_requires_grad in enumerate(ctx.needs_input_grad[1:]):\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/cuda/comm.py in broadcast_coalesced(tensors, devices, buffer_size)\n38         corresponding to indices from devices.\n39     \"\"\"\n---> 40     return torch._C._broadcast_coalesced(tensors, devices, buffer_size)\n41\n42\nRuntimeError: NCCL Error 1: unhandled cuda error\nSystem Info\n\nPyTorch\nHow you installed PyTorch : conda\nOS: ubuntu-server-16.04\nPyTorch version: 0.4.1\nPython version: 3.5\nCUDA/cuDNN version: cuda8, cudnn6 and cuda9,cudnn7 got the same problem\nGPU models and configuration: Nvidia Titan Xp * 4\nGCC version (if compiling from source): 5.4\nCMake version:\nVersions of any other relevant libraries:", "body": "## Issue description\r\nGet `NCCL Error 1: unhandled cuda error` when using DataParallel\r\nI wonder  what's wrong with it because it works when using only 1 GPU, and cuda9/cuda8 got the same problem\r\n\r\n## Code example\r\nI ran:\r\n`testdata = torch.rand(12,3,112,112)`\r\n`model = torch.nn.DataParallel(model, device_ids=[0,1,2,3]).cuda()`\r\n`out = model(testdata)`\r\n\r\nthen i got:\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-8-f69e71bd3d26> in <module>()\r\n----> 1 out = model(testdata)\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    475             result = self._slow_forward(*input, **kwargs)\r\n    476         else:\r\n--> 477             result = self.forward(*input, **kwargs)\r\n    478         for hook in self._forward_hooks.values():\r\n    479             hook_result = hook(self, input, result)\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)\r\n    120         if len(self.device_ids) == 1:\r\n    121             return self.module(*inputs[0], **kwargs[0])\r\n--> 122         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\r\n    123         outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n    124         return self.gather(outputs, self.output_device)\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in replicate(self, module, device_ids)\r\n    125 \r\n    126     def replicate(self, module, device_ids):\r\n--> 127         return replicate(module, device_ids)\r\n    128 \r\n    129     def scatter(self, inputs, kwargs, device_ids):\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/replicate.py in replicate(network, devices, detach)\r\n     10     params = list(network.parameters())\r\n     11     param_indices = {param: idx for idx, param in enumerate(params)}\r\n---> 12     param_copies = Broadcast.apply(devices, *params)\r\n     13     if len(params) > 0:\r\n     14         param_copies = [param_copies[i:i + len(params)]\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/_functions.py in forward(ctx, target_gpus, *inputs)\r\n     17         ctx.num_inputs = len(inputs)\r\n     18         ctx.input_device = inputs[0].get_device()\r\n---> 19         outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)\r\n     20         non_differentiables = []\r\n     21         for idx, input_requires_grad in enumerate(ctx.needs_input_grad[1:]):\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/cuda/comm.py in broadcast_coalesced(tensors, devices, buffer_size)\r\n     38         corresponding to indices from ``devices``.\r\n     39     \"\"\"\r\n---> 40     return torch._C._broadcast_coalesced(tensors, devices, buffer_size)\r\n     41 \r\n     42 \r\n\r\n **RuntimeError: NCCL Error 1: unhandled cuda error**\r\n\r\n## System Info\r\n\r\n- PyTorch\r\n- How you installed PyTorch : conda\r\n- OS: ubuntu-server-16.04\r\n- PyTorch version: 0.4.1\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: cuda8, cudnn6 and cuda9,cudnn7 got the same problem\r\n- GPU models and configuration: Nvidia Titan Xp * 4\r\n- GCC version (if compiling from source): 5.4\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n"}