{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422890312", "html_url": "https://github.com/pytorch/pytorch/issues/11756#issuecomment-422890312", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11756", "id": 422890312, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjg5MDMxMg==", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-19T17:29:45Z", "updated_at": "2018-09-19T17:29:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=40051091\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/chenwc07\">@chenwc07</a> I thought the typical usage of DataParallel was to either call .cuda() on your model before wrapping it in DataParallel, or not to call .cuda() on your model at all (because DataParallel will do it for you under the hood).</p>\n<p>Can you try one of the following:</p>\n<pre><code>testdata = torch.rand(12,3,112,112)\nmodel = torch.nn.DataParallel(model.cuda(), device_ids=[0,1,2,3])\nout = model(testdata)\n</code></pre>\n<p>or just</p>\n<pre><code>testdata = torch.rand(12,3,112,112)\nmodel = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\nout = model(testdata)\n</code></pre>\n<p>This is probably irrelevant (especially since <code>torch.nn.DataParallel(model, device_ids=[0,1,2,3]).cuda()</code> seems to work on P100s) but it's easy to test.</p>", "body_text": "@chenwc07 I thought the typical usage of DataParallel was to either call .cuda() on your model before wrapping it in DataParallel, or not to call .cuda() on your model at all (because DataParallel will do it for you under the hood).\nCan you try one of the following:\ntestdata = torch.rand(12,3,112,112)\nmodel = torch.nn.DataParallel(model.cuda(), device_ids=[0,1,2,3])\nout = model(testdata)\n\nor just\ntestdata = torch.rand(12,3,112,112)\nmodel = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\nout = model(testdata)\n\nThis is probably irrelevant (especially since torch.nn.DataParallel(model, device_ids=[0,1,2,3]).cuda() seems to work on P100s) but it's easy to test.", "body": "@chenwc07 I thought the typical usage of DataParallel was to either call .cuda() on your model before wrapping it in DataParallel, or not to call .cuda() on your model at all (because DataParallel will do it for you under the hood).\r\n\r\nCan you try one of the following:\r\n```\r\ntestdata = torch.rand(12,3,112,112)\r\nmodel = torch.nn.DataParallel(model.cuda(), device_ids=[0,1,2,3])\r\nout = model(testdata)\r\n```\r\nor just \r\n```\r\ntestdata = torch.rand(12,3,112,112)\r\nmodel = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\r\nout = model(testdata)\r\n```\r\n\r\nThis is probably irrelevant (especially since `torch.nn.DataParallel(model, device_ids=[0,1,2,3]).cuda()` seems to work on P100s) but it's easy to test."}