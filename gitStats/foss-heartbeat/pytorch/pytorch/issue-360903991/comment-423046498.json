{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423046498", "html_url": "https://github.com/pytorch/pytorch/issues/11756#issuecomment-423046498", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11756", "id": 423046498, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzA0NjQ5OA==", "user": {"login": "chenwc07", "id": 40051091, "node_id": "MDQ6VXNlcjQwMDUxMDkx", "avatar_url": "https://avatars2.githubusercontent.com/u/40051091?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenwc07", "html_url": "https://github.com/chenwc07", "followers_url": "https://api.github.com/users/chenwc07/followers", "following_url": "https://api.github.com/users/chenwc07/following{/other_user}", "gists_url": "https://api.github.com/users/chenwc07/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenwc07/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenwc07/subscriptions", "organizations_url": "https://api.github.com/users/chenwc07/orgs", "repos_url": "https://api.github.com/users/chenwc07/repos", "events_url": "https://api.github.com/users/chenwc07/events{/privacy}", "received_events_url": "https://api.github.com/users/chenwc07/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T05:40:06Z", "updated_at": "2018-09-20T05:40:06Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7799218\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mcarilli\">@mcarilli</a>  Thank you for your reply.<br>\nThe first code outputs the same error:</p>\n<p>RuntimeError: NCCL Error 1: unhandled cuda error</p>\n<p>and the second code comes to:</p>\n<p>TypeError                                 Traceback (most recent call last)<br>\n in ()<br>\n2     adjust_learning_rate(optimizer, epoch, [15, 30, 45], 0.001, 5e-4)<br>\n3     # train for one epoch<br>\n----&gt; 4     train(train_loader, model, criterion, optimizer, epoch)<br>\n5     if epoch%2 == 0:<br>\n6         validate(val_loader,model=model,criterion=criterion)</p>\n<p>/home/developers/chenweicong/projects/ECO/ECO-pytorch/train_and_val.py in train(train_loader, model, criterion, optimizer, epoch)<br>\n39<br>\n40         # compute output, output size: [batch_size, num_class]<br>\n---&gt; 41         output = model(input_var)<br>\n42<br>\n43         loss = criterion(output, target_var)</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py in <strong>call</strong>(self, *input, **kwargs)<br>\n475             result = self._slow_forward(*input, **kwargs)<br>\n476         else:<br>\n--&gt; 477             result = self.forward(*input, **kwargs)<br>\n478         for hook in self._forward_hooks.values():<br>\n479             hook_result = hook(self, input, result)</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)<br>\n120         if len(self.device_ids) == 1:<br>\n121             return self.module(*inputs[0], **kwargs[0])<br>\n--&gt; 122         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])<br>\n123         outputs = self.parallel_apply(replicas, inputs, kwargs)<br>\n124         return self.gather(outputs, self.output_device)</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in replicate(self, module, device_ids)<br>\n125<br>\n126     def replicate(self, module, device_ids):<br>\n--&gt; 127         return replicate(module, device_ids)<br>\n128<br>\n129     def scatter(self, inputs, kwargs, device_ids):</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/replicate.py in replicate(network, devices, detach)<br>\n10     params = list(network.parameters())<br>\n11     param_indices = {param: idx for idx, param in enumerate(params)}<br>\n---&gt; 12     param_copies = Broadcast.apply(devices, *params)<br>\n13     if len(params) &gt; 0:<br>\n14         param_copies = [param_copies[i:i + len(params)]</p>\n<p>/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/_functions.py in forward(ctx, target_gpus, *inputs)<br>\n11     def forward(ctx, target_gpus, *inputs):<br>\n12         if not all(input.is_cuda for input in inputs):<br>\n---&gt; 13             raise TypeError('Broadcast function not implemented for CPU tensors')<br>\n14         ctx.target_gpus = target_gpus<br>\n15         if len(inputs) == 0:</p>\n<p>TypeError: Broadcast function not implemented for CPU tensors</p>\n<p>so I think I still have to call .cuda() before or after DataParallel.</p>", "body_text": "@mcarilli  Thank you for your reply.\nThe first code outputs the same error:\nRuntimeError: NCCL Error 1: unhandled cuda error\nand the second code comes to:\nTypeError                                 Traceback (most recent call last)\n in ()\n2     adjust_learning_rate(optimizer, epoch, [15, 30, 45], 0.001, 5e-4)\n3     # train for one epoch\n----> 4     train(train_loader, model, criterion, optimizer, epoch)\n5     if epoch%2 == 0:\n6         validate(val_loader,model=model,criterion=criterion)\n/home/developers/chenweicong/projects/ECO/ECO-pytorch/train_and_val.py in train(train_loader, model, criterion, optimizer, epoch)\n39\n40         # compute output, output size: [batch_size, num_class]\n---> 41         output = model(input_var)\n42\n43         loss = criterion(output, target_var)\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py in call(self, *input, **kwargs)\n475             result = self._slow_forward(*input, **kwargs)\n476         else:\n--> 477             result = self.forward(*input, **kwargs)\n478         for hook in self._forward_hooks.values():\n479             hook_result = hook(self, input, result)\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)\n120         if len(self.device_ids) == 1:\n121             return self.module(*inputs[0], **kwargs[0])\n--> 122         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n123         outputs = self.parallel_apply(replicas, inputs, kwargs)\n124         return self.gather(outputs, self.output_device)\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in replicate(self, module, device_ids)\n125\n126     def replicate(self, module, device_ids):\n--> 127         return replicate(module, device_ids)\n128\n129     def scatter(self, inputs, kwargs, device_ids):\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/replicate.py in replicate(network, devices, detach)\n10     params = list(network.parameters())\n11     param_indices = {param: idx for idx, param in enumerate(params)}\n---> 12     param_copies = Broadcast.apply(devices, *params)\n13     if len(params) > 0:\n14         param_copies = [param_copies[i:i + len(params)]\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/_functions.py in forward(ctx, target_gpus, *inputs)\n11     def forward(ctx, target_gpus, *inputs):\n12         if not all(input.is_cuda for input in inputs):\n---> 13             raise TypeError('Broadcast function not implemented for CPU tensors')\n14         ctx.target_gpus = target_gpus\n15         if len(inputs) == 0:\nTypeError: Broadcast function not implemented for CPU tensors\nso I think I still have to call .cuda() before or after DataParallel.", "body": "@mcarilli  Thank you for your reply.\r\nThe first code outputs the same error: \r\n\r\nRuntimeError: NCCL Error 1: unhandled cuda error\r\n\r\nand the second code comes to:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-14-b9a0b7bacb33> in <module>()\r\n      2     adjust_learning_rate(optimizer, epoch, [15, 30, 45], 0.001, 5e-4)\r\n      3     # train for one epoch\r\n----> 4     train(train_loader, model, criterion, optimizer, epoch)\r\n      5     if epoch%2 == 0:\r\n      6         validate(val_loader,model=model,criterion=criterion)\r\n\r\n/home/developers/chenweicong/projects/ECO/ECO-pytorch/train_and_val.py in train(train_loader, model, criterion, optimizer, epoch)\r\n     39 \r\n     40         # compute output, output size: [batch_size, num_class]\r\n---> 41         output = model(input_var)\r\n     42 \r\n     43         loss = criterion(output, target_var)\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    475             result = self._slow_forward(*input, **kwargs)\r\n    476         else:\r\n--> 477             result = self.forward(*input, **kwargs)\r\n    478         for hook in self._forward_hooks.values():\r\n    479             hook_result = hook(self, input, result)\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in forward(self, *inputs, **kwargs)\r\n    120         if len(self.device_ids) == 1:\r\n    121             return self.module(*inputs[0], **kwargs[0])\r\n--> 122         replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\r\n    123         outputs = self.parallel_apply(replicas, inputs, kwargs)\r\n    124         return self.gather(outputs, self.output_device)\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py in replicate(self, module, device_ids)\r\n    125 \r\n    126     def replicate(self, module, device_ids):\r\n--> 127         return replicate(module, device_ids)\r\n    128 \r\n    129     def scatter(self, inputs, kwargs, device_ids):\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/replicate.py in replicate(network, devices, detach)\r\n     10     params = list(network.parameters())\r\n     11     param_indices = {param: idx for idx, param in enumerate(params)}\r\n---> 12     param_copies = Broadcast.apply(devices, *params)\r\n     13     if len(params) > 0:\r\n     14         param_copies = [param_copies[i:i + len(params)]\r\n\r\n/home/zhangd/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/parallel/_functions.py in forward(ctx, target_gpus, *inputs)\r\n     11     def forward(ctx, target_gpus, *inputs):\r\n     12         if not all(input.is_cuda for input in inputs):\r\n---> 13             raise TypeError('Broadcast function not implemented for CPU tensors')\r\n     14         ctx.target_gpus = target_gpus\r\n     15         if len(inputs) == 0:\r\n\r\nTypeError: Broadcast function not implemented for CPU tensors\r\n\r\nso I think I still have to call .cuda() before or after DataParallel.\r\n"}