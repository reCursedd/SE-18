{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1384", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1384/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1384/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1384/events", "html_url": "https://github.com/pytorch/pytorch/issues/1384", "id": 224886304, "node_id": "MDU6SXNzdWUyMjQ4ODYzMDQ=", "number": 1384, "title": "Perform autograd directly on Tensor, not Variable", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-04-27T19:18:53Z", "updated_at": "2017-04-28T18:48:39Z", "closed_at": "2017-04-27T20:53:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>What does <code>Variable</code> do? Historically, it meant \"wrapper for a tensor that can hold a graph and grad\". But generally, a user only interacts with <code>Variable</code>s for which <code>requires_grad=False</code>, so it doesn't hold a graph or a grad. It's just a confusing wrapper to keep track of, with the additional confusion that you can't mix <code>Tensor</code> and <code>Variable</code> in torch ops (even though they're the same thing).</p>\n<p>I think <code>Tensor</code> can directly have a <code>requires_grad</code> method and can optionally hold pointers to the grad and the graph. The advantages to the user are that they don't need to understand what a <code>Variable</code> is and they don't need to worry about calling <code>Variable</code> and <code>.data</code> at all the right places.</p>\n<p>We can maybe do this while maintaining backwards compatibility with Variables (Variables can just be a thin wrapper around tensors that do nothing; and Tensor.data can just point at self).</p>", "body_text": "What does Variable do? Historically, it meant \"wrapper for a tensor that can hold a graph and grad\". But generally, a user only interacts with Variables for which requires_grad=False, so it doesn't hold a graph or a grad. It's just a confusing wrapper to keep track of, with the additional confusion that you can't mix Tensor and Variable in torch ops (even though they're the same thing).\nI think Tensor can directly have a requires_grad method and can optionally hold pointers to the grad and the graph. The advantages to the user are that they don't need to understand what a Variable is and they don't need to worry about calling Variable and .data at all the right places.\nWe can maybe do this while maintaining backwards compatibility with Variables (Variables can just be a thin wrapper around tensors that do nothing; and Tensor.data can just point at self).", "body": "What does `Variable` do? Historically, it meant \"wrapper for a tensor that can hold a graph and grad\". But generally, a user only interacts with `Variable`s for which `requires_grad=False`, so it doesn't hold a graph or a grad. It's just a confusing wrapper to keep track of, with the additional confusion that you can't mix `Tensor` and `Variable` in torch ops (even though they're the same thing).\r\n\r\nI think `Tensor` can directly have a `requires_grad` method and can optionally hold pointers to the grad and the graph. The advantages to the user are that they don't need to understand what a `Variable` is and they don't need to worry about calling `Variable` and `.data` at all the right places.\r\n\r\nWe can maybe do this while maintaining backwards compatibility with Variables (Variables can just be a thin wrapper around tensors that do nothing; and Tensor.data can just point at self)."}