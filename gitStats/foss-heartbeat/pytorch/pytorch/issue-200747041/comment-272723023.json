{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/272723023", "html_url": "https://github.com/pytorch/pytorch/pull/451#issuecomment-272723023", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/451", "id": 272723023, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MjcyMzAyMw==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-15T20:54:05Z", "updated_at": "2017-01-15T20:54:05Z", "author_association": "MEMBER", "body_html": "<p>The copying semantics are intentional and designed for checkpointing. It means you can save and load checkpoints that are on a different type (e.g. CPU on a GPU model). It's less likely to break the semantics of the model (such as parameter sharing). If the model is currently in shared memory, it will stay in shared memory.</p>\n<p>As you point out, it means that <code>state_dict()</code>/<code>load_state_dict()</code> are not a good way of sharing models across processes. There's a Python saying \"there's only one way to do it.\" We should have one, obvious way of sharing models and one obvious way of checkpointing them. <code>state_dict()</code> is designed for checkpointing.</p>", "body_text": "The copying semantics are intentional and designed for checkpointing. It means you can save and load checkpoints that are on a different type (e.g. CPU on a GPU model). It's less likely to break the semantics of the model (such as parameter sharing). If the model is currently in shared memory, it will stay in shared memory.\nAs you point out, it means that state_dict()/load_state_dict() are not a good way of sharing models across processes. There's a Python saying \"there's only one way to do it.\" We should have one, obvious way of sharing models and one obvious way of checkpointing them. state_dict() is designed for checkpointing.", "body": "The copying semantics are intentional and designed for checkpointing. It means you can save and load checkpoints that are on a different type (e.g. CPU on a GPU model). It's less likely to break the semantics of the model (such as parameter sharing). If the model is currently in shared memory, it will stay in shared memory.\r\n\r\nAs you point out, it means that `state_dict()`/`load_state_dict()` are not a good way of sharing models across processes. There's a Python saying \"there's only one way to do it.\" We should have one, obvious way of sharing models and one obvious way of checkpointing them. `state_dict()` is designed for checkpointing."}