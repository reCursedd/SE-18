{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/413935344", "html_url": "https://github.com/pytorch/pytorch/issues/10223#issuecomment-413935344", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10223", "id": 413935344, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzkzNTM0NA==", "user": {"login": "jeisner", "id": 2395181, "node_id": "MDQ6VXNlcjIzOTUxODE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2395181?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeisner", "html_url": "https://github.com/jeisner", "followers_url": "https://api.github.com/users/jeisner/followers", "following_url": "https://api.github.com/users/jeisner/following{/other_user}", "gists_url": "https://api.github.com/users/jeisner/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeisner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeisner/subscriptions", "organizations_url": "https://api.github.com/users/jeisner/orgs", "repos_url": "https://api.github.com/users/jeisner/repos", "events_url": "https://api.github.com/users/jeisner/events{/privacy}", "received_events_url": "https://api.github.com/users/jeisner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-17T17:29:25Z", "updated_at": "2018-08-17T17:59:48Z", "author_association": "NONE", "body_html": "<p>Yes!  Another use case is to compute Hessian-vector products, which are useful in nested optimization such as <a href=\"https://arxiv.org/pdf/1703.05667.pdf\" rel=\"nofollow\">SPEN</a>, as well as in second-order optimizers such as stochastic meta-descent.</p>\n<p><a href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.6143\" rel=\"nofollow\">Pearlmutter (1994)</a> gives the best algorithm.  You want the Hessian of f(<strong>x</strong>) times the vector <strong>v</strong>.  First compute \u2207f(<strong>x</strong>+\u03b7<strong>v</strong>) (e.g., by backprop) with respect to input <strong>x</strong> at \u03b7=0.  Then apply forward-mode AD to <em>that</em> computation to determine the sensitivity of that gradient vector to \u03b7, which represents the size of the step in direction <strong>v</strong>.</p>\n<p>This is computing the partials of an output <em>vector</em> \u2207f(<strong>x</strong>+\u03b7<strong>v</strong>) with respect to an input <em>scalar</em> \u03b7-- exactly the situation where forward-mode AD is O(n) times faster than reverse-mode.  Reverse mode computes a whole vector at each node of the computation graph, whereas forward mode computes only a scalar at each node.</p>\n<p>(Admittedly, there is a simple <a href=\"https://justindomke.wordpress.com/2009/01/17/hessian-vector-products/\" rel=\"nofollow\">alternative approach</a> that doesn't require forward-mode AD, but that approach uses the finite-difference method, so it will suffer a little numerical error without being any faster, and it requires you to pick a magic stepsize.  So it would be neat if PyTorch could do it right.)</p>", "body_text": "Yes!  Another use case is to compute Hessian-vector products, which are useful in nested optimization such as SPEN, as well as in second-order optimizers such as stochastic meta-descent.\nPearlmutter (1994) gives the best algorithm.  You want the Hessian of f(x) times the vector v.  First compute \u2207f(x+\u03b7v) (e.g., by backprop) with respect to input x at \u03b7=0.  Then apply forward-mode AD to that computation to determine the sensitivity of that gradient vector to \u03b7, which represents the size of the step in direction v.\nThis is computing the partials of an output vector \u2207f(x+\u03b7v) with respect to an input scalar \u03b7-- exactly the situation where forward-mode AD is O(n) times faster than reverse-mode.  Reverse mode computes a whole vector at each node of the computation graph, whereas forward mode computes only a scalar at each node.\n(Admittedly, there is a simple alternative approach that doesn't require forward-mode AD, but that approach uses the finite-difference method, so it will suffer a little numerical error without being any faster, and it requires you to pick a magic stepsize.  So it would be neat if PyTorch could do it right.)", "body": "Yes!  Another use case is to compute Hessian-vector products, which are useful in nested optimization such as [SPEN](https://arxiv.org/pdf/1703.05667.pdf), as well as in second-order optimizers such as stochastic meta-descent.  \r\n\r\n[Pearlmutter (1994)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.29.6143) gives the best algorithm.  You want the Hessian of f(**x**) times the vector **v**.  First compute \u2207f(**x**+\u03b7**v**) (e.g., by backprop) with respect to input **x** at \u03b7=0.  Then apply forward-mode AD to *that* computation to determine the sensitivity of that gradient vector to \u03b7, which represents the size of the step in direction **v**.  \r\n\r\nThis is computing the partials of an output *vector* \u2207f(**x**+\u03b7**v**) with respect to an input *scalar* \u03b7-- exactly the situation where forward-mode AD is O(n) times faster than reverse-mode.  Reverse mode computes a whole vector at each node of the computation graph, whereas forward mode computes only a scalar at each node.\r\n\r\n(Admittedly, there is a simple [alternative approach](https://justindomke.wordpress.com/2009/01/17/hessian-vector-products/) that doesn't require forward-mode AD, but that approach uses the finite-difference method, so it will suffer a little numerical error without being any faster, and it requires you to pick a magic stepsize.  So it would be neat if PyTorch could do it right.)"}