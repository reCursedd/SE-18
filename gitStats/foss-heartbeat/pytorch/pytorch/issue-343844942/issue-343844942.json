{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9741", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9741/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9741/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9741/events", "html_url": "https://github.com/pytorch/pytorch/pull/9741", "id": 343844942, "node_id": "MDExOlB1bGxSZXF1ZXN0MjAzMzgxNTIx", "number": 9741, "title": "[JIT] fix add/sub autodiff", "user": {"login": "ChunliF", "id": 36351432, "node_id": "MDQ6VXNlcjM2MzUxNDMy", "avatar_url": "https://avatars0.githubusercontent.com/u/36351432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ChunliF", "html_url": "https://github.com/ChunliF", "followers_url": "https://api.github.com/users/ChunliF/followers", "following_url": "https://api.github.com/users/ChunliF/following{/other_user}", "gists_url": "https://api.github.com/users/ChunliF/gists{/gist_id}", "starred_url": "https://api.github.com/users/ChunliF/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ChunliF/subscriptions", "organizations_url": "https://api.github.com/users/ChunliF/orgs", "repos_url": "https://api.github.com/users/ChunliF/repos", "events_url": "https://api.github.com/users/ChunliF/events{/privacy}", "received_events_url": "https://api.github.com/users/ChunliF/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-07-24T00:50:08Z", "updated_at": "2018-07-24T01:38:57Z", "closed_at": "2018-07-24T01:38:45Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/9741", "html_url": "https://github.com/pytorch/pytorch/pull/9741", "diff_url": "https://github.com/pytorch/pytorch/pull/9741.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/9741.patch"}, "body_html": "<p>Add special cases for <code>aten::add</code> and <code>aten::sub</code> w/o <code>attr::alpha</code> in <code>bool isDifferentiable(Node * n)</code> to return False.</p>\n<p>eg:</p>\n<pre><code>@torch.jit.script  \ndef add(a, b, alpha): \n    c = torch.add(a, b, alpha)\n    d = torch.mul(a, c)\n    return d\n</code></pre>\n<p>This graph will be transformed to</p>\n<pre><code>graph() {\n  %5 : Dynamic, %6 : Dynamic, %7 : Dynamic = prim::Store()\n  %3 : Number = prim::TensorToNum(%7)\n  %4 : Dynamic = prim::GraphExecutor_0(%5, %6, %3)\n   = prim::Load(%4)\n  return ();\n}\nwith prim::GraphExecutor_0 = graph(%1 : Dynamic\n      %2 : Dynamic\n      %3 : Number) {\n  %c : Dynamic = aten::add(%1, %2, %3)\n  %d : Dynamic = aten::mul(%1, %c)\n  return (%d);\n}\n</code></pre>\n<p>Then <code>isDifferentiable(*prim::GraphExecutor_0)</code> would be True, but we need it to be False because <code>aten::add(self, other, scalar)</code>w/o <code>attr::alpha</code> cannot be symbolically differentiated.</p>", "body_text": "Add special cases for aten::add and aten::sub w/o attr::alpha in bool isDifferentiable(Node * n) to return False.\neg:\n@torch.jit.script  \ndef add(a, b, alpha): \n    c = torch.add(a, b, alpha)\n    d = torch.mul(a, c)\n    return d\n\nThis graph will be transformed to\ngraph() {\n  %5 : Dynamic, %6 : Dynamic, %7 : Dynamic = prim::Store()\n  %3 : Number = prim::TensorToNum(%7)\n  %4 : Dynamic = prim::GraphExecutor_0(%5, %6, %3)\n   = prim::Load(%4)\n  return ();\n}\nwith prim::GraphExecutor_0 = graph(%1 : Dynamic\n      %2 : Dynamic\n      %3 : Number) {\n  %c : Dynamic = aten::add(%1, %2, %3)\n  %d : Dynamic = aten::mul(%1, %c)\n  return (%d);\n}\n\nThen isDifferentiable(*prim::GraphExecutor_0) would be True, but we need it to be False because aten::add(self, other, scalar)w/o attr::alpha cannot be symbolically differentiated.", "body": "Add special cases for `aten::add` and `aten::sub` w/o `attr::alpha` in `bool isDifferentiable(Node * n)` to return False.\r\n\r\neg:\r\n```\r\n@torch.jit.script  \r\ndef add(a, b, alpha): \r\n    c = torch.add(a, b, alpha)\r\n    d = torch.mul(a, c)\r\n    return d\r\n```\r\nThis graph will be transformed to \r\n```\r\ngraph() {\r\n  %5 : Dynamic, %6 : Dynamic, %7 : Dynamic = prim::Store()\r\n  %3 : Number = prim::TensorToNum(%7)\r\n  %4 : Dynamic = prim::GraphExecutor_0(%5, %6, %3)\r\n   = prim::Load(%4)\r\n  return ();\r\n}\r\nwith prim::GraphExecutor_0 = graph(%1 : Dynamic\r\n      %2 : Dynamic\r\n      %3 : Number) {\r\n  %c : Dynamic = aten::add(%1, %2, %3)\r\n  %d : Dynamic = aten::mul(%1, %c)\r\n  return (%d);\r\n}\r\n```\r\n\r\nThen `isDifferentiable(*prim::GraphExecutor_0)` would be True, but we need it to be False because `aten::add(self, other, scalar)`w/o `attr::alpha` cannot be symbolically differentiated.  \r\n"}