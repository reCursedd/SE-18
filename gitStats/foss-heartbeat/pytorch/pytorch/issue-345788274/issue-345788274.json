{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10006", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10006/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10006/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10006/events", "html_url": "https://github.com/pytorch/pytorch/issues/10006", "id": 345788274, "node_id": "MDU6SXNzdWUzNDU3ODgyNzQ=", "number": 10006, "title": "RNN gradients in eval mode in pytorch 0.4", "user": {"login": "benkrause", "id": 22875509, "node_id": "MDQ6VXNlcjIyODc1NTA5", "avatar_url": "https://avatars1.githubusercontent.com/u/22875509?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benkrause", "html_url": "https://github.com/benkrause", "followers_url": "https://api.github.com/users/benkrause/followers", "following_url": "https://api.github.com/users/benkrause/following{/other_user}", "gists_url": "https://api.github.com/users/benkrause/gists{/gist_id}", "starred_url": "https://api.github.com/users/benkrause/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benkrause/subscriptions", "organizations_url": "https://api.github.com/users/benkrause/orgs", "repos_url": "https://api.github.com/users/benkrause/repos", "events_url": "https://api.github.com/users/benkrause/events{/privacy}", "received_events_url": "https://api.github.com/users/benkrause/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-07-30T14:42:09Z", "updated_at": "2018-10-31T18:07:00Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I need to be able to compute the gradients of an RNN with dropout temporarily turned off. In earlier versions if pytorch, it was possible to do this just by setting model.eval() and calling loss.backward(), but in pytorch 0.4 I get this error message:</p>\n<p>Traceback (most recent call last):<br>\nFile \"dynamiceval.py\", line 271, in <br>\ngradstat()<br>\nFile \"dynamiceval.py\", line 137, in gradstat<br>\nloss.backward()<br>\nFile \"/opt/conda/envs/pytorch04/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward<br>\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)<br>\nFile \"/opt/conda/envs/pytorch04/lib/python3.6/site-packages/torch/autograd/<strong>init</strong>.py\", line 90, in backward<br>\nallow_unreachable=True)  # allow_unreachable flag<br>\nRuntimeError: cudnn RNN backward can only be called in training mode</p>\n<p>Is there or could there be any work around that allows for dropout to be temporarily turned off, and still allow for gradients to be computed?</p>\n<p>Thanks!</p>", "body_text": "I need to be able to compute the gradients of an RNN with dropout temporarily turned off. In earlier versions if pytorch, it was possible to do this just by setting model.eval() and calling loss.backward(), but in pytorch 0.4 I get this error message:\nTraceback (most recent call last):\nFile \"dynamiceval.py\", line 271, in \ngradstat()\nFile \"dynamiceval.py\", line 137, in gradstat\nloss.backward()\nFile \"/opt/conda/envs/pytorch04/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)\nFile \"/opt/conda/envs/pytorch04/lib/python3.6/site-packages/torch/autograd/init.py\", line 90, in backward\nallow_unreachable=True)  # allow_unreachable flag\nRuntimeError: cudnn RNN backward can only be called in training mode\nIs there or could there be any work around that allows for dropout to be temporarily turned off, and still allow for gradients to be computed?\nThanks!", "body": "I need to be able to compute the gradients of an RNN with dropout temporarily turned off. In earlier versions if pytorch, it was possible to do this just by setting model.eval() and calling loss.backward(), but in pytorch 0.4 I get this error message:\r\n\r\nTraceback (most recent call last):\r\n  File \"dynamiceval.py\", line 271, in <module>\r\n    gradstat()\r\n  File \"dynamiceval.py\", line 137, in gradstat\r\n    loss.backward()\r\n  File \"/opt/conda/envs/pytorch04/lib/python3.6/site-packages/torch/tensor.py\", line 93, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/opt/conda/envs/pytorch04/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 90, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: cudnn RNN backward can only be called in training mode\r\n\r\nIs there or could there be any work around that allows for dropout to be temporarily turned off, and still allow for gradients to be computed?\r\n\r\nThanks!\r\n\r\n"}