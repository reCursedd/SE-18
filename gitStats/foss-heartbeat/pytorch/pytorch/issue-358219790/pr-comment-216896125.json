{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216896125", "pull_request_review_id": 154483316, "id": 216896125, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNjg5NjEyNQ==", "diff_hunk": "@@ -0,0 +1,160 @@\n+#include \"caffe2/operators/order_switch_ops.h\"\n+\n+#include <algorithm>\n+#include <functional>\n+#include <vector>\n+\n+#include \"caffe2/core/context_gpu.h\"\n+#include \"caffe2/core/cudnn_wrappers.h\"\n+#include \"caffe2/core/types.h\"\n+\n+namespace caffe2 {\n+\n+namespace {\n+\n+class CuDNNOrderSwithOpBase : public Operator<CUDAContext> {\n+ public:\n+  USE_OPERATOR_FUNCTIONS(CUDAContext);\n+\n+  CuDNNOrderSwithOpBase(const OperatorDef& operator_def, Workspace* ws)\n+      : Operator<CUDAContext>(operator_def, ws), cudnn_wrapper_(&context_) {\n+    CUDNN_ENFORCE(cudnnCreateTensorDescriptor(&X_desc_));\n+    CUDNN_ENFORCE(cudnnCreateTensorDescriptor(&Y_desc_));\n+  }\n+\n+  virtual ~CuDNNOrderSwithOpBase() {\n+    CUDNN_ENFORCE(cudnnDestroyTensorDescriptor(X_desc_));\n+    CUDNN_ENFORCE(cudnnDestroyTensorDescriptor(Y_desc_));\n+  }\n+\n+ protected:\n+  void SetTensorDescriptor(\n+      const cudnnDataType_t data_type,\n+      const StorageOrder order,\n+      const std::vector<int>& data_dims,\n+      cudnnTensorDescriptor_t data_desc) const {\n+    const int ndim = data_dims.size();\n+    const int N = data_dims[0];\n+    const int C = order == StorageOrder::NCHW ? data_dims[1] : data_dims.back();\n+    if (ndim == 3) {\n+      const int H = 1;\n+      const int W = order == StorageOrder::NCHW ? data_dims[2] : data_dims[1];\n+      CUDNN_ENFORCE(cudnnSetTensor4dDescriptor(\n+          data_desc, GetCudnnTensorFormat(order), data_type, N, C, H, W));\n+    } else if (ndim == 4) {\n+      const int H = order == StorageOrder::NCHW ? data_dims[2] : data_dims[1];\n+      const int W = order == StorageOrder::NCHW ? data_dims[3] : data_dims[2];\n+      CUDNN_ENFORCE(cudnnSetTensor4dDescriptor(\n+          data_desc, GetCudnnTensorFormat(order), data_type, N, C, H, W));\n+    } else {", "path": "caffe2/operators/order_switch_ops_cudnn.cc", "position": 49, "original_position": 49, "commit_id": "7a59f2a3aebd73dd113bb359e2dc8effdbf08bb3", "original_commit_id": "81e3a9c42d6ed392941660eeea98215fa9c0f289", "user": {"login": "BIT-silence", "id": 3357667, "node_id": "MDQ6VXNlcjMzNTc2Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/3357667?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BIT-silence", "html_url": "https://github.com/BIT-silence", "followers_url": "https://api.github.com/users/BIT-silence/followers", "following_url": "https://api.github.com/users/BIT-silence/following{/other_user}", "gists_url": "https://api.github.com/users/BIT-silence/gists{/gist_id}", "starred_url": "https://api.github.com/users/BIT-silence/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BIT-silence/subscriptions", "organizations_url": "https://api.github.com/users/BIT-silence/orgs", "repos_url": "https://api.github.com/users/BIT-silence/repos", "events_url": "https://api.github.com/users/BIT-silence/events{/privacy}", "received_events_url": "https://api.github.com/users/BIT-silence/received_events", "type": "User", "site_admin": false}, "body": "It is fine to combine the dims greater than 5 together. It may not be as fast as 5d, but there will be almost no such cases. So I think it's fine to keep it as that.", "created_at": "2018-09-12T04:50:59Z", "updated_at": "2018-11-23T15:51:08Z", "html_url": "https://github.com/pytorch/pytorch/pull/11404#discussion_r216896125", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11404", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/216896125"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11404#discussion_r216896125"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11404"}}, "body_html": "<p>It is fine to combine the dims greater than 5 together. It may not be as fast as 5d, but there will be almost no such cases. So I think it's fine to keep it as that.</p>", "body_text": "It is fine to combine the dims greater than 5 together. It may not be as fast as 5d, but there will be almost no such cases. So I think it's fine to keep it as that.", "in_reply_to_id": 216887662}