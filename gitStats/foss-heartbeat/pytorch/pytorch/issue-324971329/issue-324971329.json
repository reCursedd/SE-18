{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7732", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7732/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7732/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7732/events", "html_url": "https://github.com/pytorch/pytorch/issues/7732", "id": 324971329, "node_id": "MDU6SXNzdWUzMjQ5NzEzMjk=", "number": 7732, "title": "[feature request] Support output size parameter in Upsample ONNX operator", "user": {"login": "aachigorin", "id": 21040224, "node_id": "MDQ6VXNlcjIxMDQwMjI0", "avatar_url": "https://avatars1.githubusercontent.com/u/21040224?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aachigorin", "html_url": "https://github.com/aachigorin", "followers_url": "https://api.github.com/users/aachigorin/followers", "following_url": "https://api.github.com/users/aachigorin/following{/other_user}", "gists_url": "https://api.github.com/users/aachigorin/gists{/gist_id}", "starred_url": "https://api.github.com/users/aachigorin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aachigorin/subscriptions", "organizations_url": "https://api.github.com/users/aachigorin/orgs", "repos_url": "https://api.github.com/users/aachigorin/repos", "events_url": "https://api.github.com/users/aachigorin/events{/privacy}", "received_events_url": "https://api.github.com/users/aachigorin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx", "name": "onnx", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-21T16:05:22Z", "updated_at": "2018-10-15T11:19:45Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Right now <a href=\"https://github.com/onnx/onnx/blob/master/docs/Operators.md#Upsample\">Upsample</a> in onnx supports fixed (calculated for specific input size) scale factors. Which are <a href=\"https://github.com/pytorch/pytorch/blob/0a11018db690b02a52dd25b2470fdde794a8fac6/torch/onnx/symbolic.py#L498\">calculated</a> during graph tracing based on current input and output sizes.</p>\n<p>At the same time <a href=\"https://pytorch.org/docs/master/nn.html?highlight=pad#torch.nn.functional.upsample\" rel=\"nofollow\">F.upsample</a> allows to specify exact output size, which is very convenient if we want to evaluate models with backward connections (like <a href=\"https://arxiv.org/pdf/1612.03144.pdf\" rel=\"nofollow\">Feature Pyramid Network</a>) on inputs with varying sizes.</p>\n<p>Could we extend onnx Upsample operator to support desired output size (exactly like in F.upsample), taken as additional input to the operator?</p>\n<p>Simple example to check (this should export the graph with upsample taking its output size dynamically as h1.size()[-2:]):</p>\n<pre><code>class SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1)\n        \n    def forward(self, x):\n        h1 = self.conv1(x)\n        h2 = F.upsample(x, size=h1.size()[-2:], mode='bilinear', align_corners=False)        \n        h = torch.cat([h1, h2], dim=1)\n        return h\n\n\nmodel = SimpleNet()\nsave_path = '/Users/aachigorin/temp/test_bilinear_onnx_export.txt'\nx = Variable(torch.randn(1, 3, 10, 10)).cpu()\ntorch_out = torch.onnx._export(model, x, save_path, export_params=True, verbose=True)\nprint('finished onnx export')\n\nmodel = onnx.load(save_path)\nprint('check = {}'.format(onnx.checker.check_model(model)))\nprint(onnx.helper.printable_graph(model.graph))\n</code></pre>", "body_text": "Right now Upsample in onnx supports fixed (calculated for specific input size) scale factors. Which are calculated during graph tracing based on current input and output sizes.\nAt the same time F.upsample allows to specify exact output size, which is very convenient if we want to evaluate models with backward connections (like Feature Pyramid Network) on inputs with varying sizes.\nCould we extend onnx Upsample operator to support desired output size (exactly like in F.upsample), taken as additional input to the operator?\nSimple example to check (this should export the graph with upsample taking its output size dynamically as h1.size()[-2:]):\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1)\n        \n    def forward(self, x):\n        h1 = self.conv1(x)\n        h2 = F.upsample(x, size=h1.size()[-2:], mode='bilinear', align_corners=False)        \n        h = torch.cat([h1, h2], dim=1)\n        return h\n\n\nmodel = SimpleNet()\nsave_path = '/Users/aachigorin/temp/test_bilinear_onnx_export.txt'\nx = Variable(torch.randn(1, 3, 10, 10)).cpu()\ntorch_out = torch.onnx._export(model, x, save_path, export_params=True, verbose=True)\nprint('finished onnx export')\n\nmodel = onnx.load(save_path)\nprint('check = {}'.format(onnx.checker.check_model(model)))\nprint(onnx.helper.printable_graph(model.graph))", "body": "Right now [Upsample](https://github.com/onnx/onnx/blob/master/docs/Operators.md#Upsample) in onnx supports fixed (calculated for specific input size) scale factors. Which are [calculated](https://github.com/pytorch/pytorch/blob/0a11018db690b02a52dd25b2470fdde794a8fac6/torch/onnx/symbolic.py#L498) during graph tracing based on current input and output sizes.\r\n\r\nAt the same time [F.upsample](https://pytorch.org/docs/master/nn.html?highlight=pad#torch.nn.functional.upsample) allows to specify exact output size, which is very convenient if we want to evaluate models with backward connections (like [Feature Pyramid Network](https://arxiv.org/pdf/1612.03144.pdf)) on inputs with varying sizes.\r\n\r\nCould we extend onnx Upsample operator to support desired output size (exactly like in F.upsample), taken as additional input to the operator?\r\n\r\nSimple example to check (this should export the graph with upsample taking its output size dynamically as h1.size()[-2:]):\r\n```\r\nclass SimpleNet(nn.Module):\r\n    def __init__(self):\r\n        super(SimpleNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1)\r\n        self.conv2 = nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1)\r\n        \r\n    def forward(self, x):\r\n        h1 = self.conv1(x)\r\n        h2 = F.upsample(x, size=h1.size()[-2:], mode='bilinear', align_corners=False)        \r\n        h = torch.cat([h1, h2], dim=1)\r\n        return h\r\n\r\n\r\nmodel = SimpleNet()\r\nsave_path = '/Users/aachigorin/temp/test_bilinear_onnx_export.txt'\r\nx = Variable(torch.randn(1, 3, 10, 10)).cpu()\r\ntorch_out = torch.onnx._export(model, x, save_path, export_params=True, verbose=True)\r\nprint('finished onnx export')\r\n\r\nmodel = onnx.load(save_path)\r\nprint('check = {}'.format(onnx.checker.check_model(model)))\r\nprint(onnx.helper.printable_graph(model.graph))\r\n```"}