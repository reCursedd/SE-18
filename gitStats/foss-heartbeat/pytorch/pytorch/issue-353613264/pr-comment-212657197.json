{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212657197", "pull_request_review_id": 149339425, "id": 212657197, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjY1NzE5Nw==", "diff_hunk": "@@ -1047,67 +1140,409 @@ struct CPUFusionFunction : public CompiledFusionFunction {\n   void (*kernel)(uint32_t, void**) = nullptr;\n };\n \n-std::shared_ptr<CompiledFusionFunction> FusionCompiler::getOrCompile(AnnotatedGraph & agraph) {\n-  std::stringstream key;\n-  key << *agraph.graph << \"\\n\";\n-  key << \"device \" << agraph.device << \"\\n\";\n-  for(auto & i : agraph.input_desc)\n-    key << i << \"\\n\";\n-  for(auto & i : agraph.output_desc)\n-    key << i << \"\\n\";\n-  std::string key_ = key.str();\n+////////////////////////////////////////////////////////////////////////////////\n+// FusedKernelCache\n+\n+// Note [Run-time shape checking code]\n+// There are multiple assumptions that our codegen makes, which we can't check\n+// in the fusion pass, because we don't have the shape information. Most notably,\n+// that all values (post-input-chunk, and pre-output-concat) have the same shape\n+// (hereinafter referred to as map size). One way to check this would be to run\n+// shape propagation for every size configuration we get as an input, but that\n+// requires a full graph traversal, and might incur unnecessary overhead. The code\n+// below uses a few nice properties of broadcasting rules and their interactions with\n+// pointwise operations, and takes a smarter approach, to quickly verify validity of\n+// the kernel.\n+//\n+// Notation:\n+//   - a.s when a is a tensor is a shorthand for a.shape.\n+//   - B is a shorthand for the broadcasting/expanding function. It is used as a\n+//     vararg function.\n+//   - E is a shorthand for expand function.\n+//   - Every pointwise operation can be equivalently rewritten as\n+//     f(a, b) = f^(E(a, B(a.s, b.s)), E(b, B(a.s, b.s))),\n+//     where f^ is a non-broadcasting verison of f.\n+//   - A set of inputs that are used to produce a certain graph output is referred to\n+//     as the output's broadcasting group (see Lemma 2. for explanation why).\n+//\n+// Lemma 1. Set of lists of integers (shapes) + { _|_ (bottom/error marker) }, with the\n+//          operation of broadcasting (returning bottom upon shape mismatch) forms a monoid.\n+//          In simpler terms: broadcasting is associative, i.e. B(a, B(b, c)) == B(B(a, b), c).\n+//\n+// Proof.   Satisfies all monoid laws:\n+//            - Closed under broadcasting (trivial)\n+//            - Empty shape is the identity element: B(a, []) == B([], a) == a\n+//            - Associativity: A simple visual proof is that you can expand 3 tensors\n+//                at the same time by stacking their sizes (with alignment to the right),\n+//                just as you'd do in the case of 2 tensors, but with an intermediate\n+//                (the algorithm ends up being pretty much the same).", "path": "torch/csrc/jit/fusion_compiler.cpp", "position": 381, "original_position": 377, "commit_id": "acc2435ce6aed9b51258e32ff56124a11eaacd82", "original_commit_id": "5c300b6e70abb13121d37deac2f2bb9563d92a7b", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Except it's not such a class, and I don't want to make the whole proof 500 LOC long, because that's not where the whole difficulty lies. It's an exercise to the reader to fill in the simple parts \ud83d\ude0a\r\n\r\nIt basically boils down to a single dim case, where its shape takes one of the values: missing, 1, 2, 3, 4 (where 2, 3, 4 are basically to make them non-broadcastable). Then you take all combinations of 3 values from this set and show that the order of broadcasting doesn't matter. I didn't really feel like writing out the solutions for 64 cases, but you can see it is true. I can write a Python script to verify that for you haha", "created_at": "2018-08-24T14:58:57Z", "updated_at": "2018-11-23T15:49:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/10844#discussion_r212657197", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10844", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212657197"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10844#discussion_r212657197"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10844"}}, "body_html": "<p>Except it's not such a class, and I don't want to make the whole proof 500 LOC long, because that's not where the whole difficulty lies. It's an exercise to the reader to fill in the simple parts <g-emoji class=\"g-emoji\" alias=\"blush\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f60a.png\">\ud83d\ude0a</g-emoji></p>\n<p>It basically boils down to a single dim case, where its shape takes one of the values: missing, 1, 2, 3, 4 (where 2, 3, 4 are basically to make them non-broadcastable). Then you take all combinations of 3 values from this set and show that the order of broadcasting doesn't matter. I didn't really feel like writing out the solutions for 64 cases, but you can see it is true. I can write a Python script to verify that for you haha</p>", "body_text": "Except it's not such a class, and I don't want to make the whole proof 500 LOC long, because that's not where the whole difficulty lies. It's an exercise to the reader to fill in the simple parts \ud83d\ude0a\nIt basically boils down to a single dim case, where its shape takes one of the values: missing, 1, 2, 3, 4 (where 2, 3, 4 are basically to make them non-broadcastable). Then you take all combinations of 3 values from this set and show that the order of broadcasting doesn't matter. I didn't really feel like writing out the solutions for 64 cases, but you can see it is true. I can write a Python script to verify that for you haha", "in_reply_to_id": 212643070}