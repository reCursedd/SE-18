{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212643070", "pull_request_review_id": 149322140, "id": 212643070, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjY0MzA3MA==", "diff_hunk": "@@ -1047,67 +1140,409 @@ struct CPUFusionFunction : public CompiledFusionFunction {\n   void (*kernel)(uint32_t, void**) = nullptr;\n };\n \n-std::shared_ptr<CompiledFusionFunction> FusionCompiler::getOrCompile(AnnotatedGraph & agraph) {\n-  std::stringstream key;\n-  key << *agraph.graph << \"\\n\";\n-  key << \"device \" << agraph.device << \"\\n\";\n-  for(auto & i : agraph.input_desc)\n-    key << i << \"\\n\";\n-  for(auto & i : agraph.output_desc)\n-    key << i << \"\\n\";\n-  std::string key_ = key.str();\n+////////////////////////////////////////////////////////////////////////////////\n+// FusedKernelCache\n+\n+// Note [Run-time shape checking code]\n+// There are multiple assumptions that our codegen makes, which we can't check\n+// in the fusion pass, because we don't have the shape information. Most notably,\n+// that all values (post-input-chunk, and pre-output-concat) have the same shape\n+// (hereinafter referred to as map size). One way to check this would be to run\n+// shape propagation for every size configuration we get as an input, but that\n+// requires a full graph traversal, and might incur unnecessary overhead. The code\n+// below uses a few nice properties of broadcasting rules and their interactions with\n+// pointwise operations, and takes a smarter approach, to quickly verify validity of\n+// the kernel.\n+//\n+// Notation:\n+//   - a.s when a is a tensor is a shorthand for a.shape.\n+//   - B is a shorthand for the broadcasting/expanding function. It is used as a\n+//     vararg function.\n+//   - E is a shorthand for expand function.\n+//   - Every pointwise operation can be equivalently rewritten as\n+//     f(a, b) = f^(E(a, B(a.s, b.s)), E(b, B(a.s, b.s))),\n+//     where f^ is a non-broadcasting verison of f.\n+//   - A set of inputs that are used to produce a certain graph output is referred to\n+//     as the output's broadcasting group (see Lemma 2. for explanation why).\n+//\n+// Lemma 1. Set of lists of integers (shapes) + { _|_ (bottom/error marker) }, with the\n+//          operation of broadcasting (returning bottom upon shape mismatch) forms a monoid.\n+//          In simpler terms: broadcasting is associative, i.e. B(a, B(b, c)) == B(B(a, b), c).\n+//\n+// Proof.   Satisfies all monoid laws:\n+//            - Closed under broadcasting (trivial)\n+//            - Empty shape is the identity element: B(a, []) == B([], a) == a\n+//            - Associativity: A simple visual proof is that you can expand 3 tensors\n+//                at the same time by stacking their sizes (with alignment to the right),\n+//                just as you'd do in the case of 2 tensors, but with an intermediate\n+//                (the algorithm ends up being pretty much the same).", "path": "torch/csrc/jit/fusion_compiler.cpp", "position": 381, "original_position": 377, "commit_id": "acc2435ce6aed9b51258e32ff56124a11eaacd82", "original_commit_id": "5c300b6e70abb13121d37deac2f2bb9563d92a7b", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "If this were an undergrad inductive proofs class I would fail this proof :P I agree the lemma is true though.", "created_at": "2018-08-24T14:17:33Z", "updated_at": "2018-11-23T15:49:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/10844#discussion_r212643070", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10844", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212643070"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10844#discussion_r212643070"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10844"}}, "body_html": "<p>If this were an undergrad inductive proofs class I would fail this proof :P I agree the lemma is true though.</p>", "body_text": "If this were an undergrad inductive proofs class I would fail this proof :P I agree the lemma is true though."}