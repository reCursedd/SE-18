{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212656169", "pull_request_review_id": 149338108, "id": 212656169, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjY1NjE2OQ==", "diff_hunk": "@@ -45,111 +53,84 @@ struct TensorDesc {\n     const at::IntList& sizes,\n     const at::IntList& strides);\n \n+  bool operator==(const TensorDesc & desc) const {\n+    return scalar_type == desc.scalar_type && contiguity == desc.contiguity;\n+  }\n+  bool operator!=(const TensorDesc & desc) const {\n+    return !(*this == desc);\n+  }\n+  static size_t hash(const TensorDesc& spec) {\n+    return torch::get_hash(spec.scalar_type, spec.nDim_, std::hash<std::vector<bool>>{}(spec.contiguity));\n+  }\n+\n private:\n   size_t nDim_;\n };\n \n+struct FusedKernelArgSpec {\n+  FusedKernelArgSpec(at::TensorList inputs)\n+    : descs_(fmap<TensorDesc>(inputs))\n+    , hash_code_(torch::get_hash(inputs.size(), descs_)) {}\n+\n+  bool operator==(const FusedKernelArgSpec & spec) const {\n+    return hash_code_ == spec.hash_code_ && descs_ == spec.descs_;\n+  }\n+  bool operator!=(const FusedKernelArgSpec & spec) const {\n+    return !(*this == spec);\n+  }\n+  static size_t hash(const FusedKernelArgSpec& spec) {\n+    return spec.hash_code_;\n+  }\n+  const std::vector<TensorDesc>& descs() const {\n+    return descs_;\n+  }\n+\n+private:\n+  std::vector<TensorDesc> descs_;\n+  size_t hash_code_;\n+};\n+\n constexpr int kCPUDevice = -1;\n struct AnnotatedGraph {\n   // short-term storage only, so it borrows Graph.\n   AnnotatedGraph(Graph & graph, int device)\n   : graph(&graph), device(device) {}\n-  Graph* graph = nullptr;\n+  Graph* graph = nullptr; // TODO: this should really be const\n   int device = kCPUDevice;\n   std::vector<TensorDesc> input_desc;\n   std::vector<TensorDesc> output_desc;\n };\n \n-// Descriptor for chunk-ing an input tensor into subtensors\n-// OR concat-ing an output tensor from subtensors\n-struct PartitionDesc {\n-  size_t nSubtensors; // == 1 for tensors that should not be operated on via chunk/cat\n-  size_t dim; // dimension along which the chunk/concat occurs\n-  std::unique_ptr<TensorDesc> subtensorDesc; // descriptor for the subtensor, if it exists\n-  PartitionDesc()\n-  : nSubtensors(1), dim(0) {}\n-\n-  // Constructor for cat descriptors\n-  // desc: TensorDesc for output tensor\n-  PartitionDesc(const TensorDesc & desc, size_t nSubtensors, size_t dim)\n-  : nSubtensors(nSubtensors), dim(dim) {\n-    JIT_ASSERT(nSubtensors > 1);\n-    std::vector<bool> cont = desc.contiguity;\n-    if(dim > 0) {\n-      // when we narrow the concatenated output\n-      // we make the size[dim] smaller while keeping the stride[dim] the same,\n-      // meaning: stride[dim - 1] != stride[dim]*size[dim]\n-      // so dim - 1 is no longer contiguous\n-      cont[dim - 1] = false;\n-    }\n-    subtensorDesc.reset(new TensorDesc(desc.scalar_type, cont));\n-  }\n-\n-  // Constructor for chunk descriptors\n-  // tensor_type: the type of the input to the chunk node\n-  // ignored: XXX: Compiler gets confused without this arg\n-  PartitionDesc(TensorTypePtr tensor_type, size_t chunks, size_t dim, bool ignored)\n-  : nSubtensors(chunks), dim(dim) {\n-    (void)ignored;\n-    JIT_ASSERT(chunks > 1);\n-\n-    std::vector<int64_t> sizes(tensor_type->sizes().begin(), tensor_type->sizes().end());\n-    JIT_ASSERT(sizes[dim] % chunks == 0); // Should have been checked in graph fuser\n-    sizes[dim] /= chunks;\n-    // Computes contiguity, which is what we really care about\n-    subtensorDesc.reset(new TensorDesc(tensor_type->scalarType(), sizes, tensor_type->strides()));\n-  }\n-\n-  bool isNoop() const {\n-    return nSubtensors == 1;\n-  }\n-};\n-\n-struct CompiledFusionFunction {\n-  TH_DISALLOW_COPY_AND_ASSIGN(CompiledFusionFunction);\n-\n-  CompiledFusionFunction(const std::string & name, AnnotatedGraph & agraph);\n-  virtual ~CompiledFusionFunction() = default;\n-\n-  // expects outputs to be pre-allocated\n-  void launch_with_tensors(at::ArrayRef<at::Tensor> inputs, at::ArrayRef<at::Tensor> outputs);\n-\n-  // creates new tensors for outputs\n-  void launch(at::ArrayRef<at::Tensor> inputs, std::vector<at::Tensor> & outputs);\n-  const std::vector<TensorDesc> & outputDescriptors() const {\n-    return output_desc;\n-  }\n-protected:\n-  virtual at::Backend backend() const = 0;\n-\n-  // arguments is a list of pointers to the arguments for the compiled CUDA/CPU\n-  // code.\n-  // The format of arguments is suitable for directly passing to a call to\n-  // cuLaunchKernel as the kernel arguments.\n-  // Currently the first argument is a pointer to numel (for passing to\n-  // CUDA code), and the remainder are pointers to the TensorInfo<T> structs\n-  // that compiled code uses to load Tensor data.\n-  // launch_with_tensors handles packing at::Tensors into this arguments array.\n-  // CPU code uses the same convension so that launch_with_tensors can be shared.\n-  virtual void launch_raw(uint32_t numel, void ** arguments) = 0;\n-\n-  virtual uint64_t get_rand_offset(uint32_t numel) = 0;\n-  bool has_random;\n-  std::string name;\n-  // We keep these around for debugging\n-  std::string compilation_unit;\n-  std::vector<TensorDesc> input_desc;\n-  std::vector<TensorDesc> output_desc;\n-\n-  // same size as output_desc, describes whether\n-  // an output is actually a concatenation of\n-  // many subtensors that the fusion group produces\n-  std::vector<PartitionDesc> concat_desc;\n+struct FusedKernelCache {", "path": "torch/csrc/jit/fusion_compiler.h", "position": null, "original_position": 172, "commit_id": "acc2435ce6aed9b51258e32ff56124a11eaacd82", "original_commit_id": "5c300b6e70abb13121d37deac2f2bb9563d92a7b", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "Needs a comment. In particular, what does it \"cache\" over?", "created_at": "2018-08-24T14:55:49Z", "updated_at": "2018-11-23T15:49:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/10844#discussion_r212656169", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10844", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212656169"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10844#discussion_r212656169"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10844"}}, "body_html": "<p>Needs a comment. In particular, what does it \"cache\" over?</p>", "body_text": "Needs a comment. In particular, what does it \"cache\" over?"}