{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212661142", "pull_request_review_id": 149344392, "id": 212661142, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMjY2MTE0Mg==", "diff_hunk": "@@ -1047,67 +1140,409 @@ struct CPUFusionFunction : public CompiledFusionFunction {\n   void (*kernel)(uint32_t, void**) = nullptr;\n };\n \n-std::shared_ptr<CompiledFusionFunction> FusionCompiler::getOrCompile(AnnotatedGraph & agraph) {\n-  std::stringstream key;\n-  key << *agraph.graph << \"\\n\";\n-  key << \"device \" << agraph.device << \"\\n\";\n-  for(auto & i : agraph.input_desc)\n-    key << i << \"\\n\";\n-  for(auto & i : agraph.output_desc)\n-    key << i << \"\\n\";\n-  std::string key_ = key.str();\n+////////////////////////////////////////////////////////////////////////////////\n+// FusedKernelCache\n+\n+// Note [Run-time shape checking code]\n+// There are multiple assumptions that our codegen makes, which we can't check\n+// in the fusion pass, because we don't have the shape information. Most notably,\n+// that all values (post-input-chunk, and pre-output-concat) have the same shape\n+// (hereinafter referred to as map size). One way to check this would be to run\n+// shape propagation for every size configuration we get as an input, but that\n+// requires a full graph traversal, and might incur unnecessary overhead. The code\n+// below uses a few nice properties of broadcasting rules and their interactions with\n+// pointwise operations, and takes a smarter approach, to quickly verify validity of\n+// the kernel.\n+//\n+// Notation:\n+//   - a.s when a is a tensor is a shorthand for a.shape.\n+//   - B is a shorthand for the broadcasting/expanding function. It is used as a\n+//     vararg function.\n+//   - E is a shorthand for expand function.\n+//   - Every pointwise operation can be equivalently rewritten as\n+//     f(a, b) = f^(E(a, B(a.s, b.s)), E(b, B(a.s, b.s))),\n+//     where f^ is a non-broadcasting verison of f.\n+//   - A set of inputs that are used to produce a certain graph output is referred to\n+//     as the output's broadcasting group (see Lemma 2. for explanation why).\n+//\n+// Lemma 1. Set of lists of integers (shapes) + { _|_ (bottom/error marker) }, with the\n+//          operation of broadcasting (returning bottom upon shape mismatch) forms a monoid.\n+//          In simpler terms: broadcasting is associative, i.e. B(a, B(b, c)) == B(B(a, b), c).\n+//\n+// Proof.   Satisfies all monoid laws:\n+//            - Closed under broadcasting (trivial)\n+//            - Empty shape is the identity element: B(a, []) == B([], a) == a\n+//            - Associativity: A simple visual proof is that you can expand 3 tensors\n+//                at the same time by stacking their sizes (with alignment to the right),\n+//                just as you'd do in the case of 2 tensors, but with an intermediate\n+//                (the algorithm ends up being pretty much the same).\n+//\n+// Lemma 2. Shape of an output of an arbitrary DAG of pointwise ops depends only on the set\n+//          of inputs used in this DAG and is equal to B([i.shape for i in used_inputs]).\n+//\n+// Proof.   Let G be any DAG of pointwise ops and < be any valid topological\n+//          ordering on nodes of G. Proof by induction over <.\n+//          Base case (graph input):\n+//            Trivial (input is also an output).\n+//          Step (n = f(q, r)):\n+//            Let QS (RS) be the set of shapes of inputs that q (r) depends on.\n+//            Note that the set of inputs that n depends on is exactly QS + RS.\n+//            shape(n) == shape(f(q, r))\n+//                          (def of f)\n+//                     == shape(f^(E(q, B(q.s, r.s)), E(r, B(q.s, r.s))))\n+//                          (output shape of f^ is equal to either of argument shapes)\n+//                     == shape(E(q, B(q.s, r.s)))\n+//                          (property of expand)\n+//                     == B(q.s, r.s)\n+//                          (induction assumption)\n+//                     == B(B(QS...), B(RS...))\n+//                          (Lemma 1.)\n+//                     == B(QS..., RS...)\n+//                          (repeated shapes don't matter for broadcasting)\n+//                     == B((QS + RS)...)\n+//\n+// Lemma 3. Expands are distributive over pointwise ops, i.e. E(f(a, b), s) = f(E(a, s), E(b, s))\n+// Lemma 4. Expands can be collapsed, i.e. E(E(x, s1), s2) = E(x, B(s1, s2)).\n+// Proof.   A simple exercise for the reader :)\n+//\n+// Theorem. If all (pre-concat-)outputs have equal shapes, then we can push the expands to\n+//          (pre-chunk-)inputs, and have all intermediates of the same shape\n+//          (no broadcasting happening in the body).\n+//\n+// Proof.   Using the above lemmas we can easily show that a graph with a single output\n+//          can be easily rewritten by taking the shape given by B applied to all input\n+//          shapes, expanding inputs to it, and using only non-broadcasting operations.\n+//          Example:\n+//\n+//          let d = f(a, b) in\n+//          let e = h(b, c) in\n+//          g(d, e)\n+//\n+//          (By def. of broadcasting pointwise ops applied to g, f and h)\n+//          (Lemma 2. for a closed formula for the size of g = gs)\n+//\n+//          let gs = B(a.s, b.s, c.s) in\n+//          let d' = E(f^(E(a, B(a.s, b.s)), E(b, B(a.s, b.s))), gs) in\n+//          let e' = E(h^(E(b, B(b.s, c.s)), E(c, B(b.s, c.s))), gs) in\n+//          g^(d', e')\n+//\n+//          (Lemma 3.)\n+//\n+//          let gs = B(a.s, b.s, c.s) in\n+//          let d' = f^(E(E(a, B(a.s, b.s)), gs), E(E(b, B(a.s, b.s)), gs)) in\n+//          let e' = h^(E(E(b, B(b.s, c.s)), gs), E(E(c, B(b.s, c.s)), gs)) in\n+//          g^(d', e')\n+//\n+//          (Lemma 4. + Lemma 1. to simplify broadcasting function)\n+//\n+//          let gs = B(a.s, b.s, c.s) in\n+//          let d' = f^(E(a, gs), E(b, gs)) in\n+//          let e' = h^(E(b, gs), E(c, gs)) in\n+//          g^(d', e')\n+//\n+//          (Simple rewrite)\n+//\n+//          let gs = B(a.s, b.s, c.s) in\n+//          let a' = E(a, gs) in\n+//          let b' = E(b, gs) in\n+//          let c' = E(c, gs) in\n+//          let d' = f^(a', b') in\n+//          let e' = h^(b', c') in\n+//          g^(d', e')\n+//\n+//          This example can be easily formalized to arbitrary DAGs using induction\n+//          over topological ordering, similar to Lemma 2. Now, if broadcasting groups\n+//          for all outputs have the same shape, then performing an expand to this size\n+//          on all inputs will ensure that all intermediates on all paths to outputs\n+//          will have the same shape, proving that the body of the kernel is valid.\n+//\n+//          This shows the part until post-chunk-inputs. Extending it to pre-chunk-inputs\n+//          is straightforward (needs a simple lemma for moving expands through chunks).\n+\n+// Register implementations of fused operators, so that we can reuse the fused graph\n+// to generate fallback code.\n+RegisterOperators reg_fused_operators({\n+  Operator(\n+    prim::FusedChunk,\n+    [](Node* node) {\n+      int64_t dim = node->i(attr::dim);\n+      int64_t chunks = node->outputs().size();\n+      return [dim, chunks](Stack& stack) {\n+        auto result = at::chunk(std::move(peek(stack, 0, 1)).toTensor(), chunks, dim);\n+        drop(stack, 1);\n+        pack(stack, std::move(result));\n+        return 0;\n+      };\n+    }),\n+  Operator(\n+    prim::FusedConcat,\n+    [](Node* node) {\n+      int64_t dim = node->i(attr::dim);\n+      int64_t num_inputs = node->inputs().size();\n+      return [dim, num_inputs](Stack& stack) {\n+        auto result = at::cat(\n+          fmap(last(stack, num_inputs), [](const IValue& i) { return i.toTensor(); }),\n+          dim\n+        );\n+        drop(stack, num_inputs);\n+        pack(stack, std::move(result));\n+        return 0;\n+      };\n+    })\n+});\n+\n+FusedKernelCache::FusedKernelCache(FusionCompiler& compiler, std::shared_ptr<Graph> _graph, int device)\n+  : device(device)\n+  , fallback_code(_graph)\n+  , compiler(compiler)\n+  , graph(std::move(_graph))\n+  , input_broadcast_groups(getInputBroadcastGroups())\n+  , input_chunks(getInputChunkDescriptors())\n+  , kernels() {}\n+\n+std::atomic<size_t> FusedKernelCache::next_kernel_id {0};\n+\n+auto FusedKernelCache::getInputChunkDescriptors() -> std::vector<PartitionInfo> {\n+  std::vector<PartitionInfo> descs;\n+  descs.reserve(graph->inputs().size());\n+  for (Value * input : graph->inputs()) {\n+    if (Node * chunk = usedInFusedChunk(input)) {\n+      descs.emplace_back(chunk->i(attr::chunks), chunk->i(attr::dim));\n+    } else {\n+      descs.emplace_back(1, 0);\n+    }\n+  }\n+  return descs;\n+}\n \n-  auto it = cache.find(key_);\n-  if (it == cache.end()) {\n-    std::string name = \"kernel_\" + std::to_string(cache.size());\n-    CompiledFusionFunction * raw_func;\n-    if(agraph.device != kCPUDevice) {\n-#ifdef USE_CUDA\n-      raw_func = new CUDAFusionFunction(name, agraph);\n-#else\n-      throw std::runtime_error(\"cannot compile a CUDA fusion group, CUDA is not enabled.\");\n-#endif\n+// NB: this vector is really a set, but we want to keep it contiguous in memory for faster access\n+static std::vector<int64_t> getInputDependencies(Value* output) {\n+  // Run a DFS traversal to find all inputs that affect a given output value\n+  std::vector<Value*> queue { output };\n+  std::unordered_set<Value*> inputs;\n+  std::unordered_set<Value*> seen;\n+  while (!queue.empty()) {\n+    Value * val = queue.back(); queue.pop_back();\n+    Node * producer = val->node();\n+    if (producer->kind() == prim::Param) {\n+      inputs.insert(val);\n+      continue;\n+    }\n+    for (Value * input : producer->inputs()) {\n+      if (/*bool inserted = */seen.insert(input).second) {\n+        queue.push_back(input);\n+      }\n+    }\n+  }\n+\n+  // Convert Value* into offsets into the graph's input list\n+  std::vector<int64_t> offsets;\n+  offsets.reserve(inputs.size());\n+  for (Value * input : inputs) {\n+    offsets.push_back(input->offset());\n+  }\n+  std::sort(offsets.begin(), offsets.end());\n+  return offsets;\n+}\n+\n+std::vector<std::vector<int64_t>> FusedKernelCache::getInputBroadcastGroups() {\n+  std::unordered_set<std::vector<int64_t>, torch::hash<std::vector<int64_t>>> broadcast_groups;\n+  for (Value * output : graph->outputs()) {\n+    broadcast_groups.insert(getInputDependencies(output));\n+  }\n+  return std::vector<std::vector<int64_t>>{ broadcast_groups.begin(), broadcast_groups.end() };\n+}\n+\n+void FusedKernelCache::run(Stack& stack) {\n+  int64_t num_inputs = graph->inputs().size();\n+  auto args = fmap(last(stack, num_inputs), [](const IValue& i) {\n+                return i.toTensor();\n+              });\n+\n+  auto maybe_map_size = canRunKernel(args);\n+  if (!maybe_map_size) {\n+    return runFallback(stack);\n+  }\n+  expandArgs(args, *maybe_map_size);\n+\n+  FusedKernelArgSpec spec { args };\n+  auto it = kernels.find(spec);\n+  if (it == kernels.end()) {\n+    std::tie(it, std::ignore) = kernels.emplace(spec, compileSpec(spec, *maybe_map_size));\n+  }\n+  auto & fn = it->second;\n+\n+  std::vector<at::Tensor> outputs;\n+  fn->launch(args, outputs);\n+  drop(stack, num_inputs);\n+  pack(stack, std::move(outputs));\n+}\n+\n+at::optional<std::vector<int64_t>> FusedKernelCache::getMapSize(at::TensorList args, at::IntList arg_subset) {\n+  // NB: we leave this uninitialized, because an empty size is trivially\n+  // broadcastable to any other size.\n+  // TODO: this keeps reallocating map_size at every iteration, but we know\n+  // exactly how much storage do we need, so this could be fixed in-place at\n+  // every step. We're just missing a few functions for ATen, but the fix\n+  // should be straightforward.\n+  int64_t dim_after_broadcast = 0;\n+  for (int64_t arg_idx : arg_subset) {\n+    dim_after_broadcast = std::max(dim_after_broadcast, args[arg_idx].dim());\n+  }\n+  std::vector<int64_t> map_size;\n+  for (size_t i = 0; i < arg_subset.size(); ++i) {\n+    auto & arg = args.at(arg_subset[i]);\n+    auto & chunk_desc = input_chunks.at(arg_subset[i]);\n+    if (chunk_desc.nSubtensors == 1) {\n+      try {\n+        map_size = at::infer_size(map_size, arg.sizes());\n+      } catch (std::exception& e) {\n+        return at::nullopt;\n+      }\n     } else {\n-      JIT_ASSERT(canCompileOnCPU());\n-      raw_func = new CPUFusionFunction(name, agraph, config_);\n+      auto tensor_sizes = arg.sizes().vec();\n+      int64_t num_chunks = chunk_desc.nSubtensors;\n+      int64_t dim = chunk_desc.dim;\n+      if (dim < 0) {\n+        dim += arg.dim();", "path": "torch/csrc/jit/fusion_compiler.cpp", "position": null, "original_position": 618, "commit_id": "acc2435ce6aed9b51258e32ff56124a11eaacd82", "original_commit_id": "5c300b6e70abb13121d37deac2f2bb9563d92a7b", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Sure, will use that.", "created_at": "2018-08-24T15:11:54Z", "updated_at": "2018-11-23T15:49:55Z", "html_url": "https://github.com/pytorch/pytorch/pull/10844#discussion_r212661142", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10844", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/212661142"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10844#discussion_r212661142"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10844"}}, "body_html": "<p>Sure, will use that.</p>", "body_text": "Sure, will use that.", "in_reply_to_id": 212660773}