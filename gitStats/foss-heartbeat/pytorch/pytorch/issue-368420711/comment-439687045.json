{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/439687045", "html_url": "https://github.com/pytorch/pytorch/issues/12506#issuecomment-439687045", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12506", "id": 439687045, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTY4NzA0NQ==", "user": {"login": "bhack", "id": 1710528, "node_id": "MDQ6VXNlcjE3MTA1Mjg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1710528?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bhack", "html_url": "https://github.com/bhack", "followers_url": "https://api.github.com/users/bhack/followers", "following_url": "https://api.github.com/users/bhack/following{/other_user}", "gists_url": "https://api.github.com/users/bhack/gists{/gist_id}", "starred_url": "https://api.github.com/users/bhack/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bhack/subscriptions", "organizations_url": "https://api.github.com/users/bhack/orgs", "repos_url": "https://api.github.com/users/bhack/repos", "events_url": "https://api.github.com/users/bhack/events{/privacy}", "received_events_url": "https://api.github.com/users/bhack/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-18T11:52:34Z", "updated_at": "2018-11-18T11:52:34Z", "author_association": "NONE", "body_html": "<p>So cause in majority of the CPU cases that Mat could be continuous In the case you want to inference on a GPU model from a CPU Mat probably it is better to load <code>torch::from_blob(image.data, {1, image.rows, image.cols, 3}, at::kByte);</code> then move to cuda and make a permute and float conversion on the GPU tensor.<br>\nFor the BGR to Rgb I don't know if in the pipeline it will be faster to make a preliminar Cvtcolor on CPU before <code>from_blob</code> or also this could be moved to an ATen op on the Cuda Tensor.</p>", "body_text": "So cause in majority of the CPU cases that Mat could be continuous In the case you want to inference on a GPU model from a CPU Mat probably it is better to load torch::from_blob(image.data, {1, image.rows, image.cols, 3}, at::kByte); then move to cuda and make a permute and float conversion on the GPU tensor.\nFor the BGR to Rgb I don't know if in the pipeline it will be faster to make a preliminar Cvtcolor on CPU before from_blob or also this could be moved to an ATen op on the Cuda Tensor.", "body": "So cause in majority of the CPU cases that Mat could be continuous In the case you want to inference on a GPU model from a CPU Mat probably it is better to load `torch::from_blob(image.data, {1, image.rows, image.cols, 3}, at::kByte);` then move to cuda and make a permute and float conversion on the GPU tensor. \r\nFor the BGR to Rgb I don't know if in the pipeline it will be faster to make a preliminar Cvtcolor on CPU before `from_blob` or also this could be moved to an ATen op on the Cuda Tensor."}