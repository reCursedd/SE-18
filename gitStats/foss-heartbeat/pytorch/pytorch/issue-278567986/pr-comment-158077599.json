{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/158077599", "pull_request_review_id": 84827918, "id": 158077599, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODA3NzU5OQ==", "diff_hunk": "@@ -137,26 +125,15 @@ std::shared_ptr<Function>& VariableViewImpl::get_grad_fn() {\n   return _grad_fn;\n }\n \n-void VariableViewImpl::rebase_history(VarFlags flags, int output_nr, std::shared_ptr<Function> grad_fn) {\n+void VariableViewImpl::rebase_history(int output_nr, std::shared_ptr<Function> grad_fn) {\n   TORCH_ASSERT(output_nr == 0);\n-  TORCH_ASSERT(flags.requires_grad == bool(grad_fn));\n-  if (grad_fn) {\n-    TORCH_ASSERTM(grad_fn->num_inputs == 1, \"Functions which modify views in-place must return a single Variable\");\n-  } else {\n-    // TODO: perhaps we should enable this case by setting base.requires_grad=False\n-    // and base.grad_fn = nullptr.\n-    TORCH_ASSERTM(!base.requires_grad(), \"base.requires_grad does not match view.requires_grad\");\n-  }\n-  this->requires_grad = flags.requires_grad;\n-  this->is_volatile = flags.is_volatile;\n+  TORCH_ASSERT(grad_fn);\n+  TORCH_ASSERTM(grad_fn->num_inputs == 1, \"Functions which modify views in-place must return a single Variable\");\n   this->output_nr = output_nr;\n-  base.requires_grad() |= flags.requires_grad;\n-  base.is_volatile() |= flags.is_volatile;\n-  if (grad_fn) {\n-    base.output_nr() = 0;\n-    base.get()->_grad_fn = std::make_shared<CopySlices>(\n-        base, TensorGeometry(data), std::move(grad_fn));\n-  }\n+  base.output_nr() = 0;\n+  base.get()->_grad_fn = std::make_shared<CopySlices>(\n+      base, TensorGeometry(data), std::move(grad_fn));\n+  get_grad_fn();  // trigger an update to the view's grad_fn", "path": "torch/csrc/autograd/variable.cpp", "position": 101, "original_position": 101, "commit_id": "5aa6a4195c92aa7a2c416cbdff72ae50e94d4e03", "original_commit_id": "5aa6a4195c92aa7a2c416cbdff72ae50e94d4e03", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "You won't get the immediate update to `grad_fn` but it will still be correct if you swap the order. The next time `grad_fn()` is accessed it will update the function.", "created_at": "2017-12-20T16:52:20Z", "updated_at": "2018-11-23T15:37:34Z", "html_url": "https://github.com/pytorch/pytorch/pull/3970#discussion_r158077599", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3970", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/158077599"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3970#discussion_r158077599"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3970"}}, "body_html": "<p>You won't get the immediate update to <code>grad_fn</code> but it will still be correct if you swap the order. The next time <code>grad_fn()</code> is accessed it will update the function.</p>", "body_text": "You won't get the immediate update to grad_fn but it will still be correct if you swap the order. The next time grad_fn() is accessed it will update the function.", "in_reply_to_id": 158025051}