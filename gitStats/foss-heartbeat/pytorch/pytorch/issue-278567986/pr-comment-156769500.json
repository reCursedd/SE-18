{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156769500", "pull_request_review_id": 80934135, "id": 156769500, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Njc2OTUwMA==", "diff_hunk": "@@ -1771,20 +1736,23 @@ def test_inplace_view_backprop_view(self):\n         self.assertEqual(b.grad.data.tolist(), [5])\n         self.assertIsNone(a.grad)\n \n-    def test_inplace_view_flags(self):\n-        # check that an exception is thrown if the flags on the base do not\n-        # match the flags on the view\n-        x = Variable(torch.ones(5))\n+    def test_inplace_view_modify_base(self):\n+        # Test that an in-place operation on a base that forced it to require\n+        # grad also forces any previous views to require grad and backprop\n+        # correctly\n         r = Variable(torch.ones(1), requires_grad=True)\n-        r2 = Variable(torch.ones(1), requires_grad=True)\n-        v = x.select(0, 1)\n-        x.add_(r)\n-        self.assertFalse(v.requires_grad)\n-        self.assertTrue(x.requires_grad)\n-        # v is dependent on r due to the addition above, but v still doesn't\n-        # requires_grad. The addition to r2 should raise an error until we\n-        # share requires_grad between base and views.\n-        self.assertRaisesRegex(RuntimeError, 'requires_grad', lambda: v + r2)\n+\n+        def fn(r):\n+            x = Variable(torch.ones(5))\n+            v = x.select(0, 1)\n+            self.assertFalse(v.requires_grad)\n+            self.assertIsNone(v.grad_fn)\n+            x.add_(r)  # v is now dependent on r due to the in-place op on x\n+            self.assertTrue(v.requires_grad)\n+            return v\n+\n+        gradcheck(fn, [r])\n+        gradgradcheck(fn, [r])", "path": "test/test_autograd.py", "position": 320, "original_position": 238, "commit_id": "5aa6a4195c92aa7a2c416cbdff72ae50e94d4e03", "original_commit_id": "aca0b327182511a74a0ff1ef2deff0656935b1d4", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "BTW can you add a test that makes sure that if you do an in-place modification in no grad mode, and then try to backprop through a graph that had that value saved, then an error will be raised?", "created_at": "2017-12-13T20:11:00Z", "updated_at": "2018-11-23T15:37:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/3970#discussion_r156769500", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3970", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156769500"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3970#discussion_r156769500"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3970"}}, "body_html": "<p>BTW can you add a test that makes sure that if you do an in-place modification in no grad mode, and then try to backprop through a graph that had that value saved, then an error will be raised?</p>", "body_text": "BTW can you add a test that makes sure that if you do an in-place modification in no grad mode, and then try to backprop through a graph that had that value saved, then an error will be raised?"}