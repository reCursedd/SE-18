{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/154742834", "pull_request_review_id": 80934135, "id": 154742834, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDc0MjgzNA==", "diff_hunk": "@@ -137,26 +124,17 @@ std::shared_ptr<Function>& VariableViewImpl::get_grad_fn() {\n   return _grad_fn;\n }\n \n-void VariableViewImpl::rebase_history(VarFlags flags, int output_nr, std::shared_ptr<Function> grad_fn) {\n-  TORCH_ASSERT(output_nr == 0);\n-  TORCH_ASSERT(flags.requires_grad == bool(grad_fn));\n-  if (grad_fn) {\n-    TORCH_ASSERTM(grad_fn->num_inputs == 1, \"Functions which modify views in-place must return a single Variable\");\n-  } else {\n-    // TODO: perhaps we should enable this case by setting base.requires_grad=False\n-    // and base.grad_fn = nullptr.\n-    TORCH_ASSERTM(!base.requires_grad(), \"base.requires_grad does not match view.requires_grad\");\n+void VariableViewImpl::rebase_history(int output_nr, std::shared_ptr<Function> grad_fn) {\n+  if (!grad_fn) {\n+    return;\n   }\n-  this->requires_grad = flags.requires_grad;\n-  this->is_volatile = flags.is_volatile;\n+  TORCH_ASSERT(output_nr == 0);\n+  TORCH_ASSERTM(grad_fn->num_inputs == 1, \"Functions which modify views in-place must return a single Variable\");\n   this->output_nr = output_nr;\n-  base.requires_grad() |= flags.requires_grad;\n-  base.is_volatile() |= flags.is_volatile;\n-  if (grad_fn) {\n-    base.output_nr() = 0;\n-    base.get()->_grad_fn = std::make_shared<CopySlices>(\n-        base, TensorGeometry(data), std::move(grad_fn));\n-  }\n+  this->_grad_fn = grad_fn;", "path": "torch/csrc/autograd/variable.cpp", "position": null, "original_position": 96, "commit_id": "5aa6a4195c92aa7a2c416cbdff72ae50e94d4e03", "original_commit_id": "aca0b327182511a74a0ff1ef2deff0656935b1d4", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Not setting the `grad_fn` of the view (like we did before) was an error, right? Can we add a test for it? \r\n\r\nAlso, it's a bit weird, because it's no longer true that we rebase the views on top of the most current version of `base`. After this runs the graph will change from this:\r\n```\r\nBaseOp <- base\r\n     \\\r\n    ViewOp <- view \r\n```\r\nto this:\r\n```\r\nBaseOp <- CopySlices <- base\r\n     \\\r\n    ViewOp <- InplaceOp <- view \r\n```\r\nwhile I would imagine that we want this:\r\n```\r\nBaseOp <- CopySlices <- base\r\n                   \\\r\n                  ViewOp <- view \r\n```\r\n\r\nAlso, `grad_fn` will be shared with CopySlices, and if we ever end up changing the way we process graphs in autograd engine, we might end up with highly non-trivial race conditions.", "created_at": "2017-12-04T19:00:06Z", "updated_at": "2018-11-23T15:37:04Z", "html_url": "https://github.com/pytorch/pytorch/pull/3970#discussion_r154742834", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3970", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/154742834"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3970#discussion_r154742834"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3970"}}, "body_html": "<p>Not setting the <code>grad_fn</code> of the view (like we did before) was an error, right? Can we add a test for it?</p>\n<p>Also, it's a bit weird, because it's no longer true that we rebase the views on top of the most current version of <code>base</code>. After this runs the graph will change from this:</p>\n<pre><code>BaseOp &lt;- base\n     \\\n    ViewOp &lt;- view \n</code></pre>\n<p>to this:</p>\n<pre><code>BaseOp &lt;- CopySlices &lt;- base\n     \\\n    ViewOp &lt;- InplaceOp &lt;- view \n</code></pre>\n<p>while I would imagine that we want this:</p>\n<pre><code>BaseOp &lt;- CopySlices &lt;- base\n                   \\\n                  ViewOp &lt;- view \n</code></pre>\n<p>Also, <code>grad_fn</code> will be shared with CopySlices, and if we ever end up changing the way we process graphs in autograd engine, we might end up with highly non-trivial race conditions.</p>", "body_text": "Not setting the grad_fn of the view (like we did before) was an error, right? Can we add a test for it?\nAlso, it's a bit weird, because it's no longer true that we rebase the views on top of the most current version of base. After this runs the graph will change from this:\nBaseOp <- base\n     \\\n    ViewOp <- view \n\nto this:\nBaseOp <- CopySlices <- base\n     \\\n    ViewOp <- InplaceOp <- view \n\nwhile I would imagine that we want this:\nBaseOp <- CopySlices <- base\n                   \\\n                  ViewOp <- view \n\nAlso, grad_fn will be shared with CopySlices, and if we ever end up changing the way we process graphs in autograd engine, we might end up with highly non-trivial race conditions."}