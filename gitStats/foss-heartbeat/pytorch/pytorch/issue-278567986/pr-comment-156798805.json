{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156798805", "pull_request_review_id": 83330017, "id": 156798805, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Njc5ODgwNQ==", "diff_hunk": "@@ -270,39 +271,37 @@ static function_list compute_next_functions(TensorList tensors) {\n \n static void check_inplace(const Tensor& tensor) {\n   auto& var = static_cast<const Variable&>(tensor);\n-  if (var.requires_grad() && !var.grad_fn()) {\n+  if (var.requires_grad() && var.is_leaf() && BackpropMode::is_enabled()) {\n     at::runtime_error(\n       \"a leaf Variable that requires grad has been used in an in-place operation.\");\n   }\n }\n \n-static void set_flags(Variable& var, VarFlags flags, std::shared_ptr<Function> grad_fn, bool inplace=false, int output_nr = 0) {\n+static void rebase_history(Variable& var, std::shared_ptr<Function> grad_fn, int output_nr=0) {\n   if (grad_fn) {\n     grad_fn->num_inputs = 1;\n+    var.rebase_history(output_nr, std::move(grad_fn));\n   }\n-  if (inplace) {\n-    var.rebase_history(flags, output_nr, std::move(grad_fn));\n-  } else {\n-    // TODO: combine this code path with the Variable construction\n-    var.get()->requires_grad = flags.requires_grad;\n-    var.get()->is_volatile = flags.is_volatile;\n+}\n+\n+static void set_history(Variable& var, std::shared_ptr<Function> grad_fn, int output_nr=0) {\n+  if (grad_fn) {\n+    grad_fn->num_inputs = 1;\n     var.get()->output_nr = output_nr;\n     var.get()->_grad_fn = std::move(grad_fn);\n   }", "path": "tools/autograd/templates/VariableType.cpp", "position": 82, "original_position": 80, "commit_id": "5aa6a4195c92aa7a2c416cbdff72ae50e94d4e03", "original_commit_id": "aca0b327182511a74a0ff1ef2deff0656935b1d4", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Yes. There's a semantic change here from `volatile=True` to `with torch.no_grad()`:\r\n\r\nin-place ops with `volatile=True` would drop any existing grad_fn\r\nin-place ops with no_grad mode do not modify the grad_fn", "created_at": "2017-12-13T22:02:31Z", "updated_at": "2018-11-23T15:37:21Z", "html_url": "https://github.com/pytorch/pytorch/pull/3970#discussion_r156798805", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3970", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156798805"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3970#discussion_r156798805"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3970"}}, "body_html": "<p>Yes. There's a semantic change here from <code>volatile=True</code> to <code>with torch.no_grad()</code>:</p>\n<p>in-place ops with <code>volatile=True</code> would drop any existing grad_fn<br>\nin-place ops with no_grad mode do not modify the grad_fn</p>", "body_text": "Yes. There's a semantic change here from volatile=True to with torch.no_grad():\nin-place ops with volatile=True would drop any existing grad_fn\nin-place ops with no_grad mode do not modify the grad_fn", "in_reply_to_id": 154730628}