{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/154827135", "pull_request_review_id": 80934135, "id": 154827135, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NDgyNzEzNQ==", "diff_hunk": "@@ -652,11 +652,8 @@ def zero_grad(self):\n         \"\"\"Sets gradients of all model parameters to zero.\"\"\"\n         for p in self.parameters():\n             if p.grad is not None:\n-                if p.grad.volatile:\n-                    p.grad.data.zero_()\n-                else:\n-                    data = p.grad.data\n-                    p.grad = Variable(data.new().resize_as_(data).zero_())\n+                p.grad.detach_()\n+                p.grad.data.zero_()", "path": "torch/nn/modules/module.py", "position": null, "original_position": 10, "commit_id": "5aa6a4195c92aa7a2c416cbdff72ae50e94d4e03", "original_commit_id": "aca0b327182511a74a0ff1ef2deff0656935b1d4", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "The reason why the code looked that way before is that `detach_()` doesn't affect the previous uses `p.grad` in any graphs that are currently alive, and this might cause (silent!!) errors if you ever try to backprop through them (as you modified the data in-place, outside of autograd).", "created_at": "2017-12-05T01:45:06Z", "updated_at": "2018-11-23T15:37:06Z", "html_url": "https://github.com/pytorch/pytorch/pull/3970#discussion_r154827135", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3970", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/154827135"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3970#discussion_r154827135"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3970"}}, "body_html": "<p>The reason why the code looked that way before is that <code>detach_()</code> doesn't affect the previous uses <code>p.grad</code> in any graphs that are currently alive, and this might cause (silent!!) errors if you ever try to backprop through them (as you modified the data in-place, outside of autograd).</p>", "body_text": "The reason why the code looked that way before is that detach_() doesn't affect the previous uses p.grad in any graphs that are currently alive, and this might cause (silent!!) errors if you ever try to backprop through them (as you modified the data in-place, outside of autograd)."}