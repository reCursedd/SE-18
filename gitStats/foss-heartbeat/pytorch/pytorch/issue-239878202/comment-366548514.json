{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/366548514", "html_url": "https://github.com/pytorch/pytorch/issues/1959#issuecomment-366548514", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1959", "id": 366548514, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NjU0ODUxNA==", "user": {"login": "grantsrb", "id": 18670065, "node_id": "MDQ6VXNlcjE4NjcwMDY1", "avatar_url": "https://avatars1.githubusercontent.com/u/18670065?v=4", "gravatar_id": "", "url": "https://api.github.com/users/grantsrb", "html_url": "https://github.com/grantsrb", "followers_url": "https://api.github.com/users/grantsrb/followers", "following_url": "https://api.github.com/users/grantsrb/following{/other_user}", "gists_url": "https://api.github.com/users/grantsrb/gists{/gist_id}", "starred_url": "https://api.github.com/users/grantsrb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/grantsrb/subscriptions", "organizations_url": "https://api.github.com/users/grantsrb/orgs", "repos_url": "https://api.github.com/users/grantsrb/repos", "events_url": "https://api.github.com/users/grantsrb/events{/privacy}", "received_events_url": "https://api.github.com/users/grantsrb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-18T21:18:04Z", "updated_at": "2018-02-21T02:53:27Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=34556457\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Blade6570\">@Blade6570</a> according to the <a href=\"https://arxiv.org/abs/1607.06450\" rel=\"nofollow\">Layer Normalization paper</a>, yes the mean and standard deviation should be a single number shared between all activations (see section 3 of the paper). This is true for all network types.</p>\n<p><strong>EDIT:</strong> Although the mean and standard deviation are each single numbers for all activations, the trainable gain and bias terms should be of the same size as the vector being normalized ignoring the batch size.</p>\n<p>The paper also notes that Batch Normalization works better than Layer Normalization for Convolutional Neural Networks (see section 6.7).</p>\n<p>Note that there is the possibility that new research has been released since the release of this paper (July 21, 2016) that I am unaware of.</p>", "body_text": "@Blade6570 according to the Layer Normalization paper, yes the mean and standard deviation should be a single number shared between all activations (see section 3 of the paper). This is true for all network types.\nEDIT: Although the mean and standard deviation are each single numbers for all activations, the trainable gain and bias terms should be of the same size as the vector being normalized ignoring the batch size.\nThe paper also notes that Batch Normalization works better than Layer Normalization for Convolutional Neural Networks (see section 6.7).\nNote that there is the possibility that new research has been released since the release of this paper (July 21, 2016) that I am unaware of.", "body": "@Blade6570 according to the [Layer Normalization paper](https://arxiv.org/abs/1607.06450), yes the mean and standard deviation should be a single number shared between all activations (see section 3 of the paper). This is true for all network types.\r\n\r\n**EDIT:** Although the mean and standard deviation are each single numbers for all activations, the trainable gain and bias terms should be of the same size as the vector being normalized ignoring the batch size.\r\n\r\nThe paper also notes that Batch Normalization works better than Layer Normalization for Convolutional Neural Networks (see section 6.7).\r\n\r\nNote that there is the possibility that new research has been released since the release of this paper (July 21, 2016) that I am unaware of."}