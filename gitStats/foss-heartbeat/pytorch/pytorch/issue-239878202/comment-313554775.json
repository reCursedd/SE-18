{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/313554775", "html_url": "https://github.com/pytorch/pytorch/issues/1959#issuecomment-313554775", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1959", "id": 313554775, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzU1NDc3NQ==", "user": {"login": "1nadequacy", "id": 15959920, "node_id": "MDQ6VXNlcjE1OTU5OTIw", "avatar_url": "https://avatars3.githubusercontent.com/u/15959920?v=4", "gravatar_id": "", "url": "https://api.github.com/users/1nadequacy", "html_url": "https://github.com/1nadequacy", "followers_url": "https://api.github.com/users/1nadequacy/followers", "following_url": "https://api.github.com/users/1nadequacy/following{/other_user}", "gists_url": "https://api.github.com/users/1nadequacy/gists{/gist_id}", "starred_url": "https://api.github.com/users/1nadequacy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/1nadequacy/subscriptions", "organizations_url": "https://api.github.com/users/1nadequacy/orgs", "repos_url": "https://api.github.com/users/1nadequacy/repos", "events_url": "https://api.github.com/users/1nadequacy/events{/privacy}", "received_events_url": "https://api.github.com/users/1nadequacy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T00:27:31Z", "updated_at": "2017-07-07T00:27:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I have created a naive implementation of LayerNorm for GRU recently. It makes things quite a bit slower, mainly because everything is being done manually, but hopefully it can be helpful.</p>\n<p>Also note, that I don't divide hidden state by std, because I usually initialize the hidden state with zeros.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">LayerNormGRUCell</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">GRUCell</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">hidden_size</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">super</span>(LayerNormGRUCell, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>(input_size, hidden_size, bias)\n\n        <span class=\"pl-c1\">self</span>.gamma_ih <span class=\"pl-k\">=</span> nn.Parameter(torch.ones(<span class=\"pl-c1\">3</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size))\n        <span class=\"pl-c1\">self</span>.gamma_hh <span class=\"pl-k\">=</span> nn.Parameter(torch.ones(<span class=\"pl-c1\">3</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size))\n        <span class=\"pl-c1\">self</span>.eps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_layer_norm_x</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">g</span>, <span class=\"pl-smi\">b</span>):\n        mean <span class=\"pl-k\">=</span> x.mean(<span class=\"pl-c1\">1</span>).expand_as(x)\n        std <span class=\"pl-k\">=</span> x.std(<span class=\"pl-c1\">1</span>).expand_as(x)\n        <span class=\"pl-k\">return</span> g.expand_as(x) <span class=\"pl-k\">*</span> ((x <span class=\"pl-k\">-</span> mean) <span class=\"pl-k\">/</span> (std <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.eps)) <span class=\"pl-k\">+</span> b.expand_as(x)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_layer_norm_h</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">g</span>, <span class=\"pl-smi\">b</span>):\n        mean <span class=\"pl-k\">=</span> x.mean(<span class=\"pl-c1\">1</span>).expand_as(x)\n        <span class=\"pl-k\">return</span> g.expand_as(x) <span class=\"pl-k\">*</span> (x <span class=\"pl-k\">-</span> mean) <span class=\"pl-k\">+</span> b.expand_as(x)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">h</span>):\n\n        ih_rz <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._layer_norm_x(\n            torch.mm(x, <span class=\"pl-c1\">self</span>.weight_ih.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size).transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)),\n            <span class=\"pl-c1\">self</span>.gamma_ih.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size),\n            <span class=\"pl-c1\">self</span>.bias_ih.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size))\n\n        hh_rz <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._layer_norm_h(\n            torch.mm(h, <span class=\"pl-c1\">self</span>.weight_hh.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size).transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)),\n            <span class=\"pl-c1\">self</span>.gamma_hh.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size),\n            <span class=\"pl-c1\">self</span>.bias_hh.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size))\n\n        rz <span class=\"pl-k\">=</span> torch.sigmoid(ih_rz <span class=\"pl-k\">+</span> hh_rz)\n        r <span class=\"pl-k\">=</span> rz.narrow(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">self</span>.hidden_size)\n        z <span class=\"pl-k\">=</span> rz.narrow(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-c1\">self</span>.hidden_size)\n\n        ih_n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._layer_norm_x(\n            torch.mm(x, <span class=\"pl-c1\">self</span>.weight_ih.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-c1\">self</span>.hidden_size).transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)),\n            <span class=\"pl-c1\">self</span>.gamma_ih.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-c1\">self</span>.hidden_size),\n            <span class=\"pl-c1\">self</span>.bias_ih.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-c1\">self</span>.hidden_size))\n\n        hh_n <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._layer_norm_h(\n            torch.mm(h, <span class=\"pl-c1\">self</span>.weight_hh.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-c1\">self</span>.hidden_size).transpose(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)),\n            <span class=\"pl-c1\">self</span>.gamma_hh.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-c1\">self</span>.hidden_size),\n            <span class=\"pl-c1\">self</span>.bias_hh.narrow(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-c1\">self</span>.hidden_size))\n\n        n <span class=\"pl-k\">=</span> torch.tanh(ih_n <span class=\"pl-k\">+</span> r <span class=\"pl-k\">*</span> hh_n)\n        h <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> z) <span class=\"pl-k\">*</span> n <span class=\"pl-k\">+</span> z <span class=\"pl-k\">*</span> h\n        <span class=\"pl-k\">return</span> h\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">LayerNormGRU</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">hidden_size</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">super</span>(LayerNormGRU, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.cell <span class=\"pl-k\">=</span> LayerNormGRUCell(input_size, hidden_size, bias)\n        <span class=\"pl-c1\">self</span>.weight_ih_l0 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.cell.weight_ih\n        <span class=\"pl-c1\">self</span>.weight_hh_l0 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.cell.weight_hh\n        <span class=\"pl-c1\">self</span>.bias_ih_l0 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.cell.bias_ih\n        <span class=\"pl-c1\">self</span>.bias_hh_l0 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.cell.bias_hh\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">xs</span>, <span class=\"pl-smi\">h</span>):\n        h <span class=\"pl-k\">=</span> h.squeeze(<span class=\"pl-c1\">0</span>)\n        ys <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(xs.size(<span class=\"pl-c1\">0</span>)):\n            x <span class=\"pl-k\">=</span> xs.narrow(<span class=\"pl-c1\">0</span>, i, <span class=\"pl-c1\">1</span>).squeeze(<span class=\"pl-c1\">0</span>)\n            h <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.cell(x, h)\n            ys.append(h.unsqueeze(<span class=\"pl-c1\">0</span>))\n        y <span class=\"pl-k\">=</span> torch.cat(ys, <span class=\"pl-c1\">0</span>)\n        h <span class=\"pl-k\">=</span> h.unsqueeze(<span class=\"pl-c1\">0</span>)\n        <span class=\"pl-k\">return</span> y, h</pre></div>", "body_text": "I have created a naive implementation of LayerNorm for GRU recently. It makes things quite a bit slower, mainly because everything is being done manually, but hopefully it can be helpful.\nAlso note, that I don't divide hidden state by std, because I usually initialize the hidden state with zeros.\nclass LayerNormGRUCell(nn.GRUCell):\n    def __init__(self, input_size, hidden_size, bias=True):\n        super(LayerNormGRUCell, self).__init__(input_size, hidden_size, bias)\n\n        self.gamma_ih = nn.Parameter(torch.ones(3 * self.hidden_size))\n        self.gamma_hh = nn.Parameter(torch.ones(3 * self.hidden_size))\n        self.eps = 0\n\n    def _layer_norm_x(self, x, g, b):\n        mean = x.mean(1).expand_as(x)\n        std = x.std(1).expand_as(x)\n        return g.expand_as(x) * ((x - mean) / (std + self.eps)) + b.expand_as(x)\n\n    def _layer_norm_h(self, x, g, b):\n        mean = x.mean(1).expand_as(x)\n        return g.expand_as(x) * (x - mean) + b.expand_as(x)\n\n    def forward(self, x, h):\n\n        ih_rz = self._layer_norm_x(\n            torch.mm(x, self.weight_ih.narrow(0, 0, 2 * self.hidden_size).transpose(0, 1)),\n            self.gamma_ih.narrow(0, 0, 2 * self.hidden_size),\n            self.bias_ih.narrow(0, 0, 2 * self.hidden_size))\n\n        hh_rz = self._layer_norm_h(\n            torch.mm(h, self.weight_hh.narrow(0, 0, 2 * self.hidden_size).transpose(0, 1)),\n            self.gamma_hh.narrow(0, 0, 2 * self.hidden_size),\n            self.bias_hh.narrow(0, 0, 2 * self.hidden_size))\n\n        rz = torch.sigmoid(ih_rz + hh_rz)\n        r = rz.narrow(1, 0, self.hidden_size)\n        z = rz.narrow(1, self.hidden_size, self.hidden_size)\n\n        ih_n = self._layer_norm_x(\n            torch.mm(x, self.weight_ih.narrow(0, 2 * self.hidden_size, self.hidden_size).transpose(0, 1)),\n            self.gamma_ih.narrow(0, 2 * self.hidden_size, self.hidden_size),\n            self.bias_ih.narrow(0, 2 * self.hidden_size, self.hidden_size))\n\n        hh_n = self._layer_norm_h(\n            torch.mm(h, self.weight_hh.narrow(0, 2 * self.hidden_size, self.hidden_size).transpose(0, 1)),\n            self.gamma_hh.narrow(0, 2 * self.hidden_size, self.hidden_size),\n            self.bias_hh.narrow(0, 2 * self.hidden_size, self.hidden_size))\n\n        n = torch.tanh(ih_n + r * hh_n)\n        h = (1 - z) * n + z * h\n        return h\n\nclass LayerNormGRU(nn.Module):\n    def __init__(self, input_size, hidden_size, bias=True):\n        super(LayerNormGRU, self).__init__()\n        self.cell = LayerNormGRUCell(input_size, hidden_size, bias)\n        self.weight_ih_l0 = self.cell.weight_ih\n        self.weight_hh_l0 = self.cell.weight_hh\n        self.bias_ih_l0 = self.cell.bias_ih\n        self.bias_hh_l0 = self.cell.bias_hh\n\n    def forward(self, xs, h):\n        h = h.squeeze(0)\n        ys = []\n        for i in range(xs.size(0)):\n            x = xs.narrow(0, i, 1).squeeze(0)\n            h = self.cell(x, h)\n            ys.append(h.unsqueeze(0))\n        y = torch.cat(ys, 0)\n        h = h.unsqueeze(0)\n        return y, h", "body": "I have created a naive implementation of LayerNorm for GRU recently. It makes things quite a bit slower, mainly because everything is being done manually, but hopefully it can be helpful.\r\n\r\nAlso note, that I don't divide hidden state by std, because I usually initialize the hidden state with zeros. \r\n\r\n```python\r\nclass LayerNormGRUCell(nn.GRUCell):\r\n    def __init__(self, input_size, hidden_size, bias=True):\r\n        super(LayerNormGRUCell, self).__init__(input_size, hidden_size, bias)\r\n\r\n        self.gamma_ih = nn.Parameter(torch.ones(3 * self.hidden_size))\r\n        self.gamma_hh = nn.Parameter(torch.ones(3 * self.hidden_size))\r\n        self.eps = 0\r\n\r\n    def _layer_norm_x(self, x, g, b):\r\n        mean = x.mean(1).expand_as(x)\r\n        std = x.std(1).expand_as(x)\r\n        return g.expand_as(x) * ((x - mean) / (std + self.eps)) + b.expand_as(x)\r\n\r\n    def _layer_norm_h(self, x, g, b):\r\n        mean = x.mean(1).expand_as(x)\r\n        return g.expand_as(x) * (x - mean) + b.expand_as(x)\r\n\r\n    def forward(self, x, h):\r\n\r\n        ih_rz = self._layer_norm_x(\r\n            torch.mm(x, self.weight_ih.narrow(0, 0, 2 * self.hidden_size).transpose(0, 1)),\r\n            self.gamma_ih.narrow(0, 0, 2 * self.hidden_size),\r\n            self.bias_ih.narrow(0, 0, 2 * self.hidden_size))\r\n\r\n        hh_rz = self._layer_norm_h(\r\n            torch.mm(h, self.weight_hh.narrow(0, 0, 2 * self.hidden_size).transpose(0, 1)),\r\n            self.gamma_hh.narrow(0, 0, 2 * self.hidden_size),\r\n            self.bias_hh.narrow(0, 0, 2 * self.hidden_size))\r\n\r\n        rz = torch.sigmoid(ih_rz + hh_rz)\r\n        r = rz.narrow(1, 0, self.hidden_size)\r\n        z = rz.narrow(1, self.hidden_size, self.hidden_size)\r\n\r\n        ih_n = self._layer_norm_x(\r\n            torch.mm(x, self.weight_ih.narrow(0, 2 * self.hidden_size, self.hidden_size).transpose(0, 1)),\r\n            self.gamma_ih.narrow(0, 2 * self.hidden_size, self.hidden_size),\r\n            self.bias_ih.narrow(0, 2 * self.hidden_size, self.hidden_size))\r\n\r\n        hh_n = self._layer_norm_h(\r\n            torch.mm(h, self.weight_hh.narrow(0, 2 * self.hidden_size, self.hidden_size).transpose(0, 1)),\r\n            self.gamma_hh.narrow(0, 2 * self.hidden_size, self.hidden_size),\r\n            self.bias_hh.narrow(0, 2 * self.hidden_size, self.hidden_size))\r\n\r\n        n = torch.tanh(ih_n + r * hh_n)\r\n        h = (1 - z) * n + z * h\r\n        return h\r\n\r\nclass LayerNormGRU(nn.Module):\r\n    def __init__(self, input_size, hidden_size, bias=True):\r\n        super(LayerNormGRU, self).__init__()\r\n        self.cell = LayerNormGRUCell(input_size, hidden_size, bias)\r\n        self.weight_ih_l0 = self.cell.weight_ih\r\n        self.weight_hh_l0 = self.cell.weight_hh\r\n        self.bias_ih_l0 = self.cell.bias_ih\r\n        self.bias_hh_l0 = self.cell.bias_hh\r\n\r\n    def forward(self, xs, h):\r\n        h = h.squeeze(0)\r\n        ys = []\r\n        for i in range(xs.size(0)):\r\n            x = xs.narrow(0, i, 1).squeeze(0)\r\n            h = self.cell(x, h)\r\n            ys.append(h.unsqueeze(0))\r\n        y = torch.cat(ys, 0)\r\n        h = h.unsqueeze(0)\r\n        return y, h\r\n```"}