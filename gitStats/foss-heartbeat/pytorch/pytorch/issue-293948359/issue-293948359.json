{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5014", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5014/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5014/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5014/events", "html_url": "https://github.com/pytorch/pytorch/issues/5014", "id": 293948359, "node_id": "MDU6SXNzdWUyOTM5NDgzNTk=", "number": 5014, "title": "[feature request] match numpy behavior on zero-length and zero-dimension tensors", "user": {"login": "davidbau", "id": 3458792, "node_id": "MDQ6VXNlcjM0NTg3OTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3458792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidbau", "html_url": "https://github.com/davidbau", "followers_url": "https://api.github.com/users/davidbau/followers", "following_url": "https://api.github.com/users/davidbau/following{/other_user}", "gists_url": "https://api.github.com/users/davidbau/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidbau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidbau/subscriptions", "organizations_url": "https://api.github.com/users/davidbau/orgs", "repos_url": "https://api.github.com/users/davidbau/repos", "events_url": "https://api.github.com/users/davidbau/events{/privacy}", "received_events_url": "https://api.github.com/users/davidbau/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-02-02T16:36:40Z", "updated_at": "2018-05-10T01:03:38Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Request for numpy-style support for zero-length tensor dimensions.</p>\n<p>Zero-length tensor dimensions are useful and important when doing fancy indexing, because it allows you to write straight-line code without littering it with python \"if\"s and without moving data off the gpu.</p>\n<p>Example:</p>\n<pre><code>&gt;&gt;&gt; numpy.arange(12).reshape(3,4)[:,[0,2]]\narray([[ 0,  2],\n       [ 4,  6],\n       [ 8, 10]])\n&gt;&gt;&gt; numpy.arange(12).reshape(3,4)[:,[]]\narray([], shape=(3, 0), dtype=int64)\n</code></pre>\n<p>The shape (3,0) continues to fit wherever needed.  However, pytorch cannot maintain a tensor with any zero dimension:</p>\n<pre><code>&gt;&gt;&gt; torch.arange(12).view(3,4)[:,[0,2]]\n  0   2\n  4   6\n  8  10\n[torch.FloatTensor of size 3x2]\n&gt;&gt;&gt; torch.arange(12).view(3,4)[:,[]]\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nIndexError: The advanced indexing objects could not be broadcast\n&gt;&gt;&gt;\n</code></pre>\n<p>This comes up often when using array.nonzero() to gather boolean logic: lack of support for zero-length tensors means that you have to make special-case python checks for empty results rather than using straight-line vectorized pytorch code.  Numpy's design of zero-length dimensions avoids this problem.  The number of scalars held by a tensor should always be the product of its dimensions, which means the a zero dimension is fine - it holds no data, but it still has shape.</p>\n<p>A similar and related issue: numpy also supports tensors with no dimensions at all, which, following the product of dimensions rule, will hold a single scalar value.  Doing this also avoids a bunch of special cases when writing code when the number of tensor dimensions can vary.</p>\n<pre><code>&gt;&gt;&gt; a = numpy.ones(())\n&gt;&gt;&gt; a, a.shape, a[()]\n(array(1.0), (), 1.0)\n&gt;&gt;&gt;\n</code></pre>", "body_text": "Request for numpy-style support for zero-length tensor dimensions.\nZero-length tensor dimensions are useful and important when doing fancy indexing, because it allows you to write straight-line code without littering it with python \"if\"s and without moving data off the gpu.\nExample:\n>>> numpy.arange(12).reshape(3,4)[:,[0,2]]\narray([[ 0,  2],\n       [ 4,  6],\n       [ 8, 10]])\n>>> numpy.arange(12).reshape(3,4)[:,[]]\narray([], shape=(3, 0), dtype=int64)\n\nThe shape (3,0) continues to fit wherever needed.  However, pytorch cannot maintain a tensor with any zero dimension:\n>>> torch.arange(12).view(3,4)[:,[0,2]]\n  0   2\n  4   6\n  8  10\n[torch.FloatTensor of size 3x2]\n>>> torch.arange(12).view(3,4)[:,[]]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: The advanced indexing objects could not be broadcast\n>>>\n\nThis comes up often when using array.nonzero() to gather boolean logic: lack of support for zero-length tensors means that you have to make special-case python checks for empty results rather than using straight-line vectorized pytorch code.  Numpy's design of zero-length dimensions avoids this problem.  The number of scalars held by a tensor should always be the product of its dimensions, which means the a zero dimension is fine - it holds no data, but it still has shape.\nA similar and related issue: numpy also supports tensors with no dimensions at all, which, following the product of dimensions rule, will hold a single scalar value.  Doing this also avoids a bunch of special cases when writing code when the number of tensor dimensions can vary.\n>>> a = numpy.ones(())\n>>> a, a.shape, a[()]\n(array(1.0), (), 1.0)\n>>>", "body": "Request for numpy-style support for zero-length tensor dimensions.\r\n\r\nZero-length tensor dimensions are useful and important when doing fancy indexing, because it allows you to write straight-line code without littering it with python \"if\"s and without moving data off the gpu.\r\n\r\nExample:\r\n```\r\n>>> numpy.arange(12).reshape(3,4)[:,[0,2]]\r\narray([[ 0,  2],\r\n       [ 4,  6],\r\n       [ 8, 10]])\r\n>>> numpy.arange(12).reshape(3,4)[:,[]]\r\narray([], shape=(3, 0), dtype=int64)\r\n```\r\nThe shape (3,0) continues to fit wherever needed.  However, pytorch cannot maintain a tensor with any zero dimension:\r\n\r\n```\r\n>>> torch.arange(12).view(3,4)[:,[0,2]]\r\n  0   2\r\n  4   6\r\n  8  10\r\n[torch.FloatTensor of size 3x2]\r\n>>> torch.arange(12).view(3,4)[:,[]]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nIndexError: The advanced indexing objects could not be broadcast\r\n>>>\r\n```\r\n\r\nThis comes up often when using array.nonzero() to gather boolean logic: lack of support for zero-length tensors means that you have to make special-case python checks for empty results rather than using straight-line vectorized pytorch code.  Numpy's design of zero-length dimensions avoids this problem.  The number of scalars held by a tensor should always be the product of its dimensions, which means the a zero dimension is fine - it holds no data, but it still has shape.\r\n\r\nA similar and related issue: numpy also supports tensors with no dimensions at all, which, following the product of dimensions rule, will hold a single scalar value.  Doing this also avoids a bunch of special cases when writing code when the number of tensor dimensions can vary.\r\n\r\n```\r\n>>> a = numpy.ones(())\r\n>>> a, a.shape, a[()]\r\n(array(1.0), (), 1.0)\r\n>>>\r\n```\r\n\r\n\r\n"}