{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4772", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4772/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4772/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4772/events", "html_url": "https://github.com/pytorch/pytorch/pull/4772", "id": 290330456, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY0MjA5NTUz", "number": 4772, "title": "Use variadic templates instead of initializer lists and overloads (ROUND 2)", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-22T02:01:58Z", "updated_at": "2018-11-23T15:38:28Z", "closed_at": "2018-01-26T20:56:40Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4772", "html_url": "https://github.com/pytorch/pytorch/pull/4772", "diff_url": "https://github.com/pytorch/pytorch/pull/4772.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4772.patch"}, "body_html": "<p>Suppose you are given a list of arguments, each of which may be Tensor or<br>\nTensorList.  How can you write a function that can treat these arguments<br>\nuniformly as a list of tensors?  This patch solves the problem using<br>\nvariadic templates.</p>\n<p>Why variadic templates?  Use of variadic templates means anyone working<br>\nwith this code has to understand universal references, perfect<br>\nforwarding, parameter packs and some idioms of C++ template design.<br>\nHowever, I argue that variadic templates are the <em>right</em> tool for<br>\nsupporting the implementation of functions which must take an<br>\narbitrarily heterogenous set of inputs.  We were able to limp by<br>\nin old code because, for the most part, tensor inputs were homogenous,<br>\nbut this is no longer the case for some non-primitively differentiable<br>\nfunctions; and with the upcoming cuDNN RNN in ATen PR, will no longer be<br>\nthe case for primitively differentiable functions too.</p>\n<p>There are two parts to the PR.</p>\n<p>First, we add torch/csrc/utils/variadic.h, which defines a mix-in<br>\nIterArgs that takes any class which supports operator(), and augments<br>\nwith a new variadic function apply() which calls operator() on each<br>\nargument passed to it.  In an original draft of the patch, I wrote the<br>\nrecursion for each parameter pack from scratch for each function;<br>\nhowever, it turns out there are no fewer than seven instances where we<br>\nneed this idiom, and the mix-in reduces the lines of code, and also<br>\nhelps centralize the most important (and easy to forget) boilerplate<br>\nfor perfect forwarding.</p>\n<p>To verify that IterArgs is compiled away into an unrolled form per<br>\ncall site, I inspected the assembly on some synthetic examples.</p>\n<p>Next, we modify the following functions to make use of IterArgs:</p>\n<ul>\n<li>compute_requires_grad</li>\n<li>Function::flags (Variable and Tensor variants)</li>\n<li>flatten</li>\n<li>isTracing</li>\n<li>count_tensors / count_variables</li>\n</ul>\n<p>Finally, the tuple packer is rewritten to be variadic, although we<br>\ncannot make use of IterArgs (since we are given a tuple).  It might<br>\nmake sense to refactor the code into a generic piece which invokes<br>\na function with the arguments specified by a tuple, and then an<br>\nappropriate IterArgs, but we leave this for future work.</p>\n<p>One thing to note: we cannot write a function with overloads for both<br>\nTensor and Variable, because both ArrayRef and Tensor have<br>\nimplicit conversions from Variable, making such an overload ambiguous.<br>\nIt may be interesting to remove the implicit conversion from ArrayRef.</p>\n<p>Signed-off-by: Edward Z. Yang <a href=\"mailto:ezyang@fb.com\">ezyang@fb.com</a></p>", "body_text": "Suppose you are given a list of arguments, each of which may be Tensor or\nTensorList.  How can you write a function that can treat these arguments\nuniformly as a list of tensors?  This patch solves the problem using\nvariadic templates.\nWhy variadic templates?  Use of variadic templates means anyone working\nwith this code has to understand universal references, perfect\nforwarding, parameter packs and some idioms of C++ template design.\nHowever, I argue that variadic templates are the right tool for\nsupporting the implementation of functions which must take an\narbitrarily heterogenous set of inputs.  We were able to limp by\nin old code because, for the most part, tensor inputs were homogenous,\nbut this is no longer the case for some non-primitively differentiable\nfunctions; and with the upcoming cuDNN RNN in ATen PR, will no longer be\nthe case for primitively differentiable functions too.\nThere are two parts to the PR.\nFirst, we add torch/csrc/utils/variadic.h, which defines a mix-in\nIterArgs that takes any class which supports operator(), and augments\nwith a new variadic function apply() which calls operator() on each\nargument passed to it.  In an original draft of the patch, I wrote the\nrecursion for each parameter pack from scratch for each function;\nhowever, it turns out there are no fewer than seven instances where we\nneed this idiom, and the mix-in reduces the lines of code, and also\nhelps centralize the most important (and easy to forget) boilerplate\nfor perfect forwarding.\nTo verify that IterArgs is compiled away into an unrolled form per\ncall site, I inspected the assembly on some synthetic examples.\nNext, we modify the following functions to make use of IterArgs:\n\ncompute_requires_grad\nFunction::flags (Variable and Tensor variants)\nflatten\nisTracing\ncount_tensors / count_variables\n\nFinally, the tuple packer is rewritten to be variadic, although we\ncannot make use of IterArgs (since we are given a tuple).  It might\nmake sense to refactor the code into a generic piece which invokes\na function with the arguments specified by a tuple, and then an\nappropriate IterArgs, but we leave this for future work.\nOne thing to note: we cannot write a function with overloads for both\nTensor and Variable, because both ArrayRef and Tensor have\nimplicit conversions from Variable, making such an overload ambiguous.\nIt may be interesting to remove the implicit conversion from ArrayRef.\nSigned-off-by: Edward Z. Yang ezyang@fb.com", "body": "Suppose you are given a list of arguments, each of which may be Tensor or\r\nTensorList.  How can you write a function that can treat these arguments\r\nuniformly as a list of tensors?  This patch solves the problem using\r\nvariadic templates.\r\n\r\nWhy variadic templates?  Use of variadic templates means anyone working\r\nwith this code has to understand universal references, perfect\r\nforwarding, parameter packs and some idioms of C++ template design.\r\nHowever, I argue that variadic templates are the *right* tool for\r\nsupporting the implementation of functions which must take an\r\narbitrarily heterogenous set of inputs.  We were able to limp by\r\nin old code because, for the most part, tensor inputs were homogenous,\r\nbut this is no longer the case for some non-primitively differentiable\r\nfunctions; and with the upcoming cuDNN RNN in ATen PR, will no longer be\r\nthe case for primitively differentiable functions too.\r\n\r\nThere are two parts to the PR.\r\n\r\nFirst, we add torch/csrc/utils/variadic.h, which defines a mix-in\r\nIterArgs that takes any class which supports operator(), and augments\r\nwith a new variadic function apply() which calls operator() on each\r\nargument passed to it.  In an original draft of the patch, I wrote the\r\nrecursion for each parameter pack from scratch for each function;\r\nhowever, it turns out there are no fewer than seven instances where we\r\nneed this idiom, and the mix-in reduces the lines of code, and also\r\nhelps centralize the most important (and easy to forget) boilerplate\r\nfor perfect forwarding.\r\n\r\nTo verify that IterArgs is compiled away into an unrolled form per\r\ncall site, I inspected the assembly on some synthetic examples.\r\n\r\nNext, we modify the following functions to make use of IterArgs:\r\n\r\n  - compute_requires_grad\r\n  - Function::flags (Variable and Tensor variants)\r\n  - flatten\r\n  - isTracing\r\n  - count_tensors / count_variables\r\n\r\nFinally, the tuple packer is rewritten to be variadic, although we\r\ncannot make use of IterArgs (since we are given a tuple).  It might\r\nmake sense to refactor the code into a generic piece which invokes\r\na function with the arguments specified by a tuple, and then an\r\nappropriate IterArgs, but we leave this for future work.\r\n\r\nOne thing to note: we cannot write a function with overloads for both\r\nTensor and Variable, because both ArrayRef<Variable> and Tensor have\r\nimplicit conversions from Variable, making such an overload ambiguous.\r\nIt may be interesting to remove the implicit conversion from ArrayRef.\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>"}