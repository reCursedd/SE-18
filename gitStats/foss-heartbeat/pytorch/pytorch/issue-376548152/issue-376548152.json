{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13470", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13470/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13470/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13470/events", "html_url": "https://github.com/pytorch/pytorch/pull/13470", "id": 376548152, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI3NzQxNTAx", "number": 13470, "title": "requires_grad=False when no_grad()", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-01T20:02:03Z", "updated_at": "2018-11-23T15:54:17Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/13470", "html_url": "https://github.com/pytorch/pytorch/pull/13470", "diff_url": "https://github.com/pytorch/pytorch/pull/13470.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/13470.patch"}, "body_html": "<p>This allows native functions to do computation conditioning on whether gradient is needed.</p>\n<p>For example, SVD backward needs U and V tensors, but if we just need the singular values and the decomposed tensor doesn't require grad, we can safely skip computing U and V. Empirically, this speeds up nuclear norm calculation for tenors with <code>requires_grad=False</code> a lot.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> CUDA_LAUNCH_BLOCKING=1</span>\n\nx <span class=\"pl-k\">=</span> (torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>) <span class=\"pl-k\">+</span> torch.eye(<span class=\"pl-c1\">1000</span>).div_(<span class=\"pl-c1\">20</span>)).cuda()\n\n<span class=\"pl-k\">%</span>timeit <span class=\"pl-k\">-</span>r <span class=\"pl-c1\">100</span> x.norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nuc<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> pre patch: 21.3 ms \u00b1 1.99 ms per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> post patch: 9.77 ms \u00b1 295 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)</span>\n\nx.requires_grad_()\n\n<span class=\"pl-k\">%</span>timeit <span class=\"pl-k\">-</span>r <span class=\"pl-c1\">100</span> x.norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nuc<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> pre patch: 25.4 ms \u00b1 2.24 ms per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> post patch: 24.9 ms \u00b1 1.46 ms per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)</span>\n\nx <span class=\"pl-k\">=</span> (torch.randn(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>) <span class=\"pl-k\">+</span> torch.eye(<span class=\"pl-c1\">1000</span>).div_(<span class=\"pl-c1\">20</span>)).cpu()\n\n<span class=\"pl-k\">%</span>timeit <span class=\"pl-k\">-</span>r <span class=\"pl-c1\">100</span> x.norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nuc<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> pre patch: 3.73 ms \u00b1 396 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> post patch: 1.86 ms \u00b1 52.7 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)</span>\n\nx.requires_grad_()\n\n<span class=\"pl-k\">%</span>timeit <span class=\"pl-k\">-</span>r <span class=\"pl-c1\">100</span> x.norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>nuc<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> pre patch: 3.3 ms \u00b1 478 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> post patch: 3.55 ms \u00b1 434 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)</span></pre></div>", "body_text": "This allows native functions to do computation conditioning on whether gradient is needed.\nFor example, SVD backward needs U and V tensors, but if we just need the singular values and the decomposed tensor doesn't require grad, we can safely skip computing U and V. Empirically, this speeds up nuclear norm calculation for tenors with requires_grad=False a lot.\n# CUDA_LAUNCH_BLOCKING=1\n\nx = (torch.randn(1000, 1000) + torch.eye(1000).div_(20)).cuda()\n\n%timeit -r 100 x.norm('nuc')\n\n# pre patch: 21.3 ms \u00b1 1.99 ms per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\n# post patch: 9.77 ms \u00b1 295 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\n\nx.requires_grad_()\n\n%timeit -r 100 x.norm('nuc')\n\n# pre patch: 25.4 ms \u00b1 2.24 ms per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\n# post patch: 24.9 ms \u00b1 1.46 ms per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\n\nx = (torch.randn(1000, 1000) + torch.eye(1000).div_(20)).cpu()\n\n%timeit -r 100 x.norm('nuc')\n\n# pre patch: 3.73 ms \u00b1 396 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\n# post patch: 1.86 ms \u00b1 52.7 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\n\nx.requires_grad_()\n\n%timeit -r 100 x.norm('nuc')\n\n# pre patch: 3.3 ms \u00b1 478 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\n# post patch: 3.55 ms \u00b1 434 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)", "body": "This allows native functions to do computation conditioning on whether gradient is needed. \r\n\r\nFor example, SVD backward needs U and V tensors, but if we just need the singular values and the decomposed tensor doesn't require grad, we can safely skip computing U and V. Empirically, this speeds up nuclear norm calculation for tenors with `requires_grad=False` a lot.\r\n\r\n```py\r\n# CUDA_LAUNCH_BLOCKING=1\r\n\r\nx = (torch.randn(1000, 1000) + torch.eye(1000).div_(20)).cuda()\r\n\r\n%timeit -r 100 x.norm('nuc')\r\n\r\n# pre patch: 21.3 ms \u00b1 1.99 ms per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\r\n# post patch: 9.77 ms \u00b1 295 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\r\n\r\nx.requires_grad_()\r\n\r\n%timeit -r 100 x.norm('nuc')\r\n\r\n# pre patch: 25.4 ms \u00b1 2.24 ms per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\r\n# post patch: 24.9 ms \u00b1 1.46 ms per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\r\n\r\nx = (torch.randn(1000, 1000) + torch.eye(1000).div_(20)).cpu()\r\n\r\n%timeit -r 100 x.norm('nuc')\r\n\r\n# pre patch: 3.73 ms \u00b1 396 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\r\n# post patch: 1.86 ms \u00b1 52.7 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\r\n\r\nx.requires_grad_()\r\n\r\n%timeit -r 100 x.norm('nuc')\r\n\r\n# pre patch: 3.3 ms \u00b1 478 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\r\n# post patch: 3.55 ms \u00b1 434 \u00b5s per loop (mean \u00b1 std. dev. of 100 runs, 1 loop each)\r\n```\r\n\r\n"}