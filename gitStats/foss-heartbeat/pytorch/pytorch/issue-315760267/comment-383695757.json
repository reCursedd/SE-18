{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383695757", "html_url": "https://github.com/pytorch/pytorch/issues/6750#issuecomment-383695757", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6750", "id": 383695757, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzY5NTc1Nw==", "user": {"login": "prigoyal", "id": 13488275, "node_id": "MDQ6VXNlcjEzNDg4Mjc1", "avatar_url": "https://avatars0.githubusercontent.com/u/13488275?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prigoyal", "html_url": "https://github.com/prigoyal", "followers_url": "https://api.github.com/users/prigoyal/followers", "following_url": "https://api.github.com/users/prigoyal/following{/other_user}", "gists_url": "https://api.github.com/users/prigoyal/gists{/gist_id}", "starred_url": "https://api.github.com/users/prigoyal/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prigoyal/subscriptions", "organizations_url": "https://api.github.com/users/prigoyal/orgs", "repos_url": "https://api.github.com/users/prigoyal/repos", "events_url": "https://api.github.com/users/prigoyal/events{/privacy}", "received_events_url": "https://api.github.com/users/prigoyal/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-23T19:34:00Z", "updated_at": "2018-04-24T03:01:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I took a deeper look and TL;DR is that this model is not the right fit for using checkpointing on to see memory savings. The first segment of the model is checkpointed and has two linear layers which take almost all model memory. Of these two linear layers: one is input layer, one is the output layer. In checkpointing, input data is saved for use in backwards. The outputs activation is in memory as it serves as output to the second model segment. So almost all the data needs to stay in memory here. Hence checkpointing is useless.</p>\n<p>Here is the breakdown of memory usage and comparison between checkpoint and non-checkpoint run.</p>\n<p>non-checkpoint:<br>\nthe input buffer + params allocates 2969 MiB memory, the forward pass takes 7004 MiB (all activations are still in memory). Total memory usage in forward pass is 7004 MiB. In the backwards pass, the peak memory usage is 9007 MiB so about 2000MiB extra is needed for the backwards.</p>\n<p>checkpointed:<br>\nthe input buffer + params allocates 2969 MiB memory, the model is divided in two segments and first segment is checkpointed i.e. no intermediate activations are saved except the input. The input 2969MiB is saved and at the end of forward pass on the checkpointed segment, allocated memory* is 5668 MiB. The output of the checkpoint is taken and the forward pass is run on second segment. This allocates no extra memory. Total memory usage at the end of forward pass is 5668 MiB. Now in the backwards pass, memory usage after backward on second segment is 5680 MiB. Now for the backward on 1st segment, we run the forward again, this allocates more memory to 8350 MiB. The activations are retained in memory this time. For backward on this segment, we need extra  ~2000 MiB but this reaches the peak of memory available and OOMs</p>", "body_text": "I took a deeper look and TL;DR is that this model is not the right fit for using checkpointing on to see memory savings. The first segment of the model is checkpointed and has two linear layers which take almost all model memory. Of these two linear layers: one is input layer, one is the output layer. In checkpointing, input data is saved for use in backwards. The outputs activation is in memory as it serves as output to the second model segment. So almost all the data needs to stay in memory here. Hence checkpointing is useless.\nHere is the breakdown of memory usage and comparison between checkpoint and non-checkpoint run.\nnon-checkpoint:\nthe input buffer + params allocates 2969 MiB memory, the forward pass takes 7004 MiB (all activations are still in memory). Total memory usage in forward pass is 7004 MiB. In the backwards pass, the peak memory usage is 9007 MiB so about 2000MiB extra is needed for the backwards.\ncheckpointed:\nthe input buffer + params allocates 2969 MiB memory, the model is divided in two segments and first segment is checkpointed i.e. no intermediate activations are saved except the input. The input 2969MiB is saved and at the end of forward pass on the checkpointed segment, allocated memory* is 5668 MiB. The output of the checkpoint is taken and the forward pass is run on second segment. This allocates no extra memory. Total memory usage at the end of forward pass is 5668 MiB. Now in the backwards pass, memory usage after backward on second segment is 5680 MiB. Now for the backward on 1st segment, we run the forward again, this allocates more memory to 8350 MiB. The activations are retained in memory this time. For backward on this segment, we need extra  ~2000 MiB but this reaches the peak of memory available and OOMs", "body": "I took a deeper look and TL;DR is that this model is not the right fit for using checkpointing on to see memory savings. The first segment of the model is checkpointed and has two linear layers which take almost all model memory. Of these two linear layers: one is input layer, one is the output layer. In checkpointing, input data is saved for use in backwards. The outputs activation is in memory as it serves as output to the second model segment. So almost all the data needs to stay in memory here. Hence checkpointing is useless.\r\n\r\nHere is the breakdown of memory usage and comparison between checkpoint and non-checkpoint run.\r\n\r\nnon-checkpoint:\r\nthe input buffer + params allocates 2969 MiB memory, the forward pass takes 7004 MiB (all activations are still in memory). Total memory usage in forward pass is 7004 MiB. In the backwards pass, the peak memory usage is 9007 MiB so about 2000MiB extra is needed for the backwards.\r\n\r\ncheckpointed:\r\nthe input buffer + params allocates 2969 MiB memory, the model is divided in two segments and first segment is checkpointed i.e. no intermediate activations are saved except the input. The input 2969MiB is saved and at the end of forward pass on the checkpointed segment, allocated memory* is 5668 MiB. The output of the checkpoint is taken and the forward pass is run on second segment. This allocates no extra memory. Total memory usage at the end of forward pass is 5668 MiB. Now in the backwards pass, memory usage after backward on second segment is 5680 MiB. Now for the backward on 1st segment, we run the forward again, this allocates more memory to 8350 MiB. The activations are retained in memory this time. For backward on this segment, we need extra  ~2000 MiB but this reaches the peak of memory available and OOMs"}