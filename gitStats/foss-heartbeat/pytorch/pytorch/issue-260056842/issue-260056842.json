{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2844", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2844/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2844/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2844/events", "html_url": "https://github.com/pytorch/pytorch/issues/2844", "id": 260056842, "node_id": "MDU6SXNzdWUyNjAwNTY4NDI=", "number": 2844, "title": "Implementation for distributed batch normalisation ", "user": {"login": "banditboi", "id": 32233703, "node_id": "MDQ6VXNlcjMyMjMzNzAz", "avatar_url": "https://avatars0.githubusercontent.com/u/32233703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/banditboi", "html_url": "https://github.com/banditboi", "followers_url": "https://api.github.com/users/banditboi/followers", "following_url": "https://api.github.com/users/banditboi/following{/other_user}", "gists_url": "https://api.github.com/users/banditboi/gists{/gist_id}", "starred_url": "https://api.github.com/users/banditboi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/banditboi/subscriptions", "organizations_url": "https://api.github.com/users/banditboi/orgs", "repos_url": "https://api.github.com/users/banditboi/repos", "events_url": "https://api.github.com/users/banditboi/events{/privacy}", "received_events_url": "https://api.github.com/users/banditboi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-24T04:21:21Z", "updated_at": "2017-10-03T08:49:12Z", "closed_at": "2017-10-03T08:49:12Z", "author_association": "NONE", "body_html": "<p>Hello,<br>\nI want to implement distributed batch normalisation for multiple GPUs.<br>\nIf someone already implemented this for Pytorch, I would be greatful if you can share the code with me.<br>\nI am also looking for various ideas on how to implement multi GPU batchnorm.</p>\n<p>My idea is to maintain different statistics for each respective GPU and randomly select one GPU statistic during test time. Is this a good idea?</p>", "body_text": "Hello,\nI want to implement distributed batch normalisation for multiple GPUs.\nIf someone already implemented this for Pytorch, I would be greatful if you can share the code with me.\nI am also looking for various ideas on how to implement multi GPU batchnorm.\nMy idea is to maintain different statistics for each respective GPU and randomly select one GPU statistic during test time. Is this a good idea?", "body": "Hello,\r\nI want to implement distributed batch normalisation for multiple GPUs. \r\nIf someone already implemented this for Pytorch, I would be greatful if you can share the code with me.\r\nI am also looking for various ideas on how to implement multi GPU batchnorm. \r\n\r\nMy idea is to maintain different statistics for each respective GPU and randomly select one GPU statistic during test time. Is this a good idea? "}