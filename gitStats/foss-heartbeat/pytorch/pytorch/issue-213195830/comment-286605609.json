{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/286605609", "html_url": "https://github.com/pytorch/pytorch/issues/967#issuecomment-286605609", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/967", "id": 286605609, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjYwNTYwOQ==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-15T00:36:11Z", "updated_at": "2017-03-15T00:36:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think your benchmarking is still wrong. Here's what it should be</p>\n<div class=\"highlight highlight-source-python\"><pre>            torch.cuda.synchronize()\n            start <span class=\"pl-k\">=</span> time.time()\n            <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_runs):\n                output <span class=\"pl-k\">=</span> m(Variable(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n                output.backward(output.data)\n            torch.cuda.synchronize()\n            mean_time <span class=\"pl-k\">=</span> (time.time() <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">float</span>(num_runs)\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_size: <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span>dilation: <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span>seqlen: <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span> time <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span>(batch_size, dilation, seqlen, mean_time))</pre></div>\n<p>and here are my results on P100. Cudnn gives quite a speed-up and there is no drastic difference between non-dilated and dilated case. It looks like cudnn is still not used for dilated convs in your example - that would explain 10x difference.</p>\n<pre><code>True\nCUDNN  VERSION 6005\nFloat\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.107555\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.010256\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.009971\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.009653\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.009572\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.009549\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.009563\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.009490\nbatch_size: 1\tdilation: 1\tseqlen: 110250\t time 0.008920\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.067781\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.064634\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.067730\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.064171\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.058004\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.055389\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.054575\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.053758\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.046727\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 0.269592\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 0.254314\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 0.269055\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 0.253480\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 0.229877\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 0.219808\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 0.215970\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 0.211928\nbatch_size: 32\tdilation: 1\tseqlen: 110250\t time 0.186116\n</code></pre>\n<pre><code>True\nCUDNN  VERSION 5110\nFloat\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.185996\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.098966\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.087154\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.081256\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.078166\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.076655\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.075930\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.075088\nbatch_size: 1\tdilation: 1\tseqlen: 110250\t time 0.082590\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.972021\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.786541\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.693780\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.646535\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.622609\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.610893\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.604885\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.597967\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.046462\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 3.904609\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 3.162177\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 2.789177\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 2.597142\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 2.501121\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 2.453064\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 2.429012\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 2.401431\n</code></pre>", "body_text": "I think your benchmarking is still wrong. Here's what it should be\n            torch.cuda.synchronize()\n            start = time.time()\n            for j in range(num_runs):\n                output = m(Variable(input, requires_grad=True))\n                output.backward(output.data)\n            torch.cuda.synchronize()\n            mean_time = (time.time() - start) / float(num_runs)\n            print('batch_size: %i\\tdilation: %i\\tseqlen: %i\\t time %f' %(batch_size, dilation, seqlen, mean_time))\nand here are my results on P100. Cudnn gives quite a speed-up and there is no drastic difference between non-dilated and dilated case. It looks like cudnn is still not used for dilated convs in your example - that would explain 10x difference.\nTrue\nCUDNN  VERSION 6005\nFloat\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.107555\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.010256\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.009971\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.009653\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.009572\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.009549\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.009563\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.009490\nbatch_size: 1\tdilation: 1\tseqlen: 110250\t time 0.008920\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.067781\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.064634\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.067730\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.064171\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.058004\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.055389\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.054575\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.053758\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.046727\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 0.269592\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 0.254314\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 0.269055\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 0.253480\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 0.229877\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 0.219808\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 0.215970\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 0.211928\nbatch_size: 32\tdilation: 1\tseqlen: 110250\t time 0.186116\n\nTrue\nCUDNN  VERSION 5110\nFloat\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.185996\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.098966\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.087154\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.081256\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.078166\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.076655\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.075930\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.075088\nbatch_size: 1\tdilation: 1\tseqlen: 110250\t time 0.082590\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.972021\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.786541\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.693780\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.646535\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.622609\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.610893\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.604885\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.597967\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.046462\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 3.904609\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 3.162177\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 2.789177\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 2.597142\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 2.501121\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 2.453064\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 2.429012\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 2.401431", "body": "I think your benchmarking is still wrong. Here's what it should be\r\n```.py\r\n            torch.cuda.synchronize()\r\n            start = time.time()\r\n            for j in range(num_runs):\r\n                output = m(Variable(input, requires_grad=True))\r\n                output.backward(output.data)\r\n            torch.cuda.synchronize()\r\n            mean_time = (time.time() - start) / float(num_runs)\r\n            print('batch_size: %i\\tdilation: %i\\tseqlen: %i\\t time %f' %(batch_size, dilation, seqlen, mean_time))\r\n```\r\nand here are my results on P100. Cudnn gives quite a speed-up and there is no drastic difference between non-dilated and dilated case. It looks like cudnn is still not used for dilated convs in your example - that would explain 10x difference. \r\n```\r\nTrue\r\nCUDNN  VERSION 6005\r\nFloat\r\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.107555\r\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.010256\r\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.009971\r\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.009653\r\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.009572\r\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.009549\r\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.009563\r\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.009490\r\nbatch_size: 1\tdilation: 1\tseqlen: 110250\t time 0.008920\r\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.067781\r\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.064634\r\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.067730\r\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.064171\r\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.058004\r\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.055389\r\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.054575\r\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.053758\r\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.046727\r\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 0.269592\r\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 0.254314\r\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 0.269055\r\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 0.253480\r\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 0.229877\r\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 0.219808\r\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 0.215970\r\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 0.211928\r\nbatch_size: 32\tdilation: 1\tseqlen: 110250\t time 0.186116\r\n```\r\n```\r\nTrue\r\nCUDNN  VERSION 5110\r\nFloat\r\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.185996\r\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.098966\r\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.087154\r\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.081256\r\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.078166\r\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.076655\r\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.075930\r\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.075088\r\nbatch_size: 1\tdilation: 1\tseqlen: 110250\t time 0.082590\r\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.972021\r\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.786541\r\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.693780\r\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.646535\r\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.622609\r\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.610893\r\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.604885\r\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.597967\r\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.046462\r\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 3.904609\r\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 3.162177\r\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 2.789177\r\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 2.597142\r\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 2.501121\r\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 2.453064\r\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 2.429012\r\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 2.401431\r\n```"}