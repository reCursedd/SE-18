{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/286293771", "html_url": "https://github.com/pytorch/pytorch/issues/967#issuecomment-286293771", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/967", "id": 286293771, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjI5Mzc3MQ==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-14T01:23:52Z", "updated_at": "2017-03-14T01:23:52Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It looks like this is due to commit @34ce58c. Pytorch compiled at @c238ee3 runs the following snippet Ok, and at @34ce58c segfaults at conv1d backward (not necessarily dilated):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.backends.cudnn <span class=\"pl-k\">as</span> cudnn\n<span class=\"pl-c1\">print</span>(torch.backends.cudnn.is_acceptable(torch.cuda.FloatTensor(<span class=\"pl-c1\">1</span>)))\ncudnn_version <span class=\"pl-k\">=</span> torch.backends.cudnn.version()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>CUDNN  VERSION<span class=\"pl-pds\">'</span></span>, torch.backends.cudnn.version())\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">10</span>):\n    <span class=\"pl-k\">try</span>:\n        m <span class=\"pl-k\">=</span> nn.Conv1d(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">33</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>i).cuda()\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">50</span>).cuda())\n        output <span class=\"pl-k\">=</span> m(<span class=\"pl-c1\">input</span>)\n        loss <span class=\"pl-k\">=</span> output.sum()\n        loss.backward()\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dilation OK    <span class=\"pl-pds\">'</span></span>, i)\n    <span class=\"pl-k\">except</span>:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dilation FAILED<span class=\"pl-pds\">'</span></span>, i )\n<span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">10</span>):\n    <span class=\"pl-k\">try</span>:\n        m <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">33</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>), <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>i).cuda()\n        <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">50</span>, <span class=\"pl-c1\">100</span>)).cuda()\n        output <span class=\"pl-k\">=</span> m(<span class=\"pl-c1\">input</span>)\n        loss <span class=\"pl-k\">=</span> output.sum()\n        loss.backward()\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dilation OK    <span class=\"pl-pds\">'</span></span>, i)\n    <span class=\"pl-k\">except</span>:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dilation FAILED<span class=\"pl-pds\">'</span></span>, i )</pre></div>", "body_text": "It looks like this is due to commit @34ce58c. Pytorch compiled at @c238ee3 runs the following snippet Ok, and at @34ce58c segfaults at conv1d backward (not necessarily dilated):\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nprint(torch.backends.cudnn.is_acceptable(torch.cuda.FloatTensor(1)))\ncudnn_version = torch.backends.cudnn.version()\nprint('CUDNN  VERSION', torch.backends.cudnn.version())\nfor i in range(1,10):\n    try:\n        m = nn.Conv1d(16, 33, 3, dilation=i).cuda()\n        input = Variable(torch.randn(20, 16, 50).cuda())\n        output = m(input)\n        loss = output.sum()\n        loss.backward()\n        print('dilation OK    ', i)\n    except:\n        print('dilation FAILED', i )\n#\nfor i in range(2,10):\n    try:\n        m = nn.Conv2d(16, 33, (3, 5), dilation=i).cuda()\n        input = Variable(torch.randn(20, 16, 50, 100)).cuda()\n        output = m(input)\n        loss = output.sum()\n        loss.backward()\n        print('dilation OK    ', i)\n    except:\n        print('dilation FAILED', i )", "body": "It looks like this is due to commit @34ce58c. Pytorch compiled at @c238ee3 runs the following snippet Ok, and at @34ce58c segfaults at conv1d backward (not necessarily dilated):\r\n```.py\r\nfrom torch.autograd import Variable\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.backends.cudnn as cudnn\r\nprint(torch.backends.cudnn.is_acceptable(torch.cuda.FloatTensor(1)))\r\ncudnn_version = torch.backends.cudnn.version()\r\nprint('CUDNN  VERSION', torch.backends.cudnn.version())\r\nfor i in range(1,10):\r\n    try:\r\n        m = nn.Conv1d(16, 33, 3, dilation=i).cuda()\r\n        input = Variable(torch.randn(20, 16, 50).cuda())\r\n        output = m(input)\r\n        loss = output.sum()\r\n        loss.backward()\r\n        print('dilation OK    ', i)\r\n    except:\r\n        print('dilation FAILED', i )\r\n#\r\nfor i in range(2,10):\r\n    try:\r\n        m = nn.Conv2d(16, 33, (3, 5), dilation=i).cuda()\r\n        input = Variable(torch.randn(20, 16, 50, 100)).cuda()\r\n        output = m(input)\r\n        loss = output.sum()\r\n        loss.backward()\r\n        print('dilation OK    ', i)\r\n    except:\r\n        print('dilation FAILED', i )\r\n```"}