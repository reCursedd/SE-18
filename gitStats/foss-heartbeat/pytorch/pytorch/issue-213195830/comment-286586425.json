{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/286586425", "html_url": "https://github.com/pytorch/pytorch/issues/967#issuecomment-286586425", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/967", "id": 286586425, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjU4NjQyNQ==", "user": {"login": "skaae", "id": 2623134, "node_id": "MDQ6VXNlcjI2MjMxMzQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/2623134?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skaae", "html_url": "https://github.com/skaae", "followers_url": "https://api.github.com/users/skaae/followers", "following_url": "https://api.github.com/users/skaae/following{/other_user}", "gists_url": "https://api.github.com/users/skaae/gists{/gist_id}", "starred_url": "https://api.github.com/users/skaae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skaae/subscriptions", "organizations_url": "https://api.github.com/users/skaae/orgs", "repos_url": "https://api.github.com/users/skaae/repos", "events_url": "https://api.github.com/users/skaae/events{/privacy}", "received_events_url": "https://api.github.com/users/skaae/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-14T22:51:29Z", "updated_at": "2017-03-14T22:51:29Z", "author_association": "NONE", "body_html": "<p>I benchmarked dilations using cudnn 5.1 and 6.0. It seems that dilations != 1 are really slow for anything but batchsize 1?</p>\n<pre><code>CUDNN  VERSION 5110\nFloat\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.013589\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.001548\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.001471\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.001159\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.001457\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.001279\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.001069\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.001068\nbatch_size: 1\tdilation: 1     seqlen: 110250\t time 0.023048\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.902854\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.751134\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.584407\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.494308\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.469266\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.457277\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.450981\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.444093\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.025876\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 4.717062\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 3.425079\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 2.801741\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 2.545884\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 2.420491\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 2.356115\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 2.327302\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 2.289761\nbatch_size: 32\tdilation: 1\tseqlen: 110250\t time 0.097122\n\n\n\n\nTrue\nCUDNN  VERSION 6005\nFloat\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.013422\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.001136\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.001040\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.001356\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.001070\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.000860\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.000863\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.000852\nbatch_size: 1\tdilation: 1\tseqlen: 110250\t time 0.023259\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.820482\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.638027\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.543342\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.493492\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.469106\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.457294\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.451203\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.450795\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.027164\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 4.883585\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 3.500894\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 2.873164\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 2.561955\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 2.420982\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 2.356729\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 2.327069\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 2.289957\nbatch_size: 32\tdilation: 1\tseqlen: 110250\t time 0.096707\n</code></pre>\n<p><strong>Conv1d script</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">import</span> torch.backends.cudnn <span class=\"pl-k\">as</span> cudnn\ncudnn.benchmark <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n\n<span class=\"pl-c1\">print</span>(torch.backends.cudnn.is_acceptable(torch.cuda.FloatTensor(<span class=\"pl-c1\">1</span>)))\ncudnn_version <span class=\"pl-k\">=</span> torch.backends.cudnn.version()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>CUDNN  VERSION<span class=\"pl-pds\">'</span></span>, torch.backends.cudnn.version())\n<span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> Conv1d <span class=\"pl-k\">as</span> Conv1d\n\nnum_runs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\ns <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span><span class=\"pl-k\">*</span><span class=\"pl-c1\">22050</span>\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Float<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">for</span> seqlen <span class=\"pl-k\">in</span> [s]:\n    <span class=\"pl-k\">for</span> batch_size <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">32</span>]:\n        <span class=\"pl-k\">for</span> dilation <span class=\"pl-k\">in</span> <span class=\"pl-c1\">reversed</span>([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">256</span>]):\n            m <span class=\"pl-k\">=</span> nn.Sequential(Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation),\n                              Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation),\n                              Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation),\n                              Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation),\n                              Conv1d(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dilation</span><span class=\"pl-k\">=</span>dilation)).cuda()\n            <span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> torch.randn(batch_size, <span class=\"pl-c1\">32</span>, seqlen).float().cuda()\n\n            start <span class=\"pl-k\">=</span> time.time()\n            <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_runs):\n                output <span class=\"pl-k\">=</span> m(Variable(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n                output.backward(output.data)\n            mean_time <span class=\"pl-k\">=</span> (time.time() <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">float</span>(num_runs)\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_size: <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span>dilation: <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span>seqlen: <span class=\"pl-c1\">%i</span><span class=\"pl-cce\">\\t</span> time <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span>(batch_size, dilation, seqlen, mean_time))</pre></div>", "body_text": "I benchmarked dilations using cudnn 5.1 and 6.0. It seems that dilations != 1 are really slow for anything but batchsize 1?\nCUDNN  VERSION 5110\nFloat\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.013589\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.001548\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.001471\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.001159\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.001457\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.001279\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.001069\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.001068\nbatch_size: 1\tdilation: 1     seqlen: 110250\t time 0.023048\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.902854\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.751134\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.584407\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.494308\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.469266\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.457277\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.450981\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.444093\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.025876\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 4.717062\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 3.425079\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 2.801741\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 2.545884\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 2.420491\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 2.356115\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 2.327302\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 2.289761\nbatch_size: 32\tdilation: 1\tseqlen: 110250\t time 0.097122\n\n\n\n\nTrue\nCUDNN  VERSION 6005\nFloat\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.013422\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.001136\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.001040\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.001356\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.001070\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.000860\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.000863\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.000852\nbatch_size: 1\tdilation: 1\tseqlen: 110250\t time 0.023259\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.820482\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.638027\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.543342\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.493492\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.469106\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.457294\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.451203\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.450795\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.027164\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 4.883585\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 3.500894\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 2.873164\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 2.561955\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 2.420982\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 2.356729\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 2.327069\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 2.289957\nbatch_size: 32\tdilation: 1\tseqlen: 110250\t time 0.096707\n\nConv1d script\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\ncudnn.benchmark = True\n\n\nprint(torch.backends.cudnn.is_acceptable(torch.cuda.FloatTensor(1)))\ncudnn_version = torch.backends.cudnn.version()\nprint('CUDNN  VERSION', torch.backends.cudnn.version())\nimport time\nfrom torch.nn import Conv1d as Conv1d\n\nnum_runs = 10\ns = 5*22050\n\nprint('Float')\nfor seqlen in [s]:\n    for batch_size in [1, 8, 32]:\n        for dilation in reversed([1, 2, 4, 8, 16, 32, 64, 128, 256]):\n            m = nn.Sequential(Conv1d(32, 32, kernel_size=2, dilation=dilation),\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation)).cuda()\n            input = torch.randn(batch_size, 32, seqlen).float().cuda()\n\n            start = time.time()\n            for j in range(num_runs):\n                output = m(Variable(input, requires_grad=True))\n                output.backward(output.data)\n            mean_time = (time.time() - start) / float(num_runs)\n            print('batch_size: %i\\tdilation: %i\\tseqlen: %i\\t time %f' %(batch_size, dilation, seqlen, mean_time))", "body": "I benchmarked dilations using cudnn 5.1 and 6.0. It seems that dilations != 1 are really slow for anything but batchsize 1? \r\n\r\n```\r\nCUDNN  VERSION 5110\r\nFloat\r\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.013589\r\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.001548\r\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.001471\r\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.001159\r\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.001457\r\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.001279\r\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.001069\r\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.001068\r\nbatch_size: 1\tdilation: 1     seqlen: 110250\t time 0.023048\r\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.902854\r\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.751134\r\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.584407\r\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.494308\r\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.469266\r\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.457277\r\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.450981\r\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.444093\r\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.025876\r\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 4.717062\r\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 3.425079\r\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 2.801741\r\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 2.545884\r\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 2.420491\r\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 2.356115\r\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 2.327302\r\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 2.289761\r\nbatch_size: 32\tdilation: 1\tseqlen: 110250\t time 0.097122\r\n\r\n\r\n\r\n\r\nTrue\r\nCUDNN  VERSION 6005\r\nFloat\r\nbatch_size: 1\tdilation: 256\tseqlen: 110250\t time 0.013422\r\nbatch_size: 1\tdilation: 128\tseqlen: 110250\t time 0.001136\r\nbatch_size: 1\tdilation: 64\tseqlen: 110250\t time 0.001040\r\nbatch_size: 1\tdilation: 32\tseqlen: 110250\t time 0.001356\r\nbatch_size: 1\tdilation: 16\tseqlen: 110250\t time 0.001070\r\nbatch_size: 1\tdilation: 8\tseqlen: 110250\t time 0.000860\r\nbatch_size: 1\tdilation: 4\tseqlen: 110250\t time 0.000863\r\nbatch_size: 1\tdilation: 2\tseqlen: 110250\t time 0.000852\r\nbatch_size: 1\tdilation: 1\tseqlen: 110250\t time 0.023259\r\nbatch_size: 8\tdilation: 256\tseqlen: 110250\t time 0.820482\r\nbatch_size: 8\tdilation: 128\tseqlen: 110250\t time 0.638027\r\nbatch_size: 8\tdilation: 64\tseqlen: 110250\t time 0.543342\r\nbatch_size: 8\tdilation: 32\tseqlen: 110250\t time 0.493492\r\nbatch_size: 8\tdilation: 16\tseqlen: 110250\t time 0.469106\r\nbatch_size: 8\tdilation: 8\tseqlen: 110250\t time 0.457294\r\nbatch_size: 8\tdilation: 4\tseqlen: 110250\t time 0.451203\r\nbatch_size: 8\tdilation: 2\tseqlen: 110250\t time 0.450795\r\nbatch_size: 8\tdilation: 1\tseqlen: 110250\t time 0.027164\r\nbatch_size: 32\tdilation: 256\tseqlen: 110250\t time 4.883585\r\nbatch_size: 32\tdilation: 128\tseqlen: 110250\t time 3.500894\r\nbatch_size: 32\tdilation: 64\tseqlen: 110250\t time 2.873164\r\nbatch_size: 32\tdilation: 32\tseqlen: 110250\t time 2.561955\r\nbatch_size: 32\tdilation: 16\tseqlen: 110250\t time 2.420982\r\nbatch_size: 32\tdilation: 8\tseqlen: 110250\t time 2.356729\r\nbatch_size: 32\tdilation: 4\tseqlen: 110250\t time 2.327069\r\nbatch_size: 32\tdilation: 2\tseqlen: 110250\t time 2.289957\r\nbatch_size: 32\tdilation: 1\tseqlen: 110250\t time 0.096707\r\n```\r\n\r\n\r\n\r\n__Conv1d script__\r\n```Python\r\nfrom torch.autograd import Variable\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.backends.cudnn as cudnn\r\ncudnn.benchmark = True\r\n\r\n\r\nprint(torch.backends.cudnn.is_acceptable(torch.cuda.FloatTensor(1)))\r\ncudnn_version = torch.backends.cudnn.version()\r\nprint('CUDNN  VERSION', torch.backends.cudnn.version())\r\nimport time\r\nfrom torch.nn import Conv1d as Conv1d\r\n\r\nnum_runs = 10\r\ns = 5*22050\r\n\r\nprint('Float')\r\nfor seqlen in [s]:\r\n    for batch_size in [1, 8, 32]:\r\n        for dilation in reversed([1, 2, 4, 8, 16, 32, 64, 128, 256]):\r\n            m = nn.Sequential(Conv1d(32, 32, kernel_size=2, dilation=dilation),\r\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\r\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\r\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation),\r\n                              Conv1d(32, 32, kernel_size=2, dilation=dilation)).cuda()\r\n            input = torch.randn(batch_size, 32, seqlen).float().cuda()\r\n\r\n            start = time.time()\r\n            for j in range(num_runs):\r\n                output = m(Variable(input, requires_grad=True))\r\n                output.backward(output.data)\r\n            mean_time = (time.time() - start) / float(num_runs)\r\n            print('batch_size: %i\\tdilation: %i\\tseqlen: %i\\t time %f' %(batch_size, dilation, seqlen, mean_time))\r\n```\r\n"}