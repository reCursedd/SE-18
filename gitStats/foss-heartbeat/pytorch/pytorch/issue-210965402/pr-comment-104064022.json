{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104064022", "pull_request_review_id": 24879125, "id": 104064022, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNDA2NDAyMg==", "diff_hunk": "@@ -0,0 +1,503 @@\n+#include \"convolution.h\"\n+\n+#include \"torch/csrc/autograd/variable.h\"\n+#include \"torch/csrc/nn/THNN_generic.h\"\n+#include \"torch/csrc/utils/auto_gpu.h\"\n+\n+#ifdef WITH_CUDNN\n+#include \"torch/csrc/cudnn/Conv.h\"\n+#include \"torch/csrc/cudnn/Handles.h\"\n+#include \"torch/csrc/cudnn/Types.h\"\n+extern THCState* state;\n+using namespace torch::cudnn;\n+#endif\n+\n+using namespace torch::nn;\n+using thpp::Tensor;\n+using torch::cudnn::Convolution;\n+using tensor_pair = std::pair<std::unique_ptr<Tensor>, std::unique_ptr<Tensor>>;\n+\n+namespace torch { namespace autograd {\n+\n+auto ConvParams::is_dilated() const -> bool {\n+  bool is_dilated = false;\n+  for (int d : dilation) {\n+    is_dilated |= (d != 1);\n+  }\n+  return is_dilated;\n+}\n+\n+auto ConvParams::view1d_as_2d() -> void {\n+  if (stride.size() == 1) {\n+    stride.insert(stride.begin(), 1);\n+    padding.insert(padding.begin(), 0);\n+    dilation.insert(dilation.begin(), 1);\n+    output_padding.insert(output_padding.begin(), 0);\n+  }\n+}\n+\n+auto ConvForward::output_size(Tensor& input, Tensor& weight) -> std::vector<long> {\n+  auto in_size = input.sizes();\n+  auto weight_size = weight.sizes();\n+  auto dim = input.nDim();\n+\n+  std::vector<long> output_size(dim);\n+  output_size[0] = in_size[0];\n+  output_size[1] = transposed ? weight_size[1] * groups : weight_size[0];\n+  for (int d = 2; d < dim; ++d) {\n+    int kernel = dilation[d - 2] * (weight_size[d] - 1) + 1;\n+    if (transposed) {\n+      output_size[d] = (in_size[d] - 1) * stride[d - 2] - (2 * padding[d - 2]) +\n+                       kernel + output_padding[d - 2];\n+    } else {\n+      output_size[d] = (in_size[d] + (2 * padding[d - 2]) - kernel) / stride[d - 2] + 1;\n+    }\n+  }\n+  return output_size;\n+}\n+\n+static std::unique_ptr<Tensor> subtensor(Tensor* tensor, int dim, int groups, int g);\n+\n+static std::unique_ptr<Tensor> compute_output(\n+  Tensor* input, Tensor* weight, Tensor* bias, Tensor* columns, Tensor* ones,\n+  const std::vector<long>& kernel_size, const ConvParams& params);\n+\n+static std::unique_ptr<Tensor> compute_grad_input(\n+  Tensor* input, Tensor* grad_output, Tensor* weight, Tensor* columns, Tensor* ones,\n+  const std::vector<long>& kernel_size, const ConvParams& params);\n+\n+static tensor_pair compute_grad_params(\n+  Tensor* input, Tensor* grad_output, Tensor* weight, Tensor* bias, Tensor* columns, Tensor* ones,\n+  const std::vector<long>& kernel_size, const ConvBackward& params);\n+\n+static std::unique_ptr<Tensor> cat(const tensor_list& tensors, int dim);\n+\n+static auto view4d(const Tensor& tensor) -> std::unique_ptr<Tensor> {\n+  if (tensor.nDim() != 3) throw std::runtime_error(\"expected 3D tensor\");\n+  auto result = tensor.newTensor();\n+  result->unsqueeze(tensor, 2);\n+  return result;\n+}\n+\n+static auto view3d(const Tensor& tensor) -> std::unique_ptr<Tensor> {\n+  if (tensor.nDim() != 4) throw std::runtime_error(\"expected 4D tensor\");\n+  auto result = tensor.newTensor();\n+  result->squeeze(tensor, 2);\n+  return result;\n+}\n+\n+auto ConvForward::apply(const variable_list& inputs) -> variable_list {\n+  if (inputs.size() != 3) throw std::runtime_error(\"expected three inputs\");\n+\n+  AutoGPU guard(inputs[0]->data->getDevice());\n+  auto input = inputs[0]->data->contiguous();\n+  std::unique_ptr<Tensor> weight(inputs[1]->data->clone_shallow());\n+  std::unique_ptr<Tensor> bias(inputs[2] ? inputs[2]->data->clone_shallow() : nullptr);\n+\n+  int k = input->nDim();\n+  if (k == 3) {\n+    view1d_as_2d();\n+    input = view4d(*input);\n+    weight = view4d(*weight);\n+  }\n+\n+  auto weight_size = weight->sizes();\n+  std::vector<long> kernel_size(weight_size.begin() + 2, weight_size.end());\n+\n+  bool use_cudnn = false;\n+#ifdef WITH_CUDNN\n+  // TODO: cuDNN version", "path": "torch/csrc/autograd/functions/convolution.cpp", "position": null, "original_position": 109, "commit_id": "6336300880349038c5bf6f5dfe3b37864eb39acb", "original_commit_id": "b132877316ccb20083d70b969d96c0a7d6420cc2", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Newer versions of cuDNN support dilated convolution. I'll fix this.", "created_at": "2017-03-03T00:22:49Z", "updated_at": "2018-11-23T15:32:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/881#discussion_r104064022", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/881", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104064022"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/881#discussion_r104064022"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/881"}}, "body_html": "<p>Newer versions of cuDNN support dilated convolution. I'll fix this.</p>", "body_text": "Newer versions of cuDNN support dilated convolution. I'll fix this.", "in_reply_to_id": 104059506}