{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103812960", "pull_request_review_id": 24613304, "id": 103812960, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMzgxMjk2MA==", "diff_hunk": "@@ -1,15 +1,198 @@\n #include \"torch/csrc/autograd/engine.h\"\n \n-#include <unordered_set>\n+#include <atomic>\n+#include <condition_variable>\n+#include <cstdint>\n+#include <iostream>\n+#include <mutex>\n+#include <set>\n #include <string>\n #include <THPP/THPP.h>\n+#include <thread>\n+#include <unordered_set>\n+#include <typeinfo>\n+#include <sstream>\n+\n+#ifdef WITH_CUDA\n+#include <cuda.h>\n+#include <THC/THC.h>\n+#endif\n \n using thpp::Tensor;\n \n namespace torch { namespace autograd {\n \n-auto Engine::compute_dependencies(function_queue queue, ready_queue_type& ready) -> dependencies_type {\n-  // First, search the graph and find all stochastic functions. Append them to the queue.\n+struct ReadyQueue {\n+  std::deque<FunctionTask> queue;\n+  std::condition_variable not_empty;\n+  std::mutex mutex;\n+\n+  void push_front(FunctionTask item);\n+  FunctionTask pop_back();\n+};\n+\n+struct BackwardTask {\n+  std::exception_ptr exception;\n+  std::atomic_bool has_error;\n+  std::atomic<uint64_t> outstanding_tasks;\n+  bool retain_variables;\n+\n+  std::mutex mutex;\n+  std::condition_variable not_done;\n+  std::unordered_map<Function*, GradBuffer> not_ready;\n+  std::unordered_map<Function*, int> dependencies;\n+\n+  BackwardTask(bool retain_variables)\n+    : exception()\n+    , has_error(false)\n+    , outstanding_tasks(0)\n+    , retain_variables(retain_variables)\n+    , mutex()\n+    , not_done()\n+    , not_ready()\n+    , dependencies() {}\n+};\n+\n+struct FunctionTask {\n+  BackwardTask* base;\n+  std::shared_ptr<Function> fn;\n+  GradBuffer grad;\n+\n+  FunctionTask(BackwardTask* base, std::shared_ptr<Function> fn, GradBuffer grad)\n+    : base(base)\n+    , fn(fn)\n+    , grad(std::move(grad)) {}\n+};\n+\n+auto ReadyQueue::push_front(FunctionTask item) -> void {\n+  ++item.base->outstanding_tasks;\n+  {\n+    std::lock_guard<std::mutex> lock(mutex);\n+    queue.push_front(std::move(item));\n+  }\n+  not_empty.notify_one();\n+}\n+\n+auto ReadyQueue::pop_back() -> FunctionTask {\n+  std::unique_lock<std::mutex> lock(mutex);\n+  not_empty.wait(lock, [this]{ return !queue.empty(); });\n+  auto value = std::move(queue.back()); queue.pop_back();\n+  return value;\n+}\n+\n+Engine::Engine() : ready_queues() {\n+}\n+\n+Engine::~Engine() = default;\n+\n+auto Engine::thread_main(ReadyQueue& queue) -> void {\n+  while (1) {\n+    FunctionTask task = queue.pop_back();\n+    if (!task.base->has_error.load()) {\n+      try {\n+        evaluate_function(task);\n+      } catch (std::exception& e) {\n+        thread_on_exception(task, e);\n+      }\n+    }\n+    if (--task.base->outstanding_tasks == 0) {\n+      task.base->not_done.notify_all();\n+    }\n+  }\n+}\n+\n+auto Engine::thread_on_exception(FunctionTask& task, std::exception& e) -> void {\n+  std::lock_guard<std::mutex> lock(task.base->mutex);\n+  if (!task.base->has_error.load()) {\n+    task.base->exception = std::current_exception();\n+    task.base->has_error = true;\n+  }\n+}\n+\n+auto Engine::evaluate_function(FunctionTask& task) -> void\n+{\n+  auto& fn = *task.fn;\n+  auto grad_output = fn.call_hooks(GradBuffer::variables(std::move(task.grad)));\n+  auto grad_inputs = fn.apply(grad_output);\n+  if (!task.base->retain_variables) {\n+    fn.releaseVariables();\n+  }\n+\n+  if (grad_inputs.size() != fn.previous_functions.size()) {\n+    std::stringstream ss;\n+    ss << \"Function '\" << fn.name() << \"' returned an invalid number of gradients - expected \";\n+    ss << fn.previous_functions.size() << \", but got \" << grad_inputs.size();\n+    throw std::runtime_error(ss.str());\n+  }\n+\n+  int size = grad_inputs.size();\n+  for (int i = 0; i < size; ++i) {\n+    auto& grad_input = grad_inputs[i];\n+    auto& prev_fn = fn.previous_functions[i].first;\n+    int output_nr = fn.previous_functions[i].second;\n+\n+    // null inputs have no previous_function and we skip them here\n+    if (!prev_fn) {\n+      continue;\n+    }\n+\n+    // Stochastic functions are placed in the ready queue by\n+    // compute_dependencies, so we can skip them here.\n+    if (prev_fn->is_stochastic || !prev_fn->requires_grad) {\n+      continue;\n+    }\n+\n+    std::lock_guard<std::mutex> lock(task.base->mutex);\n+    if (auto var = dynamic_cast<Variable*>(prev_fn.get())) {\n+      if (!grad_input) {", "path": "torch/csrc/autograd/engine.cpp", "position": 171, "original_position": 150, "commit_id": "6336300880349038c5bf6f5dfe3b37864eb39acb", "original_commit_id": "d01ba88a0dec1960c06e4817fc746576335728fb", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I originally had it outside the if, but it broke the MaxUnpooling test. (It returns a None grad_input for a function which requires_grad).\r\n\r\nThere's probably a better way, which involves if checking if the previous function requires a gradient for *this output*, but we're not keeping track of that currently.", "created_at": "2017-03-01T23:04:22Z", "updated_at": "2018-11-23T15:32:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/881#discussion_r103812960", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/881", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/103812960"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/881#discussion_r103812960"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/881"}}, "body_html": "<p>I originally had it outside the if, but it broke the MaxUnpooling test. (It returns a None grad_input for a function which requires_grad).</p>\n<p>There's probably a better way, which involves if checking if the previous function requires a gradient for <em>this output</em>, but we're not keeping track of that currently.</p>", "body_text": "I originally had it outside the if, but it broke the MaxUnpooling test. (It returns a None grad_input for a function which requires_grad).\nThere's probably a better way, which involves if checking if the previous function requires a gradient for this output, but we're not keeping track of that currently.", "in_reply_to_id": 103778192}