{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104141106", "pull_request_review_id": 24947698, "id": 104141106, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNDE0MTEwNg==", "diff_hunk": "@@ -82,106 +288,75 @@ auto Engine::backward(const variable_list& variables,\n       }\n     }\n   }\n+\n   for (auto& entry: creator_grad) {\n     const auto& creator = entry.first;\n-    auto& buf = entry.second; // WARNING: this is nullptr if !creator->requires_grad\n     creators.push_back(creator.get());\n     if (creator->requires_grad) {\n-      ready.emplace_back(creator, std::move(*buf));\n+      // NOTE: buf is null if creator doesn't require gradient\n+      auto& buf = entry.second;\n+      auto& queue = ready_queue(buf->device());\n+      queue.push_front(FunctionTask(&task, creator, std::move(*buf)));\n+      task.node_requires_grad = true;\n     }\n   }\n-  creator_grad.clear(); // Clear the shared pointers\n \n-  auto dependencies = compute_dependencies(std::move(creators), ready);\n-\n-  if (!did_leaf_backward && ready.size() == 0) {\n-    throw std::runtime_error(\n-        \"there are no graph nodes that require computing gradients\");\n-  }\n+  return creators;\n+}\n \n-  std::unordered_map<Function*, GradBuffer> not_ready;\n-  while (ready.size() > 0) {\n-    auto ready_pair = std::move(ready.back()); ready.pop_back();\n-    auto& fn = ready_pair.first;\n+auto Engine::backward(const variable_list& variables,\n+                      tensor_list& grad_variables,\n+                      bool retain_variables) -> void {\n+  static std::once_flag once_flag;\n+  std::call_once(once_flag, &Engine::start_threads, this);\n \n-    auto grad_inputs = fn->apply(GradBuffer::variables(std::move(ready_pair.second)));\n-    if (!retain_variables) {\n-      fn->releaseVariables();\n-    }\n+  BackwardTask backward_task(retain_variables);\n+  std::unique_lock<std::mutex> lock(backward_task.mutex);\n \n-    if (grad_inputs.size() != fn->previous_functions.size()) {\n-      std::string msg(\"Function returned an invalid number of gradients - expected \");\n-      msg += fn->previous_functions.size();\n-      msg += \",  but got \";\n-      msg += grad_inputs.size();\n-      throw std::runtime_error(msg);\n-    }\n+  // Find the unique creators and backprop into variables which don't have creators.\n+  auto creators = find_creators(variables, grad_variables, backward_task);\n \n-    int size = grad_inputs.size();\n-    for (int i = 0; i < size; ++i) {\n-      auto& grad_input = grad_inputs[i];\n-      auto& prev_fn = fn->previous_functions[i].first;\n-      int output_nr = fn->previous_functions[i].second;\n+  // Search the graph and find all stochastic functions. Append them to the queue.\n+  find_stochastic_functions(creators, backward_task);\n \n-      // null inputs have no previous_function and we skip them here\n-      if (!prev_fn) {\n-        continue;\n-      }\n+  if (!backward_task.node_requires_grad) {\n+    throw std::runtime_error(\n+      \"there are no graph nodes that require computing gradients\");\n+  }\n \n-      if (auto var = dynamic_cast<Variable*>(prev_fn.get())) {\n-        if (var->requires_grad) {\n-          var->backward(grad_input);\n-        }\n-        continue;\n-      }\n+  // Now compute the dependencies for each function which requires grad\n+  compute_dependencies(std::move(creators), backward_task);\n \n-      // Stochastic functions are placed in the ready queue by\n-      // compute_dependencies, so we can skip them here.\n-      if (prev_fn->is_stochastic || !prev_fn->requires_grad) {\n-        continue;\n-      }\n+  // wait for all tasks to complete\n+  backward_task.not_done.wait(lock, [&backward_task]{\n+    return backward_task.outstanding_tasks.load() == 0;\n+  });\n \n-      // Check if the function is ready for backward\n-      bool is_ready = false;\n-      auto it = dependencies.find(prev_fn.get());\n-      if (it == dependencies.end()) {\n-        throw std::runtime_error(\"dependency not found\");\n-      } else if (--it->second == 0) {\n-        dependencies.erase(it);\n-        is_ready = true;\n-      }\n-\n-      auto not_ready_it = not_ready.find(prev_fn.get());\n-      if (is_ready) {\n-        if (not_ready_it == not_ready.end()) {\n-          // The function is ready and no buffers have been allocated for it\n-          GradBuffer prev_buffer(prev_fn->num_outputs);\n-          prev_buffer.addGrad(output_nr, std::move(grad_input));\n-          ready.emplace_front(prev_fn, std::move(prev_buffer));\n-        } else {\n-          // The function is ready and it already has a buffer allocated.\n-          auto prev_buffer = std::move(not_ready_it->second);\n-          not_ready.erase(not_ready_it);\n-          prev_buffer.addGrad(output_nr, std::move(grad_input));\n-          ready.emplace_front(prev_fn, std::move(prev_buffer));\n-        }\n-      } else {\n-        // Allocate a buffer if necessary and accumulate gradient\n-        if (not_ready_it == not_ready.end()) {\n-          GradBuffer prev_buffer(prev_fn->num_outputs);\n-          prev_buffer.addGrad(output_nr, std::move(grad_input));\n-          not_ready.emplace(prev_fn.get(), std::move(prev_buffer));\n-        } else {\n-          auto &prev_buffer = not_ready_it->second;\n-          prev_buffer.addGrad(output_nr, std::move(grad_input));\n-        }\n-      }\n-    }\n+  // check for an exception while running backwards\n+  if (backward_task.has_error.load()) {", "path": "torch/csrc/autograd/engine.cpp", "position": 417, "original_position": 417, "commit_id": "6336300880349038c5bf6f5dfe3b37864eb39acb", "original_commit_id": "732f464faa2a72534be9a4bfcf848d06cde9e87c", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "nit: `std::atomic` always puts a memory fece around reads, so there's no need to call `load()` explicitly. But we can leave that ", "created_at": "2017-03-03T12:07:35Z", "updated_at": "2018-11-23T15:32:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/881#discussion_r104141106", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/881", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104141106"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/881#discussion_r104141106"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/881"}}, "body_html": "<p>nit: <code>std::atomic</code> always puts a memory fece around reads, so there's no need to call <code>load()</code> explicitly. But we can leave that</p>", "body_text": "nit: std::atomic always puts a memory fece around reads, so there's no need to call load() explicitly. But we can leave that"}