{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104059840", "pull_request_review_id": 24874462, "id": 104059840, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNDA1OTg0MA==", "diff_hunk": "@@ -0,0 +1,503 @@\n+#include \"convolution.h\"\n+\n+#include \"torch/csrc/autograd/variable.h\"\n+#include \"torch/csrc/nn/THNN_generic.h\"\n+#include \"torch/csrc/utils/auto_gpu.h\"\n+\n+#ifdef WITH_CUDNN\n+#include \"torch/csrc/cudnn/Conv.h\"\n+#include \"torch/csrc/cudnn/Handles.h\"\n+#include \"torch/csrc/cudnn/Types.h\"\n+extern THCState* state;\n+using namespace torch::cudnn;\n+#endif\n+\n+using namespace torch::nn;\n+using thpp::Tensor;\n+using torch::cudnn::Convolution;\n+using tensor_pair = std::pair<std::unique_ptr<Tensor>, std::unique_ptr<Tensor>>;\n+\n+namespace torch { namespace autograd {\n+\n+auto ConvParams::is_dilated() const -> bool {\n+  bool is_dilated = false;\n+  for (int d : dilation) {\n+    is_dilated |= (d != 1);\n+  }\n+  return is_dilated;\n+}\n+\n+auto ConvParams::view1d_as_2d() -> void {\n+  if (stride.size() == 1) {\n+    stride.insert(stride.begin(), 1);\n+    padding.insert(padding.begin(), 0);\n+    dilation.insert(dilation.begin(), 1);\n+    output_padding.insert(output_padding.begin(), 0);\n+  }\n+}\n+\n+auto ConvForward::output_size(Tensor& input, Tensor& weight) -> std::vector<long> {\n+  auto in_size = input.sizes();\n+  auto weight_size = weight.sizes();\n+  auto dim = input.nDim();\n+\n+  std::vector<long> output_size(dim);\n+  output_size[0] = in_size[0];\n+  output_size[1] = transposed ? weight_size[1] * groups : weight_size[0];\n+  for (int d = 2; d < dim; ++d) {\n+    int kernel = dilation[d - 2] * (weight_size[d] - 1) + 1;\n+    if (transposed) {\n+      output_size[d] = (in_size[d] - 1) * stride[d - 2] - (2 * padding[d - 2]) +\n+                       kernel + output_padding[d - 2];\n+    } else {\n+      output_size[d] = (in_size[d] + (2 * padding[d - 2]) - kernel) / stride[d - 2] + 1;\n+    }\n+  }\n+  return output_size;\n+}\n+\n+static std::unique_ptr<Tensor> subtensor(Tensor* tensor, int dim, int groups, int g);\n+\n+static std::unique_ptr<Tensor> compute_output(\n+  Tensor* input, Tensor* weight, Tensor* bias, Tensor* columns, Tensor* ones,\n+  const std::vector<long>& kernel_size, const ConvParams& params);\n+\n+static std::unique_ptr<Tensor> compute_grad_input(\n+  Tensor* input, Tensor* grad_output, Tensor* weight, Tensor* columns, Tensor* ones,\n+  const std::vector<long>& kernel_size, const ConvParams& params);\n+\n+static tensor_pair compute_grad_params(\n+  Tensor* input, Tensor* grad_output, Tensor* weight, Tensor* bias, Tensor* columns, Tensor* ones,\n+  const std::vector<long>& kernel_size, const ConvBackward& params);\n+\n+static std::unique_ptr<Tensor> cat(const tensor_list& tensors, int dim);\n+\n+static auto view4d(const Tensor& tensor) -> std::unique_ptr<Tensor> {\n+  if (tensor.nDim() != 3) throw std::runtime_error(\"expected 3D tensor\");\n+  auto result = tensor.newTensor();\n+  result->unsqueeze(tensor, 2);\n+  return result;\n+}\n+\n+static auto view3d(const Tensor& tensor) -> std::unique_ptr<Tensor> {\n+  if (tensor.nDim() != 4) throw std::runtime_error(\"expected 4D tensor\");\n+  auto result = tensor.newTensor();\n+  result->squeeze(tensor, 2);\n+  return result;\n+}\n+\n+auto ConvForward::apply(const variable_list& inputs) -> variable_list {\n+  if (inputs.size() != 3) throw std::runtime_error(\"expected three inputs\");\n+\n+  AutoGPU guard(inputs[0]->data->getDevice());\n+  auto input = inputs[0]->data->contiguous();\n+  std::unique_ptr<Tensor> weight(inputs[1]->data->clone_shallow());\n+  std::unique_ptr<Tensor> bias(inputs[2] ? inputs[2]->data->clone_shallow() : nullptr);\n+\n+  int k = input->nDim();\n+  if (k == 3) {\n+    view1d_as_2d();\n+    input = view4d(*input);\n+    weight = view4d(*weight);\n+  }\n+\n+  auto weight_size = weight->sizes();\n+  std::vector<long> kernel_size(weight_size.begin() + 2, weight_size.end());\n+\n+  bool use_cudnn = false;\n+#ifdef WITH_CUDNN\n+  // TODO: cuDNN version\n+  use_cudnn = (input->isCuda() && (!is_dilated() || false));\n+#endif\n+\n+  std::unique_ptr<Tensor> output;\n+  tensor_list columns(groups);\n+  tensor_list ones(groups);\n+  std::unique_ptr<Convolution> convolution;\n+\n+  if (use_cudnn) {\n+#ifdef WITH_CUDNN\n+    output = input->newTensor();\n+    output->resize(output_size(*input, *weight));\n+    if (transposed) {\n+      convolution.reset(cudnn_convolution_transpose_full_forward(\n+          state, torch::cudnn::getCudnnHandle(), torch::cudnn::getCudnnDataType(*input),\n+          (THVoidTensor*)input->cdata(), (THVoidTensor*)weight->cdata(),\n+          bias ? (THVoidTensor*)bias->cdata() : nullptr, (THVoidTensor*)output->cdata(),\n+          padding, stride, dilation, groups, benchmark));\n+    } else {\n+      convolution.reset(cudnn_convolution_full_forward(\n+          state, torch::cudnn::getCudnnHandle(), torch::cudnn::getCudnnDataType(*input),\n+          (THVoidTensor*)input->cdata(), (THVoidTensor*)weight->cdata(),\n+          bias ? (THVoidTensor*)bias->cdata() : nullptr, (THVoidTensor*)output->cdata(),\n+          padding, stride, dilation, groups, benchmark));\n+    }\n+#endif\n+  } else {\n+    for (int g = 0; g < groups; ++g) {\n+      columns[g] = input->newTensor();\n+      ones[g] = input->newTensor();\n+    }\n+    if (groups == 1) {\n+      output = compute_output(\n+          input.get(), weight.get(), bias.get(),\n+          columns[0].get(), ones[0].get(), kernel_size, *this);\n+    } else {\n+      tensor_list outputs(groups);\n+      for (int g = 0; g < groups; ++g) {\n+        auto input_g = subtensor(input.get(), 1, groups, g);\n+        auto weight_g = subtensor(weight.get(), 0, groups, g);\n+        auto bias_g = subtensor(bias.get(), 0, groups, g);\n+        outputs[g] = compute_output(\n+            input_g.get(), weight_g.get(), bias_g.get(),\n+            columns[g].get(), ones[g].get(), kernel_size, *this);\n+      }\n+      output = cat(outputs, 1);\n+    }\n+  }\n+\n+  if (k == 3) {\n+    output = view3d(*output);\n+  }\n+\n+  auto creator = std::make_shared<ConvBackward>(\n+      flags(inputs), *this,\n+      inputs[0]->save(), inputs[1]->save(), Variable::save_opt(inputs[2].get()),\n+      std::move(columns), std::move(ones), std::move(convolution));\n+\n+  variable_list results(1);\n+  results[0] = std::make_shared<Variable>(std::move(output), creator);\n+  return results;\n+};\n+\n+auto ConvBackward::apply(const variable_list& grad_outputs) -> variable_list {\n+  if (grad_outputs.size() != 1) throw std::runtime_error(\"expected one grad_output\");\n+\n+  AutoGPU guard(input_.data->getDevice());\n+\n+  auto input = input_.unpack()->contiguous();\n+  std::unique_ptr<Tensor> weight(weight_.unpack()->clone_shallow());\n+  std::unique_ptr<Tensor> bias(bias_.unpack() ? bias_.unpack()->clone_shallow() : nullptr);\n+  auto grad_output = grad_outputs[0]->data->contiguous();\n+\n+  int k = input->nDim();\n+  if (k == 3) {\n+    input = view4d(*input);\n+    weight = view4d(*weight);\n+    grad_output = view4d(*grad_output);\n+  }\n+\n+  auto weight_size = weight->sizes();\n+  std::vector<long> kernel_size(weight_size.begin() + 2, weight_size.end());\n+\n+  bool use_cudnn = false;\n+#ifdef WITH_CUDNN\n+  // TODO: cuDNN version", "path": "torch/csrc/autograd/functions/convolution.cpp", "position": null, "original_position": 195, "commit_id": "6336300880349038c5bf6f5dfe3b37864eb39acb", "original_commit_id": "b132877316ccb20083d70b969d96c0a7d6420cc2", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "As above. It seems that it's implemented.", "created_at": "2017-03-02T23:49:06Z", "updated_at": "2018-11-23T15:32:41Z", "html_url": "https://github.com/pytorch/pytorch/pull/881#discussion_r104059840", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/881", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104059840"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/881#discussion_r104059840"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/881"}}, "body_html": "<p>As above. It seems that it's implemented.</p>", "body_text": "As above. It seems that it's implemented."}