{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362102549", "html_url": "https://github.com/pytorch/pytorch/issues/4890#issuecomment-362102549", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4890", "id": 362102549, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjEwMjU0OQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T23:12:03Z", "updated_at": "2018-01-31T23:12:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So here is what's going on. After the first backward, every model parameters has a <code>.grad</code> attribute. Calling with <code>zero_grad()</code> only set its values to zero rather than deleting it. That is why you are seeing exact doubled memory usage after first iteration. You can verify this by changing <code>model.zero_grad()</code> to</p>\n<pre><code>for p in model.parameters();\n  p.grad = None\n</code></pre>\n<p>and move it to after <code>backward()</code> and before printing.</p>\n<p>Whether it should set these <code>.grad</code> to <code>None</code> is debatable. The current scheme allocates unnecessary memory and launches unnecessary kernels. But <code>None</code> and zero conceptually are different. As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> mentioned, it would be great if we can special case zero tensors.</p>", "body_text": "So here is what's going on. After the first backward, every model parameters has a .grad attribute. Calling with zero_grad() only set its values to zero rather than deleting it. That is why you are seeing exact doubled memory usage after first iteration. You can verify this by changing model.zero_grad() to\nfor p in model.parameters();\n  p.grad = None\n\nand move it to after backward() and before printing.\nWhether it should set these .grad to None is debatable. The current scheme allocates unnecessary memory and launches unnecessary kernels. But None and zero conceptually are different. As @ezyang mentioned, it would be great if we can special case zero tensors.", "body": "So here is what's going on. After the first backward, every model parameters has a `.grad` attribute. Calling with `zero_grad()` only set its values to zero rather than deleting it. That is why you are seeing exact doubled memory usage after first iteration. You can verify this by changing `model.zero_grad()` to \r\n```\r\nfor p in model.parameters();\r\n  p.grad = None\r\n```\r\nand move it to after `backward()` and before printing.\r\n\r\nWhether it should set these `.grad` to `None` is debatable. The current scheme allocates unnecessary memory and launches unnecessary kernels. But `None` and zero conceptually are different. As @ezyang mentioned, it would be great if we can special case zero tensors."}