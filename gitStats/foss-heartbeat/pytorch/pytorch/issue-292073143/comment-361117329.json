{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/361117329", "html_url": "https://github.com/pytorch/pytorch/issues/4890#issuecomment-361117329", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4890", "id": 361117329, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTExNzMyOQ==", "user": {"login": "Smerity", "id": 32325, "node_id": "MDQ6VXNlcjMyMzI1", "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Smerity", "html_url": "https://github.com/Smerity", "followers_url": "https://api.github.com/users/Smerity/followers", "following_url": "https://api.github.com/users/Smerity/following{/other_user}", "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}", "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions", "organizations_url": "https://api.github.com/users/Smerity/orgs", "repos_url": "https://api.github.com/users/Smerity/repos", "events_url": "https://api.github.com/users/Smerity/events{/privacy}", "received_events_url": "https://api.github.com/users/Smerity/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-29T01:35:18Z", "updated_at": "2018-01-29T01:35:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hey <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>,</p>\n<p>As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> mentioned, I shifted the training of a batch into the separate function <a href=\"https://github.com/Smerity/examples/blob/8153be2a2f61d1db2a0e3b7f3968f164ec97a297/word_language_model/main.py#L144\">run_train_batch</a> to avoid scoping issues. This ensures all the variables are deleted as they fall out of the <code>run_train_batch</code> scope.  The function only returns <code>hidden</code> (after having <code>repackage_hidden(hidden)</code> called) and a raw float of the loss for that batch.</p>\n<p>I experimented with using <code>del</code> in my larger model and had the same memory issue.</p>\n<p>Just as a sanity check, I inserted this code at the bottom of the <code>run_train_batch</code> function:</p>\n<pre><code>    hidden = repackage_hidden(hidden)\n    raw_loss = loss.data\n    del output\n    del loss\n    del data\n    del targets\n    return hidden, raw_loss\n</code></pre>\n<p>and still experienced the same issue:</p>\n<pre><code>Before model run: 645\nAfter model run: 1270\nBefore model run: 1270\nAfter model run: 1270\n</code></pre>\n<p>For <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a>, I also tried using <code>detach</code> rather than the discouraged method but that had no effect. Admittedly <code>detach</code> should be preferred now :)</p>", "body_text": "Hey @apaszke,\nAs @SsnL mentioned, I shifted the training of a batch into the separate function run_train_batch to avoid scoping issues. This ensures all the variables are deleted as they fall out of the run_train_batch scope.  The function only returns hidden (after having repackage_hidden(hidden) called) and a raw float of the loss for that batch.\nI experimented with using del in my larger model and had the same memory issue.\nJust as a sanity check, I inserted this code at the bottom of the run_train_batch function:\n    hidden = repackage_hidden(hidden)\n    raw_loss = loss.data\n    del output\n    del loss\n    del data\n    del targets\n    return hidden, raw_loss\n\nand still experienced the same issue:\nBefore model run: 645\nAfter model run: 1270\nBefore model run: 1270\nAfter model run: 1270\n\nFor @SsnL, I also tried using detach rather than the discouraged method but that had no effect. Admittedly detach should be preferred now :)", "body": "Hey @apaszke,\r\n\r\nAs @SsnL mentioned, I shifted the training of a batch into the separate function [run_train_batch](https://github.com/Smerity/examples/blob/8153be2a2f61d1db2a0e3b7f3968f164ec97a297/word_language_model/main.py#L144) to avoid scoping issues. This ensures all the variables are deleted as they fall out of the `run_train_batch` scope.  The function only returns `hidden` (after having `repackage_hidden(hidden)` called) and a raw float of the loss for that batch.\r\n\r\nI experimented with using `del` in my larger model and had the same memory issue.\r\n\r\nJust as a sanity check, I inserted this code at the bottom of the `run_train_batch` function:\r\n\r\n```\r\n    hidden = repackage_hidden(hidden)\r\n    raw_loss = loss.data\r\n    del output\r\n    del loss\r\n    del data\r\n    del targets\r\n    return hidden, raw_loss\r\n```\r\n\r\nand still experienced the same issue:\r\n\r\n```\r\nBefore model run: 645\r\nAfter model run: 1270\r\nBefore model run: 1270\r\nAfter model run: 1270\r\n```\r\n\r\nFor @SsnL, I also tried using `detach` rather than the discouraged method but that had no effect. Admittedly `detach` should be preferred now :)"}