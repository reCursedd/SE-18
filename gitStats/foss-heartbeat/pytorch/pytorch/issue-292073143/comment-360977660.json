{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/360977660", "html_url": "https://github.com/pytorch/pytorch/issues/4890#issuecomment-360977660", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4890", "id": 360977660, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDk3NzY2MA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-27T11:10:22Z", "updated_at": "2018-01-27T11:11:00Z", "author_association": "MEMBER", "body_html": "<p>Can you try adding <code>del output, loss</code> (in general all intermediates that won't be used in the next iteration of a loop) at the end of <a href=\"https://github.com/pytorch/examples/blob/master/word_language_model/main.py#L151\">this loop</a>?</p>\n<p>I think it's a problem created by Python's scoping rules - <code>for</code> loop uses the outer scope and all variables persist across iterations. In this case, the first iteration runs normally, but when you run the second one <code>output</code> and <code>loss</code> are still keeping the previous iteration graph alive until you actually reassign them.</p>\n<p>It shouldn't be a big issue, as we should be dropping all larger buffers progressively during backward, so it might actually be a bug in our current codegen for autograd. This would also explain why you don't see those objects in the explorer - they're living entirely on the C++ side. Let me know if that change helps!</p>", "body_text": "Can you try adding del output, loss (in general all intermediates that won't be used in the next iteration of a loop) at the end of this loop?\nI think it's a problem created by Python's scoping rules - for loop uses the outer scope and all variables persist across iterations. In this case, the first iteration runs normally, but when you run the second one output and loss are still keeping the previous iteration graph alive until you actually reassign them.\nIt shouldn't be a big issue, as we should be dropping all larger buffers progressively during backward, so it might actually be a bug in our current codegen for autograd. This would also explain why you don't see those objects in the explorer - they're living entirely on the C++ side. Let me know if that change helps!", "body": "Can you try adding `del output, loss` (in general all intermediates that won't be used in the next iteration of a loop) at the end of [this loop](https://github.com/pytorch/examples/blob/master/word_language_model/main.py#L151)?\r\n\r\nI think it's a problem created by Python's scoping rules - `for` loop uses the outer scope and all variables persist across iterations. In this case, the first iteration runs normally, but when you run the second one `output` and `loss` are still keeping the previous iteration graph alive until you actually reassign them.\r\n\r\nIt shouldn't be a big issue, as we should be dropping all larger buffers progressively during backward, so it might actually be a bug in our current codegen for autograd. This would also explain why you don't see those objects in the explorer - they're living entirely on the C++ side. Let me know if that change helps!"}