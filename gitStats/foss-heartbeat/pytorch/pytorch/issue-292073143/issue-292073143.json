{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4890", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4890/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4890/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4890/events", "html_url": "https://github.com/pytorch/pytorch/issues/4890", "id": 292073143, "node_id": "MDU6SXNzdWUyOTIwNzMxNDM=", "number": 4890, "title": "Model memory leak according to `torch.cuda.memory_allocated` but no matching tracked objects (with example)", "user": {"login": "Smerity", "id": 32325, "node_id": "MDQ6VXNlcjMyMzI1", "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Smerity", "html_url": "https://github.com/Smerity", "followers_url": "https://api.github.com/users/Smerity/followers", "following_url": "https://api.github.com/users/Smerity/following{/other_user}", "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}", "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions", "organizations_url": "https://api.github.com/users/Smerity/orgs", "repos_url": "https://api.github.com/users/Smerity/repos", "events_url": "https://api.github.com/users/Smerity/events{/privacy}", "received_events_url": "https://api.github.com/users/Smerity/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-01-27T00:44:21Z", "updated_at": "2018-01-31T23:41:00Z", "closed_at": "2018-01-31T23:36:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This memory issue seems to double the required memory for a model but does so without leaving an obvious trace as to where that memory goes. This is doubly confusing as to double the model size would require doubling the model's weights too. The <a href=\"https://github.com/Smerity/examples/blob/wlm_memory_bug/word_language_model/main.py#L172\">example code</a> demonstrates this issue using a minimally modified word language modeling example with both LSTM and QRNN (i.e. non-cuDNN based) RNNs.</p>\n<p>LSTM:</p>\n<pre><code>/home/smerity/smer-examples/word_language_model# python main.py --cuda --emsize 2800 --nhid 2800 --nlayers 3 --dropout 0.65 --epochs 40 --tied\nBefore model run: 1098\nBefore model run: 2173\nBefore model run: 2173\n</code></pre>\n<p>QRNN (shows issue is independent of cuDNN RNN):</p>\n<pre><code>/home/smerity/smer-examples/word_language_model# python main.py --cuda --emsize 2800 --nhid 2800 --nlayers 3 --dropout 0.65 --epochs 40 --tied --model QRNN\nBefore model run: 645\nBefore model run: 1270\nBefore model run: 1270\n</code></pre>\n<p>When using a helpful <a href=\"https://github.com/Smerity/examples/blob/wlm_memory_bug/word_language_model/memtree.py\">object tree</a> written by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11729078\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jekbradbury\">@jekbradbury</a>, we can see (log at bottom) that no extra Python objects are added that refer to tensors or Variables, yet the memory still increases, even when calling <code>torch.cuda.empty_cache</code> immediately beforehand.</p>\n<p>The example is minimally changed from the Word Language Modeling example and was constructed from a larger model which has the same issue. The changes involve (a) adding QRNN for a non-cuDNN example, (b) moving training of a single batch into its own function so Python objects would be guaranteed to fall out of scope and cause CUDA deletions, and (c) memory diagnostics code.</p>\n<p>At this stage I believe something iffy is going on, though at which level I'm not certain, and I also don't rule out my own stupidity :) I also looked at the memory usage before and after every forward/backward via registering a hook and can see these are not buffers that are reused as the memory increases from the \"new\" baseline, returning to that baseline later.</p>\n<p>Happy to help work the issue with anyone who is interested in solving it. Thanks! &lt;3</p>\n<hr>\n<p>Object tree code (which is commented out by default due to verbosity):</p>\n<pre><code>/home/smerity/smer-examples/word_language_model# python main.py --cuda --emsize 2800 --nhid 2800 --nlayers 3 --dropout 0.65 --epochs 40 --tied --model QRNN\n140349451688920 Parameter of size (8400, 2800) with references held by:\n  140348625257064 OrderedDict in which its key is weight\n  140348625607560 cell\n140348625230328 Parameter of size (8400, 2800) with references held by:\n  140348625258696 OrderedDict in which its key is weight\n  140348625607560 cell\n140348625230568 Parameter of size (8400, 2800) with references held by:\n  140348625260328 OrderedDict in which its key is weight\n  140348625607560 cell\n140348625245256 FloatTensor of size (33278, 2800) with references held by:\n  140348625607560 cell\n  140349451687640 Parameter of size (33278, 2800)\n140349451877832 FloatTensor of size (8400, 2800) with references held by:\n  140349451688920 Parameter of size (8400, 2800)\n  140348625607560 cell\n140348625731848 FloatTensor of size (8400, 2800) with references held by:\n  140348625230328 Parameter of size (8400, 2800)\n  140348625607560 cell\n140348625733064 FloatTensor of size (8400, 2800) with references held by:\n  140348625230568 Parameter of size (8400, 2800)\n  140348625607560 cell\n140349451876168 LongTensor of size (2088628,) with references held by:\n  140348625607560 cell\n  140349451835504 dict in which its key is train\n140349451876552 LongTensor of size (104431, 20) with references held by:\n  140348625607560 cell\n  140350689706200 dict in which its key is train_data\n140349451687640 Parameter of size (33278, 2800) with references held by:\n  140348625298840 OrderedDict in which its key is weight\n  140348625607560 cell\n  140349451881464 OrderedDict in which its key is weight\nBefore model run: 645\nAfter model run: 1270\nBefore model run: 2173\n140349451688920 Parameter of size (8400, 2800) with references held by:\n  140348625257064 OrderedDict in which its key is weight\n  140348625607656 cell\n140348625230328 Parameter of size (8400, 2800) with references held by:\n  140348625258696 OrderedDict in which its key is weight\n  140348625607656 cell\n140348625230568 Parameter of size (8400, 2800) with references held by:\n  140348625260328 OrderedDict in which its key is weight\n  140348625607656 cell\n140348625245256 FloatTensor of size (33278, 2800) with references held by:\n  140348625607656 cell\n  140349451687640 Parameter of size (33278, 2800)\n140349451877832 FloatTensor of size (8400, 2800) with references held by:\n  140349451688920 Parameter of size (8400, 2800)\n  140348625607656 cell\n140348625731848 FloatTensor of size (8400, 2800) with references held by:\n  140348625230328 Parameter of size (8400, 2800)\n  140348625607656 cell\n140348625733064 FloatTensor of size (8400, 2800) with references held by:\n  140348625230568 Parameter of size (8400, 2800)\n  140348625607656 cell\n140349451876168 LongTensor of size (2088628,) with references held by:\n  140348625607656 cell\n  140349451835504 dict in which its key is train\n140349451876552 LongTensor of size (104431, 20) with references held by:\n  140348625607656 cell\n  140350689706200 dict in which its key is train_data\n140349451687640 Parameter of size (33278, 2800) with references held by:\n  140348625298840 OrderedDict in which its key is weight\n  140348625607656 cell\n  140349451881464 OrderedDict in which its key is weight\nBefore model run: 1270\nAfter model run: 1270\n</code></pre>\n<hr>\n<p>The Docker image was built recently (last day or two) from <code>docker build -t pytorch/pytorch9 -f tools/docker/Dockerfile9 .</code></p>\n<ul>\n<li>OS: Linux (Ubuntu)</li>\n<li>PyTorch version: 0.4.0a0+0844b5b</li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: release 9.0, V9.0.176</li>\n<li>GPU models and configuration: NVIDIA Volta</li>\n<li>GCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)</li>\n</ul>", "body_text": "This memory issue seems to double the required memory for a model but does so without leaving an obvious trace as to where that memory goes. This is doubly confusing as to double the model size would require doubling the model's weights too. The example code demonstrates this issue using a minimally modified word language modeling example with both LSTM and QRNN (i.e. non-cuDNN based) RNNs.\nLSTM:\n/home/smerity/smer-examples/word_language_model# python main.py --cuda --emsize 2800 --nhid 2800 --nlayers 3 --dropout 0.65 --epochs 40 --tied\nBefore model run: 1098\nBefore model run: 2173\nBefore model run: 2173\n\nQRNN (shows issue is independent of cuDNN RNN):\n/home/smerity/smer-examples/word_language_model# python main.py --cuda --emsize 2800 --nhid 2800 --nlayers 3 --dropout 0.65 --epochs 40 --tied --model QRNN\nBefore model run: 645\nBefore model run: 1270\nBefore model run: 1270\n\nWhen using a helpful object tree written by @jekbradbury, we can see (log at bottom) that no extra Python objects are added that refer to tensors or Variables, yet the memory still increases, even when calling torch.cuda.empty_cache immediately beforehand.\nThe example is minimally changed from the Word Language Modeling example and was constructed from a larger model which has the same issue. The changes involve (a) adding QRNN for a non-cuDNN example, (b) moving training of a single batch into its own function so Python objects would be guaranteed to fall out of scope and cause CUDA deletions, and (c) memory diagnostics code.\nAt this stage I believe something iffy is going on, though at which level I'm not certain, and I also don't rule out my own stupidity :) I also looked at the memory usage before and after every forward/backward via registering a hook and can see these are not buffers that are reused as the memory increases from the \"new\" baseline, returning to that baseline later.\nHappy to help work the issue with anyone who is interested in solving it. Thanks! <3\n\nObject tree code (which is commented out by default due to verbosity):\n/home/smerity/smer-examples/word_language_model# python main.py --cuda --emsize 2800 --nhid 2800 --nlayers 3 --dropout 0.65 --epochs 40 --tied --model QRNN\n140349451688920 Parameter of size (8400, 2800) with references held by:\n  140348625257064 OrderedDict in which its key is weight\n  140348625607560 cell\n140348625230328 Parameter of size (8400, 2800) with references held by:\n  140348625258696 OrderedDict in which its key is weight\n  140348625607560 cell\n140348625230568 Parameter of size (8400, 2800) with references held by:\n  140348625260328 OrderedDict in which its key is weight\n  140348625607560 cell\n140348625245256 FloatTensor of size (33278, 2800) with references held by:\n  140348625607560 cell\n  140349451687640 Parameter of size (33278, 2800)\n140349451877832 FloatTensor of size (8400, 2800) with references held by:\n  140349451688920 Parameter of size (8400, 2800)\n  140348625607560 cell\n140348625731848 FloatTensor of size (8400, 2800) with references held by:\n  140348625230328 Parameter of size (8400, 2800)\n  140348625607560 cell\n140348625733064 FloatTensor of size (8400, 2800) with references held by:\n  140348625230568 Parameter of size (8400, 2800)\n  140348625607560 cell\n140349451876168 LongTensor of size (2088628,) with references held by:\n  140348625607560 cell\n  140349451835504 dict in which its key is train\n140349451876552 LongTensor of size (104431, 20) with references held by:\n  140348625607560 cell\n  140350689706200 dict in which its key is train_data\n140349451687640 Parameter of size (33278, 2800) with references held by:\n  140348625298840 OrderedDict in which its key is weight\n  140348625607560 cell\n  140349451881464 OrderedDict in which its key is weight\nBefore model run: 645\nAfter model run: 1270\nBefore model run: 2173\n140349451688920 Parameter of size (8400, 2800) with references held by:\n  140348625257064 OrderedDict in which its key is weight\n  140348625607656 cell\n140348625230328 Parameter of size (8400, 2800) with references held by:\n  140348625258696 OrderedDict in which its key is weight\n  140348625607656 cell\n140348625230568 Parameter of size (8400, 2800) with references held by:\n  140348625260328 OrderedDict in which its key is weight\n  140348625607656 cell\n140348625245256 FloatTensor of size (33278, 2800) with references held by:\n  140348625607656 cell\n  140349451687640 Parameter of size (33278, 2800)\n140349451877832 FloatTensor of size (8400, 2800) with references held by:\n  140349451688920 Parameter of size (8400, 2800)\n  140348625607656 cell\n140348625731848 FloatTensor of size (8400, 2800) with references held by:\n  140348625230328 Parameter of size (8400, 2800)\n  140348625607656 cell\n140348625733064 FloatTensor of size (8400, 2800) with references held by:\n  140348625230568 Parameter of size (8400, 2800)\n  140348625607656 cell\n140349451876168 LongTensor of size (2088628,) with references held by:\n  140348625607656 cell\n  140349451835504 dict in which its key is train\n140349451876552 LongTensor of size (104431, 20) with references held by:\n  140348625607656 cell\n  140350689706200 dict in which its key is train_data\n140349451687640 Parameter of size (33278, 2800) with references held by:\n  140348625298840 OrderedDict in which its key is weight\n  140348625607656 cell\n  140349451881464 OrderedDict in which its key is weight\nBefore model run: 1270\nAfter model run: 1270\n\n\nThe Docker image was built recently (last day or two) from docker build -t pytorch/pytorch9 -f tools/docker/Dockerfile9 .\n\nOS: Linux (Ubuntu)\nPyTorch version: 0.4.0a0+0844b5b\nHow you installed PyTorch (conda, pip, source): source\nPython version: 3.6\nCUDA/cuDNN version: release 9.0, V9.0.176\nGPU models and configuration: NVIDIA Volta\nGCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)", "body": "This memory issue seems to double the required memory for a model but does so without leaving an obvious trace as to where that memory goes. This is doubly confusing as to double the model size would require doubling the model's weights too. The [example code](https://github.com/Smerity/examples/blob/wlm_memory_bug/word_language_model/main.py#L172) demonstrates this issue using a minimally modified word language modeling example with both LSTM and QRNN (i.e. non-cuDNN based) RNNs.\r\n\r\nLSTM:\r\n\r\n```\r\n/home/smerity/smer-examples/word_language_model# python main.py --cuda --emsize 2800 --nhid 2800 --nlayers 3 --dropout 0.65 --epochs 40 --tied\r\nBefore model run: 1098\r\nBefore model run: 2173\r\nBefore model run: 2173\r\n```\r\n\r\nQRNN (shows issue is independent of cuDNN RNN):\r\n\r\n```\r\n/home/smerity/smer-examples/word_language_model# python main.py --cuda --emsize 2800 --nhid 2800 --nlayers 3 --dropout 0.65 --epochs 40 --tied --model QRNN\r\nBefore model run: 645\r\nBefore model run: 1270\r\nBefore model run: 1270\r\n```\r\n\r\nWhen using a helpful [object tree](https://github.com/Smerity/examples/blob/wlm_memory_bug/word_language_model/memtree.py) written by @jekbradbury, we can see (log at bottom) that no extra Python objects are added that refer to tensors or Variables, yet the memory still increases, even when calling `torch.cuda.empty_cache` immediately beforehand.\r\n\r\nThe example is minimally changed from the Word Language Modeling example and was constructed from a larger model which has the same issue. The changes involve (a) adding QRNN for a non-cuDNN example, (b) moving training of a single batch into its own function so Python objects would be guaranteed to fall out of scope and cause CUDA deletions, and (c) memory diagnostics code.\r\n\r\nAt this stage I believe something iffy is going on, though at which level I'm not certain, and I also don't rule out my own stupidity :) I also looked at the memory usage before and after every forward/backward via registering a hook and can see these are not buffers that are reused as the memory increases from the \"new\" baseline, returning to that baseline later.\r\n\r\nHappy to help work the issue with anyone who is interested in solving it. Thanks! <3\r\n\r\n---\r\n\r\nObject tree code (which is commented out by default due to verbosity):\r\n\r\n```\r\n/home/smerity/smer-examples/word_language_model# python main.py --cuda --emsize 2800 --nhid 2800 --nlayers 3 --dropout 0.65 --epochs 40 --tied --model QRNN\r\n140349451688920 Parameter of size (8400, 2800) with references held by:\r\n  140348625257064 OrderedDict in which its key is weight\r\n  140348625607560 cell\r\n140348625230328 Parameter of size (8400, 2800) with references held by:\r\n  140348625258696 OrderedDict in which its key is weight\r\n  140348625607560 cell\r\n140348625230568 Parameter of size (8400, 2800) with references held by:\r\n  140348625260328 OrderedDict in which its key is weight\r\n  140348625607560 cell\r\n140348625245256 FloatTensor of size (33278, 2800) with references held by:\r\n  140348625607560 cell\r\n  140349451687640 Parameter of size (33278, 2800)\r\n140349451877832 FloatTensor of size (8400, 2800) with references held by:\r\n  140349451688920 Parameter of size (8400, 2800)\r\n  140348625607560 cell\r\n140348625731848 FloatTensor of size (8400, 2800) with references held by:\r\n  140348625230328 Parameter of size (8400, 2800)\r\n  140348625607560 cell\r\n140348625733064 FloatTensor of size (8400, 2800) with references held by:\r\n  140348625230568 Parameter of size (8400, 2800)\r\n  140348625607560 cell\r\n140349451876168 LongTensor of size (2088628,) with references held by:\r\n  140348625607560 cell\r\n  140349451835504 dict in which its key is train\r\n140349451876552 LongTensor of size (104431, 20) with references held by:\r\n  140348625607560 cell\r\n  140350689706200 dict in which its key is train_data\r\n140349451687640 Parameter of size (33278, 2800) with references held by:\r\n  140348625298840 OrderedDict in which its key is weight\r\n  140348625607560 cell\r\n  140349451881464 OrderedDict in which its key is weight\r\nBefore model run: 645\r\nAfter model run: 1270\r\nBefore model run: 2173\r\n140349451688920 Parameter of size (8400, 2800) with references held by:\r\n  140348625257064 OrderedDict in which its key is weight\r\n  140348625607656 cell\r\n140348625230328 Parameter of size (8400, 2800) with references held by:\r\n  140348625258696 OrderedDict in which its key is weight\r\n  140348625607656 cell\r\n140348625230568 Parameter of size (8400, 2800) with references held by:\r\n  140348625260328 OrderedDict in which its key is weight\r\n  140348625607656 cell\r\n140348625245256 FloatTensor of size (33278, 2800) with references held by:\r\n  140348625607656 cell\r\n  140349451687640 Parameter of size (33278, 2800)\r\n140349451877832 FloatTensor of size (8400, 2800) with references held by:\r\n  140349451688920 Parameter of size (8400, 2800)\r\n  140348625607656 cell\r\n140348625731848 FloatTensor of size (8400, 2800) with references held by:\r\n  140348625230328 Parameter of size (8400, 2800)\r\n  140348625607656 cell\r\n140348625733064 FloatTensor of size (8400, 2800) with references held by:\r\n  140348625230568 Parameter of size (8400, 2800)\r\n  140348625607656 cell\r\n140349451876168 LongTensor of size (2088628,) with references held by:\r\n  140348625607656 cell\r\n  140349451835504 dict in which its key is train\r\n140349451876552 LongTensor of size (104431, 20) with references held by:\r\n  140348625607656 cell\r\n  140350689706200 dict in which its key is train_data\r\n140349451687640 Parameter of size (33278, 2800) with references held by:\r\n  140348625298840 OrderedDict in which its key is weight\r\n  140348625607656 cell\r\n  140349451881464 OrderedDict in which its key is weight\r\nBefore model run: 1270\r\nAfter model run: 1270\r\n```\r\n\r\n---\r\n\r\nThe Docker image was built recently (last day or two) from `docker build -t pytorch/pytorch9 -f tools/docker/Dockerfile9 .`\r\n\r\n- OS: Linux (Ubuntu)\r\n- PyTorch version: 0.4.0a0+0844b5b\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: release 9.0, V9.0.176\r\n- GPU models and configuration: NVIDIA Volta\r\n- GCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)"}