{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6283", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6283/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6283/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6283/events", "html_url": "https://github.com/pytorch/pytorch/pull/6283", "id": 311415881, "node_id": "MDExOlB1bGxSZXF1ZXN0MTc5NTMxNjY2", "number": 6283, "title": "Add string-style devices to all tensors.", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-04-04T22:43:13Z", "updated_at": "2018-11-23T15:41:56Z", "closed_at": "2018-04-06T19:12:06Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6283", "html_url": "https://github.com/pytorch/pytorch/pull/6283", "diff_url": "https://github.com/pytorch/pytorch/pull/6283.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6283.patch"}, "body_html": "<p>Previously, tensors only had a <code>get_device</code> method which would throw an exception on a CPU tensor.   This made it necessary to if/else code that was meant to be device agnostic.</p>\n<p>This PR implements the following:</p>\n<ol>\n<li>\n<p>Adds a 'device' property to all tensors that returns a string representation of the device for all tensors.  For cpu tensors this is 'cpu'.  For cuda tensors this is 'cuda:X', where X is the cuda device ordinal.</p>\n</li>\n<li>\n<p>Adds a DeviceSpec class.  This is just a helper class for separating device_type and device_index specification and to allow partial specification.</p>\n<p>For example, you can call <code>DeviceSpec('cuda')</code>, <code>DeviceSpec('cuda:0')</code>, <code>DeviceSpec('cuda', 1)</code>.<br>\nAlso has backwards compatibility support for specifying integers (<code>DeviceSpec(1)</code>), which are treated as cuda devices.  DeviceSpecs have the following properties:</p>\n<ul>\n<li><code>device_type</code>: string representation of the device type (i.e. 'cpu' or 'cuda')</li>\n<li><code>device_index</code>: integer for the device index (None if not specified)</li>\n<li><code>cuda_device_index</code>: for backwards compatibility; behaves roughly like <code>get_device</code> did previously.  I.e. if a function previously took integers for cuda devices, it can now take DeviceSpecs (or strings), and can maintain the old functionality by calling <code>old_index = DeviceSpec(old).cuda_device_index</code>.</li>\n</ul>\n</li>\n<li>\n<p>tensor methods and torch. functions that took integer devices can now take integers, strings, or DeviceSpecs.  For example:<br>\n<code>torch.randn((2,3), dtype=torch.cuda.float32, device='cuda:1')</code></p>\n</li>\n</ol>\n<p>TODO in future PRs:</p>\n<ul>\n<li>Split out cuda from dtype so you don't need to overspecify cuda-ness</li>\n<li>We currently only support strings/DeviceSpecs in tensor methods and torch. functions.  We should have equivalents <code>torch.cuda.device(...)</code>, <code>torch.cuda.device_of</code>, etc. at the torch. level that work on strings/DeviceSpecs</li>\n</ul>", "body_text": "Previously, tensors only had a get_device method which would throw an exception on a CPU tensor.   This made it necessary to if/else code that was meant to be device agnostic.\nThis PR implements the following:\n\n\nAdds a 'device' property to all tensors that returns a string representation of the device for all tensors.  For cpu tensors this is 'cpu'.  For cuda tensors this is 'cuda:X', where X is the cuda device ordinal.\n\n\nAdds a DeviceSpec class.  This is just a helper class for separating device_type and device_index specification and to allow partial specification.\nFor example, you can call DeviceSpec('cuda'), DeviceSpec('cuda:0'), DeviceSpec('cuda', 1).\nAlso has backwards compatibility support for specifying integers (DeviceSpec(1)), which are treated as cuda devices.  DeviceSpecs have the following properties:\n\ndevice_type: string representation of the device type (i.e. 'cpu' or 'cuda')\ndevice_index: integer for the device index (None if not specified)\ncuda_device_index: for backwards compatibility; behaves roughly like get_device did previously.  I.e. if a function previously took integers for cuda devices, it can now take DeviceSpecs (or strings), and can maintain the old functionality by calling old_index = DeviceSpec(old).cuda_device_index.\n\n\n\ntensor methods and torch. functions that took integer devices can now take integers, strings, or DeviceSpecs.  For example:\ntorch.randn((2,3), dtype=torch.cuda.float32, device='cuda:1')\n\n\nTODO in future PRs:\n\nSplit out cuda from dtype so you don't need to overspecify cuda-ness\nWe currently only support strings/DeviceSpecs in tensor methods and torch. functions.  We should have equivalents torch.cuda.device(...), torch.cuda.device_of, etc. at the torch. level that work on strings/DeviceSpecs", "body": "Previously, tensors only had a `get_device` method which would throw an exception on a CPU tensor.   This made it necessary to if/else code that was meant to be device agnostic.\r\n\r\nThis PR implements the following:\r\n\r\n1) Adds a 'device' property to all tensors that returns a string representation of the device for all tensors.  For cpu tensors this is 'cpu'.  For cuda tensors this is 'cuda:X', where X is the cuda device ordinal.\r\n\r\n2) Adds a DeviceSpec class.  This is just a helper class for separating device_type and device_index specification and to allow partial specification.\r\n\r\n   For example, you can call `DeviceSpec('cuda')`, `DeviceSpec('cuda:0')`, `DeviceSpec('cuda', 1)`.\r\nAlso has backwards compatibility support for specifying integers (`DeviceSpec(1)`), which are treated as cuda devices.  DeviceSpecs have the following properties:\r\n\r\n   - `device_type`: string representation of the device type (i.e. 'cpu' or 'cuda')\r\n   - `device_index`: integer for the device index (None if not specified)\r\n   - `cuda_device_index`: for backwards compatibility; behaves roughly like `get_device` did previously.  I.e. if a function previously took integers for cuda devices, it can now take DeviceSpecs (or strings), and can maintain the old functionality by calling `old_index = DeviceSpec(old).cuda_device_index`.\r\n\r\n3) tensor methods and torch. functions that took integer devices can now take integers, strings, or DeviceSpecs.  For example:\r\n`torch.randn((2,3), dtype=torch.cuda.float32, device='cuda:1')`\r\n\r\nTODO in future PRs:\r\n\r\n* Split out cuda from dtype so you don't need to overspecify cuda-ness\r\n* We currently only support strings/DeviceSpecs in tensor methods and torch. functions.  We should have equivalents `torch.cuda.device(...)`, `torch.cuda.device_of`, etc. at the torch. level that work on strings/DeviceSpecs"}