{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2796", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2796/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2796/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2796/events", "html_url": "https://github.com/pytorch/pytorch/pull/2796", "id": 259037227, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQyMDA4NTM5", "number": 2796, "title": "address issue #1488 by using defaultdict in load_state_dict", "user": {"login": "randxie", "id": 10520307, "node_id": "MDQ6VXNlcjEwNTIwMzA3", "avatar_url": "https://avatars0.githubusercontent.com/u/10520307?v=4", "gravatar_id": "", "url": "https://api.github.com/users/randxie", "html_url": "https://github.com/randxie", "followers_url": "https://api.github.com/users/randxie/followers", "following_url": "https://api.github.com/users/randxie/following{/other_user}", "gists_url": "https://api.github.com/users/randxie/gists{/gist_id}", "starred_url": "https://api.github.com/users/randxie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/randxie/subscriptions", "organizations_url": "https://api.github.com/users/randxie/orgs", "repos_url": "https://api.github.com/users/randxie/repos", "events_url": "https://api.github.com/users/randxie/events{/privacy}", "received_events_url": "https://api.github.com/users/randxie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-20T04:34:39Z", "updated_at": "2017-09-20T18:56:24Z", "closed_at": "2017-09-20T18:56:22Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2796", "html_url": "https://github.com/pytorch/pytorch/pull/2796", "diff_url": "https://github.com/pytorch/pytorch/pull/2796.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2796.patch"}, "body_html": "<p>Try PR another time. Based on Adam's feedback, when restoring state in the function \"load_state_dict\" of class \"optimizer.py\", use defaultdict to maintain consistency with self.state.</p>\n<p><strong>Test Procedure</strong><br>\n(1) Use the following test case to verify the restored state is indeed a defaultdict.<br>\n(2) Run test_optim.py to make sure no other impact.</p>\n<p>----------------------------- Test Case -------------------------------------------------</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.optim <span class=\"pl-k\">import</span> <span class=\"pl-c1\">SGD</span>\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> initialize model</span>\nN, D_in, H, D_out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10</span>\nx <span class=\"pl-k\">=</span> Variable(torch.randn(N, D_in))\ny <span class=\"pl-k\">=</span> Variable(torch.randn(N, D_out), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nmodel <span class=\"pl-k\">=</span> torch.nn.Sequential(\n          torch.nn.Linear(D_in, H),\n          torch.nn.ReLU(),\n          torch.nn.Linear(H, D_out),\n        )\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> initialize optimizer</span>\noptimizer <span class=\"pl-k\">=</span> torch.optim.SGD(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> state should be defaultdict</span>\noriginal_type <span class=\"pl-k\">=</span> <span class=\"pl-c1\">type</span>(optimizer.state)\n<span class=\"pl-c1\">print</span>(original_type)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> after saving and loading, the restored state should also be a defaultdict</span>\ntorch.save(optimizer.state_dict(), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>optim.pth<span class=\"pl-pds\">'</span></span>) \noptimizer.load_state_dict(torch.load(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>optim.pth<span class=\"pl-pds\">'</span></span>)) \nrestored_type <span class=\"pl-k\">=</span> <span class=\"pl-c1\">type</span>(optimizer.state)\n<span class=\"pl-c1\">print</span>(restored_type)\n\n<span class=\"pl-k\">assert</span>(original_type<span class=\"pl-k\">==</span>restored_type)</pre></div>", "body_text": "Try PR another time. Based on Adam's feedback, when restoring state in the function \"load_state_dict\" of class \"optimizer.py\", use defaultdict to maintain consistency with self.state.\nTest Procedure\n(1) Use the following test case to verify the restored state is indeed a defaultdict.\n(2) Run test_optim.py to make sure no other impact.\n----------------------------- Test Case -------------------------------------------------\nimport torch\nfrom torch.optim import SGD\nfrom torch.autograd import Variable\n\n# initialize model\nN, D_in, H, D_out = 64, 1000, 100, 10\nx = Variable(torch.randn(N, D_in))\ny = Variable(torch.randn(N, D_out), requires_grad=False)\nmodel = torch.nn.Sequential(\n          torch.nn.Linear(D_in, H),\n          torch.nn.ReLU(),\n          torch.nn.Linear(H, D_out),\n        )\n\n# initialize optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n\n# state should be defaultdict\noriginal_type = type(optimizer.state)\nprint(original_type)\n\n# after saving and loading, the restored state should also be a defaultdict\ntorch.save(optimizer.state_dict(), 'optim.pth') \noptimizer.load_state_dict(torch.load('optim.pth')) \nrestored_type = type(optimizer.state)\nprint(restored_type)\n\nassert(original_type==restored_type)", "body": "Try PR another time. Based on Adam's feedback, when restoring state in the function \"load_state_dict\" of class \"optimizer.py\", use defaultdict to maintain consistency with self.state. \r\n\r\n**Test Procedure**\r\n(1) Use the following test case to verify the restored state is indeed a defaultdict.\r\n(2) Run test_optim.py to make sure no other impact.\r\n\r\n----------------------------- Test Case -------------------------------------------------\r\n```python\r\nimport torch\r\nfrom torch.optim import SGD\r\nfrom torch.autograd import Variable\r\n\r\n# initialize model\r\nN, D_in, H, D_out = 64, 1000, 100, 10\r\nx = Variable(torch.randn(N, D_in))\r\ny = Variable(torch.randn(N, D_out), requires_grad=False)\r\nmodel = torch.nn.Sequential(\r\n          torch.nn.Linear(D_in, H),\r\n          torch.nn.ReLU(),\r\n          torch.nn.Linear(H, D_out),\r\n        )\r\n\r\n# initialize optimizer\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\n\r\n# state should be defaultdict\r\noriginal_type = type(optimizer.state)\r\nprint(original_type)\r\n\r\n# after saving and loading, the restored state should also be a defaultdict\r\ntorch.save(optimizer.state_dict(), 'optim.pth') \r\noptimizer.load_state_dict(torch.load('optim.pth')) \r\nrestored_type = type(optimizer.state)\r\nprint(restored_type)\r\n\r\nassert(original_type==restored_type)\r\n```"}