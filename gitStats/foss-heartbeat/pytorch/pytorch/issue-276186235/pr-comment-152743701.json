{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152743701", "pull_request_review_id": 78649160, "id": 152743701, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Mjc0MzcwMQ==", "diff_hunk": "@@ -210,20 +214,22 @@ class Gamma(Distribution):\n     \"\"\"\n \n     def __init__(self, alpha, beta):\n+        if isinstance(alpha, Number):\n+            if isinstance(beta, Number):\n+                alpha = torch.Tensor([alpha])\n+                beta = torch.Tensor([beta])\n+            else:\n+                alpha = type(beta)(*beta.size()).fill_(alpha)\n+        elif isinstance(beta, Number):\n+            beta = type(alpha)(*alpha.size()).fill_(beta)", "path": "torch/distributions.py", "position": null, "original_position": 43, "commit_id": "1872fa2d35560a324c1ad86f85a43bc5debf9672", "original_commit_id": "e16fc77a4a1a4887015d10d61af026d4022119f6", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "BTW I think it might be rewritten in a bit clearer way:\r\n```python\r\nalpha_num = isinstance(alpha, Number)\r\nbeta_num = isinstance(beta, Number)\r\nif alpha_num and not beta_num:\r\n    alpha = beta.new(beta.size()).fill_(alpha)\r\nelif not alpha_num and beta_num:\r\n    beta = alpha.new(alpha.size()).fill_(beta)\r\nelif alpha_num and beta_num:\r\n    alpha, beta = torch.Tensor([alpha]), torch.Tensor([beta])\r\n# NB: Nothing to do if both are tensors\r\n```", "created_at": "2017-11-23T08:46:47Z", "updated_at": "2018-11-23T15:36:44Z", "html_url": "https://github.com/pytorch/pytorch/pull/3841#discussion_r152743701", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3841", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152743701"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3841#discussion_r152743701"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3841"}}, "body_html": "<p>BTW I think it might be rewritten in a bit clearer way:</p>\n<div class=\"highlight highlight-source-python\"><pre>alpha_num <span class=\"pl-k\">=</span> <span class=\"pl-c1\">isinstance</span>(alpha, Number)\nbeta_num <span class=\"pl-k\">=</span> <span class=\"pl-c1\">isinstance</span>(beta, Number)\n<span class=\"pl-k\">if</span> alpha_num <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> beta_num:\n    alpha <span class=\"pl-k\">=</span> beta.new(beta.size()).fill_(alpha)\n<span class=\"pl-k\">elif</span> <span class=\"pl-k\">not</span> alpha_num <span class=\"pl-k\">and</span> beta_num:\n    beta <span class=\"pl-k\">=</span> alpha.new(alpha.size()).fill_(beta)\n<span class=\"pl-k\">elif</span> alpha_num <span class=\"pl-k\">and</span> beta_num:\n    alpha, beta <span class=\"pl-k\">=</span> torch.Tensor([alpha]), torch.Tensor([beta])\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> NB: Nothing to do if both are tensors</span></pre></div>", "body_text": "BTW I think it might be rewritten in a bit clearer way:\nalpha_num = isinstance(alpha, Number)\nbeta_num = isinstance(beta, Number)\nif alpha_num and not beta_num:\n    alpha = beta.new(beta.size()).fill_(alpha)\nelif not alpha_num and beta_num:\n    beta = alpha.new(alpha.size()).fill_(beta)\nelif alpha_num and beta_num:\n    alpha, beta = torch.Tensor([alpha]), torch.Tensor([beta])\n# NB: Nothing to do if both are tensors", "in_reply_to_id": 152742998}