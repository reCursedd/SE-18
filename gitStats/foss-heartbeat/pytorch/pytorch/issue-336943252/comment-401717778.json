{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/401717778", "html_url": "https://github.com/pytorch/pytorch/issues/9025#issuecomment-401717778", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9025", "id": 401717778, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTcxNzc3OA==", "user": {"login": "arogozhnikov", "id": 6318811, "node_id": "MDQ6VXNlcjYzMTg4MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6318811?v=4", "gravatar_id": "", "url": "https://api.github.com/users/arogozhnikov", "html_url": "https://github.com/arogozhnikov", "followers_url": "https://api.github.com/users/arogozhnikov/followers", "following_url": "https://api.github.com/users/arogozhnikov/following{/other_user}", "gists_url": "https://api.github.com/users/arogozhnikov/gists{/gist_id}", "starred_url": "https://api.github.com/users/arogozhnikov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/arogozhnikov/subscriptions", "organizations_url": "https://api.github.com/users/arogozhnikov/orgs", "repos_url": "https://api.github.com/users/arogozhnikov/repos", "events_url": "https://api.github.com/users/arogozhnikov/events{/privacy}", "received_events_url": "https://api.github.com/users/arogozhnikov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-02T08:48:59Z", "updated_at": "2018-07-02T08:48:59Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>this changes output shape?</p>\n</blockquote>\n<p>Yes, it does. Below is detailed proposed behavior</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> numpy\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">embed</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">embedding</span>, <span class=\"pl-smi\">added_dim</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>):\n    x <span class=\"pl-k\">=</span> embedding(x)\n    d <span class=\"pl-k\">=</span> numpy.arange(<span class=\"pl-c1\">len</span>(x.shape))\n    d[added_dim:] <span class=\"pl-k\">=</span> numpy.roll(d[added_dim:], <span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">return</span> x.permute(<span class=\"pl-c1\">tuple</span>(d))</pre></div>\n<p>And below is some demo how it affects shape:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>).long()\n<span class=\"pl-k\">for</span> added_dim <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-k\">-</span><span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>):\n    <span class=\"pl-c1\">print</span>(added_dim, embed(x, torch.nn.Embedding(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">30</span>), <span class=\"pl-v\">added_dim</span><span class=\"pl-k\">=</span>added_dim).shape)\n\noutput:\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">5</span> torch.Size([<span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">4</span> torch.Size([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">3</span> torch.Size([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span> torch.Size([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">4</span>])\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span> torch.Size([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">30</span>])\n<span class=\"pl-c1\">0</span> torch.Size([<span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n<span class=\"pl-c1\">1</span> torch.Size([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n<span class=\"pl-c1\">2</span> torch.Size([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n<span class=\"pl-c1\">3</span> torch.Size([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">30</span>, <span class=\"pl-c1\">4</span>])\n<span class=\"pl-c1\">4</span> torch.Size([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">30</span>])</pre></div>", "body_text": "this changes output shape?\n\nYes, it does. Below is detailed proposed behavior\nimport torch\nimport numpy\n\ndef embed(x, embedding, added_dim=-1):\n    x = embedding(x)\n    d = numpy.arange(len(x.shape))\n    d[added_dim:] = numpy.roll(d[added_dim:], 1)\n    return x.permute(tuple(d))\nAnd below is some demo how it affects shape:\nx = torch.zeros(1, 2, 3, 4).long()\nfor added_dim in range(-5, 5):\n    print(added_dim, embed(x, torch.nn.Embedding(100, 30), added_dim=added_dim).shape)\n\noutput:\n-5 torch.Size([30, 1, 2, 3, 4])\n-4 torch.Size([1, 30, 2, 3, 4])\n-3 torch.Size([1, 2, 30, 3, 4])\n-2 torch.Size([1, 2, 3, 30, 4])\n-1 torch.Size([1, 2, 3, 4, 30])\n0 torch.Size([30, 1, 2, 3, 4])\n1 torch.Size([1, 30, 2, 3, 4])\n2 torch.Size([1, 2, 30, 3, 4])\n3 torch.Size([1, 2, 3, 30, 4])\n4 torch.Size([1, 2, 3, 4, 30])", "body": "> this changes output shape?\r\n\r\nYes, it does. Below is detailed proposed behavior\r\n\r\n```python\r\nimport torch\r\nimport numpy\r\n\r\ndef embed(x, embedding, added_dim=-1):\r\n    x = embedding(x)\r\n    d = numpy.arange(len(x.shape))\r\n    d[added_dim:] = numpy.roll(d[added_dim:], 1)\r\n    return x.permute(tuple(d))\r\n```\r\n\r\nAnd below is some demo how it affects shape:\r\n\r\n```python\r\nx = torch.zeros(1, 2, 3, 4).long()\r\nfor added_dim in range(-5, 5):\r\n    print(added_dim, embed(x, torch.nn.Embedding(100, 30), added_dim=added_dim).shape)\r\n\r\noutput:\r\n-5 torch.Size([30, 1, 2, 3, 4])\r\n-4 torch.Size([1, 30, 2, 3, 4])\r\n-3 torch.Size([1, 2, 30, 3, 4])\r\n-2 torch.Size([1, 2, 3, 30, 4])\r\n-1 torch.Size([1, 2, 3, 4, 30])\r\n0 torch.Size([30, 1, 2, 3, 4])\r\n1 torch.Size([1, 30, 2, 3, 4])\r\n2 torch.Size([1, 2, 30, 3, 4])\r\n3 torch.Size([1, 2, 3, 30, 4])\r\n4 torch.Size([1, 2, 3, 4, 30])\r\n```\r\n\r\n"}