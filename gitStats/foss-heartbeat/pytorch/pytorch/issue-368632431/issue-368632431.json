{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12530", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12530/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12530/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12530/events", "html_url": "https://github.com/pytorch/pytorch/issues/12530", "id": 368632431, "node_id": "MDU6SXNzdWUzNjg2MzI0MzE=", "number": 12530, "title": "[caffe2] Memory usage", "user": {"login": "TheGreenEvil", "id": 15787219, "node_id": "MDQ6VXNlcjE1Nzg3MjE5", "avatar_url": "https://avatars0.githubusercontent.com/u/15787219?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TheGreenEvil", "html_url": "https://github.com/TheGreenEvil", "followers_url": "https://api.github.com/users/TheGreenEvil/followers", "following_url": "https://api.github.com/users/TheGreenEvil/following{/other_user}", "gists_url": "https://api.github.com/users/TheGreenEvil/gists{/gist_id}", "starred_url": "https://api.github.com/users/TheGreenEvil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TheGreenEvil/subscriptions", "organizations_url": "https://api.github.com/users/TheGreenEvil/orgs", "repos_url": "https://api.github.com/users/TheGreenEvil/repos", "events_url": "https://api.github.com/users/TheGreenEvil/events{/privacy}", "received_events_url": "https://api.github.com/users/TheGreenEvil/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-10T12:03:51Z", "updated_at": "2018-10-16T21:55:06Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi. I am getting unexpected big amount of memory usage when running onnx models in Caffe2 on C++.</p>\n<p>For example for SSD with VGG feature extractor it gets as high as 1.2Gb, while same model with almost equal size in serialized form (about 104Mb) in TensorFlow consume 130Mb. And i getting similar behavior for other models I tried.</p>\n<p>Just wanna know is that expected to memory usage be such high in Caffe2 or I doing something wrong?</p>\n<p>Here is my code to evaluate model on Caffe2:</p>\n<pre><code>#include &lt;string&gt;\n#include &lt;vector&gt;\n#include &lt;caffe2/onnx/backend.h&gt;\n#include &lt;caffe2/core/init.h&gt;\n#include &lt;caffe2/utils/proto_utils.h&gt;\n#include &lt;onnx/proto_utils.h&gt;\n#include &lt;onnx/onnx_ONNX_NAMESPACE.pb.h&gt;\n#include &lt;fstream&gt;\n\nstatic int sleeptime = 1;\nstatic int res = 300;\n\nONNX_NAMESPACE::ModelProto read_model(std::string path){\n    std::ifstream model_file(path, std::ios::binary|std::ios::ate);\n\n    auto size = model_file.tellg();\n    model_file.seekg(0, model_file.beg);\n\n    ONNX_NAMESPACE::ModelProto model_proto;\n    std::vector&lt;char&gt; model_bin_str(size);\n\n    model_file.readsome(model_bin_str.data(), size);\n\n    ONNX_NAMESPACE::ParseProtoFromBytes(&amp;model_proto, model_bin_str.data(), size);\n\n    model_file.close();\n\n    return model_proto;\n}\n\nvoid f(std::string model_path){\n\n    caffe2::onnx::Caffe2Backend backend;\n    auto back = std::unique_ptr&lt;caffe2::onnx::Caffe2BackendRep&gt;(backend.Prepare(read_model(model_path).SerializeAsString(),\"CPU :0\",{}));\n\n    std::cout &lt;&lt; \"Backend prepared\" &lt;&lt; std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n\n    caffe2::Predictor::TensorVector inputs, outputs;\n    caffe2::TensorCPU input_tensor;\n    std::vector&lt;caffe2::TIndex&gt; tensor_size = {1, 3, res, res};\n    input_tensor.Resize(tensor_size);\n    auto tensor_data = input_tensor.mutable_data&lt;float&gt;();\n    auto tensor_size_ = std::accumulate(tensor_size.begin(), tensor_size.end(), 1, std::multiplies&lt;int&gt;());\n    for (int i = 0; i &lt; tensor_size_; ++i) {\n        *(tensor_data+i) = 1.f;\n    }\n    inputs.push_back(&amp;input_tensor);\n\n    std::cout &lt;&lt; \"Input prepared\" &lt;&lt; std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n\n    back-&gt;Run(inputs, &amp;outputs);\n\n    std::cout &lt;&lt; \"Model inferenced\" &lt;&lt; std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n\n    inputs.clear();\n    outputs.clear();\n\n    input_tensor.Resize(tensor_size);\n    for (int i = 0; i &lt; tensor_size_; ++i) {\n        *(tensor_data+i) = 0.f;\n    }\n\n    inputs.push_back(&amp;input_tensor);\n\n    std::cout &lt;&lt; \"Input prepared\" &lt;&lt; std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n\n    back-&gt;Run(inputs, &amp;outputs);\n\n    std::cout &lt;&lt; \"Model inferenced\" &lt;&lt; std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n}\n\nint main(int argc, char *argv[])\n{\n    sleeptime = std::stoi(std::string(argv[2]));\n    res = std::stoi(std::string(argv[3]));\n    \n    std::cout &lt;&lt; \"Started\" &lt;&lt; std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n    \n    caffe2::GlobalInit();\n    \n    f(std::string(argv[1]));\n    \n    std::cout &lt;&lt; \"Pass done\" &lt;&lt; std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n    \n    caffe2::ShutdownProtobufLibrary();\n    \n    return 0;\n}\n</code></pre>\n<p>And here is my cmake to build it</p>\n<pre><code>project(caffe2_onnx_mem)\ncmake_minimum_required(VERSION 3.5)\nset(CMAKE_CXX_STANDARD 11)\n\ninclude(${CMAKE_ROOT}/Modules/ExternalProject.cmake)\n\nset(CAFFE2_DIR ${CMAKE_CURRENT_BINARY_DIR}/caffe2)\nset(CAFFE2_PREFIX ${CMAKE_CURRENT_BINARY_DIR}/Caffe2-prefix)\nset(CAFFE2_BUILD_DIR ${CAFFE2_PREFIX}/src/Caffe2-build)\nset(CAFFE2_SRC_DIR ${CAFFE2_PREFIX}/src/Caffe2/)\n\nExternalProject_Add(\n        Caffe2\n        UPDATE_COMMAND \"\"\n        GIT_REPOSITORY \"https://github.com/pytorch/pytorch\"\n        GIT_TAG \"v0.4.1\"\n        CMAKE_ARGS\n                -DONNX_NAMESPACE=ONNX_NAMESPACE\n                -DBUILD_BINARY=OFF\n                -DUSE_CUDA=OFF\n                -DBUILD_PYTHON=OFF\n                -DUSE_PROF=OFF\n                -DUSE_ATEN=OFF\n                -DUSE_OPENCV=OFF\n                -DUSE_LMDB=OFF\n                -DUSE_LEVELDB=OFF\n                -DUSE_GLOO=OFF\n                -DUSE_GLOG=OFF\n                -DUSE_GFLAGS=OFF\n                -DUSE_NNPACK=ON\n                -DUSE_MKL=OFF\n                -DUSE_MKLML=OFF\n                -DUSE_IDEEP=OFF\n                -DUSE_NATIVE_ARCH=ON\n                -DUSE_LITE_PROTO=OFF\n                -DBUILD_SHARED_LIBS=OFF\n                -DCMAKE_INSTALL_PREFIX=${CAFFE2_DIR}\n        INSTALL_COMMAND make install\n        )\n\nadd_executable($caffe2_onnx_mem \"c2_onnx_mem.cpp\")\n\ntarget_include_directories(caffe2_onnx_mem PUBLIC\n        ${CAFFE2_DIR}/include\n        CAFFE2_SRC_DIR\n        )\n\ntarget_link_libraries(caffe2_onnx_mem\n        -Wl,--whole-archive\n        ${CAFFE2_DIR}/lib/libcaffe2.a\n        ${CAFFE2_BUILD_DIR}/lib/libcaffe2_protos.a\n        -Wl,--no-whole-archive\n        ${CAFFE2_DIR}/lib/libcpuinfo.a\n        ${CAFFE2_DIR}/lib/libnomnigraph.a\n        ${CAFFE2_DIR}/lib/libonnx.a\n        ${CAFFE2_DIR}/lib/libnnpack.a\n        ${CAFFE2_DIR}/lib/libonnx_proto.a\n        ${CAFFE2_BUILD_DIR}/lib/libonnxifi_loader.a\n        ${CAFFE2_DIR}/lib/libprotobuf.a\n        ${CAFFE2_DIR}/lib/libprotoc.a\n        pthread\n        dl\n        )\n</code></pre>\n<p>I measuring memory usage with</p>\n<pre><code>watch -n 0.1 'pidof ./caffe2_onnx_mem | xargs -i cat /proc/{}/status | grep \"RssAnon\"'\n</code></pre>\n<p>And here is the python code that i using to make a model</p>\n<pre><code>import torch\n\nclass ConvBnAct(torch.nn.Module):\n    def __init__(self, inp, out, stride=1, kernel=3, groups=1, dilations=1,bias=False, pad=None):\n        super().__init__()\n        \n        if pad is not None:\n            is_pad = pad\n        else:\n            is_pad = True if kernel != 1 else False\n        \n        self.conv = torch.nn.Conv2d(inp,out,kernel,stride,int(is_pad),dilations,groups,bias)\n        self.bn = torch.nn.BatchNorm2d(out)\n        self.relu = torch.nn.LeakyReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n    \nclass ConvAct(torch.nn.Module):\n    def __init__(self, inp, out, stride=1, kernel=3, groups=1, dilations=1,bias=False, pad=None):\n        super().__init__()\n        \n        if pad is not None:\n            is_pad = pad\n        else:\n            is_pad = True if kernel != 1 else False\n        \n        self.conv = torch.nn.Conv2d(inp,out,kernel,stride,int(is_pad),dilations,groups,bias)\n        self.relu = torch.nn.LeakyReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n    \nclass VGGStage(torch.nn.Module):\n    def __init__(self, input_depth, output_depth, conv_count=2, pool=True):\n        super().__init__()\n        \n        self.ops = []\n        input_depth_ = input_depth\n        \n        if pool:\n            self.ops.append(torch.nn.MaxPool2d(3,stride=2,padding=1))\n        \n        for i in range(conv_count):\n            self.ops.append(ConvBnAct(input_depth_, output_depth))\n            input_depth_ = output_depth\n                \n        self.ops = torch.nn.Sequential(\n            *self.ops\n        )\n        \n    def forward(self, x):\n        return self.ops(x)\n\nclass VGG16_SSD(torch.nn.Module):\n    def __init__(self, cls_count=20):\n        super().__init__()\n        stage_defs = [\n            (3,64,2, False),\n            (64,128,2, True),\n            (128,256,3, True),\n            (256,512,3, True),\n            (512,512,3, True)\n        ]\n        \n        bbox_counts=[4,6,6,6,4]\n        inputs=[512,1024,512,256,256]\n        \n        self.stages = []\n        \n        for inp, out, num, pool in stage_defs:\n            self.stages.append(VGGStage(inp, out, num, pool))\n        \n        self.stages += [\n            ConvBnAct(512,1024,kernel=3),\n            ConvBnAct(1024,1024,kernel=1),\n            torch.nn.Sequential(\n                ConvBnAct(1024,256,kernel=1),\n                ConvBnAct(256,512,kernel=3, stride=2)\n            ),\n            torch.nn.Sequential(\n                ConvBnAct(512,128,kernel=1),\n                ConvBnAct(128,256,kernel=3, stride=2)\n            ),\n            torch.nn.Sequential(\n                ConvBnAct(256,128,kernel=1, pad=False),\n                ConvBnAct(128,256,kernel=3, pad=False)\n            ),\n            torch.nn.Sequential(\n                ConvBnAct(256,128,kernel=1, pad=False),\n                ConvAct(128,256,kernel=3, pad=False)\n            ),\n        ]\n            \n        self.stages = torch.nn.ModuleList(self.stages)\n        \n        self.extra_convs = torch.nn.ModuleList([\n           ConvBnAct(input_size,bbox_cnt * (4 + cls_count),kernel=3) for bbox_cnt, input_size in zip(bbox_counts, inputs) \n        ] + [\n            ConvAct(256,4,kernel=1)\n        ])\n    \n    \n    def forward(self, x):\n        \n        outputs = []\n        \n        for stage in self.stages[:4]:\n            x = stage(x)\n        \n        outputs.append(x)\n        \n        for stage in self.stages[4:7]:\n            x = stage(x)\n        \n        outputs.append(x)\n        \n        for stage in self.stages[7:]:\n            x = stage(x)\n            outputs.append(x)\n        \n        detections = tuple(\n            c(o) for o, c in zip(outputs, self.extra_convs)\n        )\n        \n        return detections\n\nnet = VGG16_SSD()\ntensor = torch.ones((1,3,300,300))\nout = net(tensor)\ntorch.onnx.export(net, tensor, \"./vggssd.onnx\", verbose=False, input_names=['Input'])        \n\n</code></pre>", "body_text": "Hi. I am getting unexpected big amount of memory usage when running onnx models in Caffe2 on C++.\nFor example for SSD with VGG feature extractor it gets as high as 1.2Gb, while same model with almost equal size in serialized form (about 104Mb) in TensorFlow consume 130Mb. And i getting similar behavior for other models I tried.\nJust wanna know is that expected to memory usage be such high in Caffe2 or I doing something wrong?\nHere is my code to evaluate model on Caffe2:\n#include <string>\n#include <vector>\n#include <caffe2/onnx/backend.h>\n#include <caffe2/core/init.h>\n#include <caffe2/utils/proto_utils.h>\n#include <onnx/proto_utils.h>\n#include <onnx/onnx_ONNX_NAMESPACE.pb.h>\n#include <fstream>\n\nstatic int sleeptime = 1;\nstatic int res = 300;\n\nONNX_NAMESPACE::ModelProto read_model(std::string path){\n    std::ifstream model_file(path, std::ios::binary|std::ios::ate);\n\n    auto size = model_file.tellg();\n    model_file.seekg(0, model_file.beg);\n\n    ONNX_NAMESPACE::ModelProto model_proto;\n    std::vector<char> model_bin_str(size);\n\n    model_file.readsome(model_bin_str.data(), size);\n\n    ONNX_NAMESPACE::ParseProtoFromBytes(&model_proto, model_bin_str.data(), size);\n\n    model_file.close();\n\n    return model_proto;\n}\n\nvoid f(std::string model_path){\n\n    caffe2::onnx::Caffe2Backend backend;\n    auto back = std::unique_ptr<caffe2::onnx::Caffe2BackendRep>(backend.Prepare(read_model(model_path).SerializeAsString(),\"CPU :0\",{}));\n\n    std::cout << \"Backend prepared\" << std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n\n    caffe2::Predictor::TensorVector inputs, outputs;\n    caffe2::TensorCPU input_tensor;\n    std::vector<caffe2::TIndex> tensor_size = {1, 3, res, res};\n    input_tensor.Resize(tensor_size);\n    auto tensor_data = input_tensor.mutable_data<float>();\n    auto tensor_size_ = std::accumulate(tensor_size.begin(), tensor_size.end(), 1, std::multiplies<int>());\n    for (int i = 0; i < tensor_size_; ++i) {\n        *(tensor_data+i) = 1.f;\n    }\n    inputs.push_back(&input_tensor);\n\n    std::cout << \"Input prepared\" << std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n\n    back->Run(inputs, &outputs);\n\n    std::cout << \"Model inferenced\" << std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n\n    inputs.clear();\n    outputs.clear();\n\n    input_tensor.Resize(tensor_size);\n    for (int i = 0; i < tensor_size_; ++i) {\n        *(tensor_data+i) = 0.f;\n    }\n\n    inputs.push_back(&input_tensor);\n\n    std::cout << \"Input prepared\" << std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n\n    back->Run(inputs, &outputs);\n\n    std::cout << \"Model inferenced\" << std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n}\n\nint main(int argc, char *argv[])\n{\n    sleeptime = std::stoi(std::string(argv[2]));\n    res = std::stoi(std::string(argv[3]));\n    \n    std::cout << \"Started\" << std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n    \n    caffe2::GlobalInit();\n    \n    f(std::string(argv[1]));\n    \n    std::cout << \"Pass done\" << std::endl;\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\n    \n    caffe2::ShutdownProtobufLibrary();\n    \n    return 0;\n}\n\nAnd here is my cmake to build it\nproject(caffe2_onnx_mem)\ncmake_minimum_required(VERSION 3.5)\nset(CMAKE_CXX_STANDARD 11)\n\ninclude(${CMAKE_ROOT}/Modules/ExternalProject.cmake)\n\nset(CAFFE2_DIR ${CMAKE_CURRENT_BINARY_DIR}/caffe2)\nset(CAFFE2_PREFIX ${CMAKE_CURRENT_BINARY_DIR}/Caffe2-prefix)\nset(CAFFE2_BUILD_DIR ${CAFFE2_PREFIX}/src/Caffe2-build)\nset(CAFFE2_SRC_DIR ${CAFFE2_PREFIX}/src/Caffe2/)\n\nExternalProject_Add(\n        Caffe2\n        UPDATE_COMMAND \"\"\n        GIT_REPOSITORY \"https://github.com/pytorch/pytorch\"\n        GIT_TAG \"v0.4.1\"\n        CMAKE_ARGS\n                -DONNX_NAMESPACE=ONNX_NAMESPACE\n                -DBUILD_BINARY=OFF\n                -DUSE_CUDA=OFF\n                -DBUILD_PYTHON=OFF\n                -DUSE_PROF=OFF\n                -DUSE_ATEN=OFF\n                -DUSE_OPENCV=OFF\n                -DUSE_LMDB=OFF\n                -DUSE_LEVELDB=OFF\n                -DUSE_GLOO=OFF\n                -DUSE_GLOG=OFF\n                -DUSE_GFLAGS=OFF\n                -DUSE_NNPACK=ON\n                -DUSE_MKL=OFF\n                -DUSE_MKLML=OFF\n                -DUSE_IDEEP=OFF\n                -DUSE_NATIVE_ARCH=ON\n                -DUSE_LITE_PROTO=OFF\n                -DBUILD_SHARED_LIBS=OFF\n                -DCMAKE_INSTALL_PREFIX=${CAFFE2_DIR}\n        INSTALL_COMMAND make install\n        )\n\nadd_executable($caffe2_onnx_mem \"c2_onnx_mem.cpp\")\n\ntarget_include_directories(caffe2_onnx_mem PUBLIC\n        ${CAFFE2_DIR}/include\n        CAFFE2_SRC_DIR\n        )\n\ntarget_link_libraries(caffe2_onnx_mem\n        -Wl,--whole-archive\n        ${CAFFE2_DIR}/lib/libcaffe2.a\n        ${CAFFE2_BUILD_DIR}/lib/libcaffe2_protos.a\n        -Wl,--no-whole-archive\n        ${CAFFE2_DIR}/lib/libcpuinfo.a\n        ${CAFFE2_DIR}/lib/libnomnigraph.a\n        ${CAFFE2_DIR}/lib/libonnx.a\n        ${CAFFE2_DIR}/lib/libnnpack.a\n        ${CAFFE2_DIR}/lib/libonnx_proto.a\n        ${CAFFE2_BUILD_DIR}/lib/libonnxifi_loader.a\n        ${CAFFE2_DIR}/lib/libprotobuf.a\n        ${CAFFE2_DIR}/lib/libprotoc.a\n        pthread\n        dl\n        )\n\nI measuring memory usage with\nwatch -n 0.1 'pidof ./caffe2_onnx_mem | xargs -i cat /proc/{}/status | grep \"RssAnon\"'\n\nAnd here is the python code that i using to make a model\nimport torch\n\nclass ConvBnAct(torch.nn.Module):\n    def __init__(self, inp, out, stride=1, kernel=3, groups=1, dilations=1,bias=False, pad=None):\n        super().__init__()\n        \n        if pad is not None:\n            is_pad = pad\n        else:\n            is_pad = True if kernel != 1 else False\n        \n        self.conv = torch.nn.Conv2d(inp,out,kernel,stride,int(is_pad),dilations,groups,bias)\n        self.bn = torch.nn.BatchNorm2d(out)\n        self.relu = torch.nn.LeakyReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n    \nclass ConvAct(torch.nn.Module):\n    def __init__(self, inp, out, stride=1, kernel=3, groups=1, dilations=1,bias=False, pad=None):\n        super().__init__()\n        \n        if pad is not None:\n            is_pad = pad\n        else:\n            is_pad = True if kernel != 1 else False\n        \n        self.conv = torch.nn.Conv2d(inp,out,kernel,stride,int(is_pad),dilations,groups,bias)\n        self.relu = torch.nn.LeakyReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n    \nclass VGGStage(torch.nn.Module):\n    def __init__(self, input_depth, output_depth, conv_count=2, pool=True):\n        super().__init__()\n        \n        self.ops = []\n        input_depth_ = input_depth\n        \n        if pool:\n            self.ops.append(torch.nn.MaxPool2d(3,stride=2,padding=1))\n        \n        for i in range(conv_count):\n            self.ops.append(ConvBnAct(input_depth_, output_depth))\n            input_depth_ = output_depth\n                \n        self.ops = torch.nn.Sequential(\n            *self.ops\n        )\n        \n    def forward(self, x):\n        return self.ops(x)\n\nclass VGG16_SSD(torch.nn.Module):\n    def __init__(self, cls_count=20):\n        super().__init__()\n        stage_defs = [\n            (3,64,2, False),\n            (64,128,2, True),\n            (128,256,3, True),\n            (256,512,3, True),\n            (512,512,3, True)\n        ]\n        \n        bbox_counts=[4,6,6,6,4]\n        inputs=[512,1024,512,256,256]\n        \n        self.stages = []\n        \n        for inp, out, num, pool in stage_defs:\n            self.stages.append(VGGStage(inp, out, num, pool))\n        \n        self.stages += [\n            ConvBnAct(512,1024,kernel=3),\n            ConvBnAct(1024,1024,kernel=1),\n            torch.nn.Sequential(\n                ConvBnAct(1024,256,kernel=1),\n                ConvBnAct(256,512,kernel=3, stride=2)\n            ),\n            torch.nn.Sequential(\n                ConvBnAct(512,128,kernel=1),\n                ConvBnAct(128,256,kernel=3, stride=2)\n            ),\n            torch.nn.Sequential(\n                ConvBnAct(256,128,kernel=1, pad=False),\n                ConvBnAct(128,256,kernel=3, pad=False)\n            ),\n            torch.nn.Sequential(\n                ConvBnAct(256,128,kernel=1, pad=False),\n                ConvAct(128,256,kernel=3, pad=False)\n            ),\n        ]\n            \n        self.stages = torch.nn.ModuleList(self.stages)\n        \n        self.extra_convs = torch.nn.ModuleList([\n           ConvBnAct(input_size,bbox_cnt * (4 + cls_count),kernel=3) for bbox_cnt, input_size in zip(bbox_counts, inputs) \n        ] + [\n            ConvAct(256,4,kernel=1)\n        ])\n    \n    \n    def forward(self, x):\n        \n        outputs = []\n        \n        for stage in self.stages[:4]:\n            x = stage(x)\n        \n        outputs.append(x)\n        \n        for stage in self.stages[4:7]:\n            x = stage(x)\n        \n        outputs.append(x)\n        \n        for stage in self.stages[7:]:\n            x = stage(x)\n            outputs.append(x)\n        \n        detections = tuple(\n            c(o) for o, c in zip(outputs, self.extra_convs)\n        )\n        \n        return detections\n\nnet = VGG16_SSD()\ntensor = torch.ones((1,3,300,300))\nout = net(tensor)\ntorch.onnx.export(net, tensor, \"./vggssd.onnx\", verbose=False, input_names=['Input'])", "body": "Hi. I am getting unexpected big amount of memory usage when running onnx models in Caffe2 on C++. \r\n\r\nFor example for SSD with VGG feature extractor it gets as high as 1.2Gb, while same model with almost equal size in serialized form (about 104Mb) in TensorFlow consume 130Mb. And i getting similar behavior for other models I tried.\r\n\r\nJust wanna know is that expected to memory usage be such high in Caffe2 or I doing something wrong?\r\n\r\nHere is my code to evaluate model on Caffe2:\r\n```\r\n#include <string>\r\n#include <vector>\r\n#include <caffe2/onnx/backend.h>\r\n#include <caffe2/core/init.h>\r\n#include <caffe2/utils/proto_utils.h>\r\n#include <onnx/proto_utils.h>\r\n#include <onnx/onnx_ONNX_NAMESPACE.pb.h>\r\n#include <fstream>\r\n\r\nstatic int sleeptime = 1;\r\nstatic int res = 300;\r\n\r\nONNX_NAMESPACE::ModelProto read_model(std::string path){\r\n    std::ifstream model_file(path, std::ios::binary|std::ios::ate);\r\n\r\n    auto size = model_file.tellg();\r\n    model_file.seekg(0, model_file.beg);\r\n\r\n    ONNX_NAMESPACE::ModelProto model_proto;\r\n    std::vector<char> model_bin_str(size);\r\n\r\n    model_file.readsome(model_bin_str.data(), size);\r\n\r\n    ONNX_NAMESPACE::ParseProtoFromBytes(&model_proto, model_bin_str.data(), size);\r\n\r\n    model_file.close();\r\n\r\n    return model_proto;\r\n}\r\n\r\nvoid f(std::string model_path){\r\n\r\n    caffe2::onnx::Caffe2Backend backend;\r\n    auto back = std::unique_ptr<caffe2::onnx::Caffe2BackendRep>(backend.Prepare(read_model(model_path).SerializeAsString(),\"CPU :0\",{}));\r\n\r\n    std::cout << \"Backend prepared\" << std::endl;\r\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\r\n\r\n    caffe2::Predictor::TensorVector inputs, outputs;\r\n    caffe2::TensorCPU input_tensor;\r\n    std::vector<caffe2::TIndex> tensor_size = {1, 3, res, res};\r\n    input_tensor.Resize(tensor_size);\r\n    auto tensor_data = input_tensor.mutable_data<float>();\r\n    auto tensor_size_ = std::accumulate(tensor_size.begin(), tensor_size.end(), 1, std::multiplies<int>());\r\n    for (int i = 0; i < tensor_size_; ++i) {\r\n        *(tensor_data+i) = 1.f;\r\n    }\r\n    inputs.push_back(&input_tensor);\r\n\r\n    std::cout << \"Input prepared\" << std::endl;\r\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\r\n\r\n    back->Run(inputs, &outputs);\r\n\r\n    std::cout << \"Model inferenced\" << std::endl;\r\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\r\n\r\n    inputs.clear();\r\n    outputs.clear();\r\n\r\n    input_tensor.Resize(tensor_size);\r\n    for (int i = 0; i < tensor_size_; ++i) {\r\n        *(tensor_data+i) = 0.f;\r\n    }\r\n\r\n    inputs.push_back(&input_tensor);\r\n\r\n    std::cout << \"Input prepared\" << std::endl;\r\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\r\n\r\n    back->Run(inputs, &outputs);\r\n\r\n    std::cout << \"Model inferenced\" << std::endl;\r\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\r\n}\r\n\r\nint main(int argc, char *argv[])\r\n{\r\n    sleeptime = std::stoi(std::string(argv[2]));\r\n    res = std::stoi(std::string(argv[3]));\r\n    \r\n    std::cout << \"Started\" << std::endl;\r\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\r\n    \r\n    caffe2::GlobalInit();\r\n    \r\n    f(std::string(argv[1]));\r\n    \r\n    std::cout << \"Pass done\" << std::endl;\r\n    std::this_thread::sleep_for(std::chrono::seconds(sleeptime));\r\n    \r\n    caffe2::ShutdownProtobufLibrary();\r\n    \r\n    return 0;\r\n}\r\n```\r\n\r\nAnd here is my cmake to build it\r\n```\r\nproject(caffe2_onnx_mem)\r\ncmake_minimum_required(VERSION 3.5)\r\nset(CMAKE_CXX_STANDARD 11)\r\n\r\ninclude(${CMAKE_ROOT}/Modules/ExternalProject.cmake)\r\n\r\nset(CAFFE2_DIR ${CMAKE_CURRENT_BINARY_DIR}/caffe2)\r\nset(CAFFE2_PREFIX ${CMAKE_CURRENT_BINARY_DIR}/Caffe2-prefix)\r\nset(CAFFE2_BUILD_DIR ${CAFFE2_PREFIX}/src/Caffe2-build)\r\nset(CAFFE2_SRC_DIR ${CAFFE2_PREFIX}/src/Caffe2/)\r\n\r\nExternalProject_Add(\r\n        Caffe2\r\n        UPDATE_COMMAND \"\"\r\n        GIT_REPOSITORY \"https://github.com/pytorch/pytorch\"\r\n        GIT_TAG \"v0.4.1\"\r\n        CMAKE_ARGS\r\n                -DONNX_NAMESPACE=ONNX_NAMESPACE\r\n                -DBUILD_BINARY=OFF\r\n                -DUSE_CUDA=OFF\r\n                -DBUILD_PYTHON=OFF\r\n                -DUSE_PROF=OFF\r\n                -DUSE_ATEN=OFF\r\n                -DUSE_OPENCV=OFF\r\n                -DUSE_LMDB=OFF\r\n                -DUSE_LEVELDB=OFF\r\n                -DUSE_GLOO=OFF\r\n                -DUSE_GLOG=OFF\r\n                -DUSE_GFLAGS=OFF\r\n                -DUSE_NNPACK=ON\r\n                -DUSE_MKL=OFF\r\n                -DUSE_MKLML=OFF\r\n                -DUSE_IDEEP=OFF\r\n                -DUSE_NATIVE_ARCH=ON\r\n                -DUSE_LITE_PROTO=OFF\r\n                -DBUILD_SHARED_LIBS=OFF\r\n                -DCMAKE_INSTALL_PREFIX=${CAFFE2_DIR}\r\n        INSTALL_COMMAND make install\r\n        )\r\n\r\nadd_executable($caffe2_onnx_mem \"c2_onnx_mem.cpp\")\r\n\r\ntarget_include_directories(caffe2_onnx_mem PUBLIC\r\n        ${CAFFE2_DIR}/include\r\n        CAFFE2_SRC_DIR\r\n        )\r\n\r\ntarget_link_libraries(caffe2_onnx_mem\r\n        -Wl,--whole-archive\r\n        ${CAFFE2_DIR}/lib/libcaffe2.a\r\n        ${CAFFE2_BUILD_DIR}/lib/libcaffe2_protos.a\r\n        -Wl,--no-whole-archive\r\n        ${CAFFE2_DIR}/lib/libcpuinfo.a\r\n        ${CAFFE2_DIR}/lib/libnomnigraph.a\r\n        ${CAFFE2_DIR}/lib/libonnx.a\r\n        ${CAFFE2_DIR}/lib/libnnpack.a\r\n        ${CAFFE2_DIR}/lib/libonnx_proto.a\r\n        ${CAFFE2_BUILD_DIR}/lib/libonnxifi_loader.a\r\n        ${CAFFE2_DIR}/lib/libprotobuf.a\r\n        ${CAFFE2_DIR}/lib/libprotoc.a\r\n        pthread\r\n        dl\r\n        )\r\n```\r\n\r\nI measuring memory usage with\r\n```\r\nwatch -n 0.1 'pidof ./caffe2_onnx_mem | xargs -i cat /proc/{}/status | grep \"RssAnon\"'\r\n```\r\n\r\nAnd here is the python code that i using to make a model\r\n```\r\nimport torch\r\n\r\nclass ConvBnAct(torch.nn.Module):\r\n    def __init__(self, inp, out, stride=1, kernel=3, groups=1, dilations=1,bias=False, pad=None):\r\n        super().__init__()\r\n        \r\n        if pad is not None:\r\n            is_pad = pad\r\n        else:\r\n            is_pad = True if kernel != 1 else False\r\n        \r\n        self.conv = torch.nn.Conv2d(inp,out,kernel,stride,int(is_pad),dilations,groups,bias)\r\n        self.bn = torch.nn.BatchNorm2d(out)\r\n        self.relu = torch.nn.LeakyReLU(inplace=True)\r\n\r\n    def forward(self, x):\r\n        x = self.conv(x)\r\n        x = self.bn(x)\r\n        x = self.relu(x)\r\n        return x\r\n    \r\nclass ConvAct(torch.nn.Module):\r\n    def __init__(self, inp, out, stride=1, kernel=3, groups=1, dilations=1,bias=False, pad=None):\r\n        super().__init__()\r\n        \r\n        if pad is not None:\r\n            is_pad = pad\r\n        else:\r\n            is_pad = True if kernel != 1 else False\r\n        \r\n        self.conv = torch.nn.Conv2d(inp,out,kernel,stride,int(is_pad),dilations,groups,bias)\r\n        self.relu = torch.nn.LeakyReLU(inplace=True)\r\n\r\n    def forward(self, x):\r\n        x = self.conv(x)\r\n        x = self.relu(x)\r\n        return x\r\n    \r\nclass VGGStage(torch.nn.Module):\r\n    def __init__(self, input_depth, output_depth, conv_count=2, pool=True):\r\n        super().__init__()\r\n        \r\n        self.ops = []\r\n        input_depth_ = input_depth\r\n        \r\n        if pool:\r\n            self.ops.append(torch.nn.MaxPool2d(3,stride=2,padding=1))\r\n        \r\n        for i in range(conv_count):\r\n            self.ops.append(ConvBnAct(input_depth_, output_depth))\r\n            input_depth_ = output_depth\r\n                \r\n        self.ops = torch.nn.Sequential(\r\n            *self.ops\r\n        )\r\n        \r\n    def forward(self, x):\r\n        return self.ops(x)\r\n\r\nclass VGG16_SSD(torch.nn.Module):\r\n    def __init__(self, cls_count=20):\r\n        super().__init__()\r\n        stage_defs = [\r\n            (3,64,2, False),\r\n            (64,128,2, True),\r\n            (128,256,3, True),\r\n            (256,512,3, True),\r\n            (512,512,3, True)\r\n        ]\r\n        \r\n        bbox_counts=[4,6,6,6,4]\r\n        inputs=[512,1024,512,256,256]\r\n        \r\n        self.stages = []\r\n        \r\n        for inp, out, num, pool in stage_defs:\r\n            self.stages.append(VGGStage(inp, out, num, pool))\r\n        \r\n        self.stages += [\r\n            ConvBnAct(512,1024,kernel=3),\r\n            ConvBnAct(1024,1024,kernel=1),\r\n            torch.nn.Sequential(\r\n                ConvBnAct(1024,256,kernel=1),\r\n                ConvBnAct(256,512,kernel=3, stride=2)\r\n            ),\r\n            torch.nn.Sequential(\r\n                ConvBnAct(512,128,kernel=1),\r\n                ConvBnAct(128,256,kernel=3, stride=2)\r\n            ),\r\n            torch.nn.Sequential(\r\n                ConvBnAct(256,128,kernel=1, pad=False),\r\n                ConvBnAct(128,256,kernel=3, pad=False)\r\n            ),\r\n            torch.nn.Sequential(\r\n                ConvBnAct(256,128,kernel=1, pad=False),\r\n                ConvAct(128,256,kernel=3, pad=False)\r\n            ),\r\n        ]\r\n            \r\n        self.stages = torch.nn.ModuleList(self.stages)\r\n        \r\n        self.extra_convs = torch.nn.ModuleList([\r\n           ConvBnAct(input_size,bbox_cnt * (4 + cls_count),kernel=3) for bbox_cnt, input_size in zip(bbox_counts, inputs) \r\n        ] + [\r\n            ConvAct(256,4,kernel=1)\r\n        ])\r\n    \r\n    \r\n    def forward(self, x):\r\n        \r\n        outputs = []\r\n        \r\n        for stage in self.stages[:4]:\r\n            x = stage(x)\r\n        \r\n        outputs.append(x)\r\n        \r\n        for stage in self.stages[4:7]:\r\n            x = stage(x)\r\n        \r\n        outputs.append(x)\r\n        \r\n        for stage in self.stages[7:]:\r\n            x = stage(x)\r\n            outputs.append(x)\r\n        \r\n        detections = tuple(\r\n            c(o) for o, c in zip(outputs, self.extra_convs)\r\n        )\r\n        \r\n        return detections\r\n\r\nnet = VGG16_SSD()\r\ntensor = torch.ones((1,3,300,300))\r\nout = net(tensor)\r\ntorch.onnx.export(net, tensor, \"./vggssd.onnx\", verbose=False, input_names=['Input'])        \r\n\r\n```"}