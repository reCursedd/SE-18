{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12672", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12672/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12672/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12672/events", "html_url": "https://github.com/pytorch/pytorch/issues/12672", "id": 370328717, "node_id": "MDU6SXNzdWUzNzAzMjg3MTc=", "number": 12672, "title": "Move collate_fn functionality / responsibility into Dataset object", "user": {"login": "mboratko", "id": 611553, "node_id": "MDQ6VXNlcjYxMTU1Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/611553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mboratko", "html_url": "https://github.com/mboratko", "followers_url": "https://api.github.com/users/mboratko/followers", "following_url": "https://api.github.com/users/mboratko/following{/other_user}", "gists_url": "https://api.github.com/users/mboratko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mboratko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mboratko/subscriptions", "organizations_url": "https://api.github.com/users/mboratko/orgs", "repos_url": "https://api.github.com/users/mboratko/repos", "events_url": "https://api.github.com/users/mboratko/events{/privacy}", "received_events_url": "https://api.github.com/users/mboratko/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-10-15T20:23:09Z", "updated_at": "2018-10-16T17:44:33Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>The <code>Dataset</code> <code>__getitem__</code> class should be expected to handle advanced indexing, as opposed to just single integer indexing, and batching functionality provided by DataLoader should then use this instead of collate_fn, i.e.</p>\n<p><code>batch = collate_fn([dataset[i] for i in indices])</code></p>\n<p>would become</p>\n<p><code>batch = dataset[indices]</code></p>\n<h2>Motivation</h2>\n<ul>\n<li>The type of collation necessary for a dataset is more a property of the Dataset than the DataLoader.</li>\n<li>In particular, for <code>TensorDataset</code> objects, this will massively speed up batch creation. (Currently DataLoader is prohibitively slow, see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"293204680\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4959\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4959/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4959\">#4959</a>.)</li>\n<li>It also allows Datasets to be used more flexibly outside of a DataLoader.</li>\n</ul>\n<h2>Pitch</h2>\n<p>I think it's quite natural to embed this collate functionality into the Dataset rather than the DataLoader. There are no downsides that I can see to changing this functionality. By default, you could simply pass it back through the current default_collate argument. For example, the class for Dataset could become</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Dataset</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">index</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">type</span>(index) <span class=\"pl-k\">is</span> <span class=\"pl-c1\">int</span>:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">NotImplementedError</span>\n        <span class=\"pl-k\">else</span>:\n            requested_elements <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">self</span>[i] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> index]\n            <span class=\"pl-k\">return</span> default_collate(requested_elements)</pre></div>\n<p>This isn't completely backwards-compatible with the current API, however it's a simple fix. Exising implementations which are working can just change their implementation of the <code>__getitem__</code> method to</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> <span class=\"pl-c1\">type</span>(index) <span class=\"pl-k\">is</span> <span class=\"pl-c1\">int</span>:\n    (<span class=\"pl-c1\">...</span> existing implementation<span class=\"pl-c1\">...</span>)\n<span class=\"pl-k\">else</span>:\n    <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-c1\">self</span>, index)</pre></div>\n<p>I've made a quick-and-dirty change to this effect <a href=\"https://github.com/mboratko/pytorch/commit/e16804206c867201afbc283885e3a743ec486115\">in this fork</a>. (I'm having trouble with CUDA versions blocking me setting up an actual build, and there's been no discussion yet, so I haven't set up a formal pull request.)</p>\n<h2>Alternatives</h2>\n<p>An alternative workaround to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"293204680\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4959\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/4959/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/4959\">#4959</a> is simply not to use a <code>DataLoader</code> for <code>TensorDatasets</code> (or any other datasets that support faster native advanced indexing) and instead write the batch functionality directly. This results in code duplication and also isn't possible for libraries that expect <code>DataLoader</code> objects (eg. fastai). The solution so far has been that I wrap <code>DataLoader</code> and <code>_DataLoaderIter</code> objects into new objects which make exactly the changes I am suggesting, however this obviously is less than desireable.</p>\n<p>Obviously, there are also alternatives to my proposed implementation.</p>", "body_text": "\ud83d\ude80 Feature\nThe Dataset __getitem__ class should be expected to handle advanced indexing, as opposed to just single integer indexing, and batching functionality provided by DataLoader should then use this instead of collate_fn, i.e.\nbatch = collate_fn([dataset[i] for i in indices])\nwould become\nbatch = dataset[indices]\nMotivation\n\nThe type of collation necessary for a dataset is more a property of the Dataset than the DataLoader.\nIn particular, for TensorDataset objects, this will massively speed up batch creation. (Currently DataLoader is prohibitively slow, see #4959.)\nIt also allows Datasets to be used more flexibly outside of a DataLoader.\n\nPitch\nI think it's quite natural to embed this collate functionality into the Dataset rather than the DataLoader. There are no downsides that I can see to changing this functionality. By default, you could simply pass it back through the current default_collate argument. For example, the class for Dataset could become\nclass Dataset(object):\n    def __getitem__(self, index):\n        if type(index) is int:\n            raise NotImplementedError\n        else:\n            requested_elements = [self[i] for i in index]\n            return default_collate(requested_elements)\nThis isn't completely backwards-compatible with the current API, however it's a simple fix. Exising implementations which are working can just change their implementation of the __getitem__ method to\nif type(index) is int:\n    (... existing implementation...)\nelse:\n    super().__getitem__(self, index)\nI've made a quick-and-dirty change to this effect in this fork. (I'm having trouble with CUDA versions blocking me setting up an actual build, and there's been no discussion yet, so I haven't set up a formal pull request.)\nAlternatives\nAn alternative workaround to #4959 is simply not to use a DataLoader for TensorDatasets (or any other datasets that support faster native advanced indexing) and instead write the batch functionality directly. This results in code duplication and also isn't possible for libraries that expect DataLoader objects (eg. fastai). The solution so far has been that I wrap DataLoader and _DataLoaderIter objects into new objects which make exactly the changes I am suggesting, however this obviously is less than desireable.\nObviously, there are also alternatives to my proposed implementation.", "body": "## \ud83d\ude80 Feature\r\nThe `Dataset` `__getitem__` class should be expected to handle advanced indexing, as opposed to just single integer indexing, and batching functionality provided by DataLoader should then use this instead of collate_fn, i.e.\r\n\r\n`batch = collate_fn([dataset[i] for i in indices])`\r\n\r\nwould become\r\n\r\n`batch = dataset[indices]`\r\n\r\n## Motivation\r\n\r\n- The type of collation necessary for a dataset is more a property of the Dataset than the DataLoader.\r\n- In particular, for `TensorDataset` objects, this will massively speed up batch creation. (Currently DataLoader is prohibitively slow, see #4959.)\r\n- It also allows Datasets to be used more flexibly outside of a DataLoader.\r\n\r\n## Pitch\r\n\r\nI think it's quite natural to embed this collate functionality into the Dataset rather than the DataLoader. There are no downsides that I can see to changing this functionality. By default, you could simply pass it back through the current default_collate argument. For example, the class for Dataset could become\r\n\r\n```python\r\nclass Dataset(object):\r\n    def __getitem__(self, index):\r\n        if type(index) is int:\r\n            raise NotImplementedError\r\n        else:\r\n            requested_elements = [self[i] for i in index]\r\n            return default_collate(requested_elements)\r\n```\r\n\r\nThis isn't completely backwards-compatible with the current API, however it's a simple fix. Exising implementations which are working can just change their implementation of the `__getitem__` method to\r\n```python\r\nif type(index) is int:\r\n    (... existing implementation...)\r\nelse:\r\n    super().__getitem__(self, index)\r\n```\r\n\r\nI've made a quick-and-dirty change to this effect [in this fork](https://github.com/mboratko/pytorch/commit/e16804206c867201afbc283885e3a743ec486115). (I'm having trouble with CUDA versions blocking me setting up an actual build, and there's been no discussion yet, so I haven't set up a formal pull request.)\r\n\r\n\r\n## Alternatives\r\n\r\nAn alternative workaround to #4959 is simply not to use a `DataLoader` for `TensorDatasets` (or any other datasets that support faster native advanced indexing) and instead write the batch functionality directly. This results in code duplication and also isn't possible for libraries that expect `DataLoader` objects (eg. fastai). The solution so far has been that I wrap `DataLoader` and `_DataLoaderIter` objects into new objects which make exactly the changes I am suggesting, however this obviously is less than desireable.\r\n\r\nObviously, there are also alternatives to my proposed implementation."}