{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/430007710", "html_url": "https://github.com/pytorch/pytorch/issues/12672#issuecomment-430007710", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12672", "id": 430007710, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDAwNzcxMA==", "user": {"login": "mboratko", "id": 611553, "node_id": "MDQ6VXNlcjYxMTU1Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/611553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mboratko", "html_url": "https://github.com/mboratko", "followers_url": "https://api.github.com/users/mboratko/followers", "following_url": "https://api.github.com/users/mboratko/following{/other_user}", "gists_url": "https://api.github.com/users/mboratko/gists{/gist_id}", "starred_url": "https://api.github.com/users/mboratko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mboratko/subscriptions", "organizations_url": "https://api.github.com/users/mboratko/orgs", "repos_url": "https://api.github.com/users/mboratko/repos", "events_url": "https://api.github.com/users/mboratko/events{/privacy}", "received_events_url": "https://api.github.com/users/mboratko/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-15T20:48:51Z", "updated_at": "2018-10-16T17:44:33Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>the use-cases where you want to handle batch-loading in the dataset are limited</p>\n</blockquote>\n<p>Any <code>TensorDataset</code> is currently extremely slow if you use a <code>DataLoader</code> with it, so at least this fixes that.</p>\n<blockquote>\n<p>and you'd need to write all the parallelisation logic yourself</p>\n</blockquote>\n<p>With my proposed change, isn't it exactly the same as the situation is now? In my suggested change, when DataLoader calls</p>\n<p><code>batch = dataset[indices]</code></p>\n<p>it would, under the hood, do the same thing as</p>\n<p><code>batch = collate_fn([dataset[i] for i in indices])</code></p>\n<p>which is how the line currently reads, unless you override the default behavior (eg. let the underlying Tensor indexing return the nodes directly) by omitting the call back to <code>super().__getindex__(self, index)</code>. I don't know the details of how DataLoader handles parallelism, so perhaps moving this logic into the Dataset has some effect that I'm unaware of here.</p>", "body_text": "the use-cases where you want to handle batch-loading in the dataset are limited\n\nAny TensorDataset is currently extremely slow if you use a DataLoader with it, so at least this fixes that.\n\nand you'd need to write all the parallelisation logic yourself\n\nWith my proposed change, isn't it exactly the same as the situation is now? In my suggested change, when DataLoader calls\nbatch = dataset[indices]\nit would, under the hood, do the same thing as\nbatch = collate_fn([dataset[i] for i in indices])\nwhich is how the line currently reads, unless you override the default behavior (eg. let the underlying Tensor indexing return the nodes directly) by omitting the call back to super().__getindex__(self, index). I don't know the details of how DataLoader handles parallelism, so perhaps moving this logic into the Dataset has some effect that I'm unaware of here.", "body": "> the use-cases where you want to handle batch-loading in the dataset are limited\r\n\r\nAny `TensorDataset` is currently extremely slow if you use a `DataLoader` with it, so at least this fixes that.\r\n\r\n> and you'd need to write all the parallelisation logic yourself\r\n\r\nWith my proposed change, isn't it exactly the same as the situation is now? In my suggested change, when DataLoader calls\r\n\r\n`batch = dataset[indices]`\r\n\r\nit would, under the hood, do the same thing as\r\n\r\n`batch = collate_fn([dataset[i] for i in indices])`\r\n\r\nwhich is how the line currently reads, unless you override the default behavior (eg. let the underlying Tensor indexing return the nodes directly) by omitting the call back to `super().__getindex__(self, index)`. I don't know the details of how DataLoader handles parallelism, so perhaps moving this logic into the Dataset has some effect that I'm unaware of here."}