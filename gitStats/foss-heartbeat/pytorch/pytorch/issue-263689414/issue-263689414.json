{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3020", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3020/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3020/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3020/events", "html_url": "https://github.com/pytorch/pytorch/issues/3020", "id": 263689414, "node_id": "MDU6SXNzdWUyNjM2ODk0MTQ=", "number": 3020, "title": "Calling `backward` in parent leads to `backward` call in child process to hang", "user": {"login": "quanvuong", "id": 6988600, "node_id": "MDQ6VXNlcjY5ODg2MDA=", "avatar_url": "https://avatars1.githubusercontent.com/u/6988600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/quanvuong", "html_url": "https://github.com/quanvuong", "followers_url": "https://api.github.com/users/quanvuong/followers", "following_url": "https://api.github.com/users/quanvuong/following{/other_user}", "gists_url": "https://api.github.com/users/quanvuong/gists{/gist_id}", "starred_url": "https://api.github.com/users/quanvuong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/quanvuong/subscriptions", "organizations_url": "https://api.github.com/users/quanvuong/orgs", "repos_url": "https://api.github.com/users/quanvuong/repos", "events_url": "https://api.github.com/users/quanvuong/events{/privacy}", "received_events_url": "https://api.github.com/users/quanvuong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-08T03:23:50Z", "updated_at": "2017-10-08T03:33:01Z", "closed_at": "2017-10-08T03:33:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p><strong>Minimal Example</strong></p>\n<pre><code>import sys\nimport torch\nimport torch.nn as nn\nimport torch.multiprocessing as mp\nfrom torch.autograd import Variable\n\n\nclass Model(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.lstm = nn.LSTMCell(layers[0], layers[1])\n        self.linear = nn.Linear(layers[1], layers[2])\n        self.sf = nn.Softmax()\n        self.layers = layers\n\n    def forward(self, x, h0, c0):\n        h1, c1 = self.lstm(x, (h0, c0))\n        o1 = self.linear(h1)\n        o1 = self.sf(o1)\n        return o1, h1, c1\n\n\ndef backward(model, process='child'):\n    # Prepare initial inputs\n    h_n, c_n = Variable(torch.zeros(1, model.layers[1])), Variable(torch.zeros(1, model.layers[1]))\n    x_n = Variable(torch.FloatTensor(1))\n\n    for n in range(3):\n        dist, h_n, c_n = model(x_n, h_n, c_n)\n        print(f'backward in {process} {n}')\n        dist[0, 1].backward(retain_graph=True)\n\n\ndef backward_in_child(model):\n    child_model = Model(model.layers).share_memory()\n    with mp.Pool(1) as pool:\n        pool.map(backward, [child_model])\n\n\nif __name__ == '__main__':\n\n    print(f'Pytorch version: {torch.__version__}')\n    print(f'Python version: {sys.version_info}')\n\n    layers = [1, 2, 3]\n    model = Model(layers)\n\n    for i in range(5):\n        backward_in_child(model)\n        backward(model, process='parent')\n        print(f'{i}')\n</code></pre>\n<p>Running this code hangs at</p>\n<pre><code>Pytorch version: 0.2.0_4\nPython version: sys.version_info(major=3, minor=6, micro=2, releaselevel='final', serial=0)\nbackward in child 0\nbackward in child 1\nbackward in child 2\nbackward in parent 0\nbackward in parent 1\nbackward in parent 2\n0\nbackward in child 0 \n</code></pre>\n<p>If I comment out <code>backward(model, process='parent')</code>, the code runs till completion.</p>\n<p>We're hoping to finish this experiment in time for ICLR deadline late October. So any help is really appreciated. Thank you!</p>", "body_text": "Minimal Example\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.multiprocessing as mp\nfrom torch.autograd import Variable\n\n\nclass Model(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.lstm = nn.LSTMCell(layers[0], layers[1])\n        self.linear = nn.Linear(layers[1], layers[2])\n        self.sf = nn.Softmax()\n        self.layers = layers\n\n    def forward(self, x, h0, c0):\n        h1, c1 = self.lstm(x, (h0, c0))\n        o1 = self.linear(h1)\n        o1 = self.sf(o1)\n        return o1, h1, c1\n\n\ndef backward(model, process='child'):\n    # Prepare initial inputs\n    h_n, c_n = Variable(torch.zeros(1, model.layers[1])), Variable(torch.zeros(1, model.layers[1]))\n    x_n = Variable(torch.FloatTensor(1))\n\n    for n in range(3):\n        dist, h_n, c_n = model(x_n, h_n, c_n)\n        print(f'backward in {process} {n}')\n        dist[0, 1].backward(retain_graph=True)\n\n\ndef backward_in_child(model):\n    child_model = Model(model.layers).share_memory()\n    with mp.Pool(1) as pool:\n        pool.map(backward, [child_model])\n\n\nif __name__ == '__main__':\n\n    print(f'Pytorch version: {torch.__version__}')\n    print(f'Python version: {sys.version_info}')\n\n    layers = [1, 2, 3]\n    model = Model(layers)\n\n    for i in range(5):\n        backward_in_child(model)\n        backward(model, process='parent')\n        print(f'{i}')\n\nRunning this code hangs at\nPytorch version: 0.2.0_4\nPython version: sys.version_info(major=3, minor=6, micro=2, releaselevel='final', serial=0)\nbackward in child 0\nbackward in child 1\nbackward in child 2\nbackward in parent 0\nbackward in parent 1\nbackward in parent 2\n0\nbackward in child 0 \n\nIf I comment out backward(model, process='parent'), the code runs till completion.\nWe're hoping to finish this experiment in time for ICLR deadline late October. So any help is really appreciated. Thank you!", "body": "**Minimal Example**\r\n\r\n```\r\nimport sys\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.multiprocessing as mp\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass Model(nn.Module):\r\n    def __init__(self, layers):\r\n        super().__init__()\r\n        self.lstm = nn.LSTMCell(layers[0], layers[1])\r\n        self.linear = nn.Linear(layers[1], layers[2])\r\n        self.sf = nn.Softmax()\r\n        self.layers = layers\r\n\r\n    def forward(self, x, h0, c0):\r\n        h1, c1 = self.lstm(x, (h0, c0))\r\n        o1 = self.linear(h1)\r\n        o1 = self.sf(o1)\r\n        return o1, h1, c1\r\n\r\n\r\ndef backward(model, process='child'):\r\n    # Prepare initial inputs\r\n    h_n, c_n = Variable(torch.zeros(1, model.layers[1])), Variable(torch.zeros(1, model.layers[1]))\r\n    x_n = Variable(torch.FloatTensor(1))\r\n\r\n    for n in range(3):\r\n        dist, h_n, c_n = model(x_n, h_n, c_n)\r\n        print(f'backward in {process} {n}')\r\n        dist[0, 1].backward(retain_graph=True)\r\n\r\n\r\ndef backward_in_child(model):\r\n    child_model = Model(model.layers).share_memory()\r\n    with mp.Pool(1) as pool:\r\n        pool.map(backward, [child_model])\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    print(f'Pytorch version: {torch.__version__}')\r\n    print(f'Python version: {sys.version_info}')\r\n\r\n    layers = [1, 2, 3]\r\n    model = Model(layers)\r\n\r\n    for i in range(5):\r\n        backward_in_child(model)\r\n        backward(model, process='parent')\r\n        print(f'{i}')\r\n```\r\n\r\nRunning this code hangs at\r\n\r\n```\r\nPytorch version: 0.2.0_4\r\nPython version: sys.version_info(major=3, minor=6, micro=2, releaselevel='final', serial=0)\r\nbackward in child 0\r\nbackward in child 1\r\nbackward in child 2\r\nbackward in parent 0\r\nbackward in parent 1\r\nbackward in parent 2\r\n0\r\nbackward in child 0 \r\n```\r\n\r\nIf I comment out `backward(model, process='parent')`, the code runs till completion.\r\n\r\nWe're hoping to finish this experiment in time for ICLR deadline late October. So any help is really appreciated. Thank you!"}