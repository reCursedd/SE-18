{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4159", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4159/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4159/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4159/events", "html_url": "https://github.com/pytorch/pytorch/issues/4159", "id": 281890693, "node_id": "MDU6SXNzdWUyODE4OTA2OTM=", "number": 4159, "title": "Variable outputs of stochastic functions should never require grad", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2017-12-13T20:50:43Z", "updated_at": "2017-12-13T20:50:43Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>I don't think we should bother to implement reparametrized gradients for Variable functions (<code>torch.distributions</code> will have them + I don't think it's possible for bernoulli), and if we keep setting these outputs to <code>requires_grad=True</code> and pushing back zeros some people might be confused. Also, this doesn't match the legacy behavior, and can make some in-place checks that passed before fail.</p>\n<p>Let's just make them non-differentiable...</p>", "body_text": "I don't think we should bother to implement reparametrized gradients for Variable functions (torch.distributions will have them + I don't think it's possible for bernoulli), and if we keep setting these outputs to requires_grad=True and pushing back zeros some people might be confused. Also, this doesn't match the legacy behavior, and can make some in-place checks that passed before fail.\nLet's just make them non-differentiable...", "body": "I don't think we should bother to implement reparametrized gradients for Variable functions (`torch.distributions` will have them + I don't think it's possible for bernoulli), and if we keep setting these outputs to `requires_grad=True` and pushing back zeros some people might be confused. Also, this doesn't match the legacy behavior, and can make some in-place checks that passed before fail.\r\n\r\nLet's just make them non-differentiable..."}