{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7496", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7496/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7496/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7496/events", "html_url": "https://github.com/pytorch/pytorch/issues/7496", "id": 322237852, "node_id": "MDU6SXNzdWUzMjIyMzc4NTI=", "number": 7496, "title": "Train a model using multiple GPUs ", "user": {"login": "minghao-wu", "id": 17817832, "node_id": "MDQ6VXNlcjE3ODE3ODMy", "avatar_url": "https://avatars3.githubusercontent.com/u/17817832?v=4", "gravatar_id": "", "url": "https://api.github.com/users/minghao-wu", "html_url": "https://github.com/minghao-wu", "followers_url": "https://api.github.com/users/minghao-wu/followers", "following_url": "https://api.github.com/users/minghao-wu/following{/other_user}", "gists_url": "https://api.github.com/users/minghao-wu/gists{/gist_id}", "starred_url": "https://api.github.com/users/minghao-wu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/minghao-wu/subscriptions", "organizations_url": "https://api.github.com/users/minghao-wu/orgs", "repos_url": "https://api.github.com/users/minghao-wu/repos", "events_url": "https://api.github.com/users/minghao-wu/events{/privacy}", "received_events_url": "https://api.github.com/users/minghao-wu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-11T09:54:51Z", "updated_at": "2018-05-18T07:08:32Z", "closed_at": "2018-05-18T07:08:32Z", "author_association": "NONE", "body_html": "<h2>Issue Description</h2>\n<p>I tried to train my model on multiple gpus. However, when I launch the program, it seems that the model is allocated to gpus, but no data is fed into model. Using <code>nvidia-smi</code>, i find hundreds of MB of memory is consumed on each gpu. I guess these memory usage is for model initialization in each gpu.</p>\n<p>I am sharing 8 gpus with others on the server, so I limit my program on GPU 2 and GPU 3 by following command.</p>\n<div class=\"highlight highlight-source-python\"><pre>os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>CUDA_VISIBLE_DEVICES<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2,3<span class=\"pl-pds\">'</span></span></pre></div>\n<h2>Code example</h2>\n<p>I ran this official tutorial on my machine and the same thing happens again. <a href=\"https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html\" rel=\"nofollow\">https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html</a></p>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.3 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 7.5.17<br>\nGPU models and configuration:<br>\nGPU 0: GeForce GTX TITAN X<br>\nGPU 1: GeForce GTX TITAN X<br>\nGPU 2: GeForce GTX TITAN X<br>\nGPU 3: GeForce GTX TITAN X<br>\nGPU 4: GeForce GTX TITAN X<br>\nGPU 5: GeForce GTX TITAN X<br>\nGPU 6: GeForce GTX TITAN X<br>\nGPU 7: GeForce GTX TITAN X</p>\n<p>Nvidia driver version: 375.88<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.20<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a<br>\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn.so.6</p>\n<p>Versions of relevant libraries:<br>\n[pip3] msgpack-numpy (0.4.1)<br>\n[pip3] numpy (1.14.0)<br>\n[pip3] numpydoc (0.7.0)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchtext (0.2.3)<br>\n[pip3] torchvision (0.2.1)<br>\n[conda] torch                     0.4.0                     <br>\n[conda] torchtext                 0.2.3                     <br>\n[conda] torchvision               0.2.1                     </p>", "body_text": "Issue Description\nI tried to train my model on multiple gpus. However, when I launch the program, it seems that the model is allocated to gpus, but no data is fed into model. Using nvidia-smi, i find hundreds of MB of memory is consumed on each gpu. I guess these memory usage is for model initialization in each gpu.\nI am sharing 8 gpus with others on the server, so I limit my program on GPU 2 and GPU 3 by following command.\nos.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\nCode example\nI ran this official tutorial on my machine and the same thing happens again. https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.3 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 7.5.17\nGPU models and configuration:\nGPU 0: GeForce GTX TITAN X\nGPU 1: GeForce GTX TITAN X\nGPU 2: GeForce GTX TITAN X\nGPU 3: GeForce GTX TITAN X\nGPU 4: GeForce GTX TITAN X\nGPU 5: GeForce GTX TITAN X\nGPU 6: GeForce GTX TITAN X\nGPU 7: GeForce GTX TITAN X\nNvidia driver version: 375.88\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.20\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn.so.6\nVersions of relevant libraries:\n[pip3] msgpack-numpy (0.4.1)\n[pip3] numpy (1.14.0)\n[pip3] numpydoc (0.7.0)\n[pip3] torch (0.4.0)\n[pip3] torchtext (0.2.3)\n[pip3] torchvision (0.2.1)\n[conda] torch                     0.4.0                     \n[conda] torchtext                 0.2.3                     \n[conda] torchvision               0.2.1", "body": "## Issue Description\r\nI tried to train my model on multiple gpus. However, when I launch the program, it seems that the model is allocated to gpus, but no data is fed into model. Using ``nvidia-smi``, i find hundreds of MB of memory is consumed on each gpu. I guess these memory usage is for model initialization in each gpu.\r\n\r\nI am sharing 8 gpus with others on the server, so I limit my program on GPU 2 and GPU 3 by following command.\r\n```python\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\r\n```\r\n\r\n## Code example\r\nI ran this official tutorial on my machine and the same thing happens again. https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html\r\n\r\n## System Info\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 7.5.17\r\nGPU models and configuration:\r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN X\r\nGPU 2: GeForce GTX TITAN X\r\nGPU 3: GeForce GTX TITAN X\r\nGPU 4: GeForce GTX TITAN X\r\nGPU 5: GeForce GTX TITAN X\r\nGPU 6: GeForce GTX TITAN X\r\nGPU 7: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 375.88\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.5.1.10\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.20\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v5.a\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a\r\n/usr/local/lib/python2.7/dist-packages/torch/lib/libcudnn.so.6\r\n\r\nVersions of relevant libraries:\r\n[pip3] msgpack-numpy (0.4.1)\r\n[pip3] numpy (1.14.0)\r\n[pip3] numpydoc (0.7.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchtext (0.2.3)\r\n[pip3] torchvision (0.2.1)\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torchtext                 0.2.3                     <pip>\r\n[conda] torchvision               0.2.1                     <pip>"}