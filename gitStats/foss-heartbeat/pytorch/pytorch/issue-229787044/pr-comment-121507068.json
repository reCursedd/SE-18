{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121507068", "pull_request_review_id": 43541341, "id": 121507068, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMTUwNzA2OA==", "diff_hunk": "@@ -496,14 +503,354 @@ static bool THPTensor_(_indexOnce)(PyObject *index, int &indexed_dim,\n   return true;\n }\n \n+#ifndef TH_REAL_IS_HALF\n+static bool THPTensor_(_checkAdvancedIndexing)(THPTensor *indexed, PyObject *arg) {\n+  // Currently we only support the integer-array indexing strategy for advanced\n+  // indexing - where we have ndim sequence/LongTensor arguments\n+\n+  long ndim = THTensor_(nDimension)(LIBRARY_STATE indexed->cdata);\n+\n+  // Verify that all of the inputs are either Tensors or Sequences.\n+  if (PySequence_Check(arg) && PySequence_Size(arg) == ndim) {\n+    for (Py_ssize_t i = 0; i < ndim; ++i) {\n+      PyObject *item = PySequence_GetItem(arg, i);\n+      if (!THPIndexTensor_Check(item) && !PySequence_Check(item)) {\n+        Py_DECREF(item);\n+        return false;\n+      }\n+      Py_DECREF(item);\n+    }\n+    return true;\n+  }\n+  return false;\n+\n+  // Full NumPy advanced indexing requirements are coded up below. To fully support\n+  // such indexing will require changes to the actual indexing logic, so we will\n+  // leave this commented out as a reference\n+\n+  /**\n+  // Checks whether the specified selection object should trigger advanced\n+  // indexing\n+\n+  // Case 1: arg is a non-tuple sequence object\n+  if (PySequence_Check(arg) && !PyTuple_Check(arg)) return true;\n+\n+#ifdef WITH_NUMPY\n+  // Case 2: arg is an nd-array with type integer or bool\n+  if (PyArray_Check(arg) && (PyArray_TYPE((PyArrayObject*)arg) == NPY_INT64 || PyArray_TYPE((PyArrayObject*)arg) == NPY_BOOL)) return true;\n+#endif\n+\n+  // Case 3: arg is a tuple containing at least one sequence object, ndarray, or LongTensor\n+  if (PyTuple_Check(arg)) {\n+    for (Py_ssize_t i = 0; i < PyTuple_GET_SIZE(arg); ++i) {\n+      PyObject *item = PyTuple_GET_ITEM(arg, i);\n+      if (PySequence_Check(item)) {\n+        return true;\n+      }\n+#ifdef WITH_NUMPY\n+      if (PyArray_Check(item) && (PyArray_TYPE((PyArrayObject*)item) == NPY_INT64 || PyArray_TYPE((PyArrayObject*)item) == NPY_BOOL)) return true;\n+#endif\n+      if (THPIndexTensor_Check(item)) return true;\n+    }\n+  }\n+\n+  return false;\n+  **/\n+}\n+\n+// Exposed at the interpreter level\n+static PyObject* THPTensor_(checkAdvancedIndexing)(THPTensor *self, PyObject *arg) {\n+  if (THPTensor_(_checkAdvancedIndexing)(self, arg)) {\n+    Py_RETURN_TRUE;\n+  }\n+  Py_RETURN_FALSE;\n+}\n+\n+static bool THPTensor_(_convertToTensorIndexers)(PyObject *index, std::vector<THIndexTensor*>& broadcasted) {\n+  // At the top-level, index must be a sequence. PySequence_Fast returns a new\n+  // reference\n+  PyObject *fast = PySequence_Fast(index, NULL);\n+  Py_ssize_t seqCount = PySequence_Fast_GET_SIZE(fast);\n+  std::vector<THPIndexTensor*> indexers;\n+\n+  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n+    THPIndexTensor *indexer = (THPIndexTensor *)PyObject_CallFunctionObjArgs(THPLongTensorClass, PySequence_Fast_GET_ITEM(fast, i), NULL);\n+    if (!indexer) {\n+      PyErr_Format(PyExc_IndexError,\n+          \"When performing advanced indexing the indexing objects must be LongTensors or convertible to such\");\n+      return false;\n+    }\n+    indexers.push_back(indexer);\n+  }\n+  Py_DECREF(fast);\n+\n+  THIndexTensor **maybeBroadcasted = (THIndexTensor **)THAlloc(seqCount * sizeof(THIndexTensor*));\n+  THIndexTensor **candidates = (THIndexTensor **)THAlloc(seqCount * sizeof(THIndexTensor*));\n+\n+  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n+    maybeBroadcasted[i] = THIndexTensor_(new)(LIBRARY_STATE_NOARGS);\n+    candidates[i] = THIndexTensor_(newWithTensor)(LIBRARY_STATE indexers[i]->cdata);\n+  }\n+\n+  // Broadcast/Expand indexing Tensors as necessary\n+  bool broadcastSuccess = true;\n+  try {\n+    THIndexTensor_(expandNd)(LIBRARY_STATE maybeBroadcasted, candidates, seqCount);\n+    // Place Broadcasted Tensors into output vector, implicitly transferring\n+    // ownership\n+    for (Py_ssize_t i = 0; i < seqCount; ++i) {\n+      broadcasted.push_back(maybeBroadcasted[i]);\n+    }\n+  } catch (std::exception& e) {\n+    // Broadcasted failed, cleanup and set error\n+    for (int i = 0; i < seqCount; ++i) {\n+      THIndexTensor_(free)(LIBRARY_STATE maybeBroadcasted[i]);\n+    }\n+    broadcastSuccess = false;\n+    PyErr_Format(PyExc_IndexError, \"The advanced indexing objects could not be broadcast\");\n+  }\n+\n+  // No matter what, need to cleanup the candidates\n+  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n+    THIndexTensor_(free)(LIBRARY_STATE candidates[i]);\n+  }\n+  THFree(candidates);\n+  THFree(maybeBroadcasted);\n+\n+  return broadcastSuccess;\n+}\n+\n+// Caller takes ownership of the returned IndexTensor\n+static THIndexTensor* THPTensor_(_calculateLinearIndices)(\n+    THTensorPtr& indexed, std::vector<THIndexTensor *>& broadcasted) {\n+\n+  // Get the number of indices to generate - this will be equal to the number\n+  // of elements in each broadcasted Tensor\n+  ptrdiff_t indexingElements = THIndexTensor_(nElement)(LIBRARY_STATE broadcasted.at(0));\n+  THIndexTensor *linearIndices = THIndexTensor_(newWithSize1d)(LIBRARY_STATE indexingElements);\n+  THLongStorage *indexerSize = THLongStorage_newWithSize(1);\n+  THLongStorage_set(indexerSize, 0, indexingElements);\n+\n+  for (ptrdiff_t i = 0; i < indexingElements; ++i) {\n+    long linearIdx = THTensor_(storageOffset)(LIBRARY_STATE indexed);\n+    for (int j = broadcasted.size() - 1; j >= 0; --j) {\n+      THIndexTensor *indexer = THIndexTensor_(newContiguous)(LIBRARY_STATE broadcasted.at(j));\n+\n+      // The indexing tensor might not be one-dimensional, but we are generating a vector of\n+      // indices, so we need to view the indexer as 1D prior to getting the value for the\n+      // particular dimension\n+      THIndexTensor *oned = THIndexTensor_(newView)(LIBRARY_STATE indexer, indexerSize);\n+      linearIdx += THTensor_(stride)(LIBRARY_STATE indexed, j) * THIndexTensor_(get1d)(LIBRARY_STATE oned, i);\n+      THIndexTensor_(free)(LIBRARY_STATE oned);\n+      THIndexTensor_(free)(LIBRARY_STATE indexer);\n+    }\n+    THIndexTensor_(set1d)(LIBRARY_STATE linearIndices, i, linearIdx);\n+  }\n+  THLongStorage_free(indexerSize);\n+  return linearIndices;\n+}\n+\n+static bool THPTensor_(_advancedIndexCommonInit)(\n+    PyObject *index,\n+    THTensorPtr &indexed,\n+    std::vector<THIndexTensor*>& broadcasted,\n+    THIndexTensor **linearIndices,\n+    THTensor **flattened) {\n+\n+  // Precondition: index is an object that specifies advanced indexing.\n+  // For now, we only support the simple integer-array indexing strategy\n+  // where there are ndim(self) indexing sequences/LongTensors that can be\n+  // broadcasted and iterated as one\n+  // Precondition: tresult points to the Tensor we are indexing, and is also where\n+  // we will store the output Tensor\n+\n+  // First attempt to convert to Tensor indexers from the arbitrary\n+  // python/tensor objects passed\n+  if (!THPTensor_(_convertToTensorIndexers)(index, broadcasted)) {\n+    return false;\n+  }\n+\n+  // At this point broadcasted should store our indexing Tensors.\n+  // Our strategy is to view the indexed Tensor as a 1D Tensor, calculate\n+  // the linear indices for each tuple of indexing elements, and then call\n+  // indexSelect using those linear indices\n+  *linearIndices = THPTensor_(_calculateLinearIndices)(indexed, broadcasted);\n+\n+  *flattened = THTensor_(newWithStorage1d)(LIBRARY_STATE\n+                                           THTensor_(storage)(LIBRARY_STATE indexed.get()),\n+                                           0,\n+                                           THStorage_(size)(LIBRARY_STATE\n+                                               THTensor_(storage)(LIBRARY_STATE indexed.get())),\n+                                           1);\n+\n+  return true;\n+}\n \n+// Should called, written in such a way that if any of the parameters are not\n+// initialized we still don't crash\n+static void THPTensor_(_advancedIndexCommonCleanup)(\n+    std::vector<THIndexTensor*>& broadcasted,\n+    THIndexTensor *linearIndices,\n+    THTensor *flattened) {\n+\n+  // Cleanup all TH memory\n+  for (const auto& bTensor : broadcasted) {\n+    THIndexTensor_(free)(LIBRARY_STATE bTensor);\n+  }\n+  if (linearIndices) THIndexTensor_(free)(LIBRARY_STATE linearIndices);\n+  if (flattened) THTensor_(free)(LIBRARY_STATE flattened);\n+}\n+\n+static bool THPTensor_(_advancedIndexGet)(PyObject *index, THTensorPtr &tresult)\n+{\n+  std::vector<THIndexTensor*> broadcasted;\n+  THIndexTensor *linearIndices = NULL;\n+  THTensor *flattened = NULL;\n+  bool success = THPTensor_(_advancedIndexCommonInit)(\n+      index, tresult, broadcasted, &linearIndices, &flattened);\n+\n+  if (success) {\n+    THTensor *result = THTensor_(new)(LIBRARY_STATE_NOARGS);\n+\n+    // Index Select makes a copy of the storage, thus it is enforcing NumPy semantics, which says that the\n+    // array returned by advanced indexing is a copy, not a view\n+    THTensor_(indexSelect)(LIBRARY_STATE result, flattened, 0, linearIndices);\n+\n+    // In the event that the indexing Tensors are not vectors, we need to reshape\n+    // the result to be the appropriate shape. Note that we do not use the strides\n+    // of a broadcast Tensor, as they may have been mucked with to support the\n+    // broadcast semantics\n+    THTensor_(resizeNd)(LIBRARY_STATE result,\n+                        THIndexTensor_(nDimension)(LIBRARY_STATE broadcasted.at(0)),\n+                        broadcasted.at(0)->size,\n+                        NULL);\n+\n+    // result ptr takes ownership of result tensor, and implicitly frees the\n+    // indexed one\n+    tresult = result;\n+  }\n+\n+  THPTensor_(_advancedIndexCommonCleanup)(broadcasted, linearIndices, flattened);\n+  return success;\n+}\n+\n+static bool THPTensor_(_advancedIndexSet)(PyObject *index, THTensorPtr &dest, PyObject *src)\n+{\n+\n+  std::vector<THIndexTensor*> broadcasted;\n+  THIndexTensor *linearIndices = NULL;\n+  THTensor *flattened = NULL;\n+  bool success = THPTensor_(_advancedIndexCommonInit)(\n+      index, dest, broadcasted, &linearIndices, &flattened);\n+\n+  if (success) {\n+    if (THPUtils_(checkReal)(src)) {\n+      real v = THPUtils_(unpackReal)(src);\n+      THTensor_(indexFill)(LIBRARY_STATE flattened, 0, linearIndices, v);\n+    } else if (THPTensor_(Check)(src)) {\n+      // Because we are doing an index copy, we need to make sure of two things:\n+      // 1. the src Tensor is 1D and\n+      // 2. the src is made contiguous before being flattened into a 1D view, if\n+      // necessary\n+\n+      THTensor *contiguous;\n+      if (THTensor_(isContiguous(LIBRARY_STATE ((THPTensor*)src)->cdata))) {\n+        contiguous = THTensor_(newWithTensor)(LIBRARY_STATE ((THPTensor*)src)->cdata);\n+      } else {\n+        contiguous = THTensor_(newContiguous)(LIBRARY_STATE ((THPTensor*)src)->cdata);\n+      }\n+\n+      THTensor *cviewed = THTensor_(newWithStorage1d)(LIBRARY_STATE\n+                                                      THTensor_(storage)(LIBRARY_STATE contiguous),\n+                                                      THTensor_(storageOffset)(LIBRARY_STATE contiguous),\n+                                                      THTensor_(nElement)(LIBRARY_STATE contiguous),\n+                                                      1);\n+\n+      THTensor_(indexCopy)(LIBRARY_STATE flattened, 0, linearIndices, cviewed);\n+      THTensor_(free)(LIBRARY_STATE contiguous);\n+      THTensor_(free)(LIBRARY_STATE cviewed);\n+    } else {\n+      THPUtils_setError(\"can't assign %s to a \" THPTensorStr \" using a LongTensor \"\n+          \"(only \" THPTensorStr \" or %s are supported)\",\n+          THPUtils_typename(src), THPUtils_typeTraits<real>::python_type_str);\n+      success = false;\n+    }\n+  }\n+\n+  THPTensor_(_advancedIndexCommonCleanup)(broadcasted, linearIndices, flattened);\n+  return success;\n+}\n+\n+static bool THPTensor_(_advancedIndexAdd)(PyObject *index, THTensorPtr &dest, THTensorPtr &src) {\n+  std::vector<THIndexTensor*> broadcasted;\n+  THIndexTensor *linearIndices = NULL;\n+  THTensor *flattened = NULL;\n+  bool success = THPTensor_(_advancedIndexCommonInit)(\n+      index, dest, broadcasted, &linearIndices, &flattened);\n+\n+  if (success) {\n+    // Verify src tensor is contiguous before flattening\n+    THTensor *contiguous;\n+    if (THTensor_(isContiguous(LIBRARY_STATE src))) {", "path": "torch/csrc/generic/Tensor.cpp", "position": null, "original_position": 340, "commit_id": "99ce824c967a7ed87303842db5478d6b7d509b2f", "original_commit_id": "0518639ea4708a64d459f13c40e7a0b79b438a69", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "Same thing here about the `contiguous` check.", "created_at": "2017-06-12T19:40:31Z", "updated_at": "2018-11-23T15:33:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/1588#discussion_r121507068", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1588", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/121507068"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1588#discussion_r121507068"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1588"}}, "body_html": "<p>Same thing here about the <code>contiguous</code> check.</p>", "body_text": "Same thing here about the contiguous check."}