{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/122078233", "pull_request_review_id": 44163008, "id": 122078233, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyMjA3ODIzMw==", "diff_hunk": "@@ -496,14 +503,363 @@ static bool THPTensor_(_indexOnce)(PyObject *index, int &indexed_dim,\n   return true;\n }\n \n+#ifndef TH_REAL_IS_HALF\n+static bool THPTensor_(_checkAdvancedIndexing)(THPTensor *indexed, PyObject *arg) {\n+  // Currently we only support the integer-array indexing strategy for advanced\n+  // indexing - where we have ndim sequence/LongTensor arguments\n+\n+  long ndim = THTensor_(nDimension)(LIBRARY_STATE indexed->cdata);\n+\n+  // Verify that all of the inputs are either Tensors or Sequences.\n+  if (PySequence_Check(arg) && PySequence_Size(arg) == ndim) {\n+    for (Py_ssize_t i = 0; i < ndim; ++i) {\n+      PyObject *item = PySequence_GetItem(arg, i);\n+      if (!THPIndexTensor_Check(item) && !PySequence_Check(item)) {\n+        Py_DECREF(item);\n+        return false;\n+      }\n+      Py_DECREF(item);\n+    }\n+    return true;\n+  }\n+  return false;\n+\n+  // Full NumPy advanced indexing requirements are coded up below. To fully support\n+  // such indexing will require changes to the actual indexing logic, so we will\n+  // leave this commented out as a reference\n+\n+  /**\n+  // Checks whether the specified selection object should trigger advanced\n+  // indexing\n+\n+  // Case 1: arg is a non-tuple sequence object\n+  if (PySequence_Check(arg) && !PyTuple_Check(arg)) return true;\n+\n+#ifdef WITH_NUMPY\n+  // Case 2: arg is an nd-array with type integer or bool\n+  if (PyArray_Check(arg) && (PyArray_TYPE((PyArrayObject*)arg) == NPY_INT64 || PyArray_TYPE((PyArrayObject*)arg) == NPY_BOOL)) return true;\n+#endif\n+\n+  // Case 3: arg is a tuple containing at least one sequence object, ndarray, or LongTensor\n+  if (PyTuple_Check(arg)) {\n+    for (Py_ssize_t i = 0; i < PyTuple_GET_SIZE(arg); ++i) {\n+      PyObject *item = PyTuple_GET_ITEM(arg, i);\n+      if (PySequence_Check(item)) {\n+        return true;\n+      }\n+#ifdef WITH_NUMPY\n+      if (PyArray_Check(item) && (PyArray_TYPE((PyArrayObject*)item) == NPY_INT64 || PyArray_TYPE((PyArrayObject*)item) == NPY_BOOL)) return true;\n+#endif\n+      if (THPIndexTensor_Check(item)) return true;\n+    }\n+  }\n+\n+  return false;\n+  **/\n+}\n+\n+// Exposed at the interpreter level\n+static PyObject* THPTensor_(checkAdvancedIndexing)(THPTensor *self, PyObject *arg) {\n+  if (THPTensor_(_checkAdvancedIndexing)(self, arg)) {\n+    Py_RETURN_TRUE;\n+  }\n+  Py_RETURN_FALSE;\n+}\n+\n+static bool THPTensor_(_convertToTensorIndexers)(PyObject *index, std::vector<THIndexTensor*>& broadcasted) {\n+  // At the top-level, index must be a sequence. PySequence_Fast returns a new\n+  // reference\n+  PyObject *fast = PySequence_Fast(index, NULL);\n+  Py_ssize_t seqCount = PySequence_Fast_GET_SIZE(fast);\n+  std::vector<THPIndexTensor*> indexers;\n \n+  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n+    THPIndexTensor *indexer = (THPIndexTensor *)PyObject_CallFunctionObjArgs(THPLongTensorClass, PySequence_Fast_GET_ITEM(fast, i), NULL);\n+    if (!indexer) {\n+      PyErr_Format(PyExc_IndexError,\n+          \"When performing advanced indexing the indexing objects must be LongTensors or convertible to such\");\n+      return false;\n+    }\n+    indexers.push_back(indexer);\n+  }\n+  Py_DECREF(fast);\n+\n+  THIndexTensor **maybeBroadcasted = (THIndexTensor **)THAlloc(seqCount * sizeof(THIndexTensor*));\n+  THIndexTensor **candidates = (THIndexTensor **)THAlloc(seqCount * sizeof(THIndexTensor*));\n+\n+  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n+    maybeBroadcasted[i] = THIndexTensor_(new)(LIBRARY_STATE_NOARGS);\n+    candidates[i] = THIndexTensor_(newWithTensor)(LIBRARY_STATE indexers[i]->cdata);\n+  }\n+\n+  // Broadcast/Expand indexing Tensors as necessary\n+  bool broadcastSuccess = true;\n+  try {\n+    THIndexTensor_(expandNd)(LIBRARY_STATE maybeBroadcasted, candidates, seqCount);\n+    // Place Broadcasted Tensors into output vector, implicitly transferring\n+    // ownership\n+    for (Py_ssize_t i = 0; i < seqCount; ++i) {\n+      broadcasted.push_back(maybeBroadcasted[i]);\n+    }\n+  } catch (std::exception& e) {\n+    // Broadcasted failed, cleanup and set error\n+    for (int i = 0; i < seqCount; ++i) {\n+      THIndexTensor_(free)(LIBRARY_STATE maybeBroadcasted[i]);\n+    }\n+    broadcastSuccess = false;\n+    PyErr_Format(PyExc_IndexError, \"The advanced indexing objects could not be broadcast\");\n+  }\n+\n+  // No matter what, need to cleanup the candidates\n+  for (Py_ssize_t i = 0; i < seqCount; ++i) {\n+    THIndexTensor_(free)(LIBRARY_STATE candidates[i]);\n+  }\n+  THFree(candidates);\n+  THFree(maybeBroadcasted);\n+\n+  return broadcastSuccess;\n+}\n+\n+// Caller takes ownership of the returned IndexTensor\n+static THIndexTensor* THPTensor_(_calculateLinearIndices)(\n+    THTensorPtr& indexed, std::vector<THIndexTensor *>& broadcasted) {\n+\n+  // Get the number of indices to generate - this will be equal to the number\n+  // of elements in each broadcasted Tensor\n+  ptrdiff_t indexingElements = THIndexTensor_(nElement)(LIBRARY_STATE broadcasted.at(0));\n+  THIndexTensor *linearIndices = THIndexTensor_(newWithSize1d)(LIBRARY_STATE indexingElements);\n+  THLongStorage *indexerSize = THLongStorage_newWithSize(1);\n+  THLongStorage_set(indexerSize, 0, indexingElements);\n+\n+  for (ptrdiff_t i = 0; i < indexingElements; ++i) {\n+    long linearIdx = THTensor_(storageOffset)(LIBRARY_STATE indexed);\n+    for (int j = broadcasted.size() - 1; j >= 0; --j) {\n+      // Given a series of broadcasted Tensors, we take the jth tensor from the sequence (representing\n+      // the indices at dim j) and grab the ith value (used in generating the ith output value)\n+      THIndexTensor *indexer = THIndexTensor_(newContiguous)(LIBRARY_STATE broadcasted.at(j));\n+\n+      // The indexing tensor might not be one-dimensional, but we are generating a vector of\n+      // indices, so we need to view the indexer as 1D prior to getting the value for the\n+      // particular dimension\n+      THIndexTensor *oned = THIndexTensor_(newView)(LIBRARY_STATE indexer, indexerSize);\n+\n+      // Actually laod the value, and verify it is not out-of-bounds\n+      long indexAtDim = THIndexTensor_(get1d)(LIBRARY_STATE oned, i);", "path": "torch/csrc/generic/Tensor.cpp", "position": null, "original_position": 193, "commit_id": "99ce824c967a7ed87303842db5478d6b7d509b2f", "original_commit_id": "93155cecf3952514ce56ef7254657bd024d80c92", "user": {"login": "killeent", "id": 4529377, "node_id": "MDQ6VXNlcjQ1MjkzNzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/4529377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/killeent", "html_url": "https://github.com/killeent", "followers_url": "https://api.github.com/users/killeent/followers", "following_url": "https://api.github.com/users/killeent/following{/other_user}", "gists_url": "https://api.github.com/users/killeent/gists{/gist_id}", "starred_url": "https://api.github.com/users/killeent/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/killeent/subscriptions", "organizations_url": "https://api.github.com/users/killeent/orgs", "repos_url": "https://api.github.com/users/killeent/repos", "events_url": "https://api.github.com/users/killeent/events{/privacy}", "received_events_url": "https://api.github.com/users/killeent/received_events", "type": "User", "site_admin": false}, "body": "In the short term to get this checked in, I may have to keep it all on the GPU. Unfortunately dealing with non-generic types in `generic/Tensor.cpp` is a little tricky. Ultimately, all the index calculation will be done on the GPU, and only once we are ready to do the indexSelect will we copy that indexing Tensor back to the GPU.\r\n\r\nIn the long term, index calculation could be a kernel in THC.", "created_at": "2017-06-14T22:14:50Z", "updated_at": "2018-11-23T15:33:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/1588#discussion_r122078233", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1588", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/122078233"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1588#discussion_r122078233"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1588"}}, "body_html": "<p>In the short term to get this checked in, I may have to keep it all on the GPU. Unfortunately dealing with non-generic types in <code>generic/Tensor.cpp</code> is a little tricky. Ultimately, all the index calculation will be done on the GPU, and only once we are ready to do the indexSelect will we copy that indexing Tensor back to the GPU.</p>\n<p>In the long term, index calculation could be a kernel in THC.</p>", "body_text": "In the short term to get this checked in, I may have to keep it all on the GPU. Unfortunately dealing with non-generic types in generic/Tensor.cpp is a little tricky. Ultimately, all the index calculation will be done on the GPU, and only once we are ready to do the indexSelect will we copy that indexing Tensor back to the GPU.\nIn the long term, index calculation could be a kernel in THC.", "in_reply_to_id": 122041916}