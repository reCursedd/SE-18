{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/117783217", "pull_request_review_id": 39518753, "id": 117783217, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNzc4MzIxNw==", "diff_hunk": "@@ -498,14 +498,115 @@ static bool THPTensor_(_indexOnce)(PyObject *index, int &indexed_dim,\n   return true;\n }\n \n-static bool THPTensor_(_advancedIndex)(THPTensor *self, PyObject *index,\n-    THTensorPtr &tresult, THStorage * &sresult, long &storage_offset)\n+#ifndef TH_REAL_IS_HALF\n+static bool THPTensor_(checkAdvancedIndexing)(THPTensor *indexed, PyObject *arg) {\n+  // MVP: ndim LongTensor arguments\n+  long ndim = THTensor_(nDimension)(LIBRARY_STATE indexed->cdata);\n+\n+  if (PySequence_Check(arg)) {\n+    Py_ssize_t indexers = PySequence_Size(arg);\n+    if (indexers == ndim) {\n+      for (Py_ssize_t i = 0; i < indexers; ++i) {\n+        PyObject *item = PySequence_GetItem(arg, i);\n+        if (!THPIndexTensor_Check(item)) {\n+          return false;\n+        }\n+      }\n+      return true;\n+    }\n+  }\n+  return false;\n+\n+  /**\n+  // Checks whether the specified selection object should trigger advanced\n+  // indexing\n+\n+  // Case 1: arg is a non-tuple sequence object\n+  if (PySequence_Check(arg) && !PyTuple_Check(arg)) return true;\n+\n+#ifdef WITH_NUMPY\n+  // Case 2: arg is an nd-array with type integer or bool\n+  if (PyArray_Check(arg) && (PyArray_TYPE((PyArrayObject*)arg) == NPY_INT64 || PyArray_TYPE((PyArrayObject*)arg) == NPY_BOOL)) return true;\n+#endif\n+\n+  // Case 3: arg is a tuple containing at least one sequence object or ndarray\n+  if (PyTuple_Check(arg)) {\n+    for (Py_ssize_t i = 0; i < PyTuple_GET_SIZE(arg); ++i) {\n+      PyObject *item = PyTuple_GET_ITEM(arg, i);\n+      if (PySequence_Check(item)) {\n+        return true;\n+      }\n+      // TODO: add check for LongTensor?\n+#ifdef WITH_NUMPY\n+      if (PyArray_Check(item) && (PyArray_TYPE((PyArrayObject*)item) == NPY_INT64 || PyArray_TYPE((PyArrayObject*)item) == NPY_BOOL)) return true;\n+#endif\n+    }\n+  }\n+\n+  return false;\n+  **/\n+}\n+\n+static bool THPTensor_(_advancedIndex)(\n+    PyObject *index, THTensorPtr &tresult, THStorage * &sresult, long &storage_offset)\n {\n   // Precondition: index is an object that specifies advanced indexing.\n   // For now, we only support the simple integer-array indexing strategy\n   // where there are ndim(self) indexing Long Tensors that can be broadcasted\n   // and iterated as one\n+  // TODO: empty indexer?\n+\n+  // First, verify that all of the indexers have the same shape, later we will\n+  // incorporate broadcasting\n+  // Also assuming for now they are all 1D, and not 2D, 3D, etc.\n+  Py_ssize_t size = PySequence_Size(index);\n+  THIndexTensor *first = ((THPIndexTensor *)PySequence_GetItem(index, 0))->cdata;\n+  for (Py_ssize_t i = 1; i < size; ++i) {\n+    if (!THIndexTensor_(isSameSizeAs)(LIBRARY_STATE first, ((THPIndexTensor *)PySequence_GetItem(index, i))->cdata)) {\n+      PyErr_Format(PyExc_IndexError,\n+          \"When performing advanced indexing the shapes of the indexing Tensors must be the same\");\n+      return false;\n+    }\n+  }\n+\n+  // Our strategy is to view the indexed Tensor as a 1D Tensor, calculate\n+  // the linear indices for each tuple of indexing elements, and then call\n+  // indexSelect using those linear indices\n+  // TODO: example\n+\n+  THIndexTensor *linearIndices = THIndexTensor_(newWithSize1d)(LIBRARY_STATE THIndexTensor_(nElement)(LIBRARY_STATE first));\n+  for (ptrdiff_t i = 0; i < THIndexTensor_(nElement)(LIBRARY_STATE linearIndices); ++i) {\n+    long stride = 1; // is this an incorrect assumption? could the stride at the last dim be non-one?", "path": "torch/csrc/generic/Tensor.cpp", "position": null, "original_position": 84, "commit_id": "99ce824c967a7ed87303842db5478d6b7d509b2f", "original_commit_id": "55c970440787fb9f813bbbc091782fac91cbc04d", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "If the tensor is not contiguous, then the stride of the last dimension might not be 1.", "created_at": "2017-05-22T16:01:40Z", "updated_at": "2018-11-23T15:33:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/1588#discussion_r117783217", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1588", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/117783217"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1588#discussion_r117783217"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1588"}}, "body_html": "<p>If the tensor is not contiguous, then the stride of the last dimension might not be 1.</p>", "body_text": "If the tensor is not contiguous, then the stride of the last dimension might not be 1."}