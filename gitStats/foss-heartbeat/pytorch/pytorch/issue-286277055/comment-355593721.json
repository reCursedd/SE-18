{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355593721", "html_url": "https://github.com/pytorch/pytorch/issues/4495#issuecomment-355593721", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4495", "id": 355593721, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTU5MzcyMQ==", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-05T16:11:16Z", "updated_at": "2018-01-05T16:11:16Z", "author_association": "COLLABORATOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2005323\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lanpa\">@lanpa</a>, I want to make sure I'm reproducing what you see. This is what I get on AlexNet (I'm working on current master <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/408c84de7c4d852beffb9c64e2351b16cfff8a1f/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/408c84de7c4d852beffb9c64e2351b16cfff8a1f\"><tt>408c84d</tt></a>):</p>\n<pre><code>class AlexNet(nn.Module):\n\n        def __init__(self, num_classes=1000):\n            super(AlexNet, self).__init__()\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                nn.Conv2d(64, 192, kernel_size=5, padding=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                nn.Conv2d(192, 384, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(384, 256, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n            )\n            self.classifier = nn.Sequential(\n                nn.Dropout(),\n                nn.Linear(256 * 6 * 6, 4096),\n                nn.ReLU(inplace=True),\n                nn.Dropout(),\n                nn.Linear(4096, 4096),\n                nn.ReLU(inplace=True),\n                nn.Linear(4096, num_classes),\n            )\n\n        def forward(self, x):\n            x = self.features(x)\n            x = x.view(x.size(0), 256 * 6 * 6)\n            x = self.classifier(x)\n            return x\n\n\na = AlexNet()\n\nt = Variable(torch.ones(1, 3, 227, 227), requires_grad=True)\n\ntraced, _ = torch.jit.trace(a, (t, ))\ng = torch._C._jit_get_graph(traced)\nprint(g)\n</code></pre>\n<p>outputs</p>\n<pre><code>graph(%0 : Float(1, 3, 227, 227)) {\n  %1 : Float(64, 3, 11, 11) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %2 : Float(64) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %3 : Float(1, 64, 56, 56), %4 : Float(1, 363, 3136), %5 : Float(0) = thnn_conv2d_forward[kernel_size=[11, 11], stride=[4, 4], padding=[2, 2]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %6 : Float(1, 64, 56, 56) = thnn_conv2d[kernel_size=[11, 11], stride=[4, 4], padding=[2, 2]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %7 : Float(1, 64, 56, 56) = _convolution_nogroup[stride=[4, 4], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %8 : Float(1, 64, 56, 56) = _convolution[stride=[4, 4], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %9 : Float(1, 64, 56, 56) = threshold_forward[threshold={0}, value={0}](%8), scope: AlexNet/Sequential[features]/ReLU[1]\n  %10 : Float(1, 64, 56, 56) = threshold[threshold={0}, value={0}](%9), scope: AlexNet/Sequential[features]/ReLU[1]\n  %11 : Float(1, 64, 27, 27), %12 : Long(1, 64, 27, 27) = max_pool2d_forward[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%10), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  %13 : Float(1, 64, 27, 27), %14 : Long(1, 64, 27, 27) = max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%10), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  %15 : Float(192, 64, 5, 5) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %16 : Float(192) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %17 : Float(1, 192, 27, 27), %18 : Float(1, 1600, 729), %19 : Float(0) = thnn_conv2d_forward[kernel_size=[5, 5], stride=[1, 1], padding=[2, 2]](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %20 : Float(1, 192, 27, 27) = thnn_conv2d[kernel_size=[5, 5], stride=[1, 1], padding=[2, 2]](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %21 : Float(1, 192, 27, 27) = _convolution_nogroup[stride=[1, 1], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %22 : Float(1, 192, 27, 27) = _convolution[stride=[1, 1], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %23 : Float(1, 192, 27, 27) = threshold_forward[threshold={0}, value={0}](%22), scope: AlexNet/Sequential[features]/ReLU[4]\n  %24 : Float(1, 192, 27, 27) = threshold[threshold={0}, value={0}](%23), scope: AlexNet/Sequential[features]/ReLU[4]\n  %25 : Float(1, 192, 13, 13), %26 : Long(1, 192, 13, 13) = max_pool2d_forward[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%24), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n  %27 : Float(1, 192, 13, 13), %28 : Long(1, 192, 13, 13) = max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%24), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n  %29 : Float(384, 192, 3, 3) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %30 : Float(384) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %31 : Float(1, 384, 13, 13), %32 : Float(1, 1728, 169), %33 : Float(0) = thnn_conv2d_forward[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %34 : Float(1, 384, 13, 13) = thnn_conv2d[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %35 : Float(1, 384, 13, 13) = _convolution_nogroup[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %36 : Float(1, 384, 13, 13) = _convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %37 : Float(1, 384, 13, 13) = threshold_forward[threshold={0}, value={0}](%36), scope: AlexNet/Sequential[features]/ReLU[7]\n  %38 : Float(1, 384, 13, 13) = threshold[threshold={0}, value={0}](%37), scope: AlexNet/Sequential[features]/ReLU[7]\n  %39 : Float(256, 384, 3, 3) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %40 : Float(256) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %41 : Float(1, 256, 13, 13), %42 : Float(1, 3456, 169), %43 : Float(0) = thnn_conv2d_forward[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %44 : Float(1, 256, 13, 13) = thnn_conv2d[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %45 : Float(1, 256, 13, 13) = _convolution_nogroup[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %46 : Float(1, 256, 13, 13) = _convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %47 : Float(1, 256, 13, 13) = threshold_forward[threshold={0}, value={0}](%46), scope: AlexNet/Sequential[features]/ReLU[9]\n  %48 : Float(1, 256, 13, 13) = threshold[threshold={0}, value={0}](%47), scope: AlexNet/Sequential[features]/ReLU[9]\n  %49 : Float(256, 256, 3, 3) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %50 : Float(256) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %51 : Float(1, 256, 13, 13), %52 : Float(1, 2304, 169), %53 : Float(0) = thnn_conv2d_forward[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %54 : Float(1, 256, 13, 13) = thnn_conv2d[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %55 : Float(1, 256, 13, 13) = _convolution_nogroup[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %56 : Float(1, 256, 13, 13) = _convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %57 : Float(1, 256, 13, 13) = threshold_forward[threshold={0}, value={0}](%56), scope: AlexNet/Sequential[features]/ReLU[11]\n  %58 : Float(1, 256, 13, 13) = threshold[threshold={0}, value={0}](%57), scope: AlexNet/Sequential[features]/ReLU[11]\n  %59 : Float(1, 256, 6, 6), %60 : Long(1, 256, 6, 6) = max_pool2d_forward[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%58), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  %61 : Float(1, 256, 6, 6), %62 : Long(1, 256, 6, 6) = max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%58), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  %63 : Float(1, 9216) = view[size=[1, 9216]](%61), scope: AlexNet\n  %64 : Float(1, 9216), %65 : Handle = ^Dropout(0.5, True, False)(%63), scope: AlexNet/Sequential[classifier]/Dropout[0]\n  %66 : Float(1, 4096) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[classifier]/Linear[1]\n  %67 : Float(9216!, 4096!) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[classifier]/Linear[1]\n  %68 : Float(1, 4096) = addmm[beta={1}, alpha={1}](%66, %64, %67), scope: AlexNet/Sequential[classifier]/Linear[1]\n  %69 : Float(1, 4096) = threshold_forward[threshold={0}, value={0}](%68), scope: AlexNet/Sequential[classifier]/ReLU[2]\n  %70 : Float(1, 4096) = threshold[threshold={0}, value={0}](%69), scope: AlexNet/Sequential[classifier]/ReLU[2]\n  %71 : Float(1, 4096), %72 : Handle = ^Dropout(0.5, True, False)(%70), scope: AlexNet/Sequential[classifier]/Dropout[3]\n  %73 : Float(1, 4096) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[classifier]/Linear[4]\n  %74 : Float(4096!, 4096!) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[classifier]/Linear[4]\n  %75 : Float(1, 4096) = addmm[beta={1}, alpha={1}](%73, %71, %74), scope: AlexNet/Sequential[classifier]/Linear[4]\n  %76 : Float(1, 4096) = threshold_forward[threshold={0}, value={0}](%75), scope: AlexNet/Sequential[classifier]/ReLU[5]\n  %77 : Float(1, 4096) = threshold[threshold={0}, value={0}](%76), scope: AlexNet/Sequential[classifier]/ReLU[5]\n  %78 : Float(1, 1000) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[classifier]/Linear[6]\n  %79 : Float(4096!, 1000!) = Constant[value=&lt;Tensor&gt;](), scope: AlexNet/Sequential[classifier]/Linear[6]\n  %80 : Float(1, 1000) = addmm[beta={1}, alpha={1}](%78, %77, %79), scope: AlexNet/Sequential[classifier]/Linear[6]\n  return (%80);\n}\n</code></pre>", "body_text": "Hi @lanpa, I want to make sure I'm reproducing what you see. This is what I get on AlexNet (I'm working on current master 408c84d):\nclass AlexNet(nn.Module):\n\n        def __init__(self, num_classes=1000):\n            super(AlexNet, self).__init__()\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                nn.Conv2d(64, 192, kernel_size=5, padding=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n                nn.Conv2d(192, 384, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(384, 256, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2),\n            )\n            self.classifier = nn.Sequential(\n                nn.Dropout(),\n                nn.Linear(256 * 6 * 6, 4096),\n                nn.ReLU(inplace=True),\n                nn.Dropout(),\n                nn.Linear(4096, 4096),\n                nn.ReLU(inplace=True),\n                nn.Linear(4096, num_classes),\n            )\n\n        def forward(self, x):\n            x = self.features(x)\n            x = x.view(x.size(0), 256 * 6 * 6)\n            x = self.classifier(x)\n            return x\n\n\na = AlexNet()\n\nt = Variable(torch.ones(1, 3, 227, 227), requires_grad=True)\n\ntraced, _ = torch.jit.trace(a, (t, ))\ng = torch._C._jit_get_graph(traced)\nprint(g)\n\noutputs\ngraph(%0 : Float(1, 3, 227, 227)) {\n  %1 : Float(64, 3, 11, 11) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %2 : Float(64) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %3 : Float(1, 64, 56, 56), %4 : Float(1, 363, 3136), %5 : Float(0) = thnn_conv2d_forward[kernel_size=[11, 11], stride=[4, 4], padding=[2, 2]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %6 : Float(1, 64, 56, 56) = thnn_conv2d[kernel_size=[11, 11], stride=[4, 4], padding=[2, 2]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %7 : Float(1, 64, 56, 56) = _convolution_nogroup[stride=[4, 4], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %8 : Float(1, 64, 56, 56) = _convolution[stride=[4, 4], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %9 : Float(1, 64, 56, 56) = threshold_forward[threshold={0}, value={0}](%8), scope: AlexNet/Sequential[features]/ReLU[1]\n  %10 : Float(1, 64, 56, 56) = threshold[threshold={0}, value={0}](%9), scope: AlexNet/Sequential[features]/ReLU[1]\n  %11 : Float(1, 64, 27, 27), %12 : Long(1, 64, 27, 27) = max_pool2d_forward[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%10), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  %13 : Float(1, 64, 27, 27), %14 : Long(1, 64, 27, 27) = max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%10), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  %15 : Float(192, 64, 5, 5) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %16 : Float(192) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %17 : Float(1, 192, 27, 27), %18 : Float(1, 1600, 729), %19 : Float(0) = thnn_conv2d_forward[kernel_size=[5, 5], stride=[1, 1], padding=[2, 2]](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %20 : Float(1, 192, 27, 27) = thnn_conv2d[kernel_size=[5, 5], stride=[1, 1], padding=[2, 2]](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %21 : Float(1, 192, 27, 27) = _convolution_nogroup[stride=[1, 1], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %22 : Float(1, 192, 27, 27) = _convolution[stride=[1, 1], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %23 : Float(1, 192, 27, 27) = threshold_forward[threshold={0}, value={0}](%22), scope: AlexNet/Sequential[features]/ReLU[4]\n  %24 : Float(1, 192, 27, 27) = threshold[threshold={0}, value={0}](%23), scope: AlexNet/Sequential[features]/ReLU[4]\n  %25 : Float(1, 192, 13, 13), %26 : Long(1, 192, 13, 13) = max_pool2d_forward[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%24), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n  %27 : Float(1, 192, 13, 13), %28 : Long(1, 192, 13, 13) = max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%24), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n  %29 : Float(384, 192, 3, 3) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %30 : Float(384) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %31 : Float(1, 384, 13, 13), %32 : Float(1, 1728, 169), %33 : Float(0) = thnn_conv2d_forward[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %34 : Float(1, 384, 13, 13) = thnn_conv2d[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %35 : Float(1, 384, 13, 13) = _convolution_nogroup[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %36 : Float(1, 384, 13, 13) = _convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %37 : Float(1, 384, 13, 13) = threshold_forward[threshold={0}, value={0}](%36), scope: AlexNet/Sequential[features]/ReLU[7]\n  %38 : Float(1, 384, 13, 13) = threshold[threshold={0}, value={0}](%37), scope: AlexNet/Sequential[features]/ReLU[7]\n  %39 : Float(256, 384, 3, 3) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %40 : Float(256) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %41 : Float(1, 256, 13, 13), %42 : Float(1, 3456, 169), %43 : Float(0) = thnn_conv2d_forward[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %44 : Float(1, 256, 13, 13) = thnn_conv2d[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %45 : Float(1, 256, 13, 13) = _convolution_nogroup[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %46 : Float(1, 256, 13, 13) = _convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %47 : Float(1, 256, 13, 13) = threshold_forward[threshold={0}, value={0}](%46), scope: AlexNet/Sequential[features]/ReLU[9]\n  %48 : Float(1, 256, 13, 13) = threshold[threshold={0}, value={0}](%47), scope: AlexNet/Sequential[features]/ReLU[9]\n  %49 : Float(256, 256, 3, 3) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %50 : Float(256) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %51 : Float(1, 256, 13, 13), %52 : Float(1, 2304, 169), %53 : Float(0) = thnn_conv2d_forward[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %54 : Float(1, 256, 13, 13) = thnn_conv2d[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %55 : Float(1, 256, 13, 13) = _convolution_nogroup[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %56 : Float(1, 256, 13, 13) = _convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %57 : Float(1, 256, 13, 13) = threshold_forward[threshold={0}, value={0}](%56), scope: AlexNet/Sequential[features]/ReLU[11]\n  %58 : Float(1, 256, 13, 13) = threshold[threshold={0}, value={0}](%57), scope: AlexNet/Sequential[features]/ReLU[11]\n  %59 : Float(1, 256, 6, 6), %60 : Long(1, 256, 6, 6) = max_pool2d_forward[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%58), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  %61 : Float(1, 256, 6, 6), %62 : Long(1, 256, 6, 6) = max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%58), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  %63 : Float(1, 9216) = view[size=[1, 9216]](%61), scope: AlexNet\n  %64 : Float(1, 9216), %65 : Handle = ^Dropout(0.5, True, False)(%63), scope: AlexNet/Sequential[classifier]/Dropout[0]\n  %66 : Float(1, 4096) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[1]\n  %67 : Float(9216!, 4096!) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[1]\n  %68 : Float(1, 4096) = addmm[beta={1}, alpha={1}](%66, %64, %67), scope: AlexNet/Sequential[classifier]/Linear[1]\n  %69 : Float(1, 4096) = threshold_forward[threshold={0}, value={0}](%68), scope: AlexNet/Sequential[classifier]/ReLU[2]\n  %70 : Float(1, 4096) = threshold[threshold={0}, value={0}](%69), scope: AlexNet/Sequential[classifier]/ReLU[2]\n  %71 : Float(1, 4096), %72 : Handle = ^Dropout(0.5, True, False)(%70), scope: AlexNet/Sequential[classifier]/Dropout[3]\n  %73 : Float(1, 4096) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[4]\n  %74 : Float(4096!, 4096!) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[4]\n  %75 : Float(1, 4096) = addmm[beta={1}, alpha={1}](%73, %71, %74), scope: AlexNet/Sequential[classifier]/Linear[4]\n  %76 : Float(1, 4096) = threshold_forward[threshold={0}, value={0}](%75), scope: AlexNet/Sequential[classifier]/ReLU[5]\n  %77 : Float(1, 4096) = threshold[threshold={0}, value={0}](%76), scope: AlexNet/Sequential[classifier]/ReLU[5]\n  %78 : Float(1, 1000) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[6]\n  %79 : Float(4096!, 1000!) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[6]\n  %80 : Float(1, 1000) = addmm[beta={1}, alpha={1}](%78, %77, %79), scope: AlexNet/Sequential[classifier]/Linear[6]\n  return (%80);\n}", "body": "Hi @lanpa, I want to make sure I'm reproducing what you see. This is what I get on AlexNet (I'm working on current master 408c84de7c4d852beffb9c64e2351b16cfff8a1f):\r\n\r\n```\r\nclass AlexNet(nn.Module):\r\n\r\n        def __init__(self, num_classes=1000):\r\n            super(AlexNet, self).__init__()\r\n            self.features = nn.Sequential(\r\n                nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\r\n                nn.ReLU(inplace=True),\r\n                nn.MaxPool2d(kernel_size=3, stride=2),\r\n                nn.Conv2d(64, 192, kernel_size=5, padding=2),\r\n                nn.ReLU(inplace=True),\r\n                nn.MaxPool2d(kernel_size=3, stride=2),\r\n                nn.Conv2d(192, 384, kernel_size=3, padding=1),\r\n                nn.ReLU(inplace=True),\r\n                nn.Conv2d(384, 256, kernel_size=3, padding=1),\r\n                nn.ReLU(inplace=True),\r\n                nn.Conv2d(256, 256, kernel_size=3, padding=1),\r\n                nn.ReLU(inplace=True),\r\n                nn.MaxPool2d(kernel_size=3, stride=2),\r\n            )\r\n            self.classifier = nn.Sequential(\r\n                nn.Dropout(),\r\n                nn.Linear(256 * 6 * 6, 4096),\r\n                nn.ReLU(inplace=True),\r\n                nn.Dropout(),\r\n                nn.Linear(4096, 4096),\r\n                nn.ReLU(inplace=True),\r\n                nn.Linear(4096, num_classes),\r\n            )\r\n\r\n        def forward(self, x):\r\n            x = self.features(x)\r\n            x = x.view(x.size(0), 256 * 6 * 6)\r\n            x = self.classifier(x)\r\n            return x\r\n\r\n\r\na = AlexNet()\r\n\r\nt = Variable(torch.ones(1, 3, 227, 227), requires_grad=True)\r\n\r\ntraced, _ = torch.jit.trace(a, (t, ))\r\ng = torch._C._jit_get_graph(traced)\r\nprint(g)\r\n```\r\n\r\noutputs\r\n\r\n```\r\ngraph(%0 : Float(1, 3, 227, 227)) {\r\n  %1 : Float(64, 3, 11, 11) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[0]\r\n  %2 : Float(64) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[0]\r\n  %3 : Float(1, 64, 56, 56), %4 : Float(1, 363, 3136), %5 : Float(0) = thnn_conv2d_forward[kernel_size=[11, 11], stride=[4, 4], padding=[2, 2]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\r\n  %6 : Float(1, 64, 56, 56) = thnn_conv2d[kernel_size=[11, 11], stride=[4, 4], padding=[2, 2]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\r\n  %7 : Float(1, 64, 56, 56) = _convolution_nogroup[stride=[4, 4], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\r\n  %8 : Float(1, 64, 56, 56) = _convolution[stride=[4, 4], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\r\n  %9 : Float(1, 64, 56, 56) = threshold_forward[threshold={0}, value={0}](%8), scope: AlexNet/Sequential[features]/ReLU[1]\r\n  %10 : Float(1, 64, 56, 56) = threshold[threshold={0}, value={0}](%9), scope: AlexNet/Sequential[features]/ReLU[1]\r\n  %11 : Float(1, 64, 27, 27), %12 : Long(1, 64, 27, 27) = max_pool2d_forward[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%10), scope: AlexNet/Sequential[features]/MaxPool2d[2]\r\n  %13 : Float(1, 64, 27, 27), %14 : Long(1, 64, 27, 27) = max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%10), scope: AlexNet/Sequential[features]/MaxPool2d[2]\r\n  %15 : Float(192, 64, 5, 5) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[3]\r\n  %16 : Float(192) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[3]\r\n  %17 : Float(1, 192, 27, 27), %18 : Float(1, 1600, 729), %19 : Float(0) = thnn_conv2d_forward[kernel_size=[5, 5], stride=[1, 1], padding=[2, 2]](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\r\n  %20 : Float(1, 192, 27, 27) = thnn_conv2d[kernel_size=[5, 5], stride=[1, 1], padding=[2, 2]](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\r\n  %21 : Float(1, 192, 27, 27) = _convolution_nogroup[stride=[1, 1], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\r\n  %22 : Float(1, 192, 27, 27) = _convolution[stride=[1, 1], padding=[2, 2], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%13, %15, %16), scope: AlexNet/Sequential[features]/Conv2d[3]\r\n  %23 : Float(1, 192, 27, 27) = threshold_forward[threshold={0}, value={0}](%22), scope: AlexNet/Sequential[features]/ReLU[4]\r\n  %24 : Float(1, 192, 27, 27) = threshold[threshold={0}, value={0}](%23), scope: AlexNet/Sequential[features]/ReLU[4]\r\n  %25 : Float(1, 192, 13, 13), %26 : Long(1, 192, 13, 13) = max_pool2d_forward[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%24), scope: AlexNet/Sequential[features]/MaxPool2d[5]\r\n  %27 : Float(1, 192, 13, 13), %28 : Long(1, 192, 13, 13) = max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%24), scope: AlexNet/Sequential[features]/MaxPool2d[5]\r\n  %29 : Float(384, 192, 3, 3) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[6]\r\n  %30 : Float(384) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[6]\r\n  %31 : Float(1, 384, 13, 13), %32 : Float(1, 1728, 169), %33 : Float(0) = thnn_conv2d_forward[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\r\n  %34 : Float(1, 384, 13, 13) = thnn_conv2d[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\r\n  %35 : Float(1, 384, 13, 13) = _convolution_nogroup[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\r\n  %36 : Float(1, 384, 13, 13) = _convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%27, %29, %30), scope: AlexNet/Sequential[features]/Conv2d[6]\r\n  %37 : Float(1, 384, 13, 13) = threshold_forward[threshold={0}, value={0}](%36), scope: AlexNet/Sequential[features]/ReLU[7]\r\n  %38 : Float(1, 384, 13, 13) = threshold[threshold={0}, value={0}](%37), scope: AlexNet/Sequential[features]/ReLU[7]\r\n  %39 : Float(256, 384, 3, 3) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[8]\r\n  %40 : Float(256) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[8]\r\n  %41 : Float(1, 256, 13, 13), %42 : Float(1, 3456, 169), %43 : Float(0) = thnn_conv2d_forward[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\r\n  %44 : Float(1, 256, 13, 13) = thnn_conv2d[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\r\n  %45 : Float(1, 256, 13, 13) = _convolution_nogroup[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\r\n  %46 : Float(1, 256, 13, 13) = _convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%38, %39, %40), scope: AlexNet/Sequential[features]/Conv2d[8]\r\n  %47 : Float(1, 256, 13, 13) = threshold_forward[threshold={0}, value={0}](%46), scope: AlexNet/Sequential[features]/ReLU[9]\r\n  %48 : Float(1, 256, 13, 13) = threshold[threshold={0}, value={0}](%47), scope: AlexNet/Sequential[features]/ReLU[9]\r\n  %49 : Float(256, 256, 3, 3) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[10]\r\n  %50 : Float(256) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[features]/Conv2d[10]\r\n  %51 : Float(1, 256, 13, 13), %52 : Float(1, 2304, 169), %53 : Float(0) = thnn_conv2d_forward[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\r\n  %54 : Float(1, 256, 13, 13) = thnn_conv2d[kernel_size=[3, 3], stride=[1, 1], padding=[1, 1]](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\r\n  %55 : Float(1, 256, 13, 13) = _convolution_nogroup[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0]](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\r\n  %56 : Float(1, 256, 13, 13) = _convolution[stride=[1, 1], padding=[1, 1], dilation=[1, 1], transposed=0, output_padding=[0, 0], groups=1, benchmark=0, deterministic=0, cudnn_enabled=1](%48, %49, %50), scope: AlexNet/Sequential[features]/Conv2d[10]\r\n  %57 : Float(1, 256, 13, 13) = threshold_forward[threshold={0}, value={0}](%56), scope: AlexNet/Sequential[features]/ReLU[11]\r\n  %58 : Float(1, 256, 13, 13) = threshold[threshold={0}, value={0}](%57), scope: AlexNet/Sequential[features]/ReLU[11]\r\n  %59 : Float(1, 256, 6, 6), %60 : Long(1, 256, 6, 6) = max_pool2d_forward[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%58), scope: AlexNet/Sequential[features]/MaxPool2d[12]\r\n  %61 : Float(1, 256, 6, 6), %62 : Long(1, 256, 6, 6) = max_pool2d[kernel_size=[3, 3], stride=[2, 2], padding=[0, 0], dilation=[1, 1], ceil_mode=0](%58), scope: AlexNet/Sequential[features]/MaxPool2d[12]\r\n  %63 : Float(1, 9216) = view[size=[1, 9216]](%61), scope: AlexNet\r\n  %64 : Float(1, 9216), %65 : Handle = ^Dropout(0.5, True, False)(%63), scope: AlexNet/Sequential[classifier]/Dropout[0]\r\n  %66 : Float(1, 4096) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[1]\r\n  %67 : Float(9216!, 4096!) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[1]\r\n  %68 : Float(1, 4096) = addmm[beta={1}, alpha={1}](%66, %64, %67), scope: AlexNet/Sequential[classifier]/Linear[1]\r\n  %69 : Float(1, 4096) = threshold_forward[threshold={0}, value={0}](%68), scope: AlexNet/Sequential[classifier]/ReLU[2]\r\n  %70 : Float(1, 4096) = threshold[threshold={0}, value={0}](%69), scope: AlexNet/Sequential[classifier]/ReLU[2]\r\n  %71 : Float(1, 4096), %72 : Handle = ^Dropout(0.5, True, False)(%70), scope: AlexNet/Sequential[classifier]/Dropout[3]\r\n  %73 : Float(1, 4096) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[4]\r\n  %74 : Float(4096!, 4096!) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[4]\r\n  %75 : Float(1, 4096) = addmm[beta={1}, alpha={1}](%73, %71, %74), scope: AlexNet/Sequential[classifier]/Linear[4]\r\n  %76 : Float(1, 4096) = threshold_forward[threshold={0}, value={0}](%75), scope: AlexNet/Sequential[classifier]/ReLU[5]\r\n  %77 : Float(1, 4096) = threshold[threshold={0}, value={0}](%76), scope: AlexNet/Sequential[classifier]/ReLU[5]\r\n  %78 : Float(1, 1000) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[6]\r\n  %79 : Float(4096!, 1000!) = Constant[value=<Tensor>](), scope: AlexNet/Sequential[classifier]/Linear[6]\r\n  %80 : Float(1, 1000) = addmm[beta={1}, alpha={1}](%78, %77, %79), scope: AlexNet/Sequential[classifier]/Linear[6]\r\n  return (%80);\r\n}\r\n```"}