{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4495", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4495/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4495/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4495/events", "html_url": "https://github.com/pytorch/pytorch/issues/4495", "id": 286277055, "node_id": "MDU6SXNzdWUyODYyNzcwNTU=", "number": 4495, "title": "Incorrect scoped tracing result", "user": {"login": "lanpa", "id": 2005323, "node_id": "MDQ6VXNlcjIwMDUzMjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2005323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lanpa", "html_url": "https://github.com/lanpa", "followers_url": "https://api.github.com/users/lanpa/followers", "following_url": "https://api.github.com/users/lanpa/following{/other_user}", "gists_url": "https://api.github.com/users/lanpa/gists{/gist_id}", "starred_url": "https://api.github.com/users/lanpa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lanpa/subscriptions", "organizations_url": "https://api.github.com/users/lanpa/orgs", "repos_url": "https://api.github.com/users/lanpa/repos", "events_url": "https://api.github.com/users/lanpa/events{/privacy}", "received_events_url": "https://api.github.com/users/lanpa/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx", "name": "onnx", "color": "e99695", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 19, "created_at": "2018-01-05T12:33:38Z", "updated_at": "2018-01-09T20:57:55Z", "closed_at": "2018-01-09T20:57:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Using torch 0.4.0a0+408c84d, tracing code adapted from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"282550193\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4200\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4200/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4200\">#4200</a></p>\n<ol>\n<li><code>F.log_softmax</code> is interpreted as:</li>\n</ol>\n<pre><code>  %29 : Float(13, 10) = Softmax[axis=1](%28)  &lt;--missing scopename\n  %30 : Float(13, 10) = Log(%29), scope: Net1\n</code></pre>\n<ol start=\"2\">\n<li>Tracing result of VGG19/Alexnet from torchvision model: The scope of conv2d layers is <code>ReLU[x]</code> instead of <code>conv2d[x]</code></li>\n</ol>\n<p>alexnet:</p>\n<pre><code>  %17 : Float(1, 64, 55, 55) = Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%0, %1, %2), scope: AlexNet/Sequential[features]/**ReLU[1]**\n  %18 : Float(1, 64, 55, 55) = Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(1, 64, 27, 27) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  %20 : Float(1, 192, 27, 27) = Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %3, %4), scope: AlexNet/Sequential[features]/**ReLU[4]**\n  %21 : Float(1, 192, 27, 27) = Relu(%20), scope: AlexNet/Sequential[features]/ReLU[4]\n  %22 : Float(1, 192, 13, 13) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n</code></pre>\n<p>vgg19:</p>\n<pre><code>  %39 : Float(1, 64, 224, 224) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%0, %1, %2), scope: VGG/Sequential[features]/ReLU[1]\n  %40 : Float(1, 64, 224, 224) = Relu(%39), scope: VGG/Sequential[features]/ReLU[1]\n  %41 : Float(1, 64, 224, 224) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%40, %3, %4), scope: VGG/Sequential[features]/ReLU[3]\n  %42 : Float(1, 64, 224, 224) = Relu(%41), scope: VGG/Sequential[features]/ReLU[3]\n  %43 : Float(1, 64, 112, 112) = MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%42), scope: VGG/Sequential[features]/MaxPool2d[4]\n  %44 : Float(1, 128, 112, 112) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%43, %5, %6), scope: VGG/Sequential[features]/ReLU[6]\n  %45 : Float(1, 128, 112, 112) = Relu(%44), scope: VGG/Sequential[features]/ReLU[6]\n  %46 : Float(1, 128, 112, 112) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%45, %7, %8), scope: VGG/Sequential[features]/ReLU[8]\n  %47 : Float(1, 128, 112, 112) = Relu(%46), scope: VGG/Sequential[features]/ReLU[8]\n  %48 : Float(1, 128, 56, 56) = MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%47), scope: VGG/Sequential[features]/MaxPool2d[9]\n</code></pre>\n<ol start=\"3\">\n<li>[debug hint?] Tracing result of Resnet can be fixed if I rewrite <code>self.relu</code> as functional (<code>F.relu()</code>)<br>\n<a href=\"http://35.197.26.245:6006/#graphs\" rel=\"nofollow\">http://35.197.26.245:6006/#graphs</a></li>\n</ol>\n<p>Thanks</p>", "body_text": "Using torch 0.4.0a0+408c84d, tracing code adapted from #4200\n\nF.log_softmax is interpreted as:\n\n  %29 : Float(13, 10) = Softmax[axis=1](%28)  <--missing scopename\n  %30 : Float(13, 10) = Log(%29), scope: Net1\n\n\nTracing result of VGG19/Alexnet from torchvision model: The scope of conv2d layers is ReLU[x] instead of conv2d[x]\n\nalexnet:\n  %17 : Float(1, 64, 55, 55) = Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%0, %1, %2), scope: AlexNet/Sequential[features]/**ReLU[1]**\n  %18 : Float(1, 64, 55, 55) = Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(1, 64, 27, 27) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  %20 : Float(1, 192, 27, 27) = Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %3, %4), scope: AlexNet/Sequential[features]/**ReLU[4]**\n  %21 : Float(1, 192, 27, 27) = Relu(%20), scope: AlexNet/Sequential[features]/ReLU[4]\n  %22 : Float(1, 192, 13, 13) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n\nvgg19:\n  %39 : Float(1, 64, 224, 224) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%0, %1, %2), scope: VGG/Sequential[features]/ReLU[1]\n  %40 : Float(1, 64, 224, 224) = Relu(%39), scope: VGG/Sequential[features]/ReLU[1]\n  %41 : Float(1, 64, 224, 224) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%40, %3, %4), scope: VGG/Sequential[features]/ReLU[3]\n  %42 : Float(1, 64, 224, 224) = Relu(%41), scope: VGG/Sequential[features]/ReLU[3]\n  %43 : Float(1, 64, 112, 112) = MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%42), scope: VGG/Sequential[features]/MaxPool2d[4]\n  %44 : Float(1, 128, 112, 112) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%43, %5, %6), scope: VGG/Sequential[features]/ReLU[6]\n  %45 : Float(1, 128, 112, 112) = Relu(%44), scope: VGG/Sequential[features]/ReLU[6]\n  %46 : Float(1, 128, 112, 112) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%45, %7, %8), scope: VGG/Sequential[features]/ReLU[8]\n  %47 : Float(1, 128, 112, 112) = Relu(%46), scope: VGG/Sequential[features]/ReLU[8]\n  %48 : Float(1, 128, 56, 56) = MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%47), scope: VGG/Sequential[features]/MaxPool2d[9]\n\n\n[debug hint?] Tracing result of Resnet can be fixed if I rewrite self.relu as functional (F.relu())\nhttp://35.197.26.245:6006/#graphs\n\nThanks", "body": "Using torch 0.4.0a0+408c84d, tracing code adapted from #4200\r\n\r\n1. `F.log_softmax` is interpreted as:\r\n```\r\n  %29 : Float(13, 10) = Softmax[axis=1](%28)  <--missing scopename\r\n  %30 : Float(13, 10) = Log(%29), scope: Net1\r\n```\r\n\r\n\r\n2. Tracing result of VGG19/Alexnet from torchvision model: The scope of conv2d layers is `ReLU[x]` instead of `conv2d[x]`\r\n\r\nalexnet:\r\n```\r\n  %17 : Float(1, 64, 55, 55) = Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%0, %1, %2), scope: AlexNet/Sequential[features]/**ReLU[1]**\r\n  %18 : Float(1, 64, 55, 55) = Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\r\n  %19 : Float(1, 64, 27, 27) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\r\n  %20 : Float(1, 192, 27, 27) = Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %3, %4), scope: AlexNet/Sequential[features]/**ReLU[4]**\r\n  %21 : Float(1, 192, 27, 27) = Relu(%20), scope: AlexNet/Sequential[features]/ReLU[4]\r\n  %22 : Float(1, 192, 13, 13) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21), scope: AlexNet/Sequential[features]/MaxPool2d[5]\r\n```\r\n\r\n\r\nvgg19:\r\n```\r\n  %39 : Float(1, 64, 224, 224) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%0, %1, %2), scope: VGG/Sequential[features]/ReLU[1]\r\n  %40 : Float(1, 64, 224, 224) = Relu(%39), scope: VGG/Sequential[features]/ReLU[1]\r\n  %41 : Float(1, 64, 224, 224) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%40, %3, %4), scope: VGG/Sequential[features]/ReLU[3]\r\n  %42 : Float(1, 64, 224, 224) = Relu(%41), scope: VGG/Sequential[features]/ReLU[3]\r\n  %43 : Float(1, 64, 112, 112) = MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%42), scope: VGG/Sequential[features]/MaxPool2d[4]\r\n  %44 : Float(1, 128, 112, 112) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%43, %5, %6), scope: VGG/Sequential[features]/ReLU[6]\r\n  %45 : Float(1, 128, 112, 112) = Relu(%44), scope: VGG/Sequential[features]/ReLU[6]\r\n  %46 : Float(1, 128, 112, 112) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%45, %7, %8), scope: VGG/Sequential[features]/ReLU[8]\r\n  %47 : Float(1, 128, 112, 112) = Relu(%46), scope: VGG/Sequential[features]/ReLU[8]\r\n  %48 : Float(1, 128, 56, 56) = MaxPool[kernel_shape=[2, 2], pads=[0, 0, 0, 0], strides=[2, 2]](%47), scope: VGG/Sequential[features]/MaxPool2d[9]\r\n```\r\n\r\n3. [debug hint?] Tracing result of Resnet can be fixed if I rewrite `self.relu` as functional (`F.relu()`) \r\nhttp://35.197.26.245:6006/#graphs\r\n\r\nThanks\r\n\r\n"}