{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355713002", "html_url": "https://github.com/pytorch/pytorch/issues/4495#issuecomment-355713002", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4495", "id": 355713002, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTcxMzAwMg==", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-06T01:36:57Z", "updated_at": "2018-01-06T01:36:57Z", "author_association": "COLLABORATOR", "body_html": "<p>The <code>log_softmax</code> issue is clear, I have a fix.</p>\n<p>The issue with AlexNet, Resnet etc is trickier: it's related to Relu having <code>inplace=True</code> (it's independent from the use of Sequential).</p>\n<p>Essentially, during the ONNX pass, the scope is correctly set to <code>AlexNet/Sequential[features]/Conv2d[0]</code> when the <code>_convolution</code> op is created, but then it gets overwritten with <code>AlexNet/Sequential[features]/ReLU[1]</code>.</p>\n<p>This is because <code>outputs[i]</code> in</p>\n<pre><code>if (outputs[i]) {\n        // Allow symbolic() to skip specifying the type of the return node.\n        // Unfortunately, they are on the hook for all internal nodes\n        // (though in practice, the types are not computed.)\n        if (!outputs[i]-&gt;hasType()) {\n          outputs[i]-&gt;setType(old-&gt;typeOption());\n        }\n        // Copy over source location information to all nodes created by\n        // the symbolic\n        outputs[i]-&gt;node()-&gt;setSourceLocation(node-&gt;getSourceLocation());\n        outputs[i]-&gt;node()-&gt;setScope(node-&gt;scope());\n        env[old] = outputs[i];\n}\n</code></pre>\n<p>points to the same address for Conv and the subsequent ReLU.</p>\n<p>Since the symbolic pass has no use for inplace, the output for ReLU should really be a new object even with <code>inplace=True</code>.</p>\n<p>/cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a></p>", "body_text": "The log_softmax issue is clear, I have a fix.\nThe issue with AlexNet, Resnet etc is trickier: it's related to Relu having inplace=True (it's independent from the use of Sequential).\nEssentially, during the ONNX pass, the scope is correctly set to AlexNet/Sequential[features]/Conv2d[0] when the _convolution op is created, but then it gets overwritten with AlexNet/Sequential[features]/ReLU[1].\nThis is because outputs[i] in\nif (outputs[i]) {\n        // Allow symbolic() to skip specifying the type of the return node.\n        // Unfortunately, they are on the hook for all internal nodes\n        // (though in practice, the types are not computed.)\n        if (!outputs[i]->hasType()) {\n          outputs[i]->setType(old->typeOption());\n        }\n        // Copy over source location information to all nodes created by\n        // the symbolic\n        outputs[i]->node()->setSourceLocation(node->getSourceLocation());\n        outputs[i]->node()->setScope(node->scope());\n        env[old] = outputs[i];\n}\n\npoints to the same address for Conv and the subsequent ReLU.\nSince the symbolic pass has no use for inplace, the output for ReLU should really be a new object even with inplace=True.\n/cc @ezyang", "body": "The `log_softmax` issue is clear, I have a fix.\r\n\r\nThe issue with AlexNet, Resnet etc is trickier: it's related to Relu having `inplace=True` (it's independent from the use of Sequential).\r\n\r\nEssentially, during the ONNX pass, the scope is correctly set to `AlexNet/Sequential[features]/Conv2d[0]` when the `_convolution` op is created, but then it gets overwritten with `AlexNet/Sequential[features]/ReLU[1]`.\r\n\r\nThis is because `outputs[i]` in\r\n```\r\nif (outputs[i]) {\r\n        // Allow symbolic() to skip specifying the type of the return node.\r\n        // Unfortunately, they are on the hook for all internal nodes\r\n        // (though in practice, the types are not computed.)\r\n        if (!outputs[i]->hasType()) {\r\n          outputs[i]->setType(old->typeOption());\r\n        }\r\n        // Copy over source location information to all nodes created by\r\n        // the symbolic\r\n        outputs[i]->node()->setSourceLocation(node->getSourceLocation());\r\n        outputs[i]->node()->setScope(node->scope());\r\n        env[old] = outputs[i];\r\n}\r\n```\r\npoints to the same address for Conv and the subsequent ReLU.\r\n\r\nSince the symbolic pass has no use for inplace, the output for ReLU should really be a new object even with `inplace=True`.\r\n\r\n/cc @ezyang "}