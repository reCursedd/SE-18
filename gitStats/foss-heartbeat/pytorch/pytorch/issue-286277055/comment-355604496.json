{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355604496", "html_url": "https://github.com/pytorch/pytorch/issues/4495#issuecomment-355604496", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4495", "id": 355604496, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTYwNDQ5Ng==", "user": {"login": "lanpa", "id": 2005323, "node_id": "MDQ6VXNlcjIwMDUzMjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2005323?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lanpa", "html_url": "https://github.com/lanpa", "followers_url": "https://api.github.com/users/lanpa/followers", "following_url": "https://api.github.com/users/lanpa/following{/other_user}", "gists_url": "https://api.github.com/users/lanpa/gists{/gist_id}", "starred_url": "https://api.github.com/users/lanpa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lanpa/subscriptions", "organizations_url": "https://api.github.com/users/lanpa/orgs", "repos_url": "https://api.github.com/users/lanpa/repos", "events_url": "https://api.github.com/users/lanpa/events{/privacy}", "received_events_url": "https://api.github.com/users/lanpa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-05T16:50:20Z", "updated_at": "2018-01-05T16:50:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=191033\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lantiga\">@lantiga</a> Here's my modified network: (I forward tensors explicitly)<br>\n<a href=\"https://gist.github.com/lanpa/452268a4e48106d79357dabf82d76054\">https://gist.github.com/lanpa/452268a4e48106d79357dabf82d76054</a></p>\n<p>And sorry, I forgot that I also called optimize:<br>\n<a href=\"https://github.com/lanpa/tensorboard-pytorch/blob/master/tensorboardX/graph.py#L45\">https://github.com/lanpa/tensorboard-pytorch/blob/master/tensorboardX/graph.py#L45</a></p>\n<p>so the output is much concise</p>\n<pre><code>graph(%0 : Float(1, 3, 224, 224)\n      %1 : Float(64, 3, 11, 11)\n      %2 : Float(64)\n      %3 : Float(192, 64, 5, 5)\n      %4 : Float(192)\n      %5 : Float(384, 192, 3, 3)\n      %6 : Float(384)\n      %7 : Float(256, 384, 3, 3)\n      %8 : Float(256)\n      %9 : Float(256, 256, 3, 3)\n      %10 : Float(256)\n      %11 : Float(4096, 9216)\n      %12 : Float(4096)\n      %13 : Float(4096, 4096)\n      %14 : Float(4096)\n      %15 : Float(1000, 4096)\n      %16 : Float(1000)) {\n  %17 : Float(1, 64, 55, 55) = Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %18 : Float(1, 64, 55, 55) = Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(1, 64, 27, 27) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  %20 : Float(1, 192, 27, 27) = Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %3, %4), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %21 : Float(1, 192, 27, 27) = Relu(%20), scope: AlexNet/Sequential[features]/ReLU[4]\n  %22 : Float(1, 192, 13, 13) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n  %23 : Float(1, 384, 13, 13) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%22, %5, %6), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %24 : Float(1, 384, 13, 13) = Relu(%23), scope: AlexNet/Sequential[features]/ReLU[7]\n  %25 : Float(1, 256, 13, 13) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%24, %7, %8), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %26 : Float(1, 256, 13, 13) = Relu(%25), scope: AlexNet/Sequential[features]/ReLU[9]\n  %27 : Float(1, 256, 13, 13) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%26, %9, %10), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %28 : Float(1, 256, 13, 13) = Relu(%27), scope: AlexNet/Sequential[features]/ReLU[11]\n  %29 : Float(1, 256, 6, 6) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  %30 : Float(1, 9216) = Flatten[axis=1](%29), scope: AlexNet\n  %31 : Float(1, 9216), %32 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%30), scope: AlexNet\n  %33 : Float(1, 4096) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%31, %11, %12), scope: AlexNet/Linear[fc1]\n  %34 : Float(1, 4096) = Relu(%33), scope: AlexNet/ReLU[relu1]\n  %35 : Float(1, 4096), %36 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%34), scope: AlexNet\n  %37 : Float(1, 4096) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%35, %13, %14), scope: AlexNet/Linear[fc2]\n  %38 : Float(1, 4096) = Relu(%37), scope: AlexNet/ReLU[relu2]\n  %39 : Float(1, 1000) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%38, %15, %16), scope: AlexNet/Linear[fc3]\n  return (%39);\n}\n</code></pre>", "body_text": "@lantiga Here's my modified network: (I forward tensors explicitly)\nhttps://gist.github.com/lanpa/452268a4e48106d79357dabf82d76054\nAnd sorry, I forgot that I also called optimize:\nhttps://github.com/lanpa/tensorboard-pytorch/blob/master/tensorboardX/graph.py#L45\nso the output is much concise\ngraph(%0 : Float(1, 3, 224, 224)\n      %1 : Float(64, 3, 11, 11)\n      %2 : Float(64)\n      %3 : Float(192, 64, 5, 5)\n      %4 : Float(192)\n      %5 : Float(384, 192, 3, 3)\n      %6 : Float(384)\n      %7 : Float(256, 384, 3, 3)\n      %8 : Float(256)\n      %9 : Float(256, 256, 3, 3)\n      %10 : Float(256)\n      %11 : Float(4096, 9216)\n      %12 : Float(4096)\n      %13 : Float(4096, 4096)\n      %14 : Float(4096)\n      %15 : Float(1000, 4096)\n      %16 : Float(1000)) {\n  %17 : Float(1, 64, 55, 55) = Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %18 : Float(1, 64, 55, 55) = Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(1, 64, 27, 27) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  %20 : Float(1, 192, 27, 27) = Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %3, %4), scope: AlexNet/Sequential[features]/Conv2d[3]\n  %21 : Float(1, 192, 27, 27) = Relu(%20), scope: AlexNet/Sequential[features]/ReLU[4]\n  %22 : Float(1, 192, 13, 13) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n  %23 : Float(1, 384, 13, 13) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%22, %5, %6), scope: AlexNet/Sequential[features]/Conv2d[6]\n  %24 : Float(1, 384, 13, 13) = Relu(%23), scope: AlexNet/Sequential[features]/ReLU[7]\n  %25 : Float(1, 256, 13, 13) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%24, %7, %8), scope: AlexNet/Sequential[features]/Conv2d[8]\n  %26 : Float(1, 256, 13, 13) = Relu(%25), scope: AlexNet/Sequential[features]/ReLU[9]\n  %27 : Float(1, 256, 13, 13) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%26, %9, %10), scope: AlexNet/Sequential[features]/Conv2d[10]\n  %28 : Float(1, 256, 13, 13) = Relu(%27), scope: AlexNet/Sequential[features]/ReLU[11]\n  %29 : Float(1, 256, 6, 6) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  %30 : Float(1, 9216) = Flatten[axis=1](%29), scope: AlexNet\n  %31 : Float(1, 9216), %32 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%30), scope: AlexNet\n  %33 : Float(1, 4096) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%31, %11, %12), scope: AlexNet/Linear[fc1]\n  %34 : Float(1, 4096) = Relu(%33), scope: AlexNet/ReLU[relu1]\n  %35 : Float(1, 4096), %36 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%34), scope: AlexNet\n  %37 : Float(1, 4096) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%35, %13, %14), scope: AlexNet/Linear[fc2]\n  %38 : Float(1, 4096) = Relu(%37), scope: AlexNet/ReLU[relu2]\n  %39 : Float(1, 1000) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%38, %15, %16), scope: AlexNet/Linear[fc3]\n  return (%39);\n}", "body": "@lantiga Here's my modified network: (I forward tensors explicitly)\r\nhttps://gist.github.com/lanpa/452268a4e48106d79357dabf82d76054\r\n\r\nAnd sorry, I forgot that I also called optimize:\r\nhttps://github.com/lanpa/tensorboard-pytorch/blob/master/tensorboardX/graph.py#L45\r\n\r\nso the output is much concise\r\n```\r\ngraph(%0 : Float(1, 3, 224, 224)\r\n      %1 : Float(64, 3, 11, 11)\r\n      %2 : Float(64)\r\n      %3 : Float(192, 64, 5, 5)\r\n      %4 : Float(192)\r\n      %5 : Float(384, 192, 3, 3)\r\n      %6 : Float(384)\r\n      %7 : Float(256, 384, 3, 3)\r\n      %8 : Float(256)\r\n      %9 : Float(256, 256, 3, 3)\r\n      %10 : Float(256)\r\n      %11 : Float(4096, 9216)\r\n      %12 : Float(4096)\r\n      %13 : Float(4096, 4096)\r\n      %14 : Float(4096)\r\n      %15 : Float(1000, 4096)\r\n      %16 : Float(1000)) {\r\n  %17 : Float(1, 64, 55, 55) = Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%0, %1, %2), scope: AlexNet/Sequential[features]/Conv2d[0]\r\n  %18 : Float(1, 64, 55, 55) = Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\r\n  %19 : Float(1, 64, 27, 27) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\r\n  %20 : Float(1, 192, 27, 27) = Conv[dilations=[1, 1], group=1, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%19, %3, %4), scope: AlexNet/Sequential[features]/Conv2d[3]\r\n  %21 : Float(1, 192, 27, 27) = Relu(%20), scope: AlexNet/Sequential[features]/ReLU[4]\r\n  %22 : Float(1, 192, 13, 13) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%21), scope: AlexNet/Sequential[features]/MaxPool2d[5]\r\n  %23 : Float(1, 384, 13, 13) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%22, %5, %6), scope: AlexNet/Sequential[features]/Conv2d[6]\r\n  %24 : Float(1, 384, 13, 13) = Relu(%23), scope: AlexNet/Sequential[features]/ReLU[7]\r\n  %25 : Float(1, 256, 13, 13) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%24, %7, %8), scope: AlexNet/Sequential[features]/Conv2d[8]\r\n  %26 : Float(1, 256, 13, 13) = Relu(%25), scope: AlexNet/Sequential[features]/ReLU[9]\r\n  %27 : Float(1, 256, 13, 13) = Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%26, %9, %10), scope: AlexNet/Sequential[features]/Conv2d[10]\r\n  %28 : Float(1, 256, 13, 13) = Relu(%27), scope: AlexNet/Sequential[features]/ReLU[11]\r\n  %29 : Float(1, 256, 6, 6) = MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\r\n  %30 : Float(1, 9216) = Flatten[axis=1](%29), scope: AlexNet\r\n  %31 : Float(1, 9216), %32 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%30), scope: AlexNet\r\n  %33 : Float(1, 4096) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%31, %11, %12), scope: AlexNet/Linear[fc1]\r\n  %34 : Float(1, 4096) = Relu(%33), scope: AlexNet/ReLU[relu1]\r\n  %35 : Float(1, 4096), %36 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%34), scope: AlexNet\r\n  %37 : Float(1, 4096) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%35, %13, %14), scope: AlexNet/Linear[fc2]\r\n  %38 : Float(1, 4096) = Relu(%37), scope: AlexNet/ReLU[relu2]\r\n  %39 : Float(1, 1000) = Gemm[alpha=1, beta=1, broadcast=1, transB=1](%38, %15, %16), scope: AlexNet/Linear[fc3]\r\n  return (%39);\r\n}\r\n```"}