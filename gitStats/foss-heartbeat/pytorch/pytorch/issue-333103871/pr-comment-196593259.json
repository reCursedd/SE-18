{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196593259", "pull_request_review_id": 130177109, "id": 196593259, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NjU5MzI1OQ==", "diff_hunk": "@@ -5,152 +5,91 @@\n #include \"../common.h\"\n \n static inline void THNN_(TemporalUpSamplingNearest_shapeCheck)\n-                        (THCState *state,THCTensor *input, THCTensor *gradOutput,\n-                         int scale_factor) {\n-  THArgCheck(input != NULL, 2, \"3D input tensor expected but got NULL\");\n-  THArgCheck(scale_factor > 1, 4,\n-             \"scale_factor must be greater than 1, but got: %d\", scale_factor);\n-  THCUNN_argCheck(state, input->_dim() == 2 || input->_dim() == 3, 2, input,\n-                  \"2D or 3D input tensor expected but got: %s\");\n-  if (input->_dim() == 2) {\n-    int nChannels    = THCTensor_(size)(state, input, 0);\n-    int inputWidth   = THCTensor_(size)(state, input, 1);\n-    int outputWidth  = inputWidth  * scale_factor;\n-    if (gradOutput != NULL) {\n-      THCUNN_check_dim_size(state, gradOutput, 2, 0, nChannels);\n-      THCUNN_check_dim_size(state, gradOutput, 2, 1, outputWidth);\n-    }\n-  } else {\n-    int nBatch       = THCTensor_(size)(state, input, 0);\n-    int nChannels    = THCTensor_(size)(state, input, 1);\n-    int inputWidth   = THCTensor_(size)(state, input, 2);\n-    int outputWidth  = inputWidth  * scale_factor;\n-    if (gradOutput != NULL) {\n-      THCUNN_check_dim_size(state, gradOutput, 3, 0, nBatch);\n-      THCUNN_check_dim_size(state, gradOutput, 3, 1, nChannels);\n-      THCUNN_check_dim_size(state, gradOutput, 3, 2, outputWidth);\n-    }\n+                        (THCState *state,\n+                         THCTensor *input, THCTensor *gradOutput,\n+                         int nBatch, int nChannels,\n+                         int inputWidth,\n+                         int outputWidth) {\n+  THArgCheck(inputWidth > 0 && outputWidth > 0, 2,\n+             \"input and output sizes should be greater than 0,\"\n+             \" but got input (W: %d) output (W: %d)\",\n+             inputWidth, outputWidth);\n+  if (input != NULL) {\n+     THCUNN_argCheck(state, input->_dim() == 3, 2, input,\n+                     \"3D input tensor expected but got: %s\");\n+  }\n+\n+  if (gradOutput != NULL) {\n+    THCUNN_check_dim_size(state, gradOutput, 3, 0, nBatch);\n+    THCUNN_check_dim_size(state, gradOutput, 3, 1, nChannels);\n+    THCUNN_check_dim_size(state, gradOutput, 3, 2, outputWidth);\n   }\n }\n \n void THNN_(TemporalUpSamplingNearest_updateOutput)(\n            THCState *state,\n            THCTensor *input,\n            THCTensor *output,\n-           int scale_factor)\n+           int outputWidth)\n {\n-  THCTensor_(zero)(state, output);\n-\n   THCUNN_assertSameGPU(state, 2, input, output);\n-  THNN_(TemporalUpSamplingNearest_shapeCheck)(state, input, NULL, scale_factor);\n-  int inputWidth  = THCTensor_(size)(state, input,  input->_dim()-1);\n-  int outputWidth = inputWidth * scale_factor;\n-\n-   if (input->_dim() == 2) {\n-     THCTensor_(resize2d)(state, output,\n-                          THCTensor_(size)(state, input, 0),\n-                          outputWidth);\n-   } else {\n-     THCTensor_(resize3d)(state, output,\n-                          THCTensor_(size)(state, input, 0),\n-                          THCTensor_(size)(state, input, 1),\n-                          outputWidth);\n-  }\n-\n-  input = THCTensor_(newContiguous)(state, input);\n-  // This is for allocating output Tensor\n-  int64_t no_elements = 1;\n-  for(int i = 0; i < input->_dim(); i++){\n-    no_elements *= input->size[i];\n-  }\n-  no_elements *= scale_factor;\n-\n-  int d1;\n-  int d2;\n+  int nbatch = THCTensor_(size)(state, input, 0);\n+  int channels = THCTensor_(size)(state, input, 1);\n+  int inputWidth  = THCTensor_(size)(state, input, 2);\n \n-  if (input->_dim() == 2) {\n-    d1 = output->size[0];\n-    d2 = output->size[1];\n-  } else {\n-    d1 = output->size[1];\n-    d2 = output->size[2];\n-  }\n+  THNN_(TemporalUpSamplingNearest_shapeCheck)(state, input, NULL, nbatch, channels, inputWidth, outputWidth);\n+  THAssert(inputWidth > 0 && outputWidth > 0);\n \n-  real *input_data = THCTensor_(data)(state, input);\n-  real *output_data = THCTensor_(data)(state, output);\n-\n-  // cuda blocks & threads:\n-  int64_t nthreads = 256;\n-  // Max number of blocks: http://en.wikipedia.org/wiki/CUDA\n-  // 65535 for SM 2.x, 2^32 -1 for >= 3.0\n-  // TODO: When we move to SM 3.5 we should update this\n-  int64_t n_xblocks = min(max((int)ceil((float)no_elements / nthreads), 1), 65535);\n-  int64_t n_yblocks = (int64_t)ceil((float)no_elements / (float)(n_xblocks * nthreads));\n-  if (n_yblocks > 65535) {\n-    THError(\"Input size is too large!  aborting\");\n-  }\n-  dim3 blocks(n_xblocks, n_yblocks);\n-  dim3 threads(nthreads);\n+  THCTensor_(resize3d)(state, output,\n+                       THCTensor_(size)(state, input, 0),\n+                       THCTensor_(size)(state, input, 1),\n+                       outputWidth);\n+  THCTensor_(zero)(state, output);\n \n-  // kernel:\n-  upscale<<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (input_data, output_data, no_elements, scale_factor, d1, d2);\n+  input = THCTensor_(newContiguous)(state, input);\n+  THCDeviceTensor<real, 3> idata = toDeviceTensor<real, 3>(state, input);\n+  THCDeviceTensor<real, 3> odata = toDeviceTensor<real, 3>(state, output);\n+\n+  const int num_kernels = outputWidth;\n+  const int num_threads = THCState_getCurrentDeviceProperties(state)->maxThreadsPerBlock;\n+  cudaStream_t stream = THCState_getCurrentStream(state);\n+  nearest_neighbor_interp2_kernel<real, accreal> <<<THCCeilDiv(num_kernels, num_threads), num_threads,\n+\t 0, stream>>>(num_kernels, idata, odata);\n   THCudaCheck(cudaGetLastError());\n-\n-  // final cut:\n   THCTensor_(free)(state, input);\n }\n \n+\n+\n void THNN_(TemporalUpSamplingNearest_updateGradInput)(\n            THCState *state,\n-           THCTensor *input,\n            THCTensor *gradOutput,\n            THCTensor *gradInput,\n-           int scale_factor)\n+           int nbatch,\n+\t   int nchannels,\n+\t   int inputWidth,\n+\t   int outputWidth)\n {\n-\n+  gradInput = THCTensor_(newContiguous)(state, gradInput);", "path": "aten/src/THCUNN/generic/TemporalUpSamplingNearest.cu", "position": null, "original_position": 151, "commit_id": "f99c543ab514043a5a842d80d166f4ef7e36d93b", "original_commit_id": "1958eb421c916bd3864353ffed4e36f574774c4e", "user": {"login": "ailzhang", "id": 5248122, "node_id": "MDQ6VXNlcjUyNDgxMjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5248122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ailzhang", "html_url": "https://github.com/ailzhang", "followers_url": "https://api.github.com/users/ailzhang/followers", "following_url": "https://api.github.com/users/ailzhang/following{/other_user}", "gists_url": "https://api.github.com/users/ailzhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ailzhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ailzhang/subscriptions", "organizations_url": "https://api.github.com/users/ailzhang/orgs", "repos_url": "https://api.github.com/users/ailzhang/repos", "events_url": "https://api.github.com/users/ailzhang/events{/privacy}", "received_events_url": "https://api.github.com/users/ailzhang/received_events", "type": "User", "site_admin": false}, "body": "Aha I think I get why it's different. So here is context: with the old kernels, it works without initializing gradInput as contiguous. But with the current kernels(like *linear), it segfaults when gradInput is not initialized as contiguous(without last commit). \r\nAnd here are my 2 cents: with old code, it didn't initialize as contiguous but get a real * pointer and pass along to the kernel code. In the kernel we actually use the memory contiguously. https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/TemporalUpSamplingNearest.cu#L40\r\nWith the new code, we are passing along a THCDeviceTensor, so if it's not initialized as contiguous, it might access some wild memory space during the assignment here. https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/TemporalUpSamplingLinear.cu#L72 \r\nBasically we made sure the memory is contiguous in both cases. And I would prefer to keep it as it is since it's consistent with how *linear methods were implemented. \r\nWhat do you think? ", "created_at": "2018-06-19T22:02:48Z", "updated_at": "2018-11-23T15:45:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/8591#discussion_r196593259", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8591", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/196593259"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8591#discussion_r196593259"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8591"}}, "body_html": "<p>Aha I think I get why it's different. So here is context: with the old kernels, it works without initializing gradInput as contiguous. But with the current kernels(like *linear), it segfaults when gradInput is not initialized as contiguous(without last commit).<br>\nAnd here are my 2 cents: with old code, it didn't initialize as contiguous but get a real * pointer and pass along to the kernel code. In the kernel we actually use the memory contiguously. <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/TemporalUpSamplingNearest.cu#L40\">https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/TemporalUpSamplingNearest.cu#L40</a><br>\nWith the new code, we are passing along a THCDeviceTensor, so if it's not initialized as contiguous, it might access some wild memory space during the assignment here. <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/TemporalUpSamplingLinear.cu#L72\">https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/TemporalUpSamplingLinear.cu#L72</a><br>\nBasically we made sure the memory is contiguous in both cases. And I would prefer to keep it as it is since it's consistent with how *linear methods were implemented.<br>\nWhat do you think?</p>", "body_text": "Aha I think I get why it's different. So here is context: with the old kernels, it works without initializing gradInput as contiguous. But with the current kernels(like *linear), it segfaults when gradInput is not initialized as contiguous(without last commit).\nAnd here are my 2 cents: with old code, it didn't initialize as contiguous but get a real * pointer and pass along to the kernel code. In the kernel we actually use the memory contiguously. https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/TemporalUpSamplingNearest.cu#L40\nWith the new code, we are passing along a THCDeviceTensor, so if it's not initialized as contiguous, it might access some wild memory space during the assignment here. https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/TemporalUpSamplingLinear.cu#L72\nBasically we made sure the memory is contiguous in both cases. And I would prefer to keep it as it is since it's consistent with how *linear methods were implemented.\nWhat do you think?", "in_reply_to_id": 196466414}