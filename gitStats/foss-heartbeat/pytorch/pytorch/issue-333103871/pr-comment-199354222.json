{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199354222", "pull_request_review_id": 133458676, "id": 199354222, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5OTM1NDIyMg==", "diff_hunk": "@@ -5,151 +5,89 @@\n #include \"../common.h\"\n \n static inline void THNN_(TemporalUpSamplingNearest_shapeCheck)\n-                        (THCState *state,THCTensor *input, THCTensor *gradOutput,\n-                         int scale_factor) {\n-  THArgCheck(input != NULL, 2, \"3D input tensor expected but got NULL\");\n-  THArgCheck(scale_factor > 1, 4,\n-             \"scale_factor must be greater than 1, but got: %d\", scale_factor);\n-  THCUNN_argCheck(state, !input->is_empty() && (input->dim() == 2 || input->dim() == 3), 2, input,\n-                  \"non-empty 2D or 3D input tensor expected but got: %s\");\n-  if (input->dim() == 2) {\n-    int nChannels    = THCTensor_(size)(state, input, 0);\n-    int inputWidth   = THCTensor_(size)(state, input, 1);\n-    int outputWidth  = inputWidth  * scale_factor;\n-    if (gradOutput != NULL) {\n-      THCUNN_check_dim_size(state, gradOutput, 2, 0, nChannels);\n-      THCUNN_check_dim_size(state, gradOutput, 2, 1, outputWidth);\n-    }\n-  } else {\n-    int nBatch       = THCTensor_(size)(state, input, 0);\n-    int nChannels    = THCTensor_(size)(state, input, 1);\n-    int inputWidth   = THCTensor_(size)(state, input, 2);\n-    int outputWidth  = inputWidth  * scale_factor;\n-    if (gradOutput != NULL) {\n-      THCUNN_check_dim_size(state, gradOutput, 3, 0, nBatch);\n-      THCUNN_check_dim_size(state, gradOutput, 3, 1, nChannels);\n-      THCUNN_check_dim_size(state, gradOutput, 3, 2, outputWidth);\n-    }\n+                        (THCState *state,\n+                         THCTensor *input, THCTensor *gradOutput,\n+                         int nBatch, int nChannels,\n+                         int inputWidth,\n+                         int outputWidth) {\n+  THArgCheck(inputWidth > 0 && outputWidth > 0, 2,\n+             \"input and output sizes should be greater than 0,\"\n+             \" but got input (W: %d) output (W: %d)\",\n+             inputWidth, outputWidth);\n+  if (input != NULL) {\n+     THCUNN_argCheck(state, input->_dim() == 3, 2, input,\n+                     \"3D input tensor expected but got: %s\");\n+  }\n+\n+  if (gradOutput != NULL) {\n+    THCUNN_check_dim_size(state, gradOutput, 3, 0, nBatch);\n+    THCUNN_check_dim_size(state, gradOutput, 3, 1, nChannels);\n+    THCUNN_check_dim_size(state, gradOutput, 3, 2, outputWidth);\n   }\n }\n \n void THNN_(TemporalUpSamplingNearest_updateOutput)(\n            THCState *state,\n            THCTensor *input,\n            THCTensor *output,\n-           int scale_factor)\n+           int outputWidth)\n {\n-  THCTensor_(zero)(state, output);\n-\n   THCUNN_assertSameGPU(state, 2, input, output);\n-  THNN_(TemporalUpSamplingNearest_shapeCheck)(state, input, NULL, scale_factor);\n-  int inputWidth  = THCTensor_(size)(state, input,  input->dim()-1);\n-  int outputWidth = inputWidth * scale_factor;\n-\n-   if (input->dim() == 2) {\n-     THCTensor_(resize2d)(state, output,\n-                          THCTensor_(size)(state, input, 0),\n-                          outputWidth);\n-   } else {\n-     THCTensor_(resize3d)(state, output,\n-                          THCTensor_(size)(state, input, 0),\n-                          THCTensor_(size)(state, input, 1),\n-                          outputWidth);\n-  }\n-\n-  input = THCTensor_(newContiguous)(state, input);\n-  // This is for allocating output Tensor\n-  int64_t no_elements = 1;\n-  for(int i = 0; i < input->dim(); i++){\n-    no_elements *= input->size[i];\n-  }\n-  no_elements *= scale_factor;\n+  int nbatch = THCTensor_(size)(state, input, 0);\n+  int channels = THCTensor_(size)(state, input, 1);\n+  int inputWidth  = THCTensor_(size)(state, input, 2);\n \n-  int d1;\n-  int d2;\n+  THNN_(TemporalUpSamplingNearest_shapeCheck)(state, input, NULL, nbatch, channels, inputWidth, outputWidth);\n+  THAssert(inputWidth > 0 && outputWidth > 0);\n \n-  if (input->dim() == 2) {\n-    d1 = output->size[0];\n-    d2 = output->size[1];\n-  } else {\n-    d1 = output->size[1];\n-    d2 = output->size[2];\n-  }\n \n-  real *input_data = THCTensor_(data)(state, input);\n-  real *output_data = THCTensor_(data)(state, output);\n-\n-  // cuda blocks & threads:\n-  int64_t nthreads = 256;\n-  // Max number of blocks: http://en.wikipedia.org/wiki/CUDA\n-  // 65535 for SM 2.x, 2^32 -1 for >= 3.0\n-  // TODO: When we move to SM 3.5 we should update this\n-  int64_t n_xblocks = min(max((int)ceil((float)no_elements / nthreads), 1), 65535);\n-  int64_t n_yblocks = (int64_t)ceil((float)no_elements / (float)(n_xblocks * nthreads));\n-  if (n_yblocks > 65535) {\n-    THError(\"Input size is too large!  aborting\");\n-  }\n-  dim3 blocks(n_xblocks, n_yblocks);\n-  dim3 threads(nthreads);\n+  THCTensor_(resize3d)(state, output,\n+                       THCTensor_(size)(state, input, 0),\n+                       THCTensor_(size)(state, input, 1),\n+                       outputWidth);\n+  THCTensor_(zero)(state, output);\n \n-  // kernel:\n-  upscale<<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (input_data, output_data, no_elements, scale_factor, d1, d2);\n+  input = THCTensor_(newContiguous)(state, input);", "path": "aten/src/THCUNN/generic/TemporalUpSamplingNearest.cu", "position": null, "original_position": 122, "commit_id": "f99c543ab514043a5a842d80d166f4ef7e36d93b", "original_commit_id": "03cfb5387bfea8f995540b427c7e8a2855140963", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "same question as before: because we are using the `THCDeviceTensor`, and we index all the tensors using the `[]` operator, do we need to keep the restriction that the input is contiguous?", "created_at": "2018-07-01T16:21:17Z", "updated_at": "2018-11-23T15:46:38Z", "html_url": "https://github.com/pytorch/pytorch/pull/8591#discussion_r199354222", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8591", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199354222"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8591#discussion_r199354222"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8591"}}, "body_html": "<p>same question as before: because we are using the <code>THCDeviceTensor</code>, and we index all the tensors using the <code>[]</code> operator, do we need to keep the restriction that the input is contiguous?</p>", "body_text": "same question as before: because we are using the THCDeviceTensor, and we index all the tensors using the [] operator, do we need to keep the restriction that the input is contiguous?"}