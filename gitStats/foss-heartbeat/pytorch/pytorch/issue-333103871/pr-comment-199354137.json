{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199354137", "pull_request_review_id": 133458676, "id": 199354137, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5OTM1NDEzNw==", "diff_hunk": "@@ -5,165 +5,99 @@\n #include \"../common.h\"\n \n static inline void THNN_(SpatialUpSamplingNearest_shapeCheck)\n-                        (THCState *state,THCTensor *input, THCTensor *gradOutput,\n-                         int scale_factor) {\n-  THArgCheck(input != NULL, 2, \"4D input tensor expected but got NULL\");\n-  THArgCheck(scale_factor > 1, 4,\n-             \"scale_factor must be greater than 1, but got: %d\", scale_factor);\n-  THCUNN_argCheck(state, !input->is_empty() && (input->dim() == 3 || input->dim() == 4), 2, input,\n-                  \"non-empty 3D or 4D input tensor expected but got: %s\");\n-  if (input->dim() == 3) {\n-    int nChannels    = THCTensor_(size)(state, input, 0);\n-    int inputHeight  = THCTensor_(size)(state, input, 1);\n-    int inputWidth   = THCTensor_(size)(state, input, 2);\n-    int outputHeight = inputHeight * scale_factor;\n-    int outputWidth  = inputWidth  * scale_factor;\n-    if (gradOutput != NULL) {\n-      THCUNN_check_dim_size(state, gradOutput, 3, 0, nChannels);\n-      THCUNN_check_dim_size(state, gradOutput, 3, 1, outputHeight);\n-      THCUNN_check_dim_size(state, gradOutput, 3, 2, outputWidth);\n-    }\n-  } else {\n-    int nBatch       = THCTensor_(size)(state, input, 0);\n-    int nChannels    = THCTensor_(size)(state, input, 1);\n-    int inputHeight  = THCTensor_(size)(state, input, 2);\n-    int inputWidth   = THCTensor_(size)(state, input, 3);\n-    int outputHeight = inputHeight * scale_factor;\n-    int outputWidth  = inputWidth  * scale_factor;\n-    if (gradOutput != NULL) {\n-      THCUNN_check_dim_size(state, gradOutput, 4, 0, nBatch);\n-      THCUNN_check_dim_size(state, gradOutput, 4, 1, nChannels);\n-      THCUNN_check_dim_size(state, gradOutput, 4, 2, outputHeight);\n-      THCUNN_check_dim_size(state, gradOutput, 4, 3, outputWidth);\n-    }\n+                        (THCState *state,\n+                         THCTensor *input, THCTensor *gradOutput,\n+                         int nBatch, int nChannels,\n+                         int inputHeight, int inputWidth,\n+                         int outputHeight, int outputWidth) {\n+  THArgCheck(inputHeight > 0 && inputWidth > 0\n+             && outputHeight > 0 && outputWidth > 0, 2,\n+             \"input and output sizes should be greater than 0,\"\n+             \" but got input (H: %d, W: %d) output (H: %d, W: %d)\",\n+             inputHeight, inputWidth, outputHeight, outputWidth);\n+  if (input != NULL) {\n+     THCUNN_argCheck(state, input->_dim() == 4, 2, input,\n+                     \"4D input tensor expected but got: %s\");\n+  }\n+\n+  if (gradOutput != NULL) {\n+    THCUNN_check_dim_size(state, gradOutput, 4, 0, nBatch);\n+    THCUNN_check_dim_size(state, gradOutput, 4, 1, nChannels);\n+    THCUNN_check_dim_size(state, gradOutput, 4, 2, outputHeight);\n+    THCUNN_check_dim_size(state, gradOutput, 4, 3, outputWidth);\n   }\n }\n \n+\n void THNN_(SpatialUpSamplingNearest_updateOutput)(\n            THCState *state,\n            THCTensor *input,\n            THCTensor *output,\n-           int scale_factor)\n+\t   int outputHeight,\n+           int outputWidth)\n {\n-  THCTensor_(zero)(state, output);\n-\n   THCUNN_assertSameGPU(state, 2, input, output);\n-  THNN_(SpatialUpSamplingNearest_shapeCheck)(state, input, NULL, scale_factor);\n-  int inputHeight = THCTensor_(size)(state, input, input->dim()-2);\n-  int inputWidth  = THCTensor_(size)(state, input,  input->dim()-1);\n-  int outputHeight = inputHeight * scale_factor;\n-  int outputWidth = inputWidth * scale_factor;\n-\n-   if (input->dim() == 3) {\n-     THCTensor_(resize3d)(state, output,\n-                          THCTensor_(size)(state, input, 0),\n-                          outputHeight, outputWidth);\n-   } else {\n-     THCTensor_(resize4d)(state, output,\n-                          THCTensor_(size)(state, input, 0),\n-                          THCTensor_(size)(state, input, 1),\n-                          outputHeight, outputWidth);\n-  }\n+  int nbatch = THCTensor_(size)(state, input, 0);\n+  int channels = THCTensor_(size)(state, input, 1);\n+  int inputHeight = THCTensor_(size)(state, input, 2);\n+  int inputWidth  = THCTensor_(size)(state, input, 3);\n+\n+  THNN_(SpatialUpSamplingNearest_shapeCheck)(state, input, NULL, nbatch, channels,\n+\t\t  inputHeight, inputWidth,\n+\t\t  outputHeight, outputWidth);\n+  THAssert(inputHeight > 0 && inputWidth > 0 && outputHeight > 0 && outputWidth > 0);\n+\n+  THCTensor_(resize4d)(state, output,\n+                       THCTensor_(size)(state, input, 0),\n+                       THCTensor_(size)(state, input, 1),\n+\t\t       outputHeight,\n+                       outputWidth);\n+  THCTensor_(zero)(state, output);\n \n   input = THCTensor_(newContiguous)(state, input);\n-  // This is for allocating output Tensor\n-  int64_t no_elements = 1;\n-  for(int i = 0; i < input->dim(); i++){\n-    no_elements *= input->size[i];\n-  }\n-  no_elements *= scale_factor * scale_factor;\n-\n-  int d1;\n-  int d2;\n-  int d3;\n-\n-  if (input->dim() == 3) {\n-    d1 = output->size[0];\n-    d2 = output->size[1];\n-    d3 = output->size[2];\n-  } else {\n-    d1 = output->size[1];\n-    d2 = output->size[2];\n-    d3 = output->size[3];\n-  }\n+  THCDeviceTensor<real, 4> idata = toDeviceTensor<real, 4>(state, input);\n+  THCDeviceTensor<real, 4> odata = toDeviceTensor<real, 4>(state, output);\n \n-  real *input_data = THCTensor_(data)(state, input);\n-  real *output_data = THCTensor_(data)(state, output);\n-\n-  // cuda blocks & threads:\n-  int64_t nthreads = 256;\n-  // Max number of blocks: http://en.wikipedia.org/wiki/CUDA\n-  // 65535 for SM 2.x, 2^32 -1 for >= 3.0\n-  // TODO: When we move to SM 3.5 we should update this\n-  int64_t n_xblocks = min(max((int)ceil((float)no_elements / nthreads), 1), 65535);\n-  int64_t n_yblocks = (int64_t)ceil((float)no_elements / (float)(n_xblocks * nthreads));\n-  if (n_yblocks > 65535) {\n-    THError(\"Input size is too large!  aborting\");\n-  }\n-  dim3 blocks(n_xblocks, n_yblocks);\n-  dim3 threads(nthreads);\n+  const int num_kernels = outputHeight * outputWidth;\n+  const int num_threads = THCState_getCurrentDeviceProperties(state)->maxThreadsPerBlock;\n+  cudaStream_t stream = THCState_getCurrentStream(state);\n+  nearest_neighbor_4d_kernel<real, accreal> <<<THCCeilDiv(num_kernels, num_threads), num_threads,\n+\t 0, stream>>>(num_kernels, idata, odata);\n \n-  // kernel:\n-  upscale<<<blocks, threads, 0, THCState_getCurrentStream(state)>>> (input_data, output_data, no_elements, scale_factor, d1, d2, d3);\n   THCudaCheck(cudaGetLastError());\n-\n-  // final cut:\n   THCTensor_(free)(state, input);\n }\n \n+\n+\n void THNN_(SpatialUpSamplingNearest_updateGradInput)(\n            THCState *state,\n-           THCTensor *input,\n            THCTensor *gradOutput,\n            THCTensor *gradInput,\n-           int scale_factor)\n+           int nbatch,\n+\t   int nchannels,\n+\t   int inputHeight,\n+\t   int inputWidth,\n+\t   int outputHeight,\n+\t   int outputWidth)\n {\n-\n   THCUNN_assertSameGPU(state, 2, gradOutput, gradInput);\n-  THNN_(SpatialUpSamplingNearest_shapeCheck)(state, input, gradOutput, scale_factor);\n+  THNN_(SpatialUpSamplingNearest_shapeCheck)(state, NULL, gradOutput, nbatch, nchannels,\n+\t\t  inputHeight, inputWidth, outputHeight, outputWidth);\n   gradOutput = THCTensor_(newContiguous)(state, gradOutput);\n-  THCTensor_(resizeAs)(state, gradInput, input);\n+  THCTensor_(resize4d)(state, gradInput, nbatch, nchannels, inputHeight, inputWidth);\n \n   THCTensor_(zero)(state, gradInput);\n+  THCDeviceTensor<real, 4> data1 = toDeviceTensor<real, 4>(state, gradInput);\n+  THCDeviceTensor<real, 4> data2 = toDeviceTensor<real, 4>(state, gradOutput);\n \n-  real *gradInput_data = THCTensor_(data)(state, gradInput);\n-  real *gradOutput_data = THCTensor_(data)(state, gradOutput);\n-\n-  int64_t no_elements = 1;\n-  for(int i = 0; i < gradInput->dim(); i++){\n-    no_elements *= gradInput->size[i];\n-  }\n-\n-  int d1;\n-  int d2;\n-  int d3;\n-\n-  if (gradInput->dim() == 3) {\n-    d1 = gradInput->size[0];\n-    d2 = gradInput->size[1];\n-    d3 = gradInput->size[2];\n-  } else {\n-    d1 = gradInput->size[1];\n-    d2 = gradInput->size[2];\n-    d3 = gradInput->size[3];\n-  }\n-\n-  // cuda blocks & threads:\n-  int64_t nthreads = 256;\n-  // Max number of blocks: http://en.wikipedia.org/wiki/CUDA\n-  // 65535 for SM 2.x, 2^32 -1 for >= 3.0\n-  // TODO: When we move to SM 3.5 we should update this\n-  int64_t n_xblocks = min(max((int)ceil((float)no_elements / nthreads), 1), 65535);\n-  int64_t n_yblocks = (int64_t)ceil((float)no_elements / (float)(n_xblocks * nthreads));\n-  if (n_yblocks > 65535) {\n-    THError(\"Input size is too large!  aborting\");\n-  }\n-  dim3 blocks(n_xblocks, n_yblocks);\n-  dim3 threads(nthreads);\n+  const int num_kernels = outputHeight * outputWidth;\n+  const int num_threads = \n+\t  THCState_getCurrentDeviceProperties(state)->maxThreadsPerBlock;", "path": "aten/src/THCUNN/generic/SpatialUpSamplingNearest.cu", "position": null, "original_position": 220, "commit_id": "f99c543ab514043a5a842d80d166f4ef7e36d93b", "original_commit_id": "03cfb5387bfea8f995540b427c7e8a2855140963", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "nit: there is a TAB here", "created_at": "2018-07-01T16:16:55Z", "updated_at": "2018-11-23T15:46:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/8591#discussion_r199354137", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8591", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199354137"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8591#discussion_r199354137"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8591"}}, "body_html": "<p>nit: there is a TAB here</p>", "body_text": "nit: there is a TAB here"}