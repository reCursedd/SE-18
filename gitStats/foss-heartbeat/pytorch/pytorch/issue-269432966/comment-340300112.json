{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340300112", "html_url": "https://github.com/pytorch/pytorch/issues/3358#issuecomment-340300112", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3358", "id": 340300112, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDMwMDExMg==", "user": {"login": "IssamLaradji", "id": 3382128, "node_id": "MDQ6VXNlcjMzODIxMjg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3382128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IssamLaradji", "html_url": "https://github.com/IssamLaradji", "followers_url": "https://api.github.com/users/IssamLaradji/followers", "following_url": "https://api.github.com/users/IssamLaradji/following{/other_user}", "gists_url": "https://api.github.com/users/IssamLaradji/gists{/gist_id}", "starred_url": "https://api.github.com/users/IssamLaradji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IssamLaradji/subscriptions", "organizations_url": "https://api.github.com/users/IssamLaradji/orgs", "repos_url": "https://api.github.com/users/IssamLaradji/repos", "events_url": "https://api.github.com/users/IssamLaradji/events{/privacy}", "received_events_url": "https://api.github.com/users/IssamLaradji/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-29T21:28:38Z", "updated_at": "2017-10-29T21:35:08Z", "author_association": "NONE", "body_html": "<p>Let's see if the computed gradients have non-zero values.<br>\nTry running this code just under <code>loss.backward()</code>,</p>\n<div class=\"highlight highlight-source-python\"><pre>loss.backward()\n\n<span class=\"pl-k\">for</span> param <span class=\"pl-k\">in</span> net.parameters():\n    <span class=\"pl-c1\">print</span>(param.grad.data.sum())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> start debugger</span>\n<span class=\"pl-k\">import</span> pdb; pdb.set_trace()\n\noptimizer.step()</pre></div>\n<p>are all the gradient values zero ? if not, then the learning rate might be too small in the optimizer.<br>\nI added the <code>pdb</code> debugger to halt the execution at that point so you can inspect the parameter values.</p>", "body_text": "Let's see if the computed gradients have non-zero values.\nTry running this code just under loss.backward(),\nloss.backward()\n\nfor param in net.parameters():\n    print(param.grad.data.sum())\n\n# start debugger\nimport pdb; pdb.set_trace()\n\noptimizer.step()\nare all the gradient values zero ? if not, then the learning rate might be too small in the optimizer.\nI added the pdb debugger to halt the execution at that point so you can inspect the parameter values.", "body": "Let's see if the computed gradients have non-zero values.  \r\nTry running this code just under `loss.backward()`,\r\n```python\r\nloss.backward()\r\n\r\nfor param in net.parameters():\r\n    print(param.grad.data.sum())\r\n\r\n# start debugger\r\nimport pdb; pdb.set_trace()\r\n\r\noptimizer.step()\r\n```\r\nare all the gradient values zero ? if not, then the learning rate might be too small in the optimizer.\r\nI added the `pdb` debugger to halt the execution at that point so you can inspect the parameter values.\r\n"}