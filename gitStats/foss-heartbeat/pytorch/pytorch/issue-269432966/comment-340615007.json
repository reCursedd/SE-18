{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340615007", "html_url": "https://github.com/pytorch/pytorch/issues/3358#issuecomment-340615007", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3358", "id": 340615007, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDYxNTAwNw==", "user": {"login": "IssamLaradji", "id": 3382128, "node_id": "MDQ6VXNlcjMzODIxMjg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3382128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IssamLaradji", "html_url": "https://github.com/IssamLaradji", "followers_url": "https://api.github.com/users/IssamLaradji/followers", "following_url": "https://api.github.com/users/IssamLaradji/following{/other_user}", "gists_url": "https://api.github.com/users/IssamLaradji/gists{/gist_id}", "starred_url": "https://api.github.com/users/IssamLaradji/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IssamLaradji/subscriptions", "organizations_url": "https://api.github.com/users/IssamLaradji/orgs", "repos_url": "https://api.github.com/users/IssamLaradji/repos", "events_url": "https://api.github.com/users/IssamLaradji/events{/privacy}", "received_events_url": "https://api.github.com/users/IssamLaradji/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-30T23:27:58Z", "updated_at": "2017-10-30T23:27:58Z", "author_association": "NONE", "body_html": "<p>Yes, the learning rate has to be smaller for mini-batch learning,  because the smaller the batch size the larger the gradient variance you have which can cause bad optimization steps when the step size is not decreased.</p>\n<p>Stochastic gradient descent usually oscillate heavily between different loss function values and would asymptotically decrease with lower step size. The convergence rate of stochastic gradient descent depends heavily on the step size [1].</p>\n<p>With full-batch gradient descent, you have the full gradient information of the batch which allows you to take a bigger step size without risking having \"bad\" optimization steps.</p>\n<p>[1] <a href=\"https://www.cs.ubc.ca/~schmidtm/Documents/2014_Google_SAG.pdf\" rel=\"nofollow\">https://www.cs.ubc.ca/~schmidtm/Documents/2014_Google_SAG.pdf</a></p>", "body_text": "Yes, the learning rate has to be smaller for mini-batch learning,  because the smaller the batch size the larger the gradient variance you have which can cause bad optimization steps when the step size is not decreased.\nStochastic gradient descent usually oscillate heavily between different loss function values and would asymptotically decrease with lower step size. The convergence rate of stochastic gradient descent depends heavily on the step size [1].\nWith full-batch gradient descent, you have the full gradient information of the batch which allows you to take a bigger step size without risking having \"bad\" optimization steps.\n[1] https://www.cs.ubc.ca/~schmidtm/Documents/2014_Google_SAG.pdf", "body": "Yes, the learning rate has to be smaller for mini-batch learning,  because the smaller the batch size the larger the gradient variance you have which can cause bad optimization steps when the step size is not decreased. \r\n\r\nStochastic gradient descent usually oscillate heavily between different loss function values and would asymptotically decrease with lower step size. The convergence rate of stochastic gradient descent depends heavily on the step size [1]. \r\n\r\nWith full-batch gradient descent, you have the full gradient information of the batch which allows you to take a bigger step size without risking having \"bad\" optimization steps.\r\n\r\n[1] https://www.cs.ubc.ca/~schmidtm/Documents/2014_Google_SAG.pdf"}