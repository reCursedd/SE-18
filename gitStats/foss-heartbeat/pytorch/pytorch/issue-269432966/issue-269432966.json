{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3358", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3358/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3358/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3358/events", "html_url": "https://github.com/pytorch/pytorch/issues/3358", "id": 269432966, "node_id": "MDU6SXNzdWUyNjk0MzI5NjY=", "number": 3358, "title": "Weights won't update during backpropogation", "user": {"login": "Mickey-Livesgood", "id": 31745473, "node_id": "MDQ6VXNlcjMxNzQ1NDcz", "avatar_url": "https://avatars2.githubusercontent.com/u/31745473?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mickey-Livesgood", "html_url": "https://github.com/Mickey-Livesgood", "followers_url": "https://api.github.com/users/Mickey-Livesgood/followers", "following_url": "https://api.github.com/users/Mickey-Livesgood/following{/other_user}", "gists_url": "https://api.github.com/users/Mickey-Livesgood/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mickey-Livesgood/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mickey-Livesgood/subscriptions", "organizations_url": "https://api.github.com/users/Mickey-Livesgood/orgs", "repos_url": "https://api.github.com/users/Mickey-Livesgood/repos", "events_url": "https://api.github.com/users/Mickey-Livesgood/events{/privacy}", "received_events_url": "https://api.github.com/users/Mickey-Livesgood/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-10-29T20:42:52Z", "updated_at": "2018-10-11T01:56:47Z", "closed_at": "2017-11-01T13:35:20Z", "author_association": "NONE", "body_html": "<p>I have updated my model to use a dataset to train it in batches. Before I did that (and didn't use mini-batches) the model worked great and the backpropogation was training my network, getting it to reach high accuracy).<br>\nHowever, it seems now that the optimizer.step() does not update my weights. I'm not sure what the problem is. I tried to change the learning rate, the loss function and the optimization function but nothing work.</p>\n<p>This is the code of my model (removed irrelevant parts):</p>\n<div class=\"highlight highlight-source-python\"><pre>train_file_name <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>train.csv<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DataSet</span>(<span class=\"pl-e\">Dataset</span>):\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">file_name</span>):\n        xy <span class=\"pl-k\">=</span> np.loadtxt(file_name, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32, <span class=\"pl-v\">skiprows</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">delimiter</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>,<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>xy= pd.read_csv(train_file_name, encoding = \"ISO-8859-1\")</span>\n        <span class=\"pl-c1\">self</span>.len <span class=\"pl-k\">=</span> xy.shape[<span class=\"pl-c1\">0</span>]\n        <span class=\"pl-c1\">self</span>.x_data <span class=\"pl-k\">=</span> torch.from_numpy(xy[:,<span class=\"pl-c1\">3</span>:<span class=\"pl-c1\">303</span>])\n        <span class=\"pl-c1\">self</span>.y_data <span class=\"pl-k\">=</span> torch.from_numpy(xy[:,<span class=\"pl-c1\">1</span>:<span class=\"pl-c1\">3</span>])\n        \n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,<span class=\"pl-smi\">index</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.x_data[index], <span class=\"pl-c1\">self</span>.y_data[index]\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.len\n\nD_in, H, D_out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">300</span>, <span class=\"pl-c1\">300</span>, <span class=\"pl-c1\">2</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">model</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">D_in</span>, <span class=\"pl-smi\">H</span>, <span class=\"pl-smi\">D_out</span>):\n        <span class=\"pl-c1\">super</span>(model, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span># Initiate two linear levels with the dimensions given to the object</span>\n        <span class=\"pl-c1\">self</span>.linear1 <span class=\"pl-k\">=</span> torch.nn.Linear(D_in, H)\n        <span class=\"pl-c1\">self</span>.linear2 <span class=\"pl-k\">=</span> torch.nn.Linear(H, H)\n        <span class=\"pl-c1\">self</span>.linear3 <span class=\"pl-k\">=</span> torch.nn.Linear(H,H)\n        <span class=\"pl-c1\">self</span>.linear4 <span class=\"pl-k\">=</span> torch.nn.Linear(H, H)\n        <span class=\"pl-c1\">self</span>.linear5 <span class=\"pl-k\">=</span> torch.nn.Linear(H, D_out)\n        \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        func <span class=\"pl-k\">=</span> torch.nn.Softmax()\n        z1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear1(x).clamp(<span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> relu</span>\n        z2 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear2(z1).clamp(<span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> relu</span>\n        z3 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear3(z2).clamp(<span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> relu</span>\n        z4 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear4(z3).clamp(<span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> relu</span>\n        z5 <span class=\"pl-k\">=</span> func(<span class=\"pl-c1\">self</span>.linear5(z4))\n        <span class=\"pl-k\">return</span> z5\n \n\nnet <span class=\"pl-k\">=</span> model(D_in, H, D_out)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>criterion = torch.nn.MSELoss(size_average = False)</span>\ncriterion <span class=\"pl-k\">=</span> torch.nn.BCELoss(<span class=\"pl-v\">size_average</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>)\noptimizer <span class=\"pl-k\">=</span> torch.optim.SGD(net.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.00002</span>)\n\n\ntrain_data_set <span class=\"pl-k\">=</span> DataSet(train_file_name)\ntrain_loader <span class=\"pl-k\">=</span> DataLoader(<span class=\"pl-v\">dataset</span><span class=\"pl-k\">=</span>train_data_set, <span class=\"pl-v\">batch_size</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>, <span class=\"pl-v\">shuffle</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">500</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> go in batches of 32</span>\n    <span class=\"pl-k\">for</span> i, data <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(train_loader, <span class=\"pl-c1\">0</span>):\n        inputs, labels <span class=\"pl-k\">=</span> data\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> wrap the input in a Variable</span>\n        inputs, labels <span class=\"pl-k\">=</span> Variable(inputs), Variable(labels)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(inputs, labels)</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> forward pass</span>\n        y_pred <span class=\"pl-k\">=</span> net(inputs)\n        \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> compute the loss</span>\n        loss <span class=\"pl-k\">=</span> criterion(y_pred, labels)\n        <span class=\"pl-c1\">print</span>(epoch, i, loss.data[<span class=\"pl-c1\">0</span>])\n        \n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> backpropogation</span>\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        </pre></div>\n<p>I would love some help on this issue. I'm guessing it has to do with the Dataset and batch loop that I added. Thanks!</p>", "body_text": "I have updated my model to use a dataset to train it in batches. Before I did that (and didn't use mini-batches) the model worked great and the backpropogation was training my network, getting it to reach high accuracy).\nHowever, it seems now that the optimizer.step() does not update my weights. I'm not sure what the problem is. I tried to change the learning rate, the loss function and the optimization function but nothing work.\nThis is the code of my model (removed irrelevant parts):\ntrain_file_name = \"train.csv\"\n\nclass DataSet(Dataset):\n    \n    def __init__(self, file_name):\n        xy = np.loadtxt(file_name, dtype=np.float32, skiprows=1, delimiter=\",\")\n        #xy= pd.read_csv(train_file_name, encoding = \"ISO-8859-1\")\n        self.len = xy.shape[0]\n        self.x_data = torch.from_numpy(xy[:,3:303])\n        self.y_data = torch.from_numpy(xy[:,1:3])\n        \n    def __getitem__(self,index):\n        return self.x_data[index], self.y_data[index]\n    \n    def __len__(self):\n        return self.len\n\nD_in, H, D_out = 300, 300, 2\n\nclass model(torch.nn.Module):\n    \n    def __init__(self, D_in, H, D_out):\n        super(model, self).__init__()\n        ## Initiate two linear levels with the dimensions given to the object\n        self.linear1 = torch.nn.Linear(D_in, H)\n        self.linear2 = torch.nn.Linear(H, H)\n        self.linear3 = torch.nn.Linear(H,H)\n        self.linear4 = torch.nn.Linear(H, H)\n        self.linear5 = torch.nn.Linear(H, D_out)\n        \n    def forward(self, x):\n        func = torch.nn.Softmax()\n        z1 = self.linear1(x).clamp(min=0) # relu\n        z2 = self.linear2(z1).clamp(min=0) # relu\n        z3 = self.linear3(z2).clamp(min=0) # relu\n        z4 = self.linear4(z3).clamp(min=0) # relu\n        z5 = func(self.linear5(z4))\n        return z5\n \n\nnet = model(D_in, H, D_out)\n\n#criterion = torch.nn.MSELoss(size_average = False)\ncriterion = torch.nn.BCELoss(size_average = False)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.00002)\n\n\ntrain_data_set = DataSet(train_file_name)\ntrain_loader = DataLoader(dataset=train_data_set, batch_size = 128, shuffle = True)\n\nfor epoch in range(500):\n    # go in batches of 32\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data\n        # wrap the input in a Variable\n        inputs, labels = Variable(inputs), Variable(labels)\n        #print(inputs, labels)\n        # forward pass\n        y_pred = net(inputs)\n        \n        # compute the loss\n        loss = criterion(y_pred, labels)\n        print(epoch, i, loss.data[0])\n        \n        # backpropogation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \nI would love some help on this issue. I'm guessing it has to do with the Dataset and batch loop that I added. Thanks!", "body": "I have updated my model to use a dataset to train it in batches. Before I did that (and didn't use mini-batches) the model worked great and the backpropogation was training my network, getting it to reach high accuracy).\r\nHowever, it seems now that the optimizer.step() does not update my weights. I'm not sure what the problem is. I tried to change the learning rate, the loss function and the optimization function but nothing work.\r\n\r\nThis is the code of my model (removed irrelevant parts):\r\n```python\r\n\r\ntrain_file_name = \"train.csv\"\r\n\r\nclass DataSet(Dataset):\r\n    \r\n    def __init__(self, file_name):\r\n        xy = np.loadtxt(file_name, dtype=np.float32, skiprows=1, delimiter=\",\")\r\n        #xy= pd.read_csv(train_file_name, encoding = \"ISO-8859-1\")\r\n        self.len = xy.shape[0]\r\n        self.x_data = torch.from_numpy(xy[:,3:303])\r\n        self.y_data = torch.from_numpy(xy[:,1:3])\r\n        \r\n    def __getitem__(self,index):\r\n        return self.x_data[index], self.y_data[index]\r\n    \r\n    def __len__(self):\r\n        return self.len\r\n\r\nD_in, H, D_out = 300, 300, 2\r\n\r\nclass model(torch.nn.Module):\r\n    \r\n    def __init__(self, D_in, H, D_out):\r\n        super(model, self).__init__()\r\n        ## Initiate two linear levels with the dimensions given to the object\r\n        self.linear1 = torch.nn.Linear(D_in, H)\r\n        self.linear2 = torch.nn.Linear(H, H)\r\n        self.linear3 = torch.nn.Linear(H,H)\r\n        self.linear4 = torch.nn.Linear(H, H)\r\n        self.linear5 = torch.nn.Linear(H, D_out)\r\n        \r\n    def forward(self, x):\r\n        func = torch.nn.Softmax()\r\n        z1 = self.linear1(x).clamp(min=0) # relu\r\n        z2 = self.linear2(z1).clamp(min=0) # relu\r\n        z3 = self.linear3(z2).clamp(min=0) # relu\r\n        z4 = self.linear4(z3).clamp(min=0) # relu\r\n        z5 = func(self.linear5(z4))\r\n        return z5\r\n \r\n\r\nnet = model(D_in, H, D_out)\r\n\r\n#criterion = torch.nn.MSELoss(size_average = False)\r\ncriterion = torch.nn.BCELoss(size_average = False)\r\noptimizer = torch.optim.SGD(net.parameters(), lr=0.00002)\r\n\r\n\r\ntrain_data_set = DataSet(train_file_name)\r\ntrain_loader = DataLoader(dataset=train_data_set, batch_size = 128, shuffle = True)\r\n\r\nfor epoch in range(500):\r\n    # go in batches of 32\r\n    for i, data in enumerate(train_loader, 0):\r\n        inputs, labels = data\r\n        # wrap the input in a Variable\r\n        inputs, labels = Variable(inputs), Variable(labels)\r\n        #print(inputs, labels)\r\n        # forward pass\r\n        y_pred = net(inputs)\r\n        \r\n        # compute the loss\r\n        loss = criterion(y_pred, labels)\r\n        print(epoch, i, loss.data[0])\r\n        \r\n        # backpropogation\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n```\r\n\r\nI would love some help on this issue. I'm guessing it has to do with the Dataset and batch loop that I added. Thanks!"}