{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/311981042", "html_url": "https://github.com/pytorch/pytorch/issues/1939#issuecomment-311981042", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1939", "id": 311981042, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTk4MTA0Mg==", "user": {"login": "alykhantejani", "id": 687194, "node_id": "MDQ6VXNlcjY4NzE5NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/687194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alykhantejani", "html_url": "https://github.com/alykhantejani", "followers_url": "https://api.github.com/users/alykhantejani/followers", "following_url": "https://api.github.com/users/alykhantejani/following{/other_user}", "gists_url": "https://api.github.com/users/alykhantejani/gists{/gist_id}", "starred_url": "https://api.github.com/users/alykhantejani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alykhantejani/subscriptions", "organizations_url": "https://api.github.com/users/alykhantejani/orgs", "repos_url": "https://api.github.com/users/alykhantejani/repos", "events_url": "https://api.github.com/users/alykhantejani/events{/privacy}", "received_events_url": "https://api.github.com/users/alykhantejani/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-29T14:19:05Z", "updated_at": "2017-06-29T14:32:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for reporting this <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5272722\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinarjovsky\">@martinarjovsky</a>.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> not sure this is a valid use of the losses - i.e. the output and target have different shapes (one is batched and one is not)?</p>\n<p>Because of the different shapes the broadcasting has side effects (<a href=\"https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#backwards-compatibility\">https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#backwards-compatibility</a>)</p>\n<p>According to the docstrings of <code>binary_cross_entropy</code> and <code>binary_cross_entropy_with_logits</code> the <code>input</code> and <code>target</code> should have the same shape. Shall we just add a check and raise a <code>ValueError</code> if they are not? Happy to send a PR for this</p>", "body_text": "Thanks for reporting this @martinarjovsky.\n@soumith not sure this is a valid use of the losses - i.e. the output and target have different shapes (one is batched and one is not)?\nBecause of the different shapes the broadcasting has side effects (https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#backwards-compatibility)\nAccording to the docstrings of binary_cross_entropy and binary_cross_entropy_with_logits the input and target should have the same shape. Shall we just add a check and raise a ValueError if they are not? Happy to send a PR for this", "body": "Thanks for reporting this @martinarjovsky.\r\n\r\n@soumith not sure this is a valid use of the losses - i.e. the output and target have different shapes (one is batched and one is not)?\r\n\r\nBecause of the different shapes the broadcasting has side effects (https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#backwards-compatibility)\r\n\r\nAccording to the docstrings of `binary_cross_entropy` and `binary_cross_entropy_with_logits` the `input` and `target` should have the same shape. Shall we just add a check and raise a `ValueError` if they are not? Happy to send a PR for this"}