{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1939", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1939/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1939/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1939/events", "html_url": "https://github.com/pytorch/pytorch/issues/1939", "id": 239443715, "node_id": "MDU6SXNzdWUyMzk0NDM3MTU=", "number": 1939, "title": "Different behaviour of BCEWithLogitsLoss and BCELoss + Sigmoid", "user": {"login": "martinarjovsky", "id": 5272722, "node_id": "MDQ6VXNlcjUyNzI3MjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/5272722?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinarjovsky", "html_url": "https://github.com/martinarjovsky", "followers_url": "https://api.github.com/users/martinarjovsky/followers", "following_url": "https://api.github.com/users/martinarjovsky/following{/other_user}", "gists_url": "https://api.github.com/users/martinarjovsky/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinarjovsky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinarjovsky/subscriptions", "organizations_url": "https://api.github.com/users/martinarjovsky/orgs", "repos_url": "https://api.github.com/users/martinarjovsky/repos", "events_url": "https://api.github.com/users/martinarjovsky/events{/privacy}", "received_events_url": "https://api.github.com/users/martinarjovsky/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-06-29T11:09:24Z", "updated_at": "2017-07-04T21:29:53Z", "closed_at": "2017-07-04T21:29:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In certain situations BCEWithLogitsLoss returns the wrong answer. This seems to happen in the case where the output has singleton dimensions (such as the typical case when it's the output of an nn.Linear in classification).</p>\n<p>Snippet to reproduce</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nsigmoid <span class=\"pl-k\">=</span> nn.Sigmoid()\n\nt <span class=\"pl-k\">=</span> np.round(np.random.rand(<span class=\"pl-c1\">64</span>))\no <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">0.5</span>\n\nt <span class=\"pl-k\">=</span> Variable(torch.Tensor(t))\no <span class=\"pl-k\">=</span> Variable(torch.Tensor(o))\n\n<span class=\"pl-c1\">print</span>(nn.BCEWithLogitsLoss()(o, t))\n<span class=\"pl-c1\">print</span>(nn.BCELoss()(sigmoid(o), t)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Different numbers</span>\n\no <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">64</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">0.5</span>\no <span class=\"pl-k\">=</span> Variable(torch.Tensor(o))\n\n<span class=\"pl-c1\">print</span>(nn.BCEWithLogitsLoss()(o, t))\n<span class=\"pl-c1\">print</span>(nn.BCELoss()(sigmoid(o), t)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Same numbers</span></pre></div>\n<p>Thanks :)</p>", "body_text": "In certain situations BCEWithLogitsLoss returns the wrong answer. This seems to happen in the case where the output has singleton dimensions (such as the typical case when it's the output of an nn.Linear in classification).\nSnippet to reproduce\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\nsigmoid = nn.Sigmoid()\n\nt = np.round(np.random.rand(64))\no = np.random.rand(64,1) - 0.5\n\nt = Variable(torch.Tensor(t))\no = Variable(torch.Tensor(o))\n\nprint(nn.BCEWithLogitsLoss()(o, t))\nprint(nn.BCELoss()(sigmoid(o), t)) # Different numbers\n\no = np.random.rand(64) - 0.5\no = Variable(torch.Tensor(o))\n\nprint(nn.BCEWithLogitsLoss()(o, t))\nprint(nn.BCELoss()(sigmoid(o), t)) # Same numbers\nThanks :)", "body": "In certain situations BCEWithLogitsLoss returns the wrong answer. This seems to happen in the case where the output has singleton dimensions (such as the typical case when it's the output of an nn.Linear in classification).\r\n\r\nSnippet to reproduce\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\n\r\nsigmoid = nn.Sigmoid()\r\n\r\nt = np.round(np.random.rand(64))\r\no = np.random.rand(64,1) - 0.5\r\n\r\nt = Variable(torch.Tensor(t))\r\no = Variable(torch.Tensor(o))\r\n\r\nprint(nn.BCEWithLogitsLoss()(o, t))\r\nprint(nn.BCELoss()(sigmoid(o), t)) # Different numbers\r\n\r\no = np.random.rand(64) - 0.5\r\no = Variable(torch.Tensor(o))\r\n\r\nprint(nn.BCEWithLogitsLoss()(o, t))\r\nprint(nn.BCELoss()(sigmoid(o), t)) # Same numbers\r\n```\r\n\r\nThanks :)"}