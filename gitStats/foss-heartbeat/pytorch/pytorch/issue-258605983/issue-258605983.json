{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2778", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2778/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2778/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2778/events", "html_url": "https://github.com/pytorch/pytorch/issues/2778", "id": 258605983, "node_id": "MDU6SXNzdWUyNTg2MDU5ODM=", "number": 2778, "title": "Slightly different gradients on GPU", "user": {"login": "bkj", "id": 6086781, "node_id": "MDQ6VXNlcjYwODY3ODE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6086781?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bkj", "html_url": "https://github.com/bkj", "followers_url": "https://api.github.com/users/bkj/followers", "following_url": "https://api.github.com/users/bkj/following{/other_user}", "gists_url": "https://api.github.com/users/bkj/gists{/gist_id}", "starred_url": "https://api.github.com/users/bkj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bkj/subscriptions", "organizations_url": "https://api.github.com/users/bkj/orgs", "repos_url": "https://api.github.com/users/bkj/repos", "events_url": "https://api.github.com/users/bkj/events{/privacy}", "received_events_url": "https://api.github.com/users/bkj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-18T20:11:38Z", "updated_at": "2017-09-18T21:56:38Z", "closed_at": "2017-09-18T21:56:37Z", "author_association": "NONE", "body_html": "<p>Hi All --</p>\n<p>I noticed that if I compute the gradient of a loss w.r.t. the parameters of the network multiple times, I get slightly different results.  ** EDIT: I've only observed this happening on the GPU, and appears to disappear when I set <code>torch.backends.cudnn.enabled=False</code>  That obviously slows things down though, so ideally I'd like to use <code>cudnn</code> if possible. **</p>\n<pre><code>class Net(nn.Module):\n    def __init__(self):\n        super(Net2, self).__init__()\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 10, kernel_size=5)\n        self.fc1 = nn.Linear(10, 16)\n        self.fc2 = nn.Linear(16, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = x.mean(dim=-1).mean(dim=-1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x\n\n# init network + optimizer\nnet = Net().cuda()\nX_train, y_train = X_train.cuda(), y_train.cuda()\nopt = torch.optim.SGD(net.parameters(), lr=0.1)\n\n# compute gradient w.r.t. parameters\nopt.zero_grad()\nloss = F.cross_entropy(net(X_train), y_train)\nloss.backward()\na = [p.grad.data.clone() for p in opt.param_groups[0]['params']]\n\n# compute gradient again\nopt.zero_grad()\nloss = F.cross_entropy(net(X_train), y_train)\nloss.backward()\nb = [p.grad.data.clone() for p in opt.param_groups[0]['params']]\n\n[(aa == bb).all() for aa, bb in zip(a, b)]\n# returns [True, True, False, True, True, True, True, True]\n\na[2][0][0] - b[2][0][0]\n# returns:\n# 1.00000e-11 *\n#   0.0000 -2.9104  2.9104  0.0000 -2.9104\n#  -2.9104  0.0000 -2.9104  2.9104  2.9104\n#   0.0000  0.0000  0.0000 -2.9104  2.9104\n#   0.0000  2.9104  0.0000  0.0000  0.0000\n#  -2.9104  0.0000 -2.9104  2.9104  2.9104\n# [torch.cuda.FloatTensor of size 5x5 (GPU 0)]\n</code></pre>\n<p>I've only observed this happening on the GPU, rather than the CPU.  Also, when I change the number of channels in <code>conv1</code> and <code>conv2</code>, often the behavior goes away -- eg if I increase <code>10</code> to <code>32</code> in the above example, the behavior goes away.  Anyone have any thoughts on why this might be happening and how I might be able to mitigate it?</p>\n<p>Thanks<br>\nBen</p>\n<p>PS -- If you want to replicate the error, the dataset is as follows:</p>\n<pre><code>from keras.datasets import mnist\n\n(X_train, y_train), _ = mnist.load_data()\nX_train, y_train = X_train[:100], y_train[:100]\n\nX_train = Variable(torch.FloatTensor(X_train.astype('float'))).cuda()\ny_train = Variable(torch.LongTensor(y_train.astype('int'))).cuda()\n\n# expand to 3 channels\nX_train = X_train.view(X_train.size(0), 1, 28, 28)\nX_train = X_train.expand(X_train.size(0), 3, 28, 28)\n\nX_train /= 255\n</code></pre>", "body_text": "Hi All --\nI noticed that if I compute the gradient of a loss w.r.t. the parameters of the network multiple times, I get slightly different results.  ** EDIT: I've only observed this happening on the GPU, and appears to disappear when I set torch.backends.cudnn.enabled=False  That obviously slows things down though, so ideally I'd like to use cudnn if possible. **\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net2, self).__init__()\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 10, kernel_size=5)\n        self.fc1 = nn.Linear(10, 16)\n        self.fc2 = nn.Linear(16, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = x.mean(dim=-1).mean(dim=-1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        return x\n\n# init network + optimizer\nnet = Net().cuda()\nX_train, y_train = X_train.cuda(), y_train.cuda()\nopt = torch.optim.SGD(net.parameters(), lr=0.1)\n\n# compute gradient w.r.t. parameters\nopt.zero_grad()\nloss = F.cross_entropy(net(X_train), y_train)\nloss.backward()\na = [p.grad.data.clone() for p in opt.param_groups[0]['params']]\n\n# compute gradient again\nopt.zero_grad()\nloss = F.cross_entropy(net(X_train), y_train)\nloss.backward()\nb = [p.grad.data.clone() for p in opt.param_groups[0]['params']]\n\n[(aa == bb).all() for aa, bb in zip(a, b)]\n# returns [True, True, False, True, True, True, True, True]\n\na[2][0][0] - b[2][0][0]\n# returns:\n# 1.00000e-11 *\n#   0.0000 -2.9104  2.9104  0.0000 -2.9104\n#  -2.9104  0.0000 -2.9104  2.9104  2.9104\n#   0.0000  0.0000  0.0000 -2.9104  2.9104\n#   0.0000  2.9104  0.0000  0.0000  0.0000\n#  -2.9104  0.0000 -2.9104  2.9104  2.9104\n# [torch.cuda.FloatTensor of size 5x5 (GPU 0)]\n\nI've only observed this happening on the GPU, rather than the CPU.  Also, when I change the number of channels in conv1 and conv2, often the behavior goes away -- eg if I increase 10 to 32 in the above example, the behavior goes away.  Anyone have any thoughts on why this might be happening and how I might be able to mitigate it?\nThanks\nBen\nPS -- If you want to replicate the error, the dataset is as follows:\nfrom keras.datasets import mnist\n\n(X_train, y_train), _ = mnist.load_data()\nX_train, y_train = X_train[:100], y_train[:100]\n\nX_train = Variable(torch.FloatTensor(X_train.astype('float'))).cuda()\ny_train = Variable(torch.LongTensor(y_train.astype('int'))).cuda()\n\n# expand to 3 channels\nX_train = X_train.view(X_train.size(0), 1, 28, 28)\nX_train = X_train.expand(X_train.size(0), 3, 28, 28)\n\nX_train /= 255", "body": "Hi All --\r\n\r\nI noticed that if I compute the gradient of a loss w.r.t. the parameters of the network multiple times, I get slightly different results.  ** EDIT: I've only observed this happening on the GPU, and appears to disappear when I set `torch.backends.cudnn.enabled=False`  That obviously slows things down though, so ideally I'd like to use `cudnn` if possible. **\r\n\r\n```\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net2, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\r\n        self.conv2 = nn.Conv2d(10, 10, kernel_size=5)\r\n        self.fc1 = nn.Linear(10, 16)\r\n        self.fc2 = nn.Linear(16, 10)\r\n        \r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.conv2(x)\r\n        x = x.mean(dim=-1).mean(dim=-1)\r\n        x = self.fc1(x)\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n# init network + optimizer\r\nnet = Net().cuda()\r\nX_train, y_train = X_train.cuda(), y_train.cuda()\r\nopt = torch.optim.SGD(net.parameters(), lr=0.1)\r\n\r\n# compute gradient w.r.t. parameters\r\nopt.zero_grad()\r\nloss = F.cross_entropy(net(X_train), y_train)\r\nloss.backward()\r\na = [p.grad.data.clone() for p in opt.param_groups[0]['params']]\r\n\r\n# compute gradient again\r\nopt.zero_grad()\r\nloss = F.cross_entropy(net(X_train), y_train)\r\nloss.backward()\r\nb = [p.grad.data.clone() for p in opt.param_groups[0]['params']]\r\n\r\n[(aa == bb).all() for aa, bb in zip(a, b)]\r\n# returns [True, True, False, True, True, True, True, True]\r\n\r\na[2][0][0] - b[2][0][0]\r\n# returns:\r\n# 1.00000e-11 *\r\n#   0.0000 -2.9104  2.9104  0.0000 -2.9104\r\n#  -2.9104  0.0000 -2.9104  2.9104  2.9104\r\n#   0.0000  0.0000  0.0000 -2.9104  2.9104\r\n#   0.0000  2.9104  0.0000  0.0000  0.0000\r\n#  -2.9104  0.0000 -2.9104  2.9104  2.9104\r\n# [torch.cuda.FloatTensor of size 5x5 (GPU 0)]\r\n```\r\n\r\nI've only observed this happening on the GPU, rather than the CPU.  Also, when I change the number of channels in `conv1` and `conv2`, often the behavior goes away -- eg if I increase `10` to `32` in the above example, the behavior goes away.  Anyone have any thoughts on why this might be happening and how I might be able to mitigate it?\r\n\r\nThanks\r\nBen\r\n\r\nPS -- If you want to replicate the error, the dataset is as follows:\r\n```\r\nfrom keras.datasets import mnist\r\n\r\n(X_train, y_train), _ = mnist.load_data()\r\nX_train, y_train = X_train[:100], y_train[:100]\r\n\r\nX_train = Variable(torch.FloatTensor(X_train.astype('float'))).cuda()\r\ny_train = Variable(torch.LongTensor(y_train.astype('int'))).cuda()\r\n\r\n# expand to 3 channels\r\nX_train = X_train.view(X_train.size(0), 1, 28, 28)\r\nX_train = X_train.expand(X_train.size(0), 3, 28, 28)\r\n\r\nX_train /= 255\r\n```"}