{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/219991879", "pull_request_review_id": 158298942, "id": 219991879, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxOTk5MTg3OQ==", "diff_hunk": "@@ -1129,9 +1129,9 @@\n   grad_output: softplus_backward(grad, self, beta, threshold, output)\n   self: softplus_double_backward(grad * grad_output, self, beta, threshold)\n \n-- name: softmax_backward_data(Tensor grad_output, Tensor output, int64_t dim, Tensor self)\n-  grad_output: softmax_backward_data(grad, output, dim, self)\n-  self: softmax_double_backward(grad, grad_output, dim, output)\n+- name: _softmax_backward_data(Tensor grad_output, Tensor output, int64_t dim, Tensor self)\n+  grad_output: _softmax_backward_data(grad, output, dim, self)\n+  self: softmax_double_backward(grad, grad_output, dim, output).type_as(self)", "path": "tools/autograd/derivatives.yaml", "position": 43, "original_position": 43, "commit_id": "f2c4397716d665f676a22db6056ecd721e130753", "original_commit_id": "21f616bbd747d6c7fa4d8c5c3b8820d4abfc0fd4", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "No, these are derivatives for double backward, so just for half training w/o double backward both forward and backward will use faster kernels, and avoid cast before softmax/after softmax backward. Double backward for mixed precision will be on fp32 tensors, true, but that's the same as converting input to fp32 and using fp32 softmax all around, so it's never worse than what we have currently. ", "created_at": "2018-09-24T21:16:03Z", "updated_at": "2018-11-23T15:51:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/11719#discussion_r219991879", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11719", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/219991879"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11719#discussion_r219991879"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11719"}}, "body_html": "<p>No, these are derivatives for double backward, so just for half training w/o double backward both forward and backward will use faster kernels, and avoid cast before softmax/after softmax backward. Double backward for mixed precision will be on fp32 tensors, true, but that's the same as converting input to fp32 and using fp32 softmax all around, so it's never worse than what we have currently.</p>", "body_text": "No, these are derivatives for double backward, so just for half training w/o double backward both forward and backward will use faster kernels, and avoid cast before softmax/after softmax backward. Double backward for mixed precision will be on fp32 tensors, true, but that's the same as converting input to fp32 and using fp32 softmax all around, so it's never worse than what we have currently.", "in_reply_to_id": 219977755}