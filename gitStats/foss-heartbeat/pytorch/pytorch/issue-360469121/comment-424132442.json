{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424132442", "html_url": "https://github.com/pytorch/pytorch/pull/11719#issuecomment-424132442", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11719", "id": 424132442, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDEzMjQ0Mg==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-24T21:30:47Z", "updated_at": "2018-09-24T21:30:47Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>I'm not super happy with the upconvert flag. It doesn't really specify the destination type. Should it be float? Should it be double? The context is probably dependent on the device, and this seems to overfit the CUDA context. Can't we apply a simple modification to our kernels, or simply have a _log_softmax_half_to_float implemented only for CUDA, and dispatch to _log_softmax(...).to(dtype) otherwise?</p>\n</blockquote>\n<p><code>upconvert</code> is true only for cuda half inputs with fp32 dtype argument. I could dispatch to _log_softmax_half_to_float in this case, but it would require it's own entries for forward and backward in native_functions and in derivatives, and overall I don't think it would be any prettier.<br>\nAdding modification to kernels to support more input type /dtype combinations with a fast path can be done (in fact, for cuda kernels output type can be anything, it's a separate template parameter), but then dispatch will have to be really tricky (right now dispatch defines <code>scalar_t</code> from which I can derive <code>acc_type</code>, but any other combinations of input/output types would require changes to types defined in dispatch, and instantiating a cross-product of kernels with different input/output types, which no one wants.)</p>", "body_text": "I'm not super happy with the upconvert flag. It doesn't really specify the destination type. Should it be float? Should it be double? The context is probably dependent on the device, and this seems to overfit the CUDA context. Can't we apply a simple modification to our kernels, or simply have a _log_softmax_half_to_float implemented only for CUDA, and dispatch to _log_softmax(...).to(dtype) otherwise?\n\nupconvert is true only for cuda half inputs with fp32 dtype argument. I could dispatch to _log_softmax_half_to_float in this case, but it would require it's own entries for forward and backward in native_functions and in derivatives, and overall I don't think it would be any prettier.\nAdding modification to kernels to support more input type /dtype combinations with a fast path can be done (in fact, for cuda kernels output type can be anything, it's a separate template parameter), but then dispatch will have to be really tricky (right now dispatch defines scalar_t from which I can derive acc_type, but any other combinations of input/output types would require changes to types defined in dispatch, and instantiating a cross-product of kernels with different input/output types, which no one wants.)", "body": "> I'm not super happy with the upconvert flag. It doesn't really specify the destination type. Should it be float? Should it be double? The context is probably dependent on the device, and this seems to overfit the CUDA context. Can't we apply a simple modification to our kernels, or simply have a _log_softmax_half_to_float implemented only for CUDA, and dispatch to _log_softmax(...).to(dtype) otherwise?\r\n\r\n`upconvert` is true only for cuda half inputs with fp32 dtype argument. I could dispatch to _log_softmax_half_to_float in this case, but it would require it's own entries for forward and backward in native_functions and in derivatives, and overall I don't think it would be any prettier. \r\nAdding modification to kernels to support more input type /dtype combinations with a fast path can be done (in fact, for cuda kernels output type can be anything, it's a separate template parameter), but then dispatch will have to be really tricky (right now dispatch defines `scalar_t` from which I can derive `acc_type`, but any other combinations of input/output types would require changes to types defined in dispatch, and instantiating a cross-product of kernels with different input/output types, which no one wants.)"}