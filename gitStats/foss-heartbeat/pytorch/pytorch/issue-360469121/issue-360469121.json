{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11719", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11719/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11719/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11719/events", "html_url": "https://github.com/pytorch/pytorch/pull/11719", "id": 360469121, "node_id": "MDExOlB1bGxSZXF1ZXN0MjE1NzIyNDc3", "number": 11719, "title": "dtype option for softmax", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2018-09-14T22:06:25Z", "updated_at": "2018-11-23T15:51:53Z", "closed_at": "2018-10-14T00:58:29Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11719", "html_url": "https://github.com/pytorch/pytorch/pull/11719", "diff_url": "https://github.com/pytorch/pytorch/pull/11719.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11719.patch"}, "body_html": "<p>Add dtype argument to softmax/log_softmax functions.<br>\nComputing softmax in fp32 precision is necessary for mixed precision training, and converting output of the previous layer into fp32 and then reading it as fp32 in softmax is expensive, memory and perf-wise, this PR allows one to avoid it.<br>\nFor most input data/dtype combinations, input data is converted to dtype and then softmax is computed. If input data is half type and dtype is fp32, kernels with the corresponding template arguments are called.</p>", "body_text": "Add dtype argument to softmax/log_softmax functions.\nComputing softmax in fp32 precision is necessary for mixed precision training, and converting output of the previous layer into fp32 and then reading it as fp32 in softmax is expensive, memory and perf-wise, this PR allows one to avoid it.\nFor most input data/dtype combinations, input data is converted to dtype and then softmax is computed. If input data is half type and dtype is fp32, kernels with the corresponding template arguments are called.", "body": "Add dtype argument to softmax/log_softmax functions. \r\nComputing softmax in fp32 precision is necessary for mixed precision training, and converting output of the previous layer into fp32 and then reading it as fp32 in softmax is expensive, memory and perf-wise, this PR allows one to avoid it. \r\nFor most input data/dtype combinations, input data is converted to dtype and then softmax is computed. If input data is half type and dtype is fp32, kernels with the corresponding template arguments are called. "}