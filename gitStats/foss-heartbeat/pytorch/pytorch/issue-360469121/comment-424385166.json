{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424385166", "html_url": "https://github.com/pytorch/pytorch/pull/11719#issuecomment-424385166", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11719", "id": 424385166, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDM4NTE2Ng==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-25T15:15:02Z", "updated_at": "2018-09-25T15:15:02Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>upconvert is true only for cuda half inputs with fp32 dtype argument</p>\n</blockquote>\n<p>That's precisely the problem. It's a very specific flag, with a very specific meaning, which it not at all implied by its name/function name/function signature.</p>\n<p>I don't understand why the dispatch would be a problem. Can't you just declare the derivatives for the top-level native function <code>log_softmax</code>, and have it take full responsibility for providing the derivative no matter which implementation it chooses?</p>", "body_text": "upconvert is true only for cuda half inputs with fp32 dtype argument\n\nThat's precisely the problem. It's a very specific flag, with a very specific meaning, which it not at all implied by its name/function name/function signature.\nI don't understand why the dispatch would be a problem. Can't you just declare the derivatives for the top-level native function log_softmax, and have it take full responsibility for providing the derivative no matter which implementation it chooses?", "body": "> upconvert is true only for cuda half inputs with fp32 dtype argument\r\n\r\nThat's precisely the problem. It's a very specific flag, with a very specific meaning, which it not at all implied by its name/function name/function signature.\r\n\r\nI don't understand why the dispatch would be a problem. Can't you just declare the derivatives for the top-level native function `log_softmax`, and have it take full responsibility for providing the derivative no matter which implementation it chooses?"}