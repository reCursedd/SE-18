{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424130086", "html_url": "https://github.com/pytorch/pytorch/pull/9238#issuecomment-424130086", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9238", "id": 424130086, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDEzMDA4Ng==", "user": {"login": "YannDubs", "id": 24327668, "node_id": "MDQ6VXNlcjI0MzI3NjY4", "avatar_url": "https://avatars3.githubusercontent.com/u/24327668?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YannDubs", "html_url": "https://github.com/YannDubs", "followers_url": "https://api.github.com/users/YannDubs/followers", "following_url": "https://api.github.com/users/YannDubs/following{/other_user}", "gists_url": "https://api.github.com/users/YannDubs/gists{/gist_id}", "starred_url": "https://api.github.com/users/YannDubs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YannDubs/subscriptions", "organizations_url": "https://api.github.com/users/YannDubs/orgs", "repos_url": "https://api.github.com/users/YannDubs/repos", "events_url": "https://api.github.com/users/YannDubs/events{/privacy}", "received_events_url": "https://api.github.com/users/YannDubs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-24T21:23:39Z", "updated_at": "2018-09-24T21:32:07Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23639302\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vishwakftw\">@vishwakftw</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> not sure if the derivative you computed are correct. It seems to me that the formulas you have were derived adding only <code>eps</code> to one dimension, but what we really want is to use a <code>eps</code> at each dimensions. Indeed, we only get <code>nan</code>s when all the xi's are constant (you don't have 0/0 in the other case).</p>\n<p>I.e what you have :</p>\n<pre><code>N = 7\neps = 1e-9\nx0 = torch.zeros(N, dtype=torch.double, requires_grad=True)\nepsis = torch.zeros(N, dtype=torch.double)\nepsis[0] = eps\nx = x0 + epsis\ny=torch.std(x)\ny.backward()\nprint(x0.grad)\n# tensor([ 0.3780, -0.0630, -0.0630, -0.0630, -0.0630, -0.0630, -0.0630],  dtype=torch.float64)\n</code></pre>\n<p>what I think we want to solve:</p>\n<pre><code>N = 7\neps = 1e-9\nx0 = torch.zeros(N, dtype=torch.double, requires_grad=True)\nx = x0 +eps\ny=torch.std(x)\ny.backward()\nprint(x0.grad)\n# tensor([nan, nan, nan, nan, nan, nan, nan], dtype=torch.float64)\n</code></pre>\n<p>For the second point you have to add eps to each xi's in the definition of the derivative. The limit should give 0.</p>\n<p>Numerical check:</p>\n<pre><code>N = 7\neps = 1e-9\nx = torch.zeros(N, dtype=torch.double)\nprint(float(torch.std(x+eps) / eps))\n# 0.0\n</code></pre>\n<p>I hope I understood correctly the issue.</p>", "body_text": "@vishwakftw @colesbury not sure if the derivative you computed are correct. It seems to me that the formulas you have were derived adding only eps to one dimension, but what we really want is to use a eps at each dimensions. Indeed, we only get nans when all the xi's are constant (you don't have 0/0 in the other case).\nI.e what you have :\nN = 7\neps = 1e-9\nx0 = torch.zeros(N, dtype=torch.double, requires_grad=True)\nepsis = torch.zeros(N, dtype=torch.double)\nepsis[0] = eps\nx = x0 + epsis\ny=torch.std(x)\ny.backward()\nprint(x0.grad)\n# tensor([ 0.3780, -0.0630, -0.0630, -0.0630, -0.0630, -0.0630, -0.0630],  dtype=torch.float64)\n\nwhat I think we want to solve:\nN = 7\neps = 1e-9\nx0 = torch.zeros(N, dtype=torch.double, requires_grad=True)\nx = x0 +eps\ny=torch.std(x)\ny.backward()\nprint(x0.grad)\n# tensor([nan, nan, nan, nan, nan, nan, nan], dtype=torch.float64)\n\nFor the second point you have to add eps to each xi's in the definition of the derivative. The limit should give 0.\nNumerical check:\nN = 7\neps = 1e-9\nx = torch.zeros(N, dtype=torch.double)\nprint(float(torch.std(x+eps) / eps))\n# 0.0\n\nI hope I understood correctly the issue.", "body": "@vishwakftw @colesbury not sure if the derivative you computed are correct. It seems to me that the formulas you have were derived adding only `eps` to one dimension, but what we really want is to use a `eps` at each dimensions. Indeed, we only get `nan`s when all the xi's are constant (you don't have 0/0 in the other case). \r\n\r\nI.e what you have : \r\n\r\n```\r\nN = 7\r\neps = 1e-9\r\nx0 = torch.zeros(N, dtype=torch.double, requires_grad=True)\r\nepsis = torch.zeros(N, dtype=torch.double)\r\nepsis[0] = eps\r\nx = x0 + epsis\r\ny=torch.std(x)\r\ny.backward()\r\nprint(x0.grad)\r\n# tensor([ 0.3780, -0.0630, -0.0630, -0.0630, -0.0630, -0.0630, -0.0630],  dtype=torch.float64)\r\n```\r\n\r\nwhat I think we want to solve:\r\n\r\n```\r\nN = 7\r\neps = 1e-9\r\nx0 = torch.zeros(N, dtype=torch.double, requires_grad=True)\r\nx = x0 +eps\r\ny=torch.std(x)\r\ny.backward()\r\nprint(x0.grad)\r\n# tensor([nan, nan, nan, nan, nan, nan, nan], dtype=torch.float64)\r\n```\r\n\r\nFor the second point you have to add eps to each xi's in the definition of the derivative. The limit should give 0.\r\n\r\nNumerical check:\r\n\r\n```\r\nN = 7\r\neps = 1e-9\r\nx = torch.zeros(N, dtype=torch.double)\r\nprint(float(torch.std(x+eps) / eps))\r\n# 0.0\r\n```\r\n\r\nI hope I understood correctly the issue."}