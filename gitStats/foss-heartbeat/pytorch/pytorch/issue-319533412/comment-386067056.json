{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386067056", "html_url": "https://github.com/pytorch/pytorch/pull/7177#issuecomment-386067056", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7177", "id": 386067056, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjA2NzA1Ng==", "user": {"login": "zuoxingdong", "id": 18168681, "node_id": "MDQ6VXNlcjE4MTY4Njgx", "avatar_url": "https://avatars0.githubusercontent.com/u/18168681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuoxingdong", "html_url": "https://github.com/zuoxingdong", "followers_url": "https://api.github.com/users/zuoxingdong/followers", "following_url": "https://api.github.com/users/zuoxingdong/following{/other_user}", "gists_url": "https://api.github.com/users/zuoxingdong/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuoxingdong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuoxingdong/subscriptions", "organizations_url": "https://api.github.com/users/zuoxingdong/orgs", "repos_url": "https://api.github.com/users/zuoxingdong/repos", "events_url": "https://api.github.com/users/zuoxingdong/events{/privacy}", "received_events_url": "https://api.github.com/users/zuoxingdong/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-02T18:01:22Z", "updated_at": "2018-05-02T18:08:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=648532\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fritzo\">@fritzo</a> For example, in mixture density networks, the loss function is defined as<br>\n<code>L = -1/N*\\sum_{i=1}^{N}ln( \\sum_{k=1}^{K} \\pi_k(x_i) N(\\mu_k(x_i), \\sigma_k(x_i)) )</code><br>\nwhere the Gaussian probability for each input dimension is calculated and multiplied with another vector <code>\\pi</code> for mixing coefficient, and then summed up together, before finally fit into <code>log</code> operation. So I feel it is nontrivial to use current <code>log_prob</code> API to do it neatly.</p>\n<p>In other words, the code might looks like</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">mdn_loss</span>(<span class=\"pl-smi\">pi</span>, <span class=\"pl-smi\">mu</span>, <span class=\"pl-smi\">sigma</span>, <span class=\"pl-smi\">target</span>):\n   prob <span class=\"pl-k\">=</span> pi<span class=\"pl-k\">*</span>Normal(mu, sigma).prob(target)\n   prob <span class=\"pl-k\">=</span> torch.sum(prob, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n   nll_loss <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>torch.log(prob)\n   \n   <span class=\"pl-k\">return</span> nll_loss.mean()</pre></div>\n<p>And in general, this should apply to many different probability distributions as density components. By providing <code>.prob()</code> API, it could be done neatly.</p>", "body_text": "@fritzo For example, in mixture density networks, the loss function is defined as\nL = -1/N*\\sum_{i=1}^{N}ln( \\sum_{k=1}^{K} \\pi_k(x_i) N(\\mu_k(x_i), \\sigma_k(x_i)) )\nwhere the Gaussian probability for each input dimension is calculated and multiplied with another vector \\pi for mixing coefficient, and then summed up together, before finally fit into log operation. So I feel it is nontrivial to use current log_prob API to do it neatly.\nIn other words, the code might looks like\ndef mdn_loss(pi, mu, sigma, target):\n   prob = pi*Normal(mu, sigma).prob(target)\n   prob = torch.sum(prob, dim=1)\n   nll_loss = -torch.log(prob)\n   \n   return nll_loss.mean()\nAnd in general, this should apply to many different probability distributions as density components. By providing .prob() API, it could be done neatly.", "body": "@fritzo For example, in mixture density networks, the loss function is defined as \r\n`L = -1/N*\\sum_{i=1}^{N}ln( \\sum_{k=1}^{K} \\pi_k(x_i) N(\\mu_k(x_i), \\sigma_k(x_i)) )`\r\nwhere the Gaussian probability for each input dimension is calculated and multiplied with another vector `\\pi` for mixing coefficient, and then summed up together, before finally fit into `log` operation. So I feel it is nontrivial to use current `log_prob` API to do it neatly.  \r\n\r\nIn other words, the code might looks like\r\n```python\r\ndef mdn_loss(pi, mu, sigma, target):\r\n   prob = pi*Normal(mu, sigma).prob(target)\r\n   prob = torch.sum(prob, dim=1)\r\n   nll_loss = -torch.log(prob)\r\n   \r\n   return nll_loss.mean()\r\n```\r\nAnd in general, this should apply to many different probability distributions as density components. By providing `.prob()` API, it could be done neatly. "}