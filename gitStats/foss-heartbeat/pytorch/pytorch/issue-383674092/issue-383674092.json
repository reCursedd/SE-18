{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14328", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14328/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14328/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14328/events", "html_url": "https://github.com/pytorch/pytorch/issues/14328", "id": 383674092, "node_id": "MDU6SXNzdWUzODM2NzQwOTI=", "number": 14328, "title": "[caffe2] How to use the euclidean loss (L2) as output of a CNN model?", "user": {"login": "CarlosYeverino", "id": 25825048, "node_id": "MDQ6VXNlcjI1ODI1MDQ4", "avatar_url": "https://avatars0.githubusercontent.com/u/25825048?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CarlosYeverino", "html_url": "https://github.com/CarlosYeverino", "followers_url": "https://api.github.com/users/CarlosYeverino/followers", "following_url": "https://api.github.com/users/CarlosYeverino/following{/other_user}", "gists_url": "https://api.github.com/users/CarlosYeverino/gists{/gist_id}", "starred_url": "https://api.github.com/users/CarlosYeverino/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CarlosYeverino/subscriptions", "organizations_url": "https://api.github.com/users/CarlosYeverino/orgs", "repos_url": "https://api.github.com/users/CarlosYeverino/repos", "events_url": "https://api.github.com/users/CarlosYeverino/events{/privacy}", "received_events_url": "https://api.github.com/users/CarlosYeverino/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-22T23:49:23Z", "updated_at": "2018-11-22T23:49:23Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Dear all,</p>\n<p>I would like to use the euclidean loss (L2) as the output of my CNN model. I am using the brew.db_input to feed the input layer. Should I proceed in the following way?</p>\n<pre><code>def add_input(self, model, batch_size, db, db_type, device_opts):\n        with core.DeviceScope(device_opts):\n            # load the data\n            data_uint8, label = brew.db_input(\n                model,\n                blobs_out=[\"data_uint8\", \"label\"],\n                batch_size=batch_size,\n                db=db,\n                db_type=db_type,\n            )\n            # cast the data to float\n            data = model.Cast(data_uint8, \"data\", to=core.DataType.FLOAT)\n\n            # scale data from [0,255] down to [0,1]\n            data = model.Scale(data, data, scale=float(1./256))\n\n            # don't need the gradient for the backward pass\n            data = model.StopGradient(data, data)\n\n            dataset_size = int (lmdb.open(db).stat()['entries'])\n\n            return data, label, dataset_size\n\n\n\ndata, label, train_dataset_size = self.add_input(train_model, batch_size=batch_size, db=os.path.join(self._data_dir_, 'train-nchw-lmdb'), db_type='lmdb', device_opts=device_opts)\n\npredictions = self.create_model(train_model, data, label, device_opts=device_opts)\n\ndef create_model(self, model, data, label, device_opts):\n    \twith core.DeviceScope(device_opts):\n\n    \t\tdata = data\n      \t\tconv1_ = brew.conv(model, data, 'conv1_', dim_in=3, dim_out=96, kernel=11, stride=4)\n    \t\trelu1_ = brew.relu(model, conv1_, conv1_)\n    \t\tpool1_ = brew.max_pool(model, relu1_, 'pool1_', kernel=3, stride=2)\n      \t\tconv2_ = brew.conv(model, pool1_, 'conv2_', dim_in=96, dim_out=256, kernel=5, stride=4)\n    \t\trelu2_ = brew.relu(model, conv2_, conv2_)\n    \t\tpool2_ = brew.max_pool(model, relu2_, 'pool2_', kernel=3, stride=2)\n      \t\tconv3_ = brew.conv(model, pool2_, 'conv3_', dim_in=256, dim_out=384, kernel=3, stride=1)\n    \t\trelu3_ = brew.relu(model, conv3_, conv3_)\n      \t\tconv4_ = brew.conv(model, relu3_, 'conv4_', dim_in=384, dim_out=384, kernel=3, stride=1)\n    \t\trelu4_ = brew.relu(model, conv4_, conv4_)\n      \t\tconv5_ = brew.conv(model, relu4_, 'conv5_', dim_in=384, dim_out=256, kernel=3, stride=1)\n    \t\trelu5_ = brew.relu(model, conv5_, conv5_)\n    \t\tpool5_ = brew.max_pool(model, relu5_, 'pool5_', kernel=3, stride=2)\n    \t\tfc5_ = brew.fc(model, pool5_, 'fc5_', dim_in=256 * 2 * 3, dim_out=4096)\n    \t\trelu6_ = brew.relu(model, fc5_, fc5_)\n\t\tdropout6_ = brew.dropout(model, relu6_, 'dropout6_', ratio=0.5, is_test=False)\n    \t\tfc6_ = brew.fc(model, dropout6_, 'fc6_', dim_in=4096, dim_out=4096)\n    \t\trelu7_ = brew.relu(model, fc6_, fc6_)\n\t\tdropout7_ = brew.dropout(model, relu7_, 'dropout7_', ratio=0.5, is_test=False)\n    \t\tfc7_ = brew.fc(model, dropout7_, 'fc7_', dim_in=4096, dim_out=256)\n    \t\trelu8_ = brew.relu(model, fc7_, fc7_)\n\t\tdropout8_ = brew.dropout(model, relu8_, 'dropout8_', ratio=0.5, is_test=False)\n    \t\trelu9_ = brew.relu(model, dropout8_, dropout8_)\n    \t\tfc9_ = brew.fc(model, relu9_, 'fc9_', dim_in=256, dim_out=14)\n    \t\t\n    \t\tdist = model.net.SquaredL2Distance([label, fc9_], 'dist')    \n    \t\tpredictions = dist.AveragedLoss([], ['predictions'])\n\n    \t\treturn predictions\n</code></pre>", "body_text": "Dear all,\nI would like to use the euclidean loss (L2) as the output of my CNN model. I am using the brew.db_input to feed the input layer. Should I proceed in the following way?\ndef add_input(self, model, batch_size, db, db_type, device_opts):\n        with core.DeviceScope(device_opts):\n            # load the data\n            data_uint8, label = brew.db_input(\n                model,\n                blobs_out=[\"data_uint8\", \"label\"],\n                batch_size=batch_size,\n                db=db,\n                db_type=db_type,\n            )\n            # cast the data to float\n            data = model.Cast(data_uint8, \"data\", to=core.DataType.FLOAT)\n\n            # scale data from [0,255] down to [0,1]\n            data = model.Scale(data, data, scale=float(1./256))\n\n            # don't need the gradient for the backward pass\n            data = model.StopGradient(data, data)\n\n            dataset_size = int (lmdb.open(db).stat()['entries'])\n\n            return data, label, dataset_size\n\n\n\ndata, label, train_dataset_size = self.add_input(train_model, batch_size=batch_size, db=os.path.join(self._data_dir_, 'train-nchw-lmdb'), db_type='lmdb', device_opts=device_opts)\n\npredictions = self.create_model(train_model, data, label, device_opts=device_opts)\n\ndef create_model(self, model, data, label, device_opts):\n    \twith core.DeviceScope(device_opts):\n\n    \t\tdata = data\n      \t\tconv1_ = brew.conv(model, data, 'conv1_', dim_in=3, dim_out=96, kernel=11, stride=4)\n    \t\trelu1_ = brew.relu(model, conv1_, conv1_)\n    \t\tpool1_ = brew.max_pool(model, relu1_, 'pool1_', kernel=3, stride=2)\n      \t\tconv2_ = brew.conv(model, pool1_, 'conv2_', dim_in=96, dim_out=256, kernel=5, stride=4)\n    \t\trelu2_ = brew.relu(model, conv2_, conv2_)\n    \t\tpool2_ = brew.max_pool(model, relu2_, 'pool2_', kernel=3, stride=2)\n      \t\tconv3_ = brew.conv(model, pool2_, 'conv3_', dim_in=256, dim_out=384, kernel=3, stride=1)\n    \t\trelu3_ = brew.relu(model, conv3_, conv3_)\n      \t\tconv4_ = brew.conv(model, relu3_, 'conv4_', dim_in=384, dim_out=384, kernel=3, stride=1)\n    \t\trelu4_ = brew.relu(model, conv4_, conv4_)\n      \t\tconv5_ = brew.conv(model, relu4_, 'conv5_', dim_in=384, dim_out=256, kernel=3, stride=1)\n    \t\trelu5_ = brew.relu(model, conv5_, conv5_)\n    \t\tpool5_ = brew.max_pool(model, relu5_, 'pool5_', kernel=3, stride=2)\n    \t\tfc5_ = brew.fc(model, pool5_, 'fc5_', dim_in=256 * 2 * 3, dim_out=4096)\n    \t\trelu6_ = brew.relu(model, fc5_, fc5_)\n\t\tdropout6_ = brew.dropout(model, relu6_, 'dropout6_', ratio=0.5, is_test=False)\n    \t\tfc6_ = brew.fc(model, dropout6_, 'fc6_', dim_in=4096, dim_out=4096)\n    \t\trelu7_ = brew.relu(model, fc6_, fc6_)\n\t\tdropout7_ = brew.dropout(model, relu7_, 'dropout7_', ratio=0.5, is_test=False)\n    \t\tfc7_ = brew.fc(model, dropout7_, 'fc7_', dim_in=4096, dim_out=256)\n    \t\trelu8_ = brew.relu(model, fc7_, fc7_)\n\t\tdropout8_ = brew.dropout(model, relu8_, 'dropout8_', ratio=0.5, is_test=False)\n    \t\trelu9_ = brew.relu(model, dropout8_, dropout8_)\n    \t\tfc9_ = brew.fc(model, relu9_, 'fc9_', dim_in=256, dim_out=14)\n    \t\t\n    \t\tdist = model.net.SquaredL2Distance([label, fc9_], 'dist')    \n    \t\tpredictions = dist.AveragedLoss([], ['predictions'])\n\n    \t\treturn predictions", "body": "Dear all,\r\n\r\nI would like to use the euclidean loss (L2) as the output of my CNN model. I am using the brew.db_input to feed the input layer. Should I proceed in the following way?\r\n\r\n```\r\ndef add_input(self, model, batch_size, db, db_type, device_opts):\r\n        with core.DeviceScope(device_opts):\r\n            # load the data\r\n            data_uint8, label = brew.db_input(\r\n                model,\r\n                blobs_out=[\"data_uint8\", \"label\"],\r\n                batch_size=batch_size,\r\n                db=db,\r\n                db_type=db_type,\r\n            )\r\n            # cast the data to float\r\n            data = model.Cast(data_uint8, \"data\", to=core.DataType.FLOAT)\r\n\r\n            # scale data from [0,255] down to [0,1]\r\n            data = model.Scale(data, data, scale=float(1./256))\r\n\r\n            # don't need the gradient for the backward pass\r\n            data = model.StopGradient(data, data)\r\n\r\n            dataset_size = int (lmdb.open(db).stat()['entries'])\r\n\r\n            return data, label, dataset_size\r\n\r\n\r\n\r\ndata, label, train_dataset_size = self.add_input(train_model, batch_size=batch_size, db=os.path.join(self._data_dir_, 'train-nchw-lmdb'), db_type='lmdb', device_opts=device_opts)\r\n\r\npredictions = self.create_model(train_model, data, label, device_opts=device_opts)\r\n\r\ndef create_model(self, model, data, label, device_opts):\r\n    \twith core.DeviceScope(device_opts):\r\n\r\n    \t\tdata = data\r\n      \t\tconv1_ = brew.conv(model, data, 'conv1_', dim_in=3, dim_out=96, kernel=11, stride=4)\r\n    \t\trelu1_ = brew.relu(model, conv1_, conv1_)\r\n    \t\tpool1_ = brew.max_pool(model, relu1_, 'pool1_', kernel=3, stride=2)\r\n      \t\tconv2_ = brew.conv(model, pool1_, 'conv2_', dim_in=96, dim_out=256, kernel=5, stride=4)\r\n    \t\trelu2_ = brew.relu(model, conv2_, conv2_)\r\n    \t\tpool2_ = brew.max_pool(model, relu2_, 'pool2_', kernel=3, stride=2)\r\n      \t\tconv3_ = brew.conv(model, pool2_, 'conv3_', dim_in=256, dim_out=384, kernel=3, stride=1)\r\n    \t\trelu3_ = brew.relu(model, conv3_, conv3_)\r\n      \t\tconv4_ = brew.conv(model, relu3_, 'conv4_', dim_in=384, dim_out=384, kernel=3, stride=1)\r\n    \t\trelu4_ = brew.relu(model, conv4_, conv4_)\r\n      \t\tconv5_ = brew.conv(model, relu4_, 'conv5_', dim_in=384, dim_out=256, kernel=3, stride=1)\r\n    \t\trelu5_ = brew.relu(model, conv5_, conv5_)\r\n    \t\tpool5_ = brew.max_pool(model, relu5_, 'pool5_', kernel=3, stride=2)\r\n    \t\tfc5_ = brew.fc(model, pool5_, 'fc5_', dim_in=256 * 2 * 3, dim_out=4096)\r\n    \t\trelu6_ = brew.relu(model, fc5_, fc5_)\r\n\t\tdropout6_ = brew.dropout(model, relu6_, 'dropout6_', ratio=0.5, is_test=False)\r\n    \t\tfc6_ = brew.fc(model, dropout6_, 'fc6_', dim_in=4096, dim_out=4096)\r\n    \t\trelu7_ = brew.relu(model, fc6_, fc6_)\r\n\t\tdropout7_ = brew.dropout(model, relu7_, 'dropout7_', ratio=0.5, is_test=False)\r\n    \t\tfc7_ = brew.fc(model, dropout7_, 'fc7_', dim_in=4096, dim_out=256)\r\n    \t\trelu8_ = brew.relu(model, fc7_, fc7_)\r\n\t\tdropout8_ = brew.dropout(model, relu8_, 'dropout8_', ratio=0.5, is_test=False)\r\n    \t\trelu9_ = brew.relu(model, dropout8_, dropout8_)\r\n    \t\tfc9_ = brew.fc(model, relu9_, 'fc9_', dim_in=256, dim_out=14)\r\n    \t\t\r\n    \t\tdist = model.net.SquaredL2Distance([label, fc9_], 'dist')    \r\n    \t\tpredictions = dist.AveragedLoss([], ['predictions'])\r\n\r\n    \t\treturn predictions\r\n```\r\n"}