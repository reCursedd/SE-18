{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8805", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8805/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8805/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8805/events", "html_url": "https://github.com/pytorch/pytorch/issues/8805", "id": 335020535, "node_id": "MDU6SXNzdWUzMzUwMjA1MzU=", "number": 8805, "title": "Problem with terminating dataloader workers", "user": {"login": "warmspringwinds", "id": 2501383, "node_id": "MDQ6VXNlcjI1MDEzODM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2501383?v=4", "gravatar_id": "", "url": "https://api.github.com/users/warmspringwinds", "html_url": "https://github.com/warmspringwinds", "followers_url": "https://api.github.com/users/warmspringwinds/followers", "following_url": "https://api.github.com/users/warmspringwinds/following{/other_user}", "gists_url": "https://api.github.com/users/warmspringwinds/gists{/gist_id}", "starred_url": "https://api.github.com/users/warmspringwinds/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/warmspringwinds/subscriptions", "organizations_url": "https://api.github.com/users/warmspringwinds/orgs", "repos_url": "https://api.github.com/users/warmspringwinds/repos", "events_url": "https://api.github.com/users/warmspringwinds/events{/privacy}", "received_events_url": "https://api.github.com/users/warmspringwinds/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-06-22T21:11:55Z", "updated_at": "2018-06-27T18:59:15Z", "closed_at": "2018-06-27T18:51:50Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p>We have tried spawning a process which uses dataloader with multiple workers.<br>\nAfter trying to terminate the main process, the children processes created by dataloader<br>\ndidn't exit and gpu memory wasn't released.</p>\n<h2>Code example</h2>\n<p>We have used this example:<br>\n<a href=\"https://github.com/pytorch/examples/tree/master/mnist\">https://github.com/pytorch/examples/tree/master/mnist</a></p>\n<p>and launched a process</p>\n<pre><code>from multiprocessing import Process\nfrom main import main\n\nprocess = Process(target=main)\n#process.daemon = False\nprocess.start()\n</code></pre>\n<p>After terminating the process with:</p>\n<pre><code>process.terminate()\nprocess.join()\n</code></pre>\n<p>The memory wasn't released and we had to manually kill processes by<br>\ninspecting <code>lsof /dev/nvidia0</code> and using <code>kill -9</code> on them.<br>\nSince the worker processes are daemonic, I was expecting them to terminate once the parent process is terminated.</p>\n<p>This behavior is only present in pytorch 0.4 and was solved in pytorch 0.5 with this commit:<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"314351529\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6606\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6606/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/6606\">#6606</a></p>\n<p>I still don't understand why the aforementioned example doesn't work in pytorch 0.4 though.</p>\n<h2>System Info</h2>\n<pre><code>PyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.11.1\n\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: \nGPU 0: GeForce GTX TITAN X\nGPU 1: GeForce GTX TITAN X\nGPU 2: GeForce GTX TITAN X\nGPU 3: GeForce GTX TITAN X\n\nNvidia driver version: 384.130\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] numpy (1.14.3)\n[pip] numpydoc (0.8.0)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.1)\n[conda] pytorch                   0.4.0           py27_cuda8.0.61_cudnn7.1.2_1    pytorch\n[conda] torchvision               0.2.1                    py27_1    pytorch\n</code></pre>", "body_text": "Issue description\nWe have tried spawning a process which uses dataloader with multiple workers.\nAfter trying to terminate the main process, the children processes created by dataloader\ndidn't exit and gpu memory wasn't released.\nCode example\nWe have used this example:\nhttps://github.com/pytorch/examples/tree/master/mnist\nand launched a process\nfrom multiprocessing import Process\nfrom main import main\n\nprocess = Process(target=main)\n#process.daemon = False\nprocess.start()\n\nAfter terminating the process with:\nprocess.terminate()\nprocess.join()\n\nThe memory wasn't released and we had to manually kill processes by\ninspecting lsof /dev/nvidia0 and using kill -9 on them.\nSince the worker processes are daemonic, I was expecting them to terminate once the parent process is terminated.\nThis behavior is only present in pytorch 0.4 and was solved in pytorch 0.5 with this commit:\n#6606\nI still don't understand why the aforementioned example doesn't work in pytorch 0.4 though.\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\n\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: version 3.11.1\n\nPython version: 2.7\nIs CUDA available: Yes\nCUDA runtime version: Could not collect\nGPU models and configuration: \nGPU 0: GeForce GTX TITAN X\nGPU 1: GeForce GTX TITAN X\nGPU 2: GeForce GTX TITAN X\nGPU 3: GeForce GTX TITAN X\n\nNvidia driver version: 384.130\ncuDNN version: Could not collect\n\nVersions of relevant libraries:\n[pip] numpy (1.14.3)\n[pip] numpydoc (0.8.0)\n[pip] torch (0.4.0)\n[pip] torchvision (0.2.1)\n[conda] pytorch                   0.4.0           py27_cuda8.0.61_cudnn7.1.2_1    pytorch\n[conda] torchvision               0.2.1                    py27_1    pytorch", "body": "## Issue description\r\n\r\nWe have tried spawning a process which uses dataloader with multiple workers.\r\nAfter trying to terminate the main process, the children processes created by dataloader\r\ndidn't exit and gpu memory wasn't released.\r\n\r\n## Code example\r\n\r\nWe have used this example:\r\nhttps://github.com/pytorch/examples/tree/master/mnist\r\n\r\nand launched a process\r\n\r\n```\r\nfrom multiprocessing import Process\r\nfrom main import main\r\n\r\nprocess = Process(target=main)\r\n#process.daemon = False\r\nprocess.start()\r\n```\r\nAfter terminating the process with:\r\n```\r\nprocess.terminate()\r\nprocess.join()\r\n```\r\n\r\nThe memory wasn't released and we had to manually kill processes by\r\ninspecting ```lsof /dev/nvidia0``` and using ```kill -9``` on them.\r\nSince the worker processes are daemonic, I was expecting them to terminate once the parent process is terminated.\r\n\r\nThis behavior is only present in pytorch 0.4 and was solved in pytorch 0.5 with this commit:\r\nhttps://github.com/pytorch/pytorch/pull/6606\r\n\r\nI still don't understand why the aforementioned example doesn't work in pytorch 0.4 though.\r\n\r\n## System Info\r\n\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: version 3.11.1\r\n\r\nPython version: 2.7\r\nIs CUDA available: Yes\r\nCUDA runtime version: Could not collect\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX TITAN X\r\nGPU 1: GeForce GTX TITAN X\r\nGPU 2: GeForce GTX TITAN X\r\nGPU 3: GeForce GTX TITAN X\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] numpydoc (0.8.0)\r\n[pip] torch (0.4.0)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.0           py27_cuda8.0.61_cudnn7.1.2_1    pytorch\r\n[conda] torchvision               0.2.1                    py27_1    pytorch\r\n```"}