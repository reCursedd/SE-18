{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152454824", "pull_request_review_id": 78315770, "id": 152454824, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MjQ1NDgyNA==", "diff_hunk": "@@ -0,0 +1,168 @@\n+#include \"BatchNorm.h\"\n+\n+#include \"Descriptors.h\"\n+#include \"Types.h\"\n+#include \"Utils.h\"\n+\n+#include <ATen/Check.h>\n+\n+\n+namespace at { namespace cudnn {\n+\n+namespace {\n+\n+void batchnorm_shape_check(CheckedFrom c, TensorArg input, TensorArg output, TensorArg running_mean) {\n+  checkDimRange(c, input, 2, 6 /* exclusive */);\n+  checkDim(c, running_mean, 1);\n+  checkContiguous(c, running_mean);\n+  // TODO: Check the rest of the parameters\n+  // TODO: Check output\n+}\n+\n+// TODO: Scale is float even when input is half\n+\n+}  // namespace\n+\n+Tensor cudnn_batch_norm_forward(\n+    const Tensor& input_t, const Tensor& weight_t,\n+    const Tensor& bias_t, const Tensor& running_mean_t, const Tensor& running_var_t,\n+    const Tensor& save_mean_t, const Tensor& save_var_t, bool training,\n+    double exponential_average_factor, double epsilon)\n+{\n+  TensorArg input{ input_t, \"input\", 1 },\n+            weight{ weight_t, \"weight\", 2 },\n+            bias{ bias_t, \"bias\", 3 },\n+            running_mean{ running_mean_t, \"running_mean\", 4 },\n+            running_var{ running_var_t, \"running_var\", 5 },\n+            save_mean{ save_mean_t, \"save_mean\", 6 },\n+            save_var{ save_var_t, \"save_var\", 7 };\n+  CheckedFrom c = \"cudnn_batch_norm_forward\";\n+  cudnnSetStreamToCurrent();\n+  checkSameType(c, {weight, bias, running_mean, running_var, save_mean, save_var});\n+  // Check that if input is half, weight is float (but otherwise, they\n+  // match)\n+  checkSameGPU(c, {input, weight, bias, running_mean, running_var, save_mean, save_var});\n+  cudnnBatchNormMode_t mode;\n+  if (input->dim() == 2) {\n+    mode = CUDNN_BATCHNORM_PER_ACTIVATION;\n+  } else {\n+    mode = CUDNN_BATCHNORM_SPATIAL;\n+#if CUDNN_VERSION >= 7003\n+    if(training)\n+      mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;\n+#endif\n+  }\n+\n+  auto output_t = input->type().tensor();\n+  output_t.resize_(input->sizes());\n+  TensorArg output{ output_t, \"output\", 0 };\n+\n+  // TODO: Don't forget to check shape compatibility\n+  batchnorm_shape_check(c, input, output, running_mean);\n+\n+  auto handle = getCudnnHandle();\n+  auto dataType = getCudnnDataType(*input);\n+  TensorDescriptor idesc{ *input, 4 };  // input descriptor\n+  TensorDescriptor odesc{ *output, 4 };  // output descriptor\n+  // TODO: For some reason, this was previously, inexplicably initialized from\n+  // running_mean.  Check if there was a reason.\n+  TensorDescriptor wdesc{ weight->expand({1, weight->size(0)}), std::max<int64_t>(4, input->dim()) };  // descriptor for weight, bias, running_mean, etc.\n+\n+  Constant one(dataType, 1);\n+  Constant zero(dataType, 0);\n+  if (training) {\n+    checkContiguous(c, {input, bias, running_mean, running_var, save_mean, save_var});\n+    CUDNN_CHECK(cudnnBatchNormalizationForwardTraining(\n+      handle, mode, &one, &zero,\n+      idesc.desc, idesc.ptr,\n+      odesc.desc, odesc.ptr,\n+      wdesc.desc, wdesc.ptr,\n+      bias->data_ptr(),\n+      exponential_average_factor,\n+      running_mean->data_ptr(),\n+      running_var->data_ptr(),\n+      epsilon,\n+      save_mean->data_ptr(),\n+      save_var->data_ptr()));\n+  } else {\n+    checkContiguous(c, {input, bias, running_mean, running_var});\n+    CUDNN_CHECK(cudnnBatchNormalizationForwardInference(\n+      handle, mode, &one, &zero,\n+      idesc.desc, idesc.ptr,\n+      odesc.desc, odesc.ptr,\n+      wdesc.desc, wdesc.ptr,\n+      bias->data_ptr(),\n+      running_mean->data_ptr(),\n+      running_var->data_ptr(),\n+      epsilon));\n+  }\n+\n+  return output_t;\n+}\n+\n+std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(\n+    const Tensor& input_t, const Tensor& grad_output_t, const Tensor& weight_t,\n+    const Tensor& running_mean_t, const Tensor& running_var_t,\n+    const Tensor& save_mean_t, const Tensor& save_var_t, bool training,\n+    double epsilon)\n+{\n+  TensorArg input{ input_t, \"input\", 1 },\n+            grad_output{ grad_output_t, \"grad_output\", 2 },\n+            weight{ weight_t, \"weight\", 3 },\n+            running_mean{ running_mean_t, \"running_mean\", 4 },\n+            running_var{ running_var_t, \"running_var\", 5 },\n+            save_mean{ save_mean_t, \"save_mean\", 6 },\n+            save_var{ save_var_t, \"save_var\", 7 };\n+  CheckedFrom c = \"cudnn_batch_norm_backward\";\n+  cudnnSetStreamToCurrent();\n+  checkSameType(c, {input, grad_output});\n+  checkSameType(c, {weight, running_mean, running_var, save_mean, save_var});\n+  checkSameGPU(c, {input, grad_output, weight, running_mean, running_var, save_mean, save_var});\n+  cudnnBatchNormMode_t mode;\n+  if (input->dim() == 2) {\n+    mode = CUDNN_BATCHNORM_PER_ACTIVATION;\n+  } else {\n+    mode = CUDNN_BATCHNORM_SPATIAL;\n+#if CUDNN_VERSION >= 7003\n+    if(training)\n+      mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;\n+#endif\n+\n+  }\n+\n+  auto grad_input_t = input->type().tensor();", "path": "aten/src/ATen/cudnn/BatchNorm.cpp", "position": null, "original_position": 133, "commit_id": "6c637bca42913c8377068a1bee874b2160c7f6c1", "original_commit_id": "2073551c2b87c8b4045d6fd1dc3d38dd1653504a", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "`auto grad_input_t = input->type().tensor(input->sizes());`\r\n\r\n(We should just add [`empty_like`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.empty_like.html) for this pattern)", "created_at": "2017-11-22T02:25:29Z", "updated_at": "2018-11-23T15:36:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/3666#discussion_r152454824", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3666", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152454824"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3666#discussion_r152454824"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3666"}}, "body_html": "<p><code>auto grad_input_t = input-&gt;type().tensor(input-&gt;sizes());</code></p>\n<p>(We should just add <a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.empty_like.html\" rel=\"nofollow\"><code>empty_like</code></a> for this pattern)</p>", "body_text": "auto grad_input_t = input->type().tensor(input->sizes());\n(We should just add empty_like for this pattern)"}