{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/345893168", "html_url": "https://github.com/pytorch/pytorch/pull/3666#issuecomment-345893168", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3666", "id": 345893168, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTg5MzE2OA==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-21T02:07:19Z", "updated_at": "2017-11-21T02:07:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>More notes about changes:</p>\n<ul>\n<li>\n<p>Some extra functions were added for handling conv versus conv_transpose. Hypothetically, you can just use conv where you need conv_transpose backward, and vice versa, but ACTUALLY (1) in the case of conv transpose and conv backward, the most convenient signatures for users in these two cases are different (one wants an <code>output_padding</code>, the other wants to just provide the actual expected output size and (2) you get better error messages this way, because the argument names are setup correctly (they would not be if you just called the \"wrong\" function.</p>\n</li>\n<li>\n<p>To assist in debugging check arguments failures, I beefed up <code>ATen/Check.h</code> to take an argument <code>CheckedFrom</code>, which is used to say what function we were checking the arguments for when it failed. This is especially helpful for backwards. This also got me thinking about call stacks; I wrote an issue at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"275359064\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/zdevito/ATen/issues/175\" data-hovercard-type=\"issue\" data-hovercard-url=\"/zdevito/ATen/issues/175/hovercard\" href=\"https://github.com/zdevito/ATen/issues/175\">zdevito/ATen#175</a></p>\n</li>\n<li>\n<p>Interestingly, the ported code <em>immediately</em> triggered the deterministic algorithm selection problem. Porting Natalias' fix solved this problem. This is probably because I fixed the benchmarking code to properly account for deterministic versus nondeterministic cases in parameter hashing, see <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"273233378\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3659\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/3659/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/3659\">#3659</a></p>\n</li>\n<li>\n<p>I added a CuDNN tensor descriptor printer for debugging purposes</p>\n</li>\n</ul>\n<p>This is a pretty big patchset. Should you trust it? Bugs that were not caught at compile time but caught at runtime:</p>\n<ul>\n<li>\n<p>I accidentally swapped the input/output parameters for transposed convolution weight backwards. This caused an immediate <code>check</code> failure in the test suite. After fixing this I checked the original code for other invocations of <code>std::swap</code> but this particular case was the only one.</p>\n</li>\n<li>\n<p>I totally misunderstood the function of <code>output_padding</code> and thought that I should eliminate the asymmetry (actually, the asymmetry is necessary because output padding resolves the output size ambiguity in the case of transposed convolution.) My discoveries are now enshrined in code comments but they might be helpful in the user facing documentation as well.</p>\n</li>\n<li>\n<p>I accidentally messed up narrowing for groups by precomputing the tensor size per group once and using it for all values, even though actually the size varies between input and output. I fixed this by introducing a <code>narrowGroup</code> function which serves the same role as <code>tensorPointer</code> in the previous code.</p>\n</li>\n<li>\n<p>I misunderstood the API of <code>narrow</code> and thought that it took an end index in its second argument; it actually takes a length.</p>\n</li>\n<li>\n<p>I wrote some code for weight padding in batch norm incorrectly, and failed to expand it to 5D when the input was 5D. The current coding style is honestly a bit hazardous but I am not sure how to fix it (look for <code>wdesc{ weight-&gt;expand({1, weight-&gt;size(0)}), std::max(4, input-&gt;dim()) }</code>); the \"near miss\" for the other descriptors doesn't apply because descriptor construction with padding will take a max of the inputs; the problem is that weight isn't initially set to the correct dimension.</p>\n</li>\n</ul>\n<p>Affine grid sampler and generator are not yet wired up but they should be easy enough to do.</p>", "body_text": "More notes about changes:\n\n\nSome extra functions were added for handling conv versus conv_transpose. Hypothetically, you can just use conv where you need conv_transpose backward, and vice versa, but ACTUALLY (1) in the case of conv transpose and conv backward, the most convenient signatures for users in these two cases are different (one wants an output_padding, the other wants to just provide the actual expected output size and (2) you get better error messages this way, because the argument names are setup correctly (they would not be if you just called the \"wrong\" function.\n\n\nTo assist in debugging check arguments failures, I beefed up ATen/Check.h to take an argument CheckedFrom, which is used to say what function we were checking the arguments for when it failed. This is especially helpful for backwards. This also got me thinking about call stacks; I wrote an issue at zdevito/ATen#175\n\n\nInterestingly, the ported code immediately triggered the deterministic algorithm selection problem. Porting Natalias' fix solved this problem. This is probably because I fixed the benchmarking code to properly account for deterministic versus nondeterministic cases in parameter hashing, see #3659\n\n\nI added a CuDNN tensor descriptor printer for debugging purposes\n\n\nThis is a pretty big patchset. Should you trust it? Bugs that were not caught at compile time but caught at runtime:\n\n\nI accidentally swapped the input/output parameters for transposed convolution weight backwards. This caused an immediate check failure in the test suite. After fixing this I checked the original code for other invocations of std::swap but this particular case was the only one.\n\n\nI totally misunderstood the function of output_padding and thought that I should eliminate the asymmetry (actually, the asymmetry is necessary because output padding resolves the output size ambiguity in the case of transposed convolution.) My discoveries are now enshrined in code comments but they might be helpful in the user facing documentation as well.\n\n\nI accidentally messed up narrowing for groups by precomputing the tensor size per group once and using it for all values, even though actually the size varies between input and output. I fixed this by introducing a narrowGroup function which serves the same role as tensorPointer in the previous code.\n\n\nI misunderstood the API of narrow and thought that it took an end index in its second argument; it actually takes a length.\n\n\nI wrote some code for weight padding in batch norm incorrectly, and failed to expand it to 5D when the input was 5D. The current coding style is honestly a bit hazardous but I am not sure how to fix it (look for wdesc{ weight->expand({1, weight->size(0)}), std::max(4, input->dim()) }); the \"near miss\" for the other descriptors doesn't apply because descriptor construction with padding will take a max of the inputs; the problem is that weight isn't initially set to the correct dimension.\n\n\nAffine grid sampler and generator are not yet wired up but they should be easy enough to do.", "body": "More notes about changes:\r\n\r\n* Some extra functions were added for handling conv versus conv_transpose. Hypothetically, you can just use conv where you need conv_transpose backward, and vice versa, but ACTUALLY (1) in the case of conv transpose and conv backward, the most convenient signatures for users in these two cases are different (one wants an `output_padding`, the other wants to just provide the actual expected output size and (2) you get better error messages this way, because the argument names are setup correctly (they would not be if you just called the \"wrong\" function.\r\n\r\n* To assist in debugging check arguments failures, I beefed up `ATen/Check.h` to take an argument `CheckedFrom`, which is used to say what function we were checking the arguments for when it failed. This is especially helpful for backwards. This also got me thinking about call stacks; I wrote an issue at https://github.com/zdevito/ATen/issues/175\r\n\r\n* Interestingly, the ported code *immediately* triggered the deterministic algorithm selection problem. Porting Natalias' fix solved this problem. This is probably because I fixed the benchmarking code to properly account for deterministic versus nondeterministic cases in parameter hashing, see https://github.com/pytorch/pytorch/issues/3659\r\n\r\n* I added a CuDNN tensor descriptor printer for debugging purposes\r\n\r\nThis is a pretty big patchset. Should you trust it? Bugs that were not caught at compile time but caught at runtime:\r\n\r\n* I accidentally swapped the input/output parameters for transposed convolution weight backwards. This caused an immediate `check` failure in the test suite. After fixing this I checked the original code for other invocations of `std::swap` but this particular case was the only one.\r\n\r\n* I totally misunderstood the function of `output_padding` and thought that I should eliminate the asymmetry (actually, the asymmetry is necessary because output padding resolves the output size ambiguity in the case of transposed convolution.) My discoveries are now enshrined in code comments but they might be helpful in the user facing documentation as well.\r\n\r\n* I accidentally messed up narrowing for groups by precomputing the tensor size per group once and using it for all values, even though actually the size varies between input and output. I fixed this by introducing a `narrowGroup` function which serves the same role as `tensorPointer` in the previous code.\r\n\r\n* I misunderstood the API of `narrow` and thought that it took an end index in its second argument; it actually takes a length.\r\n\r\n* I wrote some code for weight padding in batch norm incorrectly, and failed to expand it to 5D when the input was 5D. The current coding style is honestly a bit hazardous but I am not sure how to fix it (look for `wdesc{ weight->expand({1, weight->size(0)}), std::max(4, input->dim()) }`); the \"near miss\" for the other descriptors doesn't apply because descriptor construction with padding will take a max of the inputs; the problem is that weight isn't initially set to the correct dimension.\r\n\r\nAffine grid sampler and generator are not yet wired up but they should be easy enough to do."}