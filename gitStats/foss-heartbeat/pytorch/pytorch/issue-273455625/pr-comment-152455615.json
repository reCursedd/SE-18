{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152455615", "pull_request_review_id": 78315770, "id": 152455615, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MjQ1NTYxNQ==", "diff_hunk": "@@ -0,0 +1,1164 @@\n+#include \"Conv.h\"\n+\n+#include \"THC/THC.h\"\n+#include \"Exceptions.h\"\n+#include \"Utils.h\"\n+#include \"Types.h\"\n+\n+#include <ATen/Check.h>\n+\n+#include \"cudnn-wrapper.h\"\n+#include <functional>\n+#include <iterator>\n+#include <sstream>\n+#include <algorithm>\n+#include <memory>\n+#include <mutex>\n+#include <stdint.h>\n+#include <unordered_map>\n+\n+namespace at { namespace cudnn {\n+\n+// TODO: Go through all the checking code again and make sure\n+// we haven't missed anything.\n+\n+// ---------------------------------------------------------------------\n+//\n+// Math\n+//\n+// ---------------------------------------------------------------------\n+\n+constexpr int input_batch_size_dim = 0;  // also grad_input\n+constexpr int input_channels_dim = 1;\n+constexpr int output_batch_size_dim = 0;  // also grad_output\n+constexpr int output_channels_dim = 1;\n+constexpr int weight_output_channels_dim = 0;\n+constexpr int weight_input_channels_dim = 1;\n+\n+// Often written as 2 + max_dim (extra dims for batch size and channels)\n+constexpr int max_dim = 3;\n+\n+// NB: conv_output_size and conv_input_size are not bijections,\n+// as conv_output_size loses information; this is why conv_input_size\n+// takes an extra output_padding argument to resolve the ambiguity.\n+\n+std::vector<int64_t> conv_output_size(\n+    IntList input_size, IntList weight_size,\n+    IntList padding, IntList stride, IntList dilation, int groups\n+) {\n+  // ASSERT(input_size.size() > 2)\n+  // ASSERT(input_size.size() == weight_size.size())\n+  auto dim = input_size.size();\n+  std::vector<int64_t> output_size(dim);\n+  output_size[0] = input_size[input_batch_size_dim];\n+  output_size[1] = weight_size[weight_output_channels_dim];\n+  for (int d = 2; d < dim; ++d) {\n+    auto kernel = dilation[d - 2] * (weight_size[d] - 1) + 1;\n+    output_size[d] = (input_size[d] + (2 * padding[d - 2])\n+                        - kernel) / stride[d - 2] + 1;\n+  }\n+  return output_size;\n+}\n+\n+std::vector<int64_t> conv_input_size(\n+    IntList output_size, IntList weight_size,\n+    IntList padding, IntList output_padding, IntList stride, IntList dilation, int groups\n+) {\n+  // ASSERT(output_size.size() > 2)\n+  // ASSERT(output_size.size() == weight_size.size())\n+  auto dim = output_size.size();\n+  std::vector<int64_t> input_size(dim);\n+  input_size[0] = output_size[output_batch_size_dim];\n+  input_size[1] = weight_size[weight_input_channels_dim] * groups;\n+  for (int d = 2; d < dim; ++d) {\n+    int kernel = dilation[d - 2] * (weight_size[d] - 1) + 1;\n+    input_size[d] = (output_size[d] - 1) * stride[d - 2] - (2 * padding[d - 2]) +\n+                     kernel + output_padding[d - 2];\n+  }\n+  return input_size;\n+}\n+\n+std::vector<int64_t> conv_weight_size(\n+    IntList input_size, IntList output_size,\n+    IntList padding, IntList output_padding, IntList stride, IntList dilation, int groups\n+) {\n+  auto dim = input_size.size();\n+  std::vector<int64_t> weight_size(dim);\n+  weight_size[0] = output_size[1];\n+  weight_size[1] = input_size[1] / groups;\n+  for (int d = 2; d < dim; ++d) {\n+    int kernel = input_size[d] - (output_size[d] - 1) * stride[d - 2]\n+               + 2 * padding[d - 2] - output_padding[d - 2];\n+    weight_size[d] = (kernel - 1) / dilation[d - 2] + 1;\n+  }\n+  return weight_size;\n+}\n+\n+// TODO: Move this into the standard library, with a better name?\n+Tensor narrowGroup(const Tensor& t, int dim, int group_idx, int groups) {\n+  auto group_size = t.size(dim) / groups;\n+  return t.narrow(dim, group_idx * group_size, group_size);\n+}\n+\n+// ---------------------------------------------------------------------\n+//\n+// Checking\n+//\n+// ---------------------------------------------------------------------\n+\n+// Note [Legacy CuDNN grouped convolution support]\n+// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+// CuDNN earlier than CuDNN 7 does not directly support group\n+// convolution, so we provide support for it by sequentially\n+// running a convolution per group  with appropriately\n+// adjusted sizes.  https://blog.yani.io/filter-group-tutorial/\n+// has a fairly good diagram explaining how it works.\n+\n+// Used on pad, stride and dilation\n+static void check_args(CheckedFrom c, IntList args, size_t expected_size, const char* arg_name)\n+{\n+  if (args.size() > expected_size){\n+    std::stringstream ss;\n+    ss << \"Too many \" << arg_name << \" values (\" << args.size() << \") supplied, expecting \" << expected_size << \" (while checking arguments for \" << c << \")\";\n+    throw std::runtime_error(ss.str());\n+  }\n+  else if (args.size() < expected_size){\n+    std::stringstream ss;\n+    ss << \"Not enough \" << arg_name << \" values (\" << args.size() << \") supplied, expecting \" << expected_size << \" (while checking arguments for \" << c << \")\";\n+    throw std::runtime_error(ss.str());\n+  }\n+\n+  auto num_negative_values = std::count_if(args.begin(), args.end(), [](int x){return x < 0;});\n+  if (num_negative_values > 0){\n+    std::stringstream ss;\n+    ss << arg_name << \" should be greater than zero but got (\";\n+    std::copy(args.begin(), args.end() - 1, std::ostream_iterator<int>(ss,\", \"));\n+    ss << args.back() <<  \")\" << \" (while checking arguments for \" << c << \")\";\n+    throw std::runtime_error(ss.str());\n+  }\n+}\n+\n+\n+// NB: For many call sites, it is not strictly necessary to check all of\n+// these relationships (for example, for forward convolution, we compute\n+// the size of output ourselves, so we don't actually need to check\n+// output.  However, writing a single function that does everything\n+// means we get to reuse it for both forwards and all backwards\n+// variants, even when the set of \"real\" inputs varies.  The magic of\n+// relational computing!\n+//\n+// (There is one downside, which is that it is slightly harder to write\n+// error messages which are able to distinguish between real inputs\n+// (which the user can change) and computed inputs (which the user can\n+// only indirectly affect).  It would be an interesting exercise to\n+// come up with a general framework to handle such situations.)\n+static void convolution_shape_check(\n+    CheckedFrom c,\n+    TensorGeometryArg input, TensorGeometryArg weight, TensorGeometryArg output,\n+    IntList padding, IntList stride, IntList dilation, int groups)\n+{\n+  check_args(c, padding, input->dim() - 2, \"padding\");\n+  check_args(c, stride, padding.size(), \"stride\");\n+  check_args(c, dilation, padding.size(), \"dilation\");\n+\n+  // Input\n+  checkDimRange(c, input, 3, 6 /* exclusive */);\n+  checkSize(c, input, input_channels_dim, weight->size(1) * groups);\n+\n+  // Weight\n+  checkContiguous(c, weight);\n+  checkSameDim(c, input, weight);\n+\n+  // Output\n+  {\n+    auto output_sizes = conv_output_size(input->sizes(), weight->sizes(),\n+                                         padding, stride, dilation, groups);\n+    bool invalid_dim_size = false;\n+    for (auto output : output_sizes) {\n+      if (output < 1) invalid_dim_size = true;\n+    }\n+    if (invalid_dim_size){\n+      // NB: This message only makes sense for convolution (not\n+      // transposed convolution); however, you'd be hard pressed to\n+      // trigger it in a transposed convolution, as it would imply that\n+      // you had passed in tensor with zero-size dimensions.\n+      std::stringstream ss;\n+      ss <<  \"Given input size \" << input->sizes()\n+         << \", calculated output size\" << IntList(output_sizes)\n+         << \", but got too small size for \" << output\n+         << \" (while checking arguments for \" << c << \")\";\n+      throw std::runtime_error(ss.str());\n+    }\n+  }\n+  // TODO: check that output->size() matches output_sizes\n+  // TODO: check that weight matches output->sizes()\n+  checkSameDim(c, input, output);\n+}\n+\n+// This POD struct is used to let us easily compute hashes of the\n+// parameters\n+struct ConvolutionParams\n+{\n+  cudnnDataType_t dataType;\n+  int input_size[2 + max_dim];\n+  int input_stride[2 + max_dim];\n+  int weight_size[2 + max_dim];\n+  int padding[max_dim];\n+  int stride[max_dim];\n+  int dilation[max_dim];\n+  int groups;\n+  bool deterministic;\n+  // NB: transposed purposely omitted: transposed just swaps\n+  // forward and backward, so you can reuse the benchmark entry,\n+};\n+// ConvolutionParams must be a POD because we read out its memory\n+// contenst as char* when hashing\n+static_assert(std::is_pod<ConvolutionParams>::value, \"ConvolutionParams not POD\");\n+\n+// NB: This can't be a constructor, because then ConvolutionParams\n+// would not be a POD anymore.\n+// TODO: Use TensorGeometry here instead of the entire Tensor, which we\n+// don't actually need.  (OTOH: We can always pass in\n+// grad_input/grad_output, so this is not very pressing)\n+void setConvolutionParams(\n+    ConvolutionParams* params,\n+    const at::Tensor& input, const at::Tensor& weight,\n+    IntList padding, IntList stride, IntList dilation,\n+    int groups, bool deterministic) {\n+\n+  cudnnDataType_t dataType = getCudnnDataType(input);\n+  memset(params, 0, sizeof(ConvolutionParams));\n+  params->dataType = dataType;\n+  // ASSERT(weight.dim() == input.dim())\n+  for (int i = 0; i != input.dim(); ++i) {\n+    params->input_size[i] = (int) input.size(i);\n+    params->input_stride[i] = (int) input.stride(i);\n+    params->weight_size[i] = (int) weight.size(i);\n+  }\n+  // ASSERT(padding.size() == stride.size())\n+  // ASSERT(padding.size() == dilation.size())\n+  for (size_t i = 0; i != padding.size(); ++i) {\n+    params->padding[i] = padding[i];\n+    params->stride[i] = stride[i];\n+    params->dilation[i] = dilation[i];\n+  }\n+  // In principle, we shouldn't parametrize by groups for legacy\n+  // CuDNN, but it doesn't seem worth the effort to actually do this.\n+  params->groups = groups;\n+  params->deterministic = deterministic;\n+}\n+\n+// ---------------------------------------------------------------------\n+//\n+// Benchmarking\n+//\n+// ---------------------------------------------------------------------\n+\n+// Hashing machinery for ConvolutionParams\n+struct ParamsHash {\n+  std::size_t operator()(const ConvolutionParams& params) const {\n+    auto ptr = reinterpret_cast<const uint8_t*>(&params);\n+    uint32_t value = 0x811C9DC5;\n+    for (int i = 0; i < (int)sizeof(ConvolutionParams); ++i) {\n+      value ^= ptr[i];\n+      value *= 0x01000193;\n+    }\n+    return (size_t)value;\n+  }\n+};\n+\n+struct ParamsEqual {\n+  bool operator()(const ConvolutionParams& a, const ConvolutionParams& b) const {\n+    auto ptr1 = reinterpret_cast<const uint8_t*>(&a);\n+    auto ptr2 = reinterpret_cast<const uint8_t*>(&b);\n+    return memcmp(ptr1, ptr2, sizeof(ConvolutionParams)) == 0;\n+  }\n+};\n+\n+// TODO: Use something less heavy duty than a big honking mutex\n+template <typename T>\n+struct BenchmarkCache {\n+  std::mutex mutex;\n+  std::unordered_map<ConvolutionParams, T, ParamsHash, ParamsEqual> map;\n+\n+  bool find(const ConvolutionParams& params, T* results) {\n+    std::lock_guard<std::mutex> guard(mutex);\n+    auto it = map.find(params);\n+    if (it == map.end()) {\n+      return false;\n+    }\n+    *results = it->second;\n+    return true;\n+  }\n+\n+  void insert(const ConvolutionParams& params, const T& results) {\n+    std::lock_guard<std::mutex> guard(mutex);\n+    map[params] = results;\n+  }\n+};\n+\n+BenchmarkCache<cudnnConvolutionFwdAlgo_t> fwd_algos;\n+BenchmarkCache<cudnnConvolutionBwdDataAlgo_t> bwd_data_algos;\n+BenchmarkCache<cudnnConvolutionBwdFilterAlgo_t> bwd_filter_algos;\n+\n+// TODO: Stop manually allocating CUDA memory; go through ATen\n+// (ATen should provide an API for this!)\n+struct Workspace {\n+  Workspace(size_t size) : size(size), data(NULL) {\n+    CUDA_CHECK(THCudaMalloc(globalContext().lazyInitCUDA(), &data, size));\n+  }\n+  Workspace(const Workspace&) = delete;\n+  Workspace(Workspace&&) = default;\n+  ~Workspace() {\n+    if (data) {\n+      THCudaFree(globalContext().lazyInitCUDA(), data);\n+    }\n+  }\n+\n+  size_t size;\n+  void* data;\n+};\n+\n+template<typename algo_t>\n+struct algorithm_search {\n+};\n+\n+cudnnStatus_t getWorkspaceSize(\n+    cudnnHandle_t handle,\n+    const ConvolutionDescriptor& cdesc,\n+    const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc,\n+    cudnnConvolutionFwdAlgo_t algo, size_t* sz)\n+{\n+    return cudnnGetConvolutionForwardWorkspaceSize(\n+        handle,\n+        idesc.desc,\n+        wdesc.desc,\n+        cdesc.desc,\n+        odesc.desc,\n+        algo,\n+        sz\n+    );\n+}\n+cudnnStatus_t getWorkspaceSize(\n+    cudnnHandle_t handle,\n+    const ConvolutionDescriptor& cdesc,\n+    const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc,\n+    cudnnConvolutionBwdDataAlgo_t algo, size_t* sz)\n+{\n+    return cudnnGetConvolutionBackwardDataWorkspaceSize(\n+        handle,\n+        wdesc.desc,\n+        odesc.desc,\n+        cdesc.desc,\n+        idesc.desc,\n+        algo,\n+        sz);\n+}\n+cudnnStatus_t getWorkspaceSize(\n+    cudnnHandle_t handle,\n+    const ConvolutionDescriptor& cdesc,\n+    const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc,\n+    cudnnConvolutionBwdFilterAlgo_t algo, size_t* sz)\n+{\n+    return cudnnGetConvolutionBackwardFilterWorkspaceSize(\n+        handle,\n+        idesc.desc,\n+        odesc.desc,\n+        cdesc.desc,\n+        wdesc.desc,\n+        algo,\n+        sz);\n+}\n+\n+template<typename algo_t>\n+size_t getMaxWorkspaceSize(\n+    cudnnHandle_t handle,\n+    const ConvolutionDescriptor& cdesc,\n+    const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc,\n+    const algo_t *algo, int n_algo)\n+{\n+    THCState *state = globalContext().lazyInitCUDA();\n+\n+    size_t max_ws_size = 0;\n+    size_t max_block_size = 0;\n+    size_t total_gpu_mem = 0;\n+    size_t free_gpu_mem = 0;\n+\n+    THCudaCheck(THCudaMemGetInfoCached(state, &free_gpu_mem, &total_gpu_mem, &max_block_size));\n+\n+    for (int i = 0; i < n_algo; i++) {\n+        cudnnStatus_t err;\n+        size_t sz;\n+        err = getWorkspaceSize(handle, cdesc, idesc, odesc, wdesc, algo[i], &sz);\n+        if (CUDNN_STATUS_SUCCESS != err || sz == 0\n+            || sz < max_ws_size || sz > max_block_size) continue;\n+        max_ws_size = sz;\n+    }\n+    return max_ws_size;\n+}\n+\n+template<typename perf_t>\n+perf_t getBestAlgorithm(perf_t *perfResults, bool deterministic, int n_algo) {\n+  if (deterministic) {\n+    // iterate over perf results of all algorithms and find the best deterministic algo\n+    for (int i = 0; i < n_algo; i++) {\n+      // TODO: Shouldn't all returned results be successful?\n+      // Double check documentation for cudnnFindConvolutionForwardAlgorithmEx\n+      if (perfResults[i].status == CUDNN_STATUS_SUCCESS &&\n+          perfResults[i].determinism == CUDNN_DETERMINISTIC) {\n+        return perfResults[i];\n+      }\n+    }\n+    throw std::runtime_error(\"no deterministic convolution algorithms available in CuDNN\");\n+  } else {\n+    return perfResults[0];\n+  }\n+}\n+\n+template<>\n+struct algorithm_search<cudnnConvolutionFwdAlgo_t> {\n+  using perf_t = cudnnConvolutionFwdAlgoPerf_t;\n+  using algo_t = cudnnConvolutionFwdAlgo_t;\n+\n+  static constexpr auto DEFAULT_ALGO = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;\n+  static BenchmarkCache<algo_t>& cache() { return fwd_algos; }\n+\n+  static perf_t findAlgorithm(\n+      cudnnHandle_t handle,\n+      ConvolutionParams* params,\n+      const ConvolutionDescriptor& cdesc,\n+      const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc)\n+  {\n+    static const algo_t algos[] = {\n+         CUDNN_CONVOLUTION_FWD_ALGO_GEMM,\n+         CUDNN_CONVOLUTION_FWD_ALGO_FFT,\n+         CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING,\n+         CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,\n+         CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM,\n+         CUDNN_CONVOLUTION_FWD_ALGO_DIRECT,\n+         CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD,\n+         CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED,\n+    };\n+    static constexpr int num_algos = CUDNN_CONVOLUTION_FWD_ALGO_COUNT;\n+    static_assert(sizeof(algos) / sizeof(algos[0]) == num_algos,\n+                  \"Missing cuDNN convolution forward algorithms\");\n+    int perf_count;\n+    std::unique_ptr<perf_t[]> perf_results(new perf_t[num_algos]);\n+    size_t max_ws_size = getMaxWorkspaceSize(handle, cdesc, idesc, odesc, wdesc, algos, num_algos);\n+    Workspace ws(max_ws_size);\n+    CUDNN_CHECK(cudnnFindConvolutionForwardAlgorithmEx(\n+        handle,\n+        idesc.desc, idesc.ptr,\n+        wdesc.desc, wdesc.ptr,\n+        cdesc.desc,\n+        odesc.desc, odesc.ptr,\n+        num_algos,\n+        &perf_count,\n+        perf_results.get(),\n+        ws.data,\n+        ws.size));\n+    return getBestAlgorithm(perf_results.get(), params->deterministic, perf_count);\n+  }\n+\n+  static void getAlgorithm(\n+    cudnnHandle_t handle,\n+    const ConvolutionDescriptor& cdesc,\n+    const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc,\n+    algo_t* algo)\n+  {\n+    cudnnConvolutionFwdPreference_t pref = CUDNN_CONVOLUTION_FWD_PREFER_FASTEST;\n+    CUDNN_CHECK(cudnnGetConvolutionForwardAlgorithm(\n+        handle,\n+        idesc.desc,\n+        wdesc.desc,\n+        cdesc.desc,\n+        odesc.desc,\n+        pref,\n+        0,\n+        algo));\n+  }\n+\n+  static void getWorkspaceSize(\n+    cudnnHandle_t handle,\n+    const ConvolutionDescriptor& cdesc,\n+    const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc,\n+    algo_t algo, size_t* workspaceSize)\n+  {\n+    CUDNN_CHECK(cudnnGetConvolutionForwardWorkspaceSize(\n+        handle,\n+        idesc.desc,\n+        wdesc.desc,\n+        cdesc.desc,\n+        odesc.desc,\n+        algo,\n+        workspaceSize));\n+  }\n+};\n+\n+template<>\n+struct algorithm_search<cudnnConvolutionBwdDataAlgo_t> {\n+  using perf_t = cudnnConvolutionBwdDataAlgoPerf_t;\n+  using algo_t = cudnnConvolutionBwdDataAlgo_t;\n+\n+  static constexpr auto DEFAULT_ALGO = CUDNN_CONVOLUTION_BWD_DATA_ALGO_1;\n+  static BenchmarkCache<algo_t>& cache() { return bwd_data_algos; }\n+\n+  static perf_t findAlgorithm(\n+        cudnnHandle_t handle,\n+        ConvolutionParams* params,\n+        const ConvolutionDescriptor& cdesc,\n+        const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc)\n+  {\n+    static const algo_t algos[] = {\n+        CUDNN_CONVOLUTION_BWD_DATA_ALGO_0,\n+        CUDNN_CONVOLUTION_BWD_DATA_ALGO_1,\n+        CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT,\n+        CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING,\n+        CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD,\n+        CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD_NONFUSED\n+    };\n+    static constexpr int num_algos = CUDNN_CONVOLUTION_BWD_DATA_ALGO_COUNT;\n+    static_assert(sizeof(algos) / sizeof(algos[0]) == num_algos,\n+                  \"Missing cuDNN convolution backward data algorithms.\");\n+    int perf_count;\n+    std::unique_ptr<perf_t[]> perf_results(new perf_t[num_algos]);\n+    size_t max_ws_size = getMaxWorkspaceSize(handle, cdesc, idesc, odesc, wdesc, algos, num_algos);\n+    Workspace ws(max_ws_size);\n+    CUDNN_CHECK(cudnnFindConvolutionBackwardDataAlgorithmEx(\n+        handle,\n+        wdesc.desc, wdesc.ptr,\n+        odesc.desc, odesc.ptr,\n+        cdesc.desc,\n+        idesc.desc, idesc.ptr,\n+        num_algos,\n+        &perf_count,\n+        perf_results.get(),\n+        ws.data,\n+        ws.size));\n+    return getBestAlgorithm(perf_results.get(), params->deterministic, perf_count);\n+  }\n+\n+  static void getAlgorithm(cudnnHandle_t handle,\n+        const ConvolutionDescriptor& cdesc,\n+        const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc,\n+        algo_t* algo) {\n+    CUDNN_CHECK(cudnnGetConvolutionBackwardDataAlgorithm(\n+        handle,\n+        wdesc.desc,\n+        odesc.desc,\n+        cdesc.desc,\n+        idesc.desc,\n+        CUDNN_CONVOLUTION_BWD_DATA_PREFER_FASTEST,\n+        0,\n+        algo));\n+  }\n+\n+  static void getWorkspaceSize(\n+    cudnnHandle_t handle,\n+    const ConvolutionDescriptor& cdesc,\n+    const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc,\n+    cudnnConvolutionBwdDataAlgo_t algo, size_t* workspaceSize)\n+  {\n+    CUDNN_CHECK(cudnnGetConvolutionBackwardDataWorkspaceSize(\n+        handle,\n+        wdesc.desc,\n+        odesc.desc,\n+        cdesc.desc,\n+        idesc.desc,\n+        algo,\n+        workspaceSize));\n+  }\n+};\n+\n+template<>\n+struct algorithm_search<cudnnConvolutionBwdFilterAlgo_t> {\n+  using perf_t = cudnnConvolutionBwdFilterAlgoPerf_t;\n+  using algo_t = cudnnConvolutionBwdFilterAlgo_t;\n+\n+  static constexpr auto DEFAULT_ALGO = CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1;\n+\n+  static BenchmarkCache<algo_t>& cache() { return bwd_filter_algos; }\n+\n+  static perf_t findAlgorithm(\n+        cudnnHandle_t handle, ConvolutionParams* params,\n+        const ConvolutionDescriptor& cdesc,\n+        const TensorDescriptor& idesc, const TensorDescriptor& odesc, const FilterDescriptor& wdesc)\n+  {\n+    static const algo_t algos[] = {\n+        CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0,\n+        CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1,\n+        CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT,\n+        CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3,\n+        CUDNN_CONVOLUTION_BWD_FILTER_ALGO_WINOGRAD_NONFUSED,\n+#if CUDNN_VERSION >= 6000\n+        CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT_TILING,\n+#endif\n+    };\n+    // NOTE: - 1 because ALGO_WINOGRAD is not implemented\n+    static constexpr int num_algos = CUDNN_CONVOLUTION_BWD_FILTER_ALGO_COUNT - 1;\n+    static_assert(sizeof(algos) / sizeof(algos[0]) == num_algos,", "path": "aten/src/ATen/cudnn/Conv.cpp", "position": 560, "original_position": 599, "commit_id": "6c637bca42913c8377068a1bee874b2160c7f6c1", "original_commit_id": "2073551c2b87c8b4045d6fd1dc3d38dd1653504a", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "This will break now if you compile with new versions of cuDNN that have new algos.", "created_at": "2017-11-22T02:32:40Z", "updated_at": "2018-11-23T15:36:43Z", "html_url": "https://github.com/pytorch/pytorch/pull/3666#discussion_r152455615", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3666", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152455615"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3666#discussion_r152455615"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3666"}}, "body_html": "<p>This will break now if you compile with new versions of cuDNN that have new algos.</p>", "body_text": "This will break now if you compile with new versions of cuDNN that have new algos."}