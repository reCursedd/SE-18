{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153821731", "pull_request_review_id": 79881942, "id": 153821731, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MzgyMTczMQ==", "diff_hunk": "@@ -0,0 +1,39 @@\n+#pragma once\n+\n+#include <ATen/Type.h>\n+\n+namespace at {\n+\n+struct TensorGeometry {\n+  TensorGeometry() : storage_offset_(0) {}\n+\n+  explicit TensorGeometry(const Tensor& t)\n+    : sizes_(t.sizes())\n+    , strides_(t.strides())\n+    , storage_offset_(t.storage_offset()) {}\n+\n+  // true if the tensor is contiguous\n+  bool is_contiguous() const;\n+\n+  // creates a new tensor with the sizes and strides of the source\n+  Tensor zeros_with_stride(const Type& type) const;\n+\n+  int64_t dim() const { return sizes_.size(); }\n+  int64_t size(int64_t dim) const { return sizes_.at(static_cast<size_t>(dim)); }\n+  IntList sizes() const { return IntList{ sizes_ }; }\n+  IntList strides() const { return IntList{ strides_ }; }\n+  int64_t storage_offset() const { return storage_offset_; }\n+  int64_t numel() const {\n+    int64_t r = 1;\n+    for (auto s : sizes()) {\n+      r *= s;\n+    }\n+    return r;\n+  }\n+\n+  std::vector<int64_t> sizes_;", "path": "aten/src/ATen/TensorGeometry.h", "position": 34, "original_position": 34, "commit_id": "6c637bca42913c8377068a1bee874b2160c7f6c1", "original_commit_id": "bb46318fddcce1244d22a0a131f7b384d31dedb5", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "body": "If I understand this, every time you run a checking function, it takes a TensorGeometryArg, so every checking function _allocates_  and frees two lists per _every_ tensor passed in. That seems like extremely high overhead for what the expected behavior for checking is (no allocations). Is there a way to do this without allocations for the cases where you will not be keeping the Tensor beyond its normal scoped lifetime?", "created_at": "2017-11-29T15:32:37Z", "updated_at": "2018-11-23T15:36:57Z", "html_url": "https://github.com/pytorch/pytorch/pull/3666#discussion_r153821731", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3666", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/153821731"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3666#discussion_r153821731"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3666"}}, "body_html": "<p>If I understand this, every time you run a checking function, it takes a TensorGeometryArg, so every checking function <em>allocates</em>  and frees two lists per <em>every</em> tensor passed in. That seems like extremely high overhead for what the expected behavior for checking is (no allocations). Is there a way to do this without allocations for the cases where you will not be keeping the Tensor beyond its normal scoped lifetime?</p>", "body_text": "If I understand this, every time you run a checking function, it takes a TensorGeometryArg, so every checking function allocates  and frees two lists per every tensor passed in. That seems like extremely high overhead for what the expected behavior for checking is (no allocations). Is there a way to do this without allocations for the cases where you will not be keeping the Tensor beyond its normal scoped lifetime?"}