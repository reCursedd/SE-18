{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152447227", "pull_request_review_id": 78310897, "id": 152447227, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MjQ0NzIyNw==", "diff_hunk": "@@ -0,0 +1,168 @@\n+#include \"BatchNorm.h\"\n+\n+#include \"Descriptors.h\"\n+#include \"Types.h\"\n+#include \"Utils.h\"\n+\n+#include <ATen/Check.h>\n+\n+\n+namespace at { namespace cudnn {\n+\n+namespace {\n+\n+void batchnorm_shape_check(CheckedFrom c, TensorArg input, TensorArg output, TensorArg running_mean) {\n+  checkDimRange(c, input, 2, 6 /* exclusive */);\n+  checkDim(c, running_mean, 1);\n+  checkContiguous(c, running_mean);\n+  // TODO: Check the rest of the parameters\n+  // TODO: Check output\n+}\n+\n+// TODO: Scale is float even when input is half\n+\n+}  // namespace\n+\n+Tensor cudnn_batch_norm_forward(\n+    const Tensor& input_t, const Tensor& weight_t,\n+    const Tensor& bias_t, const Tensor& running_mean_t, const Tensor& running_var_t,\n+    const Tensor& save_mean_t, const Tensor& save_var_t, bool training,\n+    double exponential_average_factor, double epsilon)\n+{\n+  TensorArg input{ input_t, \"input\", 1 },\n+            weight{ weight_t, \"weight\", 2 },\n+            bias{ bias_t, \"bias\", 3 },\n+            running_mean{ running_mean_t, \"running_mean\", 4 },\n+            running_var{ running_var_t, \"running_var\", 5 },\n+            save_mean{ save_mean_t, \"save_mean\", 6 },\n+            save_var{ save_var_t, \"save_var\", 7 };\n+  CheckedFrom c = \"cudnn_batch_norm_forward\";\n+  cudnnSetStreamToCurrent();\n+  checkSameType(c, {weight, bias, running_mean, running_var, save_mean, save_var});\n+  // Check that if input is half, weight is float (but otherwise, they\n+  // match)", "path": "aten/src/ATen/cudnn/BatchNorm.cpp", "position": null, "original_position": 43, "commit_id": "6c637bca42913c8377068a1bee874b2160c7f6c1", "original_commit_id": "2073551c2b87c8b4045d6fd1dc3d38dd1653504a", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "There's still no check for half/float here?", "created_at": "2017-11-22T01:20:13Z", "updated_at": "2018-11-23T15:36:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/3666#discussion_r152447227", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3666", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152447227"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3666#discussion_r152447227"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3666"}}, "body_html": "<p>There's still no check for half/float here?</p>", "body_text": "There's still no check for half/float here?"}