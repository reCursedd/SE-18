{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152447376", "pull_request_review_id": 78310897, "id": 152447376, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MjQ0NzM3Ng==", "diff_hunk": "@@ -0,0 +1,168 @@\n+#include \"BatchNorm.h\"\n+\n+#include \"Descriptors.h\"\n+#include \"Types.h\"\n+#include \"Utils.h\"\n+\n+#include <ATen/Check.h>\n+\n+\n+namespace at { namespace cudnn {\n+\n+namespace {\n+\n+void batchnorm_shape_check(CheckedFrom c, TensorArg input, TensorArg output, TensorArg running_mean) {\n+  checkDimRange(c, input, 2, 6 /* exclusive */);\n+  checkDim(c, running_mean, 1);\n+  checkContiguous(c, running_mean);\n+  // TODO: Check the rest of the parameters\n+  // TODO: Check output\n+}\n+\n+// TODO: Scale is float even when input is half\n+\n+}  // namespace\n+\n+Tensor cudnn_batch_norm_forward(\n+    const Tensor& input_t, const Tensor& weight_t,\n+    const Tensor& bias_t, const Tensor& running_mean_t, const Tensor& running_var_t,\n+    const Tensor& save_mean_t, const Tensor& save_var_t, bool training,\n+    double exponential_average_factor, double epsilon)\n+{\n+  TensorArg input{ input_t, \"input\", 1 },\n+            weight{ weight_t, \"weight\", 2 },\n+            bias{ bias_t, \"bias\", 3 },\n+            running_mean{ running_mean_t, \"running_mean\", 4 },\n+            running_var{ running_var_t, \"running_var\", 5 },\n+            save_mean{ save_mean_t, \"save_mean\", 6 },\n+            save_var{ save_var_t, \"save_var\", 7 };\n+  CheckedFrom c = \"cudnn_batch_norm_forward\";\n+  cudnnSetStreamToCurrent();\n+  checkSameType(c, {weight, bias, running_mean, running_var, save_mean, save_var});\n+  // Check that if input is half, weight is float (but otherwise, they\n+  // match)\n+  checkSameGPU(c, {input, weight, bias, running_mean, running_var, save_mean, save_var});\n+  cudnnBatchNormMode_t mode;\n+  if (input->dim() == 2) {\n+    mode = CUDNN_BATCHNORM_PER_ACTIVATION;\n+  } else {\n+    mode = CUDNN_BATCHNORM_SPATIAL;\n+#if CUDNN_VERSION >= 7003\n+    if(training)\n+      mode = CUDNN_BATCHNORM_SPATIAL_PERSISTENT;\n+#endif\n+  }\n+\n+  auto output_t = input->type().tensor();\n+  output_t.resize_(input->sizes());\n+  TensorArg output{ output_t, \"output\", 0 };\n+\n+  // TODO: Don't forget to check shape compatibility\n+  batchnorm_shape_check(c, input, output, running_mean);\n+\n+  auto handle = getCudnnHandle();\n+  auto dataType = getCudnnDataType(*input);\n+  TensorDescriptor idesc{ *input, 4 };  // input descriptor\n+  TensorDescriptor odesc{ *output, 4 };  // output descriptor\n+  // TODO: For some reason, this was previously, inexplicably initialized from\n+  // running_mean.  Check if there was a reason.\n+  TensorDescriptor wdesc{ weight->expand({1, weight->size(0)}), std::max<int64_t>(4, input->dim()) };  // descriptor for weight, bias, running_mean, etc.\n+\n+  Constant one(dataType, 1);\n+  Constant zero(dataType, 0);\n+  if (training) {\n+    checkContiguous(c, {input, bias, running_mean, running_var, save_mean, save_var});", "path": "aten/src/ATen/cudnn/BatchNorm.cpp", "position": null, "original_position": 74, "commit_id": "6c637bca42913c8377068a1bee874b2160c7f6c1", "original_commit_id": "2073551c2b87c8b4045d6fd1dc3d38dd1653504a", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "is there a reason weight is not checked for being contiguous?", "created_at": "2017-11-22T01:21:22Z", "updated_at": "2018-11-23T15:36:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/3666#discussion_r152447376", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3666", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/152447376"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3666#discussion_r152447376"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3666"}}, "body_html": "<p>is there a reason weight is not checked for being contiguous?</p>", "body_text": "is there a reason weight is not checked for being contiguous?"}