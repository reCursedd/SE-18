{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/3666", "id": 152257216, "node_id": "MDExOlB1bGxSZXF1ZXN0MTUyMjU3MjE2", "html_url": "https://github.com/pytorch/pytorch/pull/3666", "diff_url": "https://github.com/pytorch/pytorch/pull/3666.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/3666.patch", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3666", "number": 3666, "state": "closed", "locked": false, "title": "CuDNN bindings rewrite (into ATen)", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "```\r\nThe executive summary is that this moves the torch/csrc/cudnn\r\nlibrary into ATen, adding a number of new cudnn_ methods to ATen\r\nfor batchnorm, convolution, affine grid generator and grid sampler.\r\n\r\nATen infra changes:\r\n\r\n- TensorGeometry was moved to ATen\r\n- TensorGeometry was modified to make its interface resemble that of\r\n  Tensor; in particular, sizes is no longer a field, it's a method.\r\n- AT_CUDA_ENABLED macro is set via ATen/Config.h header which is\r\n  generated at cmake configure time.\r\n  Fixes zdevito/ATen#168\r\n- Change AT_CUDA_ENABLED macro to be a function macro, so that we\r\n  error if it is not defined\r\n- Introduce a new TensorArg class, which is a Tensor plus a little\r\n  metadata.  This helps us give good error messages when checking\r\n  dimensions/shapes of tensors.\r\n  Fixes zdevito/ATen#169\r\n- Also introduce a TensorGeometryArg class, for when you don't\r\n  need the actual tensor data (which is most of the time.)\r\n- Add ATen/Check.h, which contains a number of utility functions\r\n  for testing shapes, types and devices of input tensors.  This\r\n  will be particulary useful for native methods, which don't get\r\n  code generated input testing code.  These functions take a\r\n  'CheckedFrom' argument, at the moment just a string, which\r\n  specifies some extra information about what function was\r\n  doing the actual checking; this greatly improves error messages.\r\n    - Many check functions take initializer lists, which let you\r\n      test that all tensors have some property.  This API is\r\n      peculiar, in that we IGNORE undefined tensors in this case.\r\n      This is handled by filterDefined.\r\n- Add AT_CUDNN_ENABLED macro\r\n- CuDNN linking from ATen was improved; for example, we now actually\r\n  add the CuDNN headers to our include path.\r\n- Add some missing override specifiers to some methods\r\n- We now actually build tests with CUDA functionality accessible\r\n  (previously, AT_CUDA_ENABLED was not defined, meaning that\r\n  the headers were missing all CUDA-only functionality.)\r\n- Native functions now support giving explicit names to return\r\n  outputs in yaml.  This makes it possible to hook into the NN\r\n  autogenerated derivatives codepath using native functions.\r\n\r\nCuDNN rewrite changes:\r\n\r\n- torch/csrc/cudnn now uses ATen (rather than passing around\r\n  THVoidTensor) and lives in ATen.  This lets us remove tensorPointer\r\n  shenanigans.  The functions are exposed to ATen as native functions\r\n  described in aten/src/ATen/cudnn/cuDNN.yaml\r\n- ATen now builds and links against CuDNN when enabled.  The cmake\r\n  package script was taken from Caffe2.\r\n- Some header reorganization was done to help reduce dependencies\r\n  on headers (this reorg is no longer used but I've kept it)\r\n- Rename CHECK to CUDNN_CHECK\r\n- Rip out old shape/type testing code in favor of modern ATen/Check.h\r\n  interface using TensorArg.  In many cases, increase the robustness of\r\n  the checking code.\r\n- Change the inputs of the public facing functions, so that they can\r\n  be bound by ATen\r\n  - Delete THCState*; this is retrieved from the global ATen context\r\n  - Delete cudnnHandle_t, this is retrieved from the global Handles.h\r\n  - Delete cudnnDataType_t, this is retrieved from the Tensor type\r\n  - Delete Convolution class, instead its constituent arguments are\r\n    passed individually\r\n- Change functions to return tensors, rather than take an appropriately\r\n  sized output tensor as an input.\r\n- Redo how transposed convolution / backward convolution is implemented\r\n  (knock on effect of returning tensors).  Previously it was assumed\r\n  that you would always pass an appropriately sized output tensor, but\r\n  we don't want to do this anymore.  For backwards, we instead give\r\n  the desired output tensor (input, really) size, because that is\r\n  readily available.  For *transposed* convolution, however, we take\r\n  output_padding, and otherwise do the shape calculation.\r\n- Redo how legacy group convolution is implemented (knock on effect from\r\n  porting cudnn to ATen.)  Previously, group convolution was implemented\r\n  by manually constructing sizes and strides and then outputting\r\n  appropriate, with macros switching between individual groups and\r\n  all-at-once based on CuDNN version.  Now, the code looks exactly what\r\n  you'd expect: there's a top-level wrapping function that supports\r\n  group convolution no matter the version of CuDNN, and a low-level\r\n  wrapper which supports only what CuDNN supports.  The top-level\r\n  function conditions on CuDNN version, and invokes the low-level\r\n  interface 1 or n times.\r\n- There is now a debugging printer for tensor descriptors.\r\n- Convolution struct is replaced with ConvolutionArgs, which is not\r\n  part of the public API but is used internally to conveniently\r\n  pass around all of the arguments needed for Convolution.\r\n- Add some constexprs for well-known dimensions, reduce amount of\r\n  magic numbers in code.\r\n- Put 'deterministic' in to ConvParams.  Fixes #3659\r\n- Lots more comments.\r\n- Some pessimizations, in the name of code clarity:\r\n  - The descriptors are initialized on every invocation of convolution\r\n    forward/backward.  Previously, the descriptors were cached, so that\r\n    you didn't have to initialize them again on backwards.  This is\r\n    difficult to support in the ATen interface so I didn't support it.\r\n  - Legacy group convolution initializes its workspace for *every* group\r\n    it performs.  I did not feel motivated to fix this because the\r\n    legacy codepath is already quite slow.\r\n- Affine grid generator and grid sampler automatically call contiguous\r\n  on their arguments as necessary.\r\n- Batchnorm input checking is greatly beefed up, it now checks for\r\n  the following input characteristics:\r\n    - Definedness\r\n    - GPU location\r\n    - Type\r\n    - Contiguity\r\n    - Size\r\n\r\nPyTorch binding code changes\r\n\r\n- batchnorm now uses consistent var/data naming\r\n- batchnorm and convolution make use of new ATen bindings\r\n- Affine grid generator and grid sampler make use of ATen CuDNN\r\n  bindings via derivatives.yaml.  This means I had to restructure\r\n  the code a little, since the THNN bindings still go through\r\n  a legacy Python class.\r\n- I fixed some warnings:\r\n  - s/friend class/friend struct/ on InterpreterStateImpl\r\n  - Removed pessimizing move 'detached' in torch/csrc/autograd/variable.cpp\r\n  - Removed unused pack_list on Scalar\r\n```", "created_at": "2017-11-13T14:49:23Z", "updated_at": "2018-11-23T15:36:59Z", "closed_at": "2017-12-01T04:06:58Z", "merged_at": "2017-12-01T04:06:58Z", "merge_commit_sha": "1c0fbd27a10b7036b1db643014e65cae7f6d0266", "assignee": null, "assignees": [], "requested_reviewers": [], "requested_teams": [], "labels": [], "milestone": null, "commits_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3666/commits", "review_comments_url": "https://api.github.com/repos/pytorch/pytorch/pulls/3666/comments", "review_comment_url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments{/number}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3666/comments", "statuses_url": "https://api.github.com/repos/pytorch/pytorch/statuses/6c637bca42913c8377068a1bee874b2160c7f6c1", "head": {"label": "ezyang:pr/cudnn-rewrite", "ref": "pr/cudnn-rewrite", "sha": "6c637bca42913c8377068a1bee874b2160c7f6c1", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "repo": {"id": 101798885, "node_id": "MDEwOlJlcG9zaXRvcnkxMDE3OTg4ODU=", "name": "pytorch", "full_name": "ezyang/pytorch", "private": false, "owner": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "html_url": "https://github.com/ezyang/pytorch", "description": "Tensors and Dynamic neural networks in Python  with strong GPU acceleration", "fork": true, "url": "https://api.github.com/repos/ezyang/pytorch", "forks_url": "https://api.github.com/repos/ezyang/pytorch/forks", "keys_url": "https://api.github.com/repos/ezyang/pytorch/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/ezyang/pytorch/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/ezyang/pytorch/teams", "hooks_url": "https://api.github.com/repos/ezyang/pytorch/hooks", "issue_events_url": "https://api.github.com/repos/ezyang/pytorch/issues/events{/number}", "events_url": "https://api.github.com/repos/ezyang/pytorch/events", "assignees_url": "https://api.github.com/repos/ezyang/pytorch/assignees{/user}", "branches_url": "https://api.github.com/repos/ezyang/pytorch/branches{/branch}", "tags_url": "https://api.github.com/repos/ezyang/pytorch/tags", "blobs_url": "https://api.github.com/repos/ezyang/pytorch/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/ezyang/pytorch/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/ezyang/pytorch/git/refs{/sha}", "trees_url": "https://api.github.com/repos/ezyang/pytorch/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/ezyang/pytorch/statuses/{sha}", "languages_url": "https://api.github.com/repos/ezyang/pytorch/languages", "stargazers_url": "https://api.github.com/repos/ezyang/pytorch/stargazers", "contributors_url": "https://api.github.com/repos/ezyang/pytorch/contributors", "subscribers_url": "https://api.github.com/repos/ezyang/pytorch/subscribers", "subscription_url": "https://api.github.com/repos/ezyang/pytorch/subscription", "commits_url": "https://api.github.com/repos/ezyang/pytorch/commits{/sha}", "git_commits_url": "https://api.github.com/repos/ezyang/pytorch/git/commits{/sha}", "comments_url": "https://api.github.com/repos/ezyang/pytorch/comments{/number}", "issue_comment_url": "https://api.github.com/repos/ezyang/pytorch/issues/comments{/number}", "contents_url": "https://api.github.com/repos/ezyang/pytorch/contents/{+path}", "compare_url": "https://api.github.com/repos/ezyang/pytorch/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/ezyang/pytorch/merges", "archive_url": "https://api.github.com/repos/ezyang/pytorch/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/ezyang/pytorch/downloads", "issues_url": "https://api.github.com/repos/ezyang/pytorch/issues{/number}", "pulls_url": "https://api.github.com/repos/ezyang/pytorch/pulls{/number}", "milestones_url": "https://api.github.com/repos/ezyang/pytorch/milestones{/number}", "notifications_url": "https://api.github.com/repos/ezyang/pytorch/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/ezyang/pytorch/labels{/name}", "releases_url": "https://api.github.com/repos/ezyang/pytorch/releases{/id}", "deployments_url": "https://api.github.com/repos/ezyang/pytorch/deployments", "created_at": "2017-08-29T19:28:39Z", "updated_at": "2018-10-29T15:06:40Z", "pushed_at": "2018-11-21T22:30:09Z", "git_url": "git://github.com/ezyang/pytorch.git", "ssh_url": "git@github.com:ezyang/pytorch.git", "clone_url": "https://github.com/ezyang/pytorch.git", "svn_url": "https://github.com/ezyang/pytorch", "homepage": "http://pytorch.org", "size": 88254, "stargazers_count": 1, "watchers_count": 1, "language": "C++", "has_issues": false, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "forks_count": 0, "mirror_url": null, "archived": false, "open_issues_count": 2, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "forks": 0, "open_issues": 2, "watchers": 1, "default_branch": "master"}}, "base": {"label": "pytorch:master", "ref": "master", "sha": "ab0a7eb7bfb669485eb024a0baec4c28bcf516d4", "user": {"login": "pytorch", "id": 21003710, "node_id": "MDEyOk9yZ2FuaXphdGlvbjIxMDAzNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/21003710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pytorch", "html_url": "https://github.com/pytorch", "followers_url": "https://api.github.com/users/pytorch/followers", "following_url": "https://api.github.com/users/pytorch/following{/other_user}", "gists_url": "https://api.github.com/users/pytorch/gists{/gist_id}", "starred_url": "https://api.github.com/users/pytorch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pytorch/subscriptions", "organizations_url": "https://api.github.com/users/pytorch/orgs", "repos_url": "https://api.github.com/users/pytorch/repos", "events_url": "https://api.github.com/users/pytorch/events{/privacy}", "received_events_url": "https://api.github.com/users/pytorch/received_events", "type": "Organization", "site_admin": false}, "repo": {"id": 65600975, "node_id": "MDEwOlJlcG9zaXRvcnk2NTYwMDk3NQ==", "name": "pytorch", "full_name": "pytorch/pytorch", "private": false, "owner": {"login": "pytorch", "id": 21003710, "node_id": "MDEyOk9yZ2FuaXphdGlvbjIxMDAzNzEw", "avatar_url": "https://avatars3.githubusercontent.com/u/21003710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pytorch", "html_url": "https://github.com/pytorch", "followers_url": "https://api.github.com/users/pytorch/followers", "following_url": "https://api.github.com/users/pytorch/following{/other_user}", "gists_url": "https://api.github.com/users/pytorch/gists{/gist_id}", "starred_url": "https://api.github.com/users/pytorch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pytorch/subscriptions", "organizations_url": "https://api.github.com/users/pytorch/orgs", "repos_url": "https://api.github.com/users/pytorch/repos", "events_url": "https://api.github.com/users/pytorch/events{/privacy}", "received_events_url": "https://api.github.com/users/pytorch/received_events", "type": "Organization", "site_admin": false}, "html_url": "https://github.com/pytorch/pytorch", "description": "Tensors and Dynamic neural networks in Python  with strong GPU acceleration", "fork": false, "url": "https://api.github.com/repos/pytorch/pytorch", "forks_url": "https://api.github.com/repos/pytorch/pytorch/forks", "keys_url": "https://api.github.com/repos/pytorch/pytorch/keys{/key_id}", "collaborators_url": "https://api.github.com/repos/pytorch/pytorch/collaborators{/collaborator}", "teams_url": "https://api.github.com/repos/pytorch/pytorch/teams", "hooks_url": "https://api.github.com/repos/pytorch/pytorch/hooks", "issue_events_url": "https://api.github.com/repos/pytorch/pytorch/issues/events{/number}", "events_url": "https://api.github.com/repos/pytorch/pytorch/events", "assignees_url": "https://api.github.com/repos/pytorch/pytorch/assignees{/user}", "branches_url": "https://api.github.com/repos/pytorch/pytorch/branches{/branch}", "tags_url": "https://api.github.com/repos/pytorch/pytorch/tags", "blobs_url": "https://api.github.com/repos/pytorch/pytorch/git/blobs{/sha}", "git_tags_url": "https://api.github.com/repos/pytorch/pytorch/git/tags{/sha}", "git_refs_url": "https://api.github.com/repos/pytorch/pytorch/git/refs{/sha}", "trees_url": "https://api.github.com/repos/pytorch/pytorch/git/trees{/sha}", "statuses_url": "https://api.github.com/repos/pytorch/pytorch/statuses/{sha}", "languages_url": "https://api.github.com/repos/pytorch/pytorch/languages", "stargazers_url": "https://api.github.com/repos/pytorch/pytorch/stargazers", "contributors_url": "https://api.github.com/repos/pytorch/pytorch/contributors", "subscribers_url": "https://api.github.com/repos/pytorch/pytorch/subscribers", "subscription_url": "https://api.github.com/repos/pytorch/pytorch/subscription", "commits_url": "https://api.github.com/repos/pytorch/pytorch/commits{/sha}", "git_commits_url": "https://api.github.com/repos/pytorch/pytorch/git/commits{/sha}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/comments{/number}", "issue_comment_url": "https://api.github.com/repos/pytorch/pytorch/issues/comments{/number}", "contents_url": "https://api.github.com/repos/pytorch/pytorch/contents/{+path}", "compare_url": "https://api.github.com/repos/pytorch/pytorch/compare/{base}...{head}", "merges_url": "https://api.github.com/repos/pytorch/pytorch/merges", "archive_url": "https://api.github.com/repos/pytorch/pytorch/{archive_format}{/ref}", "downloads_url": "https://api.github.com/repos/pytorch/pytorch/downloads", "issues_url": "https://api.github.com/repos/pytorch/pytorch/issues{/number}", "pulls_url": "https://api.github.com/repos/pytorch/pytorch/pulls{/number}", "milestones_url": "https://api.github.com/repos/pytorch/pytorch/milestones{/number}", "notifications_url": "https://api.github.com/repos/pytorch/pytorch/notifications{?since,all,participating}", "labels_url": "https://api.github.com/repos/pytorch/pytorch/labels{/name}", "releases_url": "https://api.github.com/repos/pytorch/pytorch/releases{/id}", "deployments_url": "https://api.github.com/repos/pytorch/pytorch/deployments", "created_at": "2016-08-13T05:26:41Z", "updated_at": "2018-11-24T05:35:41Z", "pushed_at": "2018-11-24T05:34:07Z", "git_url": "git://github.com/pytorch/pytorch.git", "ssh_url": "git@github.com:pytorch/pytorch.git", "clone_url": "https://github.com/pytorch/pytorch.git", "svn_url": "https://github.com/pytorch/pytorch", "homepage": "http://pytorch.org", "size": 89651, "stargazers_count": 21577, "watchers_count": 21577, "language": "C++", "has_issues": true, "has_projects": true, "has_downloads": true, "has_wiki": true, "has_pages": false, "forks_count": 5149, "mirror_url": null, "archived": false, "open_issues_count": 2193, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "forks": 5149, "open_issues": 2193, "watchers": 21577, "default_branch": "master"}}, "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3666"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/3666"}, "issue": {"href": "https://api.github.com/repos/pytorch/pytorch/issues/3666"}, "comments": {"href": "https://api.github.com/repos/pytorch/pytorch/issues/3666/comments"}, "review_comments": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3666/comments"}, "review_comment": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments{/number}"}, "commits": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/3666/commits"}, "statuses": {"href": "https://api.github.com/repos/pytorch/pytorch/statuses/6c637bca42913c8377068a1bee874b2160c7f6c1"}}, "author_association": "CONTRIBUTOR", "body_html": "<pre><code>The executive summary is that this moves the torch/csrc/cudnn\nlibrary into ATen, adding a number of new cudnn_ methods to ATen\nfor batchnorm, convolution, affine grid generator and grid sampler.\n\nATen infra changes:\n\n- TensorGeometry was moved to ATen\n- TensorGeometry was modified to make its interface resemble that of\n  Tensor; in particular, sizes is no longer a field, it's a method.\n- AT_CUDA_ENABLED macro is set via ATen/Config.h header which is\n  generated at cmake configure time.\n  Fixes zdevito/ATen#168\n- Change AT_CUDA_ENABLED macro to be a function macro, so that we\n  error if it is not defined\n- Introduce a new TensorArg class, which is a Tensor plus a little\n  metadata.  This helps us give good error messages when checking\n  dimensions/shapes of tensors.\n  Fixes zdevito/ATen#169\n- Also introduce a TensorGeometryArg class, for when you don't\n  need the actual tensor data (which is most of the time.)\n- Add ATen/Check.h, which contains a number of utility functions\n  for testing shapes, types and devices of input tensors.  This\n  will be particulary useful for native methods, which don't get\n  code generated input testing code.  These functions take a\n  'CheckedFrom' argument, at the moment just a string, which\n  specifies some extra information about what function was\n  doing the actual checking; this greatly improves error messages.\n    - Many check functions take initializer lists, which let you\n      test that all tensors have some property.  This API is\n      peculiar, in that we IGNORE undefined tensors in this case.\n      This is handled by filterDefined.\n- Add AT_CUDNN_ENABLED macro\n- CuDNN linking from ATen was improved; for example, we now actually\n  add the CuDNN headers to our include path.\n- Add some missing override specifiers to some methods\n- We now actually build tests with CUDA functionality accessible\n  (previously, AT_CUDA_ENABLED was not defined, meaning that\n  the headers were missing all CUDA-only functionality.)\n- Native functions now support giving explicit names to return\n  outputs in yaml.  This makes it possible to hook into the NN\n  autogenerated derivatives codepath using native functions.\n\nCuDNN rewrite changes:\n\n- torch/csrc/cudnn now uses ATen (rather than passing around\n  THVoidTensor) and lives in ATen.  This lets us remove tensorPointer\n  shenanigans.  The functions are exposed to ATen as native functions\n  described in aten/src/ATen/cudnn/cuDNN.yaml\n- ATen now builds and links against CuDNN when enabled.  The cmake\n  package script was taken from Caffe2.\n- Some header reorganization was done to help reduce dependencies\n  on headers (this reorg is no longer used but I've kept it)\n- Rename CHECK to CUDNN_CHECK\n- Rip out old shape/type testing code in favor of modern ATen/Check.h\n  interface using TensorArg.  In many cases, increase the robustness of\n  the checking code.\n- Change the inputs of the public facing functions, so that they can\n  be bound by ATen\n  - Delete THCState*; this is retrieved from the global ATen context\n  - Delete cudnnHandle_t, this is retrieved from the global Handles.h\n  - Delete cudnnDataType_t, this is retrieved from the Tensor type\n  - Delete Convolution class, instead its constituent arguments are\n    passed individually\n- Change functions to return tensors, rather than take an appropriately\n  sized output tensor as an input.\n- Redo how transposed convolution / backward convolution is implemented\n  (knock on effect of returning tensors).  Previously it was assumed\n  that you would always pass an appropriately sized output tensor, but\n  we don't want to do this anymore.  For backwards, we instead give\n  the desired output tensor (input, really) size, because that is\n  readily available.  For *transposed* convolution, however, we take\n  output_padding, and otherwise do the shape calculation.\n- Redo how legacy group convolution is implemented (knock on effect from\n  porting cudnn to ATen.)  Previously, group convolution was implemented\n  by manually constructing sizes and strides and then outputting\n  appropriate, with macros switching between individual groups and\n  all-at-once based on CuDNN version.  Now, the code looks exactly what\n  you'd expect: there's a top-level wrapping function that supports\n  group convolution no matter the version of CuDNN, and a low-level\n  wrapper which supports only what CuDNN supports.  The top-level\n  function conditions on CuDNN version, and invokes the low-level\n  interface 1 or n times.\n- There is now a debugging printer for tensor descriptors.\n- Convolution struct is replaced with ConvolutionArgs, which is not\n  part of the public API but is used internally to conveniently\n  pass around all of the arguments needed for Convolution.\n- Add some constexprs for well-known dimensions, reduce amount of\n  magic numbers in code.\n- Put 'deterministic' in to ConvParams.  Fixes #3659\n- Lots more comments.\n- Some pessimizations, in the name of code clarity:\n  - The descriptors are initialized on every invocation of convolution\n    forward/backward.  Previously, the descriptors were cached, so that\n    you didn't have to initialize them again on backwards.  This is\n    difficult to support in the ATen interface so I didn't support it.\n  - Legacy group convolution initializes its workspace for *every* group\n    it performs.  I did not feel motivated to fix this because the\n    legacy codepath is already quite slow.\n- Affine grid generator and grid sampler automatically call contiguous\n  on their arguments as necessary.\n- Batchnorm input checking is greatly beefed up, it now checks for\n  the following input characteristics:\n    - Definedness\n    - GPU location\n    - Type\n    - Contiguity\n    - Size\n\nPyTorch binding code changes\n\n- batchnorm now uses consistent var/data naming\n- batchnorm and convolution make use of new ATen bindings\n- Affine grid generator and grid sampler make use of ATen CuDNN\n  bindings via derivatives.yaml.  This means I had to restructure\n  the code a little, since the THNN bindings still go through\n  a legacy Python class.\n- I fixed some warnings:\n  - s/friend class/friend struct/ on InterpreterStateImpl\n  - Removed pessimizing move 'detached' in torch/csrc/autograd/variable.cpp\n  - Removed unused pack_list on Scalar\n</code></pre>", "body_text": "The executive summary is that this moves the torch/csrc/cudnn\nlibrary into ATen, adding a number of new cudnn_ methods to ATen\nfor batchnorm, convolution, affine grid generator and grid sampler.\n\nATen infra changes:\n\n- TensorGeometry was moved to ATen\n- TensorGeometry was modified to make its interface resemble that of\n  Tensor; in particular, sizes is no longer a field, it's a method.\n- AT_CUDA_ENABLED macro is set via ATen/Config.h header which is\n  generated at cmake configure time.\n  Fixes zdevito/ATen#168\n- Change AT_CUDA_ENABLED macro to be a function macro, so that we\n  error if it is not defined\n- Introduce a new TensorArg class, which is a Tensor plus a little\n  metadata.  This helps us give good error messages when checking\n  dimensions/shapes of tensors.\n  Fixes zdevito/ATen#169\n- Also introduce a TensorGeometryArg class, for when you don't\n  need the actual tensor data (which is most of the time.)\n- Add ATen/Check.h, which contains a number of utility functions\n  for testing shapes, types and devices of input tensors.  This\n  will be particulary useful for native methods, which don't get\n  code generated input testing code.  These functions take a\n  'CheckedFrom' argument, at the moment just a string, which\n  specifies some extra information about what function was\n  doing the actual checking; this greatly improves error messages.\n    - Many check functions take initializer lists, which let you\n      test that all tensors have some property.  This API is\n      peculiar, in that we IGNORE undefined tensors in this case.\n      This is handled by filterDefined.\n- Add AT_CUDNN_ENABLED macro\n- CuDNN linking from ATen was improved; for example, we now actually\n  add the CuDNN headers to our include path.\n- Add some missing override specifiers to some methods\n- We now actually build tests with CUDA functionality accessible\n  (previously, AT_CUDA_ENABLED was not defined, meaning that\n  the headers were missing all CUDA-only functionality.)\n- Native functions now support giving explicit names to return\n  outputs in yaml.  This makes it possible to hook into the NN\n  autogenerated derivatives codepath using native functions.\n\nCuDNN rewrite changes:\n\n- torch/csrc/cudnn now uses ATen (rather than passing around\n  THVoidTensor) and lives in ATen.  This lets us remove tensorPointer\n  shenanigans.  The functions are exposed to ATen as native functions\n  described in aten/src/ATen/cudnn/cuDNN.yaml\n- ATen now builds and links against CuDNN when enabled.  The cmake\n  package script was taken from Caffe2.\n- Some header reorganization was done to help reduce dependencies\n  on headers (this reorg is no longer used but I've kept it)\n- Rename CHECK to CUDNN_CHECK\n- Rip out old shape/type testing code in favor of modern ATen/Check.h\n  interface using TensorArg.  In many cases, increase the robustness of\n  the checking code.\n- Change the inputs of the public facing functions, so that they can\n  be bound by ATen\n  - Delete THCState*; this is retrieved from the global ATen context\n  - Delete cudnnHandle_t, this is retrieved from the global Handles.h\n  - Delete cudnnDataType_t, this is retrieved from the Tensor type\n  - Delete Convolution class, instead its constituent arguments are\n    passed individually\n- Change functions to return tensors, rather than take an appropriately\n  sized output tensor as an input.\n- Redo how transposed convolution / backward convolution is implemented\n  (knock on effect of returning tensors).  Previously it was assumed\n  that you would always pass an appropriately sized output tensor, but\n  we don't want to do this anymore.  For backwards, we instead give\n  the desired output tensor (input, really) size, because that is\n  readily available.  For *transposed* convolution, however, we take\n  output_padding, and otherwise do the shape calculation.\n- Redo how legacy group convolution is implemented (knock on effect from\n  porting cudnn to ATen.)  Previously, group convolution was implemented\n  by manually constructing sizes and strides and then outputting\n  appropriate, with macros switching between individual groups and\n  all-at-once based on CuDNN version.  Now, the code looks exactly what\n  you'd expect: there's a top-level wrapping function that supports\n  group convolution no matter the version of CuDNN, and a low-level\n  wrapper which supports only what CuDNN supports.  The top-level\n  function conditions on CuDNN version, and invokes the low-level\n  interface 1 or n times.\n- There is now a debugging printer for tensor descriptors.\n- Convolution struct is replaced with ConvolutionArgs, which is not\n  part of the public API but is used internally to conveniently\n  pass around all of the arguments needed for Convolution.\n- Add some constexprs for well-known dimensions, reduce amount of\n  magic numbers in code.\n- Put 'deterministic' in to ConvParams.  Fixes #3659\n- Lots more comments.\n- Some pessimizations, in the name of code clarity:\n  - The descriptors are initialized on every invocation of convolution\n    forward/backward.  Previously, the descriptors were cached, so that\n    you didn't have to initialize them again on backwards.  This is\n    difficult to support in the ATen interface so I didn't support it.\n  - Legacy group convolution initializes its workspace for *every* group\n    it performs.  I did not feel motivated to fix this because the\n    legacy codepath is already quite slow.\n- Affine grid generator and grid sampler automatically call contiguous\n  on their arguments as necessary.\n- Batchnorm input checking is greatly beefed up, it now checks for\n  the following input characteristics:\n    - Definedness\n    - GPU location\n    - Type\n    - Contiguity\n    - Size\n\nPyTorch binding code changes\n\n- batchnorm now uses consistent var/data naming\n- batchnorm and convolution make use of new ATen bindings\n- Affine grid generator and grid sampler make use of ATen CuDNN\n  bindings via derivatives.yaml.  This means I had to restructure\n  the code a little, since the THNN bindings still go through\n  a legacy Python class.\n- I fixed some warnings:\n  - s/friend class/friend struct/ on InterpreterStateImpl\n  - Removed pessimizing move 'detached' in torch/csrc/autograd/variable.cpp\n  - Removed unused pack_list on Scalar", "merged": true, "mergeable": null, "rebaseable": null, "mergeable_state": "unknown", "merged_by": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "comments": 11, "review_comments": 78, "maintainer_can_modify": false, "commits": 2, "additions": 2671, "deletions": 2267, "changed_files": 73}