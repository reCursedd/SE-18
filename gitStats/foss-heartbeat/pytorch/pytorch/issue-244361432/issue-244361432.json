{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2163", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2163/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2163/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2163/events", "html_url": "https://github.com/pytorch/pytorch/issues/2163", "id": 244361432, "node_id": "MDU6SXNzdWUyNDQzNjE0MzI=", "number": 2163, "title": "Noticeable speed difference between conda install and PyPI version", "user": {"login": "alykhantejani", "id": 687194, "node_id": "MDQ6VXNlcjY4NzE5NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/687194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alykhantejani", "html_url": "https://github.com/alykhantejani", "followers_url": "https://api.github.com/users/alykhantejani/followers", "following_url": "https://api.github.com/users/alykhantejani/following{/other_user}", "gists_url": "https://api.github.com/users/alykhantejani/gists{/gist_id}", "starred_url": "https://api.github.com/users/alykhantejani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alykhantejani/subscriptions", "organizations_url": "https://api.github.com/users/alykhantejani/orgs", "repos_url": "https://api.github.com/users/alykhantejani/repos", "events_url": "https://api.github.com/users/alykhantejani/events{/privacy}", "received_events_url": "https://api.github.com/users/alykhantejani/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-07-20T13:24:45Z", "updated_at": "2017-08-18T08:20:00Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Found by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=437122\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mirandaconrado\">@mirandaconrado</a></p>\n<p>We wrote a simple benchmarking script and are comparing speed on OSX using the conda install of pytorch and the PyPI install.</p>\n<p>Here are the results:</p>\n<pre><code>**Install from PyPI wheel (using pip)**\n0.1.12_2\nvolatile = False \tbatchsize = 1\ttime = 2.668\t samples/sec = 3.748\nvolatile = True  \tbatchsize = 1\ttime = 2.125\t samples/sec = 4.707\nvolatile = False \tbatchsize = 2\ttime = 3.031\t samples/sec = 6.598\nvolatile = True  \tbatchsize = 2\ttime = 2.222\t samples/sec = 9.002\nvolatile = False \tbatchsize = 4\ttime = 2.714\t samples/sec = 14.736\nvolatile = True  \tbatchsize = 4\ttime = 2.305\t samples/sec = 17.356\nvolatile = False \tbatchsize = 8\ttime = 3.506\t samples/sec = 22.821\nvolatile = True  \tbatchsize = 8\ttime = 3.012\t samples/sec = 26.558\nvolatile = False \tbatchsize = 16\ttime = 4.008\t samples/sec = 39.916\nvolatile = True  \tbatchsize = 16\ttime = 3.616\t samples/sec = 44.243\nvolatile = False \tbatchsize = 32\ttime = 4.557\t samples/sec = 70.220\nvolatile = True  \tbatchsize = 32\ttime = 3.822\t samples/sec = 83.730\n\n**From Conda install**\n0.1.12_2\nvolatile = False \tbatchsize = 1\ttime = 2.234\t samples/sec = 4.476\nvolatile = True  \tbatchsize = 1\ttime = 1.711\t samples/sec = 5.843\nvolatile = False \tbatchsize = 2\ttime = 2.359\t samples/sec = 8.479\nvolatile = True  \tbatchsize = 2\ttime = 1.939\t samples/sec = 10.316\nvolatile = False \tbatchsize = 4\ttime = 2.443\t samples/sec = 16.371\nvolatile = True  \tbatchsize = 4\ttime = 2.017\t samples/sec = 19.831\nvolatile = False \tbatchsize = 8\ttime = 2.444\t samples/sec = 32.730\nvolatile = True  \tbatchsize = 8\ttime = 2.172\t samples/sec = 36.828\nvolatile = False \tbatchsize = 16\ttime = 2.773\t samples/sec = 57.708\nvolatile = True  \tbatchsize = 16\ttime = 2.351\t samples/sec = 68.052\nvolatile = False \tbatchsize = 32\ttime = 3.424\t samples/sec = 93.453\nvolatile = True  \tbatchsize = 32\ttime = 2.996\t samples/sec = 106.800\n</code></pre>\n<p>Here's the script I'm using to generate the results:<br>\n<code>speed_comparisson_test.sh</code></p>\n<div class=\"highlight highlight-source-shell\"><pre>conda create --name pytorch_speed_from_pypi -y python=2.7.13 numpy pyyaml\n<span class=\"pl-c1\">source</span> activate pytorch_speed_from_pypi\nwget http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl\npip uninstall -y torch\npip install torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl --user\npython test.py\npip uninstall -y torch\n<span class=\"pl-c1\">source</span> deactivate\nconda-env remove --name pytorch_speed_from_pypi -y\nrm torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl\n\nconda create --name pytorch_speed_conda_only -y python=2.7.13 numpy pyyaml\n<span class=\"pl-c1\">source</span> activate pytorch_speed_conda_only\nconda install pytorch -y -c soumith\npython test.py\n<span class=\"pl-c1\">source</span> deactivate\nconda-env remove -y --name pytorch_speed_conda_only</pre></div>\n<p>And <a href=\"https://pastebin.com/f8kzP3Ff\" rel=\"nofollow\">here's a dump</a> of <code>test.py</code>.</p>\n<p>It's also worth noting that when <code>speed_comparisson_test.sh</code> is run with <code>OMP_NUM_THREADS=1</code> I don't see a significant difference in speed from the conda install vs the wheel.</p>", "body_text": "Found by @mirandaconrado\nWe wrote a simple benchmarking script and are comparing speed on OSX using the conda install of pytorch and the PyPI install.\nHere are the results:\n**Install from PyPI wheel (using pip)**\n0.1.12_2\nvolatile = False \tbatchsize = 1\ttime = 2.668\t samples/sec = 3.748\nvolatile = True  \tbatchsize = 1\ttime = 2.125\t samples/sec = 4.707\nvolatile = False \tbatchsize = 2\ttime = 3.031\t samples/sec = 6.598\nvolatile = True  \tbatchsize = 2\ttime = 2.222\t samples/sec = 9.002\nvolatile = False \tbatchsize = 4\ttime = 2.714\t samples/sec = 14.736\nvolatile = True  \tbatchsize = 4\ttime = 2.305\t samples/sec = 17.356\nvolatile = False \tbatchsize = 8\ttime = 3.506\t samples/sec = 22.821\nvolatile = True  \tbatchsize = 8\ttime = 3.012\t samples/sec = 26.558\nvolatile = False \tbatchsize = 16\ttime = 4.008\t samples/sec = 39.916\nvolatile = True  \tbatchsize = 16\ttime = 3.616\t samples/sec = 44.243\nvolatile = False \tbatchsize = 32\ttime = 4.557\t samples/sec = 70.220\nvolatile = True  \tbatchsize = 32\ttime = 3.822\t samples/sec = 83.730\n\n**From Conda install**\n0.1.12_2\nvolatile = False \tbatchsize = 1\ttime = 2.234\t samples/sec = 4.476\nvolatile = True  \tbatchsize = 1\ttime = 1.711\t samples/sec = 5.843\nvolatile = False \tbatchsize = 2\ttime = 2.359\t samples/sec = 8.479\nvolatile = True  \tbatchsize = 2\ttime = 1.939\t samples/sec = 10.316\nvolatile = False \tbatchsize = 4\ttime = 2.443\t samples/sec = 16.371\nvolatile = True  \tbatchsize = 4\ttime = 2.017\t samples/sec = 19.831\nvolatile = False \tbatchsize = 8\ttime = 2.444\t samples/sec = 32.730\nvolatile = True  \tbatchsize = 8\ttime = 2.172\t samples/sec = 36.828\nvolatile = False \tbatchsize = 16\ttime = 2.773\t samples/sec = 57.708\nvolatile = True  \tbatchsize = 16\ttime = 2.351\t samples/sec = 68.052\nvolatile = False \tbatchsize = 32\ttime = 3.424\t samples/sec = 93.453\nvolatile = True  \tbatchsize = 32\ttime = 2.996\t samples/sec = 106.800\n\nHere's the script I'm using to generate the results:\nspeed_comparisson_test.sh\nconda create --name pytorch_speed_from_pypi -y python=2.7.13 numpy pyyaml\nsource activate pytorch_speed_from_pypi\nwget http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl\npip uninstall -y torch\npip install torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl --user\npython test.py\npip uninstall -y torch\nsource deactivate\nconda-env remove --name pytorch_speed_from_pypi -y\nrm torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl\n\nconda create --name pytorch_speed_conda_only -y python=2.7.13 numpy pyyaml\nsource activate pytorch_speed_conda_only\nconda install pytorch -y -c soumith\npython test.py\nsource deactivate\nconda-env remove -y --name pytorch_speed_conda_only\nAnd here's a dump of test.py.\nIt's also worth noting that when speed_comparisson_test.sh is run with OMP_NUM_THREADS=1 I don't see a significant difference in speed from the conda install vs the wheel.", "body": "Found by @mirandaconrado\r\n\r\nWe wrote a simple benchmarking script and are comparing speed on OSX using the conda install of pytorch and the PyPI install. \r\n\r\nHere are the results:\r\n```\r\n**Install from PyPI wheel (using pip)**\r\n0.1.12_2\r\nvolatile = False \tbatchsize = 1\ttime = 2.668\t samples/sec = 3.748\r\nvolatile = True  \tbatchsize = 1\ttime = 2.125\t samples/sec = 4.707\r\nvolatile = False \tbatchsize = 2\ttime = 3.031\t samples/sec = 6.598\r\nvolatile = True  \tbatchsize = 2\ttime = 2.222\t samples/sec = 9.002\r\nvolatile = False \tbatchsize = 4\ttime = 2.714\t samples/sec = 14.736\r\nvolatile = True  \tbatchsize = 4\ttime = 2.305\t samples/sec = 17.356\r\nvolatile = False \tbatchsize = 8\ttime = 3.506\t samples/sec = 22.821\r\nvolatile = True  \tbatchsize = 8\ttime = 3.012\t samples/sec = 26.558\r\nvolatile = False \tbatchsize = 16\ttime = 4.008\t samples/sec = 39.916\r\nvolatile = True  \tbatchsize = 16\ttime = 3.616\t samples/sec = 44.243\r\nvolatile = False \tbatchsize = 32\ttime = 4.557\t samples/sec = 70.220\r\nvolatile = True  \tbatchsize = 32\ttime = 3.822\t samples/sec = 83.730\r\n\r\n**From Conda install**\r\n0.1.12_2\r\nvolatile = False \tbatchsize = 1\ttime = 2.234\t samples/sec = 4.476\r\nvolatile = True  \tbatchsize = 1\ttime = 1.711\t samples/sec = 5.843\r\nvolatile = False \tbatchsize = 2\ttime = 2.359\t samples/sec = 8.479\r\nvolatile = True  \tbatchsize = 2\ttime = 1.939\t samples/sec = 10.316\r\nvolatile = False \tbatchsize = 4\ttime = 2.443\t samples/sec = 16.371\r\nvolatile = True  \tbatchsize = 4\ttime = 2.017\t samples/sec = 19.831\r\nvolatile = False \tbatchsize = 8\ttime = 2.444\t samples/sec = 32.730\r\nvolatile = True  \tbatchsize = 8\ttime = 2.172\t samples/sec = 36.828\r\nvolatile = False \tbatchsize = 16\ttime = 2.773\t samples/sec = 57.708\r\nvolatile = True  \tbatchsize = 16\ttime = 2.351\t samples/sec = 68.052\r\nvolatile = False \tbatchsize = 32\ttime = 3.424\t samples/sec = 93.453\r\nvolatile = True  \tbatchsize = 32\ttime = 2.996\t samples/sec = 106.800\r\n```\r\n\r\nHere's the script I'm using to generate the results:\r\n`speed_comparisson_test.sh`\r\n```bash\r\nconda create --name pytorch_speed_from_pypi -y python=2.7.13 numpy pyyaml\r\nsource activate pytorch_speed_from_pypi\r\nwget http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl\r\npip uninstall -y torch\r\npip install torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl --user\r\npython test.py\r\npip uninstall -y torch\r\nsource deactivate\r\nconda-env remove --name pytorch_speed_from_pypi -y\r\nrm torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl\r\n\r\nconda create --name pytorch_speed_conda_only -y python=2.7.13 numpy pyyaml\r\nsource activate pytorch_speed_conda_only\r\nconda install pytorch -y -c soumith\r\npython test.py\r\nsource deactivate\r\nconda-env remove -y --name pytorch_speed_conda_only\r\n```\r\n\r\nAnd [here's a dump](https://pastebin.com/f8kzP3Ff) of `test.py`.\r\n\r\nIt's also worth noting that when `speed_comparisson_test.sh` is run with `OMP_NUM_THREADS=1` I don't see a significant difference in speed from the conda install vs the wheel.\r\n"}