{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/333881056", "html_url": "https://github.com/pytorch/pytorch/issues/2769#issuecomment-333881056", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2769", "id": 333881056, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzg4MTA1Ng==", "user": {"login": "qijiezhao", "id": 20857077, "node_id": "MDQ6VXNlcjIwODU3MDc3", "avatar_url": "https://avatars1.githubusercontent.com/u/20857077?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qijiezhao", "html_url": "https://github.com/qijiezhao", "followers_url": "https://api.github.com/users/qijiezhao/followers", "following_url": "https://api.github.com/users/qijiezhao/following{/other_user}", "gists_url": "https://api.github.com/users/qijiezhao/gists{/gist_id}", "starred_url": "https://api.github.com/users/qijiezhao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qijiezhao/subscriptions", "organizations_url": "https://api.github.com/users/qijiezhao/orgs", "repos_url": "https://api.github.com/users/qijiezhao/repos", "events_url": "https://api.github.com/users/qijiezhao/events{/privacy}", "received_events_url": "https://api.github.com/users/qijiezhao/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-03T15:34:49Z", "updated_at": "2017-10-03T15:35:05Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=25442895\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aa1607\">@aa1607</a> thx for your answer!<br>\nI solved this problem not long after I propose this issue. As you mentioned, the hidden state is updated following the chained RNN, So the solution is to change another variable to replace the hidden state.<br>\nThe code is as follows:</p>\n<pre><code>def init_hidden(self,x=None):\n    if x==None:\n        return (Variable(torch.zeros(self.n_layers, training_batch_size, self.hidden_len)),\n            Variable(torch.zeros(self.n_layers, training_batch_size, self.hidden_len)))\n    else:\n        return (Variable(x[0].data),Variable(x[1].data))\n\ndef forward(self,x):\n    lstm_out,self.hidden_out=self.lstm(\n        x,self.hidden\n    )\n    cls_scores=self.hidden2class(self.dropout(lstm_out[:,-1,:])).view(-1,self.target_size)\n    if self.is_softmax:\n        cls_scores=self.log_softmax(cls_scores)\n    self.hidden=self.init_hidden(self.hidden_out)\n    return cls_scores\n</code></pre>", "body_text": "@aa1607 thx for your answer!\nI solved this problem not long after I propose this issue. As you mentioned, the hidden state is updated following the chained RNN, So the solution is to change another variable to replace the hidden state.\nThe code is as follows:\ndef init_hidden(self,x=None):\n    if x==None:\n        return (Variable(torch.zeros(self.n_layers, training_batch_size, self.hidden_len)),\n            Variable(torch.zeros(self.n_layers, training_batch_size, self.hidden_len)))\n    else:\n        return (Variable(x[0].data),Variable(x[1].data))\n\ndef forward(self,x):\n    lstm_out,self.hidden_out=self.lstm(\n        x,self.hidden\n    )\n    cls_scores=self.hidden2class(self.dropout(lstm_out[:,-1,:])).view(-1,self.target_size)\n    if self.is_softmax:\n        cls_scores=self.log_softmax(cls_scores)\n    self.hidden=self.init_hidden(self.hidden_out)\n    return cls_scores", "body": "@aa1607 thx for your answer! \r\nI solved this problem not long after I propose this issue. As you mentioned, the hidden state is updated following the chained RNN, So the solution is to change another variable to replace the hidden state.\r\nThe code is as follows:\r\n    \r\n    def init_hidden(self,x=None):\r\n        if x==None:\r\n            return (Variable(torch.zeros(self.n_layers, training_batch_size, self.hidden_len)),\r\n                Variable(torch.zeros(self.n_layers, training_batch_size, self.hidden_len)))\r\n        else:\r\n            return (Variable(x[0].data),Variable(x[1].data))\r\n\r\n    def forward(self,x):\r\n        lstm_out,self.hidden_out=self.lstm(\r\n            x,self.hidden\r\n        )\r\n        cls_scores=self.hidden2class(self.dropout(lstm_out[:,-1,:])).view(-1,self.target_size)\r\n        if self.is_softmax:\r\n            cls_scores=self.log_softmax(cls_scores)\r\n        self.hidden=self.init_hidden(self.hidden_out)\r\n        return cls_scores\r\n\r\n"}