{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2769", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2769/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2769/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2769/events", "html_url": "https://github.com/pytorch/pytorch/issues/2769", "id": 258374515, "node_id": "MDU6SXNzdWUyNTgzNzQ1MTU=", "number": 2769, "title": "LSTM architecture has met a explosive growth in the training process", "user": {"login": "qijiezhao", "id": 20857077, "node_id": "MDQ6VXNlcjIwODU3MDc3", "avatar_url": "https://avatars1.githubusercontent.com/u/20857077?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qijiezhao", "html_url": "https://github.com/qijiezhao", "followers_url": "https://api.github.com/users/qijiezhao/followers", "following_url": "https://api.github.com/users/qijiezhao/following{/other_user}", "gists_url": "https://api.github.com/users/qijiezhao/gists{/gist_id}", "starred_url": "https://api.github.com/users/qijiezhao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qijiezhao/subscriptions", "organizations_url": "https://api.github.com/users/qijiezhao/orgs", "repos_url": "https://api.github.com/users/qijiezhao/repos", "events_url": "https://api.github.com/users/qijiezhao/events{/privacy}", "received_events_url": "https://api.github.com/users/qijiezhao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-18T05:40:23Z", "updated_at": "2018-04-03T17:44:03Z", "closed_at": "2017-10-03T09:01:38Z", "author_association": "NONE", "body_html": "<p>Hi,  everyone:</p>\n<p>I build a model based on LSTM as follows:</p>\n<pre><code>class LSTM(nn.Module):\n    def __init__(self,\n                 input_len=1024,\n                 hidden_len=1024,\n                 model_len=12,\n                 target_size=101,\n                 is_softmax=True):\n        super(LSTM,self).__init__()\n        self.hidden_len=hidden_len\n        self.is_softmax=is_softmax\n        self.lstm=nn.LSTM(input_len,hidden_len,batch_first=True)\n        self.target_size=target_size\n        self.hidden2class=nn.Linear(hidden_len,target_size)\n        self.hidden=self.init_hidden()\n    def init_hidden(self):\n        return (Variable(torch.zeros(1, 1, self.hidden_len)),\n            Variable(torch.zeros(1, 1, self.hidden_len)))\n    def forward(self,x):\n        lstm_out,self.hidden=self.lstm(\n            x,self.hidden\n        )\n        cls_scores=self.hidden2class(lstm_out[:,-1,:]).view(-1,self.target_size)\n        if self.is_softmax:\n            cls_scores=F.log_softmax(cls_scores)\n        return cls_scores\n</code></pre>\n<p>and the training process is as follows:</p>\n<pre><code>def train(train_data,model,criterion,optimizer,epoch):\n    model.train()\n    for i,(input,target) in enumerate(train_data):\n\n        if use_cuda:\n            target=target.cuda(async=True)\n        input_var=torch.autograd.Variable(input)\n        target_var=torch.autograd.Variable(target)\n\n        output=model(input_var)\n        loss=criterion(output,target_var)\n\n        prec1,prec5=accuracy(output.data,target,topk=(1,5))\n        losses.update(loss.data[0],input.size(0))\n\n        optimizer.zero_grad()\n        loss.backward(retain_graph=True)\n        optimizer.step()\n</code></pre>\n<p>`</p>\n<p>When not add the parameter \"retain_graph\" in the loss.backward function, I received an error message:</p>\n<pre><code>Traceback (most recent call last):\n  File \"temporal_pool.py\", line 328, in &lt;module&gt;\n    main()\n  File \"temporal_pool.py\", line 174, in main\n    train(train_data,model,loss_function,optimizer,epoch)\n  File \"temporal_pool.py\", line 216, in train\n    loss.backward()\n  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/function.py\", line 91, in apply\n    return self._forward_cls.backward(self, *args)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py\", line 52, in backward\n    a, b = ctx.saved_variables\nRuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n</code></pre>\n<p>But, when added the parameter,  My training process has met a trouble that, the time consumed each iteration endlessly increasing!  The snapshot is like below:</p>\n<pre><code>2:36,671 - INFO - Epoch: [0][84/190], lr: 0.01000\tTime 7.042 (3.650)\tData 0.003 (0.004)\tLoss 1.0369 (2.5610)\tPrec@1 90.000 (59.741)\tPrec@5 94.000 (77)\n2017-09-18 13:22:43,930 - INFO - Epoch: [0][85/190], lr: 0.01000\tTime 7.259 (3.692)\tData 0.003 (0.004)\tLoss 0.6710 (2.5391)\tPrec@1 96.000 (60.163)\tPrec@5 100.000 (7)\n2017-09-18 13:22:51,523 - INFO - Epoch: [0][86/190], lr: 0.01000\tTime 7.593 (3.737)\tData 0.003 (0.004)\tLoss 0.8195 (2.5193)\tPrec@1 98.000 (60.598)\tPrec@5 100.000 (7)\n2017-09-18 13:22:58,701 - INFO - Epoch: [0][87/190], lr: 0.01000\tTime 7.179 (3.776)\tData 0.002 (0.004)\tLoss 0.7179 (2.4988)\tPrec@1 94.000 (60.977)\tPrec@5 100.000 (7)\n2017-09-18 13:23:06,277 - INFO - Epoch: [0][88/190], lr: 0.01000\tTime 7.576 (3.818)\tData 0.002 (0.004)\tLoss 0.9274 (2.4812)\tPrec@1 80.000 (61.191)\tPrec@5 98.000 (78)\n2017-09-18 13:23:14,097 - INFO - Epoch: [0][89/190], lr: 0.01000\tTime 7.820 (3.863)\tData 0.003 (0.004)\tLoss 0.7612 (2.4621)\tPrec@1 96.000 (61.578)\tPrec@5 100.000 (7)\n2017-09-18 13:23:21,841 - INFO - Epoch: [0][90/190], lr: 0.01000\tTime 7.744 (3.905)\tData 0.002 (0.004)\tLoss 0.6913 (2.4426)\tPrec@1 98.000 (61.978)\tPrec@5 100.000 (7)\n2017-09-18 13:23:29,396 - INFO - Epoch: [0][91/190], lr: 0.01000\tTime 7.555 (3.945)\tData 0.004 (0.004)\tLoss 0.8286 (2.4251)\tPrec@1 92.000 (62.304)\tPrec@5 94.000 (78)\n2017-09-18 13:23:36,920 - INFO - Epoch: [0][92/190], lr: 0.01000\tTime 7.524 (3.984)\tData 0.002 (0.004)\tLoss 0.8006 (2.4076)\tPrec@1 90.000 (62.602)\tPrec@5 98.000 (79)\n2017-09-18 13:23:44,831 - INFO - Epoch: [0][93/190], lr: 0.01000\tTime 7.911 (4.025)\tData 0.002 (0.004)\tLoss 0.7698 (2.3902)\tPrec@1 96.000 (62.957)\tPrec@5 100.000 (7)\n2017-09-18 13:23:53,035 - INFO - Epoch: [0][94/190], lr: 0.01000\tTime 8.204 (4.069)\tData 0.003 (0.004)\tLoss 0.8519 (2.3740)\tPrec@1 92.000 (63.263)\tPrec@5 98.000 (79)\n2017-09-18 13:24:00,845 - INFO - Epoch: [0][95/190], lr: 0.01000\tTime 7.810 (4.108)\tData 0.002 (0.004)\tLoss 0.7559 (2.3571)\tPrec@1 90.000 (63.542)\tPrec@5 98.000 (79)\n2017-09-18 13:24:08,728 - INFO - Epoch: [0][96/190], lr: 0.01000\tTime 7.883 (4.147)\tData 0.002 (0.004)\tLoss 0.7015 (2.3400)\tPrec@1 92.000 (63.835)\tPrec@5 100.000 (7)\n2017-09-18 13:24:17,098 - INFO - Epoch: [0][97/190], lr: 0.01000\tTime 8.370 (4.190)\tData 0.002 (0.004)\tLoss 0.8148 (2.3245)\tPrec@1 84.000 (64.041)\tPrec@5 98.000 (80)\n2017-09-18 13:24:25,550 - INFO - Epoch: [0][98/190], lr: 0.01000\tTime 8.452 (4.233)\tData 0.002 (0.004)\tLoss 0.7163 (2.3082)\tPrec@1 86.000 (64.263)\tPrec@5 98.000 (80)\n2017-09-18 13:24:33,836 - INFO - Epoch: [0][99/190], lr: 0.01000\tTime 8.286 (4.274)\tData 0.002 (0.004)\tLoss 0.6683 (2.2918)\tPrec@1 86.000 (64.480)\tPrec@5 98.000 (80)\n2017-09-18 13:24:42,290 - INFO - Epoch: [0][100/190], lr: 0.01000\tTime 8.454 (4.315)\tData 0.002 (0.004)\tLoss 0.6422 (2.2755)\tPrec@1 94.000 (64.772)\tPrec@5 100.000 (8)\n2017-09-18 13:24:50,672 - INFO - Epoch: [0][101/190], lr: 0.01000\tTime 8.382 (4.355)\tData 0.002 (0.004)\tLoss 0.7038 (2.2601)\tPrec@1 90.000 (65.020)\tPrec@5 100.000 (8)\n2017-09-18 13:25:00,062 - INFO - Epoch: [0][102/190], lr: 0.01000\tTime 9.389 (4.404)\tData 0.002 (0.004)\tLoss 0.6784 (2.2447)\tPrec@1 88.000 (65.243)\tPrec@5 98.000 (80)\n2017-09-18 13:25:09,428 - INFO - Epoch: [0][103/190], lr: 0.01000\tTime 9.366 (4.452)\tData 0.002 (0.004)\tLoss 0.7145 (2.2300)\tPrec@1 90.000 (65.481)\tPrec@5 98.000 (81)\n2017-09-18 13:25:18,487 - INFO - Epoch: [0][104/190], lr: 0.01000\tTime 9.059 (4.496)\tData 0.004 (0.004)\tLoss 0.6637 (2.2151)\tPrec@1 90.000 (65.714)\tPrec@5 98.000 (81)\n2017-09-18 13:25:27,151 - INFO - Epoch: [0][105/190], lr: 0.01000\tTime 8.664 (4.535)\tData 0.002 (0.004)\tLoss 0.6512 (2.2004)\tPrec@1 92.000 (65.962)\tPrec@5 100.000 (8)\n2017-09-18 13:25:36,368 - INFO - Epoch: [0][106/190], lr: 0.01000\tTime 9.217 (4.579)\tData 0.002 (0.004)\tLoss 0.6434 (2.1858)\tPrec@1 92.000 (66.206)\tPrec@5 100.000 (8)\n2017-09-18 13:25:45,407 - INFO - Epoch: [0][107/190], lr: 0.01000\tTime 9.039 (4.620)\tData 0.002 (0.004)\tLoss 0.6722 (2.1718)\tPrec@1 82.000 (66.352)\tPrec@5 100.000 (8)\n2017-09-18 13:25:55,124 - INFO - Epoch: [0][108/190], lr: 0.01000\tTime 9.717 (4.667)\tData 0.002 (0.004)\tLoss 0.5507 (2.1569)\tPrec@1 94.000 (66.606)\tPrec@5 100.000 (8)\n2017-09-18 13:26:04,149 - INFO - Epoch: [0][109/190], lr: 0.01000\tTime 9.024 (4.706)\tData 0.002 (0.004)\tLoss 0.5493 (2.1423)\tPrec@1 96.000 (66.873)\tPrec@5 98.000 (82)\n2017-09-18 13:26:13,310 - INFO - Epoch: [0][110/190], lr: 0.01000\tTime 9.161 (4.747)\tData 0.002 (0.004)\tLoss 0.6473 (2.1288)\tPrec@1 90.000 (67.081)\tPrec@5 100.000 (8)\n2017-09-18 13:26:22,946 - INFO - Epoch: [0][111/190], lr: 0.01000\tTime 9.636 (4.790)\tData 0.002 (0.004)\tLoss 0.5715 (2.1149)\tPrec@1 94.000 (67.321)\tPrec@5 100.000 (8)\n2017-09-18 13:26:33,026 - INFO - Epoch: [0][112/190], lr: 0.01000\tTime 10.081 (4.837)\tData 0.002 (0.004)\tLoss 0.5555 (2.1011)\tPrec@1 92.000 (67.540)\tPrec@5 100.000 (8)\n2017-09-18 13:26:42,249 - INFO - Epoch: [0][113/190], lr: 0.01000\tTime 9.223 (4.876)\tData 0.003 (0.004)\tLoss 0.4574 (2.0867)\tPrec@1 98.000 (67.807)\tPrec@5 100.000 (8)\n2017-09-18 13:26:51,917 - INFO - Epoch: [0][114/190], lr: 0.01000\tTime 9.668 (4.917)\tData 0.002 (0.004)\tLoss 0.7171 (2.0748)\tPrec@1 88.000 (67.983)\tPrec@5 100.000 (8)\n2017-09-18 13:27:01,647 - INFO - Epoch: [0][115/190], lr: 0.01000\tTime 9.730 (4.959)\tData 0.002 (0.004)\tLoss 0.4380 (2.0607)\tPrec@1 98.000 (68.241)\tPrec@5 100.000 (8)\n2017-09-18 13:27:11,674 - INFO - Epoch: [0][116/190], lr: 0.01000\tTime 10.027 (5.002)\tData 0.002 (0.004)\tLoss 0.5357 (2.0477)\tPrec@1 96.000 (68.479)\tPrec@5 98.000 (83)\n2017-09-18 13:27:21,527 - INFO - Epoch: [0][117/190], lr: 0.01000\tTime 9.853 (5.043)\tData 0.003 (0.004)\tLoss 0.5669 (2.0351)\tPrec@1 94.000 (68.695)\tPrec@5 98.000 (83)\n2017-09-18 13:27:31,056 - INFO - Epoch: [0][118/190], lr: 0.01000\tTime 9.529 (5.081)\tData 0.002 (0.004)\tLoss 0.6435 (2.0234)\tPrec@1 92.000 (68.891)\tPrec@5 94.000 (83)\n2017-09-18 13:27:41,082 - INFO - Epoch: [0][119/190], lr: 0.01000\tTime 10.026 (5.122)\tData 0.002 (0.004)\tLoss 0.5540 (2.0112)\tPrec@1 88.000 (69.050)\tPrec@5 100.000 (8)\n2017-09-18 13:27:50,734 - INFO - Epoch: [0][120/190], lr: 0.01000\tTime 9.653 (5.159)\tData 0.002 (0.004)\tLoss 0.5452 (1.9991)\tPrec@1 92.000 (69.240)\tPrec@5 100.000 (8)\n2017-09-18 13:28:00,682 - INFO - Epoch: [0][121/190], lr: 0.01000\tTime 9.948 (5.199)\tData 0.002 (0.004)\tLoss 0.8453 (1.9896)\tPrec@1 82.000 (69.344)\tPrec@5 96.000 (83)\n2017-09-18 13:28:10,677 - INFO - Epoch: [0][122/190], lr: 0.01000\tTime 9.995 (5.238)\tData 0.002 (0.004)\tLoss 0.6531 (1.9787)\tPrec@1 92.000 (69.528)\tPrec@5 100.000 (8)\n2017-09-18 13:28:21,240 - INFO - Epoch: [0][123/190], lr: 0.01000\tTime 10.562 (5.281)\tData 0.002 (0.004)\tLoss 0.5329 (1.9671)\tPrec@1 88.000 (69.677)\tPrec@5 100.000 (8)\n2017-09-18 13:28:31,558 - INFO - Epoch: [0][124/190], lr: 0.01000\tTime 10.318 (5.321)\tData 0.002 (0.004)\tLoss 0.6132 (1.9562)\tPrec@1 90.000 (69.840)\tPrec@5 100.000 (8)\n2017-09-18 13:28:42,142 - INFO - Epoch: [0][125/190], lr: 0.01000\tTime 10.585 (5.363)\tData 0.002 (0.004)\tLoss 0.4761 (1.9445)\tPrec@1 96.000 (70.048)\tPrec@5 100.000 (8)\n2017-09-18 13:28:52,802 - INFO - Epoch: [0][126/190], lr: 0.01000\tTime 10.660 (5.404)\tData 0.002 (0.004)\tLoss 0.4652 (1.9328)\tPrec@1 96.000 (70.252)\tPrec@5 100.000 (8)\n2017-09-18 13:29:03,661 - INFO - Epoch: [0][127/190], lr: 0.01000\tTime 10.859 (5.447)\tData 0.003 (0.004)\tLoss 0.4837 (1.9215)\tPrec@1 98.000 (70.469)\tPrec@5 100.000 (8)\n2017-09-18 13:29:14,862 - INFO - Epoch: [0][128/190], lr: 0.01000\tTime 11.201 (5.492)\tData 0.003 (0.004)\tLoss 0.4328 (1.9100)\tPrec@1 96.000 (70.667)\tPrec@5 100.000 (8)\n2017-09-18 13:29:25,954 - INFO - Epoch: [0][129/190], lr: 0.01000\tTime 11.092 (5.535)\tData 0.002 (0.004)\tLoss 0.6569 (1.9003)\tPrec@1 90.000 (70.815)\tPrec@5 96.000 (84)\n2017-09-18 13:29:36,764 - INFO - Epoch: [0][130/190], lr: 0.01000\tTime 10.810 (5.575)\tData 0.004 (0.004)\tLoss 0.4948 (1.8896)\tPrec@1 94.000 (70.992)\tPrec@5 100.000 (8)\n2017-09-18 13:29:47,688 - INFO - Epoch: [0][131/190], lr: 0.01000\tTime 10.923 (5.615)\tData 0.002 (0.004)\tLoss 0.4707 (1.8789)\tPrec@1 94.000 (71.167)\tPrec@5 100.000 (8)\n2017-09-18 13:29:58,851 - INFO - Epoch: [0][132/190], lr: 0.01000\tTime 11.164 (5.657)\tData 0.006 (0.004)\tLoss 0.5153 (1.8686)\tPrec@1 92.000 (71.323)\tPrec@5 100.000 (8)\n2017-09-18 13:30:10,097 - INFO - Epoch: [0][133/190], lr: 0.01000\tTime 11.246 (5.699)\tData 0.002 (0.004)\tLoss 0.4860 (1.8583)\tPrec@1 94.000 (71.493)\tPrec@5 98.000 (85)\n2017-09-18 13:30:21,123 - INFO - Epoch: [0][134/190], lr: 0.01000\tTime 11.026 (5.738)\tData 0.002 (0.004)\tLoss 0.4441 (1.8478)\tPrec@1 98.000 (71.689)\tPrec@5 100.000 (8)\n2017-09-18 13:30:32,364 - INFO - Epoch: [0][135/190], lr: 0.01000\tTime 11.242 (5.779)\tData 0.002 (0.004)\tLoss 0.6027 (1.8387)\tPrec@1 94.000 (71.853)\tPrec@5 98.000 (85)\n2017-09-18 13:30:43,511 - INFO - Epoch: [0][136/190], lr: 0.01000\tTime 11.147 (5.818)\tData 0.002 (0.004)\tLoss 0.4439 (1.8285)\tPrec@1 94.000 (72.015)\tPrec@5 100.000 (8)\n2017-09-18 13:30:56,065 - INFO - Epoch: [0][137/190], lr: 0.01000\tTime 12.553 (5.867)\tData 0.002 (0.004)\tLoss 0.4715 (1.8187)\tPrec@1 92.000 (72.159)\tPrec@5 100.000 (8)\n2017-09-18 13:31:07,832 - INFO - Epoch: [0][138/190], lr: 0.01000\tTime 11.768 (5.909)\tData 0.003 (0.004)\tLoss 0.4727 (1.8090)\tPrec@1 96.000 (72.331)\tPrec@5 100.000 (8)\n2017-09-18 13:31:19,450 - INFO - Epoch: [0][139/190], lr: 0.01000\tTime 11.618 (5.950)\tData 0.002 (0.004)\tLoss 0.4232 (1.7991)\tPrec@1 96.000 (72.500)\tPrec@5 100.000 (8)\n2017-09-18 13:31:31,482 - INFO - Epoch: [0][140/190], lr: 0.01000\tTime 12.032 (5.993)\tData 0.002 (0.004)\tLoss 0.5206 (1.7900)\tPrec@1 92.000 (72.638)\tPrec@5 98.000 (85)\n2017-09-18 13:31:42,916 - INFO - Epoch: [0][141/190], lr: 0.01000\tTime 11.434 (6.032)\tData 0.002 (0.003)\tLoss 0.3838 (1.7801)\tPrec@1 98.000 (72.817)\tPrec@5 100.000 (8)\n2017-09-18 13:31:54,853 - INFO - Epoch: [0][142/190], lr: 0.01000\tTime 11.936 (6.073)\tData 0.002 (0.003)\tLoss 0.6089 (1.7719)\tPrec@1 90.000 (72.937)\tPrec@5 98.000 (86)\n2017-09-18 13:32:06,966 - INFO - Epoch: [0][143/190], lr: 0.01000\tTime 12.113 (6.115)\tData 0.002 (0.003)\tLoss 0.4373 (1.7626)\tPrec@1 96.000 (73.097)\tPrec@5 100.000 (8)\n2017-09-18 13:32:20,037 - INFO - Epoch: [0][144/190], lr: 0.01000\tTime 13.070 (6.163)\tData 0.002 (0.003)\tLoss 0.5992 (1.7546)\tPrec@1 86.000 (73.186)\tPrec@5 100.000 (8)\n2017-09-18 13:32:33,020 - INFO - Epoch: [0][145/190], lr: 0.01000\tTime 12.983 (6.209)\tData 0.002 (0.003)\tLoss 0.4786 (1.7459)\tPrec@1 90.000 (73.301)\tPrec@5 100.000 (8)\n2017-09-18 13:32:44,395 - INFO - Epoch: [0][146/190], lr: 0.01000\tTime 11.375 (6.245)\tData 0.002 (0.003)\tLoss 0.3436 (1.7363)\tPrec@1 100.000 (73.483)\tPrec@5 100.000 (8)\n2017-09-18 13:32:55,576 - INFO - Epoch: [0][147/190], lr: 0.01000\tTime 11.181 (6.278)\tData 0.002 (0.003)\tLoss 0.3963 (1.7273)\tPrec@1 98.000 (73.649)\tPrec@5 100.000 (8)\n2017-09-18 13:33:07,442 - INFO - Epoch: [0][148/190], lr: 0.01000\tTime 11.867 (6.315)\tData 0.002 (0.003)\tLoss 0.4661 (1.7188)\tPrec@1 92.000 (73.772)\tPrec@5 98.000 (86)\n2017-09-18 13:33:19,375 - INFO - Epoch: [0][149/190], lr: 0.01000\tTime 11.932 (6.353)\tData 0.002 (0.003)\tLoss 0.4874 (1.7106)\tPrec@1 96.000 (73.920)\tPrec@5 100.000 (8)\n2017-09-18 13:33:31,265 - INFO - Epoch: [0][150/190], lr: 0.01000\tTime 11.890 (6.390)\tData 0.002 (0.003)\tLoss 0.3156 (1.7014)\tPrec@1 98.000 (74.079)\tPrec@5 100.000 (8)\n2017-09-18 13:33:42,987 - INFO - Epoch: [0][151/190], lr: 0.01000\tTime 11.722 (6.425)\tData 0.002 (0.003)\tLoss 0.2942 (1.6921)\tPrec@1 100.000 (74.250)\tPrec@5 100.000 (8)\n2017-09-18 13:33:55,927 - INFO - Epoch: [0][152/190], lr: 0.01000\tTime 12.940 (6.467)\tData 0.002 (0.003)\tLoss 0.4054 (1.6837)\tPrec@1 96.000 (74.392)\tPrec@5 100.000 (8)\n2017-09-18 13:34:08,886 - INFO - Epoch: [0][153/190], lr: 0.01000\tTime 12.959 (6.509)\tData 0.002 (0.003)\tLoss 0.5405 (1.6763)\tPrec@1 92.000 (74.506)\tPrec@5 100.000 (8)\n2017-09-18 13:34:21,638 - INFO - Epoch: [0][154/190], lr: 0.01000\tTime 12.752 (6.550)\tData 0.002 (0.003)\tLoss 0.4229 (1.6682)\tPrec@1 94.000 (74.632)\tPrec@5 100.000 (8)\n2017-09-18 13:34:34,436 - INFO - Epoch: [0][155/190], lr: 0.01000\tTime 12.798 (6.590)\tData 0.004 (0.003)\tLoss 0.4031 (1.6601)\tPrec@1 96.000 (74.769)\tPrec@5 100.000 (8)\n2017-09-18 13:34:47,209 - INFO - Epoch: [0][156/190], lr: 0.01000\tTime 12.773 (6.629)\tData 0.002 (0.003)\tLoss 0.4090 (1.6521)\tPrec@1 94.000 (74.892)\tPrec@5 100.000 (8)\n2017-09-18 13:34:59,683 - INFO - Epoch: [0][157/190], lr: 0.01000\tTime 12.474 (6.666)\tData 0.003 (0.003)\tLoss 0.3058 (1.6436)\tPrec@1 100.000 (75.051)\tPrec@5 100.000 (8)\n2017-09-18 13:35:12,320 - INFO - Epoch: [0][158/190], lr: 0.01000\tTime 12.636 (6.704)\tData 0.002 (0.003)\tLoss 0.5007 (1.6364)\tPrec@1 92.000 (75.157)\tPrec@5 98.000 (87)\n2017-09-18 13:35:24,694 - INFO - Epoch: [0][159/190], lr: 0.01000\tTime 12.374 (6.739)\tData 0.003 (0.003)\tLoss 0.4987 (1.6293)\tPrec@1 92.000 (75.263)\tPrec@5 98.000 (87)\n2017-09-18 13:35:37,976 - INFO - Epoch: [0][160/190], lr: 0.01000\tTime 13.282 (6.780)\tData 0.002 (0.003)\tLoss 0.2993 (1.6210)\tPrec@1 98.000 (75.404)\tPrec@5 100.000 (8)\n2017-09-18 13:35:51,475 - INFO - Epoch: [0][161/190], lr: 0.01000\tTime 13.499 (6.821)\tData 0.002 (0.003)\tLoss 0.5275 (1.6143)\tPrec@1 88.000 (75.481)\tPrec@5 96.000 (87)\n2017-09-18 13:36:05,019 - INFO - Epoch: [0][162/190], lr: 0.01000\tTime 13.545 (6.862)\tData 0.0\n</code></pre>\n<p>So, why did this happen?   Anyone who also ever met this could give me a hand?  Many thanks!</p>", "body_text": "Hi,  everyone:\nI build a model based on LSTM as follows:\nclass LSTM(nn.Module):\n    def __init__(self,\n                 input_len=1024,\n                 hidden_len=1024,\n                 model_len=12,\n                 target_size=101,\n                 is_softmax=True):\n        super(LSTM,self).__init__()\n        self.hidden_len=hidden_len\n        self.is_softmax=is_softmax\n        self.lstm=nn.LSTM(input_len,hidden_len,batch_first=True)\n        self.target_size=target_size\n        self.hidden2class=nn.Linear(hidden_len,target_size)\n        self.hidden=self.init_hidden()\n    def init_hidden(self):\n        return (Variable(torch.zeros(1, 1, self.hidden_len)),\n            Variable(torch.zeros(1, 1, self.hidden_len)))\n    def forward(self,x):\n        lstm_out,self.hidden=self.lstm(\n            x,self.hidden\n        )\n        cls_scores=self.hidden2class(lstm_out[:,-1,:]).view(-1,self.target_size)\n        if self.is_softmax:\n            cls_scores=F.log_softmax(cls_scores)\n        return cls_scores\n\nand the training process is as follows:\ndef train(train_data,model,criterion,optimizer,epoch):\n    model.train()\n    for i,(input,target) in enumerate(train_data):\n\n        if use_cuda:\n            target=target.cuda(async=True)\n        input_var=torch.autograd.Variable(input)\n        target_var=torch.autograd.Variable(target)\n\n        output=model(input_var)\n        loss=criterion(output,target_var)\n\n        prec1,prec5=accuracy(output.data,target,topk=(1,5))\n        losses.update(loss.data[0],input.size(0))\n\n        optimizer.zero_grad()\n        loss.backward(retain_graph=True)\n        optimizer.step()\n\n`\nWhen not add the parameter \"retain_graph\" in the loss.backward function, I received an error message:\nTraceback (most recent call last):\n  File \"temporal_pool.py\", line 328, in <module>\n    main()\n  File \"temporal_pool.py\", line 174, in main\n    train(train_data,model,loss_function,optimizer,epoch)\n  File \"temporal_pool.py\", line 216, in train\n    loss.backward()\n  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\n    variables, grad_variables, retain_graph)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/function.py\", line 91, in apply\n    return self._forward_cls.backward(self, *args)\n  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py\", line 52, in backward\n    a, b = ctx.saved_variables\nRuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n\nBut, when added the parameter,  My training process has met a trouble that, the time consumed each iteration endlessly increasing!  The snapshot is like below:\n2:36,671 - INFO - Epoch: [0][84/190], lr: 0.01000\tTime 7.042 (3.650)\tData 0.003 (0.004)\tLoss 1.0369 (2.5610)\tPrec@1 90.000 (59.741)\tPrec@5 94.000 (77)\n2017-09-18 13:22:43,930 - INFO - Epoch: [0][85/190], lr: 0.01000\tTime 7.259 (3.692)\tData 0.003 (0.004)\tLoss 0.6710 (2.5391)\tPrec@1 96.000 (60.163)\tPrec@5 100.000 (7)\n2017-09-18 13:22:51,523 - INFO - Epoch: [0][86/190], lr: 0.01000\tTime 7.593 (3.737)\tData 0.003 (0.004)\tLoss 0.8195 (2.5193)\tPrec@1 98.000 (60.598)\tPrec@5 100.000 (7)\n2017-09-18 13:22:58,701 - INFO - Epoch: [0][87/190], lr: 0.01000\tTime 7.179 (3.776)\tData 0.002 (0.004)\tLoss 0.7179 (2.4988)\tPrec@1 94.000 (60.977)\tPrec@5 100.000 (7)\n2017-09-18 13:23:06,277 - INFO - Epoch: [0][88/190], lr: 0.01000\tTime 7.576 (3.818)\tData 0.002 (0.004)\tLoss 0.9274 (2.4812)\tPrec@1 80.000 (61.191)\tPrec@5 98.000 (78)\n2017-09-18 13:23:14,097 - INFO - Epoch: [0][89/190], lr: 0.01000\tTime 7.820 (3.863)\tData 0.003 (0.004)\tLoss 0.7612 (2.4621)\tPrec@1 96.000 (61.578)\tPrec@5 100.000 (7)\n2017-09-18 13:23:21,841 - INFO - Epoch: [0][90/190], lr: 0.01000\tTime 7.744 (3.905)\tData 0.002 (0.004)\tLoss 0.6913 (2.4426)\tPrec@1 98.000 (61.978)\tPrec@5 100.000 (7)\n2017-09-18 13:23:29,396 - INFO - Epoch: [0][91/190], lr: 0.01000\tTime 7.555 (3.945)\tData 0.004 (0.004)\tLoss 0.8286 (2.4251)\tPrec@1 92.000 (62.304)\tPrec@5 94.000 (78)\n2017-09-18 13:23:36,920 - INFO - Epoch: [0][92/190], lr: 0.01000\tTime 7.524 (3.984)\tData 0.002 (0.004)\tLoss 0.8006 (2.4076)\tPrec@1 90.000 (62.602)\tPrec@5 98.000 (79)\n2017-09-18 13:23:44,831 - INFO - Epoch: [0][93/190], lr: 0.01000\tTime 7.911 (4.025)\tData 0.002 (0.004)\tLoss 0.7698 (2.3902)\tPrec@1 96.000 (62.957)\tPrec@5 100.000 (7)\n2017-09-18 13:23:53,035 - INFO - Epoch: [0][94/190], lr: 0.01000\tTime 8.204 (4.069)\tData 0.003 (0.004)\tLoss 0.8519 (2.3740)\tPrec@1 92.000 (63.263)\tPrec@5 98.000 (79)\n2017-09-18 13:24:00,845 - INFO - Epoch: [0][95/190], lr: 0.01000\tTime 7.810 (4.108)\tData 0.002 (0.004)\tLoss 0.7559 (2.3571)\tPrec@1 90.000 (63.542)\tPrec@5 98.000 (79)\n2017-09-18 13:24:08,728 - INFO - Epoch: [0][96/190], lr: 0.01000\tTime 7.883 (4.147)\tData 0.002 (0.004)\tLoss 0.7015 (2.3400)\tPrec@1 92.000 (63.835)\tPrec@5 100.000 (7)\n2017-09-18 13:24:17,098 - INFO - Epoch: [0][97/190], lr: 0.01000\tTime 8.370 (4.190)\tData 0.002 (0.004)\tLoss 0.8148 (2.3245)\tPrec@1 84.000 (64.041)\tPrec@5 98.000 (80)\n2017-09-18 13:24:25,550 - INFO - Epoch: [0][98/190], lr: 0.01000\tTime 8.452 (4.233)\tData 0.002 (0.004)\tLoss 0.7163 (2.3082)\tPrec@1 86.000 (64.263)\tPrec@5 98.000 (80)\n2017-09-18 13:24:33,836 - INFO - Epoch: [0][99/190], lr: 0.01000\tTime 8.286 (4.274)\tData 0.002 (0.004)\tLoss 0.6683 (2.2918)\tPrec@1 86.000 (64.480)\tPrec@5 98.000 (80)\n2017-09-18 13:24:42,290 - INFO - Epoch: [0][100/190], lr: 0.01000\tTime 8.454 (4.315)\tData 0.002 (0.004)\tLoss 0.6422 (2.2755)\tPrec@1 94.000 (64.772)\tPrec@5 100.000 (8)\n2017-09-18 13:24:50,672 - INFO - Epoch: [0][101/190], lr: 0.01000\tTime 8.382 (4.355)\tData 0.002 (0.004)\tLoss 0.7038 (2.2601)\tPrec@1 90.000 (65.020)\tPrec@5 100.000 (8)\n2017-09-18 13:25:00,062 - INFO - Epoch: [0][102/190], lr: 0.01000\tTime 9.389 (4.404)\tData 0.002 (0.004)\tLoss 0.6784 (2.2447)\tPrec@1 88.000 (65.243)\tPrec@5 98.000 (80)\n2017-09-18 13:25:09,428 - INFO - Epoch: [0][103/190], lr: 0.01000\tTime 9.366 (4.452)\tData 0.002 (0.004)\tLoss 0.7145 (2.2300)\tPrec@1 90.000 (65.481)\tPrec@5 98.000 (81)\n2017-09-18 13:25:18,487 - INFO - Epoch: [0][104/190], lr: 0.01000\tTime 9.059 (4.496)\tData 0.004 (0.004)\tLoss 0.6637 (2.2151)\tPrec@1 90.000 (65.714)\tPrec@5 98.000 (81)\n2017-09-18 13:25:27,151 - INFO - Epoch: [0][105/190], lr: 0.01000\tTime 8.664 (4.535)\tData 0.002 (0.004)\tLoss 0.6512 (2.2004)\tPrec@1 92.000 (65.962)\tPrec@5 100.000 (8)\n2017-09-18 13:25:36,368 - INFO - Epoch: [0][106/190], lr: 0.01000\tTime 9.217 (4.579)\tData 0.002 (0.004)\tLoss 0.6434 (2.1858)\tPrec@1 92.000 (66.206)\tPrec@5 100.000 (8)\n2017-09-18 13:25:45,407 - INFO - Epoch: [0][107/190], lr: 0.01000\tTime 9.039 (4.620)\tData 0.002 (0.004)\tLoss 0.6722 (2.1718)\tPrec@1 82.000 (66.352)\tPrec@5 100.000 (8)\n2017-09-18 13:25:55,124 - INFO - Epoch: [0][108/190], lr: 0.01000\tTime 9.717 (4.667)\tData 0.002 (0.004)\tLoss 0.5507 (2.1569)\tPrec@1 94.000 (66.606)\tPrec@5 100.000 (8)\n2017-09-18 13:26:04,149 - INFO - Epoch: [0][109/190], lr: 0.01000\tTime 9.024 (4.706)\tData 0.002 (0.004)\tLoss 0.5493 (2.1423)\tPrec@1 96.000 (66.873)\tPrec@5 98.000 (82)\n2017-09-18 13:26:13,310 - INFO - Epoch: [0][110/190], lr: 0.01000\tTime 9.161 (4.747)\tData 0.002 (0.004)\tLoss 0.6473 (2.1288)\tPrec@1 90.000 (67.081)\tPrec@5 100.000 (8)\n2017-09-18 13:26:22,946 - INFO - Epoch: [0][111/190], lr: 0.01000\tTime 9.636 (4.790)\tData 0.002 (0.004)\tLoss 0.5715 (2.1149)\tPrec@1 94.000 (67.321)\tPrec@5 100.000 (8)\n2017-09-18 13:26:33,026 - INFO - Epoch: [0][112/190], lr: 0.01000\tTime 10.081 (4.837)\tData 0.002 (0.004)\tLoss 0.5555 (2.1011)\tPrec@1 92.000 (67.540)\tPrec@5 100.000 (8)\n2017-09-18 13:26:42,249 - INFO - Epoch: [0][113/190], lr: 0.01000\tTime 9.223 (4.876)\tData 0.003 (0.004)\tLoss 0.4574 (2.0867)\tPrec@1 98.000 (67.807)\tPrec@5 100.000 (8)\n2017-09-18 13:26:51,917 - INFO - Epoch: [0][114/190], lr: 0.01000\tTime 9.668 (4.917)\tData 0.002 (0.004)\tLoss 0.7171 (2.0748)\tPrec@1 88.000 (67.983)\tPrec@5 100.000 (8)\n2017-09-18 13:27:01,647 - INFO - Epoch: [0][115/190], lr: 0.01000\tTime 9.730 (4.959)\tData 0.002 (0.004)\tLoss 0.4380 (2.0607)\tPrec@1 98.000 (68.241)\tPrec@5 100.000 (8)\n2017-09-18 13:27:11,674 - INFO - Epoch: [0][116/190], lr: 0.01000\tTime 10.027 (5.002)\tData 0.002 (0.004)\tLoss 0.5357 (2.0477)\tPrec@1 96.000 (68.479)\tPrec@5 98.000 (83)\n2017-09-18 13:27:21,527 - INFO - Epoch: [0][117/190], lr: 0.01000\tTime 9.853 (5.043)\tData 0.003 (0.004)\tLoss 0.5669 (2.0351)\tPrec@1 94.000 (68.695)\tPrec@5 98.000 (83)\n2017-09-18 13:27:31,056 - INFO - Epoch: [0][118/190], lr: 0.01000\tTime 9.529 (5.081)\tData 0.002 (0.004)\tLoss 0.6435 (2.0234)\tPrec@1 92.000 (68.891)\tPrec@5 94.000 (83)\n2017-09-18 13:27:41,082 - INFO - Epoch: [0][119/190], lr: 0.01000\tTime 10.026 (5.122)\tData 0.002 (0.004)\tLoss 0.5540 (2.0112)\tPrec@1 88.000 (69.050)\tPrec@5 100.000 (8)\n2017-09-18 13:27:50,734 - INFO - Epoch: [0][120/190], lr: 0.01000\tTime 9.653 (5.159)\tData 0.002 (0.004)\tLoss 0.5452 (1.9991)\tPrec@1 92.000 (69.240)\tPrec@5 100.000 (8)\n2017-09-18 13:28:00,682 - INFO - Epoch: [0][121/190], lr: 0.01000\tTime 9.948 (5.199)\tData 0.002 (0.004)\tLoss 0.8453 (1.9896)\tPrec@1 82.000 (69.344)\tPrec@5 96.000 (83)\n2017-09-18 13:28:10,677 - INFO - Epoch: [0][122/190], lr: 0.01000\tTime 9.995 (5.238)\tData 0.002 (0.004)\tLoss 0.6531 (1.9787)\tPrec@1 92.000 (69.528)\tPrec@5 100.000 (8)\n2017-09-18 13:28:21,240 - INFO - Epoch: [0][123/190], lr: 0.01000\tTime 10.562 (5.281)\tData 0.002 (0.004)\tLoss 0.5329 (1.9671)\tPrec@1 88.000 (69.677)\tPrec@5 100.000 (8)\n2017-09-18 13:28:31,558 - INFO - Epoch: [0][124/190], lr: 0.01000\tTime 10.318 (5.321)\tData 0.002 (0.004)\tLoss 0.6132 (1.9562)\tPrec@1 90.000 (69.840)\tPrec@5 100.000 (8)\n2017-09-18 13:28:42,142 - INFO - Epoch: [0][125/190], lr: 0.01000\tTime 10.585 (5.363)\tData 0.002 (0.004)\tLoss 0.4761 (1.9445)\tPrec@1 96.000 (70.048)\tPrec@5 100.000 (8)\n2017-09-18 13:28:52,802 - INFO - Epoch: [0][126/190], lr: 0.01000\tTime 10.660 (5.404)\tData 0.002 (0.004)\tLoss 0.4652 (1.9328)\tPrec@1 96.000 (70.252)\tPrec@5 100.000 (8)\n2017-09-18 13:29:03,661 - INFO - Epoch: [0][127/190], lr: 0.01000\tTime 10.859 (5.447)\tData 0.003 (0.004)\tLoss 0.4837 (1.9215)\tPrec@1 98.000 (70.469)\tPrec@5 100.000 (8)\n2017-09-18 13:29:14,862 - INFO - Epoch: [0][128/190], lr: 0.01000\tTime 11.201 (5.492)\tData 0.003 (0.004)\tLoss 0.4328 (1.9100)\tPrec@1 96.000 (70.667)\tPrec@5 100.000 (8)\n2017-09-18 13:29:25,954 - INFO - Epoch: [0][129/190], lr: 0.01000\tTime 11.092 (5.535)\tData 0.002 (0.004)\tLoss 0.6569 (1.9003)\tPrec@1 90.000 (70.815)\tPrec@5 96.000 (84)\n2017-09-18 13:29:36,764 - INFO - Epoch: [0][130/190], lr: 0.01000\tTime 10.810 (5.575)\tData 0.004 (0.004)\tLoss 0.4948 (1.8896)\tPrec@1 94.000 (70.992)\tPrec@5 100.000 (8)\n2017-09-18 13:29:47,688 - INFO - Epoch: [0][131/190], lr: 0.01000\tTime 10.923 (5.615)\tData 0.002 (0.004)\tLoss 0.4707 (1.8789)\tPrec@1 94.000 (71.167)\tPrec@5 100.000 (8)\n2017-09-18 13:29:58,851 - INFO - Epoch: [0][132/190], lr: 0.01000\tTime 11.164 (5.657)\tData 0.006 (0.004)\tLoss 0.5153 (1.8686)\tPrec@1 92.000 (71.323)\tPrec@5 100.000 (8)\n2017-09-18 13:30:10,097 - INFO - Epoch: [0][133/190], lr: 0.01000\tTime 11.246 (5.699)\tData 0.002 (0.004)\tLoss 0.4860 (1.8583)\tPrec@1 94.000 (71.493)\tPrec@5 98.000 (85)\n2017-09-18 13:30:21,123 - INFO - Epoch: [0][134/190], lr: 0.01000\tTime 11.026 (5.738)\tData 0.002 (0.004)\tLoss 0.4441 (1.8478)\tPrec@1 98.000 (71.689)\tPrec@5 100.000 (8)\n2017-09-18 13:30:32,364 - INFO - Epoch: [0][135/190], lr: 0.01000\tTime 11.242 (5.779)\tData 0.002 (0.004)\tLoss 0.6027 (1.8387)\tPrec@1 94.000 (71.853)\tPrec@5 98.000 (85)\n2017-09-18 13:30:43,511 - INFO - Epoch: [0][136/190], lr: 0.01000\tTime 11.147 (5.818)\tData 0.002 (0.004)\tLoss 0.4439 (1.8285)\tPrec@1 94.000 (72.015)\tPrec@5 100.000 (8)\n2017-09-18 13:30:56,065 - INFO - Epoch: [0][137/190], lr: 0.01000\tTime 12.553 (5.867)\tData 0.002 (0.004)\tLoss 0.4715 (1.8187)\tPrec@1 92.000 (72.159)\tPrec@5 100.000 (8)\n2017-09-18 13:31:07,832 - INFO - Epoch: [0][138/190], lr: 0.01000\tTime 11.768 (5.909)\tData 0.003 (0.004)\tLoss 0.4727 (1.8090)\tPrec@1 96.000 (72.331)\tPrec@5 100.000 (8)\n2017-09-18 13:31:19,450 - INFO - Epoch: [0][139/190], lr: 0.01000\tTime 11.618 (5.950)\tData 0.002 (0.004)\tLoss 0.4232 (1.7991)\tPrec@1 96.000 (72.500)\tPrec@5 100.000 (8)\n2017-09-18 13:31:31,482 - INFO - Epoch: [0][140/190], lr: 0.01000\tTime 12.032 (5.993)\tData 0.002 (0.004)\tLoss 0.5206 (1.7900)\tPrec@1 92.000 (72.638)\tPrec@5 98.000 (85)\n2017-09-18 13:31:42,916 - INFO - Epoch: [0][141/190], lr: 0.01000\tTime 11.434 (6.032)\tData 0.002 (0.003)\tLoss 0.3838 (1.7801)\tPrec@1 98.000 (72.817)\tPrec@5 100.000 (8)\n2017-09-18 13:31:54,853 - INFO - Epoch: [0][142/190], lr: 0.01000\tTime 11.936 (6.073)\tData 0.002 (0.003)\tLoss 0.6089 (1.7719)\tPrec@1 90.000 (72.937)\tPrec@5 98.000 (86)\n2017-09-18 13:32:06,966 - INFO - Epoch: [0][143/190], lr: 0.01000\tTime 12.113 (6.115)\tData 0.002 (0.003)\tLoss 0.4373 (1.7626)\tPrec@1 96.000 (73.097)\tPrec@5 100.000 (8)\n2017-09-18 13:32:20,037 - INFO - Epoch: [0][144/190], lr: 0.01000\tTime 13.070 (6.163)\tData 0.002 (0.003)\tLoss 0.5992 (1.7546)\tPrec@1 86.000 (73.186)\tPrec@5 100.000 (8)\n2017-09-18 13:32:33,020 - INFO - Epoch: [0][145/190], lr: 0.01000\tTime 12.983 (6.209)\tData 0.002 (0.003)\tLoss 0.4786 (1.7459)\tPrec@1 90.000 (73.301)\tPrec@5 100.000 (8)\n2017-09-18 13:32:44,395 - INFO - Epoch: [0][146/190], lr: 0.01000\tTime 11.375 (6.245)\tData 0.002 (0.003)\tLoss 0.3436 (1.7363)\tPrec@1 100.000 (73.483)\tPrec@5 100.000 (8)\n2017-09-18 13:32:55,576 - INFO - Epoch: [0][147/190], lr: 0.01000\tTime 11.181 (6.278)\tData 0.002 (0.003)\tLoss 0.3963 (1.7273)\tPrec@1 98.000 (73.649)\tPrec@5 100.000 (8)\n2017-09-18 13:33:07,442 - INFO - Epoch: [0][148/190], lr: 0.01000\tTime 11.867 (6.315)\tData 0.002 (0.003)\tLoss 0.4661 (1.7188)\tPrec@1 92.000 (73.772)\tPrec@5 98.000 (86)\n2017-09-18 13:33:19,375 - INFO - Epoch: [0][149/190], lr: 0.01000\tTime 11.932 (6.353)\tData 0.002 (0.003)\tLoss 0.4874 (1.7106)\tPrec@1 96.000 (73.920)\tPrec@5 100.000 (8)\n2017-09-18 13:33:31,265 - INFO - Epoch: [0][150/190], lr: 0.01000\tTime 11.890 (6.390)\tData 0.002 (0.003)\tLoss 0.3156 (1.7014)\tPrec@1 98.000 (74.079)\tPrec@5 100.000 (8)\n2017-09-18 13:33:42,987 - INFO - Epoch: [0][151/190], lr: 0.01000\tTime 11.722 (6.425)\tData 0.002 (0.003)\tLoss 0.2942 (1.6921)\tPrec@1 100.000 (74.250)\tPrec@5 100.000 (8)\n2017-09-18 13:33:55,927 - INFO - Epoch: [0][152/190], lr: 0.01000\tTime 12.940 (6.467)\tData 0.002 (0.003)\tLoss 0.4054 (1.6837)\tPrec@1 96.000 (74.392)\tPrec@5 100.000 (8)\n2017-09-18 13:34:08,886 - INFO - Epoch: [0][153/190], lr: 0.01000\tTime 12.959 (6.509)\tData 0.002 (0.003)\tLoss 0.5405 (1.6763)\tPrec@1 92.000 (74.506)\tPrec@5 100.000 (8)\n2017-09-18 13:34:21,638 - INFO - Epoch: [0][154/190], lr: 0.01000\tTime 12.752 (6.550)\tData 0.002 (0.003)\tLoss 0.4229 (1.6682)\tPrec@1 94.000 (74.632)\tPrec@5 100.000 (8)\n2017-09-18 13:34:34,436 - INFO - Epoch: [0][155/190], lr: 0.01000\tTime 12.798 (6.590)\tData 0.004 (0.003)\tLoss 0.4031 (1.6601)\tPrec@1 96.000 (74.769)\tPrec@5 100.000 (8)\n2017-09-18 13:34:47,209 - INFO - Epoch: [0][156/190], lr: 0.01000\tTime 12.773 (6.629)\tData 0.002 (0.003)\tLoss 0.4090 (1.6521)\tPrec@1 94.000 (74.892)\tPrec@5 100.000 (8)\n2017-09-18 13:34:59,683 - INFO - Epoch: [0][157/190], lr: 0.01000\tTime 12.474 (6.666)\tData 0.003 (0.003)\tLoss 0.3058 (1.6436)\tPrec@1 100.000 (75.051)\tPrec@5 100.000 (8)\n2017-09-18 13:35:12,320 - INFO - Epoch: [0][158/190], lr: 0.01000\tTime 12.636 (6.704)\tData 0.002 (0.003)\tLoss 0.5007 (1.6364)\tPrec@1 92.000 (75.157)\tPrec@5 98.000 (87)\n2017-09-18 13:35:24,694 - INFO - Epoch: [0][159/190], lr: 0.01000\tTime 12.374 (6.739)\tData 0.003 (0.003)\tLoss 0.4987 (1.6293)\tPrec@1 92.000 (75.263)\tPrec@5 98.000 (87)\n2017-09-18 13:35:37,976 - INFO - Epoch: [0][160/190], lr: 0.01000\tTime 13.282 (6.780)\tData 0.002 (0.003)\tLoss 0.2993 (1.6210)\tPrec@1 98.000 (75.404)\tPrec@5 100.000 (8)\n2017-09-18 13:35:51,475 - INFO - Epoch: [0][161/190], lr: 0.01000\tTime 13.499 (6.821)\tData 0.002 (0.003)\tLoss 0.5275 (1.6143)\tPrec@1 88.000 (75.481)\tPrec@5 96.000 (87)\n2017-09-18 13:36:05,019 - INFO - Epoch: [0][162/190], lr: 0.01000\tTime 13.545 (6.862)\tData 0.0\n\nSo, why did this happen?   Anyone who also ever met this could give me a hand?  Many thanks!", "body": "Hi,  everyone:\r\n\r\nI build a model based on LSTM as follows:\r\n    \r\n\tclass LSTM(nn.Module):\r\n\t    def __init__(self,\r\n\t                 input_len=1024,\r\n\t                 hidden_len=1024,\r\n\t                 model_len=12,\r\n\t                 target_size=101,\r\n\t                 is_softmax=True):\r\n\t        super(LSTM,self).__init__()\r\n\t        self.hidden_len=hidden_len\r\n\t        self.is_softmax=is_softmax\r\n\t        self.lstm=nn.LSTM(input_len,hidden_len,batch_first=True)\r\n\t        self.target_size=target_size\r\n\t        self.hidden2class=nn.Linear(hidden_len,target_size)\r\n\t        self.hidden=self.init_hidden()\r\n\t    def init_hidden(self):\r\n\t        return (Variable(torch.zeros(1, 1, self.hidden_len)),\r\n\t            Variable(torch.zeros(1, 1, self.hidden_len)))\r\n\t    def forward(self,x):\r\n\t        lstm_out,self.hidden=self.lstm(\r\n\t            x,self.hidden\r\n\t        )\r\n\t        cls_scores=self.hidden2class(lstm_out[:,-1,:]).view(-1,self.target_size)\r\n\t        if self.is_softmax:\r\n\t            cls_scores=F.log_softmax(cls_scores)\r\n\t        return cls_scores\r\nand the training process is as follows:\r\n\r\n\tdef train(train_data,model,criterion,optimizer,epoch):\r\n\t    model.train()\r\n\t    for i,(input,target) in enumerate(train_data):\r\n\t\r\n\t        if use_cuda:\r\n\t            target=target.cuda(async=True)\r\n\t        input_var=torch.autograd.Variable(input)\r\n\t        target_var=torch.autograd.Variable(target)\r\n\t\r\n\t        output=model(input_var)\r\n\t        loss=criterion(output,target_var)\r\n\t\r\n\t        prec1,prec5=accuracy(output.data,target,topk=(1,5))\r\n\t        losses.update(loss.data[0],input.size(0))\r\n\t\r\n\t        optimizer.zero_grad()\r\n\t        loss.backward(retain_graph=True)\r\n\t        optimizer.step()\r\n       \r\n `\r\n\r\nWhen not add the parameter \"retain_graph\" in the loss.backward function, I received an error message:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"temporal_pool.py\", line 328, in <module>\r\n\t    main()\r\n\t  File \"temporal_pool.py\", line 174, in main\r\n\t    train(train_data,model,loss_function,optimizer,epoch)\r\n\t  File \"temporal_pool.py\", line 216, in train\r\n\t    loss.backward()\r\n\t  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/variable.py\", line 156, in backward\r\n\t    torch.autograd.backward(self, gradient, retain_graph, create_graph, retain_variables)\r\n\t  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/__init__.py\", line 98, in backward\r\n\t    variables, grad_variables, retain_graph)\r\n\t  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/function.py\", line 91, in apply\r\n\t    return self._forward_cls.backward(self, *args)\r\n\t  File \"/opt/anaconda2/lib/python2.7/site-packages/torch/autograd/_functions/basic_ops.py\", line 52, in backward\r\n\t    a, b = ctx.saved_variables\r\n\tRuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\r\n\r\n\r\nBut, when added the parameter,  My training process has met a trouble that, the time consumed each iteration endlessly increasing!  The snapshot is like below:\r\n\r\n\t2:36,671 - INFO - Epoch: [0][84/190], lr: 0.01000\tTime 7.042 (3.650)\tData 0.003 (0.004)\tLoss 1.0369 (2.5610)\tPrec@1 90.000 (59.741)\tPrec@5 94.000 (77)\r\n\t2017-09-18 13:22:43,930 - INFO - Epoch: [0][85/190], lr: 0.01000\tTime 7.259 (3.692)\tData 0.003 (0.004)\tLoss 0.6710 (2.5391)\tPrec@1 96.000 (60.163)\tPrec@5 100.000 (7)\r\n\t2017-09-18 13:22:51,523 - INFO - Epoch: [0][86/190], lr: 0.01000\tTime 7.593 (3.737)\tData 0.003 (0.004)\tLoss 0.8195 (2.5193)\tPrec@1 98.000 (60.598)\tPrec@5 100.000 (7)\r\n\t2017-09-18 13:22:58,701 - INFO - Epoch: [0][87/190], lr: 0.01000\tTime 7.179 (3.776)\tData 0.002 (0.004)\tLoss 0.7179 (2.4988)\tPrec@1 94.000 (60.977)\tPrec@5 100.000 (7)\r\n\t2017-09-18 13:23:06,277 - INFO - Epoch: [0][88/190], lr: 0.01000\tTime 7.576 (3.818)\tData 0.002 (0.004)\tLoss 0.9274 (2.4812)\tPrec@1 80.000 (61.191)\tPrec@5 98.000 (78)\r\n\t2017-09-18 13:23:14,097 - INFO - Epoch: [0][89/190], lr: 0.01000\tTime 7.820 (3.863)\tData 0.003 (0.004)\tLoss 0.7612 (2.4621)\tPrec@1 96.000 (61.578)\tPrec@5 100.000 (7)\r\n\t2017-09-18 13:23:21,841 - INFO - Epoch: [0][90/190], lr: 0.01000\tTime 7.744 (3.905)\tData 0.002 (0.004)\tLoss 0.6913 (2.4426)\tPrec@1 98.000 (61.978)\tPrec@5 100.000 (7)\r\n\t2017-09-18 13:23:29,396 - INFO - Epoch: [0][91/190], lr: 0.01000\tTime 7.555 (3.945)\tData 0.004 (0.004)\tLoss 0.8286 (2.4251)\tPrec@1 92.000 (62.304)\tPrec@5 94.000 (78)\r\n\t2017-09-18 13:23:36,920 - INFO - Epoch: [0][92/190], lr: 0.01000\tTime 7.524 (3.984)\tData 0.002 (0.004)\tLoss 0.8006 (2.4076)\tPrec@1 90.000 (62.602)\tPrec@5 98.000 (79)\r\n\t2017-09-18 13:23:44,831 - INFO - Epoch: [0][93/190], lr: 0.01000\tTime 7.911 (4.025)\tData 0.002 (0.004)\tLoss 0.7698 (2.3902)\tPrec@1 96.000 (62.957)\tPrec@5 100.000 (7)\r\n\t2017-09-18 13:23:53,035 - INFO - Epoch: [0][94/190], lr: 0.01000\tTime 8.204 (4.069)\tData 0.003 (0.004)\tLoss 0.8519 (2.3740)\tPrec@1 92.000 (63.263)\tPrec@5 98.000 (79)\r\n\t2017-09-18 13:24:00,845 - INFO - Epoch: [0][95/190], lr: 0.01000\tTime 7.810 (4.108)\tData 0.002 (0.004)\tLoss 0.7559 (2.3571)\tPrec@1 90.000 (63.542)\tPrec@5 98.000 (79)\r\n\t2017-09-18 13:24:08,728 - INFO - Epoch: [0][96/190], lr: 0.01000\tTime 7.883 (4.147)\tData 0.002 (0.004)\tLoss 0.7015 (2.3400)\tPrec@1 92.000 (63.835)\tPrec@5 100.000 (7)\r\n\t2017-09-18 13:24:17,098 - INFO - Epoch: [0][97/190], lr: 0.01000\tTime 8.370 (4.190)\tData 0.002 (0.004)\tLoss 0.8148 (2.3245)\tPrec@1 84.000 (64.041)\tPrec@5 98.000 (80)\r\n\t2017-09-18 13:24:25,550 - INFO - Epoch: [0][98/190], lr: 0.01000\tTime 8.452 (4.233)\tData 0.002 (0.004)\tLoss 0.7163 (2.3082)\tPrec@1 86.000 (64.263)\tPrec@5 98.000 (80)\r\n\t2017-09-18 13:24:33,836 - INFO - Epoch: [0][99/190], lr: 0.01000\tTime 8.286 (4.274)\tData 0.002 (0.004)\tLoss 0.6683 (2.2918)\tPrec@1 86.000 (64.480)\tPrec@5 98.000 (80)\r\n\t2017-09-18 13:24:42,290 - INFO - Epoch: [0][100/190], lr: 0.01000\tTime 8.454 (4.315)\tData 0.002 (0.004)\tLoss 0.6422 (2.2755)\tPrec@1 94.000 (64.772)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:24:50,672 - INFO - Epoch: [0][101/190], lr: 0.01000\tTime 8.382 (4.355)\tData 0.002 (0.004)\tLoss 0.7038 (2.2601)\tPrec@1 90.000 (65.020)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:25:00,062 - INFO - Epoch: [0][102/190], lr: 0.01000\tTime 9.389 (4.404)\tData 0.002 (0.004)\tLoss 0.6784 (2.2447)\tPrec@1 88.000 (65.243)\tPrec@5 98.000 (80)\r\n\t2017-09-18 13:25:09,428 - INFO - Epoch: [0][103/190], lr: 0.01000\tTime 9.366 (4.452)\tData 0.002 (0.004)\tLoss 0.7145 (2.2300)\tPrec@1 90.000 (65.481)\tPrec@5 98.000 (81)\r\n\t2017-09-18 13:25:18,487 - INFO - Epoch: [0][104/190], lr: 0.01000\tTime 9.059 (4.496)\tData 0.004 (0.004)\tLoss 0.6637 (2.2151)\tPrec@1 90.000 (65.714)\tPrec@5 98.000 (81)\r\n\t2017-09-18 13:25:27,151 - INFO - Epoch: [0][105/190], lr: 0.01000\tTime 8.664 (4.535)\tData 0.002 (0.004)\tLoss 0.6512 (2.2004)\tPrec@1 92.000 (65.962)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:25:36,368 - INFO - Epoch: [0][106/190], lr: 0.01000\tTime 9.217 (4.579)\tData 0.002 (0.004)\tLoss 0.6434 (2.1858)\tPrec@1 92.000 (66.206)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:25:45,407 - INFO - Epoch: [0][107/190], lr: 0.01000\tTime 9.039 (4.620)\tData 0.002 (0.004)\tLoss 0.6722 (2.1718)\tPrec@1 82.000 (66.352)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:25:55,124 - INFO - Epoch: [0][108/190], lr: 0.01000\tTime 9.717 (4.667)\tData 0.002 (0.004)\tLoss 0.5507 (2.1569)\tPrec@1 94.000 (66.606)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:26:04,149 - INFO - Epoch: [0][109/190], lr: 0.01000\tTime 9.024 (4.706)\tData 0.002 (0.004)\tLoss 0.5493 (2.1423)\tPrec@1 96.000 (66.873)\tPrec@5 98.000 (82)\r\n\t2017-09-18 13:26:13,310 - INFO - Epoch: [0][110/190], lr: 0.01000\tTime 9.161 (4.747)\tData 0.002 (0.004)\tLoss 0.6473 (2.1288)\tPrec@1 90.000 (67.081)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:26:22,946 - INFO - Epoch: [0][111/190], lr: 0.01000\tTime 9.636 (4.790)\tData 0.002 (0.004)\tLoss 0.5715 (2.1149)\tPrec@1 94.000 (67.321)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:26:33,026 - INFO - Epoch: [0][112/190], lr: 0.01000\tTime 10.081 (4.837)\tData 0.002 (0.004)\tLoss 0.5555 (2.1011)\tPrec@1 92.000 (67.540)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:26:42,249 - INFO - Epoch: [0][113/190], lr: 0.01000\tTime 9.223 (4.876)\tData 0.003 (0.004)\tLoss 0.4574 (2.0867)\tPrec@1 98.000 (67.807)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:26:51,917 - INFO - Epoch: [0][114/190], lr: 0.01000\tTime 9.668 (4.917)\tData 0.002 (0.004)\tLoss 0.7171 (2.0748)\tPrec@1 88.000 (67.983)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:27:01,647 - INFO - Epoch: [0][115/190], lr: 0.01000\tTime 9.730 (4.959)\tData 0.002 (0.004)\tLoss 0.4380 (2.0607)\tPrec@1 98.000 (68.241)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:27:11,674 - INFO - Epoch: [0][116/190], lr: 0.01000\tTime 10.027 (5.002)\tData 0.002 (0.004)\tLoss 0.5357 (2.0477)\tPrec@1 96.000 (68.479)\tPrec@5 98.000 (83)\r\n\t2017-09-18 13:27:21,527 - INFO - Epoch: [0][117/190], lr: 0.01000\tTime 9.853 (5.043)\tData 0.003 (0.004)\tLoss 0.5669 (2.0351)\tPrec@1 94.000 (68.695)\tPrec@5 98.000 (83)\r\n\t2017-09-18 13:27:31,056 - INFO - Epoch: [0][118/190], lr: 0.01000\tTime 9.529 (5.081)\tData 0.002 (0.004)\tLoss 0.6435 (2.0234)\tPrec@1 92.000 (68.891)\tPrec@5 94.000 (83)\r\n\t2017-09-18 13:27:41,082 - INFO - Epoch: [0][119/190], lr: 0.01000\tTime 10.026 (5.122)\tData 0.002 (0.004)\tLoss 0.5540 (2.0112)\tPrec@1 88.000 (69.050)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:27:50,734 - INFO - Epoch: [0][120/190], lr: 0.01000\tTime 9.653 (5.159)\tData 0.002 (0.004)\tLoss 0.5452 (1.9991)\tPrec@1 92.000 (69.240)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:28:00,682 - INFO - Epoch: [0][121/190], lr: 0.01000\tTime 9.948 (5.199)\tData 0.002 (0.004)\tLoss 0.8453 (1.9896)\tPrec@1 82.000 (69.344)\tPrec@5 96.000 (83)\r\n\t2017-09-18 13:28:10,677 - INFO - Epoch: [0][122/190], lr: 0.01000\tTime 9.995 (5.238)\tData 0.002 (0.004)\tLoss 0.6531 (1.9787)\tPrec@1 92.000 (69.528)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:28:21,240 - INFO - Epoch: [0][123/190], lr: 0.01000\tTime 10.562 (5.281)\tData 0.002 (0.004)\tLoss 0.5329 (1.9671)\tPrec@1 88.000 (69.677)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:28:31,558 - INFO - Epoch: [0][124/190], lr: 0.01000\tTime 10.318 (5.321)\tData 0.002 (0.004)\tLoss 0.6132 (1.9562)\tPrec@1 90.000 (69.840)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:28:42,142 - INFO - Epoch: [0][125/190], lr: 0.01000\tTime 10.585 (5.363)\tData 0.002 (0.004)\tLoss 0.4761 (1.9445)\tPrec@1 96.000 (70.048)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:28:52,802 - INFO - Epoch: [0][126/190], lr: 0.01000\tTime 10.660 (5.404)\tData 0.002 (0.004)\tLoss 0.4652 (1.9328)\tPrec@1 96.000 (70.252)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:29:03,661 - INFO - Epoch: [0][127/190], lr: 0.01000\tTime 10.859 (5.447)\tData 0.003 (0.004)\tLoss 0.4837 (1.9215)\tPrec@1 98.000 (70.469)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:29:14,862 - INFO - Epoch: [0][128/190], lr: 0.01000\tTime 11.201 (5.492)\tData 0.003 (0.004)\tLoss 0.4328 (1.9100)\tPrec@1 96.000 (70.667)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:29:25,954 - INFO - Epoch: [0][129/190], lr: 0.01000\tTime 11.092 (5.535)\tData 0.002 (0.004)\tLoss 0.6569 (1.9003)\tPrec@1 90.000 (70.815)\tPrec@5 96.000 (84)\r\n\t2017-09-18 13:29:36,764 - INFO - Epoch: [0][130/190], lr: 0.01000\tTime 10.810 (5.575)\tData 0.004 (0.004)\tLoss 0.4948 (1.8896)\tPrec@1 94.000 (70.992)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:29:47,688 - INFO - Epoch: [0][131/190], lr: 0.01000\tTime 10.923 (5.615)\tData 0.002 (0.004)\tLoss 0.4707 (1.8789)\tPrec@1 94.000 (71.167)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:29:58,851 - INFO - Epoch: [0][132/190], lr: 0.01000\tTime 11.164 (5.657)\tData 0.006 (0.004)\tLoss 0.5153 (1.8686)\tPrec@1 92.000 (71.323)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:30:10,097 - INFO - Epoch: [0][133/190], lr: 0.01000\tTime 11.246 (5.699)\tData 0.002 (0.004)\tLoss 0.4860 (1.8583)\tPrec@1 94.000 (71.493)\tPrec@5 98.000 (85)\r\n\t2017-09-18 13:30:21,123 - INFO - Epoch: [0][134/190], lr: 0.01000\tTime 11.026 (5.738)\tData 0.002 (0.004)\tLoss 0.4441 (1.8478)\tPrec@1 98.000 (71.689)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:30:32,364 - INFO - Epoch: [0][135/190], lr: 0.01000\tTime 11.242 (5.779)\tData 0.002 (0.004)\tLoss 0.6027 (1.8387)\tPrec@1 94.000 (71.853)\tPrec@5 98.000 (85)\r\n\t2017-09-18 13:30:43,511 - INFO - Epoch: [0][136/190], lr: 0.01000\tTime 11.147 (5.818)\tData 0.002 (0.004)\tLoss 0.4439 (1.8285)\tPrec@1 94.000 (72.015)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:30:56,065 - INFO - Epoch: [0][137/190], lr: 0.01000\tTime 12.553 (5.867)\tData 0.002 (0.004)\tLoss 0.4715 (1.8187)\tPrec@1 92.000 (72.159)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:31:07,832 - INFO - Epoch: [0][138/190], lr: 0.01000\tTime 11.768 (5.909)\tData 0.003 (0.004)\tLoss 0.4727 (1.8090)\tPrec@1 96.000 (72.331)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:31:19,450 - INFO - Epoch: [0][139/190], lr: 0.01000\tTime 11.618 (5.950)\tData 0.002 (0.004)\tLoss 0.4232 (1.7991)\tPrec@1 96.000 (72.500)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:31:31,482 - INFO - Epoch: [0][140/190], lr: 0.01000\tTime 12.032 (5.993)\tData 0.002 (0.004)\tLoss 0.5206 (1.7900)\tPrec@1 92.000 (72.638)\tPrec@5 98.000 (85)\r\n\t2017-09-18 13:31:42,916 - INFO - Epoch: [0][141/190], lr: 0.01000\tTime 11.434 (6.032)\tData 0.002 (0.003)\tLoss 0.3838 (1.7801)\tPrec@1 98.000 (72.817)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:31:54,853 - INFO - Epoch: [0][142/190], lr: 0.01000\tTime 11.936 (6.073)\tData 0.002 (0.003)\tLoss 0.6089 (1.7719)\tPrec@1 90.000 (72.937)\tPrec@5 98.000 (86)\r\n\t2017-09-18 13:32:06,966 - INFO - Epoch: [0][143/190], lr: 0.01000\tTime 12.113 (6.115)\tData 0.002 (0.003)\tLoss 0.4373 (1.7626)\tPrec@1 96.000 (73.097)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:32:20,037 - INFO - Epoch: [0][144/190], lr: 0.01000\tTime 13.070 (6.163)\tData 0.002 (0.003)\tLoss 0.5992 (1.7546)\tPrec@1 86.000 (73.186)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:32:33,020 - INFO - Epoch: [0][145/190], lr: 0.01000\tTime 12.983 (6.209)\tData 0.002 (0.003)\tLoss 0.4786 (1.7459)\tPrec@1 90.000 (73.301)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:32:44,395 - INFO - Epoch: [0][146/190], lr: 0.01000\tTime 11.375 (6.245)\tData 0.002 (0.003)\tLoss 0.3436 (1.7363)\tPrec@1 100.000 (73.483)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:32:55,576 - INFO - Epoch: [0][147/190], lr: 0.01000\tTime 11.181 (6.278)\tData 0.002 (0.003)\tLoss 0.3963 (1.7273)\tPrec@1 98.000 (73.649)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:33:07,442 - INFO - Epoch: [0][148/190], lr: 0.01000\tTime 11.867 (6.315)\tData 0.002 (0.003)\tLoss 0.4661 (1.7188)\tPrec@1 92.000 (73.772)\tPrec@5 98.000 (86)\r\n\t2017-09-18 13:33:19,375 - INFO - Epoch: [0][149/190], lr: 0.01000\tTime 11.932 (6.353)\tData 0.002 (0.003)\tLoss 0.4874 (1.7106)\tPrec@1 96.000 (73.920)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:33:31,265 - INFO - Epoch: [0][150/190], lr: 0.01000\tTime 11.890 (6.390)\tData 0.002 (0.003)\tLoss 0.3156 (1.7014)\tPrec@1 98.000 (74.079)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:33:42,987 - INFO - Epoch: [0][151/190], lr: 0.01000\tTime 11.722 (6.425)\tData 0.002 (0.003)\tLoss 0.2942 (1.6921)\tPrec@1 100.000 (74.250)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:33:55,927 - INFO - Epoch: [0][152/190], lr: 0.01000\tTime 12.940 (6.467)\tData 0.002 (0.003)\tLoss 0.4054 (1.6837)\tPrec@1 96.000 (74.392)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:34:08,886 - INFO - Epoch: [0][153/190], lr: 0.01000\tTime 12.959 (6.509)\tData 0.002 (0.003)\tLoss 0.5405 (1.6763)\tPrec@1 92.000 (74.506)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:34:21,638 - INFO - Epoch: [0][154/190], lr: 0.01000\tTime 12.752 (6.550)\tData 0.002 (0.003)\tLoss 0.4229 (1.6682)\tPrec@1 94.000 (74.632)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:34:34,436 - INFO - Epoch: [0][155/190], lr: 0.01000\tTime 12.798 (6.590)\tData 0.004 (0.003)\tLoss 0.4031 (1.6601)\tPrec@1 96.000 (74.769)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:34:47,209 - INFO - Epoch: [0][156/190], lr: 0.01000\tTime 12.773 (6.629)\tData 0.002 (0.003)\tLoss 0.4090 (1.6521)\tPrec@1 94.000 (74.892)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:34:59,683 - INFO - Epoch: [0][157/190], lr: 0.01000\tTime 12.474 (6.666)\tData 0.003 (0.003)\tLoss 0.3058 (1.6436)\tPrec@1 100.000 (75.051)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:35:12,320 - INFO - Epoch: [0][158/190], lr: 0.01000\tTime 12.636 (6.704)\tData 0.002 (0.003)\tLoss 0.5007 (1.6364)\tPrec@1 92.000 (75.157)\tPrec@5 98.000 (87)\r\n\t2017-09-18 13:35:24,694 - INFO - Epoch: [0][159/190], lr: 0.01000\tTime 12.374 (6.739)\tData 0.003 (0.003)\tLoss 0.4987 (1.6293)\tPrec@1 92.000 (75.263)\tPrec@5 98.000 (87)\r\n\t2017-09-18 13:35:37,976 - INFO - Epoch: [0][160/190], lr: 0.01000\tTime 13.282 (6.780)\tData 0.002 (0.003)\tLoss 0.2993 (1.6210)\tPrec@1 98.000 (75.404)\tPrec@5 100.000 (8)\r\n\t2017-09-18 13:35:51,475 - INFO - Epoch: [0][161/190], lr: 0.01000\tTime 13.499 (6.821)\tData 0.002 (0.003)\tLoss 0.5275 (1.6143)\tPrec@1 88.000 (75.481)\tPrec@5 96.000 (87)\r\n\t2017-09-18 13:36:05,019 - INFO - Epoch: [0][162/190], lr: 0.01000\tTime 13.545 (6.862)\tData 0.0\r\n\r\n\r\nSo, why did this happen?   Anyone who also ever met this could give me a hand?  Many thanks!"}