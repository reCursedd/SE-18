{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2038", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2038/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2038/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2038/events", "html_url": "https://github.com/pytorch/pytorch/issues/2038", "id": 241789630, "node_id": "MDU6SXNzdWUyNDE3ODk2MzA=", "number": 2038, "title": "Race condition in CUDA IndexCopy[Large/Small]Index kernels (Maybe others?)", "user": {"login": "csarofeen", "id": 22205833, "node_id": "MDQ6VXNlcjIyMjA1ODMz", "avatar_url": "https://avatars2.githubusercontent.com/u/22205833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/csarofeen", "html_url": "https://github.com/csarofeen", "followers_url": "https://api.github.com/users/csarofeen/followers", "following_url": "https://api.github.com/users/csarofeen/following{/other_user}", "gists_url": "https://api.github.com/users/csarofeen/gists{/gist_id}", "starred_url": "https://api.github.com/users/csarofeen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/csarofeen/subscriptions", "organizations_url": "https://api.github.com/users/csarofeen/orgs", "repos_url": "https://api.github.com/users/csarofeen/repos", "events_url": "https://api.github.com/users/csarofeen/events{/privacy}", "received_events_url": "https://api.github.com/users/csarofeen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-07-10T17:35:32Z", "updated_at": "2017-07-10T19:28:24Z", "closed_at": "2017-07-10T19:28:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There appears to be a race condition in the index CUDA kernels where multiple threads are writing to the same address.</p>\n<p>Reproducer code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\nref <span class=\"pl-k\">=</span> torch.arange(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">20</span>).view(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>).float()\n\nindexer<span class=\"pl-k\">=</span> [<span class=\"pl-c1\">slice</span>(<span class=\"pl-c1\">None</span>), [<span class=\"pl-c1\">1</span> <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">5</span>)]]\n\nsrc <span class=\"pl-k\">=</span> torch.randperm(ref[indexer].numel()).view(ref[indexer].size()).float()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>ref<span class=\"pl-pds\">'</span></span>, ref)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>indexer<span class=\"pl-pds\">'</span></span>, indexer)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>ref[indxer]<span class=\"pl-pds\">'</span></span>, ref[indexer])\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>src<span class=\"pl-pds\">'</span></span>, src)\n\ncpu_ans <span class=\"pl-k\">=</span> ref.clone()\ncpu_ans[indexer]<span class=\"pl-k\">=</span>src\n\nnump_ans <span class=\"pl-k\">=</span> ref.double().numpy()\nidxs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">tuple</span>(indexer)\nnump_ans[idxs] <span class=\"pl-k\">=</span> src.double().numpy()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>ref numpy<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>, ref.numpy())\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>idx numpy<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>, idxs)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>src numpy<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>, src.numpy())\n\ngpu_ans <span class=\"pl-k\">=</span> ref.cuda()\ngpu_ans[indexer]<span class=\"pl-k\">=</span>src.cuda()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu result<span class=\"pl-pds\">'</span></span>, cpu_ans)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>numpy result<span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>, nump_ans)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cuda result<span class=\"pl-pds\">'</span></span>, gpu_ans)</pre></div>\n<p>Output:</p>\n<pre><code>cpu result \n  0   6   2   3   4\n  5   4   7   8   9\n 10  13  12  13  14\n 15  19  17  18  19\n[torch.FloatTensor of size 4x5]\nnumpy result\n [[  0.   6.   2.   3.   4.]\n  [  5.   4.   7.   8.   9.]\n  [ 10.  13.  12.  13.  14.]\n  [ 15.  19.  17.  18.  19.]]\ncuda result \n  0     6     2   3   4\n  5  **16**   7   8   9\n 10    13    12  13  14\n 15   **5**  17  18  19\n[torch.cuda.FloatTensor of size 4x5 (GPU 0)]\n</code></pre>\n<p>Looks like  a preprocessing step will need to be added to identify which source indices is allowed to write to destination.</p>\n<p>I'm not sure if this will also be a problem in the other advanced indexing kernels.</p>", "body_text": "There appears to be a race condition in the index CUDA kernels where multiple threads are writing to the same address.\nReproducer code:\nimport torch\n\nref = torch.arange(0, 20).view(4, 5).float()\n\nindexer= [slice(None), [1 for i in range(5)]]\n\nsrc = torch.randperm(ref[indexer].numel()).view(ref[indexer].size()).float()\n\nprint('ref', ref)\nprint('indexer', indexer)\nprint('ref[indxer]', ref[indexer])\nprint('src', src)\n\ncpu_ans = ref.clone()\ncpu_ans[indexer]=src\n\nnump_ans = ref.double().numpy()\nidxs = tuple(indexer)\nnump_ans[idxs] = src.double().numpy()\n\nprint('ref numpy\\n', ref.numpy())\nprint('idx numpy\\n', idxs)\nprint('src numpy\\n', src.numpy())\n\ngpu_ans = ref.cuda()\ngpu_ans[indexer]=src.cuda()\n\nprint('cpu result', cpu_ans)\nprint('numpy result\\n', nump_ans)\nprint('cuda result', gpu_ans)\nOutput:\ncpu result \n  0   6   2   3   4\n  5   4   7   8   9\n 10  13  12  13  14\n 15  19  17  18  19\n[torch.FloatTensor of size 4x5]\nnumpy result\n [[  0.   6.   2.   3.   4.]\n  [  5.   4.   7.   8.   9.]\n  [ 10.  13.  12.  13.  14.]\n  [ 15.  19.  17.  18.  19.]]\ncuda result \n  0     6     2   3   4\n  5  **16**   7   8   9\n 10    13    12  13  14\n 15   **5**  17  18  19\n[torch.cuda.FloatTensor of size 4x5 (GPU 0)]\n\nLooks like  a preprocessing step will need to be added to identify which source indices is allowed to write to destination.\nI'm not sure if this will also be a problem in the other advanced indexing kernels.", "body": "There appears to be a race condition in the index CUDA kernels where multiple threads are writing to the same address.\r\n\r\nReproducer code:\r\n\r\n```.py\r\nimport torch\r\n\r\nref = torch.arange(0, 20).view(4, 5).float()\r\n\r\nindexer= [slice(None), [1 for i in range(5)]]\r\n\r\nsrc = torch.randperm(ref[indexer].numel()).view(ref[indexer].size()).float()\r\n\r\nprint('ref', ref)\r\nprint('indexer', indexer)\r\nprint('ref[indxer]', ref[indexer])\r\nprint('src', src)\r\n\r\ncpu_ans = ref.clone()\r\ncpu_ans[indexer]=src\r\n\r\nnump_ans = ref.double().numpy()\r\nidxs = tuple(indexer)\r\nnump_ans[idxs] = src.double().numpy()\r\n\r\nprint('ref numpy\\n', ref.numpy())\r\nprint('idx numpy\\n', idxs)\r\nprint('src numpy\\n', src.numpy())\r\n\r\ngpu_ans = ref.cuda()\r\ngpu_ans[indexer]=src.cuda()\r\n\r\nprint('cpu result', cpu_ans)\r\nprint('numpy result\\n', nump_ans)\r\nprint('cuda result', gpu_ans)\r\n```\r\n\r\nOutput:\r\n```\r\ncpu result \r\n  0   6   2   3   4\r\n  5   4   7   8   9\r\n 10  13  12  13  14\r\n 15  19  17  18  19\r\n[torch.FloatTensor of size 4x5]\r\nnumpy result\r\n [[  0.   6.   2.   3.   4.]\r\n  [  5.   4.   7.   8.   9.]\r\n  [ 10.  13.  12.  13.  14.]\r\n  [ 15.  19.  17.  18.  19.]]\r\ncuda result \r\n  0     6     2   3   4\r\n  5  **16**   7   8   9\r\n 10    13    12  13  14\r\n 15   **5**  17  18  19\r\n[torch.cuda.FloatTensor of size 4x5 (GPU 0)]\r\n```\r\n\r\nLooks like  a preprocessing step will need to be added to identify which source indices is allowed to write to destination.\r\n\r\nI'm not sure if this will also be a problem in the other advanced indexing kernels."}