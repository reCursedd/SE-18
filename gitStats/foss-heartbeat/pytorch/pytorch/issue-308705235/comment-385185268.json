{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/385185268", "html_url": "https://github.com/pytorch/pytorch/issues/6013#issuecomment-385185268", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6013", "id": 385185268, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTE4NTI2OA==", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-28T15:44:02Z", "updated_at": "2018-04-28T15:44:02Z", "author_association": "COLLABORATOR", "body_html": "<p>With <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"312252805\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/6392\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6392/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/6392\">#6392</a>, which should be merged soon, it is now going to be possible to do:</p>\n<pre><code>script = '''\ndef func(a, b):\n    if sum(a) &gt; 2.0:\n        c = a + b\n    else:\n        c = a\n    return c\n'''\n\ncu = torch.jit.CompilationUnit(script)\n\ngraph = cu.module._get_method('func').graph\n\n# export the graph in an ONNX protobuf, but with original IR ops (not translated to ONNX ops)\nproto, _ = graph.export([], 0, export_raw_ir=True)\n\n# import the IR ops back and return the graph (together with a list of initializer tensors, empty in this case)\ngraph2, _ = torch._C._jit_import_graph(proto)\n\nprint(graph2)\n</code></pre>\n<pre><code>graph(%0 : Dynamic\n      %1 : Dynamic) {\n  %2 : Dynamic = aten::sum(%0)\n  %3 : Dynamic = prim::Constant[value={2}]()\n  %4 : Dynamic = aten::gt(%2, %3)\n  %5 : Dynamic = prim::If(%4)\n    block0() {\n      %6 : Dynamic = aten::add[alpha={1}](%0, %1)\n      -&gt; (%6)\n    }\n    block1() {\n      -&gt; (%0)\n    }\n  return (%5);\n}\n</code></pre>", "body_text": "With #6392, which should be merged soon, it is now going to be possible to do:\nscript = '''\ndef func(a, b):\n    if sum(a) > 2.0:\n        c = a + b\n    else:\n        c = a\n    return c\n'''\n\ncu = torch.jit.CompilationUnit(script)\n\ngraph = cu.module._get_method('func').graph\n\n# export the graph in an ONNX protobuf, but with original IR ops (not translated to ONNX ops)\nproto, _ = graph.export([], 0, export_raw_ir=True)\n\n# import the IR ops back and return the graph (together with a list of initializer tensors, empty in this case)\ngraph2, _ = torch._C._jit_import_graph(proto)\n\nprint(graph2)\n\ngraph(%0 : Dynamic\n      %1 : Dynamic) {\n  %2 : Dynamic = aten::sum(%0)\n  %3 : Dynamic = prim::Constant[value={2}]()\n  %4 : Dynamic = aten::gt(%2, %3)\n  %5 : Dynamic = prim::If(%4)\n    block0() {\n      %6 : Dynamic = aten::add[alpha={1}](%0, %1)\n      -> (%6)\n    }\n    block1() {\n      -> (%0)\n    }\n  return (%5);\n}", "body": "With #6392, which should be merged soon, it is now going to be possible to do:\r\n\r\n```\r\nscript = '''\r\ndef func(a, b):\r\n    if sum(a) > 2.0:\r\n        c = a + b\r\n    else:\r\n        c = a\r\n    return c\r\n'''\r\n\r\ncu = torch.jit.CompilationUnit(script)\r\n\r\ngraph = cu.module._get_method('func').graph\r\n\r\n# export the graph in an ONNX protobuf, but with original IR ops (not translated to ONNX ops)\r\nproto, _ = graph.export([], 0, export_raw_ir=True)\r\n\r\n# import the IR ops back and return the graph (together with a list of initializer tensors, empty in this case)\r\ngraph2, _ = torch._C._jit_import_graph(proto)\r\n\r\nprint(graph2)\r\n```\r\n\r\n```\r\ngraph(%0 : Dynamic\r\n      %1 : Dynamic) {\r\n  %2 : Dynamic = aten::sum(%0)\r\n  %3 : Dynamic = prim::Constant[value={2}]()\r\n  %4 : Dynamic = aten::gt(%2, %3)\r\n  %5 : Dynamic = prim::If(%4)\r\n    block0() {\r\n      %6 : Dynamic = aten::add[alpha={1}](%0, %1)\r\n      -> (%6)\r\n    }\r\n    block1() {\r\n      -> (%0)\r\n    }\r\n  return (%5);\r\n}\r\n```"}