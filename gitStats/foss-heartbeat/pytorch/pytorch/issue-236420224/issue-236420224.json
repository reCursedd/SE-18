{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1820", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1820/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1820/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1820/events", "html_url": "https://github.com/pytorch/pytorch/issues/1820", "id": 236420224, "node_id": "MDU6SXNzdWUyMzY0MjAyMjQ=", "number": 1820, "title": "pack_padded_sequence output is not as expected", "user": {"login": "stefbraun", "id": 13469638, "node_id": "MDQ6VXNlcjEzNDY5NjM4", "avatar_url": "https://avatars0.githubusercontent.com/u/13469638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefbraun", "html_url": "https://github.com/stefbraun", "followers_url": "https://api.github.com/users/stefbraun/followers", "following_url": "https://api.github.com/users/stefbraun/following{/other_user}", "gists_url": "https://api.github.com/users/stefbraun/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefbraun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefbraun/subscriptions", "organizations_url": "https://api.github.com/users/stefbraun/orgs", "repos_url": "https://api.github.com/users/stefbraun/repos", "events_url": "https://api.github.com/users/stefbraun/events{/privacy}", "received_events_url": "https://api.github.com/users/stefbraun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-16T08:50:53Z", "updated_at": "2017-06-17T08:03:00Z", "closed_at": "2017-06-17T08:03:00Z", "author_association": "NONE", "body_html": "<p>Hi, <code>pack_padded_sequence</code> creates a <code>Packed Sequence</code> object with <code>(data, batch_sizes)</code>. The way how <code>data</code> is created does not meet (standard?) expectations: instead of concatenating samples from the batch without padding, it seems to do something else, see the plot below.</p>\n<p>In fact, if I were to run a fully connected layer on <code>data</code> - e.g. to map a RNN output to classes when using CTC - the structure of <code>data</code> would not make sense. I have a script that produces a plot to showcase the behaviour. Can you comment and possibly clarify the documentation if this is not a bug?<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13469638/27219641-b7b8cbec-5282-11e7-8a57-805d149792f2.png\"><img src=\"https://user-images.githubusercontent.com/13469638/27219641-b7b8cbec-5282-11e7-8a57-805d149792f2.png\" alt=\"issue\" style=\"max-width:100%;\"></a></p>\n<pre><code>import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.autograd import Variable\n\n# Get data\nsingle_sample = np.array([[1, 2, 3, 0, 0], [1, 2, 3, 0, 0], [1, 2, 3, 0, 0], [1, 2, 3, 0, 0]], dtype='float32')  # (features, max_len)\nsingle_sample = single_sample.T  # (max_len, features)\nbatch = np.array([single_sample] * 3)  # (batch_size, max_len, features)\nlengths = np.array([3, 3, 3])\n\n# Move to pytorch\nbatch_pt = Variable(torch.from_numpy(batch))\n\n# Pack the sequence\nxp = pack_padded_sequence(batch_pt, lengths, batch_first=True)\n\n# Expected output of xp.data\nxp_exp = np.array([single_sample[:3, :]] * 3)  # (batch_size, max_len, features)\nxp_exp = np.reshape(xp_exp, (9, -1))\n\n# Plot\nfig, (ax1, ax2, ax3) = plt.subplots(3)\n# ax1\nim1 = ax1.imshow(single_sample.T, aspect='auto', interpolation='none', vmin=0, vmax=3)\nfig.colorbar(im1, ax=ax1)\nax1.set_title('Batch contains three of these single samples')\nax1.set_xlabel('Time')\nax1.set_ylabel('Features')\n# ax2\nim2 = ax2.imshow(xp.data.data.numpy().T, aspect='auto', interpolation='none', vmin=0, vmax=3)\nfig.colorbar(im2, ax=ax2)\nax2.set_title('Pytorch packed_sequence.data')\nax2.set_xlabel('Time')\nax2.set_ylabel('Features')\n# ax3\nim3 = ax3.imshow(xp_exp.T, aspect='auto', interpolation='none', vmin=0, vmax=3)\nfig.colorbar(im3, ax=ax3)\nax3.set_title('Expected packed_sequence.data')\nax3.set_xlabel('Time')\nax3.set_ylabel('Features')\n# Other\nplt.tight_layout()\nplt.show()\n</code></pre>", "body_text": "Hi, pack_padded_sequence creates a Packed Sequence object with (data, batch_sizes). The way how data is created does not meet (standard?) expectations: instead of concatenating samples from the batch without padding, it seems to do something else, see the plot below.\nIn fact, if I were to run a fully connected layer on data - e.g. to map a RNN output to classes when using CTC - the structure of data would not make sense. I have a script that produces a plot to showcase the behaviour. Can you comment and possibly clarify the documentation if this is not a bug?\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.autograd import Variable\n\n# Get data\nsingle_sample = np.array([[1, 2, 3, 0, 0], [1, 2, 3, 0, 0], [1, 2, 3, 0, 0], [1, 2, 3, 0, 0]], dtype='float32')  # (features, max_len)\nsingle_sample = single_sample.T  # (max_len, features)\nbatch = np.array([single_sample] * 3)  # (batch_size, max_len, features)\nlengths = np.array([3, 3, 3])\n\n# Move to pytorch\nbatch_pt = Variable(torch.from_numpy(batch))\n\n# Pack the sequence\nxp = pack_padded_sequence(batch_pt, lengths, batch_first=True)\n\n# Expected output of xp.data\nxp_exp = np.array([single_sample[:3, :]] * 3)  # (batch_size, max_len, features)\nxp_exp = np.reshape(xp_exp, (9, -1))\n\n# Plot\nfig, (ax1, ax2, ax3) = plt.subplots(3)\n# ax1\nim1 = ax1.imshow(single_sample.T, aspect='auto', interpolation='none', vmin=0, vmax=3)\nfig.colorbar(im1, ax=ax1)\nax1.set_title('Batch contains three of these single samples')\nax1.set_xlabel('Time')\nax1.set_ylabel('Features')\n# ax2\nim2 = ax2.imshow(xp.data.data.numpy().T, aspect='auto', interpolation='none', vmin=0, vmax=3)\nfig.colorbar(im2, ax=ax2)\nax2.set_title('Pytorch packed_sequence.data')\nax2.set_xlabel('Time')\nax2.set_ylabel('Features')\n# ax3\nim3 = ax3.imshow(xp_exp.T, aspect='auto', interpolation='none', vmin=0, vmax=3)\nfig.colorbar(im3, ax=ax3)\nax3.set_title('Expected packed_sequence.data')\nax3.set_xlabel('Time')\nax3.set_ylabel('Features')\n# Other\nplt.tight_layout()\nplt.show()", "body": "Hi, ```pack_padded_sequence``` creates a ```Packed Sequence``` object with ```(data, batch_sizes)```. The way how ```data``` is created does not meet (standard?) expectations: instead of concatenating samples from the batch without padding, it seems to do something else, see the plot below.\r\n\r\nIn fact, if I were to run a fully connected layer on ```data``` - e.g. to map a RNN output to classes when using CTC - the structure of ```data``` would not make sense. I have a script that produces a plot to showcase the behaviour. Can you comment and possibly clarify the documentation if this is not a bug?\r\n![issue](https://user-images.githubusercontent.com/13469638/27219641-b7b8cbec-5282-11e7-8a57-805d149792f2.png)\r\n\r\n```\r\nimport torch\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\r\nfrom torch.autograd import Variable\r\n\r\n# Get data\r\nsingle_sample = np.array([[1, 2, 3, 0, 0], [1, 2, 3, 0, 0], [1, 2, 3, 0, 0], [1, 2, 3, 0, 0]], dtype='float32')  # (features, max_len)\r\nsingle_sample = single_sample.T  # (max_len, features)\r\nbatch = np.array([single_sample] * 3)  # (batch_size, max_len, features)\r\nlengths = np.array([3, 3, 3])\r\n\r\n# Move to pytorch\r\nbatch_pt = Variable(torch.from_numpy(batch))\r\n\r\n# Pack the sequence\r\nxp = pack_padded_sequence(batch_pt, lengths, batch_first=True)\r\n\r\n# Expected output of xp.data\r\nxp_exp = np.array([single_sample[:3, :]] * 3)  # (batch_size, max_len, features)\r\nxp_exp = np.reshape(xp_exp, (9, -1))\r\n\r\n# Plot\r\nfig, (ax1, ax2, ax3) = plt.subplots(3)\r\n# ax1\r\nim1 = ax1.imshow(single_sample.T, aspect='auto', interpolation='none', vmin=0, vmax=3)\r\nfig.colorbar(im1, ax=ax1)\r\nax1.set_title('Batch contains three of these single samples')\r\nax1.set_xlabel('Time')\r\nax1.set_ylabel('Features')\r\n# ax2\r\nim2 = ax2.imshow(xp.data.data.numpy().T, aspect='auto', interpolation='none', vmin=0, vmax=3)\r\nfig.colorbar(im2, ax=ax2)\r\nax2.set_title('Pytorch packed_sequence.data')\r\nax2.set_xlabel('Time')\r\nax2.set_ylabel('Features')\r\n# ax3\r\nim3 = ax3.imshow(xp_exp.T, aspect='auto', interpolation='none', vmin=0, vmax=3)\r\nfig.colorbar(im3, ax=ax3)\r\nax3.set_title('Expected packed_sequence.data')\r\nax3.set_xlabel('Time')\r\nax3.set_ylabel('Features')\r\n# Other\r\nplt.tight_layout()\r\nplt.show()\r\n```\r\n"}