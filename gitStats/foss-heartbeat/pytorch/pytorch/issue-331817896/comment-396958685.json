{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396958685", "html_url": "https://github.com/pytorch/pytorch/issues/8410#issuecomment-396958685", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/8410", "id": 396958685, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Njk1ODY4NQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-13T14:31:06Z", "updated_at": "2018-06-13T14:31:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This sounds like it should work. It's also interesting to note that existing formulas with multiple inputs, e.g., as in convolution double backwards (in <code>aten/src/ATen/native/Convolution.cpp</code>) already handle undefined inputs for <code>ggI</code>, <code>ggO</code>, etc. correctly.</p>\n<p>That being said:</p>\n<blockquote>\n<p>assuming that the backwards function has a single input and multiple outputs (which is the case for all of the ops we currently handle, and 90%+ of autograd formulas in general), we can treat AutogradZero as a poison value -- a single input that is AutogradZero makes an op's output become AutogradZero.</p>\n</blockquote>\n<p>You shouldn't assume this if you want double backwards to work :)</p>\n<blockquote>\n<p>To ensure we only generate AutogradZero in gradient graphs, the autodiff can introduce autogradZeroIfUndef nodes for any undefined input, which will be replaced during the autogradZero propagation pass with autogradZero.</p>\n</blockquote>\n<p>I would rather we encode linearity as an intrinsic property of operations, so that we can apply this optimization more widely (not just to backward graphs). It seems dangerous to assume that <em>any</em> computation reachable from <code>AutogradZero</code> is eligible for zeroing.</p>", "body_text": "This sounds like it should work. It's also interesting to note that existing formulas with multiple inputs, e.g., as in convolution double backwards (in aten/src/ATen/native/Convolution.cpp) already handle undefined inputs for ggI, ggO, etc. correctly.\nThat being said:\n\nassuming that the backwards function has a single input and multiple outputs (which is the case for all of the ops we currently handle, and 90%+ of autograd formulas in general), we can treat AutogradZero as a poison value -- a single input that is AutogradZero makes an op's output become AutogradZero.\n\nYou shouldn't assume this if you want double backwards to work :)\n\nTo ensure we only generate AutogradZero in gradient graphs, the autodiff can introduce autogradZeroIfUndef nodes for any undefined input, which will be replaced during the autogradZero propagation pass with autogradZero.\n\nI would rather we encode linearity as an intrinsic property of operations, so that we can apply this optimization more widely (not just to backward graphs). It seems dangerous to assume that any computation reachable from AutogradZero is eligible for zeroing.", "body": "This sounds like it should work. It's also interesting to note that existing formulas with multiple inputs, e.g., as in convolution double backwards (in `aten/src/ATen/native/Convolution.cpp`) already handle undefined inputs for `ggI`, `ggO`, etc. correctly.\r\n\r\nThat being said:\r\n\r\n> assuming that the backwards function has a single input and multiple outputs (which is the case for all of the ops we currently handle, and 90%+ of autograd formulas in general), we can treat AutogradZero as a poison value -- a single input that is AutogradZero makes an op's output become AutogradZero.\r\n\r\nYou shouldn't assume this if you want double backwards to work :)\r\n\r\n> To ensure we only generate AutogradZero in gradient graphs, the autodiff can introduce autogradZeroIfUndef nodes for any undefined input, which will be replaced during the autogradZero propagation pass with autogradZero.\r\n\r\nI would rather we encode linearity as an intrinsic property of operations, so that we can apply this optimization more widely (not just to backward graphs). It seems dangerous to assume that *any* computation reachable from `AutogradZero` is eligible for zeroing."}