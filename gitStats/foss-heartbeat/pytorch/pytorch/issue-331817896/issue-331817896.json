{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8410", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8410/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8410/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8410/events", "html_url": "https://github.com/pytorch/pytorch/issues/8410", "id": 331817896, "node_id": "MDU6SXNzdWUzMzE4MTc4OTY=", "number": 8410, "title": "[jit] Remove symbolic autodiff's reliance on shape information", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-06-13T02:22:24Z", "updated_at": "2018-07-10T16:23:39Z", "closed_at": "2018-07-10T16:23:39Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Problem</h3>\n<p>A value is output from a function. That value is never used in the program. When computing the gradient of that function, we get an undefined tensor as the gradient of that output. However, that value should actually be zero (it isn't used in the loss so dL/dV should be all zeros). Currently we have a node 'ReplaceIfUndef' that substitutes the zero tensor in. However, we need to know the shape of the output to generate the right shape in the zero tensor. This is not always possible to statically derive. We do not want to rely on the shape being available, nor do we want to add the bookkeeping to remember the shape between forward and backward.</p>\n<h3>Additional Background</h3>\n<p>We always specialize a different graph when a tensor is undefined, so<br>\nwe can perform graph optimizations knowing that certain inputs to the graph are undefined, including the optimization of the autograd graph when certain outputs are zeros.</p>\n<h3>Proposed Solution</h3>\n<p>If we have a function <code>y = f(x)</code> with jacobian <code>J</code>, the backwards of f is dx = J^t dy.  Note that because the backwards always implements this matrix multiply, we know that it maps an input vector of zeros to an output vector of zero regardless of what operations it choses to do inside to actually implement the matrix multiply (most use some optimized form and never generate J^t). Hence it is ok to replace any backward function with known-zero inputs with something that produces known-zero outputs.</p>\n<p>The entire backwards pass is composed of operators that either (1) compute a backwards function, or (2) sum the results of backward functions when something is used multiple times.</p>\n<p>We can introduce nodes before and after each backward function that gate the inputs and outputs to the op. If we can prove that <em>all</em> of the inputs to the op are Undef nodes, then we can remove the entire graph representing the op, replacing the outputs with Undef nodes. To do this systematically, we introduce two ops:</p>\n<ul>\n<li><code>y0, y1, y2, ... = UndefToPoison(x0, x1, x2...)</code> if all of the inputs are undef, the all of the outputs become poison values. Otherwise this op does nothing. For all ops, if a single input can be proven to be a poison value, then all of its outputs become poison values.</li>\n<li><code>y = PoisonToUndef(x)</code> if x is a poison value then y turns into an Undef. Otherwise this op does nothing. This is the one exception to the poison propagation described above.</li>\n</ul>\n<p>A backwards op graph for a single backward function <code>y0, y1... = f'(x0, x1...)</code> then will get transformed into:</p>\n<pre><code>x0', x1', ... = UndefToPoison(x0, x1, ...)\ny0', y1' = f'(x0', x1') # note f' represents a whole graph not necessarily a single op\ny0 = PoisonToUndef(y0')\ny1 = PoisonToUndef(y1')\n</code></pre>\n<p>In the event that all the inputs are Undef, then poison propagation will propagate through the entire body of <code>f'</code>, and the outputs will become Undefs. If some inputs are not Undef, then it will not propagate and the body will be retained as is. When generating this graph, we can use the property that <code>UndefToPoison(PoisonToUndef(x)) == x</code> to keep the graph simple.  With this simplification, ops that are simply composed together will not have these nodes inserted in between.</p>\n<p>A pass to propagate Undefs would perform the following actions, given an set of input values known to be Undefs:</p>\n<ul>\n<li>If all values into UndefToPoison are proven to be Undefs, it replaces the outputs of the op with Poison values.</li>\n<li>If a node that is not PoisonToUndef has one poison input value, it replaces the outputs of the with Poison values</li>\n<li>If the input PoisonToUndef is a poison value, it replaces the output with Undef</li>\n<li><code>add(Undef, x) -&gt; x</code>, <code>add(x, Undef) -&gt; x</code>, and <code>add(Undef, Undef) -&gt; Undef</code></li>\n</ul>\n<p>Given the fact that we always specialize inputs to be defined or undefined, functions with single output values can generate gradient graph that do not have to handle Undef inputs. Functions with multiple output will need to handle the gradient case where some but not all inputs may be Undef. This is already true of many of the backwards ops with multiple outputs.</p>\n<h3>Previous Proposed Solution</h3>\n<p>(this is similar to the above, but does not handle ops with multiple outputs well, and makes it hard to distinguish autograd addition from addition inside a single operator).</p>\n<p>We can introduce a 'AutogradZero', which represents a place where we know the value is zero but we do not know its shape, and define rules to propagate it through the graph. We have to handle propagating AutogradZero through both groups of statements that might appear:</p>\n<p>(1) a group of operators representing a backwards function (typically each backward has more than a single op to compute it. Only the composition of these operators is guaranteed to map 0 to 0).<br>\n(2) an addition of the outputs of (1)</p>\n<p>For (1), assuming that the backwards function has a single input and multiple outputs (which is the case for all of the ops we currently handle, and 90%+ of autograd formulas in general), we can treat AutogradZero as a poison value -- a single input that is AutogradZero makes an op's output become AutogradZero. This will propagate through the entire formula of a group of operators representing a backward. If we ever have to write a formula with multiple inputs, we can insert ops that change autogradzero back to undef and allow the op to handle the behavior on its own. To handle (2), we need to distinguish between additions that appear inside of a backward function (which should propagate the AutogradeZero) and those which sum the derivatives from multiple uses of a value (which should drop any AutogradZeros). We can do this by introducing a special AutogradAdd op to indicate case (2), which is removed when we do the propagation pass of AutogradZeros.</p>\n<p>Finally, it is important that the AutogradZero token is only propagated when the graph actually represents a gradient. The poison-based propagation only works because we know that the the backwards functions are implementing a matrix multiply, which maps 0 to 0. To ensure we only generate AutogradZero in gradient graphs, the autodiff can introduce autogradZeroIfUndef nodes for any undefined input, which will be replaced during the autogradZero propagation pass with autogradZero.</p>\n<p>Alternative considered: An alternative formulation would guard each output of every backward function with <code>propagateUndef(v, input)</code> which would replace the computed output <code>v</code> with undef if input is undef. This would replace the \"poison\" value AutogradZero in the above formulation. However this has the nasty property that it inserts a huge number of <code>propagateUndef</code> nodes, which makes reading and debugging the backward output much harder. In contrast, the poison-value formulation only inserts autogradZeroIfUndef nodes on the inputs to the graph and the body of the graph remains the same.</p>", "body_text": "Problem\nA value is output from a function. That value is never used in the program. When computing the gradient of that function, we get an undefined tensor as the gradient of that output. However, that value should actually be zero (it isn't used in the loss so dL/dV should be all zeros). Currently we have a node 'ReplaceIfUndef' that substitutes the zero tensor in. However, we need to know the shape of the output to generate the right shape in the zero tensor. This is not always possible to statically derive. We do not want to rely on the shape being available, nor do we want to add the bookkeeping to remember the shape between forward and backward.\nAdditional Background\nWe always specialize a different graph when a tensor is undefined, so\nwe can perform graph optimizations knowing that certain inputs to the graph are undefined, including the optimization of the autograd graph when certain outputs are zeros.\nProposed Solution\nIf we have a function y = f(x) with jacobian J, the backwards of f is dx = J^t dy.  Note that because the backwards always implements this matrix multiply, we know that it maps an input vector of zeros to an output vector of zero regardless of what operations it choses to do inside to actually implement the matrix multiply (most use some optimized form and never generate J^t). Hence it is ok to replace any backward function with known-zero inputs with something that produces known-zero outputs.\nThe entire backwards pass is composed of operators that either (1) compute a backwards function, or (2) sum the results of backward functions when something is used multiple times.\nWe can introduce nodes before and after each backward function that gate the inputs and outputs to the op. If we can prove that all of the inputs to the op are Undef nodes, then we can remove the entire graph representing the op, replacing the outputs with Undef nodes. To do this systematically, we introduce two ops:\n\ny0, y1, y2, ... = UndefToPoison(x0, x1, x2...) if all of the inputs are undef, the all of the outputs become poison values. Otherwise this op does nothing. For all ops, if a single input can be proven to be a poison value, then all of its outputs become poison values.\ny = PoisonToUndef(x) if x is a poison value then y turns into an Undef. Otherwise this op does nothing. This is the one exception to the poison propagation described above.\n\nA backwards op graph for a single backward function y0, y1... = f'(x0, x1...) then will get transformed into:\nx0', x1', ... = UndefToPoison(x0, x1, ...)\ny0', y1' = f'(x0', x1') # note f' represents a whole graph not necessarily a single op\ny0 = PoisonToUndef(y0')\ny1 = PoisonToUndef(y1')\n\nIn the event that all the inputs are Undef, then poison propagation will propagate through the entire body of f', and the outputs will become Undefs. If some inputs are not Undef, then it will not propagate and the body will be retained as is. When generating this graph, we can use the property that UndefToPoison(PoisonToUndef(x)) == x to keep the graph simple.  With this simplification, ops that are simply composed together will not have these nodes inserted in between.\nA pass to propagate Undefs would perform the following actions, given an set of input values known to be Undefs:\n\nIf all values into UndefToPoison are proven to be Undefs, it replaces the outputs of the op with Poison values.\nIf a node that is not PoisonToUndef has one poison input value, it replaces the outputs of the with Poison values\nIf the input PoisonToUndef is a poison value, it replaces the output with Undef\nadd(Undef, x) -> x, add(x, Undef) -> x, and add(Undef, Undef) -> Undef\n\nGiven the fact that we always specialize inputs to be defined or undefined, functions with single output values can generate gradient graph that do not have to handle Undef inputs. Functions with multiple output will need to handle the gradient case where some but not all inputs may be Undef. This is already true of many of the backwards ops with multiple outputs.\nPrevious Proposed Solution\n(this is similar to the above, but does not handle ops with multiple outputs well, and makes it hard to distinguish autograd addition from addition inside a single operator).\nWe can introduce a 'AutogradZero', which represents a place where we know the value is zero but we do not know its shape, and define rules to propagate it through the graph. We have to handle propagating AutogradZero through both groups of statements that might appear:\n(1) a group of operators representing a backwards function (typically each backward has more than a single op to compute it. Only the composition of these operators is guaranteed to map 0 to 0).\n(2) an addition of the outputs of (1)\nFor (1), assuming that the backwards function has a single input and multiple outputs (which is the case for all of the ops we currently handle, and 90%+ of autograd formulas in general), we can treat AutogradZero as a poison value -- a single input that is AutogradZero makes an op's output become AutogradZero. This will propagate through the entire formula of a group of operators representing a backward. If we ever have to write a formula with multiple inputs, we can insert ops that change autogradzero back to undef and allow the op to handle the behavior on its own. To handle (2), we need to distinguish between additions that appear inside of a backward function (which should propagate the AutogradeZero) and those which sum the derivatives from multiple uses of a value (which should drop any AutogradZeros). We can do this by introducing a special AutogradAdd op to indicate case (2), which is removed when we do the propagation pass of AutogradZeros.\nFinally, it is important that the AutogradZero token is only propagated when the graph actually represents a gradient. The poison-based propagation only works because we know that the the backwards functions are implementing a matrix multiply, which maps 0 to 0. To ensure we only generate AutogradZero in gradient graphs, the autodiff can introduce autogradZeroIfUndef nodes for any undefined input, which will be replaced during the autogradZero propagation pass with autogradZero.\nAlternative considered: An alternative formulation would guard each output of every backward function with propagateUndef(v, input) which would replace the computed output v with undef if input is undef. This would replace the \"poison\" value AutogradZero in the above formulation. However this has the nasty property that it inserts a huge number of propagateUndef nodes, which makes reading and debugging the backward output much harder. In contrast, the poison-value formulation only inserts autogradZeroIfUndef nodes on the inputs to the graph and the body of the graph remains the same.", "body": "### Problem\r\n\r\nA value is output from a function. That value is never used in the program. When computing the gradient of that function, we get an undefined tensor as the gradient of that output. However, that value should actually be zero (it isn't used in the loss so dL/dV should be all zeros). Currently we have a node 'ReplaceIfUndef' that substitutes the zero tensor in. However, we need to know the shape of the output to generate the right shape in the zero tensor. This is not always possible to statically derive. We do not want to rely on the shape being available, nor do we want to add the bookkeeping to remember the shape between forward and backward.\r\n\r\n### Additional Background\r\nWe always specialize a different graph when a tensor is undefined, so\r\nwe can perform graph optimizations knowing that certain inputs to the graph are undefined, including the optimization of the autograd graph when certain outputs are zeros.\r\n\r\n### Proposed Solution\r\n\r\n If we have a function `y = f(x)` with jacobian `J`, the backwards of f is dx = J^t dy.  Note that because the backwards always implements this matrix multiply, we know that it maps an input vector of zeros to an output vector of zero regardless of what operations it choses to do inside to actually implement the matrix multiply (most use some optimized form and never generate J^t). Hence it is ok to replace any backward function with known-zero inputs with something that produces known-zero outputs. \r\n\r\nThe entire backwards pass is composed of operators that either (1) compute a backwards function, or (2) sum the results of backward functions when something is used multiple times.\r\n\r\nWe can introduce nodes before and after each backward function that gate the inputs and outputs to the op. If we can prove that _all_ of the inputs to the op are Undef nodes, then we can remove the entire graph representing the op, replacing the outputs with Undef nodes. To do this systematically, we introduce two ops:\r\n* `y0, y1, y2, ... = UndefToPoison(x0, x1, x2...)` if all of the inputs are undef, the all of the outputs become poison values. Otherwise this op does nothing. For all ops, if a single input can be proven to be a poison value, then all of its outputs become poison values.\r\n* `y = PoisonToUndef(x)` if x is a poison value then y turns into an Undef. Otherwise this op does nothing. This is the one exception to the poison propagation described above.\r\n\r\nA backwards op graph for a single backward function `y0, y1... = f'(x0, x1...)` then will get transformed into:\r\n\r\n```\r\nx0', x1', ... = UndefToPoison(x0, x1, ...)\r\ny0', y1' = f'(x0', x1') # note f' represents a whole graph not necessarily a single op\r\ny0 = PoisonToUndef(y0')\r\ny1 = PoisonToUndef(y1')\r\n```\r\nIn the event that all the inputs are Undef, then poison propagation will propagate through the entire body of `f'`, and the outputs will become Undefs. If some inputs are not Undef, then it will not propagate and the body will be retained as is. When generating this graph, we can use the property that `UndefToPoison(PoisonToUndef(x)) == x` to keep the graph simple.  With this simplification, ops that are simply composed together will not have these nodes inserted in between.\r\n\r\nA pass to propagate Undefs would perform the following actions, given an set of input values known to be Undefs:\r\n* If all values into UndefToPoison are proven to be Undefs, it replaces the outputs of the op with Poison values.\r\n* If a node that is not PoisonToUndef has one poison input value, it replaces the outputs of the with Poison values\r\n* If the input PoisonToUndef is a poison value, it replaces the output with Undef\r\n* `add(Undef, x) -> x`, `add(x, Undef) -> x`, and `add(Undef, Undef) -> Undef`\r\n\r\nGiven the fact that we always specialize inputs to be defined or undefined, functions with single output values can generate gradient graph that do not have to handle Undef inputs. Functions with multiple output will need to handle the gradient case where some but not all inputs may be Undef. This is already true of many of the backwards ops with multiple outputs.\r\n\r\n### Previous Proposed Solution\r\n\r\n(this is similar to the above, but does not handle ops with multiple outputs well, and makes it hard to distinguish autograd addition from addition inside a single operator).\r\n\r\nWe can introduce a 'AutogradZero', which represents a place where we know the value is zero but we do not know its shape, and define rules to propagate it through the graph. We have to handle propagating AutogradZero through both groups of statements that might appear:\r\n\r\n(1) a group of operators representing a backwards function (typically each backward has more than a single op to compute it. Only the composition of these operators is guaranteed to map 0 to 0).\r\n(2) an addition of the outputs of (1)\r\n\r\nFor (1), assuming that the backwards function has a single input and multiple outputs (which is the case for all of the ops we currently handle, and 90%+ of autograd formulas in general), we can treat AutogradZero as a poison value -- a single input that is AutogradZero makes an op's output become AutogradZero. This will propagate through the entire formula of a group of operators representing a backward. If we ever have to write a formula with multiple inputs, we can insert ops that change autogradzero back to undef and allow the op to handle the behavior on its own. To handle (2), we need to distinguish between additions that appear inside of a backward function (which should propagate the AutogradeZero) and those which sum the derivatives from multiple uses of a value (which should drop any AutogradZeros). We can do this by introducing a special AutogradAdd op to indicate case (2), which is removed when we do the propagation pass of AutogradZeros.\r\n\r\nFinally, it is important that the AutogradZero token is only propagated when the graph actually represents a gradient. The poison-based propagation only works because we know that the the backwards functions are implementing a matrix multiply, which maps 0 to 0. To ensure we only generate AutogradZero in gradient graphs, the autodiff can introduce autogradZeroIfUndef nodes for any undefined input, which will be replaced during the autogradZero propagation pass with autogradZero.\r\n\r\n\r\nAlternative considered: An alternative formulation would guard each output of every backward function with `propagateUndef(v, input)` which would replace the computed output `v` with undef if input is undef. This would replace the \"poison\" value AutogradZero in the above formulation. However this has the nasty property that it inserts a huge number of `propagateUndef` nodes, which makes reading and debugging the backward output much harder. In contrast, the poison-value formulation only inserts autogradZeroIfUndef nodes on the inputs to the graph and the body of the graph remains the same.\r\n"}