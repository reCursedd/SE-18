{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/433567569", "html_url": "https://github.com/pytorch/pytorch/issues/13192#issuecomment-433567569", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13192", "id": 433567569, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzU2NzU2OQ==", "user": {"login": "jjsjann123", "id": 3709243, "node_id": "MDQ6VXNlcjM3MDkyNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjsjann123", "html_url": "https://github.com/jjsjann123", "followers_url": "https://api.github.com/users/jjsjann123/followers", "following_url": "https://api.github.com/users/jjsjann123/following{/other_user}", "gists_url": "https://api.github.com/users/jjsjann123/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjsjann123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjsjann123/subscriptions", "organizations_url": "https://api.github.com/users/jjsjann123/orgs", "repos_url": "https://api.github.com/users/jjsjann123/repos", "events_url": "https://api.github.com/users/jjsjann123/events{/privacy}", "received_events_url": "https://api.github.com/users/jjsjann123/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-26T23:18:39Z", "updated_at": "2018-10-26T23:18:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm currently working on this. Will issue a PR once I got it into decent shape.</p>\n<p>Here's the code to repro (with my test kernel part remove)</p>\n<pre><code>import torch\n\nimport timeit\nimport time\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nWIDTH = 1.0\nALPHA = 1.0\n\ndef run_func(func, inp_t, *args):\n  if len(args) == 0:\n    out = func(inp_t)\n  elif len(args) == 1:\n    out = func(inp_t, args[0])\n  torch.cuda.synchronize()\n\n\nclass Timer:\n  def __init__(self):\n    self.data = np.array([])\n\n  def new_item(self, t):\n    self.data = np.append(self.data, t)\n\n  def mean(self):\n    return self.data.mean()\n    \n  def median(self):\n    return np.percentile(self.data, 50)\n\ndef pyplot_draw(data, xticks, title, xlabel, ylabel, log, axis=None):\n  plt.title(title)\n  if axis != None:\n    x = xticks\n  else:\n    x = range(len(xticks))\n    plt.xticks(x, xticks)\n\n  if log:\n    plot = plt.plot\n  else:\n    plot = plt.loglog\n\n  for key in data:\n    plot(x, data[key], label=key, linewidth=WIDTH, alpha=ALPHA)\n  plt.xlabel(xlabel)\n  plt.ylabel(ylabel)\n\ndef draw_plot(result, axis = None, title = None, xlabel = None, ylabel = None, log = False):\n  data = {}\n  xticks = []\n  for item in next(iter(result.values())):\n    data[item] = []\n  for key in result:\n    if axis != None:\n      xticks.append(key[axis])\n    else:\n      xticks.append(key)\n    for item in data:\n      data[item].append( result[key][item] )\n  if title:\n    pyplot_draw(data, xticks, title, xlabel, ylabel, log, axis)\n  print(xticks)\n  print(data)\n\n\n################################################################################\n# reduce timing\n################################################################################\niteration = 100\n\nreduce_dim_base = 128\noutput_dim_base = 64\n\nreduce_dim_iter = 10\noutput_dim_iter = 5\n\n\n\nfor i in range(output_dim_iter):\n  output_dim = output_dim_base * (2**i)\n  configs = []\n  result = {}\n  \n  plt.subplot(output_dim_iter, 1, i+1)\n\n  for j in range(reduce_dim_iter):\n    reduce_dim = reduce_dim_base * (2**j)\n    configs.append((reduce_dim, output_dim))\n  \n  for reduce_dim, output_dim in configs:\n    result[(reduce_dim, output_dim)] = {}\n    res = result[(reduce_dim, output_dim)]\n  \n    torch.cuda.cudart().cudaProfilerStart()\n  \n    inp_t = torch.randn(output_dim, reduce_dim).requires_grad_().cuda()\n    timer = Timer()\n    for i in range(iteration):\n      timer.new_item(timeit.timeit('run_func(torch.mean, inp_t, 1)', number=1, globals=globals()))\n    res['contig_dim_mean'] = timer.median()\n    timer = Timer()\n    for i in range(iteration):\n      timer.new_item(timeit.timeit('run_func(torch.var, inp_t, 1)', number=1, globals=globals()))\n    res['contig_dim_var'] = timer.median()\n  \n    inp_t = torch.randn(reduce_dim, output_dim).requires_grad_().cuda()\n    timer = Timer()\n    for i in range(iteration):\n      timer.new_item(timeit.timeit('run_func(torch.mean, inp_t, 0)', number=1, globals=globals()))\n    res['noncontig_dim_mean'] = timer.median()\n    timer = Timer()\n    for i in range(iteration):\n      timer.new_item(timeit.timeit('run_func(torch.var, inp_t, 0)', number=1, globals=globals()))\n    res['noncontig_dim_var'] = timer.median()\n  \n    torch.cuda.cudart().cudaProfilerStop()\n  \n  draw_plot(result, 0, \"reduction_output_size_{0}\".format(output_dim), \"reduction_size\", \"median time\")\n  plt.legend()\nplt.show()\n#========================end batchnorm timing===================================\n\n</code></pre>", "body_text": "I'm currently working on this. Will issue a PR once I got it into decent shape.\nHere's the code to repro (with my test kernel part remove)\nimport torch\n\nimport timeit\nimport time\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nWIDTH = 1.0\nALPHA = 1.0\n\ndef run_func(func, inp_t, *args):\n  if len(args) == 0:\n    out = func(inp_t)\n  elif len(args) == 1:\n    out = func(inp_t, args[0])\n  torch.cuda.synchronize()\n\n\nclass Timer:\n  def __init__(self):\n    self.data = np.array([])\n\n  def new_item(self, t):\n    self.data = np.append(self.data, t)\n\n  def mean(self):\n    return self.data.mean()\n    \n  def median(self):\n    return np.percentile(self.data, 50)\n\ndef pyplot_draw(data, xticks, title, xlabel, ylabel, log, axis=None):\n  plt.title(title)\n  if axis != None:\n    x = xticks\n  else:\n    x = range(len(xticks))\n    plt.xticks(x, xticks)\n\n  if log:\n    plot = plt.plot\n  else:\n    plot = plt.loglog\n\n  for key in data:\n    plot(x, data[key], label=key, linewidth=WIDTH, alpha=ALPHA)\n  plt.xlabel(xlabel)\n  plt.ylabel(ylabel)\n\ndef draw_plot(result, axis = None, title = None, xlabel = None, ylabel = None, log = False):\n  data = {}\n  xticks = []\n  for item in next(iter(result.values())):\n    data[item] = []\n  for key in result:\n    if axis != None:\n      xticks.append(key[axis])\n    else:\n      xticks.append(key)\n    for item in data:\n      data[item].append( result[key][item] )\n  if title:\n    pyplot_draw(data, xticks, title, xlabel, ylabel, log, axis)\n  print(xticks)\n  print(data)\n\n\n################################################################################\n# reduce timing\n################################################################################\niteration = 100\n\nreduce_dim_base = 128\noutput_dim_base = 64\n\nreduce_dim_iter = 10\noutput_dim_iter = 5\n\n\n\nfor i in range(output_dim_iter):\n  output_dim = output_dim_base * (2**i)\n  configs = []\n  result = {}\n  \n  plt.subplot(output_dim_iter, 1, i+1)\n\n  for j in range(reduce_dim_iter):\n    reduce_dim = reduce_dim_base * (2**j)\n    configs.append((reduce_dim, output_dim))\n  \n  for reduce_dim, output_dim in configs:\n    result[(reduce_dim, output_dim)] = {}\n    res = result[(reduce_dim, output_dim)]\n  \n    torch.cuda.cudart().cudaProfilerStart()\n  \n    inp_t = torch.randn(output_dim, reduce_dim).requires_grad_().cuda()\n    timer = Timer()\n    for i in range(iteration):\n      timer.new_item(timeit.timeit('run_func(torch.mean, inp_t, 1)', number=1, globals=globals()))\n    res['contig_dim_mean'] = timer.median()\n    timer = Timer()\n    for i in range(iteration):\n      timer.new_item(timeit.timeit('run_func(torch.var, inp_t, 1)', number=1, globals=globals()))\n    res['contig_dim_var'] = timer.median()\n  \n    inp_t = torch.randn(reduce_dim, output_dim).requires_grad_().cuda()\n    timer = Timer()\n    for i in range(iteration):\n      timer.new_item(timeit.timeit('run_func(torch.mean, inp_t, 0)', number=1, globals=globals()))\n    res['noncontig_dim_mean'] = timer.median()\n    timer = Timer()\n    for i in range(iteration):\n      timer.new_item(timeit.timeit('run_func(torch.var, inp_t, 0)', number=1, globals=globals()))\n    res['noncontig_dim_var'] = timer.median()\n  \n    torch.cuda.cudart().cudaProfilerStop()\n  \n  draw_plot(result, 0, \"reduction_output_size_{0}\".format(output_dim), \"reduction_size\", \"median time\")\n  plt.legend()\nplt.show()\n#========================end batchnorm timing===================================", "body": "I'm currently working on this. Will issue a PR once I got it into decent shape.\r\n\r\nHere's the code to repro (with my test kernel part remove)\r\n```\r\nimport torch\r\n\r\nimport timeit\r\nimport time\r\nimport numpy as np\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nWIDTH = 1.0\r\nALPHA = 1.0\r\n\r\ndef run_func(func, inp_t, *args):\r\n  if len(args) == 0:\r\n    out = func(inp_t)\r\n  elif len(args) == 1:\r\n    out = func(inp_t, args[0])\r\n  torch.cuda.synchronize()\r\n\r\n\r\nclass Timer:\r\n  def __init__(self):\r\n    self.data = np.array([])\r\n\r\n  def new_item(self, t):\r\n    self.data = np.append(self.data, t)\r\n\r\n  def mean(self):\r\n    return self.data.mean()\r\n    \r\n  def median(self):\r\n    return np.percentile(self.data, 50)\r\n\r\ndef pyplot_draw(data, xticks, title, xlabel, ylabel, log, axis=None):\r\n  plt.title(title)\r\n  if axis != None:\r\n    x = xticks\r\n  else:\r\n    x = range(len(xticks))\r\n    plt.xticks(x, xticks)\r\n\r\n  if log:\r\n    plot = plt.plot\r\n  else:\r\n    plot = plt.loglog\r\n\r\n  for key in data:\r\n    plot(x, data[key], label=key, linewidth=WIDTH, alpha=ALPHA)\r\n  plt.xlabel(xlabel)\r\n  plt.ylabel(ylabel)\r\n\r\ndef draw_plot(result, axis = None, title = None, xlabel = None, ylabel = None, log = False):\r\n  data = {}\r\n  xticks = []\r\n  for item in next(iter(result.values())):\r\n    data[item] = []\r\n  for key in result:\r\n    if axis != None:\r\n      xticks.append(key[axis])\r\n    else:\r\n      xticks.append(key)\r\n    for item in data:\r\n      data[item].append( result[key][item] )\r\n  if title:\r\n    pyplot_draw(data, xticks, title, xlabel, ylabel, log, axis)\r\n  print(xticks)\r\n  print(data)\r\n\r\n\r\n################################################################################\r\n# reduce timing\r\n################################################################################\r\niteration = 100\r\n\r\nreduce_dim_base = 128\r\noutput_dim_base = 64\r\n\r\nreduce_dim_iter = 10\r\noutput_dim_iter = 5\r\n\r\n\r\n\r\nfor i in range(output_dim_iter):\r\n  output_dim = output_dim_base * (2**i)\r\n  configs = []\r\n  result = {}\r\n  \r\n  plt.subplot(output_dim_iter, 1, i+1)\r\n\r\n  for j in range(reduce_dim_iter):\r\n    reduce_dim = reduce_dim_base * (2**j)\r\n    configs.append((reduce_dim, output_dim))\r\n  \r\n  for reduce_dim, output_dim in configs:\r\n    result[(reduce_dim, output_dim)] = {}\r\n    res = result[(reduce_dim, output_dim)]\r\n  \r\n    torch.cuda.cudart().cudaProfilerStart()\r\n  \r\n    inp_t = torch.randn(output_dim, reduce_dim).requires_grad_().cuda()\r\n    timer = Timer()\r\n    for i in range(iteration):\r\n      timer.new_item(timeit.timeit('run_func(torch.mean, inp_t, 1)', number=1, globals=globals()))\r\n    res['contig_dim_mean'] = timer.median()\r\n    timer = Timer()\r\n    for i in range(iteration):\r\n      timer.new_item(timeit.timeit('run_func(torch.var, inp_t, 1)', number=1, globals=globals()))\r\n    res['contig_dim_var'] = timer.median()\r\n  \r\n    inp_t = torch.randn(reduce_dim, output_dim).requires_grad_().cuda()\r\n    timer = Timer()\r\n    for i in range(iteration):\r\n      timer.new_item(timeit.timeit('run_func(torch.mean, inp_t, 0)', number=1, globals=globals()))\r\n    res['noncontig_dim_mean'] = timer.median()\r\n    timer = Timer()\r\n    for i in range(iteration):\r\n      timer.new_item(timeit.timeit('run_func(torch.var, inp_t, 0)', number=1, globals=globals()))\r\n    res['noncontig_dim_var'] = timer.median()\r\n  \r\n    torch.cuda.cudart().cudaProfilerStop()\r\n  \r\n  draw_plot(result, 0, \"reduction_output_size_{0}\".format(output_dim), \"reduction_size\", \"median time\")\r\n  plt.legend()\r\nplt.show()\r\n#========================end batchnorm timing===================================\r\n\r\n```"}