{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13192", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13192/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13192/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13192/events", "html_url": "https://github.com/pytorch/pytorch/issues/13192", "id": 374573348, "node_id": "MDU6SXNzdWUzNzQ1NzMzNDg=", "number": 13192, "title": "THC variance kernel runs slow and does not scale well with reduction dimension", "user": {"login": "jjsjann123", "id": 3709243, "node_id": "MDQ6VXNlcjM3MDkyNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjsjann123", "html_url": "https://github.com/jjsjann123", "followers_url": "https://api.github.com/users/jjsjann123/followers", "following_url": "https://api.github.com/users/jjsjann123/following{/other_user}", "gists_url": "https://api.github.com/users/jjsjann123/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjsjann123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjsjann123/subscriptions", "organizations_url": "https://api.github.com/users/jjsjann123/orgs", "repos_url": "https://api.github.com/users/jjsjann123/repos", "events_url": "https://api.github.com/users/jjsjann123/events{/privacy}", "received_events_url": "https://api.github.com/users/jjsjann123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-26T22:55:50Z", "updated_at": "2018-10-31T23:27:50Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p>Merge <code>torch.mean</code> and <code>torch.var</code> kernels on GPU.</p>\n<h2>Motivation</h2>\n<p><code>torch.var</code> GPU kernel launch config does not scale with larger reduction size. Meanwhile, <code>torch.mean</code> performs really well and the math are similar (<code>torch.var</code> uses welford so it is pratically the same single pass reduction).<br>\nAt larger scale I am seeing 100x speed up.</p>\n<h2>Pitch</h2>\n<p>We should figure out a reduction op API so we can support different math in our reduction op. That would enable us to merge <code>torch.mean</code> and <code>torch.var</code> kernel launch configurations.</p>\n<h2>Alternatives</h2>\n<p>We can definitely improve <code>torch.var</code> kernel separately, since it has slightly more complicated math, we might expect it requires different heuristics than the one used in <code>torch.mean</code>. But this might not be that much of a problem.</p>\n<h2>Additional context</h2>\n<p>I have some monkey benchmark. Comparing <code>torch.mean</code> <code>torch.var</code> and my test kernel <code>welford</code> (which does a mean/var on contiguous dimension)<br>\nI have a 2d input tensor and does reduction on each dimension separately.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/3709243/47595875-683c2180-d937-11e8-9663-7cf1a6736a24.png\"><img src=\"https://user-images.githubusercontent.com/3709243/47595875-683c2180-d937-11e8-9663-7cf1a6736a24.png\" alt=\"1\" style=\"max-width:100%;\"></a></p>", "body_text": "\ud83d\ude80 Feature\nMerge torch.mean and torch.var kernels on GPU.\nMotivation\ntorch.var GPU kernel launch config does not scale with larger reduction size. Meanwhile, torch.mean performs really well and the math are similar (torch.var uses welford so it is pratically the same single pass reduction).\nAt larger scale I am seeing 100x speed up.\nPitch\nWe should figure out a reduction op API so we can support different math in our reduction op. That would enable us to merge torch.mean and torch.var kernel launch configurations.\nAlternatives\nWe can definitely improve torch.var kernel separately, since it has slightly more complicated math, we might expect it requires different heuristics than the one used in torch.mean. But this might not be that much of a problem.\nAdditional context\nI have some monkey benchmark. Comparing torch.mean torch.var and my test kernel welford (which does a mean/var on contiguous dimension)\nI have a 2d input tensor and does reduction on each dimension separately.", "body": "## \ud83d\ude80 Feature\r\nMerge `torch.mean` and `torch.var` kernels on GPU.\r\n\r\n## Motivation\r\n\r\n`torch.var` GPU kernel launch config does not scale with larger reduction size. Meanwhile, `torch.mean` performs really well and the math are similar (`torch.var` uses welford so it is pratically the same single pass reduction).\r\nAt larger scale I am seeing 100x speed up.\r\n\r\n## Pitch\r\n\r\nWe should figure out a reduction op API so we can support different math in our reduction op. That would enable us to merge `torch.mean` and `torch.var` kernel launch configurations.\r\n\r\n## Alternatives\r\n\r\nWe can definitely improve `torch.var` kernel separately, since it has slightly more complicated math, we might expect it requires different heuristics than the one used in `torch.mean`. But this might not be that much of a problem. \r\n\r\n## Additional context\r\n\r\nI have some monkey benchmark. Comparing `torch.mean` `torch.var` and my test kernel `welford` (which does a mean/var on contiguous dimension)\r\nI have a 2d input tensor and does reduction on each dimension separately.\r\n\r\n![1](https://user-images.githubusercontent.com/3709243/47595875-683c2180-d937-11e8-9663-7cf1a6736a24.png)\r\n\r\n\r\n"}