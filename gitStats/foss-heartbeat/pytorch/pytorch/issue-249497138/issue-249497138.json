{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2379", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2379/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2379/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2379/events", "html_url": "https://github.com/pytorch/pytorch/issues/2379", "id": 249497138, "node_id": "MDU6SXNzdWUyNDk0OTcxMzg=", "number": 2379, "title": "`torch.cuda.device_count` is misleading", "user": {"login": "PetrochukM", "id": 7424737, "node_id": "MDQ6VXNlcjc0MjQ3Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7424737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PetrochukM", "html_url": "https://github.com/PetrochukM", "followers_url": "https://api.github.com/users/PetrochukM/followers", "following_url": "https://api.github.com/users/PetrochukM/following{/other_user}", "gists_url": "https://api.github.com/users/PetrochukM/gists{/gist_id}", "starred_url": "https://api.github.com/users/PetrochukM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PetrochukM/subscriptions", "organizations_url": "https://api.github.com/users/PetrochukM/orgs", "repos_url": "https://api.github.com/users/PetrochukM/repos", "events_url": "https://api.github.com/users/PetrochukM/events{/privacy}", "received_events_url": "https://api.github.com/users/PetrochukM/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-08-10T22:43:32Z", "updated_at": "2017-08-14T13:43:00Z", "closed_at": "2017-08-14T13:43:00Z", "author_association": "NONE", "body_html": "<pre><code>def device_count():\n    \"\"\"Returns the number of GPUs available.\"\"\"\n    if is_available():\n        _lazy_init()\n        return torch._C._cuda_getDeviceCount()\n    else:\n        return 0\n</code></pre>\n<p>Device count does not return the number of GPUs available. It returns the number of GPUs. Those GPUs may not be available as they are being used by another process.</p>\n<p>Will torch in the future support getting the GPUs that have no processes running on them?</p>\n<p>For the mean time, I use this method to determine a list of GPUs with &gt;98% free memory. This allows me to only use the GPUs that do not have processes running on them.</p>\n<pre><code>def cuda_devices():\n    \"\"\"\n    Checks for all CUDA devices with free memory.\n\n    Returns:\n        (list [int]) the CUDA devices available\n    \"\"\"\n\n    # Find Cuda\n    cuda = None\n    for libname in ('libcuda.so', 'libcuda.dylib', 'cuda.dll'):\n        try:\n            cuda = ctypes.CDLL(libname)\n        except OSError:\n            continue\n        else:\n            break\n\n    # Constants taken from cuda.h\n    CUDA_SUCCESS = 0\n\n    num_gpu = ctypes.c_int()\n    error = ctypes.c_char_p()\n    free_memory = ctypes.c_size_t()\n    total_memory = ctypes.c_size_t()\n    context = ctypes.c_void_p()\n    device = ctypes.c_int()\n    ret = []  # Device IDs that are not used.\n\n    def run(result, func, *args):\n        nonlocal error\n        result = func(*args)\n        if result != CUDA_SUCCESS:\n            cuda.cuGetErrorString(result, ctypes.byref(error))\n            logger.warn(\"%s failed with error code %d: %s\", func.__name__, result,\n                        error.value.decode())\n            return False\n        return True\n\n    # Check if Cuda is available\n    if not cuda:\n        return ret\n\n    result = cuda.cuInit(0)\n\n    # Get number of GPU\n    if not run(result, cuda.cuDeviceGetCount, ctypes.byref(num_gpu)):\n        return ret\n\n    for i in range(num_gpu.value):\n        if (not run(result, cuda.cuDeviceGet, ctypes.byref(device), i) or\n                not run(result, cuda.cuDeviceGet, ctypes.byref(device), i) or\n                not run(result, cuda.cuCtxCreate, ctypes.byref(context), 0, device) or\n                not run(result, cuda.cuMemGetInfo,\n                        ctypes.byref(free_memory), ctypes.byref(total_memory))):\n            continue\n\n        percent_free_memory = float(free_memory.value) / total_memory.value\n        logger.info('CUDA device %d has %f free memory [%d MiB of %d MiB]', i, percent_free_memory,\n                    free_memory.value / 1024**2, total_memory.value / 1024**2)\n        if percent_free_memory &gt; 0.98:\n            logger.info('CUDA device %d is available', i)\n            ret.append(i)\n\n        cuda.cuCtxDetach(context)\n\n    return ret\n</code></pre>", "body_text": "def device_count():\n    \"\"\"Returns the number of GPUs available.\"\"\"\n    if is_available():\n        _lazy_init()\n        return torch._C._cuda_getDeviceCount()\n    else:\n        return 0\n\nDevice count does not return the number of GPUs available. It returns the number of GPUs. Those GPUs may not be available as they are being used by another process.\nWill torch in the future support getting the GPUs that have no processes running on them?\nFor the mean time, I use this method to determine a list of GPUs with >98% free memory. This allows me to only use the GPUs that do not have processes running on them.\ndef cuda_devices():\n    \"\"\"\n    Checks for all CUDA devices with free memory.\n\n    Returns:\n        (list [int]) the CUDA devices available\n    \"\"\"\n\n    # Find Cuda\n    cuda = None\n    for libname in ('libcuda.so', 'libcuda.dylib', 'cuda.dll'):\n        try:\n            cuda = ctypes.CDLL(libname)\n        except OSError:\n            continue\n        else:\n            break\n\n    # Constants taken from cuda.h\n    CUDA_SUCCESS = 0\n\n    num_gpu = ctypes.c_int()\n    error = ctypes.c_char_p()\n    free_memory = ctypes.c_size_t()\n    total_memory = ctypes.c_size_t()\n    context = ctypes.c_void_p()\n    device = ctypes.c_int()\n    ret = []  # Device IDs that are not used.\n\n    def run(result, func, *args):\n        nonlocal error\n        result = func(*args)\n        if result != CUDA_SUCCESS:\n            cuda.cuGetErrorString(result, ctypes.byref(error))\n            logger.warn(\"%s failed with error code %d: %s\", func.__name__, result,\n                        error.value.decode())\n            return False\n        return True\n\n    # Check if Cuda is available\n    if not cuda:\n        return ret\n\n    result = cuda.cuInit(0)\n\n    # Get number of GPU\n    if not run(result, cuda.cuDeviceGetCount, ctypes.byref(num_gpu)):\n        return ret\n\n    for i in range(num_gpu.value):\n        if (not run(result, cuda.cuDeviceGet, ctypes.byref(device), i) or\n                not run(result, cuda.cuDeviceGet, ctypes.byref(device), i) or\n                not run(result, cuda.cuCtxCreate, ctypes.byref(context), 0, device) or\n                not run(result, cuda.cuMemGetInfo,\n                        ctypes.byref(free_memory), ctypes.byref(total_memory))):\n            continue\n\n        percent_free_memory = float(free_memory.value) / total_memory.value\n        logger.info('CUDA device %d has %f free memory [%d MiB of %d MiB]', i, percent_free_memory,\n                    free_memory.value / 1024**2, total_memory.value / 1024**2)\n        if percent_free_memory > 0.98:\n            logger.info('CUDA device %d is available', i)\n            ret.append(i)\n\n        cuda.cuCtxDetach(context)\n\n    return ret", "body": "```\r\ndef device_count():\r\n    \"\"\"Returns the number of GPUs available.\"\"\"\r\n    if is_available():\r\n        _lazy_init()\r\n        return torch._C._cuda_getDeviceCount()\r\n    else:\r\n        return 0\r\n```\r\n\r\nDevice count does not return the number of GPUs available. It returns the number of GPUs. Those GPUs may not be available as they are being used by another process.\r\n\r\nWill torch in the future support getting the GPUs that have no processes running on them? \r\n\r\nFor the mean time, I use this method to determine a list of GPUs with >98% free memory. This allows me to only use the GPUs that do not have processes running on them.\r\n```\r\ndef cuda_devices():\r\n    \"\"\"\r\n    Checks for all CUDA devices with free memory.\r\n\r\n    Returns:\r\n        (list [int]) the CUDA devices available\r\n    \"\"\"\r\n\r\n    # Find Cuda\r\n    cuda = None\r\n    for libname in ('libcuda.so', 'libcuda.dylib', 'cuda.dll'):\r\n        try:\r\n            cuda = ctypes.CDLL(libname)\r\n        except OSError:\r\n            continue\r\n        else:\r\n            break\r\n\r\n    # Constants taken from cuda.h\r\n    CUDA_SUCCESS = 0\r\n\r\n    num_gpu = ctypes.c_int()\r\n    error = ctypes.c_char_p()\r\n    free_memory = ctypes.c_size_t()\r\n    total_memory = ctypes.c_size_t()\r\n    context = ctypes.c_void_p()\r\n    device = ctypes.c_int()\r\n    ret = []  # Device IDs that are not used.\r\n\r\n    def run(result, func, *args):\r\n        nonlocal error\r\n        result = func(*args)\r\n        if result != CUDA_SUCCESS:\r\n            cuda.cuGetErrorString(result, ctypes.byref(error))\r\n            logger.warn(\"%s failed with error code %d: %s\", func.__name__, result,\r\n                        error.value.decode())\r\n            return False\r\n        return True\r\n\r\n    # Check if Cuda is available\r\n    if not cuda:\r\n        return ret\r\n\r\n    result = cuda.cuInit(0)\r\n\r\n    # Get number of GPU\r\n    if not run(result, cuda.cuDeviceGetCount, ctypes.byref(num_gpu)):\r\n        return ret\r\n\r\n    for i in range(num_gpu.value):\r\n        if (not run(result, cuda.cuDeviceGet, ctypes.byref(device), i) or\r\n                not run(result, cuda.cuDeviceGet, ctypes.byref(device), i) or\r\n                not run(result, cuda.cuCtxCreate, ctypes.byref(context), 0, device) or\r\n                not run(result, cuda.cuMemGetInfo,\r\n                        ctypes.byref(free_memory), ctypes.byref(total_memory))):\r\n            continue\r\n\r\n        percent_free_memory = float(free_memory.value) / total_memory.value\r\n        logger.info('CUDA device %d has %f free memory [%d MiB of %d MiB]', i, percent_free_memory,\r\n                    free_memory.value / 1024**2, total_memory.value / 1024**2)\r\n        if percent_free_memory > 0.98:\r\n            logger.info('CUDA device %d is available', i)\r\n            ret.append(i)\r\n\r\n        cuda.cuCtxDetach(context)\r\n\r\n    return ret\r\n```"}