{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/412598294", "html_url": "https://github.com/pytorch/pytorch/issues/10470#issuecomment-412598294", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10470", "id": 412598294, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjU5ODI5NA==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-13T17:27:49Z", "updated_at": "2018-08-13T17:27:49Z", "author_association": "MEMBER", "body_html": "<p>The paper is about increasing the batch size by increasing the effective number of machines / doing more parallelization.</p>\n<p>Decreasing the learning rate, or increasing the batch size is equivalent (according to the paper).</p>\n<p>I'm not sure how easy it is from an API design perspective to add a batch size scheduler, as it interacts with DataLoader.</p>\n<p>It's trivial to do it in user code, using a standard learning rate scheduler (that we provide), and then just doing <code>batch_size = initial_batch_size * (initial_learning_rate / current_learning_rate)</code>.</p>\n<p>Because it's not a trivial API design, and it's trivial to handle it in user code, I'm closing the issue.</p>", "body_text": "The paper is about increasing the batch size by increasing the effective number of machines / doing more parallelization.\nDecreasing the learning rate, or increasing the batch size is equivalent (according to the paper).\nI'm not sure how easy it is from an API design perspective to add a batch size scheduler, as it interacts with DataLoader.\nIt's trivial to do it in user code, using a standard learning rate scheduler (that we provide), and then just doing batch_size = initial_batch_size * (initial_learning_rate / current_learning_rate).\nBecause it's not a trivial API design, and it's trivial to handle it in user code, I'm closing the issue.", "body": "The paper is about increasing the batch size by increasing the effective number of machines / doing more parallelization.\r\n\r\nDecreasing the learning rate, or increasing the batch size is equivalent (according to the paper).\r\n\r\nI'm not sure how easy it is from an API design perspective to add a batch size scheduler, as it interacts with DataLoader.\r\n\r\nIt's trivial to do it in user code, using a standard learning rate scheduler (that we provide), and then just doing `batch_size = initial_batch_size * (initial_learning_rate / current_learning_rate)`.\r\n\r\nBecause it's not a trivial API design, and it's trivial to handle it in user code, I'm closing the issue."}