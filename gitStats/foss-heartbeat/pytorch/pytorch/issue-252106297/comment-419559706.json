{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419559706", "html_url": "https://github.com/pytorch/pytorch/issues/2515#issuecomment-419559706", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2515", "id": 419559706, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTU1OTcwNg==", "user": {"login": "PetrochukM", "id": 7424737, "node_id": "MDQ6VXNlcjc0MjQ3Mzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/7424737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PetrochukM", "html_url": "https://github.com/PetrochukM", "followers_url": "https://api.github.com/users/PetrochukM/followers", "following_url": "https://api.github.com/users/PetrochukM/following{/other_user}", "gists_url": "https://api.github.com/users/PetrochukM/gists{/gist_id}", "starred_url": "https://api.github.com/users/PetrochukM/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PetrochukM/subscriptions", "organizations_url": "https://api.github.com/users/PetrochukM/orgs", "repos_url": "https://api.github.com/users/PetrochukM/repos", "events_url": "https://api.github.com/users/PetrochukM/events{/privacy}", "received_events_url": "https://api.github.com/users/PetrochukM/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-07T20:40:39Z", "updated_at": "2018-09-07T20:44:52Z", "author_association": "NONE", "body_html": "<p>Checking back in:</p>\n<pre><code>import torch\nimport torch.nn as nn\n\ng = nn.GRU(100, 20, bias=False).cuda()\ng = nn.utils.weight_norm(g, name='weight_hh_l0')\ng = nn.utils.weight_norm(g, name='weight_ih_l0')\ng.flatten_parameters()\n\nx = torch.randn(20, 32, 100).cuda()\noutput, state = g(x)\n</code></pre>\n<p>Trace:</p>\n<pre><code>Traceback (most recent call last):\n  File \"abc.py\", line 10, in &lt;module&gt;\n    output, state = g(x)\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py\", line 173, in forward\n    assert first_data.storage().size() == self._param_buf_size\nAssertionError\n</code></pre>\n<p>Tried to get rid of <code>flatten_parameters</code> but I quickly run out of memory:<br>\n<code>RuntimeError: CUDA error: out of memory</code></p>\n<p>Found another error:</p>\n<pre><code>import torch\nimport torch.nn as nn\n\ng = nn.LSTM(100, 20)\ng = nn.utils.weight_norm(g, name='weight_hh_l0')\ng = nn.utils.weight_norm(g, name='weight_ih_l0')\ng.flatten_parameters()\ng = g.cuda()\n\nx = torch.randn(20, 32, 100).cuda()\noutput, state = g(x)\n</code></pre>\n<p>Trace:</p>\n<pre><code>Traceback (most recent call last):\n  File \"abc.py\", line 8, in &lt;module&gt;\n    g = g.cuda()\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 258, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py\", line 112, in _apply\n    self.flatten_parameters()\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py\", line 105, in flatten_parameters\n    self.batch_first, bool(self.bidirectional))\nRuntimeError: _cudnn_rnn_flatten_weight not supported on torch.FloatTensor\n</code></pre>", "body_text": "Checking back in:\nimport torch\nimport torch.nn as nn\n\ng = nn.GRU(100, 20, bias=False).cuda()\ng = nn.utils.weight_norm(g, name='weight_hh_l0')\ng = nn.utils.weight_norm(g, name='weight_ih_l0')\ng.flatten_parameters()\n\nx = torch.randn(20, 32, 100).cuda()\noutput, state = g(x)\n\nTrace:\nTraceback (most recent call last):\n  File \"abc.py\", line 10, in <module>\n    output, state = g(x)\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py\", line 173, in forward\n    assert first_data.storage().size() == self._param_buf_size\nAssertionError\n\nTried to get rid of flatten_parameters but I quickly run out of memory:\nRuntimeError: CUDA error: out of memory\nFound another error:\nimport torch\nimport torch.nn as nn\n\ng = nn.LSTM(100, 20)\ng = nn.utils.weight_norm(g, name='weight_hh_l0')\ng = nn.utils.weight_norm(g, name='weight_ih_l0')\ng.flatten_parameters()\ng = g.cuda()\n\nx = torch.randn(20, 32, 100).cuda()\noutput, state = g(x)\n\nTrace:\nTraceback (most recent call last):\n  File \"abc.py\", line 8, in <module>\n    g = g.cuda()\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 258, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py\", line 112, in _apply\n    self.flatten_parameters()\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py\", line 105, in flatten_parameters\n    self.batch_first, bool(self.bidirectional))\nRuntimeError: _cudnn_rnn_flatten_weight not supported on torch.FloatTensor", "body": "Checking back in:\r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ng = nn.GRU(100, 20, bias=False).cuda()\r\ng = nn.utils.weight_norm(g, name='weight_hh_l0')\r\ng = nn.utils.weight_norm(g, name='weight_ih_l0')\r\ng.flatten_parameters()\r\n\r\nx = torch.randn(20, 32, 100).cuda()\r\noutput, state = g(x)\r\n```\r\n\r\nTrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"abc.py\", line 10, in <module>\r\n    output, state = g(x)\r\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py\", line 173, in forward\r\n    assert first_data.storage().size() == self._param_buf_size\r\nAssertionError\r\n```\r\n\r\nTried to get rid of ``flatten_parameters`` but I quickly run out of memory:\r\n```RuntimeError: CUDA error: out of memory```\r\n\r\nFound another error:\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\ng = nn.LSTM(100, 20)\r\ng = nn.utils.weight_norm(g, name='weight_hh_l0')\r\ng = nn.utils.weight_norm(g, name='weight_ih_l0')\r\ng.flatten_parameters()\r\ng = g.cuda()\r\n\r\nx = torch.randn(20, 32, 100).cuda()\r\noutput, state = g(x)\r\n```\r\n\r\nTrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"abc.py\", line 8, in <module>\r\n    g = g.cuda()\r\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 258, in cuda\r\n    return self._apply(lambda t: t.cuda(device))\r\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py\", line 112, in _apply\r\n    self.flatten_parameters()\r\n  File \"/home/michaelp/.local/lib/python3.5/site-packages/torch/nn/modules/rnn.py\", line 105, in flatten_parameters\r\n    self.batch_first, bool(self.bidirectional))\r\nRuntimeError: _cudnn_rnn_flatten_weight not supported on torch.FloatTensor\r\n```"}