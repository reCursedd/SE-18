{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/324494207", "html_url": "https://github.com/pytorch/pytorch/issues/2515#issuecomment-324494207", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2515", "id": 324494207, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNDQ5NDIwNw==", "user": {"login": "jramapuram", "id": 8204807, "node_id": "MDQ6VXNlcjgyMDQ4MDc=", "avatar_url": "https://avatars2.githubusercontent.com/u/8204807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jramapuram", "html_url": "https://github.com/jramapuram", "followers_url": "https://api.github.com/users/jramapuram/followers", "following_url": "https://api.github.com/users/jramapuram/following{/other_user}", "gists_url": "https://api.github.com/users/jramapuram/gists{/gist_id}", "starred_url": "https://api.github.com/users/jramapuram/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jramapuram/subscriptions", "organizations_url": "https://api.github.com/users/jramapuram/orgs", "repos_url": "https://api.github.com/users/jramapuram/repos", "events_url": "https://api.github.com/users/jramapuram/events{/privacy}", "received_events_url": "https://api.github.com/users/jramapuram/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-23T23:54:29Z", "updated_at": "2017-08-23T23:56:14Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> : Doesn't explain why it works without cuda though.  i.e note <code>bias=True</code> below (it works for both cases on CPU)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n\ng <span class=\"pl-k\">=</span> nn.GRU(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ng <span class=\"pl-k\">=</span> nn.utils.weight_norm(g, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight_hh_l0<span class=\"pl-pds\">'</span></span>)\ng <span class=\"pl-k\">=</span> nn.utils.weight_norm(g, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight_ih_l0<span class=\"pl-pds\">'</span></span>)\ng.flatten_parameters()\nzero_state <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">20</span>))\n\nx <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">100</span>))\noutput, state <span class=\"pl-k\">=</span> g(x, zero_state)</pre></div>", "body_text": "@ngimel : Doesn't explain why it works without cuda though.  i.e note bias=True below (it works for both cases on CPU)\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\ng = nn.GRU(100, 20, bias=True)\ng = nn.utils.weight_norm(g, name='weight_hh_l0')\ng = nn.utils.weight_norm(g, name='weight_ih_l0')\ng.flatten_parameters()\nzero_state = Variable(torch.zeros(1, 32, 20))\n\nx = Variable(torch.randn(20, 32, 100))\noutput, state = g(x, zero_state)", "body": "@ngimel : Doesn't explain why it works without cuda though.  i.e note `bias=True` below (it works for both cases on CPU)\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\n\r\ng = nn.GRU(100, 20, bias=True)\r\ng = nn.utils.weight_norm(g, name='weight_hh_l0')\r\ng = nn.utils.weight_norm(g, name='weight_ih_l0')\r\ng.flatten_parameters()\r\nzero_state = Variable(torch.zeros(1, 32, 20))\r\n\r\nx = Variable(torch.randn(20, 32, 100))\r\noutput, state = g(x, zero_state)\r\n```"}