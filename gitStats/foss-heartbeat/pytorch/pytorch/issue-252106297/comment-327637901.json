{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/327637901", "html_url": "https://github.com/pytorch/pytorch/issues/2515#issuecomment-327637901", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2515", "id": 327637901, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzYzNzkwMQ==", "user": {"login": "Smerity", "id": 32325, "node_id": "MDQ6VXNlcjMyMzI1", "avatar_url": "https://avatars0.githubusercontent.com/u/32325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Smerity", "html_url": "https://github.com/Smerity", "followers_url": "https://api.github.com/users/Smerity/followers", "following_url": "https://api.github.com/users/Smerity/following{/other_user}", "gists_url": "https://api.github.com/users/Smerity/gists{/gist_id}", "starred_url": "https://api.github.com/users/Smerity/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Smerity/subscriptions", "organizations_url": "https://api.github.com/users/Smerity/orgs", "repos_url": "https://api.github.com/users/Smerity/repos", "events_url": "https://api.github.com/users/Smerity/events{/privacy}", "received_events_url": "https://api.github.com/users/Smerity/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-06T23:17:28Z", "updated_at": "2017-09-06T23:54:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This issue is the same issue as raised in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"248751413\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2343\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2343/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2343\">#2343</a> and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the <a href=\"https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py\">weight dropped LSTM</a>.</p>\n<p>I realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)</p>\n<p>I've fixed it by setting <code>rnn.flatten_parameters = lambda *args, **kwargs: None</code>, which results in a warning (below) but otherwise running code (except see caveat re: lambda).<br>\n<code>UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().</code><br>\n(Just realized the error has a typo in \"greatly\" but oh well ;))</p>\n<p>You could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the <code>flatten_parameters</code> function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (<code>AttributeError: Can't pickle local object 'WeightDrop._setup.&lt;locals&gt;.&lt;lambda&gt;'</code>) on model save.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, have you got any insights as to a more elegant solution either temporarily or longer form? I know <code>flatten_parameters</code> is already a complicated and low level beast.</p>", "body_text": "This issue is the same issue as raised in #2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the weight dropped LSTM.\nI realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)\nI've fixed it by setting rnn.flatten_parameters = lambda *args, **kwargs: None, which results in a warning (below) but otherwise running code (except see caveat re: lambda).\nUserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().\n(Just realized the error has a typo in \"greatly\" but oh well ;))\nYou could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the flatten_parameters function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (AttributeError: Can't pickle local object 'WeightDrop._setup.<locals>.<lambda>') on model save.\n@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know flatten_parameters is already a complicated and low level beast.", "body": "This issue is the same issue as raised in https://github.com/pytorch/pytorch/issues/2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the [weight dropped LSTM](https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py).\r\n\r\nI realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)\r\n\r\nI've fixed it by setting `rnn.flatten_parameters = lambda *args, **kwargs: None`, which results in a warning (below) but otherwise running code (except see caveat re: lambda).\r\n`UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().`\r\n(Just realized the error has a typo in \"greatly\" but oh well ;))\r\n\r\nYou could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the `flatten_parameters` function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (`AttributeError: Can't pickle local object 'WeightDrop._setup.<locals>.<lambda>'`) on model save.\r\n\r\n@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know `flatten_parameters` is already a complicated and low level beast."}