{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3396", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3396/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3396/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3396/events", "html_url": "https://github.com/pytorch/pytorch/issues/3396", "id": 270010833, "node_id": "MDU6SXNzdWUyNzAwMTA4MzM=", "number": 3396, "title": "CUDA topk is slow for some input sizes", "user": {"login": "trypag", "id": 13003839, "node_id": "MDQ6VXNlcjEzMDAzODM5", "avatar_url": "https://avatars1.githubusercontent.com/u/13003839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trypag", "html_url": "https://github.com/trypag", "followers_url": "https://api.github.com/users/trypag/followers", "following_url": "https://api.github.com/users/trypag/following{/other_user}", "gists_url": "https://api.github.com/users/trypag/gists{/gist_id}", "starred_url": "https://api.github.com/users/trypag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trypag/subscriptions", "organizations_url": "https://api.github.com/users/trypag/orgs", "repos_url": "https://api.github.com/users/trypag/repos", "events_url": "https://api.github.com/users/trypag/events{/privacy}", "received_events_url": "https://api.github.com/users/trypag/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-10-31T15:47:39Z", "updated_at": "2017-11-09T17:05:33Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,<br>\nI was able to reproduce a configuration in which I have what I believe is a GPU synchronization issue :</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">accuracy_2d</span>(<span class=\"pl-smi\">output</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">topk</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>,)):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Computes the precision@k for the specified values of k</span>\n<span class=\"pl-s\">    Considers output is : NxCxHxW and target is : NxHxW</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    maxk <span class=\"pl-k\">=</span> <span class=\"pl-c1\">max</span>(topk)\n    total_nelem <span class=\"pl-k\">=</span> target.size(<span class=\"pl-c1\">0</span>) <span class=\"pl-k\">*</span> target.size(<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> target.size(<span class=\"pl-c1\">2</span>)\n    _, pred <span class=\"pl-k\">=</span> output.topk(maxk, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">True</span>, <span class=\"pl-c1\">True</span>)\n    correct <span class=\"pl-k\">=</span> target.unsqueeze(<span class=\"pl-c1\">1</span>).expand(pred.size())\n    correct <span class=\"pl-k\">=</span> pred.eq(correct)\n\n    res <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> topk:\n        correct_k <span class=\"pl-k\">=</span> correct[:, :k].contiguous().view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>).float().sum(<span class=\"pl-c1\">0</span>)\n        res.append(correct_k.mul_(<span class=\"pl-c1\">100.0</span> <span class=\"pl-k\">/</span> total_nelem))\n\n    <span class=\"pl-k\">return</span> res\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">AverageMeter</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Computes and stores the average and current value<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">self</span>.reset()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">reset</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">self</span>.val <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-c1\">self</span>.avg <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-c1\">self</span>.sum <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-c1\">self</span>.count <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">update</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">val</span>, <span class=\"pl-smi\">n</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n        <span class=\"pl-c1\">self</span>.val <span class=\"pl-k\">=</span> val\n        <span class=\"pl-c1\">self</span>.sum <span class=\"pl-k\">+=</span> val <span class=\"pl-k\">*</span> n\n        <span class=\"pl-c1\">self</span>.count <span class=\"pl-k\">+=</span> n\n        <span class=\"pl-c1\">self</span>.avg <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.sum <span class=\"pl-k\">/</span> <span class=\"pl-c1\">self</span>.count\n\n\ntop1 <span class=\"pl-k\">=</span> AverageMeter()\ntop3 <span class=\"pl-k\">=</span> AverageMeter()\n\ntarget <span class=\"pl-k\">=</span> torch.LongTensor(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">172</span>, <span class=\"pl-c1\">172</span>).cuda()\ntarget_var <span class=\"pl-k\">=</span> Variable(target)\npred <span class=\"pl-k\">=</span> Variable(torch.FloatTensor(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">135</span>, <span class=\"pl-c1\">172</span>, <span class=\"pl-c1\">172</span>).cuda())\n\nprec1, prec3 <span class=\"pl-k\">=</span> accuracy_2d(pred.data, target, <span class=\"pl-v\">topk</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>))\ntop1.update(prec1[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">16</span>)\ntop3.update(prec3[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">16</span>)</pre></div>\n<p>I noticed that removing the last two lines of code does not produce the expected behaviour, I suggest this is where the synchronization happens. Also, I think this is related to how <code>.topk</code> handles large kernels (cf  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>).</p>", "body_text": "Hi,\nI was able to reproduce a configuration in which I have what I believe is a GPU synchronization issue :\nimport torch\nfrom torch.autograd import Variable\n\n\ndef accuracy_2d(output, target, topk=(1,)):\n    \"\"\"\n    Computes the precision@k for the specified values of k\n    Considers output is : NxCxHxW and target is : NxHxW\n    \"\"\"\n    maxk = max(topk)\n    total_nelem = target.size(0) * target.size(1) * target.size(2)\n    _, pred = output.topk(maxk, 1, True, True)\n    correct = target.unsqueeze(1).expand(pred.size())\n    correct = pred.eq(correct)\n\n    res = []\n    for k in topk:\n        correct_k = correct[:, :k].contiguous().view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / total_nelem))\n\n    return res\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ntop1 = AverageMeter()\ntop3 = AverageMeter()\n\ntarget = torch.LongTensor(16, 172, 172).cuda()\ntarget_var = Variable(target)\npred = Variable(torch.FloatTensor(16, 135, 172, 172).cuda())\n\nprec1, prec3 = accuracy_2d(pred.data, target, topk=(1, 3))\ntop1.update(prec1[0], 16)\ntop3.update(prec3[0], 16)\nI noticed that removing the last two lines of code does not produce the expected behaviour, I suggest this is where the synchronization happens. Also, I think this is related to how .topk handles large kernels (cf  @soumith).", "body": "Hi,\r\nI was able to reproduce a configuration in which I have what I believe is a GPU synchronization issue :\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n\r\ndef accuracy_2d(output, target, topk=(1,)):\r\n    \"\"\"\r\n    Computes the precision@k for the specified values of k\r\n    Considers output is : NxCxHxW and target is : NxHxW\r\n    \"\"\"\r\n    maxk = max(topk)\r\n    total_nelem = target.size(0) * target.size(1) * target.size(2)\r\n    _, pred = output.topk(maxk, 1, True, True)\r\n    correct = target.unsqueeze(1).expand(pred.size())\r\n    correct = pred.eq(correct)\r\n\r\n    res = []\r\n    for k in topk:\r\n        correct_k = correct[:, :k].contiguous().view(-1).float().sum(0)\r\n        res.append(correct_k.mul_(100.0 / total_nelem))\r\n\r\n    return res\r\n\r\n\r\nclass AverageMeter(object):\r\n    \"\"\"Computes and stores the average and current value\"\"\"\r\n    def __init__(self):\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.val = 0\r\n        self.avg = 0\r\n        self.sum = 0\r\n        self.count = 0\r\n\r\n    def update(self, val, n=1):\r\n        self.val = val\r\n        self.sum += val * n\r\n        self.count += n\r\n        self.avg = self.sum / self.count\r\n\r\n\r\ntop1 = AverageMeter()\r\ntop3 = AverageMeter()\r\n\r\ntarget = torch.LongTensor(16, 172, 172).cuda()\r\ntarget_var = Variable(target)\r\npred = Variable(torch.FloatTensor(16, 135, 172, 172).cuda())\r\n\r\nprec1, prec3 = accuracy_2d(pred.data, target, topk=(1, 3))\r\ntop1.update(prec1[0], 16)\r\ntop3.update(prec3[0], 16)\r\n```\r\n\r\nI noticed that removing the last two lines of code does not produce the expected behaviour, I suggest this is where the synchronization happens. Also, I think this is related to how `.topk` handles large kernels (cf  @soumith)."}