{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11305", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11305/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11305/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11305/events", "html_url": "https://github.com/pytorch/pytorch/issues/11305", "id": 357428603, "node_id": "MDU6SXNzdWUzNTc0Mjg2MDM=", "number": 11305, "title": "ReduceLROnPlateau has a weird dependence on patience", "user": {"login": "cod3licious", "id": 1093567, "node_id": "MDQ6VXNlcjEwOTM1Njc=", "avatar_url": "https://avatars1.githubusercontent.com/u/1093567?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cod3licious", "html_url": "https://github.com/cod3licious", "followers_url": "https://api.github.com/users/cod3licious/followers", "following_url": "https://api.github.com/users/cod3licious/following{/other_user}", "gists_url": "https://api.github.com/users/cod3licious/gists{/gist_id}", "starred_url": "https://api.github.com/users/cod3licious/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cod3licious/subscriptions", "organizations_url": "https://api.github.com/users/cod3licious/orgs", "repos_url": "https://api.github.com/users/cod3licious/repos", "events_url": "https://api.github.com/users/cod3licious/events{/privacy}", "received_events_url": "https://api.github.com/users/cod3licious/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-05T22:34:57Z", "updated_at": "2018-09-05T22:53:56Z", "closed_at": "2018-09-05T22:53:56Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>As far as I understand, <code>patience</code> only determines after how many epochs the LR will be reduced, but has nothing to do with how much worse the value has to be for the LR to be reduced (which should be handled by <code>threshold</code>). Nevertheless, with the same losses, which stop decreasing after around epoch 10, the point at which the LR is reduced is far more than <code>patience</code> epochs, and how much more depends (weirdly) on the value of <code>patience</code>.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre>model <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">2</span>)\noptimizer <span class=\"pl-k\">=</span> optim.Adam(model.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-3</span>)\nscheduler <span class=\"pl-k\">=</span> optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, <span class=\"pl-v\">patience</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">verbose</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\ntest_loss <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">301427.7770613</span>, <span class=\"pl-c1\">160481.1660669</span>, <span class=\"pl-c1\">117763.6467930</span>, <span class=\"pl-c1\">81528.4339311</span>, <span class=\"pl-c1\">61726.3664427</span>, <span class=\"pl-c1\">46429.0652291</span>, <span class=\"pl-c1\">35138.6479489</span>, <span class=\"pl-c1\">23490.1695382</span>, <span class=\"pl-c1\">17156.3515503</span>, <span class=\"pl-c1\">13263.6756480</span>, <span class=\"pl-c1\">10980.2816269</span>, <span class=\"pl-c1\">11231.1720589</span>, <span class=\"pl-c1\">7322.0585567</span>, <span class=\"pl-c1\">9150.4534486</span>, <span class=\"pl-c1\">12504.9455976</span>, <span class=\"pl-c1\">10636.9359541</span>, <span class=\"pl-c1\">8429.2563446</span>, <span class=\"pl-c1\">5167.7016573</span>, <span class=\"pl-c1\">7893.0735840</span>, <span class=\"pl-c1\">7127.2575889</span>, <span class=\"pl-c1\">8848.1689746</span>, <span class=\"pl-c1\">7766.2236686</span>, <span class=\"pl-c1\">8599.2677275</span>, <span class=\"pl-c1\">9155.7630087</span>, <span class=\"pl-c1\">7540.2192680</span>]\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> test_loss:\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss: <span class=\"pl-pds\">'</span></span>, i)\n    scheduler.step(i)</pre></div>\n<p>With <code>patience=0</code>, the output is:</p>\n<pre><code>loss:  301427.7770613\nloss:  160481.1660669\nloss:  117763.646793\nloss:  81528.4339311\nloss:  61726.3664427\nloss:  46429.0652291\nloss:  35138.6479489\nloss:  23490.1695382\nloss:  17156.3515503\nloss:  13263.675648\nloss:  10980.2816269\nloss:  11231.1720589\nEpoch    11: reducing learning rate of group 0 to 1.0000e-04.\nloss:  7322.0585567\nloss:  9150.4534486\nEpoch    13: reducing learning rate of group 0 to 1.0000e-05.\nloss:  12504.9455976\nEpoch    14: reducing learning rate of group 0 to 1.0000e-06.\nloss:  10636.9359541\nEpoch    15: reducing learning rate of group 0 to 1.0000e-07.\nloss:  8429.2563446\nEpoch    16: reducing learning rate of group 0 to 1.0000e-08.\nloss:  5167.7016573\nloss:  7893.073584\nloss:  7127.2575889\nloss:  8848.1689746\nloss:  7766.2236686\nloss:  8599.2677275\nloss:  9155.7630087\nloss:  7540.219268\n</code></pre>\n<p>with <code>patience=5</code> it is</p>\n<pre><code>loss:  301427.7770613\nloss:  160481.1660669\nloss:  117763.646793\nloss:  81528.4339311\nloss:  61726.3664427\nloss:  46429.0652291\nloss:  35138.6479489\nloss:  23490.1695382\nloss:  17156.3515503\nloss:  13263.675648\nloss:  10980.2816269\nloss:  11231.1720589\nloss:  7322.0585567\nloss:  9150.4534486\nloss:  12504.9455976\nloss:  10636.9359541\nloss:  8429.2563446\nloss:  5167.7016573\nloss:  7893.073584\nloss:  7127.2575889\nloss:  8848.1689746\nloss:  7766.2236686\nloss:  8599.2677275\nloss:  9155.7630087\nEpoch    23: reducing learning rate of group 0 to 1.0000e-04.\nloss:  7540.219268\n</code></pre>\n<p>and with <code>patience=10</code> the LR is not reduced at all. From my understanding, however, in all cases the LR should be reduced after epoch 11.</p>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.2.148</p>\n<p>OS: Peppermint 9 Nine<br>\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.2.148<br>\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB<br>\nNvidia driver version: 396.54<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/cuda-9.2/lib64/libcudnn.so<br>\n/usr/local/cuda-9.2/lib64/libcudnn.so.7<br>\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1<br>\n/usr/local/cuda-9.2/lib64/libcudnn_static.a</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.3)<br>\n[pip] numpydoc (0.8.0)<br>\n[pip] torch (0.4.1)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] cuda92                    1.0                           0    pytorch<br>\n[conda] pytorch                   0.4.1           py36_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch<br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>", "body_text": "Issue description\nAs far as I understand, patience only determines after how many epochs the LR will be reduced, but has nothing to do with how much worse the value has to be for the LR to be reduced (which should be handled by threshold). Nevertheless, with the same losses, which stop decreasing after around epoch 10, the point at which the LR is reduced is far more than patience epochs, and how much more depends (weirdly) on the value of patience.\nCode example\nmodel = nn.Linear(10, 2)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, patience=0, verbose=True)\n\ntest_loss = [301427.7770613, 160481.1660669, 117763.6467930, 81528.4339311, 61726.3664427, 46429.0652291, 35138.6479489, 23490.1695382, 17156.3515503, 13263.6756480, 10980.2816269, 11231.1720589, 7322.0585567, 9150.4534486, 12504.9455976, 10636.9359541, 8429.2563446, 5167.7016573, 7893.0735840, 7127.2575889, 8848.1689746, 7766.2236686, 8599.2677275, 9155.7630087, 7540.2192680]\n\nfor i in test_loss:\n    print('loss: ', i)\n    scheduler.step(i)\nWith patience=0, the output is:\nloss:  301427.7770613\nloss:  160481.1660669\nloss:  117763.646793\nloss:  81528.4339311\nloss:  61726.3664427\nloss:  46429.0652291\nloss:  35138.6479489\nloss:  23490.1695382\nloss:  17156.3515503\nloss:  13263.675648\nloss:  10980.2816269\nloss:  11231.1720589\nEpoch    11: reducing learning rate of group 0 to 1.0000e-04.\nloss:  7322.0585567\nloss:  9150.4534486\nEpoch    13: reducing learning rate of group 0 to 1.0000e-05.\nloss:  12504.9455976\nEpoch    14: reducing learning rate of group 0 to 1.0000e-06.\nloss:  10636.9359541\nEpoch    15: reducing learning rate of group 0 to 1.0000e-07.\nloss:  8429.2563446\nEpoch    16: reducing learning rate of group 0 to 1.0000e-08.\nloss:  5167.7016573\nloss:  7893.073584\nloss:  7127.2575889\nloss:  8848.1689746\nloss:  7766.2236686\nloss:  8599.2677275\nloss:  9155.7630087\nloss:  7540.219268\n\nwith patience=5 it is\nloss:  301427.7770613\nloss:  160481.1660669\nloss:  117763.646793\nloss:  81528.4339311\nloss:  61726.3664427\nloss:  46429.0652291\nloss:  35138.6479489\nloss:  23490.1695382\nloss:  17156.3515503\nloss:  13263.675648\nloss:  10980.2816269\nloss:  11231.1720589\nloss:  7322.0585567\nloss:  9150.4534486\nloss:  12504.9455976\nloss:  10636.9359541\nloss:  8429.2563446\nloss:  5167.7016573\nloss:  7893.073584\nloss:  7127.2575889\nloss:  8848.1689746\nloss:  7766.2236686\nloss:  8599.2677275\nloss:  9155.7630087\nEpoch    23: reducing learning rate of group 0 to 1.0000e-04.\nloss:  7540.219268\n\nand with patience=10 the LR is not reduced at all. From my understanding, however, in all cases the LR should be reduced after epoch 11.\nSystem Info\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\nOS: Peppermint 9 Nine\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0\nCMake version: Could not collect\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.2.148\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB\nNvidia driver version: 396.54\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.2/lib64/libcudnn.so\n/usr/local/cuda-9.2/lib64/libcudnn.so.7\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\nVersions of relevant libraries:\n[pip] numpy (1.14.3)\n[pip] numpydoc (0.8.0)\n[pip] torch (0.4.1)\n[pip] torchvision (0.2.1)\n[conda] cuda92                    1.0                           0    pytorch\n[conda] pytorch                   0.4.1           py36_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "## Issue description\r\n\r\nAs far as I understand, `patience` only determines after how many epochs the LR will be reduced, but has nothing to do with how much worse the value has to be for the LR to be reduced (which should be handled by `threshold`). Nevertheless, with the same losses, which stop decreasing after around epoch 10, the point at which the LR is reduced is far more than `patience` epochs, and how much more depends (weirdly) on the value of `patience`.\r\n\r\n## Code example\r\n```python\r\nmodel = nn.Linear(10, 2)\r\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\r\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\r\n    optimizer, patience=0, verbose=True)\r\n\r\ntest_loss = [301427.7770613, 160481.1660669, 117763.6467930, 81528.4339311, 61726.3664427, 46429.0652291, 35138.6479489, 23490.1695382, 17156.3515503, 13263.6756480, 10980.2816269, 11231.1720589, 7322.0585567, 9150.4534486, 12504.9455976, 10636.9359541, 8429.2563446, 5167.7016573, 7893.0735840, 7127.2575889, 8848.1689746, 7766.2236686, 8599.2677275, 9155.7630087, 7540.2192680]\r\n\r\nfor i in test_loss:\r\n    print('loss: ', i)\r\n    scheduler.step(i)\r\n```\r\nWith `patience=0`, the output is:\r\n```\r\nloss:  301427.7770613\r\nloss:  160481.1660669\r\nloss:  117763.646793\r\nloss:  81528.4339311\r\nloss:  61726.3664427\r\nloss:  46429.0652291\r\nloss:  35138.6479489\r\nloss:  23490.1695382\r\nloss:  17156.3515503\r\nloss:  13263.675648\r\nloss:  10980.2816269\r\nloss:  11231.1720589\r\nEpoch    11: reducing learning rate of group 0 to 1.0000e-04.\r\nloss:  7322.0585567\r\nloss:  9150.4534486\r\nEpoch    13: reducing learning rate of group 0 to 1.0000e-05.\r\nloss:  12504.9455976\r\nEpoch    14: reducing learning rate of group 0 to 1.0000e-06.\r\nloss:  10636.9359541\r\nEpoch    15: reducing learning rate of group 0 to 1.0000e-07.\r\nloss:  8429.2563446\r\nEpoch    16: reducing learning rate of group 0 to 1.0000e-08.\r\nloss:  5167.7016573\r\nloss:  7893.073584\r\nloss:  7127.2575889\r\nloss:  8848.1689746\r\nloss:  7766.2236686\r\nloss:  8599.2677275\r\nloss:  9155.7630087\r\nloss:  7540.219268\r\n```\r\nwith `patience=5` it is\r\n```\r\nloss:  301427.7770613\r\nloss:  160481.1660669\r\nloss:  117763.646793\r\nloss:  81528.4339311\r\nloss:  61726.3664427\r\nloss:  46429.0652291\r\nloss:  35138.6479489\r\nloss:  23490.1695382\r\nloss:  17156.3515503\r\nloss:  13263.675648\r\nloss:  10980.2816269\r\nloss:  11231.1720589\r\nloss:  7322.0585567\r\nloss:  9150.4534486\r\nloss:  12504.9455976\r\nloss:  10636.9359541\r\nloss:  8429.2563446\r\nloss:  5167.7016573\r\nloss:  7893.073584\r\nloss:  7127.2575889\r\nloss:  8848.1689746\r\nloss:  7766.2236686\r\nloss:  8599.2677275\r\nloss:  9155.7630087\r\nEpoch    23: reducing learning rate of group 0 to 1.0000e-04.\r\nloss:  7540.219268\r\n```\r\nand with `patience=10` the LR is not reduced at all. From my understanding, however, in all cases the LR should be reduced after epoch 11.\r\n\r\n## System Info\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Peppermint 9 Nine\r\nGCC version: (Ubuntu 7.3.0-16ubuntu3) 7.3.0\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1060 6GB\r\nNvidia driver version: 396.54\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] numpydoc (0.8.0)\r\n[pip] torch (0.4.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] cuda92                    1.0                           0    pytorch\r\n[conda] pytorch                   0.4.1           py36_cuda9.2.148_cudnn7.1.4_1  [cuda92]  pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n\r\n"}