{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12362", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12362/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12362/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12362/events", "html_url": "https://github.com/pytorch/pytorch/issues/12362", "id": 367078558, "node_id": "MDU6SXNzdWUzNjcwNzg1NTg=", "number": 12362, "title": "affine_grid function doesn't check the data type of theta", "user": {"login": "Steve-Tod", "id": 26958167, "node_id": "MDQ6VXNlcjI2OTU4MTY3", "avatar_url": "https://avatars1.githubusercontent.com/u/26958167?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Steve-Tod", "html_url": "https://github.com/Steve-Tod", "followers_url": "https://api.github.com/users/Steve-Tod/followers", "following_url": "https://api.github.com/users/Steve-Tod/following{/other_user}", "gists_url": "https://api.github.com/users/Steve-Tod/gists{/gist_id}", "starred_url": "https://api.github.com/users/Steve-Tod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Steve-Tod/subscriptions", "organizations_url": "https://api.github.com/users/Steve-Tod/orgs", "repos_url": "https://api.github.com/users/Steve-Tod/repos", "events_url": "https://api.github.com/users/Steve-Tod/events{/privacy}", "received_events_url": "https://api.github.com/users/Steve-Tod/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-10-05T06:37:45Z", "updated_at": "2018-10-15T18:00:01Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>I was trying to use torch.nn.functional.affine_grid() to generate grid and grid_sample() to warp a image. However, if I use the identical affine matrix, I can't get the right output, which should be the same as the input.</p>\n\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<ol>\n<li>let theta be the identical affine matrix</li>\n<li>warp affine with affine_grid and grid_sample</li>\n<li>display the image</li>\n</ol>\n\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> matplotlib.pyplot <span class=\"pl-k\">as</span> plt\n\nimg <span class=\"pl-k\">=</span> plt.imread(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lena.jpeg<span class=\"pl-pds\">\"</span></span>)\nimg <span class=\"pl-k\">=</span> img[<span class=\"pl-c1\">110</span>:<span class=\"pl-c1\">120</span>, <span class=\"pl-c1\">110</span>:<span class=\"pl-c1\">120</span>]\nimg <span class=\"pl-k\">=</span> img.reshape(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">3</span>)\nimg_tensor <span class=\"pl-k\">=</span> torch.from_numpy(img)\nimg_tensor <span class=\"pl-k\">=</span> img_tensor.permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>)\n\ntheta <span class=\"pl-k\">=</span> torch.tensor([[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>]])\ntheta <span class=\"pl-k\">=</span> theta.view(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>)\n\ngrid <span class=\"pl-k\">=</span> torch.nn.functional.affine_grid(theta, img_tensor.size())\nout <span class=\"pl-k\">=</span> torch.nn.functional.grid_sample(img_tensor.float(), grid.float())\n\nout_img <span class=\"pl-k\">=</span> out.permute(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>).data.numpy()\nout_img <span class=\"pl-k\">=</span> out_img.reshape(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">3</span>)\nplt.imshow(out_img.astype(<span class=\"pl-c1\">int</span>))\nplt.show()</pre></div>\n<h2>Expected behavior</h2>\n<p>The output should be the same as the input.</p>\n\n<h2>Environment</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: None</p>\n<p>OS: Mac OSX 10.13.6<br>\nGCC version: Could not collect<br>\nCMake version: version 3.12.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: No<br>\nCUDA runtime version: No CUDA<br>\nGPU models and configuration: No CUDA<br>\nNvidia driver version: No CUDA<br>\ncuDNN version: No CUDA</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.15.1)<br>\n[pip] torch (0.4.1)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] pytorch                   0.4.1           py36_cuda0.0_cudnn0.0_1    pytorch<br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>\n<h2>Additional context</h2>\n<p>After I debug step by step, I found these codes in vision.py, form line 43 to 52:</p>\n<div class=\"highlight highlight-source-python\"><pre>base_grid <span class=\"pl-k\">=</span> theta.new(N, H, W, <span class=\"pl-c1\">3</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> base_grid has the same data type as theta</span>\nlinear_points <span class=\"pl-k\">=</span> torch.linspace(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, W) <span class=\"pl-k\">if</span> W <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">else</span> torch.Tensor([<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\nbase_grid[:, :, :, <span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> torch.ger(torch.ones(H), linear_points).expand_as(base_grid[:, :, :, <span class=\"pl-c1\">0</span>])\nlinear_points <span class=\"pl-k\">=</span> torch.linspace(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, H) <span class=\"pl-k\">if</span> H <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">else</span> torch.Tensor([<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\nbase_grid[:, :, :, <span class=\"pl-c1\">1</span>] <span class=\"pl-k\">=</span> torch.ger(linear_points, torch.ones(W)).expand_as(base_grid[:, :, :, <span class=\"pl-c1\">1</span>])\nbase_grid[:, :, :, <span class=\"pl-c1\">2</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nctx.base_grid <span class=\"pl-k\">=</span> base_grid\ngrid <span class=\"pl-k\">=</span> torch.bmm(base_grid.view(N, H <span class=\"pl-k\">*</span> W, <span class=\"pl-c1\">3</span>), theta.transpose(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>))\ngrid <span class=\"pl-k\">=</span> grid.view(N, H, W, <span class=\"pl-c1\">2</span>)\n<span class=\"pl-k\">return</span> grid</pre></div>\n<p>Since affine_grid doesn't check the data type of theta, when I use identical affine matrix, the data type of theta is int, so is the base_grid. Finally, there are only 1, -1 and 0 in base_grid and grid. That's why the problem happens. After I use theta.float() rather than theta, everything works well.</p>\n", "body_text": "\ud83d\udc1b Bug\nI was trying to use torch.nn.functional.affine_grid() to generate grid and grid_sample() to warp a image. However, if I use the identical affine matrix, I can't get the right output, which should be the same as the input.\n\nTo Reproduce\nSteps to reproduce the behavior:\n\nlet theta be the identical affine matrix\nwarp affine with affine_grid and grid_sample\ndisplay the image\n\n\nimport torch\nimport matplotlib.pyplot as plt\n\nimg = plt.imread(\"lena.jpeg\")\nimg = img[110:120, 110:120]\nimg = img.reshape(1,10,10,3)\nimg_tensor = torch.from_numpy(img)\nimg_tensor = img_tensor.permute(0, 3, 1, 2)\n\ntheta = torch.tensor([[1, 0, 0], [0, 1, 0]])\ntheta = theta.view(1, 2, 3)\n\ngrid = torch.nn.functional.affine_grid(theta, img_tensor.size())\nout = torch.nn.functional.grid_sample(img_tensor.float(), grid.float())\n\nout_img = out.permute(0, 2, 3, 1).data.numpy()\nout_img = out_img.reshape(10, 10, 3)\nplt.imshow(out_img.astype(int))\nplt.show()\nExpected behavior\nThe output should be the same as the input.\n\nEnvironment\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: None\nOS: Mac OSX 10.13.6\nGCC version: Could not collect\nCMake version: version 3.12.2\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nVersions of relevant libraries:\n[pip] numpy (1.15.1)\n[pip] torch (0.4.1)\n[pip] torchvision (0.2.1)\n[conda] pytorch                   0.4.1           py36_cuda0.0_cudnn0.0_1    pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch\nAdditional context\nAfter I debug step by step, I found these codes in vision.py, form line 43 to 52:\nbase_grid = theta.new(N, H, W, 3) # base_grid has the same data type as theta\nlinear_points = torch.linspace(-1, 1, W) if W > 1 else torch.Tensor([-1])\nbase_grid[:, :, :, 0] = torch.ger(torch.ones(H), linear_points).expand_as(base_grid[:, :, :, 0])\nlinear_points = torch.linspace(-1, 1, H) if H > 1 else torch.Tensor([-1])\nbase_grid[:, :, :, 1] = torch.ger(linear_points, torch.ones(W)).expand_as(base_grid[:, :, :, 1])\nbase_grid[:, :, :, 2] = 1\nctx.base_grid = base_grid\ngrid = torch.bmm(base_grid.view(N, H * W, 3), theta.transpose(1, 2))\ngrid = grid.view(N, H, W, 2)\nreturn grid\nSince affine_grid doesn't check the data type of theta, when I use identical affine matrix, the data type of theta is int, so is the base_grid. Finally, there are only 1, -1 and 0 in base_grid and grid. That's why the problem happens. After I use theta.float() rather than theta, everything works well.", "body": "## \ud83d\udc1b Bug\r\n\r\nI was trying to use torch.nn.functional.affine_grid() to generate grid and grid_sample() to warp a image. However, if I use the identical affine matrix, I can't get the right output, which should be the same as the input.\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1. let theta be the identical affine matrix\r\n2. warp affine with affine_grid and grid_sample\r\n3. display the image\r\n\r\n<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->\r\n\r\n```python\r\nimport torch\r\nimport matplotlib.pyplot as plt\r\n\r\nimg = plt.imread(\"lena.jpeg\")\r\nimg = img[110:120, 110:120]\r\nimg = img.reshape(1,10,10,3)\r\nimg_tensor = torch.from_numpy(img)\r\nimg_tensor = img_tensor.permute(0, 3, 1, 2)\r\n\r\ntheta = torch.tensor([[1, 0, 0], [0, 1, 0]])\r\ntheta = theta.view(1, 2, 3)\r\n\r\ngrid = torch.nn.functional.affine_grid(theta, img_tensor.size())\r\nout = torch.nn.functional.grid_sample(img_tensor.float(), grid.float())\r\n\r\nout_img = out.permute(0, 2, 3, 1).data.numpy()\r\nout_img = out_img.reshape(10, 10, 3)\r\nplt.imshow(out_img.astype(int))\r\nplt.show()\r\n```\r\n\r\n## Expected behavior\r\n\r\nThe output should be the same as the input.\r\n<!-- A clear and concise description of what you expected to happen. -->\r\n\r\n## Environment\r\n\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.13.6\r\nGCC version: Could not collect\r\nCMake version: version 3.12.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.1)\r\n[pip] torch (0.4.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.1           py36_cuda0.0_cudnn0.0_1    pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n\r\n\r\n## Additional context\r\n\r\nAfter I debug step by step, I found these codes in vision.py, form line 43 to 52:\r\n\r\n```python\r\nbase_grid = theta.new(N, H, W, 3) # base_grid has the same data type as theta\r\nlinear_points = torch.linspace(-1, 1, W) if W > 1 else torch.Tensor([-1])\r\nbase_grid[:, :, :, 0] = torch.ger(torch.ones(H), linear_points).expand_as(base_grid[:, :, :, 0])\r\nlinear_points = torch.linspace(-1, 1, H) if H > 1 else torch.Tensor([-1])\r\nbase_grid[:, :, :, 1] = torch.ger(linear_points, torch.ones(W)).expand_as(base_grid[:, :, :, 1])\r\nbase_grid[:, :, :, 2] = 1\r\nctx.base_grid = base_grid\r\ngrid = torch.bmm(base_grid.view(N, H * W, 3), theta.transpose(1, 2))\r\ngrid = grid.view(N, H, W, 2)\r\nreturn grid\r\n```\r\n\r\nSince affine_grid doesn't check the data type of theta, when I use identical affine matrix, the data type of theta is int, so is the base_grid. Finally, there are only 1, -1 and 0 in base_grid and grid. That's why the problem happens. After I use theta.float() rather than theta, everything works well.\r\n<!-- Add any other context about the problem here. -->"}