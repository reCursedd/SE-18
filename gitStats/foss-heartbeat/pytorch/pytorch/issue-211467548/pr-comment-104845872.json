{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104845872", "pull_request_review_id": 25699277, "id": 104845872, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNDg0NTg3Mg==", "diff_hunk": "@@ -344,31 +344,25 @@ def forward(self, input, output_size=None):\n \n     def _output_padding(self, input, output_size):\n         if output_size is None:\n-            return self.output_padding\n+            return self.padding, self.output_padding\n \n         output_size = list(output_size)\n         k = input.dim() - 2\n         if len(output_size) == k + 2:\n-            output_size = output_size[-2:]\n+            output_size = output_size[2:]\n         if len(output_size) != k:\n             raise ValueError(\n                 \"output_size must have {} or {} elements (got {})\"\n                 .format(k, k + 2, len(output_size)))\n \n-        def dim_size(d):\n-            return ((input.size(d + 2) - 1) * self.stride[d] -\n-                    2 * self.padding[d] + self.kernel_size[d])\n-\n-        min_sizes = [dim_size(d) for d in range(k)]\n-        max_sizes = [min_sizes[d] + self.stride[d] - 1 for d in range(k)]\n-        for size, min_size, max_size in zip(output_size, min_sizes, max_sizes):\n-            if size < min_size or size > max_size:\n-                raise ValueError((\n-                    \"requested an output size of {}, but valid sizes range \"\n-                    \"from {} to {} (for an input of {})\").format(\n-                        output_size, min_sizes, max_sizes, input.size()[2:]))\n-\n-        return tuple([output_size[d] - min_sizes[d] for d in range(k)])\n+        # 2*padding - output_padding = (input - 1)*stride - output + kernel_size\n+        #                            = no_pad_diff\n+        # padding = math.ceil(no_pad_diff / 2)\n+        def no_pad_diff(d):\n+            return ((input.size(d + 2) - 1) * self.stride[d] - output_size[d] + self.kernel_size[d])\n+        padding = tuple([(no_pad_diff(d) + 1) // 2 for d in range(k)])", "path": "torch/nn/modules/conv.py", "position": null, "original_position": 45, "commit_id": "45dc0d547fabbe4eb22b1ce544dda4b0d62d8e3e", "original_commit_id": "f097fb3dd19b19b84196eb029a86a12df2d0cc79", "user": {"login": "felixgwu", "id": 7753366, "node_id": "MDQ6VXNlcjc3NTMzNjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/7753366?v=4", "gravatar_id": "", "url": "https://api.github.com/users/felixgwu", "html_url": "https://github.com/felixgwu", "followers_url": "https://api.github.com/users/felixgwu/followers", "following_url": "https://api.github.com/users/felixgwu/following{/other_user}", "gists_url": "https://api.github.com/users/felixgwu/gists{/gist_id}", "starred_url": "https://api.github.com/users/felixgwu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/felixgwu/subscriptions", "organizations_url": "https://api.github.com/users/felixgwu/orgs", "repos_url": "https://api.github.com/users/felixgwu/repos", "events_url": "https://api.github.com/users/felixgwu/events{/privacy}", "received_events_url": "https://api.github.com/users/felixgwu/received_events", "type": "User", "site_admin": false}, "body": "That's right. It's not compatible to the old version.\r\nThe original version tries to keep the self.padding and pad the output on one side. \r\nHowever, this version pads the output symmetrically on both sides.\r\nYou may just modify your code as follows and print the output and look at the values on the boundaries.\r\n```python\r\nm = nn.ConvTranspose2d(3, 4, 3, 3)\r\ni = Variable(torch.randn(2, 3, 6, 6))\r\no = m(i, output_size=(20, 20))\r\nprint(o)\r\n```\r\n\r\nI wonder what would be a better practice.\r\nFrom my point of view, ConvTranspose2d is wildly used in GANs or AutoEncoders, so padding outputs symmetrically when generating images is more reasonable.\r\n\r\nIf you think compatibility is more important, then there two ways to adjust this:\r\n1. Try to use self.padding first. If it is impossible to find such output_padding that matches the output_size, then we use the old method. Otherwise, we use the new one.\r\n2. Find a new padding that is the closest to self.padding such that there exists a output_padding (between 0 and self.stride-1) matches the output_size.\r\n", "created_at": "2017-03-08T05:17:41Z", "updated_at": "2018-11-23T15:32:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/904#discussion_r104845872", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/904", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104845872"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/904#discussion_r104845872"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/904"}}, "body_html": "<p>That's right. It's not compatible to the old version.<br>\nThe original version tries to keep the self.padding and pad the output on one side.<br>\nHowever, this version pads the output symmetrically on both sides.<br>\nYou may just modify your code as follows and print the output and look at the values on the boundaries.</p>\n<div class=\"highlight highlight-source-python\"><pre>m <span class=\"pl-k\">=</span> nn.ConvTranspose2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>)\ni <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">6</span>))\no <span class=\"pl-k\">=</span> m(i, <span class=\"pl-v\">output_size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">20</span>))\n<span class=\"pl-c1\">print</span>(o)</pre></div>\n<p>I wonder what would be a better practice.<br>\nFrom my point of view, ConvTranspose2d is wildly used in GANs or AutoEncoders, so padding outputs symmetrically when generating images is more reasonable.</p>\n<p>If you think compatibility is more important, then there two ways to adjust this:</p>\n<ol>\n<li>Try to use self.padding first. If it is impossible to find such output_padding that matches the output_size, then we use the old method. Otherwise, we use the new one.</li>\n<li>Find a new padding that is the closest to self.padding such that there exists a output_padding (between 0 and self.stride-1) matches the output_size.</li>\n</ol>", "body_text": "That's right. It's not compatible to the old version.\nThe original version tries to keep the self.padding and pad the output on one side.\nHowever, this version pads the output symmetrically on both sides.\nYou may just modify your code as follows and print the output and look at the values on the boundaries.\nm = nn.ConvTranspose2d(3, 4, 3, 3)\ni = Variable(torch.randn(2, 3, 6, 6))\no = m(i, output_size=(20, 20))\nprint(o)\nI wonder what would be a better practice.\nFrom my point of view, ConvTranspose2d is wildly used in GANs or AutoEncoders, so padding outputs symmetrically when generating images is more reasonable.\nIf you think compatibility is more important, then there two ways to adjust this:\n\nTry to use self.padding first. If it is impossible to find such output_padding that matches the output_size, then we use the old method. Otherwise, we use the new one.\nFind a new padding that is the closest to self.padding such that there exists a output_padding (between 0 and self.stride-1) matches the output_size.", "in_reply_to_id": 104782801}