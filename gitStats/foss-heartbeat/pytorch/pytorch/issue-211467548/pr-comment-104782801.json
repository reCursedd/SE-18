{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104782801", "pull_request_review_id": 25634701, "id": 104782801, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNDc4MjgwMQ==", "diff_hunk": "@@ -344,31 +344,25 @@ def forward(self, input, output_size=None):\n \n     def _output_padding(self, input, output_size):\n         if output_size is None:\n-            return self.output_padding\n+            return self.padding, self.output_padding\n \n         output_size = list(output_size)\n         k = input.dim() - 2\n         if len(output_size) == k + 2:\n-            output_size = output_size[-2:]\n+            output_size = output_size[2:]\n         if len(output_size) != k:\n             raise ValueError(\n                 \"output_size must have {} or {} elements (got {})\"\n                 .format(k, k + 2, len(output_size)))\n \n-        def dim_size(d):\n-            return ((input.size(d + 2) - 1) * self.stride[d] -\n-                    2 * self.padding[d] + self.kernel_size[d])\n-\n-        min_sizes = [dim_size(d) for d in range(k)]\n-        max_sizes = [min_sizes[d] + self.stride[d] - 1 for d in range(k)]\n-        for size, min_size, max_size in zip(output_size, min_sizes, max_sizes):\n-            if size < min_size or size > max_size:\n-                raise ValueError((\n-                    \"requested an output size of {}, but valid sizes range \"\n-                    \"from {} to {} (for an input of {})\").format(\n-                        output_size, min_sizes, max_sizes, input.size()[2:]))\n-\n-        return tuple([output_size[d] - min_sizes[d] for d in range(k)])\n+        # 2*padding - output_padding = (input - 1)*stride - output + kernel_size\n+        #                            = no_pad_diff\n+        # padding = math.ceil(no_pad_diff / 2)\n+        def no_pad_diff(d):\n+            return ((input.size(d + 2) - 1) * self.stride[d] - output_size[d] + self.kernel_size[d])\n+        padding = tuple([(no_pad_diff(d) + 1) // 2 for d in range(k)])", "path": "torch/nn/modules/conv.py", "position": null, "original_position": 45, "commit_id": "45dc0d547fabbe4eb22b1ce544dda4b0d62d8e3e", "original_commit_id": "f097fb3dd19b19b84196eb029a86a12df2d0cc79", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "My only concern is that I think that it's not compatible with what we've had before, is it? For example take a look at this code:\r\n```\r\nm = nn.ConvTranspose2d(3, 4, 3, 3)\r\ni = Variable(torch.randn(2, 3, 6, 6))\r\no = m(i)\r\n```\r\n`o` will be of size `(2, 4, 18, 18)`, but we might actually want to get `(2, 4, 20, 20)`. The old formula would simply add `2` of output padding, but as far as I understand, this one will use -1 for padding and 0 for output padding (`no_pad_diff` will be -2 for each dim)? ", "created_at": "2017-03-07T21:25:14Z", "updated_at": "2018-11-23T15:32:46Z", "html_url": "https://github.com/pytorch/pytorch/pull/904#discussion_r104782801", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/904", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/104782801"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/904#discussion_r104782801"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/904"}}, "body_html": "<p>My only concern is that I think that it's not compatible with what we've had before, is it? For example take a look at this code:</p>\n<pre><code>m = nn.ConvTranspose2d(3, 4, 3, 3)\ni = Variable(torch.randn(2, 3, 6, 6))\no = m(i)\n</code></pre>\n<p><code>o</code> will be of size <code>(2, 4, 18, 18)</code>, but we might actually want to get <code>(2, 4, 20, 20)</code>. The old formula would simply add <code>2</code> of output padding, but as far as I understand, this one will use -1 for padding and 0 for output padding (<code>no_pad_diff</code> will be -2 for each dim)?</p>", "body_text": "My only concern is that I think that it's not compatible with what we've had before, is it? For example take a look at this code:\nm = nn.ConvTranspose2d(3, 4, 3, 3)\ni = Variable(torch.randn(2, 3, 6, 6))\no = m(i)\n\no will be of size (2, 4, 18, 18), but we might actually want to get (2, 4, 20, 20). The old formula would simply add 2 of output padding, but as far as I understand, this one will use -1 for padding and 0 for output padding (no_pad_diff will be -2 for each dim)?"}