{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/378472105", "html_url": "https://github.com/pytorch/pytorch/issues/1668#issuecomment-378472105", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1668", "id": 378472105, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODQ3MjEwNQ==", "user": {"login": "DuaneNielsen", "id": 11070508, "node_id": "MDQ6VXNlcjExMDcwNTA4", "avatar_url": "https://avatars0.githubusercontent.com/u/11070508?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DuaneNielsen", "html_url": "https://github.com/DuaneNielsen", "followers_url": "https://api.github.com/users/DuaneNielsen/followers", "following_url": "https://api.github.com/users/DuaneNielsen/following{/other_user}", "gists_url": "https://api.github.com/users/DuaneNielsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/DuaneNielsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DuaneNielsen/subscriptions", "organizations_url": "https://api.github.com/users/DuaneNielsen/orgs", "repos_url": "https://api.github.com/users/DuaneNielsen/repos", "events_url": "https://api.github.com/users/DuaneNielsen/events{/privacy}", "received_events_url": "https://api.github.com/users/DuaneNielsen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-04T03:51:36Z", "updated_at": "2018-04-04T03:51:36Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16739780\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/BoyuanJiang\">@BoyuanJiang</a> I second that...</p>\n<p>If there is 1 CUDA capable device on the system, I think it should by default use it, unless some global setting says otherwise, or the user specifically codes it.  If no CUDA, then default to CPU.</p>\n<p>If you have a CUDA device, and want to use CPU instead, then I think it's OK to ask the developer to specify the CPU, as its kinda an edge case.  (avoiding kernel launches for really small recurrent stuff, or whatever).  In most of these cases, using CPU instead of CUDA is a for optimization, so only the developer can figure that out anway.</p>\n<p>As it stands, requiring the developer to specifically say \"I want to run this on GPU\", seems a bit odd.  Since that's kinda the point of deep learning, at least in it's current state of the art.</p>", "body_text": "@BoyuanJiang I second that...\nIf there is 1 CUDA capable device on the system, I think it should by default use it, unless some global setting says otherwise, or the user specifically codes it.  If no CUDA, then default to CPU.\nIf you have a CUDA device, and want to use CPU instead, then I think it's OK to ask the developer to specify the CPU, as its kinda an edge case.  (avoiding kernel launches for really small recurrent stuff, or whatever).  In most of these cases, using CPU instead of CUDA is a for optimization, so only the developer can figure that out anway.\nAs it stands, requiring the developer to specifically say \"I want to run this on GPU\", seems a bit odd.  Since that's kinda the point of deep learning, at least in it's current state of the art.", "body": "@BoyuanJiang I second that...\r\n\r\nIf there is 1 CUDA capable device on the system, I think it should by default use it, unless some global setting says otherwise, or the user specifically codes it.  If no CUDA, then default to CPU.\r\n\r\nIf you have a CUDA device, and want to use CPU instead, then I think it's OK to ask the developer to specify the CPU, as its kinda an edge case.  (avoiding kernel launches for really small recurrent stuff, or whatever).  In most of these cases, using CPU instead of CUDA is a for optimization, so only the developer can figure that out anway.\r\n\r\nAs it stands, requiring the developer to specifically say \"I want to run this on GPU\", seems a bit odd.  Since that's kinda the point of deep learning, at least in it's current state of the art."}