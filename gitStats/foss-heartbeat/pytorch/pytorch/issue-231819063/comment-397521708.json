{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/397521708", "html_url": "https://github.com/pytorch/pytorch/issues/1668#issuecomment-397521708", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1668", "id": 397521708, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NzUyMTcwOA==", "user": {"login": "gobbedy", "id": 26829704, "node_id": "MDQ6VXNlcjI2ODI5NzA0", "avatar_url": "https://avatars3.githubusercontent.com/u/26829704?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gobbedy", "html_url": "https://github.com/gobbedy", "followers_url": "https://api.github.com/users/gobbedy/followers", "following_url": "https://api.github.com/users/gobbedy/following{/other_user}", "gists_url": "https://api.github.com/users/gobbedy/gists{/gist_id}", "starred_url": "https://api.github.com/users/gobbedy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gobbedy/subscriptions", "organizations_url": "https://api.github.com/users/gobbedy/orgs", "repos_url": "https://api.github.com/users/gobbedy/repos", "events_url": "https://api.github.com/users/gobbedy/events{/privacy}", "received_events_url": "https://api.github.com/users/gobbedy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-15T05:58:40Z", "updated_at": "2018-06-15T06:20:51Z", "author_association": "NONE", "body_html": "<p>Thank you <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a>.</p>\n<p>I see this as a step in the right direction. Now code can be written device-agnostically by adding .to(device) wherever you create a tensor.</p>\n<p>This removes an if statement around each creation of a tensor, which is great.</p>\n<p>However, this is remains a very code-heavy and not backward-compatible solution. It does not address <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16739780\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/BoyuanJiang\">@BoyuanJiang</a>  and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11070508\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/DuaneNielsen\">@DuaneNielsen</a>'s request to have code run on GPU by default.</p>\n<p>I'd like to be able to add a single line to my otherwise CPU-only code to make it run on a GPU.</p>\n<p>Something like<br>\n<code>if torch.cuda.is_available():</code><br>\n\u00a0\u00a0\u00a0\u00a0<code>torch.set_gpu_as_default_device()</code></p>\n<p>(or a more flexible/smart function that allows you to pick the device, yet achieves the same result.)</p>\n<p>Then I'd like any subsequent code such as this<br>\n<code>my_tensor = torch.empty(3,3)</code></p>\n<p>to automatically run on GPU without requiring either .cuda() or .to(device) -- nor dtype or device arguments. Just plain pythonic KISS.</p>", "body_text": "Thank you @fmassa.\nI see this as a step in the right direction. Now code can be written device-agnostically by adding .to(device) wherever you create a tensor.\nThis removes an if statement around each creation of a tensor, which is great.\nHowever, this is remains a very code-heavy and not backward-compatible solution. It does not address @BoyuanJiang  and @DuaneNielsen's request to have code run on GPU by default.\nI'd like to be able to add a single line to my otherwise CPU-only code to make it run on a GPU.\nSomething like\nif torch.cuda.is_available():\n\u00a0\u00a0\u00a0\u00a0torch.set_gpu_as_default_device()\n(or a more flexible/smart function that allows you to pick the device, yet achieves the same result.)\nThen I'd like any subsequent code such as this\nmy_tensor = torch.empty(3,3)\nto automatically run on GPU without requiring either .cuda() or .to(device) -- nor dtype or device arguments. Just plain pythonic KISS.", "body": "Thank you @fmassa.\r\n\r\nI see this as a step in the right direction. Now code can be written device-agnostically by adding .to(device) wherever you create a tensor.\r\n\r\nThis removes an if statement around each creation of a tensor, which is great.\r\n\r\nHowever, this is remains a very code-heavy and not backward-compatible solution. It does not address @BoyuanJiang  and @DuaneNielsen's request to have code run on GPU by default.\r\n\r\nI'd like to be able to add a single line to my otherwise CPU-only code to make it run on a GPU.\r\n\r\nSomething like\r\n`if torch.cuda.is_available():`  \r\n&nbsp;&nbsp;&nbsp;&nbsp;`torch.set_gpu_as_default_device()`  \r\n\r\n(or a more flexible/smart function that allows you to pick the device, yet achieves the same result.)\r\n\r\nThen I'd like any subsequent code such as this  \r\n`my_tensor = torch.empty(3,3)`  \r\n\r\nto automatically run on GPU without requiring either .cuda() or .to(device) -- nor dtype or device arguments. Just plain pythonic KISS."}