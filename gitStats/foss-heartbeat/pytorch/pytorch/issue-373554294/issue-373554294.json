{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13053", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13053/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13053/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13053/events", "html_url": "https://github.com/pytorch/pytorch/issues/13053", "id": 373554294, "node_id": "MDU6SXNzdWUzNzM1NTQyOTQ=", "number": 13053, "title": "Generalized Data Class", "user": {"login": "AntoinePrv", "id": 11088808, "node_id": "MDQ6VXNlcjExMDg4ODA4", "avatar_url": "https://avatars0.githubusercontent.com/u/11088808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AntoinePrv", "html_url": "https://github.com/AntoinePrv", "followers_url": "https://api.github.com/users/AntoinePrv/followers", "following_url": "https://api.github.com/users/AntoinePrv/following{/other_user}", "gists_url": "https://api.github.com/users/AntoinePrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/AntoinePrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AntoinePrv/subscriptions", "organizations_url": "https://api.github.com/users/AntoinePrv/orgs", "repos_url": "https://api.github.com/users/AntoinePrv/repos", "events_url": "https://api.github.com/users/AntoinePrv/events{/privacy}", "received_events_url": "https://api.github.com/users/AntoinePrv/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 466131885, "node_id": "MDU6TGFiZWw0NjYxMzE4ODU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs%20discussion", "name": "needs discussion", "color": "cc317c", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-24T15:53:20Z", "updated_at": "2018-10-29T17:51:59Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Generalized Data Class Feature</h2>\n<p>I suggest to add an abstract <code>Data</code> class in the <code>torch.utils.data</code> module. The goal of which is to group a couple of data batching behaviors for the user to define: batch together arbitrary <code>Data</code> objects, <code>chunk</code>/<code>split</code> arbitrary batched data objects across batch dimension, move arbitrary data to <code>device</code>, <code>shared_memory</code>, <code>pin_memory</code>...</p>\n<h2>Motivation</h2>\n<p>Deep learning is becoming more and more flexible and not all data is in the form <code>(X, y)</code>, that is why the current <a href=\"https://github.com/pytorch/pytorch/blob/ca03c10cefa1e126eab1446d490f9314bd236c1b/torch/utils/data/dataloader.py#L196\"><code>default_collate</code></a> of the <code>Dataloader</code> already supports <code>tuple</code> and <code>dict</code>. Nonetheless:</p>\n<ul>\n<li>A user may want to organize its data in an object with stronger type than <code>dict</code> and <code>tuple</code>;</li>\n<li>Not all batching is done by applying <code>torch.stack</code> on individual elements.</li>\n</ul>\n<p>The propose class is an easy to understand abstraction that could be passed from a <code>Dataset</code> to a <code>Dataloader</code>.</p>\n<p>In addition to <code>Dataloader</code>, this could also be used to define how to split data in <code>DataParallel</code>.</p>\n<h2>Pitch</h2>\n<p>A incomplete implementation could look like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> abc <span class=\"pl-k\">import</span> <span class=\"pl-c1\">ABC</span>, abstractmethod\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Data</span>(<span class=\"pl-c1\">ABC</span>):\n    <span class=\"pl-en\">@abstractmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">apply</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">func</span>: Callable) -&gt; <span class=\"pl-c1\">None</span>:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Apply a function on all tensors in the data class.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Could be passed functions like `lambda t: t.pin_memory()`, `lambda t: t.to(device)`.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-en\">@abstractmethod</span>\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">classmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">batch</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">cls</span></span>, <span class=\"pl-smi\">data_points</span>: List[Data]) -&gt; Data:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Batch together multiple data object.<span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-en\">@abstractmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">split</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">chunks</span>) -&gt; List[Data]:\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Split the data into chunks across the data dimension.<span class=\"pl-pds\">\"\"\"</span></span></pre></div>\n<p>Here I've represented individual data points and batched data with the same class, but this need not be the case.</p>\n<p>And in the <code>default_collate</code>, we would have something like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">if</span> <span class=\"pl-c1\">isinstance</span>(batch[<span class=\"pl-c1\">0</span>], Data):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> apply logic using` batch[0].__class__.batch`, and `batch.apply`</span></pre></div>\n<h2>Alternatives</h2>\n<p>It is already possible to have some of the desired behavior using a custom <code>collate_fn</code>. However if the user returns a class instead of <code>tuple</code> or <code>dict</code>, <code>pin_memory</code> will <a href=\"https://github.com/pytorch/pytorch/blob/ca03c10cefa1e126eab1446d490f9314bd236c1b/torch/utils/data/dataloader.py#L237\">not be applied</a>. Furthermore, the user also needs to be aware of the <code>_shared_memory</code> <a href=\"https://github.com/pytorch/pytorch/blob/ca03c10cefa1e126eab1446d490f9314bd236c1b/torch/utils/data/dataloader.py#L203\">variable</a> to reproduce in their own <code>collate_fn</code> to leverage this.</p>\n<h2>Additional context</h2>\n<p>A pratical example can be found in <a href=\"https://rusty1s.github.io/pytorch_geometric/build/html/notes/introduction.html\" rel=\"nofollow\">Pytroch_geometric</a>, where the author went into the trouble of redefining many <code>Dataloader</code> behaviors.</p>", "body_text": "Generalized Data Class Feature\nI suggest to add an abstract Data class in the torch.utils.data module. The goal of which is to group a couple of data batching behaviors for the user to define: batch together arbitrary Data objects, chunk/split arbitrary batched data objects across batch dimension, move arbitrary data to device, shared_memory, pin_memory...\nMotivation\nDeep learning is becoming more and more flexible and not all data is in the form (X, y), that is why the current default_collate of the Dataloader already supports tuple and dict. Nonetheless:\n\nA user may want to organize its data in an object with stronger type than dict and tuple;\nNot all batching is done by applying torch.stack on individual elements.\n\nThe propose class is an easy to understand abstraction that could be passed from a Dataset to a Dataloader.\nIn addition to Dataloader, this could also be used to define how to split data in DataParallel.\nPitch\nA incomplete implementation could look like this:\nfrom abc import ABC, abstractmethod\n\nclass Data(ABC):\n    @abstractmethod\n    def apply(self, func: Callable) -> None:\n    \"\"\"Apply a function on all tensors in the data class.\n\n    Could be passed functions like `lambda t: t.pin_memory()`, `lambda t: t.to(device)`.\n    \"\"\"\n\n    @abstractmethod\n    @classmethod\n    def batch(cls, data_points: List[Data]) -> Data:\n    \"\"\"Batch together multiple data object.\"\"\"\n\n    @abstractmethod\n    def split(self, chunks) -> List[Data]:\n    \"\"\"Split the data into chunks across the data dimension.\"\"\"\nHere I've represented individual data points and batched data with the same class, but this need not be the case.\nAnd in the default_collate, we would have something like:\nif isinstance(batch[0], Data):\n    # apply logic using` batch[0].__class__.batch`, and `batch.apply`\nAlternatives\nIt is already possible to have some of the desired behavior using a custom collate_fn. However if the user returns a class instead of tuple or dict, pin_memory will not be applied. Furthermore, the user also needs to be aware of the _shared_memory variable to reproduce in their own collate_fn to leverage this.\nAdditional context\nA pratical example can be found in Pytroch_geometric, where the author went into the trouble of redefining many Dataloader behaviors.", "body": "## Generalized Data Class Feature\r\nI suggest to add an abstract `Data` class in the `torch.utils.data` module. The goal of which is to group a couple of data batching behaviors for the user to define: batch together arbitrary `Data` objects, `chunk`/`split` arbitrary batched data objects across batch dimension, move arbitrary data to `device`, `shared_memory`, `pin_memory`...\r\n\r\n## Motivation\r\n\r\nDeep learning is becoming more and more flexible and not all data is in the form `(X, y)`, that is why the current [`default_collate`](https://github.com/pytorch/pytorch/blob/ca03c10cefa1e126eab1446d490f9314bd236c1b/torch/utils/data/dataloader.py#L196) of the `Dataloader` already supports `tuple` and `dict`. Nonetheless:\r\n  - A user may want to organize its data in an object with stronger type than `dict` and `tuple`;\r\n  - Not all batching is done by applying `torch.stack` on individual elements.\r\n\r\nThe propose class is an easy to understand abstraction that could be passed from a `Dataset` to a `Dataloader`.\r\n\r\nIn addition to `Dataloader`, this could also be used to define how to split data in `DataParallel`.\r\n \r\n## Pitch\r\n\r\nA incomplete implementation could look like this:\r\n```python\r\nfrom abc import ABC, abstractmethod\r\n\r\nclass Data(ABC):\r\n    @abstractmethod\r\n    def apply(self, func: Callable) -> None:\r\n    \"\"\"Apply a function on all tensors in the data class.\r\n\r\n    Could be passed functions like `lambda t: t.pin_memory()`, `lambda t: t.to(device)`.\r\n    \"\"\"\r\n\r\n    @abstractmethod\r\n    @classmethod\r\n    def batch(cls, data_points: List[Data]) -> Data:\r\n    \"\"\"Batch together multiple data object.\"\"\"\r\n\r\n    @abstractmethod\r\n    def split(self, chunks) -> List[Data]:\r\n    \"\"\"Split the data into chunks across the data dimension.\"\"\"\r\n```\r\nHere I've represented individual data points and batched data with the same class, but this need not be the case.\r\n\r\nAnd in the `default_collate`, we would have something like:\r\n```python\r\nif isinstance(batch[0], Data):\r\n    # apply logic using` batch[0].__class__.batch`, and `batch.apply`\r\n```\r\n## Alternatives\r\n\r\nIt is already possible to have some of the desired behavior using a custom `collate_fn`. However if the user returns a class instead of `tuple` or `dict`, `pin_memory` will [not be applied](https://github.com/pytorch/pytorch/blob/ca03c10cefa1e126eab1446d490f9314bd236c1b/torch/utils/data/dataloader.py#L237). Furthermore, the user also needs to be aware of the `_shared_memory` [variable](https://github.com/pytorch/pytorch/blob/ca03c10cefa1e126eab1446d490f9314bd236c1b/torch/utils/data/dataloader.py#L203) to reproduce in their own `collate_fn` to leverage this.\r\n\r\n## Additional context\r\n\r\nA pratical example can be found in [Pytroch_geometric](https://rusty1s.github.io/pytorch_geometric/build/html/notes/introduction.html), where the author went into the trouble of redefining many `Dataloader` behaviors.\r\n"}