{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/231018485", "pull_request_review_id": 171887436, "id": 231018485, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMTAxODQ4NQ==", "diff_hunk": "@@ -0,0 +1,41 @@\n+#pragma once\n+\n+/* This file defines math functions compatible across different gpu\n+ * platforms (currently CUDA and HIP).\n+ */\n+\n+#ifdef __HIPCC__\n+#define __MATH_FUNCTIONS_DECL__ __device__ inline\n+#else /* __HIPCC__ */\n+#ifdef __CUDACC_RTC__\n+#define __MATH_FUNCTIONS_DECL__ __host__ __device__\n+#else /* __CUDACC_RTC__ */\n+#define __MATH_FUNCTIONS_DECL__ static inline __host__ __device__\n+#endif /* __CUDACC_RTC__ */\n+#endif /* __HIPCC__ */\n+\n+namespace c10 {\n+namespace cuda {\n+namespace compat {\n+\n+__MATH_FUNCTIONS_DECL__ float abs(float x) {\n+  return fabsf(x);\n+}\n+__MATH_FUNCTIONS_DECL__ double abs(double x) {\n+  return fabs(x);\n+}\n+\n+__MATH_FUNCTIONS_DECL__ float max(float x, float y) {\n+  return fmaxf(x, y);\n+}\n+__MATH_FUNCTIONS_DECL__ double max(double x, double y) {\n+  return fmax(x, y);\n+}\n+\n+__MATH_FUNCTIONS_DECL__ float pow(float x, float y) {", "path": "c10/cuda/math_compat.h", "position": 37, "original_position": 35, "commit_id": "509ff6660a571c0ab9334260df0bddc562c1b826", "original_commit_id": "49069f0c8ca2e45cb9de8a2109a26e3610f0a725", "user": {"login": "bddppq", "id": 9300575, "node_id": "MDQ6VXNlcjkzMDA1NzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9300575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bddppq", "html_url": "https://github.com/bddppq", "followers_url": "https://api.github.com/users/bddppq/followers", "following_url": "https://api.github.com/users/bddppq/following{/other_user}", "gists_url": "https://api.github.com/users/bddppq/gists{/gist_id}", "starred_url": "https://api.github.com/users/bddppq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bddppq/subscriptions", "organizations_url": "https://api.github.com/users/bddppq/orgs", "repos_url": "https://api.github.com/users/bddppq/repos", "events_url": "https://api.github.com/users/bddppq/events{/privacy}", "received_events_url": "https://api.github.com/users/bddppq/received_events", "type": "User", "site_admin": false}, "body": "I was dumb, I thought the the double version of pow has the same name as the overload then there is no need to add an overload, but actually it's needed.", "created_at": "2018-11-06T07:25:28Z", "updated_at": "2018-11-23T15:54:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/13602#discussion_r231018485", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13602", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/231018485"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13602#discussion_r231018485"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13602"}}, "body_html": "<p>I was dumb, I thought the the double version of pow has the same name as the overload then there is no need to add an overload, but actually it's needed.</p>", "body_text": "I was dumb, I thought the the double version of pow has the same name as the overload then there is no need to add an overload, but actually it's needed.", "in_reply_to_id": 230994663}