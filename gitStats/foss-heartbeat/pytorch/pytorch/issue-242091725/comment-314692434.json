{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/314692434", "html_url": "https://github.com/pytorch/pytorch/issues/2052#issuecomment-314692434", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2052", "id": 314692434, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNDY5MjQzNA==", "user": {"login": "trypag", "id": 13003839, "node_id": "MDQ6VXNlcjEzMDAzODM5", "avatar_url": "https://avatars1.githubusercontent.com/u/13003839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trypag", "html_url": "https://github.com/trypag", "followers_url": "https://api.github.com/users/trypag/followers", "following_url": "https://api.github.com/users/trypag/following{/other_user}", "gists_url": "https://api.github.com/users/trypag/gists{/gist_id}", "starred_url": "https://api.github.com/users/trypag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trypag/subscriptions", "organizations_url": "https://api.github.com/users/trypag/orgs", "repos_url": "https://api.github.com/users/trypag/repos", "events_url": "https://api.github.com/users/trypag/events{/privacy}", "received_events_url": "https://api.github.com/users/trypag/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-12T08:24:37Z", "updated_at": "2017-07-12T08:24:37Z", "author_association": "NONE", "body_html": "<p>Adding this from conversation on slack, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> talking :</p>\n<pre><code>- the dataset is not copied, it all depends on what multiprocess module does\n\n- if you use fork, and you don't modify the numpy array, then it will be kept shared \namong the processes.\n\n- they will use the same memory pages as long as they're not modified.\n\n- if you use spawn, the dataset will be recreated from scratch and the numpy array will be probably replicated among them.\n\n- ah I'm sorry. I just realized that it might actually be getting pickled\n\n- in such case there are two options:\n\n1. make the numpy array mmap a file &lt;- the kernel will take care of everything for you and won't duplicate the pages\n\n2. use a torch tensor inside your dataset and call .share_memory_() before you start iterating over the data loader\n</code></pre>", "body_text": "Adding this from conversation on slack, @apaszke talking :\n- the dataset is not copied, it all depends on what multiprocess module does\n\n- if you use fork, and you don't modify the numpy array, then it will be kept shared \namong the processes.\n\n- they will use the same memory pages as long as they're not modified.\n\n- if you use spawn, the dataset will be recreated from scratch and the numpy array will be probably replicated among them.\n\n- ah I'm sorry. I just realized that it might actually be getting pickled\n\n- in such case there are two options:\n\n1. make the numpy array mmap a file <- the kernel will take care of everything for you and won't duplicate the pages\n\n2. use a torch tensor inside your dataset and call .share_memory_() before you start iterating over the data loader", "body": "Adding this from conversation on slack, @apaszke talking :\r\n```\r\n- the dataset is not copied, it all depends on what multiprocess module does\r\n\r\n- if you use fork, and you don't modify the numpy array, then it will be kept shared \r\namong the processes.\r\n\r\n- they will use the same memory pages as long as they're not modified.\r\n\r\n- if you use spawn, the dataset will be recreated from scratch and the numpy array will be probably replicated among them.\r\n\r\n- ah I'm sorry. I just realized that it might actually be getting pickled\r\n\r\n- in such case there are two options:\r\n\r\n1. make the numpy array mmap a file <- the kernel will take care of everything for you and won't duplicate the pages\r\n\r\n2. use a torch tensor inside your dataset and call .share_memory_() before you start iterating over the data loader\r\n```"}