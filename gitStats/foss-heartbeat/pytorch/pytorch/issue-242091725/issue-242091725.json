{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2052", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2052/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2052/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2052/events", "html_url": "https://github.com/pytorch/pytorch/issues/2052", "id": 242091725, "node_id": "MDU6SXNzdWUyNDIwOTE3MjU=", "number": 2052, "title": "DataLoader without dataset replica", "user": {"login": "trypag", "id": 13003839, "node_id": "MDQ6VXNlcjEzMDAzODM5", "avatar_url": "https://avatars1.githubusercontent.com/u/13003839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/trypag", "html_url": "https://github.com/trypag", "followers_url": "https://api.github.com/users/trypag/followers", "following_url": "https://api.github.com/users/trypag/following{/other_user}", "gists_url": "https://api.github.com/users/trypag/gists{/gist_id}", "starred_url": "https://api.github.com/users/trypag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/trypag/subscriptions", "organizations_url": "https://api.github.com/users/trypag/orgs", "repos_url": "https://api.github.com/users/trypag/repos", "events_url": "https://api.github.com/users/trypag/events{/privacy}", "received_events_url": "https://api.github.com/users/trypag/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-07-11T15:35:33Z", "updated_at": "2017-07-12T11:59:12Z", "closed_at": "2017-07-12T11:59:12Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I have tried to understand the code behind the DataLoader, I think I got the global meaning.</p>\n<p><strong>Problem</strong><br>\nI have noticed that while increasing the number of workers, the number of memory used increases linearly. I recall that someone told that each worker has its own replica of the dataset. I guess this is happening because when lauching a Process, the parent environment is copied for the newly created Process. It might happen differently, If I am wrong please explain me. :)</p>\n<p><strong>Context</strong><br>\nMy main concern here is to have only one instance of my dataset in memory. For example my reference dataset is composed with a 8 GB file, scaling with 10 workers, you can imagine how fast memory burns. This situation has no sense because the dataset is static.<br>\nI may have misinterpreted something along the way, I might not be the first one to deal with this problem.</p>\n<p><strong>Expected workaround</strong><br>\nI already built a custom dataset and cannot break this 8 GB file into more pieces, but just using a reference to the object rather than copy would save my day.</p>\n<p>Thanks</p>", "body_text": "Hi,\nI have tried to understand the code behind the DataLoader, I think I got the global meaning.\nProblem\nI have noticed that while increasing the number of workers, the number of memory used increases linearly. I recall that someone told that each worker has its own replica of the dataset. I guess this is happening because when lauching a Process, the parent environment is copied for the newly created Process. It might happen differently, If I am wrong please explain me. :)\nContext\nMy main concern here is to have only one instance of my dataset in memory. For example my reference dataset is composed with a 8 GB file, scaling with 10 workers, you can imagine how fast memory burns. This situation has no sense because the dataset is static.\nI may have misinterpreted something along the way, I might not be the first one to deal with this problem.\nExpected workaround\nI already built a custom dataset and cannot break this 8 GB file into more pieces, but just using a reference to the object rather than copy would save my day.\nThanks", "body": "Hi,\r\n\r\nI have tried to understand the code behind the DataLoader, I think I got the global meaning. \r\n\r\n**Problem**\r\nI have noticed that while increasing the number of workers, the number of memory used increases linearly. I recall that someone told that each worker has its own replica of the dataset. I guess this is happening because when lauching a Process, the parent environment is copied for the newly created Process. It might happen differently, If I am wrong please explain me. :)\r\n\r\n**Context**\r\nMy main concern here is to have only one instance of my dataset in memory. For example my reference dataset is composed with a 8 GB file, scaling with 10 workers, you can imagine how fast memory burns. This situation has no sense because the dataset is static.\r\nI may have misinterpreted something along the way, I might not be the first one to deal with this problem. \r\n\r\n**Expected workaround**\r\nI already built a custom dataset and cannot break this 8 GB file into more pieces, but just using a reference to the object rather than copy would save my day.\r\n\r\nThanks"}