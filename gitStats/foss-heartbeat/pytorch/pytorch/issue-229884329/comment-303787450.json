{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/303787450", "html_url": "https://github.com/pytorch/pytorch/pull/1594#issuecomment-303787450", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1594", "id": 303787450, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzc4NzQ1MA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-24T17:01:17Z", "updated_at": "2017-05-24T17:01:41Z", "author_association": "MEMBER", "body_html": "<p>Right, I meant to use the max and:</p>\n<ul>\n<li>if there's more free space on the device then use cudaMalloc,</li>\n<li>if the largest cached (and free!) block is larger, use that amount with the caching allocator. This shouldn't alter its state (no mallocs/frees called).</li>\n</ul>\n<p>cudnnFind (no ex) used to call cudaFree too, and this part is already guarded by a free mutex.</p>", "body_text": "Right, I meant to use the max and:\n\nif there's more free space on the device then use cudaMalloc,\nif the largest cached (and free!) block is larger, use that amount with the caching allocator. This shouldn't alter its state (no mallocs/frees called).\n\ncudnnFind (no ex) used to call cudaFree too, and this part is already guarded by a free mutex.", "body": "Right, I meant to use the max and:\r\n* if there's more free space on the device then use cudaMalloc,\r\n* if the largest cached (and free!) block is larger, use that amount with the caching allocator. This shouldn't alter its state (no mallocs/frees called).\r\n\r\ncudnnFind (no ex) used to call cudaFree too, and this part is already guarded by a free mutex."}