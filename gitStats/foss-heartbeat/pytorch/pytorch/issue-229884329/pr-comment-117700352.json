{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/117700352", "pull_request_review_id": 39426402, "id": 117700352, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNzcwMDM1Mg==", "diff_hunk": "@@ -106,18 +106,63 @@ template<typename algo_t>\n struct algorithm_search {\n };\n \n+cudnnStatus_t getWorkspaceSize(cudnnHandle_t handle, const Convolution& conv, cudnnConvolutionFwdAlgo_t algo, size_t* sz){\n+    return cudnnGetConvolutionForwardWorkspaceSize(handle, conv.idesc.desc, conv.wdesc.desc, conv.cdesc.desc, conv.odesc.desc, algo, sz);\n+}\n+cudnnStatus_t getWorkspaceSize(cudnnHandle_t handle, const Convolution& conv, cudnnConvolutionBwdDataAlgo_t algo, size_t* sz){\n+    return cudnnGetConvolutionBackwardDataWorkspaceSize(handle, conv.wdesc.desc, conv.odesc.desc, conv.cdesc.desc, conv.idesc.desc, algo, sz);\n+}\n+cudnnStatus_t getWorkspaceSize(cudnnHandle_t handle, const Convolution& conv, cudnnConvolutionBwdFilterAlgo_t algo, size_t* sz){\n+    return cudnnGetConvolutionBackwardFilterWorkspaceSize(handle, conv.idesc.desc, conv.odesc.desc, conv.cdesc.desc, conv.wdesc.desc, algo, sz);\n+}\n+\n+template<typename algo_t>\n+size_t getMaxWorkspaceSize(cudnnHandle_t handle, const Convolution& conv, algo_t *algo, int n_algo, THCState* state){\n+    size_t max_ws_size = 0;\n+    size_t max_block_size = 0;\n+    size_t total_gpu_mem = 0;\n+    size_t free_gpu_mem = 0;\n+    \n+    THCudaCheck(THCudaMemGetInfoCached(state,&free_gpu_mem,&total_gpu_mem,&max_block_size));\n+    \n+    for(int i=0; i<n_algo; i++) {\n+        cudnnStatus_t err;\n+        size_t sz;\n+        err = getWorkspaceSize(handle, conv, algo[i], &sz);\n+        if(CUDNN_STATUS_SUCCESS != err || sz == 0 || sz < max_ws_size || sz > max_block_size) continue;\n+        max_ws_size = sz;\n+    }\n+    return max_ws_size;\n+}\n+\n template<>\n struct algorithm_search<cudnnConvolutionFwdAlgo_t> {\n   static constexpr auto DEFAULT_ALGO = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;\n   static BenchmarkCache<cudnnConvolutionFwdAlgo_t>& cache() {\n     return fwd_algos;\n   }\n \n-  static cudnnConvolutionFwdAlgoPerf_t findAlgorithm(cudnnHandle_t handle, const Convolution& conv) {\n+  static cudnnConvolutionFwdAlgoPerf_t findAlgorithm(THCState* state, cudnnHandle_t handle, const Convolution& conv,\n+\t\t\t\t\t\t      void* in, void* out, void* wght) {\n     int algoCount;\n     cudnnConvolutionFwdAlgoPerf_t perfResults;\n-    CHECK(cudnnFindConvolutionForwardAlgorithm(handle, conv.idesc.desc,\n-        conv.wdesc.desc, conv.cdesc.desc, conv.odesc.desc, 1, &algoCount, &perfResults));\n+    cudnnConvolutionFwdAlgo_t algo[] = {\n+         CUDNN_CONVOLUTION_FWD_ALGO_GEMM,\n+         CUDNN_CONVOLUTION_FWD_ALGO_FFT,\n+         CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING,\n+         CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,\n+         CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM,\n+         CUDNN_CONVOLUTION_FWD_ALGO_DIRECT,\n+         CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD,\n+         CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED\n+    };\n+    size_t max_ws_size = getMaxWorkspaceSize<cudnnConvolutionFwdAlgo_t>(handle,conv,algo,sizeof(algo)/sizeof(algo[0]),state);\n+    Workspace ws(state, max_ws_size);\n+\n+    CHECK(cudnnFindConvolutionForwardAlgorithmEx(handle, conv.idesc.desc, in,\n+        conv.wdesc.desc, wght, conv.cdesc.desc, conv.odesc.desc, out, 1, &algoCount, \n+        &perfResults, ws.data, ws.size));", "path": "torch/csrc/cudnn/Conv.cpp", "position": 62, "original_position": 62, "commit_id": "12ef45d7388a60b7733b5c389d6299fdde43e7de", "original_commit_id": "cb1dfa62c80aaa64689784fde1e7cbfc7eaf777f", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I talked to @colesbury and we've decided that we should flush the block cache of the allocator after the algorithm is found (to get rid of any unnecessarily large blocks). You'd need to wrap this call and workspace in a separate block, and call `emptyCache` on the caching allocator. We don't have any API exposed for this purpose but it should be quite straightforward ([the relevant lines are here](https://github.com/aromnvidia/pytorch/blob/cb1dfa62c80aaa64689784fde1e7cbfc7eaf777f/torch/lib/THC/THCCachingAllocator.cpp#L456-L482)). It'd be best to add this function to `torch/lib/THC/THCCachingAllocator.h`.", "created_at": "2017-05-22T09:25:34Z", "updated_at": "2018-11-23T15:33:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/1594#discussion_r117700352", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1594", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/117700352"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1594#discussion_r117700352"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1594"}}, "body_html": "<p>I talked to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> and we've decided that we should flush the block cache of the allocator after the algorithm is found (to get rid of any unnecessarily large blocks). You'd need to wrap this call and workspace in a separate block, and call <code>emptyCache</code> on the caching allocator. We don't have any API exposed for this purpose but it should be quite straightforward (<a href=\"https://github.com/aromnvidia/pytorch/blob/cb1dfa62c80aaa64689784fde1e7cbfc7eaf777f/torch/lib/THC/THCCachingAllocator.cpp#L456-L482\">the relevant lines are here</a>). It'd be best to add this function to <code>torch/lib/THC/THCCachingAllocator.h</code>.</p>", "body_text": "I talked to @colesbury and we've decided that we should flush the block cache of the allocator after the algorithm is found (to get rid of any unnecessarily large blocks). You'd need to wrap this call and workspace in a separate block, and call emptyCache on the caching allocator. We don't have any API exposed for this purpose but it should be quite straightforward (the relevant lines are here). It'd be best to add this function to torch/lib/THC/THCCachingAllocator.h."}