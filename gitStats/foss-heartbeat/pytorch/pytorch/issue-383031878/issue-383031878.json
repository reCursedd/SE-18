{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14271", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14271/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14271/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14271/events", "html_url": "https://github.com/pytorch/pytorch/pull/14271", "id": 383031878, "node_id": "MDExOlB1bGxSZXF1ZXN0MjMyNTk1NDEw", "number": 14271, "title": "[c10d] Robust NCCL barrier improvement to cover all devices combinations", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-11-21T09:40:16Z", "updated_at": "2018-11-22T02:25:28Z", "closed_at": "2018-11-22T02:25:28Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/14271", "html_url": "https://github.com/pytorch/pytorch/pull/14271", "diff_url": "https://github.com/pytorch/pytorch/pull/14271.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/14271.patch"}, "body_html": "<p>This covers the very edgy case when we run the same NCCL process group with multiple GPU combinations instead of the last GPU combination. We always keep track of what GPUs have been used previously in the NCCL process group and barrier() itself will synchronize on each GPU's NCCL stream.</p>\n<p>Test covered as well. Tested on 8-GPU machine</p>\n<p>as part of improving: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"377470335\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/13573\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/13573/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/13573\">#13573</a></p>", "body_text": "This covers the very edgy case when we run the same NCCL process group with multiple GPU combinations instead of the last GPU combination. We always keep track of what GPUs have been used previously in the NCCL process group and barrier() itself will synchronize on each GPU's NCCL stream.\nTest covered as well. Tested on 8-GPU machine\nas part of improving: #13573", "body": "This covers the very edgy case when we run the same NCCL process group with multiple GPU combinations instead of the last GPU combination. We always keep track of what GPUs have been used previously in the NCCL process group and barrier() itself will synchronize on each GPU's NCCL stream.\r\n\r\nTest covered as well. Tested on 8-GPU machine\r\n\r\nas part of improving: https://github.com/pytorch/pytorch/issues/13573"}