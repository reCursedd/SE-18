{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1206", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1206/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1206/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1206/events", "html_url": "https://github.com/pytorch/pytorch/issues/1206", "id": 220106369, "node_id": "MDU6SXNzdWUyMjAxMDYzNjk=", "number": 1206, "title": "nan of BN running_mean and running_var when finetuning resnet on my own dataset", "user": {"login": "nicklhy", "id": 1146226, "node_id": "MDQ6VXNlcjExNDYyMjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1146226?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nicklhy", "html_url": "https://github.com/nicklhy", "followers_url": "https://api.github.com/users/nicklhy/followers", "following_url": "https://api.github.com/users/nicklhy/following{/other_user}", "gists_url": "https://api.github.com/users/nicklhy/gists{/gist_id}", "starred_url": "https://api.github.com/users/nicklhy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nicklhy/subscriptions", "organizations_url": "https://api.github.com/users/nicklhy/orgs", "repos_url": "https://api.github.com/users/nicklhy/repos", "events_url": "https://api.github.com/users/nicklhy/events{/privacy}", "received_events_url": "https://api.github.com/users/nicklhy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-04-07T04:22:17Z", "updated_at": "2017-04-07T05:54:37Z", "closed_at": "2017-04-07T05:54:37Z", "author_association": "NONE", "body_html": "<p>Hi everyone,<br>\nI am finetuing resnet152 on my own dataset recently. The training phase is perfect that the accuracy increased from random to around 90%.</p>\n<p>But unfortunately, there is a serious problem when I tried to evaluate it. The model keeps producing the same results for all samples because all BN runing_mean and runing_var parameters in resnet152 are nan. (I do really not know why these parameters changed to nan when training the model.)</p>\n<p>In addition, if I remove <code>model.eval()</code>(run this model in training mode) and set a relatively large batch-size, the models can once again output reasonable results. But in this case, BN layers will not use the learned parameters on train data. They just compute mean and var in the current mini-batch.</p>\n<p>Do anyone know some potential reasons for this nan problem?</p>", "body_text": "Hi everyone,\nI am finetuing resnet152 on my own dataset recently. The training phase is perfect that the accuracy increased from random to around 90%.\nBut unfortunately, there is a serious problem when I tried to evaluate it. The model keeps producing the same results for all samples because all BN runing_mean and runing_var parameters in resnet152 are nan. (I do really not know why these parameters changed to nan when training the model.)\nIn addition, if I remove model.eval()(run this model in training mode) and set a relatively large batch-size, the models can once again output reasonable results. But in this case, BN layers will not use the learned parameters on train data. They just compute mean and var in the current mini-batch.\nDo anyone know some potential reasons for this nan problem?", "body": "Hi everyone,\r\nI am finetuing resnet152 on my own dataset recently. The training phase is perfect that the accuracy increased from random to around 90%.\r\n\r\nBut unfortunately, there is a serious problem when I tried to evaluate it. The model keeps producing the same results for all samples because all BN runing_mean and runing_var parameters in resnet152 are nan. (I do really not know why these parameters changed to nan when training the model.)\r\n\r\nIn addition, if I remove `model.eval()`(run this model in training mode) and set a relatively large batch-size, the models can once again output reasonable results. But in this case, BN layers will not use the learned parameters on train data. They just compute mean and var in the current mini-batch.\r\n\r\nDo anyone know some potential reasons for this nan problem?"}