{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/292448004", "html_url": "https://github.com/pytorch/pytorch/issues/1206#issuecomment-292448004", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1206", "id": 292448004, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MjQ0ODAwNA==", "user": {"login": "nicklhy", "id": 1146226, "node_id": "MDQ6VXNlcjExNDYyMjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/1146226?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nicklhy", "html_url": "https://github.com/nicklhy", "followers_url": "https://api.github.com/users/nicklhy/followers", "following_url": "https://api.github.com/users/nicklhy/following{/other_user}", "gists_url": "https://api.github.com/users/nicklhy/gists{/gist_id}", "starred_url": "https://api.github.com/users/nicklhy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nicklhy/subscriptions", "organizations_url": "https://api.github.com/users/nicklhy/orgs", "repos_url": "https://api.github.com/users/nicklhy/repos", "events_url": "https://api.github.com/users/nicklhy/events{/privacy}", "received_events_url": "https://api.github.com/users/nicklhy/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-07T05:54:37Z", "updated_at": "2017-04-07T05:54:37Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> , thank you so much and I just found out the exact bug. You are right, the nan comes from forwarding an input data with all zero/nan values.<br>\nI have a function which can infer the feature shape of backbone network when building my custom network. Something like:</p>\n<pre><code>backbone = ...\nsym_data = torch.autograd.Variable(torch.Tensor(1, 3, height, width))\nsym_feat = self.backbone(sym_data)\nnum_features = sym_feat.data.size()[1]\n</code></pre>\n<p>Thus, the bn parameters are already nan before the train loop really began.</p>\n<p>Now, the nan values disappeared after changing <code>sym_data = torch.autograd.Variable(torch.Tensor(1, 3, height, width))</code> to <code>sym_data = torch.autograd.Variable(torch.rand(1, 3, height, width))</code>.</p>", "body_text": "@soumith , thank you so much and I just found out the exact bug. You are right, the nan comes from forwarding an input data with all zero/nan values.\nI have a function which can infer the feature shape of backbone network when building my custom network. Something like:\nbackbone = ...\nsym_data = torch.autograd.Variable(torch.Tensor(1, 3, height, width))\nsym_feat = self.backbone(sym_data)\nnum_features = sym_feat.data.size()[1]\n\nThus, the bn parameters are already nan before the train loop really began.\nNow, the nan values disappeared after changing sym_data = torch.autograd.Variable(torch.Tensor(1, 3, height, width)) to sym_data = torch.autograd.Variable(torch.rand(1, 3, height, width)).", "body": "@soumith , thank you so much and I just found out the exact bug. You are right, the nan comes from forwarding an input data with all zero/nan values.\r\nI have a function which can infer the feature shape of backbone network when building my custom network. Something like:\r\n\r\n```\r\nbackbone = ...\r\nsym_data = torch.autograd.Variable(torch.Tensor(1, 3, height, width))\r\nsym_feat = self.backbone(sym_data)\r\nnum_features = sym_feat.data.size()[1]\r\n```\r\nThus, the bn parameters are already nan before the train loop really began.\r\n\r\nNow, the nan values disappeared after changing `sym_data = torch.autograd.Variable(torch.Tensor(1, 3, height, width))` to `sym_data = torch.autograd.Variable(torch.rand(1, 3, height, width))`."}