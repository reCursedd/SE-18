{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213483029", "pull_request_review_id": 150329548, "id": 213483029, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxMzQ4MzAyOQ==", "diff_hunk": "@@ -252,6 +253,142 @@ def _verify_equal(xs, ys):\n             raise RuntimeError(\"JIT and real computation mismatch\")\n \n \n+def indent(s):\n+    return '\\n'.join(['\\t' + line for line in s.splitlines()])\n+\n+\n+class TracingCheckError(Exception):\n+    def __init__(self, graph_diff_error, tensor_compare_error, nondeterm_warning, extra_msg=None):\n+        self.message = 'Tracing failed sanity checks!\\n'\n+        if extra_msg is not None:\n+            self.message += extra_msg + '\\n'\n+        if graph_diff_error is not None:\n+            self.message += 'ERROR: Graphs differed across invocations!\\n'\n+            self.message += indent(graph_diff_error) + '\\n'\n+        if nondeterm_warning is not None:\n+            self.message += 'WARNING: '\n+            self.message += nondeterm_warning + '\\n'\n+        if tensor_compare_error is not None:\n+            self.message += 'ERROR: Tensor-valued Constant nodes differed in value ' \\\n+                            'across invocations. This often indicates that the tracer has' \\\n+                            ' encountered untraceable code.\\n'\n+            self.message += indent(tensor_compare_error) + '\\n'\n+        super(TracingCheckError, self).__init__(self.message)\n+\n+\n+# Check the traced module against a set of user-provided validation inputs\n+def _check_trace(check_inputs, func, executor_options, module, check_tolerance):\n+    for inputs in check_inputs:\n+        check_mod = torch.jit.trace(*_clone_inputs(inputs), disable_checks=True, **executor_options)(func)\n+\n+        def graph_diagnostic_info():\n+            mod_canonicalized = torch._C._jit_pass_canonicalize(module.graph)\n+            torch._C._jit_pass_erase_shape_information(mod_canonicalized)\n+            check_canonicalized = torch._C._jit_pass_canonicalize(check_mod.graph)\n+            torch._C._jit_pass_erase_shape_information(check_canonicalized)\n+\n+            graph_diff_errors = None\n+            if str(mod_canonicalized) != str(check_canonicalized):\n+                import difflib\n+                graph_diff = difflib.ndiff(str(mod_canonicalized).splitlines(True),\n+                                           str(check_canonicalized).splitlines(True))\n+                graph_diff_errors = 'Graph diff:\\n' + indent(''.join(graph_diff)) + '\\n'\n+\n+                for n_mod, n_check in zip(mod_canonicalized.nodes(), check_canonicalized.nodes()):\n+                    if str(n_mod) != str(n_check):\n+                        graph_diff_errors += 'First diverging operator:\\n'\n+                        node_diff = difflib.ndiff(str(n_mod).splitlines(True),\n+                                                  str(n_check).splitlines(True))\n+                        source_printout = 'Node diff:\\n' + indent(''.join(node_diff)) + '\\n'\n+                        mod_stack = n_mod.getSourceLocation()\n+                        if mod_stack:\n+                            source_printout += 'Trace source location:\\n' + indent(mod_stack) + '\\n'\n+                        check_stack = n_check.getSourceLocation()\n+                        if check_stack:\n+                            source_printout += 'Check source location:\\n' + indent(check_stack) + '\\n'\n+                        graph_diff_errors += source_printout\n+\n+                        break  # For now, only print out the first pair of nodes that diverges\n+\n+            tensor_compare_errors = None\n+            # Check Tensor-valued constant nodes\n+            for n_mod, n_check in zip(mod_canonicalized.nodes(), check_canonicalized.nodes()):\n+                if n_mod.kind() != n_check.kind():\n+                    break  # Graphs have already diverged\n+\n+                if n_mod.kind() == n_check.kind() and n_mod.kind() == 'prim::Constant':\n+                    if n_mod.kindOf('value') != 't' or n_check.kindOf('value') != 't':\n+                        continue\n+\n+                    mod_tensor_val = n_mod.t('value')\n+                    check_tensor_val = n_check.t('value')\n+\n+                    try:\n+                        torch.testing.assert_allclose(mod_tensor_val, check_tensor_val)\n+                    except (RuntimeError, AssertionError) as e:\n+                        if not tensor_compare_errors:\n+                            tensor_compare_errors = ''\n+                        tensor_compare_errors += 'Node:\\n' + indent(str(n_mod)) + '\\n'\n+                        compare_stack = n_mod.getSourceLocation()\n+                        if compare_stack:\n+                            tensor_compare_errors += 'Source Location:\\n' + indent(compare_stack) + '\\n'\n+                        tensor_compare_errors += 'Comparison exception: ' + indent(str(e))\n+\n+                        break  # For now, only print the first diverging pair\n+\n+            nondeterministic_ops_warning = None\n+            nondeterm_ops = [op for op in module.graph.nodes() if op.isNondeterministic()]\n+            if len(nondeterm_ops) > 0:\n+                nondeterministic_ops_warning = \"Trace had nondeterministic nodes. Nodes:\\n\"\n+                for op in nondeterm_ops:\n+                    nondeterministic_ops_warning += indent(str(op))\n+                nondeterministic_ops_warning += \"\\nThis may cause errors in trace checking. To disable trace checking,\"\\\n+                                                \" pass disable_checks=True to torch.jit.trace()\"\n+\n+            return graph_diff_errors, tensor_compare_errors, nondeterministic_ops_warning\n+\n+        def wrap_retval(x):\n+            return x if isinstance(x, tuple) else (x,)\n+\n+        def run_mod_and_filter_tensor_outputs(mod, inputs):\n+            outs = wrap_retval(mod(*_clone_inputs(inputs)))\n+            outs = [out for out in outs if isinstance(out, torch.Tensor)]\n+            return outs\n+\n+        try:\n+            traced_outs = run_mod_and_filter_tensor_outputs(module, inputs)\n+        except Exception as e:\n+            msg = 'Encountered an exception while running trace with check inputs.\\nException:\\n' + indent(str(e))\n+            raise TracingCheckError(*graph_diagnostic_info(), extra_msg=msg)\n+\n+        try:\n+            fn_outs = run_mod_and_filter_tensor_outputs(func, inputs)\n+            for orig, check in zip(traced_outs, fn_outs):\n+                torch.testing.assert_allclose(orig.double(), check.double(), rtol=check_tolerance,\n+                                              atol=torch.testing._get_default_tolerance(orig, check)[1])\n+        except (RuntimeError, AssertionError) as e:\n+            # TODO: interpose on tracing the function again and check for\n+            # divergence? then we can point to where in the source code\n+            # we start diverging in python v.s. the trace\n+            msg = 'ERROR: Traced function outputs do not match the Python function outputs.\\nException: ' + str(e)\n+            raise TracingCheckError(*graph_diagnostic_info(),\n+                                    extra_msg=msg)\n+\n+        try:\n+            check_outs = run_mod_and_filter_tensor_outputs(check_mod, inputs)\n+        except Exception as e:\n+            msg = 'Encountered an exception while running checking trace with check inputs.\\nException:\\n' \\\n+                + indent(str(e))\n+            raise TracingCheckError(*graph_diagnostic_info(), extra_msg=msg)", "path": "torch/jit/__init__.py", "position": 140, "original_position": 140, "commit_id": "dc008cc6bf20f723ea9b9f4e419e039aec3224d7", "original_commit_id": "3d749fe61c97e05a3405987c51d52df1d7eab9ed", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Those 3 try block are still repeated (once you move the `allclose` assertion out of the second one).", "created_at": "2018-08-28T21:45:19Z", "updated_at": "2018-11-23T15:50:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/10841#discussion_r213483029", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10841", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/213483029"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10841#discussion_r213483029"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10841"}}, "body_html": "<p>Those 3 try block are still repeated (once you move the <code>allclose</code> assertion out of the second one).</p>", "body_text": "Those 3 try block are still repeated (once you move the allclose assertion out of the second one)."}