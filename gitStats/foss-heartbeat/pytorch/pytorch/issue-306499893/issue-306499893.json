{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5877", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5877/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5877/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5877/events", "html_url": "https://github.com/pytorch/pytorch/issues/5877", "id": 306499893, "node_id": "MDU6SXNzdWUzMDY0OTk4OTM=", "number": 5877, "title": "Distributed NCCL test suite doesn't work on Maxwells / test_all_gather_cuda (and possibly others) deadlock with CUDA 9 and system NCCL on latest nvidia/cuda images (driver 384.111)", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2018-03-19T15:00:56Z", "updated_at": "2018-04-16T18:12:00Z", "closed_at": "2018-04-16T18:12:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Steps to reproduce:</p>\n<ol>\n<li>Build a copy of PyTorch on nvidia/cuda Docker image</li>\n<li><code>mkdir /tmp/foo &amp;&amp; mkdir /tmp/foo/barrier &amp;&amp; mkdir /tmp/foo/test_dir</code></li>\n<li>Run <code>WORLD_SIZE=2 INIT_METHOD=env:// BACKEND=nccl TEMP_DIR=/tmp/foo python test/test_distributed.py TestDistBackend.test_all_gather_cuda</code></li>\n</ol>\n<p>Expected result: pass.</p>\n<p>Actual result:</p>\n<pre><code>05:06:40 ======================================================================\n05:06:40 FAIL: test_all_gather_cuda (__main__.TestDistBackend)\n05:06:40 ----------------------------------------------------------------------\n05:06:40 Traceback (most recent call last):\n05:06:40   File \"test_distributed.py\", line 858, in wrapper\n05:06:40     self._join_and_reduce(fn)\n05:06:40   File \"test_distributed.py\", line 924, in _join_and_reduce\n05:06:40     or first_process.exitcode == SKIP_IF_SMALL_WORLDSIZE_EXIT_CODE\n05:06:40 AssertionError\n05:06:40 \n</code></pre>\n<p>The assert is failing because process joining is timing out. I get a few left over Python processes and I have to <code>kill -9</code> them.</p>\n<p>Sample log: <a href=\"https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-cuda9-cudnn7-py2-test/4738//consoleFull\" rel=\"nofollow\">https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-cuda9-cudnn7-py2-test/4738//consoleFull</a> (you can tell the processes deadlock because subsequent tests fail because they can't grab the address that is occupied by the deadlocked process.)</p>\n<p>If you don't feel like building PyTorch, you can get a Docker image with a build with:</p>\n<pre><code>docker run -it -w /var/lib/jenkins --runtime=nvidia -e CUDA_VERSION=9 -e NVIDIA_VISIBLE_DEVICES=all ezyang/pytorch:distributed-fail /bin/bash\nsource /opt/conda/bin/activate\n</code></pre>\n<p>Don't forget to activate the conda environment. Then do instructions (2) and (3).</p>\n<p>The problem does NOT occur on CUDA 8.</p>\n<p>Also, running <code>apt-get remove -y libnccl-dev libnccl2</code> so that our bundled NCCL gets used also solves the problem.</p>\n<p>CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a></p>", "body_text": "Steps to reproduce:\n\nBuild a copy of PyTorch on nvidia/cuda Docker image\nmkdir /tmp/foo && mkdir /tmp/foo/barrier && mkdir /tmp/foo/test_dir\nRun WORLD_SIZE=2 INIT_METHOD=env:// BACKEND=nccl TEMP_DIR=/tmp/foo python test/test_distributed.py TestDistBackend.test_all_gather_cuda\n\nExpected result: pass.\nActual result:\n05:06:40 ======================================================================\n05:06:40 FAIL: test_all_gather_cuda (__main__.TestDistBackend)\n05:06:40 ----------------------------------------------------------------------\n05:06:40 Traceback (most recent call last):\n05:06:40   File \"test_distributed.py\", line 858, in wrapper\n05:06:40     self._join_and_reduce(fn)\n05:06:40   File \"test_distributed.py\", line 924, in _join_and_reduce\n05:06:40     or first_process.exitcode == SKIP_IF_SMALL_WORLDSIZE_EXIT_CODE\n05:06:40 AssertionError\n05:06:40 \n\nThe assert is failing because process joining is timing out. I get a few left over Python processes and I have to kill -9 them.\nSample log: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-cuda9-cudnn7-py2-test/4738//consoleFull (you can tell the processes deadlock because subsequent tests fail because they can't grab the address that is occupied by the deadlocked process.)\nIf you don't feel like building PyTorch, you can get a Docker image with a build with:\ndocker run -it -w /var/lib/jenkins --runtime=nvidia -e CUDA_VERSION=9 -e NVIDIA_VISIBLE_DEVICES=all ezyang/pytorch:distributed-fail /bin/bash\nsource /opt/conda/bin/activate\n\nDon't forget to activate the conda environment. Then do instructions (2) and (3).\nThe problem does NOT occur on CUDA 8.\nAlso, running apt-get remove -y libnccl-dev libnccl2 so that our bundled NCCL gets used also solves the problem.\nCC @ngimel @teng-li", "body": "Steps to reproduce:\r\n1. Build a copy of PyTorch on nvidia/cuda Docker image\r\n2. `mkdir /tmp/foo && mkdir /tmp/foo/barrier && mkdir /tmp/foo/test_dir`\r\n3. Run `WORLD_SIZE=2 INIT_METHOD=env:// BACKEND=nccl TEMP_DIR=/tmp/foo python test/test_distributed.py TestDistBackend.test_all_gather_cuda`\r\n\r\nExpected result: pass.\r\n\r\nActual result:\r\n\r\n```\r\n05:06:40 ======================================================================\r\n05:06:40 FAIL: test_all_gather_cuda (__main__.TestDistBackend)\r\n05:06:40 ----------------------------------------------------------------------\r\n05:06:40 Traceback (most recent call last):\r\n05:06:40   File \"test_distributed.py\", line 858, in wrapper\r\n05:06:40     self._join_and_reduce(fn)\r\n05:06:40   File \"test_distributed.py\", line 924, in _join_and_reduce\r\n05:06:40     or first_process.exitcode == SKIP_IF_SMALL_WORLDSIZE_EXIT_CODE\r\n05:06:40 AssertionError\r\n05:06:40 \r\n```\r\n\r\nThe assert is failing because process joining is timing out. I get a few left over Python processes and I have to `kill -9` them.\r\n\r\nSample log: https://ci.pytorch.org/jenkins/job/pytorch-builds/job/pytorch-linux-xenial-cuda9-cudnn7-py2-test/4738//consoleFull (you can tell the processes deadlock because subsequent tests fail because they can't grab the address that is occupied by the deadlocked process.)\r\n\r\nIf you don't feel like building PyTorch, you can get a Docker image with a build with:\r\n\r\n```\r\ndocker run -it -w /var/lib/jenkins --runtime=nvidia -e CUDA_VERSION=9 -e NVIDIA_VISIBLE_DEVICES=all ezyang/pytorch:distributed-fail /bin/bash\r\nsource /opt/conda/bin/activate\r\n```\r\n\r\nDon't forget to activate the conda environment. Then do instructions (2) and (3).\r\n\r\nThe problem does NOT occur on CUDA 8.\r\n\r\nAlso, running `apt-get remove -y libnccl-dev libnccl2` so that our bundled NCCL gets used also solves the problem.\r\n\r\nCC @ngimel @teng-li "}