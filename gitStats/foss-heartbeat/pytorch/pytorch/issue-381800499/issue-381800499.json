{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14131", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14131/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14131/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14131/events", "html_url": "https://github.com/pytorch/pytorch/issues/14131", "id": 381800499, "node_id": "MDU6SXNzdWUzODE4MDA0OTk=", "number": 14131, "title": "running caffe2 float16 tensors results in aten runtime error", "user": {"login": "NathanSegerlind", "id": 4999427, "node_id": "MDQ6VXNlcjQ5OTk0Mjc=", "avatar_url": "https://avatars3.githubusercontent.com/u/4999427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NathanSegerlind", "html_url": "https://github.com/NathanSegerlind", "followers_url": "https://api.github.com/users/NathanSegerlind/followers", "following_url": "https://api.github.com/users/NathanSegerlind/following{/other_user}", "gists_url": "https://api.github.com/users/NathanSegerlind/gists{/gist_id}", "starred_url": "https://api.github.com/users/NathanSegerlind/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NathanSegerlind/subscriptions", "organizations_url": "https://api.github.com/users/NathanSegerlind/orgs", "repos_url": "https://api.github.com/users/NathanSegerlind/repos", "events_url": "https://api.github.com/users/NathanSegerlind/events{/privacy}", "received_events_url": "https://api.github.com/users/NathanSegerlind/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-16T23:53:34Z", "updated_at": "2018-11-19T18:34:55Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>I am trying to create an FP16 tensor in caffe2, and I am following the method described in <a href=\"https://github.com/pytorch/pytorch/blob/master/caffe2/proto/caffe2.proto#L86\">https://github.com/pytorch/pytorch/blob/master/caffe2/proto/caffe2.proto#L86</a>  (set type to FLOAT16, store float16 as unsigned short in int32_data)</p>\n<p>However, upon attempting to run the network using workspace.RunNet, I get:</p>\n<pre><code>RuntimeError: storage_.IsType&lt;T&gt;() ASSERT FAILED at /home/nathan/pytorch/aten/src/ATen/core/TensorImpl.h:588, please report a bug to PyTorch. Tensor type mismatch, caller expects elements to be float, while tensor contains int. Error from operator: \ninput: \"data\" input: \"scaled_datascale_w\" output: \"scaled_data\" type: \"Mul\" arg { name: \"axis\" i: 1 } arg { name: \"broadcast\" i: 1 } arg { name: \"float16_compute\" i: 1 } \n</code></pre>\n<h2>To Reproduce</h2>\n<p>This comes from converting an existing Float32 model into Float16. The model is loaded and the ops and blobs are copied.  To do the Float32-&gt;Float16 conversion of the tensors,<br>\nI follow the comments at <a href=\"https://github.com/pytorch/pytorch/blob/master/caffe2/proto/caffe2.proto#L86\">https://github.com/pytorch/pytorch/blob/master/caffe2/proto/caffe2.proto#L86</a>:<br>\n// Note about float16: in storage we will basically convert float16 byte-wise<br>\n// to unsigned short and then store them in the int32_data field.</p>\n<p>which gets done as follows:</p>\n<pre><code>tensor.data_type = caffe2_pb2.TensorProto.FLOAT16\nfloat16_list = list(arr.flatten().astype(np.float16))  \nushort_list = [ struct.unpack('H', struct.pack('e', f16))[0] for f16 in float16_list]  \n</code></pre>\n<p>so the tensor looks like:</p>\n<pre><code>[dims: 32\n dims: 32\n dims: 3\n dims: 3\n data_type: FLOAT16\n int32_data: 10967\n int32_data: 10247\n ... ]\n</code></pre>\n<p>For the initialization, since float16 data went in the int32 data, I use a GivenTensorIntFill like this:</p>\n<pre><code>        op = core.CreateOperator(\n            \"GivenTensorIntFill\", [], [tensor.name],\n            arg=[\n                MakeArgument(\"shape\", list(tensor.dims)),\n                MakeArgument(\"values\", tensor.int32_data)])\n        init_net.op.extend([op]) \n</code></pre>\n<p>RunNetOnce with the init net seems okay, but doing workspace.RunNet with the predict net gets the runtime error out of ATen</p>\n<h2>Environment</h2>\n<p>PyTorch version: 1.0.0a0+5b15a50<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.5 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.61<br>\nGPU models and configuration:<br>\nGPU 0: GeForce GTX 1060 6GB<br>\nGPU 1: GeForce GTX 1080</p>\n<p>Nvidia driver version: 410.48<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a</p>", "body_text": "\ud83d\udc1b Bug\nI am trying to create an FP16 tensor in caffe2, and I am following the method described in https://github.com/pytorch/pytorch/blob/master/caffe2/proto/caffe2.proto#L86  (set type to FLOAT16, store float16 as unsigned short in int32_data)\nHowever, upon attempting to run the network using workspace.RunNet, I get:\nRuntimeError: storage_.IsType<T>() ASSERT FAILED at /home/nathan/pytorch/aten/src/ATen/core/TensorImpl.h:588, please report a bug to PyTorch. Tensor type mismatch, caller expects elements to be float, while tensor contains int. Error from operator: \ninput: \"data\" input: \"scaled_datascale_w\" output: \"scaled_data\" type: \"Mul\" arg { name: \"axis\" i: 1 } arg { name: \"broadcast\" i: 1 } arg { name: \"float16_compute\" i: 1 } \n\nTo Reproduce\nThis comes from converting an existing Float32 model into Float16. The model is loaded and the ops and blobs are copied.  To do the Float32->Float16 conversion of the tensors,\nI follow the comments at https://github.com/pytorch/pytorch/blob/master/caffe2/proto/caffe2.proto#L86:\n// Note about float16: in storage we will basically convert float16 byte-wise\n// to unsigned short and then store them in the int32_data field.\nwhich gets done as follows:\ntensor.data_type = caffe2_pb2.TensorProto.FLOAT16\nfloat16_list = list(arr.flatten().astype(np.float16))  \nushort_list = [ struct.unpack('H', struct.pack('e', f16))[0] for f16 in float16_list]  \n\nso the tensor looks like:\n[dims: 32\n dims: 32\n dims: 3\n dims: 3\n data_type: FLOAT16\n int32_data: 10967\n int32_data: 10247\n ... ]\n\nFor the initialization, since float16 data went in the int32 data, I use a GivenTensorIntFill like this:\n        op = core.CreateOperator(\n            \"GivenTensorIntFill\", [], [tensor.name],\n            arg=[\n                MakeArgument(\"shape\", list(tensor.dims)),\n                MakeArgument(\"values\", tensor.int32_data)])\n        init_net.op.extend([op]) \n\nRunNetOnce with the init net seems okay, but doing workspace.RunNet with the predict net gets the runtime error out of ATen\nEnvironment\nPyTorch version: 1.0.0a0+5b15a50\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.5 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.61\nGPU models and configuration:\nGPU 0: GeForce GTX 1060 6GB\nGPU 1: GeForce GTX 1080\nNvidia driver version: 410.48\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a", "body": "## \ud83d\udc1b Bug\r\n\r\nI am trying to create an FP16 tensor in caffe2, and I am following the method described in https://github.com/pytorch/pytorch/blob/master/caffe2/proto/caffe2.proto#L86  (set type to FLOAT16, store float16 as unsigned short in int32_data)\r\n\r\nHowever, upon attempting to run the network using workspace.RunNet, I get:\r\n\r\n    RuntimeError: storage_.IsType<T>() ASSERT FAILED at /home/nathan/pytorch/aten/src/ATen/core/TensorImpl.h:588, please report a bug to PyTorch. Tensor type mismatch, caller expects elements to be float, while tensor contains int. Error from operator: \r\n    input: \"data\" input: \"scaled_datascale_w\" output: \"scaled_data\" type: \"Mul\" arg { name: \"axis\" i: 1 } arg { name: \"broadcast\" i: 1 } arg { name: \"float16_compute\" i: 1 } \r\n\r\n\r\n## To Reproduce\r\n\r\nThis comes from converting an existing Float32 model into Float16. The model is loaded and the ops and blobs are copied.  To do the Float32->Float16 conversion of the tensors, \r\nI follow the comments at https://github.com/pytorch/pytorch/blob/master/caffe2/proto/caffe2.proto#L86:\r\n    // Note about float16: in storage we will basically convert float16 byte-wise\r\n    // to unsigned short and then store them in the int32_data field.\r\n \r\nwhich gets done as follows:\r\n\r\n    tensor.data_type = caffe2_pb2.TensorProto.FLOAT16\r\n    float16_list = list(arr.flatten().astype(np.float16))  \r\n    ushort_list = [ struct.unpack('H', struct.pack('e', f16))[0] for f16 in float16_list]  \r\n\r\nso the tensor looks like:\r\n\r\n\t[dims: 32\r\n\t dims: 32\r\n\t dims: 3\r\n\t dims: 3\r\n\t data_type: FLOAT16\r\n\t int32_data: 10967\r\n\t int32_data: 10247\r\n\t ... ]\r\n\r\nFor the initialization, since float16 data went in the int32 data, I use a GivenTensorIntFill like this:\r\n\r\n            op = core.CreateOperator(\r\n                \"GivenTensorIntFill\", [], [tensor.name],\r\n                arg=[\r\n                    MakeArgument(\"shape\", list(tensor.dims)),\r\n                    MakeArgument(\"values\", tensor.int32_data)])\r\n            init_net.op.extend([op]) \r\n\r\nRunNetOnce with the init net seems okay, but doing workspace.RunNet with the predict net gets the runtime error out of ATen\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0a0+5b15a50\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.5 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.61\r\nGPU models and configuration: \r\nGPU 0: GeForce GTX 1060 6GB\r\nGPU 1: GeForce GTX 1080\r\n\r\nNvidia driver version: 410.48\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v6.a\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\n"}