{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201870282", "pull_request_review_id": 136457808, "id": 201870282, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTg3MDI4Mg==", "diff_hunk": "@@ -0,0 +1,67 @@\n+#pragma once\n+\n+#include <ATen/CUDAStream.h>\n+#include <ATen/Context.h>\n+\n+#include <cstddef>\n+\n+namespace at {\n+\n+/// RAII guard that sets the current CUDA Stream for a particular CUDA device,\n+/// identified by its device index, and sets it back to the original stream upon\n+/// destruction.\n+///\n+/// The stream is always reset to the one that was active at the time of\n+/// construction of the guard. Even if you `set_stream` after construction, the\n+/// destructor will still reset the stream to the one that was active at\n+/// construction time.\n+struct CUDAStreamGuard {\n+  /// Default constructor, does nothing and causes no change in the current\n+  /// stream until `set_stream` is called.\n+  CUDAStreamGuard() = default;\n+\n+  /// Sets the CUDA stream on the given CUDA device (calls `set_stream`).\n+  CUDAStreamGuard(int32_t device_index, const CUDAStream& stream) {\n+    set_stream(device_index, stream);\n+  }\n+\n+  /// Resets the CUDA stream on the stored device to the one that was active at\n+  /// the time of construction of this object.\n+  ~CUDAStreamGuard() {\n+    if (original_stream_) {\n+      globalContext().uncheckedSetCurrentCUDAStreamOnDevice(\n+          device_index_, original_stream_);\n+    }\n+  }\n+\n+  /// Sets the CUDA stream on the CUDA device identified by the given\n+  /// `device_index` to the one specified by `stream`.\n+  void set_stream(int32_t device_index, const CUDAStream& stream) {", "path": "aten/src/ATen/CUDAStreamGuard.h", "position": null, "original_position": 39, "commit_id": "785cdd4e4be06c0de0ba165b59f52be4faa553be", "original_commit_id": "abf6063adfbf877d434444bf7ab1c43e46dc622a", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "As for `int32_t` vs. `int64_t` -- I went for `int32_t` in `at::Device` because in CUDA-land it's just `int`, which is usually 32 bits (but I prefer fixed width types). That said, extending it to 64 bit should be pretty harmless right now if we wanted to changed it. Any opinions @ezyang @mruberry?", "created_at": "2018-07-11T23:30:05Z", "updated_at": "2018-11-23T15:47:12Z", "html_url": "https://github.com/pytorch/pytorch/pull/9277#discussion_r201870282", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9277", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201870282"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9277#discussion_r201870282"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9277"}}, "body_html": "<p>As for <code>int32_t</code> vs. <code>int64_t</code> -- I went for <code>int32_t</code> in <code>at::Device</code> because in CUDA-land it's just <code>int</code>, which is usually 32 bits (but I prefer fixed width types). That said, extending it to 64 bit should be pretty harmless right now if we wanted to changed it. Any opinions <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38511765\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mruberry\">@mruberry</a>?</p>", "body_text": "As for int32_t vs. int64_t -- I went for int32_t in at::Device because in CUDA-land it's just int, which is usually 32 bits (but I prefer fixed width types). That said, extending it to 64 bit should be pretty harmless right now if we wanted to changed it. Any opinions @ezyang @mruberry?", "in_reply_to_id": 201574451}