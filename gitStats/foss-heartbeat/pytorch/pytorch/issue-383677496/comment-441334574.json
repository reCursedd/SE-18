{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/441334574", "html_url": "https://github.com/pytorch/pytorch/issues/14329#issuecomment-441334574", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/14329", "id": 441334574, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MTMzNDU3NA==", "user": {"login": "lliimsft", "id": 29957883, "node_id": "MDQ6VXNlcjI5OTU3ODgz", "avatar_url": "https://avatars0.githubusercontent.com/u/29957883?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lliimsft", "html_url": "https://github.com/lliimsft", "followers_url": "https://api.github.com/users/lliimsft/followers", "following_url": "https://api.github.com/users/lliimsft/following{/other_user}", "gists_url": "https://api.github.com/users/lliimsft/gists{/gist_id}", "starred_url": "https://api.github.com/users/lliimsft/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lliimsft/subscriptions", "organizations_url": "https://api.github.com/users/lliimsft/orgs", "repos_url": "https://api.github.com/users/lliimsft/repos", "events_url": "https://api.github.com/users/lliimsft/events{/privacy}", "received_events_url": "https://api.github.com/users/lliimsft/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-24T01:03:42Z", "updated_at": "2018-11-24T01:03:42Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I have a related question about Multi-GPU vs Distributed training, probably unrelated to BERT itself.</p>\n<p>I have a 4-GPU server, and was trying to run my training in two ways:</p>\n<p>(a) Run single-node distributed training using DistributedDataParallel with 4 processes and minibatch of N each<br>\n(b) Run Multi-GPU training with minibatch of 4N using DataParallel, and all other hyperparams keep the same</p>\n<p>Intuitively I believe (a) and (b) should yield similar accuracy and training times. Below please find my observations:</p>\n<p>(a) runs ~20% faster than (b).<br>\n(b) yields a better final evaluation accuracy of ~4% than (a)</p>\n<p>The first looks like reasonable since I guess the loss.mean() is done by CPU which may be slower than using NCCL directly? However, I don't quite understand the second observation. Can you please give any hint or reference about the possible cause?</p>\n<p>I am using ADAM optimizer FYI...</p>\n<p>Thanks!</p>", "body_text": "Hi,\nI have a related question about Multi-GPU vs Distributed training, probably unrelated to BERT itself.\nI have a 4-GPU server, and was trying to run my training in two ways:\n(a) Run single-node distributed training using DistributedDataParallel with 4 processes and minibatch of N each\n(b) Run Multi-GPU training with minibatch of 4N using DataParallel, and all other hyperparams keep the same\nIntuitively I believe (a) and (b) should yield similar accuracy and training times. Below please find my observations:\n(a) runs ~20% faster than (b).\n(b) yields a better final evaluation accuracy of ~4% than (a)\nThe first looks like reasonable since I guess the loss.mean() is done by CPU which may be slower than using NCCL directly? However, I don't quite understand the second observation. Can you please give any hint or reference about the possible cause?\nI am using ADAM optimizer FYI...\nThanks!", "body": "Hi,\r\n\r\nI have a related question about Multi-GPU vs Distributed training, probably unrelated to BERT itself.\r\n\r\nI have a 4-GPU server, and was trying to run my training in two ways:\r\n\r\n(a) Run single-node distributed training using DistributedDataParallel with 4 processes and minibatch of N each\r\n(b) Run Multi-GPU training with minibatch of 4N using DataParallel, and all other hyperparams keep the same\r\n\r\nIntuitively I believe (a) and (b) should yield similar accuracy and training times. Below please find my observations:\r\n\r\n(a) runs ~20% faster than (b).\r\n(b) yields a better final evaluation accuracy of ~4% than (a)\r\n\r\nThe first looks like reasonable since I guess the loss.mean() is done by CPU which may be slower than using NCCL directly? However, I don't quite understand the second observation. Can you please give any hint or reference about the possible cause?\r\n\r\nI am using ADAM optimizer FYI... \r\n\r\nThanks!"}