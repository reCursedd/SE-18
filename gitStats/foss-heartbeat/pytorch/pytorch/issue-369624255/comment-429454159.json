{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/429454159", "html_url": "https://github.com/pytorch/pytorch/issues/12609#issuecomment-429454159", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12609", "id": 429454159, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTQ1NDE1OQ==", "user": {"login": "zeryx", "id": 1892175, "node_id": "MDQ6VXNlcjE4OTIxNzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1892175?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zeryx", "html_url": "https://github.com/zeryx", "followers_url": "https://api.github.com/users/zeryx/followers", "following_url": "https://api.github.com/users/zeryx/following{/other_user}", "gists_url": "https://api.github.com/users/zeryx/gists{/gist_id}", "starred_url": "https://api.github.com/users/zeryx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zeryx/subscriptions", "organizations_url": "https://api.github.com/users/zeryx/orgs", "repos_url": "https://api.github.com/users/zeryx/repos", "events_url": "https://api.github.com/users/zeryx/events{/privacy}", "received_events_url": "https://api.github.com/users/zeryx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-12T20:33:29Z", "updated_at": "2018-10-12T20:33:29Z", "author_association": "NONE", "body_html": "<p>CPU only definitely gets us somewhere - although we still find it slower to download and unpack than some other ML frameworks. However, we do also want to ensure that our users can access a gpu enabled wheel as well, while not sacrificing load time performance anywhere nearly as severely as they are today.<br>\nAt the moment, because of how long the pytorch GPU wheel takes to zip &amp; unzip on our infrastructure, inference algorithms (think autoencoders &amp; other DNN models) that depend on the GPU wheel are nearly impossible to use.</p>", "body_text": "CPU only definitely gets us somewhere - although we still find it slower to download and unpack than some other ML frameworks. However, we do also want to ensure that our users can access a gpu enabled wheel as well, while not sacrificing load time performance anywhere nearly as severely as they are today.\nAt the moment, because of how long the pytorch GPU wheel takes to zip & unzip on our infrastructure, inference algorithms (think autoencoders & other DNN models) that depend on the GPU wheel are nearly impossible to use.", "body": "CPU only definitely gets us somewhere - although we still find it slower to download and unpack than some other ML frameworks. However, we do also want to ensure that our users can access a gpu enabled wheel as well, while not sacrificing load time performance anywhere nearly as severely as they are today. \r\nAt the moment, because of how long the pytorch GPU wheel takes to zip & unzip on our infrastructure, inference algorithms (think autoencoders & other DNN models) that depend on the GPU wheel are nearly impossible to use."}