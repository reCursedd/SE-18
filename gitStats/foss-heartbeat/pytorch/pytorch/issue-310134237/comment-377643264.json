{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377643264", "html_url": "https://github.com/pytorch/pytorch/pull/6145#issuecomment-377643264", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6145", "id": 377643264, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzY0MzI2NA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T23:11:17Z", "updated_at": "2018-03-30T23:11:17Z", "author_association": "MEMBER", "body_html": "<p>Sorry about deviating from the content of this PR, but my 2 cents on dtypes containing cuda information: I think it might be cleaner to separate device from dtype.</p>\n<h2>1.</h2>\n<p>One thing that is a bit annoying to me is that (I believe) the only way of getting the device of a tensor is by calling <code>tensor.get_device()</code>. But this fails for CPU tensors, so we need to wrap the calling code with a conditional.</p>\n<div class=\"highlight highlight-source-python\"><pre>device <span class=\"pl-k\">=</span> tensor.get_device() <span class=\"pl-k\">if</span> tensor.is_cuda <span class=\"pl-k\">else</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>\ntensor1 <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.cuda.float64, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>device)</pre></div>\n<p>A solution is to do something like</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> torch.cuda.device_of(tensor2):\n        tensor1 <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tensor2.dtype)</pre></div>\n<p>but that makes the code mode verbose (maybe without necessity).<br>\nSo we might want to allow <code>get_device</code> to also work on the CPU.</p>\n<h2>2.</h2>\n<p>Another point is that it is a bit verbose to generate tensors from different types that are device-agnostic using some factory functions, like <code>rand</code>.<br>\nFor example</p>\n<div class=\"highlight highlight-source-python\"><pre>tensor1 <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">5</span>).cuda(<span class=\"pl-c1\">1</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> want to have a DoubleTensor on the same device as tensor1</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> but don't know in advance if it's in the CPU or GPU</span>\n<span class=\"pl-k\">with</span> torch.cuda.device_of(tensor1):\n    dtype <span class=\"pl-k\">=</span> torch.cuda.float64 <span class=\"pl-k\">if</span> tensor1.is_cuda <span class=\"pl-k\">else</span> torch.float64\n    tensor2 <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> or</span>\n    dtype <span class=\"pl-k\">=</span> torch.cuda <span class=\"pl-k\">if</span> tensor1.is_cuda <span class=\"pl-k\">else</span> torch\n    tensor2 <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype.float64)</pre></div>\n<p>In this case, it's often simpler to use <code>tensor.new().double().resize_(5).uniform_()</code>, but well, this is a bit ugly and can be unintuitive.</p>\n<p>If we can get the device for CPU tensors with <code>get_device()</code>, we could have</p>\n<div class=\"highlight highlight-source-python\"><pre>t <span class=\"pl-k\">=</span> torch.cuda <span class=\"pl-k\">if</span> tensor1.is_cuda <span class=\"pl-k\">else</span> torch\ntensor2 <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>t.float64, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>tensor1.get_device())</pre></div>\n<p>which is better, but still not great.</p>\n<h2>Why to separate</h2>\n<p>There has been a lot of demand to have a clean way of writing device-agnostic code in pytorch. without having to wrap every few lines with a conditional. The approach</p>\n<div class=\"highlight highlight-source-python\"><pre>t <span class=\"pl-k\">=</span> torch.cuda <span class=\"pl-k\">if</span> use_cuda <span class=\"pl-k\">else</span> torch\ntensor <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>t.float64)</pre></div>\n<p>is a solution, but it doesn't handle same-device / different types that well (as mentioned in point 2).</p>\n<p>But if we split the device from the type, we could instead have an interface which is more device-agnostic-friendly:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> get a handler to CUDA device 0</span>\ndevice <span class=\"pl-k\">=</span> torch.device.cuda(<span class=\"pl-v\">gpu_id</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> can specify the device in every factory</span>\ntensor <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float64, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>device)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> tensor has a device attribute</span>\n<span class=\"pl-k\">assert</span> tensor.device <span class=\"pl-k\">==</span> device\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the same for CPU</span>\ncpu_device <span class=\"pl-k\">=</span> torch.device.cpu()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> or cpu(cpu_id='tcp://192.000.000.1') for distributed?</span>\ntensor_cpu <span class=\"pl-k\">=</span> torch.rand(<span class=\"pl-c1\">5</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>cpu_device)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> dummy operation if device is on CPU, if not copy it</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> to cuda / opencl / whatever is specified by device</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> maybe even copy to different CPUs in distributed?</span>\ntensor_cpu_or_cuda <span class=\"pl-k\">=</span> tensor_cpu.to_device(device, <span class=\"pl-v\">non_blocking</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> or maybe</span>\ntensor_cpu_or_cuda_diff_type <span class=\"pl-k\">=</span> tensor_cpu.astype(\n    <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float64, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>device, <span class=\"pl-v\">non_blocking</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> same applies to nn</span>\nmy_net <span class=\"pl-k\">=</span> nn.Sequential()\nmy_net.to_device(device)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> or</span>\nmy_net.astype(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float16, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>device)</pre></div>\n<h3>end of my divagation :-)</h3>", "body_text": "Sorry about deviating from the content of this PR, but my 2 cents on dtypes containing cuda information: I think it might be cleaner to separate device from dtype.\n1.\nOne thing that is a bit annoying to me is that (I believe) the only way of getting the device of a tensor is by calling tensor.get_device(). But this fails for CPU tensors, so we need to wrap the calling code with a conditional.\ndevice = tensor.get_device() if tensor.is_cuda else -1\ntensor1 = torch.rand(1, 2, dtype=torch.cuda.float64, device=device)\nA solution is to do something like\nwith torch.cuda.device_of(tensor2):\n        tensor1 = torch.rand(1, 2, dtype=tensor2.dtype)\nbut that makes the code mode verbose (maybe without necessity).\nSo we might want to allow get_device to also work on the CPU.\n2.\nAnother point is that it is a bit verbose to generate tensors from different types that are device-agnostic using some factory functions, like rand.\nFor example\ntensor1 = torch.rand(5).cuda(1)\n# want to have a DoubleTensor on the same device as tensor1\n# but don't know in advance if it's in the CPU or GPU\nwith torch.cuda.device_of(tensor1):\n    dtype = torch.cuda.float64 if tensor1.is_cuda else torch.float64\n    tensor2 = torch.rand(5, dtype=dtype)\n    # or\n    dtype = torch.cuda if tensor1.is_cuda else torch\n    tensor2 = torch.rand(5, dtype=dtype.float64)\nIn this case, it's often simpler to use tensor.new().double().resize_(5).uniform_(), but well, this is a bit ugly and can be unintuitive.\nIf we can get the device for CPU tensors with get_device(), we could have\nt = torch.cuda if tensor1.is_cuda else torch\ntensor2 = torch.rand(5, dtype=t.float64, device=tensor1.get_device())\nwhich is better, but still not great.\nWhy to separate\nThere has been a lot of demand to have a clean way of writing device-agnostic code in pytorch. without having to wrap every few lines with a conditional. The approach\nt = torch.cuda if use_cuda else torch\ntensor = torch.rand(5, dtype=t.float64)\nis a solution, but it doesn't handle same-device / different types that well (as mentioned in point 2).\nBut if we split the device from the type, we could instead have an interface which is more device-agnostic-friendly:\n# get a handler to CUDA device 0\ndevice = torch.device.cuda(gpu_id=0)\n\n# can specify the device in every factory\ntensor = torch.rand(5, dtype=torch.float64, device=device)\n\n# tensor has a device attribute\nassert tensor.device == device\n\n# the same for CPU\ncpu_device = torch.device.cpu()  # or cpu(cpu_id='tcp://192.000.000.1') for distributed?\ntensor_cpu = torch.rand(5, device=cpu_device)\n\n# dummy operation if device is on CPU, if not copy it\n# to cuda / opencl / whatever is specified by device\n# maybe even copy to different CPUs in distributed?\ntensor_cpu_or_cuda = tensor_cpu.to_device(device, non_blocking=True)\n# or maybe\ntensor_cpu_or_cuda_diff_type = tensor_cpu.astype(\n    dtype=torch.float64, device=device, non_blocking=True)\n\n# same applies to nn\nmy_net = nn.Sequential()\nmy_net.to_device(device)\n# or\nmy_net.astype(dtype=torch.float16, device=device)\nend of my divagation :-)", "body": "Sorry about deviating from the content of this PR, but my 2 cents on dtypes containing cuda information: I think it might be cleaner to separate device from dtype.\r\n\r\n## 1.\r\n\r\nOne thing that is a bit annoying to me is that (I believe) the only way of getting the device of a tensor is by calling `tensor.get_device()`. But this fails for CPU tensors, so we need to wrap the calling code with a conditional.\r\n```python\r\ndevice = tensor.get_device() if tensor.is_cuda else -1\r\ntensor1 = torch.rand(1, 2, dtype=torch.cuda.float64, device=device)\r\n```\r\n\r\nA solution is to do something like\r\n```python\r\nwith torch.cuda.device_of(tensor2):\r\n        tensor1 = torch.rand(1, 2, dtype=tensor2.dtype)\r\n```\r\nbut that makes the code mode verbose (maybe without necessity). \r\nSo we might want to allow `get_device` to also work on the CPU.\r\n\r\n\r\n## 2.\r\n\r\nAnother point is that it is a bit verbose to generate tensors from different types that are device-agnostic using some factory functions, like `rand`.\r\nFor example\r\n```python\r\ntensor1 = torch.rand(5).cuda(1)\r\n# want to have a DoubleTensor on the same device as tensor1\r\n# but don't know in advance if it's in the CPU or GPU\r\nwith torch.cuda.device_of(tensor1):\r\n    dtype = torch.cuda.float64 if tensor1.is_cuda else torch.float64\r\n    tensor2 = torch.rand(5, dtype=dtype)\r\n    # or\r\n    dtype = torch.cuda if tensor1.is_cuda else torch\r\n    tensor2 = torch.rand(5, dtype=dtype.float64)\r\n```\r\nIn this case, it's often simpler to use `tensor.new().double().resize_(5).uniform_()`, but well, this is a bit ugly and can be unintuitive.\r\n\r\nIf we can get the device for CPU tensors with `get_device()`, we could have\r\n```python\r\nt = torch.cuda if tensor1.is_cuda else torch\r\ntensor2 = torch.rand(5, dtype=t.float64, device=tensor1.get_device())\r\n```\r\nwhich is better, but still not great.\r\n\r\n## Why to separate\r\nThere has been a lot of demand to have a clean way of writing device-agnostic code in pytorch. without having to wrap every few lines with a conditional. The approach\r\n```python\r\nt = torch.cuda if use_cuda else torch\r\ntensor = torch.rand(5, dtype=t.float64)\r\n```\r\nis a solution, but it doesn't handle same-device / different types that well (as mentioned in point 2).\r\n\r\nBut if we split the device from the type, we could instead have an interface which is more device-agnostic-friendly:\r\n```python\r\n# get a handler to CUDA device 0\r\ndevice = torch.device.cuda(gpu_id=0)\r\n\r\n# can specify the device in every factory\r\ntensor = torch.rand(5, dtype=torch.float64, device=device)\r\n\r\n# tensor has a device attribute\r\nassert tensor.device == device\r\n\r\n# the same for CPU\r\ncpu_device = torch.device.cpu()  # or cpu(cpu_id='tcp://192.000.000.1') for distributed?\r\ntensor_cpu = torch.rand(5, device=cpu_device)\r\n\r\n# dummy operation if device is on CPU, if not copy it\r\n# to cuda / opencl / whatever is specified by device\r\n# maybe even copy to different CPUs in distributed?\r\ntensor_cpu_or_cuda = tensor_cpu.to_device(device, non_blocking=True)\r\n# or maybe\r\ntensor_cpu_or_cuda_diff_type = tensor_cpu.astype(\r\n    dtype=torch.float64, device=device, non_blocking=True)\r\n\r\n# same applies to nn\r\nmy_net = nn.Sequential()\r\nmy_net.to_device(device)\r\n# or\r\nmy_net.astype(dtype=torch.float16, device=device)\r\n```\r\n\r\n### end of my divagation :-)"}