{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/321723262", "html_url": "https://github.com/pytorch/pytorch/issues/2366#issuecomment-321723262", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2366", "id": 321723262, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTcyMzI2Mg==", "user": {"login": "mjchen611", "id": 20677117, "node_id": "MDQ6VXNlcjIwNjc3MTE3", "avatar_url": "https://avatars1.githubusercontent.com/u/20677117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mjchen611", "html_url": "https://github.com/mjchen611", "followers_url": "https://api.github.com/users/mjchen611/followers", "following_url": "https://api.github.com/users/mjchen611/following{/other_user}", "gists_url": "https://api.github.com/users/mjchen611/gists{/gist_id}", "starred_url": "https://api.github.com/users/mjchen611/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mjchen611/subscriptions", "organizations_url": "https://api.github.com/users/mjchen611/orgs", "repos_url": "https://api.github.com/users/mjchen611/repos", "events_url": "https://api.github.com/users/mjchen611/events{/privacy}", "received_events_url": "https://api.github.com/users/mjchen611/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-11T03:06:27Z", "updated_at": "2017-08-30T23:14:39Z", "author_association": "NONE", "body_html": "<p>Thank you.</p>\n<p>But I am still wondering about the weights and biases initialization about the following code:<br>\nthis code seems not have obvious initialization operator about weights and biases.</p>\n<p>Can you give me some suggestions?</p>\n<p>Thank you so much.</p>\n<pre><code>from __future__ import print_function\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\n\nif __name__ == '__main__':\n parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                     help='input batch size for training (default: 64)')\n parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                     help='input batch size for testing (default: 1000)')\n parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                     help='number of epochs to train (default: 10)')\n parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                     help='learning rate (default: 0.01)')\n parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                     help='SGD momentum (default: 0.5)')\n parser.add_argument('--no-cuda', action='store_true', default=False,\n                     help='disables CUDA training')\n parser.add_argument('--seed', type=int, default=1, metavar='S',\n                     help='random seed (default: 1)')\n parser.add_argument('--log-interval', type=int, default=10000, metavar='N',\n                     help='how many batches to wait before logging training status')\n args = parser.parse_args()\n args.cuda = not args.no_cuda and torch.cuda.is_available()\n\n print('Using CUDA:' + str(args.cuda))\n\n torch.manual_seed(args.seed)\n if args.cuda:\n     torch.cuda.manual_seed(args.seed)\n\n class Net(nn.Module):\n     def __init__(self):\n         super(Net, self).__init__()\n         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n         self.conv2_drop = nn.Dropout2d()\n         self.fc1 = nn.Linear(320, 50)\n         self.fc2 = nn.Linear(50, 10)\n\n\n     def forward(self, x):\n         x = F.relu(F.max_pool2d(self.conv1(x), 2))  \n         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n         x = x.view(-1, 320)\n         x = F.relu(self.fc1(x))\n         x = F.dropout(x, training=self.training)\n         x = self.fc2(x)\n         return F.log_softmax(x)\n\n model = Net()\n if args.cuda:\n     model.cuda()\n\n cudnn.benchmark = True\n\n kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n     transforms.ToTensor(),\n     transforms.Normalize((0.1307,), (0.3081,))\n ]))\n test_dataset = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n     transforms.ToTensor(),\n     transforms.Normalize((0.1307,), (0.3081,))\n ]))\n train_loader = torch.utils.data.DataLoader(\n     train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n test_loader = torch.utils.data.DataLoader(\n     test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n optimizer = optim.SGD(model.parameters(), lr=args.lr,\n                       momentum=args.momentum)\n\n def train(epoch):\n     model.train()\n\n     for batch_idx, (data, target) in enumerate(train_loader):\n         if args.cuda:\n             data, target = data.cuda(), target.cuda()\n         data, target = Variable(data), Variable(target)\n         optimizer.zero_grad()\n         output = model(data)\n\n         loss = F.nll_loss(output, target)\n         loss.backward()\n         optimizer.step()\n\n         if batch_idx % args.log_interval == 0:\n             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                       epoch, batch_idx *\n                       len(data), len(train_loader.dataset),\n                       100. * batch_idx / len(train_loader), loss.data[0]))\n\n def test(epoch):\n     model.eval()\n     test_loss = 0\n     correct = 0\n     for data, target in test_loader:\n         if args.cuda:\n             data, target = data.cuda(), target.cuda()\n         data, target = Variable(data, volatile=True), Variable(target)\n         output = model(data)\n         test_loss += F.nll_loss(output, target).data[0]\n         # get the index of the max log-probability\n         pred = output.data.max(1)[1]\n         correct += pred.eq(target.data).cpu().sum()\n\n     test_loss = test_loss\n     # loss function already averages over batch size\n     test_loss /= len(test_loader)\n     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n         test_loss, correct, len(test_loader.dataset),\n         100. * correct / len(test_loader.dataset)))\n\n for epoch in range(1, args.epochs + 1):\n     train(epoch)\n     test(epoch) \n</code></pre>", "body_text": "Thank you.\nBut I am still wondering about the weights and biases initialization about the following code:\nthis code seems not have obvious initialization operator about weights and biases.\nCan you give me some suggestions?\nThank you so much.\nfrom __future__ import print_function\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\n\nif __name__ == '__main__':\n parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                     help='input batch size for training (default: 64)')\n parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                     help='input batch size for testing (default: 1000)')\n parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                     help='number of epochs to train (default: 10)')\n parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                     help='learning rate (default: 0.01)')\n parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                     help='SGD momentum (default: 0.5)')\n parser.add_argument('--no-cuda', action='store_true', default=False,\n                     help='disables CUDA training')\n parser.add_argument('--seed', type=int, default=1, metavar='S',\n                     help='random seed (default: 1)')\n parser.add_argument('--log-interval', type=int, default=10000, metavar='N',\n                     help='how many batches to wait before logging training status')\n args = parser.parse_args()\n args.cuda = not args.no_cuda and torch.cuda.is_available()\n\n print('Using CUDA:' + str(args.cuda))\n\n torch.manual_seed(args.seed)\n if args.cuda:\n     torch.cuda.manual_seed(args.seed)\n\n class Net(nn.Module):\n     def __init__(self):\n         super(Net, self).__init__()\n         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n         self.conv2_drop = nn.Dropout2d()\n         self.fc1 = nn.Linear(320, 50)\n         self.fc2 = nn.Linear(50, 10)\n\n\n     def forward(self, x):\n         x = F.relu(F.max_pool2d(self.conv1(x), 2))  \n         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n         x = x.view(-1, 320)\n         x = F.relu(self.fc1(x))\n         x = F.dropout(x, training=self.training)\n         x = self.fc2(x)\n         return F.log_softmax(x)\n\n model = Net()\n if args.cuda:\n     model.cuda()\n\n cudnn.benchmark = True\n\n kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n     transforms.ToTensor(),\n     transforms.Normalize((0.1307,), (0.3081,))\n ]))\n test_dataset = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n     transforms.ToTensor(),\n     transforms.Normalize((0.1307,), (0.3081,))\n ]))\n train_loader = torch.utils.data.DataLoader(\n     train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n test_loader = torch.utils.data.DataLoader(\n     test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n\n optimizer = optim.SGD(model.parameters(), lr=args.lr,\n                       momentum=args.momentum)\n\n def train(epoch):\n     model.train()\n\n     for batch_idx, (data, target) in enumerate(train_loader):\n         if args.cuda:\n             data, target = data.cuda(), target.cuda()\n         data, target = Variable(data), Variable(target)\n         optimizer.zero_grad()\n         output = model(data)\n\n         loss = F.nll_loss(output, target)\n         loss.backward()\n         optimizer.step()\n\n         if batch_idx % args.log_interval == 0:\n             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                       epoch, batch_idx *\n                       len(data), len(train_loader.dataset),\n                       100. * batch_idx / len(train_loader), loss.data[0]))\n\n def test(epoch):\n     model.eval()\n     test_loss = 0\n     correct = 0\n     for data, target in test_loader:\n         if args.cuda:\n             data, target = data.cuda(), target.cuda()\n         data, target = Variable(data, volatile=True), Variable(target)\n         output = model(data)\n         test_loss += F.nll_loss(output, target).data[0]\n         # get the index of the max log-probability\n         pred = output.data.max(1)[1]\n         correct += pred.eq(target.data).cpu().sum()\n\n     test_loss = test_loss\n     # loss function already averages over batch size\n     test_loss /= len(test_loader)\n     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n         test_loss, correct, len(test_loader.dataset),\n         100. * correct / len(test_loader.dataset)))\n\n for epoch in range(1, args.epochs + 1):\n     train(epoch)\n     test(epoch)", "body": "Thank you.\r\n\r\nBut I am still wondering about the weights and biases initialization about the following code:\r\nthis code seems not have obvious initialization operator about weights and biases.\r\n\r\nCan you give me some suggestions?\r\n\r\nThank you so much.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport argparse\r\nimport time\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import datasets, transforms\r\nfrom torch.autograd import Variable\r\nfrom torch.backends import cudnn\r\n\r\nif __name__ == '__main__':\r\n parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\r\n parser.add_argument('--batch-size', type=int, default=64, metavar='N',\r\n                     help='input batch size for training (default: 64)')\r\n parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\r\n                     help='input batch size for testing (default: 1000)')\r\n parser.add_argument('--epochs', type=int, default=10, metavar='N',\r\n                     help='number of epochs to train (default: 10)')\r\n parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\r\n                     help='learning rate (default: 0.01)')\r\n parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\r\n                     help='SGD momentum (default: 0.5)')\r\n parser.add_argument('--no-cuda', action='store_true', default=False,\r\n                     help='disables CUDA training')\r\n parser.add_argument('--seed', type=int, default=1, metavar='S',\r\n                     help='random seed (default: 1)')\r\n parser.add_argument('--log-interval', type=int, default=10000, metavar='N',\r\n                     help='how many batches to wait before logging training status')\r\n args = parser.parse_args()\r\n args.cuda = not args.no_cuda and torch.cuda.is_available()\r\n\r\n print('Using CUDA:' + str(args.cuda))\r\n\r\n torch.manual_seed(args.seed)\r\n if args.cuda:\r\n     torch.cuda.manual_seed(args.seed)\r\n\r\n class Net(nn.Module):\r\n     def __init__(self):\r\n         super(Net, self).__init__()\r\n         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\r\n         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\r\n         self.conv2_drop = nn.Dropout2d()\r\n         self.fc1 = nn.Linear(320, 50)\r\n         self.fc2 = nn.Linear(50, 10)\r\n\r\n\r\n     def forward(self, x):\r\n         x = F.relu(F.max_pool2d(self.conv1(x), 2))  \r\n         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\r\n         x = x.view(-1, 320)\r\n         x = F.relu(self.fc1(x))\r\n         x = F.dropout(x, training=self.training)\r\n         x = self.fc2(x)\r\n         return F.log_softmax(x)\r\n\r\n model = Net()\r\n if args.cuda:\r\n     model.cuda()\r\n\r\n cudnn.benchmark = True\r\n\r\n kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\r\n train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\r\n     transforms.ToTensor(),\r\n     transforms.Normalize((0.1307,), (0.3081,))\r\n ]))\r\n test_dataset = datasets.MNIST('../data', train=False, transform=transforms.Compose([\r\n     transforms.ToTensor(),\r\n     transforms.Normalize((0.1307,), (0.3081,))\r\n ]))\r\n train_loader = torch.utils.data.DataLoader(\r\n     train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\r\n test_loader = torch.utils.data.DataLoader(\r\n     test_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\r\n\r\n optimizer = optim.SGD(model.parameters(), lr=args.lr,\r\n                       momentum=args.momentum)\r\n\r\n def train(epoch):\r\n     model.train()\r\n\r\n     for batch_idx, (data, target) in enumerate(train_loader):\r\n         if args.cuda:\r\n             data, target = data.cuda(), target.cuda()\r\n         data, target = Variable(data), Variable(target)\r\n         optimizer.zero_grad()\r\n         output = model(data)\r\n\r\n         loss = F.nll_loss(output, target)\r\n         loss.backward()\r\n         optimizer.step()\r\n\r\n         if batch_idx % args.log_interval == 0:\r\n             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n                       epoch, batch_idx *\r\n                       len(data), len(train_loader.dataset),\r\n                       100. * batch_idx / len(train_loader), loss.data[0]))\r\n\r\n def test(epoch):\r\n     model.eval()\r\n     test_loss = 0\r\n     correct = 0\r\n     for data, target in test_loader:\r\n         if args.cuda:\r\n             data, target = data.cuda(), target.cuda()\r\n         data, target = Variable(data, volatile=True), Variable(target)\r\n         output = model(data)\r\n         test_loss += F.nll_loss(output, target).data[0]\r\n         # get the index of the max log-probability\r\n         pred = output.data.max(1)[1]\r\n         correct += pred.eq(target.data).cpu().sum()\r\n\r\n     test_loss = test_loss\r\n     # loss function already averages over batch size\r\n     test_loss /= len(test_loader)\r\n     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n         test_loss, correct, len(test_loader.dataset),\r\n         100. * correct / len(test_loader.dataset)))\r\n\r\n for epoch in range(1, args.epochs + 1):\r\n     train(epoch)\r\n     test(epoch) \r\n```"}