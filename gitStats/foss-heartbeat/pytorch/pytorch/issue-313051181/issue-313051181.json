{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6479", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6479/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6479/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6479/events", "html_url": "https://github.com/pytorch/pytorch/issues/6479", "id": 313051181, "node_id": "MDU6SXNzdWUzMTMwNTExODE=", "number": 6479, "title": "[feature request] enhance diagonal functionality", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 897288569, "node_id": "MDU6TGFiZWw4OTcyODg1Njk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/pytorch", "name": "pytorch", "color": "f05732", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-04-10T19:04:55Z", "updated_at": "2018-04-26T15:11:49Z", "closed_at": "2018-04-26T15:11:49Z", "author_association": "CONTRIBUTOR", "body_html": "<h1>Objective</h1>\n<p>PyTorch currently does not have a way to take diagonals with respect to arbitrary axes as e.g. <code>numpy.diagonal</code> has.<br>\nTwo immediate use cases are</p>\n<ul>\n<li>Taking the diagonal of a batch of matrices. This is handy e.g. in Gaussian processes, but I have seen this a number of times.</li>\n<li>One of the missing <code>einsum</code> features (<code>torch.einsum('jii-&gt;ji', a)</code>) would benefit from this as well.</li>\n</ul>\n<h1>Plan</h1>\n<p>I propose to do the following (see <a href=\"https://github.com/t-vi/pytorch/tree/diagonal_with_dim\">https://github.com/t-vi/pytorch/tree/diagonal_with_dim</a> for a preliminary implementation)</p>\n<ul>\n<li>Implement <code>diagonal</code> natively in ATen instead of referring to TH/THC <code>diag</code>.</li>\n<li>Make <code>diagonal</code> return a view by adjusting shape, stride, and offset. <code>diag</code> copies, but this seems impractical for higher dimensional arrays and also unneeded. <em>This is not backward compatible.</em></li>\n<li>Add two dimension parameters dim1 and dim2.</li>\n<li>Make a diagonal_backward method.</li>\n<li>Include the Tensor method, too.</li>\n</ul>\n<p><em>Edit</em>: Based on your feedback, I updated the code to use numpy semantics.</p>\n<h1><del>Limiting numpy compatibility?</del></h1>\n<p><del>NumPy's implementation has two features that might be worth to differ from:</del></p>\n<ul>\n<li><del>NumPy's default axes are 0 and 1. I think it is more natural to use -2 and -1 (\"batch thinking\").</del></li>\n<li><del>NumPy moves the new axis replacing the two old ones at the end of the tensor. I think it might be more natural to put it into the place of the first axis to be removed. I must admit that this is more a gut feeling than based on hard facts.</del></li>\n</ul>\n<p>I would greatly appreciate your input on this, in particular regarding the potential numpy deviations.</p>", "body_text": "Objective\nPyTorch currently does not have a way to take diagonals with respect to arbitrary axes as e.g. numpy.diagonal has.\nTwo immediate use cases are\n\nTaking the diagonal of a batch of matrices. This is handy e.g. in Gaussian processes, but I have seen this a number of times.\nOne of the missing einsum features (torch.einsum('jii->ji', a)) would benefit from this as well.\n\nPlan\nI propose to do the following (see https://github.com/t-vi/pytorch/tree/diagonal_with_dim for a preliminary implementation)\n\nImplement diagonal natively in ATen instead of referring to TH/THC diag.\nMake diagonal return a view by adjusting shape, stride, and offset. diag copies, but this seems impractical for higher dimensional arrays and also unneeded. This is not backward compatible.\nAdd two dimension parameters dim1 and dim2.\nMake a diagonal_backward method.\nInclude the Tensor method, too.\n\nEdit: Based on your feedback, I updated the code to use numpy semantics.\nLimiting numpy compatibility?\nNumPy's implementation has two features that might be worth to differ from:\n\nNumPy's default axes are 0 and 1. I think it is more natural to use -2 and -1 (\"batch thinking\").\nNumPy moves the new axis replacing the two old ones at the end of the tensor. I think it might be more natural to put it into the place of the first axis to be removed. I must admit that this is more a gut feeling than based on hard facts.\n\nI would greatly appreciate your input on this, in particular regarding the potential numpy deviations.", "body": "# Objective\r\nPyTorch currently does not have a way to take diagonals with respect to arbitrary axes as e.g. `numpy.diagonal` has.\r\nTwo immediate use cases are\r\n- Taking the diagonal of a batch of matrices. This is handy e.g. in Gaussian processes, but I have seen this a number of times.\r\n- One of the missing `einsum` features (`torch.einsum('jii->ji', a)`) would benefit from this as well.\r\n# Plan\r\nI propose to do the following (see https://github.com/t-vi/pytorch/tree/diagonal_with_dim for a preliminary implementation)\r\n- Implement `diagonal` natively in ATen instead of referring to TH/THC `diag`.\r\n- Make `diagonal` return a view by adjusting shape, stride, and offset. `diag` copies, but this seems impractical for higher dimensional arrays and also unneeded. *This is not backward compatible.*\r\n- Add two dimension parameters dim1 and dim2.\r\n- Make a diagonal_backward method.\r\n- Include the Tensor method, too.\r\n\r\n*Edit*: Based on your feedback, I updated the code to use numpy semantics.\r\n\r\n# ~~Limiting numpy compatibility?~~\r\n~~NumPy's implementation has two features that might be worth to differ from:~~\r\n- ~~NumPy's default axes are 0 and 1. I think it is more natural to use -2 and -1 (\"batch thinking\").~~\r\n- ~~NumPy moves the new axis replacing the two old ones at the end of the tensor. I think it might be more natural to put it into the place of the first axis to be removed. I must admit that this is more a gut feeling than based on hard facts.~~\r\n\r\nI would greatly appreciate your input on this, in particular regarding the potential numpy deviations."}