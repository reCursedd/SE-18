{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427192654", "html_url": "https://github.com/pytorch/pytorch/issues/12327#issuecomment-427192654", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12327", "id": 427192654, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzE5MjY1NA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-04T22:41:32Z", "updated_at": "2018-10-04T22:41:32Z", "author_association": "MEMBER", "body_html": "<p>In the meantime, you can implement <code>smooth_l1_loss</code> via the following:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">smooth_l1_loss</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">beta</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    very similar to the smooth_l1_loss from pytorch, but with</span>\n<span class=\"pl-s\">    the extra beta parameter</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    n <span class=\"pl-k\">=</span> torch.abs(<span class=\"pl-c1\">input</span> <span class=\"pl-k\">-</span> target)\n    cond <span class=\"pl-k\">=</span> n <span class=\"pl-k\">&lt;</span> beta\n    loss <span class=\"pl-k\">=</span> torch.where(cond, <span class=\"pl-c1\">0.5</span> <span class=\"pl-k\">*</span> n <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">/</span> beta, n <span class=\"pl-k\">-</span> <span class=\"pl-c1\">0.5</span> <span class=\"pl-k\">*</span> beta)\n    <span class=\"pl-k\">if</span> size_average:\n        <span class=\"pl-k\">return</span> loss.mean()\n    <span class=\"pl-k\">return</span> loss.sum()</pre></div>", "body_text": "In the meantime, you can implement smooth_l1_loss via the following:\ndef smooth_l1_loss(input, target, beta=1, size_average=True):\n    \"\"\"\n    very similar to the smooth_l1_loss from pytorch, but with\n    the extra beta parameter\n    \"\"\"\n    n = torch.abs(input - target)\n    cond = n < beta\n    loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)\n    if size_average:\n        return loss.mean()\n    return loss.sum()", "body": "In the meantime, you can implement `smooth_l1_loss` via the following:\r\n```python\r\ndef smooth_l1_loss(input, target, beta=1, size_average=True):\r\n    \"\"\"\r\n    very similar to the smooth_l1_loss from pytorch, but with\r\n    the extra beta parameter\r\n    \"\"\"\r\n    n = torch.abs(input - target)\r\n    cond = n < beta\r\n    loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)\r\n    if size_average:\r\n        return loss.mean()\r\n    return loss.sum()\r\n```"}