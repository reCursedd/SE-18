{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218950986", "pull_request_review_id": 157005798, "id": 218950986, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxODk1MDk4Ng==", "diff_hunk": "@@ -531,9 +531,39 @@ Tensor as_tensor(const Type& type, PyObject* args, PyObject* kwargs) {\n   ParsedArgs<3> parsed_args;\n   auto r = parser.parse(args, kwargs, parsed_args);\n   if (r.idx == 0) {\n+    at::optional<Device> device_opt = r.deviceOptional(2);", "path": "torch/csrc/utils/tensor_new.cpp", "position": null, "original_position": 69, "commit_id": "36d298cb704d694bee62dd72e88973fd72e343c2", "original_commit_id": "d2769423060c797280f006b501cd687ed369d271", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "body": "just so that I understand this correctly, so the documented behavior of `as_tensor` is that: either returns the tensor passed in or it returns a new Tensor with `requires_grad = True` and `grad_fn = CopyBackwards`. Do I need to change  this behavior? If true, then I will still need to call `detach`. Is it right?", "created_at": "2018-09-19T20:25:04Z", "updated_at": "2018-11-23T15:51:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/11815#discussion_r218950986", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11815", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218950986"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11815#discussion_r218950986"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11815"}}, "body_html": "<p>just so that I understand this correctly, so the documented behavior of <code>as_tensor</code> is that: either returns the tensor passed in or it returns a new Tensor with <code>requires_grad = True</code> and <code>grad_fn = CopyBackwards</code>. Do I need to change  this behavior? If true, then I will still need to call <code>detach</code>. Is it right?</p>", "body_text": "just so that I understand this correctly, so the documented behavior of as_tensor is that: either returns the tensor passed in or it returns a new Tensor with requires_grad = True and grad_fn = CopyBackwards. Do I need to change  this behavior? If true, then I will still need to call detach. Is it right?", "in_reply_to_id": 218859225}