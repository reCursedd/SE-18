{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208831912", "pull_request_review_id": 144721078, "id": 208831912, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwODgzMTkxMg==", "diff_hunk": "@@ -140,6 +141,80 @@ def btriunpack(LU_data, LU_pivots, unpack_data=True, unpack_pivots=True):\n     return P, L, U\n \n \n+def einsum(equation, *operands):\n+    r\"\"\"einsum(equation, *operands) -> Tensor\n+\n+This function provides a way of computing multilinear expressions (i.e. sums of products) using the\n+Einstein summation convention.\n+\n+Args:\n+    equation (string): The equation is given in terms of lower case letters (indices) to be associated\n+           with each dimension of the operands and result. The left hand side lists the operands\n+           dimensions, separated by commas. There should be one index letter per tensor dimension.\n+           The right hand side follows after `->` and gives the indices for the output.\n+           If the `->` and right hand side are omitted, it implicitly defined as the alphabetically\n+           sorted list of all indices appearing exactly once in the left hand side.\n+           The indices not apprearing in the output are summed over after multiplying the operands\n+           entries.\n+           If an index appears several times for the same operand, a diagonal is taken.\n+           Ellipses `...` represent a fixed number of dimensions. If the right hand side is inferred,\n+           the ellipsis dimensions are at the beginning of the output.\n+    operands (list of Tensors): The operands to compute the Einstein sum of.\n+           Note that the operands are passed as a list, not as individual arguments.\n+\n+Examples::\n+\n+    >>> x = torch.randn(5)\n+    >>> y = torch.randn(4)\n+    >>> torch.einsum('i,j->ij', x, y)  # outer product\n+    tensor([[-0.0570, -0.0286, -0.0231,  0.0197],\n+            [ 1.2616,  0.6335,  0.5113, -0.4351],\n+            [ 1.4452,  0.7257,  0.5857, -0.4984],\n+            [-0.4647, -0.2333, -0.1883,  0.1603],\n+            [-1.1130, -0.5588, -0.4510,  0.3838]])\n+\n+\n+    >>> A = torch.randn(3,5,4)\n+    >>> l = torch.randn(2,5)\n+    >>> r = torch.randn(2,4)\n+    >>> torch.einsum('bn,anm,bm->ba', l, A, r) # compare torch.nn.functional.bilinear\n+    tensor([[-0.3430, -5.2405,  0.4494],\n+            [ 0.3311,  5.5201, -3.0356]])\n+\n+\n+    >>> As = torch.randn(3,2,5)\n+    >>> Bs = torch.randn(3,5,4)\n+    >>> torch.einsum('bij,bjk->bik', As, Bs) # batch matrix multiplication\n+    tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n+             [-1.6706, -0.8097, -0.8025, -2.1183]],\n+\n+            [[ 4.2239,  0.3107, -0.5756, -0.2354],\n+             [-1.4558, -0.3460,  1.5087, -0.8530]],\n+\n+            [[ 2.8153,  1.8787, -4.3839, -1.2112],\n+             [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n+\n+    >>> A = torch.randn(3, 3)\n+    >>> torch.einsum('ii->i', A) # diagonal\n+    tensor([-0.7825,  0.8291, -0.1936])\n+\n+    >>> A = torch.randn(4, 3, 3)\n+    >>> torch.einsum('...ii->...i', A) # batch diagonal\n+    tensor([[-1.0864,  0.7292,  0.0569],\n+            [-0.9725, -1.0270,  0.6493],\n+            [ 0.5832, -1.1716, -1.5084],\n+            [ 0.4041, -1.1690,  0.8570]])\n+\n+    >>> A = torch.randn(2, 3, 4, 5)\n+    >>> torch.einsum('...ij->...ji', A).shape # batch permute\n+    torch.Size([2, 3, 5, 4])\n+\"\"\"\n+    if len(operands) == 1 and isinstance(operands[0], (list, tuple)):\n+        # the old interface of passing the operands as one list argument\n+        operands = operands[0]", "path": "torch/functional.py", "position": 82, "original_position": 82, "commit_id": "eaa3dd8387aafdc632f064ba4d77446c771550bb", "original_commit_id": "267284934a28ac821f0af19230023ba7cc5b0b8a", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "I don't know and will adjust to your preference. The C++ side will still have it.", "created_at": "2018-08-09T07:36:02Z", "updated_at": "2018-11-23T15:49:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/10067#discussion_r208831912", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10067", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/208831912"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10067#discussion_r208831912"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10067"}}, "body_html": "<p>I don't know and will adjust to your preference. The C++ side will still have it.</p>", "body_text": "I don't know and will adjust to your preference. The C++ side will still have it.", "in_reply_to_id": 206604273}