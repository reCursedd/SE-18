{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11944", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11944/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11944/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11944/events", "html_url": "https://github.com/pytorch/pytorch/issues/11944", "id": 362719375, "node_id": "MDU6SXNzdWUzNjI3MTkzNzU=", "number": 11944, "title": "`torch.utils.checkpoint` fail to propagate `backward` to all parameters", "user": {"login": "CharlesJQuarra", "id": 7565570, "node_id": "MDQ6VXNlcjc1NjU1NzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/7565570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CharlesJQuarra", "html_url": "https://github.com/CharlesJQuarra", "followers_url": "https://api.github.com/users/CharlesJQuarra/followers", "following_url": "https://api.github.com/users/CharlesJQuarra/following{/other_user}", "gists_url": "https://api.github.com/users/CharlesJQuarra/gists{/gist_id}", "starred_url": "https://api.github.com/users/CharlesJQuarra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CharlesJQuarra/subscriptions", "organizations_url": "https://api.github.com/users/CharlesJQuarra/orgs", "repos_url": "https://api.github.com/users/CharlesJQuarra/repos", "events_url": "https://api.github.com/users/CharlesJQuarra/events{/privacy}", "received_events_url": "https://api.github.com/users/CharlesJQuarra/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-09-21T18:01:53Z", "updated_at": "2018-09-21T19:29:34Z", "closed_at": "2018-09-21T19:29:34Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Discovered while attempting to write a linear unit that supports splitting the parameter space in a grid of gradient checkpoint nodes. The issue right now is that when there is more than one segment, the <code>backward()</code> only updates the gradient for the last parameter. More details in the code example:</p>\n<h2>Code example</h2>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\n\ndef get_segments(total, max_length):\n  segments = (total // max_length)\n  seg_array = segments*[max_length]\n  remainder = total % max_length\n  if remainder &gt; 0:\n    seg_array += [remainder]\n  return seg_array\n\nclass GradCheckpoint_Linear(nn.Module):\n  def __init__(self, in_features, out_features, cpc_specs={}):\n    super(GradCheckpoint_Linear, self).__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n\n    if 'in_max_segment' in cpc_specs:\n      in_max_segment = cpc_specs['in_max_segment']\n    else:\n      in_max_segment = in_features\n    self.in_segment_lengths = get_segments(in_features, in_max_segment)\n    if 'out_max_segment' in cpc_specs:\n      out_max_segment = cpc_specs['out_max_segment']\n    else:\n      out_max_segment = out_features\n    if 'initializer' in cpc_specs:\n      self.initializer = cpc_specs['initializer']\n    else:\n      def get_init(w, h):\n        return torch.randn(w,h)\n      self.initializer = get_init\n    self.out_segment_lengths = get_segments(out_features, out_max_segment)\n\n    print(\"in_segment_lengths: {0}\".format(self.in_segment_lengths))\n    print(\"out_segment_lengths: {0}\".format(self.out_segment_lengths))\n    weight_parameters_ = []\n    bias_parameters_ = []\n\n    self.array_to_weight_param = -torch.ones(len(self.in_segment_lengths), len(self.out_segment_lengths), dtype=torch.int32)\n    for in_idx, in_s_length in enumerate(self.in_segment_lengths):\n      for out_idx, out_s_length in enumerate(self.out_segment_lengths):\n        param = nn.Parameter( self.initializer(out_s_length,in_s_length) )\n        self.array_to_weight_param[in_idx,out_idx]=len(weight_parameters_)\n        weight_parameters_.append( param )\n\n    self.weight_parameters = nn.ParameterList( weight_parameters_ )\n\n    for out_s_length in self.out_segment_lengths:\n      bias_parameters_.append( nn.Parameter( self.initializer(1, out_s_length).view(out_s_length) ) )\n    self.bias_parameters = nn.ParameterList( bias_parameters_ )\n\n  def reset_parameters(self):\n    pass\n\n  def forward(self, inp):\n    unit_outs = []\n    for out_idx, out_s_length in enumerate(self.out_segment_lengths):\n      bias_param = self.bias_parameters[out_idx]\n      in_offset = 0\n      weight_outs = []\n      for in_idx, in_s_length in enumerate(self.in_segment_lengths):\n        weight_param = self.weight_parameters[ self.array_to_weight_param[in_idx, out_idx] ]\n        def fwd_unit_segment(inp_):\n          return torch.mv(weight_param, inp_)\n        weight_out = checkpoint.checkpoint( fwd_unit_segment , inp[in_offset:in_offset+in_s_length] )\n        in_offset += in_s_length\n        weight_outs.append(weight_out)\n      unit_outs.append( bias_param + sum(weight_outs) )\n    result = torch.cat(unit_outs)\n    return result\n\ndef init_(w,h):\n  return torch.ones(w,h)\n\nu_original = GradCheckpoint_Linear(6, 2, cpc_specs={'initializer': init_})\nu_split = GradCheckpoint_Linear(6, 2, cpc_specs={'in_max_segment': 2, 'out_max_segment': 1, 'initializer': init_})\n\ninp_0_orig = torch.ones(6, requires_grad=True)\ninp_0_split = torch.ones(6, requires_grad=True)\n\nu_original(inp_0_orig).sum().backward()\nu_original.weight_parameters[0].grad  #&lt;--- looks good\n\ndef top_node(inp_):\n  return u_split(inp_)\n\nv = checkpoint.checkpoint(top_node, inp_0_split )\nv.sum().backward()\n\nu_split.weight_parameters[0].grad #&lt;--- grad is None\nu_split.weight_parameters[5].grad #&lt;--- only grad parameter that has values is the last one?\n\n</code></pre>\n<h2>System Info</h2>\n<p>Collecting environment information...<br>\nPyTorch version: 0.4.0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609<br>\nCMake version: Could not collect</p>\n<p>Python version: 3.6<br>\nIs CUDA available: No<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nNvidia driver version: Could not collect<br>\ncuDNN version: Probably one of the following:<br>\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4<br>\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a</p>\n<p>Versions of relevant libraries:<br>\n[pip3] numpy (1.14.3)<br>\n[pip3] torch (0.4.0)<br>\n[pip3] torchfile (0.1.0)<br>\n[pip3] torchnet (0.0.2)<br>\n[conda] cuda91                    1.0                  h4c16780_0    pytorch<br>\n[conda] pytorch                   0.4.0           py36_cuda9.1.85_cudnn7.1.2_1  [cuda91]  pytorch<br>\n[conda] torchvision               0.2.0            py36h17b6947_1    pytorch</p>", "body_text": "Issue description\nDiscovered while attempting to write a linear unit that supports splitting the parameter space in a grid of gradient checkpoint nodes. The issue right now is that when there is more than one segment, the backward() only updates the gradient for the last parameter. More details in the code example:\nCode example\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\n\ndef get_segments(total, max_length):\n  segments = (total // max_length)\n  seg_array = segments*[max_length]\n  remainder = total % max_length\n  if remainder > 0:\n    seg_array += [remainder]\n  return seg_array\n\nclass GradCheckpoint_Linear(nn.Module):\n  def __init__(self, in_features, out_features, cpc_specs={}):\n    super(GradCheckpoint_Linear, self).__init__()\n    self.in_features = in_features\n    self.out_features = out_features\n\n    if 'in_max_segment' in cpc_specs:\n      in_max_segment = cpc_specs['in_max_segment']\n    else:\n      in_max_segment = in_features\n    self.in_segment_lengths = get_segments(in_features, in_max_segment)\n    if 'out_max_segment' in cpc_specs:\n      out_max_segment = cpc_specs['out_max_segment']\n    else:\n      out_max_segment = out_features\n    if 'initializer' in cpc_specs:\n      self.initializer = cpc_specs['initializer']\n    else:\n      def get_init(w, h):\n        return torch.randn(w,h)\n      self.initializer = get_init\n    self.out_segment_lengths = get_segments(out_features, out_max_segment)\n\n    print(\"in_segment_lengths: {0}\".format(self.in_segment_lengths))\n    print(\"out_segment_lengths: {0}\".format(self.out_segment_lengths))\n    weight_parameters_ = []\n    bias_parameters_ = []\n\n    self.array_to_weight_param = -torch.ones(len(self.in_segment_lengths), len(self.out_segment_lengths), dtype=torch.int32)\n    for in_idx, in_s_length in enumerate(self.in_segment_lengths):\n      for out_idx, out_s_length in enumerate(self.out_segment_lengths):\n        param = nn.Parameter( self.initializer(out_s_length,in_s_length) )\n        self.array_to_weight_param[in_idx,out_idx]=len(weight_parameters_)\n        weight_parameters_.append( param )\n\n    self.weight_parameters = nn.ParameterList( weight_parameters_ )\n\n    for out_s_length in self.out_segment_lengths:\n      bias_parameters_.append( nn.Parameter( self.initializer(1, out_s_length).view(out_s_length) ) )\n    self.bias_parameters = nn.ParameterList( bias_parameters_ )\n\n  def reset_parameters(self):\n    pass\n\n  def forward(self, inp):\n    unit_outs = []\n    for out_idx, out_s_length in enumerate(self.out_segment_lengths):\n      bias_param = self.bias_parameters[out_idx]\n      in_offset = 0\n      weight_outs = []\n      for in_idx, in_s_length in enumerate(self.in_segment_lengths):\n        weight_param = self.weight_parameters[ self.array_to_weight_param[in_idx, out_idx] ]\n        def fwd_unit_segment(inp_):\n          return torch.mv(weight_param, inp_)\n        weight_out = checkpoint.checkpoint( fwd_unit_segment , inp[in_offset:in_offset+in_s_length] )\n        in_offset += in_s_length\n        weight_outs.append(weight_out)\n      unit_outs.append( bias_param + sum(weight_outs) )\n    result = torch.cat(unit_outs)\n    return result\n\ndef init_(w,h):\n  return torch.ones(w,h)\n\nu_original = GradCheckpoint_Linear(6, 2, cpc_specs={'initializer': init_})\nu_split = GradCheckpoint_Linear(6, 2, cpc_specs={'in_max_segment': 2, 'out_max_segment': 1, 'initializer': init_})\n\ninp_0_orig = torch.ones(6, requires_grad=True)\ninp_0_split = torch.ones(6, requires_grad=True)\n\nu_original(inp_0_orig).sum().backward()\nu_original.weight_parameters[0].grad  #<--- looks good\n\ndef top_node(inp_):\n  return u_split(inp_)\n\nv = checkpoint.checkpoint(top_node, inp_0_split )\nv.sum().backward()\n\nu_split.weight_parameters[0].grad #<--- grad is None\nu_split.weight_parameters[5].grad #<--- only grad parameter that has values is the last one?\n\n\nSystem Info\nCollecting environment information...\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\nCMake version: Could not collect\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nNvidia driver version: Could not collect\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\nVersions of relevant libraries:\n[pip3] numpy (1.14.3)\n[pip3] torch (0.4.0)\n[pip3] torchfile (0.1.0)\n[pip3] torchnet (0.0.2)\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\n[conda] pytorch                   0.4.0           py36_cuda9.1.85_cudnn7.1.2_1  [cuda91]  pytorch\n[conda] torchvision               0.2.0            py36h17b6947_1    pytorch", "body": "## Issue description\r\n\r\nDiscovered while attempting to write a linear unit that supports splitting the parameter space in a grid of gradient checkpoint nodes. The issue right now is that when there is more than one segment, the `backward()` only updates the gradient for the last parameter. More details in the code example:\r\n\r\n## Code example\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.utils.checkpoint as checkpoint\r\n\r\ndef get_segments(total, max_length):\r\n  segments = (total // max_length)\r\n  seg_array = segments*[max_length]\r\n  remainder = total % max_length\r\n  if remainder > 0:\r\n    seg_array += [remainder]\r\n  return seg_array\r\n\r\nclass GradCheckpoint_Linear(nn.Module):\r\n  def __init__(self, in_features, out_features, cpc_specs={}):\r\n    super(GradCheckpoint_Linear, self).__init__()\r\n    self.in_features = in_features\r\n    self.out_features = out_features\r\n\r\n    if 'in_max_segment' in cpc_specs:\r\n      in_max_segment = cpc_specs['in_max_segment']\r\n    else:\r\n      in_max_segment = in_features\r\n    self.in_segment_lengths = get_segments(in_features, in_max_segment)\r\n    if 'out_max_segment' in cpc_specs:\r\n      out_max_segment = cpc_specs['out_max_segment']\r\n    else:\r\n      out_max_segment = out_features\r\n    if 'initializer' in cpc_specs:\r\n      self.initializer = cpc_specs['initializer']\r\n    else:\r\n      def get_init(w, h):\r\n        return torch.randn(w,h)\r\n      self.initializer = get_init\r\n    self.out_segment_lengths = get_segments(out_features, out_max_segment)\r\n\r\n    print(\"in_segment_lengths: {0}\".format(self.in_segment_lengths))\r\n    print(\"out_segment_lengths: {0}\".format(self.out_segment_lengths))\r\n    weight_parameters_ = []\r\n    bias_parameters_ = []\r\n\r\n    self.array_to_weight_param = -torch.ones(len(self.in_segment_lengths), len(self.out_segment_lengths), dtype=torch.int32)\r\n    for in_idx, in_s_length in enumerate(self.in_segment_lengths):\r\n      for out_idx, out_s_length in enumerate(self.out_segment_lengths):\r\n        param = nn.Parameter( self.initializer(out_s_length,in_s_length) )\r\n        self.array_to_weight_param[in_idx,out_idx]=len(weight_parameters_)\r\n        weight_parameters_.append( param )\r\n\r\n    self.weight_parameters = nn.ParameterList( weight_parameters_ )\r\n\r\n    for out_s_length in self.out_segment_lengths:\r\n      bias_parameters_.append( nn.Parameter( self.initializer(1, out_s_length).view(out_s_length) ) )\r\n    self.bias_parameters = nn.ParameterList( bias_parameters_ )\r\n\r\n  def reset_parameters(self):\r\n    pass\r\n\r\n  def forward(self, inp):\r\n    unit_outs = []\r\n    for out_idx, out_s_length in enumerate(self.out_segment_lengths):\r\n      bias_param = self.bias_parameters[out_idx]\r\n      in_offset = 0\r\n      weight_outs = []\r\n      for in_idx, in_s_length in enumerate(self.in_segment_lengths):\r\n        weight_param = self.weight_parameters[ self.array_to_weight_param[in_idx, out_idx] ]\r\n        def fwd_unit_segment(inp_):\r\n          return torch.mv(weight_param, inp_)\r\n        weight_out = checkpoint.checkpoint( fwd_unit_segment , inp[in_offset:in_offset+in_s_length] )\r\n        in_offset += in_s_length\r\n        weight_outs.append(weight_out)\r\n      unit_outs.append( bias_param + sum(weight_outs) )\r\n    result = torch.cat(unit_outs)\r\n    return result\r\n\r\ndef init_(w,h):\r\n  return torch.ones(w,h)\r\n\r\nu_original = GradCheckpoint_Linear(6, 2, cpc_specs={'initializer': init_})\r\nu_split = GradCheckpoint_Linear(6, 2, cpc_specs={'in_max_segment': 2, 'out_max_segment': 1, 'initializer': init_})\r\n\r\ninp_0_orig = torch.ones(6, requires_grad=True)\r\ninp_0_split = torch.ones(6, requires_grad=True)\r\n\r\nu_original(inp_0_orig).sum().backward()\r\nu_original.weight_parameters[0].grad  #<--- looks good\r\n\r\ndef top_node(inp_):\r\n  return u_split(inp_)\r\n\r\nv = checkpoint.checkpoint(top_node, inp_0_split )\r\nv.sum().backward()\r\n\r\nu_split.weight_parameters[0].grad #<--- grad is None\r\nu_split.weight_parameters[5].grad #<--- only grad parameter that has values is the last one?\r\n\r\n```\r\n\r\n## System Info\r\nCollecting environment information...\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCMake version: Could not collect\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nNvidia driver version: Could not collect\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.4\r\n/usr/lib/x86_64-linux-gnu/libcudnn_static_v7.a\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.3)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchfile (0.1.0)\r\n[pip3] torchnet (0.0.2)\r\n[conda] cuda91                    1.0                  h4c16780_0    pytorch\r\n[conda] pytorch                   0.4.0           py36_cuda9.1.85_cudnn7.1.2_1  [cuda91]  pytorch\r\n[conda] torchvision               0.2.0            py36h17b6947_1    pytorch\r\n\r\n\r\n"}