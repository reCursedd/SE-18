{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2883", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2883/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2883/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2883/events", "html_url": "https://github.com/pytorch/pytorch/pull/2883", "id": 261183685, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQzNTQ4NDEw", "number": 2883, "title": "make torch.save() atomic by default", "user": {"login": "eklitzke", "id": 2734, "node_id": "MDQ6VXNlcjI3MzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eklitzke", "html_url": "https://github.com/eklitzke", "followers_url": "https://api.github.com/users/eklitzke/followers", "following_url": "https://api.github.com/users/eklitzke/following{/other_user}", "gists_url": "https://api.github.com/users/eklitzke/gists{/gist_id}", "starred_url": "https://api.github.com/users/eklitzke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eklitzke/subscriptions", "organizations_url": "https://api.github.com/users/eklitzke/orgs", "repos_url": "https://api.github.com/users/eklitzke/repos", "events_url": "https://api.github.com/users/eklitzke/events{/privacy}", "received_events_url": "https://api.github.com/users/eklitzke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-28T03:37:03Z", "updated_at": "2017-09-28T03:47:33Z", "closed_at": "2017-09-28T03:47:33Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2883", "html_url": "https://github.com/pytorch/pytorch/pull/2883", "diff_url": "https://github.com/pytorch/pytorch/pull/2883.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2883.patch"}, "body_html": "<p>This makes <code>torch.save()</code> atomic, by using a temporary file and then renaming it. This approach is guaranteed to be atomic on POSIX filesystems. I didn't make an option to use non-atomic saves since the overhead of doing this is very low (esp. compared to the other things that <code>torch.save()</code> has to do), so it didn't seem like it was worth the cognitive burden. I added some basic sanity tests.</p>\n<p>Short version of why I want this: I have a training job that serializes model state every <em>N</em> iterations during training, and I have another process that periodically scans the state directory to look at the state files. Making <code>torch.save()</code> atomic ensures that there aren't any race conditions between process <em>A</em> saving a file and process <em>B</em> loading the file.</p>", "body_text": "This makes torch.save() atomic, by using a temporary file and then renaming it. This approach is guaranteed to be atomic on POSIX filesystems. I didn't make an option to use non-atomic saves since the overhead of doing this is very low (esp. compared to the other things that torch.save() has to do), so it didn't seem like it was worth the cognitive burden. I added some basic sanity tests.\nShort version of why I want this: I have a training job that serializes model state every N iterations during training, and I have another process that periodically scans the state directory to look at the state files. Making torch.save() atomic ensures that there aren't any race conditions between process A saving a file and process B loading the file.", "body": "This makes `torch.save()` atomic, by using a temporary file and then renaming it. This approach is guaranteed to be atomic on POSIX filesystems. I didn't make an option to use non-atomic saves since the overhead of doing this is very low (esp. compared to the other things that `torch.save()` has to do), so it didn't seem like it was worth the cognitive burden. I added some basic sanity tests.\r\n\r\nShort version of why I want this: I have a training job that serializes model state every *N* iterations during training, and I have another process that periodically scans the state directory to look at the state files. Making `torch.save()` atomic ensures that there aren't any race conditions between process *A* saving a file and process *B* loading the file."}