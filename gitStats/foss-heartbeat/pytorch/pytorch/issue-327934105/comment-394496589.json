{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/394496589", "html_url": "https://github.com/pytorch/pytorch/pull/7973#issuecomment-394496589", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7973", "id": 394496589, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDQ5NjU4OQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-04T20:56:28Z", "updated_at": "2018-06-04T20:56:28Z", "author_association": "MEMBER", "body_html": "<p>I disagree.</p>\n<ol>\n<li>If people want to calculate their losses at each GPU then using the stock DP is just dumb. It inserts this extra concat step which involves initiation transfers across PCIe, when you could have just started backward from multiple losses stored in a Python list (basically you should replace the <code>gather</code> part).</li>\n<li>Not at all. DP assumes that <code>dim</code> is the batch dimension, and it should be present both in the input and output. The contract is that it will slice and concat along it, and so the output won't change, no matter how many GPUs you have.</li>\n<li>Really? What would happen in 0.3 if you were to return a Python scalar from DataParallel? I'm 99.99% sure it was an error.</li>\n<li>See 1. That is an unnecessary performance hit.</li>\n</ol>", "body_text": "I disagree.\n\nIf people want to calculate their losses at each GPU then using the stock DP is just dumb. It inserts this extra concat step which involves initiation transfers across PCIe, when you could have just started backward from multiple losses stored in a Python list (basically you should replace the gather part).\nNot at all. DP assumes that dim is the batch dimension, and it should be present both in the input and output. The contract is that it will slice and concat along it, and so the output won't change, no matter how many GPUs you have.\nReally? What would happen in 0.3 if you were to return a Python scalar from DataParallel? I'm 99.99% sure it was an error.\nSee 1. That is an unnecessary performance hit.", "body": "I disagree.\r\n\r\n1. If people want to calculate their losses at each GPU then using the stock DP is just dumb. It inserts this extra concat step which involves initiation transfers across PCIe, when you could have just started backward from multiple losses stored in a Python list (basically you should replace the `gather` part).\r\n2. Not at all. DP assumes that `dim` is the batch dimension, and it should be present both in the input and output. The contract is that it will slice and concat along it, and so the output won't change, no matter how many GPUs you have.\r\n3. Really? What would happen in 0.3 if you were to return a Python scalar from DataParallel? I'm 99.99% sure it was an error.\r\n4. See 1. That is an unnecessary performance hit."}