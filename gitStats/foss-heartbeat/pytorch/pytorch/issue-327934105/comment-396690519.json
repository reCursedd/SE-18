{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/396690519", "html_url": "https://github.com/pytorch/pytorch/pull/7973#issuecomment-396690519", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7973", "id": 396690519, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NjY5MDUxOQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-12T18:33:56Z", "updated_at": "2018-06-12T18:33:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> Sorry I somehow missed your previous message. Regarding to the points you raised,</p>\n<p>1 &amp; 4. I agree that it takes a performance hit. But the same thing happens for the usual DP pattern. Instead of calculating loss on a single GPU on the gathered tensors (e.g. last fc outputs), one would be better off just <code>.backward</code> on each GPU in parallel. In fact, the usual pattern is also arguably worse than calculating loss on each GPU because the broadcasted tensors are larger.<br>\n2. I worded it badly, I meant setting <code>dim=batch_dimension</code> when using DP.<br>\n3. It worked before because loss functions gives 1-dimensional Variables rather than scalars. I think autograd also worked correctly before.</p>\n<p>So the points really are that (1) it worked before 0.4, and DP wasn't a non-existent wrapper in this case, and that (2) DP already takes a performance hit by trying to be a non-existent wrapper, so I don't feel the perf concern is really valid.</p>", "body_text": "@apaszke Sorry I somehow missed your previous message. Regarding to the points you raised,\n1 & 4. I agree that it takes a performance hit. But the same thing happens for the usual DP pattern. Instead of calculating loss on a single GPU on the gathered tensors (e.g. last fc outputs), one would be better off just .backward on each GPU in parallel. In fact, the usual pattern is also arguably worse than calculating loss on each GPU because the broadcasted tensors are larger.\n2. I worded it badly, I meant setting dim=batch_dimension when using DP.\n3. It worked before because loss functions gives 1-dimensional Variables rather than scalars. I think autograd also worked correctly before.\nSo the points really are that (1) it worked before 0.4, and DP wasn't a non-existent wrapper in this case, and that (2) DP already takes a performance hit by trying to be a non-existent wrapper, so I don't feel the perf concern is really valid.", "body": "@apaszke Sorry I somehow missed your previous message. Regarding to the points you raised,\r\n\r\n1 & 4. I agree that it takes a performance hit. But the same thing happens for the usual DP pattern. Instead of calculating loss on a single GPU on the gathered tensors (e.g. last fc outputs), one would be better off just `.backward` on each GPU in parallel. In fact, the usual pattern is also arguably worse than calculating loss on each GPU because the broadcasted tensors are larger.\r\n2. I worded it badly, I meant setting `dim=batch_dimension` when using DP. \r\n3. It worked before because loss functions gives 1-dimensional Variables rather than scalars. I think autograd also worked correctly before.\r\n\r\nSo the points really are that (1) it worked before 0.4, and DP wasn't a non-existent wrapper in this case, and that (2) DP already takes a performance hit by trying to be a non-existent wrapper, so I don't feel the perf concern is really valid. "}