{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/395009837", "html_url": "https://github.com/pytorch/pytorch/pull/7973#issuecomment-395009837", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7973", "id": 395009837, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NTAwOTgzNw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-06T09:41:21Z", "updated_at": "2018-06-06T09:41:21Z", "author_association": "MEMBER", "body_html": "<p>1 &amp; 4. I can't see how is this relevant. I'm not saying you're not supposed to calculate losses on multiple GPUs in parallel. I mean that once you do it, you'd be much better off with returning a Python list of PyTorch scalars, and starting <code>torch.autograd.backward</code> from all of them in parallel. If you use DataParallel as it is written today it will unnecessarily gather them to a single device, just to let you call <code>.backward()</code> on their mean.<br>\n2. Not sure what you mean by \"reduce along the batch dimension\", but you've got a point in here. It will depend on the number of GPUs, which generally doesn't fit the DataParallel invariant, and breaks the interface (slice along <code>dim</code>, concat along <code>dim</code>, but you now ignore <code>dim</code> completely and set it to some unrepresentable value, since <code>cat</code> on scalars in generally not supported).<br>\n3. Ok, that's weird. I have no idea why it did those things, but they don't feel right (this Variable probably wasn't even attached to the computational graph).</p>", "body_text": "1 & 4. I can't see how is this relevant. I'm not saying you're not supposed to calculate losses on multiple GPUs in parallel. I mean that once you do it, you'd be much better off with returning a Python list of PyTorch scalars, and starting torch.autograd.backward from all of them in parallel. If you use DataParallel as it is written today it will unnecessarily gather them to a single device, just to let you call .backward() on their mean.\n2. Not sure what you mean by \"reduce along the batch dimension\", but you've got a point in here. It will depend on the number of GPUs, which generally doesn't fit the DataParallel invariant, and breaks the interface (slice along dim, concat along dim, but you now ignore dim completely and set it to some unrepresentable value, since cat on scalars in generally not supported).\n3. Ok, that's weird. I have no idea why it did those things, but they don't feel right (this Variable probably wasn't even attached to the computational graph).", "body": "1 & 4. I can't see how is this relevant. I'm not saying you're not supposed to calculate losses on multiple GPUs in parallel. I mean that once you do it, you'd be much better off with returning a Python list of PyTorch scalars, and starting `torch.autograd.backward` from all of them in parallel. If you use DataParallel as it is written today it will unnecessarily gather them to a single device, just to let you call `.backward()` on their mean.\r\n2. Not sure what you mean by \"reduce along the batch dimension\", but you've got a point in here. It will depend on the number of GPUs, which generally doesn't fit the DataParallel invariant, and breaks the interface (slice along `dim`, concat along `dim`, but you now ignore `dim` completely and set it to some unrepresentable value, since `cat` on scalars in generally not supported).\r\n3. Ok, that's weird. I have no idea why it did those things, but they don't feel right (this Variable probably wasn't even attached to the computational graph)."}