{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/394780338", "html_url": "https://github.com/pytorch/pytorch/pull/7973#issuecomment-394780338", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7973", "id": 394780338, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NDc4MDMzOA==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-05T16:46:16Z", "updated_at": "2018-06-05T16:46:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>1 &amp; 4. There can be expensive loss functions, e.g., the recently merged adaptive softmax, so I think this is reasonable.<br>\n2. No, the output shape will depend on number of GPUs, if you reduce along the batch dimension.<br>\n3. I meant returning something like <code>x.mean()</code> rather than a Python scalar. In 0.3.1, the same pattern returns a vector:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.<span class=\"pl-c1\">__version__</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'</span>0.3.1.post2<span class=\"pl-pds\">'</span></span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">class</span> <span class=\"pl-en\">Foo</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n<span class=\"pl-c1\">...</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n<span class=\"pl-c1\">...</span>         <span class=\"pl-k\">return</span> x.mean()\n<span class=\"pl-c1\">...</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> foo <span class=\"pl-k\">=</span> torch.nn.DataParallel(Foo(),[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>]).cuda()\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.autograd.Variable(torch.randn(<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>))\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> foo(x)\n\nVariable containing:\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1141</span>\n<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.0785</span>\n[torch.cuda.FloatTensor of size <span class=\"pl-c1\">2</span> (<span class=\"pl-c1\">GPU</span> <span class=\"pl-c1\">0</span>)]</pre></div>", "body_text": "1 & 4. There can be expensive loss functions, e.g., the recently merged adaptive softmax, so I think this is reasonable.\n2. No, the output shape will depend on number of GPUs, if you reduce along the batch dimension.\n3. I meant returning something like x.mean() rather than a Python scalar. In 0.3.1, the same pattern returns a vector:\n>>> import torch\n>>> torch.__version__\n'0.3.1.post2'\n>>> class Foo(torch.nn.Module):\n...     def forward(self, x):\n...         return x.mean()\n...\n>>> foo = torch.nn.DataParallel(Foo(),[0,1]).cuda()\n>>> x = torch.autograd.Variable(torch.randn(2,2))\n>>> foo(x)\n\nVariable containing:\n-0.1141\n-0.0785\n[torch.cuda.FloatTensor of size 2 (GPU 0)]", "body": "1 & 4. There can be expensive loss functions, e.g., the recently merged adaptive softmax, so I think this is reasonable.\r\n2. No, the output shape will depend on number of GPUs, if you reduce along the batch dimension.\r\n3. I meant returning something like `x.mean()` rather than a Python scalar. In 0.3.1, the same pattern returns a vector:\r\n```py\r\n>>> import torch\r\n>>> torch.__version__\r\n'0.3.1.post2'\r\n>>> class Foo(torch.nn.Module):\r\n...     def forward(self, x):\r\n...         return x.mean()\r\n...\r\n>>> foo = torch.nn.DataParallel(Foo(),[0,1]).cuda()\r\n>>> x = torch.autograd.Variable(torch.randn(2,2))\r\n>>> foo(x)\r\n\r\nVariable containing:\r\n-0.1141\r\n-0.0785\r\n[torch.cuda.FloatTensor of size 2 (GPU 0)]\r\n```"}