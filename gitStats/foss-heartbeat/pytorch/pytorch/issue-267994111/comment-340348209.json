{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340348209", "html_url": "https://github.com/pytorch/pytorch/issues/3257#issuecomment-340348209", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3257", "id": 340348209, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDM0ODIwOQ==", "user": {"login": "imai-lm", "id": 31943828, "node_id": "MDQ6VXNlcjMxOTQzODI4", "avatar_url": "https://avatars0.githubusercontent.com/u/31943828?v=4", "gravatar_id": "", "url": "https://api.github.com/users/imai-lm", "html_url": "https://github.com/imai-lm", "followers_url": "https://api.github.com/users/imai-lm/followers", "following_url": "https://api.github.com/users/imai-lm/following{/other_user}", "gists_url": "https://api.github.com/users/imai-lm/gists{/gist_id}", "starred_url": "https://api.github.com/users/imai-lm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/imai-lm/subscriptions", "organizations_url": "https://api.github.com/users/imai-lm/orgs", "repos_url": "https://api.github.com/users/imai-lm/repos", "events_url": "https://api.github.com/users/imai-lm/events{/privacy}", "received_events_url": "https://api.github.com/users/imai-lm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-30T05:36:20Z", "updated_at": "2017-10-30T05:36:20Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> yes, my exported ONNX model had Expand ops in it, and there was no error. I tried it under different environments (Ubuntu, and macOS) with reinstallation and got the same result. What I tried was the following:</p>\n<pre><code>from torch.autograd import Variable\nimport torch.onnx\nimport torchvision\n\n\ndummy_input = Variable(torch.randn(10, 3, 224, 224))\nmodel = torchvision.models.alexnet(pretrained=True)\ntorch.onnx.export(model, dummy_input, \"alexnet.proto\", verbose=True)\n</code></pre>\n<p>The result model was:</p>\n<pre><code>graph(%1 : Float(10, 3, 224, 224)\n      %2 : Float(64, 3, 11, 11)\n      %3 : Float(64)\n      %4 : Float(192, 64, 5, 5)\n      %5 : Float(192)\n      %6 : Float(384, 192, 3, 3)\n      %7 : Float(384)\n      %8 : Float(256, 384, 3, 3)\n      %9 : Float(256)\n      %10 : Float(256, 256, 3, 3)\n      %11 : Float(256)\n      %12 : Float(4096, 9216)\n      %13 : Float(4096)\n      %14 : Float(4096, 4096)\n      %15 : Float(4096)\n      %16 : Float(1000, 4096)\n      %17 : Float(1000)) {\n  %19 : UNKNOWN_TYPE = Conv[kernel_shape=[11, 11], strides=[4, 4], pads=[2, 2, 2, 2], dilations=[1, 1], group=1](%1, %2), uses = [[%20.i0]];\n  %20 : Float(10, 64, 55, 55) = Add[broadcast=1, axis=1](%19, %3), uses = [%21.i0];\n  %21 : Float(10, 64, 55, 55) = Relu(%20), uses = [%22.i0];\n  %22 : Float(10, 64, 27, 27) = MaxPool[dilations=[1, 1], kernel_shape=[3, 3], pads=[0, 0], strides=[2, 2]](%21), uses = [%23.i0];\n  %24 : UNKNOWN_TYPE = Conv[kernel_shape=[5, 5], strides=[1, 1], pads=[2, 2, 2, 2], dilations=[1, 1], group=1](%22, %4), uses = [[%25.i0]];\n  %25 : Float(10, 192, 27, 27) = Add[broadcast=1, axis=1](%24, %5), uses = [%26.i0];\n  %26 : Float(10, 192, 27, 27) = Relu(%25), uses = [%27.i0];\n  %27 : Float(10, 192, 13, 13) = MaxPool[dilations=[1, 1], kernel_shape=[3, 3], pads=[0, 0], strides=[2, 2]](%26), uses = [%28.i0];\n  %29 : UNKNOWN_TYPE = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%27, %6), uses = [[%30.i0]];\n  %30 : Float(10, 384, 13, 13) = Add[broadcast=1, axis=1](%29, %7), uses = [%31.i0];\n  %31 : Float(10, 384, 13, 13) = Relu(%30), uses = [%32.i0];\n  %33 : UNKNOWN_TYPE = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%31, %8), uses = [[%34.i0]];\n  %34 : Float(10, 256, 13, 13) = Add[broadcast=1, axis=1](%33, %9), uses = [%35.i0];\n  %35 : Float(10, 256, 13, 13) = Relu(%34), uses = [%36.i0];\n  %37 : UNKNOWN_TYPE = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%35, %10), uses = [[%38.i0]];\n  %38 : Float(10, 256, 13, 13) = Add[broadcast=1, axis=1](%37, %11), uses = [%39.i0];\n  %39 : Float(10, 256, 13, 13) = Relu(%38), uses = [%40.i0];\n  %40 : Float(10, 256, 6, 6) = MaxPool[dilations=[1, 1], kernel_shape=[3, 3], pads=[0, 0], strides=[2, 2]](%39), uses = [%41.i0];\n  %41 : Float(10, 9216) = Reshape[shape=[10, 9216]](%40), uses = [%42.i0];\n  %43 : Float(10, 9216), %44 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%41), uses = [[%47.i0], []];\n  %45 : Float(9216!, 4096!) = Transpose[perm=[1, 0]](%12), uses = [%47.i1];\n  %46 : Float(10!, 4096) = Expand[shape=[10, 4096]](%13), uses = [%47.i2];\n  %47 : Float(10, 4096) = Gemm[alpha=1, beta=1](%43, %45, %46), uses = [%48.i0];\n  %48 : Float(10, 4096) = Relu(%47), uses = [%49.i0];\n  %50 : Float(10, 4096), %51 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%48), uses = [[%54.i0], []];\n  %52 : Float(4096!, 4096!) = Transpose[perm=[1, 0]](%14), uses = [%54.i1];\n  %53 : Float(10!, 4096) = Expand[shape=[10, 4096]](%15), uses = [%54.i2];\n  %54 : Float(10, 4096) = Gemm[alpha=1, beta=1](%50, %52, %53), uses = [%55.i0];\n  %55 : Float(10, 4096) = Relu(%54), uses = [%58.i0];\n  %56 : Float(4096!, 1000!) = Transpose[perm=[1, 0]](%16), uses = [%58.i1];\n  %57 : Float(10!, 1000) = Expand[shape=[10, 1000]](%17), uses = [%58.i2];\n  %58 : Float(10, 1000) = Gemm[alpha=1, beta=1](%55, %56, %57), uses = [%0.i0];\n  return (%58);\n}\n</code></pre>", "body_text": "@ezyang yes, my exported ONNX model had Expand ops in it, and there was no error. I tried it under different environments (Ubuntu, and macOS) with reinstallation and got the same result. What I tried was the following:\nfrom torch.autograd import Variable\nimport torch.onnx\nimport torchvision\n\n\ndummy_input = Variable(torch.randn(10, 3, 224, 224))\nmodel = torchvision.models.alexnet(pretrained=True)\ntorch.onnx.export(model, dummy_input, \"alexnet.proto\", verbose=True)\n\nThe result model was:\ngraph(%1 : Float(10, 3, 224, 224)\n      %2 : Float(64, 3, 11, 11)\n      %3 : Float(64)\n      %4 : Float(192, 64, 5, 5)\n      %5 : Float(192)\n      %6 : Float(384, 192, 3, 3)\n      %7 : Float(384)\n      %8 : Float(256, 384, 3, 3)\n      %9 : Float(256)\n      %10 : Float(256, 256, 3, 3)\n      %11 : Float(256)\n      %12 : Float(4096, 9216)\n      %13 : Float(4096)\n      %14 : Float(4096, 4096)\n      %15 : Float(4096)\n      %16 : Float(1000, 4096)\n      %17 : Float(1000)) {\n  %19 : UNKNOWN_TYPE = Conv[kernel_shape=[11, 11], strides=[4, 4], pads=[2, 2, 2, 2], dilations=[1, 1], group=1](%1, %2), uses = [[%20.i0]];\n  %20 : Float(10, 64, 55, 55) = Add[broadcast=1, axis=1](%19, %3), uses = [%21.i0];\n  %21 : Float(10, 64, 55, 55) = Relu(%20), uses = [%22.i0];\n  %22 : Float(10, 64, 27, 27) = MaxPool[dilations=[1, 1], kernel_shape=[3, 3], pads=[0, 0], strides=[2, 2]](%21), uses = [%23.i0];\n  %24 : UNKNOWN_TYPE = Conv[kernel_shape=[5, 5], strides=[1, 1], pads=[2, 2, 2, 2], dilations=[1, 1], group=1](%22, %4), uses = [[%25.i0]];\n  %25 : Float(10, 192, 27, 27) = Add[broadcast=1, axis=1](%24, %5), uses = [%26.i0];\n  %26 : Float(10, 192, 27, 27) = Relu(%25), uses = [%27.i0];\n  %27 : Float(10, 192, 13, 13) = MaxPool[dilations=[1, 1], kernel_shape=[3, 3], pads=[0, 0], strides=[2, 2]](%26), uses = [%28.i0];\n  %29 : UNKNOWN_TYPE = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%27, %6), uses = [[%30.i0]];\n  %30 : Float(10, 384, 13, 13) = Add[broadcast=1, axis=1](%29, %7), uses = [%31.i0];\n  %31 : Float(10, 384, 13, 13) = Relu(%30), uses = [%32.i0];\n  %33 : UNKNOWN_TYPE = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%31, %8), uses = [[%34.i0]];\n  %34 : Float(10, 256, 13, 13) = Add[broadcast=1, axis=1](%33, %9), uses = [%35.i0];\n  %35 : Float(10, 256, 13, 13) = Relu(%34), uses = [%36.i0];\n  %37 : UNKNOWN_TYPE = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%35, %10), uses = [[%38.i0]];\n  %38 : Float(10, 256, 13, 13) = Add[broadcast=1, axis=1](%37, %11), uses = [%39.i0];\n  %39 : Float(10, 256, 13, 13) = Relu(%38), uses = [%40.i0];\n  %40 : Float(10, 256, 6, 6) = MaxPool[dilations=[1, 1], kernel_shape=[3, 3], pads=[0, 0], strides=[2, 2]](%39), uses = [%41.i0];\n  %41 : Float(10, 9216) = Reshape[shape=[10, 9216]](%40), uses = [%42.i0];\n  %43 : Float(10, 9216), %44 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%41), uses = [[%47.i0], []];\n  %45 : Float(9216!, 4096!) = Transpose[perm=[1, 0]](%12), uses = [%47.i1];\n  %46 : Float(10!, 4096) = Expand[shape=[10, 4096]](%13), uses = [%47.i2];\n  %47 : Float(10, 4096) = Gemm[alpha=1, beta=1](%43, %45, %46), uses = [%48.i0];\n  %48 : Float(10, 4096) = Relu(%47), uses = [%49.i0];\n  %50 : Float(10, 4096), %51 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%48), uses = [[%54.i0], []];\n  %52 : Float(4096!, 4096!) = Transpose[perm=[1, 0]](%14), uses = [%54.i1];\n  %53 : Float(10!, 4096) = Expand[shape=[10, 4096]](%15), uses = [%54.i2];\n  %54 : Float(10, 4096) = Gemm[alpha=1, beta=1](%50, %52, %53), uses = [%55.i0];\n  %55 : Float(10, 4096) = Relu(%54), uses = [%58.i0];\n  %56 : Float(4096!, 1000!) = Transpose[perm=[1, 0]](%16), uses = [%58.i1];\n  %57 : Float(10!, 1000) = Expand[shape=[10, 1000]](%17), uses = [%58.i2];\n  %58 : Float(10, 1000) = Gemm[alpha=1, beta=1](%55, %56, %57), uses = [%0.i0];\n  return (%58);\n}", "body": "@ezyang yes, my exported ONNX model had Expand ops in it, and there was no error. I tried it under different environments (Ubuntu, and macOS) with reinstallation and got the same result. What I tried was the following:\r\n```\r\nfrom torch.autograd import Variable\r\nimport torch.onnx\r\nimport torchvision\r\n\r\n\r\ndummy_input = Variable(torch.randn(10, 3, 224, 224))\r\nmodel = torchvision.models.alexnet(pretrained=True)\r\ntorch.onnx.export(model, dummy_input, \"alexnet.proto\", verbose=True)\r\n```\r\nThe result model was:\r\n```\r\ngraph(%1 : Float(10, 3, 224, 224)\r\n      %2 : Float(64, 3, 11, 11)\r\n      %3 : Float(64)\r\n      %4 : Float(192, 64, 5, 5)\r\n      %5 : Float(192)\r\n      %6 : Float(384, 192, 3, 3)\r\n      %7 : Float(384)\r\n      %8 : Float(256, 384, 3, 3)\r\n      %9 : Float(256)\r\n      %10 : Float(256, 256, 3, 3)\r\n      %11 : Float(256)\r\n      %12 : Float(4096, 9216)\r\n      %13 : Float(4096)\r\n      %14 : Float(4096, 4096)\r\n      %15 : Float(4096)\r\n      %16 : Float(1000, 4096)\r\n      %17 : Float(1000)) {\r\n  %19 : UNKNOWN_TYPE = Conv[kernel_shape=[11, 11], strides=[4, 4], pads=[2, 2, 2, 2], dilations=[1, 1], group=1](%1, %2), uses = [[%20.i0]];\r\n  %20 : Float(10, 64, 55, 55) = Add[broadcast=1, axis=1](%19, %3), uses = [%21.i0];\r\n  %21 : Float(10, 64, 55, 55) = Relu(%20), uses = [%22.i0];\r\n  %22 : Float(10, 64, 27, 27) = MaxPool[dilations=[1, 1], kernel_shape=[3, 3], pads=[0, 0], strides=[2, 2]](%21), uses = [%23.i0];\r\n  %24 : UNKNOWN_TYPE = Conv[kernel_shape=[5, 5], strides=[1, 1], pads=[2, 2, 2, 2], dilations=[1, 1], group=1](%22, %4), uses = [[%25.i0]];\r\n  %25 : Float(10, 192, 27, 27) = Add[broadcast=1, axis=1](%24, %5), uses = [%26.i0];\r\n  %26 : Float(10, 192, 27, 27) = Relu(%25), uses = [%27.i0];\r\n  %27 : Float(10, 192, 13, 13) = MaxPool[dilations=[1, 1], kernel_shape=[3, 3], pads=[0, 0], strides=[2, 2]](%26), uses = [%28.i0];\r\n  %29 : UNKNOWN_TYPE = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%27, %6), uses = [[%30.i0]];\r\n  %30 : Float(10, 384, 13, 13) = Add[broadcast=1, axis=1](%29, %7), uses = [%31.i0];\r\n  %31 : Float(10, 384, 13, 13) = Relu(%30), uses = [%32.i0];\r\n  %33 : UNKNOWN_TYPE = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%31, %8), uses = [[%34.i0]];\r\n  %34 : Float(10, 256, 13, 13) = Add[broadcast=1, axis=1](%33, %9), uses = [%35.i0];\r\n  %35 : Float(10, 256, 13, 13) = Relu(%34), uses = [%36.i0];\r\n  %37 : UNKNOWN_TYPE = Conv[kernel_shape=[3, 3], strides=[1, 1], pads=[1, 1, 1, 1], dilations=[1, 1], group=1](%35, %10), uses = [[%38.i0]];\r\n  %38 : Float(10, 256, 13, 13) = Add[broadcast=1, axis=1](%37, %11), uses = [%39.i0];\r\n  %39 : Float(10, 256, 13, 13) = Relu(%38), uses = [%40.i0];\r\n  %40 : Float(10, 256, 6, 6) = MaxPool[dilations=[1, 1], kernel_shape=[3, 3], pads=[0, 0], strides=[2, 2]](%39), uses = [%41.i0];\r\n  %41 : Float(10, 9216) = Reshape[shape=[10, 9216]](%40), uses = [%42.i0];\r\n  %43 : Float(10, 9216), %44 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%41), uses = [[%47.i0], []];\r\n  %45 : Float(9216!, 4096!) = Transpose[perm=[1, 0]](%12), uses = [%47.i1];\r\n  %46 : Float(10!, 4096) = Expand[shape=[10, 4096]](%13), uses = [%47.i2];\r\n  %47 : Float(10, 4096) = Gemm[alpha=1, beta=1](%43, %45, %46), uses = [%48.i0];\r\n  %48 : Float(10, 4096) = Relu(%47), uses = [%49.i0];\r\n  %50 : Float(10, 4096), %51 : UNKNOWN_TYPE = Dropout[is_test=1, ratio=0.5](%48), uses = [[%54.i0], []];\r\n  %52 : Float(4096!, 4096!) = Transpose[perm=[1, 0]](%14), uses = [%54.i1];\r\n  %53 : Float(10!, 4096) = Expand[shape=[10, 4096]](%15), uses = [%54.i2];\r\n  %54 : Float(10, 4096) = Gemm[alpha=1, beta=1](%50, %52, %53), uses = [%55.i0];\r\n  %55 : Float(10, 4096) = Relu(%54), uses = [%58.i0];\r\n  %56 : Float(4096!, 1000!) = Transpose[perm=[1, 0]](%16), uses = [%58.i1];\r\n  %57 : Float(10!, 1000) = Expand[shape=[10, 1000]](%17), uses = [%58.i2];\r\n  %58 : Float(10, 1000) = Gemm[alpha=1, beta=1](%55, %56, %57), uses = [%0.i0];\r\n  return (%58);\r\n}\r\n```\r\n"}