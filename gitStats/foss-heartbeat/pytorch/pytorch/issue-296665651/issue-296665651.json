{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5210", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5210/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5210/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5210/events", "html_url": "https://github.com/pytorch/pytorch/issues/5210", "id": 296665651, "node_id": "MDU6SXNzdWUyOTY2NjU2NTE=", "number": 5210, "title": "RuntimeError: cuda runtime error (2) : out of memory when using loss function", "user": {"login": "ahmed-shariff", "id": 15046660, "node_id": "MDQ6VXNlcjE1MDQ2NjYw", "avatar_url": "https://avatars1.githubusercontent.com/u/15046660?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahmed-shariff", "html_url": "https://github.com/ahmed-shariff", "followers_url": "https://api.github.com/users/ahmed-shariff/followers", "following_url": "https://api.github.com/users/ahmed-shariff/following{/other_user}", "gists_url": "https://api.github.com/users/ahmed-shariff/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahmed-shariff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahmed-shariff/subscriptions", "organizations_url": "https://api.github.com/users/ahmed-shariff/orgs", "repos_url": "https://api.github.com/users/ahmed-shariff/repos", "events_url": "https://api.github.com/users/ahmed-shariff/events{/privacy}", "received_events_url": "https://api.github.com/users/ahmed-shariff/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-02-13T09:23:24Z", "updated_at": "2018-02-21T06:56:14Z", "closed_at": "2018-02-14T17:37:56Z", "author_association": "NONE", "body_html": "<p>When I am trying to run my model, during the evaluation loop, when I call the loss function, it results in a <code>out of memory error</code>. If I comment out the loss in the evaluation loop, I don't get this problem. Note that the loss function in the training loop does not cause this problem.</p>\n<p>The code of the model I am running:</p>\n<pre><code>class TestingModel(Model):\n    def __init__(self, versions, **args):\n        super().__init__(versions, **args)\n        self.model = ResNet(Bottleneck, [2,3,4,4], 4)\n        self.topk_k = 3\n        self.logging_iteration = 10\n        self.criterion = torch.nn.CrossEntropyLoss()\n        if torch.cuda.is_available() and use_cuda:\n            self.criterion.cuda()\n            self.model.cuda()\n            \n    def train_model(self, input_fn, steps):\n        #criterion = torch.nn.CrossEntropyLoss()\n        top1 = matricContainer()\n        topk = matricContainer()\n        loss_average = matricContainer()\n        #if use_cuda:\n        #    criterion.cuda()\n        optimizer = torch.optim.Adam(self.model.parameters())\n        epocs = math.ceil(steps/self.current_version[version_parameters.EPOC_COUNT])\n        datasize = self.current_version[version_parameters.DATALOADER].get_train_sample_count()\n        epocs = 1\n        for epoc in range(epocs):\n            for idx, i in enumerate(input_fn):\n                #print(idx)\n                if idx &gt; 1:\n                    break\n                if use_cuda:\n                    input_var = torch.autograd.Variable(i[0].cuda())\n                    label_var = torch.autograd.Variable(i[1].cuda())\n                else:\n                    input_var = torch.autograd.Variable(i[0])\n                    label_var = torch.autograd.Variable(i[1])\n                    pass\n                out = self.model(input_var)\n                loss = self.criterion(out, label_var)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                t1, tk, total = accuracy(out, label_var, self.topk_k)\n                top1.update(t1, total)\n                topk.update(tk, total)\n                loss_average.update(loss, 1)\n            \n                if idx % self.logging_iteration == 0:\n                    self.log(\"Step: {}   Epoc: {}\".format(epoc*datasize + idx, epoc + 1))\n                    self.log(\"Top1: {:.4f}  Top{}: {:.4f}  Loss: {:.4f}\".format(\n                        top1.avg(),\n                        self.topk_k,\n                        topk.avg(),\n                        loss_average.avg().data[0]))\n\n    def log(self, message,log_to_file=False, **kargs):\n        log(\"{}Model- {}{}\".format(console_colors.CYAN_FG,\n                                   console_colors.RESET,\n                                   message),\n            log=log_to_file, **kargs)\n\n      \n    def evaluate_model(self, input_fn, steps):\n        top1 = matricContainer()\n        topk = matricContainer()\n        loss_average = matricContainer()\n        #print(self.model.state_dict())\n        for idx, i in enumerate(input_fn):\n            #print(idx)\n            if idx &gt; 50:\n                break\n            if use_cuda:\n                input_var = torch.autograd.Variable(i[0].cuda())\n                label_var = torch.autograd.Variable(i[1].cuda())\n            else:\n                input_var = torch.autograd.Variable(i[0])\n                label_var = torch.autograd.Variable(i[1])\n                pass\n            out = self.model(input_var)\n            print(\"*\"*20)\n            \n            #LINE CAUSING THE PROBLEM\n            #The error does not happen when I comment out this line\n            loss = self.criterion(out, label_var)\n            \n            t1, tk, total = accuracy(out, label_var, self.topk_k)\n            top1.update(t1, total)\n            self.model.zero_grad()\n            topk.update(tk, total)\n            loss_average.update(loss, 1)\n            #print()rint(idx % self.logging_iteration)\n        self.log(\"Step: {}\".format(idx))\n        self.log(\"Top1: {:.4f}  Top{}: {:.4f}  Loss: {:.4f}\".format(\n            top1.avg(),\n            self.topk_k,\n            topk.avg(),\n            loss_average.avg().data[0]))\n</code></pre>\n<p>The <code>input_fn</code> pass above (in both places) is the return value of the following:</p>\n<pre><code> def get_train_input_fn(self, mode= ModeKeys.TRAIN, **kargs):\n        _size = 440\n        self.log(\"batch size: {}\".format(self.batch_size))\n        if self.train_files is None or self.train_labels is None:\n            self.set_classes(self.use_all_classes, self.classes_count)\n        pre = [RandomCrop(450), transforms.ToPILImage(), transforms.Resize((256,256))]\n        post = [transforms.ToTensor()]\n\n        dataset = parent_dataset(self.train_files,\n                                 self.train_labels,\n                                 transforms.Compose(pre + post))\n        dl = torch.utils.data.DataLoader(dataset, self.batch_size, True)\n        \n        return dl\n</code></pre>\n<p>The model:</p>\n<pre><code>class Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(8, stride=8)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n        self.sigmoid = nn.Sigmoid()\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = self.sigmoid(x)\n\n        return x\n</code></pre>\n<p>Error output:</p>\n<pre><code>2018-02-13 14:28:06:INFO:mlp- Model- Step: 0   Epoc: 1\n2018-02-13 14:28:06:INFO:mlp- Model- Top1: 0.2000  Top3: 0.4000  Loss: 1.7698\n2018-02-13 14:28:06:INFO:mlp- Model traning output: None\n2018-02-13 14:28:06:INFO:mlp- Model trained\n2018-02-13 14:28:06:INFO:mlp- Training evaluation started: 1 steps\n2018-02-13 14:28:06:INFO:mlp- DataLoader- batch size: 5\n********************\n********************\n********************\n********************\n********************\nTHCudaCheck FAIL file=/home/amsha/builds/pytorch/aten/src/THC/generic/THCStorage.cu line=58 error=2 : out of memory\nTraceback (most recent call last):\n  File \"pipeline.py\", line 598, in &lt;module&gt;\n    main(parser.parse_args())\n  File \"pipeline.py\", line 591, in main\n    _main()\n  File \"pipeline.py\", line 170, in _main\n    steps = train_eval_steps)\n  File \"/home/amsha/Research/FoodClassification/models/test_torch.py\", line 244, in evaluate_model\n    out = self.model(input_var)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/amsha/Research/FoodClassification/models/test_torch.py\", line 137, in forward\n    x = self.layer1(x)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 81, in forward\n    input = module(input)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/amsha/Research/FoodClassification/models/test_torch.py\", line 75, in forward\n    out = self.conv3(out)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 292, in forward\n    self.padding, self.dilation, self.groups)\nRuntimeError: cuda runtime error (2) : out of memory at /home/amsha/builds/pytorch/aten/src/THC/generic/THCStorage.cu:58\n</code></pre>\n<p>OS: Arch Linux<br>\nGPU: GTX 1050<br>\nCUDA: 8.0.61-3 &amp; 9.1.85-1<br>\ncudnn: 7.0.5-2<br>\n(cudnn and cuda installed from arch packages)<br>\npytorch: built from source ( <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/b608ea9178eeb32b66a3480aad16d439ba039bdc/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/b608ea9178eeb32b66a3480aad16d439ba039bdc\"><tt>b608ea9</tt></a> )<br>\nwithout conda using gcc 4.8</p>", "body_text": "When I am trying to run my model, during the evaluation loop, when I call the loss function, it results in a out of memory error. If I comment out the loss in the evaluation loop, I don't get this problem. Note that the loss function in the training loop does not cause this problem.\nThe code of the model I am running:\nclass TestingModel(Model):\n    def __init__(self, versions, **args):\n        super().__init__(versions, **args)\n        self.model = ResNet(Bottleneck, [2,3,4,4], 4)\n        self.topk_k = 3\n        self.logging_iteration = 10\n        self.criterion = torch.nn.CrossEntropyLoss()\n        if torch.cuda.is_available() and use_cuda:\n            self.criterion.cuda()\n            self.model.cuda()\n            \n    def train_model(self, input_fn, steps):\n        #criterion = torch.nn.CrossEntropyLoss()\n        top1 = matricContainer()\n        topk = matricContainer()\n        loss_average = matricContainer()\n        #if use_cuda:\n        #    criterion.cuda()\n        optimizer = torch.optim.Adam(self.model.parameters())\n        epocs = math.ceil(steps/self.current_version[version_parameters.EPOC_COUNT])\n        datasize = self.current_version[version_parameters.DATALOADER].get_train_sample_count()\n        epocs = 1\n        for epoc in range(epocs):\n            for idx, i in enumerate(input_fn):\n                #print(idx)\n                if idx > 1:\n                    break\n                if use_cuda:\n                    input_var = torch.autograd.Variable(i[0].cuda())\n                    label_var = torch.autograd.Variable(i[1].cuda())\n                else:\n                    input_var = torch.autograd.Variable(i[0])\n                    label_var = torch.autograd.Variable(i[1])\n                    pass\n                out = self.model(input_var)\n                loss = self.criterion(out, label_var)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                t1, tk, total = accuracy(out, label_var, self.topk_k)\n                top1.update(t1, total)\n                topk.update(tk, total)\n                loss_average.update(loss, 1)\n            \n                if idx % self.logging_iteration == 0:\n                    self.log(\"Step: {}   Epoc: {}\".format(epoc*datasize + idx, epoc + 1))\n                    self.log(\"Top1: {:.4f}  Top{}: {:.4f}  Loss: {:.4f}\".format(\n                        top1.avg(),\n                        self.topk_k,\n                        topk.avg(),\n                        loss_average.avg().data[0]))\n\n    def log(self, message,log_to_file=False, **kargs):\n        log(\"{}Model- {}{}\".format(console_colors.CYAN_FG,\n                                   console_colors.RESET,\n                                   message),\n            log=log_to_file, **kargs)\n\n      \n    def evaluate_model(self, input_fn, steps):\n        top1 = matricContainer()\n        topk = matricContainer()\n        loss_average = matricContainer()\n        #print(self.model.state_dict())\n        for idx, i in enumerate(input_fn):\n            #print(idx)\n            if idx > 50:\n                break\n            if use_cuda:\n                input_var = torch.autograd.Variable(i[0].cuda())\n                label_var = torch.autograd.Variable(i[1].cuda())\n            else:\n                input_var = torch.autograd.Variable(i[0])\n                label_var = torch.autograd.Variable(i[1])\n                pass\n            out = self.model(input_var)\n            print(\"*\"*20)\n            \n            #LINE CAUSING THE PROBLEM\n            #The error does not happen when I comment out this line\n            loss = self.criterion(out, label_var)\n            \n            t1, tk, total = accuracy(out, label_var, self.topk_k)\n            top1.update(t1, total)\n            self.model.zero_grad()\n            topk.update(tk, total)\n            loss_average.update(loss, 1)\n            #print()rint(idx % self.logging_iteration)\n        self.log(\"Step: {}\".format(idx))\n        self.log(\"Top1: {:.4f}  Top{}: {:.4f}  Loss: {:.4f}\".format(\n            top1.avg(),\n            self.topk_k,\n            topk.avg(),\n            loss_average.avg().data[0]))\n\nThe input_fn pass above (in both places) is the return value of the following:\n def get_train_input_fn(self, mode= ModeKeys.TRAIN, **kargs):\n        _size = 440\n        self.log(\"batch size: {}\".format(self.batch_size))\n        if self.train_files is None or self.train_labels is None:\n            self.set_classes(self.use_all_classes, self.classes_count)\n        pre = [RandomCrop(450), transforms.ToPILImage(), transforms.Resize((256,256))]\n        post = [transforms.ToTensor()]\n\n        dataset = parent_dataset(self.train_files,\n                                 self.train_labels,\n                                 transforms.Compose(pre + post))\n        dl = torch.utils.data.DataLoader(dataset, self.batch_size, True)\n        \n        return dl\n\nThe model:\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(8, stride=8)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n        self.sigmoid = nn.Sigmoid()\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        x = self.sigmoid(x)\n\n        return x\n\nError output:\n2018-02-13 14:28:06:INFO:mlp- Model- Step: 0   Epoc: 1\n2018-02-13 14:28:06:INFO:mlp- Model- Top1: 0.2000  Top3: 0.4000  Loss: 1.7698\n2018-02-13 14:28:06:INFO:mlp- Model traning output: None\n2018-02-13 14:28:06:INFO:mlp- Model trained\n2018-02-13 14:28:06:INFO:mlp- Training evaluation started: 1 steps\n2018-02-13 14:28:06:INFO:mlp- DataLoader- batch size: 5\n********************\n********************\n********************\n********************\n********************\nTHCudaCheck FAIL file=/home/amsha/builds/pytorch/aten/src/THC/generic/THCStorage.cu line=58 error=2 : out of memory\nTraceback (most recent call last):\n  File \"pipeline.py\", line 598, in <module>\n    main(parser.parse_args())\n  File \"pipeline.py\", line 591, in main\n    _main()\n  File \"pipeline.py\", line 170, in _main\n    steps = train_eval_steps)\n  File \"/home/amsha/Research/FoodClassification/models/test_torch.py\", line 244, in evaluate_model\n    out = self.model(input_var)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/amsha/Research/FoodClassification/models/test_torch.py\", line 137, in forward\n    x = self.layer1(x)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 81, in forward\n    input = module(input)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/amsha/Research/FoodClassification/models/test_torch.py\", line 75, in forward\n    out = self.conv3(out)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 292, in forward\n    self.padding, self.dilation, self.groups)\nRuntimeError: cuda runtime error (2) : out of memory at /home/amsha/builds/pytorch/aten/src/THC/generic/THCStorage.cu:58\n\nOS: Arch Linux\nGPU: GTX 1050\nCUDA: 8.0.61-3 & 9.1.85-1\ncudnn: 7.0.5-2\n(cudnn and cuda installed from arch packages)\npytorch: built from source ( b608ea9 )\nwithout conda using gcc 4.8", "body": "When I am trying to run my model, during the evaluation loop, when I call the loss function, it results in a `out of memory error`. If I comment out the loss in the evaluation loop, I don't get this problem. Note that the loss function in the training loop does not cause this problem.\r\n\r\nThe code of the model I am running:\r\n```\r\nclass TestingModel(Model):\r\n    def __init__(self, versions, **args):\r\n        super().__init__(versions, **args)\r\n        self.model = ResNet(Bottleneck, [2,3,4,4], 4)\r\n        self.topk_k = 3\r\n        self.logging_iteration = 10\r\n        self.criterion = torch.nn.CrossEntropyLoss()\r\n        if torch.cuda.is_available() and use_cuda:\r\n            self.criterion.cuda()\r\n            self.model.cuda()\r\n            \r\n    def train_model(self, input_fn, steps):\r\n        #criterion = torch.nn.CrossEntropyLoss()\r\n        top1 = matricContainer()\r\n        topk = matricContainer()\r\n        loss_average = matricContainer()\r\n        #if use_cuda:\r\n        #    criterion.cuda()\r\n        optimizer = torch.optim.Adam(self.model.parameters())\r\n        epocs = math.ceil(steps/self.current_version[version_parameters.EPOC_COUNT])\r\n        datasize = self.current_version[version_parameters.DATALOADER].get_train_sample_count()\r\n        epocs = 1\r\n        for epoc in range(epocs):\r\n            for idx, i in enumerate(input_fn):\r\n                #print(idx)\r\n                if idx > 1:\r\n                    break\r\n                if use_cuda:\r\n                    input_var = torch.autograd.Variable(i[0].cuda())\r\n                    label_var = torch.autograd.Variable(i[1].cuda())\r\n                else:\r\n                    input_var = torch.autograd.Variable(i[0])\r\n                    label_var = torch.autograd.Variable(i[1])\r\n                    pass\r\n                out = self.model(input_var)\r\n                loss = self.criterion(out, label_var)\r\n                optimizer.zero_grad()\r\n                loss.backward()\r\n                optimizer.step()\r\n                t1, tk, total = accuracy(out, label_var, self.topk_k)\r\n                top1.update(t1, total)\r\n                topk.update(tk, total)\r\n                loss_average.update(loss, 1)\r\n            \r\n                if idx % self.logging_iteration == 0:\r\n                    self.log(\"Step: {}   Epoc: {}\".format(epoc*datasize + idx, epoc + 1))\r\n                    self.log(\"Top1: {:.4f}  Top{}: {:.4f}  Loss: {:.4f}\".format(\r\n                        top1.avg(),\r\n                        self.topk_k,\r\n                        topk.avg(),\r\n                        loss_average.avg().data[0]))\r\n\r\n    def log(self, message,log_to_file=False, **kargs):\r\n        log(\"{}Model- {}{}\".format(console_colors.CYAN_FG,\r\n                                   console_colors.RESET,\r\n                                   message),\r\n            log=log_to_file, **kargs)\r\n\r\n      \r\n    def evaluate_model(self, input_fn, steps):\r\n        top1 = matricContainer()\r\n        topk = matricContainer()\r\n        loss_average = matricContainer()\r\n        #print(self.model.state_dict())\r\n        for idx, i in enumerate(input_fn):\r\n            #print(idx)\r\n            if idx > 50:\r\n                break\r\n            if use_cuda:\r\n                input_var = torch.autograd.Variable(i[0].cuda())\r\n                label_var = torch.autograd.Variable(i[1].cuda())\r\n            else:\r\n                input_var = torch.autograd.Variable(i[0])\r\n                label_var = torch.autograd.Variable(i[1])\r\n                pass\r\n            out = self.model(input_var)\r\n            print(\"*\"*20)\r\n            \r\n            #LINE CAUSING THE PROBLEM\r\n            #The error does not happen when I comment out this line\r\n            loss = self.criterion(out, label_var)\r\n            \r\n            t1, tk, total = accuracy(out, label_var, self.topk_k)\r\n            top1.update(t1, total)\r\n            self.model.zero_grad()\r\n            topk.update(tk, total)\r\n            loss_average.update(loss, 1)\r\n            #print()rint(idx % self.logging_iteration)\r\n        self.log(\"Step: {}\".format(idx))\r\n        self.log(\"Top1: {:.4f}  Top{}: {:.4f}  Loss: {:.4f}\".format(\r\n            top1.avg(),\r\n            self.topk_k,\r\n            topk.avg(),\r\n            loss_average.avg().data[0]))\r\n```\r\nThe `input_fn` pass above (in both places) is the return value of the following:\r\n```\r\n def get_train_input_fn(self, mode= ModeKeys.TRAIN, **kargs):\r\n        _size = 440\r\n        self.log(\"batch size: {}\".format(self.batch_size))\r\n        if self.train_files is None or self.train_labels is None:\r\n            self.set_classes(self.use_all_classes, self.classes_count)\r\n        pre = [RandomCrop(450), transforms.ToPILImage(), transforms.Resize((256,256))]\r\n        post = [transforms.ToTensor()]\r\n\r\n        dataset = parent_dataset(self.train_files,\r\n                                 self.train_labels,\r\n                                 transforms.Compose(pre + post))\r\n        dl = torch.utils.data.DataLoader(dataset, self.batch_size, True)\r\n        \r\n        return dl\r\n```\r\n\r\nThe model:\r\n```\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n\r\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = nn.BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n                               padding=1, bias=False)\r\n        self.bn2 = nn.BatchNorm2d(planes)\r\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n        self.bn3 = nn.BatchNorm2d(planes * 4)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out += residual\r\n        out = self.relu(out)\r\n\r\n        return out\r\n\r\n\r\nclass ResNet(nn.Module):\r\n\r\n    def __init__(self, block, layers, num_classes=1000):\r\n        self.inplanes = 64\r\n        super(ResNet, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\r\n                               bias=False)\r\n        self.bn1 = nn.BatchNorm2d(64)\r\n        self.relu = nn.ReLU(inplace=True)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n        self.layer1 = self._make_layer(block, 64, layers[0])\r\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\r\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\r\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\r\n        self.avgpool = nn.AvgPool2d(8, stride=8)\r\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\r\n        self.sigmoid = nn.Sigmoid()\r\n\r\n        for m in self.modules():\r\n            if isinstance(m, nn.Conv2d):\r\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n            elif isinstance(m, nn.BatchNorm2d):\r\n                m.weight.data.fill_(1)\r\n                m.bias.data.zero_()\r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1):\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                nn.BatchNorm2d(planes * block.expansion),\r\n            )\r\n\r\n        layers = []\r\n        layers.append(block(self.inplanes, planes, stride, downsample))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        \r\n        x = self.conv1(x)\r\n        x = self.bn1(x)\r\n        x = self.relu(x)\r\n        x = self.maxpool(x)\r\n        \r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x = self.layer4(x)\r\n        \r\n        x = self.avgpool(x)\r\n        x = x.view(x.size(0), -1)\r\n        x = self.fc(x)\r\n        x = self.sigmoid(x)\r\n\r\n        return x\r\n```\r\n\r\nError output:\r\n```\r\n2018-02-13 14:28:06:INFO:mlp- Model- Step: 0   Epoc: 1\r\n2018-02-13 14:28:06:INFO:mlp- Model- Top1: 0.2000  Top3: 0.4000  Loss: 1.7698\r\n2018-02-13 14:28:06:INFO:mlp- Model traning output: None\r\n2018-02-13 14:28:06:INFO:mlp- Model trained\r\n2018-02-13 14:28:06:INFO:mlp- Training evaluation started: 1 steps\r\n2018-02-13 14:28:06:INFO:mlp- DataLoader- batch size: 5\r\n********************\r\n********************\r\n********************\r\n********************\r\n********************\r\nTHCudaCheck FAIL file=/home/amsha/builds/pytorch/aten/src/THC/generic/THCStorage.cu line=58 error=2 : out of memory\r\nTraceback (most recent call last):\r\n  File \"pipeline.py\", line 598, in <module>\r\n    main(parser.parse_args())\r\n  File \"pipeline.py\", line 591, in main\r\n    _main()\r\n  File \"pipeline.py\", line 170, in _main\r\n    steps = train_eval_steps)\r\n  File \"/home/amsha/Research/FoodClassification/models/test_torch.py\", line 244, in evaluate_model\r\n    out = self.model(input_var)\r\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/amsha/Research/FoodClassification/models/test_torch.py\", line 137, in forward\r\n    x = self.layer1(x)\r\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 81, in forward\r\n    input = module(input)\r\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/amsha/Research/FoodClassification/models/test_torch.py\", line 75, in forward\r\n    out = self.conv3(out)\r\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 357, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/amsha/virtualenv/torch-master-12022018/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 292, in forward\r\n    self.padding, self.dilation, self.groups)\r\nRuntimeError: cuda runtime error (2) : out of memory at /home/amsha/builds/pytorch/aten/src/THC/generic/THCStorage.cu:58\r\n```\r\n\r\nOS: Arch Linux\r\nGPU: GTX 1050\r\nCUDA: 8.0.61-3 & 9.1.85-1\r\ncudnn: 7.0.5-2\r\n(cudnn and cuda installed from arch packages)\r\npytorch: built from source ( b608ea9178eeb32b66a3480aad16d439ba039bdc ) \r\n              without conda using gcc 4.8\r\n\r\n\r\n"}