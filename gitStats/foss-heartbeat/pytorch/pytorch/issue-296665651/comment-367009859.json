{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367009859", "html_url": "https://github.com/pytorch/pytorch/issues/5210#issuecomment-367009859", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5210", "id": 367009859, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzAwOTg1OQ==", "user": {"login": "ahmed-shariff", "id": 15046660, "node_id": "MDQ6VXNlcjE1MDQ2NjYw", "avatar_url": "https://avatars1.githubusercontent.com/u/15046660?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahmed-shariff", "html_url": "https://github.com/ahmed-shariff", "followers_url": "https://api.github.com/users/ahmed-shariff/followers", "following_url": "https://api.github.com/users/ahmed-shariff/following{/other_user}", "gists_url": "https://api.github.com/users/ahmed-shariff/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahmed-shariff/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahmed-shariff/subscriptions", "organizations_url": "https://api.github.com/users/ahmed-shariff/orgs", "repos_url": "https://api.github.com/users/ahmed-shariff/repos", "events_url": "https://api.github.com/users/ahmed-shariff/events{/privacy}", "received_events_url": "https://api.github.com/users/ahmed-shariff/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-20T15:16:46Z", "updated_at": "2018-02-20T15:16:46Z", "author_association": "NONE", "body_html": "<p>I accidentally found that the problem was not with the loss function. But with the <code>matricContainer</code> I am using. When I am adding the values returned by the <code>self.criterion</code>, I am actually adding <code>torch.autograd.variable.Variable</code>'s instead of just <code>float</code>'s. So when i replace <code>loss_average.update(loss, 1)</code> with <code>loss_average.update(loss.data[0], 1)</code>, the problem gets solved.</p>\n<p>While it fixes the problem, I don't understand <strong>why adding <code>Variable</code>s would cause a memory overflow?</strong></p>", "body_text": "I accidentally found that the problem was not with the loss function. But with the matricContainer I am using. When I am adding the values returned by the self.criterion, I am actually adding torch.autograd.variable.Variable's instead of just float's. So when i replace loss_average.update(loss, 1) with loss_average.update(loss.data[0], 1), the problem gets solved.\nWhile it fixes the problem, I don't understand why adding Variables would cause a memory overflow?", "body": "I accidentally found that the problem was not with the loss function. But with the `matricContainer` I am using. When I am adding the values returned by the `self.criterion`, I am actually adding `torch.autograd.variable.Variable`'s instead of just `float`'s. So when i replace `loss_average.update(loss, 1)` with `loss_average.update(loss.data[0], 1)`, the problem gets solved. \r\n\r\nWhile it fixes the problem, I don't understand **why adding `Variable`s would cause a memory overflow?**"}