{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/305610414", "html_url": "https://github.com/pytorch/pytorch/pull/1691#issuecomment-305610414", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1691", "id": 305610414, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTYxMDQxNA==", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-01T20:26:42Z", "updated_at": "2017-06-01T20:32:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I forgot that there was repro code in the issue... If you use the repro code with sequence length 20 (about the most that will fit in memory on pytorch master), the memory usage goes from 11433 MiB to 6778 MiB and the time for 100 epochs goes from 63.7 s to 49.7 s. Looks like I really need to figure out how to reuse the gradient buffer, since I'm only getting half the savings we need to say the issue is closed.</p>\n<p>(Actually, I'm confused about that...what I currently do is replace the gradient buffers passed to cudnn.rnn.backward_weights with views on the gradients allocated by cuDNN; those should then be added in-place to the gradient accumulation buffers, so we should never have more than two copies of each parameter's gradient?)</p>", "body_text": "I forgot that there was repro code in the issue... If you use the repro code with sequence length 20 (about the most that will fit in memory on pytorch master), the memory usage goes from 11433 MiB to 6778 MiB and the time for 100 epochs goes from 63.7 s to 49.7 s. Looks like I really need to figure out how to reuse the gradient buffer, since I'm only getting half the savings we need to say the issue is closed.\n(Actually, I'm confused about that...what I currently do is replace the gradient buffers passed to cudnn.rnn.backward_weights with views on the gradients allocated by cuDNN; those should then be added in-place to the gradient accumulation buffers, so we should never have more than two copies of each parameter's gradient?)", "body": "I forgot that there was repro code in the issue... If you use the repro code with sequence length 20 (about the most that will fit in memory on pytorch master), the memory usage goes from 11433 MiB to 6778 MiB and the time for 100 epochs goes from 63.7 s to 49.7 s. Looks like I really need to figure out how to reuse the gradient buffer, since I'm only getting half the savings we need to say the issue is closed.\r\n\r\n(Actually, I'm confused about that...what I currently do is replace the gradient buffers passed to cudnn.rnn.backward_weights with views on the gradients allocated by cuDNN; those should then be added in-place to the gradient accumulation buffers, so we should never have more than two copies of each parameter's gradient?)"}