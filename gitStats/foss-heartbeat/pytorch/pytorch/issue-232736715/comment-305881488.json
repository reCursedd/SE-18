{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/305881488", "html_url": "https://github.com/pytorch/pytorch/pull/1691#issuecomment-305881488", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1691", "id": 305881488, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTg4MTQ4OA==", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-02T18:59:26Z", "updated_at": "2017-06-02T18:59:26Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hmm. What do people think of the following?<br>\nWe want to avoid both the extra memory usage (which prevents using cuDNN for unrolled RNNs) and the unnecessary memcpy (which takes up &gt;25% of computation time for unrolled cuDNN RNNs of batch size 64, and a proportionally higher fraction for smaller batch sizes). We also want to support user-provided parameters with arbitrary Storage, Tensor, or Parameter sharing relationships, without breaking those relationships.<br>\nThis requires at least one <code>malloc</code> and one <code>memcpy</code> per RNN, per forward pass of the overall network (because we can't mutate the original parameter objects), and at least one <code>malloc</code> and one <code>memcpy</code> per RNN per backward pass of the network (because we need to copy the accumulated gradients into the right places in the potentially-not-moveable gradient tensors). In particular, we want the first <code>malloc</code>/<code>memcpy</code> to happen the first time an RNN is used during a particular iteration, and we want the second one to happen at the corresponding time during the backward pass. But neither <code>Module</code>s nor <code>Function</code>s are aware of iterations, and what matters isn't actually iterations, it's <em>parameter updates</em>.<br>\nSo it should be possible to make <code>zero_grads</code> flush the coalesced param/grad cache, so that the next time an RNN's forward pass happens it knows to perform the <code>malloc</code>/<code>memcpy</code>, cache the result as a class attribute of the Function, and store in an instance attribute the fact that the gradient copy needs to happen on the backward pass.</p>\n<p>Maybe this interacts in complicated ways with repeated gradient passes? It definitely doesn't need to support double backward because the double-backward gradient codepath for RNNs won't use cuDNN.</p>", "body_text": "Hmm. What do people think of the following?\nWe want to avoid both the extra memory usage (which prevents using cuDNN for unrolled RNNs) and the unnecessary memcpy (which takes up >25% of computation time for unrolled cuDNN RNNs of batch size 64, and a proportionally higher fraction for smaller batch sizes). We also want to support user-provided parameters with arbitrary Storage, Tensor, or Parameter sharing relationships, without breaking those relationships.\nThis requires at least one malloc and one memcpy per RNN, per forward pass of the overall network (because we can't mutate the original parameter objects), and at least one malloc and one memcpy per RNN per backward pass of the network (because we need to copy the accumulated gradients into the right places in the potentially-not-moveable gradient tensors). In particular, we want the first malloc/memcpy to happen the first time an RNN is used during a particular iteration, and we want the second one to happen at the corresponding time during the backward pass. But neither Modules nor Functions are aware of iterations, and what matters isn't actually iterations, it's parameter updates.\nSo it should be possible to make zero_grads flush the coalesced param/grad cache, so that the next time an RNN's forward pass happens it knows to perform the malloc/memcpy, cache the result as a class attribute of the Function, and store in an instance attribute the fact that the gradient copy needs to happen on the backward pass.\nMaybe this interacts in complicated ways with repeated gradient passes? It definitely doesn't need to support double backward because the double-backward gradient codepath for RNNs won't use cuDNN.", "body": "Hmm. What do people think of the following?\r\nWe want to avoid both the extra memory usage (which prevents using cuDNN for unrolled RNNs) and the unnecessary memcpy (which takes up >25% of computation time for unrolled cuDNN RNNs of batch size 64, and a proportionally higher fraction for smaller batch sizes). We also want to support user-provided parameters with arbitrary Storage, Tensor, or Parameter sharing relationships, without breaking those relationships.\r\nThis requires at least one `malloc` and one `memcpy` per RNN, per forward pass of the overall network (because we can't mutate the original parameter objects), and at least one `malloc` and one `memcpy` per RNN per backward pass of the network (because we need to copy the accumulated gradients into the right places in the potentially-not-moveable gradient tensors). In particular, we want the first `malloc`/`memcpy` to happen the first time an RNN is used during a particular iteration, and we want the second one to happen at the corresponding time during the backward pass. But neither `Module`s nor `Function`s are aware of iterations, and what matters isn't actually iterations, it's _parameter updates_.\r\nSo it should be possible to make `zero_grads` flush the coalesced param/grad cache, so that the next time an RNN's forward pass happens it knows to perform the `malloc`/`memcpy`, cache the result as a class attribute of the Function, and store in an instance attribute the fact that the gradient copy needs to happen on the backward pass.\r\n\r\nMaybe this interacts in complicated ways with repeated gradient passes? It definitely doesn't need to support double backward because the double-backward gradient codepath for RNNs won't use cuDNN."}