{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/305622937", "html_url": "https://github.com/pytorch/pytorch/pull/1691#issuecomment-305622937", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1691", "id": 305622937, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNTYyMjkzNw==", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-01T21:16:03Z", "updated_at": "2017-06-01T21:16:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I don't believe it actually breaks any sharing that people can currently rely on. For instance, if you share two of the LSTM parameters with each other (e.g., you share the fwd and bwd weights), two of the tensors in the list of weights will be identical. Then the coalescing will mutate that shared tensor to point to a different storage, and on the next <code>cudnn.rnn.forward</code> call <code>check_coalesced</code> will return <code>False</code> and the parameters will have to be copied (there's no alternative in any case). If you share an LSTM parameter with another parameter elsewhere in the model, the shared tensor will be mutated to point to the relevant view on the cuDNN buffer; this will allow the next call to avoid a parameter copy while maintaining the sharing.</p>", "body_text": "I don't believe it actually breaks any sharing that people can currently rely on. For instance, if you share two of the LSTM parameters with each other (e.g., you share the fwd and bwd weights), two of the tensors in the list of weights will be identical. Then the coalescing will mutate that shared tensor to point to a different storage, and on the next cudnn.rnn.forward call check_coalesced will return False and the parameters will have to be copied (there's no alternative in any case). If you share an LSTM parameter with another parameter elsewhere in the model, the shared tensor will be mutated to point to the relevant view on the cuDNN buffer; this will allow the next call to avoid a parameter copy while maintaining the sharing.", "body": "I don't believe it actually breaks any sharing that people can currently rely on. For instance, if you share two of the LSTM parameters with each other (e.g., you share the fwd and bwd weights), two of the tensors in the list of weights will be identical. Then the coalescing will mutate that shared tensor to point to a different storage, and on the next `cudnn.rnn.forward` call `check_coalesced` will return `False` and the parameters will have to be copied (there's no alternative in any case). If you share an LSTM parameter with another parameter elsewhere in the model, the shared tensor will be mutated to point to the relevant view on the cuDNN buffer; this will allow the next call to avoid a parameter copy while maintaining the sharing."}