{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12463", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12463/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12463/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12463/events", "html_url": "https://github.com/pytorch/pytorch/issues/12463", "id": 367925991, "node_id": "MDU6SXNzdWUzNjc5MjU5OTE=", "number": 12463, "title": "test_sync_params_ in test_c10d.py is broken", "user": {"login": "ygfeng", "id": 43613210, "node_id": "MDQ6VXNlcjQzNjEzMjEw", "avatar_url": "https://avatars1.githubusercontent.com/u/43613210?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ygfeng", "html_url": "https://github.com/ygfeng", "followers_url": "https://api.github.com/users/ygfeng/followers", "following_url": "https://api.github.com/users/ygfeng/following{/other_user}", "gists_url": "https://api.github.com/users/ygfeng/gists{/gist_id}", "starred_url": "https://api.github.com/users/ygfeng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ygfeng/subscriptions", "organizations_url": "https://api.github.com/users/ygfeng/orgs", "repos_url": "https://api.github.com/users/ygfeng/repos", "events_url": "https://api.github.com/users/ygfeng/events{/privacy}", "received_events_url": "https://api.github.com/users/ygfeng/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-10-08T19:39:10Z", "updated_at": "2018-10-08T19:39:10Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>Running unittest for suite c10d, encountering failure on <code>test_sync_params_no_buffers</code> &amp; <code>test_sync_params_with_buffers</code></p>\n<pre><code>python test/run_test.py -i c10d -v\nSelected tests: c10d\nRunning test_c10d ...\ncommand run python test_c10d.py --verbose\ntest_dist_broadcast_coalesced (__main__.DistributedDataParallelTest) ... ok\ntest_fp16 (__main__.DistributedDataParallelTest) ... ok\ntest_gloo_backend (__main__.DistributedDataParallelTest) ... ok\ntest_nccl_backend (__main__.DistributedDataParallelTest) ... ok\ntest_sync_params_no_buffers (__main__.DistributedDataParallelTest) ... Process process 1:\nTraceback (most recent call last):\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n    self.run()\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"test_c10d.py\", line 338, in _run\n    getattr(self, self.id().split(\".\")[2])()\n  File \"test_c10d.py\", line 302, in wrapper\n    fn(self)\n  File \"test_c10d.py\", line 45, in wrapper\n    return func(*args, **kwargs)\n  File \"test_c10d.py\", line 756, in test_sync_params_no_buffers\n    broadcast_buffers=False)\nRuntimeError: all tensors must be on devices[0]\nFAIL\ntest_sync_params_with_buffers (__main__.DistributedDataParallelTest) ... Process process 1:\nTraceback (most recent call last):\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n    self.run()\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"test_c10d.py\", line 338, in _run\n    getattr(self, self.id().split(\".\")[2])()\n  File \"test_c10d.py\", line 302, in wrapper\n    fn(self)\n  File \"test_c10d.py\", line 45, in wrapper\n    return func(*args, **kwargs)\n  File \"test_c10d.py\", line 790, in test_sync_params_with_buffers\n    broadcast_buffers=True)\nRuntimeError: all tensors must be on devices[0]\n\n</code></pre>\n<ul>\n<li>PyTorch Version (e.g., 1.0): 1.0</li>\n<li>OS (e.g., Linux): Linux</li>\n<li>How you installed PyTorch (<code>conda</code>, <code>pip</code>, source): source</li>\n<li>Build command you used (if compiling from source): python setup.py install</li>\n<li>Python version: 2.7</li>\n<li>CUDA/cuDNN version: 10.0</li>\n<li>GPU models and configuration: 4 Tesla v100</li>\n</ul>\n<h2>Additional context</h2>\n<pre><code>def test_sync_params_no_buffers(self):\n...\nparameter_data += [torch.zeros(10, device=torch.device('cuda', d)).chunk(5) for d in devices[1:]]\n...\n        c10d._sync_params(\n            process_group,\n            parameter_data=parameter_data,\n            buffer_data=buffer_data,\n            devices=devices,\n            broadcast_bucket_size=10,\n            broadcast_buffers=True)\n</code></pre>\n<p>Testcase is assigning tensors to all cuda devices in the  parameter_data, it seems the c code is expecting the list of tensors to all reside on gpu0.</p>\n<pre><code> pytorch/torch/csrc/cuda/comm.cpp\ntensor_list2d broadcast_coalesced(TensorList tensors, IntList devices, size_t buffer_size) {\n  if (!std::all_of(tensors.begin(), tensors.end(),\n                   [&amp;](const at::Tensor&amp; t) { return t.get_device() == devices[0]; })) {\n    throw std::runtime_error(\"all tensors must be on devices[0]\");\n}\n</code></pre>", "body_text": "\ud83d\udc1b Bug\nRunning unittest for suite c10d, encountering failure on test_sync_params_no_buffers & test_sync_params_with_buffers\npython test/run_test.py -i c10d -v\nSelected tests: c10d\nRunning test_c10d ...\ncommand run python test_c10d.py --verbose\ntest_dist_broadcast_coalesced (__main__.DistributedDataParallelTest) ... ok\ntest_fp16 (__main__.DistributedDataParallelTest) ... ok\ntest_gloo_backend (__main__.DistributedDataParallelTest) ... ok\ntest_nccl_backend (__main__.DistributedDataParallelTest) ... ok\ntest_sync_params_no_buffers (__main__.DistributedDataParallelTest) ... Process process 1:\nTraceback (most recent call last):\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n    self.run()\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"test_c10d.py\", line 338, in _run\n    getattr(self, self.id().split(\".\")[2])()\n  File \"test_c10d.py\", line 302, in wrapper\n    fn(self)\n  File \"test_c10d.py\", line 45, in wrapper\n    return func(*args, **kwargs)\n  File \"test_c10d.py\", line 756, in test_sync_params_no_buffers\n    broadcast_buffers=False)\nRuntimeError: all tensors must be on devices[0]\nFAIL\ntest_sync_params_with_buffers (__main__.DistributedDataParallelTest) ... Process process 1:\nTraceback (most recent call last):\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n    self.run()\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"test_c10d.py\", line 338, in _run\n    getattr(self, self.id().split(\".\")[2])()\n  File \"test_c10d.py\", line 302, in wrapper\n    fn(self)\n  File \"test_c10d.py\", line 45, in wrapper\n    return func(*args, **kwargs)\n  File \"test_c10d.py\", line 790, in test_sync_params_with_buffers\n    broadcast_buffers=True)\nRuntimeError: all tensors must be on devices[0]\n\n\n\nPyTorch Version (e.g., 1.0): 1.0\nOS (e.g., Linux): Linux\nHow you installed PyTorch (conda, pip, source): source\nBuild command you used (if compiling from source): python setup.py install\nPython version: 2.7\nCUDA/cuDNN version: 10.0\nGPU models and configuration: 4 Tesla v100\n\nAdditional context\ndef test_sync_params_no_buffers(self):\n...\nparameter_data += [torch.zeros(10, device=torch.device('cuda', d)).chunk(5) for d in devices[1:]]\n...\n        c10d._sync_params(\n            process_group,\n            parameter_data=parameter_data,\n            buffer_data=buffer_data,\n            devices=devices,\n            broadcast_bucket_size=10,\n            broadcast_buffers=True)\n\nTestcase is assigning tensors to all cuda devices in the  parameter_data, it seems the c code is expecting the list of tensors to all reside on gpu0.\n pytorch/torch/csrc/cuda/comm.cpp\ntensor_list2d broadcast_coalesced(TensorList tensors, IntList devices, size_t buffer_size) {\n  if (!std::all_of(tensors.begin(), tensors.end(),\n                   [&](const at::Tensor& t) { return t.get_device() == devices[0]; })) {\n    throw std::runtime_error(\"all tensors must be on devices[0]\");\n}", "body": "## \ud83d\udc1b Bug\r\nRunning unittest for suite c10d, encountering failure on `test_sync_params_no_buffers` & `test_sync_params_with_buffers`\r\n\r\n```\r\npython test/run_test.py -i c10d -v\r\nSelected tests: c10d\r\nRunning test_c10d ...\r\ncommand run python test_c10d.py --verbose\r\ntest_dist_broadcast_coalesced (__main__.DistributedDataParallelTest) ... ok\r\ntest_fp16 (__main__.DistributedDataParallelTest) ... ok\r\ntest_gloo_backend (__main__.DistributedDataParallelTest) ... ok\r\ntest_nccl_backend (__main__.DistributedDataParallelTest) ... ok\r\ntest_sync_params_no_buffers (__main__.DistributedDataParallelTest) ... Process process 1:\r\nTraceback (most recent call last):\r\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\r\n    self.run()\r\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test_c10d.py\", line 338, in _run\r\n    getattr(self, self.id().split(\".\")[2])()\r\n  File \"test_c10d.py\", line 302, in wrapper\r\n    fn(self)\r\n  File \"test_c10d.py\", line 45, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"test_c10d.py\", line 756, in test_sync_params_no_buffers\r\n    broadcast_buffers=False)\r\nRuntimeError: all tensors must be on devices[0]\r\nFAIL\r\ntest_sync_params_with_buffers (__main__.DistributedDataParallelTest) ... Process process 1:\r\nTraceback (most recent call last):\r\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\r\n    self.run()\r\n  File \"/home/builder/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"test_c10d.py\", line 338, in _run\r\n    getattr(self, self.id().split(\".\")[2])()\r\n  File \"test_c10d.py\", line 302, in wrapper\r\n    fn(self)\r\n  File \"test_c10d.py\", line 45, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"test_c10d.py\", line 790, in test_sync_params_with_buffers\r\n    broadcast_buffers=True)\r\nRuntimeError: all tensors must be on devices[0]\r\n\r\n```\r\n\r\n - PyTorch Version (e.g., 1.0): 1.0\r\n - OS (e.g., Linux): Linux\r\n - How you installed PyTorch (`conda`, `pip`, source): source\r\n - Build command you used (if compiling from source): python setup.py install\r\n - Python version: 2.7\r\n - CUDA/cuDNN version: 10.0\r\n - GPU models and configuration: 4 Tesla v100\r\n\r\n## Additional context\r\n\r\n```\r\ndef test_sync_params_no_buffers(self):\r\n...\r\nparameter_data += [torch.zeros(10, device=torch.device('cuda', d)).chunk(5) for d in devices[1:]]\r\n...\r\n        c10d._sync_params(\r\n            process_group,\r\n            parameter_data=parameter_data,\r\n            buffer_data=buffer_data,\r\n            devices=devices,\r\n            broadcast_bucket_size=10,\r\n            broadcast_buffers=True)\r\n```\r\n\r\nTestcase is assigning tensors to all cuda devices in the  parameter_data, it seems the c code is expecting the list of tensors to all reside on gpu0.\r\n\r\n\r\n```\r\n pytorch/torch/csrc/cuda/comm.cpp\r\ntensor_list2d broadcast_coalesced(TensorList tensors, IntList devices, size_t buffer_size) {\r\n  if (!std::all_of(tensors.begin(), tensors.end(),\r\n                   [&](const at::Tensor& t) { return t.get_device() == devices[0]; })) {\r\n    throw std::runtime_error(\"all tensors must be on devices[0]\");\r\n}\r\n```"}