{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/131678197", "pull_request_review_id": 54680799, "id": 131678197, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMTY3ODE5Nw==", "diff_hunk": "@@ -217,6 +217,22 @@ def fn(t, dim, keepdim=False):\n             self.assertEqual(x.ndimension(), fn(x, dim).ndimension())\n             self.assertEqual(x.ndimension(), fn(x, dim, keepdim=True).ndimension())\n \n+            # check reducing of a singleton dimension\n+            dims = [3, 4, 5]\n+            singleton_dim = random.randint(0, 2)\n+            dims[singleton_dim] = 1\n+            x = torch.randn(dims)\n+            fn_attr = getattr(torch, fn_name) if fn_name != \"norm\" else normfn_attr\n+\n+            def fn(t, dim, keepdim=False):\n+                ans = fn_attr(x, dim, keepdim=keepdim)", "path": "test/test_torch.py", "position": null, "original_position": 12, "commit_id": "f8dc6a5d6ede7501c7e3eea34df9ca79e2f7791b", "original_commit_id": "efd0ac0e004bdb4b82c7517b793e8ff6c1fcb937", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "same here.  One issue might be that this function (and the above function) take in a \"t\" but use \"x\", so you have to redefine.  Can you fix that above and remove this?", "created_at": "2017-08-07T15:02:59Z", "updated_at": "2018-11-23T15:34:18Z", "html_url": "https://github.com/pytorch/pytorch/pull/2318#discussion_r131678197", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/2318", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/131678197"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/2318#discussion_r131678197"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/2318"}}, "body_html": "<p>same here.  One issue might be that this function (and the above function) take in a \"t\" but use \"x\", so you have to redefine.  Can you fix that above and remove this?</p>", "body_text": "same here.  One issue might be that this function (and the above function) take in a \"t\" but use \"x\", so you have to redefine.  Can you fix that above and remove this?"}