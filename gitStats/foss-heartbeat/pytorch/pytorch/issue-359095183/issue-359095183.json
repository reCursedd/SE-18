{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11516", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11516/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11516/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11516/events", "html_url": "https://github.com/pytorch/pytorch/issues/11516", "id": 359095183, "node_id": "MDU6SXNzdWUzNTkwOTUxODM=", "number": 11516, "title": "at::Device makes it very easy to write buggy code", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-09-11T15:16:58Z", "updated_at": "2018-09-12T02:04:09Z", "closed_at": null, "author_association": "MEMBER", "body_html": "<p>Imagine this method:</p>\n<div class=\"highlight highlight-source-c++\"><pre>Tensor <span class=\"pl-en\">Tensor::cuda</span>(Device dev = at::Device(at::DeviceKind::CUDA)) {\n  <span class=\"pl-k\">if</span> (<span class=\"pl-c1\">this</span>-&gt;<span class=\"pl-c1\">device</span>() == dev) {\n    <span class=\"pl-k\">return</span> *<span class=\"pl-c1\">this</span>;\n  }\n  <span class=\"pl-k\">return</span> ...; <span class=\"pl-c\"><span class=\"pl-c\">//</span> do the transfer</span>\n}</pre></div>\n<p>Can you see the error? I didn't.</p>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<p>The problem lies in <code>operator==</code> of <code>at::Device</code>, which enforces strict equality of device kinds and indices. This would be fine if indices were always guaranteed to be valid, but they are in fact optional. On the other hand, the device objects returned from <code>this-&gt;device()</code> are always guaranteed to have a device set, and so they will always compare as not-equal to the default argument, <em>always forcing the tensor copy</em>.</p>\n<p>It would be great if we could resolve this in some way. I have three proposals:</p>\n<h4>Create another device type</h4>\n<p>We want to allow people to specify a device type for their scripts, which often involves specifying the kind only and ignoring the index. This is what <code>at::Device</code> aims to do. On the other hand, internal library functions are almost always functional, and use the device to decide what to do <em>right now</em>. Those use cases pretty much always require us to have a device index bound to that object.</p>\n<p>This proposal would be to add <code>at::FullDevice</code> (better name ideas welcome), which acts similarly to <code>at::Device</code>, but is guaranteed to have a device index set. We would need implicit conversions going in both ways because:</p>\n<ul>\n<li><code>at::Device -&gt; at::FullDevice</code> would be needed when users are calling into library functions (they would take <code>FullDevice</code> as arguments now). This conversion would be a no-op if the index is specified, and would retrieve the currently selected CUDA device otherwise.</li>\n<li><code>at::FullDevice -&gt; at::Device</code> this is a trivial conversion. <code>Tensor::device()</code> should now return <code>at::FullDevice</code>, but user code will always work with <code>at::Device</code>, which is why we need the conversion.</li>\n</ul>\n<h4>Do nothing. Add a warning maybe.</h4>\n<p>Plus a method like <code>ensure_has_index()</code>, which would set the index to currently selected device of this kind if it's missing.</p>\n<h4>Change <code>operator==</code> of <code>at::Device</code> to depend on <code>thread_local</code> state</h4>\n<p>This means that if we have a CUDA device with no index set, inside <code>operator==</code> we will assume that it has an index equal to the currently selected device. I don't like this one all that much, because it makes a supposedly simple operation quite complicated, and might break <code>operator==</code> invariants when someone e.g. uses a set of devices, and changes the thread local state during its lifetime.</p>\n<hr>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6429851\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/goldsborough\">@goldsborough</a></p>", "body_text": "Imagine this method:\nTensor Tensor::cuda(Device dev = at::Device(at::DeviceKind::CUDA)) {\n  if (this->device() == dev) {\n    return *this;\n  }\n  return ...; // do the transfer\n}\nCan you see the error? I didn't.\n\n\n\n\n\n\n\n\n\n\n\nThe problem lies in operator== of at::Device, which enforces strict equality of device kinds and indices. This would be fine if indices were always guaranteed to be valid, but they are in fact optional. On the other hand, the device objects returned from this->device() are always guaranteed to have a device set, and so they will always compare as not-equal to the default argument, always forcing the tensor copy.\nIt would be great if we could resolve this in some way. I have three proposals:\nCreate another device type\nWe want to allow people to specify a device type for their scripts, which often involves specifying the kind only and ignoring the index. This is what at::Device aims to do. On the other hand, internal library functions are almost always functional, and use the device to decide what to do right now. Those use cases pretty much always require us to have a device index bound to that object.\nThis proposal would be to add at::FullDevice (better name ideas welcome), which acts similarly to at::Device, but is guaranteed to have a device index set. We would need implicit conversions going in both ways because:\n\nat::Device -> at::FullDevice would be needed when users are calling into library functions (they would take FullDevice as arguments now). This conversion would be a no-op if the index is specified, and would retrieve the currently selected CUDA device otherwise.\nat::FullDevice -> at::Device this is a trivial conversion. Tensor::device() should now return at::FullDevice, but user code will always work with at::Device, which is why we need the conversion.\n\nDo nothing. Add a warning maybe.\nPlus a method like ensure_has_index(), which would set the index to currently selected device of this kind if it's missing.\nChange operator== of at::Device to depend on thread_local state\nThis means that if we have a CUDA device with no index set, inside operator== we will assume that it has an index equal to the currently selected device. I don't like this one all that much, because it makes a supposedly simple operation quite complicated, and might break operator== invariants when someone e.g. uses a set of devices, and changes the thread local state during its lifetime.\n\ncc @goldsborough", "body": "Imagine this method:\r\n```cpp\r\nTensor Tensor::cuda(Device dev = at::Device(at::DeviceKind::CUDA)) {\r\n  if (this->device() == dev) {\r\n    return *this;\r\n  }\r\n  return ...; // do the transfer\r\n}\r\n```\r\n\r\nCan you see the error? I didn't.\r\n\r\n<br/>\r\n<br/>\r\n<br/>\r\n<br/>\r\n<br/>\r\n<br/>\r\n<br/>\r\n<br/>\r\n<br/>\r\n<br/>\r\n<br/>\r\n\r\nThe problem lies in `operator==` of `at::Device`, which enforces strict equality of device kinds and indices. This would be fine if indices were always guaranteed to be valid, but they are in fact optional. On the other hand, the device objects returned from `this->device()` are always guaranteed to have a device set, and so they will always compare as not-equal to the default argument, *always forcing the tensor copy*.\r\n\r\nIt would be great if we could resolve this in some way. I have three proposals:\r\n\r\n#### Create another device type\r\n\r\nWe want to allow people to specify a device type for their scripts, which often involves specifying the kind only and ignoring the index. This is what `at::Device` aims to do. On the other hand, internal library functions are almost always functional, and use the device to decide what to do *right now*. Those use cases pretty much always require us to have a device index bound to that object.\r\n\r\nThis proposal would be to add `at::FullDevice` (better name ideas welcome), which acts similarly to `at::Device`, but is guaranteed to have a device index set. We would need implicit conversions going in both ways because:\r\n- `at::Device -> at::FullDevice` would be needed when users are calling into library functions (they would take `FullDevice` as arguments now). This conversion would be a no-op if the index is specified, and would retrieve the currently selected CUDA device otherwise.\r\n- `at::FullDevice -> at::Device` this is a trivial conversion. `Tensor::device()` should now return `at::FullDevice`, but user code will always work with `at::Device`, which is why we need the conversion.\r\n\r\n#### Do nothing. Add a warning maybe.\r\n\r\nPlus a method like `ensure_has_index()`, which would set the index to currently selected device of this kind if it's missing.\r\n\r\n#### Change `operator==` of `at::Device` to depend on `thread_local` state\r\n\r\nThis means that if we have a CUDA device with no index set, inside `operator==` we will assume that it has an index equal to the currently selected device. I don't like this one all that much, because it makes a supposedly simple operation quite complicated, and might break `operator==` invariants when someone e.g. uses a set of devices, and changes the thread local state during its lifetime.\r\n\r\n---\r\n\r\ncc @goldsborough "}