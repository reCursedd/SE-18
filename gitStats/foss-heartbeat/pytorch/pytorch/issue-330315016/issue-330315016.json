{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8241", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8241/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8241/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8241/events", "html_url": "https://github.com/pytorch/pytorch/issues/8241", "id": 330315016, "node_id": "MDU6SXNzdWUzMzAzMTUwMTY=", "number": 8241, "title": "Order of AT_FORALL_SCALAR_TYPES matters, but it shouldn't", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-07T15:06:10Z", "updated_at": "2018-07-13T19:24:51Z", "closed_at": "2018-07-13T19:24:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Steps to reproduce:</p>\n<ol>\n<li>Go to <code>aten/src/ATen/ScalarType.h</code> and reorder <code>AT_FORALL_SCALAR_TYPES</code>, e.g., by moving half to the beginning or end of the list</li>\n<li>Build and run <code>python test/test_torch.py</code></li>\n</ol>\n<p>Expected result: tests pass</p>\n<p>Actual result:</p>\n<pre><code>======================================================================\nERROR: test_sum_all (test_torch.TestTorch)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 1520, in test_sum_all\n    check_sum_all(torch.tensor([1, 2, 3, 4, 5]))\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 1518, in check_sum_all\n    self.assertEqual(tensor.sum(), sum(pylist))\nRuntimeError: _sum is not implemented for type torch.HalfTensor\n\n======================================================================\nERROR: test_tensor_factory (test_torch.TestTorch)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 1996, in test_tensor_factory\n    self.assertEqual(res1, expected)\n  File \"/data/users/ezyang/pytorch-tmp/test/common.py\", line 306, in assertEqual\n    assertTensorsEqual(x, y)\n  File \"/data/users/ezyang/pytorch-tmp/test/common.py\", line 290, in assertTensorsEqual\n    nan_mask = a != a\nRuntimeError: s_ne is not implemented for type torch.HalfTensor\n\n======================================================================\nFAIL: test_tensor_factory_type_inference (test_torch.TestTorch)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 2046, in test_tensor_factory_type_inference\n    test_inference(torch.float64)\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 2031, in test_inference\n    self.assertIs(torch.int64, torch.tensor(((5, 3), (3, 5))).dtype)\nAssertionError: torch.int64 is not torch.uint8\n\n----------------------------------------------------------------------\n</code></pre>\n<p>So, somehow, we are encoding some assumptions about how the enum is laid out; maybe it's something like, integer types are contiguously numbered, and maybe an assumption about the low bound and the high bound. This is a footgun waiting to happen when someone tries to add a new scalar type.</p>", "body_text": "Steps to reproduce:\n\nGo to aten/src/ATen/ScalarType.h and reorder AT_FORALL_SCALAR_TYPES, e.g., by moving half to the beginning or end of the list\nBuild and run python test/test_torch.py\n\nExpected result: tests pass\nActual result:\n======================================================================\nERROR: test_sum_all (test_torch.TestTorch)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 1520, in test_sum_all\n    check_sum_all(torch.tensor([1, 2, 3, 4, 5]))\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 1518, in check_sum_all\n    self.assertEqual(tensor.sum(), sum(pylist))\nRuntimeError: _sum is not implemented for type torch.HalfTensor\n\n======================================================================\nERROR: test_tensor_factory (test_torch.TestTorch)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 1996, in test_tensor_factory\n    self.assertEqual(res1, expected)\n  File \"/data/users/ezyang/pytorch-tmp/test/common.py\", line 306, in assertEqual\n    assertTensorsEqual(x, y)\n  File \"/data/users/ezyang/pytorch-tmp/test/common.py\", line 290, in assertTensorsEqual\n    nan_mask = a != a\nRuntimeError: s_ne is not implemented for type torch.HalfTensor\n\n======================================================================\nFAIL: test_tensor_factory_type_inference (test_torch.TestTorch)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 2046, in test_tensor_factory_type_inference\n    test_inference(torch.float64)\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 2031, in test_inference\n    self.assertIs(torch.int64, torch.tensor(((5, 3), (3, 5))).dtype)\nAssertionError: torch.int64 is not torch.uint8\n\n----------------------------------------------------------------------\n\nSo, somehow, we are encoding some assumptions about how the enum is laid out; maybe it's something like, integer types are contiguously numbered, and maybe an assumption about the low bound and the high bound. This is a footgun waiting to happen when someone tries to add a new scalar type.", "body": "Steps to reproduce:\r\n1. Go to `aten/src/ATen/ScalarType.h` and reorder `AT_FORALL_SCALAR_TYPES`, e.g., by moving half to the beginning or end of the list\r\n2. Build and run `python test/test_torch.py`\r\n\r\nExpected result: tests pass\r\n\r\nActual result:\r\n\r\n```\r\n======================================================================\r\nERROR: test_sum_all (test_torch.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 1520, in test_sum_all\r\n    check_sum_all(torch.tensor([1, 2, 3, 4, 5]))\r\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 1518, in check_sum_all\r\n    self.assertEqual(tensor.sum(), sum(pylist))\r\nRuntimeError: _sum is not implemented for type torch.HalfTensor\r\n\r\n======================================================================\r\nERROR: test_tensor_factory (test_torch.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 1996, in test_tensor_factory\r\n    self.assertEqual(res1, expected)\r\n  File \"/data/users/ezyang/pytorch-tmp/test/common.py\", line 306, in assertEqual\r\n    assertTensorsEqual(x, y)\r\n  File \"/data/users/ezyang/pytorch-tmp/test/common.py\", line 290, in assertTensorsEqual\r\n    nan_mask = a != a\r\nRuntimeError: s_ne is not implemented for type torch.HalfTensor\r\n\r\n======================================================================\r\nFAIL: test_tensor_factory_type_inference (test_torch.TestTorch)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 2046, in test_tensor_factory_type_inference\r\n    test_inference(torch.float64)\r\n  File \"/data/users/ezyang/pytorch-tmp/test/test_torch.py\", line 2031, in test_inference\r\n    self.assertIs(torch.int64, torch.tensor(((5, 3), (3, 5))).dtype)\r\nAssertionError: torch.int64 is not torch.uint8\r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\nSo, somehow, we are encoding some assumptions about how the enum is laid out; maybe it's something like, integer types are contiguously numbered, and maybe an assumption about the low bound and the high bound. This is a footgun waiting to happen when someone tries to add a new scalar type."}