{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377402783", "html_url": "https://github.com/pytorch/pytorch/pull/5987#issuecomment-377402783", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5987", "id": 377402783, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzQwMjc4Mw==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-29T23:28:42Z", "updated_at": "2018-03-29T23:29:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13946458\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vedanuj\">@vedanuj</a>  avx_mathfun.h has been <a href=\"https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/cpu/avx_mathfun.h\">moved into ATen</a> and is also being used by <a href=\"https://github.com/pytorch/pytorch/tree/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/cpu/vec256\">vec256</a>. It might be worthwhile implementing this operation in context of vec256 at this point rather than mathfun. Specifically <a href=\"https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/cpu/vec256/vec256_float.h\">vec256_float.h</a> contains the specializations for floating point numbers. This is the place you'd want to move your implementation of tanh.</p>\n<p>In order to get this to work, you'd then also need to add tanh to <a href=\"https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/cpu/vec256/vec256_double.h\">vec256_double.h</a> and <a href=\"https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/cpu/vec256/vec256_base.h\">vec256_base.h</a>. This doesn't require a special vectorized implementation, the existing code is hopefully clear enough to see how to add this (just look at exp()). A first useful place to then make use of this is within <a href=\"https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/UnaryOps.cpp\">UnaryOps.cpp</a>. For that you'll only need to <a href=\"https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp\">add a tanh_kernel</a> and place a macro in <a href=\"https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/cpu/UnaryOpsKernel.h\">UnaryOpsKernel.h</a> and again <a href=\"https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/UnaryOps.cpp\">UnaryOps.cpp</a> (you can look at the exp implementation again). Also don't forget to modify <a href=\"https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/Declarations.cwrap\">Declarations.cwrap</a> by removing the reference to \"tanh_\" and changing \"tanh\" to \"_tanh\" and adding \"cname: tanh\" to the new \"_tanh\" (again you can look at \"_exp\").</p>\n<p>UnaryOps will soon be moved entirely into ATen. For now it only is used for the contiguous cases, but support for non-contiguous tensors will follow <a href=\"https://github.com/pytorch/pytorch/pull/6119\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/6119/hovercard\">soon</a>. At that point UnaryOps will only call into THC for CUDA support, but not anymore into TH. I'll then also port your implementation of tanh together with exp, log, etc. as part of the non-contiguous extension.</p>\n<p>Supporting this new layout should not take you more than a few minutes. Please let me know if that's not the case and also how it's confusing so we can some documentation etc.</p>", "body_text": "@vedanuj  avx_mathfun.h has been moved into ATen and is also being used by vec256. It might be worthwhile implementing this operation in context of vec256 at this point rather than mathfun. Specifically vec256_float.h contains the specializations for floating point numbers. This is the place you'd want to move your implementation of tanh.\nIn order to get this to work, you'd then also need to add tanh to vec256_double.h and vec256_base.h. This doesn't require a special vectorized implementation, the existing code is hopefully clear enough to see how to add this (just look at exp()). A first useful place to then make use of this is within UnaryOps.cpp. For that you'll only need to add a tanh_kernel and place a macro in UnaryOpsKernel.h and again UnaryOps.cpp (you can look at the exp implementation again). Also don't forget to modify Declarations.cwrap by removing the reference to \"tanh_\" and changing \"tanh\" to \"_tanh\" and adding \"cname: tanh\" to the new \"_tanh\" (again you can look at \"_exp\").\nUnaryOps will soon be moved entirely into ATen. For now it only is used for the contiguous cases, but support for non-contiguous tensors will follow soon. At that point UnaryOps will only call into THC for CUDA support, but not anymore into TH. I'll then also port your implementation of tanh together with exp, log, etc. as part of the non-contiguous extension.\nSupporting this new layout should not take you more than a few minutes. Please let me know if that's not the case and also how it's confusing so we can some documentation etc.", "body": "@vedanuj  avx_mathfun.h has been [moved into ATen](https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/cpu/avx_mathfun.h) and is also being used by [vec256](https://github.com/pytorch/pytorch/tree/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/cpu/vec256). It might be worthwhile implementing this operation in context of vec256 at this point rather than mathfun. Specifically [vec256_float.h](https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/cpu/vec256/vec256_float.h) contains the specializations for floating point numbers. This is the place you'd want to move your implementation of tanh.\r\n\r\nIn order to get this to work, you'd then also need to add tanh to [vec256_double.h](https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/cpu/vec256/vec256_double.h) and [vec256_base.h](https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/cpu/vec256/vec256_base.h). This doesn't require a special vectorized implementation, the existing code is hopefully clear enough to see how to add this (just look at exp()). A first useful place to then make use of this is within [UnaryOps.cpp](https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/UnaryOps.cpp). For that you'll only need to [add a tanh_kernel](https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp) and place a macro in [UnaryOpsKernel.h](https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/cpu/UnaryOpsKernel.h) and again [UnaryOps.cpp](https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/native/UnaryOps.cpp) (you can look at the exp implementation again). Also don't forget to modify [Declarations.cwrap](https://github.com/pytorch/pytorch/blob/e4c0bb1809fd9bf9161392bfff7d06092adc224d/aten/src/ATen/Declarations.cwrap) by removing the reference to \"tanh_\" and changing \"tanh\" to \"_tanh\" and adding \"cname: tanh\" to the new \"_tanh\" (again you can look at \"_exp\"). \r\n\r\nUnaryOps will soon be moved entirely into ATen. For now it only is used for the contiguous cases, but support for non-contiguous tensors will follow [soon](https://github.com/pytorch/pytorch/pull/6119). At that point UnaryOps will only call into THC for CUDA support, but not anymore into TH. I'll then also port your implementation of tanh together with exp, log, etc. as part of the non-contiguous extension.\r\n\r\nSupporting this new layout should not take you more than a few minutes. Please let me know if that's not the case and also how it's confusing so we can some documentation etc."}