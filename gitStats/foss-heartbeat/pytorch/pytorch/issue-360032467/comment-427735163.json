{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427735163", "html_url": "https://github.com/pytorch/pytorch/issues/11662#issuecomment-427735163", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11662", "id": 427735163, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzczNTE2Mw==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-08T06:48:31Z", "updated_at": "2018-10-08T06:48:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>copied code here for md format:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> random\n<span class=\"pl-k\">from</span> copy <span class=\"pl-k\">import</span> deepcopy\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">from</span> pdb <span class=\"pl-k\">import</span> set_trace\n<span class=\"pl-k\">import</span> math\n\n<span class=\"pl-k\">from</span> pdb <span class=\"pl-k\">import</span> set_trace\n\n\nparser <span class=\"pl-k\">=</span> argparse.ArgumentParser(<span class=\"pl-v\">description</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rnn cpu and gpu tests<span class=\"pl-pds\">\"</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--seed<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--use_gpu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--has_bias<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> without this flag = no bias</span>\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--use_cuda_relu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> without this flag = custom relu is the same as in pytorch; with flag = same as cuda</span>\nseed <span class=\"pl-k\">=</span> parser.parse_args().seed\nuse_gpu <span class=\"pl-k\">=</span> parser.parse_args().use_gpu\nhas_bias <span class=\"pl-k\">=</span> parser.parse_args().has_bias\nuse_cuda_relu<span class=\"pl-k\">=</span> parser.parse_args().use_cuda_relu\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>use gpu.<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">if</span> use_gpu <span class=\"pl-k\">else</span> <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>use cpu.<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>use cuda relu<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">if</span> use_cuda_relu <span class=\"pl-k\">else</span> <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>use pytorch relu<span class=\"pl-pds\">'</span></span>)\n\ntorch.cuda.manual_seed(seed)\ntorch.manual_seed(seed)\nrandom.seed(seed)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span># manually create input, target, initial hidden state and criterion</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> dim = (seq_len, batch_size, input_size)</span>\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1</span>)).cuda() <span class=\"pl-k\">if</span> use_gpu \\\n        <span class=\"pl-k\">else</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">1</span>))\ntarget <span class=\"pl-k\">=</span> Variable(torch.randint(<span class=\"pl-v\">low</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">high</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">64</span>, ),\n        <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.long)).cuda() <span class=\"pl-k\">if</span> use_gpu <span class=\"pl-k\">else</span> \\\n        Variable(torch.randint(<span class=\"pl-v\">low</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">high</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">64</span>, ), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.long))\nhx_init <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">20</span>)).cuda() <span class=\"pl-k\">if</span> use_gpu \\\n        <span class=\"pl-k\">else</span> Variable(torch.randn(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">20</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> dim = (batch_size, hidden_size)</span>\ncriterion <span class=\"pl-k\">=</span> nn.CrossEntropyLoss() <span class=\"pl-c\"><span class=\"pl-c\">#</span> use cross entropy loss</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> define an init function</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> no bias and eye init to make sure two networks have the same parameters</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">param_init</span>(<span class=\"pl-smi\">rnn</span>, <span class=\"pl-smi\">linear</span>):\n    <span class=\"pl-k\">for</span> name, param <span class=\"pl-k\">in</span> rnn.named_parameters():\n        <span class=\"pl-k\">if</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">in</span> name:\n            nn.init.eye_(param)\n        <span class=\"pl-k\">else</span>:\n            nn.init.constant_(param, <span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">for</span> name, param <span class=\"pl-k\">in</span> linear.named_parameters():\n        <span class=\"pl-k\">if</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">in</span> name:\n            nn.init.eye_(param)\n        <span class=\"pl-k\">else</span>:\n            nn.init.constant_(param, <span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>##########################################################################</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span># first network, its output and rnn gradients</span>\nrnn1 <span class=\"pl-k\">=</span> nn.RNN(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">nonlinearity</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias).cuda() \\\n        <span class=\"pl-k\">if</span> use_gpu <span class=\"pl-k\">else</span> nn.RNN(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">nonlinearity</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias)\nlinear1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias).cuda() \\\n        <span class=\"pl-k\">if</span> use_gpu <span class=\"pl-k\">else</span> nn.Linear(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> param init</span>\nparam_init(rnn1, linear1)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> run the net</span>\nrnn1.zero_grad()\nlinear1.zero_grad()\n\noutput1, hx1 <span class=\"pl-k\">=</span> rnn1(<span class=\"pl-c1\">input</span>, hx_init.unsqueeze(<span class=\"pl-c1\">0</span>))\nlogit1 <span class=\"pl-k\">=</span> linear1(output1[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\nloss1 <span class=\"pl-k\">=</span> criterion(logit1, target)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> calculate gradients and sum of gradient norms</span>\ngrad_params1 <span class=\"pl-k\">=</span> torch.autograd.grad(loss1, rnn1.parameters(),\n        <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">allow_unused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ngrad_norm1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n<span class=\"pl-k\">for</span> idx <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(grad_params1)):\n    grad_norm1 <span class=\"pl-k\">+=</span> torch.norm(grad_params1[idx])\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn1 - nn.RNN<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn1 - loss: <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (loss1))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn1 - sum of gradient norm is: <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (grad_norm1))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>----<span class=\"pl-pds\">'</span></span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>##########################################################################</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span># second network, its output and rnn gradients</span>\nrnn2 <span class=\"pl-k\">=</span> nn.RNNCell(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">nonlinearity</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias).cuda() \\\n        <span class=\"pl-k\">if</span> use_gpu <span class=\"pl-k\">else</span> nn.RNNCell(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">20</span>, <span class=\"pl-v\">nonlinearity</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias)\nlinear2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias).cuda() \\\n        <span class=\"pl-k\">if</span> use_gpu <span class=\"pl-k\">else</span> nn.Linear(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias)\n\nparam_init(rnn2, linear2)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> run the net</span>\nrnn2.zero_grad()\nlinear2.zero_grad()\n\nhx2 <span class=\"pl-k\">=</span> deepcopy(hx_init)\noutput2 <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n    hx2 <span class=\"pl-k\">=</span> rnn2(<span class=\"pl-c1\">input</span>[i], hx2)\n    output2.append(hx2)\nlogit2 <span class=\"pl-k\">=</span> linear2(hx2)\nloss2 <span class=\"pl-k\">=</span> criterion(logit2, target)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> calculate gradients and sum of gradient norms</span>\ngrad_params2 <span class=\"pl-k\">=</span> torch.autograd.grad(loss2, rnn2.parameters(),\n        <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">allow_unused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ngrad_norm2 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n<span class=\"pl-k\">for</span> idx <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(grad_params2)):\n    grad_norm2 <span class=\"pl-k\">+=</span> torch.norm(grad_params2[idx])\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn2 - nn.RNNCell<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn2 - loss: <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (loss2))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn2: sum of gradient norm is: <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (grad_norm2))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>----<span class=\"pl-pds\">'</span></span>)\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>##########################################################################</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span># third network, its output and rnn gradients</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">myReLU</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">autograd</span>.<span class=\"pl-e\">Function</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    custom RNN cell</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">relutype</span><span class=\"pl-k\">=</span>use_cuda_relu):\n        <span class=\"pl-k\">if</span> relutype:\n            ctx._mask <span class=\"pl-k\">=</span> (x <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">0</span>)\n        <span class=\"pl-k\">else</span>:\n            ctx._mask <span class=\"pl-k\">=</span> (x <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>)\n        <span class=\"pl-k\">return</span> x.relu()\n    <span class=\"pl-en\">@</span><span class=\"pl-c1\">staticmethod</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\">ctx</span>, <span class=\"pl-smi\">grad_out</span>):\n        <span class=\"pl-k\">return</span> grad_out <span class=\"pl-k\">*</span> ctx._mask.to(grad_out.dtype), <span class=\"pl-c1\">None</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">ReLURNN</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    a ReLU RNN cell with ReLU implementation consistent with cuda</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">hidden_size</span>, <span class=\"pl-smi\">bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">nonlinearity</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>):\n        <span class=\"pl-c1\">super</span>(ReLURNN, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.input_size <span class=\"pl-k\">=</span> input_size\n        <span class=\"pl-c1\">self</span>.hidden_size <span class=\"pl-k\">=</span> hidden_size\n        <span class=\"pl-c1\">self</span>.bias <span class=\"pl-k\">=</span> bias\n        <span class=\"pl-c1\">self</span>.nonlinearity <span class=\"pl-k\">=</span> nonlinearity\n\n        <span class=\"pl-c1\">self</span>.weight_ih <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(hidden_size, input_size))\n        <span class=\"pl-c1\">self</span>.weight_hh <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n        <span class=\"pl-k\">if</span> bias:\n            <span class=\"pl-c1\">self</span>.bias_ih <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(hidden_size))\n            <span class=\"pl-c1\">self</span>.bias_hh <span class=\"pl-k\">=</span> nn.Parameter(torch.Tensor(hidden_size))\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">self</span>.register_parameter(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias_ih<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">None</span>)\n            <span class=\"pl-c1\">self</span>.register_parameter(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias_hh<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">None</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> self.reset_parameters()</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">extra_repr</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        s <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">{input_size}</span>, <span class=\"pl-c1\">{hidden_size}</span><span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bias<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.<span class=\"pl-c1\">__dict__</span> <span class=\"pl-k\">and</span> <span class=\"pl-c1\">self</span>.bias <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">True</span>:\n            s <span class=\"pl-k\">+=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>, bias=<span class=\"pl-c1\">{bias}</span><span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>nonlinearity<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.<span class=\"pl-c1\">__dict__</span> <span class=\"pl-k\">and</span> <span class=\"pl-c1\">self</span>.nonlinearity <span class=\"pl-k\">!=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tanh<span class=\"pl-pds\">\"</span></span>:\n            s <span class=\"pl-k\">+=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>, nonlinearity=<span class=\"pl-c1\">{nonlinearity}</span><span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-k\">return</span> s.format(<span class=\"pl-k\">**</span><span class=\"pl-c1\">self</span>.<span class=\"pl-c1\">__dict__</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">check_forward_input</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">self</span>.input_size:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>(\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input has inconsistent input_size: got <span class=\"pl-c1\">{}</span>, expected <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(\n                    <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">self</span>.input_size))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">check_forward_hidden</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">hx</span>, <span class=\"pl-smi\">hidden_label</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>) <span class=\"pl-k\">!=</span> hx.size(<span class=\"pl-c1\">0</span>):\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>(\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Input batch size <span class=\"pl-c1\">{}</span> doesn't match hidden<span class=\"pl-c1\">{}</span> batch size <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(\n                    <span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>), hidden_label, hx.size(<span class=\"pl-c1\">0</span>)))\n\n        <span class=\"pl-k\">if</span> hx.size(<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">self</span>.hidden_size:\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>(\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>hidden<span class=\"pl-c1\">{}</span> has inconsistent hidden_size: got <span class=\"pl-c1\">{}</span>, expected <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(\n                    hidden_label, hx.size(<span class=\"pl-c1\">1</span>), <span class=\"pl-c1\">self</span>.hidden_size))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">hx</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">self</span>.check_forward_input(<span class=\"pl-c1\">input</span>)\n        <span class=\"pl-k\">if</span> hx <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            hx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">input</span>.new_zeros(<span class=\"pl-c1\">input</span>.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-c1\">self</span>.hidden_size, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> set_trace()</span>\n        <span class=\"pl-c1\">self</span>.check_forward_hidden(<span class=\"pl-c1\">input</span>, hx)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> set_trace()</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span> take a look at the primitive implementation and implement this myself</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.nonlinearity <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>:\n            <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.bias:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>    return  myReLU.apply(torch.mm(input, self.weight_ih.t()) + self.bias_ih</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>                    + torch.mm(hx, self.weight_hh.t()) + self.bias_hh)</span>\n                <span class=\"pl-k\">return</span>  myReLU.apply(torch.mm(<span class=\"pl-c1\">input</span>, <span class=\"pl-c1\">self</span>.weight_ih.t()) <span class=\"pl-k\">+</span>\n                               <span class=\"pl-c1\">self</span>.bias_ih.expand(<span class=\"pl-c1\">input</span>.shape[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">self</span>.bias_ih.shape[<span class=\"pl-c1\">0</span>]).contiguous() <span class=\"pl-k\">+</span>\n                               torch.mm(hx, <span class=\"pl-c1\">self</span>.weight_hh.t())  <span class=\"pl-k\">+</span>\n                               <span class=\"pl-c1\">self</span>.bias_hh.expand(<span class=\"pl-c1\">input</span>.shape[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">self</span>.bias_hh.shape[<span class=\"pl-c1\">0</span>]).contiguous())\n            <span class=\"pl-k\">else</span>:\n                <span class=\"pl-k\">return</span>  myReLU.apply(torch.mm(<span class=\"pl-c1\">input</span>, <span class=\"pl-c1\">self</span>.weight_ih.t())\n                                    <span class=\"pl-k\">+</span> torch.mm(hx, <span class=\"pl-c1\">self</span>.weight_hh.t()))\n\nrnn3 <span class=\"pl-k\">=</span> ReLURNN(<span class=\"pl-v\">input_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">hidden_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">20</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias).cuda() \\\n        <span class=\"pl-k\">if</span> use_gpu <span class=\"pl-k\">else</span> ReLURNN(<span class=\"pl-v\">input_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">hidden_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">20</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias)\nlinear3 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias).cuda() <span class=\"pl-k\">if</span> use_gpu \\\n        <span class=\"pl-k\">else</span> nn.Linear(<span class=\"pl-c1\">20</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">bias</span><span class=\"pl-k\">=</span>has_bias)\n\nparam_init(rnn3, linear3)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> run the net</span>\nrnn3.zero_grad()\nlinear3.zero_grad()\n\noutput3 <span class=\"pl-k\">=</span> []\nhx3 <span class=\"pl-k\">=</span> deepcopy(hx_init)\n<span class=\"pl-k\">for</span> inp <span class=\"pl-k\">in</span> <span class=\"pl-c1\">input</span>:\n    hx3 <span class=\"pl-k\">=</span> rnn3(inp, hx3)\n    output3.append(hx3)\n\nlogit3 <span class=\"pl-k\">=</span> linear3(output3[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\nloss3 <span class=\"pl-k\">=</span> criterion(logit3, target)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> calculate gradients and sum of gradient norms</span>\ngrad_params3 <span class=\"pl-k\">=</span> torch.autograd.grad(loss3, rnn3.parameters(), <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">retain_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">allow_unused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ngrad_norm3 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n<span class=\"pl-k\">for</span> idx <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(grad_params3)):\n    grad_norm3 <span class=\"pl-k\">+=</span> torch.norm(grad_params3[idx])\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn3 - ReLURNN + myReLU<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn3 - loss: <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (loss3))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rnn3: sum of gradient norm is: <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (grad_norm3))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> set_trace()</span>\n</pre></div>", "body_text": "copied code here for md format:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport random\nfrom copy import deepcopy\nimport argparse\nfrom pdb import set_trace\nimport math\n\nfrom pdb import set_trace\n\n\nparser = argparse.ArgumentParser(description=\"rnn cpu and gpu tests\")\nparser.add_argument('--seed', type=int, default=0)\nparser.add_argument('--use_gpu', action='store_true')\nparser.add_argument('--has_bias', action='store_true') # without this flag = no bias\nparser.add_argument('--use_cuda_relu', action='store_true') # without this flag = custom relu is the same as in pytorch; with flag = same as cuda\nseed = parser.parse_args().seed\nuse_gpu = parser.parse_args().use_gpu\nhas_bias = parser.parse_args().has_bias\nuse_cuda_relu= parser.parse_args().use_cuda_relu\n\nprint('use gpu.') if use_gpu else print('use cpu.')\nprint('use cuda relu') if use_cuda_relu else print('use pytorch relu')\n\ntorch.cuda.manual_seed(seed)\ntorch.manual_seed(seed)\nrandom.seed(seed)\n\n\n## manually create input, target, initial hidden state and criterion\n# dim = (seq_len, batch_size, input_size)\ninput = Variable(torch.randn(100, 64, 1)).cuda() if use_gpu \\\n        else Variable(torch.randn(100, 64, 1))\ntarget = Variable(torch.randint(low=0, high=1, size=(64, ),\n        dtype=torch.long)).cuda() if use_gpu else \\\n        Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long))\nhx_init = Variable(torch.randn(64, 20)).cuda() if use_gpu \\\n        else Variable(torch.randn(64, 20)) # dim = (batch_size, hidden_size)\ncriterion = nn.CrossEntropyLoss() # use cross entropy loss\n\n# define an init function\n# no bias and eye init to make sure two networks have the same parameters\ndef param_init(rnn, linear):\n    for name, param in rnn.named_parameters():\n        if 'weight' in name:\n            nn.init.eye_(param)\n        else:\n            nn.init.constant_(param, 1)\n    for name, param in linear.named_parameters():\n        if 'weight' in name:\n            nn.init.eye_(param)\n        else:\n            nn.init.constant_(param, 1)\n\n###########################################################################\n## first network, its output and rnn gradients\nrnn1 = nn.RNN(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\n        if use_gpu else nn.RNN(1, 20, nonlinearity='relu', bias=has_bias)\nlinear1 = nn.Linear(20, 2, bias=has_bias).cuda() \\\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\n\n# param init\nparam_init(rnn1, linear1)\n\n# run the net\nrnn1.zero_grad()\nlinear1.zero_grad()\n\noutput1, hx1 = rnn1(input, hx_init.unsqueeze(0))\nlogit1 = linear1(output1[-1])\nloss1 = criterion(logit1, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(),\n        create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm1 = 0\nfor idx in range(len(grad_params1)):\n    grad_norm1 += torch.norm(grad_params1[idx])\n\nprint('rnn1 - nn.RNN')\nprint('rnn1 - loss: %f' % (loss1))\nprint('rnn1 - sum of gradient norm is: %f' % (grad_norm1))\nprint('----')\n\n\n###########################################################################\n## second network, its output and rnn gradients\nrnn2 = nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\n        if use_gpu else nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias)\nlinear2 = nn.Linear(20, 2, bias=has_bias).cuda() \\\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\n\nparam_init(rnn2, linear2)\n\n# run the net\nrnn2.zero_grad()\nlinear2.zero_grad()\n\nhx2 = deepcopy(hx_init)\noutput2 = []\nfor i in range(100):\n    hx2 = rnn2(input[i], hx2)\n    output2.append(hx2)\nlogit2 = linear2(hx2)\nloss2 = criterion(logit2, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(),\n        create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm2 = 0\nfor idx in range(len(grad_params2)):\n    grad_norm2 += torch.norm(grad_params2[idx])\n\nprint('rnn2 - nn.RNNCell')\nprint('rnn2 - loss: %f' % (loss2))\nprint('rnn2: sum of gradient norm is: %f' % (grad_norm2))\nprint('----')\n\n\n###########################################################################\n## third network, its output and rnn gradients\n\nclass myReLU(torch.autograd.Function):\n    '''\n    custom RNN cell\n    '''\n    @staticmethod\n    def forward(ctx, x, relutype=use_cuda_relu):\n        if relutype:\n            ctx._mask = (x >= 0)\n        else:\n            ctx._mask = (x > 0)\n        return x.relu()\n    @staticmethod\n    def backward(ctx, grad_out):\n        return grad_out * ctx._mask.to(grad_out.dtype), None\n\nclass ReLURNN(nn.Module):\n    '''\n    a ReLU RNN cell with ReLU implementation consistent with cuda\n    '''\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity='relu'):\n        super(ReLURNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.nonlinearity = nonlinearity\n\n        self.weight_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n        if bias:\n            self.bias_ih = nn.Parameter(torch.Tensor(hidden_size))\n            self.bias_hh = nn.Parameter(torch.Tensor(hidden_size))\n        else:\n            self.register_parameter('bias_ih', None)\n            self.register_parameter('bias_hh', None)\n        # self.reset_parameters()\n\n    def extra_repr(self):\n        s = '{input_size}, {hidden_size}'\n        if 'bias' in self.__dict__ and self.bias is not True:\n            s += ', bias={bias}'\n        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\n            s += ', nonlinearity={nonlinearity}'\n        return s.format(**self.__dict__)\n\n    def check_forward_input(self, input):\n        if input.size(1) != self.input_size:\n            raise RuntimeError(\n                \"input has inconsistent input_size: got {}, expected {}\".format(\n                    input.size(1), self.input_size))\n\n    def check_forward_hidden(self, input, hx, hidden_label=''):\n        if input.size(0) != hx.size(0):\n            raise RuntimeError(\n                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n                    input.size(0), hidden_label, hx.size(0)))\n\n        if hx.size(1) != self.hidden_size:\n            raise RuntimeError(\n                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\n                    hidden_label, hx.size(1), self.hidden_size))\n\n    def forward(self, input, hx=None):\n        self.check_forward_input(input)\n        if hx is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n        # set_trace()\n        self.check_forward_hidden(input, hx)\n\n        # set_trace()\n        # TODO take a look at the primitive implementation and implement this myself\n        if self.nonlinearity == 'relu':\n            if self.bias:\n            #    return  myReLU.apply(torch.mm(input, self.weight_ih.t()) + self.bias_ih\n            #                    + torch.mm(hx, self.weight_hh.t()) + self.bias_hh)\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t()) +\n                               self.bias_ih.expand(input.shape[0], self.bias_ih.shape[0]).contiguous() +\n                               torch.mm(hx, self.weight_hh.t())  +\n                               self.bias_hh.expand(input.shape[0], self.bias_hh.shape[0]).contiguous())\n            else:\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t())\n                                    + torch.mm(hx, self.weight_hh.t()))\n\nrnn3 = ReLURNN(input_size=1, hidden_size=20, bias=has_bias).cuda() \\\n        if use_gpu else ReLURNN(input_size=1, hidden_size=20, bias=has_bias)\nlinear3 = nn.Linear(20, 2, bias=has_bias).cuda() if use_gpu \\\n        else nn.Linear(20, 2, bias=has_bias)\n\nparam_init(rnn3, linear3)\n\n# run the net\nrnn3.zero_grad()\nlinear3.zero_grad()\n\noutput3 = []\nhx3 = deepcopy(hx_init)\nfor inp in input:\n    hx3 = rnn3(inp, hx3)\n    output3.append(hx3)\n\nlogit3 = linear3(output3[-1])\nloss3 = criterion(logit3, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params3 = torch.autograd.grad(loss3, rnn3.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm3 = 0\nfor idx in range(len(grad_params3)):\n    grad_norm3 += torch.norm(grad_params3[idx])\n\nprint('rnn3 - ReLURNN + myReLU')\nprint('rnn3 - loss: %f' % (loss3))\nprint('rnn3: sum of gradient norm is: %f' % (grad_norm3))\n\n# set_trace()", "body": "copied code here for md format:\r\n```py\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport random\r\nfrom copy import deepcopy\r\nimport argparse\r\nfrom pdb import set_trace\r\nimport math\r\n\r\nfrom pdb import set_trace\r\n\r\n\r\nparser = argparse.ArgumentParser(description=\"rnn cpu and gpu tests\")\r\nparser.add_argument('--seed', type=int, default=0)\r\nparser.add_argument('--use_gpu', action='store_true')\r\nparser.add_argument('--has_bias', action='store_true') # without this flag = no bias\r\nparser.add_argument('--use_cuda_relu', action='store_true') # without this flag = custom relu is the same as in pytorch; with flag = same as cuda\r\nseed = parser.parse_args().seed\r\nuse_gpu = parser.parse_args().use_gpu\r\nhas_bias = parser.parse_args().has_bias\r\nuse_cuda_relu= parser.parse_args().use_cuda_relu\r\n\r\nprint('use gpu.') if use_gpu else print('use cpu.')\r\nprint('use cuda relu') if use_cuda_relu else print('use pytorch relu')\r\n\r\ntorch.cuda.manual_seed(seed)\r\ntorch.manual_seed(seed)\r\nrandom.seed(seed)\r\n\r\n\r\n## manually create input, target, initial hidden state and criterion\r\n# dim = (seq_len, batch_size, input_size)\r\ninput = Variable(torch.randn(100, 64, 1)).cuda() if use_gpu \\\r\n        else Variable(torch.randn(100, 64, 1))\r\ntarget = Variable(torch.randint(low=0, high=1, size=(64, ),\r\n        dtype=torch.long)).cuda() if use_gpu else \\\r\n        Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long))\r\nhx_init = Variable(torch.randn(64, 20)).cuda() if use_gpu \\\r\n        else Variable(torch.randn(64, 20)) # dim = (batch_size, hidden_size)\r\ncriterion = nn.CrossEntropyLoss() # use cross entropy loss\r\n\r\n# define an init function\r\n# no bias and eye init to make sure two networks have the same parameters\r\ndef param_init(rnn, linear):\r\n    for name, param in rnn.named_parameters():\r\n        if 'weight' in name:\r\n            nn.init.eye_(param)\r\n        else:\r\n            nn.init.constant_(param, 1)\r\n    for name, param in linear.named_parameters():\r\n        if 'weight' in name:\r\n            nn.init.eye_(param)\r\n        else:\r\n            nn.init.constant_(param, 1)\r\n\r\n###########################################################################\r\n## first network, its output and rnn gradients\r\nrnn1 = nn.RNN(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\r\n        if use_gpu else nn.RNN(1, 20, nonlinearity='relu', bias=has_bias)\r\nlinear1 = nn.Linear(20, 2, bias=has_bias).cuda() \\\r\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\r\n\r\n# param init\r\nparam_init(rnn1, linear1)\r\n\r\n# run the net\r\nrnn1.zero_grad()\r\nlinear1.zero_grad()\r\n\r\noutput1, hx1 = rnn1(input, hx_init.unsqueeze(0))\r\nlogit1 = linear1(output1[-1])\r\nloss1 = criterion(logit1, target)\r\n\r\n# calculate gradients and sum of gradient norms\r\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(),\r\n        create_graph=True, retain_graph=True, allow_unused=True)\r\ngrad_norm1 = 0\r\nfor idx in range(len(grad_params1)):\r\n    grad_norm1 += torch.norm(grad_params1[idx])\r\n\r\nprint('rnn1 - nn.RNN')\r\nprint('rnn1 - loss: %f' % (loss1))\r\nprint('rnn1 - sum of gradient norm is: %f' % (grad_norm1))\r\nprint('----')\r\n\r\n\r\n###########################################################################\r\n## second network, its output and rnn gradients\r\nrnn2 = nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\r\n        if use_gpu else nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias)\r\nlinear2 = nn.Linear(20, 2, bias=has_bias).cuda() \\\r\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\r\n\r\nparam_init(rnn2, linear2)\r\n\r\n# run the net\r\nrnn2.zero_grad()\r\nlinear2.zero_grad()\r\n\r\nhx2 = deepcopy(hx_init)\r\noutput2 = []\r\nfor i in range(100):\r\n    hx2 = rnn2(input[i], hx2)\r\n    output2.append(hx2)\r\nlogit2 = linear2(hx2)\r\nloss2 = criterion(logit2, target)\r\n\r\n# calculate gradients and sum of gradient norms\r\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(),\r\n        create_graph=True, retain_graph=True, allow_unused=True)\r\ngrad_norm2 = 0\r\nfor idx in range(len(grad_params2)):\r\n    grad_norm2 += torch.norm(grad_params2[idx])\r\n\r\nprint('rnn2 - nn.RNNCell')\r\nprint('rnn2 - loss: %f' % (loss2))\r\nprint('rnn2: sum of gradient norm is: %f' % (grad_norm2))\r\nprint('----')\r\n\r\n\r\n###########################################################################\r\n## third network, its output and rnn gradients\r\n\r\nclass myReLU(torch.autograd.Function):\r\n    '''\r\n    custom RNN cell\r\n    '''\r\n    @staticmethod\r\n    def forward(ctx, x, relutype=use_cuda_relu):\r\n        if relutype:\r\n            ctx._mask = (x >= 0)\r\n        else:\r\n            ctx._mask = (x > 0)\r\n        return x.relu()\r\n    @staticmethod\r\n    def backward(ctx, grad_out):\r\n        return grad_out * ctx._mask.to(grad_out.dtype), None\r\n\r\nclass ReLURNN(nn.Module):\r\n    '''\r\n    a ReLU RNN cell with ReLU implementation consistent with cuda\r\n    '''\r\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity='relu'):\r\n        super(ReLURNN, self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.bias = bias\r\n        self.nonlinearity = nonlinearity\r\n\r\n        self.weight_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\r\n        self.weight_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\r\n        if bias:\r\n            self.bias_ih = nn.Parameter(torch.Tensor(hidden_size))\r\n            self.bias_hh = nn.Parameter(torch.Tensor(hidden_size))\r\n        else:\r\n            self.register_parameter('bias_ih', None)\r\n            self.register_parameter('bias_hh', None)\r\n        # self.reset_parameters()\r\n\r\n    def extra_repr(self):\r\n        s = '{input_size}, {hidden_size}'\r\n        if 'bias' in self.__dict__ and self.bias is not True:\r\n            s += ', bias={bias}'\r\n        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\r\n            s += ', nonlinearity={nonlinearity}'\r\n        return s.format(**self.__dict__)\r\n\r\n    def check_forward_input(self, input):\r\n        if input.size(1) != self.input_size:\r\n            raise RuntimeError(\r\n                \"input has inconsistent input_size: got {}, expected {}\".format(\r\n                    input.size(1), self.input_size))\r\n\r\n    def check_forward_hidden(self, input, hx, hidden_label=''):\r\n        if input.size(0) != hx.size(0):\r\n            raise RuntimeError(\r\n                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\r\n                    input.size(0), hidden_label, hx.size(0)))\r\n\r\n        if hx.size(1) != self.hidden_size:\r\n            raise RuntimeError(\r\n                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\r\n                    hidden_label, hx.size(1), self.hidden_size))\r\n\r\n    def forward(self, input, hx=None):\r\n        self.check_forward_input(input)\r\n        if hx is None:\r\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\r\n        # set_trace()\r\n        self.check_forward_hidden(input, hx)\r\n\r\n        # set_trace()\r\n        # TODO take a look at the primitive implementation and implement this myself\r\n        if self.nonlinearity == 'relu':\r\n            if self.bias:\r\n            #    return  myReLU.apply(torch.mm(input, self.weight_ih.t()) + self.bias_ih\r\n            #                    + torch.mm(hx, self.weight_hh.t()) + self.bias_hh)\r\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t()) +\r\n                               self.bias_ih.expand(input.shape[0], self.bias_ih.shape[0]).contiguous() +\r\n                               torch.mm(hx, self.weight_hh.t())  +\r\n                               self.bias_hh.expand(input.shape[0], self.bias_hh.shape[0]).contiguous())\r\n            else:\r\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t())\r\n                                    + torch.mm(hx, self.weight_hh.t()))\r\n\r\nrnn3 = ReLURNN(input_size=1, hidden_size=20, bias=has_bias).cuda() \\\r\n        if use_gpu else ReLURNN(input_size=1, hidden_size=20, bias=has_bias)\r\nlinear3 = nn.Linear(20, 2, bias=has_bias).cuda() if use_gpu \\\r\n        else nn.Linear(20, 2, bias=has_bias)\r\n\r\nparam_init(rnn3, linear3)\r\n\r\n# run the net\r\nrnn3.zero_grad()\r\nlinear3.zero_grad()\r\n\r\noutput3 = []\r\nhx3 = deepcopy(hx_init)\r\nfor inp in input:\r\n    hx3 = rnn3(inp, hx3)\r\n    output3.append(hx3)\r\n\r\nlogit3 = linear3(output3[-1])\r\nloss3 = criterion(logit3, target)\r\n\r\n# calculate gradients and sum of gradient norms\r\ngrad_params3 = torch.autograd.grad(loss3, rnn3.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\r\ngrad_norm3 = 0\r\nfor idx in range(len(grad_params3)):\r\n    grad_norm3 += torch.norm(grad_params3[idx])\r\n\r\nprint('rnn3 - ReLURNN + myReLU')\r\nprint('rnn3 - loss: %f' % (loss3))\r\nprint('rnn3: sum of gradient norm is: %f' % (grad_norm3))\r\n\r\n# set_trace()\r\n\r\n```\r\n"}