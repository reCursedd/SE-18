{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/427603577", "html_url": "https://github.com/pytorch/pytorch/issues/11662#issuecomment-427603577", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11662", "id": 427603577, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzYwMzU3Nw==", "user": {"login": "moonlightlane", "id": 8587554, "node_id": "MDQ6VXNlcjg1ODc1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/8587554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/moonlightlane", "html_url": "https://github.com/moonlightlane", "followers_url": "https://api.github.com/users/moonlightlane/followers", "following_url": "https://api.github.com/users/moonlightlane/following{/other_user}", "gists_url": "https://api.github.com/users/moonlightlane/gists{/gist_id}", "starred_url": "https://api.github.com/users/moonlightlane/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/moonlightlane/subscriptions", "organizations_url": "https://api.github.com/users/moonlightlane/orgs", "repos_url": "https://api.github.com/users/moonlightlane/repos", "events_url": "https://api.github.com/users/moonlightlane/events{/privacy}", "received_events_url": "https://api.github.com/users/moonlightlane/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-06T20:22:45Z", "updated_at": "2018-10-08T06:48:15Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Thanks! That repo has a few data loading issues that someone has pointed out a while back\u2026 I was working on a deadline last few weeks so didn\u2019t have time to fix but am going to fix it in the next few days.\n\nI took a closer look at the inconsistent ReLU implementation in pytorch and cuda; For RNNs without bias, the custom ReLU implementation works fine, and the outputs are the same as those of nn.RNN.\n\nHowever, there seems to be numerical computation inconsistencies between pytorch matrix calculus implementation and the C implementation underlying both nn.RNNCell and nn.RNN. It is more obvious when bias is added in the RNN models. I have a piece of code to demonstrate this, see below. If you run with\n\npython3 rnn_test.py --use_cuda_relu -\u2014use_gpu -\u2014has_bias -\u2014seed 123456\n\nyou\u2019ll see that output of nn.RNN and the custom ReLURNN actually calculates different gradients (sum of norms of gradients are slightly different). In fact, some hidden states are also calculated differently, although only few of them.\n\nThe hidden states calculations are the same between nn.RNN and nn.RNNCell, although the backward pass are different (different gradients) because of the issue of different ReLU gradient you pointed out a while ago.\n\nYou can try with various flags (different seeds, whether has bias, whether using GPU, whether using the cuda-consistent ReLU implementation), and in a lot of cases the outputs between nn.RNN and the custom ReLURNN are different.\n\nShould I be concerned about the difference in numerical calculations? Or perhaps there is a bug in my code? I haven\u2019t tested on actually training on a dataset, though.\n\n\n\n\u2014\u2014\u2014\u2014\u2014\u2014 code (also in attachment) \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport random\nfrom copy import deepcopy\nimport argparse\nfrom pdb import set_trace\nimport math\n\nfrom pdb import set_trace\n\n\nparser = argparse.ArgumentParser(description=\"rnn cpu and gpu tests\")\nparser.add_argument('--seed', type=int, default=0)\nparser.add_argument('--use_gpu', action='store_true')\nparser.add_argument('--has_bias', action='store_true') # without this flag = no bias\nparser.add_argument('--use_cuda_relu', action='store_true') # without this flag = custom relu is the same as in pytorch; with flag = same as cuda\nseed = parser.parse_args().seed\nuse_gpu = parser.parse_args().use_gpu\nhas_bias = parser.parse_args().has_bias\nuse_cuda_relu= parser.parse_args().use_cuda_relu\n\nprint('use gpu.') if use_gpu else print('use cpu.')\nprint('use cuda relu') if use_cuda_relu else print('use pytorch relu')\n\ntorch.cuda.manual_seed(seed)\ntorch.manual_seed(seed)\nrandom.seed(seed)\n\n\n## manually create input, target, initial hidden state and criterion\n# dim = (seq_len, batch_size, input_size)\ninput = Variable(torch.randn(100, 64, 1)).cuda() if use_gpu \\\n        else Variable(torch.randn(100, 64, 1))\ntarget = Variable(torch.randint(low=0, high=1, size=(64, ),\n        dtype=torch.long)).cuda() if use_gpu else \\\n        Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long))\nhx_init = Variable(torch.randn(64, 20)).cuda() if use_gpu \\\n        else Variable(torch.randn(64, 20)) # dim = (batch_size, hidden_size)\ncriterion = nn.CrossEntropyLoss() # use cross entropy loss\n\n# define an init function\n# no bias and eye init to make sure two networks have the same parameters\ndef param_init(rnn, linear):\n    for name, param in rnn.named_parameters():\n        if 'weight' in name:\n            nn.init.eye_(param)\n        else:\n            nn.init.constant_(param, 1)\n    for name, param in linear.named_parameters():\n        if 'weight' in name:\n            nn.init.eye_(param)\n        else:\n            nn.init.constant_(param, 1)\n\n###########################################################################\n## first network, its output and rnn gradients\nrnn1 = nn.RNN(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\n        if use_gpu else nn.RNN(1, 20, nonlinearity='relu', bias=has_bias)\nlinear1 = nn.Linear(20, 2, bias=has_bias).cuda() \\\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\n\n# param init\nparam_init(rnn1, linear1)\n\n# run the net\nrnn1.zero_grad()\nlinear1.zero_grad()\n\noutput1, hx1 = rnn1(input, hx_init.unsqueeze(0))\nlogit1 = linear1(output1[-1])\nloss1 = criterion(logit1, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(),\n        create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm1 = 0\nfor idx in range(len(grad_params1)):\n    grad_norm1 += torch.norm(grad_params1[idx])\n\nprint('rnn1 - nn.RNN')\nprint('rnn1 - loss: %f' % (loss1))\nprint('rnn1 - sum of gradient norm is: %f' % (grad_norm1))\nprint('----')\n\n\n###########################################################################\n## second network, its output and rnn gradients\nrnn2 = nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\n        if use_gpu else nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias)\nlinear2 = nn.Linear(20, 2, bias=has_bias).cuda() \\\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\n\nparam_init(rnn2, linear2)\n\n# run the net\nrnn2.zero_grad()\nlinear2.zero_grad()\n\nhx2 = deepcopy(hx_init)\noutput2 = []\nfor i in range(100):\n    hx2 = rnn2(input[i], hx2)\n    output2.append(hx2)\nlogit2 = linear2(hx2)\nloss2 = criterion(logit2, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(),\n        create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm2 = 0\nfor idx in range(len(grad_params2)):\n    grad_norm2 += torch.norm(grad_params2[idx])\n\nprint('rnn2 - nn.RNNCell')\nprint('rnn2 - loss: %f' % (loss2))\nprint('rnn2: sum of gradient norm is: %f' % (grad_norm2))\nprint('----')\n\n\n###########################################################################\n## third network, its output and rnn gradients\n\nclass myReLU(torch.autograd.Function):\n    '''\n    custom RNN cell\n    '''\n    <a class=\"user-mention\" href=\"https://github.com/staticmethod\">@staticmethod</a>\n    def forward(ctx, x, relutype=use_cuda_relu):\n        if relutype:\n            ctx._mask = (x &gt;= 0)\n        else:\n            ctx._mask = (x &gt; 0)\n        return x.relu()\n    <a class=\"user-mention\" href=\"https://github.com/staticmethod\">@staticmethod</a>\n    def backward(ctx, grad_out):\n        return grad_out * ctx._mask.to(grad_out.dtype), None\n\nclass ReLURNN(nn.Module):\n    '''\n    a ReLU RNN cell with ReLU implementation consistent with cuda\n    '''\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity='relu'):\n        super(ReLURNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.nonlinearity = nonlinearity\n\n        self.weight_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n        if bias:\n            self.bias_ih = nn.Parameter(torch.Tensor(hidden_size))\n            self.bias_hh = nn.Parameter(torch.Tensor(hidden_size))\n        else:\n            self.register_parameter('bias_ih', None)\n            self.register_parameter('bias_hh', None)\n        # self.reset_parameters()\n\n    def extra_repr(self):\n        s = '{input_size}, {hidden_size}'\n        if 'bias' in self.__dict__ and self.bias is not True:\n            s += ', bias={bias}'\n        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\n            s += ', nonlinearity={nonlinearity}'\n        return s.format(**self.__dict__)\n\n    def check_forward_input(self, input):\n        if input.size(1) != self.input_size:\n            raise RuntimeError(\n                \"input has inconsistent input_size: got {}, expected {}\".format(\n                    input.size(1), self.input_size))\n\n    def check_forward_hidden(self, input, hx, hidden_label=''):\n        if input.size(0) != hx.size(0):\n            raise RuntimeError(\n                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n                    input.size(0), hidden_label, hx.size(0)))\n\n        if hx.size(1) != self.hidden_size:\n            raise RuntimeError(\n                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\n                    hidden_label, hx.size(1), self.hidden_size))\n\n    def forward(self, input, hx=None):\n        self.check_forward_input(input)\n        if hx is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n        # set_trace()\n        self.check_forward_hidden(input, hx)\n\n        # set_trace()\n        # TODO take a look at the primitive implementation and implement this myself\n        if self.nonlinearity == 'relu':\n            if self.bias:\n            #    return  myReLU.apply(torch.mm(input, self.weight_ih.t()) + self.bias_ih\n            #                    + torch.mm(hx, self.weight_hh.t()) + self.bias_hh)\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t()) +\n                               self.bias_ih.expand(input.shape[0], self.bias_ih.shape[0]).contiguous() +\n                               torch.mm(hx, self.weight_hh.t())  +\n                               self.bias_hh.expand(input.shape[0], self.bias_hh.shape[0]).contiguous())\n            else:\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t())\n                                    + torch.mm(hx, self.weight_hh.t()))\n\nrnn3 = ReLURNN(input_size=1, hidden_size=20, bias=has_bias).cuda() \\\n        if use_gpu else ReLURNN(input_size=1, hidden_size=20, bias=has_bias)\nlinear3 = nn.Linear(20, 2, bias=has_bias).cuda() if use_gpu \\\n        else nn.Linear(20, 2, bias=has_bias)\n\nparam_init(rnn3, linear3)\n\n# run the net\nrnn3.zero_grad()\nlinear3.zero_grad()\n\noutput3 = []\nhx3 = deepcopy(hx_init)\nfor inp in input:\n    hx3 = rnn3(inp, hx3)\n    output3.append(hx3)\n\nlogit3 = linear3(output3[-1])\nloss3 = criterion(logit3, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params3 = torch.autograd.grad(loss3, rnn3.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm3 = 0\nfor idx in range(len(grad_params3)):\n    grad_norm3 += torch.norm(grad_params3[idx])\n\nprint('rnn3 - ReLURNN + myReLU')\nprint('rnn3 - loss: %f' % (loss3))\nprint('rnn3: sum of gradient norm is: %f' % (grad_norm3))\n\n# set_trace()</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\"> \u5728 2018\u5e749\u670820\u65e5\uff0c\u4e0b\u534812:25\uff0cThomas Viehmann ***@***.***&gt; \u5199\u9053\uff1a\n\n <a class=\"user-mention\" href=\"https://github.com/moonlightlane\">@moonlightlane</a> &lt;<a href=\"https://github.com/moonlightlane\">https://github.com/moonlightlane</a>&gt; We love to keep you on PyTorch. <g-emoji class=\"g-emoji\" alias=\"hugs\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f917.png\">\ud83e\udd17</g-emoji>\n You QG-Net-Readme looks awefully cool, but I didn't try it out.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"360032467\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11662\" href=\"https://github.com/pytorch/pytorch/issues/11662#issuecomment-423266027\">#11662 (comment)</a>&gt;, or mute the thread &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AIMJIiJFI4QaBzhcwEXXsF_36ZNINdKzks5uc8-DgaJpZM4WoDk3\">https://github.com/notifications/unsubscribe-auth/AIMJIiJFI4QaBzhcwEXXsF_36ZNINdKzks5uc8-DgaJpZM4WoDk3</a>&gt;.\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Thanks! That repo has a few data loading issues that someone has pointed out a while back\u2026 I was working on a deadline last few weeks so didn\u2019t have time to fix but am going to fix it in the next few days.\n\nI took a closer look at the inconsistent ReLU implementation in pytorch and cuda; For RNNs without bias, the custom ReLU implementation works fine, and the outputs are the same as those of nn.RNN.\n\nHowever, there seems to be numerical computation inconsistencies between pytorch matrix calculus implementation and the C implementation underlying both nn.RNNCell and nn.RNN. It is more obvious when bias is added in the RNN models. I have a piece of code to demonstrate this, see below. If you run with\n\npython3 rnn_test.py --use_cuda_relu -\u2014use_gpu -\u2014has_bias -\u2014seed 123456\n\nyou\u2019ll see that output of nn.RNN and the custom ReLURNN actually calculates different gradients (sum of norms of gradients are slightly different). In fact, some hidden states are also calculated differently, although only few of them.\n\nThe hidden states calculations are the same between nn.RNN and nn.RNNCell, although the backward pass are different (different gradients) because of the issue of different ReLU gradient you pointed out a while ago.\n\nYou can try with various flags (different seeds, whether has bias, whether using GPU, whether using the cuda-consistent ReLU implementation), and in a lot of cases the outputs between nn.RNN and the custom ReLURNN are different.\n\nShould I be concerned about the difference in numerical calculations? Or perhaps there is a bug in my code? I haven\u2019t tested on actually training on a dataset, though.\n\n\n\n\u2014\u2014\u2014\u2014\u2014\u2014 code (also in attachment) \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport random\nfrom copy import deepcopy\nimport argparse\nfrom pdb import set_trace\nimport math\n\nfrom pdb import set_trace\n\n\nparser = argparse.ArgumentParser(description=\"rnn cpu and gpu tests\")\nparser.add_argument('--seed', type=int, default=0)\nparser.add_argument('--use_gpu', action='store_true')\nparser.add_argument('--has_bias', action='store_true') # without this flag = no bias\nparser.add_argument('--use_cuda_relu', action='store_true') # without this flag = custom relu is the same as in pytorch; with flag = same as cuda\nseed = parser.parse_args().seed\nuse_gpu = parser.parse_args().use_gpu\nhas_bias = parser.parse_args().has_bias\nuse_cuda_relu= parser.parse_args().use_cuda_relu\n\nprint('use gpu.') if use_gpu else print('use cpu.')\nprint('use cuda relu') if use_cuda_relu else print('use pytorch relu')\n\ntorch.cuda.manual_seed(seed)\ntorch.manual_seed(seed)\nrandom.seed(seed)\n\n\n## manually create input, target, initial hidden state and criterion\n# dim = (seq_len, batch_size, input_size)\ninput = Variable(torch.randn(100, 64, 1)).cuda() if use_gpu \\\n        else Variable(torch.randn(100, 64, 1))\ntarget = Variable(torch.randint(low=0, high=1, size=(64, ),\n        dtype=torch.long)).cuda() if use_gpu else \\\n        Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long))\nhx_init = Variable(torch.randn(64, 20)).cuda() if use_gpu \\\n        else Variable(torch.randn(64, 20)) # dim = (batch_size, hidden_size)\ncriterion = nn.CrossEntropyLoss() # use cross entropy loss\n\n# define an init function\n# no bias and eye init to make sure two networks have the same parameters\ndef param_init(rnn, linear):\n    for name, param in rnn.named_parameters():\n        if 'weight' in name:\n            nn.init.eye_(param)\n        else:\n            nn.init.constant_(param, 1)\n    for name, param in linear.named_parameters():\n        if 'weight' in name:\n            nn.init.eye_(param)\n        else:\n            nn.init.constant_(param, 1)\n\n###########################################################################\n## first network, its output and rnn gradients\nrnn1 = nn.RNN(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\n        if use_gpu else nn.RNN(1, 20, nonlinearity='relu', bias=has_bias)\nlinear1 = nn.Linear(20, 2, bias=has_bias).cuda() \\\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\n\n# param init\nparam_init(rnn1, linear1)\n\n# run the net\nrnn1.zero_grad()\nlinear1.zero_grad()\n\noutput1, hx1 = rnn1(input, hx_init.unsqueeze(0))\nlogit1 = linear1(output1[-1])\nloss1 = criterion(logit1, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(),\n        create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm1 = 0\nfor idx in range(len(grad_params1)):\n    grad_norm1 += torch.norm(grad_params1[idx])\n\nprint('rnn1 - nn.RNN')\nprint('rnn1 - loss: %f' % (loss1))\nprint('rnn1 - sum of gradient norm is: %f' % (grad_norm1))\nprint('----')\n\n\n###########################################################################\n## second network, its output and rnn gradients\nrnn2 = nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\n        if use_gpu else nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias)\nlinear2 = nn.Linear(20, 2, bias=has_bias).cuda() \\\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\n\nparam_init(rnn2, linear2)\n\n# run the net\nrnn2.zero_grad()\nlinear2.zero_grad()\n\nhx2 = deepcopy(hx_init)\noutput2 = []\nfor i in range(100):\n    hx2 = rnn2(input[i], hx2)\n    output2.append(hx2)\nlogit2 = linear2(hx2)\nloss2 = criterion(logit2, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(),\n        create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm2 = 0\nfor idx in range(len(grad_params2)):\n    grad_norm2 += torch.norm(grad_params2[idx])\n\nprint('rnn2 - nn.RNNCell')\nprint('rnn2 - loss: %f' % (loss2))\nprint('rnn2: sum of gradient norm is: %f' % (grad_norm2))\nprint('----')\n\n\n###########################################################################\n## third network, its output and rnn gradients\n\nclass myReLU(torch.autograd.Function):\n    '''\n    custom RNN cell\n    '''\n    @staticmethod\n    def forward(ctx, x, relutype=use_cuda_relu):\n        if relutype:\n            ctx._mask = (x >= 0)\n        else:\n            ctx._mask = (x > 0)\n        return x.relu()\n    @staticmethod\n    def backward(ctx, grad_out):\n        return grad_out * ctx._mask.to(grad_out.dtype), None\n\nclass ReLURNN(nn.Module):\n    '''\n    a ReLU RNN cell with ReLU implementation consistent with cuda\n    '''\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity='relu'):\n        super(ReLURNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.nonlinearity = nonlinearity\n\n        self.weight_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n        if bias:\n            self.bias_ih = nn.Parameter(torch.Tensor(hidden_size))\n            self.bias_hh = nn.Parameter(torch.Tensor(hidden_size))\n        else:\n            self.register_parameter('bias_ih', None)\n            self.register_parameter('bias_hh', None)\n        # self.reset_parameters()\n\n    def extra_repr(self):\n        s = '{input_size}, {hidden_size}'\n        if 'bias' in self.__dict__ and self.bias is not True:\n            s += ', bias={bias}'\n        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\n            s += ', nonlinearity={nonlinearity}'\n        return s.format(**self.__dict__)\n\n    def check_forward_input(self, input):\n        if input.size(1) != self.input_size:\n            raise RuntimeError(\n                \"input has inconsistent input_size: got {}, expected {}\".format(\n                    input.size(1), self.input_size))\n\n    def check_forward_hidden(self, input, hx, hidden_label=''):\n        if input.size(0) != hx.size(0):\n            raise RuntimeError(\n                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\n                    input.size(0), hidden_label, hx.size(0)))\n\n        if hx.size(1) != self.hidden_size:\n            raise RuntimeError(\n                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\n                    hidden_label, hx.size(1), self.hidden_size))\n\n    def forward(self, input, hx=None):\n        self.check_forward_input(input)\n        if hx is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n        # set_trace()\n        self.check_forward_hidden(input, hx)\n\n        # set_trace()\n        # TODO take a look at the primitive implementation and implement this myself\n        if self.nonlinearity == 'relu':\n            if self.bias:\n            #    return  myReLU.apply(torch.mm(input, self.weight_ih.t()) + self.bias_ih\n            #                    + torch.mm(hx, self.weight_hh.t()) + self.bias_hh)\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t()) +\n                               self.bias_ih.expand(input.shape[0], self.bias_ih.shape[0]).contiguous() +\n                               torch.mm(hx, self.weight_hh.t())  +\n                               self.bias_hh.expand(input.shape[0], self.bias_hh.shape[0]).contiguous())\n            else:\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t())\n                                    + torch.mm(hx, self.weight_hh.t()))\n\nrnn3 = ReLURNN(input_size=1, hidden_size=20, bias=has_bias).cuda() \\\n        if use_gpu else ReLURNN(input_size=1, hidden_size=20, bias=has_bias)\nlinear3 = nn.Linear(20, 2, bias=has_bias).cuda() if use_gpu \\\n        else nn.Linear(20, 2, bias=has_bias)\n\nparam_init(rnn3, linear3)\n\n# run the net\nrnn3.zero_grad()\nlinear3.zero_grad()\n\noutput3 = []\nhx3 = deepcopy(hx_init)\nfor inp in input:\n    hx3 = rnn3(inp, hx3)\n    output3.append(hx3)\n\nlogit3 = linear3(output3[-1])\nloss3 = criterion(logit3, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params3 = torch.autograd.grad(loss3, rnn3.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm3 = 0\nfor idx in range(len(grad_params3)):\n    grad_norm3 += torch.norm(grad_params3[idx])\n\nprint('rnn3 - ReLURNN + myReLU')\nprint('rnn3 - loss: %f' % (loss3))\nprint('rnn3: sum of gradient norm is: %f' % (grad_norm3))\n\n# set_trace()\n\u2026\n \u5728 2018\u5e749\u670820\u65e5\uff0c\u4e0b\u534812:25\uff0cThomas Viehmann ***@***.***> \u5199\u9053\uff1a\n\n @moonlightlane <https://github.com/moonlightlane> We love to keep you on PyTorch. \ud83e\udd17\n You QG-Net-Readme looks awefully cool, but I didn't try it out.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub <#11662 (comment)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AIMJIiJFI4QaBzhcwEXXsF_36ZNINdKzks5uc8-DgaJpZM4WoDk3>.", "body": "Thanks! That repo has a few data loading issues that someone has pointed out a while back\u2026 I was working on a deadline last few weeks so didn\u2019t have time to fix but am going to fix it in the next few days.\r\n\r\nI took a closer look at the inconsistent ReLU implementation in pytorch and cuda; For RNNs without bias, the custom ReLU implementation works fine, and the outputs are the same as those of nn.RNN. \r\n\r\nHowever, there seems to be numerical computation inconsistencies between pytorch matrix calculus implementation and the C implementation underlying both nn.RNNCell and nn.RNN. It is more obvious when bias is added in the RNN models. I have a piece of code to demonstrate this, see below. If you run with \r\n\r\npython3 rnn_test.py --use_cuda_relu -\u2014use_gpu -\u2014has_bias -\u2014seed 123456\r\n\r\nyou\u2019ll see that output of nn.RNN and the custom ReLURNN actually calculates different gradients (sum of norms of gradients are slightly different). In fact, some hidden states are also calculated differently, although only few of them. \r\n\r\nThe hidden states calculations are the same between nn.RNN and nn.RNNCell, although the backward pass are different (different gradients) because of the issue of different ReLU gradient you pointed out a while ago.\r\n\r\nYou can try with various flags (different seeds, whether has bias, whether using GPU, whether using the cuda-consistent ReLU implementation), and in a lot of cases the outputs between nn.RNN and the custom ReLURNN are different.\r\n\r\nShould I be concerned about the difference in numerical calculations? Or perhaps there is a bug in my code? I haven\u2019t tested on actually training on a dataset, though. \r\n\r\n\r\n\r\n\u2014\u2014\u2014\u2014\u2014\u2014 code (also in attachment) \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 \r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport random\r\nfrom copy import deepcopy\r\nimport argparse\r\nfrom pdb import set_trace\r\nimport math\r\n\r\nfrom pdb import set_trace\r\n\r\n\r\nparser = argparse.ArgumentParser(description=\"rnn cpu and gpu tests\")\r\nparser.add_argument('--seed', type=int, default=0)\r\nparser.add_argument('--use_gpu', action='store_true')\r\nparser.add_argument('--has_bias', action='store_true') # without this flag = no bias\r\nparser.add_argument('--use_cuda_relu', action='store_true') # without this flag = custom relu is the same as in pytorch; with flag = same as cuda\r\nseed = parser.parse_args().seed\r\nuse_gpu = parser.parse_args().use_gpu\r\nhas_bias = parser.parse_args().has_bias\r\nuse_cuda_relu= parser.parse_args().use_cuda_relu\r\n\r\nprint('use gpu.') if use_gpu else print('use cpu.')\r\nprint('use cuda relu') if use_cuda_relu else print('use pytorch relu')\r\n\r\ntorch.cuda.manual_seed(seed)\r\ntorch.manual_seed(seed)\r\nrandom.seed(seed)\r\n\r\n\r\n## manually create input, target, initial hidden state and criterion\r\n# dim = (seq_len, batch_size, input_size)\r\ninput = Variable(torch.randn(100, 64, 1)).cuda() if use_gpu \\\r\n        else Variable(torch.randn(100, 64, 1))\r\ntarget = Variable(torch.randint(low=0, high=1, size=(64, ),\r\n        dtype=torch.long)).cuda() if use_gpu else \\\r\n        Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long))\r\nhx_init = Variable(torch.randn(64, 20)).cuda() if use_gpu \\\r\n        else Variable(torch.randn(64, 20)) # dim = (batch_size, hidden_size)\r\ncriterion = nn.CrossEntropyLoss() # use cross entropy loss\r\n\r\n# define an init function\r\n# no bias and eye init to make sure two networks have the same parameters\r\ndef param_init(rnn, linear):\r\n    for name, param in rnn.named_parameters():\r\n        if 'weight' in name:\r\n            nn.init.eye_(param)\r\n        else:\r\n            nn.init.constant_(param, 1)\r\n    for name, param in linear.named_parameters():\r\n        if 'weight' in name:\r\n            nn.init.eye_(param)\r\n        else:\r\n            nn.init.constant_(param, 1)\r\n\r\n###########################################################################\r\n## first network, its output and rnn gradients\r\nrnn1 = nn.RNN(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\r\n        if use_gpu else nn.RNN(1, 20, nonlinearity='relu', bias=has_bias)\r\nlinear1 = nn.Linear(20, 2, bias=has_bias).cuda() \\\r\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\r\n\r\n# param init\r\nparam_init(rnn1, linear1)\r\n\r\n# run the net\r\nrnn1.zero_grad()\r\nlinear1.zero_grad()\r\n\r\noutput1, hx1 = rnn1(input, hx_init.unsqueeze(0))\r\nlogit1 = linear1(output1[-1])\r\nloss1 = criterion(logit1, target)\r\n\r\n# calculate gradients and sum of gradient norms\r\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(),\r\n        create_graph=True, retain_graph=True, allow_unused=True)\r\ngrad_norm1 = 0\r\nfor idx in range(len(grad_params1)):\r\n    grad_norm1 += torch.norm(grad_params1[idx])\r\n\r\nprint('rnn1 - nn.RNN')\r\nprint('rnn1 - loss: %f' % (loss1))\r\nprint('rnn1 - sum of gradient norm is: %f' % (grad_norm1))\r\nprint('----')\r\n\r\n\r\n###########################################################################\r\n## second network, its output and rnn gradients\r\nrnn2 = nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias).cuda() \\\r\n        if use_gpu else nn.RNNCell(1, 20, nonlinearity='relu', bias=has_bias)\r\nlinear2 = nn.Linear(20, 2, bias=has_bias).cuda() \\\r\n        if use_gpu else nn.Linear(20, 2, bias=has_bias)\r\n\r\nparam_init(rnn2, linear2)\r\n\r\n# run the net\r\nrnn2.zero_grad()\r\nlinear2.zero_grad()\r\n\r\nhx2 = deepcopy(hx_init)\r\noutput2 = []\r\nfor i in range(100):\r\n    hx2 = rnn2(input[i], hx2)\r\n    output2.append(hx2)\r\nlogit2 = linear2(hx2)\r\nloss2 = criterion(logit2, target)\r\n\r\n# calculate gradients and sum of gradient norms\r\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(),\r\n        create_graph=True, retain_graph=True, allow_unused=True)\r\ngrad_norm2 = 0\r\nfor idx in range(len(grad_params2)):\r\n    grad_norm2 += torch.norm(grad_params2[idx])\r\n\r\nprint('rnn2 - nn.RNNCell')\r\nprint('rnn2 - loss: %f' % (loss2))\r\nprint('rnn2: sum of gradient norm is: %f' % (grad_norm2))\r\nprint('----')\r\n\r\n\r\n###########################################################################\r\n## third network, its output and rnn gradients\r\n\r\nclass myReLU(torch.autograd.Function):\r\n    '''\r\n    custom RNN cell\r\n    '''\r\n    @staticmethod\r\n    def forward(ctx, x, relutype=use_cuda_relu):\r\n        if relutype:\r\n            ctx._mask = (x >= 0)\r\n        else:\r\n            ctx._mask = (x > 0)\r\n        return x.relu()\r\n    @staticmethod\r\n    def backward(ctx, grad_out):\r\n        return grad_out * ctx._mask.to(grad_out.dtype), None\r\n\r\nclass ReLURNN(nn.Module):\r\n    '''\r\n    a ReLU RNN cell with ReLU implementation consistent with cuda\r\n    '''\r\n    def __init__(self, input_size, hidden_size, bias=True, nonlinearity='relu'):\r\n        super(ReLURNN, self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.bias = bias\r\n        self.nonlinearity = nonlinearity\r\n\r\n        self.weight_ih = nn.Parameter(torch.Tensor(hidden_size, input_size))\r\n        self.weight_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\r\n        if bias:\r\n            self.bias_ih = nn.Parameter(torch.Tensor(hidden_size))\r\n            self.bias_hh = nn.Parameter(torch.Tensor(hidden_size))\r\n        else:\r\n            self.register_parameter('bias_ih', None)\r\n            self.register_parameter('bias_hh', None)\r\n        # self.reset_parameters()\r\n\r\n    def extra_repr(self):\r\n        s = '{input_size}, {hidden_size}'\r\n        if 'bias' in self.__dict__ and self.bias is not True:\r\n            s += ', bias={bias}'\r\n        if 'nonlinearity' in self.__dict__ and self.nonlinearity != \"tanh\":\r\n            s += ', nonlinearity={nonlinearity}'\r\n        return s.format(**self.__dict__)\r\n\r\n    def check_forward_input(self, input):\r\n        if input.size(1) != self.input_size:\r\n            raise RuntimeError(\r\n                \"input has inconsistent input_size: got {}, expected {}\".format(\r\n                    input.size(1), self.input_size))\r\n\r\n    def check_forward_hidden(self, input, hx, hidden_label=''):\r\n        if input.size(0) != hx.size(0):\r\n            raise RuntimeError(\r\n                \"Input batch size {} doesn't match hidden{} batch size {}\".format(\r\n                    input.size(0), hidden_label, hx.size(0)))\r\n\r\n        if hx.size(1) != self.hidden_size:\r\n            raise RuntimeError(\r\n                \"hidden{} has inconsistent hidden_size: got {}, expected {}\".format(\r\n                    hidden_label, hx.size(1), self.hidden_size))\r\n\r\n    def forward(self, input, hx=None):\r\n        self.check_forward_input(input)\r\n        if hx is None:\r\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\r\n        # set_trace()\r\n        self.check_forward_hidden(input, hx)\r\n\r\n        # set_trace()\r\n        # TODO take a look at the primitive implementation and implement this myself\r\n        if self.nonlinearity == 'relu':\r\n            if self.bias:\r\n            #    return  myReLU.apply(torch.mm(input, self.weight_ih.t()) + self.bias_ih\r\n            #                    + torch.mm(hx, self.weight_hh.t()) + self.bias_hh)\r\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t()) +\r\n                               self.bias_ih.expand(input.shape[0], self.bias_ih.shape[0]).contiguous() +\r\n                               torch.mm(hx, self.weight_hh.t())  +\r\n                               self.bias_hh.expand(input.shape[0], self.bias_hh.shape[0]).contiguous())\r\n            else:\r\n                return  myReLU.apply(torch.mm(input, self.weight_ih.t())\r\n                                    + torch.mm(hx, self.weight_hh.t()))\r\n\r\nrnn3 = ReLURNN(input_size=1, hidden_size=20, bias=has_bias).cuda() \\\r\n        if use_gpu else ReLURNN(input_size=1, hidden_size=20, bias=has_bias)\r\nlinear3 = nn.Linear(20, 2, bias=has_bias).cuda() if use_gpu \\\r\n        else nn.Linear(20, 2, bias=has_bias)\r\n\r\nparam_init(rnn3, linear3)\r\n\r\n# run the net\r\nrnn3.zero_grad()\r\nlinear3.zero_grad()\r\n\r\noutput3 = []\r\nhx3 = deepcopy(hx_init)\r\nfor inp in input:\r\n    hx3 = rnn3(inp, hx3)\r\n    output3.append(hx3)\r\n\r\nlogit3 = linear3(output3[-1])\r\nloss3 = criterion(logit3, target)\r\n\r\n# calculate gradients and sum of gradient norms\r\ngrad_params3 = torch.autograd.grad(loss3, rnn3.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\r\ngrad_norm3 = 0\r\nfor idx in range(len(grad_params3)):\r\n    grad_norm3 += torch.norm(grad_params3[idx])\r\n\r\nprint('rnn3 - ReLURNN + myReLU')\r\nprint('rnn3 - loss: %f' % (loss3))\r\nprint('rnn3: sum of gradient norm is: %f' % (grad_norm3))\r\n\r\n# set_trace()\r\n\r\n\r\n\r\n> \u5728 2018\u5e749\u670820\u65e5\uff0c\u4e0b\u534812:25\uff0cThomas Viehmann <notifications@github.com> \u5199\u9053\uff1a\r\n> \r\n> @moonlightlane <https://github.com/moonlightlane> We love to keep you on PyTorch. \ud83e\udd17\r\n> You QG-Net-Readme looks awefully cool, but I didn't try it out.\r\n> \r\n> \u2014\r\n> You are receiving this because you were mentioned.\r\n> Reply to this email directly, view it on GitHub <https://github.com/pytorch/pytorch/issues/11662#issuecomment-423266027>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AIMJIiJFI4QaBzhcwEXXsF_36ZNINdKzks5uc8-DgaJpZM4WoDk3>.\r\n> \r\n\r\n"}