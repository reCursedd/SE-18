{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423138052", "html_url": "https://github.com/pytorch/pytorch/issues/11662#issuecomment-423138052", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11662", "id": 423138052, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzEzODA1Mg==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T10:50:19Z", "updated_at": "2018-09-20T16:10:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a> No, it's not a bug:</p>\n<p>It's a corner case of whether the grad of relu at 0 should be 0 or 1. CuDNN uses 1, we use 0.</p>\n<pre><code>import torch\n\ntorch.backends.cudnn.enabled = True\ntorch.cuda.manual_seed(0)\ntorch.manual_seed(0)\ndevice = 'cuda'\nT = 2\nbs = 1\nH = 3\n\ninput = torch.randn(T, bs, 1, device=device)\nhx0 = torch.randn(bs, H, device=device)\nrnn1 = torch.nn.RNNCell(1, H, nonlinearity=nonlinearity, bias=False).to(device)\nnn.init.eye_(rnn1.weight_hh)\nnn.init.eye_(rnn1.weight_ih)\n\nhx1 = hx0\noutput1 = []\nfor i in range(T):\n    hx1 = rnn1(input[i], hx1)\nloss1 = hx1.sum()\n\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\n\n\ngrad_norm1 = 0\nfor idx in range(len(grad_params1)):\n    grad_norm1 += torch.norm(grad_params1[idx])\nprint('rnn1 - loss: %f' % (loss1))\nprint('rnn1 -  sum of gradient norm is: %f' % (grad_norm1))\nprint('---')\n\nrnn2 = torch.nn.RNN(1, H, nonlinearity=nonlinearity, bias=False).to(device)\nwith torch.no_grad():\n    rnn2.weight_hh_l0.copy_(rnn1.weight_hh)\n    rnn2.weight_ih_l0.copy_(rnn1.weight_ih)\n\noutput2, hx2 = rnn2(input, hx0.unsqueeze(0))\n\nloss2 = hx2.sum()\n\n# calculate gradients and sum of gradient norms\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm2 = 0\nfor idx in range(len(grad_params2)):\n    grad_norm2 += torch.norm(grad_params2[idx])\nprint('rnn2 - loss: %f' % (loss2))\nprint('rnn2 - sum of gradient norm is: %f' % (grad_norm2))\n\n\nclass myReLU(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, relutype):\n        if relutype:\n            ctx._mask = (x &gt;= 0)\n        else:\n            ctx._mask = (x &gt; 0)\n        return x.relu()\n    @staticmethod\n    def backward(ctx, grad_out):\n        return grad_out * ctx._mask.to(grad_out.dtype), None\n\ndef myrnn(input, hx, w_ih, w_hh, relutype=False):\n    for aninp in input:\n        i_ = torch.mm(aninp, w_ih.t())\n        h_ = torch.mm(hx, w_hh.t())\n        hx = myReLU.apply(i_ + h_, relutype)\n    return hx\n\nw_ih = rnn1.weight_ih.detach().requires_grad_()\nw_hh = rnn1.weight_hh.detach().requires_grad_()\n\nloss3 = myrnn(input, hx0, w_ih, w_hh).sum()\ngrads3 = torch.autograd.grad(loss3, [w_ih, w_hh])\nprint (\"3:\",loss3.item(),\"-\",(torch.norm(grads3[0])+torch.norm(grads3[1]).sum()).item())\nloss4 = myrnn(input, hx0, w_ih, w_hh, True).sum()\ngrads4 = torch.autograd.grad(loss4, [w_ih, w_hh])\nprint (\"4:\", loss4.item(), \"-\",(torch.norm(grads4[0])+torch.norm(grads4[1]).sum()).item())\n</code></pre>", "body_text": "@SsnL No, it's not a bug:\nIt's a corner case of whether the grad of relu at 0 should be 0 or 1. CuDNN uses 1, we use 0.\nimport torch\n\ntorch.backends.cudnn.enabled = True\ntorch.cuda.manual_seed(0)\ntorch.manual_seed(0)\ndevice = 'cuda'\nT = 2\nbs = 1\nH = 3\n\ninput = torch.randn(T, bs, 1, device=device)\nhx0 = torch.randn(bs, H, device=device)\nrnn1 = torch.nn.RNNCell(1, H, nonlinearity=nonlinearity, bias=False).to(device)\nnn.init.eye_(rnn1.weight_hh)\nnn.init.eye_(rnn1.weight_ih)\n\nhx1 = hx0\noutput1 = []\nfor i in range(T):\n    hx1 = rnn1(input[i], hx1)\nloss1 = hx1.sum()\n\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\n\n\ngrad_norm1 = 0\nfor idx in range(len(grad_params1)):\n    grad_norm1 += torch.norm(grad_params1[idx])\nprint('rnn1 - loss: %f' % (loss1))\nprint('rnn1 -  sum of gradient norm is: %f' % (grad_norm1))\nprint('---')\n\nrnn2 = torch.nn.RNN(1, H, nonlinearity=nonlinearity, bias=False).to(device)\nwith torch.no_grad():\n    rnn2.weight_hh_l0.copy_(rnn1.weight_hh)\n    rnn2.weight_ih_l0.copy_(rnn1.weight_ih)\n\noutput2, hx2 = rnn2(input, hx0.unsqueeze(0))\n\nloss2 = hx2.sum()\n\n# calculate gradients and sum of gradient norms\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm2 = 0\nfor idx in range(len(grad_params2)):\n    grad_norm2 += torch.norm(grad_params2[idx])\nprint('rnn2 - loss: %f' % (loss2))\nprint('rnn2 - sum of gradient norm is: %f' % (grad_norm2))\n\n\nclass myReLU(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, relutype):\n        if relutype:\n            ctx._mask = (x >= 0)\n        else:\n            ctx._mask = (x > 0)\n        return x.relu()\n    @staticmethod\n    def backward(ctx, grad_out):\n        return grad_out * ctx._mask.to(grad_out.dtype), None\n\ndef myrnn(input, hx, w_ih, w_hh, relutype=False):\n    for aninp in input:\n        i_ = torch.mm(aninp, w_ih.t())\n        h_ = torch.mm(hx, w_hh.t())\n        hx = myReLU.apply(i_ + h_, relutype)\n    return hx\n\nw_ih = rnn1.weight_ih.detach().requires_grad_()\nw_hh = rnn1.weight_hh.detach().requires_grad_()\n\nloss3 = myrnn(input, hx0, w_ih, w_hh).sum()\ngrads3 = torch.autograd.grad(loss3, [w_ih, w_hh])\nprint (\"3:\",loss3.item(),\"-\",(torch.norm(grads3[0])+torch.norm(grads3[1]).sum()).item())\nloss4 = myrnn(input, hx0, w_ih, w_hh, True).sum()\ngrads4 = torch.autograd.grad(loss4, [w_ih, w_hh])\nprint (\"4:\", loss4.item(), \"-\",(torch.norm(grads4[0])+torch.norm(grads4[1]).sum()).item())", "body": "@SsnL No, it's not a bug:\r\n\r\nIt's a corner case of whether the grad of relu at 0 should be 0 or 1. CuDNN uses 1, we use 0.\r\n```\r\nimport torch\r\n\r\ntorch.backends.cudnn.enabled = True\r\ntorch.cuda.manual_seed(0)\r\ntorch.manual_seed(0)\r\ndevice = 'cuda'\r\nT = 2\r\nbs = 1\r\nH = 3\r\n\r\ninput = torch.randn(T, bs, 1, device=device)\r\nhx0 = torch.randn(bs, H, device=device)\r\nrnn1 = torch.nn.RNNCell(1, H, nonlinearity=nonlinearity, bias=False).to(device)\r\nnn.init.eye_(rnn1.weight_hh)\r\nnn.init.eye_(rnn1.weight_ih)\r\n\r\nhx1 = hx0\r\noutput1 = []\r\nfor i in range(T):\r\n    hx1 = rnn1(input[i], hx1)\r\nloss1 = hx1.sum()\r\n\r\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\r\n\r\n\r\ngrad_norm1 = 0\r\nfor idx in range(len(grad_params1)):\r\n    grad_norm1 += torch.norm(grad_params1[idx])\r\nprint('rnn1 - loss: %f' % (loss1))\r\nprint('rnn1 -  sum of gradient norm is: %f' % (grad_norm1))\r\nprint('---')\r\n\r\nrnn2 = torch.nn.RNN(1, H, nonlinearity=nonlinearity, bias=False).to(device)\r\nwith torch.no_grad():\r\n    rnn2.weight_hh_l0.copy_(rnn1.weight_hh)\r\n    rnn2.weight_ih_l0.copy_(rnn1.weight_ih)\r\n\r\noutput2, hx2 = rnn2(input, hx0.unsqueeze(0))\r\n\r\nloss2 = hx2.sum()\r\n\r\n# calculate gradients and sum of gradient norms\r\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\r\ngrad_norm2 = 0\r\nfor idx in range(len(grad_params2)):\r\n    grad_norm2 += torch.norm(grad_params2[idx])\r\nprint('rnn2 - loss: %f' % (loss2))\r\nprint('rnn2 - sum of gradient norm is: %f' % (grad_norm2))\r\n\r\n\r\nclass myReLU(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x, relutype):\r\n        if relutype:\r\n            ctx._mask = (x >= 0)\r\n        else:\r\n            ctx._mask = (x > 0)\r\n        return x.relu()\r\n    @staticmethod\r\n    def backward(ctx, grad_out):\r\n        return grad_out * ctx._mask.to(grad_out.dtype), None\r\n\r\ndef myrnn(input, hx, w_ih, w_hh, relutype=False):\r\n    for aninp in input:\r\n        i_ = torch.mm(aninp, w_ih.t())\r\n        h_ = torch.mm(hx, w_hh.t())\r\n        hx = myReLU.apply(i_ + h_, relutype)\r\n    return hx\r\n\r\nw_ih = rnn1.weight_ih.detach().requires_grad_()\r\nw_hh = rnn1.weight_hh.detach().requires_grad_()\r\n\r\nloss3 = myrnn(input, hx0, w_ih, w_hh).sum()\r\ngrads3 = torch.autograd.grad(loss3, [w_ih, w_hh])\r\nprint (\"3:\",loss3.item(),\"-\",(torch.norm(grads3[0])+torch.norm(grads3[1]).sum()).item())\r\nloss4 = myrnn(input, hx0, w_ih, w_hh, True).sum()\r\ngrads4 = torch.autograd.grad(loss4, [w_ih, w_hh])\r\nprint (\"4:\", loss4.item(), \"-\",(torch.norm(grads4[0])+torch.norm(grads4[1]).sum()).item())\r\n```\r\n"}