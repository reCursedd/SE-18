{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11662", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11662/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11662/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11662/events", "html_url": "https://github.com/pytorch/pytorch/issues/11662", "id": 360032467, "node_id": "MDU6SXNzdWUzNjAwMzI0Njc=", "number": 11662, "title": "Wrong gradient in RNNCell with ReLU?", "user": {"login": "moonlightlane", "id": 8587554, "node_id": "MDQ6VXNlcjg1ODc1NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/8587554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/moonlightlane", "html_url": "https://github.com/moonlightlane", "followers_url": "https://api.github.com/users/moonlightlane/followers", "following_url": "https://api.github.com/users/moonlightlane/following{/other_user}", "gists_url": "https://api.github.com/users/moonlightlane/gists{/gist_id}", "starred_url": "https://api.github.com/users/moonlightlane/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/moonlightlane/subscriptions", "organizations_url": "https://api.github.com/users/moonlightlane/orgs", "repos_url": "https://api.github.com/users/moonlightlane/repos", "events_url": "https://api.github.com/users/moonlightlane/events{/privacy}", "received_events_url": "https://api.github.com/users/moonlightlane/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-09-13T19:24:24Z", "updated_at": "2018-10-08T06:48:32Z", "closed_at": "2018-09-20T15:45:26Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>It seems that <code>nn.RNNCell</code> + ReLU and <code>nn.RNN</code> + ReLU + CPU do not calculate gradient properly. This can be observed from the different gradients calculated by RNNs constructed with <code>nn.RNNCell</code> + ReLU on CPU and on GPU, and <code>nn.RNN</code> + ReLU on CPU and GPU. Of these 4 configurations, the gradients from <code>nn.RNN</code> + ReLU + GPU, which I think is the correct gradient, are different from the gradients calculated by the other three configurations.</p>\n<p>All other RNN modules (<code>nn.LSTM</code>, <code>nn.GRU</code>, <code>nn.LSTMCell</code>, <code>nn.GRUCell</code>) do not have this problem. It could be due to a bug in my test script, but a few of my friends have verified and reproduced this problem, so I'm opening an issue here.</p>\n<h2>Code example</h2>\n<p>See below. If you save the script as <code>rnn_test.py</code>, you can and run it with: <code>python3 rnn_test.py --model &lt;model&gt; --nonlinearity &lt;nonlinearity&gt; --use_gpu</code></p>\n<p>For example:</p>\n<pre><code>python3 rnn_test.py --model rnn --nonlinearity relu --use_gpu\npython3 rnn_test.py --model rnn --nonlinearity relu\n</code></pre>\n<p>The first line will give the following results:</p>\n<pre><code>use gpu.\nrnn1 - loss: 0.182044\nrnn1 -  sum of gradient norm is: 22.836142\n---\nrnn2 - loss: 0.182044\nrnn2 - sum of gradient norm is: 44.010647\n</code></pre>\n<p>The second line will give the following results:</p>\n<pre><code>use cpu.\nrnn1 - loss: 0.182044\nrnn1 -  sum of gradient norm is: 22.836143\n---\nrnn2 - loss: 0.182044\nrnn2 - sum of gradient norm is: 22.836143\n</code></pre>\n<p>Here, <code>rnn1</code> is the RNN constructed with <code>nn.RNNCell</code>, and <code>rnn2</code> is RNN constructed with <code>nn.RNN</code>. As you can see, <code>nn.RNN</code> gives different gradients when running on CPU and on GPU. But I think the gradient when it runs on GPU is the correct one (<code>44.010647</code>), which suggests that <code>nn.RNNCell</code> running on either CPU or GPU and <code>nn.RNN</code> running on CPU give the wrong gradients.</p>\n<p>Code example:</p>\n<pre><code>import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport random\nfrom copy import deepcopy\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"rnn cpu and gpu tests\")\nparser.add_argument('--use_gpu', action='store_true')\nparser.add_argument('--model', type=str, default='rnn', choices=['rnn', 'gru', 'lstm'])\nparser.add_argument('--nonlinearity', type=str, default='relu', choices=['relu', 'tanh'])\nuse_gpu = parser.parse_args().use_gpu\nmodel = parser.parse_args().model\nnonlinearity = parser.parse_args().nonlinearity\nprint('use gpu.') if use_gpu else print('use cpu.')\n\ntorch.cuda.manual_seed(0)\ntorch.manual_seed(0)\nrandom.seed(0)\n\n## manually create input, target, initial hidden state and criterion\ninput = Variable(torch.randn(100, 64, 1).cuda()) if use_gpu else Variable(torch.randn(100, 64, 1)) # dim = (seq_len, batch_size, input_size)\ntarget = Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long).cuda()) if use_gpu else Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long))\nhx0 = Variable(torch.randn(64, 20).cuda()) if use_gpu else Variable(torch.randn(64, 20)) # dim = (batch_size, hidden_size)\nif model == 'lstm':\n    c0 = Variable(torch.zeros(64, 20).cuda()) if use_gpu else Variable(torch.zeros(64, 20)) # dim = (batch_size, hidden_size)\ncriterion = nn.CrossEntropyLoss() # use cross entropy loss\n\n\n## first network, its output and rnn gradients\nif model=='rnn':\n    rnn1 = nn.RNNCell(1, 20, nonlinearity=nonlinearity, bias=False).cuda() if use_gpu else nn.RNNCell(1, 20, nonlinearity=nonlinearity, bias=False)\nelif model=='gru':\n    rnn1 = nn.GRUCell(1, 20, bias=False).cuda() if use_gpu else nn.GRUCell(1, 20, bias=False)\nelif model=='lstm':\n    rnn1 = nn.LSTMCell(1, 20, bias=False).cuda() if use_gpu else nn.LSTMCell(1, 20, bias=False)\nlinear1 = nn.Linear(20, 2, bias=False).cuda() if use_gpu else nn.Linear(20, 2, bias=False)\n\n# no bias and eye init to make sure two networks have the same parameters\nfor name, param in rnn1.named_parameters():\n    if 'weight' in name:\n        nn.init.eye_(param)\nfor name, param in linear1.named_parameters():\n    if 'weight' in name:\n        nn.init.eye_(param)\n\n# run the net\nhx1 = deepcopy(hx0)\nif model=='lstm':\n    c1 = deepcopy(c0)\noutput1 = []\nfor i in range(100):\n    if model != 'lstm':\n        hx1 = rnn1(input[i], hx1)\n    else:\n        hx1, c1 = rnn1(input[i], (hx1, c1))\n    output1.append(hx1)\n\nlogit1 = linear1(hx1)\nloss1 = criterion(logit1, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm1 = 0\nfor idx in range(len(grad_params1)):\n    grad_norm1 += torch.norm(grad_params1[idx])\nprint('rnn1 - loss: %f' % (loss1))\nprint('rnn1 -  sum of gradient norm is: %f' % (grad_norm1))\nprint('---')\n\n## second network, its output and rnn gradients\n## first network, its output and rnn gradients\nif model=='rnn':\n    rnn2 = nn.RNN(1, 20, nonlinearity=nonlinearity, bias=False).cuda() if use_gpu else nn.RNN(1, 20, nonlinearity=nonlinearity, bias=False)\nelif model=='gru':\n    rnn2 = nn.GRU(1, 20, bias=False).cuda() if use_gpu else nn.GRU(1, 20, bias=False)\nelif model=='lstm':\n    rnn2 = nn.LSTM(1, 20, bias=False).cuda() if use_gpu else nn.LSTM(1, 20, bias=False)\nlinear2 = nn.Linear(20, 2, bias=False).cuda() if use_gpu else nn.Linear(20, 2, bias=False)\n\n# same init as the first network\nfor name, param in rnn2.named_parameters():\n    if 'weight' in name:\n        nn.init.eye_(param)\nfor name, param in linear2.named_parameters():\n    if 'weight' in name:\n        nn.init.eye_(param)\n        \n# run the net \nif model != 'lstm':\n    output2, hx2 = rnn2(input, hx0.unsqueeze(0))\nelse:\n    output2, (hx2, _) = rnn2(input, (hx0.unsqueeze(0), c0.unsqueeze(0)))\nlogit2 = linear2(hx2[-1])\nloss2 = criterion(logit2, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm2 = 0\nfor idx in range(len(grad_params2)):\n    grad_norm2 += torch.norm(grad_params2[idx])\nprint('rnn2 - loss: %f' % (loss2))\nprint('rnn2 - sum of gradient norm is: %f' % (grad_norm2))\n</code></pre>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2: pyTorch</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS: ubuntu 16.04</li>\n<li>PyTorch version: 0.4.1</li>\n<li>Python version: 3.5.2</li>\n<li>CUDA/cuDNN version: 8.0</li>\n<li>GPU models and configuration: TITAN X</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version: 3.5.1</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "Issue description\nIt seems that nn.RNNCell + ReLU and nn.RNN + ReLU + CPU do not calculate gradient properly. This can be observed from the different gradients calculated by RNNs constructed with nn.RNNCell + ReLU on CPU and on GPU, and nn.RNN + ReLU on CPU and GPU. Of these 4 configurations, the gradients from nn.RNN + ReLU + GPU, which I think is the correct gradient, are different from the gradients calculated by the other three configurations.\nAll other RNN modules (nn.LSTM, nn.GRU, nn.LSTMCell, nn.GRUCell) do not have this problem. It could be due to a bug in my test script, but a few of my friends have verified and reproduced this problem, so I'm opening an issue here.\nCode example\nSee below. If you save the script as rnn_test.py, you can and run it with: python3 rnn_test.py --model <model> --nonlinearity <nonlinearity> --use_gpu\nFor example:\npython3 rnn_test.py --model rnn --nonlinearity relu --use_gpu\npython3 rnn_test.py --model rnn --nonlinearity relu\n\nThe first line will give the following results:\nuse gpu.\nrnn1 - loss: 0.182044\nrnn1 -  sum of gradient norm is: 22.836142\n---\nrnn2 - loss: 0.182044\nrnn2 - sum of gradient norm is: 44.010647\n\nThe second line will give the following results:\nuse cpu.\nrnn1 - loss: 0.182044\nrnn1 -  sum of gradient norm is: 22.836143\n---\nrnn2 - loss: 0.182044\nrnn2 - sum of gradient norm is: 22.836143\n\nHere, rnn1 is the RNN constructed with nn.RNNCell, and rnn2 is RNN constructed with nn.RNN. As you can see, nn.RNN gives different gradients when running on CPU and on GPU. But I think the gradient when it runs on GPU is the correct one (44.010647), which suggests that nn.RNNCell running on either CPU or GPU and nn.RNN running on CPU give the wrong gradients.\nCode example:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport random\nfrom copy import deepcopy\nimport argparse\n\nparser = argparse.ArgumentParser(description=\"rnn cpu and gpu tests\")\nparser.add_argument('--use_gpu', action='store_true')\nparser.add_argument('--model', type=str, default='rnn', choices=['rnn', 'gru', 'lstm'])\nparser.add_argument('--nonlinearity', type=str, default='relu', choices=['relu', 'tanh'])\nuse_gpu = parser.parse_args().use_gpu\nmodel = parser.parse_args().model\nnonlinearity = parser.parse_args().nonlinearity\nprint('use gpu.') if use_gpu else print('use cpu.')\n\ntorch.cuda.manual_seed(0)\ntorch.manual_seed(0)\nrandom.seed(0)\n\n## manually create input, target, initial hidden state and criterion\ninput = Variable(torch.randn(100, 64, 1).cuda()) if use_gpu else Variable(torch.randn(100, 64, 1)) # dim = (seq_len, batch_size, input_size)\ntarget = Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long).cuda()) if use_gpu else Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long))\nhx0 = Variable(torch.randn(64, 20).cuda()) if use_gpu else Variable(torch.randn(64, 20)) # dim = (batch_size, hidden_size)\nif model == 'lstm':\n    c0 = Variable(torch.zeros(64, 20).cuda()) if use_gpu else Variable(torch.zeros(64, 20)) # dim = (batch_size, hidden_size)\ncriterion = nn.CrossEntropyLoss() # use cross entropy loss\n\n\n## first network, its output and rnn gradients\nif model=='rnn':\n    rnn1 = nn.RNNCell(1, 20, nonlinearity=nonlinearity, bias=False).cuda() if use_gpu else nn.RNNCell(1, 20, nonlinearity=nonlinearity, bias=False)\nelif model=='gru':\n    rnn1 = nn.GRUCell(1, 20, bias=False).cuda() if use_gpu else nn.GRUCell(1, 20, bias=False)\nelif model=='lstm':\n    rnn1 = nn.LSTMCell(1, 20, bias=False).cuda() if use_gpu else nn.LSTMCell(1, 20, bias=False)\nlinear1 = nn.Linear(20, 2, bias=False).cuda() if use_gpu else nn.Linear(20, 2, bias=False)\n\n# no bias and eye init to make sure two networks have the same parameters\nfor name, param in rnn1.named_parameters():\n    if 'weight' in name:\n        nn.init.eye_(param)\nfor name, param in linear1.named_parameters():\n    if 'weight' in name:\n        nn.init.eye_(param)\n\n# run the net\nhx1 = deepcopy(hx0)\nif model=='lstm':\n    c1 = deepcopy(c0)\noutput1 = []\nfor i in range(100):\n    if model != 'lstm':\n        hx1 = rnn1(input[i], hx1)\n    else:\n        hx1, c1 = rnn1(input[i], (hx1, c1))\n    output1.append(hx1)\n\nlogit1 = linear1(hx1)\nloss1 = criterion(logit1, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm1 = 0\nfor idx in range(len(grad_params1)):\n    grad_norm1 += torch.norm(grad_params1[idx])\nprint('rnn1 - loss: %f' % (loss1))\nprint('rnn1 -  sum of gradient norm is: %f' % (grad_norm1))\nprint('---')\n\n## second network, its output and rnn gradients\n## first network, its output and rnn gradients\nif model=='rnn':\n    rnn2 = nn.RNN(1, 20, nonlinearity=nonlinearity, bias=False).cuda() if use_gpu else nn.RNN(1, 20, nonlinearity=nonlinearity, bias=False)\nelif model=='gru':\n    rnn2 = nn.GRU(1, 20, bias=False).cuda() if use_gpu else nn.GRU(1, 20, bias=False)\nelif model=='lstm':\n    rnn2 = nn.LSTM(1, 20, bias=False).cuda() if use_gpu else nn.LSTM(1, 20, bias=False)\nlinear2 = nn.Linear(20, 2, bias=False).cuda() if use_gpu else nn.Linear(20, 2, bias=False)\n\n# same init as the first network\nfor name, param in rnn2.named_parameters():\n    if 'weight' in name:\n        nn.init.eye_(param)\nfor name, param in linear2.named_parameters():\n    if 'weight' in name:\n        nn.init.eye_(param)\n        \n# run the net \nif model != 'lstm':\n    output2, hx2 = rnn2(input, hx0.unsqueeze(0))\nelse:\n    output2, (hx2, _) = rnn2(input, (hx0.unsqueeze(0), c0.unsqueeze(0)))\nlogit2 = linear2(hx2[-1])\nloss2 = criterion(logit2, target)\n\n# calculate gradients and sum of gradient norms\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\ngrad_norm2 = 0\nfor idx in range(len(grad_params2)):\n    grad_norm2 += torch.norm(grad_params2[idx])\nprint('rnn2 - loss: %f' % (loss2))\nprint('rnn2 - sum of gradient norm is: %f' % (grad_norm2))\n\nSystem Info\n\nPyTorch or Caffe2: pyTorch\nHow you installed PyTorch (conda, pip, source): pip\nBuild command you used (if compiling from source):\nOS: ubuntu 16.04\nPyTorch version: 0.4.1\nPython version: 3.5.2\nCUDA/cuDNN version: 8.0\nGPU models and configuration: TITAN X\nGCC version (if compiling from source):\nCMake version: 3.5.1\nVersions of any other relevant libraries:", "body": "## Issue description\r\n\r\nIt seems that `nn.RNNCell` + ReLU and `nn.RNN` + ReLU + CPU do not calculate gradient properly. This can be observed from the different gradients calculated by RNNs constructed with `nn.RNNCell` + ReLU on CPU and on GPU, and `nn.RNN` + ReLU on CPU and GPU. Of these 4 configurations, the gradients from `nn.RNN` + ReLU + GPU, which I think is the correct gradient, are different from the gradients calculated by the other three configurations. \r\n\r\nAll other RNN modules (`nn.LSTM`, `nn.GRU`, `nn.LSTMCell`, `nn.GRUCell`) do not have this problem. It could be due to a bug in my test script, but a few of my friends have verified and reproduced this problem, so I'm opening an issue here.\r\n\r\n## Code example\r\nSee below. If you save the script as `rnn_test.py`, you can and run it with: `python3 rnn_test.py --model <model> --nonlinearity <nonlinearity> --use_gpu`\r\n\r\nFor example:\r\n```\r\npython3 rnn_test.py --model rnn --nonlinearity relu --use_gpu\r\npython3 rnn_test.py --model rnn --nonlinearity relu\r\n```\r\nThe first line will give the following results:\r\n```\r\nuse gpu.\r\nrnn1 - loss: 0.182044\r\nrnn1 -  sum of gradient norm is: 22.836142\r\n---\r\nrnn2 - loss: 0.182044\r\nrnn2 - sum of gradient norm is: 44.010647\r\n```\r\nThe second line will give the following results:\r\n```\r\nuse cpu.\r\nrnn1 - loss: 0.182044\r\nrnn1 -  sum of gradient norm is: 22.836143\r\n---\r\nrnn2 - loss: 0.182044\r\nrnn2 - sum of gradient norm is: 22.836143\r\n```\r\nHere, `rnn1` is the RNN constructed with `nn.RNNCell`, and `rnn2` is RNN constructed with `nn.RNN`. As you can see, `nn.RNN` gives different gradients when running on CPU and on GPU. But I think the gradient when it runs on GPU is the correct one (`44.010647`), which suggests that `nn.RNNCell` running on either CPU or GPU and `nn.RNN` running on CPU give the wrong gradients.\r\n\r\nCode example:\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport random\r\nfrom copy import deepcopy\r\nimport argparse\r\n\r\nparser = argparse.ArgumentParser(description=\"rnn cpu and gpu tests\")\r\nparser.add_argument('--use_gpu', action='store_true')\r\nparser.add_argument('--model', type=str, default='rnn', choices=['rnn', 'gru', 'lstm'])\r\nparser.add_argument('--nonlinearity', type=str, default='relu', choices=['relu', 'tanh'])\r\nuse_gpu = parser.parse_args().use_gpu\r\nmodel = parser.parse_args().model\r\nnonlinearity = parser.parse_args().nonlinearity\r\nprint('use gpu.') if use_gpu else print('use cpu.')\r\n\r\ntorch.cuda.manual_seed(0)\r\ntorch.manual_seed(0)\r\nrandom.seed(0)\r\n\r\n## manually create input, target, initial hidden state and criterion\r\ninput = Variable(torch.randn(100, 64, 1).cuda()) if use_gpu else Variable(torch.randn(100, 64, 1)) # dim = (seq_len, batch_size, input_size)\r\ntarget = Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long).cuda()) if use_gpu else Variable(torch.randint(low=0, high=1, size=(64, ), dtype=torch.long))\r\nhx0 = Variable(torch.randn(64, 20).cuda()) if use_gpu else Variable(torch.randn(64, 20)) # dim = (batch_size, hidden_size)\r\nif model == 'lstm':\r\n    c0 = Variable(torch.zeros(64, 20).cuda()) if use_gpu else Variable(torch.zeros(64, 20)) # dim = (batch_size, hidden_size)\r\ncriterion = nn.CrossEntropyLoss() # use cross entropy loss\r\n\r\n\r\n## first network, its output and rnn gradients\r\nif model=='rnn':\r\n    rnn1 = nn.RNNCell(1, 20, nonlinearity=nonlinearity, bias=False).cuda() if use_gpu else nn.RNNCell(1, 20, nonlinearity=nonlinearity, bias=False)\r\nelif model=='gru':\r\n    rnn1 = nn.GRUCell(1, 20, bias=False).cuda() if use_gpu else nn.GRUCell(1, 20, bias=False)\r\nelif model=='lstm':\r\n    rnn1 = nn.LSTMCell(1, 20, bias=False).cuda() if use_gpu else nn.LSTMCell(1, 20, bias=False)\r\nlinear1 = nn.Linear(20, 2, bias=False).cuda() if use_gpu else nn.Linear(20, 2, bias=False)\r\n\r\n# no bias and eye init to make sure two networks have the same parameters\r\nfor name, param in rnn1.named_parameters():\r\n    if 'weight' in name:\r\n        nn.init.eye_(param)\r\nfor name, param in linear1.named_parameters():\r\n    if 'weight' in name:\r\n        nn.init.eye_(param)\r\n\r\n# run the net\r\nhx1 = deepcopy(hx0)\r\nif model=='lstm':\r\n    c1 = deepcopy(c0)\r\noutput1 = []\r\nfor i in range(100):\r\n    if model != 'lstm':\r\n        hx1 = rnn1(input[i], hx1)\r\n    else:\r\n        hx1, c1 = rnn1(input[i], (hx1, c1))\r\n    output1.append(hx1)\r\n\r\nlogit1 = linear1(hx1)\r\nloss1 = criterion(logit1, target)\r\n\r\n# calculate gradients and sum of gradient norms\r\ngrad_params1 = torch.autograd.grad(loss1, rnn1.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\r\ngrad_norm1 = 0\r\nfor idx in range(len(grad_params1)):\r\n    grad_norm1 += torch.norm(grad_params1[idx])\r\nprint('rnn1 - loss: %f' % (loss1))\r\nprint('rnn1 -  sum of gradient norm is: %f' % (grad_norm1))\r\nprint('---')\r\n\r\n## second network, its output and rnn gradients\r\n## first network, its output and rnn gradients\r\nif model=='rnn':\r\n    rnn2 = nn.RNN(1, 20, nonlinearity=nonlinearity, bias=False).cuda() if use_gpu else nn.RNN(1, 20, nonlinearity=nonlinearity, bias=False)\r\nelif model=='gru':\r\n    rnn2 = nn.GRU(1, 20, bias=False).cuda() if use_gpu else nn.GRU(1, 20, bias=False)\r\nelif model=='lstm':\r\n    rnn2 = nn.LSTM(1, 20, bias=False).cuda() if use_gpu else nn.LSTM(1, 20, bias=False)\r\nlinear2 = nn.Linear(20, 2, bias=False).cuda() if use_gpu else nn.Linear(20, 2, bias=False)\r\n\r\n# same init as the first network\r\nfor name, param in rnn2.named_parameters():\r\n    if 'weight' in name:\r\n        nn.init.eye_(param)\r\nfor name, param in linear2.named_parameters():\r\n    if 'weight' in name:\r\n        nn.init.eye_(param)\r\n        \r\n# run the net \r\nif model != 'lstm':\r\n    output2, hx2 = rnn2(input, hx0.unsqueeze(0))\r\nelse:\r\n    output2, (hx2, _) = rnn2(input, (hx0.unsqueeze(0), c0.unsqueeze(0)))\r\nlogit2 = linear2(hx2[-1])\r\nloss2 = criterion(logit2, target)\r\n\r\n# calculate gradients and sum of gradient norms\r\ngrad_params2 = torch.autograd.grad(loss2, rnn2.parameters(), create_graph=True, retain_graph=True, allow_unused=True)\r\ngrad_norm2 = 0\r\nfor idx in range(len(grad_params2)):\r\n    grad_norm2 += torch.norm(grad_params2[idx])\r\nprint('rnn2 - loss: %f' % (loss2))\r\nprint('rnn2 - sum of gradient norm is: %f' % (grad_norm2))\r\n```\r\n\r\n## System Info\r\n- PyTorch or Caffe2: pyTorch\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Build command you used (if compiling from source):\r\n- OS: ubuntu 16.04\r\n- PyTorch version: 0.4.1\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 8.0\r\n- GPU models and configuration: TITAN X\r\n- GCC version (if compiling from source): \r\n- CMake version: 3.5.1\r\n- Versions of any other relevant libraries: \r\n"}