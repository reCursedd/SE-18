{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199331086", "pull_request_review_id": 133435017, "id": 199331086, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5OTMzMTA4Ng==", "diff_hunk": "@@ -2486,6 +2486,11 @@ def random_fullrank_matrix_distinct_singular_value(l):\n     s = torch.arange(1., l + 1).mul_(1.0 / (l + 1))\n     return u.mm(torch.diag(s)).mm(v.t())\n \n+def random_matrix_large_singular_value(m, n):", "path": "test/test_autograd.py", "position": null, "original_position": 4, "commit_id": "6bac86610365da9f0afe3a4d7f12275295a2918a", "original_commit_id": "ac0e33c52389c84469c2b39a97ce43ef00a4675f", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "This is different from what I had in mind. It was totally my bad for not describing my proposal clearly. The tested function I was thinking is\r\n```\r\ndef fn(x):\r\n  S = torch.cat([x, zeros], 0)\r\n  M = U @ S.diag() @ V\r\n  return M.pinv()\r\n```\r\nand the gradcheck is run on this entire `fn` with `input` being a large positive vector (e.g., `torch.rand(...) + 1`) so that no matter how it touches `x`, as long as the perturbation is small, we always get rank `|x|`.\r\n\r\nThis might work too if the random numbers treat us well, but point perturbation touches the returned matrix, and may change its rank. Let's see what CI says.", "created_at": "2018-06-30T20:01:18Z", "updated_at": "2018-11-23T15:46:37Z", "html_url": "https://github.com/pytorch/pytorch/pull/9052#discussion_r199331086", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9052", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199331086"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9052#discussion_r199331086"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9052"}}, "body_html": "<p>This is different from what I had in mind. It was totally my bad for not describing my proposal clearly. The tested function I was thinking is</p>\n<pre><code>def fn(x):\n  S = torch.cat([x, zeros], 0)\n  M = U @ S.diag() @ V\n  return M.pinv()\n</code></pre>\n<p>and the gradcheck is run on this entire <code>fn</code> with <code>input</code> being a large positive vector (e.g., <code>torch.rand(...) + 1</code>) so that no matter how it touches <code>x</code>, as long as the perturbation is small, we always get rank <code>|x|</code>.</p>\n<p>This might work too if the random numbers treat us well, but point perturbation touches the returned matrix, and may change its rank. Let's see what CI says.</p>", "body_text": "This is different from what I had in mind. It was totally my bad for not describing my proposal clearly. The tested function I was thinking is\ndef fn(x):\n  S = torch.cat([x, zeros], 0)\n  M = U @ S.diag() @ V\n  return M.pinv()\n\nand the gradcheck is run on this entire fn with input being a large positive vector (e.g., torch.rand(...) + 1) so that no matter how it touches x, as long as the perturbation is small, we always get rank |x|.\nThis might work too if the random numbers treat us well, but point perturbation touches the returned matrix, and may change its rank. Let's see what CI says."}