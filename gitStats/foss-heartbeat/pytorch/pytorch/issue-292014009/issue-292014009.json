{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4880", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4880/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4880/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4880/events", "html_url": "https://github.com/pytorch/pytorch/issues/4880", "id": 292014009, "node_id": "MDU6SXNzdWUyOTIwMTQwMDk=", "number": 4880, "title": "Accumulate absolute value of gradients instead of signed value", "user": {"login": "JakobHavtorn", "id": 10236734, "node_id": "MDQ6VXNlcjEwMjM2NzM0", "avatar_url": "https://avatars0.githubusercontent.com/u/10236734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JakobHavtorn", "html_url": "https://github.com/JakobHavtorn", "followers_url": "https://api.github.com/users/JakobHavtorn/followers", "following_url": "https://api.github.com/users/JakobHavtorn/following{/other_user}", "gists_url": "https://api.github.com/users/JakobHavtorn/gists{/gist_id}", "starred_url": "https://api.github.com/users/JakobHavtorn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JakobHavtorn/subscriptions", "organizations_url": "https://api.github.com/users/JakobHavtorn/orgs", "repos_url": "https://api.github.com/users/JakobHavtorn/repos", "events_url": "https://api.github.com/users/JakobHavtorn/events{/privacy}", "received_events_url": "https://api.github.com/users/JakobHavtorn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-01-26T19:56:21Z", "updated_at": "2018-01-26T20:25:41Z", "closed_at": "2018-01-26T20:25:41Z", "author_association": "NONE", "body_html": "<p>When you call backward on an output unit of a network which contains the outputs of an entire batch, pytorch computes the sum of <em>signed</em> gradients of these outputs wrt. network weights.</p>\n<p>Is there currently any way to accumulate the <em>absolute</em> values of these gradients instead of the <em>signed</em> values?</p>\n<p>The only way I can think of is to call backward on every single observation in a for loop and manually set</p>\n<pre><code>param.grad.data = param.grad.data.abs()\n</code></pre>\n<p>This is however, horribly slow (as could be imagined)</p>", "body_text": "When you call backward on an output unit of a network which contains the outputs of an entire batch, pytorch computes the sum of signed gradients of these outputs wrt. network weights.\nIs there currently any way to accumulate the absolute values of these gradients instead of the signed values?\nThe only way I can think of is to call backward on every single observation in a for loop and manually set\nparam.grad.data = param.grad.data.abs()\n\nThis is however, horribly slow (as could be imagined)", "body": "When you call backward on an output unit of a network which contains the outputs of an entire batch, pytorch computes the sum of _signed_ gradients of these outputs wrt. network weights.\r\n\r\nIs there currently any way to accumulate the _absolute_ values of these gradients instead of the _signed_ values?\r\n\r\nThe only way I can think of is to call backward on every single observation in a for loop and manually set \r\n```\r\nparam.grad.data = param.grad.data.abs()\r\n```\r\nThis is however, horribly slow (as could be imagined)"}