{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3880", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3880/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3880/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3880/events", "html_url": "https://github.com/pytorch/pytorch/issues/3880", "id": 276793186, "node_id": "MDU6SXNzdWUyNzY3OTMxODY=", "number": 3880, "title": "Reinitialize the random generator in worker processes of `DataLoader`", "user": {"login": "netheril96", "id": 836839, "node_id": "MDQ6VXNlcjgzNjgzOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/836839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/netheril96", "html_url": "https://github.com/netheril96", "followers_url": "https://api.github.com/users/netheril96/followers", "following_url": "https://api.github.com/users/netheril96/following{/other_user}", "gists_url": "https://api.github.com/users/netheril96/gists{/gist_id}", "starred_url": "https://api.github.com/users/netheril96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/netheril96/subscriptions", "organizations_url": "https://api.github.com/users/netheril96/orgs", "repos_url": "https://api.github.com/users/netheril96/repos", "events_url": "https://api.github.com/users/netheril96/events{/privacy}", "received_events_url": "https://api.github.com/users/netheril96/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-11-26T02:14:05Z", "updated_at": "2017-12-18T07:19:09Z", "closed_at": "2017-12-18T07:19:09Z", "author_association": "NONE", "body_html": "<p><code>torch.utils.data.DataLoader</code> can use multiprocessing to load and preprocess the data. It is commonly used to overlap the GPU computation and data loading. It has a minor flaw, though. On Unix, the processes are by default created with <code>fork</code>, so the global random state is copied. If each worker does something pseudo-randomly (common in data augmentation), their actions are all the same. For example, random crop is commonly used in image classification task, and in this case, all <code>DataLoader</code> workers will crop image in the same way, reducing the effectiveness of data augmentation.</p>\n<p>I propose a new option <code>worker_init_fn</code> be added to <code>DataLoader</code> constructor, and the function is called with the process ID right in the beginning of each worker loop. By default, the function ignores the process ID and reseeds both pytorch and numpy default random generator. The users can override it to do other things that each worker might need differently.</p>", "body_text": "torch.utils.data.DataLoader can use multiprocessing to load and preprocess the data. It is commonly used to overlap the GPU computation and data loading. It has a minor flaw, though. On Unix, the processes are by default created with fork, so the global random state is copied. If each worker does something pseudo-randomly (common in data augmentation), their actions are all the same. For example, random crop is commonly used in image classification task, and in this case, all DataLoader workers will crop image in the same way, reducing the effectiveness of data augmentation.\nI propose a new option worker_init_fn be added to DataLoader constructor, and the function is called with the process ID right in the beginning of each worker loop. By default, the function ignores the process ID and reseeds both pytorch and numpy default random generator. The users can override it to do other things that each worker might need differently.", "body": "`torch.utils.data.DataLoader` can use multiprocessing to load and preprocess the data. It is commonly used to overlap the GPU computation and data loading. It has a minor flaw, though. On Unix, the processes are by default created with `fork`, so the global random state is copied. If each worker does something pseudo-randomly (common in data augmentation), their actions are all the same. For example, random crop is commonly used in image classification task, and in this case, all `DataLoader` workers will crop image in the same way, reducing the effectiveness of data augmentation.\r\n\r\nI propose a new option `worker_init_fn` be added to `DataLoader` constructor, and the function is called with the process ID right in the beginning of each worker loop. By default, the function ignores the process ID and reseeds both pytorch and numpy default random generator. The users can override it to do other things that each worker might need differently."}