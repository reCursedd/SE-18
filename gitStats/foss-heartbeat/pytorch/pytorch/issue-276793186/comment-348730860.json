{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/348730860", "html_url": "https://github.com/pytorch/pytorch/issues/3880#issuecomment-348730860", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3880", "id": 348730860, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODczMDg2MA==", "user": {"login": "vfdev-5", "id": 2459423, "node_id": "MDQ6VXNlcjI0NTk0MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2459423?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vfdev-5", "html_url": "https://github.com/vfdev-5", "followers_url": "https://api.github.com/users/vfdev-5/followers", "following_url": "https://api.github.com/users/vfdev-5/following{/other_user}", "gists_url": "https://api.github.com/users/vfdev-5/gists{/gist_id}", "starred_url": "https://api.github.com/users/vfdev-5/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vfdev-5/subscriptions", "organizations_url": "https://api.github.com/users/vfdev-5/orgs", "repos_url": "https://api.github.com/users/vfdev-5/repos", "events_url": "https://api.github.com/users/vfdev-5/events{/privacy}", "received_events_url": "https://api.github.com/users/vfdev-5/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-03T00:28:48Z", "updated_at": "2017-12-03T00:29:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I tried to reproduce this issue and there is an interesting behaviour (at least for me) that I observe. If random data augmentation is based on <code>np.random</code> then forking the dataset with random transformations produces batches with exactly the same augmentations. However, if random data augmentation is based on <code>random</code> from python stdlib there is no problem with data augmentations between batches. <a href=\"https://gist.github.com/vfdev-5/aa61ea2bac2494ca852963fc941fcd7e\">Here</a> is a gist.</p>", "body_text": "I tried to reproduce this issue and there is an interesting behaviour (at least for me) that I observe. If random data augmentation is based on np.random then forking the dataset with random transformations produces batches with exactly the same augmentations. However, if random data augmentation is based on random from python stdlib there is no problem with data augmentations between batches. Here is a gist.", "body": "I tried to reproduce this issue and there is an interesting behaviour (at least for me) that I observe. If random data augmentation is based on `np.random` then forking the dataset with random transformations produces batches with exactly the same augmentations. However, if random data augmentation is based on `random` from python stdlib there is no problem with data augmentations between batches. [Here](https://gist.github.com/vfdev-5/aa61ea2bac2494ca852963fc941fcd7e) is a gist."}