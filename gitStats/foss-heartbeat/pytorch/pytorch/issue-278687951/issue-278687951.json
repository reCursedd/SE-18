{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3981", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3981/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3981/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3981/events", "html_url": "https://github.com/pytorch/pytorch/issues/3981", "id": 278687951, "node_id": "MDU6SXNzdWUyNzg2ODc5NTE=", "number": 3981, "title": " Errors when computing second order gradients on gpu", "user": {"login": "uuujf", "id": 17348384, "node_id": "MDQ6VXNlcjE3MzQ4Mzg0", "avatar_url": "https://avatars2.githubusercontent.com/u/17348384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/uuujf", "html_url": "https://github.com/uuujf", "followers_url": "https://api.github.com/users/uuujf/followers", "following_url": "https://api.github.com/users/uuujf/following{/other_user}", "gists_url": "https://api.github.com/users/uuujf/gists{/gist_id}", "starred_url": "https://api.github.com/users/uuujf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/uuujf/subscriptions", "organizations_url": "https://api.github.com/users/uuujf/orgs", "repos_url": "https://api.github.com/users/uuujf/repos", "events_url": "https://api.github.com/users/uuujf/events{/privacy}", "received_events_url": "https://api.github.com/users/uuujf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-12-02T14:27:18Z", "updated_at": "2017-12-05T08:11:19Z", "closed_at": "2017-12-05T08:11:19Z", "author_association": "NONE", "body_html": "<p>Here is a piece of test code:</p>\n<pre><code>import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1_weight = Parameter(torch.randn(10,1,3,3))\n        self.conv1_bias = Parameter(torch.randn(10))\n\n    def forward(self, x):\n        out = x\n        out = F.conv2d(out, self.conv1_weight, bias=self.conv1_bias)\n        return out\n\nif __name__ == '__main__':\n    from torch.autograd import grad\n    model = ConvNet()\n    model.cuda()\n    x = Variable(torch.randn(1,1,28,28)).cuda()\n    print(x.size())\n    y = model(x)\n    print(y.size())\n    loss = torch.mean(y.pow(2))\n    g = grad(loss, model.parameters(), create_graph=True, retain_graph=True)[0]\n    gg = grad(g[0,0,0,0], model.parameters(), retain_graph=True)[0] ## bug\n</code></pre>\n<p>If the following conditions meet, the second order gradient cannot be computed:</p>\n<ol>\n<li>you want to compute second order grad (not first order grad)</li>\n<li>you network contains convolution (fc layer functions normally)</li>\n<li>Computation on gpu (on cpu, everything was fine)</li>\n</ol>\n<p>And the error is:</p>\n<pre><code>RuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\n</code></pre>\n<p>Add  <code>.contiguous()</code>  won't fix anything.</p>", "body_text": "Here is a piece of test code:\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1_weight = Parameter(torch.randn(10,1,3,3))\n        self.conv1_bias = Parameter(torch.randn(10))\n\n    def forward(self, x):\n        out = x\n        out = F.conv2d(out, self.conv1_weight, bias=self.conv1_bias)\n        return out\n\nif __name__ == '__main__':\n    from torch.autograd import grad\n    model = ConvNet()\n    model.cuda()\n    x = Variable(torch.randn(1,1,28,28)).cuda()\n    print(x.size())\n    y = model(x)\n    print(y.size())\n    loss = torch.mean(y.pow(2))\n    g = grad(loss, model.parameters(), create_graph=True, retain_graph=True)[0]\n    gg = grad(g[0,0,0,0], model.parameters(), retain_graph=True)[0] ## bug\n\nIf the following conditions meet, the second order gradient cannot be computed:\n\nyou want to compute second order grad (not first order grad)\nyou network contains convolution (fc layer functions normally)\nComputation on gpu (on cpu, everything was fine)\n\nAnd the error is:\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\n\nAdd  .contiguous()  won't fix anything.", "body": "Here is a piece of test code:\r\n\r\n```\r\nimport math\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\nfrom torch.nn.parameter import Parameter\r\n\r\nclass ConvNet(nn.Module):\r\n    def __init__(self):\r\n        super(ConvNet, self).__init__()\r\n        self.conv1_weight = Parameter(torch.randn(10,1,3,3))\r\n        self.conv1_bias = Parameter(torch.randn(10))\r\n\r\n    def forward(self, x):\r\n        out = x\r\n        out = F.conv2d(out, self.conv1_weight, bias=self.conv1_bias)\r\n        return out\r\n\r\nif __name__ == '__main__':\r\n    from torch.autograd import grad\r\n    model = ConvNet()\r\n    model.cuda()\r\n    x = Variable(torch.randn(1,1,28,28)).cuda()\r\n    print(x.size())\r\n    y = model(x)\r\n    print(y.size())\r\n    loss = torch.mean(y.pow(2))\r\n    g = grad(loss, model.parameters(), create_graph=True, retain_graph=True)[0]\r\n    gg = grad(g[0,0,0,0], model.parameters(), retain_graph=True)[0] ## bug\r\n```\r\n\r\nIf the following conditions meet, the second order gradient cannot be computed:\r\n\r\n1. you want to compute second order grad (not first order grad)\r\n2. you network contains convolution (fc layer functions normally)\r\n3. Computation on gpu (on cpu, everything was fine)\r\n\r\nAnd the error is:\r\n```\r\nRuntimeError: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.\r\n```\r\nAdd  `.contiguous()`  won't fix anything."}