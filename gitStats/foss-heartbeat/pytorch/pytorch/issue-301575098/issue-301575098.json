{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5506", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5506/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5506/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5506/events", "html_url": "https://github.com/pytorch/pytorch/issues/5506", "id": 301575098, "node_id": "MDU6SXNzdWUzMDE1NzUwOTg=", "number": 5506, "title": "Distributed Gather does not work properly?", "user": {"login": "fang150", "id": 19598633, "node_id": "MDQ6VXNlcjE5NTk4NjMz", "avatar_url": "https://avatars0.githubusercontent.com/u/19598633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fang150", "html_url": "https://github.com/fang150", "followers_url": "https://api.github.com/users/fang150/followers", "following_url": "https://api.github.com/users/fang150/following{/other_user}", "gists_url": "https://api.github.com/users/fang150/gists{/gist_id}", "starred_url": "https://api.github.com/users/fang150/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fang150/subscriptions", "organizations_url": "https://api.github.com/users/fang150/orgs", "repos_url": "https://api.github.com/users/fang150/repos", "events_url": "https://api.github.com/users/fang150/events{/privacy}", "received_events_url": "https://api.github.com/users/fang150/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-03-01T21:58:45Z", "updated_at": "2018-03-03T19:52:21Z", "closed_at": "2018-03-03T19:52:21Z", "author_association": "NONE", "body_html": "<p>Environment:</p>\n<ul>\n<li>OS: Linux 4.4.0-83-generic x86_64</li>\n<li>PyTorch version: 0.3.1</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Python version:2.7.10</li>\n</ul>\n<p>Code:</p>\n<p>import os<br>\nimport torch<br>\nimport torch.distributed as dist<br>\nimport sys<br>\nrank= int(sys.argv[1])<br>\ndef run(rank, size):<br>\nif(rank==0):<br>\ngroup = dist.new_group(range(size))<br>\ng_list=[torch.DoubleTensor([1.0])]*size<br>\nalphak=torch.DoubleTensor([1.0])<br>\ndist.gather(tensor=alphak,gather_list=g_list,group=group)<br>\nprint(g_list)<br>\nelse:<br>\ngroup = dist.new_group(range(size))<br>\nalphak=torch.DoubleTensor([rank])<br>\ndist.gather(tensor=alphak,dst=0,group=group)<br>\ndef init_processes(rank, size, fn, backend='tcp'):<br>\n\"\"\" Initialize the distributed environment. \"\"\"<br>\nos.environ['MASTER_ADDR'] = '127.0.0.1'<br>\nos.environ['MASTER_PORT'] = '9000'<br>\ndist.init_process_group(backend, rank=rank, world_size=size)<br>\nfn(rank, size)<br>\nif <strong>name</strong> == \"<strong>main</strong>\":<br>\nsize = 3<br>\ninit_processes(rank, size, run, backend='tcp')</p>\n<p>Issue:</p>\n<p>In the code, I spawn 3 process, master as rank 0 , worker 1 as rank 1, worker 2 as rank2.</p>\n<p>master should receive tensors from itself and 2 workers.</p>\n<p>What I expected in tensor list should be [1,1,2] (first from master, second from worker 1 , third from worker 2), but I received [2,2,2].</p>\n<p>any help would be greatly appreciated!!</p>", "body_text": "Environment:\n\nOS: Linux 4.4.0-83-generic x86_64\nPyTorch version: 0.3.1\nHow you installed PyTorch (conda, pip, source): pip\nPython version:2.7.10\n\nCode:\nimport os\nimport torch\nimport torch.distributed as dist\nimport sys\nrank= int(sys.argv[1])\ndef run(rank, size):\nif(rank==0):\ngroup = dist.new_group(range(size))\ng_list=[torch.DoubleTensor([1.0])]*size\nalphak=torch.DoubleTensor([1.0])\ndist.gather(tensor=alphak,gather_list=g_list,group=group)\nprint(g_list)\nelse:\ngroup = dist.new_group(range(size))\nalphak=torch.DoubleTensor([rank])\ndist.gather(tensor=alphak,dst=0,group=group)\ndef init_processes(rank, size, fn, backend='tcp'):\n\"\"\" Initialize the distributed environment. \"\"\"\nos.environ['MASTER_ADDR'] = '127.0.0.1'\nos.environ['MASTER_PORT'] = '9000'\ndist.init_process_group(backend, rank=rank, world_size=size)\nfn(rank, size)\nif name == \"main\":\nsize = 3\ninit_processes(rank, size, run, backend='tcp')\nIssue:\nIn the code, I spawn 3 process, master as rank 0 , worker 1 as rank 1, worker 2 as rank2.\nmaster should receive tensors from itself and 2 workers.\nWhat I expected in tensor list should be [1,1,2] (first from master, second from worker 1 , third from worker 2), but I received [2,2,2].\nany help would be greatly appreciated!!", "body": "Environment:\r\n- OS: Linux 4.4.0-83-generic x86_64\r\n- PyTorch version: 0.3.1\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Python version:2.7.10\r\n\r\nCode:\r\n\r\nimport os\r\nimport torch\r\nimport torch.distributed as dist\r\nimport sys\r\nrank= int(sys.argv[1])\r\ndef run(rank, size):\r\n    if(rank==0):\r\n        group = dist.new_group(range(size))\r\n        g_list=[torch.DoubleTensor([1.0])]*size \r\n        alphak=torch.DoubleTensor([1.0])\r\n        dist.gather(tensor=alphak,gather_list=g_list,group=group) \r\n        print(g_list) \r\n    else:\r\n        group = dist.new_group(range(size))\r\n        alphak=torch.DoubleTensor([rank])\r\n        dist.gather(tensor=alphak,dst=0,group=group)\r\ndef init_processes(rank, size, fn, backend='tcp'):\r\n    \"\"\" Initialize the distributed environment. \"\"\"\r\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\r\n    os.environ['MASTER_PORT'] = '9000'\r\n    dist.init_process_group(backend, rank=rank, world_size=size)\r\n    fn(rank, size)\r\nif __name__ == \"__main__\":\r\n    size = 3\r\n    init_processes(rank, size, run, backend='tcp')\r\n\r\n\r\nIssue:\r\n\r\nIn the code, I spawn 3 process, master as rank 0 , worker 1 as rank 1, worker 2 as rank2.\r\n\r\nmaster should receive tensors from itself and 2 workers.\r\n\r\nWhat I expected in tensor list should be [1,1,2] (first from master, second from worker 1 , third from worker 2), but I received [2,2,2].\r\n\r\nany help would be greatly appreciated!! \r\n\r\n"}