{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4291", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4291/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4291/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4291/events", "html_url": "https://github.com/pytorch/pytorch/issues/4291", "id": 283783458, "node_id": "MDU6SXNzdWUyODM3ODM0NTg=", "number": 4291, "title": "RuntimeError: reduce failed to synchronize: unspecified launch failure", "user": {"login": "naba89", "id": 12119806, "node_id": "MDQ6VXNlcjEyMTE5ODA2", "avatar_url": "https://avatars3.githubusercontent.com/u/12119806?v=4", "gravatar_id": "", "url": "https://api.github.com/users/naba89", "html_url": "https://github.com/naba89", "followers_url": "https://api.github.com/users/naba89/followers", "following_url": "https://api.github.com/users/naba89/following{/other_user}", "gists_url": "https://api.github.com/users/naba89/gists{/gist_id}", "starred_url": "https://api.github.com/users/naba89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/naba89/subscriptions", "organizations_url": "https://api.github.com/users/naba89/orgs", "repos_url": "https://api.github.com/users/naba89/repos", "events_url": "https://api.github.com/users/naba89/events{/privacy}", "received_events_url": "https://api.github.com/users/naba89/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-12-21T06:18:32Z", "updated_at": "2018-09-27T08:54:47Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I am facing this error after few epochs of training.</p>\n<blockquote>\n<p>loss_re = loss_fn(output_re, target_re_v)<br>\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 325, in <code>__call__</code><br>\nresult = self.forward(*input, **kwargs)<br>\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\", line 329, in forward<br>\nreturn F.mse_loss(input, target, size_average=self.size_average, reduce=self.reduce)<br>\nRuntimeError: reduce failed to synchronize: unspecified launch failure</p>\n</blockquote>\n<p>Following is the system configuration:<br>\nOS: Ubuntu 16.04<br>\nCuda: 9.1.85<br>\nPytorch: 0.3.0.post4<br>\nGPU: 2x 1080ti (training on a single GPU without DataParallel specified using CUDA_VISIBLE_DEVICES)</p>\n<p>Is this a known issue or any known workarounds?</p>\n<p>Regards<br>\nNabarun</p>", "body_text": "Hi,\nI am facing this error after few epochs of training.\n\nloss_re = loss_fn(output_re, target_re_v)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 325, in __call__\nresult = self.forward(*input, **kwargs)\nFile \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\", line 329, in forward\nreturn F.mse_loss(input, target, size_average=self.size_average, reduce=self.reduce)\nRuntimeError: reduce failed to synchronize: unspecified launch failure\n\nFollowing is the system configuration:\nOS: Ubuntu 16.04\nCuda: 9.1.85\nPytorch: 0.3.0.post4\nGPU: 2x 1080ti (training on a single GPU without DataParallel specified using CUDA_VISIBLE_DEVICES)\nIs this a known issue or any known workarounds?\nRegards\nNabarun", "body": "Hi,\r\n\r\nI am facing this error after few epochs of training.\r\n\r\n> loss_re = loss_fn(output_re, target_re_v)\r\n>   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 325, in `__call__`\r\n>     result = self.forward(*input, **kwargs)\r\n>   File \"/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/loss.py\", line 329, in forward\r\n>     return F.mse_loss(input, target, size_average=self.size_average, reduce=self.reduce)\r\n> RuntimeError: reduce failed to synchronize: unspecified launch failure\r\n\r\nFollowing is the system configuration:\r\nOS: Ubuntu 16.04\r\nCuda: 9.1.85\r\nPytorch: 0.3.0.post4\r\nGPU: 2x 1080ti (training on a single GPU without DataParallel specified using CUDA_VISIBLE_DEVICES)\r\n\r\nIs this a known issue or any known workarounds?\r\n\r\nRegards\r\nNabarun"}