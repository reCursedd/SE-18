{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/425012983", "html_url": "https://github.com/pytorch/pytorch/issues/4291#issuecomment-425012983", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4291", "id": 425012983, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTAxMjk4Mw==", "user": {"login": "Oracen", "id": 25865068, "node_id": "MDQ6VXNlcjI1ODY1MDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/25865068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Oracen", "html_url": "https://github.com/Oracen", "followers_url": "https://api.github.com/users/Oracen/followers", "following_url": "https://api.github.com/users/Oracen/following{/other_user}", "gists_url": "https://api.github.com/users/Oracen/gists{/gist_id}", "starred_url": "https://api.github.com/users/Oracen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Oracen/subscriptions", "organizations_url": "https://api.github.com/users/Oracen/orgs", "repos_url": "https://api.github.com/users/Oracen/repos", "events_url": "https://api.github.com/users/Oracen/events{/privacy}", "received_events_url": "https://api.github.com/users/Oracen/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-27T08:54:47Z", "updated_at": "2018-09-27T08:54:47Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3345261\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/juliohm\">@juliohm</a>, he's using MSE loss.</p>\n<p>I'm seeing similar errors. I have two near identical matfac models, one trains fine, the other kills the kernel within a single loop. Running within the Fast.Ai library, but the error is with Pytorch functional</p>\n<p>OS: Ubuntu 16.04<br>\nCUDA: V9.0.176<br>\nPytorch: 0.4.1<br>\nGPU: GTX1080TI (3 available, but only training on one)</p>\n<p>Reproduction code seems kind of tough, because it's sporadic. I'd release the full dataset but it's PID.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-ii\">--------------------------------------------------------------------------</span><span class=\"pl-k\">-</span>\n<span class=\"pl-c1\">RuntimeError</span>                              Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">5</span><span class=\"pl-k\">-</span>acdfebe571c4<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n<span class=\"pl-ii\">----</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span> learn.fit(<span class=\"pl-c1\">1e-3</span>, <span class=\"pl-c1\">1</span>)\n      <span class=\"pl-c1\">2</span> learn.pretrain<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>\n      <span class=\"pl-c1\">3</span> learn.fit(<span class=\"pl-c1\">1e-4</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">wds</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">cycle_len</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">cycle_mult</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n      <span class=\"pl-c1\">4</span> learn.fit(<span class=\"pl-c1\">3e-5</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">cycle_len</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n      <span class=\"pl-c1\">5</span> patient_embed <span class=\"pl-k\">=</span> to_np(learn.model.p.weight)\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>fastai<span class=\"pl-k\">/</span>learner.py <span class=\"pl-k\">in</span> fit(<span class=\"pl-c1\">self</span>, lrs, n_cycle, wds, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">300</span>         <span class=\"pl-c1\">self</span>.sched <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    <span class=\"pl-c1\">301</span>         layer_opt <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.get_layer_opt(lrs, wds)\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">302</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.fit_gen(<span class=\"pl-c1\">self</span>.model, <span class=\"pl-c1\">self</span>.data, layer_opt, n_cycle, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">303</span> \n    <span class=\"pl-c1\">304</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">warm_up</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">lr</span>, <span class=\"pl-smi\">wds</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>fastai<span class=\"pl-k\">/</span>learner.py <span class=\"pl-k\">in</span> fit_gen(<span class=\"pl-c1\">self</span>, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">247</span>             metrics<span class=\"pl-k\">=</span>metrics, callbacks<span class=\"pl-k\">=</span>callbacks, reg_fn<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.reg_fn, clip<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.clip, fp16<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.fp16,\n    <span class=\"pl-c1\">248</span>             swa_model<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.swa_model <span class=\"pl-k\">if</span> use_swa <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>, swa_start<span class=\"pl-k\">=</span>swa_start,\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">249</span>             swa_eval_freq<span class=\"pl-k\">=</span>swa_eval_freq, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">250</span> \n    <span class=\"pl-c1\">251</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_layer_groups</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>): <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.models.get_layer_groups()\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>fastai<span class=\"pl-k\">/</span>model.py <span class=\"pl-k\">in</span> fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">139</span>             batch_num <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-c1\">140</span>             <span class=\"pl-k\">for</span> cb <span class=\"pl-k\">in</span> callbacks: cb.on_batch_begin()\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">141</span>             loss <span class=\"pl-k\">=</span> model_stepper.step(V(x),V(y), epoch)\n    <span class=\"pl-c1\">142</span>             avg_loss <span class=\"pl-k\">=</span> avg_loss <span class=\"pl-k\">*</span> avg_mom <span class=\"pl-k\">+</span> loss <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>avg_mom)\n    <span class=\"pl-c1\">143</span>             debias_loss <span class=\"pl-k\">=</span> avg_loss <span class=\"pl-k\">/</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> avg_mom<span class=\"pl-k\">**</span>batch_num)\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>fastai<span class=\"pl-k\">/</span>model.py <span class=\"pl-k\">in</span> step(<span class=\"pl-c1\">self</span>, xs, y, epoch)\n     <span class=\"pl-c1\">52</span>         <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.fp16: <span class=\"pl-c1\">self</span>.m.zero_grad()\n     <span class=\"pl-c1\">53</span>         <span class=\"pl-k\">else</span>: <span class=\"pl-c1\">self</span>.opt.zero_grad()\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">54</span>         loss <span class=\"pl-k\">=</span> raw_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.crit(output, y)\n     <span class=\"pl-c1\">55</span>         <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.loss_scale <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">1</span>: <span class=\"pl-k\">assert</span>(<span class=\"pl-c1\">self</span>.fp16); loss <span class=\"pl-k\">=</span> loss<span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.loss_scale\n     <span class=\"pl-c1\">56</span>         <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.reg_fn: loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.reg_fn(output, xtra, raw_loss)\n\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">4</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">33d3aa22dcd6</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> crit_fn(<span class=\"pl-c1\">input</span>, target)\n      <span class=\"pl-c1\">2</span>   <span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">input</span>.size())\n      <span class=\"pl-c1\">3</span>   <span class=\"pl-c1\">print</span>(target.size())\n<span class=\"pl-ii\">----</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">4</span>   <span class=\"pl-k\">return</span> F.mse_loss(<span class=\"pl-c1\">input</span>, target, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sum<span class=\"pl-pds\">'</span></span>)\n      <span class=\"pl-c1\">5</span> learn.crit <span class=\"pl-k\">=</span> crit_fn\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>functional.py <span class=\"pl-k\">in</span> mse_loss(<span class=\"pl-c1\">input</span>, target, size_average, <span class=\"pl-v\">reduce</span>, reduction)\n   <span class=\"pl-c1\">1714</span>     <span class=\"pl-k\">else</span>:\n   <span class=\"pl-c1\">1715</span>         reduction <span class=\"pl-k\">=</span> _Reduction.get_enum(reduction)\n<span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">1716</span>     <span class=\"pl-k\">return</span> _pointwise_loss(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">a</span>, <span class=\"pl-smi\">b</span>: (a <span class=\"pl-k\">-</span> b) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>, torch._C._nn.mse_loss, <span class=\"pl-c1\">input</span>, target, reduction)\n   <span class=\"pl-c1\">1717</span> \n   <span class=\"pl-c1\">1718</span> \n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>functional.py <span class=\"pl-k\">in</span> _pointwise_loss(lambd, lambd_optimized, <span class=\"pl-c1\">input</span>, target, reduction)\n   <span class=\"pl-c1\">1672</span>         <span class=\"pl-k\">return</span> torch.mean(d) <span class=\"pl-k\">if</span> reduction <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>elementwise_mean<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">else</span> torch.sum(d)\n   <span class=\"pl-c1\">1673</span>     <span class=\"pl-k\">else</span>:\n<span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">1674</span>         <span class=\"pl-k\">return</span> lambd_optimized(<span class=\"pl-c1\">input</span>, target, reduction)\n   <span class=\"pl-c1\">1675</span> \n   <span class=\"pl-c1\">1676</span> \n\n<span class=\"pl-c1\">RuntimeError</span>: <span class=\"pl-v\">reduce</span> failed to synchronize: device<span class=\"pl-k\">-</span>side <span class=\"pl-k\">assert</span> triggered</pre></div>\n<p>I checked that the targets and preds aligned with the wrapper function, and I also checked that the error occurs under both 'elementwise_mean' and 'sum'</p>\n<p><code>HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value=''))) 0%|          | 0/463933 [00:00&lt;?, ?it/s]</code><br>\n<code>torch.Size([128, 1])</code><br>\n<code>torch.Size([128, 1])</code><br>\n<code>0%|          | 1/463933 [00:05&lt;662:31:34,  5.14s/it, loss=1.05]torch.Size([128, 1])</code><br>\n<code>torch.Size([128, 1])</code><br>\n<code>torch.Size([128, 1])</code><br>\n<code>torch.Size([128, 1])</code><br>\n<code>torch.Size([128, 1])</code><br>\n<code>torch.Size([128, 1])</code><br>\n<code>torch.Size([128, 1])</code><br>\n<code>torch.Size([128, 1])</code><br>\n<code>RuntimeError</code></p>\n<p>When I replaced F.mse_loss() with the following:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">crit_fn</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>):\n  x <span class=\"pl-k\">=</span> F.mse_loss(<span class=\"pl-c1\">input</span>, target, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>none<span class=\"pl-pds\">'</span></span>)\n  x <span class=\"pl-k\">=</span> x.mean()\n  <span class=\"pl-k\">return</span> x\n\nlearn.crit <span class=\"pl-k\">=</span> crit_fn</pre></div>\n<p>I got a few iterations in, before the following error was raised:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-ii\">--------------------------------------------------------------------------</span><span class=\"pl-k\">-</span>\n<span class=\"pl-c1\">RuntimeError</span>                              Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">12</span><span class=\"pl-k\">-</span>acdfebe571c4<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n<span class=\"pl-ii\">----</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span> learn.fit(<span class=\"pl-c1\">1e-3</span>, <span class=\"pl-c1\">1</span>)\n      <span class=\"pl-c1\">2</span> learn.pretrain<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>\n      <span class=\"pl-c1\">3</span> learn.fit(<span class=\"pl-c1\">1e-4</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">wds</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>, <span class=\"pl-v\">cycle_len</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">cycle_mult</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n      <span class=\"pl-c1\">4</span> learn.fit(<span class=\"pl-c1\">3e-5</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">cycle_len</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n      <span class=\"pl-c1\">5</span> patient_embed <span class=\"pl-k\">=</span> to_np(learn.model.p.weight)\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>fastai<span class=\"pl-k\">/</span>learner.py <span class=\"pl-k\">in</span> fit(<span class=\"pl-c1\">self</span>, lrs, n_cycle, wds, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">300</span>         <span class=\"pl-c1\">self</span>.sched <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n    <span class=\"pl-c1\">301</span>         layer_opt <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.get_layer_opt(lrs, wds)\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">302</span>         <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.fit_gen(<span class=\"pl-c1\">self</span>.model, <span class=\"pl-c1\">self</span>.data, layer_opt, n_cycle, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">303</span> \n    <span class=\"pl-c1\">304</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">warm_up</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">lr</span>, <span class=\"pl-smi\">wds</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>fastai<span class=\"pl-k\">/</span>learner.py <span class=\"pl-k\">in</span> fit_gen(<span class=\"pl-c1\">self</span>, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">247</span>             metrics<span class=\"pl-k\">=</span>metrics, callbacks<span class=\"pl-k\">=</span>callbacks, reg_fn<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.reg_fn, clip<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.clip, fp16<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.fp16,\n    <span class=\"pl-c1\">248</span>             swa_model<span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.swa_model <span class=\"pl-k\">if</span> use_swa <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>, swa_start<span class=\"pl-k\">=</span>swa_start,\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">249</span>             swa_eval_freq<span class=\"pl-k\">=</span>swa_eval_freq, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">250</span> \n    <span class=\"pl-c1\">251</span>     <span class=\"pl-k\">def</span> <span class=\"pl-en\">get_layer_groups</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>): <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.models.get_layer_groups()\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>fastai<span class=\"pl-k\">/</span>model.py <span class=\"pl-k\">in</span> fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">139</span>             batch_num <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-c1\">140</span>             <span class=\"pl-k\">for</span> cb <span class=\"pl-k\">in</span> callbacks: cb.on_batch_begin()\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">141</span>             loss <span class=\"pl-k\">=</span> model_stepper.step(V(x),V(y), epoch)\n    <span class=\"pl-c1\">142</span>             avg_loss <span class=\"pl-k\">=</span> avg_loss <span class=\"pl-k\">*</span> avg_mom <span class=\"pl-k\">+</span> loss <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>avg_mom)\n    <span class=\"pl-c1\">143</span>             debias_loss <span class=\"pl-k\">=</span> avg_loss <span class=\"pl-k\">/</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> avg_mom<span class=\"pl-k\">**</span>batch_num)\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>fastai<span class=\"pl-k\">/</span>model.py <span class=\"pl-k\">in</span> step(<span class=\"pl-c1\">self</span>, xs, y, epoch)\n     <span class=\"pl-c1\">52</span>         <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.fp16: <span class=\"pl-c1\">self</span>.m.zero_grad()\n     <span class=\"pl-c1\">53</span>         <span class=\"pl-k\">else</span>: <span class=\"pl-c1\">self</span>.opt.zero_grad()\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">54</span>         loss <span class=\"pl-k\">=</span> raw_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.crit(output, y)\n     <span class=\"pl-c1\">55</span>         <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.loss_scale <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">1</span>: <span class=\"pl-k\">assert</span>(<span class=\"pl-c1\">self</span>.fp16); loss <span class=\"pl-k\">=</span> loss<span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.loss_scale\n     <span class=\"pl-c1\">56</span>         <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.reg_fn: loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.reg_fn(output, xtra, raw_loss)\n\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">11</span><span class=\"pl-k\">-</span>b3d6cadfe7f1<span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> crit_fn(<span class=\"pl-c1\">input</span>, target)\n      <span class=\"pl-c1\">1</span> <span class=\"pl-k\">def</span> <span class=\"pl-en\">crit_fn</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>):\n      <span class=\"pl-c1\">2</span>   <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>raw<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-ii\">----</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">3</span>   <span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">input</span>)\n      <span class=\"pl-c1\">4</span>   <span class=\"pl-c1\">print</span>(target)\n      <span class=\"pl-c1\">5</span>   x <span class=\"pl-k\">=</span> F.mse_loss(<span class=\"pl-c1\">input</span>, target, <span class=\"pl-v\">reduction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>none<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>tensor.py <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__repr__</span>(<span class=\"pl-c1\">self</span>)\n     <span class=\"pl-c1\">55</span>         <span class=\"pl-c\"><span class=\"pl-c\">#</span> characters to replace unicode characters with.</span>\n     <span class=\"pl-c1\">56</span>         <span class=\"pl-k\">if</span> sys.version_info <span class=\"pl-k\">&gt;</span> (<span class=\"pl-c1\">3</span>,):\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">57</span>             <span class=\"pl-k\">return</span> torch._tensor_str._str(<span class=\"pl-c1\">self</span>)\n     <span class=\"pl-c1\">58</span>         <span class=\"pl-k\">else</span>:\n     <span class=\"pl-c1\">59</span>             <span class=\"pl-k\">if</span> <span class=\"pl-c1\">hasattr</span>(sys.stdout, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>encoding<span class=\"pl-pds\">'</span></span>):\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>_tensor_str.py <span class=\"pl-k\">in</span> _str(<span class=\"pl-c1\">self</span>)\n    <span class=\"pl-c1\">254</span>             suffix <span class=\"pl-k\">+=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>, dtype=<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(<span class=\"pl-c1\">self</span>.dtype)\n    <span class=\"pl-c1\">255</span> \n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">256</span>         formatter <span class=\"pl-k\">=</span> _Formatter(get_summarized_data(<span class=\"pl-c1\">self</span>) <span class=\"pl-k\">if</span> summarize <span class=\"pl-k\">else</span> <span class=\"pl-c1\">self</span>)\n    <span class=\"pl-c1\">257</span>         tensor_str <span class=\"pl-k\">=</span> _tensor_str(<span class=\"pl-c1\">self</span>, indent, formatter, summarize)\n    <span class=\"pl-c1\">258</span> \n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>.conda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>alex<span class=\"pl-k\">-</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>_tensor_str.py <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-c1\">self</span>, tensor)\n     <span class=\"pl-c1\">80</span> \n     <span class=\"pl-c1\">81</span>         <span class=\"pl-k\">else</span>:\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">82</span>             copy <span class=\"pl-k\">=</span> torch.empty(tensor.size(), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float64).copy_(tensor).view(tensor.nelement())\n     <span class=\"pl-c1\">83</span>             copy_list <span class=\"pl-k\">=</span> copy.tolist()\n     <span class=\"pl-c1\">84</span>             <span class=\"pl-k\">try</span>:\n\n<span class=\"pl-c1\">RuntimeError</span>: cuda runtime error (<span class=\"pl-c1\">59</span>) : device<span class=\"pl-k\">-</span>side <span class=\"pl-k\">assert</span> triggered at <span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">/</span>aten<span class=\"pl-k\">/</span>src<span class=\"pl-k\">/</span><span class=\"pl-c1\">THC</span><span class=\"pl-k\">/</span>generic<span class=\"pl-k\">/</span>THCTensorCopy.cpp:<span class=\"pl-c1\">70</span></pre></div>\n<p>I don't believe it's a problem with the Fast.Ai library, and I can't seem to narrow down what might be giving rise to this. As mentioned, I can switch to another Jupyter notebook and run another similar model to completion, as well as run Tensorflow models. If I can conduct any experiments to help narrow this down, let me know.</p>", "body_text": "@juliohm, he's using MSE loss.\nI'm seeing similar errors. I have two near identical matfac models, one trains fine, the other kills the kernel within a single loop. Running within the Fast.Ai library, but the error is with Pytorch functional\nOS: Ubuntu 16.04\nCUDA: V9.0.176\nPytorch: 0.4.1\nGPU: GTX1080TI (3 available, but only training on one)\nReproduction code seems kind of tough, because it's sporadic. I'd release the full dataset but it's PID.\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-acdfebe571c4> in <module>()\n----> 1 learn.fit(1e-3, 1)\n      2 learn.pretrain=False\n      3 learn.fit(1e-4, 2, wds=0, cycle_len=1, cycle_mult=2)\n      4 learn.fit(3e-5, 1, cycle_len=1)\n      5 patient_embed = to_np(learn.model.p.weight)\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/learner.py in fit(self, lrs, n_cycle, wds, **kwargs)\n    300         self.sched = None\n    301         layer_opt = self.get_layer_opt(lrs, wds)\n--> 302         return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n    303 \n    304     def warm_up(self, lr, wds=None):\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/learner.py in fit_gen(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\n    247             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\n    248             swa_model=self.swa_model if use_swa else None, swa_start=swa_start,\n--> 249             swa_eval_freq=swa_eval_freq, **kwargs)\n    250 \n    251     def get_layer_groups(self): return self.models.get_layer_groups()\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/model.py in fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\n    139             batch_num += 1\n    140             for cb in callbacks: cb.on_batch_begin()\n--> 141             loss = model_stepper.step(V(x),V(y), epoch)\n    142             avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\n    143             debias_loss = avg_loss / (1 - avg_mom**batch_num)\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/model.py in step(self, xs, y, epoch)\n     52         if self.fp16: self.m.zero_grad()\n     53         else: self.opt.zero_grad()\n---> 54         loss = raw_loss = self.crit(output, y)\n     55         if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\n     56         if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n\n<ipython-input-4-33d3aa22dcd6> in crit_fn(input, target)\n      2   print(input.size())\n      3   print(target.size())\n----> 4   return F.mse_loss(input, target, reduction='sum')\n      5 learn.crit = crit_fn\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/nn/functional.py in mse_loss(input, target, size_average, reduce, reduction)\n   1714     else:\n   1715         reduction = _Reduction.get_enum(reduction)\n-> 1716     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss, input, target, reduction)\n   1717 \n   1718 \n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/nn/functional.py in _pointwise_loss(lambd, lambd_optimized, input, target, reduction)\n   1672         return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)\n   1673     else:\n-> 1674         return lambd_optimized(input, target, reduction)\n   1675 \n   1676 \n\nRuntimeError: reduce failed to synchronize: device-side assert triggered\nI checked that the targets and preds aligned with the wrapper function, and I also checked that the error occurs under both 'elementwise_mean' and 'sum'\nHBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value=''))) 0%|          | 0/463933 [00:00<?, ?it/s]\ntorch.Size([128, 1])\ntorch.Size([128, 1])\n0%|          | 1/463933 [00:05<662:31:34,  5.14s/it, loss=1.05]torch.Size([128, 1])\ntorch.Size([128, 1])\ntorch.Size([128, 1])\ntorch.Size([128, 1])\ntorch.Size([128, 1])\ntorch.Size([128, 1])\ntorch.Size([128, 1])\ntorch.Size([128, 1])\nRuntimeError\nWhen I replaced F.mse_loss() with the following:\ndef crit_fn(input, target):\n  x = F.mse_loss(input, target, reduction='none')\n  x = x.mean()\n  return x\n\nlearn.crit = crit_fn\nI got a few iterations in, before the following error was raised:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-12-acdfebe571c4> in <module>()\n----> 1 learn.fit(1e-3, 1)\n      2 learn.pretrain=False\n      3 learn.fit(1e-4, 2, wds=0, cycle_len=1, cycle_mult=2)\n      4 learn.fit(3e-5, 1, cycle_len=1)\n      5 patient_embed = to_np(learn.model.p.weight)\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/learner.py in fit(self, lrs, n_cycle, wds, **kwargs)\n    300         self.sched = None\n    301         layer_opt = self.get_layer_opt(lrs, wds)\n--> 302         return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\n    303 \n    304     def warm_up(self, lr, wds=None):\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/learner.py in fit_gen(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\n    247             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\n    248             swa_model=self.swa_model if use_swa else None, swa_start=swa_start,\n--> 249             swa_eval_freq=swa_eval_freq, **kwargs)\n    250 \n    251     def get_layer_groups(self): return self.models.get_layer_groups()\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/model.py in fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\n    139             batch_num += 1\n    140             for cb in callbacks: cb.on_batch_begin()\n--> 141             loss = model_stepper.step(V(x),V(y), epoch)\n    142             avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\n    143             debias_loss = avg_loss / (1 - avg_mom**batch_num)\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/model.py in step(self, xs, y, epoch)\n     52         if self.fp16: self.m.zero_grad()\n     53         else: self.opt.zero_grad()\n---> 54         loss = raw_loss = self.crit(output, y)\n     55         if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\n     56         if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n\n<ipython-input-11-b3d6cadfe7f1> in crit_fn(input, target)\n      1 def crit_fn(input, target):\n      2   print('raw')\n----> 3   print(input)\n      4   print(target)\n      5   x = F.mse_loss(input, target, reduction='none')\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/tensor.py in __repr__(self)\n     55         # characters to replace unicode characters with.\n     56         if sys.version_info > (3,):\n---> 57             return torch._tensor_str._str(self)\n     58         else:\n     59             if hasattr(sys.stdout, 'encoding'):\n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py in _str(self)\n    254             suffix += ', dtype=' + str(self.dtype)\n    255 \n--> 256         formatter = _Formatter(get_summarized_data(self) if summarize else self)\n    257         tensor_str = _tensor_str(self, indent, formatter, summarize)\n    258 \n\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py in __init__(self, tensor)\n     80 \n     81         else:\n---> 82             copy = torch.empty(tensor.size(), dtype=torch.float64).copy_(tensor).view(tensor.nelement())\n     83             copy_list = copy.tolist()\n     84             try:\n\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.cpp:70\nI don't believe it's a problem with the Fast.Ai library, and I can't seem to narrow down what might be giving rise to this. As mentioned, I can switch to another Jupyter notebook and run another similar model to completion, as well as run Tensorflow models. If I can conduct any experiments to help narrow this down, let me know.", "body": "@juliohm, he's using MSE loss.\r\n\r\nI'm seeing similar errors. I have two near identical matfac models, one trains fine, the other kills the kernel within a single loop. Running within the Fast.Ai library, but the error is with Pytorch functional\r\n\r\nOS: Ubuntu 16.04\r\nCUDA: V9.0.176\r\nPytorch: 0.4.1\r\nGPU: GTX1080TI (3 available, but only training on one)\r\n\r\nReproduction code seems kind of tough, because it's sporadic. I'd release the full dataset but it's PID.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-5-acdfebe571c4> in <module>()\r\n----> 1 learn.fit(1e-3, 1)\r\n      2 learn.pretrain=False\r\n      3 learn.fit(1e-4, 2, wds=0, cycle_len=1, cycle_mult=2)\r\n      4 learn.fit(3e-5, 1, cycle_len=1)\r\n      5 patient_embed = to_np(learn.model.p.weight)\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/learner.py in fit(self, lrs, n_cycle, wds, **kwargs)\r\n    300         self.sched = None\r\n    301         layer_opt = self.get_layer_opt(lrs, wds)\r\n--> 302         return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\r\n    303 \r\n    304     def warm_up(self, lr, wds=None):\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/learner.py in fit_gen(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\r\n    247             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\r\n    248             swa_model=self.swa_model if use_swa else None, swa_start=swa_start,\r\n--> 249             swa_eval_freq=swa_eval_freq, **kwargs)\r\n    250 \r\n    251     def get_layer_groups(self): return self.models.get_layer_groups()\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/model.py in fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\r\n    139             batch_num += 1\r\n    140             for cb in callbacks: cb.on_batch_begin()\r\n--> 141             loss = model_stepper.step(V(x),V(y), epoch)\r\n    142             avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\r\n    143             debias_loss = avg_loss / (1 - avg_mom**batch_num)\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/model.py in step(self, xs, y, epoch)\r\n     52         if self.fp16: self.m.zero_grad()\r\n     53         else: self.opt.zero_grad()\r\n---> 54         loss = raw_loss = self.crit(output, y)\r\n     55         if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\r\n     56         if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\r\n\r\n<ipython-input-4-33d3aa22dcd6> in crit_fn(input, target)\r\n      2   print(input.size())\r\n      3   print(target.size())\r\n----> 4   return F.mse_loss(input, target, reduction='sum')\r\n      5 learn.crit = crit_fn\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/nn/functional.py in mse_loss(input, target, size_average, reduce, reduction)\r\n   1714     else:\r\n   1715         reduction = _Reduction.get_enum(reduction)\r\n-> 1716     return _pointwise_loss(lambda a, b: (a - b) ** 2, torch._C._nn.mse_loss, input, target, reduction)\r\n   1717 \r\n   1718 \r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/nn/functional.py in _pointwise_loss(lambd, lambd_optimized, input, target, reduction)\r\n   1672         return torch.mean(d) if reduction == 'elementwise_mean' else torch.sum(d)\r\n   1673     else:\r\n-> 1674         return lambd_optimized(input, target, reduction)\r\n   1675 \r\n   1676 \r\n\r\nRuntimeError: reduce failed to synchronize: device-side assert triggered\r\n```\r\n\r\nI checked that the targets and preds aligned with the wrapper function, and I also checked that the error occurs under both 'elementwise_mean' and 'sum'\r\n\r\n`\r\nHBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))\r\n  0%|          | 0/463933 [00:00<?, ?it/s]\r\n`\r\n`\r\ntorch.Size([128, 1])\r\n`\r\n`\r\ntorch.Size([128, 1])\r\n`\r\n`\r\n  0%|          | 1/463933 [00:05<662:31:34,  5.14s/it, loss=1.05]torch.Size([128, 1])\r\n`\r\n`\r\ntorch.Size([128, 1])\r\n`\r\n`\r\ntorch.Size([128, 1])\r\n`\r\n`\r\ntorch.Size([128, 1])\r\n`\r\n`\r\ntorch.Size([128, 1])\r\n`\r\n`\r\ntorch.Size([128, 1])\r\n`\r\n`\r\ntorch.Size([128, 1])\r\n`\r\n`\r\ntorch.Size([128, 1])\r\n`\r\n`RuntimeError`\r\n\r\nWhen I replaced F.mse_loss() with the following:\r\n```python\r\ndef crit_fn(input, target):\r\n  x = F.mse_loss(input, target, reduction='none')\r\n  x = x.mean()\r\n  return x\r\n\r\nlearn.crit = crit_fn\r\n```\r\nI got a few iterations in, before the following error was raised:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-12-acdfebe571c4> in <module>()\r\n----> 1 learn.fit(1e-3, 1)\r\n      2 learn.pretrain=False\r\n      3 learn.fit(1e-4, 2, wds=0, cycle_len=1, cycle_mult=2)\r\n      4 learn.fit(3e-5, 1, cycle_len=1)\r\n      5 patient_embed = to_np(learn.model.p.weight)\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/learner.py in fit(self, lrs, n_cycle, wds, **kwargs)\r\n    300         self.sched = None\r\n    301         layer_opt = self.get_layer_opt(lrs, wds)\r\n--> 302         return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)\r\n    303 \r\n    304     def warm_up(self, lr, wds=None):\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/learner.py in fit_gen(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\r\n    247             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,\r\n    248             swa_model=self.swa_model if use_swa else None, swa_start=swa_start,\r\n--> 249             swa_eval_freq=swa_eval_freq, **kwargs)\r\n    250 \r\n    251     def get_layer_groups(self): return self.models.get_layer_groups()\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/model.py in fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\r\n    139             batch_num += 1\r\n    140             for cb in callbacks: cb.on_batch_begin()\r\n--> 141             loss = model_stepper.step(V(x),V(y), epoch)\r\n    142             avg_loss = avg_loss * avg_mom + loss * (1-avg_mom)\r\n    143             debias_loss = avg_loss / (1 - avg_mom**batch_num)\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/fastai/model.py in step(self, xs, y, epoch)\r\n     52         if self.fp16: self.m.zero_grad()\r\n     53         else: self.opt.zero_grad()\r\n---> 54         loss = raw_loss = self.crit(output, y)\r\n     55         if self.loss_scale != 1: assert(self.fp16); loss = loss*self.loss_scale\r\n     56         if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\r\n\r\n<ipython-input-11-b3d6cadfe7f1> in crit_fn(input, target)\r\n      1 def crit_fn(input, target):\r\n      2   print('raw')\r\n----> 3   print(input)\r\n      4   print(target)\r\n      5   x = F.mse_loss(input, target, reduction='none')\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/tensor.py in __repr__(self)\r\n     55         # characters to replace unicode characters with.\r\n     56         if sys.version_info > (3,):\r\n---> 57             return torch._tensor_str._str(self)\r\n     58         else:\r\n     59             if hasattr(sys.stdout, 'encoding'):\r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py in _str(self)\r\n    254             suffix += ', dtype=' + str(self.dtype)\r\n    255 \r\n--> 256         formatter = _Formatter(get_summarized_data(self) if summarize else self)\r\n    257         tensor_str = _tensor_str(self, indent, formatter, summarize)\r\n    258 \r\n\r\n~/.conda/envs/alex-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py in __init__(self, tensor)\r\n     80 \r\n     81         else:\r\n---> 82             copy = torch.empty(tensor.size(), dtype=torch.float64).copy_(tensor).view(tensor.nelement())\r\n     83             copy_list = copy.tolist()\r\n     84             try:\r\n\r\nRuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.cpp:70\r\n```\r\n\r\nI don't believe it's a problem with the Fast.Ai library, and I can't seem to narrow down what might be giving rise to this. As mentioned, I can switch to another Jupyter notebook and run another similar model to completion, as well as run Tensorflow models. If I can conduct any experiments to help narrow this down, let me know."}