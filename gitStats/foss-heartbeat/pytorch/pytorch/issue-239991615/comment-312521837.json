{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/312521837", "html_url": "https://github.com/pytorch/pytorch/issues/1963#issuecomment-312521837", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1963", "id": 312521837, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjUyMTgzNw==", "user": {"login": "zihangdai", "id": 3943442, "node_id": "MDQ6VXNlcjM5NDM0NDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3943442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zihangdai", "html_url": "https://github.com/zihangdai", "followers_url": "https://api.github.com/users/zihangdai/followers", "following_url": "https://api.github.com/users/zihangdai/following{/other_user}", "gists_url": "https://api.github.com/users/zihangdai/gists{/gist_id}", "starred_url": "https://api.github.com/users/zihangdai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zihangdai/subscriptions", "organizations_url": "https://api.github.com/users/zihangdai/orgs", "repos_url": "https://api.github.com/users/zihangdai/repos", "events_url": "https://api.github.com/users/zihangdai/events{/privacy}", "received_events_url": "https://api.github.com/users/zihangdai/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-02T22:49:44Z", "updated_at": "2017-07-02T22:58:39Z", "author_association": "NONE", "body_html": "<p>I think I'm sure there is a memory leak due to <code>expand_as</code>, and I found a weird way to do a temporary fix under pytorch version \"0.1.12_2\".</p>\n<p>The simplest example is as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function, division\n<span class=\"pl-k\">import</span> gc\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> resource\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">args</span>):\n    tgt_var <span class=\"pl-k\">=</span> Variable(torch.rand(<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">512</span>))\n\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(args.num_steps):\n        src_var <span class=\"pl-k\">=</span> Variable(torch.rand(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">512</span>))\n        <span class=\"pl-k\">if</span> args.fix:\n            expanded <span class=\"pl-k\">=</span> src_var.expand(<span class=\"pl-k\">*</span><span class=\"pl-c1\">list</span>(tgt_var.size()))\n        <span class=\"pl-k\">else</span>:\n            expanded <span class=\"pl-k\">=</span> src_var.expand_as(tgt_var)\n\n        <span class=\"pl-k\">if</span> args.backward:\n            ret.mean().backward()\n\n        gc.collect()\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">500</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            max_mem_used <span class=\"pl-k\">=</span> resource.getrusage(resource.<span class=\"pl-c1\">RUSAGE_SELF</span>).ru_maxrss\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>#<span class=\"pl-c1\">{}</span> <span class=\"pl-c1\">{<span class=\"pl-k\">:.4f</span>}</span> MB<span class=\"pl-pds\">\"</span></span>.format(i, max_mem_used <span class=\"pl-k\">/</span> <span class=\"pl-c1\">1024</span>))\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser(<span class=\"pl-v\">description</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>PyTorch expand_as memory leak<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--num_steps<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000000</span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--backward<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--fix<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    args <span class=\"pl-k\">=</span> parser.parse_args()\n\n    main(args)</pre></div>\n<p>Basically, if we replace <code>expand_as</code> with <code>expand</code>, and pass in an unpacked list of numbers, (i.e., <code>src_var.expand(*list(tgt_var.size()))</code> instead of <code>src_var.expand(tgt_var.size())</code>), the memory leak is gone. Using <code>src_var.expand(tgt_var.size())</code> will also have the leak issue. Unfortunately, I don't know why this is working.</p>\n<p>Finally, since there are so many <code>expand_as</code> in pytorch code, to do a quick hacky fix, one can change the <code>expand_as</code> function in the file <code>torch/autograd/variable.py</code> as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">expand_as</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">tensor</span>):                                                                                                                            \n        <span class=\"pl-k\">return</span> Expand(<span class=\"pl-c1\">list</span>(tensor.size()))(<span class=\"pl-c1\">self</span>)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> return Expand(tensor.size())(self)</span></pre></div>", "body_text": "I think I'm sure there is a memory leak due to expand_as, and I found a weird way to do a temporary fix under pytorch version \"0.1.12_2\".\nThe simplest example is as follows:\nfrom __future__ import print_function, division\nimport gc\nimport argparse\nimport resource\n\nimport torch\nfrom torch.autograd import Variable\n\ndef main(args):\n    tgt_var = Variable(torch.rand(32, 512))\n\n    for i in range(args.num_steps):\n        src_var = Variable(torch.rand(1, 512))\n        if args.fix:\n            expanded = src_var.expand(*list(tgt_var.size()))\n        else:\n            expanded = src_var.expand_as(tgt_var)\n\n        if args.backward:\n            ret.mean().backward()\n\n        gc.collect()\n        if i > 0 and i % 500 == 0:\n            max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n            print(\"#{} {:.4f} MB\".format(i, max_mem_used / 1024))\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='PyTorch expand_as memory leak')\n    parser.add_argument('--num_steps', type=int, default=1000000)\n    parser.add_argument('--backward', action='store_true', default=False)\n    parser.add_argument('--fix', action='store_true', default=False)\n    args = parser.parse_args()\n\n    main(args)\nBasically, if we replace expand_as with expand, and pass in an unpacked list of numbers, (i.e., src_var.expand(*list(tgt_var.size())) instead of src_var.expand(tgt_var.size())), the memory leak is gone. Using src_var.expand(tgt_var.size()) will also have the leak issue. Unfortunately, I don't know why this is working.\nFinally, since there are so many expand_as in pytorch code, to do a quick hacky fix, one can change the expand_as function in the file torch/autograd/variable.py as follows:\n    def expand_as(self, tensor):                                                                                                                            \n        return Expand(list(tensor.size()))(self)\n        # return Expand(tensor.size())(self)", "body": "I think I'm sure there is a memory leak due to `expand_as`, and I found a weird way to do a temporary fix under pytorch version \"0.1.12_2\". \r\n\r\nThe simplest example is as follows:\r\n\r\n```python\r\nfrom __future__ import print_function, division\r\nimport gc\r\nimport argparse\r\nimport resource\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ndef main(args):\r\n    tgt_var = Variable(torch.rand(32, 512))\r\n\r\n    for i in range(args.num_steps):\r\n        src_var = Variable(torch.rand(1, 512))\r\n        if args.fix:\r\n            expanded = src_var.expand(*list(tgt_var.size()))\r\n        else:\r\n            expanded = src_var.expand_as(tgt_var)\r\n\r\n        if args.backward:\r\n            ret.mean().backward()\r\n\r\n        gc.collect()\r\n        if i > 0 and i % 500 == 0:\r\n            max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n            print(\"#{} {:.4f} MB\".format(i, max_mem_used / 1024))\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='PyTorch expand_as memory leak')\r\n    parser.add_argument('--num_steps', type=int, default=1000000)\r\n    parser.add_argument('--backward', action='store_true', default=False)\r\n    parser.add_argument('--fix', action='store_true', default=False)\r\n    args = parser.parse_args()\r\n\r\n    main(args)\r\n```\r\n\r\nBasically, if we replace `expand_as` with `expand`, and pass in an unpacked list of numbers, (i.e., `src_var.expand(*list(tgt_var.size()))` instead of `src_var.expand(tgt_var.size())`), the memory leak is gone. Using `src_var.expand(tgt_var.size())` will also have the leak issue. Unfortunately, I don't know why this is working. \r\n\r\nFinally, since there are so many `expand_as` in pytorch code, to do a quick hacky fix, one can change the `expand_as` function in the file `torch/autograd/variable.py` as follows:\r\n```python\r\n    def expand_as(self, tensor):                                                                                                                            \r\n        return Expand(list(tensor.size()))(self)\r\n        # return Expand(tensor.size())(self)\r\n```\r\n\r\n"}