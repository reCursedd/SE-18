{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/312556216", "html_url": "https://github.com/pytorch/pytorch/issues/1963#issuecomment-312556216", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1963", "id": 312556216, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjU1NjIxNg==", "user": {"login": "zihangdai", "id": 3943442, "node_id": "MDQ6VXNlcjM5NDM0NDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/3943442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zihangdai", "html_url": "https://github.com/zihangdai", "followers_url": "https://api.github.com/users/zihangdai/followers", "following_url": "https://api.github.com/users/zihangdai/following{/other_user}", "gists_url": "https://api.github.com/users/zihangdai/gists{/gist_id}", "starred_url": "https://api.github.com/users/zihangdai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zihangdai/subscriptions", "organizations_url": "https://api.github.com/users/zihangdai/orgs", "repos_url": "https://api.github.com/users/zihangdai/repos", "events_url": "https://api.github.com/users/zihangdai/events{/privacy}", "received_events_url": "https://api.github.com/users/zihangdai/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-03T06:00:48Z", "updated_at": "2017-07-03T07:39:05Z", "author_association": "NONE", "body_html": "<p>After some more research, I think we have a much bigger problem here. My current conjecture is that: whenever we use <code>tensor.size()</code> (<code>torch.Size</code> object) as an argument to a function, there will be a memory leak.</p>\n<p>An example is the <code>pad_packed_sequence</code> function in <code>torch/nn/utils/rnn.py</code>, where <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L107\">line 107</a> of the code is <code>output = var_data.data.new(len(batch_sizes), max_batch_size, *var_data.size()[1:]).zero_()</code> which uses <code>*var_data.size()</code>.</p>\n<p>Again, a minimal example for reproduction is as follows:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function, division\n<span class=\"pl-k\">import</span> gc\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> resource\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">import</span> torch.nn.utils.rnn <span class=\"pl-k\">as</span> rnn_utils\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">args</span>):\n    max_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span>\n    hid_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">16</span>\n    batch_s <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n    model <span class=\"pl-k\">=</span> nn.GRU(hid_dim, hid_dim)\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(args.num_steps):\n        \n        embedding <span class=\"pl-k\">=</span> Variable(torch.rand(max_len, batch_s, hid_dim))\n        lengths <span class=\"pl-k\">=</span> <span class=\"pl-c1\">range</span>(max_len, max_len<span class=\"pl-k\">-</span>batch_s, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        \n        packed_emb <span class=\"pl-k\">=</span> rnn_utils.pack_padded_sequence(embedding, lengths)\n        packed_out, hidden_final <span class=\"pl-k\">=</span> model(packed_emb)\n        outputs, srclens <span class=\"pl-k\">=</span> rnn_utils.pad_packed_sequence(packed_out)\n\n        <span class=\"pl-k\">if</span> args.backward:\n            outputs.mean().backward()\n\n        gc.collect()\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">500</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n            max_mem_used <span class=\"pl-k\">=</span> resource.getrusage(resource.<span class=\"pl-c1\">RUSAGE_SELF</span>).ru_maxrss\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>#<span class=\"pl-c1\">{}</span> <span class=\"pl-c1\">{<span class=\"pl-k\">:.4f</span>}</span> MB<span class=\"pl-pds\">\"</span></span>.format(i, max_mem_used <span class=\"pl-k\">/</span> <span class=\"pl-c1\">1024</span>))\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    parser <span class=\"pl-k\">=</span> argparse.ArgumentParser(<span class=\"pl-v\">description</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>PyTorch *tensor.size() memory leak<span class=\"pl-pds\">'</span></span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--num_steps<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000000</span>)\n    parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--backward<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    args <span class=\"pl-k\">=</span> parser.parse_args()\n\n    main(args)</pre></div>\n<p>As in the <code>expand_as</code> case, once I change the code to <code>output = var_data.data.new(len(batch_sizes), max_batch_size, *list(var_data.size())[1:]).zero_()</code>, memory leak is gone.</p>\n<p>So, in short, the problem is on the usage of <code>tensor.size()</code>.</p>", "body_text": "After some more research, I think we have a much bigger problem here. My current conjecture is that: whenever we use tensor.size() (torch.Size object) as an argument to a function, there will be a memory leak.\nAn example is the pad_packed_sequence function in torch/nn/utils/rnn.py, where line 107 of the code is output = var_data.data.new(len(batch_sizes), max_batch_size, *var_data.size()[1:]).zero_() which uses *var_data.size().\nAgain, a minimal example for reproduction is as follows:\nfrom __future__ import print_function, division\nimport gc\nimport argparse\nimport resource\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport torch.nn.utils.rnn as rnn_utils\n\ndef main(args):\n    max_len = 8\n    hid_dim = 16\n    batch_s = 4\n    model = nn.GRU(hid_dim, hid_dim)\n    for i in range(args.num_steps):\n        \n        embedding = Variable(torch.rand(max_len, batch_s, hid_dim))\n        lengths = range(max_len, max_len-batch_s, -1)\n        \n        packed_emb = rnn_utils.pack_padded_sequence(embedding, lengths)\n        packed_out, hidden_final = model(packed_emb)\n        outputs, srclens = rnn_utils.pad_packed_sequence(packed_out)\n\n        if args.backward:\n            outputs.mean().backward()\n\n        gc.collect()\n        if i > 0 and i % 500 == 0:\n            max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n            print(\"#{} {:.4f} MB\".format(i, max_mem_used / 1024))\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='PyTorch *tensor.size() memory leak')\n    parser.add_argument('--num_steps', type=int, default=1000000)\n    parser.add_argument('--backward', action='store_true', default=False)\n    args = parser.parse_args()\n\n    main(args)\nAs in the expand_as case, once I change the code to output = var_data.data.new(len(batch_sizes), max_batch_size, *list(var_data.size())[1:]).zero_(), memory leak is gone.\nSo, in short, the problem is on the usage of tensor.size().", "body": "After some more research, I think we have a much bigger problem here. My current conjecture is that: whenever we use `tensor.size()` (`torch.Size` object) as an argument to a function, there will be a memory leak. \r\n\r\nAn example is the `pad_packed_sequence` function in `torch/nn/utils/rnn.py`, where [line 107](https://github.com/pytorch/pytorch/blob/master/torch/nn/utils/rnn.py#L107) of the code is `output = var_data.data.new(len(batch_sizes), max_batch_size, *var_data.size()[1:]).zero_()` which uses `*var_data.size()`. \r\n\r\nAgain, a minimal example for reproduction is as follows:\r\n\r\n```python\r\nfrom __future__ import print_function, division\r\nimport gc\r\nimport argparse\r\nimport resource\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nimport torch.nn.utils.rnn as rnn_utils\r\n\r\ndef main(args):\r\n    max_len = 8\r\n    hid_dim = 16\r\n    batch_s = 4\r\n    model = nn.GRU(hid_dim, hid_dim)\r\n    for i in range(args.num_steps):\r\n        \r\n        embedding = Variable(torch.rand(max_len, batch_s, hid_dim))\r\n        lengths = range(max_len, max_len-batch_s, -1)\r\n        \r\n        packed_emb = rnn_utils.pack_padded_sequence(embedding, lengths)\r\n        packed_out, hidden_final = model(packed_emb)\r\n        outputs, srclens = rnn_utils.pad_packed_sequence(packed_out)\r\n\r\n        if args.backward:\r\n            outputs.mean().backward()\r\n\r\n        gc.collect()\r\n        if i > 0 and i % 500 == 0:\r\n            max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n            print(\"#{} {:.4f} MB\".format(i, max_mem_used / 1024))\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(description='PyTorch *tensor.size() memory leak')\r\n    parser.add_argument('--num_steps', type=int, default=1000000)\r\n    parser.add_argument('--backward', action='store_true', default=False)\r\n    args = parser.parse_args()\r\n\r\n    main(args)\r\n```\r\n\r\nAs in the `expand_as` case, once I change the code to `output = var_data.data.new(len(batch_sizes), max_batch_size, *list(var_data.size())[1:]).zero_()`, memory leak is gone. \r\n\r\nSo, in short, the problem is on the usage of `tensor.size()`. \r\n"}