{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6356", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6356/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6356/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6356/events", "html_url": "https://github.com/pytorch/pytorch/issues/6356", "id": 312062708, "node_id": "MDU6SXNzdWUzMTIwNjI3MDg=", "number": 6356, "title": "Caffe2 Model helper and Brew", "user": {"login": "BinhangYuan", "id": 6721177, "node_id": "MDQ6VXNlcjY3MjExNzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/6721177?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BinhangYuan", "html_url": "https://github.com/BinhangYuan", "followers_url": "https://api.github.com/users/BinhangYuan/followers", "following_url": "https://api.github.com/users/BinhangYuan/following{/other_user}", "gists_url": "https://api.github.com/users/BinhangYuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/BinhangYuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BinhangYuan/subscriptions", "organizations_url": "https://api.github.com/users/BinhangYuan/orgs", "repos_url": "https://api.github.com/users/BinhangYuan/repos", "events_url": "https://api.github.com/users/BinhangYuan/events{/privacy}", "received_events_url": "https://api.github.com/users/BinhangYuan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-04-06T17:52:15Z", "updated_at": "2018-04-06T18:43:22Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I am wondering how the model helper and brew works. Here is my problem, I used the following code for a simple 3 layer NN, where I manually setup the param_init_net and net for the model; the problem is that it seems the optimizer.build_sgd() does not work, the PBs are also attached:</p>\n<pre><code>import numpy as np\nfrom caffe2.python import workspace, optimizer\nfrom caffe2.python.model_helper import ModelHelper\n\nbatch_size = 100\nlearning_rate= 0.1\nD1 = 60\nD2 = 40\nD3 = D2\nC = 17\n\nX = np.random.rand(batch_size, D1).astype(np.float32)\nY = (np.random.rand(batch_size) * C).astype(np.int32)\nworkspace.FeedBlob(\"X\", X)\nworkspace.FeedBlob(\"Y\", Y)\n\nmodel = ModelHelper(name='dnn_wikipedia')\n\nfc1_w = model.param_init_net.XavierFill([],'fc1_w',shape=[D2,D1])\nfc1_b = model.param_init_net.ConstantFill([],'fc1_b',shape=[D2,])\n\nfc2_w = model.param_init_net.XavierFill([],'fc2_w',shape=[D3,D2])\nfc2_b = model.param_init_net.ConstantFill([],'fc2_b',shape=[D3,])\n\nfc3_w = model.param_init_net.XavierFill([],'fc3_w',shape=[C,D3])\nfc3_b = model.param_init_net.ConstantFill([],'fc3_b',shape=[C,])\n\nz2 = model.net.FC(['X','fc1_w','fc1_b'],'z2')\na2 = model.net.Relu(['z2'],'a2')\nz3 = model.net.FC(['a2','fc2_w','fc2_b'],'z3')\na3 = model.net.Relu(['z3'],'a3')\nz4 = model.net.FC(['a3','fc3_w','fc3_b'],'z4')\nsoftmax, loss = model.net.SoftmaxWithLoss(['z4','Y'],['softmax','loss'])\nmodel.AddGradientOperators([loss])\noptimizer.build_sgd(model,base_learning_rate=learning_rate,policy=\"step\",stepsize=1,gamma=0.999)\n\nprint(model.param_init_net.Proto())\nprint(model.net.Proto())\nworkspace.RunNetOnce(model.param_init_net)\nworkspace.CreateNet(model.net)\n\nfor i in xrange(30):\n    workspace.RunNet(model.net, 10)\n    loss = workspace.FetchBlob('loss')\n    print(\"Iteration {}\".format(i)+\": {}\".format(loss))\n</code></pre>\n<pre><code>name: \"dnn_wikipedia_init\"\nop {\n  output: \"fc1_w\"\n  name: \"\"\n  type: \"XavierFill\"\n  arg {\n    name: \"shape\"\n    ints: 40\n    ints: 60\n  }\n}\nop {\n  output: \"fc1_b\"\n  name: \"\"\n  type: \"ConstantFill\"\n  arg {\n    name: \"shape\"\n    ints: 40\n  }\n}\nop {\n  output: \"fc2_w\"\n  name: \"\"\n  type: \"XavierFill\"\n  arg {\n    name: \"shape\"\n    ints: 40\n    ints: 40\n  }\n}\nop {\n  output: \"fc2_b\"\n  name: \"\"\n  type: \"ConstantFill\"\n  arg {\n    name: \"shape\"\n    ints: 40\n  }\n}\nop {\n  output: \"fc3_w\"\n  name: \"\"\n  type: \"XavierFill\"\n  arg {\n    name: \"shape\"\n    ints: 17\n    ints: 40\n  }\n}\nop {\n  output: \"fc3_b\"\n  name: \"\"\n  type: \"ConstantFill\"\n  arg {\n    name: \"shape\"\n    ints: 17\n  }\n}\n\nname: \"dnn_wikipedia\"\nop {\n  input: \"X\"\n  input: \"fc1_w\"\n  input: \"fc1_b\"\n  output: \"z2\"\n  name: \"\"\n  type: \"FC\"\n}\nop {\n  input: \"z2\"\n  output: \"a2\"\n  name: \"\"\n  type: \"Relu\"\n}\nop {\n  input: \"a2\"\n  input: \"fc2_w\"\n  input: \"fc2_b\"\n  output: \"z3\"\n  name: \"\"\n  type: \"FC\"\n}\nop {\n  input: \"z3\"\n  output: \"a3\"\n  name: \"\"\n  type: \"Relu\"\n}\nop {\n  input: \"a3\"\n  input: \"fc3_w\"\n  input: \"fc3_b\"\n  output: \"z4\"\n  name: \"\"\n  type: \"FC\"\n}\nop {\n  input: \"z4\"\n  input: \"Y\"\n  output: \"softmax\"\n  output: \"loss\"\n  name: \"\"\n  type: \"SoftmaxWithLoss\"\n}\nop {\n  input: \"loss\"\n  output: \"loss_autogen_grad\"\n  name: \"\"\n  type: \"ConstantFill\"\n  arg {\n    name: \"value\"\n    f: 1.0\n  }\n}\nop {\n  input: \"z4\"\n  input: \"Y\"\n  input: \"softmax\"\n  input: \"loss_autogen_grad\"\n  output: \"z4_grad\"\n  name: \"\"\n  type: \"SoftmaxWithLossGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"a3\"\n  input: \"fc3_w\"\n  input: \"z4_grad\"\n  output: \"fc3_w_grad\"\n  output: \"fc3_b_grad\"\n  output: \"a3_grad\"\n  name: \"\"\n  type: \"FCGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"a3\"\n  input: \"a3_grad\"\n  output: \"z3_grad\"\n  name: \"\"\n  type: \"ReluGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"a2\"\n  input: \"fc2_w\"\n  input: \"z3_grad\"\n  output: \"fc2_w_grad\"\n  output: \"fc2_b_grad\"\n  output: \"a2_grad\"\n  name: \"\"\n  type: \"FCGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"a2\"\n  input: \"a2_grad\"\n  output: \"z2_grad\"\n  name: \"\"\n  type: \"ReluGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"X\"\n  input: \"fc1_w\"\n  input: \"z2_grad\"\n  output: \"fc1_w_grad\"\n  output: \"fc1_b_grad\"\n  output: \"X_grad\"\n  name: \"\"\n  type: \"FCGradient\"\n  is_gradient_op: true\n}\nexternal_input: \"X\"\nexternal_input: \"fc1_w\"\nexternal_input: \"fc1_b\"\nexternal_input: \"fc2_w\"\nexternal_input: \"fc2_b\"\nexternal_input: \"fc3_w\"\nexternal_input: \"fc3_b\"\nexternal_input: \"Y\"\n</code></pre>\n<p>On the other hand, if I use the brew to create the model, it will work as expected.</p>\n<pre><code>import numpy as np\nfrom caffe2.python import workspace, optimizer, brew\nfrom caffe2.python.model_helper import ModelHelper\n# Set up the hyper-parameters\nbatch_size = 100\nlearning_rate= 0.1\nD1 = 60\nD2 = 40\nD3 = D2\nC = 17\n\nX = np.random.rand(batch_size, D1).astype(np.float32)\nY = (np.random.rand(batch_size) * C).astype(np.int32)\nworkspace.FeedBlob(\"X\", X)\nworkspace.FeedBlob(\"Y\", Y)\n\nmodel = ModelHelper(name='dnn_wikipedia')\nz2 = brew.fc(model,'X','z2',dim_in=D1,dim_out=D2)\na2 = brew.relu(model,'z2','a2')\nz3 = brew.fc(model,'a2','z3',dim_in=D2,dim_out=D3)\na3 = brew.relu(model,'z3','a3')\nz4 = brew.fc(model,'a3','z4',dim_in=D3,dim_out=C)\nsoftmax, loss = model.SoftmaxWithLoss(['z4','Y'],['softmax','loss'])\nmodel.AddGradientOperators([loss])\nprint(model.param_init_net.Proto())\nprint(model.net.Proto())\noptimizer.build_sgd(model,base_learning_rate=0.05)\n\nprint(model.param_init_net.Proto())\nprint(model.net.Proto())\nworkspace.RunNetOnce(model.param_init_net)\nworkspace.CreateNet(model.net)\n\nfor i in xrange(30):\n    workspace.RunNet(model.net)\n    current_loss = workspace.FetchBlob('loss')\n    print(\"Iteration {}\".format(i)+\": {}\".format(current_loss))\n\ncurrent_loss = workspace.FetchBlob('loss')\nprint(\"Loss: {}\".format(current_loss))\n</code></pre>\n<p>I am wondering what makes the difference, and if we are required (not recommended) to use brew to specify the model. I really appreciate your help!</p>\n<p>Best wishes,<br>\nBinhang</p>", "body_text": "Hi,\nI am wondering how the model helper and brew works. Here is my problem, I used the following code for a simple 3 layer NN, where I manually setup the param_init_net and net for the model; the problem is that it seems the optimizer.build_sgd() does not work, the PBs are also attached:\nimport numpy as np\nfrom caffe2.python import workspace, optimizer\nfrom caffe2.python.model_helper import ModelHelper\n\nbatch_size = 100\nlearning_rate= 0.1\nD1 = 60\nD2 = 40\nD3 = D2\nC = 17\n\nX = np.random.rand(batch_size, D1).astype(np.float32)\nY = (np.random.rand(batch_size) * C).astype(np.int32)\nworkspace.FeedBlob(\"X\", X)\nworkspace.FeedBlob(\"Y\", Y)\n\nmodel = ModelHelper(name='dnn_wikipedia')\n\nfc1_w = model.param_init_net.XavierFill([],'fc1_w',shape=[D2,D1])\nfc1_b = model.param_init_net.ConstantFill([],'fc1_b',shape=[D2,])\n\nfc2_w = model.param_init_net.XavierFill([],'fc2_w',shape=[D3,D2])\nfc2_b = model.param_init_net.ConstantFill([],'fc2_b',shape=[D3,])\n\nfc3_w = model.param_init_net.XavierFill([],'fc3_w',shape=[C,D3])\nfc3_b = model.param_init_net.ConstantFill([],'fc3_b',shape=[C,])\n\nz2 = model.net.FC(['X','fc1_w','fc1_b'],'z2')\na2 = model.net.Relu(['z2'],'a2')\nz3 = model.net.FC(['a2','fc2_w','fc2_b'],'z3')\na3 = model.net.Relu(['z3'],'a3')\nz4 = model.net.FC(['a3','fc3_w','fc3_b'],'z4')\nsoftmax, loss = model.net.SoftmaxWithLoss(['z4','Y'],['softmax','loss'])\nmodel.AddGradientOperators([loss])\noptimizer.build_sgd(model,base_learning_rate=learning_rate,policy=\"step\",stepsize=1,gamma=0.999)\n\nprint(model.param_init_net.Proto())\nprint(model.net.Proto())\nworkspace.RunNetOnce(model.param_init_net)\nworkspace.CreateNet(model.net)\n\nfor i in xrange(30):\n    workspace.RunNet(model.net, 10)\n    loss = workspace.FetchBlob('loss')\n    print(\"Iteration {}\".format(i)+\": {}\".format(loss))\n\nname: \"dnn_wikipedia_init\"\nop {\n  output: \"fc1_w\"\n  name: \"\"\n  type: \"XavierFill\"\n  arg {\n    name: \"shape\"\n    ints: 40\n    ints: 60\n  }\n}\nop {\n  output: \"fc1_b\"\n  name: \"\"\n  type: \"ConstantFill\"\n  arg {\n    name: \"shape\"\n    ints: 40\n  }\n}\nop {\n  output: \"fc2_w\"\n  name: \"\"\n  type: \"XavierFill\"\n  arg {\n    name: \"shape\"\n    ints: 40\n    ints: 40\n  }\n}\nop {\n  output: \"fc2_b\"\n  name: \"\"\n  type: \"ConstantFill\"\n  arg {\n    name: \"shape\"\n    ints: 40\n  }\n}\nop {\n  output: \"fc3_w\"\n  name: \"\"\n  type: \"XavierFill\"\n  arg {\n    name: \"shape\"\n    ints: 17\n    ints: 40\n  }\n}\nop {\n  output: \"fc3_b\"\n  name: \"\"\n  type: \"ConstantFill\"\n  arg {\n    name: \"shape\"\n    ints: 17\n  }\n}\n\nname: \"dnn_wikipedia\"\nop {\n  input: \"X\"\n  input: \"fc1_w\"\n  input: \"fc1_b\"\n  output: \"z2\"\n  name: \"\"\n  type: \"FC\"\n}\nop {\n  input: \"z2\"\n  output: \"a2\"\n  name: \"\"\n  type: \"Relu\"\n}\nop {\n  input: \"a2\"\n  input: \"fc2_w\"\n  input: \"fc2_b\"\n  output: \"z3\"\n  name: \"\"\n  type: \"FC\"\n}\nop {\n  input: \"z3\"\n  output: \"a3\"\n  name: \"\"\n  type: \"Relu\"\n}\nop {\n  input: \"a3\"\n  input: \"fc3_w\"\n  input: \"fc3_b\"\n  output: \"z4\"\n  name: \"\"\n  type: \"FC\"\n}\nop {\n  input: \"z4\"\n  input: \"Y\"\n  output: \"softmax\"\n  output: \"loss\"\n  name: \"\"\n  type: \"SoftmaxWithLoss\"\n}\nop {\n  input: \"loss\"\n  output: \"loss_autogen_grad\"\n  name: \"\"\n  type: \"ConstantFill\"\n  arg {\n    name: \"value\"\n    f: 1.0\n  }\n}\nop {\n  input: \"z4\"\n  input: \"Y\"\n  input: \"softmax\"\n  input: \"loss_autogen_grad\"\n  output: \"z4_grad\"\n  name: \"\"\n  type: \"SoftmaxWithLossGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"a3\"\n  input: \"fc3_w\"\n  input: \"z4_grad\"\n  output: \"fc3_w_grad\"\n  output: \"fc3_b_grad\"\n  output: \"a3_grad\"\n  name: \"\"\n  type: \"FCGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"a3\"\n  input: \"a3_grad\"\n  output: \"z3_grad\"\n  name: \"\"\n  type: \"ReluGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"a2\"\n  input: \"fc2_w\"\n  input: \"z3_grad\"\n  output: \"fc2_w_grad\"\n  output: \"fc2_b_grad\"\n  output: \"a2_grad\"\n  name: \"\"\n  type: \"FCGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"a2\"\n  input: \"a2_grad\"\n  output: \"z2_grad\"\n  name: \"\"\n  type: \"ReluGradient\"\n  is_gradient_op: true\n}\nop {\n  input: \"X\"\n  input: \"fc1_w\"\n  input: \"z2_grad\"\n  output: \"fc1_w_grad\"\n  output: \"fc1_b_grad\"\n  output: \"X_grad\"\n  name: \"\"\n  type: \"FCGradient\"\n  is_gradient_op: true\n}\nexternal_input: \"X\"\nexternal_input: \"fc1_w\"\nexternal_input: \"fc1_b\"\nexternal_input: \"fc2_w\"\nexternal_input: \"fc2_b\"\nexternal_input: \"fc3_w\"\nexternal_input: \"fc3_b\"\nexternal_input: \"Y\"\n\nOn the other hand, if I use the brew to create the model, it will work as expected.\nimport numpy as np\nfrom caffe2.python import workspace, optimizer, brew\nfrom caffe2.python.model_helper import ModelHelper\n# Set up the hyper-parameters\nbatch_size = 100\nlearning_rate= 0.1\nD1 = 60\nD2 = 40\nD3 = D2\nC = 17\n\nX = np.random.rand(batch_size, D1).astype(np.float32)\nY = (np.random.rand(batch_size) * C).astype(np.int32)\nworkspace.FeedBlob(\"X\", X)\nworkspace.FeedBlob(\"Y\", Y)\n\nmodel = ModelHelper(name='dnn_wikipedia')\nz2 = brew.fc(model,'X','z2',dim_in=D1,dim_out=D2)\na2 = brew.relu(model,'z2','a2')\nz3 = brew.fc(model,'a2','z3',dim_in=D2,dim_out=D3)\na3 = brew.relu(model,'z3','a3')\nz4 = brew.fc(model,'a3','z4',dim_in=D3,dim_out=C)\nsoftmax, loss = model.SoftmaxWithLoss(['z4','Y'],['softmax','loss'])\nmodel.AddGradientOperators([loss])\nprint(model.param_init_net.Proto())\nprint(model.net.Proto())\noptimizer.build_sgd(model,base_learning_rate=0.05)\n\nprint(model.param_init_net.Proto())\nprint(model.net.Proto())\nworkspace.RunNetOnce(model.param_init_net)\nworkspace.CreateNet(model.net)\n\nfor i in xrange(30):\n    workspace.RunNet(model.net)\n    current_loss = workspace.FetchBlob('loss')\n    print(\"Iteration {}\".format(i)+\": {}\".format(current_loss))\n\ncurrent_loss = workspace.FetchBlob('loss')\nprint(\"Loss: {}\".format(current_loss))\n\nI am wondering what makes the difference, and if we are required (not recommended) to use brew to specify the model. I really appreciate your help!\nBest wishes,\nBinhang", "body": "Hi,\r\n\r\nI am wondering how the model helper and brew works. Here is my problem, I used the following code for a simple 3 layer NN, where I manually setup the param_init_net and net for the model; the problem is that it seems the optimizer.build_sgd() does not work, the PBs are also attached:\r\n```\r\nimport numpy as np\r\nfrom caffe2.python import workspace, optimizer\r\nfrom caffe2.python.model_helper import ModelHelper\r\n\r\nbatch_size = 100\r\nlearning_rate= 0.1\r\nD1 = 60\r\nD2 = 40\r\nD3 = D2\r\nC = 17\r\n\r\nX = np.random.rand(batch_size, D1).astype(np.float32)\r\nY = (np.random.rand(batch_size) * C).astype(np.int32)\r\nworkspace.FeedBlob(\"X\", X)\r\nworkspace.FeedBlob(\"Y\", Y)\r\n\r\nmodel = ModelHelper(name='dnn_wikipedia')\r\n\r\nfc1_w = model.param_init_net.XavierFill([],'fc1_w',shape=[D2,D1])\r\nfc1_b = model.param_init_net.ConstantFill([],'fc1_b',shape=[D2,])\r\n\r\nfc2_w = model.param_init_net.XavierFill([],'fc2_w',shape=[D3,D2])\r\nfc2_b = model.param_init_net.ConstantFill([],'fc2_b',shape=[D3,])\r\n\r\nfc3_w = model.param_init_net.XavierFill([],'fc3_w',shape=[C,D3])\r\nfc3_b = model.param_init_net.ConstantFill([],'fc3_b',shape=[C,])\r\n\r\nz2 = model.net.FC(['X','fc1_w','fc1_b'],'z2')\r\na2 = model.net.Relu(['z2'],'a2')\r\nz3 = model.net.FC(['a2','fc2_w','fc2_b'],'z3')\r\na3 = model.net.Relu(['z3'],'a3')\r\nz4 = model.net.FC(['a3','fc3_w','fc3_b'],'z4')\r\nsoftmax, loss = model.net.SoftmaxWithLoss(['z4','Y'],['softmax','loss'])\r\nmodel.AddGradientOperators([loss])\r\noptimizer.build_sgd(model,base_learning_rate=learning_rate,policy=\"step\",stepsize=1,gamma=0.999)\r\n\r\nprint(model.param_init_net.Proto())\r\nprint(model.net.Proto())\r\nworkspace.RunNetOnce(model.param_init_net)\r\nworkspace.CreateNet(model.net)\r\n\r\nfor i in xrange(30):\r\n    workspace.RunNet(model.net, 10)\r\n    loss = workspace.FetchBlob('loss')\r\n    print(\"Iteration {}\".format(i)+\": {}\".format(loss))\r\n```\r\n\r\n```\r\nname: \"dnn_wikipedia_init\"\r\nop {\r\n  output: \"fc1_w\"\r\n  name: \"\"\r\n  type: \"XavierFill\"\r\n  arg {\r\n    name: \"shape\"\r\n    ints: 40\r\n    ints: 60\r\n  }\r\n}\r\nop {\r\n  output: \"fc1_b\"\r\n  name: \"\"\r\n  type: \"ConstantFill\"\r\n  arg {\r\n    name: \"shape\"\r\n    ints: 40\r\n  }\r\n}\r\nop {\r\n  output: \"fc2_w\"\r\n  name: \"\"\r\n  type: \"XavierFill\"\r\n  arg {\r\n    name: \"shape\"\r\n    ints: 40\r\n    ints: 40\r\n  }\r\n}\r\nop {\r\n  output: \"fc2_b\"\r\n  name: \"\"\r\n  type: \"ConstantFill\"\r\n  arg {\r\n    name: \"shape\"\r\n    ints: 40\r\n  }\r\n}\r\nop {\r\n  output: \"fc3_w\"\r\n  name: \"\"\r\n  type: \"XavierFill\"\r\n  arg {\r\n    name: \"shape\"\r\n    ints: 17\r\n    ints: 40\r\n  }\r\n}\r\nop {\r\n  output: \"fc3_b\"\r\n  name: \"\"\r\n  type: \"ConstantFill\"\r\n  arg {\r\n    name: \"shape\"\r\n    ints: 17\r\n  }\r\n}\r\n\r\nname: \"dnn_wikipedia\"\r\nop {\r\n  input: \"X\"\r\n  input: \"fc1_w\"\r\n  input: \"fc1_b\"\r\n  output: \"z2\"\r\n  name: \"\"\r\n  type: \"FC\"\r\n}\r\nop {\r\n  input: \"z2\"\r\n  output: \"a2\"\r\n  name: \"\"\r\n  type: \"Relu\"\r\n}\r\nop {\r\n  input: \"a2\"\r\n  input: \"fc2_w\"\r\n  input: \"fc2_b\"\r\n  output: \"z3\"\r\n  name: \"\"\r\n  type: \"FC\"\r\n}\r\nop {\r\n  input: \"z3\"\r\n  output: \"a3\"\r\n  name: \"\"\r\n  type: \"Relu\"\r\n}\r\nop {\r\n  input: \"a3\"\r\n  input: \"fc3_w\"\r\n  input: \"fc3_b\"\r\n  output: \"z4\"\r\n  name: \"\"\r\n  type: \"FC\"\r\n}\r\nop {\r\n  input: \"z4\"\r\n  input: \"Y\"\r\n  output: \"softmax\"\r\n  output: \"loss\"\r\n  name: \"\"\r\n  type: \"SoftmaxWithLoss\"\r\n}\r\nop {\r\n  input: \"loss\"\r\n  output: \"loss_autogen_grad\"\r\n  name: \"\"\r\n  type: \"ConstantFill\"\r\n  arg {\r\n    name: \"value\"\r\n    f: 1.0\r\n  }\r\n}\r\nop {\r\n  input: \"z4\"\r\n  input: \"Y\"\r\n  input: \"softmax\"\r\n  input: \"loss_autogen_grad\"\r\n  output: \"z4_grad\"\r\n  name: \"\"\r\n  type: \"SoftmaxWithLossGradient\"\r\n  is_gradient_op: true\r\n}\r\nop {\r\n  input: \"a3\"\r\n  input: \"fc3_w\"\r\n  input: \"z4_grad\"\r\n  output: \"fc3_w_grad\"\r\n  output: \"fc3_b_grad\"\r\n  output: \"a3_grad\"\r\n  name: \"\"\r\n  type: \"FCGradient\"\r\n  is_gradient_op: true\r\n}\r\nop {\r\n  input: \"a3\"\r\n  input: \"a3_grad\"\r\n  output: \"z3_grad\"\r\n  name: \"\"\r\n  type: \"ReluGradient\"\r\n  is_gradient_op: true\r\n}\r\nop {\r\n  input: \"a2\"\r\n  input: \"fc2_w\"\r\n  input: \"z3_grad\"\r\n  output: \"fc2_w_grad\"\r\n  output: \"fc2_b_grad\"\r\n  output: \"a2_grad\"\r\n  name: \"\"\r\n  type: \"FCGradient\"\r\n  is_gradient_op: true\r\n}\r\nop {\r\n  input: \"a2\"\r\n  input: \"a2_grad\"\r\n  output: \"z2_grad\"\r\n  name: \"\"\r\n  type: \"ReluGradient\"\r\n  is_gradient_op: true\r\n}\r\nop {\r\n  input: \"X\"\r\n  input: \"fc1_w\"\r\n  input: \"z2_grad\"\r\n  output: \"fc1_w_grad\"\r\n  output: \"fc1_b_grad\"\r\n  output: \"X_grad\"\r\n  name: \"\"\r\n  type: \"FCGradient\"\r\n  is_gradient_op: true\r\n}\r\nexternal_input: \"X\"\r\nexternal_input: \"fc1_w\"\r\nexternal_input: \"fc1_b\"\r\nexternal_input: \"fc2_w\"\r\nexternal_input: \"fc2_b\"\r\nexternal_input: \"fc3_w\"\r\nexternal_input: \"fc3_b\"\r\nexternal_input: \"Y\"\r\n```\r\nOn the other hand, if I use the brew to create the model, it will work as expected. \r\n```\r\nimport numpy as np\r\nfrom caffe2.python import workspace, optimizer, brew\r\nfrom caffe2.python.model_helper import ModelHelper\r\n# Set up the hyper-parameters\r\nbatch_size = 100\r\nlearning_rate= 0.1\r\nD1 = 60\r\nD2 = 40\r\nD3 = D2\r\nC = 17\r\n\r\nX = np.random.rand(batch_size, D1).astype(np.float32)\r\nY = (np.random.rand(batch_size) * C).astype(np.int32)\r\nworkspace.FeedBlob(\"X\", X)\r\nworkspace.FeedBlob(\"Y\", Y)\r\n\r\nmodel = ModelHelper(name='dnn_wikipedia')\r\nz2 = brew.fc(model,'X','z2',dim_in=D1,dim_out=D2)\r\na2 = brew.relu(model,'z2','a2')\r\nz3 = brew.fc(model,'a2','z3',dim_in=D2,dim_out=D3)\r\na3 = brew.relu(model,'z3','a3')\r\nz4 = brew.fc(model,'a3','z4',dim_in=D3,dim_out=C)\r\nsoftmax, loss = model.SoftmaxWithLoss(['z4','Y'],['softmax','loss'])\r\nmodel.AddGradientOperators([loss])\r\nprint(model.param_init_net.Proto())\r\nprint(model.net.Proto())\r\noptimizer.build_sgd(model,base_learning_rate=0.05)\r\n\r\nprint(model.param_init_net.Proto())\r\nprint(model.net.Proto())\r\nworkspace.RunNetOnce(model.param_init_net)\r\nworkspace.CreateNet(model.net)\r\n\r\nfor i in xrange(30):\r\n    workspace.RunNet(model.net)\r\n    current_loss = workspace.FetchBlob('loss')\r\n    print(\"Iteration {}\".format(i)+\": {}\".format(current_loss))\r\n\r\ncurrent_loss = workspace.FetchBlob('loss')\r\nprint(\"Loss: {}\".format(current_loss))\r\n```\r\nI am wondering what makes the difference, and if we are required (not recommended) to use brew to specify the model. I really appreciate your help!\r\n\r\n\r\n\r\nBest wishes,\r\nBinhang  \r\n \r\n"}