{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12474", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12474/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12474/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12474/events", "html_url": "https://github.com/pytorch/pytorch/pull/12474", "id": 368008194, "node_id": "MDExOlB1bGxSZXF1ZXN0MjIxMjg0NzQw", "number": 12474, "title": "[distributed sampler update]", "user": {"login": "jjsjann123", "id": 3709243, "node_id": "MDQ6VXNlcjM3MDkyNDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3709243?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jjsjann123", "html_url": "https://github.com/jjsjann123", "followers_url": "https://api.github.com/users/jjsjann123/followers", "following_url": "https://api.github.com/users/jjsjann123/following{/other_user}", "gists_url": "https://api.github.com/users/jjsjann123/gists{/gist_id}", "starred_url": "https://api.github.com/users/jjsjann123/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jjsjann123/subscriptions", "organizations_url": "https://api.github.com/users/jjsjann123/orgs", "repos_url": "https://api.github.com/users/jjsjann123/repos", "events_url": "https://api.github.com/users/jjsjann123/events{/privacy}", "received_events_url": "https://api.github.com/users/jjsjann123/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-09T01:34:49Z", "updated_at": "2018-10-09T18:25:17Z", "closed_at": "2018-10-09T18:25:17Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/12474", "html_url": "https://github.com/pytorch/pytorch/pull/12474", "diff_url": "https://github.com/pytorch/pytorch/pull/12474.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/12474.patch"}, "body_html": "<p>Modifies the DistributedSampler logic. Now each process samples elements with<br>\na given interval, instead of a consecutive section.</p>\n<p>This eliminates the possibility where the DataLoader uses padded data while<br>\ndropping the real data. It happens when:</p>\n<ol>\n<li>DistributedSampler padded data; and</li>\n<li>DataLoader drops_last is effectively true, and drops less then the number<br>\nof padded data.<br>\nfrom the example down, we see that data (10, 11, 12) are padded through<br>\nduplicating data sample (1, 2, 3)<br>\nThe old sampler drops legit original data (3, 6, 9) and introduces duplication<br>\n(10, 11) into the training set; while the new sampler logic samples correct data<br>\npoints from the data set.<br>\nThis example has been added to dataloader unit test</li>\n</ol>\n<p>example:</p>\n<pre><code>  data after shuffle: 1, 2, 3, 4, 5, 6, 7, 8, 9\n  padded data : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\n  old sampler:       -&gt;  DataLoader with (batch_size=2 and drop_last=True)\n   p 1: 1, 2, 3          1, 2\n   p 2: 4, 5, 6          4, 5\n   p 3: 7, 8, 9          7, 8\n   p 4:10,11,12         10,11\n\n  new sampler:       -&gt;\n   p 1: 1, 5, 9          1, 5\n   p 2: 2, 6,10          2, 6\n   p 3: 3, 7,11          3, 7\n   p 4: 4, 8,12          4, 8\n</code></pre>", "body_text": "Modifies the DistributedSampler logic. Now each process samples elements with\na given interval, instead of a consecutive section.\nThis eliminates the possibility where the DataLoader uses padded data while\ndropping the real data. It happens when:\n\nDistributedSampler padded data; and\nDataLoader drops_last is effectively true, and drops less then the number\nof padded data.\nfrom the example down, we see that data (10, 11, 12) are padded through\nduplicating data sample (1, 2, 3)\nThe old sampler drops legit original data (3, 6, 9) and introduces duplication\n(10, 11) into the training set; while the new sampler logic samples correct data\npoints from the data set.\nThis example has been added to dataloader unit test\n\nexample:\n  data after shuffle: 1, 2, 3, 4, 5, 6, 7, 8, 9\n  padded data : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n\n  old sampler:       ->  DataLoader with (batch_size=2 and drop_last=True)\n   p 1: 1, 2, 3          1, 2\n   p 2: 4, 5, 6          4, 5\n   p 3: 7, 8, 9          7, 8\n   p 4:10,11,12         10,11\n\n  new sampler:       ->\n   p 1: 1, 5, 9          1, 5\n   p 2: 2, 6,10          2, 6\n   p 3: 3, 7,11          3, 7\n   p 4: 4, 8,12          4, 8", "body": "  Modifies the DistributedSampler logic. Now each process samples elements with\r\na given interval, instead of a consecutive section.\r\n\r\n  This eliminates the possibility where the DataLoader uses padded data while\r\ndropping the real data. It happens when:\r\n  1. DistributedSampler padded data; and\r\n  2. DataLoader drops_last is effectively true, and drops less then the number\r\nof padded data.\r\n  from the example down, we see that data (10, 11, 12) are padded through\r\nduplicating data sample (1, 2, 3)\r\n  The old sampler drops legit original data (3, 6, 9) and introduces duplication\r\n(10, 11) into the training set; while the new sampler logic samples correct data\r\npoints from the data set.\r\n  This example has been added to dataloader unit test\r\n\r\nexample:\r\n```\r\n  data after shuffle: 1, 2, 3, 4, 5, 6, 7, 8, 9\r\n  padded data : 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\r\n\r\n  old sampler:       ->  DataLoader with (batch_size=2 and drop_last=True)\r\n   p 1: 1, 2, 3          1, 2\r\n   p 2: 4, 5, 6          4, 5\r\n   p 3: 7, 8, 9          7, 8\r\n   p 4:10,11,12         10,11\r\n\r\n  new sampler:       ->\r\n   p 1: 1, 5, 9          1, 5\r\n   p 2: 2, 6,10          2, 6\r\n   p 3: 3, 7,11          3, 7\r\n   p 4: 4, 8,12          4, 8\r\n```\r\n\r\n"}