{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1508", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1508/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1508/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1508/events", "html_url": "https://github.com/pytorch/pytorch/issues/1508", "id": 226938809, "node_id": "MDU6SXNzdWUyMjY5Mzg4MDk=", "number": 1508, "title": "New feature: Dropout network for bayesian approximation", "user": {"login": "isacarnekvist", "id": 5709580, "node_id": "MDQ6VXNlcjU3MDk1ODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/5709580?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isacarnekvist", "html_url": "https://github.com/isacarnekvist", "followers_url": "https://api.github.com/users/isacarnekvist/followers", "following_url": "https://api.github.com/users/isacarnekvist/following{/other_user}", "gists_url": "https://api.github.com/users/isacarnekvist/gists{/gist_id}", "starred_url": "https://api.github.com/users/isacarnekvist/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isacarnekvist/subscriptions", "organizations_url": "https://api.github.com/users/isacarnekvist/orgs", "repos_url": "https://api.github.com/users/isacarnekvist/repos", "events_url": "https://api.github.com/users/isacarnekvist/events{/privacy}", "received_events_url": "https://api.github.com/users/isacarnekvist/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-08T05:06:54Z", "updated_at": "2017-05-15T13:14:56Z", "closed_at": "2017-05-15T13:14:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>During eval, instead of averaging over all subtrees, you can randomly draw one dropout mask and use that for inference. This has a really nice behaviour which is that samples far away from training data naturally has more variance. See the paper: <a href=\"https://arxiv.org/abs/1506.02142\" rel=\"nofollow\">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a> or read the author's <a href=\"http://mlg.eng.cam.ac.uk/yarin/blog.html\" rel=\"nofollow\">blog</a> for really nice illustrations.</p>\n<p>This needs the (minor) added feature to also drop during eval, and to resample the dropout mask manually. The reason to resample manually is for you to be able to draw an entire function at once, inspired by Gaussian processes.</p>\n<p>I implemented this as a separate Dropout, called DropoutMask, but maybe it is better as a flag to Dropout? The downside of that is that there will be some special cases, like now a new Dropout object is in the backend instantiated anew for each forward pass (fix this also?), which you cannot do here since you want to keep the state.</p>", "body_text": "During eval, instead of averaging over all subtrees, you can randomly draw one dropout mask and use that for inference. This has a really nice behaviour which is that samples far away from training data naturally has more variance. See the paper: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning or read the author's blog for really nice illustrations.\nThis needs the (minor) added feature to also drop during eval, and to resample the dropout mask manually. The reason to resample manually is for you to be able to draw an entire function at once, inspired by Gaussian processes.\nI implemented this as a separate Dropout, called DropoutMask, but maybe it is better as a flag to Dropout? The downside of that is that there will be some special cases, like now a new Dropout object is in the backend instantiated anew for each forward pass (fix this also?), which you cannot do here since you want to keep the state.", "body": "During eval, instead of averaging over all subtrees, you can randomly draw one dropout mask and use that for inference. This has a really nice behaviour which is that samples far away from training data naturally has more variance. See the paper: [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/abs/1506.02142) or read the author's [blog](http://mlg.eng.cam.ac.uk/yarin/blog.html) for really nice illustrations.\r\n\r\nThis needs the (minor) added feature to also drop during eval, and to resample the dropout mask manually. The reason to resample manually is for you to be able to draw an entire function at once, inspired by Gaussian processes.\r\n\r\nI implemented this as a separate Dropout, called DropoutMask, but maybe it is better as a flag to Dropout? The downside of that is that there will be some special cases, like now a new Dropout object is in the backend instantiated anew for each forward pass (fix this also?), which you cannot do here since you want to keep the state."}