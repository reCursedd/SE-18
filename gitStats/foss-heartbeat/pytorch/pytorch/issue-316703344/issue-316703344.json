{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6855", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6855/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6855/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6855/events", "html_url": "https://github.com/pytorch/pytorch/pull/6855", "id": 316703344, "node_id": "MDExOlB1bGxSZXF1ZXN0MTgzMzU0MjA4", "number": 6855, "title": "implement gamma cuda", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-04-23T08:04:52Z", "updated_at": "2018-11-23T15:43:10Z", "closed_at": "2018-04-26T02:22:10Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6855", "html_url": "https://github.com/pytorch/pytorch/pull/6855", "diff_url": "https://github.com/pytorch/pytorch/pull/6855.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6855.patch"}, "body_html": "<ul>\n<li>Builds on previous PR by Rachit Singh <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"291411591\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4839\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4839/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4839\">#4839</a><br>\nThank you!</li>\n</ul>\n<p>Things I could use feedback on (in addition to all the needs for improvement you spot):</p>\n<ul>\n<li>I accidentally moved digamma to ATen native before noticing that it was available for CUDA and CPU in TH/THC. Should I expose the new digamma and drop the old or should I back out the new digamma and use TH/THC?</li>\n<li>My impression is that the digamma half implementation previously used float for intermediate results. I do not do this yet. Should I?</li>\n<li>One of the possion tests seems to start to fail, but I'm not entirely sure what I have changed to effect that.</li>\n<li>I'm not super happy about the ifdef's I needed to get the code to work in Distributions.h on CUDA and CPU.</li>\n</ul>", "body_text": "Builds on previous PR by Rachit Singh #4839\nThank you!\n\nThings I could use feedback on (in addition to all the needs for improvement you spot):\n\nI accidentally moved digamma to ATen native before noticing that it was available for CUDA and CPU in TH/THC. Should I expose the new digamma and drop the old or should I back out the new digamma and use TH/THC?\nMy impression is that the digamma half implementation previously used float for intermediate results. I do not do this yet. Should I?\nOne of the possion tests seems to start to fail, but I'm not entirely sure what I have changed to effect that.\nI'm not super happy about the ifdef's I needed to get the code to work in Distributions.h on CUDA and CPU.", "body": "- Builds on previous PR by Rachit Singh https://github.com/pytorch/pytorch/pull/4839 \r\n  Thank you!\r\n\r\nThings I could use feedback on (in addition to all the needs for improvement you spot):\r\n- I accidentally moved digamma to ATen native before noticing that it was available for CUDA and CPU in TH/THC. Should I expose the new digamma and drop the old or should I back out the new digamma and use TH/THC?\r\n- My impression is that the digamma half implementation previously used float for intermediate results. I do not do this yet. Should I?\r\n- One of the possion tests seems to start to fail, but I'm not entirely sure what I have changed to effect that.\r\n- I'm not super happy about the ifdef's I needed to get the code to work in Distributions.h on CUDA and CPU."}