{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183793308", "pull_request_review_id": 114859558, "id": 183793308, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Mzc5MzMwOA==", "diff_hunk": "@@ -0,0 +1,215 @@\n+#include \"TH/THMath.h\"\n+#include \"ATen/Half.h\"\n+#ifdef __CUDA_ARCH__\n+#include <nvfunctional>\n+#endif\n+\n+namespace {\n+\n+#ifdef __CUDA_ARCH__\n+#define nvfunction_or_function nvstd::function\n+#define deviceforcuda __device__\n+#else\n+#define nvfunction_or_function std::function\n+#define deviceforcuda\n+// we cannot use std::isnan directly due to some incompatibility of\n+// gcc constexpr'ing and nvcc\n+#define isnan std::isnan\n+#endif\n+\n+template<typename scalar_t>\n+struct BaseSampler {\n+  nvfunction_or_function<scalar_t(void)> sampler;\n+  deviceforcuda BaseSampler(nvfunction_or_function<scalar_t(void)> sampler): sampler(sampler) {}\n+  deviceforcuda scalar_t sample() {\n+    return sampler();\n+  }\n+};\n+\n+// The function `sample_gamma` is\n+// is adapted from Numpy's distributions.c implementation.\n+// It is MIT licensed, so here is the copyright:\n+\n+/* Copyright 2005 Robert Kern (robert.kern@gmail.com)\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining a\n+ * copy of this software and associated documentation files (the\n+ * \"Software\"), to deal in the Software without restriction, including\n+ * without limitation the rights to use, copy, modify, merge, publish,\n+ * distribute, sublicense, and/or sell copies of the Software, and to\n+ * permit persons to whom the Software is furnished to do so, subject to\n+ * the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be included\n+ * in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+*/\n+\n+template<typename scalar_t>\n+deviceforcuda scalar_t sample_gamma(scalar_t alpha, BaseSampler<scalar_t>& standard_uniform, BaseSampler<scalar_t>& standard_normal) {\n+  scalar_t scale = 1.0;\n+\n+  // Boost alpha for higher acceptance probability.\n+  if (alpha < 1.0) {\n+    scale *= std::pow(1 - standard_uniform.sample(), static_cast<scalar_t>(1.0) / alpha);", "path": "aten/src/ATen/native/Distributions.h", "position": null, "original_position": 61, "commit_id": "c225a925ee93b8d1db2eb489b16b2d1601eb63e6", "original_commit_id": "5b32203f4e2323a506d930f7312a0440f9f1990e", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "mmmm, I think it's not ok, because philox offset is increased just by one for the next initialization https://github.com/pytorch/pytorch/blob/e089849b4a2f9437c27c435bb6ffab8dfebbb43a/aten/src/ATen/native/cuda/Distributions.cu#L23-L27, whereas it would generate a few numbers here, thus offset should be increased by no less than the number of randoms generated not to repeat parts of the sequence. ", "created_at": "2018-04-24T16:12:55Z", "updated_at": "2018-11-23T15:43:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/6855#discussion_r183793308", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6855", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183793308"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6855#discussion_r183793308"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6855"}}, "body_html": "<p>mmmm, I think it's not ok, because philox offset is increased just by one for the next initialization <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/e089849b4a2f9437c27c435bb6ffab8dfebbb43a/aten/src/ATen/native/cuda/Distributions.cu#L23-L27\">pytorch/aten/src/ATen/native/cuda/Distributions.cu</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 23 to 27\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/e089849b4a2f9437c27c435bb6ffab8dfebbb43a\">e089849</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L23\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"23\"></td>\n          <td id=\"LC23\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> std::pair&lt;<span class=\"pl-c1\">uint64_t</span>, <span class=\"pl-c1\">uint64_t</span>&gt; <span class=\"pl-en\">next_philox_seed</span>(at::Generator* gen) { </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L24\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"24\"></td>\n          <td id=\"LC24\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">auto</span> gen_ = <span class=\"pl-c1\">THCRandom_getGenerator</span>(<span class=\"pl-c1\">at::globalContext</span>().<span class=\"pl-smi\">thc_state</span>); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L25\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"25\"></td>\n          <td id=\"LC25\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">uint64_t</span> offset = <span class=\"pl-c1\">THAtomicAddLong</span>(&amp;gen_-&gt;<span class=\"pl-smi\">state</span>.<span class=\"pl-smi\">philox_seed_offset</span>, <span class=\"pl-c1\">1</span>); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L26\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"26\"></td>\n          <td id=\"LC26\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">return</span> <span class=\"pl-c1\">std::make_pair</span>(gen_-&gt;<span class=\"pl-smi\">state</span>.<span class=\"pl-smi\">initial_seed</span>, offset); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L27\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"27\"></td>\n          <td id=\"LC27\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> } </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n, whereas it would generate a few numbers here, thus offset should be increased by no less than the number of randoms generated not to repeat parts of the sequence.</p>", "body_text": "mmmm, I think it's not ok, because philox offset is increased just by one for the next initialization \n  \n    \n      pytorch/aten/src/ATen/native/cuda/Distributions.cu\n    \n    \n        Lines 23 to 27\n      in\n      e089849\n    \n    \n    \n    \n\n        \n          \n           std::pair<uint64_t, uint64_t> next_philox_seed(at::Generator* gen) { \n        \n\n        \n          \n             auto gen_ = THCRandom_getGenerator(at::globalContext().thc_state); \n        \n\n        \n          \n             uint64_t offset = THAtomicAddLong(&gen_->state.philox_seed_offset, 1); \n        \n\n        \n          \n             return std::make_pair(gen_->state.initial_seed, offset); \n        \n\n        \n          \n           } \n        \n    \n  \n\n, whereas it would generate a few numbers here, thus offset should be increased by no less than the number of randoms generated not to repeat parts of the sequence.", "in_reply_to_id": 183542859}