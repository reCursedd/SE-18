{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183867531", "pull_request_review_id": 114950167, "id": 183867531, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4Mzg2NzUzMQ==", "diff_hunk": "@@ -0,0 +1,215 @@\n+#include \"TH/THMath.h\"\n+#include \"ATen/Half.h\"\n+#ifdef __CUDA_ARCH__\n+#include <nvfunctional>\n+#endif\n+\n+namespace {\n+\n+#ifdef __CUDA_ARCH__\n+#define nvfunction_or_function nvstd::function\n+#define deviceforcuda __device__\n+#else\n+#define nvfunction_or_function std::function\n+#define deviceforcuda\n+// we cannot use std::isnan directly due to some incompatibility of\n+// gcc constexpr'ing and nvcc\n+#define isnan std::isnan\n+#endif\n+\n+template<typename scalar_t>\n+struct BaseSampler {\n+  nvfunction_or_function<scalar_t(void)> sampler;\n+  deviceforcuda BaseSampler(nvfunction_or_function<scalar_t(void)> sampler): sampler(sampler) {}\n+  deviceforcuda scalar_t sample() {\n+    return sampler();\n+  }\n+};\n+\n+// The function `sample_gamma` is\n+// is adapted from Numpy's distributions.c implementation.\n+// It is MIT licensed, so here is the copyright:\n+\n+/* Copyright 2005 Robert Kern (robert.kern@gmail.com)\n+ *\n+ * Permission is hereby granted, free of charge, to any person obtaining a\n+ * copy of this software and associated documentation files (the\n+ * \"Software\"), to deal in the Software without restriction, including\n+ * without limitation the rights to use, copy, modify, merge, publish,\n+ * distribute, sublicense, and/or sell copies of the Software, and to\n+ * permit persons to whom the Software is furnished to do so, subject to\n+ * the following conditions:\n+ *\n+ * The above copyright notice and this permission notice shall be included\n+ * in all copies or substantial portions of the Software.\n+ *\n+ * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS\n+ * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n+ * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n+ * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n+ * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n+ * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n+*/\n+\n+template<typename scalar_t>\n+deviceforcuda scalar_t sample_gamma(scalar_t alpha, BaseSampler<scalar_t>& standard_uniform, BaseSampler<scalar_t>& standard_normal) {\n+  scalar_t scale = 1.0;\n+\n+  // Boost alpha for higher acceptance probability.\n+  if (alpha < 1.0) {\n+    scale *= std::pow(1 - standard_uniform.sample(), static_cast<scalar_t>(1.0) / alpha);\n+    alpha += 1.0;\n+  }\n+\n+  // This implements the acceptance-rejection method of Marsaglia and Tsang (2000)\n+  // doi:10.1145/358407.358414\n+  const scalar_t d = alpha - 1.0 / 3.0;\n+  const scalar_t c = 1.0 / std::sqrt(9.0 * d);\n+  for (;;) {\n+    scalar_t x, y;\n+    do {\n+      x = standard_normal.sample();\n+      y = 1.0 + c * x;\n+    } while (y <= 0);\n+    const scalar_t v = y * y * y;\n+    const scalar_t u = 1 - standard_uniform.sample();\n+    const scalar_t xx = x * x;\n+    if (u < 1.0 - 0.0331 * xx * xx)\n+      return scale * d * v;\n+    if (std::log(u) < 0.5 * xx + d * (1.0 - v + std::log(v)))\n+      return scale * d * v;\n+  }\n+}\n+\n+template <typename scalar_t>\n+deviceforcuda static inline scalar_t polevl(const scalar_t x,  const scalar_t A[], size_t len) {\n+  scalar_t result = 0;\n+  for (size_t i = 0; i <= len; i++) {\n+    result = result * x + A[i];\n+  }\n+  return result;\n+}\n+\n+\n+/*\n+ * The following function comes with the following copyright notice.\n+ * It has been released under the BSD license.\n+ *\n+ * Cephes Math Library Release 2.8:  June, 2000\n+ * Copyright 1984, 1987, 1992, 2000 by Stephen L. Moshier\n+ */\n+template <typename scalar_t>\n+deviceforcuda static inline scalar_t digamma_one(scalar_t x) {\n+  using acc_scalar_t = typename std::conditional<std::is_same<scalar_t, at::Half>::value, float, scalar_t>::type;", "path": "aten/src/ATen/native/Distributions.h", "position": null, "original_position": 104, "commit_id": "c225a925ee93b8d1db2eb489b16b2d1601eb63e6", "original_commit_id": "5b32203f4e2323a506d930f7312a0440f9f1990e", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "So I used just introduced `accscalar_t` in the .h and filled it as `cuda::acc_type<scalar_t>` in the .cu and `scalar_t` in the .cpp. At least in [`digammaf`, TH](https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THMath.h#L205) seems to use `float`s in the computation, so I didn't think of changing it to double.", "created_at": "2018-04-24T20:22:54Z", "updated_at": "2018-11-23T15:43:08Z", "html_url": "https://github.com/pytorch/pytorch/pull/6855#discussion_r183867531", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6855", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/183867531"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6855#discussion_r183867531"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6855"}}, "body_html": "<p>So I used just introduced <code>accscalar_t</code> in the .h and filled it as <code>cuda::acc_type&lt;scalar_t&gt;</code> in the .cu and <code>scalar_t</code> in the .cpp. At least in <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THMath.h#L205\"><code>digammaf</code>, TH</a> seems to use <code>float</code>s in the computation, so I didn't think of changing it to double.</p>", "body_text": "So I used just introduced accscalar_t in the .h and filled it as cuda::acc_type<scalar_t> in the .cu and scalar_t in the .cpp. At least in digammaf, TH seems to use floats in the computation, so I didn't think of changing it to double.", "in_reply_to_id": 183611609}