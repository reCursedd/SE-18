{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422928730", "html_url": "https://github.com/pytorch/pytorch/issues/11813#issuecomment-422928730", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11813", "id": 422928730, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjkyODczMA==", "user": {"login": "w-hc", "id": 17956191, "node_id": "MDQ6VXNlcjE3OTU2MTkx", "avatar_url": "https://avatars2.githubusercontent.com/u/17956191?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w-hc", "html_url": "https://github.com/w-hc", "followers_url": "https://api.github.com/users/w-hc/followers", "following_url": "https://api.github.com/users/w-hc/following{/other_user}", "gists_url": "https://api.github.com/users/w-hc/gists{/gist_id}", "starred_url": "https://api.github.com/users/w-hc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w-hc/subscriptions", "organizations_url": "https://api.github.com/users/w-hc/orgs", "repos_url": "https://api.github.com/users/w-hc/repos", "events_url": "https://api.github.com/users/w-hc/events{/privacy}", "received_events_url": "https://api.github.com/users/w-hc/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-19T19:26:58Z", "updated_at": "2018-09-25T04:39:02Z", "author_association": "NONE", "body_html": "<p>Well I decided to go with a simple workaround using a stateful sampler that can be saved itself.<br>\nNote the prefetched_num in state_dict. This is necessary when the parallel loading threads run ahead and call the sampler to lead the actual number of consumed iterations. This can be computed easily using</p>\n<div class=\"highlight highlight-source-python\"><pre>prefetched_num <span class=\"pl-k\">=</span> (loader_iter.send_idx <span class=\"pl-k\">-</span> loader_iter.rcvd_idx) <span class=\"pl-k\">*</span> batch_size</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch.utils.data.sampler <span class=\"pl-k\">as</span> TorchSampler\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the initial permutation is preserved across restarts</span>\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">StatefulSampler</span>(<span class=\"pl-e\">TorchSampler</span>.<span class=\"pl-e\">Sampler</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> vanilla list is orders of magnitude faster for indexing</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> while np.array/torch.Tensor is 3x more storage efficient</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">data_source</span>, <span class=\"pl-smi\">use_random</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">self</span>.use_random <span class=\"pl-k\">=</span> use_random\n        <span class=\"pl-c1\">self</span>.dset <span class=\"pl-k\">=</span> data_source\n        <span class=\"pl-c1\">self</span>._init_indices()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_init_indices</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        indices <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.use_random:\n            indices <span class=\"pl-k\">=</span> np.random.permutation(<span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.dset))\n        <span class=\"pl-k\">else</span>:\n            indices <span class=\"pl-k\">=</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.dset))\n        <span class=\"pl-c1\">self</span>.indices <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(indices)\n        <span class=\"pl-c1\">self</span>.iter_counter <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.dset)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__iter__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__next__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.iter_counter <span class=\"pl-k\">==</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.indices):\n            <span class=\"pl-c1\">self</span>._init_indices()\n            <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">StopIteration</span>()\n        <span class=\"pl-k\">else</span>:\n            elem <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.indices[<span class=\"pl-c1\">self</span>.iter_counter]\n            <span class=\"pl-c1\">self</span>.iter_counter <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n            <span class=\"pl-k\">return</span> elem\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">load_state_dict</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">state_dict</span>):\n        <span class=\"pl-c1\">self</span>.indices <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(state_dict[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>indices<span class=\"pl-pds\">'</span></span>])\n        <span class=\"pl-c1\">self</span>.iter_counter <span class=\"pl-k\">=</span> state_dict[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>iter_counter<span class=\"pl-pds\">'</span></span>]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">state_dict</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">loader_iter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        prefetched_num <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        <span class=\"pl-k\">if</span> loader_iter <span class=\"pl-k\">and</span> loader_iter.num_workers <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n            batch_size <span class=\"pl-k\">=</span> loader_iter.batch_sampler.batch_size\n            prefetched_num <span class=\"pl-k\">=</span> \\\n                (loader_iter.send_idx <span class=\"pl-k\">-</span> loader_iter.rcvd_idx) <span class=\"pl-k\">*</span> batch_size\n        <span class=\"pl-k\">return</span> {\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>indices<span class=\"pl-pds\">'</span></span>: np.array(<span class=\"pl-c1\">self</span>.indices),\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>iter_counter<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">self</span>.iter_counter <span class=\"pl-k\">-</span> prefetched_num\n        }\n</pre></div>", "body_text": "Well I decided to go with a simple workaround using a stateful sampler that can be saved itself.\nNote the prefetched_num in state_dict. This is necessary when the parallel loading threads run ahead and call the sampler to lead the actual number of consumed iterations. This can be computed easily using\nprefetched_num = (loader_iter.send_idx - loader_iter.rcvd_idx) * batch_size\nimport numpy as np\nimport torch.utils.data.sampler as TorchSampler\n\n# the initial permutation is preserved across restarts\n\n\nclass StatefulSampler(TorchSampler.Sampler):\n    # vanilla list is orders of magnitude faster for indexing\n    # while np.array/torch.Tensor is 3x more storage efficient\n    def __init__(self, data_source, use_random=True):\n        self.use_random = use_random\n        self.dset = data_source\n        self._init_indices()\n\n    def _init_indices(self):\n        indices = None\n        if self.use_random:\n            indices = np.random.permutation(len(self.dset))\n        else:\n            indices = range(len(self.dset))\n        self.indices = list(indices)\n        self.iter_counter = 0\n\n    def __len__(self):\n        return len(self.dset)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.iter_counter == len(self.indices):\n            self._init_indices()\n            raise StopIteration()\n        else:\n            elem = self.indices[self.iter_counter]\n            self.iter_counter += 1\n            return elem\n\n    def load_state_dict(self, state_dict):\n        self.indices = list(state_dict['indices'])\n        self.iter_counter = state_dict['iter_counter']\n\n    def state_dict(self, loader_iter=None):\n        prefetched_num = 0\n        if loader_iter and loader_iter.num_workers > 0:\n            batch_size = loader_iter.batch_sampler.batch_size\n            prefetched_num = \\\n                (loader_iter.send_idx - loader_iter.rcvd_idx) * batch_size\n        return {\n            'indices': np.array(self.indices),\n            'iter_counter': self.iter_counter - prefetched_num\n        }", "body": "Well I decided to go with a simple workaround using a stateful sampler that can be saved itself.\r\nNote the prefetched_num in state_dict. This is necessary when the parallel loading threads run ahead and call the sampler to lead the actual number of consumed iterations. This can be computed easily using \r\n```Python\r\nprefetched_num = (loader_iter.send_idx - loader_iter.rcvd_idx) * batch_size\r\n```\r\n```Python\r\nimport numpy as np\r\nimport torch.utils.data.sampler as TorchSampler\r\n\r\n# the initial permutation is preserved across restarts\r\n\r\n\r\nclass StatefulSampler(TorchSampler.Sampler):\r\n    # vanilla list is orders of magnitude faster for indexing\r\n    # while np.array/torch.Tensor is 3x more storage efficient\r\n    def __init__(self, data_source, use_random=True):\r\n        self.use_random = use_random\r\n        self.dset = data_source\r\n        self._init_indices()\r\n\r\n    def _init_indices(self):\r\n        indices = None\r\n        if self.use_random:\r\n            indices = np.random.permutation(len(self.dset))\r\n        else:\r\n            indices = range(len(self.dset))\r\n        self.indices = list(indices)\r\n        self.iter_counter = 0\r\n\r\n    def __len__(self):\r\n        return len(self.dset)\r\n\r\n    def __iter__(self):\r\n        return self\r\n\r\n    def __next__(self):\r\n        if self.iter_counter == len(self.indices):\r\n            self._init_indices()\r\n            raise StopIteration()\r\n        else:\r\n            elem = self.indices[self.iter_counter]\r\n            self.iter_counter += 1\r\n            return elem\r\n\r\n    def load_state_dict(self, state_dict):\r\n        self.indices = list(state_dict['indices'])\r\n        self.iter_counter = state_dict['iter_counter']\r\n\r\n    def state_dict(self, loader_iter=None):\r\n        prefetched_num = 0\r\n        if loader_iter and loader_iter.num_workers > 0:\r\n            batch_size = loader_iter.batch_sampler.batch_size\r\n            prefetched_num = \\\r\n                (loader_iter.send_idx - loader_iter.rcvd_idx) * batch_size\r\n        return {\r\n            'indices': np.array(self.indices),\r\n            'iter_counter': self.iter_counter - prefetched_num\r\n        }\r\n\r\n```"}