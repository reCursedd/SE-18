{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10490", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10490/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10490/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10490/events", "html_url": "https://github.com/pytorch/pytorch/pull/10490", "id": 350219738, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA4MTI0MTg4", "number": 10490, "title": "Remove use of data() in optimizers", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-13T22:43:19Z", "updated_at": "2018-11-23T15:49:17Z", "closed_at": "2018-08-14T20:11:27Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10490", "html_url": "https://github.com/pytorch/pytorch/pull/10490", "diff_url": "https://github.com/pytorch/pytorch/pull/10490.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10490.patch"}, "body_html": "<p>After talking to users of the C++ API we found that having the tensor type be <code>autograd::Variable</code> causes more complications than having it be <code>at::Tensor</code>. It used to be a problem because <code>at::Tensor</code> didn't have the \"autograd API\" of variable (e.g. <code>detach()</code> or <code>grad()</code> methods), but those methods are now on <code>at::Tensor</code>. As such, we want to make a last big breaking change to have the tensor type be <code>at::Tensor</code>, while factory methods like <code>torch::ones</code> will return <code>Variable</code>s disguised as <code>at::Tensor</code>. This will make many things easier, like calling functions in ATen that take vectors of tensors.</p>\n<p>This PR makes a small step in this direction by updating the optimizer classes to not use <code>.data()</code> on <code>Variable</code> to access the underlying <code>at::Tensor</code>. Using <code>.data()</code> is effectively a hack to work around our modification rules for tensors that require grad. The proper way of doing things is to use <code>with torch.no_grad</code> or equivalently <code>NoGradGuard</code> in C++ to guard in-place operations.</p>\n<p>The next step can then simply redefine <code>torch::Tensor</code> to be <code>at::Tensor</code>. This transition should be smooth, since all methods available on <code>Variable</code> are at this point available on <code>at::Tensor</code>.</p>\n<p>For this PR I:</p>\n<ol>\n<li>Modified the implementations of optimizers to not use <code>.data()</code>. This means the implementations are now different from PyTorch, which still uses the legacy method of using <code>.data</code>.</li>\n<li>To properly verify (1), I added more fine-grained test cases to our optimizer tests, e.g. <code>SGD</code> with and without <code>weight_decay</code>, then with <code>nesterov</code> etc. Generally more tests = more happy!</li>\n<li>Minor cleanup of the optimizer codebase</li>\n</ol>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3605224\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebetica\">@ebetica</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a></p>", "body_text": "After talking to users of the C++ API we found that having the tensor type be autograd::Variable causes more complications than having it be at::Tensor. It used to be a problem because at::Tensor didn't have the \"autograd API\" of variable (e.g. detach() or grad() methods), but those methods are now on at::Tensor. As such, we want to make a last big breaking change to have the tensor type be at::Tensor, while factory methods like torch::ones will return Variables disguised as at::Tensor. This will make many things easier, like calling functions in ATen that take vectors of tensors.\nThis PR makes a small step in this direction by updating the optimizer classes to not use .data() on Variable to access the underlying at::Tensor. Using .data() is effectively a hack to work around our modification rules for tensors that require grad. The proper way of doing things is to use with torch.no_grad or equivalently NoGradGuard in C++ to guard in-place operations.\nThe next step can then simply redefine torch::Tensor to be at::Tensor. This transition should be smooth, since all methods available on Variable are at this point available on at::Tensor.\nFor this PR I:\n\nModified the implementations of optimizers to not use .data(). This means the implementations are now different from PyTorch, which still uses the legacy method of using .data.\nTo properly verify (1), I added more fine-grained test cases to our optimizer tests, e.g. SGD with and without weight_decay, then with nesterov etc. Generally more tests = more happy!\nMinor cleanup of the optimizer codebase\n\n@ebetica @apaszke", "body": "After talking to users of the C++ API we found that having the tensor type be `autograd::Variable` causes more complications than having it be `at::Tensor`. It used to be a problem because `at::Tensor` didn't have the \"autograd API\" of variable (e.g. `detach()` or `grad()` methods), but those methods are now on `at::Tensor`. As such, we want to make a last big breaking change to have the tensor type be `at::Tensor`, while factory methods like `torch::ones` will return `Variable`s disguised as `at::Tensor`. This will make many things easier, like calling functions in ATen that take vectors of tensors.\r\n\r\nThis PR makes a small step in this direction by updating the optimizer classes to not use `.data()` on `Variable` to access the underlying `at::Tensor`. Using `.data()` is effectively a hack to work around our modification rules for tensors that require grad. The proper way of doing things is to use `with torch.no_grad` or equivalently `NoGradGuard` in C++ to guard in-place operations.\r\n\r\nThe next step can then simply redefine `torch::Tensor` to be `at::Tensor`. This transition should be smooth, since all methods available on `Variable` are at this point available on `at::Tensor`.\r\n\r\nFor this PR I:\r\n\r\n1. Modified the implementations of optimizers to not use `.data()`. This means the implementations are now different from PyTorch, which still uses the legacy method of using `.data`.\r\n2. To properly verify (1), I added more fine-grained test cases to our optimizer tests, e.g. `SGD` with and without `weight_decay`, then with `nesterov` etc. Generally more tests = more happy!\r\n3. Minor cleanup of the optimizer codebase\r\n\r\n@ebetica @apaszke "}