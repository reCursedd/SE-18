{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119992631", "pull_request_review_id": 41925833, "id": 119992631, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExOTk5MjYzMQ==", "diff_hunk": "@@ -10,26 +10,29 @@\n \n \n def RNNReLUCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):\n-    hy = F.relu(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))\n+    x_h = input if w_ih is None else F.linear(input, w_ih, b_ih)\n+    hy = F.relu(x_h + F.linear(hidden, w_hh, b_hh))\n     return hy\n \n \n def RNNTanhCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):\n-    hy = F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))\n+    x_h = input if w_ih is None else F.linear(input, w_ih, b_ih)\n+    hy = F.tanh(x_h + F.linear(hidden, w_hh, b_hh))\n     return hy\n \n \n def LSTMCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):\n     if input.is_cuda:\n-        igates = F.linear(input, w_ih)\n+        igates = input.expand(4, input.size(0), input.size(1)).transpose(0, 1) if w_ih is None else F.linear(input,", "path": "torch/nn/_functions/rnn.py", "position": 20, "original_position": 20, "commit_id": "3cffe76bd63300da5ed3489df7135fad390d36ff", "original_commit_id": "01f0df327118e14fb5b878fb7f7ea067578add62", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "body": "@SeanNaren I haven't looked in detail what's your implementation, but @justinchiu is correct, with multi-layer cudnn LSTMs only the first layer skips input, for all subsequent layers it is just regular LSTM layer. I also agree that cudnn behaviour even for the first layer should be changed, however, there is no ETA on when this could happen. \r\nI'm not sure multi-layer LSTMs should skip input transformation at every layer - is there a case to be made for that?\r\nI don't have a stong opinion on the degree to which pytorch backend should be matching or not matching cudnn behaviour, and I'll defer to the opinion of core devs and power rnn users on that. ", "created_at": "2017-06-03T19:01:33Z", "updated_at": "2018-11-23T15:33:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/894#discussion_r119992631", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/894", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/119992631"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/894#discussion_r119992631"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/894"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6707363\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SeanNaren\">@SeanNaren</a> I haven't looked in detail what's your implementation, but <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2389353\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/justinchiu\">@justinchiu</a> is correct, with multi-layer cudnn LSTMs only the first layer skips input, for all subsequent layers it is just regular LSTM layer. I also agree that cudnn behaviour even for the first layer should be changed, however, there is no ETA on when this could happen.<br>\nI'm not sure multi-layer LSTMs should skip input transformation at every layer - is there a case to be made for that?<br>\nI don't have a stong opinion on the degree to which pytorch backend should be matching or not matching cudnn behaviour, and I'll defer to the opinion of core devs and power rnn users on that.</p>", "body_text": "@SeanNaren I haven't looked in detail what's your implementation, but @justinchiu is correct, with multi-layer cudnn LSTMs only the first layer skips input, for all subsequent layers it is just regular LSTM layer. I also agree that cudnn behaviour even for the first layer should be changed, however, there is no ETA on when this could happen.\nI'm not sure multi-layer LSTMs should skip input transformation at every layer - is there a case to be made for that?\nI don't have a stong opinion on the degree to which pytorch backend should be matching or not matching cudnn behaviour, and I'll defer to the opinion of core devs and power rnn users on that.", "in_reply_to_id": 116390226}