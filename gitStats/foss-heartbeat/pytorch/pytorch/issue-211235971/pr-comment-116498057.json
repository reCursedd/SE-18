{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/116498057", "pull_request_review_id": 38123728, "id": 116498057, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNjQ5ODA1Nw==", "diff_hunk": "@@ -43,17 +46,18 @@ def LSTMCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):\n \n \n def GRUCell(input, hidden, w_ih, w_hh, b_ih=None, b_hh=None):\n-\n     if input.is_cuda:\n-        gi = F.linear(input, w_ih)\n+        gi = input.expand(3, input.size(0), input.size(1)).transpose(0, 1).contiguous() if w_ih is None else \\\n+            F.linear(input, w_ih)\n         gh = F.linear(hidden, w_hh)\n         state = fusedBackend.GRUFused()\n         return state(gi, gh, hidden) if b_ih is None else state(gi, gh, hidden, b_ih, b_hh)\n \n-    gi = F.linear(input, w_ih, b_ih)\n+    gi = input.expand(3, input.size(0), input.size(1)).transpose(0, 1).contiguous() if w_ih is None else \\\n+        F.linear(input, w_ih, b_ih)\n     gh = F.linear(hidden, w_hh, b_hh)\n-    i_r, i_i, i_n = gi.chunk(3, 1)\n-    h_r, h_i, h_n = gh.chunk(3, 1)\n+    i_r, i_i, i_n = torch.unbind(gi.view(input.size(0), 3, -1), 1)\n+    h_r, h_i, h_n = torch.unbind(gh.view(input.size(0), 3, -1), 1)", "path": "torch/nn/_functions/rnn.py", "position": null, "original_position": 56, "commit_id": "3cffe76bd63300da5ed3489df7135fad390d36ff", "original_commit_id": "01f0df327118e14fb5b878fb7f7ea067578add62", "user": {"login": "SeanNaren", "id": 6707363, "node_id": "MDQ6VXNlcjY3MDczNjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6707363?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SeanNaren", "html_url": "https://github.com/SeanNaren", "followers_url": "https://api.github.com/users/SeanNaren/followers", "following_url": "https://api.github.com/users/SeanNaren/following{/other_user}", "gists_url": "https://api.github.com/users/SeanNaren/gists{/gist_id}", "starred_url": "https://api.github.com/users/SeanNaren/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SeanNaren/subscriptions", "organizations_url": "https://api.github.com/users/SeanNaren/orgs", "repos_url": "https://api.github.com/users/SeanNaren/repos", "events_url": "https://api.github.com/users/SeanNaren/events{/privacy}", "received_events_url": "https://api.github.com/users/SeanNaren/received_events", "type": "User", "site_admin": false}, "body": "Wasn't an issue for pre `skip_input` cos the output would go straight into the the next time cell state, but because of skip input we have to expand the tensor which ends up a different size (I can get this back to the normal shape, but that would involve doing for example `input.expand(4, input.size(0), input.size(1)).transpose(0, 1).contiguous().view(input.size(0), -1)` for the `x_h` variable in the LSTMCell!\r\n\r\nEDIT: Also I do the extra view on the gates to make the unbind work for both skip_rnn and non skip_rnn. The original implementation just took a 2D tensor and chunked it into pieces (thus the above line is needed if we want to go back to chunks)!\r\n\r\nEDIT2: Just force the view before doing the chunk, now chunks can be used.", "created_at": "2017-05-15T14:00:31Z", "updated_at": "2018-11-23T15:33:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/894#discussion_r116498057", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/894", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/116498057"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/894#discussion_r116498057"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/894"}}, "body_html": "<p>Wasn't an issue for pre <code>skip_input</code> cos the output would go straight into the the next time cell state, but because of skip input we have to expand the tensor which ends up a different size (I can get this back to the normal shape, but that would involve doing for example <code>input.expand(4, input.size(0), input.size(1)).transpose(0, 1).contiguous().view(input.size(0), -1)</code> for the <code>x_h</code> variable in the LSTMCell!</p>\n<p>EDIT: Also I do the extra view on the gates to make the unbind work for both skip_rnn and non skip_rnn. The original implementation just took a 2D tensor and chunked it into pieces (thus the above line is needed if we want to go back to chunks)!</p>\n<p>EDIT2: Just force the view before doing the chunk, now chunks can be used.</p>", "body_text": "Wasn't an issue for pre skip_input cos the output would go straight into the the next time cell state, but because of skip input we have to expand the tensor which ends up a different size (I can get this back to the normal shape, but that would involve doing for example input.expand(4, input.size(0), input.size(1)).transpose(0, 1).contiguous().view(input.size(0), -1) for the x_h variable in the LSTMCell!\nEDIT: Also I do the extra view on the gates to make the unbind work for both skip_rnn and non skip_rnn. The original implementation just took a 2D tensor and chunked it into pieces (thus the above line is needed if we want to go back to chunks)!\nEDIT2: Just force the view before doing the chunk, now chunks can be used.", "in_reply_to_id": 116390246}