{"url": "https://api.github.com/repos/pytorch/pytorch/issues/9534", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/9534/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/9534/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/9534/events", "html_url": "https://github.com/pytorch/pytorch/issues/9534", "id": 342353624, "node_id": "MDU6SXNzdWUzNDIzNTM2MjQ=", "number": 9534, "title": "CPU hidden state tensor in GPU lstm layer causes CUDA corruption", "user": {"login": "mittagessen", "id": 3780295, "node_id": "MDQ6VXNlcjM3ODAyOTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3780295?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mittagessen", "html_url": "https://github.com/mittagessen", "followers_url": "https://api.github.com/users/mittagessen/followers", "following_url": "https://api.github.com/users/mittagessen/following{/other_user}", "gists_url": "https://api.github.com/users/mittagessen/gists{/gist_id}", "starred_url": "https://api.github.com/users/mittagessen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mittagessen/subscriptions", "organizations_url": "https://api.github.com/users/mittagessen/orgs", "repos_url": "https://api.github.com/users/mittagessen/repos", "events_url": "https://api.github.com/users/mittagessen/events{/privacy}", "received_events_url": "https://api.github.com/users/mittagessen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-07-18T14:29:07Z", "updated_at": "2018-09-19T03:27:15Z", "closed_at": "2018-09-19T03:27:15Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>This is probably halfway between feature request and bug report.</p>\n<p>When doing a forward pass on a GPU LSTM layer with a GPU input tensor but a CPU hidden state tensor, the argument type check doesn't trip, a <code>CUDNN_STATUS_EXECUTION_FAILED</code> is raised, and the CUDA stack is left in a corrupted state (<code>.to('cuda')</code> raises error 77).</p>\n<p>While running these operations is obviously stupid, doing stupid stuff shouldn't leave the CUDA environment in a broken mess.</p>\n<h2>Code example</h2>\n<pre><code>import torch\nfrom torch import nn\n\nlayer = nn.LSTM(10, 100, batch_first=True)\nlayer.to('cuda')\ni = torch.randn(1, 30, 10).to('cuda')\nh = [torch.zeros(1, 1, 100).to('cuda'), torch.zeros(1, 1, 100).to('cuda')]\nlayer(i, h) # works fine\nlayer(i, [x.to('cpu') for x in h]) # raises cudnn error\ni # accessing i raises cuda runtime error\n</code></pre>\n<h2>System Info</h2>\n<p>Scientific Linux 6.9, pytorch 0.4.0, manually compiled against cuda 8 with cudnn 7.1 because of an ancient glibc in the distribution.</p>", "body_text": "Issue description\nThis is probably halfway between feature request and bug report.\nWhen doing a forward pass on a GPU LSTM layer with a GPU input tensor but a CPU hidden state tensor, the argument type check doesn't trip, a CUDNN_STATUS_EXECUTION_FAILED is raised, and the CUDA stack is left in a corrupted state (.to('cuda') raises error 77).\nWhile running these operations is obviously stupid, doing stupid stuff shouldn't leave the CUDA environment in a broken mess.\nCode example\nimport torch\nfrom torch import nn\n\nlayer = nn.LSTM(10, 100, batch_first=True)\nlayer.to('cuda')\ni = torch.randn(1, 30, 10).to('cuda')\nh = [torch.zeros(1, 1, 100).to('cuda'), torch.zeros(1, 1, 100).to('cuda')]\nlayer(i, h) # works fine\nlayer(i, [x.to('cpu') for x in h]) # raises cudnn error\ni # accessing i raises cuda runtime error\n\nSystem Info\nScientific Linux 6.9, pytorch 0.4.0, manually compiled against cuda 8 with cudnn 7.1 because of an ancient glibc in the distribution.", "body": "## Issue description\r\n\r\nThis is probably halfway between feature request and bug report.\r\n\r\nWhen doing a forward pass on a GPU LSTM layer with a GPU input tensor but a CPU hidden state tensor, the argument type check doesn't trip, a `CUDNN_STATUS_EXECUTION_FAILED` is raised, and the CUDA stack is left in a corrupted state (`.to('cuda')` raises error 77).\r\n\r\nWhile running these operations is obviously stupid, doing stupid stuff shouldn't leave the CUDA environment in a broken mess.\r\n\r\n## Code example\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\n\r\nlayer = nn.LSTM(10, 100, batch_first=True)\r\nlayer.to('cuda')\r\ni = torch.randn(1, 30, 10).to('cuda')\r\nh = [torch.zeros(1, 1, 100).to('cuda'), torch.zeros(1, 1, 100).to('cuda')]\r\nlayer(i, h) # works fine\r\nlayer(i, [x.to('cpu') for x in h]) # raises cudnn error\r\ni # accessing i raises cuda runtime error\r\n```\r\n\r\n## System Info\r\n\r\nScientific Linux 6.9, pytorch 0.4.0, manually compiled against cuda 8 with cudnn 7.1 because of an ancient glibc in the distribution."}