{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10654", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10654/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10654/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10654/events", "html_url": "https://github.com/pytorch/pytorch/issues/10654", "id": 351796621, "node_id": "MDU6SXNzdWUzNTE3OTY2MjE=", "number": 10654, "title": "torch.jit.trace fails when the model has the function nn.functional.interpolate", "user": {"login": "labor00", "id": 36332518, "node_id": "MDQ6VXNlcjM2MzMyNTE4", "avatar_url": "https://avatars3.githubusercontent.com/u/36332518?v=4", "gravatar_id": "", "url": "https://api.github.com/users/labor00", "html_url": "https://github.com/labor00", "followers_url": "https://api.github.com/users/labor00/followers", "following_url": "https://api.github.com/users/labor00/following{/other_user}", "gists_url": "https://api.github.com/users/labor00/gists{/gist_id}", "starred_url": "https://api.github.com/users/labor00/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/labor00/subscriptions", "organizations_url": "https://api.github.com/users/labor00/orgs", "repos_url": "https://api.github.com/users/labor00/repos", "events_url": "https://api.github.com/users/labor00/events{/privacy}", "received_events_url": "https://api.github.com/users/labor00/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-08-18T06:42:20Z", "updated_at": "2018-08-20T17:42:25Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>torch.jit.trace fails when the model contain nn.functional.interpolate, returning the error:</p>\n<div class=\"highlight highlight-source-shell\"><pre>RuntimeError: invalid argument 2: input and output sizes should be greater than 0, but got input (H: 28, W: 28) output (H: 0, W: 0) at /home/disk0/.pytorch/pytorch/aten/src/THNN/generic/SpatialUpSamplingBilinear.c:19</pre></div>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.jit <span class=\"pl-k\">import</span> trace\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">test</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n        <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n                <span class=\"pl-c1\">super</span>(test, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n                <span class=\"pl-c1\">self</span>.conv <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-v\">kernel_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>,<span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n        <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n                y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv(x)\n                w <span class=\"pl-k\">=</span> nn.functional.interpolate(y, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bilinear<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">align_corners</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">scale_factor</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)\n                <span class=\"pl-k\">return</span> w\n\nf<span class=\"pl-k\">=</span>test()\nx <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">28</span>)\nret <span class=\"pl-k\">=</span> trace(x)(f)\n\n<span class=\"pl-c1\">RuntimeError</span>                              Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">65fe10b64581</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n     <span class=\"pl-c1\">14</span> f<span class=\"pl-k\">=</span>test()\n     <span class=\"pl-c1\">15</span> x <span class=\"pl-k\">=</span> torch.zeros(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">28</span>,<span class=\"pl-c1\">28</span>)\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">16</span> ret <span class=\"pl-k\">=</span> trace(x)(f)\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>jit<span class=\"pl-k\">/</span><span class=\"pl-c1\">__init__</span>.py <span class=\"pl-k\">in</span> wrapper(func)\n    <span class=\"pl-c1\">288</span> \n    <span class=\"pl-c1\">289</span>         module <span class=\"pl-k\">=</span> TopLevelTracedModule(func, <span class=\"pl-k\">**</span>executor_options)\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">290</span>         module._create_method_from_trace(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>forward<span class=\"pl-pds\">'</span></span>, func, args)\n    <span class=\"pl-c1\">291</span>         <span class=\"pl-k\">return</span> module\n    <span class=\"pl-c1\">292</span> \n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>modules<span class=\"pl-k\">/</span>module.py <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-c1\">self</span>, <span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">473</span>             hook(<span class=\"pl-c1\">self</span>, <span class=\"pl-c1\">input</span>)\n    <span class=\"pl-c1\">474</span>         <span class=\"pl-k\">if</span> torch._C._get_tracing_state():\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">475</span>             result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._slow_forward(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">476</span>         <span class=\"pl-k\">else</span>:\n    <span class=\"pl-c1\">477</span>             result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.forward(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>modules<span class=\"pl-k\">/</span>module.py <span class=\"pl-k\">in</span> _slow_forward(<span class=\"pl-c1\">self</span>, <span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">463</span>         tracing_state._traced_module_stack.append(<span class=\"pl-c1\">self</span>)\n    <span class=\"pl-c1\">464</span>         <span class=\"pl-k\">try</span>:\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">465</span>             result <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.forward(<span class=\"pl-k\">*</span><span class=\"pl-c1\">input</span>, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-c1\">466</span>         <span class=\"pl-k\">finally</span>:\n    <span class=\"pl-c1\">467</span>             tracing_state.pop_scope()\n\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">65fe10b64581</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> forward(<span class=\"pl-c1\">self</span>, x)\n      <span class=\"pl-c1\">9</span>         <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n     <span class=\"pl-c1\">10</span>                 y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.conv(x)\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">11</span>                 w <span class=\"pl-k\">=</span> nn.functional.interpolate(y, <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bilinear<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">align_corners</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">scale_factor</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)\n     <span class=\"pl-c1\">12</span>                 <span class=\"pl-k\">return</span> w\n     <span class=\"pl-c1\">13</span> \n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>torch<span class=\"pl-k\">/</span>nn<span class=\"pl-k\">/</span>functional.py <span class=\"pl-k\">in</span> interpolate(<span class=\"pl-c1\">input</span>, size, scale_factor, mode, align_corners)\n   <span class=\"pl-c1\">2069</span>         <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">NotImplementedError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Got 4D input, but linear mode needs 3D input<span class=\"pl-pds\">\"</span></span>)\n   <span class=\"pl-c1\">2070</span>     <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">input</span>.dim() <span class=\"pl-k\">==</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">and</span> mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bilinear<span class=\"pl-pds\">'</span></span>:\n<span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">2071</span>         <span class=\"pl-k\">return</span> torch._C._nn.upsample_bilinear2d(<span class=\"pl-c1\">input</span>, _output_size(<span class=\"pl-c1\">2</span>), align_corners)\n   <span class=\"pl-c1\">2072</span>     <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">input</span>.dim() <span class=\"pl-k\">==</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">and</span> mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>trilinear<span class=\"pl-pds\">'</span></span>:\n   <span class=\"pl-c1\">2073</span>         <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">NotImplementedError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Got 4D input, but trilinear mode needs 5D input<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-c1\">RuntimeError</span>: invalid argument <span class=\"pl-c1\">2</span>: <span class=\"pl-c1\">input</span> <span class=\"pl-k\">and</span> output sizes should be greater than <span class=\"pl-c1\">0</span>, but got <span class=\"pl-c1\">input</span> (H: <span class=\"pl-c1\">28</span>, W: <span class=\"pl-c1\">28</span>) output (H: <span class=\"pl-c1\">0</span>, W: <span class=\"pl-c1\">0</span>) at <span class=\"pl-k\">/</span>home<span class=\"pl-k\">/</span>disk0<span class=\"pl-k\">/</span>.pytorch<span class=\"pl-k\">/</span>pytorch<span class=\"pl-k\">/</span>aten<span class=\"pl-k\">/</span>src<span class=\"pl-k\">/</span><span class=\"pl-c1\">THNN</span><span class=\"pl-k\">/</span>generic<span class=\"pl-k\">/</span>SpatialUpSamplingBilinear.c:<span class=\"pl-c1\">19</span></pre></div>\n<h2>System Info</h2>\n<div class=\"highlight highlight-source-shell\"><pre>Collecting environment information...\nPyTorch version: 0.5.0a0+152762a\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\n\nOS: Fedora release 28 (Twenty Eight)\nGCC version: (GCC) 7.3.0\nCMake version: version 3.11.2\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.2.148\nGPU models and configuration: GPU 0: GeForce GTX 1080\nNvidia driver version: 396.45\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\n/usr/local/cuda-9.2/lib64/libcudnn_static.a</pre></div>", "body_text": "Issue description\ntorch.jit.trace fails when the model contain nn.functional.interpolate, returning the error:\nRuntimeError: invalid argument 2: input and output sizes should be greater than 0, but got input (H: 28, W: 28) output (H: 0, W: 0) at /home/disk0/.pytorch/pytorch/aten/src/THNN/generic/SpatialUpSamplingBilinear.c:19\nCode example\nimport torch\nimport torch.nn as nn\nfrom torch.jit import trace\nclass test(nn.Module):\n        def __init__(self):\n                super(test, self).__init__()\n                self.conv = nn.Conv2d(1, 32, kernel_size=3,padding=1)\n\n        def forward(self, x):\n                y = self.conv(x)\n                w = nn.functional.interpolate(y, mode='bilinear', align_corners=False, scale_factor=0.5)\n                return w\n\nf=test()\nx = torch.zeros(1,1,28,28)\nret = trace(x)(f)\n\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-1-65fe10b64581> in <module>()\n     14 f=test()\n     15 x = torch.zeros(1,1,28,28)\n---> 16 ret = trace(x)(f)\n\n~/nn/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py in wrapper(func)\n    288 \n    289         module = TopLevelTracedModule(func, **executor_options)\n--> 290         module._create_method_from_trace('forward', func, args)\n    291         return module\n    292 \n\n~/nn/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n    473             hook(self, input)\n    474         if torch._C._get_tracing_state():\n--> 475             result = self._slow_forward(*input, **kwargs)\n    476         else:\n    477             result = self.forward(*input, **kwargs)\n\n~/nn/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\n    463         tracing_state._traced_module_stack.append(self)\n    464         try:\n--> 465             result = self.forward(*input, **kwargs)\n    466         finally:\n    467             tracing_state.pop_scope()\n\n<ipython-input-1-65fe10b64581> in forward(self, x)\n      9         def forward(self, x):\n     10                 y = self.conv(x)\n---> 11                 w = nn.functional.interpolate(y, mode='bilinear', align_corners=False, scale_factor=0.5)\n     12                 return w\n     13 \n~/nn/pytorch/lib/python3.6/site-packages/torch/nn/functional.py in interpolate(input, size, scale_factor, mode, align_corners)\n   2069         raise NotImplementedError(\"Got 4D input, but linear mode needs 3D input\")\n   2070     elif input.dim() == 4 and mode == 'bilinear':\n-> 2071         return torch._C._nn.upsample_bilinear2d(input, _output_size(2), align_corners)\n   2072     elif input.dim() == 4 and mode == 'trilinear':\n   2073         raise NotImplementedError(\"Got 4D input, but trilinear mode needs 5D input\")\n\nRuntimeError: invalid argument 2: input and output sizes should be greater than 0, but got input (H: 28, W: 28) output (H: 0, W: 0) at /home/disk0/.pytorch/pytorch/aten/src/THNN/generic/SpatialUpSamplingBilinear.c:19\nSystem Info\nCollecting environment information...\nPyTorch version: 0.5.0a0+152762a\nIs debug build: No\nCUDA used to build PyTorch: 9.2.148\n\nOS: Fedora release 28 (Twenty Eight)\nGCC version: (GCC) 7.3.0\nCMake version: version 3.11.2\n\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.2.148\nGPU models and configuration: GPU 0: GeForce GTX 1080\nNvidia driver version: 396.45\ncuDNN version: Probably one of the following:\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\n/usr/local/cuda-9.2/lib64/libcudnn_static.a", "body": "## Issue description\r\n\r\ntorch.jit.trace fails when the model contain nn.functional.interpolate, returning the error:\r\n```bash\r\nRuntimeError: invalid argument 2: input and output sizes should be greater than 0, but got input (H: 28, W: 28) output (H: 0, W: 0) at /home/disk0/.pytorch/pytorch/aten/src/THNN/generic/SpatialUpSamplingBilinear.c:19\r\n```\r\n## Code example\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.jit import trace\r\nclass test(nn.Module):\r\n        def __init__(self):\r\n                super(test, self).__init__()\r\n                self.conv = nn.Conv2d(1, 32, kernel_size=3,padding=1)\r\n\r\n        def forward(self, x):\r\n                y = self.conv(x)\r\n                w = nn.functional.interpolate(y, mode='bilinear', align_corners=False, scale_factor=0.5)\r\n                return w\r\n\r\nf=test()\r\nx = torch.zeros(1,1,28,28)\r\nret = trace(x)(f)\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-1-65fe10b64581> in <module>()\r\n     14 f=test()\r\n     15 x = torch.zeros(1,1,28,28)\r\n---> 16 ret = trace(x)(f)\r\n\r\n~/nn/pytorch/lib/python3.6/site-packages/torch/jit/__init__.py in wrapper(func)\r\n    288 \r\n    289         module = TopLevelTracedModule(func, **executor_options)\r\n--> 290         module._create_method_from_trace('forward', func, args)\r\n    291         return module\r\n    292 \r\n\r\n~/nn/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    473             hook(self, input)\r\n    474         if torch._C._get_tracing_state():\r\n--> 475             result = self._slow_forward(*input, **kwargs)\r\n    476         else:\r\n    477             result = self.forward(*input, **kwargs)\r\n\r\n~/nn/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    463         tracing_state._traced_module_stack.append(self)\r\n    464         try:\r\n--> 465             result = self.forward(*input, **kwargs)\r\n    466         finally:\r\n    467             tracing_state.pop_scope()\r\n\r\n<ipython-input-1-65fe10b64581> in forward(self, x)\r\n      9         def forward(self, x):\r\n     10                 y = self.conv(x)\r\n---> 11                 w = nn.functional.interpolate(y, mode='bilinear', align_corners=False, scale_factor=0.5)\r\n     12                 return w\r\n     13 \r\n~/nn/pytorch/lib/python3.6/site-packages/torch/nn/functional.py in interpolate(input, size, scale_factor, mode, align_corners)\r\n   2069         raise NotImplementedError(\"Got 4D input, but linear mode needs 3D input\")\r\n   2070     elif input.dim() == 4 and mode == 'bilinear':\r\n-> 2071         return torch._C._nn.upsample_bilinear2d(input, _output_size(2), align_corners)\r\n   2072     elif input.dim() == 4 and mode == 'trilinear':\r\n   2073         raise NotImplementedError(\"Got 4D input, but trilinear mode needs 5D input\")\r\n\r\nRuntimeError: invalid argument 2: input and output sizes should be greater than 0, but got input (H: 28, W: 28) output (H: 0, W: 0) at /home/disk0/.pytorch/pytorch/aten/src/THNN/generic/SpatialUpSamplingBilinear.c:19\r\n```\r\n## System Info\r\n\r\n``` bash\r\nCollecting environment information...\r\nPyTorch version: 0.5.0a0+152762a\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.2.148\r\n\r\nOS: Fedora release 28 (Twenty Eight)\r\nGCC version: (GCC) 7.3.0\r\nCMake version: version 3.11.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.2.148\r\nGPU models and configuration: GPU 0: GeForce GTX 1080\r\nNvidia driver version: 396.45\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-9.2/lib64/libcudnn.so.7.2.1\r\n/usr/local/cuda-9.2/lib64/libcudnn_static.a\r\n```"}