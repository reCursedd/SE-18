{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7438", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7438/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7438/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7438/events", "html_url": "https://github.com/pytorch/pytorch/issues/7438", "id": 321723668, "node_id": "MDU6SXNzdWUzMjE3MjM2Njg=", "number": 7438, "title": "[JIT][script] PythonOp may introduce requires_grad", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-05-09T20:56:23Z", "updated_at": "2018-05-10T09:54:10Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>GraphExecutor cannot assume there is no gradient if there's a PythonOp in the graph</p>\n<p>Example model:</p>\n<pre><code>class VAE(torch.jit.ScriptModule):\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        # self.fc1 = nn.Linear(784, 400)\n        self.fc1 = nn.Linear(784, 20)\n        self.fc21 = nn.Linear(400, 20)\n        self.fc22 = nn.Linear(400, 20)\n        self.fc3 = nn.Linear(20, 400)\n        self.fc4 = nn.Linear(400, 784)\n\n    __constants__ = ['training']\n\n    @torch.jit.script_method\n    def encode(self, x):\n        # h1 = F.relu(self.fc1(x))\n        # return self.fc21(h1), self.fc22(h1)\n        h1 = self.fc1(x)\n        return h1, h1\n\n    @torch.jit.script_method\n    def reparameterize(self, mu, logvar):\n        retval = 0\n        if self.training:\n            std = torch.exp(0.5*logvar)\n            eps = torch.randn_like(std)\n            retval = eps.mul(std) + mu\n        else:\n            retval = mu\n        return retval\n\n    @torch.jit.script_method\n    def decode(self, z):\n        h3 = F.relu(self.fc3(z))\n        return F.sigmoid(self.fc4(h3))\n\n    # @torch.jit.script_method\n    def forward(self, x):\n        print('x.requires_grad', x.requires_grad)\n        mu, logvar = self.encode(x.view(-1, 784))\n        print(self.encode.graph)\n        print('mu.requires_grad', mu.requires_grad)\n        z = self.reparameterize(mu, logvar)\n        print('z.requires_grad', z.requires_grad)\n        retvals = (self.decode(z), mu, logvar)\n        print('retvals[0].requires_grad', retvals[0].requires_grad)\n        return retvals\n</code></pre>\n<p>In <code>encode</code>, <code>fc1</code> introduces <code>requires_grad</code>, but running in the interpreter does not have this behavior because <code>fc1</code> is a PythonOp calling out to <code>nn.Linear</code>.</p>\n<p>Shouldn't use fact that we're working on Variables as a signal that we need grad. Remove <code>values_are_variables</code>.</p>", "body_text": "GraphExecutor cannot assume there is no gradient if there's a PythonOp in the graph\nExample model:\nclass VAE(torch.jit.ScriptModule):\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        # self.fc1 = nn.Linear(784, 400)\n        self.fc1 = nn.Linear(784, 20)\n        self.fc21 = nn.Linear(400, 20)\n        self.fc22 = nn.Linear(400, 20)\n        self.fc3 = nn.Linear(20, 400)\n        self.fc4 = nn.Linear(400, 784)\n\n    __constants__ = ['training']\n\n    @torch.jit.script_method\n    def encode(self, x):\n        # h1 = F.relu(self.fc1(x))\n        # return self.fc21(h1), self.fc22(h1)\n        h1 = self.fc1(x)\n        return h1, h1\n\n    @torch.jit.script_method\n    def reparameterize(self, mu, logvar):\n        retval = 0\n        if self.training:\n            std = torch.exp(0.5*logvar)\n            eps = torch.randn_like(std)\n            retval = eps.mul(std) + mu\n        else:\n            retval = mu\n        return retval\n\n    @torch.jit.script_method\n    def decode(self, z):\n        h3 = F.relu(self.fc3(z))\n        return F.sigmoid(self.fc4(h3))\n\n    # @torch.jit.script_method\n    def forward(self, x):\n        print('x.requires_grad', x.requires_grad)\n        mu, logvar = self.encode(x.view(-1, 784))\n        print(self.encode.graph)\n        print('mu.requires_grad', mu.requires_grad)\n        z = self.reparameterize(mu, logvar)\n        print('z.requires_grad', z.requires_grad)\n        retvals = (self.decode(z), mu, logvar)\n        print('retvals[0].requires_grad', retvals[0].requires_grad)\n        return retvals\n\nIn encode, fc1 introduces requires_grad, but running in the interpreter does not have this behavior because fc1 is a PythonOp calling out to nn.Linear.\nShouldn't use fact that we're working on Variables as a signal that we need grad. Remove values_are_variables.", "body": "GraphExecutor cannot assume there is no gradient if there's a PythonOp in the graph\r\n\r\nExample model:\r\n\r\n```\r\nclass VAE(torch.jit.ScriptModule):\r\n    def __init__(self):\r\n        super(VAE, self).__init__()\r\n\r\n        # self.fc1 = nn.Linear(784, 400)\r\n        self.fc1 = nn.Linear(784, 20)\r\n        self.fc21 = nn.Linear(400, 20)\r\n        self.fc22 = nn.Linear(400, 20)\r\n        self.fc3 = nn.Linear(20, 400)\r\n        self.fc4 = nn.Linear(400, 784)\r\n\r\n    __constants__ = ['training']\r\n\r\n    @torch.jit.script_method\r\n    def encode(self, x):\r\n        # h1 = F.relu(self.fc1(x))\r\n        # return self.fc21(h1), self.fc22(h1)\r\n        h1 = self.fc1(x)\r\n        return h1, h1\r\n\r\n    @torch.jit.script_method\r\n    def reparameterize(self, mu, logvar):\r\n        retval = 0\r\n        if self.training:\r\n            std = torch.exp(0.5*logvar)\r\n            eps = torch.randn_like(std)\r\n            retval = eps.mul(std) + mu\r\n        else:\r\n            retval = mu\r\n        return retval\r\n\r\n    @torch.jit.script_method\r\n    def decode(self, z):\r\n        h3 = F.relu(self.fc3(z))\r\n        return F.sigmoid(self.fc4(h3))\r\n\r\n    # @torch.jit.script_method\r\n    def forward(self, x):\r\n        print('x.requires_grad', x.requires_grad)\r\n        mu, logvar = self.encode(x.view(-1, 784))\r\n        print(self.encode.graph)\r\n        print('mu.requires_grad', mu.requires_grad)\r\n        z = self.reparameterize(mu, logvar)\r\n        print('z.requires_grad', z.requires_grad)\r\n        retvals = (self.decode(z), mu, logvar)\r\n        print('retvals[0].requires_grad', retvals[0].requires_grad)\r\n        return retvals\r\n```\r\n\r\nIn `encode`, `fc1` introduces `requires_grad`, but running in the interpreter does not have this behavior because `fc1` is a PythonOp calling out to `nn.Linear`.\r\n\r\nShouldn't use fact that we're working on Variables as a signal that we need grad. Remove `values_are_variables`."}