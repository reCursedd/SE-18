{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/357480094", "html_url": "https://github.com/pytorch/pytorch/issues/3615#issuecomment-357480094", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3615", "id": 357480094, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzQ4MDA5NA==", "user": {"login": "alsrgv", "id": 16640218, "node_id": "MDQ6VXNlcjE2NjQwMjE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16640218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alsrgv", "html_url": "https://github.com/alsrgv", "followers_url": "https://api.github.com/users/alsrgv/followers", "following_url": "https://api.github.com/users/alsrgv/following{/other_user}", "gists_url": "https://api.github.com/users/alsrgv/gists{/gist_id}", "starred_url": "https://api.github.com/users/alsrgv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alsrgv/subscriptions", "organizations_url": "https://api.github.com/users/alsrgv/orgs", "repos_url": "https://api.github.com/users/alsrgv/repos", "events_url": "https://api.github.com/users/alsrgv/events{/privacy}", "received_events_url": "https://api.github.com/users/alsrgv/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-14T00:58:41Z", "updated_at": "2018-01-14T00:59:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, sorry for delayed response.  I've added <code>setTimeout()</code>, like follows:</p>\n<div class=\"highlight highlight-source-c++\"><pre>  <span class=\"pl-c\"><span class=\"pl-c\">//</span> NOTE: this function needs to be thread safe</span>\n  std::shared_ptr&lt;context_type&gt; <span class=\"pl-en\">createContext</span>(\n    <span class=\"pl-k\">const</span> DataChannelGloo::Group&amp; group,\n    <span class=\"pl-k\">const</span> std::string&amp; prefix\n  ) {\n    <span class=\"pl-k\">auto</span> context = std::make_shared&lt;context_type&gt;(\n        group.<span class=\"pl-c1\">mustGetGroupRank</span>(_rank), group.<span class=\"pl-c1\">size</span>());\n    prefix_store_type <span class=\"pl-smi\">prefix_store</span>(prefix, *group.<span class=\"pl-smi\">_store</span>);\n    context-&gt;<span class=\"pl-c1\">setTimeout</span>(<span class=\"pl-c1\">std::chrono::minutes</span>(<span class=\"pl-c1\">15</span>));\n    context-&gt;<span class=\"pl-c1\">connectFullMesh</span>(prefix_store, _device);\n    <span class=\"pl-k\">return</span> context;\n  }</pre></div>\n<p>Seems that it helped a bit, but I will hold conclusions for a little while since there is another failure mode described below.</p>\n<p>I have added distributed validation to <a href=\"https://github.com/pytorch/examples/blob/master/imagenet/main.py\">ImageNet training example</a>, like this:</p>\n<div class=\"highlight highlight-source-python\"><pre>    metrics <span class=\"pl-k\">=</span> torch.FloatTensor([losses.avg, top1.avg, top5.avg])\n    <span class=\"pl-k\">if</span> args.distributed:\n        dist.all_reduce(metrics)\n        metrics <span class=\"pl-k\">/=</span> dist.get_world_size()</pre></div>\n<p>Unfortunately, I get errors running that operation:</p>\n<pre><code>[1,4]&lt;stderr&gt;:Traceback (most recent call last):\n[1,4]&lt;stderr&gt;:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 376, in &lt;module&gt;\n[1,4]&lt;stderr&gt;:    main()\n[1,4]&lt;stderr&gt;:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 180, in main\n[1,4]&lt;stderr&gt;:    prec1 = validate(val_loader, model, criterion, epoch)\n[1,4]&lt;stderr&gt;:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 298, in validate\n[1,4]&lt;stderr&gt;:    dist.all_reduce(metrics)\n[1,4]&lt;stderr&gt;:  File \"/usr/local/lib/python2.7/dist-packages/torch/distributed/__init__.py\", line 344, in all_reduce\n[1,4]&lt;stderr&gt;:    return torch._C._dist_all_reduce(tensor, op, group)\n[1,4]&lt;stderr&gt;:RuntimeError: [/mnt/share/asergeev/torch_imagenet/pytorch/torch/lib/gloo/gloo/transport/tcp/pair.cc:696] Socket closed [xx.xx.xx.xx]:1157\n</code></pre>\n<p>I tried to add MPI barrier before that operation, but it did not help.</p>\n<p>I have tried the same code with NCCL2 transport and it works flawlessly and initializes much faster.</p>", "body_text": "@apaszke, sorry for delayed response.  I've added setTimeout(), like follows:\n  // NOTE: this function needs to be thread safe\n  std::shared_ptr<context_type> createContext(\n    const DataChannelGloo::Group& group,\n    const std::string& prefix\n  ) {\n    auto context = std::make_shared<context_type>(\n        group.mustGetGroupRank(_rank), group.size());\n    prefix_store_type prefix_store(prefix, *group._store);\n    context->setTimeout(std::chrono::minutes(15));\n    context->connectFullMesh(prefix_store, _device);\n    return context;\n  }\nSeems that it helped a bit, but I will hold conclusions for a little while since there is another failure mode described below.\nI have added distributed validation to ImageNet training example, like this:\n    metrics = torch.FloatTensor([losses.avg, top1.avg, top5.avg])\n    if args.distributed:\n        dist.all_reduce(metrics)\n        metrics /= dist.get_world_size()\nUnfortunately, I get errors running that operation:\n[1,4]<stderr>:Traceback (most recent call last):\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 376, in <module>\n[1,4]<stderr>:    main()\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 180, in main\n[1,4]<stderr>:    prec1 = validate(val_loader, model, criterion, epoch)\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 298, in validate\n[1,4]<stderr>:    dist.all_reduce(metrics)\n[1,4]<stderr>:  File \"/usr/local/lib/python2.7/dist-packages/torch/distributed/__init__.py\", line 344, in all_reduce\n[1,4]<stderr>:    return torch._C._dist_all_reduce(tensor, op, group)\n[1,4]<stderr>:RuntimeError: [/mnt/share/asergeev/torch_imagenet/pytorch/torch/lib/gloo/gloo/transport/tcp/pair.cc:696] Socket closed [xx.xx.xx.xx]:1157\n\nI tried to add MPI barrier before that operation, but it did not help.\nI have tried the same code with NCCL2 transport and it works flawlessly and initializes much faster.", "body": "@apaszke, sorry for delayed response.  I've added `setTimeout()`, like follows:\r\n```c++\r\n  // NOTE: this function needs to be thread safe\r\n  std::shared_ptr<context_type> createContext(\r\n    const DataChannelGloo::Group& group,\r\n    const std::string& prefix\r\n  ) {\r\n    auto context = std::make_shared<context_type>(\r\n        group.mustGetGroupRank(_rank), group.size());\r\n    prefix_store_type prefix_store(prefix, *group._store);\r\n    context->setTimeout(std::chrono::minutes(15));\r\n    context->connectFullMesh(prefix_store, _device);\r\n    return context;\r\n  }\r\n```\r\n\r\nSeems that it helped a bit, but I will hold conclusions for a little while since there is another failure mode described below.\r\n\r\nI have added distributed validation to [ImageNet training example](https://github.com/pytorch/examples/blob/master/imagenet/main.py), like this:\r\n```python\r\n    metrics = torch.FloatTensor([losses.avg, top1.avg, top5.avg])\r\n    if args.distributed:\r\n        dist.all_reduce(metrics)\r\n        metrics /= dist.get_world_size()\r\n```\r\n\r\nUnfortunately, I get errors running that operation:\r\n```\r\n[1,4]<stderr>:Traceback (most recent call last):\r\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 376, in <module>\r\n[1,4]<stderr>:    main()\r\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 180, in main\r\n[1,4]<stderr>:    prec1 = validate(val_loader, model, criterion, epoch)\r\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 298, in validate\r\n[1,4]<stderr>:    dist.all_reduce(metrics)\r\n[1,4]<stderr>:  File \"/usr/local/lib/python2.7/dist-packages/torch/distributed/__init__.py\", line 344, in all_reduce\r\n[1,4]<stderr>:    return torch._C._dist_all_reduce(tensor, op, group)\r\n[1,4]<stderr>:RuntimeError: [/mnt/share/asergeev/torch_imagenet/pytorch/torch/lib/gloo/gloo/transport/tcp/pair.cc:696] Socket closed [xx.xx.xx.xx]:1157\r\n```\r\n\r\nI tried to add MPI barrier before that operation, but it did not help.\r\n\r\nI have tried the same code with NCCL2 transport and it works flawlessly and initializes much faster."}