{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/354201917", "html_url": "https://github.com/pytorch/pytorch/issues/3615#issuecomment-354201917", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3615", "id": 354201917, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDIwMTkxNw==", "user": {"login": "alsrgv", "id": 16640218, "node_id": "MDQ6VXNlcjE2NjQwMjE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16640218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alsrgv", "html_url": "https://github.com/alsrgv", "followers_url": "https://api.github.com/users/alsrgv/followers", "following_url": "https://api.github.com/users/alsrgv/following{/other_user}", "gists_url": "https://api.github.com/users/alsrgv/gists{/gist_id}", "starred_url": "https://api.github.com/users/alsrgv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alsrgv/subscriptions", "organizations_url": "https://api.github.com/users/alsrgv/orgs", "repos_url": "https://api.github.com/users/alsrgv/repos", "events_url": "https://api.github.com/users/alsrgv/events{/privacy}", "received_events_url": "https://api.github.com/users/alsrgv/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-28T00:22:07Z", "updated_at": "2017-12-28T00:22:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, I have a similar problem, too. I'm running the code from the master branch that I built 2 days ago.</p>\n<p>I got Gloo working with 4 nodes with 4 GPUs each, but it fails on 32 nodes.  I tried MPI backend with <code>DistributedDataParallel</code> and found that it doesn't converge very well.</p>\n<p>I scaled back experiment to 4 nodes with 4 GPUs each and observed that MPI backend with <code>DistributedDataParallel</code> indeed converges much worse with the same batch size &amp; lr.  On the chart below, gray line is Gloo and green line is MPI.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/16640218/34396423-934bfc4e-eb1f-11e7-8fca-8bd3a9ed8b33.png\"><img width=\"714\" alt=\"screen shot 2017-12-27 at 4 03 34 pm\" src=\"https://user-images.githubusercontent.com/16640218/34396423-934bfc4e-eb1f-11e7-8fca-8bd3a9ed8b33.png\" style=\"max-width:100%;\"></a></p>\n<p>I was finally able to run large-scale experiment with doing a simple gradient averaging instead of using <code>DistributedDataParallel</code> functionality:</p>\n<pre><code>def average_gradients(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n        param.grad.data /= size\n</code></pre>\n<p>I did two more runs - one with <code>DataParallel</code> and one worker per machine (blue line), and another is with one worker per GPU (orange line).</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/16640218/34396602-059783b6-eb22-11e7-9ba1-079bcbbe5a04.png\"><img width=\"707\" alt=\"screen shot 2017-12-27 at 4 21 27 pm\" src=\"https://user-images.githubusercontent.com/16640218/34396602-059783b6-eb22-11e7-9ba1-079bcbbe5a04.png\" style=\"max-width:100%;\"></a></p>\n<p>Unfortunately, it's slow.  Any idea how to make <code>DistributedDataParallel</code> work correctly with MPI?</p>", "body_text": "@apaszke, I have a similar problem, too. I'm running the code from the master branch that I built 2 days ago.\nI got Gloo working with 4 nodes with 4 GPUs each, but it fails on 32 nodes.  I tried MPI backend with DistributedDataParallel and found that it doesn't converge very well.\nI scaled back experiment to 4 nodes with 4 GPUs each and observed that MPI backend with DistributedDataParallel indeed converges much worse with the same batch size & lr.  On the chart below, gray line is Gloo and green line is MPI.\n\nI was finally able to run large-scale experiment with doing a simple gradient averaging instead of using DistributedDataParallel functionality:\ndef average_gradients(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\n        param.grad.data /= size\n\nI did two more runs - one with DataParallel and one worker per machine (blue line), and another is with one worker per GPU (orange line).\n\nUnfortunately, it's slow.  Any idea how to make DistributedDataParallel work correctly with MPI?", "body": "@apaszke, I have a similar problem, too. I'm running the code from the master branch that I built 2 days ago.\r\n\r\nI got Gloo working with 4 nodes with 4 GPUs each, but it fails on 32 nodes.  I tried MPI backend with `DistributedDataParallel` and found that it doesn't converge very well.\r\n\r\nI scaled back experiment to 4 nodes with 4 GPUs each and observed that MPI backend with `DistributedDataParallel` indeed converges much worse with the same batch size & lr.  On the chart below, gray line is Gloo and green line is MPI. \r\n\r\n<img width=\"714\" alt=\"screen shot 2017-12-27 at 4 03 34 pm\" src=\"https://user-images.githubusercontent.com/16640218/34396423-934bfc4e-eb1f-11e7-8fca-8bd3a9ed8b33.png\">\r\n\r\nI was finally able to run large-scale experiment with doing a simple gradient averaging instead of using `DistributedDataParallel` functionality:\r\n\r\n```\r\ndef average_gradients(model):\r\n    size = float(dist.get_world_size())\r\n    for param in model.parameters():\r\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM)\r\n        param.grad.data /= size\r\n```\r\n\r\nI did two more runs - one with `DataParallel` and one worker per machine (blue line), and another is with one worker per GPU (orange line).\r\n\r\n<img width=\"707\" alt=\"screen shot 2017-12-27 at 4 21 27 pm\" src=\"https://user-images.githubusercontent.com/16640218/34396602-059783b6-eb22-11e7-9ba1-079bcbbe5a04.png\">\r\n\r\nUnfortunately, it's slow.  Any idea how to make `DistributedDataParallel` work correctly with MPI?"}