{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/388635836", "html_url": "https://github.com/pytorch/pytorch/issues/7519#issuecomment-388635836", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7519", "id": 388635836, "node_id": "MDEyOklzc3VlQ29tbWVudDM4ODYzNTgzNg==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-13T15:39:00Z", "updated_at": "2018-05-13T15:39:00Z", "author_association": "MEMBER", "body_html": "<p>Those operations are not supposed to be exactly the inverse of the other.<br>\nWhat happens is that <code>unfold</code> repeats elements of one tensor into another, and <code>fold</code> sums the corresponding elements back. If instead an average was performed, then you'd obtain the same results.</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> torch.arange(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">17</span>).reshape(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span> ,<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>)\ny <span class=\"pl-k\">=</span> torch.nn.functional.unfold(x, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\nz <span class=\"pl-k\">=</span> torch.nn.functional.fold(y, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(z <span class=\"pl-k\">/</span> x)</pre></div>\n<p>yields</p>\n<pre><code>tensor([[[[ 4.,  6.,  6.,  4.],\n          [ 6.,  9.,  9.,  6.],\n          [ 6.,  9.,  9.,  6.],\n          [ 4.,  6.,  6.,  4.]]]])\n</code></pre>\n<p>Note that except from the boundaries (because the tensor is zero-padded), the output is 9 times the input (because kernel size is 3).</p>\n<p>Also, note that you were incorrectly passing the wrong output size for <code>fold</code>, as you were not padding the input image when doing <code>unfold</code>, the output size would be smaller.</p>\n<p>Closing as this is not a bug.</p>", "body_text": "Those operations are not supposed to be exactly the inverse of the other.\nWhat happens is that unfold repeats elements of one tensor into another, and fold sums the corresponding elements back. If instead an average was performed, then you'd obtain the same results.\nx = torch.arange(1, 17).reshape(1, 1 ,4, 4)\ny = torch.nn.functional.unfold(x, 3, padding=1)\nz = torch.nn.functional.fold(y, 4, 3, padding=1)\nprint(z / x)\nyields\ntensor([[[[ 4.,  6.,  6.,  4.],\n          [ 6.,  9.,  9.,  6.],\n          [ 6.,  9.,  9.,  6.],\n          [ 4.,  6.,  6.,  4.]]]])\n\nNote that except from the boundaries (because the tensor is zero-padded), the output is 9 times the input (because kernel size is 3).\nAlso, note that you were incorrectly passing the wrong output size for fold, as you were not padding the input image when doing unfold, the output size would be smaller.\nClosing as this is not a bug.", "body": "Those operations are not supposed to be exactly the inverse of the other.\r\nWhat happens is that `unfold` repeats elements of one tensor into another, and `fold` sums the corresponding elements back. If instead an average was performed, then you'd obtain the same results.\r\n\r\n```python\r\nx = torch.arange(1, 17).reshape(1, 1 ,4, 4)\r\ny = torch.nn.functional.unfold(x, 3, padding=1)\r\nz = torch.nn.functional.fold(y, 4, 3, padding=1)\r\nprint(z / x)\r\n```\r\nyields\r\n```\r\ntensor([[[[ 4.,  6.,  6.,  4.],\r\n          [ 6.,  9.,  9.,  6.],\r\n          [ 6.,  9.,  9.,  6.],\r\n          [ 4.,  6.,  6.,  4.]]]])\r\n```\r\nNote that except from the boundaries (because the tensor is zero-padded), the output is 9 times the input (because kernel size is 3).\r\n\r\nAlso, note that you were incorrectly passing the wrong output size for `fold`, as you were not padding the input image when doing `unfold`, the output size would be smaller.\r\n\r\nClosing as this is not a bug."}