{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/351855258", "html_url": "https://github.com/pytorch/pytorch/issues/4081#issuecomment-351855258", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4081", "id": 351855258, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTg1NTI1OA==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-14T22:26:44Z", "updated_at": "2017-12-14T22:26:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Exactly, sigmoid/tanh require outputs for derivative computation (and yaml actually lists that). Generate sigmoid_ does not clone input, and saves output:</p>\n<pre><code>Tensor &amp; VariableType::sigmoid_(Tensor &amp; self) const {\n    profiler::RecordFunction profiler(\"sigmoid_\");\n    auto&amp; self_ = unpack(self, \"self\", 0);\n    check_inplace(self);\n    std::shared_ptr&lt;SigmoidBackward&gt; grad_fn;\n    auto flags = compute_flags({ self });\n    if (flags.requires_grad) {\n      grad_fn = std::make_shared&lt;SigmoidBackward&gt;();\n      grad_fn-&gt;next_functions = compute_next_functions({ self });\n    }\n    baseType-&gt;sigmoid_(self_);\n    increment_version(self);\n    set_flags(static_cast&lt;Variable&amp;&gt;(self), flags, grad_fn, true);\n    if (jit::tracer::isTracing({ self })) {\n      jit::Node *n = jit::tracer::recordTrace( \"sigmoid\", { self }, { self } );\n      (void)n;\n    }\n    if (grad_fn) {\n      grad_fn-&gt;result_ = SavedVariable(self, true);\n    }\n    return self;\n}\n</code></pre>", "body_text": "Exactly, sigmoid/tanh require outputs for derivative computation (and yaml actually lists that). Generate sigmoid_ does not clone input, and saves output:\nTensor & VariableType::sigmoid_(Tensor & self) const {\n    profiler::RecordFunction profiler(\"sigmoid_\");\n    auto& self_ = unpack(self, \"self\", 0);\n    check_inplace(self);\n    std::shared_ptr<SigmoidBackward> grad_fn;\n    auto flags = compute_flags({ self });\n    if (flags.requires_grad) {\n      grad_fn = std::make_shared<SigmoidBackward>();\n      grad_fn->next_functions = compute_next_functions({ self });\n    }\n    baseType->sigmoid_(self_);\n    increment_version(self);\n    set_flags(static_cast<Variable&>(self), flags, grad_fn, true);\n    if (jit::tracer::isTracing({ self })) {\n      jit::Node *n = jit::tracer::recordTrace( \"sigmoid\", { self }, { self } );\n      (void)n;\n    }\n    if (grad_fn) {\n      grad_fn->result_ = SavedVariable(self, true);\n    }\n    return self;\n}", "body": "Exactly, sigmoid/tanh require outputs for derivative computation (and yaml actually lists that). Generate sigmoid_ does not clone input, and saves output:\r\n```\r\nTensor & VariableType::sigmoid_(Tensor & self) const {\r\n    profiler::RecordFunction profiler(\"sigmoid_\");\r\n    auto& self_ = unpack(self, \"self\", 0);\r\n    check_inplace(self);\r\n    std::shared_ptr<SigmoidBackward> grad_fn;\r\n    auto flags = compute_flags({ self });\r\n    if (flags.requires_grad) {\r\n      grad_fn = std::make_shared<SigmoidBackward>();\r\n      grad_fn->next_functions = compute_next_functions({ self });\r\n    }\r\n    baseType->sigmoid_(self_);\r\n    increment_version(self);\r\n    set_flags(static_cast<Variable&>(self), flags, grad_fn, true);\r\n    if (jit::tracer::isTracing({ self })) {\r\n      jit::Node *n = jit::tracer::recordTrace( \"sigmoid\", { self }, { self } );\r\n      (void)n;\r\n    }\r\n    if (grad_fn) {\r\n      grad_fn->result_ = SavedVariable(self, true);\r\n    }\r\n    return self;\r\n}\r\n```"}