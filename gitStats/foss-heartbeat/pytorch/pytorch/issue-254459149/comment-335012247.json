{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/335012247", "html_url": "https://github.com/pytorch/pytorch/issues/2591#issuecomment-335012247", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2591", "id": 335012247, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTAxMjI0Nw==", "user": {"login": "alshedivat", "id": 2126561, "node_id": "MDQ6VXNlcjIxMjY1NjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2126561?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alshedivat", "html_url": "https://github.com/alshedivat", "followers_url": "https://api.github.com/users/alshedivat/followers", "following_url": "https://api.github.com/users/alshedivat/following{/other_user}", "gists_url": "https://api.github.com/users/alshedivat/gists{/gist_id}", "starred_url": "https://api.github.com/users/alshedivat/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alshedivat/subscriptions", "organizations_url": "https://api.github.com/users/alshedivat/orgs", "repos_url": "https://api.github.com/users/alshedivat/repos", "events_url": "https://api.github.com/users/alshedivat/events{/privacy}", "received_events_url": "https://api.github.com/users/alshedivat/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-08T14:59:38Z", "updated_at": "2017-10-09T19:46:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Assuming that <code>log_softmax</code> is numerically stable, here is a one-line workaround:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch.nn.functional <span class=\"pl-k\">as</span> F\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">logsumexp</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">keepdim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-k\">return</span> (inputs <span class=\"pl-k\">-</span> F.log_softmax(inputs)).mean(dim, <span class=\"pl-v\">keepdim</span><span class=\"pl-k\">=</span>keepdim)</pre></div>\n<p>Perhaps not very efficient for large sums, but it delegates everything to <code>log_softmax</code> as well as replicates its behavior (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"214876554\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1020\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1020/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1020\">#1020</a>). Having a native implementation down the line would be nice.</p>", "body_text": "Assuming that log_softmax is numerically stable, here is a one-line workaround:\nimport torch.nn.functional as F\n\ndef logsumexp(inputs, dim=None, keepdim=False):\n    return (inputs - F.log_softmax(inputs)).mean(dim, keepdim=keepdim)\nPerhaps not very efficient for large sums, but it delegates everything to log_softmax as well as replicates its behavior (#1020). Having a native implementation down the line would be nice.", "body": "Assuming that `log_softmax` is numerically stable, here is a one-line workaround:\r\n```python\r\nimport torch.nn.functional as F\r\n\r\ndef logsumexp(inputs, dim=None, keepdim=False):\r\n    return (inputs - F.log_softmax(inputs)).mean(dim, keepdim=keepdim)\r\n```\r\nPerhaps not very efficient for large sums, but it delegates everything to `log_softmax` as well as replicates its behavior (#1020). Having a native implementation down the line would be nice."}