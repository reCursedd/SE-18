{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/347896415", "html_url": "https://github.com/pytorch/pytorch/issues/3933#issuecomment-347896415", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3933", "id": 347896415, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Nzg5NjQxNQ==", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T15:31:10Z", "updated_at": "2017-11-29T15:31:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>That sounds like a bug. The Variable you pass in for <code>target</code> should have <code>requires_grad=False</code> (this is asserted in the case of other losses, I think).  For now, a workaround is to ensure that your <code>y</code> (the second arg to F.smooth_l1_loss) has <code>requires_grad=False</code>:</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nx = Variable(torch.randn(2, 2), requires_grad=True)\nt = Variable(torch.randn(2, 2), requires_grad=False)\nF.smooth_l1_loss(x, t, reduce=False)\n</code></pre>", "body_text": "That sounds like a bug. The Variable you pass in for target should have requires_grad=False (this is asserted in the case of other losses, I think).  For now, a workaround is to ensure that your y (the second arg to F.smooth_l1_loss) has requires_grad=False:\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nx = Variable(torch.randn(2, 2), requires_grad=True)\nt = Variable(torch.randn(2, 2), requires_grad=False)\nF.smooth_l1_loss(x, t, reduce=False)", "body": "That sounds like a bug. The Variable you pass in for `target` should have `requires_grad=False` (this is asserted in the case of other losses, I think).  For now, a workaround is to ensure that your `y` (the second arg to F.smooth_l1_loss) has `requires_grad=False`:\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torch.nn.functional as F\r\n\r\nx = Variable(torch.randn(2, 2), requires_grad=True)\r\nt = Variable(torch.randn(2, 2), requires_grad=False)\r\nF.smooth_l1_loss(x, t, reduce=False)\r\n```"}