{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2560", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2560/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2560/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2560/events", "html_url": "https://github.com/pytorch/pytorch/issues/2560", "id": 253672492, "node_id": "MDU6SXNzdWUyNTM2NzI0OTI=", "number": 2560, "title": "Speed difference in .backward() between pytorch versions ", "user": {"login": "lukashermann", "id": 23167230, "node_id": "MDQ6VXNlcjIzMTY3MjMw", "avatar_url": "https://avatars0.githubusercontent.com/u/23167230?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukashermann", "html_url": "https://github.com/lukashermann", "followers_url": "https://api.github.com/users/lukashermann/followers", "following_url": "https://api.github.com/users/lukashermann/following{/other_user}", "gists_url": "https://api.github.com/users/lukashermann/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukashermann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukashermann/subscriptions", "organizations_url": "https://api.github.com/users/lukashermann/orgs", "repos_url": "https://api.github.com/users/lukashermann/repos", "events_url": "https://api.github.com/users/lukashermann/events{/privacy}", "received_events_url": "https://api.github.com/users/lukashermann/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-08-29T14:00:08Z", "updated_at": "2017-08-29T18:40:34Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi, I recently <strong>upgraded to pytorch 0.2.0 from 0.1.12</strong> and noticed that it drastically slowed down an a3c implementation I tested. I investigated what could be the problem and found out that the <strong>.backward()</strong> pass took <strong>10 times longer</strong> in the new pytorch. I wrote a little test program to reproduce the speed difference:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> numpy\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n\nd <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\ntd <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span> (<span class=\"pl-c1\">500</span>):\n    a <span class=\"pl-k\">=</span> torch.randn(<span class=\"pl-c1\">1</span>,d)\n    a_vb <span class=\"pl-k\">=</span> torch.autograd.Variable(a,<span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>):\n        fc1 <span class=\"pl-k\">=</span> nn.Linear(d,d)\n        a_vb <span class=\"pl-k\">=</span> fc1(a_vb)\n    b_vb <span class=\"pl-k\">=</span> a_vb.norm()\n    t1 <span class=\"pl-k\">=</span> time.time()\n    b_vb.backward()\n    t2 <span class=\"pl-k\">=</span> time.time()\n    td<span class=\"pl-k\">+=</span> t2<span class=\"pl-k\">-</span>t1\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time: <span class=\"pl-pds\">\"</span></span>, td)</pre></div>\n<p>I set up two virtual environments (Python 2.7.6) which are identical except the pytorch version.<br>\nPytorch was installed via pip.</p>\n<pre><code>certifi==2017.7.27.1\nchardet==3.0.4\ngym==0.9.2\nidna==2.6\nmujoco-py==0.5.7\nnumpy==1.13.1\nolefile==0.44\nPillow==4.2.1\npyglet==1.2.4\nPyOpenGL==3.1.0\nPyYAML==3.12\nrequests==2.18.4\nsix==1.10.0\ntorch==0.1.12.post2 (OR RESPECTIVELY torch==0.2.0.post3)\ntorchvision==0.1.9\nurllib3==1.22\n</code></pre>\n<p>I ran it on Ubuntu 14.04.2 LTS (GNU/Linux 3.16.0-38-generic x86_64) with an Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz</p>\n<p>the environment with pytorch <strong>0.2.0</strong> took <strong>10.44</strong> sec, while the environment with pytorch <strong>0.1.12</strong> only took <strong>2.64</strong> seconds. Not as much as in the a3c algorithm, but still a weird circumstance. Does anyone know why this could be the case?</p>\n<p>The a3c implementation I tested was <a href=\"https://github.com/ShangtongZhang/DeepRL\">https://github.com/ShangtongZhang/DeepRL</a> with a3c pendulum.</p>", "body_text": "Hi, I recently upgraded to pytorch 0.2.0 from 0.1.12 and noticed that it drastically slowed down an a3c implementation I tested. I investigated what could be the problem and found out that the .backward() pass took 10 times longer in the new pytorch. I wrote a little test program to reproduce the speed difference:\nimport time\nimport torch\nimport numpy\nimport torch.nn as nn\n\nd = 100\ntd = 0\nfor i in range (500):\n    a = torch.randn(1,d)\n    a_vb = torch.autograd.Variable(a,requires_grad=True)\n    for i in range(100):\n        fc1 = nn.Linear(d,d)\n        a_vb = fc1(a_vb)\n    b_vb = a_vb.norm()\n    t1 = time.time()\n    b_vb.backward()\n    t2 = time.time()\n    td+= t2-t1\nprint(\"time: \", td)\nI set up two virtual environments (Python 2.7.6) which are identical except the pytorch version.\nPytorch was installed via pip.\ncertifi==2017.7.27.1\nchardet==3.0.4\ngym==0.9.2\nidna==2.6\nmujoco-py==0.5.7\nnumpy==1.13.1\nolefile==0.44\nPillow==4.2.1\npyglet==1.2.4\nPyOpenGL==3.1.0\nPyYAML==3.12\nrequests==2.18.4\nsix==1.10.0\ntorch==0.1.12.post2 (OR RESPECTIVELY torch==0.2.0.post3)\ntorchvision==0.1.9\nurllib3==1.22\n\nI ran it on Ubuntu 14.04.2 LTS (GNU/Linux 3.16.0-38-generic x86_64) with an Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz\nthe environment with pytorch 0.2.0 took 10.44 sec, while the environment with pytorch 0.1.12 only took 2.64 seconds. Not as much as in the a3c algorithm, but still a weird circumstance. Does anyone know why this could be the case?\nThe a3c implementation I tested was https://github.com/ShangtongZhang/DeepRL with a3c pendulum.", "body": "Hi, I recently **upgraded to pytorch 0.2.0 from 0.1.12** and noticed that it drastically slowed down an a3c implementation I tested. I investigated what could be the problem and found out that the **.backward()** pass took **10 times longer** in the new pytorch. I wrote a little test program to reproduce the speed difference:\r\n\r\n```python\r\nimport time\r\nimport torch\r\nimport numpy\r\nimport torch.nn as nn\r\n\r\nd = 100\r\ntd = 0\r\nfor i in range (500):\r\n    a = torch.randn(1,d)\r\n    a_vb = torch.autograd.Variable(a,requires_grad=True)\r\n    for i in range(100):\r\n        fc1 = nn.Linear(d,d)\r\n        a_vb = fc1(a_vb)\r\n    b_vb = a_vb.norm()\r\n    t1 = time.time()\r\n    b_vb.backward()\r\n    t2 = time.time()\r\n    td+= t2-t1\r\nprint(\"time: \", td)\r\n```\r\n\r\nI set up two virtual environments (Python 2.7.6) which are identical except the pytorch version.\r\nPytorch was installed via pip.\r\n\r\n```\r\ncertifi==2017.7.27.1\r\nchardet==3.0.4\r\ngym==0.9.2\r\nidna==2.6\r\nmujoco-py==0.5.7\r\nnumpy==1.13.1\r\nolefile==0.44\r\nPillow==4.2.1\r\npyglet==1.2.4\r\nPyOpenGL==3.1.0\r\nPyYAML==3.12\r\nrequests==2.18.4\r\nsix==1.10.0\r\ntorch==0.1.12.post2 (OR RESPECTIVELY torch==0.2.0.post3)\r\ntorchvision==0.1.9\r\nurllib3==1.22\r\n```\r\n\r\nI ran it on Ubuntu 14.04.2 LTS (GNU/Linux 3.16.0-38-generic x86_64) with an Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz\r\n\r\nthe environment with pytorch **0.2.0** took **10.44** sec, while the environment with pytorch **0.1.12** only took **2.64** seconds. Not as much as in the a3c algorithm, but still a weird circumstance. Does anyone know why this could be the case?\r\n\r\nThe a3c implementation I tested was https://github.com/ShangtongZhang/DeepRL with a3c pendulum.\r\n"}