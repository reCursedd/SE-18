{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8285", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8285/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8285/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8285/events", "html_url": "https://github.com/pytorch/pytorch/pull/8285", "id": 330718199, "node_id": "MDExOlB1bGxSZXF1ZXN0MTkzNjU5OTgx", "number": 8285, "title": "[WIP] Better support for literals in the jit", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-08T16:08:58Z", "updated_at": "2018-06-20T17:22:15Z", "closed_at": "2018-06-20T17:22:15Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8285", "html_url": "https://github.com/pytorch/pytorch/pull/8285", "diff_url": "https://github.com/pytorch/pytorch/pull/8285.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8285.patch"}, "body_html": "<p>Second iteration of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"329654438\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/8177\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/8177/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/8177\">#8177</a></p>\n<p>WIP because some test cases are failing (loop unrolling and constant propagation of \"Scalar\" args)</p>\n<p>General approach:</p>\n<ul>\n<li>Add FloatType and IntType to type.h and assign these to constant literals (python numbers)</li>\n<li>Add a <code>prim::Cast</code> node to represent implicit type conversions. The <code>prim::Cast</code> node itself holds no information; the frontend assigns types to the output of the <code>prim::Cast</code> node as it sees fit. In additional, in <code>interpreter.cpp</code>, a <code>prim::Cast</code> node is a no-op. This is helpful to have because otherwise implicit casting becomes a series of if-statements to see if one can pass in an arg for something else.</li>\n<li>Implicitly cast python numbers to Tensors (if the schema requires a Tensor) and Tensors to python numbers (if the schema requires a python number).</li>\n<li>Propagate IntType / FloatType on basic math (+-*/) on python numbers.</li>\n<li>When adding tensors and python numbers, i.e. (x + 1) <code>prim::Cast</code> node is emitted such that the input is the python number (1) and the output type is <code>DynamicType</code>. The output of the <code>prim::Cast</code> node (1) is then changed via <code>aten::type_as</code> to match the type of the tensor (<code>x</code>).</li>\n</ul>\n<p>Patches to the other parts of the JIT to make this work:</p>\n<ul>\n<li>aten_schema now labels \"int64_t\" and \"bool\" arguments as IntType. This makes it so that constant propagation of things like <code>torch.unsqueeze(dim=0)</code> works.</li>\n<li>Before, one can only re-assign a variable if the types match (one is a subtype of the other). I overrode this logic such that you can re-assign a variable if the types \"can be implicitly casted to the other type\". There's a test case where <code>var = (0, (0, 0))</code> is being reassigned a <code>(t, (t, t))</code> where <code>t</code> is a scalar tensor.</li>\n<li>All IntLists that are emitted are still treated as Tensors rather than ListType::ofInt().</li>\n</ul>\n<p>Some problems that might need to be resolved:</p>\n<ul>\n<li>Loop unrolling fails, but I haven't looked into that yet.</li>\n<li>Constant propagation stops working for things in aten_schema that have a <code>Scalar</code> argument. For example, <code>t.addmm(x, y, alpha=1.0, beta=1.0)</code> does not get transformed into <code>addmm[alpha=1.0, beta=1.0](t, x, y)</code>, but <code>t.addmm(x, y)</code> does get transformed into <code>addmm[alpha=1.0, beta=1.0](t, x, y)</code></li>\n</ul>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a></p>", "body_text": "Second iteration of #8177\nWIP because some test cases are failing (loop unrolling and constant propagation of \"Scalar\" args)\nGeneral approach:\n\nAdd FloatType and IntType to type.h and assign these to constant literals (python numbers)\nAdd a prim::Cast node to represent implicit type conversions. The prim::Cast node itself holds no information; the frontend assigns types to the output of the prim::Cast node as it sees fit. In additional, in interpreter.cpp, a prim::Cast node is a no-op. This is helpful to have because otherwise implicit casting becomes a series of if-statements to see if one can pass in an arg for something else.\nImplicitly cast python numbers to Tensors (if the schema requires a Tensor) and Tensors to python numbers (if the schema requires a python number).\nPropagate IntType / FloatType on basic math (+-*/) on python numbers.\nWhen adding tensors and python numbers, i.e. (x + 1) prim::Cast node is emitted such that the input is the python number (1) and the output type is DynamicType. The output of the prim::Cast node (1) is then changed via aten::type_as to match the type of the tensor (x).\n\nPatches to the other parts of the JIT to make this work:\n\naten_schema now labels \"int64_t\" and \"bool\" arguments as IntType. This makes it so that constant propagation of things like torch.unsqueeze(dim=0) works.\nBefore, one can only re-assign a variable if the types match (one is a subtype of the other). I overrode this logic such that you can re-assign a variable if the types \"can be implicitly casted to the other type\". There's a test case where var = (0, (0, 0)) is being reassigned a (t, (t, t)) where t is a scalar tensor.\nAll IntLists that are emitted are still treated as Tensors rather than ListType::ofInt().\n\nSome problems that might need to be resolved:\n\nLoop unrolling fails, but I haven't looked into that yet.\nConstant propagation stops working for things in aten_schema that have a Scalar argument. For example, t.addmm(x, y, alpha=1.0, beta=1.0) does not get transformed into addmm[alpha=1.0, beta=1.0](t, x, y), but t.addmm(x, y) does get transformed into addmm[alpha=1.0, beta=1.0](t, x, y)\n\ncc @zdevito", "body": "Second iteration of #8177\r\n\r\nWIP because some test cases are failing (loop unrolling and constant propagation of \"Scalar\" args)\r\n\r\nGeneral approach:\r\n- Add FloatType and IntType to type.h and assign these to constant literals (python numbers)\r\n- Add a `prim::Cast` node to represent implicit type conversions. The `prim::Cast` node itself holds no information; the frontend assigns types to the output of the `prim::Cast` node as it sees fit. In additional, in `interpreter.cpp`, a `prim::Cast` node is a no-op. This is helpful to have because otherwise implicit casting becomes a series of if-statements to see if one can pass in an arg for something else.\r\n- Implicitly cast python numbers to Tensors (if the schema requires a Tensor) and Tensors to python numbers (if the schema requires a python number). \r\n- Propagate IntType / FloatType on basic math (+-*/) on python numbers.\r\n- When adding tensors and python numbers, i.e. (x + 1) `prim::Cast` node is emitted such that the input is the python number (1) and the output type is `DynamicType`. The output of the `prim::Cast` node (1) is then changed via `aten::type_as` to match the type of the tensor (`x`).\r\n\r\nPatches to the other parts of the JIT to make this work:\r\n- aten_schema now labels \"int64_t\" and \"bool\" arguments as IntType. This makes it so that constant propagation of things like `torch.unsqueeze(dim=0)` works.\r\n- Before, one can only re-assign a variable if the types match (one is a subtype of the other). I overrode this logic such that you can re-assign a variable if the types \"can be implicitly casted to the other type\". There's a test case where `var = (0, (0, 0))` is being reassigned a `(t, (t, t))` where `t` is a scalar tensor.\r\n- All IntLists that are emitted are still treated as Tensors rather than ListType::ofInt(). \r\n\r\nSome problems that might need to be resolved:\r\n- Loop unrolling fails, but I haven't looked into that yet.\r\n- Constant propagation stops working for things in aten_schema that have a `Scalar` argument. For example, `t.addmm(x, y, alpha=1.0, beta=1.0)` does not get transformed into `addmm[alpha=1.0, beta=1.0](t, x, y)`, but `t.addmm(x, y)` does get transformed into `addmm[alpha=1.0, beta=1.0](t, x, y)`\r\n\r\ncc @zdevito "}