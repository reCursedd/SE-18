{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/113032045", "pull_request_review_id": 34390940, "id": 113032045, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMzAzMjA0NQ==", "diff_hunk": "@@ -4,192 +4,207 @@\n \n \n # TODO: no need to save all args if the grad w.r.t. some of them is not needed\n-class _BlasBase(InplaceFunction):\n-\n-    def __init__(self, alpha=1, beta=1, inplace=False):\n-        super(_BlasBase, self).__init__(inplace)\n-        self.alpha = alpha\n-        self.beta = beta\n-\n-    def _get_output(self, arg):\n-        if self.inplace:\n-            self.mark_dirty(arg)\n-            return arg\n-        else:\n-            return arg.new().resize_as_(arg)\n-\n-\n-class Addmm(_BlasBase):\n-\n-    def forward(self, add_matrix, matrix1, matrix2):\n-        self.save_for_backward(matrix1, matrix2)\n-        output = self._get_output(add_matrix)\n-        return torch.addmm(self.alpha, add_matrix, self.beta,\n+def _get_output(ctx, arg, inplace=False):\n+    if inplace:\n+        ctx.mark_dirty(arg)\n+        return arg\n+    else:\n+        return arg.new().resize_as_(arg)\n+\n+\n+class Addmm(InplaceFunction):\n+\n+    @staticmethod\n+    def forward(ctx, add_matrix, matrix1, matrix2, alpha=1, beta=1, inplace=False):\n+        ctx.alpha = alpha\n+        ctx.beta = beta\n+        ctx.save_for_backward(matrix1, matrix2)\n+        output = _get_output(ctx, add_matrix, inplace=inplace)\n+        return torch.addmm(alpha, add_matrix, beta,\n                            matrix1, matrix2, out=output)\n \n-    def backward(self, grad_output):\n-        matrix1, matrix2 = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        matrix1, matrix2 = ctx.saved_variables\n         grad_add_matrix = grad_matrix1 = grad_matrix2 = None\n \n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_add_matrix = grad_output\n-            if self.alpha != 1:\n-                grad_add_matrix = grad_add_matrix.mul(self.alpha)\n+            if ctx.alpha != 1:\n+                grad_add_matrix = grad_add_matrix.mul(ctx.alpha)\n \n-        if self.needs_input_grad[1]:\n+        if ctx.needs_input_grad[1]:\n             grad_matrix1 = torch.mm(grad_output, matrix2.t())\n-            if self.beta != 1:\n-                grad_matrix1 *= self.beta\n+            if ctx.beta != 1:\n+                grad_matrix1 *= ctx.beta\n \n-        if self.needs_input_grad[2]:\n+        if ctx.needs_input_grad[2]:\n             grad_matrix2 = torch.mm(matrix1.t(), grad_output)\n-            if self.beta != 1:\n-                grad_matrix2 *= self.beta\n+            if ctx.beta != 1:\n+                grad_matrix2 *= ctx.beta\n \n-        return grad_add_matrix, grad_matrix1, grad_matrix2\n+        return grad_add_matrix, grad_matrix1, grad_matrix2, None, None, None\n \n \n-class Addbmm(_BlasBase):\n+class Addbmm(InplaceFunction):\n \n-    def forward(self, add_matrix, batch1, batch2):\n-        self.save_for_backward(batch1, batch2)\n-        output = self._get_output(add_matrix)\n-        return torch.addbmm(self.alpha, add_matrix, self.beta,\n+    @staticmethod\n+    def forward(ctx, add_matrix, batch1, batch2, alpha=1, beta=1, inplace=False):\n+        ctx.alpha = alpha\n+        ctx.beta = beta\n+        ctx.save_for_backward(batch1, batch2)\n+        output = _get_output(ctx, add_matrix, inplace=inplace)\n+        return torch.addbmm(alpha, add_matrix, beta,\n                             batch1, batch2, out=output)\n \n-    def backward(self, grad_output):\n-        batch1, batch2 = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        batch1, batch2 = ctx.saved_variables\n         grad_add_matrix = grad_batch1 = grad_batch2 = None\n \n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_add_matrix = grad_output\n-            if self.alpha != 1:\n-                grad_add_matrix = grad_add_matrix.mul(self.alpha)\n+            if ctx.alpha != 1:\n+                grad_add_matrix = grad_add_matrix.mul(ctx.alpha)\n \n-        if any(self.needs_input_grad[1:]):\n+        if any(ctx.needs_input_grad[1:]):\n             batch_grad_output = (grad_output\n                                  .unsqueeze(0)\n                                  .expand(batch1.size(0), batch1.size(1), batch2.size(2)))\n \n-        if self.needs_input_grad[1]:\n+        if ctx.needs_input_grad[1]:\n             grad_batch1 = torch.bmm(batch_grad_output, batch2.transpose(1, 2))\n-            if self.beta != 1:\n-                grad_batch1 *= self.beta\n+            if ctx.beta != 1:\n+                grad_batch1 *= ctx.beta\n \n-        if self.needs_input_grad[2]:\n+        if ctx.needs_input_grad[2]:\n             grad_batch2 = torch.bmm(batch1.transpose(1, 2), batch_grad_output)\n-            if self.beta != 1:\n-                grad_batch2 *= self.beta\n+            if ctx.beta != 1:\n+                grad_batch2 *= ctx.beta\n \n-        return grad_add_matrix, grad_batch1, grad_batch2\n+        return grad_add_matrix, grad_batch1, grad_batch2, None, None, None\n \n \n-class Baddbmm(_BlasBase):\n+class Baddbmm(InplaceFunction):\n \n-    def forward(self, add_batch, batch1, batch2):\n-        self.save_for_backward(batch1, batch2)\n-        output = self._get_output(add_batch)\n-        return torch.baddbmm(self.alpha, add_batch, self.beta,\n+    @staticmethod\n+    def forward(ctx, add_batch, batch1, batch2, alpha=1, beta=1, inplace=False):\n+        ctx.alpha = alpha\n+        ctx.beta = beta\n+        ctx.save_for_backward(batch1, batch2)\n+        output = _get_output(ctx, add_batch, inplace=inplace)\n+        return torch.baddbmm(alpha, add_batch, beta,\n                              batch1, batch2, out=output)\n \n-    def backward(self, grad_output):\n-        batch1, batch2 = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        batch1, batch2 = ctx.saved_variables\n         grad_add_batch = grad_batch1 = grad_batch2 = None\n \n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_add_batch = grad_output\n-            if self.alpha != 1:\n-                grad_add_batch = grad_add_batch.mul(self.alpha)\n+            if ctx.alpha != 1:\n+                grad_add_batch = grad_add_batch.mul(ctx.alpha)\n \n-        if self.needs_input_grad[1]:\n+        if ctx.needs_input_grad[1]:\n             grad_batch1 = torch.bmm(grad_output, batch2.transpose(1, 2))\n-            if self.beta != 1:\n-                grad_batch1 *= self.beta\n+            if ctx.beta != 1:\n+                grad_batch1 *= ctx.beta\n \n-        if self.needs_input_grad[2]:\n+        if ctx.needs_input_grad[2]:\n             grad_batch2 = torch.bmm(batch1.transpose(1, 2), grad_output)\n-            if self.beta != 1:\n-                grad_batch2 *= self.beta\n+            if ctx.beta != 1:\n+                grad_batch2 *= ctx.beta\n \n-        return grad_add_batch, grad_batch1, grad_batch2\n+        return grad_add_batch, grad_batch1, grad_batch2, None, None, None\n \n \n-class Addmv(_BlasBase):\n+class Addmv(InplaceFunction):\n \n-    def forward(self, add_vector, matrix, vector):\n-        self.save_for_backward(matrix, vector)\n-        output = self._get_output(add_vector)\n-        return torch.addmv(self.alpha, add_vector, self.beta,\n+    @staticmethod\n+    def forward(ctx, add_vector, matrix, vector, alpha=1, beta=1, inplace=False):\n+        ctx.alpha = alpha\n+        ctx.beta = beta\n+        ctx.save_for_backward(matrix, vector)\n+        output = _get_output(ctx, add_vector, inplace=inplace)\n+        return torch.addmv(alpha, add_vector, beta,\n                            matrix, vector, out=output)\n \n-    def backward(self, grad_output):\n-        matrix, vector = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        matrix, vector = ctx.saved_variables\n         grad_add_vector = grad_matrix = grad_vector = None\n \n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_add_vector = grad_output\n-            if self.alpha != 1:\n-                grad_add_vector = grad_add_vector.mul(self.alpha)\n+            if ctx.alpha != 1:\n+                grad_add_vector = grad_add_vector.mul(ctx.alpha)\n \n-        if self.needs_input_grad[1]:\n+        if ctx.needs_input_grad[1]:\n             grad_matrix = torch.ger(grad_output, vector)\n-            if self.beta != 1:\n-                grad_matrix *= self.beta\n+            if ctx.beta != 1:\n+                grad_matrix *= ctx.beta\n \n-        if self.needs_input_grad[2]:\n+        if ctx.needs_input_grad[2]:\n             grad_vector = torch.mv(matrix.t(), grad_output)\n-            if self.beta != 1:\n-                grad_vector *= self.beta\n+            if ctx.beta != 1:\n+                grad_vector *= ctx.beta\n \n-        return grad_add_vector, grad_matrix, grad_vector\n+        return grad_add_vector, grad_matrix, grad_vector, None, None, None\n \n \n-class Addr(_BlasBase):\n+class Addr(InplaceFunction):\n \n-    def forward(self, add_matrix, vector1, vector2):\n-        self.save_for_backward(vector1, vector2)\n-        output = self._get_output(add_matrix)\n-        return torch.addr(self.alpha, add_matrix, self.beta,\n+    @staticmethod\n+    def forward(ctx, add_matrix, vector1, vector2, alpha=1, beta=1, inplace=False):\n+        ctx.alpha = alpha\n+        ctx.beta = beta\n+        ctx.save_for_backward(vector1, vector2)\n+        output = _get_output(ctx, add_matrix, inplace=inplace)\n+        return torch.addr(alpha, add_matrix, beta,\n                           vector1, vector2, out=output)\n \n-    def backward(self, grad_output):\n-        vector1, vector2 = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        vector1, vector2 = ctx.saved_variables\n         grad_add_matrix = grad_vector1 = grad_vector2 = None\n \n-        if self.needs_input_grad[0]:\n+        if ctx.needs_input_grad[0]:\n             grad_add_matrix = grad_output\n-            if self.alpha != 1:\n-                grad_add_matrix = grad_add_matrix.mul(self.alpha)\n+            if ctx.alpha != 1:\n+                grad_add_matrix = grad_add_matrix.mul(ctx.alpha)\n \n-        if self.needs_input_grad[1]:\n+        if ctx.needs_input_grad[1]:\n             grad_vector1 = torch.mv(grad_output, vector2)\n-            if self.beta != 1:\n-                grad_vector1 *= self.beta\n+            if ctx.beta != 1:\n+                grad_vector1 *= ctx.beta\n \n-        if self.needs_input_grad[2]:\n+        if ctx.needs_input_grad[2]:\n             # TODO: maybe it's better to do transpose + mv + transpose\n             grad_vector2 = torch.mm(vector1.unsqueeze(0), grad_output).squeeze(0)\n-            if self.beta != 1:\n-                grad_vector2 *= self.beta\n+            if ctx.beta != 1:\n+                grad_vector2 *= ctx.beta\n \n-        return grad_add_matrix, grad_vector1, grad_vector2\n+        return grad_add_matrix, grad_vector1, grad_vector2, None, None, None\n \n \n class Dot(Function):\n \n-    def forward(self, vector1, vector2):\n-        self.save_for_backward(vector1, vector2)\n-        self.sizes = (vector1.size(), vector2.size())\n+    @staticmethod\n+    def forward(ctx, vector1, vector2):\n+        ctx.save_for_backward(vector1, vector2)\n+        ctx.sizes = (vector1.size(), vector2.size())\n         return vector1.new((vector1.dot(vector2),))\n \n-    def backward(self, grad_output):\n-        vector1, vector2 = self.saved_tensors\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        vector1, vector2 = ctx.saved_variables\n         grad_vector1 = grad_vector2 = None\n \n-        if self.needs_input_grad[0]:\n-            grad_vector1 = vector2.mul(grad_output[0]).view(self.sizes[0])", "path": "torch/autograd/_functions/blas.py", "position": 309, "original_position": 309, "commit_id": "6835bea1cb9b266471de3811f9e4016585c97b3a", "original_commit_id": "02a643f36becc91129cbb113f5b0775493f5dc0a", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Because `dot` accepts tensors of any shape (don't ask).", "created_at": "2017-04-24T19:12:34Z", "updated_at": "2018-11-23T15:33:08Z", "html_url": "https://github.com/pytorch/pytorch/pull/1306#discussion_r113032045", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1306", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/113032045"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1306#discussion_r113032045"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1306"}}, "body_html": "<p>Because <code>dot</code> accepts tensors of any shape (don't ask).</p>", "body_text": "Because dot accepts tensors of any shape (don't ask).", "in_reply_to_id": 112816191}