{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11914", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11914/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11914/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11914/events", "html_url": "https://github.com/pytorch/pytorch/issues/11914", "id": 362405646, "node_id": "MDU6SXNzdWUzNjI0MDU2NDY=", "number": 11914, "title": "[feature request] Implement __cuda_array_interface__ for numba compatibility.", "user": {"login": "asford", "id": 282792, "node_id": "MDQ6VXNlcjI4Mjc5Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/282792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asford", "html_url": "https://github.com/asford", "followers_url": "https://api.github.com/users/asford/followers", "following_url": "https://api.github.com/users/asford/following{/other_user}", "gists_url": "https://api.github.com/users/asford/gists{/gist_id}", "starred_url": "https://api.github.com/users/asford/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asford/subscriptions", "organizations_url": "https://api.github.com/users/asford/orgs", "repos_url": "https://api.github.com/users/asford/repos", "events_url": "https://api.github.com/users/asford/events{/privacy}", "received_events_url": "https://api.github.com/users/asford/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-09-20T23:38:38Z", "updated_at": "2018-10-16T21:51:01Z", "closed_at": "2018-10-16T21:51:01Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Issue description</h2>\n<p><code>numba</code> provides generic support for jit-optimized operations over tensor data structures, with support for cpu and cuda based operations. This is an <em>ideal</em> match for accelerating pytorch operations. Numba has proposed a standardized <a href=\"http://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html\" rel=\"nofollow\">\"<code>__cuda_array_interface__</code>\"</a>, inspired by the numpy <a href=\"https://docs.scipy.org/doc/numpy/reference/arrays.interface.html\" rel=\"nofollow\"><code>__array_interface__</code></a> standard, to provide interop between cuda-based tensors.</p>\n<p>Implementation of this standard would dramatically ease PyTorch/Numba integration. This can be enabled via a monkey-patch of the current <code>torch.Tensor</code> type, however PyTorch should provide an implementation.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">torch_cuda_array_interface</span>(<span class=\"pl-smi\">tensor</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>__cuda_array_interface__ getter implementation for torch.Tensor.<span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> tensor.device.type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda<span class=\"pl-pds\">\"</span></span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> raise AttributeError for non-cuda tensors, so that</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> hasattr(cpu_tensor, \"__cuda_array_interface__\") is False.</span>\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">AttributeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Tensor is not on cuda device: <span class=\"pl-c1\">%r</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> tensor.device)\n\n    <span class=\"pl-k\">if</span> tensor.requires_grad:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> RuntimeError, matching existing tensor.__array__() behavior.</span>\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">RuntimeError</span>(\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Can't get __cuda_array_interface__ on Variable that requires grad. <span class=\"pl-pds\">\"</span></span>\n            <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Use var.detach().__cuda_array_interface__ instead.<span class=\"pl-pds\">\"</span></span>\n        )\n\n    typestr <span class=\"pl-k\">=</span> {\n        torch.float16: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f2<span class=\"pl-pds\">\"</span></span>,\n        torch.float32: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f4<span class=\"pl-pds\">\"</span></span>,\n        torch.float64: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f8<span class=\"pl-pds\">\"</span></span>,\n        torch.uint8: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>u1<span class=\"pl-pds\">\"</span></span>,\n        torch.int8: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>i1<span class=\"pl-pds\">\"</span></span>,\n        torch.int16: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>i2<span class=\"pl-pds\">\"</span></span>,\n        torch.int32: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>i4<span class=\"pl-pds\">\"</span></span>,\n        torch.int64: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>i8<span class=\"pl-pds\">\"</span></span>,\n    }[tensor.dtype]\n\n    itemsize <span class=\"pl-k\">=</span> tensor.storage().element_size()\n\n    shape <span class=\"pl-k\">=</span> tensor.shape\n    strides <span class=\"pl-k\">=</span> <span class=\"pl-c1\">tuple</span>(s <span class=\"pl-k\">*</span> itemsize <span class=\"pl-k\">for</span> s <span class=\"pl-k\">in</span> tensor.stride())\n    data <span class=\"pl-k\">=</span> (tensor.data_ptr(), <span class=\"pl-c1\">False</span>)\n\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">dict</span>(<span class=\"pl-v\">typestr</span><span class=\"pl-k\">=</span>typestr, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>shape, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>strides, <span class=\"pl-v\">data</span><span class=\"pl-k\">=</span>data, <span class=\"pl-v\">version</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n\n\ntorch.Tensor.__cuda_array_interface__ <span class=\"pl-k\">=</span> <span class=\"pl-c1\">property</span>(torch_cuda_array_interface)</pre></div>", "body_text": "Issue description\nnumba provides generic support for jit-optimized operations over tensor data structures, with support for cpu and cuda based operations. This is an ideal match for accelerating pytorch operations. Numba has proposed a standardized \"__cuda_array_interface__\", inspired by the numpy __array_interface__ standard, to provide interop between cuda-based tensors.\nImplementation of this standard would dramatically ease PyTorch/Numba integration. This can be enabled via a monkey-patch of the current torch.Tensor type, however PyTorch should provide an implementation.\nCode example\nimport torch\n\n\ndef torch_cuda_array_interface(tensor):\n    \"\"\"__cuda_array_interface__ getter implementation for torch.Tensor.\"\"\"\n\n    if not tensor.device.type == \"cuda\":\n        # raise AttributeError for non-cuda tensors, so that\n        # hasattr(cpu_tensor, \"__cuda_array_interface__\") is False.\n        raise AttributeError(\"Tensor is not on cuda device: %r\" % tensor.device)\n\n    if tensor.requires_grad:\n        # RuntimeError, matching existing tensor.__array__() behavior.\n        raise RuntimeError(\n            \"Can't get __cuda_array_interface__ on Variable that requires grad. \"\n            \"Use var.detach().__cuda_array_interface__ instead.\"\n        )\n\n    typestr = {\n        torch.float16: \"f2\",\n        torch.float32: \"f4\",\n        torch.float64: \"f8\",\n        torch.uint8: \"u1\",\n        torch.int8: \"i1\",\n        torch.int16: \"i2\",\n        torch.int32: \"i4\",\n        torch.int64: \"i8\",\n    }[tensor.dtype]\n\n    itemsize = tensor.storage().element_size()\n\n    shape = tensor.shape\n    strides = tuple(s * itemsize for s in tensor.stride())\n    data = (tensor.data_ptr(), False)\n\n    return dict(typestr=typestr, shape=shape, strides=strides, data=data, version=0)\n\n\ntorch.Tensor.__cuda_array_interface__ = property(torch_cuda_array_interface)", "body": "## Issue description\r\n\r\n`numba` provides generic support for jit-optimized operations over tensor data structures, with support for cpu and cuda based operations. This is an _ideal_ match for accelerating pytorch operations. Numba has proposed a standardized [\"`__cuda_array_interface__`\"](http://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html), inspired by the numpy [`__array_interface__`](https://docs.scipy.org/doc/numpy/reference/arrays.interface.html) standard, to provide interop between cuda-based tensors.\r\n\r\nImplementation of this standard would dramatically ease PyTorch/Numba integration. This can be enabled via a monkey-patch of the current `torch.Tensor` type, however PyTorch should provide an implementation.\r\n\r\n## Code example\r\n\r\n```python\r\nimport torch\r\n\r\n\r\ndef torch_cuda_array_interface(tensor):\r\n    \"\"\"__cuda_array_interface__ getter implementation for torch.Tensor.\"\"\"\r\n\r\n    if not tensor.device.type == \"cuda\":\r\n        # raise AttributeError for non-cuda tensors, so that\r\n        # hasattr(cpu_tensor, \"__cuda_array_interface__\") is False.\r\n        raise AttributeError(\"Tensor is not on cuda device: %r\" % tensor.device)\r\n\r\n    if tensor.requires_grad:\r\n        # RuntimeError, matching existing tensor.__array__() behavior.\r\n        raise RuntimeError(\r\n            \"Can't get __cuda_array_interface__ on Variable that requires grad. \"\r\n            \"Use var.detach().__cuda_array_interface__ instead.\"\r\n        )\r\n\r\n    typestr = {\r\n        torch.float16: \"f2\",\r\n        torch.float32: \"f4\",\r\n        torch.float64: \"f8\",\r\n        torch.uint8: \"u1\",\r\n        torch.int8: \"i1\",\r\n        torch.int16: \"i2\",\r\n        torch.int32: \"i4\",\r\n        torch.int64: \"i8\",\r\n    }[tensor.dtype]\r\n\r\n    itemsize = tensor.storage().element_size()\r\n\r\n    shape = tensor.shape\r\n    strides = tuple(s * itemsize for s in tensor.stride())\r\n    data = (tensor.data_ptr(), False)\r\n\r\n    return dict(typestr=typestr, shape=shape, strides=strides, data=data, version=0)\r\n\r\n\r\ntorch.Tensor.__cuda_array_interface__ = property(torch_cuda_array_interface)\r\n```"}