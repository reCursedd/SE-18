{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/423773547", "html_url": "https://github.com/pytorch/pytorch/issues/11914#issuecomment-423773547", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11914", "id": 423773547, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzc3MzU0Nw==", "user": {"login": "asford", "id": 282792, "node_id": "MDQ6VXNlcjI4Mjc5Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/282792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asford", "html_url": "https://github.com/asford", "followers_url": "https://api.github.com/users/asford/followers", "following_url": "https://api.github.com/users/asford/following{/other_user}", "gists_url": "https://api.github.com/users/asford/gists{/gist_id}", "starred_url": "https://api.github.com/users/asford/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asford/subscriptions", "organizations_url": "https://api.github.com/users/asford/orgs", "repos_url": "https://api.github.com/users/asford/repos", "events_url": "https://api.github.com/users/asford/events{/privacy}", "received_events_url": "https://api.github.com/users/asford/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-22T21:07:56Z", "updated_at": "2018-09-22T21:07:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3676247\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dandelin\">@dandelin</a></p>\n<p>I've found that the easiest way to handle multi-device scenarios is to use device context managers to ensure numba operations are dispatched onto the proper device. The cuda api is used to resolve a valid pointer on the current device for the data pointer, which will raise an API error if there's an invalid cross-device reference.</p>\n<p>Eg:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> pytest\n<span class=\"pl-k\">import</span> numba\n<span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_multi_device_interconversion</span>():\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Both torch/numba default to device 0 and can interop freely</span>\n    cudat <span class=\"pl-k\">=</span> torch.arange(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">assert</span> cudat.device.index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">assert</span> numba.as_cuda_array(cudat)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Tensors on non-default device raise api error if converted</span>\n    cudat <span class=\"pl-k\">=</span> torch.arange(<span class=\"pl-c1\">10</span>, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>torch.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">1</span>))\n    <span class=\"pl-k\">with</span> pytest.raises(numba.cuda.driver.CudaAPIError):\n        numba.as_cuda_array(cudat)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> but can be converted when switching to the device's context</span>\n    <span class=\"pl-k\">with</span> numba.cuda.devices.gpus[cudat.device.index]:\n        <span class=\"pl-k\">assert</span> numba.as_cuda_array(cudat)</pre></div>", "body_text": "@dandelin\nI've found that the easiest way to handle multi-device scenarios is to use device context managers to ensure numba operations are dispatched onto the proper device. The cuda api is used to resolve a valid pointer on the current device for the data pointer, which will raise an API error if there's an invalid cross-device reference.\nEg:\nimport pytest\nimport numba\nimport torch\n\ndef test_multi_device_interconversion():\n    # Both torch/numba default to device 0 and can interop freely\n    cudat = torch.arange(10, device=\"cuda\")\n    assert cudat.device.index == 0\n    assert numba.as_cuda_array(cudat)\n\n    # Tensors on non-default device raise api error if converted\n    cudat = torch.arange(10, device=torch.device(\"cuda\", 1))\n    with pytest.raises(numba.cuda.driver.CudaAPIError):\n        numba.as_cuda_array(cudat)\n\n    # but can be converted when switching to the device's context\n    with numba.cuda.devices.gpus[cudat.device.index]:\n        assert numba.as_cuda_array(cudat)", "body": "@dandelin \r\n\r\nI've found that the easiest way to handle multi-device scenarios is to use device context managers to ensure numba operations are dispatched onto the proper device. The cuda api is used to resolve a valid pointer on the current device for the data pointer, which will raise an API error if there's an invalid cross-device reference.\r\n\r\nEg:\r\n\r\n```python\r\nimport pytest\r\nimport numba\r\nimport torch\r\n\r\ndef test_multi_device_interconversion():\r\n    # Both torch/numba default to device 0 and can interop freely\r\n    cudat = torch.arange(10, device=\"cuda\")\r\n    assert cudat.device.index == 0\r\n    assert numba.as_cuda_array(cudat)\r\n\r\n    # Tensors on non-default device raise api error if converted\r\n    cudat = torch.arange(10, device=torch.device(\"cuda\", 1))\r\n    with pytest.raises(numba.cuda.driver.CudaAPIError):\r\n        numba.as_cuda_array(cudat)\r\n\r\n    # but can be converted when switching to the device's context\r\n    with numba.cuda.devices.gpus[cudat.device.index]:\r\n        assert numba.as_cuda_array(cudat)\r\n```"}