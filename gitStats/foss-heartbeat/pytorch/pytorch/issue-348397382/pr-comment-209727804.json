{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209727804", "pull_request_review_id": 145797116, "id": 209727804, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTcyNzgwNA==", "diff_hunk": "@@ -146,6 +148,75 @@ def make_contiguous_slice(size, dtype):\n     def test_dir(self):\n         dir(torch)\n \n+    def test_doc(self):\n+        checked_types = (types.MethodType, types.FunctionType,\n+                         types.BuiltinFunctionType, types.BuiltinMethodType)\n+\n+        def test_namespace(ns, *skips):\n+            if isinstance(ns, object):\n+                ns_name = ns.__class__.__name__\n+            else:\n+                ns_name = ns.__name__\n+            skip_regexes = []\n+            for r in skips:\n+                if isinstance(r, string_classes):\n+                    skip_regexes.append(re.compile('^{}$'.format(re.escape(r))))\n+                else:\n+                    skip_regexes.append(r)\n+            for name in dir(ns):\n+                if name.startswith('_'):\n+                    continue\n+                if any(r.match(name) for r in skip_regexes):\n+                    continue\n+                var = getattr(ns, name)\n+                if isinstance(var, checked_types):\n+                    self.assertTrue(var.__doc__ is not None, ns_name + '.' + name)\n+\n+        # FIXME: fix all the skipped ones below!\n+        test_namespace(torch.randn(1),\n+                       'allclose',\n+                       'as_strided',\n+                       'as_strided_',\n+                       re.compile('^clamp_(min|max)_?$'),\n+                       'coalesce',\n+                       'digamma',\n+                       'digamma_',\n+                       'index_put',\n+                       'is_coalesced',\n+                       'is_distributed',\n+                       'is_floating_point',\n+                       'is_nonzero',\n+                       'is_same_size',\n+                       'is_signed',\n+                       'isclose',\n+                       'lgamma',\n+                       'lgamma_',\n+                       'log_softmax',\n+                       'map2_',\n+                       'new',\n+                       'pin_memory',\n+                       'polygamma',\n+                       'polygamma_',\n+                       'record_stream',\n+                       'reinforce',\n+                       'relu',\n+                       'relu_',\n+                       'resize',\n+                       'resize_as',\n+                       'smm',\n+                       'softmax',\n+                       'split_with_sizes',\n+                       'sspaddmm',\n+                       'storage_type',\n+                       'tan',\n+                       'to_dense',\n+                       )\n+        test_namespace(torch.nn)\n+        test_namespace(torch.nn.functional, 'assert_int_or_pair', 'bilinear',\n+                       'dropout', 'dropout2d', 'dropout3d', 'feature_alpha_dropout')\n+        # TODO: add torch.* tests when we have proper namespacing on ATen functions\n+        # test_namespace(torch)", "path": "test/test_torch.py", "position": 85, "original_position": 85, "commit_id": "1b7b31619a4ff996c3bfe625cb95eb330c34f377", "original_commit_id": "840830c800a1eda9423d0faf290800e38dfc7fde", "user": {"login": "li-roy", "id": 8813817, "node_id": "MDQ6VXNlcjg4MTM4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8813817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/li-roy", "html_url": "https://github.com/li-roy", "followers_url": "https://api.github.com/users/li-roy/followers", "following_url": "https://api.github.com/users/li-roy/following{/other_user}", "gists_url": "https://api.github.com/users/li-roy/gists{/gist_id}", "starred_url": "https://api.github.com/users/li-roy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/li-roy/subscriptions", "organizations_url": "https://api.github.com/users/li-roy/orgs", "repos_url": "https://api.github.com/users/li-roy/repos", "events_url": "https://api.github.com/users/li-roy/events{/privacy}", "received_events_url": "https://api.github.com/users/li-roy/received_events", "type": "User", "site_admin": false}, "body": "just a question for my own clarification: do you mean that in the future we're gonna not have all ATen functions go to torch.*?", "created_at": "2018-08-13T19:20:37Z", "updated_at": "2018-11-23T15:49:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/10311#discussion_r209727804", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10311", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209727804"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10311#discussion_r209727804"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10311"}}, "body_html": "<p>just a question for my own clarification: do you mean that in the future we're gonna not have all ATen functions go to torch.*?</p>", "body_text": "just a question for my own clarification: do you mean that in the future we're gonna not have all ATen functions go to torch.*?"}