{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5703", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5703/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5703/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5703/events", "html_url": "https://github.com/pytorch/pytorch/issues/5703", "id": 304271364, "node_id": "MDU6SXNzdWUzMDQyNzEzNjQ=", "number": 5703, "title": "Training a cascaded network using multi-GPUs", "user": {"login": "JohnnieXDU", "id": 29588335, "node_id": "MDQ6VXNlcjI5NTg4MzM1", "avatar_url": "https://avatars0.githubusercontent.com/u/29588335?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JohnnieXDU", "html_url": "https://github.com/JohnnieXDU", "followers_url": "https://api.github.com/users/JohnnieXDU/followers", "following_url": "https://api.github.com/users/JohnnieXDU/following{/other_user}", "gists_url": "https://api.github.com/users/JohnnieXDU/gists{/gist_id}", "starred_url": "https://api.github.com/users/JohnnieXDU/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JohnnieXDU/subscriptions", "organizations_url": "https://api.github.com/users/JohnnieXDU/orgs", "repos_url": "https://api.github.com/users/JohnnieXDU/repos", "events_url": "https://api.github.com/users/JohnnieXDU/events{/privacy}", "received_events_url": "https://api.github.com/users/JohnnieXDU/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-12T07:58:42Z", "updated_at": "2018-03-12T09:47:58Z", "closed_at": "2018-03-12T09:25:33Z", "author_association": "NONE", "body_html": "<p>Im training a cascaded network, for simple example, Inputs -&gt; Net1 -&gt; mid results -&gt; Net2 -&gt; outputs.</p>\n<p>For the reason that the cascaded network Net1+Net2 consuming GPU memory a lot, I can not train with a desired batch size. So i train the entire net(Net1+Net2) on multi GPUs.</p>\n<p>But when i run the code, it fails with the following:</p>\n<blockquote>\n<p>RuntimeError: arguments are located on different GPUs at /opt/conda/conda-bld/pytorch_1518238409320/work/torch/lib/THC/generic/THCTensorMathBlas.cu:236</p>\n</blockquote>\n<p>Besides, the code could  work when I replace the cascade network(Net1+Net2) to a single classification network(Vgg16).</p>\n<p>The cascade net prototype is as follows:</p>\n<pre><code>class cascade_net(nn.Module):\n    def __init__(self):\n        super(cascade_net, self).__init__()\n        self.net1 = vgg19_net()\n        self.net2 = AttnLSTM_net()\n\n    def forward(self, x):\n        img_enc = self.net1(x)\n        cur_outputs, (cur_h, cur_c) = self.net2([img_enc])\n        return cur_outputs, (cur_h, cur_c)\n</code></pre>", "body_text": "Im training a cascaded network, for simple example, Inputs -> Net1 -> mid results -> Net2 -> outputs.\nFor the reason that the cascaded network Net1+Net2 consuming GPU memory a lot, I can not train with a desired batch size. So i train the entire net(Net1+Net2) on multi GPUs.\nBut when i run the code, it fails with the following:\n\nRuntimeError: arguments are located on different GPUs at /opt/conda/conda-bld/pytorch_1518238409320/work/torch/lib/THC/generic/THCTensorMathBlas.cu:236\n\nBesides, the code could  work when I replace the cascade network(Net1+Net2) to a single classification network(Vgg16).\nThe cascade net prototype is as follows:\nclass cascade_net(nn.Module):\n    def __init__(self):\n        super(cascade_net, self).__init__()\n        self.net1 = vgg19_net()\n        self.net2 = AttnLSTM_net()\n\n    def forward(self, x):\n        img_enc = self.net1(x)\n        cur_outputs, (cur_h, cur_c) = self.net2([img_enc])\n        return cur_outputs, (cur_h, cur_c)", "body": "Im training a cascaded network, for simple example, Inputs -> Net1 -> mid results -> Net2 -> outputs.\r\n\r\nFor the reason that the cascaded network Net1+Net2 consuming GPU memory a lot, I can not train with a desired batch size. So i train the entire net(Net1+Net2) on multi GPUs.\r\n\r\nBut when i run the code, it fails with the following:\r\n\r\n> RuntimeError: arguments are located on different GPUs at /opt/conda/conda-bld/pytorch_1518238409320/work/torch/lib/THC/generic/THCTensorMathBlas.cu:236\r\n\r\nBesides, the code could  work when I replace the cascade network(Net1+Net2) to a single classification network(Vgg16).\r\n\r\nThe cascade net prototype is as follows:\r\n```\r\nclass cascade_net(nn.Module):\r\n    def __init__(self):\r\n        super(cascade_net, self).__init__()\r\n        self.net1 = vgg19_net()\r\n        self.net2 = AttnLSTM_net()\r\n\r\n    def forward(self, x):\r\n        img_enc = self.net1(x)\r\n        cur_outputs, (cur_h, cur_c) = self.net2([img_enc])\r\n        return cur_outputs, (cur_h, cur_c)\r\n```\r\n"}