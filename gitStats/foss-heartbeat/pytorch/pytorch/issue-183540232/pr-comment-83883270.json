{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83883270", "pull_request_review_id": 4696809, "id": 83883270, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDgzODgzMjcw", "diff_hunk": "@@ -68,63 +62,81 @@ def __init__(self, loader):\n         self.samples_remaining = len(self.sampler)\n         self.sample_iter = iter(self.sampler)\n \n-        if self.num_workers:\n+        if self.num_workers > 0:\n             self.index_queue = multiprocessing.Queue()\n             self.data_queue = multiprocessing.Queue()\n             self.batches_outstanding = 0\n-            self.joined = False\n+            self.shutdown = False\n+            self.send_idx = 0\n+            self.rcvd_idx = 0\n+            self.reorder_dict = {}\n \n             self.workers = [\n                 multiprocessing.Process(\n-                    target=_workerLoop,\n+                    target=_worker_loop,\n                     args=(self.dataset, self.index_queue, self.data_queue, self.collate_fn))\n-                for i in range(self.num_workers)]\n+                for _ in range(self.num_workers)]\n \n             for w in self.workers:\n-                w.daemon = True # ensure that the worker exits on process exit\n+                w.daemon = True  # ensure that the worker exits on process exit\n                 w.start()\n-                # prime the prefetch loop with exactly 1 batch per process\n-                # this ensures no deadlocks on the queues using the blocking queue API\n-                self._putBatch()\n \n-    def _nextBatch(self):\n-        batch = [next(self.sample_iter) for x in range(min(self.samples_remaining, self.batch_size))]\n+            # prime the prefetch loop\n+            self._enqueue_indices()\n+\n+    def __len__(self):\n+        return len(self.sampler)\n+\n+    def __next__(self):\n+        if self.num_workers == 0:\n+            # same-process loading\n+            if self.samples_remaining == 0:\n+                raise StopIteration\n+            indices = self._next_indices()\n+            return self.collate_fn([self.dataset[i] for i in indices])\n+\n+        # check if the next sample has already been generated\n+        if self.rcvd_idx in self.reorder_dict:\n+            batch = self.reorder_dict.pop(self.rcvd_idx)\n+            return self._receive_batch(batch)\n+\n+        if self.batches_outstanding == 0:\n+            self._shutdown_workers()\n+            raise StopIteration\n+\n+        while True:\n+            assert (not self.shutdown and self.batches_outstanding > 0)\n+            idx, batch = self.data_queue.get()\n+            self.batches_outstanding -= 1\n+            if idx != self.rcvd_idx:\n+                # store out-of-order samples\n+                self.reorder_dict[idx] = batch\n+                self._enqueue_indices()\n+                continue\n+            return self._receive_batch(batch)\n+\n+    next = __next__  # Python 2 compatibility\n+\n+    def _next_indices(self):\n+        batch_size = min(self.samples_remaining, self.batch_size)\n+        batch = [next(self.sample_iter) for _ in range(batch_size)]\n         self.samples_remaining -= len(batch)\n         return batch\n \n-    def _putBatch(self):\n-        if self.samples_remaining > 0:\n-            self.index_queue.put(self._nextBatch())\n+    def _enqueue_indices(self):\n+        while (self.samples_remaining > 0 and", "path": "torch/utils/data/dataloader.py", "position": null, "original_position": 138, "commit_id": "0ece591a9d5b1fc295bf33814eacb5f411a37c32", "original_commit_id": "cdf0e0e777a9a2255d8c3d60bcf6e482ab6720a1", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I know that you can't enqueue them immediately, that's obvious. However your solution doesn't actually change very much. It seems to me that it only increases the upper bound on the number of in-memory batches to `2 * self.num_workers`, and you can achieve the same result by priming the indices queue with a larger number of batches.\n", "created_at": "2016-10-18T15:36:24Z", "updated_at": "2018-11-23T15:31:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/135#discussion_r83883270", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/135", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83883270"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/135#discussion_r83883270"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/135"}}, "body_html": "<p>I know that you can't enqueue them immediately, that's obvious. However your solution doesn't actually change very much. It seems to me that it only increases the upper bound on the number of in-memory batches to <code>2 * self.num_workers</code>, and you can achieve the same result by priming the indices queue with a larger number of batches.</p>", "body_text": "I know that you can't enqueue them immediately, that's obvious. However your solution doesn't actually change very much. It seems to me that it only increases the upper bound on the number of in-memory batches to 2 * self.num_workers, and you can achieve the same result by priming the indices queue with a larger number of batches.", "in_reply_to_id": 83830788}