{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83881362", "pull_request_review_id": 4694966, "id": 83881362, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDgzODgxMzYy", "diff_hunk": "@@ -68,63 +62,81 @@ def __init__(self, loader):\n         self.samples_remaining = len(self.sampler)\n         self.sample_iter = iter(self.sampler)\n \n-        if self.num_workers:\n+        if self.num_workers > 0:\n             self.index_queue = multiprocessing.Queue()\n             self.data_queue = multiprocessing.Queue()\n             self.batches_outstanding = 0\n-            self.joined = False\n+            self.shutdown = False\n+            self.send_idx = 0\n+            self.rcvd_idx = 0\n+            self.reorder_dict = {}\n \n             self.workers = [\n                 multiprocessing.Process(\n-                    target=_workerLoop,\n+                    target=_worker_loop,\n                     args=(self.dataset, self.index_queue, self.data_queue, self.collate_fn))\n-                for i in range(self.num_workers)]\n+                for _ in range(self.num_workers)]\n \n             for w in self.workers:\n-                w.daemon = True # ensure that the worker exits on process exit\n+                w.daemon = True  # ensure that the worker exits on process exit\n                 w.start()\n-                # prime the prefetch loop with exactly 1 batch per process\n-                # this ensures no deadlocks on the queues using the blocking queue API\n-                self._putBatch()\n \n-    def _nextBatch(self):\n-        batch = [next(self.sample_iter) for x in range(min(self.samples_remaining, self.batch_size))]\n+            # prime the prefetch loop\n+            self._enqueue_indices()\n+\n+    def __len__(self):\n+        return len(self.sampler)\n+\n+    def __next__(self):\n+        if self.num_workers == 0:\n+            # same-process loading\n+            if self.samples_remaining == 0:\n+                raise StopIteration\n+            indices = self._next_indices()\n+            return self.collate_fn([self.dataset[i] for i in indices])\n+\n+        # check if the next sample has already been generated\n+        if self.rcvd_idx in self.reorder_dict:\n+            batch = self.reorder_dict.pop(self.rcvd_idx)\n+            return self._receive_batch(batch)", "path": "torch/utils/data/dataloader.py", "position": null, "original_position": 109, "commit_id": "0ece591a9d5b1fc295bf33814eacb5f411a37c32", "original_commit_id": "cdf0e0e777a9a2255d8c3d60bcf6e482ab6720a1", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I made it `_process_next_batch`\n", "created_at": "2016-10-18T15:28:37Z", "updated_at": "2018-11-23T15:31:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/135#discussion_r83881362", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/135", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83881362"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/135#discussion_r83881362"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/135"}}, "body_html": "<p>I made it <code>_process_next_batch</code></p>", "body_text": "I made it _process_next_batch", "in_reply_to_id": 83830462}