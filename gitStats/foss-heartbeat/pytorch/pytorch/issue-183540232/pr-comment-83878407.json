{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83878407", "pull_request_review_id": 4692184, "id": 83878407, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDgzODc4NDA3", "diff_hunk": "@@ -68,63 +62,81 @@ def __init__(self, loader):\n         self.samples_remaining = len(self.sampler)\n         self.sample_iter = iter(self.sampler)\n \n-        if self.num_workers:\n+        if self.num_workers > 0:\n             self.index_queue = multiprocessing.Queue()\n             self.data_queue = multiprocessing.Queue()\n             self.batches_outstanding = 0\n-            self.joined = False\n+            self.shutdown = False\n+            self.send_idx = 0\n+            self.rcvd_idx = 0\n+            self.reorder_dict = {}\n \n             self.workers = [\n                 multiprocessing.Process(\n-                    target=_workerLoop,\n+                    target=_worker_loop,\n                     args=(self.dataset, self.index_queue, self.data_queue, self.collate_fn))\n-                for i in range(self.num_workers)]\n+                for _ in range(self.num_workers)]\n \n             for w in self.workers:\n-                w.daemon = True # ensure that the worker exits on process exit\n+                w.daemon = True  # ensure that the worker exits on process exit\n                 w.start()\n-                # prime the prefetch loop with exactly 1 batch per process\n-                # this ensures no deadlocks on the queues using the blocking queue API\n-                self._putBatch()\n \n-    def _nextBatch(self):\n-        batch = [next(self.sample_iter) for x in range(min(self.samples_remaining, self.batch_size))]\n+            # prime the prefetch loop\n+            self._enqueue_indices()\n+\n+    def __len__(self):\n+        return len(self.sampler)\n+\n+    def __next__(self):\n+        if self.num_workers == 0:\n+            # same-process loading\n+            if self.samples_remaining == 0:\n+                raise StopIteration\n+            indices = self._next_indices()\n+            return self.collate_fn([self.dataset[i] for i in indices])\n+\n+        # check if the next sample has already been generated\n+        if self.rcvd_idx in self.reorder_dict:\n+            batch = self.reorder_dict.pop(self.rcvd_idx)\n+            return self._receive_batch(batch)\n+\n+        if self.batches_outstanding == 0:\n+            self._shutdown_workers()\n+            raise StopIteration\n+\n+        while True:\n+            assert (not self.shutdown and self.batches_outstanding > 0)\n+            idx, batch = self.data_queue.get()\n+            self.batches_outstanding -= 1\n+            if idx != self.rcvd_idx:\n+                # store out-of-order samples\n+                self.reorder_dict[idx] = batch\n+                self._enqueue_indices()\n+                continue\n+            return self._receive_batch(batch)\n+\n+    next = __next__  # Python 2 compatibility\n+\n+    def _next_indices(self):\n+        batch_size = min(self.samples_remaining, self.batch_size)\n+        batch = [next(self.sample_iter) for _ in range(batch_size)]\n         self.samples_remaining -= len(batch)\n         return batch\n \n-    def _putBatch(self):\n-        if self.samples_remaining > 0:\n-            self.index_queue.put(self._nextBatch())\n+    def _enqueue_indices(self):\n+        while (self.samples_remaining > 0 and", "path": "torch/utils/data/dataloader.py", "position": null, "original_position": 138, "commit_id": "0ece591a9d5b1fc295bf33814eacb5f411a37c32", "original_commit_id": "cdf0e0e777a9a2255d8c3d60bcf6e482ab6720a1", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "The idea is to balance two goals:\n- Keep the sub-processes busy so that samples are ready ASAP\n- Limit the amount of memory used\n\nIf you only enqueue indices when the samples are actually returned to the user, some subprocess will be idle while you wait for the next in-order sample. (This hurts throughput)\n\nIf you immediately enqueue indices when you retrieve a sample from the data queue then the re-order dictionary can grow to an unbounded size.\n", "created_at": "2016-10-18T15:16:37Z", "updated_at": "2018-11-23T15:31:45Z", "html_url": "https://github.com/pytorch/pytorch/pull/135#discussion_r83878407", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/135", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/83878407"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/135#discussion_r83878407"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/135"}}, "body_html": "<p>The idea is to balance two goals:</p>\n<ul>\n<li>Keep the sub-processes busy so that samples are ready ASAP</li>\n<li>Limit the amount of memory used</li>\n</ul>\n<p>If you only enqueue indices when the samples are actually returned to the user, some subprocess will be idle while you wait for the next in-order sample. (This hurts throughput)</p>\n<p>If you immediately enqueue indices when you retrieve a sample from the data queue then the re-order dictionary can grow to an unbounded size.</p>", "body_text": "The idea is to balance two goals:\n\nKeep the sub-processes busy so that samples are ready ASAP\nLimit the amount of memory used\n\nIf you only enqueue indices when the samples are actually returned to the user, some subprocess will be idle while you wait for the next in-order sample. (This hurts throughput)\nIf you immediately enqueue indices when you retrieve a sample from the data queue then the re-order dictionary can grow to an unbounded size.", "in_reply_to_id": 83830788}