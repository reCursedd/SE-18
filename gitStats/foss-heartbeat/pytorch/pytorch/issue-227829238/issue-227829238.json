{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1532", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1532/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1532/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1532/events", "html_url": "https://github.com/pytorch/pytorch/issues/1532", "id": 227829238, "node_id": "MDU6SXNzdWUyMjc4MjkyMzg=", "number": 1532, "title": "Rewrite fused RNN kernels to avoid clones in backward()", "user": {"login": "aosokin", "id": 2099291, "node_id": "MDQ6VXNlcjIwOTkyOTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/2099291?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aosokin", "html_url": "https://github.com/aosokin", "followers_url": "https://api.github.com/users/aosokin/followers", "following_url": "https://api.github.com/users/aosokin/following{/other_user}", "gists_url": "https://api.github.com/users/aosokin/gists{/gist_id}", "starred_url": "https://api.github.com/users/aosokin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aosokin/subscriptions", "organizations_url": "https://api.github.com/users/aosokin/orgs", "repos_url": "https://api.github.com/users/aosokin/repos", "events_url": "https://api.github.com/users/aosokin/events{/privacy}", "received_events_url": "https://api.github.com/users/aosokin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-10T22:32:47Z", "updated_at": "2017-06-07T13:55:22Z", "closed_at": "2017-06-07T13:55:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, in my model, I am applying a linear layer on top of the GRUCell.<br>\nWhen sequentially calling backward on two different outputs, the gradient w.r.t. GRUCell parameters is substantially different on CPU vs GPU. The gradient w.r.t. parameters of the linear layer is identical.<br>\nThe GRUCell layer alone seems to have correct gradient on both CPU and GPU.</p>\n<p>The code below produces</p>\n<pre><code>weight_ih 0.40665897727012634\nweight_hh 0.06703907996416092\nbias_ih 0.0623239129781723\nbias_hh 0.13187159597873688\nweight 0.3885607421398163\nbias 1.3700947761535645\n</code></pre>\n<p>on GPU and</p>\n<pre><code>weight_ih 0.18657396119047137\nweight_hh 0.05437005038803549\nbias_ih 0.053355072396595864\nbias_hh 0.03761203796902013\nweight 0.3885607195130627\nbias 1.3700947601746332\n</code></pre>\n<p>on CPU. Tested in master branch, he latest version (commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/5bb13485b8484a37f9afad67582512cf53ed13cb/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/5bb13485b8484a37f9afad67582512cf53ed13cb\"><tt>5bb1348</tt></a>)</p>\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\n\n# GPU vs CPU\nuse_cuda = False\nth = torch.cuda if use_cuda else torch\n\n# Create layers\nrnn = nn.GRUCell(2, 2)\nlin = nn.Linear(2, 27)\nif use_cuda:\n    rnn.cuda()\n    lin.cuda()\n\n# damped state to reproduce the bug\nrnn_state = OrderedDict({'weight_ih': th.FloatTensor([\n    [-0.6940, 0.7056], [0.4358, 0.6890], [-0.4579, 0.5398],\n    [0.1502, -0.1490], [-0.2306, 0.2639], [0.1209, -0.1180]]),\n    'weight_hh': th.FloatTensor([\n        [-0.2607, 0.2100], [-0.5842, -0.2695], [-0.3918, 0.1502],\n        [-0.4412, 0.0336], [-0.1018, -0.6725], [0.2238, 0.4111]]),\n    'bias_ih': th.FloatTensor([\n        -0.3340, -0.6903, -0.6900, -0.2925, 0.0268, -0.6987]),\n    'bias_hh': th.FloatTensor([\n        0.1496, -0.1751, 0.6546, -0.6178, 0.1265, 0.5848])})\n\nlin_state = OrderedDict({'weight': th.FloatTensor([\n    [-0.0408, -0.1192], [0.1038, -0.0573], [0.6051, -0.2123],\n    [0.4644, 0.1803], [-0.2503, 0.4022], [-0.3716, 0.2095],\n    [-0.3357, 0.3510], [0.0063, 0.5741], [0.3451, 0.5141],\n    [0.0046, 0.2462], [-0.6112, -0.4036], [0.3162, 0.1947],\n    [0.4327, 0.0086], [-0.2977, 0.3072], [0.2321, 0.4708],\n    [-0.1038, 0.3433], [0.5503, 0.4901], [0.4882, -0.6168],\n    [0.0360, -0.6624], [0.2708, -0.3415], [0.5421, 0.5666],\n    [-0.0061, 0.4577], [-0.2641, 0.2584], [0.3587, 0.2839],\n    [-0.3959, 0.3206], [-0.1576, 0.6259], [0.5172, 0.4698]]),\n    'bias': th.FloatTensor([\n        0.3174, 0.1745, -0.1360, 0.2344, -0.4034, -0.4048, -0.2037,\n        0.2225, -0.3053, -0.1065, -0.4581, -0.4908, 0.3441, 0.4333,\n        -0.3506, 0.0145, -0.0648, 0.6752, -0.1614, -0.3088, 0.6818,\n        -0.0273, -0.1540, -0.0735, -0.5940, -0.5363, -0.5801])})\n\nrnn.load_state_dict(rnn_state)\nlin.load_state_dict(lin_state)\n\nrnn.zero_grad()\nlin.zero_grad()\n\n# inputs\nhidden0 = Variable(th.FloatTensor(1, 2))\nhidden0[0, 0] = -0.1997\nhidden0[0, 1] = 0.4675\n\ninput1 = th.FloatTensor(1, 2).fill_(0.0)\ninput1 = Variable(input1)\n\ninput2 = Variable(th.FloatTensor(1, 2))\ninput2[0, 0] = 0.8316\ninput2[0, 1] = 1.0540\n\ntarget = Variable(th.LongTensor([24, 12]).unsqueeze(1))\n\n# process the layers\nhidden1 = rnn(input1, hidden0)\nhidden2 = rnn(input2, hidden1)\noutput1 = lin(hidden1)\noutput2 = lin(hidden2)\n\noutput = torch.stack([output1, output2], 0)\noutput = output.squeeze(1)\n\n# cross entropy loss, but separated w.r.t. outputs\nlogits = F.log_softmax(output)\nobjs = -torch.gather(logits.contiguous(), dim=1, index=target)\n\n# backward through each element\nfor i in range(objs.size(0)):\n    objs[i, 0].backward([th.FloatTensor([1.0])], retain_graph=True)\n\n# print gradient norms\nfor name, param in rnn.named_parameters():\n    print(name, torch.norm(param.grad.data))\n</code></pre>", "body_text": "Hi, in my model, I am applying a linear layer on top of the GRUCell.\nWhen sequentially calling backward on two different outputs, the gradient w.r.t. GRUCell parameters is substantially different on CPU vs GPU. The gradient w.r.t. parameters of the linear layer is identical.\nThe GRUCell layer alone seems to have correct gradient on both CPU and GPU.\nThe code below produces\nweight_ih 0.40665897727012634\nweight_hh 0.06703907996416092\nbias_ih 0.0623239129781723\nbias_hh 0.13187159597873688\nweight 0.3885607421398163\nbias 1.3700947761535645\n\non GPU and\nweight_ih 0.18657396119047137\nweight_hh 0.05437005038803549\nbias_ih 0.053355072396595864\nbias_hh 0.03761203796902013\nweight 0.3885607195130627\nbias 1.3700947601746332\n\non CPU. Tested in master branch, he latest version (commit 5bb1348)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\n\n# GPU vs CPU\nuse_cuda = False\nth = torch.cuda if use_cuda else torch\n\n# Create layers\nrnn = nn.GRUCell(2, 2)\nlin = nn.Linear(2, 27)\nif use_cuda:\n    rnn.cuda()\n    lin.cuda()\n\n# damped state to reproduce the bug\nrnn_state = OrderedDict({'weight_ih': th.FloatTensor([\n    [-0.6940, 0.7056], [0.4358, 0.6890], [-0.4579, 0.5398],\n    [0.1502, -0.1490], [-0.2306, 0.2639], [0.1209, -0.1180]]),\n    'weight_hh': th.FloatTensor([\n        [-0.2607, 0.2100], [-0.5842, -0.2695], [-0.3918, 0.1502],\n        [-0.4412, 0.0336], [-0.1018, -0.6725], [0.2238, 0.4111]]),\n    'bias_ih': th.FloatTensor([\n        -0.3340, -0.6903, -0.6900, -0.2925, 0.0268, -0.6987]),\n    'bias_hh': th.FloatTensor([\n        0.1496, -0.1751, 0.6546, -0.6178, 0.1265, 0.5848])})\n\nlin_state = OrderedDict({'weight': th.FloatTensor([\n    [-0.0408, -0.1192], [0.1038, -0.0573], [0.6051, -0.2123],\n    [0.4644, 0.1803], [-0.2503, 0.4022], [-0.3716, 0.2095],\n    [-0.3357, 0.3510], [0.0063, 0.5741], [0.3451, 0.5141],\n    [0.0046, 0.2462], [-0.6112, -0.4036], [0.3162, 0.1947],\n    [0.4327, 0.0086], [-0.2977, 0.3072], [0.2321, 0.4708],\n    [-0.1038, 0.3433], [0.5503, 0.4901], [0.4882, -0.6168],\n    [0.0360, -0.6624], [0.2708, -0.3415], [0.5421, 0.5666],\n    [-0.0061, 0.4577], [-0.2641, 0.2584], [0.3587, 0.2839],\n    [-0.3959, 0.3206], [-0.1576, 0.6259], [0.5172, 0.4698]]),\n    'bias': th.FloatTensor([\n        0.3174, 0.1745, -0.1360, 0.2344, -0.4034, -0.4048, -0.2037,\n        0.2225, -0.3053, -0.1065, -0.4581, -0.4908, 0.3441, 0.4333,\n        -0.3506, 0.0145, -0.0648, 0.6752, -0.1614, -0.3088, 0.6818,\n        -0.0273, -0.1540, -0.0735, -0.5940, -0.5363, -0.5801])})\n\nrnn.load_state_dict(rnn_state)\nlin.load_state_dict(lin_state)\n\nrnn.zero_grad()\nlin.zero_grad()\n\n# inputs\nhidden0 = Variable(th.FloatTensor(1, 2))\nhidden0[0, 0] = -0.1997\nhidden0[0, 1] = 0.4675\n\ninput1 = th.FloatTensor(1, 2).fill_(0.0)\ninput1 = Variable(input1)\n\ninput2 = Variable(th.FloatTensor(1, 2))\ninput2[0, 0] = 0.8316\ninput2[0, 1] = 1.0540\n\ntarget = Variable(th.LongTensor([24, 12]).unsqueeze(1))\n\n# process the layers\nhidden1 = rnn(input1, hidden0)\nhidden2 = rnn(input2, hidden1)\noutput1 = lin(hidden1)\noutput2 = lin(hidden2)\n\noutput = torch.stack([output1, output2], 0)\noutput = output.squeeze(1)\n\n# cross entropy loss, but separated w.r.t. outputs\nlogits = F.log_softmax(output)\nobjs = -torch.gather(logits.contiguous(), dim=1, index=target)\n\n# backward through each element\nfor i in range(objs.size(0)):\n    objs[i, 0].backward([th.FloatTensor([1.0])], retain_graph=True)\n\n# print gradient norms\nfor name, param in rnn.named_parameters():\n    print(name, torch.norm(param.grad.data))", "body": "Hi, in my model, I am applying a linear layer on top of the GRUCell.\r\nWhen sequentially calling backward on two different outputs, the gradient w.r.t. GRUCell parameters is substantially different on CPU vs GPU. The gradient w.r.t. parameters of the linear layer is identical.\r\nThe GRUCell layer alone seems to have correct gradient on both CPU and GPU.\r\n\r\nThe code below produces \r\n```\r\nweight_ih 0.40665897727012634\r\nweight_hh 0.06703907996416092\r\nbias_ih 0.0623239129781723\r\nbias_hh 0.13187159597873688\r\nweight 0.3885607421398163\r\nbias 1.3700947761535645\r\n```\r\non GPU and \r\n```\r\nweight_ih 0.18657396119047137\r\nweight_hh 0.05437005038803549\r\nbias_ih 0.053355072396595864\r\nbias_hh 0.03761203796902013\r\nweight 0.3885607195130627\r\nbias 1.3700947601746332\r\n```\r\non CPU. Tested in master branch, he latest version (commit 5bb1348)\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom torch.autograd import Variable\r\nfrom collections import OrderedDict\r\n\r\n# GPU vs CPU\r\nuse_cuda = False\r\nth = torch.cuda if use_cuda else torch\r\n\r\n# Create layers\r\nrnn = nn.GRUCell(2, 2)\r\nlin = nn.Linear(2, 27)\r\nif use_cuda:\r\n    rnn.cuda()\r\n    lin.cuda()\r\n\r\n# damped state to reproduce the bug\r\nrnn_state = OrderedDict({'weight_ih': th.FloatTensor([\r\n    [-0.6940, 0.7056], [0.4358, 0.6890], [-0.4579, 0.5398],\r\n    [0.1502, -0.1490], [-0.2306, 0.2639], [0.1209, -0.1180]]),\r\n    'weight_hh': th.FloatTensor([\r\n        [-0.2607, 0.2100], [-0.5842, -0.2695], [-0.3918, 0.1502],\r\n        [-0.4412, 0.0336], [-0.1018, -0.6725], [0.2238, 0.4111]]),\r\n    'bias_ih': th.FloatTensor([\r\n        -0.3340, -0.6903, -0.6900, -0.2925, 0.0268, -0.6987]),\r\n    'bias_hh': th.FloatTensor([\r\n        0.1496, -0.1751, 0.6546, -0.6178, 0.1265, 0.5848])})\r\n\r\nlin_state = OrderedDict({'weight': th.FloatTensor([\r\n    [-0.0408, -0.1192], [0.1038, -0.0573], [0.6051, -0.2123],\r\n    [0.4644, 0.1803], [-0.2503, 0.4022], [-0.3716, 0.2095],\r\n    [-0.3357, 0.3510], [0.0063, 0.5741], [0.3451, 0.5141],\r\n    [0.0046, 0.2462], [-0.6112, -0.4036], [0.3162, 0.1947],\r\n    [0.4327, 0.0086], [-0.2977, 0.3072], [0.2321, 0.4708],\r\n    [-0.1038, 0.3433], [0.5503, 0.4901], [0.4882, -0.6168],\r\n    [0.0360, -0.6624], [0.2708, -0.3415], [0.5421, 0.5666],\r\n    [-0.0061, 0.4577], [-0.2641, 0.2584], [0.3587, 0.2839],\r\n    [-0.3959, 0.3206], [-0.1576, 0.6259], [0.5172, 0.4698]]),\r\n    'bias': th.FloatTensor([\r\n        0.3174, 0.1745, -0.1360, 0.2344, -0.4034, -0.4048, -0.2037,\r\n        0.2225, -0.3053, -0.1065, -0.4581, -0.4908, 0.3441, 0.4333,\r\n        -0.3506, 0.0145, -0.0648, 0.6752, -0.1614, -0.3088, 0.6818,\r\n        -0.0273, -0.1540, -0.0735, -0.5940, -0.5363, -0.5801])})\r\n\r\nrnn.load_state_dict(rnn_state)\r\nlin.load_state_dict(lin_state)\r\n\r\nrnn.zero_grad()\r\nlin.zero_grad()\r\n\r\n# inputs\r\nhidden0 = Variable(th.FloatTensor(1, 2))\r\nhidden0[0, 0] = -0.1997\r\nhidden0[0, 1] = 0.4675\r\n\r\ninput1 = th.FloatTensor(1, 2).fill_(0.0)\r\ninput1 = Variable(input1)\r\n\r\ninput2 = Variable(th.FloatTensor(1, 2))\r\ninput2[0, 0] = 0.8316\r\ninput2[0, 1] = 1.0540\r\n\r\ntarget = Variable(th.LongTensor([24, 12]).unsqueeze(1))\r\n\r\n# process the layers\r\nhidden1 = rnn(input1, hidden0)\r\nhidden2 = rnn(input2, hidden1)\r\noutput1 = lin(hidden1)\r\noutput2 = lin(hidden2)\r\n\r\noutput = torch.stack([output1, output2], 0)\r\noutput = output.squeeze(1)\r\n\r\n# cross entropy loss, but separated w.r.t. outputs\r\nlogits = F.log_softmax(output)\r\nobjs = -torch.gather(logits.contiguous(), dim=1, index=target)\r\n\r\n# backward through each element\r\nfor i in range(objs.size(0)):\r\n    objs[i, 0].backward([th.FloatTensor([1.0])], retain_graph=True)\r\n\r\n# print gradient norms\r\nfor name, param in rnn.named_parameters():\r\n    print(name, torch.norm(param.grad.data))\r\n```"}