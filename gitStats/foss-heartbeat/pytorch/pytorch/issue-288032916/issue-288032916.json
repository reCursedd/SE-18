{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4628", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4628/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4628/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4628/events", "html_url": "https://github.com/pytorch/pytorch/issues/4628", "id": 288032916, "node_id": "MDU6SXNzdWUyODgwMzI5MTY=", "number": 4628, "title": "Softmax using multiple threads inhibiting parallel execution of forward passes (?)", "user": {"login": "JakobHavtorn", "id": 10236734, "node_id": "MDQ6VXNlcjEwMjM2NzM0", "avatar_url": "https://avatars0.githubusercontent.com/u/10236734?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JakobHavtorn", "html_url": "https://github.com/JakobHavtorn", "followers_url": "https://api.github.com/users/JakobHavtorn/followers", "following_url": "https://api.github.com/users/JakobHavtorn/following{/other_user}", "gists_url": "https://api.github.com/users/JakobHavtorn/gists{/gist_id}", "starred_url": "https://api.github.com/users/JakobHavtorn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JakobHavtorn/subscriptions", "organizations_url": "https://api.github.com/users/JakobHavtorn/orgs", "repos_url": "https://api.github.com/users/JakobHavtorn/repos", "events_url": "https://api.github.com/users/JakobHavtorn/events{/privacy}", "received_events_url": "https://api.github.com/users/JakobHavtorn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-01-12T07:46:10Z", "updated_at": "2018-04-03T21:29:15Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Using Linux Mint 18.3 and pytorch 0.3.0, some <code>nn.Module</code> and <code>nn.functional</code> methods appear to use multiple cores/threads and inhibit parallel execution.</p>\n<p>At the basic level, I am interested in running multiple forward passes in parallel. This works perfectly on Mac OS 10.13.2 but on Linux Mint 18.3, I face the problem described above.</p>\n<p>For a network like the one below, using <code>softmax</code> while executing multiple forward passes in parallel with <code>torch.multiprocessing.Process</code> results in a complete backlog. If changed to <code>relu</code>, the code executes fine.</p>\n<p>If testing the softmax/relu situation on a single thread, it appears that the softmax takes up multiple threads on Linux while on Mac OS it does not. Code used to test this is</p>\n<pre><code>import time\nimport gym\nimport numpy as np\nimport torch\nimport torch.multiprocessing as mp\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\n\n\nclass FFN(nn.Module):\n    def __init__(self, observation_space, action_space, acti='relu'):\n        super(FFN, self).__init__()\n        assert hasattr(observation_space, 'shape') and len(\n            observation_space.shape) == 1\n        assert hasattr(action_space, 'n')\n        in_dim = observation_space.shape[0]\n        out_dim = action_space.n\n        self.lin1 = nn.Linear(in_dim, 32)\n        self.lin2 = nn.Linear(32, 64)\n        self.lin3 = nn.Linear(64, 64)\n        self.lin4 = nn.Linear(64, 32)\n        self.lin5 = nn.Linear(32, out_dim)\n        self.acti = acti\n\n    def forward(self, x):\n        x = F.relu(self.lin1(x))\n        x = F.relu(self.lin2(x))\n        x = F.relu(self.lin3(x))\n        x = F.relu(self.lin4(x))\n        if self.acti == 'softmax':\n            x = F.log_softmax(self.lin5(x), dim=1)\n        elif self.acti == 'relu':\n            x = F.relu(self.lin5(x))\n        return x\n\n\ndef gym_rollout(max_episode_length, model, random_seed, env, is_antithetic):\n    \"\"\"\n    Function to do rollouts of a policy defined by `model` in given environment\n    \"\"\"\n    # Reset environment\n    state = env.reset()\n    state = Variable(torch.from_numpy(state).float(),\n                     requires_grad=True).unsqueeze(0)\n    retrn = 0\n    nsteps = 0\n    done = False\n    # Rollout\n    while not done and nsteps &lt; max_episode_length:\n        # Choose action\n        actions = model(state)\n        action = actions.max(1)[1].data\n        # Step\n        state, reward, done, _ = env.step(action[0])\n        retrn += reward\n        nsteps += 1\n        # Cast state\n        state = Variable(torch.from_numpy(state).float(),\n                         requires_grad=True).unsqueeze(0)\n\n\nfor acti in ['relu', 'softmax']:\n    ts = time.clock()\n    n = 10000\n    env = gym.make('CartPole-v0')\n    model = FFN(env.observation_space, env.action_space, acti)\n\n    for i in range(n):\n        gym_rollout(1000, model, 'dummy_seed', env, 'dummy_neg')\n\n    tf = time.clock()\n    print(acti + \" total: \" + str(tf-ts))\n    print(acti + \" per call: \" + str((tf - ts) / n))\n\n</code></pre>\n<p>On Mac OS, this outputs</p>\n<pre><code>relu total: 21.906198999999997\nrelu per call: 0.0021906199\nsoftmax total: 21.903843000000002\nsoftmax per call: 0.0021903843\n</code></pre>\n<p>On Linux (faster computer) this outputs</p>\n<pre><code>relu total: 8.02111\nrelu per call: 0,000802111\nsoftmax total: 41,709772\nsoftmax per call: 0,0041709772\n</code></pre>\n<p>It does however not take 41 seconds of real time for the <code>softmax</code> but more like the 8 seconds it takes the <code>relu</code>. That is, it uses multiple threads.</p>", "body_text": "Using Linux Mint 18.3 and pytorch 0.3.0, some nn.Module and nn.functional methods appear to use multiple cores/threads and inhibit parallel execution.\nAt the basic level, I am interested in running multiple forward passes in parallel. This works perfectly on Mac OS 10.13.2 but on Linux Mint 18.3, I face the problem described above.\nFor a network like the one below, using softmax while executing multiple forward passes in parallel with torch.multiprocessing.Process results in a complete backlog. If changed to relu, the code executes fine.\nIf testing the softmax/relu situation on a single thread, it appears that the softmax takes up multiple threads on Linux while on Mac OS it does not. Code used to test this is\nimport time\nimport gym\nimport numpy as np\nimport torch\nimport torch.multiprocessing as mp\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.autograd import Variable\n\n\nclass FFN(nn.Module):\n    def __init__(self, observation_space, action_space, acti='relu'):\n        super(FFN, self).__init__()\n        assert hasattr(observation_space, 'shape') and len(\n            observation_space.shape) == 1\n        assert hasattr(action_space, 'n')\n        in_dim = observation_space.shape[0]\n        out_dim = action_space.n\n        self.lin1 = nn.Linear(in_dim, 32)\n        self.lin2 = nn.Linear(32, 64)\n        self.lin3 = nn.Linear(64, 64)\n        self.lin4 = nn.Linear(64, 32)\n        self.lin5 = nn.Linear(32, out_dim)\n        self.acti = acti\n\n    def forward(self, x):\n        x = F.relu(self.lin1(x))\n        x = F.relu(self.lin2(x))\n        x = F.relu(self.lin3(x))\n        x = F.relu(self.lin4(x))\n        if self.acti == 'softmax':\n            x = F.log_softmax(self.lin5(x), dim=1)\n        elif self.acti == 'relu':\n            x = F.relu(self.lin5(x))\n        return x\n\n\ndef gym_rollout(max_episode_length, model, random_seed, env, is_antithetic):\n    \"\"\"\n    Function to do rollouts of a policy defined by `model` in given environment\n    \"\"\"\n    # Reset environment\n    state = env.reset()\n    state = Variable(torch.from_numpy(state).float(),\n                     requires_grad=True).unsqueeze(0)\n    retrn = 0\n    nsteps = 0\n    done = False\n    # Rollout\n    while not done and nsteps < max_episode_length:\n        # Choose action\n        actions = model(state)\n        action = actions.max(1)[1].data\n        # Step\n        state, reward, done, _ = env.step(action[0])\n        retrn += reward\n        nsteps += 1\n        # Cast state\n        state = Variable(torch.from_numpy(state).float(),\n                         requires_grad=True).unsqueeze(0)\n\n\nfor acti in ['relu', 'softmax']:\n    ts = time.clock()\n    n = 10000\n    env = gym.make('CartPole-v0')\n    model = FFN(env.observation_space, env.action_space, acti)\n\n    for i in range(n):\n        gym_rollout(1000, model, 'dummy_seed', env, 'dummy_neg')\n\n    tf = time.clock()\n    print(acti + \" total: \" + str(tf-ts))\n    print(acti + \" per call: \" + str((tf - ts) / n))\n\n\nOn Mac OS, this outputs\nrelu total: 21.906198999999997\nrelu per call: 0.0021906199\nsoftmax total: 21.903843000000002\nsoftmax per call: 0.0021903843\n\nOn Linux (faster computer) this outputs\nrelu total: 8.02111\nrelu per call: 0,000802111\nsoftmax total: 41,709772\nsoftmax per call: 0,0041709772\n\nIt does however not take 41 seconds of real time for the softmax but more like the 8 seconds it takes the relu. That is, it uses multiple threads.", "body": "Using Linux Mint 18.3 and pytorch 0.3.0, some `nn.Module` and `nn.functional` methods appear to use multiple cores/threads and inhibit parallel execution. \r\n\r\nAt the basic level, I am interested in running multiple forward passes in parallel. This works perfectly on Mac OS 10.13.2 but on Linux Mint 18.3, I face the problem described above.\r\n\r\nFor a network like the one below, using `softmax` while executing multiple forward passes in parallel with `torch.multiprocessing.Process` results in a complete backlog. If changed to `relu`, the code executes fine.\r\n\r\nIf testing the softmax/relu situation on a single thread, it appears that the softmax takes up multiple threads on Linux while on Mac OS it does not. Code used to test this is\r\n\r\n```\r\nimport time\r\nimport gym\r\nimport numpy as np\r\nimport torch\r\nimport torch.multiprocessing as mp\r\nimport torch.nn.functional as F\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\n\r\nclass FFN(nn.Module):\r\n    def __init__(self, observation_space, action_space, acti='relu'):\r\n        super(FFN, self).__init__()\r\n        assert hasattr(observation_space, 'shape') and len(\r\n            observation_space.shape) == 1\r\n        assert hasattr(action_space, 'n')\r\n        in_dim = observation_space.shape[0]\r\n        out_dim = action_space.n\r\n        self.lin1 = nn.Linear(in_dim, 32)\r\n        self.lin2 = nn.Linear(32, 64)\r\n        self.lin3 = nn.Linear(64, 64)\r\n        self.lin4 = nn.Linear(64, 32)\r\n        self.lin5 = nn.Linear(32, out_dim)\r\n        self.acti = acti\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.lin1(x))\r\n        x = F.relu(self.lin2(x))\r\n        x = F.relu(self.lin3(x))\r\n        x = F.relu(self.lin4(x))\r\n        if self.acti == 'softmax':\r\n            x = F.log_softmax(self.lin5(x), dim=1)\r\n        elif self.acti == 'relu':\r\n            x = F.relu(self.lin5(x))\r\n        return x\r\n\r\n\r\ndef gym_rollout(max_episode_length, model, random_seed, env, is_antithetic):\r\n    \"\"\"\r\n    Function to do rollouts of a policy defined by `model` in given environment\r\n    \"\"\"\r\n    # Reset environment\r\n    state = env.reset()\r\n    state = Variable(torch.from_numpy(state).float(),\r\n                     requires_grad=True).unsqueeze(0)\r\n    retrn = 0\r\n    nsteps = 0\r\n    done = False\r\n    # Rollout\r\n    while not done and nsteps < max_episode_length:\r\n        # Choose action\r\n        actions = model(state)\r\n        action = actions.max(1)[1].data\r\n        # Step\r\n        state, reward, done, _ = env.step(action[0])\r\n        retrn += reward\r\n        nsteps += 1\r\n        # Cast state\r\n        state = Variable(torch.from_numpy(state).float(),\r\n                         requires_grad=True).unsqueeze(0)\r\n\r\n\r\nfor acti in ['relu', 'softmax']:\r\n    ts = time.clock()\r\n    n = 10000\r\n    env = gym.make('CartPole-v0')\r\n    model = FFN(env.observation_space, env.action_space, acti)\r\n\r\n    for i in range(n):\r\n        gym_rollout(1000, model, 'dummy_seed', env, 'dummy_neg')\r\n\r\n    tf = time.clock()\r\n    print(acti + \" total: \" + str(tf-ts))\r\n    print(acti + \" per call: \" + str((tf - ts) / n))\r\n\r\n```\r\n\r\nOn Mac OS, this outputs\r\n\r\n```\r\nrelu total: 21.906198999999997\r\nrelu per call: 0.0021906199\r\nsoftmax total: 21.903843000000002\r\nsoftmax per call: 0.0021903843\r\n```\r\n\r\nOn Linux (faster computer) this outputs\r\n```\r\nrelu total: 8.02111\r\nrelu per call: 0,000802111\r\nsoftmax total: 41,709772\r\nsoftmax per call: 0,0041709772\r\n```\r\nIt does however not take 41 seconds of real time for the `softmax` but more like the 8 seconds it takes the `relu`. That is, it uses multiple threads.\r\n\r\n"}