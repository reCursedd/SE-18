{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2614", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2614/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2614/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2614/events", "html_url": "https://github.com/pytorch/pytorch/issues/2614", "id": 255078681, "node_id": "MDU6SXNzdWUyNTUwNzg2ODE=", "number": 2614, "title": "RuntimeError: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /home/gpu/dev/rt/pytorch/torch/lib/THC/generic/THCStorage.cu:66", "user": {"login": "QuantScientist", "id": 18743986, "node_id": "MDQ6VXNlcjE4NzQzOTg2", "avatar_url": "https://avatars2.githubusercontent.com/u/18743986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/QuantScientist", "html_url": "https://github.com/QuantScientist", "followers_url": "https://api.github.com/users/QuantScientist/followers", "following_url": "https://api.github.com/users/QuantScientist/following{/other_user}", "gists_url": "https://api.github.com/users/QuantScientist/gists{/gist_id}", "starred_url": "https://api.github.com/users/QuantScientist/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/QuantScientist/subscriptions", "organizations_url": "https://api.github.com/users/QuantScientist/orgs", "repos_url": "https://api.github.com/users/QuantScientist/repos", "events_url": "https://api.github.com/users/QuantScientist/events{/privacy}", "received_events_url": "https://api.github.com/users/QuantScientist/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-09-04T15:58:10Z", "updated_at": "2018-01-01T21:22:08Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>This happens very frequently whenever I stop training on Jupyter.<br>\nOf course, the exception is deceptive since I can use the CUDA device OUTSIDE PyTorch without any issues even after this error happens.</p>\n<p>Stack trace:</p>\n<pre><code>RuntimeErrorTraceback (most recent call last)\n&lt;ipython-input-19-56b1ce2851dc&gt; in &lt;module&gt;()\n     14 if use_cuda:\n     15     lgr.info (\"Using the GPU\")\n---&gt; 16     net.cuda()\n     17     loss_func.cuda()\n     18 #     cudnn.benchmark = True\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in cuda(self, device_id)\n    145                 copied to that device\n    146         \"\"\"\n--&gt; 147         return self._apply(lambda t: t.cuda(device_id))\n    148 \n    149     def cpu(self, device_id=None):\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in _apply(self, fn)\n    116     def _apply(self, fn):\n    117         for module in self.children():\n--&gt; 118             module._apply(fn)\n    119 \n    120         for param in self._parameters.values():\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in _apply(self, fn)\n    122                 # Variables stored in modules are graph leaves, and we don't\n    123                 # want to create copy nodes, so we have to unpack the data.\n--&gt; 124                 param.data = fn(param.data)\n    125                 if param._grad is not None:\n    126                     param._grad.data = fn(param._grad.data)\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in &lt;lambda&gt;(t)\n    145                 copied to that device\n    146         \"\"\"\n--&gt; 147         return self._apply(lambda t: t.cuda(device_id))\n    148 \n    149     def cpu(self, device_id=None):\n\n/usr/local/lib/python2.7/dist-packages/torch/_utils.pyc in _cuda(self, device, async)\n     63         else:\n     64             new_type = getattr(torch.cuda, self.__class__.__name__)\n---&gt; 65             return new_type(self.size()).copy_(self, async)\n     66 \n     67 \n\n/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.pyc in _lazy_new(cls, *args, **kwargs)\n    267     # We need this method only for lazy init, so we can remove it\n    268     del _CudaBase.__new__\n--&gt; 269     return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\n    270 \n    271 \n</code></pre>", "body_text": "This happens very frequently whenever I stop training on Jupyter.\nOf course, the exception is deceptive since I can use the CUDA device OUTSIDE PyTorch without any issues even after this error happens.\nStack trace:\nRuntimeErrorTraceback (most recent call last)\n<ipython-input-19-56b1ce2851dc> in <module>()\n     14 if use_cuda:\n     15     lgr.info (\"Using the GPU\")\n---> 16     net.cuda()\n     17     loss_func.cuda()\n     18 #     cudnn.benchmark = True\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in cuda(self, device_id)\n    145                 copied to that device\n    146         \"\"\"\n--> 147         return self._apply(lambda t: t.cuda(device_id))\n    148 \n    149     def cpu(self, device_id=None):\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in _apply(self, fn)\n    116     def _apply(self, fn):\n    117         for module in self.children():\n--> 118             module._apply(fn)\n    119 \n    120         for param in self._parameters.values():\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in _apply(self, fn)\n    122                 # Variables stored in modules are graph leaves, and we don't\n    123                 # want to create copy nodes, so we have to unpack the data.\n--> 124                 param.data = fn(param.data)\n    125                 if param._grad is not None:\n    126                     param._grad.data = fn(param._grad.data)\n\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in <lambda>(t)\n    145                 copied to that device\n    146         \"\"\"\n--> 147         return self._apply(lambda t: t.cuda(device_id))\n    148 \n    149     def cpu(self, device_id=None):\n\n/usr/local/lib/python2.7/dist-packages/torch/_utils.pyc in _cuda(self, device, async)\n     63         else:\n     64             new_type = getattr(torch.cuda, self.__class__.__name__)\n---> 65             return new_type(self.size()).copy_(self, async)\n     66 \n     67 \n\n/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.pyc in _lazy_new(cls, *args, **kwargs)\n    267     # We need this method only for lazy init, so we can remove it\n    268     del _CudaBase.__new__\n--> 269     return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\n    270 \n    271", "body": "This happens very frequently whenever I stop training on Jupyter. \r\nOf course, the exception is deceptive since I can use the CUDA device OUTSIDE PyTorch without any issues even after this error happens. \r\n\r\nStack trace:\r\n```\r\nRuntimeErrorTraceback (most recent call last)\r\n<ipython-input-19-56b1ce2851dc> in <module>()\r\n     14 if use_cuda:\r\n     15     lgr.info (\"Using the GPU\")\r\n---> 16     net.cuda()\r\n     17     loss_func.cuda()\r\n     18 #     cudnn.benchmark = True\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in cuda(self, device_id)\r\n    145                 copied to that device\r\n    146         \"\"\"\r\n--> 147         return self._apply(lambda t: t.cuda(device_id))\r\n    148 \r\n    149     def cpu(self, device_id=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in _apply(self, fn)\r\n    116     def _apply(self, fn):\r\n    117         for module in self.children():\r\n--> 118             module._apply(fn)\r\n    119 \r\n    120         for param in self._parameters.values():\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in _apply(self, fn)\r\n    122                 # Variables stored in modules are graph leaves, and we don't\r\n    123                 # want to create copy nodes, so we have to unpack the data.\r\n--> 124                 param.data = fn(param.data)\r\n    125                 if param._grad is not None:\r\n    126                     param._grad.data = fn(param._grad.data)\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc in <lambda>(t)\r\n    145                 copied to that device\r\n    146         \"\"\"\r\n--> 147         return self._apply(lambda t: t.cuda(device_id))\r\n    148 \r\n    149     def cpu(self, device_id=None):\r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/_utils.pyc in _cuda(self, device, async)\r\n     63         else:\r\n     64             new_type = getattr(torch.cuda, self.__class__.__name__)\r\n---> 65             return new_type(self.size()).copy_(self, async)\r\n     66 \r\n     67 \r\n\r\n/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.pyc in _lazy_new(cls, *args, **kwargs)\r\n    267     # We need this method only for lazy init, so we can remove it\r\n    268     del _CudaBase.__new__\r\n--> 269     return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\r\n    270 \r\n    271 \r\n```"}