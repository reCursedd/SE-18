{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3827", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3827/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3827/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3827/events", "html_url": "https://github.com/pytorch/pytorch/issues/3827", "id": 275938599, "node_id": "MDU6SXNzdWUyNzU5Mzg1OTk=", "number": 3827, "title": "the problem of calculating grad ", "user": {"login": "hexunshi", "id": 15814160, "node_id": "MDQ6VXNlcjE1ODE0MTYw", "avatar_url": "https://avatars1.githubusercontent.com/u/15814160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hexunshi", "html_url": "https://github.com/hexunshi", "followers_url": "https://api.github.com/users/hexunshi/followers", "following_url": "https://api.github.com/users/hexunshi/following{/other_user}", "gists_url": "https://api.github.com/users/hexunshi/gists{/gist_id}", "starred_url": "https://api.github.com/users/hexunshi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hexunshi/subscriptions", "organizations_url": "https://api.github.com/users/hexunshi/orgs", "repos_url": "https://api.github.com/users/hexunshi/repos", "events_url": "https://api.github.com/users/hexunshi/events{/privacy}", "received_events_url": "https://api.github.com/users/hexunshi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-22T04:05:43Z", "updated_at": "2017-11-23T03:12:24Z", "closed_at": "2017-11-23T03:12:24Z", "author_association": "NONE", "body_html": "<p>I register two hook and calculate the backward of  loss function for batch training</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">backward_hook</span>(<span class=\"pl-smi\">module</span>, <span class=\"pl-smi\">grad_input</span>, <span class=\"pl-smi\">grad_output</span>):\n    module.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>backward_hook_input<span class=\"pl-pds\">'</span></span>,grad_input)\n    module.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>backward_hook_output<span class=\"pl-pds\">'</span></span>,grad_output)\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">None</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">forward_hook</span>(<span class=\"pl-smi\">module</span>,<span class=\"pl-smi\">input</span>,<span class=\"pl-smi\">output</span>):\n    module.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>forward_hook_input<span class=\"pl-pds\">'</span></span>,<span class=\"pl-c1\">input</span>)\n    module.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>forward_hook_output<span class=\"pl-pds\">'</span></span>,output)\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">None</span></pre></div>\n<div class=\"highlight highlight-source-python\"><pre>optimizer <span class=\"pl-k\">=</span> torch.optim.SGD(net.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)\nloss_func <span class=\"pl-k\">=</span> torch.nn.MSELoss() \nforward_hook1 <span class=\"pl-k\">=</span> net.hidden.register_forward_hook(<span class=\"pl-v\">hook</span><span class=\"pl-k\">=</span>forward_hook)\nbackward_hook1 <span class=\"pl-k\">=</span> net.hidden.register_backward_hook(backward_hook)\nprediction <span class=\"pl-k\">=</span> net(x[<span class=\"pl-c1\">0</span>:<span class=\"pl-c1\">15</span>])\nloss <span class=\"pl-k\">=</span> loss_func(prediction, y[<span class=\"pl-c1\">0</span>:<span class=\"pl-c1\">15</span>])\noptimizer.zero_grad()\nloss.backward()\nforward_hook1.remove()\nbackward_hook1.remove()</pre></div>\n<p>calculate the following two values</p>\n<div class=\"highlight highlight-source-python\"><pre>torch.matmul(net.hidden.forward_hook_input[<span class=\"pl-c1\">0</span>].transpose(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>),net.hidden.backward_hook_output[<span class=\"pl-c1\">0</span>])\nnet.hidden.weight.grad.transpose(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>)</pre></div>\n<p>Why are these two numbers equal?<br>\nI think the grad of weight is the sum of the grad of each data,like this</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">15</span>):\n    <span class=\"pl-c1\">sum</span> <span class=\"pl-k\">+=</span> torch.matmul(net.hidden.forward_hook_input[<span class=\"pl-c1\">0</span>][i].transpose(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>),net.hidden.backward_hook_output[<span class=\"pl-c1\">0</span>][i])</pre></div>", "body_text": "I register two hook and calculate the backward of  loss function for batch training\ndef backward_hook(module, grad_input, grad_output):\n    module.register_buffer('backward_hook_input',grad_input)\n    module.register_buffer('backward_hook_output',grad_output)\n    return None\ndef forward_hook(module,input,output):\n    module.register_buffer('forward_hook_input',input)\n    module.register_buffer('forward_hook_output',output)\n    return None\noptimizer = torch.optim.SGD(net.parameters(), lr=0.5)\nloss_func = torch.nn.MSELoss() \nforward_hook1 = net.hidden.register_forward_hook(hook=forward_hook)\nbackward_hook1 = net.hidden.register_backward_hook(backward_hook)\nprediction = net(x[0:15])\nloss = loss_func(prediction, y[0:15])\noptimizer.zero_grad()\nloss.backward()\nforward_hook1.remove()\nbackward_hook1.remove()\ncalculate the following two values\ntorch.matmul(net.hidden.forward_hook_input[0].transpose(0,1),net.hidden.backward_hook_output[0])\nnet.hidden.weight.grad.transpose(0,1)\nWhy are these two numbers equal?\nI think the grad of weight is the sum of the grad of each data,like this\nfor i in range(0,15):\n    sum += torch.matmul(net.hidden.forward_hook_input[0][i].transpose(0,1),net.hidden.backward_hook_output[0][i])", "body": "I register two hook and calculate the backward of  loss function for batch training\r\n```python\r\ndef backward_hook(module, grad_input, grad_output):\r\n    module.register_buffer('backward_hook_input',grad_input)\r\n    module.register_buffer('backward_hook_output',grad_output)\r\n    return None\r\ndef forward_hook(module,input,output):\r\n    module.register_buffer('forward_hook_input',input)\r\n    module.register_buffer('forward_hook_output',output)\r\n    return None\r\n```\r\n```python\r\noptimizer = torch.optim.SGD(net.parameters(), lr=0.5)\r\nloss_func = torch.nn.MSELoss() \r\nforward_hook1 = net.hidden.register_forward_hook(hook=forward_hook)\r\nbackward_hook1 = net.hidden.register_backward_hook(backward_hook)\r\nprediction = net(x[0:15])\r\nloss = loss_func(prediction, y[0:15])\r\noptimizer.zero_grad()\r\nloss.backward()\r\nforward_hook1.remove()\r\nbackward_hook1.remove()\r\n```\r\ncalculate the following two values\r\n```python\r\ntorch.matmul(net.hidden.forward_hook_input[0].transpose(0,1),net.hidden.backward_hook_output[0])\r\nnet.hidden.weight.grad.transpose(0,1)\r\n```\r\nWhy are these two numbers equal?\r\nI think the grad of weight is the sum of the grad of each data,like this\r\n```python\r\nfor i in range(0,15):\r\n    sum += torch.matmul(net.hidden.forward_hook_input[0][i].transpose(0,1),net.hidden.backward_hook_output[0][i])\r\n```"}