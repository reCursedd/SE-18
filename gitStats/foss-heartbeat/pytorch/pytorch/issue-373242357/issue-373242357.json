{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13018", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13018/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13018/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13018/events", "html_url": "https://github.com/pytorch/pytorch/issues/13018", "id": 373242357, "node_id": "MDU6SXNzdWUzNzMyNDIzNTc=", "number": 13018, "title": "Improved performance for torch.multinomial with small batches", "user": {"login": "jcjohnson", "id": 2718714, "node_id": "MDQ6VXNlcjI3MTg3MTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/2718714?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jcjohnson", "html_url": "https://github.com/jcjohnson", "followers_url": "https://api.github.com/users/jcjohnson/followers", "following_url": "https://api.github.com/users/jcjohnson/following{/other_user}", "gists_url": "https://api.github.com/users/jcjohnson/gists{/gist_id}", "starred_url": "https://api.github.com/users/jcjohnson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jcjohnson/subscriptions", "organizations_url": "https://api.github.com/users/jcjohnson/orgs", "repos_url": "https://api.github.com/users/jcjohnson/repos", "events_url": "https://api.github.com/users/jcjohnson/events{/privacy}", "received_events_url": "https://api.github.com/users/jcjohnson/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-23T23:23:33Z", "updated_at": "2018-10-26T11:17:42Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"rocket\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f680.png\">\ud83d\ude80</g-emoji> Feature</h2>\n<p><code>torch.multinomial</code> (with replacement) has very poor performance with small batches but large numbers of samples or categories. In the regime of batch size N=1, categories C=100,000 and samples S=10,000, <code>numpy.choice</code> (running on CPU) is up to 4x faster than <code>torch.multinomial</code> running on a P100 GPU:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2718714/47396316-0da88880-d6df-11e8-8d83-1d2282975c8e.png\"><img src=\"https://user-images.githubusercontent.com/2718714/47396316-0da88880-d6df-11e8-8d83-1d2282975c8e.png\" alt=\"vary_c\" style=\"max-width:100%;\"></a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2718714/47396317-113c0f80-d6df-11e8-8d1b-6d3f1a518be6.png\"><img src=\"https://user-images.githubusercontent.com/2718714/47396317-113c0f80-d6df-11e8-8d1b-6d3f1a518be6.png\" alt=\"vary_s\" style=\"max-width:100%;\"></a></p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/2718714/47396319-1305d300-d6df-11e8-8eb0-f0c6aaaa46f8.png\"><img src=\"https://user-images.githubusercontent.com/2718714/47396319-1305d300-d6df-11e8-8eb0-f0c6aaaa46f8.png\" alt=\"vary_n\" style=\"max-width:100%;\"></a></p>\n<p>(Full benchmarking script available here: <a href=\"https://github.com/jcjohnson/pytorch-multinomial-benchmark\">https://github.com/jcjohnson/pytorch-multinomial-benchmark</a>)</p>\n<p>These benchmarks suggests that <code>torch.multinomial</code> does a good job exploiting parallelism over the batch dimension, but not over the category or sample dimensions; I feel that this could be improved to yield significantly better performance in small-batch scenarios.</p>\n<p>(There also appear to be performance issues when sampling without replacement <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"362538239\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11931\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/11931/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/11931\">#11931</a>)</p>\n<h2>Motivation</h2>\n<p>After some benchmarking I found that a project I'm working on is bottlenecked by sampling in small-batch, large-sample, large-category regimes; based on above benchmarks I'm forced to perform the sampling on CPU with numpy for the best performance; this both feels wrong and still causes sampling to be the bottleneck.</p>\n<h2>Pitch</h2>\n<p>I'm not terribly familiar with details of the current CUDA backend, but when sampling with replacement it should be possible to exploit parallelism over the sample dimension to achieve significantly improved performance over numpy.</p>", "body_text": "\ud83d\ude80 Feature\ntorch.multinomial (with replacement) has very poor performance with small batches but large numbers of samples or categories. In the regime of batch size N=1, categories C=100,000 and samples S=10,000, numpy.choice (running on CPU) is up to 4x faster than torch.multinomial running on a P100 GPU:\n\n\n\n(Full benchmarking script available here: https://github.com/jcjohnson/pytorch-multinomial-benchmark)\nThese benchmarks suggests that torch.multinomial does a good job exploiting parallelism over the batch dimension, but not over the category or sample dimensions; I feel that this could be improved to yield significantly better performance in small-batch scenarios.\n(There also appear to be performance issues when sampling without replacement #11931)\nMotivation\nAfter some benchmarking I found that a project I'm working on is bottlenecked by sampling in small-batch, large-sample, large-category regimes; based on above benchmarks I'm forced to perform the sampling on CPU with numpy for the best performance; this both feels wrong and still causes sampling to be the bottleneck.\nPitch\nI'm not terribly familiar with details of the current CUDA backend, but when sampling with replacement it should be possible to exploit parallelism over the sample dimension to achieve significantly improved performance over numpy.", "body": "## \ud83d\ude80 Feature\r\n`torch.multinomial` (with replacement) has very poor performance with small batches but large numbers of samples or categories. In the regime of batch size N=1, categories C=100,000 and samples S=10,000, `numpy.choice` (running on CPU) is up to 4x faster than `torch.multinomial` running on a P100 GPU:\r\n\r\n![vary_c](https://user-images.githubusercontent.com/2718714/47396316-0da88880-d6df-11e8-8d83-1d2282975c8e.png)\r\n\r\n![vary_s](https://user-images.githubusercontent.com/2718714/47396317-113c0f80-d6df-11e8-8d1b-6d3f1a518be6.png)\r\n\r\n![vary_n](https://user-images.githubusercontent.com/2718714/47396319-1305d300-d6df-11e8-8eb0-f0c6aaaa46f8.png)\r\n\r\n(Full benchmarking script available here: https://github.com/jcjohnson/pytorch-multinomial-benchmark)\r\n\r\nThese benchmarks suggests that `torch.multinomial` does a good job exploiting parallelism over the batch dimension, but not over the category or sample dimensions; I feel that this could be improved to yield significantly better performance in small-batch scenarios.\r\n\r\n(There also appear to be performance issues when sampling without replacement https://github.com/pytorch/pytorch/issues/11931)\r\n\r\n## Motivation\r\n\r\nAfter some benchmarking I found that a project I'm working on is bottlenecked by sampling in small-batch, large-sample, large-category regimes; based on above benchmarks I'm forced to perform the sampling on CPU with numpy for the best performance; this both feels wrong and still causes sampling to be the bottleneck.\r\n\r\n## Pitch\r\n\r\nI'm not terribly familiar with details of the current CUDA backend, but when sampling with replacement it should be possible to exploit parallelism over the sample dimension to achieve significantly improved performance over numpy.\r\n"}