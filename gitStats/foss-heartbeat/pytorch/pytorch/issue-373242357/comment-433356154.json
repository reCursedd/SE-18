{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/433356154", "html_url": "https://github.com/pytorch/pytorch/issues/13018#issuecomment-433356154", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13018", "id": 433356154, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzM1NjE1NA==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-26T09:59:18Z", "updated_at": "2018-10-26T11:17:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Oh wow. Your bug report is a work of art!<br>\nIt's not as safe (it doesn't check negative probabilities because it uses cumsum rather than it's own \"checking cumsum\", and I didn't check what kind of adjustment I need for \"be perfectly safe to not select probaility 0 categories\" which the THC code does), but the following C++ extension attempts to implement fully parallel, non-diverging binary search and might show some speedup.<br>\nIt runs with current (as in today's) master.<br>\nI'll coordinate with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8906225\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/syed-ahmed\">@syed-ahmed</a> about a proper PR.</p>\n<pre><code>multinomial_src = \"\"\"\n#include &lt;torch/extension.h&gt;\n#include &lt;THC/THCDeviceUtils.cuh&gt;\n#include &lt;THC/THCGeneral.h&gt;\n#include \"ATen/ATen.h\"\n#include \"ATen/AccumulateType.h\"\n#include \"ATen/cuda/CUDAContext.h\"\n#include &lt;assert.h&gt;\nusing namespace at;\n\ntemplate &lt;typename index_t&gt;\n__device__ int get_msb(index_t size) {\n    int c = 0;\n    for (; size &gt; 0; size &gt;&gt;= 1) {\n      c++;\n    }\n    return c;\n}\n\ntemplate &lt;typename scalar_t, typename index_t&gt;\n__global__ void uniform_to_multinomial(\n  PackedTensorAccessor&lt;int64_t, 2, at::RestrictPtrTraits, index_t&gt; res,\n  const PackedTensorAccessor&lt;scalar_t, 2, at::RestrictPtrTraits, index_t&gt; cdfs,\n  const PackedTensorAccessor&lt;scalar_t, 2, at::RestrictPtrTraits, index_t&gt; random\n  ) {\n  int sample_idx = threadIdx.x + blockDim.x * blockIdx.x;\n  int batch_idx  = threadIdx.y + blockDim.y * blockIdx.y;\n  if (batch_idx &gt;= cdfs.size(0) || sample_idx &gt; res.size(1))\n    return;\n  index_t max_pos = cdfs.size(1)-1;\n  auto cdf = cdfs[batch_idx];\n  assert(cdf[max_pos] &gt; 0);\n  scalar_t val = random[batch_idx][sample_idx] * cdf[max_pos];\n  index_t pos = 0;\n  // fixed number of steps to avoid divergence\n  for(index_t step = static_cast&lt;index_t&gt;(1)&lt;&lt;get_msb(max_pos); step&gt;0; step&gt;&gt;=1) {\n    pos += ((pos+step&lt;=max_pos) &amp;&amp; (cdf[pos+step-1]&lt;=val)) ? step : 0;\n  }\n  res[batch_idx][sample_idx] = pos;\n}\n\n\ntemplate&lt;typename scalar_t, typename index_t&gt;\n  Tensor multinomial_template(const Tensor&amp; probs_, int64_t n_samples) {\n  AT_CHECK(probs_.is_cuda(), \"This is for cuda only\");\n  AT_CHECK(probs_.dim()==1 || probs_.dim()==2, \"Probs needs to be 1d or, for batched operation, 2d\");\n  Tensor probs = probs_.view({-1, probs_.size(-1)});\n  Tensor res = at::empty({probs.size(0), n_samples}, probs.options().dtype(kLong));\n  Tensor random = at::rand({probs.size(0), n_samples}, probs.options());\n  Tensor cdf = probs.cumsum(-1);\n  int ts = std::min&lt;int&gt;(512, n_samples);\n  int tb = std::max&lt;int&gt;(512/ ts, 1);\n  dim3 threads(ts, tb);\n  dim3 blocks((n_samples+ts-1)/ts, (cdf.size(0)+tb-1)/tb);\n  uniform_to_multinomial&lt;scalar_t, index_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n    res.packed_accessor&lt;int64_t, 2, at::RestrictPtrTraits, index_t&gt;(),\n    cdf.packed_accessor&lt;scalar_t, 2, at::RestrictPtrTraits, index_t&gt;(),\n    random.packed_accessor&lt;scalar_t, 2, at::RestrictPtrTraits, index_t&gt;()\n  );\n  if (probs_.dim()==1)\n    res.squeeze_(0);\n  return res;\n}\n\n\n\nTensor multinomial_cuda(const Tensor&amp; probs, int64_t n_samples) {\n  return AT_DISPATCH_FLOATING_TYPES(probs.type(), \"multinomial\", [&amp;] {\n      return multinomial_template&lt;scalar_t, int64_t&gt;(probs, n_samples);\n    });\n}\n\n\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"multinomial\", &amp;multinomial_cuda, \"multinomial with replacement\");\n}\n\"\"\"\n\nimport torch\nimport torch.utils.cpp_extension\n\nmultinomial_ext = torch.utils.cpp_extension.load_inline(\"multinomial_ext\", [], cuda_sources=[multinomial_src], verbose=True)\n\n\na = torch.randn(5, 1_000, device='cuda').softmax(-1)\nres = ext.multinomial(a, 1_000_000)\n</code></pre>", "body_text": "Oh wow. Your bug report is a work of art!\nIt's not as safe (it doesn't check negative probabilities because it uses cumsum rather than it's own \"checking cumsum\", and I didn't check what kind of adjustment I need for \"be perfectly safe to not select probaility 0 categories\" which the THC code does), but the following C++ extension attempts to implement fully parallel, non-diverging binary search and might show some speedup.\nIt runs with current (as in today's) master.\nI'll coordinate with @syed-ahmed about a proper PR.\nmultinomial_src = \"\"\"\n#include <torch/extension.h>\n#include <THC/THCDeviceUtils.cuh>\n#include <THC/THCGeneral.h>\n#include \"ATen/ATen.h\"\n#include \"ATen/AccumulateType.h\"\n#include \"ATen/cuda/CUDAContext.h\"\n#include <assert.h>\nusing namespace at;\n\ntemplate <typename index_t>\n__device__ int get_msb(index_t size) {\n    int c = 0;\n    for (; size > 0; size >>= 1) {\n      c++;\n    }\n    return c;\n}\n\ntemplate <typename scalar_t, typename index_t>\n__global__ void uniform_to_multinomial(\n  PackedTensorAccessor<int64_t, 2, at::RestrictPtrTraits, index_t> res,\n  const PackedTensorAccessor<scalar_t, 2, at::RestrictPtrTraits, index_t> cdfs,\n  const PackedTensorAccessor<scalar_t, 2, at::RestrictPtrTraits, index_t> random\n  ) {\n  int sample_idx = threadIdx.x + blockDim.x * blockIdx.x;\n  int batch_idx  = threadIdx.y + blockDim.y * blockIdx.y;\n  if (batch_idx >= cdfs.size(0) || sample_idx > res.size(1))\n    return;\n  index_t max_pos = cdfs.size(1)-1;\n  auto cdf = cdfs[batch_idx];\n  assert(cdf[max_pos] > 0);\n  scalar_t val = random[batch_idx][sample_idx] * cdf[max_pos];\n  index_t pos = 0;\n  // fixed number of steps to avoid divergence\n  for(index_t step = static_cast<index_t>(1)<<get_msb(max_pos); step>0; step>>=1) {\n    pos += ((pos+step<=max_pos) && (cdf[pos+step-1]<=val)) ? step : 0;\n  }\n  res[batch_idx][sample_idx] = pos;\n}\n\n\ntemplate<typename scalar_t, typename index_t>\n  Tensor multinomial_template(const Tensor& probs_, int64_t n_samples) {\n  AT_CHECK(probs_.is_cuda(), \"This is for cuda only\");\n  AT_CHECK(probs_.dim()==1 || probs_.dim()==2, \"Probs needs to be 1d or, for batched operation, 2d\");\n  Tensor probs = probs_.view({-1, probs_.size(-1)});\n  Tensor res = at::empty({probs.size(0), n_samples}, probs.options().dtype(kLong));\n  Tensor random = at::rand({probs.size(0), n_samples}, probs.options());\n  Tensor cdf = probs.cumsum(-1);\n  int ts = std::min<int>(512, n_samples);\n  int tb = std::max<int>(512/ ts, 1);\n  dim3 threads(ts, tb);\n  dim3 blocks((n_samples+ts-1)/ts, (cdf.size(0)+tb-1)/tb);\n  uniform_to_multinomial<scalar_t, index_t><<<blocks, threads>>>(\n    res.packed_accessor<int64_t, 2, at::RestrictPtrTraits, index_t>(),\n    cdf.packed_accessor<scalar_t, 2, at::RestrictPtrTraits, index_t>(),\n    random.packed_accessor<scalar_t, 2, at::RestrictPtrTraits, index_t>()\n  );\n  if (probs_.dim()==1)\n    res.squeeze_(0);\n  return res;\n}\n\n\n\nTensor multinomial_cuda(const Tensor& probs, int64_t n_samples) {\n  return AT_DISPATCH_FLOATING_TYPES(probs.type(), \"multinomial\", [&] {\n      return multinomial_template<scalar_t, int64_t>(probs, n_samples);\n    });\n}\n\n\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"multinomial\", &multinomial_cuda, \"multinomial with replacement\");\n}\n\"\"\"\n\nimport torch\nimport torch.utils.cpp_extension\n\nmultinomial_ext = torch.utils.cpp_extension.load_inline(\"multinomial_ext\", [], cuda_sources=[multinomial_src], verbose=True)\n\n\na = torch.randn(5, 1_000, device='cuda').softmax(-1)\nres = ext.multinomial(a, 1_000_000)", "body": "Oh wow. Your bug report is a work of art!\r\nIt's not as safe (it doesn't check negative probabilities because it uses cumsum rather than it's own \"checking cumsum\", and I didn't check what kind of adjustment I need for \"be perfectly safe to not select probaility 0 categories\" which the THC code does), but the following C++ extension attempts to implement fully parallel, non-diverging binary search and might show some speedup.\r\nIt runs with current (as in today's) master.\r\nI'll coordinate with @syed-ahmed about a proper PR.\r\n```\r\nmultinomial_src = \"\"\"\r\n#include <torch/extension.h>\r\n#include <THC/THCDeviceUtils.cuh>\r\n#include <THC/THCGeneral.h>\r\n#include \"ATen/ATen.h\"\r\n#include \"ATen/AccumulateType.h\"\r\n#include \"ATen/cuda/CUDAContext.h\"\r\n#include <assert.h>\r\nusing namespace at;\r\n\r\ntemplate <typename index_t>\r\n__device__ int get_msb(index_t size) {\r\n    int c = 0;\r\n    for (; size > 0; size >>= 1) {\r\n      c++;\r\n    }\r\n    return c;\r\n}\r\n\r\ntemplate <typename scalar_t, typename index_t>\r\n__global__ void uniform_to_multinomial(\r\n  PackedTensorAccessor<int64_t, 2, at::RestrictPtrTraits, index_t> res,\r\n  const PackedTensorAccessor<scalar_t, 2, at::RestrictPtrTraits, index_t> cdfs,\r\n  const PackedTensorAccessor<scalar_t, 2, at::RestrictPtrTraits, index_t> random\r\n  ) {\r\n  int sample_idx = threadIdx.x + blockDim.x * blockIdx.x;\r\n  int batch_idx  = threadIdx.y + blockDim.y * blockIdx.y;\r\n  if (batch_idx >= cdfs.size(0) || sample_idx > res.size(1))\r\n    return;\r\n  index_t max_pos = cdfs.size(1)-1;\r\n  auto cdf = cdfs[batch_idx];\r\n  assert(cdf[max_pos] > 0);\r\n  scalar_t val = random[batch_idx][sample_idx] * cdf[max_pos];\r\n  index_t pos = 0;\r\n  // fixed number of steps to avoid divergence\r\n  for(index_t step = static_cast<index_t>(1)<<get_msb(max_pos); step>0; step>>=1) {\r\n    pos += ((pos+step<=max_pos) && (cdf[pos+step-1]<=val)) ? step : 0;\r\n  }\r\n  res[batch_idx][sample_idx] = pos;\r\n}\r\n\r\n\r\ntemplate<typename scalar_t, typename index_t>\r\n  Tensor multinomial_template(const Tensor& probs_, int64_t n_samples) {\r\n  AT_CHECK(probs_.is_cuda(), \"This is for cuda only\");\r\n  AT_CHECK(probs_.dim()==1 || probs_.dim()==2, \"Probs needs to be 1d or, for batched operation, 2d\");\r\n  Tensor probs = probs_.view({-1, probs_.size(-1)});\r\n  Tensor res = at::empty({probs.size(0), n_samples}, probs.options().dtype(kLong));\r\n  Tensor random = at::rand({probs.size(0), n_samples}, probs.options());\r\n  Tensor cdf = probs.cumsum(-1);\r\n  int ts = std::min<int>(512, n_samples);\r\n  int tb = std::max<int>(512/ ts, 1);\r\n  dim3 threads(ts, tb);\r\n  dim3 blocks((n_samples+ts-1)/ts, (cdf.size(0)+tb-1)/tb);\r\n  uniform_to_multinomial<scalar_t, index_t><<<blocks, threads>>>(\r\n    res.packed_accessor<int64_t, 2, at::RestrictPtrTraits, index_t>(),\r\n    cdf.packed_accessor<scalar_t, 2, at::RestrictPtrTraits, index_t>(),\r\n    random.packed_accessor<scalar_t, 2, at::RestrictPtrTraits, index_t>()\r\n  );\r\n  if (probs_.dim()==1)\r\n    res.squeeze_(0);\r\n  return res;\r\n}\r\n\r\n\r\n\r\nTensor multinomial_cuda(const Tensor& probs, int64_t n_samples) {\r\n  return AT_DISPATCH_FLOATING_TYPES(probs.type(), \"multinomial\", [&] {\r\n      return multinomial_template<scalar_t, int64_t>(probs, n_samples);\r\n    });\r\n}\r\n\r\n\r\n\r\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\r\n  m.def(\"multinomial\", &multinomial_cuda, \"multinomial with replacement\");\r\n}\r\n\"\"\"\r\n\r\nimport torch\r\nimport torch.utils.cpp_extension\r\n\r\nmultinomial_ext = torch.utils.cpp_extension.load_inline(\"multinomial_ext\", [], cuda_sources=[multinomial_src], verbose=True)\r\n\r\n\r\na = torch.randn(5, 1_000, device='cuda').softmax(-1)\r\nres = ext.multinomial(a, 1_000_000)\r\n```\r\n"}