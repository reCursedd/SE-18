{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/406822741", "html_url": "https://github.com/pytorch/pytorch/issues/9674#issuecomment-406822741", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9674", "id": 406822741, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjgyMjc0MQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-21T20:49:39Z", "updated_at": "2018-07-21T20:49:39Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>It's difficult to support operators on hybrid tensors, especially because only embedding uses hybrid.</p>\n</blockquote>\n<p>But embedding is a really important use case!</p>\n<blockquote>\n<p>Many sparse libraries use CSR because it's really efficient for ops such as mm. Ideally, we'd like to make use of these libraries, but it's difficult: CSR can't represent more than 2D, but COO requires a lot of conversions.</p>\n</blockquote>\n<p>With hybrid COO, this is not really true. The values() of a coalesced hybrid COO tensor is a valid values() of a CSR tensor; you just need to reconstruct the index tensors. Indeed, <code>_to_csr_int</code> does precisely this.</p>\n<blockquote>\n<p>Without support for 0-dim tensors, we cannot have proper support for scalar sparse tensors.</p>\n</blockquote>\n<p>We'll have 0-dim support soon.</p>\n<blockquote>\n<p>Return a dense gradient. We can have backward functions able to handle both of dense and sparse gradients. No special treatment needed during autograd, just more functions to be written.</p>\n</blockquote>\n<p>This is just as bad as returning a sparse dense tensor. The argument I have made is that we would rather error than densify a sparse tensor. Let the user explicitly densify if that is what they want.</p>\n<p>And some wild speculation: I've been wondering if it makes sense to support a lazy sparse representation of the form \"Sparse Matrix + Constant\". So, for example, a matrix of all ones could be represented as \"Sparse Matrix with nnz=0 + 1\", an O(1) representation. The point is not that these are sparse (they're not, and many operations will blow up on them) but that sometimes the dense term will cancel out and you'll be back to a sparse operation, profitably.</p>", "body_text": "It's difficult to support operators on hybrid tensors, especially because only embedding uses hybrid.\n\nBut embedding is a really important use case!\n\nMany sparse libraries use CSR because it's really efficient for ops such as mm. Ideally, we'd like to make use of these libraries, but it's difficult: CSR can't represent more than 2D, but COO requires a lot of conversions.\n\nWith hybrid COO, this is not really true. The values() of a coalesced hybrid COO tensor is a valid values() of a CSR tensor; you just need to reconstruct the index tensors. Indeed, _to_csr_int does precisely this.\n\nWithout support for 0-dim tensors, we cannot have proper support for scalar sparse tensors.\n\nWe'll have 0-dim support soon.\n\nReturn a dense gradient. We can have backward functions able to handle both of dense and sparse gradients. No special treatment needed during autograd, just more functions to be written.\n\nThis is just as bad as returning a sparse dense tensor. The argument I have made is that we would rather error than densify a sparse tensor. Let the user explicitly densify if that is what they want.\nAnd some wild speculation: I've been wondering if it makes sense to support a lazy sparse representation of the form \"Sparse Matrix + Constant\". So, for example, a matrix of all ones could be represented as \"Sparse Matrix with nnz=0 + 1\", an O(1) representation. The point is not that these are sparse (they're not, and many operations will blow up on them) but that sometimes the dense term will cancel out and you'll be back to a sparse operation, profitably.", "body": "> It's difficult to support operators on hybrid tensors, especially because only embedding uses hybrid.\r\n\r\nBut embedding is a really important use case!\r\n\r\n> Many sparse libraries use CSR because it's really efficient for ops such as mm. Ideally, we'd like to make use of these libraries, but it's difficult: CSR can't represent more than 2D, but COO requires a lot of conversions.\r\n\r\nWith hybrid COO, this is not really true. The values() of a coalesced hybrid COO tensor is a valid values() of a CSR tensor; you just need to reconstruct the index tensors. Indeed, `_to_csr_int` does precisely this.\r\n\r\n> Without support for 0-dim tensors, we cannot have proper support for scalar sparse tensors.\r\n\r\nWe'll have 0-dim support soon.\r\n\r\n> Return a dense gradient. We can have backward functions able to handle both of dense and sparse gradients. No special treatment needed during autograd, just more functions to be written.\r\n\r\nThis is just as bad as returning a sparse dense tensor. The argument I have made is that we would rather error than densify a sparse tensor. Let the user explicitly densify if that is what they want.\r\n\r\nAnd some wild speculation: I've been wondering if it makes sense to support a lazy sparse representation of the form \"Sparse Matrix + Constant\". So, for example, a matrix of all ones could be represented as \"Sparse Matrix with nnz=0 + 1\", an O(1) representation. The point is not that these are sparse (they're not, and many operations will blow up on them) but that sometimes the dense term will cancel out and you'll be back to a sparse operation, profitably."}