{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/409050525", "html_url": "https://github.com/pytorch/pytorch/issues/9674#issuecomment-409050525", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9674", "id": 409050525, "node_id": "MDEyOklzc3VlQ29tbWVudDQwOTA1MDUyNQ==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-31T00:02:30Z", "updated_at": "2018-08-06T19:51:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Some update of design decisions:</p>\n<ul>\n<li>Agreed upon to represent sparse tensor in hybrid COO format, and Sparse tensor + Constant is not needed currently</li>\n<li>Support autograd on <em>values() of sparse tensor (note that certain ops such as resize</em>() on values might change the mapping between values and indices, but this ops is rarely used)</li>\n<li><del>If sparse ops have the same name as dense, the backward should have the same behavior, i.e., dense gradient, and in sparse format for sparse tensor. Thus dense gradient is allowed for sparse ops.</del><br>\nFor sparse ops with dense gradients, we will only support forward ops and error-out in the backward. In another word, we don't support autograd for these ops. The reason is because to have backward for these ops does not gain us much compares to just using dense ops.</li>\n<li>Support special sparse ops with sparse gradient (zero out grads at non-nnz positions), e.g., sparse_add(), sparse_mul(), etc</li>\n</ul>", "body_text": "Some update of design decisions:\n\nAgreed upon to represent sparse tensor in hybrid COO format, and Sparse tensor + Constant is not needed currently\nSupport autograd on values() of sparse tensor (note that certain ops such as resize() on values might change the mapping between values and indices, but this ops is rarely used)\nIf sparse ops have the same name as dense, the backward should have the same behavior, i.e., dense gradient, and in sparse format for sparse tensor. Thus dense gradient is allowed for sparse ops.\nFor sparse ops with dense gradients, we will only support forward ops and error-out in the backward. In another word, we don't support autograd for these ops. The reason is because to have backward for these ops does not gain us much compares to just using dense ops.\nSupport special sparse ops with sparse gradient (zero out grads at non-nnz positions), e.g., sparse_add(), sparse_mul(), etc", "body": "Some update of design decisions:\r\n- Agreed upon to represent sparse tensor in hybrid COO format, and Sparse tensor + Constant is not needed currently\r\n- Support autograd on _values() of sparse tensor (note that certain ops such as resize_() on values might change the mapping between values and indices, but this ops is rarely used)\r\n- ~~If sparse ops have the same name as dense, the backward should have the same behavior, i.e., dense gradient, and in sparse format for sparse tensor. Thus dense gradient is allowed for sparse ops.~~\r\nFor sparse ops with dense gradients, we will only support forward ops and error-out in the backward. In another word, we don't support autograd for these ops. The reason is because to have backward for these ops does not gain us much compares to just using dense ops.\r\n- Support special sparse ops with sparse gradient (zero out grads at non-nnz positions), e.g., sparse_add(), sparse_mul(), etc\r\n"}