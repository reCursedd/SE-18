{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/407145706", "html_url": "https://github.com/pytorch/pytorch/issues/9674#issuecomment-407145706", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9674", "id": 407145706, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzE0NTcwNg==", "user": {"login": "li-roy", "id": 8813817, "node_id": "MDQ6VXNlcjg4MTM4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8813817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/li-roy", "html_url": "https://github.com/li-roy", "followers_url": "https://api.github.com/users/li-roy/followers", "following_url": "https://api.github.com/users/li-roy/following{/other_user}", "gists_url": "https://api.github.com/users/li-roy/gists{/gist_id}", "starred_url": "https://api.github.com/users/li-roy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/li-roy/subscriptions", "organizations_url": "https://api.github.com/users/li-roy/orgs", "repos_url": "https://api.github.com/users/li-roy/repos", "events_url": "https://api.github.com/users/li-roy/events{/privacy}", "received_events_url": "https://api.github.com/users/li-roy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-23T17:55:07Z", "updated_at": "2018-07-23T17:55:07Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>But embedding is a really important use case!</p>\n</blockquote>\n<p>Sorry if it's a bit unclear, but I wasn't suggesting that we should not support embedding. Instead, I think it might be better to have a few specialized operations for embedding and avoid supporting hybrid tensors on all ops.</p>\n<blockquote>\n<p>And some wild speculation: I've been wondering if it makes sense to support a lazy sparse representation of the form \"Sparse Matrix + Constant\". So, for example, a matrix of all ones could be represented as \"Sparse Matrix with nnz=0 + 1\", an O(1) representation. The point is not that these are sparse (they're not, and many operations will blow up on them) but that sometimes the dense term will cancel out and you'll be back to a sparse operation, profitably.</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38509346\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/weiyangfb\">@weiyangfb</a> and I actually also discussed this possibility, but it seemed like a bad idea at the time. But I can't remember exactly why we scrapped it.</p>", "body_text": "But embedding is a really important use case!\n\nSorry if it's a bit unclear, but I wasn't suggesting that we should not support embedding. Instead, I think it might be better to have a few specialized operations for embedding and avoid supporting hybrid tensors on all ops.\n\nAnd some wild speculation: I've been wondering if it makes sense to support a lazy sparse representation of the form \"Sparse Matrix + Constant\". So, for example, a matrix of all ones could be represented as \"Sparse Matrix with nnz=0 + 1\", an O(1) representation. The point is not that these are sparse (they're not, and many operations will blow up on them) but that sometimes the dense term will cancel out and you'll be back to a sparse operation, profitably.\n\n@weiyangfb and I actually also discussed this possibility, but it seemed like a bad idea at the time. But I can't remember exactly why we scrapped it.", "body": "> But embedding is a really important use case!\r\n\r\nSorry if it's a bit unclear, but I wasn't suggesting that we should not support embedding. Instead, I think it might be better to have a few specialized operations for embedding and avoid supporting hybrid tensors on all ops.\r\n\r\n> And some wild speculation: I've been wondering if it makes sense to support a lazy sparse representation of the form \"Sparse Matrix + Constant\". So, for example, a matrix of all ones could be represented as \"Sparse Matrix with nnz=0 + 1\", an O(1) representation. The point is not that these are sparse (they're not, and many operations will blow up on them) but that sometimes the dense term will cancel out and you'll be back to a sparse operation, profitably.\r\n\r\n@weiyangfb and I actually also discussed this possibility, but it seemed like a bad idea at the time. But I can't remember exactly why we scrapped it."}