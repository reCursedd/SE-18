{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/407202557", "html_url": "https://github.com/pytorch/pytorch/issues/9674#issuecomment-407202557", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9674", "id": 407202557, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzIwMjU1Nw==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-23T21:11:27Z", "updated_at": "2018-07-23T22:04:16Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>And some wild speculation: I've been wondering if it makes sense to support a lazy sparse representation of the form \"Sparse Matrix + Constant\". So, for example, a matrix of all ones could be represented as \"Sparse Matrix with nnz=0 + 1\", an O(1) representation. The point is not that these are sparse (they're not, and many operations will blow up on them) but that sometimes the dense term will cancel out and you'll be back to a sparse operation, profitably.</p>\n</blockquote>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38509346\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/weiyangfb\">@weiyangfb</a> and I actually also discussed this possibility, but it seemed like a bad idea at the time. But I can't remember exactly why we scrapped it.</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8813817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/li-roy\">@li-roy</a> I don't recall that either... but now it looks like <code>Sparse tensor + Constant</code> can be a good representation and allow generic <code>add()</code> for sparse as well. In general, this works well on all pointwise one tensor ops. Ops will get slower for pointwise two tensor and BLAS, but it uses much less memory in exchange.</p>\n<p>EDIT: output from pointwise two tensor and BLAS ops should be dense tensor only</p>", "body_text": "And some wild speculation: I've been wondering if it makes sense to support a lazy sparse representation of the form \"Sparse Matrix + Constant\". So, for example, a matrix of all ones could be represented as \"Sparse Matrix with nnz=0 + 1\", an O(1) representation. The point is not that these are sparse (they're not, and many operations will blow up on them) but that sometimes the dense term will cancel out and you'll be back to a sparse operation, profitably.\n\n\n@weiyangfb and I actually also discussed this possibility, but it seemed like a bad idea at the time. But I can't remember exactly why we scrapped it.\n\n@li-roy I don't recall that either... but now it looks like Sparse tensor + Constant can be a good representation and allow generic add() for sparse as well. In general, this works well on all pointwise one tensor ops. Ops will get slower for pointwise two tensor and BLAS, but it uses much less memory in exchange.\nEDIT: output from pointwise two tensor and BLAS ops should be dense tensor only", "body": "> And some wild speculation: I've been wondering if it makes sense to support a lazy sparse representation of the form \"Sparse Matrix + Constant\". So, for example, a matrix of all ones could be represented as \"Sparse Matrix with nnz=0 + 1\", an O(1) representation. The point is not that these are sparse (they're not, and many operations will blow up on them) but that sometimes the dense term will cancel out and you'll be back to a sparse operation, profitably.\r\n\r\n> @weiyangfb and I actually also discussed this possibility, but it seemed like a bad idea at the time. But I can't remember exactly why we scrapped it.\r\n\r\n\r\n@li-roy I don't recall that either... but now it looks like `Sparse tensor + Constant` can be a good representation and allow generic `add()` for sparse as well. In general, this works well on all pointwise one tensor ops. Ops will get slower for pointwise two tensor and BLAS, but it uses much less memory in exchange.\r\n\r\nEDIT: output from pointwise two tensor and BLAS ops should be dense tensor only"}