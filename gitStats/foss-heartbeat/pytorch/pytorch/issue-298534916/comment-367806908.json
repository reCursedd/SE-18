{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367806908", "html_url": "https://github.com/pytorch/pytorch/issues/5306#issuecomment-367806908", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5306", "id": 367806908, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzgwNjkwOA==", "user": {"login": "DavidNemeskey", "id": 690386, "node_id": "MDQ6VXNlcjY5MDM4Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/690386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidNemeskey", "html_url": "https://github.com/DavidNemeskey", "followers_url": "https://api.github.com/users/DavidNemeskey/followers", "following_url": "https://api.github.com/users/DavidNemeskey/following{/other_user}", "gists_url": "https://api.github.com/users/DavidNemeskey/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidNemeskey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidNemeskey/subscriptions", "organizations_url": "https://api.github.com/users/DavidNemeskey/orgs", "repos_url": "https://api.github.com/users/DavidNemeskey/repos", "events_url": "https://api.github.com/users/DavidNemeskey/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidNemeskey/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-22T20:11:36Z", "updated_at": "2018-02-22T21:38:47Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> I'll try to answer them. :)</p>\n<ol>\n<li>\n<p>I have no idea why this is the case. I am wondering if it has to do with the random seed not working somehow (or maybe the code also needs <code>np.random.seed</code>) -- even though I load the parameters from file, so there shouldn't be anything random left in it... But I can confirm that the phenomenon exists. Maybe ask on the TensorFlow forums (do they have one?).</p>\n</li>\n<li>\n<p>The three PPLs for the three seeds that you show in your comment are very different from one another, so I wouldn't wonder if the difference between our numbers came down to the GPU and library versions used (CUDA, cuDNN, pytorch, g++, etc.)</p>\n</li>\n<li>\n<p>Senility, I guess. But I do accept pull requests. :)</p>\n</li>\n<li>\n<p>No, it isn't, and it might actually be why this is happening. So let me check that out quickly...</p>\n</li>\n<li>\n<p>The \"small\" model has no Dropout.</p>\n</li>\n<li>\n<p>That's why I needed to write my own <code>Loss</code> class... the <a href=\"https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\">TF implementation</a> sums along the time dimension.</p>\n</li>\n</ol>", "body_text": "@ezyang I'll try to answer them. :)\n\n\nI have no idea why this is the case. I am wondering if it has to do with the random seed not working somehow (or maybe the code also needs np.random.seed) -- even though I load the parameters from file, so there shouldn't be anything random left in it... But I can confirm that the phenomenon exists. Maybe ask on the TensorFlow forums (do they have one?).\n\n\nThe three PPLs for the three seeds that you show in your comment are very different from one another, so I wouldn't wonder if the difference between our numbers came down to the GPU and library versions used (CUDA, cuDNN, pytorch, g++, etc.)\n\n\nSenility, I guess. But I do accept pull requests. :)\n\n\nNo, it isn't, and it might actually be why this is happening. So let me check that out quickly...\n\n\nThe \"small\" model has no Dropout.\n\n\nThat's why I needed to write my own Loss class... the TF implementation sums along the time dimension.", "body": "@ezyang I'll try to answer them. :)\r\n\r\n1. I have no idea why this is the case. I am wondering if it has to do with the random seed not working somehow (or maybe the code also needs `np.random.seed`) -- even though I load the parameters from file, so there shouldn't be anything random left in it... But I can confirm that the phenomenon exists. Maybe ask on the TensorFlow forums (do they have one?).\r\n\r\n2. The three PPLs for the three seeds that you show in your comment are very different from one another, so I wouldn't wonder if the difference between our numbers came down to the GPU and library versions used (CUDA, cuDNN, pytorch, g++, etc.)\r\n\r\n3. Senility, I guess. But I do accept pull requests. :)\r\n\r\n4. No, it isn't, and it might actually be why this is happening. So let me check that out quickly...\r\n\r\n5. The \"small\" model has no Dropout.\r\n\r\n6. That's why I needed to write my own `Loss` class... the [TF implementation](https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py) sums along the time dimension."}