{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5306", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5306/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5306/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5306/events", "html_url": "https://github.com/pytorch/pytorch/issues/5306", "id": 298534916, "node_id": "MDU6SXNzdWUyOTg1MzQ5MTY=", "number": 5306, "title": "Numerical instability in (LSTM) training", "user": {"login": "DavidNemeskey", "id": 690386, "node_id": "MDQ6VXNlcjY5MDM4Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/690386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidNemeskey", "html_url": "https://github.com/DavidNemeskey", "followers_url": "https://api.github.com/users/DavidNemeskey/followers", "following_url": "https://api.github.com/users/DavidNemeskey/following{/other_user}", "gists_url": "https://api.github.com/users/DavidNemeskey/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidNemeskey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidNemeskey/subscriptions", "organizations_url": "https://api.github.com/users/DavidNemeskey/orgs", "repos_url": "https://api.github.com/users/DavidNemeskey/repos", "events_url": "https://api.github.com/users/DavidNemeskey/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidNemeskey/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}, {"id": 679954866, "node_id": "MDU6TGFiZWw2Nzk5NTQ4NjY=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/numerical-stability", "name": "numerical-stability", "color": "d4c5f9", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-02-20T10:06:54Z", "updated_at": "2018-02-22T22:26:20Z", "closed_at": "2018-02-22T22:26:17Z", "author_association": "NONE", "body_html": "<p>I was trying to reproduce the numbers in the language modeling paper Zaremba et al. (2014), when I ran into some problems. I re-implemented LSTM by hand in both Pytorch and TensorFlow, and while the TF version converged, the PT version did not. I have created a <a href=\"https://github.com/DavidNemeskey/debug_pytorch_lm\">repository</a> to document my findings; please read the <a href=\"https://github.com/DavidNemeskey/debug_pytorch_lm/blob/master/README.md\">readme</a> there for all the details, as well as for information on how to reproduce the problem.</p>\n<p>In a nutshell, the signs point to a <em>numerically instability</em> somewhere in PT. With the learning rate(s) of the TF version, losses quickly become huge, and the curve of the learning rate -&gt; perplexity graph is heavily serrated. I don't think that the fault lies with the LSTM, but it is possible that the persistent state exacerbates the problem.</p>\n<p>The issue can be reproduced on both CPU and GPU, with both float and double tensors.</p>\n<p>When submitting a bug report, please include the following information (where relevant):</p>\n<ul>\n<li>OS: Linux XXX 3.16.0-4-amd64 <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"171281708\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/1/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/1\">#1</a> SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux</li>\n<li>PyTorch version: 0.4.0a0 (but 3 as well)</li>\n<li>How you installed PyTorch (conda, pip, source): source</li>\n<li>Python version: 3.6.4</li>\n<li>CUDA/cuDNN version: 8 / 6</li>\n<li>GPU models and configuration: GeForce GTX 980</li>\n<li>GCC version (if compiling from source): Debian 4.9.2-10+deb8u1</li>\n</ul>", "body_text": "I was trying to reproduce the numbers in the language modeling paper Zaremba et al. (2014), when I ran into some problems. I re-implemented LSTM by hand in both Pytorch and TensorFlow, and while the TF version converged, the PT version did not. I have created a repository to document my findings; please read the readme there for all the details, as well as for information on how to reproduce the problem.\nIn a nutshell, the signs point to a numerically instability somewhere in PT. With the learning rate(s) of the TF version, losses quickly become huge, and the curve of the learning rate -> perplexity graph is heavily serrated. I don't think that the fault lies with the LSTM, but it is possible that the persistent state exacerbates the problem.\nThe issue can be reproduced on both CPU and GPU, with both float and double tensors.\nWhen submitting a bug report, please include the following information (where relevant):\n\nOS: Linux XXX 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux\nPyTorch version: 0.4.0a0 (but 3 as well)\nHow you installed PyTorch (conda, pip, source): source\nPython version: 3.6.4\nCUDA/cuDNN version: 8 / 6\nGPU models and configuration: GeForce GTX 980\nGCC version (if compiling from source): Debian 4.9.2-10+deb8u1", "body": "I was trying to reproduce the numbers in the language modeling paper Zaremba et al. (2014), when I ran into some problems. I re-implemented LSTM by hand in both Pytorch and TensorFlow, and while the TF version converged, the PT version did not. I have created a [repository](https://github.com/DavidNemeskey/debug_pytorch_lm) to document my findings; please read the [readme](https://github.com/DavidNemeskey/debug_pytorch_lm/blob/master/README.md) there for all the details, as well as for information on how to reproduce the problem.\r\n\r\nIn a nutshell, the signs point to a _numerically instability_ somewhere in PT. With the learning rate(s) of the TF version, losses quickly become huge, and the curve of the learning rate -> perplexity graph is heavily serrated. I don't think that the fault lies with the LSTM, but it is possible that the persistent state exacerbates the problem.\r\n\r\nThe issue can be reproduced on both CPU and GPU, with both float and double tensors.\r\n\r\nWhen submitting a bug report, please include the following information (where relevant):\r\n- OS: Linux XXX 3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19) x86_64 GNU/Linux\r\n- PyTorch version: 0.4.0a0 (but 3 as well)\r\n- How you installed PyTorch (conda, pip, source): source\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: 8 / 6\r\n- GPU models and configuration: GeForce GTX 980\r\n- GCC version (if compiling from source): Debian 4.9.2-10+deb8u1"}