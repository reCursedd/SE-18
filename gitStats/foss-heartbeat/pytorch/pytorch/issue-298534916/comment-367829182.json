{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367829182", "html_url": "https://github.com/pytorch/pytorch/issues/5306#issuecomment-367829182", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5306", "id": 367829182, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzgyOTE4Mg==", "user": {"login": "DavidNemeskey", "id": 690386, "node_id": "MDQ6VXNlcjY5MDM4Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/690386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidNemeskey", "html_url": "https://github.com/DavidNemeskey", "followers_url": "https://api.github.com/users/DavidNemeskey/followers", "following_url": "https://api.github.com/users/DavidNemeskey/following{/other_user}", "gists_url": "https://api.github.com/users/DavidNemeskey/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidNemeskey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidNemeskey/subscriptions", "organizations_url": "https://api.github.com/users/DavidNemeskey/orgs", "repos_url": "https://api.github.com/users/DavidNemeskey/repos", "events_url": "https://api.github.com/users/DavidNemeskey/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidNemeskey/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-22T21:32:23Z", "updated_at": "2018-02-22T21:32:23Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> The formula is correct -- it returns the same loss as TF (at least for the first few iterations...). The difference between it and the loss used in <code>word_language_model</code> is that normally, BPTT loss is summed along time steps (like in TF / my <code>SequenceLoss</code>), while the latter averages along both axes. So basically, <code>SequenceLoss == CrossEntropyLoss * num_steps</code>. I checked. :)</p>\n<p>As for not being able to see if the parameters were loaded correctly -- just run the scripts with <code>-T 1</code>...</p>", "body_text": "@ezyang The formula is correct -- it returns the same loss as TF (at least for the first few iterations...). The difference between it and the loss used in word_language_model is that normally, BPTT loss is summed along time steps (like in TF / my SequenceLoss), while the latter averages along both axes. So basically, SequenceLoss == CrossEntropyLoss * num_steps. I checked. :)\nAs for not being able to see if the parameters were loaded correctly -- just run the scripts with -T 1...", "body": "@ezyang The formula is correct -- it returns the same loss as TF (at least for the first few iterations...). The difference between it and the loss used in `word_language_model` is that normally, BPTT loss is summed along time steps (like in TF / my `SequenceLoss`), while the latter averages along both axes. So basically, `SequenceLoss == CrossEntropyLoss * num_steps`. I checked. :)\r\n\r\nAs for not being able to see if the parameters were loaded correctly -- just run the scripts with `-T 1`..."}