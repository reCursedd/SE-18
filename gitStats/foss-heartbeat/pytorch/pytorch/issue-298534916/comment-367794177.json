{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367794177", "html_url": "https://github.com/pytorch/pytorch/issues/5306#issuecomment-367794177", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5306", "id": 367794177, "node_id": "MDEyOklzc3VlQ29tbWVudDM2Nzc5NDE3Nw==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-22T19:32:47Z", "updated_at": "2018-02-22T20:56:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=690386\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/DavidNemeskey\">@DavidNemeskey</a>, I have a few questions about the script you uploaded.</p>\n<ol>\n<li>The TensorFlow script does not appear to be deterministic.</li>\n</ol>\n<pre><code>RUN 1:\nLoaded parameters from params.npz\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 27.34 | loss  6.89 | ppl   984.01\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 23.61 | loss  6.26 | ppl   522.35\n| epoch   1 |   600/ 2323 batches | lr 1.00 | ms/batch 34.92 | loss  5.94 | ppl   379.29\n\nRUN 2:\nLoaded parameters from params.npz\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 28.18 | loss  6.89 | ppl   983.19\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 23.94 | loss  6.27 | ppl   527.35\n</code></pre>\n<p>Do you know what is causing this nondeterminism? This is on TensorFlow 1.4 with GPU support. The PyTorch script, though divergent, is deterministic.</p>\n<pre><code>RUN 1:\nLoaded parameters from params.npz\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 105.15 | loss 19.87 | ppl 427251983.10\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 97.40 | loss 19.27 | ppl 233238198.57\n| epoch   1 |   600/ 2323 batches | lr 1.00 | ms/batch 99.69 | loss 22.62 | ppl 6660614197.29\n| epoch   1 |   800/ 2323 batches | lr 1.00 | ms/batch 99.81 | loss 22.40 | ppl 5335789184.99\n| epoch   1 |  1000/ 2323 batches | lr 1.00 | ms/batch 106.92 | loss 22.22 | ppl 4445882129.82\n| epoch   1 |  1200/ 2323 batches | lr 1.00 | ms/batch 114.48 | loss 20.94 | ppl 1243664530.65\n\nRUN 2:\nLoaded parameters from params.npz\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 111.11 | loss 19.87 | ppl 427251983.10\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 111.10 | loss 19.27 | ppl 233238198.57\n| epoch   1 |   600/ 2323 batches | lr 1.00 | ms/batch 105.96 | loss 22.62 | ppl 6660614197.29\n| epoch   1 |   800/ 2323 batches | lr 1.00 | ms/batch 108.41 | loss 22.40 | ppl 5335789184.99\n| epoch   1 |  1000/ 2323 batches | lr 1.00 | ms/batch 108.93 | loss 22.22 | ppl 4445882129.82\n</code></pre>\n<p>EDIT: I also confirmed that TF is not deterministic on CPU either.</p>\n<ol start=\"2\">\n<li>As you can see above, when I run the PyTorch script, I get perplexities that are far greater than those recorded in your experiments. This result seems to be robust even when I reinitialize the parameters (by omitting the <code>-L</code> parameter and passing a different <code>--seed</code>).</li>\n</ol>\n<pre><code>(/home/ezyang/Dev/pytorch-env) [ezyang@devgpu005.ash6 ~/Dev/oss/debug_pytorch_lm] python scripts/zaremba_pytorch.py -c -d ptb --seed 0\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 111.69 | loss 206.33 | ppl 405118226085607054480875801125402120406150439251323597652139845617320125818060519996653568.00\n(/home/ezyang/Dev/pytorch-env) [ezyang@devgpu005.ash6 ~/Dev/oss/debug_pytorch_lm] python scripts/zaremba_pytorch.py -c -d ptb --seed 1\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 110.40 | loss 45.12 | ppl 39377511868908847104.00\n</code></pre>\n<p>Did you see this when running PyTorch? FWIW my commit hash is roughly equivalent to <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/031412a14b7c20e9030b00c26f30c5524a0eb028/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/031412a14b7c20e9030b00c26f30c5524a0eb028\"><tt>031412a</tt></a></p>\n<p>EDIT: I reproduced this on your reported PyTorch version 0.4.0a0+c65bd66</p>\n<p>I realize that the exact details of which dataset is being used may matter. I used your recommended link <a href=\"http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\" rel=\"nofollow\">http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz</a> to acquire</p>\n<pre><code>./simple-examples/\n./simple-examples/data/\n./simple-examples/data/ptb.test.txt\n./simple-examples/data/ptb.train.txt\n./simple-examples/data/ptb.valid.txt\n./simple-examples/data/README\n./simple-examples/data/ptb.char.train.txt\n./simple-examples/data/ptb.char.test.txt\n./simple-examples/data/ptb.char.valid.txt\n</code></pre>\n<p>I have tried your example on both the word-wise and character-wise data and had blow up in both cases. Zaremba states that they did training on words, which is the first thing I tried.</p>\n<ol start=\"3\">\n<li>In your TF script, you have written LSTM in the following way:</li>\n</ol>\n<pre><code>        i = tf.sigmoid(ifgo[:, :self.hidden_size])\n        f = tf.sigmoid(ifgo[:, self.hidden_size:2*self.hidden_size])\n        g = tf.tanh(ifgo[:, 2*self.hidden_size:3*self.hidden_size])\n        o = tf.sigmoid(ifgo[:, 3*self.hidden_size:])\n</code></pre>\n<p>Is there any particular reason you did it this way instead of using <code>tf.split</code>?</p>\n<p>Thanks!</p>\n<p>UPDATE:</p>\n<ol start=\"4\">\n<li>Your gradient clipping strategy between TF and PyTorch looks different.</li>\n</ol>\n<pre><code># PyTorch\n\n        for name, p in model.named_parameters():\n            p.grad.data.clamp_(-5.0, 5.0)\n            p.data.add_(-1 * lr, p.grad.data)\n\n# TF\n\n            if clip:\n                self.clipped_grads, _ = tf.clip_by_global_norm(self.grads, clip)\n</code></pre>\n<p>Is this intentional?</p>\n<ol start=\"5\">\n<li>\n<p>You reimplemented the LSTM layers by scratch, and in the reimplementation I don't see any reference to dropout. Is this intentional? Doesn't the Zaremba paper regularize with dropout? There are some comments where you try to turn on dropout by setting <code>module.train()</code>, but I don't see you using any modules which would actually apply dropout when in <code>train()</code> mode.</p>\n</li>\n<li>\n<p>I consulted the linked source code from the paper at <a href=\"https://github.com/wojzaremba/lstm/blob/master/main.lua\">https://github.com/wojzaremba/lstm/blob/master/main.lua</a> and in it, I don't see that loss is being summed across the time dimension; it is just calling stock NLLLoss on the function in question. Could you elaborate more on your statement, \"However, the proper BPTT loss is summed along the latter\"?</p>\n</li>\n</ol>", "body_text": "Hi @DavidNemeskey, I have a few questions about the script you uploaded.\n\nThe TensorFlow script does not appear to be deterministic.\n\nRUN 1:\nLoaded parameters from params.npz\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 27.34 | loss  6.89 | ppl   984.01\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 23.61 | loss  6.26 | ppl   522.35\n| epoch   1 |   600/ 2323 batches | lr 1.00 | ms/batch 34.92 | loss  5.94 | ppl   379.29\n\nRUN 2:\nLoaded parameters from params.npz\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 28.18 | loss  6.89 | ppl   983.19\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 23.94 | loss  6.27 | ppl   527.35\n\nDo you know what is causing this nondeterminism? This is on TensorFlow 1.4 with GPU support. The PyTorch script, though divergent, is deterministic.\nRUN 1:\nLoaded parameters from params.npz\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 105.15 | loss 19.87 | ppl 427251983.10\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 97.40 | loss 19.27 | ppl 233238198.57\n| epoch   1 |   600/ 2323 batches | lr 1.00 | ms/batch 99.69 | loss 22.62 | ppl 6660614197.29\n| epoch   1 |   800/ 2323 batches | lr 1.00 | ms/batch 99.81 | loss 22.40 | ppl 5335789184.99\n| epoch   1 |  1000/ 2323 batches | lr 1.00 | ms/batch 106.92 | loss 22.22 | ppl 4445882129.82\n| epoch   1 |  1200/ 2323 batches | lr 1.00 | ms/batch 114.48 | loss 20.94 | ppl 1243664530.65\n\nRUN 2:\nLoaded parameters from params.npz\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 111.11 | loss 19.87 | ppl 427251983.10\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 111.10 | loss 19.27 | ppl 233238198.57\n| epoch   1 |   600/ 2323 batches | lr 1.00 | ms/batch 105.96 | loss 22.62 | ppl 6660614197.29\n| epoch   1 |   800/ 2323 batches | lr 1.00 | ms/batch 108.41 | loss 22.40 | ppl 5335789184.99\n| epoch   1 |  1000/ 2323 batches | lr 1.00 | ms/batch 108.93 | loss 22.22 | ppl 4445882129.82\n\nEDIT: I also confirmed that TF is not deterministic on CPU either.\n\nAs you can see above, when I run the PyTorch script, I get perplexities that are far greater than those recorded in your experiments. This result seems to be robust even when I reinitialize the parameters (by omitting the -L parameter and passing a different --seed).\n\n(/home/ezyang/Dev/pytorch-env) [ezyang@devgpu005.ash6 ~/Dev/oss/debug_pytorch_lm] python scripts/zaremba_pytorch.py -c -d ptb --seed 0\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 111.69 | loss 206.33 | ppl 405118226085607054480875801125402120406150439251323597652139845617320125818060519996653568.00\n(/home/ezyang/Dev/pytorch-env) [ezyang@devgpu005.ash6 ~/Dev/oss/debug_pytorch_lm] python scripts/zaremba_pytorch.py -c -d ptb --seed 1\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 110.40 | loss 45.12 | ppl 39377511868908847104.00\n\nDid you see this when running PyTorch? FWIW my commit hash is roughly equivalent to 031412a\nEDIT: I reproduced this on your reported PyTorch version 0.4.0a0+c65bd66\nI realize that the exact details of which dataset is being used may matter. I used your recommended link http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz to acquire\n./simple-examples/\n./simple-examples/data/\n./simple-examples/data/ptb.test.txt\n./simple-examples/data/ptb.train.txt\n./simple-examples/data/ptb.valid.txt\n./simple-examples/data/README\n./simple-examples/data/ptb.char.train.txt\n./simple-examples/data/ptb.char.test.txt\n./simple-examples/data/ptb.char.valid.txt\n\nI have tried your example on both the word-wise and character-wise data and had blow up in both cases. Zaremba states that they did training on words, which is the first thing I tried.\n\nIn your TF script, you have written LSTM in the following way:\n\n        i = tf.sigmoid(ifgo[:, :self.hidden_size])\n        f = tf.sigmoid(ifgo[:, self.hidden_size:2*self.hidden_size])\n        g = tf.tanh(ifgo[:, 2*self.hidden_size:3*self.hidden_size])\n        o = tf.sigmoid(ifgo[:, 3*self.hidden_size:])\n\nIs there any particular reason you did it this way instead of using tf.split?\nThanks!\nUPDATE:\n\nYour gradient clipping strategy between TF and PyTorch looks different.\n\n# PyTorch\n\n        for name, p in model.named_parameters():\n            p.grad.data.clamp_(-5.0, 5.0)\n            p.data.add_(-1 * lr, p.grad.data)\n\n# TF\n\n            if clip:\n                self.clipped_grads, _ = tf.clip_by_global_norm(self.grads, clip)\n\nIs this intentional?\n\n\nYou reimplemented the LSTM layers by scratch, and in the reimplementation I don't see any reference to dropout. Is this intentional? Doesn't the Zaremba paper regularize with dropout? There are some comments where you try to turn on dropout by setting module.train(), but I don't see you using any modules which would actually apply dropout when in train() mode.\n\n\nI consulted the linked source code from the paper at https://github.com/wojzaremba/lstm/blob/master/main.lua and in it, I don't see that loss is being summed across the time dimension; it is just calling stock NLLLoss on the function in question. Could you elaborate more on your statement, \"However, the proper BPTT loss is summed along the latter\"?", "body": "Hi @DavidNemeskey, I have a few questions about the script you uploaded.\r\n\r\n1. The TensorFlow script does not appear to be deterministic.\r\n\r\n```\r\nRUN 1:\r\nLoaded parameters from params.npz\r\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 27.34 | loss  6.89 | ppl   984.01\r\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 23.61 | loss  6.26 | ppl   522.35\r\n| epoch   1 |   600/ 2323 batches | lr 1.00 | ms/batch 34.92 | loss  5.94 | ppl   379.29\r\n\r\nRUN 2:\r\nLoaded parameters from params.npz\r\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 28.18 | loss  6.89 | ppl   983.19\r\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 23.94 | loss  6.27 | ppl   527.35\r\n```\r\n\r\nDo you know what is causing this nondeterminism? This is on TensorFlow 1.4 with GPU support. The PyTorch script, though divergent, is deterministic.\r\n\r\n```\r\nRUN 1:\r\nLoaded parameters from params.npz\r\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 105.15 | loss 19.87 | ppl 427251983.10\r\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 97.40 | loss 19.27 | ppl 233238198.57\r\n| epoch   1 |   600/ 2323 batches | lr 1.00 | ms/batch 99.69 | loss 22.62 | ppl 6660614197.29\r\n| epoch   1 |   800/ 2323 batches | lr 1.00 | ms/batch 99.81 | loss 22.40 | ppl 5335789184.99\r\n| epoch   1 |  1000/ 2323 batches | lr 1.00 | ms/batch 106.92 | loss 22.22 | ppl 4445882129.82\r\n| epoch   1 |  1200/ 2323 batches | lr 1.00 | ms/batch 114.48 | loss 20.94 | ppl 1243664530.65\r\n\r\nRUN 2:\r\nLoaded parameters from params.npz\r\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 111.11 | loss 19.87 | ppl 427251983.10\r\n| epoch   1 |   400/ 2323 batches | lr 1.00 | ms/batch 111.10 | loss 19.27 | ppl 233238198.57\r\n| epoch   1 |   600/ 2323 batches | lr 1.00 | ms/batch 105.96 | loss 22.62 | ppl 6660614197.29\r\n| epoch   1 |   800/ 2323 batches | lr 1.00 | ms/batch 108.41 | loss 22.40 | ppl 5335789184.99\r\n| epoch   1 |  1000/ 2323 batches | lr 1.00 | ms/batch 108.93 | loss 22.22 | ppl 4445882129.82\r\n```\r\n\r\nEDIT: I also confirmed that TF is not deterministic on CPU either.\r\n\r\n2. As you can see above, when I run the PyTorch script, I get perplexities that are far greater than those recorded in your experiments. This result seems to be robust even when I reinitialize the parameters (by omitting the `-L` parameter and passing a different `--seed`).\r\n\r\n```\r\n(/home/ezyang/Dev/pytorch-env) [ezyang@devgpu005.ash6 ~/Dev/oss/debug_pytorch_lm] python scripts/zaremba_pytorch.py -c -d ptb --seed 0\r\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 111.69 | loss 206.33 | ppl 405118226085607054480875801125402120406150439251323597652139845617320125818060519996653568.00\r\n(/home/ezyang/Dev/pytorch-env) [ezyang@devgpu005.ash6 ~/Dev/oss/debug_pytorch_lm] python scripts/zaremba_pytorch.py -c -d ptb --seed 1\r\n| epoch   1 |   200/ 2323 batches | lr 1.00 | ms/batch 110.40 | loss 45.12 | ppl 39377511868908847104.00\r\n```\r\n\r\nDid you see this when running PyTorch? FWIW my commit hash is roughly equivalent to 031412a14b7c20e9030b00c26f30c5524a0eb028\r\n\r\nEDIT: I reproduced this on your reported PyTorch version 0.4.0a0+c65bd66\r\n\r\nI realize that the exact details of which dataset is being used may matter. I used your recommended link http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz to acquire \r\n\r\n```\r\n./simple-examples/\r\n./simple-examples/data/\r\n./simple-examples/data/ptb.test.txt\r\n./simple-examples/data/ptb.train.txt\r\n./simple-examples/data/ptb.valid.txt\r\n./simple-examples/data/README\r\n./simple-examples/data/ptb.char.train.txt\r\n./simple-examples/data/ptb.char.test.txt\r\n./simple-examples/data/ptb.char.valid.txt\r\n```\r\n\r\nI have tried your example on both the word-wise and character-wise data and had blow up in both cases. Zaremba states that they did training on words, which is the first thing I tried.\r\n\r\n3. In your TF script, you have written LSTM in the following way:\r\n\r\n```\r\n        i = tf.sigmoid(ifgo[:, :self.hidden_size])\r\n        f = tf.sigmoid(ifgo[:, self.hidden_size:2*self.hidden_size])\r\n        g = tf.tanh(ifgo[:, 2*self.hidden_size:3*self.hidden_size])\r\n        o = tf.sigmoid(ifgo[:, 3*self.hidden_size:])\r\n```\r\n\r\nIs there any particular reason you did it this way instead of using `tf.split`?\r\n\r\nThanks!\r\n\r\nUPDATE:\r\n\r\n4. Your gradient clipping strategy between TF and PyTorch looks different.\r\n\r\n```\r\n# PyTorch\r\n\r\n        for name, p in model.named_parameters():\r\n            p.grad.data.clamp_(-5.0, 5.0)\r\n            p.data.add_(-1 * lr, p.grad.data)\r\n\r\n# TF\r\n\r\n            if clip:\r\n                self.clipped_grads, _ = tf.clip_by_global_norm(self.grads, clip)\r\n```\r\n\r\nIs this intentional?\r\n\r\n5. You reimplemented the LSTM layers by scratch, and in the reimplementation I don't see any reference to dropout. Is this intentional? Doesn't the Zaremba paper regularize with dropout? There are some comments where you try to turn on dropout by setting `module.train()`, but I don't see you using any modules which would actually apply dropout when in `train()` mode.\r\n\r\n6. I consulted the linked source code from the paper at https://github.com/wojzaremba/lstm/blob/master/main.lua and in it, I don't see that loss is being summed across the time dimension; it is just calling stock NLLLoss on the function in question. Could you elaborate more on your statement, \"However, the proper BPTT loss is summed along the latter\"?"}