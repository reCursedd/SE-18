{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223124099", "pull_request_review_id": 162184123, "id": 223124099, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzEyNDA5OQ==", "diff_hunk": "@@ -207,7 +213,7 @@ __device__ void exclusiveBinaryPrefixScan(T* smem, bool in, T* out, T* carry, Bi\n   *out -= (T) in;\n \n   // The outgoing carry for all threads is the last warp's sum\n-  *carry = smem[(blockDim.x / SCAN_UTILS_WARP_SIZE) - 1];\n+  *carry = smem[THCCeilDiv<int>(blockDim.x, SCAN_UTILS_WARP_SIZE) - 1];", "path": "aten/src/THC/THCScanUtils.cuh", "position": 21, "original_position": 21, "commit_id": "b6f0860c8cb156c4c135ba531db1593e42842856", "original_commit_id": "b6f0860c8cb156c4c135ba531db1593e42842856", "user": {"login": "jithunnair-amd", "id": 37884920, "node_id": "MDQ6VXNlcjM3ODg0OTIw", "avatar_url": "https://avatars1.githubusercontent.com/u/37884920?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jithunnair-amd", "html_url": "https://github.com/jithunnair-amd", "followers_url": "https://api.github.com/users/jithunnair-amd/followers", "following_url": "https://api.github.com/users/jithunnair-amd/following{/other_user}", "gists_url": "https://api.github.com/users/jithunnair-amd/gists{/gist_id}", "starred_url": "https://api.github.com/users/jithunnair-amd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jithunnair-amd/subscriptions", "organizations_url": "https://api.github.com/users/jithunnair-amd/orgs", "repos_url": "https://api.github.com/users/jithunnair-amd/repos", "events_url": "https://api.github.com/users/jithunnair-amd/events{/privacy}", "received_events_url": "https://api.github.com/users/jithunnair-amd/received_events", "type": "User", "site_admin": false}, "body": "smem is passed in from the caller. In case of gatherTopK, yes, I think smem is big enough: https://github.com/pytorch/pytorch/blob/b6f0860c8cb156c4c135ba531db1593e42842856/aten/src/THC/THCTensorTopK.cuh#L377\r\n\r\nMore importantly, the current logic doesn't seem to follow the intent of the comment. The \"last warp\" will be in `ceil(blockDim.x,WARP_SIZE)`, as this is how the smem is populated by `inclusiveBinaryPrefixScan`:  https://github.com/pytorch/pytorch/blob/b6f0860c8cb156c4c135ba531db1593e42842856/aten/src/THC/THCScanUtils.cuh#L172", "created_at": "2018-10-05T20:00:06Z", "updated_at": "2018-11-23T15:52:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/12337#discussion_r223124099", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12337", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/223124099"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12337#discussion_r223124099"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12337"}}, "body_html": "<p>smem is passed in from the caller. In case of gatherTopK, yes, I think smem is big enough: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/b6f0860c8cb156c4c135ba531db1593e42842856/aten/src/THC/THCTensorTopK.cuh#L377\">pytorch/aten/src/THC/THCTensorTopK.cuh</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 377\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/b6f0860c8cb156c4c135ba531db1593e42842856\">b6f0860</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L377\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"377\"></td>\n          <td id=\"LC377\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> #<span class=\"pl-k\">ifdef</span> __HIP_PLATFORM_HCC__ </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>More importantly, the current logic doesn't seem to follow the intent of the comment. The \"last warp\" will be in <code>ceil(blockDim.x,WARP_SIZE)</code>, as this is how the smem is populated by <code>inclusiveBinaryPrefixScan</code>:  <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/b6f0860c8cb156c4c135ba531db1593e42842856/aten/src/THC/THCScanUtils.cuh#L172\">pytorch/aten/src/THC/THCScanUtils.cuh</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 172\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/b6f0860c8cb156c4c135ba531db1593e42842856\">b6f0860</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L172\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"172\"></td>\n          <td id=\"LC172\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">int</span> warp = <span class=\"pl-c1\">threadIdx</span>.<span class=\"pl-smi\">x</span> / SCAN_UTILS_WARP_SIZE; </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>", "body_text": "smem is passed in from the caller. In case of gatherTopK, yes, I think smem is big enough: \n  \n    \n      pytorch/aten/src/THC/THCTensorTopK.cuh\n    \n    \n         Line 377\n      in\n      b6f0860\n    \n    \n    \n    \n\n        \n          \n           #ifdef __HIP_PLATFORM_HCC__ \n        \n    \n  \n\n\nMore importantly, the current logic doesn't seem to follow the intent of the comment. The \"last warp\" will be in ceil(blockDim.x,WARP_SIZE), as this is how the smem is populated by inclusiveBinaryPrefixScan:  \n  \n    \n      pytorch/aten/src/THC/THCScanUtils.cuh\n    \n    \n         Line 172\n      in\n      b6f0860\n    \n    \n    \n    \n\n        \n          \n           int warp = threadIdx.x / SCAN_UTILS_WARP_SIZE;", "in_reply_to_id": 222845003}