{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5319", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5319/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5319/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5319/events", "html_url": "https://github.com/pytorch/pytorch/issues/5319", "id": 298768313, "node_id": "MDU6SXNzdWUyOTg3NjgzMTM=", "number": 5319, "title": "[feature request] Make weights of nn.Embedding a parameter", "user": {"login": "asoltysik", "id": 17353292, "node_id": "MDQ6VXNlcjE3MzUzMjky", "avatar_url": "https://avatars2.githubusercontent.com/u/17353292?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asoltysik", "html_url": "https://github.com/asoltysik", "followers_url": "https://api.github.com/users/asoltysik/followers", "following_url": "https://api.github.com/users/asoltysik/following{/other_user}", "gists_url": "https://api.github.com/users/asoltysik/gists{/gist_id}", "starred_url": "https://api.github.com/users/asoltysik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asoltysik/subscriptions", "organizations_url": "https://api.github.com/users/asoltysik/orgs", "repos_url": "https://api.github.com/users/asoltysik/repos", "events_url": "https://api.github.com/users/asoltysik/events{/privacy}", "received_events_url": "https://api.github.com/users/asoltysik/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-20T22:11:52Z", "updated_at": "2018-02-20T23:00:36Z", "closed_at": "2018-02-20T23:00:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Currently to load pretrained tensor into an Embedding I am running:</p>\n<div class=\"highlight highlight-source-python\"><pre>embedding <span class=\"pl-k\">=</span> torch.nn.Embedding(length, dim)\nembedding.weight.data <span class=\"pl-k\">=</span> pretrained_vectors_tensor</pre></div>\n<p>This first initializes a tensor of  size <code>length x dim</code> which can be quite large and then throws all of that away. When creating the embedding there are two big tensors in memory which fill up my RAM, while there could be only one if there was a possibility to initialize weights from a parameter.</p>", "body_text": "Currently to load pretrained tensor into an Embedding I am running:\nembedding = torch.nn.Embedding(length, dim)\nembedding.weight.data = pretrained_vectors_tensor\nThis first initializes a tensor of  size length x dim which can be quite large and then throws all of that away. When creating the embedding there are two big tensors in memory which fill up my RAM, while there could be only one if there was a possibility to initialize weights from a parameter.", "body": "Currently to load pretrained tensor into an Embedding I am running:\r\n``` python\r\nembedding = torch.nn.Embedding(length, dim)\r\nembedding.weight.data = pretrained_vectors_tensor\r\n```\r\nThis first initializes a tensor of  size `length x dim` which can be quite large and then throws all of that away. When creating the embedding there are two big tensors in memory which fill up my RAM, while there could be only one if there was a possibility to initialize weights from a parameter."}