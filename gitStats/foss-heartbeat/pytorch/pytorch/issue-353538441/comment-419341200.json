{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/419341200", "html_url": "https://github.com/pytorch/pytorch/issues/10832#issuecomment-419341200", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/10832", "id": 419341200, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTM0MTIwMA==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-07T06:51:52Z", "updated_at": "2018-09-12T17:58:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So I just researched a bit of CUDA context. <a href=\"https://devtalk.nvidia.com/default/topic/519087/cuda-programming-and-performance/cuda-context-and-threading/post/3689477/#3689477\" rel=\"nofollow\">This post</a> and <a href=\"https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DRIVER.html\" rel=\"nofollow\">this doc page</a> are fairly good reads on device API vs runtime API on managing contexts.</p>\n<p>TL;DR:</p>\n<ul>\n<li>Each thread can have its own context.</li>\n<li>A context is specific to one device, but can be shared among multiple threads.</li>\n<li>The runtime API is mostly what we use in PyTorch, e.g., <code>cudaMalloc</code>, etc. Runtime API uses a hidden API to create what's called a primary context. Primary contexts are the same as any other contexts, except that there can be only one for a device at a time. It only creates such primary contexts, if there is no context in the thread's TLS context slot.</li>\n</ul>\n<p>So, we should only have primary contexts in PyTorch. I think this method <a href=\"https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__PRIMARY__CTX.html#group__CUDA__PRIMARY__CTX_1g65f3e018721b6d90aa05cfb56250f469\" rel=\"nofollow\"><code>cuDevicePrimaryCtxGetState</code></a> can be used to query whether a primary context is active or not.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> added that we can call it via <code>ctypes</code> when testing.</p>", "body_text": "So I just researched a bit of CUDA context. This post and this doc page are fairly good reads on device API vs runtime API on managing contexts.\nTL;DR:\n\nEach thread can have its own context.\nA context is specific to one device, but can be shared among multiple threads.\nThe runtime API is mostly what we use in PyTorch, e.g., cudaMalloc, etc. Runtime API uses a hidden API to create what's called a primary context. Primary contexts are the same as any other contexts, except that there can be only one for a device at a time. It only creates such primary contexts, if there is no context in the thread's TLS context slot.\n\nSo, we should only have primary contexts in PyTorch. I think this method cuDevicePrimaryCtxGetState can be used to query whether a primary context is active or not.\n@soumith added that we can call it via ctypes when testing.", "body": "So I just researched a bit of CUDA context. [This post](https://devtalk.nvidia.com/default/topic/519087/cuda-programming-and-performance/cuda-context-and-threading/post/3689477/#3689477) and [this doc page](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DRIVER.html) are fairly good reads on device API vs runtime API on managing contexts.\r\n\r\nTL;DR: \r\n+ Each thread can have its own context. \r\n+ A context is specific to one device, but can be shared among multiple threads.\r\n+ The runtime API is mostly what we use in PyTorch, e.g., `cudaMalloc`, etc. Runtime API uses a hidden API to create what's called a primary context. Primary contexts are the same as any other contexts, except that there can be only one for a device at a time. It only creates such primary contexts, if there is no context in the thread's TLS context slot.\r\n \r\nSo, we should only have primary contexts in PyTorch. I think this method [`cuDevicePrimaryCtxGetState`](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__PRIMARY__CTX.html#group__CUDA__PRIMARY__CTX_1g65f3e018721b6d90aa05cfb56250f469) can be used to query whether a primary context is active or not.\r\n\r\n@soumith added that we can call it via `ctypes` when testing.\r\n"}