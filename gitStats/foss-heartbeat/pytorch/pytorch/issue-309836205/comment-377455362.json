{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/377455362", "html_url": "https://github.com/pytorch/pytorch/pull/6110#issuecomment-377455362", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6110", "id": 377455362, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NzQ1NTM2Mg==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-30T06:18:09Z", "updated_at": "2018-03-30T06:18:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So the simplest python implementation that supports arbitrary \"batch-like\" dimensions is</p>\n<pre><code>def slow_bilinear(left, right, weight, bias=None):\n    res = torch.matmul(torch.matmul(left.unsqueeze(-2).unsqueeze(-2), weight), right.unsqueeze(-2).unsqueeze(-1)).squeeze(-1).squeeze(-1)\n    if bias is not None:\n        return res + bias\n    return res\n</code></pre>\n<p>similar to the C++ version currently in master, it creates too large a matrix.</p>\n<p>A version with only one batch-like dimension but with easy to figure out derivatives is</p>\n<pre><code>def slow_bilinear(left, right, weight, bias):\n    left =  left[:,None,:,None]\n    right = right[:, None, None, :]\n    weight = weight[None, :,:,:]\n    return (left*weight*right).sum(-1).sum(-1)+bias\ndef slow_bilinear_derivative(grad_out, left, right, weight, bias):\n    # left = bxn; right = bxm; A = kxnxm; out = bxk\n    # b k n m \n    grad_out  = grad_out[:, :, None, None]\n    left =  left[:,None,:,None]\n    right = right[:, None, None, :]\n    weight = weight[None, :,:,:]\n    \n    grad_left   = (grad_out*weight*right).sum(1).sum(-1)\n    grad_right  = (grad_out*weight*left ).sum(1).sum(-2)\n    grad_weight = (grad_out*left*right  ).sum(0)\n    grad_bias   = grad_out.sum(0)\n    return grad_left, grad_right, grad_weight, grad_bias\ndef slow_bilinear_second_derivative(grad_left, grad_right, grad_weight, grad_bias, grad_out, left, right, weight, bias):\n    # b k n m \n    grad_out  = grad_out[:, :, None, None]\n    left   = left[:,None,:,None]\n    right  = right[:, None, None, :]\n    weight = weight[None, :,:,:]\n    grad_left   = grad_left[:,None,:,None]\n    grad_right  = grad_right[:, None, None, :]\n    grad_weight = grad_weight[None, :,:,:]\n    \n    gg_left   = (grad_out*weight*grad_right+grad_out*grad_weight*right).sum(1).sum(-1)\n    gg_right  = (grad_out*weight*grad_left +grad_out*grad_weight*left ).sum(1).sum(-2)\n    gg_weight = (grad_out*left*grad_right  +grad_out*grad_left*right ).sum(0)\n    gg_bias   = torch.zeros_like(bias)\n    gg_out    = (grad_left*weight*right+ \n                 grad_right*weight*left+\n                 grad_weight*left*right).sum(-1).sum(-1)+grad_bias[None,:]\n    return gg_out,gg_left, gg_right,gg_weight, gg_bias\n</code></pre>\n<p>Best regards</p>\n<p>Thomas</p>", "body_text": "So the simplest python implementation that supports arbitrary \"batch-like\" dimensions is\ndef slow_bilinear(left, right, weight, bias=None):\n    res = torch.matmul(torch.matmul(left.unsqueeze(-2).unsqueeze(-2), weight), right.unsqueeze(-2).unsqueeze(-1)).squeeze(-1).squeeze(-1)\n    if bias is not None:\n        return res + bias\n    return res\n\nsimilar to the C++ version currently in master, it creates too large a matrix.\nA version with only one batch-like dimension but with easy to figure out derivatives is\ndef slow_bilinear(left, right, weight, bias):\n    left =  left[:,None,:,None]\n    right = right[:, None, None, :]\n    weight = weight[None, :,:,:]\n    return (left*weight*right).sum(-1).sum(-1)+bias\ndef slow_bilinear_derivative(grad_out, left, right, weight, bias):\n    # left = bxn; right = bxm; A = kxnxm; out = bxk\n    # b k n m \n    grad_out  = grad_out[:, :, None, None]\n    left =  left[:,None,:,None]\n    right = right[:, None, None, :]\n    weight = weight[None, :,:,:]\n    \n    grad_left   = (grad_out*weight*right).sum(1).sum(-1)\n    grad_right  = (grad_out*weight*left ).sum(1).sum(-2)\n    grad_weight = (grad_out*left*right  ).sum(0)\n    grad_bias   = grad_out.sum(0)\n    return grad_left, grad_right, grad_weight, grad_bias\ndef slow_bilinear_second_derivative(grad_left, grad_right, grad_weight, grad_bias, grad_out, left, right, weight, bias):\n    # b k n m \n    grad_out  = grad_out[:, :, None, None]\n    left   = left[:,None,:,None]\n    right  = right[:, None, None, :]\n    weight = weight[None, :,:,:]\n    grad_left   = grad_left[:,None,:,None]\n    grad_right  = grad_right[:, None, None, :]\n    grad_weight = grad_weight[None, :,:,:]\n    \n    gg_left   = (grad_out*weight*grad_right+grad_out*grad_weight*right).sum(1).sum(-1)\n    gg_right  = (grad_out*weight*grad_left +grad_out*grad_weight*left ).sum(1).sum(-2)\n    gg_weight = (grad_out*left*grad_right  +grad_out*grad_left*right ).sum(0)\n    gg_bias   = torch.zeros_like(bias)\n    gg_out    = (grad_left*weight*right+ \n                 grad_right*weight*left+\n                 grad_weight*left*right).sum(-1).sum(-1)+grad_bias[None,:]\n    return gg_out,gg_left, gg_right,gg_weight, gg_bias\n\nBest regards\nThomas", "body": "So the simplest python implementation that supports arbitrary \"batch-like\" dimensions is\r\n\r\n```\r\ndef slow_bilinear(left, right, weight, bias=None):\r\n    res = torch.matmul(torch.matmul(left.unsqueeze(-2).unsqueeze(-2), weight), right.unsqueeze(-2).unsqueeze(-1)).squeeze(-1).squeeze(-1)\r\n    if bias is not None:\r\n        return res + bias\r\n    return res\r\n```\r\n\r\nsimilar to the C++ version currently in master, it creates too large a matrix.\r\n\r\nA version with only one batch-like dimension but with easy to figure out derivatives is\r\n\r\n```\r\ndef slow_bilinear(left, right, weight, bias):\r\n    left =  left[:,None,:,None]\r\n    right = right[:, None, None, :]\r\n    weight = weight[None, :,:,:]\r\n    return (left*weight*right).sum(-1).sum(-1)+bias\r\ndef slow_bilinear_derivative(grad_out, left, right, weight, bias):\r\n    # left = bxn; right = bxm; A = kxnxm; out = bxk\r\n    # b k n m \r\n    grad_out  = grad_out[:, :, None, None]\r\n    left =  left[:,None,:,None]\r\n    right = right[:, None, None, :]\r\n    weight = weight[None, :,:,:]\r\n    \r\n    grad_left   = (grad_out*weight*right).sum(1).sum(-1)\r\n    grad_right  = (grad_out*weight*left ).sum(1).sum(-2)\r\n    grad_weight = (grad_out*left*right  ).sum(0)\r\n    grad_bias   = grad_out.sum(0)\r\n    return grad_left, grad_right, grad_weight, grad_bias\r\ndef slow_bilinear_second_derivative(grad_left, grad_right, grad_weight, grad_bias, grad_out, left, right, weight, bias):\r\n    # b k n m \r\n    grad_out  = grad_out[:, :, None, None]\r\n    left   = left[:,None,:,None]\r\n    right  = right[:, None, None, :]\r\n    weight = weight[None, :,:,:]\r\n    grad_left   = grad_left[:,None,:,None]\r\n    grad_right  = grad_right[:, None, None, :]\r\n    grad_weight = grad_weight[None, :,:,:]\r\n    \r\n    gg_left   = (grad_out*weight*grad_right+grad_out*grad_weight*right).sum(1).sum(-1)\r\n    gg_right  = (grad_out*weight*grad_left +grad_out*grad_weight*left ).sum(1).sum(-2)\r\n    gg_weight = (grad_out*left*grad_right  +grad_out*grad_left*right ).sum(0)\r\n    gg_bias   = torch.zeros_like(bias)\r\n    gg_out    = (grad_left*weight*right+ \r\n                 grad_right*weight*left+\r\n                 grad_weight*left*right).sum(-1).sum(-1)+grad_bias[None,:]\r\n    return gg_out,gg_left, gg_right,gg_weight, gg_bias\r\n```\r\n\r\nBest regards\r\n\r\nThomas"}