{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178177706", "pull_request_review_id": 108199167, "id": 178177706, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3ODE3NzcwNg==", "diff_hunk": "@@ -22,15 +21,162 @@ Tensor bilinear(const Tensor& input1, const Tensor& input2, const Tensor& weight\n             \"bilinear(): bias size does not match weight size: got %lld but expected %lld\",\n             (long long)bias.size(0), (long long)weight.size(0));\n \n-  auto b_input1 = input1.unsqueeze(-2).unsqueeze(-2);\n-  auto b_input2 = input2.unsqueeze(-2).unsqueeze(-1);\n+  std::vector<int64_t> output_size;\n+  auto size1 = input1.sizes();\n+  output_size.insert(output_size.end(), size1.begin(), size1.end() - 1);\n+  output_size.push_back(weight.size(0));\n+\n+  auto output = input1.type().tensor(output_size);\n+  auto buf = input1.type().tensor(input2.sizes());\n \n-  auto output = at::matmul(at::matmul(b_input1, weight), b_input2);\n-  output = output.squeeze(-1).squeeze(-1);\n+  size_t output_features = weight.size(0);\n+  auto input1_flattened = input1.view({-1, input1.size(-1)});\n+  auto buf_flattened = buf.view({-1, buf.size(-1)});\n+  for (size_t k = 0; k < output_features; k++) {\n+    at::mm_out(buf_flattened, input1_flattened, weight[k]);\n+    buf.mul_(input2);\n+    auto output_col = output.narrow(-1, k, 1);\n+    sum_out(output_col, buf, -1, true);\n+  }\n   if (bias.defined()) {\n     output = output + bias;\n   }\n   return output;\n }\n \n+std::tuple<Tensor, Tensor, Tensor, Tensor> bilinear_backward(const Tensor& grad_out, const Tensor& input1, const Tensor& input2,\n+\t\t\t\t\t\t\t     const Tensor& weight, std::array<bool, 4> grad_mask)\n+{\n+  Tensor grad_input1, grad_input2, grad_weight, grad_bias;\n+\n+  size_t output_features = weight.size(0);\n+  auto input1_flattened = input1.view({-1, input1.size(-1)});\n+  auto input2_flattened = input2.view({-1, input2.size(-1)});\n+  auto grad_out_flattened = grad_out.view({-1, grad_out.size(-1)});\n+\n+  if (grad_mask[0]) {\n+    grad_input1 = at::mm(input2_flattened, weight[0].t());\n+    grad_input1.mul_(grad_out_flattened.narrow(1, 0, 1));\n+    for (size_t k = 1; k < output_features; k++) {\n+      auto buf = input2_flattened.mm(weight[k].t());\n+      buf.mul_(grad_out_flattened.narrow(1, k, 1));\n+      grad_input1 += buf;\n+    }\n+    grad_input1 = grad_input1.view_as(input1);\n+  }\n+  if (grad_mask[1]) {\n+    grad_input2 = at::mm(input1_flattened, weight[0]);\n+    grad_input2.mul_(grad_out_flattened.narrow(1, 0, 1));\n+    for (size_t k = 1; k < output_features; k++) {\n+      auto buf = input1_flattened.mm(weight[k]);\n+      buf.mul_(grad_out_flattened.narrow(1, k, 1));\n+      grad_input2 += buf;\n+    }\n+    grad_input2 = grad_input2.view_as(input2);\n+  }\n+  if (grad_mask[2]) {\n+    grad_weight = weight.type().tensor(weight.sizes());\n+    for (size_t k = 0; k < output_features; k++) {\n+      auto buf = input1_flattened.mul(grad_out_flattened.narrow(1, k, 1));\n+      auto weight_row = grad_weight[k];\n+      at::mm_out(weight_row, buf.t(), input2_flattened);\n+    }\n+  }\n+  if (grad_mask[3]) {\n+    grad_bias = grad_out_flattened.sum(0, false);\n+  }\n+  return std::tuple<Tensor, Tensor, Tensor, Tensor>(grad_input1, grad_input2, grad_weight, grad_bias);\n+}\n+\n+std::tuple<Tensor, Tensor, Tensor, Tensor> bilinear_double_backward(const Tensor& grad_out_grad_input1, const Tensor& grad_out_grad_input2,", "path": "aten/src/ATen/native/Linear.cpp", "position": null, "original_position": 83, "commit_id": "aa9bd2e8357c87a273fecf2bb57d2fa47a6100c7", "original_commit_id": "776bfff0ecb441b116311a8762141cadf41bffad", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "Just double backward. `derivatives.yaml` doesn't define backward for functions in `Functions.cpp`, so backward needs to be a native function. Because native functions generate a bunch of bindings we should put things in `Functions.cpp` when we don't need them or backward. :) ", "created_at": "2018-03-29T20:49:42Z", "updated_at": "2018-11-23T15:41:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/6110#discussion_r178177706", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6110", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178177706"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6110#discussion_r178177706"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6110"}}, "body_html": "<p>Just double backward. <code>derivatives.yaml</code> doesn't define backward for functions in <code>Functions.cpp</code>, so backward needs to be a native function. Because native functions generate a bunch of bindings we should put things in <code>Functions.cpp</code> when we don't need them or backward. :)</p>", "body_text": "Just double backward. derivatives.yaml doesn't define backward for functions in Functions.cpp, so backward needs to be a native function. Because native functions generate a bunch of bindings we should put things in Functions.cpp when we don't need them or backward. :)", "in_reply_to_id": 178132804}