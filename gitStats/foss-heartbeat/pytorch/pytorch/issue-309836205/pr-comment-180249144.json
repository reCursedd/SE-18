{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/180249144", "pull_request_review_id": 110638512, "id": 180249144, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MDI0OTE0NA==", "diff_hunk": "@@ -22,11 +172,13 @@ Tensor bilinear(const Tensor& input1, const Tensor& input2, const Tensor& weight\n             \"bilinear(): bias size does not match weight size: got %lld but expected %lld\",\n             (long long)bias.size(0), (long long)weight.size(0));\n \n-  auto b_input1 = input1.unsqueeze(-2).unsqueeze(-2);\n-  auto b_input2 = input2.unsqueeze(-2).unsqueeze(-1);\n-\n-  auto output = at::matmul(at::matmul(b_input1, weight), b_input2);\n-  output = output.squeeze(-1).squeeze(-1);\n+  std::vector<int64_t> output_size;\n+  auto size1 = input1.sizes();\n+  output_size.insert(output_size.end(), size1.begin(), size1.end() - 1);\n+  output_size.push_back(weight.size(0));\n+  auto input1_flattened = input1.view({-1, input1.size(-1)});\n+  auto input2_flattened = input2.view({-1, input2.size(-1)});\n+  Tensor output = at::_trilinear(input1_flattened, weight, input2_flattened, {1,3}, {0}, {1,2}, {2,3}).reshape(output_size);", "path": "aten/src/ATen/native/Linear.cpp", "position": 183, "original_position": 176, "commit_id": "aa9bd2e8357c87a273fecf2bb57d2fa47a6100c7", "original_commit_id": "385e4082cd35234c38e2d79923bfc24e57238bb1", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "`bilinear` is `torch.nn.bilinear`, i.e. `bilinear_ba = sum_n sum_m left_bn weight_anm right_bm`\r\n\r\n`_trilinear` is a generalisation in that you can specify arbitrary unsqueezing (or equivalently arbitrary indices in the factors above) and summation dimensions before multiplying the three factors. This is used to have a function that can express its own derivative in terms of itself in order to not have to define higher order derivatives explicitly. I would not know how to unify `bilinear` and `bilinear_backward` without `_trilinear`. The naming discrepancy bilinear vs. trilinear comes from viewing the weights as parameters (then it is bilinear in left and right) vs. inputs.\r\n\r\nsumproduct_pair computes an einsum-style reduction after dimensions have been aligned.\r\n\r\nI'll add comments in the Linear.cpp as well.", "created_at": "2018-04-09T22:26:27Z", "updated_at": "2018-11-23T15:42:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/6110#discussion_r180249144", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6110", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/180249144"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6110#discussion_r180249144"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6110"}}, "body_html": "<p><code>bilinear</code> is <code>torch.nn.bilinear</code>, i.e. <code>bilinear_ba = sum_n sum_m left_bn weight_anm right_bm</code></p>\n<p><code>_trilinear</code> is a generalisation in that you can specify arbitrary unsqueezing (or equivalently arbitrary indices in the factors above) and summation dimensions before multiplying the three factors. This is used to have a function that can express its own derivative in terms of itself in order to not have to define higher order derivatives explicitly. I would not know how to unify <code>bilinear</code> and <code>bilinear_backward</code> without <code>_trilinear</code>. The naming discrepancy bilinear vs. trilinear comes from viewing the weights as parameters (then it is bilinear in left and right) vs. inputs.</p>\n<p>sumproduct_pair computes an einsum-style reduction after dimensions have been aligned.</p>\n<p>I'll add comments in the Linear.cpp as well.</p>", "body_text": "bilinear is torch.nn.bilinear, i.e. bilinear_ba = sum_n sum_m left_bn weight_anm right_bm\n_trilinear is a generalisation in that you can specify arbitrary unsqueezing (or equivalently arbitrary indices in the factors above) and summation dimensions before multiplying the three factors. This is used to have a function that can express its own derivative in terms of itself in order to not have to define higher order derivatives explicitly. I would not know how to unify bilinear and bilinear_backward without _trilinear. The naming discrepancy bilinear vs. trilinear comes from viewing the weights as parameters (then it is bilinear in left and right) vs. inputs.\nsumproduct_pair computes an einsum-style reduction after dimensions have been aligned.\nI'll add comments in the Linear.cpp as well.", "in_reply_to_id": 180216611}