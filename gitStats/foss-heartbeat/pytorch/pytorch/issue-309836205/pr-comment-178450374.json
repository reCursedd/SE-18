{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178450374", "pull_request_review_id": 108511230, "id": 178450374, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3ODQ1MDM3NA==", "diff_hunk": "@@ -1164,6 +1164,97 @@ std::tuple<Tensor, Tensor, Tensor> batchnorm_double_backward(\n \n }\n \n+std::tuple<Tensor, Tensor, Tensor, Tensor> bilinear_double_backward(const Tensor& grad_out_grad_input1, const Tensor& grad_out_grad_input2,", "path": "tools/autograd/templates/Functions.cpp", "position": null, "original_position": 4, "commit_id": "aa9bd2e8357c87a273fecf2bb57d2fa47a6100c7", "original_commit_id": "1efadb2d9e4131180a4c331928c786e9ac50473a", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "body": "So based on Martin's comments on slack, the memory implication of using matrix multiplication are prohibitive.\r\n- So to avoid this and not use inplace operations one easy way could be computing the slices and use `cat` to construct the output.\r\n- A more elaborate way would be to figure out how to deal with the different summations generically. Then you are essentially producing a \"reduced einsum\" implementation. I'm looking into this for einsum (and would prefer to have exponential time instead of exponential space by using a cuda kernel with an arbitrary number of params).\r\n- the path of least resistance might be keeping it as done here and include a comment to use `einsum` in backward in the future to eliminate the double backward.\r\n\r\nFor context: The 0.3.1 version that Marcin posted does not do second derivatives, but works for use cases where the version currently in master does not work due to the excessive use of GPU memory and is slow. So if we mainly aim to keep 0.3.1 functionality, we could also drop double differentiation.\r\n\r\nI would be glad to follow your advice / choice here.\r\n\r\nBest regards\r\n\r\nThomas\r\n", "created_at": "2018-04-01T07:29:39Z", "updated_at": "2018-11-23T15:41:27Z", "html_url": "https://github.com/pytorch/pytorch/pull/6110#discussion_r178450374", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6110", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178450374"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6110#discussion_r178450374"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6110"}}, "body_html": "<p>So based on Martin's comments on slack, the memory implication of using matrix multiplication are prohibitive.</p>\n<ul>\n<li>So to avoid this and not use inplace operations one easy way could be computing the slices and use <code>cat</code> to construct the output.</li>\n<li>A more elaborate way would be to figure out how to deal with the different summations generically. Then you are essentially producing a \"reduced einsum\" implementation. I'm looking into this for einsum (and would prefer to have exponential time instead of exponential space by using a cuda kernel with an arbitrary number of params).</li>\n<li>the path of least resistance might be keeping it as done here and include a comment to use <code>einsum</code> in backward in the future to eliminate the double backward.</li>\n</ul>\n<p>For context: The 0.3.1 version that Marcin posted does not do second derivatives, but works for use cases where the version currently in master does not work due to the excessive use of GPU memory and is slow. So if we mainly aim to keep 0.3.1 functionality, we could also drop double differentiation.</p>\n<p>I would be glad to follow your advice / choice here.</p>\n<p>Best regards</p>\n<p>Thomas</p>", "body_text": "So based on Martin's comments on slack, the memory implication of using matrix multiplication are prohibitive.\n\nSo to avoid this and not use inplace operations one easy way could be computing the slices and use cat to construct the output.\nA more elaborate way would be to figure out how to deal with the different summations generically. Then you are essentially producing a \"reduced einsum\" implementation. I'm looking into this for einsum (and would prefer to have exponential time instead of exponential space by using a cuda kernel with an arbitrary number of params).\nthe path of least resistance might be keeping it as done here and include a comment to use einsum in backward in the future to eliminate the double backward.\n\nFor context: The 0.3.1 version that Marcin posted does not do second derivatives, but works for use cases where the version currently in master does not work due to the excessive use of GPU memory and is slow. So if we mainly aim to keep 0.3.1 functionality, we could also drop double differentiation.\nI would be glad to follow your advice / choice here.\nBest regards\nThomas", "in_reply_to_id": 178434819}