{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/230084292", "pull_request_review_id": 170739038, "id": 230084292, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMDA4NDI5Mg==", "diff_hunk": "@@ -305,23 +305,30 @@ bool runFusion(\n     return i.toTensor();\n   });\n \n-  // Determines device to dispatch to\n-  // Note assumes all inputs are on at most one GPU device and/or the CPU\n-  int32_t device = kCPUDevice;\n-  for (const auto& t : inputs) {\n-    const auto cur_device = t.device().index();\n-    if (cur_device < 0) continue;\n-    if (device < 0) device = cur_device;\n-    else JIT_ASSERT(device == cur_device);\n+  // Determines device to dispatch to. If there's a device mismatch in the inputs,\n+  // we use the fallback (which should give a nice error message).", "path": "torch/csrc/jit/fuser/executor.cpp", "position": 62, "original_position": 62, "commit_id": "2afe1461dfd48925dc98d0c7b741be8f868dcafb", "original_commit_id": "3867bbff9b3d5d8be56ded907547a3219ab536b1", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Well if you look at our current fusion groups, there are no operations that could cause cross-device transfers. So the subgraphs that we would fuse would error out if you substituted some inputs to be on different devices. Instead of checking those things statically, we do a dynamic check now, and use the fallback to give a nicer error message if we detect that inputs are on different devices.", "created_at": "2018-11-01T15:30:04Z", "updated_at": "2018-11-23T15:53:58Z", "html_url": "https://github.com/pytorch/pytorch/pull/13387#discussion_r230084292", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13387", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/230084292"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13387#discussion_r230084292"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13387"}}, "body_html": "<p>Well if you look at our current fusion groups, there are no operations that could cause cross-device transfers. So the subgraphs that we would fuse would error out if you substituted some inputs to be on different devices. Instead of checking those things statically, we do a dynamic check now, and use the fallback to give a nicer error message if we detect that inputs are on different devices.</p>", "body_text": "Well if you look at our current fusion groups, there are no operations that could cause cross-device transfers. So the subgraphs that we would fuse would error out if you substituted some inputs to be on different devices. Instead of checking those things statically, we do a dynamic check now, and use the fallback to give a nicer error message if we detect that inputs are on different devices.", "in_reply_to_id": 229881515}