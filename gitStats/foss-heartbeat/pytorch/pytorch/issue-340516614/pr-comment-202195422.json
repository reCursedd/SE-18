{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202195422", "pull_request_review_id": 136848171, "id": 202195422, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjE5NTQyMg==", "diff_hunk": "@@ -8,35 +8,123 @@\n from hypothesis import given\n import hypothesis.strategies as st\n \n-from caffe2.python import core\n+from caffe2.python import core, workspace\n import caffe2.python.hypothesis_test_util as hu\n+import caffe2.python.mkl_test_util as mu\n+\n+import unittest\n \n \n class TestActivations(hu.HypothesisTestCase):\n-    @given(X=hu.tensor(),\n-           alpha=st.floats(min_value=0.1, max_value=2.0),\n-           inplace=st.booleans(),\n-           **hu.gcs)\n-    def test_elu(self, X, alpha, inplace, gc, dc):\n+    @given(X=hu.tensor(), in_place=st.booleans(),\n+           engine=st.sampled_from([\"\", \"CUDNN\"]), **mu.gcs)\n+    def test_relu(self, X, in_place, engine, gc, dc):\n+        if gc == mu.mkl_do:\n+            in_place = False\n+\n+        op = core.CreateOperator(\n+            \"Relu\",\n+            [\"X\"],\n+            [\"X\"] if in_place else [\"Y\"],\n+            engine=engine,\n+        )\n+\n+        def relu_ref(X):\n+            return [np.maximum(X, 0.0)]\n+\n+        # go away from the origin point to avoid kink problems\n+        X += 0.02 * np.sign(X)\n+        X[X == 0.0] += 0.02\n+\n+        self.assertReferenceChecks(gc, op, [X], relu_ref)\n+        self.assertDeviceChecks(dc, op, [X], [0])\n+        self.assertGradientChecks(gc, op, [X], 0, [0])\n+\n+    @unittest.skipIf(not workspace.has_gpu_support,\n+                     \"Relu for float16 can only run on GPU now.\")\n+    @given(X=hu.tensor(dtype=np.float16), in_place=st.booleans(),\n+           engine=st.sampled_from([\"\", \"CUDNN\"]), **hu.gcs_gpu_only)\n+    def test_relu_fp16(self, X, in_place, engine, gc, dc):\n+        op = core.CreateOperator(\n+            \"Relu\",\n+            [\"X\"],\n+            [\"X\"] if in_place else [\"Y\"],\n+            engine=engine,\n+        )\n+\n+        def relu_ref(X):\n+            return [np.maximum(X, 0.0)]\n+\n+        def relu_grad_ref(g_out, outputs, fwd_inputs):\n+            dY = g_out\n+            [Y] = outputs\n+            dX = dY\n+            dX[Y == 0] = 0\n+            return [dX]\n+\n         # go away from the origin point to avoid kink problems\n+        X += 0.02 * np.sign(X)\n+        X[X == 0.0] += 0.02\n+\n+        self.assertReferenceChecks(\n+            hu.gpu_do,\n+            op,\n+            [X],\n+            relu_ref,\n+            output_to_grad=\"X\" if in_place else \"Y\",\n+            grad_reference=relu_grad_ref)\n+\n+    @given(X=hu.tensor(elements=st.floats(-3.0, 3.0)),\n+           n=st.floats(min_value=0.5, max_value=2.0),\n+           in_place=st.booleans(), **hu.gcs)\n+    def test_relu_n(self, X, n, in_place, gc, dc):", "path": "caffe2/python/operator_test/activation_ops_test.py", "position": 79, "original_position": 79, "commit_id": "732a7eb2dca5d1cfcae9799a29edc4fa9c488e5b", "original_commit_id": "27b5586fe3395ce571000a82f6da60a019e63914", "user": {"login": "houseroad", "id": 30275821, "node_id": "MDQ6VXNlcjMwMjc1ODIx", "avatar_url": "https://avatars0.githubusercontent.com/u/30275821?v=4", "gravatar_id": "", "url": "https://api.github.com/users/houseroad", "html_url": "https://github.com/houseroad", "followers_url": "https://api.github.com/users/houseroad/followers", "following_url": "https://api.github.com/users/houseroad/following{/other_user}", "gists_url": "https://api.github.com/users/houseroad/gists{/gist_id}", "starred_url": "https://api.github.com/users/houseroad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/houseroad/subscriptions", "organizations_url": "https://api.github.com/users/houseroad/orgs", "repos_url": "https://api.github.com/users/houseroad/repos", "events_url": "https://api.github.com/users/houseroad/events{/privacy}", "received_events_url": "https://api.github.com/users/houseroad/received_events", "type": "User", "site_admin": false}, "body": "Shall we also add a case without specifying the n to test the default value 6?", "created_at": "2018-07-12T22:14:13Z", "updated_at": "2018-11-23T15:47:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/9379#discussion_r202195422", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9379", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202195422"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9379#discussion_r202195422"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9379"}}, "body_html": "<p>Shall we also add a case without specifying the n to test the default value 6?</p>", "body_text": "Shall we also add a case without specifying the n to test the default value 6?"}