{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12436", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12436/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12436/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12436/events", "html_url": "https://github.com/pytorch/pytorch/issues/12436", "id": 367565760, "node_id": "MDU6SXNzdWUzNjc1NjU3NjA=", "number": 12436, "title": "Pytorch AttributeError: 'NoneType' object has no attribute 'log_softmax'", "user": {"login": "CHIMKIE", "id": 37504093, "node_id": "MDQ6VXNlcjM3NTA0MDkz", "avatar_url": "https://avatars1.githubusercontent.com/u/37504093?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CHIMKIE", "html_url": "https://github.com/CHIMKIE", "followers_url": "https://api.github.com/users/CHIMKIE/followers", "following_url": "https://api.github.com/users/CHIMKIE/following{/other_user}", "gists_url": "https://api.github.com/users/CHIMKIE/gists{/gist_id}", "starred_url": "https://api.github.com/users/CHIMKIE/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CHIMKIE/subscriptions", "organizations_url": "https://api.github.com/users/CHIMKIE/orgs", "repos_url": "https://api.github.com/users/CHIMKIE/repos", "events_url": "https://api.github.com/users/CHIMKIE/events{/privacy}", "received_events_url": "https://api.github.com/users/CHIMKIE/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-10-07T15:36:51Z", "updated_at": "2018-11-12T13:39:56Z", "closed_at": "2018-11-12T05:28:11Z", "author_association": "NONE", "body_html": "<p>I'm learning to use Pytorch and trying to train a model with CIFAR10 dataset.<br>\nHere is my code(the part of importing and loading data is omitted):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">CIFARModelTorch</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(CIFARModelTorch, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.conv1 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>)\n        <span class=\"pl-c1\">self</span>.conv2 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>)\n        <span class=\"pl-c1\">self</span>.pool <span class=\"pl-k\">=</span> nn.MaxPool2d(<span class=\"pl-c1\">2</span>)\n        <span class=\"pl-c1\">self</span>.conv3 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">3</span>)\n        <span class=\"pl-c1\">self</span>.conv4 <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">3</span>)\n        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">12800</span>, <span class=\"pl-c1\">256</span>)\n        <span class=\"pl-c1\">self</span>.fc2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">256</span>)\n        <span class=\"pl-c1\">self</span>.fc3 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">10</span>)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv1(x))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.pool(F.relu(<span class=\"pl-c1\">self</span>.conv2(x)))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.conv3(x))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.pool(F.relu(<span class=\"pl-c1\">self</span>.conv4(x)))\n        x <span class=\"pl-k\">=</span> x.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">12800</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Flatten</span>\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc1(x))\n        x <span class=\"pl-k\">=</span> F.relu(<span class=\"pl-c1\">self</span>.fc2(x))\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc3(x)\nnet <span class=\"pl-k\">=</span> CIFARModelTorch()\n<span class=\"pl-c1\">print</span>(net)\ncriterion <span class=\"pl-k\">=</span> nn.CrossEntropyLoss()    <span class=\"pl-c\"><span class=\"pl-c\">#</span> select loss function</span>\noptimizer <span class=\"pl-k\">=</span> optim.SGD(net.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>)   <span class=\"pl-c\"><span class=\"pl-c\">#</span> optim:SGD</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test01<span class=\"pl-pds\">'</span></span>)    <span class=\"pl-c\"><span class=\"pl-c\">#</span>testing</span>\n<span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">50</span>):\n    running_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n    <span class=\"pl-k\">for</span> i, data <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(trainloader, <span class=\"pl-c1\">0</span>):\n        inputs, labels <span class=\"pl-k\">=</span> data\n        inputs, labels <span class=\"pl-k\">=</span> Variable(inputs), Variable(labels)\n        optimizer.zero_grad()\n        outputs <span class=\"pl-k\">=</span> net(inputs)\n        loss <span class=\"pl-k\">=</span> criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss <span class=\"pl-k\">+=</span> loss.data[<span class=\"pl-c1\">0</span>]\n        <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">2000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1999</span>:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>[<span class=\"pl-c1\">%d</span>, <span class=\"pl-c1\">%5d</span>] loss: <span class=\"pl-c1\">%.3f</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (epoch <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, running_loss <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2000</span>))\n            running_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Finished Training<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>However,it raised an error. Could anyone explain what caused this problem? And if possible, how to fix it?Thanks a lot!</p>\n<pre><code>D:\\Anaconda3\\envs\\python36\\python.exe C:/pt_test/setup_cifar_torch\nFiles already downloaded and verified\nFiles already downloaded and verified\nCIFARModelTorch(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n  (fc1): Linear(in_features=12800, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\n)\ntest01\nTraceback (most recent call last):\n  File \"C:/pt_test/setup_cifar_torch\", line 61, in &lt;module&gt;\n    loss = criterion(outputs, labels)\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 862, in forward\n    ignore_index=self.ignore_index, reduction=self.reduction)\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\functional.py\", line 1550, in cross_entropy\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\functional.py\", line 975, in log_softmax\n    return input.log_softmax(dim)\nAttributeError: 'NoneType' object has no attribute 'log_softmax'\n\nProcess finished with exit code 1\n\n</code></pre>", "body_text": "I'm learning to use Pytorch and trying to train a model with CIFAR10 dataset.\nHere is my code(the part of importing and loading data is omitted):\nclass CIFARModelTorch(nn.Module):\n    def __init__(self):\n        super(CIFARModelTorch, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, 3)\n        self.conv2 = nn.Conv2d(64, 64, 3)\n        self.pool = nn.MaxPool2d(2)\n        self.conv3 = nn.Conv2d(64, 128, 3)\n        self.conv4 = nn.Conv2d(128, 128, 3)\n        self.fc1 = nn.Linear(12800, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 10)\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = F.relu(self.conv3(x))\n        x = self.pool(F.relu(self.conv4(x)))\n        x = x.view(-1, 12800)  # Flatten\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\nnet = CIFARModelTorch()\nprint(net)\ncriterion = nn.CrossEntropyLoss()    # select loss function\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)   # optim:SGD\nprint('test01')    #testing\nfor epoch in range(50):\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = Variable(inputs), Variable(labels)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.data[0]\n        if i % 2000 == 1999:\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\nprint('Finished Training')\nHowever,it raised an error. Could anyone explain what caused this problem? And if possible, how to fix it?Thanks a lot!\nD:\\Anaconda3\\envs\\python36\\python.exe C:/pt_test/setup_cifar_torch\nFiles already downloaded and verified\nFiles already downloaded and verified\nCIFARModelTorch(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n  (fc1): Linear(in_features=12800, out_features=256, bias=True)\n  (fc2): Linear(in_features=256, out_features=256, bias=True)\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\n)\ntest01\nTraceback (most recent call last):\n  File \"C:/pt_test/setup_cifar_torch\", line 61, in <module>\n    loss = criterion(outputs, labels)\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 862, in forward\n    ignore_index=self.ignore_index, reduction=self.reduction)\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\functional.py\", line 1550, in cross_entropy\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\functional.py\", line 975, in log_softmax\n    return input.log_softmax(dim)\nAttributeError: 'NoneType' object has no attribute 'log_softmax'\n\nProcess finished with exit code 1", "body": "I'm learning to use Pytorch and trying to train a model with CIFAR10 dataset.\r\nHere is my code(the part of importing and loading data is omitted):\r\n```python\r\nclass CIFARModelTorch(nn.Module):\r\n    def __init__(self):\r\n        super(CIFARModelTorch, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 64, 3)\r\n        self.conv2 = nn.Conv2d(64, 64, 3)\r\n        self.pool = nn.MaxPool2d(2)\r\n        self.conv3 = nn.Conv2d(64, 128, 3)\r\n        self.conv4 = nn.Conv2d(128, 128, 3)\r\n        self.fc1 = nn.Linear(12800, 256)\r\n        self.fc2 = nn.Linear(256, 256)\r\n        self.fc3 = nn.Linear(256, 10)\r\n    def forward(self, x):\r\n        x = F.relu(self.conv1(x))\r\n        x = self.pool(F.relu(self.conv2(x)))\r\n        x = F.relu(self.conv3(x))\r\n        x = self.pool(F.relu(self.conv4(x)))\r\n        x = x.view(-1, 12800)  # Flatten\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n        x = self.fc3(x)\r\nnet = CIFARModelTorch()\r\nprint(net)\r\ncriterion = nn.CrossEntropyLoss()    # select loss function\r\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)   # optim:SGD\r\nprint('test01')    #testing\r\nfor epoch in range(50):\r\n    running_loss = 0.0\r\n    for i, data in enumerate(trainloader, 0):\r\n        inputs, labels = data\r\n        inputs, labels = Variable(inputs), Variable(labels)\r\n        optimizer.zero_grad()\r\n        outputs = net(inputs)\r\n        loss = criterion(outputs, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n        running_loss += loss.data[0]\r\n        if i % 2000 == 1999:\r\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\r\n            running_loss = 0.0\r\nprint('Finished Training')\r\n```\r\n\r\nHowever,it raised an error. Could anyone explain what caused this problem? And if possible, how to fix it?Thanks a lot!\r\n```\r\nD:\\Anaconda3\\envs\\python36\\python.exe C:/pt_test/setup_cifar_torch\r\nFiles already downloaded and verified\r\nFiles already downloaded and verified\r\nCIFARModelTorch(\r\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\r\n  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\r\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\r\n  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\r\n  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\r\n  (fc1): Linear(in_features=12800, out_features=256, bias=True)\r\n  (fc2): Linear(in_features=256, out_features=256, bias=True)\r\n  (fc3): Linear(in_features=256, out_features=10, bias=True)\r\n)\r\ntest01\r\nTraceback (most recent call last):\r\n  File \"C:/pt_test/setup_cifar_torch\", line 61, in <module>\r\n    loss = criterion(outputs, labels)\r\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 477, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\modules\\loss.py\", line 862, in forward\r\n    ignore_index=self.ignore_index, reduction=self.reduction)\r\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\functional.py\", line 1550, in cross_entropy\r\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n  File \"D:\\Anaconda3\\envs\\python36\\lib\\site-packages\\torch\\nn\\functional.py\", line 975, in log_softmax\r\n    return input.log_softmax(dim)\r\nAttributeError: 'NoneType' object has no attribute 'log_softmax'\r\n\r\nProcess finished with exit code 1\r\n\r\n```"}