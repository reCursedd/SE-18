{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362733307", "html_url": "https://github.com/pytorch/pytorch/pull/3815#issuecomment-362733307", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3815", "id": 362733307, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjczMzMwNw==", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-02T22:57:39Z", "updated_at": "2018-02-02T23:06:01Z", "author_association": "COLLABORATOR", "body_html": "<p>Ok, for posterity: I went back to searching for an advanced indexing solution and I found a 10-liner that does the same thing as this 1.2k line PR...</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\n\ndef indexed_conv(input, weight, bias, indices):\n\n    nbatch = input.shape[0]\n    output_width = indices.shape[0]\n    out_chans, in_chans, ksize = weight.shape\n\n    padded = indices == -1\n    indices[padded] = 0\n\n    mask = torch.FloatTensor([1,0])\n    mask = mask[..., padded.t().long()]\n    if isinstance(input, Variable):\n        mask = Variable(mask)\n\n    col = input[..., indices.t()] * mask\n    col = col.view(nbatch, -1, output_width)\n\n    weight_col = weight.view(out_chans, -1)\n\n    return torch.matmul(weight_col, col) + bias\n\n\nif __name__ == '__main__':\n\n    input = torch.randn(1,2,5)\n    weight = torch.randn(1,2,3)\n    bias = torch.randn(1)\n    indices = (5 * torch.rand(4,3)).long()\n    indices[0,0] = -1\n\n    out = indexed_conv(input, weight, bias, indices)\n\n    input = Variable(input, requires_grad=True)\n    weight = Variable(weight)\n    bias = Variable(bias)\n\n    out = indexed_conv(input, weight, bias, indices)\n\n    out.sum().backward()\n\n    print(input.grad)\n</code></pre>\n<p>Plus it already allows for the output size to be different from the input size.</p>\n<p>I'm closing this PR, I'll eventually open a new one.</p>", "body_text": "Ok, for posterity: I went back to searching for an advanced indexing solution and I found a 10-liner that does the same thing as this 1.2k line PR...\nimport torch\nfrom torch.autograd import Variable\n\ndef indexed_conv(input, weight, bias, indices):\n\n    nbatch = input.shape[0]\n    output_width = indices.shape[0]\n    out_chans, in_chans, ksize = weight.shape\n\n    padded = indices == -1\n    indices[padded] = 0\n\n    mask = torch.FloatTensor([1,0])\n    mask = mask[..., padded.t().long()]\n    if isinstance(input, Variable):\n        mask = Variable(mask)\n\n    col = input[..., indices.t()] * mask\n    col = col.view(nbatch, -1, output_width)\n\n    weight_col = weight.view(out_chans, -1)\n\n    return torch.matmul(weight_col, col) + bias\n\n\nif __name__ == '__main__':\n\n    input = torch.randn(1,2,5)\n    weight = torch.randn(1,2,3)\n    bias = torch.randn(1)\n    indices = (5 * torch.rand(4,3)).long()\n    indices[0,0] = -1\n\n    out = indexed_conv(input, weight, bias, indices)\n\n    input = Variable(input, requires_grad=True)\n    weight = Variable(weight)\n    bias = Variable(bias)\n\n    out = indexed_conv(input, weight, bias, indices)\n\n    out.sum().backward()\n\n    print(input.grad)\n\nPlus it already allows for the output size to be different from the input size.\nI'm closing this PR, I'll eventually open a new one.", "body": "Ok, for posterity: I went back to searching for an advanced indexing solution and I found a 10-liner that does the same thing as this 1.2k line PR...\r\n\r\n```\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\ndef indexed_conv(input, weight, bias, indices):\r\n\r\n    nbatch = input.shape[0]\r\n    output_width = indices.shape[0]\r\n    out_chans, in_chans, ksize = weight.shape\r\n\r\n    padded = indices == -1\r\n    indices[padded] = 0\r\n\r\n    mask = torch.FloatTensor([1,0])\r\n    mask = mask[..., padded.t().long()]\r\n    if isinstance(input, Variable):\r\n        mask = Variable(mask)\r\n\r\n    col = input[..., indices.t()] * mask\r\n    col = col.view(nbatch, -1, output_width)\r\n\r\n    weight_col = weight.view(out_chans, -1)\r\n\r\n    return torch.matmul(weight_col, col) + bias\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    input = torch.randn(1,2,5)\r\n    weight = torch.randn(1,2,3)\r\n    bias = torch.randn(1)\r\n    indices = (5 * torch.rand(4,3)).long()\r\n    indices[0,0] = -1\r\n\r\n    out = indexed_conv(input, weight, bias, indices)\r\n\r\n    input = Variable(input, requires_grad=True)\r\n    weight = Variable(weight)\r\n    bias = Variable(bias)\r\n\r\n    out = indexed_conv(input, weight, bias, indices)\r\n\r\n    out.sum().backward()\r\n\r\n    print(input.grad)\r\n```\r\n\r\nPlus it already allows for the output size to be different from the input size.\r\n\r\nI'm closing this PR, I'll eventually open a new one."}