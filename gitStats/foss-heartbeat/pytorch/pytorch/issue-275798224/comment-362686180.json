{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/362686180", "html_url": "https://github.com/pytorch/pytorch/pull/3815#issuecomment-362686180", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3815", "id": 362686180, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjY4NjE4MA==", "user": {"login": "lantiga", "id": 191033, "node_id": "MDQ6VXNlcjE5MTAzMw==", "avatar_url": "https://avatars2.githubusercontent.com/u/191033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lantiga", "html_url": "https://github.com/lantiga", "followers_url": "https://api.github.com/users/lantiga/followers", "following_url": "https://api.github.com/users/lantiga/following{/other_user}", "gists_url": "https://api.github.com/users/lantiga/gists{/gist_id}", "starred_url": "https://api.github.com/users/lantiga/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lantiga/subscriptions", "organizations_url": "https://api.github.com/users/lantiga/orgs", "repos_url": "https://api.github.com/users/lantiga/repos", "events_url": "https://api.github.com/users/lantiga/events{/privacy}", "received_events_url": "https://api.github.com/users/lantiga/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-02T19:42:18Z", "updated_at": "2018-02-02T19:42:18Z", "author_association": "COLLABORATOR", "body_html": "<p>Thank you <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> !</p>\n<blockquote>\n<p>I dont understand why indices has to be of size (L, K). From my understanding, indices specifies the particular input indices that the kernel is going to convolve with. L in indices here is nothing related to input size L but is the number of times a dot-product is done as part of the convolution. To give more clarity, in a standard Conv1d, number of dot products = output_size. Here, you are specifying the convention that output_size = L, which is why indices is of size L, K, but there's no harm in relaxing this and making indices to be (output_size, K).</p>\n</blockquote>\n<p>You're absolutely right. I had my <code>L == output_size</code> use case in mind when I wrote the code, but of course there's nothing that constrains it to be that way.</p>\n<blockquote>\n<p>As mentioned in your PR description, indices are absolute and not relative. I want to reinforce that relative indices makes no sense (relative to what?)</p>\n</blockquote>\n<p>Eh eh, ok. The idea was to eventually provide kernel indices as a column K of relative indices wrt the current input element, for those cases when the arrangement of values in the input follows a regular pattern. It's not a big deal to generate the complete <code>indices</code> tensor anyway, so no point to complicate things.</p>\n<blockquote>\n<p>Add tests. Tests for correctness against known inputs / grads, and also jacobian tests.</p>\n</blockquote>\n<p>Will do.</p>\n<blockquote>\n<p>Unsure thoughts<br>\nI think the whole function can be implemented as: Tensor advanced indexing + view + conv1d. There are no downsides (because it uses im2col, there are no memory savings in doing the way you are doing). If so, the entire implementation becomes a trivial few lines of ATen or Python.</p>\n</blockquote>\n<p>I remember trying and failing, but it might easily have been me :-) I'll give it another shot.</p>\n<blockquote>\n<p>Refactor request<br>\nmove to using ATen. I think the code can be vastly simplified if you make this an ATen native function, rather than the clunky THNN</p>\n</blockquote>\n<p>I'll happily do that.</p>\n<blockquote>\n<p>Name Changes<br>\nIndexedConv -&gt; ConvIndexed1d<br>\nindexed_conv -&gt; conv_indexed1d</p>\n</blockquote>", "body_text": "Thank you @soumith !\n\nI dont understand why indices has to be of size (L, K). From my understanding, indices specifies the particular input indices that the kernel is going to convolve with. L in indices here is nothing related to input size L but is the number of times a dot-product is done as part of the convolution. To give more clarity, in a standard Conv1d, number of dot products = output_size. Here, you are specifying the convention that output_size = L, which is why indices is of size L, K, but there's no harm in relaxing this and making indices to be (output_size, K).\n\nYou're absolutely right. I had my L == output_size use case in mind when I wrote the code, but of course there's nothing that constrains it to be that way.\n\nAs mentioned in your PR description, indices are absolute and not relative. I want to reinforce that relative indices makes no sense (relative to what?)\n\nEh eh, ok. The idea was to eventually provide kernel indices as a column K of relative indices wrt the current input element, for those cases when the arrangement of values in the input follows a regular pattern. It's not a big deal to generate the complete indices tensor anyway, so no point to complicate things.\n\nAdd tests. Tests for correctness against known inputs / grads, and also jacobian tests.\n\nWill do.\n\nUnsure thoughts\nI think the whole function can be implemented as: Tensor advanced indexing + view + conv1d. There are no downsides (because it uses im2col, there are no memory savings in doing the way you are doing). If so, the entire implementation becomes a trivial few lines of ATen or Python.\n\nI remember trying and failing, but it might easily have been me :-) I'll give it another shot.\n\nRefactor request\nmove to using ATen. I think the code can be vastly simplified if you make this an ATen native function, rather than the clunky THNN\n\nI'll happily do that.\n\nName Changes\nIndexedConv -> ConvIndexed1d\nindexed_conv -> conv_indexed1d", "body": "Thank you @soumith !\r\n\r\n> I dont understand why indices has to be of size (L, K). From my understanding, indices specifies the particular input indices that the kernel is going to convolve with. L in indices here is nothing related to input size L but is the number of times a dot-product is done as part of the convolution. To give more clarity, in a standard Conv1d, number of dot products = output_size. Here, you are specifying the convention that output_size = L, which is why indices is of size L, K, but there's no harm in relaxing this and making indices to be (output_size, K).\r\n\r\nYou're absolutely right. I had my `L == output_size` use case in mind when I wrote the code, but of course there's nothing that constrains it to be that way.\r\n\r\n> As mentioned in your PR description, indices are absolute and not relative. I want to reinforce that relative indices makes no sense (relative to what?)\r\n\r\nEh eh, ok. The idea was to eventually provide kernel indices as a column K of relative indices wrt the current input element, for those cases when the arrangement of values in the input follows a regular pattern. It's not a big deal to generate the complete `indices` tensor anyway, so no point to complicate things.\r\n\r\n> Add tests. Tests for correctness against known inputs / grads, and also jacobian tests.\r\n\r\nWill do.\r\n\r\n> Unsure thoughts\r\n> I think the whole function can be implemented as: Tensor advanced indexing + view + conv1d. There are no downsides (because it uses im2col, there are no memory savings in doing the way you are doing). If so, the entire implementation becomes a trivial few lines of ATen or Python.\r\n\r\nI remember trying and failing, but it might easily have been me :-) I'll give it another shot.\r\n\r\n> Refactor request\r\n> move to using ATen. I think the code can be vastly simplified if you make this an ATen native function, rather than the clunky THNN\r\n\r\nI'll happily do that.\r\n\r\n> Name Changes\r\n> IndexedConv -> ConvIndexed1d\r\n> indexed_conv -> conv_indexed1d"}