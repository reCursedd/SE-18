{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2896", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2896/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2896/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2896/events", "html_url": "https://github.com/pytorch/pytorch/pull/2896", "id": 261678734, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQzOTE1ODQx", "number": 2896, "title": "Fix multinomial sampling with total/partial probabilities = 0", "user": {"login": "MicaelCarvalho", "id": 17184992, "node_id": "MDQ6VXNlcjE3MTg0OTky", "avatar_url": "https://avatars3.githubusercontent.com/u/17184992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MicaelCarvalho", "html_url": "https://github.com/MicaelCarvalho", "followers_url": "https://api.github.com/users/MicaelCarvalho/followers", "following_url": "https://api.github.com/users/MicaelCarvalho/following{/other_user}", "gists_url": "https://api.github.com/users/MicaelCarvalho/gists{/gist_id}", "starred_url": "https://api.github.com/users/MicaelCarvalho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MicaelCarvalho/subscriptions", "organizations_url": "https://api.github.com/users/MicaelCarvalho/orgs", "repos_url": "https://api.github.com/users/MicaelCarvalho/repos", "events_url": "https://api.github.com/users/MicaelCarvalho/events{/privacy}", "received_events_url": "https://api.github.com/users/MicaelCarvalho/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-09-29T15:26:39Z", "updated_at": "2018-08-21T16:01:32Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/2896", "html_url": "https://github.com/pytorch/pytorch/pull/2896", "diff_url": "https://github.com/pytorch/pytorch/pull/2896.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/2896.patch"}, "body_html": "<p><em>This PR probably needs to be discussed. I'm going to present the problem, what changes with the PR and the possible concerns with respect to these changes.</em></p>\n<hr>\n<h2>The problem</h2>\n<p>First known report on Slack's channel <code>beginner</code> on 27 sept, by myself.</p>\n<p>The function <code>multinomial</code> assumes non-negativity and non-zero sum for the input, however, it does not treat elements with value = zero, and returns random-like indices instead. Example:</p>\n<pre><code>import torch\nweights = torch.Tensor([0, 10, 3, 0])\ntorch.multinomial(weights, 4, replacement=False)\n# &gt; 1, 2, 0, 0\nweights = torch.Tensor([1, 10, 3, 0])\ntorch.multinomial(weights, 4, replacement=False)\n# &gt; 1, 0, 2, 1\n</code></pre>\n<p>Basically, in the first example it runs out of 'positive' probabilities and starts outputting 0 (always). But on the second example it outputs 1 when it doesn't have any probs left. The biggest problem is that there is no reason for it to output 1, and it is not always 1 (sometimes 0, never 2).</p>\n<p>Taking a closer look on the code, I realized the function was designed to work with <strong>non-zero values</strong>, and not non-zero sum, it presents erratic behavior when there are zeros inside the matrix. This seems to be an undesirable behavior, since the caller would have to manually check every probability, and it is not that uncommon to find situations in which we have a weight matrix with zeros inside and we still want to sample from it.</p>\n<h2>What changed</h2>\n<p>Two constraints were removed from the <code>multinomial</code> function:</p>\n<ol>\n<li>Number of samples must be inferior or equal to the number of categories</li>\n<li>Sum of values must be positive</li>\n</ol>\n<p>Values are sampled in the same was as before, except when there is no values left to be sampled. In the latter case, the sampled position is <code>-1</code>, and the examples given before output the following now:</p>\n<pre><code>import torch\nweights = torch.Tensor([0, 10, 3, 0])\ntorch.multinomial(weights, 4, replacement=False)\n# &gt; 1, 2, -1, -1\nweights = torch.Tensor([1, 10, 3, 0])\ntorch.multinomial(weights, 4, replacement=False)\n# &gt; 1, 2, 0, -1 # or \"1, 0, 2, -1\", etc. Still a weighted random on the probs\n</code></pre>\n<p>With the difference that zero-sum probabilities are accepted, and the number of samples can be bigger than the number of categories:</p>\n<pre><code>import torch\nweights = torch.Tensor([0, 0, 0, 0])\ntorch.multinomial(weights, 4, replacement=False) # zero sum\n# &gt; -1, -1, -1, -1\nweights = torch.Tensor([1, 10, 3, 0])\ntorch.multinomial(weights, 6, replacement=False) # 6 samples\n# &gt; 2, 1, 0, -1, -1, -1\n</code></pre>\n<h2>What did not change</h2>\n<p>Values are still supposed to be non-negative. Passing negative values for the probabilities will affect the behavior of the function:</p>\n<pre><code>import torch\nweights = torch.Tensor([0, 0, 6, -5])\ntorch.multinomial(weights, 4, replacement=False) # negative value inside\n# &gt; 2, -1, -1, -1 # positive value outweighs negative value, but things are weird in the probability vector inside the function\nweights = torch.Tensor([1, 4, 1, -5])\ntorch.multinomial(weights, 4, replacement=False) # negative value inside\n# &gt; 0, -1, -1, -1 # positive value outweighs negative value and allow for only 1 sample to be drawn, weird things in prob vector as well\nweights = torch.Tensor([0, 0, 1, -5])\ntorch.multinomial(weights, 4, replacement=False) # negative value inside\n# &gt; -1, -1, -1, -1 # Sum is negative, it will skip sampling\n</code></pre>\n<h2>What should be debated about these changes</h2>\n<p>The behavior of the function changed: it used to output \"random\" (not really, but well...) indexes when the vector reached probability 0 because of some zero-valued item inside of it ; now it outputs -1 for every sample not drawn. The previous behavior allowed for anyone to use the output of the <code>multinomial</code> function to index a matrix, later erasing invalid elements -- <em>the problem is that these invalid elements were not easily identifiable</em>.</p>\n<p>With the proposed changes, the user would have to replace the -1 indexes with a valid index before using the output of <code>multinomial</code> to index a vector/matrix. The benefit of this approach is that it would easy to identify any invalid elements (i.e. prob = 0) to remove them from the samples.</p>", "body_text": "This PR probably needs to be discussed. I'm going to present the problem, what changes with the PR and the possible concerns with respect to these changes.\n\nThe problem\nFirst known report on Slack's channel beginner on 27 sept, by myself.\nThe function multinomial assumes non-negativity and non-zero sum for the input, however, it does not treat elements with value = zero, and returns random-like indices instead. Example:\nimport torch\nweights = torch.Tensor([0, 10, 3, 0])\ntorch.multinomial(weights, 4, replacement=False)\n# > 1, 2, 0, 0\nweights = torch.Tensor([1, 10, 3, 0])\ntorch.multinomial(weights, 4, replacement=False)\n# > 1, 0, 2, 1\n\nBasically, in the first example it runs out of 'positive' probabilities and starts outputting 0 (always). But on the second example it outputs 1 when it doesn't have any probs left. The biggest problem is that there is no reason for it to output 1, and it is not always 1 (sometimes 0, never 2).\nTaking a closer look on the code, I realized the function was designed to work with non-zero values, and not non-zero sum, it presents erratic behavior when there are zeros inside the matrix. This seems to be an undesirable behavior, since the caller would have to manually check every probability, and it is not that uncommon to find situations in which we have a weight matrix with zeros inside and we still want to sample from it.\nWhat changed\nTwo constraints were removed from the multinomial function:\n\nNumber of samples must be inferior or equal to the number of categories\nSum of values must be positive\n\nValues are sampled in the same was as before, except when there is no values left to be sampled. In the latter case, the sampled position is -1, and the examples given before output the following now:\nimport torch\nweights = torch.Tensor([0, 10, 3, 0])\ntorch.multinomial(weights, 4, replacement=False)\n# > 1, 2, -1, -1\nweights = torch.Tensor([1, 10, 3, 0])\ntorch.multinomial(weights, 4, replacement=False)\n# > 1, 2, 0, -1 # or \"1, 0, 2, -1\", etc. Still a weighted random on the probs\n\nWith the difference that zero-sum probabilities are accepted, and the number of samples can be bigger than the number of categories:\nimport torch\nweights = torch.Tensor([0, 0, 0, 0])\ntorch.multinomial(weights, 4, replacement=False) # zero sum\n# > -1, -1, -1, -1\nweights = torch.Tensor([1, 10, 3, 0])\ntorch.multinomial(weights, 6, replacement=False) # 6 samples\n# > 2, 1, 0, -1, -1, -1\n\nWhat did not change\nValues are still supposed to be non-negative. Passing negative values for the probabilities will affect the behavior of the function:\nimport torch\nweights = torch.Tensor([0, 0, 6, -5])\ntorch.multinomial(weights, 4, replacement=False) # negative value inside\n# > 2, -1, -1, -1 # positive value outweighs negative value, but things are weird in the probability vector inside the function\nweights = torch.Tensor([1, 4, 1, -5])\ntorch.multinomial(weights, 4, replacement=False) # negative value inside\n# > 0, -1, -1, -1 # positive value outweighs negative value and allow for only 1 sample to be drawn, weird things in prob vector as well\nweights = torch.Tensor([0, 0, 1, -5])\ntorch.multinomial(weights, 4, replacement=False) # negative value inside\n# > -1, -1, -1, -1 # Sum is negative, it will skip sampling\n\nWhat should be debated about these changes\nThe behavior of the function changed: it used to output \"random\" (not really, but well...) indexes when the vector reached probability 0 because of some zero-valued item inside of it ; now it outputs -1 for every sample not drawn. The previous behavior allowed for anyone to use the output of the multinomial function to index a matrix, later erasing invalid elements -- the problem is that these invalid elements were not easily identifiable.\nWith the proposed changes, the user would have to replace the -1 indexes with a valid index before using the output of multinomial to index a vector/matrix. The benefit of this approach is that it would easy to identify any invalid elements (i.e. prob = 0) to remove them from the samples.", "body": "_This PR probably needs to be discussed. I'm going to present the problem, what changes with the PR and the possible concerns with respect to these changes._\r\n\r\n---\r\n\r\n## The problem\r\n\r\nFirst known report on Slack's channel `beginner` on 27 sept, by myself.\r\n\r\nThe function `multinomial` assumes non-negativity and non-zero sum for the input, however, it does not treat elements with value = zero, and returns random-like indices instead. Example:\r\n\r\n```\r\nimport torch\r\nweights = torch.Tensor([0, 10, 3, 0])\r\ntorch.multinomial(weights, 4, replacement=False)\r\n# > 1, 2, 0, 0\r\nweights = torch.Tensor([1, 10, 3, 0])\r\ntorch.multinomial(weights, 4, replacement=False)\r\n# > 1, 0, 2, 1\r\n```\r\n\r\nBasically, in the first example it runs out of 'positive' probabilities and starts outputting 0 (always). But on the second example it outputs 1 when it doesn't have any probs left. The biggest problem is that there is no reason for it to output 1, and it is not always 1 (sometimes 0, never 2).\r\n\r\nTaking a closer look on the code, I realized the function was designed to work with **non-zero values**, and not non-zero sum, it presents erratic behavior when there are zeros inside the matrix. This seems to be an undesirable behavior, since the caller would have to manually check every probability, and it is not that uncommon to find situations in which we have a weight matrix with zeros inside and we still want to sample from it.\r\n\r\n## What changed\r\n\r\nTwo constraints were removed from the `multinomial` function:\r\n  1. Number of samples must be inferior or equal to the number of categories\r\n  2. Sum of values must be positive\r\n\r\nValues are sampled in the same was as before, except when there is no values left to be sampled. In the latter case, the sampled position is `-1`, and the examples given before output the following now:\r\n\r\n```\r\nimport torch\r\nweights = torch.Tensor([0, 10, 3, 0])\r\ntorch.multinomial(weights, 4, replacement=False)\r\n# > 1, 2, -1, -1\r\nweights = torch.Tensor([1, 10, 3, 0])\r\ntorch.multinomial(weights, 4, replacement=False)\r\n# > 1, 2, 0, -1 # or \"1, 0, 2, -1\", etc. Still a weighted random on the probs\r\n```\r\n\r\nWith the difference that zero-sum probabilities are accepted, and the number of samples can be bigger than the number of categories:\r\n\r\n```\r\nimport torch\r\nweights = torch.Tensor([0, 0, 0, 0])\r\ntorch.multinomial(weights, 4, replacement=False) # zero sum\r\n# > -1, -1, -1, -1\r\nweights = torch.Tensor([1, 10, 3, 0])\r\ntorch.multinomial(weights, 6, replacement=False) # 6 samples\r\n# > 2, 1, 0, -1, -1, -1\r\n```\r\n\r\n## What did not change\r\n\r\nValues are still supposed to be non-negative. Passing negative values for the probabilities will affect the behavior of the function:\r\n\r\n```\r\nimport torch\r\nweights = torch.Tensor([0, 0, 6, -5])\r\ntorch.multinomial(weights, 4, replacement=False) # negative value inside\r\n# > 2, -1, -1, -1 # positive value outweighs negative value, but things are weird in the probability vector inside the function\r\nweights = torch.Tensor([1, 4, 1, -5])\r\ntorch.multinomial(weights, 4, replacement=False) # negative value inside\r\n# > 0, -1, -1, -1 # positive value outweighs negative value and allow for only 1 sample to be drawn, weird things in prob vector as well\r\nweights = torch.Tensor([0, 0, 1, -5])\r\ntorch.multinomial(weights, 4, replacement=False) # negative value inside\r\n# > -1, -1, -1, -1 # Sum is negative, it will skip sampling\r\n```\r\n\r\n## What should be debated about these changes\r\n\r\nThe behavior of the function changed: it used to output \"random\" (not really, but well...) indexes when the vector reached probability 0 because of some zero-valued item inside of it ; now it outputs -1 for every sample not drawn. The previous behavior allowed for anyone to use the output of the `multinomial` function to index a matrix, later erasing invalid elements -- _the problem is that these invalid elements were not easily identifiable_.\r\n\r\nWith the proposed changes, the user would have to replace the -1 indexes with a valid index before using the output of `multinomial` to index a vector/matrix. The benefit of this approach is that it would easy to identify any invalid elements (i.e. prob = 0) to remove them from the samples."}