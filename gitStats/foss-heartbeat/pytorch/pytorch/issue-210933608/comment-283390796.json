{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/283390796", "html_url": "https://github.com/pytorch/pytorch/pull/880#issuecomment-283390796", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/880", "id": 283390796, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzM5MDc5Ng==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-01T16:28:20Z", "updated_at": "2017-03-01T16:28:20Z", "author_association": "MEMBER", "body_html": "<p>It's expected interaction at this point. I don't expect it to be fixed.</p>\n<p>This covers all the cudaFree calls in PyTorch including when the allocator compacts memory.</p>\n<p>cudaMalloc isn't an issue, but there might be other calls that could trigger a deadlock if they overlap with NCCL launches. I haven't tested all the likely suspects (cudaDeviceSynchronize, cudaStreamDestroy), but they are much less likely to be accidentally overlapped with NCCL calls.</p>", "body_text": "It's expected interaction at this point. I don't expect it to be fixed.\nThis covers all the cudaFree calls in PyTorch including when the allocator compacts memory.\ncudaMalloc isn't an issue, but there might be other calls that could trigger a deadlock if they overlap with NCCL launches. I haven't tested all the likely suspects (cudaDeviceSynchronize, cudaStreamDestroy), but they are much less likely to be accidentally overlapped with NCCL calls.", "body": "It's expected interaction at this point. I don't expect it to be fixed. \r\n\r\nThis covers all the cudaFree calls in PyTorch including when the allocator compacts memory.\r\n\r\ncudaMalloc isn't an issue, but there might be other calls that could trigger a deadlock if they overlap with NCCL launches. I haven't tested all the likely suspects (cudaDeviceSynchronize, cudaStreamDestroy), but they are much less likely to be accidentally overlapped with NCCL calls."}