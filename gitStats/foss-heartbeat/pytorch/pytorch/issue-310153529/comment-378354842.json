{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/378354842", "html_url": "https://github.com/pytorch/pytorch/pull/6150#issuecomment-378354842", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6150", "id": 378354842, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODM1NDg0Mg==", "user": {"login": "yinghai", "id": 1100089, "node_id": "MDQ6VXNlcjExMDAwODk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1100089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yinghai", "html_url": "https://github.com/yinghai", "followers_url": "https://api.github.com/users/yinghai/followers", "following_url": "https://api.github.com/users/yinghai/following{/other_user}", "gists_url": "https://api.github.com/users/yinghai/gists{/gist_id}", "starred_url": "https://api.github.com/users/yinghai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yinghai/subscriptions", "organizations_url": "https://api.github.com/users/yinghai/orgs", "repos_url": "https://api.github.com/users/yinghai/repos", "events_url": "https://api.github.com/users/yinghai/events{/privacy}", "received_events_url": "https://api.github.com/users/yinghai/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-03T18:42:56Z", "updated_at": "2018-04-04T04:32:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Regarding the <code>max_batch_size</code> I did some benchmark with Tesla M40 on my machine. Each test is doing inference 100 times after 20 times of warmup runs. The test next TensorRT engine is built with <code>max_batch_size=50</code>. Here are the results, regarding batch size.</p>\n<pre><code>N=1, TRT op iter: 1\nCUDA runtime: 1.07s\nTRT runtime: 0.73s, improvement: 31.4%\n\nN=50, TRT op iter: 1\nCUDA runtime: 14.0s\nTRT runtime: 8.30s, improvement: 41.6%\n\nN=52, TRT op iter: 2\nCUDA runtime: 14.4s\nTRT runtime: 9.11s, improvement: 36.9%\n\nN=101, TRT op iter: 3\nCUDA runtime: 28.1s\nTRT runtime: 17.3s, improvement: 38.2%\n</code></pre>\n<p>I also tried <code>CUDNN</code> engine, whose performance is similar to CUDA. From the benchmark, looks like extra calls of inference engine has some cost (5% improvement drop from N=50 to N=52) but is still worth it.</p>", "body_text": "Regarding the max_batch_size I did some benchmark with Tesla M40 on my machine. Each test is doing inference 100 times after 20 times of warmup runs. The test next TensorRT engine is built with max_batch_size=50. Here are the results, regarding batch size.\nN=1, TRT op iter: 1\nCUDA runtime: 1.07s\nTRT runtime: 0.73s, improvement: 31.4%\n\nN=50, TRT op iter: 1\nCUDA runtime: 14.0s\nTRT runtime: 8.30s, improvement: 41.6%\n\nN=52, TRT op iter: 2\nCUDA runtime: 14.4s\nTRT runtime: 9.11s, improvement: 36.9%\n\nN=101, TRT op iter: 3\nCUDA runtime: 28.1s\nTRT runtime: 17.3s, improvement: 38.2%\n\nI also tried CUDNN engine, whose performance is similar to CUDA. From the benchmark, looks like extra calls of inference engine has some cost (5% improvement drop from N=50 to N=52) but is still worth it.", "body": "Regarding the `max_batch_size` I did some benchmark with Tesla M40 on my machine. Each test is doing inference 100 times after 20 times of warmup runs. The test next TensorRT engine is built with `max_batch_size=50`. Here are the results, regarding batch size. \r\n```\r\nN=1, TRT op iter: 1\r\nCUDA runtime: 1.07s\r\nTRT runtime: 0.73s, improvement: 31.4%\r\n\r\nN=50, TRT op iter: 1\r\nCUDA runtime: 14.0s\r\nTRT runtime: 8.30s, improvement: 41.6%\r\n\r\nN=52, TRT op iter: 2\r\nCUDA runtime: 14.4s\r\nTRT runtime: 9.11s, improvement: 36.9%\r\n\r\nN=101, TRT op iter: 3\r\nCUDA runtime: 28.1s\r\nTRT runtime: 17.3s, improvement: 38.2%\r\n```\r\n\r\nI also tried `CUDNN` engine, whose performance is similar to CUDA. From the benchmark, looks like extra calls of inference engine has some cost (5% improvement drop from N=50 to N=52) but is still worth it. "}