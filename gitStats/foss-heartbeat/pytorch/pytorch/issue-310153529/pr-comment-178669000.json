{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178669000", "pull_request_review_id": 108764598, "id": 178669000, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3ODY2OTAwMA==", "diff_hunk": "@@ -0,0 +1,423 @@\n+#include \"caffe2/contrib/tensorrt/tensorrt_tranformer.h\"\n+#include \"caffe2/contrib/tensorrt/trt_utils.h\"\n+#include \"caffe2/core/logging.h\"\n+#include \"caffe2/core/operator.h\"\n+#include \"caffe2/onnx/onnx_exporter.h\"\n+#include <onnx2trt.hpp>\n+#include <NvInfer.h>\n+\n+#include <google/protobuf/text_format.h>\n+#include <iostream>\n+#include <unordered_set>\n+\n+namespace caffe2 {\n+\n+namespace {\n+\n+// TODO(yinghai): Remove the awkward conversion between unordered_map and map\n+std::unordered_map<std::string, TensorShape> InferShapes(\n+    NetDef* init_net,\n+    NetDef* pred_net,\n+    const std::unordered_map<std::string, TensorShape>& input_shape_hints) {\n+  CaffeMap<std::string, TensorShape> shape_hints_ordered;\n+  for (const auto& kv : input_shape_hints) {\n+    shape_hints_ordered.emplace(kv.first, kv.second);\n+  }\n+  std::vector<std::unique_ptr<NetDef>> nets;\n+  nets.emplace_back(init_net);\n+  nets.emplace_back(pred_net);\n+  InferBlobShapesAndTypes(shape_hints_ordered, nets);\n+  for (auto& net : nets) {\n+    net.release();\n+  }\n+  std::unordered_map<std::string, TensorShape> shape_hints;\n+  for (const auto& kv : shape_hints_ordered) {\n+    shape_hints.emplace(kv.first, kv.second);\n+  }\n+\n+  return shape_hints;\n+}\n+\n+std::vector<std::string> FigureInputs(\n+    const NetDef& pred_net,\n+    int start,\n+    int end,\n+    const std::vector<OperatorDef>& new_ops,\n+    const std::unordered_set<std::string>& weights,\n+    std::unordered_set<std::string>* initialization_list) {\n+  // TODO: cache this\n+  std::unordered_set<std::string> boundary_inputs;\n+  for (const auto& i : pred_net.external_input()) {\n+    boundary_inputs.emplace(i);\n+  }\n+\n+  //\n+  for (const auto& op : new_ops) {\n+    for (const auto& output: op.output()) {\n+      boundary_inputs.emplace(output);\n+    }\n+  }\n+\n+  std::unordered_set<std::string> total_inputs;\n+  std::vector<std::string> total_inputs_vec;\n+  for(int i = start; i < end; ++i) {\n+    const auto& op = pred_net.op(i);\n+    for (const auto& input: op.input()) {\n+      auto rt = total_inputs.emplace(input);\n+      if (rt.second) {\n+        if (weights.count(input)) {\n+          // We add weights as inputs too\n+          total_inputs_vec.emplace_back(input);\n+          initialization_list->emplace(input);\n+        } else if (boundary_inputs.count(input)) {\n+          LOG(INFO) << \"Adding boundary input: \" << input;\n+          total_inputs_vec.emplace_back(input);\n+        }\n+      }\n+    }\n+  }\n+  return total_inputs_vec;\n+}\n+\n+std::vector<std::string>\n+FigureOutputs(const NetDef& pred_net, int start, int end) {\n+  std::unordered_set<std::string> all_outputs;\n+  std::vector<std::string> all_outputs_vec;\n+  std::unordered_set<std::string> ext_outputs;\n+  for (const auto& e: pred_net.external_output()) {\n+    ext_outputs.emplace(e);\n+  }\n+  for (int i = start; i < end; ++i) {\n+    const auto& op = pred_net.op(i);\n+    for (const auto& output: op.output()) {\n+      all_outputs.emplace(output);\n+      all_outputs_vec.emplace_back(output);\n+    }\n+  }\n+  std::unordered_set<std::string> referred_inputs;\n+  for (int i = end; i < pred_net.op_size(); ++i) {\n+    const auto& op = pred_net.op(i);\n+    for (const auto& input: op.input()) {\n+      referred_inputs.emplace(input);\n+    }\n+  }\n+  // Remove the output that is\n+  // 1. Not referred by the subsequent ops\n+  // 2. Not in the external ouput of the net\n+  all_outputs_vec.erase(\n+      std::remove_if(\n+          all_outputs_vec.begin(),\n+          all_outputs_vec.end(),\n+          [&ext_outputs, &referred_inputs](const std::string& output) {\n+            return (\n+                !referred_inputs.count(output) && !ext_outputs.count(output));\n+          }),\n+      all_outputs_vec.end());\n+  return all_outputs_vec;\n+}\n+\n+std::vector<::ONNX_NAMESPACE::ValueInfoProto> ConvertToValueInfo(\n+    const std::vector<std::string>& names,\n+    const std::unordered_map<std::string, TensorShape>& shape_hints) {\n+  std::vector<::ONNX_NAMESPACE::ValueInfoProto> r;\n+  for (const auto& s: names) {\n+    r.emplace_back();\n+    auto& value_info = r.back();\n+    value_info.set_name(s);\n+    const auto it = shape_hints.find(s);\n+    if (it == shape_hints.end()) {\n+      LOG(WARNING) << \"Cannot get shape of \" << s;\n+    } else {\n+      auto* tensor_type = value_info.mutable_type()->mutable_tensor_type();\n+      tensor_type->set_elem_type(\n+          ::ONNX_NAMESPACE::TensorProto_DataType::TensorProto_DataType_FLOAT);\n+      auto* shape = tensor_type->mutable_shape();\n+      for (int i = 0 ; i < it->second.dims().size(); ++i) {\n+        auto* dim = shape->add_dim();\n+        dim->set_dim_value(it->second.dims(i));\n+      }\n+    }\n+  }\n+  return r;\n+}\n+\n+void PruneWeights(const NetDef& pred_net, NetDef* init_net) {\n+  std::unordered_set<std::string> used_weights;\n+  for (const auto& op: pred_net.op()) {\n+    for (const auto& i: op.input()) {\n+      used_weights.emplace(i);\n+    }\n+  }\n+\n+  int last = init_net->op_size();\n+  for (int i = 0; i < last;) {\n+    if (!used_weights.count(init_net->op(i).output(0))) {\n+      if (i != last - 1) {\n+        init_net->mutable_op()->SwapElements(i, last - 1);\n+      } else {\n+        ++i;\n+      }\n+      --last;\n+    } else {\n+      ++i;\n+    }\n+  }\n+\n+  if (last < init_net->op_size()) {\n+    init_net->mutable_op()->DeleteSubrange(last, init_net->op_size() - last);\n+  }\n+\n+  LOG(INFO) << \"New init_net op size: \" << init_net->op_size();\n+}\n+\n+void FillModelInfo(::ONNX_NAMESPACE::ModelProto* model) {\n+  model->set_ir_version(::ONNX_NAMESPACE::Version::IR_VERSION);\n+  model->set_producer_name(\"caffe2\");\n+  auto* opset_id = model->add_opset_import();\n+  opset_id->set_domain(\"\");\n+  opset_id->set_version(3);\n+}\n+} // namespace\n+\n+OperatorDef TensorRTTransformer::BuildTrtOp(\n+    const std::string& onnx_model_str,\n+    const std::unordered_map<std::string, std::vector<int>>&\n+        output_size_hints) {\n+  OperatorDef op;\n+  op.set_type(\"TensorRT\");\n+\n+  TrtLogger logger;\n+  auto trt_builder = InferObject(nvinfer1::createInferBuilder(logger));\n+  auto trt_network = InferObject(trt_builder->createNetwork());\n+  auto importer = InferObject(onnx2trt::createImporter(trt_network.get()));\n+  auto status = importer->import(onnx_model_str.data(), onnx_model_str.size(), false);\n+  if (status.is_error()) {\n+    CAFFE_THROW(MakeString(\n+        \"TensorRTTransformer ERROR: \",\n+        status.file(),\n+        \":\",\n+        status.line(),\n+        \" In function \",\n+        status.func(),\n+        \":\\n\",\n+        \"[\",\n+        status.code(),\n+        \"] \",\n+        status.desc()));\n+  }\n+  trt_builder->setMaxBatchSize(max_batch_size_);\n+  trt_builder->setMaxWorkspaceSize(max_workspace_size_);\n+  trt_builder->setDebugSync(debug_builder_);\n+  auto trt_engine =\n+      InferObject(trt_builder->buildCudaEngine(*trt_network.get()));\n+\n+  // Set up inputs/outputs\n+  int num_bindings = trt_engine->getNbBindings();\n+  for (int b = 0; b < num_bindings; ++b) {\n+    const auto& name = trt_engine->getBindingName(b);\n+    if (trt_engine->bindingIsInput(b)) {\n+      op.add_input(name);\n+    } else {\n+      op.add_output(name);\n+    }\n+  }\n+\n+  auto engine_plan = InferObject(trt_engine->serialize());\n+\n+  auto* serialized_engine_arg = op.add_arg();\n+  serialized_engine_arg->set_s(\"\");\n+  serialized_engine_arg->set_name(\"serialized_engine\");\n+  auto* s = serialized_engine_arg->mutable_s();\n+  s->assign((char*)engine_plan->data(), engine_plan->size());\n+\n+  auto* max_batch_size_arg = op.add_arg();\n+  max_batch_size_arg->set_name(\"max_batch_size\");\n+  max_batch_size_arg->set_i(max_batch_size_);\n+\n+  auto* verbosity_arg = op.add_arg();\n+  verbosity_arg->set_name(\"log_verbosity\");\n+  verbosity_arg->set_i(verbosity_);\n+\n+  auto* output_size_hints_arg = op.add_arg();\n+  auto* output_size_names_arg = op.add_arg();\n+  output_size_hints_arg->set_name(\"output_size_hints\");\n+  output_size_names_arg->set_name(\"output_size_names\");\n+  for(const auto& o: op.output()) {\n+    const auto it = output_size_hints.find(o);\n+    if (it != output_size_hints.end()) {\n+      const auto& dims = it->second;\n+      for (const auto& i : dims) {\n+        output_size_hints_arg->add_ints(i);\n+      }\n+      // Add an extra -1 to indicate the end\n+      output_size_hints_arg->add_ints(-1);\n+      output_size_names_arg->add_strings(o);\n+      LOG(INFO) << \"Adding output hint: \" << o;\n+    }\n+  }\n+\n+  return op;\n+}\n+\n+void TensorRTTransformer::ClusterToTrtOp(\n+    const NetDef& init_net,\n+    const NetDef& pred_net,\n+    int start,\n+    int end,\n+    const std::unordered_set<std::string>& weights,\n+    const std::unordered_map<std::string, TensorShape>& shape_hints,\n+    ::ONNX_NAMESPACE::ModelProto* model,\n+    std::vector<OperatorDef>* new_ops) {\n+  if (model->graph().node_size() == 0) {\n+    return;\n+  }\n+  model->mutable_graph()->clear_input();\n+  model->mutable_graph()->clear_output();\n+  model->mutable_graph()->clear_initializer();\n+\n+  // Figure out the boundary outputs\n+  auto outputs = ConvertToValueInfo(FigureOutputs(pred_net, start, end), shape_hints);\n+  std::unordered_map<std::string, std::vector<int>> output_shape_hints;\n+  for (const auto& i: outputs) {\n+    model->mutable_graph()->add_output()->CopyFrom(i);\n+    auto ret = output_shape_hints.emplace(i.name(), std::vector<int>());\n+    auto& vec = ret.first->second;\n+    const auto it = shape_hints.find(i.name());\n+    CAFFE_ENFORCE(it != shape_hints.end(), \"Cannot find shape info for output \", i.name());\n+    const auto& shape = it->second;\n+    for (int k = 0; k < shape.dims().size(); ++k) {\n+      vec.push_back(shape.dims(k));\n+    }\n+  }\n+\n+  // Figure out the boundary inputs\n+  std::unordered_set<std::string> initialization_list;\n+  auto total_inputs_vec = FigureInputs(pred_net, start, end, *new_ops, weights, &initialization_list);\n+  auto inputs = ConvertToValueInfo(total_inputs_vec, shape_hints);\n+  for (const auto& i: inputs) {\n+    LOG(INFO) << \"Added input: \" << i.name();\n+    model->mutable_graph()->add_input()->CopyFrom(i);\n+  }\n+\n+  // Convert weights to initializing tensors\n+  onnx::OnnxExporter exporter;\n+  for (const auto& op: init_net.op()) {\n+    CAFFE_ENFORCE(op.output_size() == 1);\n+    auto it = initialization_list.find(op.output(0));\n+    if (it != initialization_list.end()) {\n+      auto* init_tensor = model->mutable_graph()->add_initializer();\n+      exporter.InitOpToTensorProto(op, init_tensor);\n+      initialization_list.erase(it);\n+    }\n+  }\n+  CAFFE_ENFORCE(initialization_list.empty(), \"Unfulfilled initialization list\");\n+  for (const auto& t: model->graph().initializer()) {\n+    LOG(INFO) << \"Initializer: \" << t.name();\n+  }\n+\n+  // Onnx model is ready. Call onnx-trt to convert to one trt c2 op\n+  std::string model_str;\n+  model->SerializeToString(&model_str);\n+  new_ops->emplace_back(BuildTrtOp(model_str, output_shape_hints));\n+\n+  model->mutable_graph()->clear_node();\n+}\n+\n+// Cutting off the runnable part and replace with tensor ops. Asssume the nets\n+// were topologically sorted\n+void TensorRTTransformer::Transform(\n+    NetDef* init_net,\n+    NetDef* pred_net,\n+    const std::unordered_map<std::string, TensorShape>& input_shape_hints) {\n+  const auto shape_hints = InferShapes(init_net, pred_net, input_shape_hints);\n+\n+  std::unordered_set<std::string> weights;\n+  if (init_net) {\n+    for (const auto& op : init_net->op()) {\n+      CAFFE_ENFORCE(op.type().find(\"GivenTensor\") == 0);\n+      CAFFE_ENFORCE(op.type().rfind(\"Fill\") == op.type().size() - 4);\n+      CAFFE_ENFORCE(op.output_size() == 1);\n+      for (const auto& op_output : op.output()) {\n+        weights.emplace(op_output);\n+      }\n+    }\n+  }\n+\n+  CAFFE_ENFORCE(pred_net, \"pred_net cannot be nullptr\");\n+  ::ONNX_NAMESPACE::ModelProto onnx_model;\n+  FillModelInfo(&onnx_model);\n+\n+  std::vector<OperatorDef> new_ops;\n+  bool trt_group = false;\n+  auto importer = InferObject(onnx2trt::createImporter(nullptr));\n+  int op_idx = 0;\n+  int start = 0;\n+  int end = 0;\n+  for (const OperatorDef& op : pred_net->op()) {\n+    bool support_trt = true;\n+    const OpSchema* schema = OpSchemaRegistry::Schema(op.type());\n+    caffe2::onnx::ConvertedResult results;\n+    if (!schema or schema->onnx_schema().empty()) {\n+      LOG(INFO) << \"Cannot export c2 op \" << op.type() << \" to onnx\";\n+      support_trt = false;\n+    } else {\n+      // One c2 op can be converted into multiple onnx nodes. For simplicity, we\n+      // enforce all or nothing here\n+      results = onnx::OnnxExporter().Caffe2OpToOnnxNodes(op, shape_hints);", "path": "caffe2/contrib/tensorrt/tensorrt_tranformer.cc", "position": null, "original_position": 366, "commit_id": "66758cc2919a2a6add56c6bcda8e33f50c94044b", "original_commit_id": "1051f2359f3ec0a4d7c28794d4a3ea67360bcd3d", "user": {"login": "yinghai", "id": 1100089, "node_id": "MDQ6VXNlcjExMDAwODk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1100089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yinghai", "html_url": "https://github.com/yinghai", "followers_url": "https://api.github.com/users/yinghai/followers", "following_url": "https://api.github.com/users/yinghai/following{/other_user}", "gists_url": "https://api.github.com/users/yinghai/gists{/gist_id}", "starred_url": "https://api.github.com/users/yinghai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yinghai/subscriptions", "organizations_url": "https://api.github.com/users/yinghai/orgs", "repos_url": "https://api.github.com/users/yinghai/repos", "events_url": "https://api.github.com/users/yinghai/events{/privacy}", "received_events_url": "https://api.github.com/users/yinghai/received_events", "type": "User", "site_admin": false}, "body": "It doesn't. ", "created_at": "2018-04-02T22:33:18Z", "updated_at": "2018-11-23T15:41:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/6150#discussion_r178669000", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/6150", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/178669000"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/6150#discussion_r178669000"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/6150"}}, "body_html": "<p>It doesn't.</p>", "body_text": "It doesn't.", "in_reply_to_id": 178607116}