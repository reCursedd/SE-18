{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157925010", "pull_request_review_id": 84649026, "id": 157925010, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1NzkyNTAxMA==", "diff_hunk": "@@ -1,471 +0,0 @@\n-from numbers import Integral\n-import torch\n-from torch.autograd.function import Function\n-from torch._thnn import type2backend\n-\n-from . import _all_functions\n-from ...modules.utils import _single, _pair, _triple\n-\n-import warnings\n-\n-\n-def _check_size_scale_factor(size, scale_factor):\n-    if size is None and scale_factor is None:\n-        raise ValueError('either size or scale_factor should be defined')\n-    if scale_factor is not None and not isinstance(scale_factor, (Integral, tuple)):\n-        raise ValueError('scale_factor must be of integer type or a tuple of integer types')\n-\n-\n-def _check_linear_scale_factor(scale_factor, dim=2):\n-    if dim == 1:\n-        scale_factor = _single(scale_factor)\n-    elif dim == 2:\n-        scale_factor = _pair(scale_factor)\n-    elif dim == 3:\n-        scale_factor = _triple(scale_factor)\n-    else:\n-        raise ValueError(\"dim has to be 1, 2 or 3\")\n-\n-    try:\n-        assert len(scale_factor) == 1 or len(scale_factor) == 2 or len(scale_factor) == 3\n-        assert all(isinstance(s, Integral) and s >= 1 for s in scale_factor)\n-    except AssertionError as e:\n-        raise ValueError('scale_factor must be a non-negative integer, '\n-                         'or a tuple of non-negative integers for linear, bilinear and trilinear upsampling, but got: '\n-                         '{}'.format(scale_factor))\n-    return scale_factor\n-\n-\n-class UpsamplingNearest1d(Function):\n-\n-    @staticmethod\n-    def forward(ctx, input, size=None, scale_factor=None):\n-        assert input.dim() == 3\n-\n-        _check_size_scale_factor(size, scale_factor)\n-\n-        ctx.size = size\n-        ctx.scale_factor = scale_factor\n-\n-        if ctx.scale_factor is not None and not isinstance(ctx.scale_factor, Integral):\n-            raise ValueError('scale_factor must be a single Integer value for nearest neighbor sampling')\n-\n-        if ctx.scale_factor is None:\n-            if (ctx.size[0] % input.size(2) != 0):\n-                raise RuntimeError(\"output size specified in UpsamplingNearest \"\n-                                   \"({}) has to be divisible by the input size, but got: \"\n-                                   \"{}\".format('x'.join(map(str, ctx.size)),\n-                                               'x'.join(map(str, input.size()))))\n-            ctx.scale_factor = ctx.size[0] // input.size(2)\n-\n-        output = input.new()\n-        backend = type2backend[type(input)]\n-        ctx.save_for_backward(input)\n-        backend.TemporalUpSamplingNearest_updateOutput(\n-            backend.library_state,\n-            input,\n-            output,\n-            ctx.scale_factor\n-        )\n-        return output\n-\n-    @staticmethod\n-    def backward(ctx, grad_output):\n-        input, = ctx.saved_variables\n-        grad_input = UpsamplingNearest1dBackward.apply(input, grad_output, ctx.scale_factor)\n-        return grad_input, None, None\n-\n-\n-class UpsamplingNearest1dBackward(Function):\n-\n-    @staticmethod\n-    def forward(ctx, input, grad_output, scale_factor):\n-        assert grad_output.dim() == 3\n-        ctx.scale_factor = scale_factor\n-\n-        grad_input = grad_output.new()\n-        backend = type2backend[type(input)]\n-        backend.TemporalUpSamplingNearest_updateGradInput(\n-            backend.library_state,\n-            input,\n-            grad_output,\n-            grad_input,\n-            ctx.scale_factor\n-        )\n-        return grad_input\n-\n-    @staticmethod\n-    def backward(ctx, ggI):\n-        gI = None\n-        ggO = UpsamplingNearest1d.apply(ggI, None, ctx.scale_factor)\n-\n-        return gI, ggO, None\n-\n-\n-class UpsamplingLinear1d(Function):\n-\n-    @staticmethod\n-    def forward(ctx, input, size=None, scale_factor=None):\n-        assert input.dim() == 3\n-\n-        ctx.size = size\n-        ctx.scale_factor = scale_factor\n-\n-        if ctx.scale_factor is not None:\n-            ctx.scale_factor = _check_linear_scale_factor(ctx.scale_factor, dim=1)\n-\n-        if ctx.scale_factor is not None:\n-            ctx.output_size = (\n-                input.size(2) * ctx.scale_factor[0],\n-            )\n-        else:\n-            ctx.output_size = ctx.size\n-\n-        ctx.input_size = input.size()\n-        output = input.new()\n-        backend = type2backend[type(input)]\n-        backend.TemporalUpSamplingLinear_updateOutput(\n-            backend.library_state,\n-            input,\n-            output,\n-            ctx.output_size[0]\n-        )\n-        return output\n-\n-    @staticmethod\n-    def backward(ctx, grad_output):\n-        grad_input = UpsamplingLinear1dBackward.apply(grad_output, ctx.input_size, ctx.output_size)\n-        return grad_input, None, None\n-\n-\n-class UpsamplingLinear1dBackward(Function):\n-\n-    @staticmethod\n-    def forward(ctx, grad_output, input_size, output_size):\n-        assert grad_output.dim() == 3\n-\n-        ctx.input_size = input_size\n-        ctx.output_size = output_size\n-\n-        grad_output = grad_output.contiguous()\n-        grad_input = grad_output.new()\n-        backend = type2backend[type(grad_output)]\n-        backend.TemporalUpSamplingLinear_updateGradInput(\n-            backend.library_state,\n-            grad_output,\n-            grad_input,\n-            ctx.input_size[0],\n-            ctx.input_size[1],\n-            ctx.input_size[2],\n-            ctx.output_size[0],\n-        )\n-        return grad_input\n-\n-    @staticmethod\n-    def backward(ctx, ggI):\n-        ggO = UpsamplingLinear1d.apply(ggI, ctx.output_size, None)\n-\n-        return ggO, None, None\n-\n-\n-class UpsamplingNearest2d(Function):\n-\n-    @staticmethod\n-    def symbolic(g, input, size=None, scale_factor=None):", "path": "torch/nn/_functions/thnn/upsampling.py", "position": 174, "original_position": 174, "commit_id": "8d2c4956e2eb3e6ec413b82d49f726e596d0fb24", "original_commit_id": "d01b7e248b38f16e83607d5e6cfada20bba4e2b8", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "This needs to get ported over to torch/onnx/symbolic.py", "created_at": "2017-12-20T02:26:05Z", "updated_at": "2018-11-23T15:37:32Z", "html_url": "https://github.com/pytorch/pytorch/pull/4264#discussion_r157925010", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4264", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/157925010"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4264#discussion_r157925010"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4264"}}, "body_html": "<p>This needs to get ported over to torch/onnx/symbolic.py</p>", "body_text": "This needs to get ported over to torch/onnx/symbolic.py"}