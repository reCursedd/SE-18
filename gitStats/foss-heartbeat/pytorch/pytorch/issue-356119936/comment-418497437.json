{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/418497437", "html_url": "https://github.com/pytorch/pytorch/pull/11159#issuecomment-418497437", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11159", "id": 418497437, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODQ5NzQzNw==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-04T19:56:00Z", "updated_at": "2018-09-04T19:56:00Z", "author_association": "MEMBER", "body_html": "<p>Reposting from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"351382946\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10594\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/10594/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/10594\">#10594</a>, since this will probably get more visibility here.</p>\n<p>Do we really believe that checked in tests are a good idea? I'm pretty sure that having more rigorous test harness will provide much better coverage, and have a benefit of not checking in a new file after every single operator change. Here's how I see it:</p>\n<p>Operator code is generally quite self-contained, so it's unlikely that a change in one part of the system will affect a random operator on the other end. Unless it's a breakage so bad that most operator tests will be red anyway. On the other hand, if someone changes the operator kernels, the tiny numerical differences might change e.g. the few least-significant bits, raising a failure when compared to the expected test. Since this was a PR that updates an op, I believe the author will be more than happy to check in a new set of expected outputs, meaning that those values don't mean much. Finally, you can't test all ops this way, because if you use an external library, just updating it to a new version (or having a different cuDNN version installed) might result in those tiny differences, flaring up the whole test suite for no good reason.</p>\n<p>We actually took a similar approach for testing the PyTorch JIT some time ago (we serialize IR as text, not binary tensor values), but it has many scalability problems, and is notoriously hard to review. We already had many situations where the test had a good expect file checked in, only to have it overwritten with a non-working one in a later PR (and we do pretty careful code review). After the release, we'll probably try to move away from comparing to expected values, and you might want to consider that too.</p>", "body_text": "Reposting from #10594, since this will probably get more visibility here.\nDo we really believe that checked in tests are a good idea? I'm pretty sure that having more rigorous test harness will provide much better coverage, and have a benefit of not checking in a new file after every single operator change. Here's how I see it:\nOperator code is generally quite self-contained, so it's unlikely that a change in one part of the system will affect a random operator on the other end. Unless it's a breakage so bad that most operator tests will be red anyway. On the other hand, if someone changes the operator kernels, the tiny numerical differences might change e.g. the few least-significant bits, raising a failure when compared to the expected test. Since this was a PR that updates an op, I believe the author will be more than happy to check in a new set of expected outputs, meaning that those values don't mean much. Finally, you can't test all ops this way, because if you use an external library, just updating it to a new version (or having a different cuDNN version installed) might result in those tiny differences, flaring up the whole test suite for no good reason.\nWe actually took a similar approach for testing the PyTorch JIT some time ago (we serialize IR as text, not binary tensor values), but it has many scalability problems, and is notoriously hard to review. We already had many situations where the test had a good expect file checked in, only to have it overwritten with a non-working one in a later PR (and we do pretty careful code review). After the release, we'll probably try to move away from comparing to expected values, and you might want to consider that too.", "body": "Reposting from #10594, since this will probably get more visibility here.\r\n\r\nDo we really believe that checked in tests are a good idea? I'm pretty sure that having more rigorous test harness will provide much better coverage, and have a benefit of not checking in a new file after every single operator change. Here's how I see it:\r\n\r\nOperator code is generally quite self-contained, so it's unlikely that a change in one part of the system will affect a random operator on the other end. Unless it's a breakage so bad that most operator tests will be red anyway. On the other hand, if someone changes the operator kernels, the tiny numerical differences might change e.g. the few least-significant bits, raising a failure when compared to the expected test. Since this was a PR that updates an op, I believe the author will be more than happy to check in a new set of expected outputs, meaning that those values don't mean much. Finally, you can't test all ops this way, because if you use an external library, just updating it to a new version (or having a different cuDNN version installed) might result in those tiny differences, flaring up the whole test suite for no good reason.\r\n\r\nWe actually took a similar approach for testing the PyTorch JIT some time ago (we serialize IR as text, not binary tensor values), but it has many scalability problems, and is notoriously hard to review. We already had many situations where the test had a good expect file checked in, only to have it overwritten with a non-working one in a later PR (and we do pretty careful code review). After the release, we'll probably try to move away from comparing to expected values, and you might want to consider that too."}