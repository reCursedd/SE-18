{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/418507839", "html_url": "https://github.com/pytorch/pytorch/pull/11159#issuecomment-418507839", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11159", "id": 418507839, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODUwNzgzOQ==", "user": {"login": "ajyu", "id": 1071670, "node_id": "MDQ6VXNlcjEwNzE2NzA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1071670?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ajyu", "html_url": "https://github.com/ajyu", "followers_url": "https://api.github.com/users/ajyu/followers", "following_url": "https://api.github.com/users/ajyu/following{/other_user}", "gists_url": "https://api.github.com/users/ajyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/ajyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ajyu/subscriptions", "organizations_url": "https://api.github.com/users/ajyu/orgs", "repos_url": "https://api.github.com/users/ajyu/repos", "events_url": "https://api.github.com/users/ajyu/events{/privacy}", "received_events_url": "https://api.github.com/users/ajyu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-04T20:31:37Z", "updated_at": "2018-09-04T20:31:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, the motivation for these tests mainly comes from a past sev where changes to operator semantics make an operator not backward compatible. There is some more context in this <a href=\"https://fb.quip.com/tVg1AImdjoKY\" rel=\"nofollow\">quip</a>. While we recognize that implementation differences may cause small differences when compared to an expected test, we think that having such a mechanism will help  us better catch more egregious cases. Backwards incompatible changes are otherwise hard to catch in unit tests since code modifiers would typically change matching tests to pass when making a semantically different change.</p>\n<p>One possibility is strongly relaxing the tolerance requirements to hopefully prevent failures due to the subtlest implementation updates.</p>\n<p>Other things that we can do that are more ambitious would be to track models run in production and track some serialized inputs matched with either outputs or higher level aggregate statistics which shouldn't differ widely with code change.</p>", "body_text": "@apaszke, the motivation for these tests mainly comes from a past sev where changes to operator semantics make an operator not backward compatible. There is some more context in this quip. While we recognize that implementation differences may cause small differences when compared to an expected test, we think that having such a mechanism will help  us better catch more egregious cases. Backwards incompatible changes are otherwise hard to catch in unit tests since code modifiers would typically change matching tests to pass when making a semantically different change.\nOne possibility is strongly relaxing the tolerance requirements to hopefully prevent failures due to the subtlest implementation updates.\nOther things that we can do that are more ambitious would be to track models run in production and track some serialized inputs matched with either outputs or higher level aggregate statistics which shouldn't differ widely with code change.", "body": "@apaszke, the motivation for these tests mainly comes from a past sev where changes to operator semantics make an operator not backward compatible. There is some more context in this [quip](https://fb.quip.com/tVg1AImdjoKY). While we recognize that implementation differences may cause small differences when compared to an expected test, we think that having such a mechanism will help  us better catch more egregious cases. Backwards incompatible changes are otherwise hard to catch in unit tests since code modifiers would typically change matching tests to pass when making a semantically different change.\r\n\r\nOne possibility is strongly relaxing the tolerance requirements to hopefully prevent failures due to the subtlest implementation updates.\r\n\r\nOther things that we can do that are more ambitious would be to track models run in production and track some serialized inputs matched with either outputs or higher level aggregate statistics which shouldn't differ widely with code change."}