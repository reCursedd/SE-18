{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/228263233", "pull_request_review_id": 168496458, "id": 228263233, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyODI2MzIzMw==", "diff_hunk": "@@ -0,0 +1,397 @@\n+#include \"torch/csrc/jit/fuser/codegen.h\"\n+\n+#include \"ATen/ATen.h\"\n+#include \"torch/csrc/jit/code_template.h\"\n+#include \"torch/csrc/jit/ir.h\"\n+#include \"torch/csrc/jit/assertions.h\"\n+#include \"torch/csrc/jit/fuser/config.h\"\n+#include \"torch/csrc/jit/fuser/interface.h\"\n+#include \"torch/csrc/jit/fuser/tensor_info.h\"\n+\n+#if USE_CUDA_FUSER\n+  #include \"torch/csrc/jit/fuser/cuda/resource_strings.h\"\n+#endif \n+\n+#if USE_CPU_FUSER\n+  #include \"torch/csrc/jit/fuser/cpu/resource_strings.h\"\n+#endif \n+\n+#include <tuple>\n+#include <iostream>\n+#include <sstream>\n+#include <cstdint>\n+#include <vector>\n+#include <cmath>\n+\n+namespace torch { namespace jit { namespace fuser {\n+\n+// Template for computing the offset into the tensor to access a value\n+static auto dim_calc = CodeTemplate(R\"(\n+//printf(\"tensor ${tensor} sizes[${d}] = %d, strides[${d}] = %d\\n\", ${tensor}.sizes[${d}],${tensor}.strides[${d}]);\n+size_t ${tensor}_dimIndex${d} = ${tensor}_linearIndex ${mod_sizes};\n+${tensor}_offset += ${tensor}_dimIndex${d} ${times_stride};\n+)\");\n+\n+\n+static std::string valueName(const Value* n) {\n+  return \"n\" + std::to_string(n->unique());\n+}\n+\n+static std::string scalarValue(const int64_t v) {\n+  return std::to_string(v);\n+}\n+\n+// Note: The NAN, NEG_INFINITY and POS_INFINITY strings map to device-specific\n+// implementations of these special values. These macros are found in the \n+// resource strings for each device.\n+static std::string scalarValue(const double v) {\n+  std::ostringstream out;\n+  if (std::isnan(v)) {\n+    out << \"NAN\";\n+  } else if (std::isinf(v)) {\n+    if (v < 0) {\n+      out << \"NEG_INFINITY\";\n+    } else {\n+      out << \"POS_INFINITY\";\n+    }\n+  } else {\n+    out << std::scientific << v << \"f\";\n+  }\n+  return out.str();\n+}\n+\n+// Note: Half is special-cased to avoid returning at::Half\n+static const char* scalarTypeName(const at::ScalarType type) {\n+  if (type == at::ScalarType::Half) {\n+    return \"half\";\n+  }\n+\n+  switch(type) {\n+    #define DEFINE_CASE(ctype,name,_) \\\n+      case at::ScalarType::name: return #ctype;\n+    AT_FORALL_SCALAR_TYPES_EXCEPT_HALF(DEFINE_CASE)\n+    #undef DEFINE_CASE\n+    default:\n+      throw std::runtime_error(\"unknown scalar type\");\n+  }\n+}\n+\n+// Writes \"simple mappable\" ops\n+static std::string encodeRHS(const Node* n) {\n+  static std::unordered_map<NodeKind, std::string> simple_map_ops = {\n+    // unary\n+    {aten::abs, \"absf(${0})\"},\n+    {aten::sigmoid, \"1.f / (1.f + expf(-${0}))\"},\n+    {aten::relu, \"${0} < 0 ? 0.f : ${0} \"},\n+    {aten::log, \"logf(${0})\"},\n+    {aten::log10, \"log10f(${0})\"},\n+    {aten::log1p, \"log1pf(${0})\"},\n+    {aten::log2,  \"log2f(${0})\"},\n+    {aten::lgamma, \"lgammaf(${0})\"},\n+    {aten::exp, \"expf(${0})\"},\n+    {aten::expm1, \"expm1f(${0})\"},\n+    {aten::cos, \"cosf(${0})\"},\n+    {aten::acos, \"acosf(${0})\"},\n+    {aten::cosh, \"coshf(${0})\"},\n+    {aten::sin, \"sinf(${0})\"},\n+    {aten::asin, \"asinf(${0})\"},\n+    {aten::sinh, \"sinhf(${0})\"},\n+    {aten::tan, \"tanf(${0})\"},\n+    {aten::atan, \"atanf(${0})\"},\n+    {aten::tanh, \"tanhf(${0})\"},\n+    {aten::sqrt, \"sqrtf(${0})\"},\n+    {aten::rsqrt, \"rsqrtf(${0})\"},\n+    {aten::ceil, \"ceilf(${0})\"},\n+    {aten::floor, \"floorf(${0})\"},\n+    {aten::round, \"roundf(${0})\"},\n+    {aten::trunc, \"truncf(${0})\"},\n+    {aten::frac, \"fracf(${0})\"},\n+    {aten::reciprocal, \"reciprocalf(${0})\"},\n+    {aten::neg, \"-${0}\"},\n+    //simple binary\n+    {aten::atan2, \"atan2(${0}, ${1})\"},\n+    {aten::min, \"fminf(${0}, ${1})\"},\n+    {aten::max, \"fmaxf(${0}, ${1})\"},\n+\n+    //binary with other\n+    // TODO: some of these ops will not get generated because\n+    // we only work on float inputs/outputs, but they are here to record\n+    // that they are valid mappable ops once we handle more type\n+\n+    {aten::__and__, \"${0} && ${1}\"},\n+    {aten::__lshift__, \"${0} << ${1}\"},\n+    {aten::__or__, \"${0} || ${1}\"},\n+    {aten::__rshift__, \"${0} >> ${1}\"},\n+    {aten::__xor__, \"${0} ^ ${1}\"},\n+    {aten::div, \"${0} / ${1}\"},\n+    {aten::eq, \"${0} == ${1}\"},\n+    {aten::fmod, \"fmodf(${0}, ${1})\"},\n+    {aten::ge, \"(${0} >= ${1})\"},\n+    {aten::gt, \"${0} > ${1}\"},\n+    {aten::le, \"(${0} <= ${1})\"},\n+    {aten::lt, \"${0} < ${1}\"},\n+    {aten::type_as, \"(${0})\"}, //everything is implicitly convertible to float\n+    {aten::mul, \"${0} * ${1}\"},\n+    {aten::ne, \"${0} != ${1}\"},\n+    {aten::remainder, \"remainderf(${0}, ${1})\"},\n+    {aten::pow, \"powf(${0}, ${1})\"},\n+\n+    //alpha\n+    {aten::add, \"${0} + ${2}*${1}\"},\n+    {aten::sub, \"(${0} - ${2}*${1})\"},\n+    {aten::rand_like, \"uniform(rnd())\"},\n+\n+    // min, max\n+    // It may seem unusual to have the bounds as the first case below,\n+    // this is so that if min or max is NaN, they are \"ignored\"\n+    // and when the input is NaN, the output is, too\n+    {aten::clamp, \"(${0}<${1}?${1}:(${0}>${2}?${2}:${0}))\"},\n+\n+    // simple derivatives\n+    {aten::_sigmoid_backward, \"${0} * ${1} * (1.f - ${1})\"},\n+    {aten::_tanh_backward,    \"${0} * (1.f - ${1} * ${1})\"},\n+  };\n+\n+  if (n->kind() == prim::Constant) {\n+    const auto val = toIValue(n->output()).value();\n+    if (val.isDouble()) {\n+      return scalarValue(val.toDouble());\n+    } else {\n+      JIT_ASSERT(val.isInt());\n+      return scalarValue(val.toInt());\n+    }\n+  }\n+\n+  TemplateEnv env;\n+  size_t i = 0;\n+  for(auto in : n->inputs()) {\n+    env.s(std::to_string(i++), valueName(in));\n+  }\n+\n+  const auto & str = simple_map_ops.at(n->kind());\n+  return format(str, env);\n+}\n+\n+// If there is a single user of a node and it's a chunk operation, returns\n+//  that user. Returns nullptr otherwise.\n+static Node* usedInFusedChunk(const Value* input) {\n+  auto uses = input->uses();\n+  if (uses.size() == 1) {\n+    Node *user = uses[0].user;\n+    if (user->kind() == prim::ConstantChunk) {\n+      return user;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n+static void emitIndexingFor(\n+  std::ostream& out\n+, const std::string& tensor\n+, const int ndim\n+, const bool last_is_cont) {\n+  TemplateEnv env;\n+  env.s(\"tensor\",tensor);\n+  out << format(\"IndexType ${tensor}_offset = 0;\\n\",env);\n+  out << format(\"IndexType ${tensor}_linearIndex = linearIndex;\\n\",env);\n+  for (int d = ndim - 1; d >= 0; --d) {\n+    env.d(\"d\",d);\n+    env.s(\"mod_sizes\", d > 0 ? format(\"% ${tensor}.sizes[${d}]\",env) : \"\");\n+    env.s(\"times_stride\",(d < ndim - 1 || !last_is_cont) ?\n+      format(\"* ${tensor}.strides[${d}]\",env) : \"\");\n+    out << dim_calc.format(env);\n+    if (d > 0) {\n+      out << format(\"${tensor}_linearIndex /= ${tensor}.sizes[${d}];\\n\",env);\n+    }\n+  }\n+}\n+\n+// TODO: handle cases where we need to generate > 2^32 element tensors\n+std::tuple<\n+  std::string\n+, std::vector<PartitionDesc>\n+, std::vector<PartitionDesc>\n+, bool> generateKernel(\n+  const std::string& name\n+, const Graph& graph\n+, const std::vector<TensorDesc>& input_desc\n+, const std::vector<TensorDesc>& output_desc\n+, const bool use_cuda) {\n+  TemplateEnv env;\n+  env.s(\"kernelName\", name);\n+  env.s(\"IndexType\",\"unsigned int\"); // Note: not uint32_t to avoid including cstdint", "path": "torch/csrc/jit/fuser/codegen.cpp", "position": 223, "original_position": 222, "commit_id": "445f1d937372bebb3a4f7717b5d4de33c77401e6", "original_commit_id": "d5339380c73db30184dd27b47fef610df1073d2e", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "body": "I prettied it up a little. ", "created_at": "2018-10-25T17:22:57Z", "updated_at": "2018-11-23T15:53:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/13108#discussion_r228263233", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13108", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/228263233"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13108#discussion_r228263233"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13108"}}, "body_html": "<p>I prettied it up a little.</p>", "body_text": "I prettied it up a little.", "in_reply_to_id": 228099163}