{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13108", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13108/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13108/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13108/events", "html_url": "https://github.com/pytorch/pytorch/pull/13108", "id": 373761549, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI1NjE1MDEw", "number": 13108, "title": "[JIT] Hierarchical device independent -> device specific architecture", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-10-25T03:45:00Z", "updated_at": "2018-11-23T15:53:44Z", "closed_at": "2018-11-01T01:14:29Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/13108", "html_url": "https://github.com/pytorch/pytorch/pull/13108", "diff_url": "https://github.com/pytorch/pytorch/pull/13108.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/13108.patch"}, "body_html": "<p>This PR principally redesigns the fuser's logical flow to be hierarchical, with device-independent logic directing (relatively little) device-specific logic. This design is based on reviews of XLA, TVM, internal design review at NVIDIA and discussions with fuser owners at Facebook. To further vet the design I have begun developing the next significant PR (extended fusion logic) on top of this architecture and it has made the work significantly easier. This PR also improves fuser modularity, which should make it easier for others to contribute to. Unfortunately, this PR is large and its nature has made breaking it into smaller pieces challenging. Future PRs should be smaller.</p>\n<p>The fusion flow is now:</p>\n<ul>\n<li>Fusions are \"registered\" and \"upfront compilation\" occurs. The fusion specifications, which includes the graph, go into a thread-safe device-independent cache. Upfront compilation generates some information used later during shape inference.</li>\n<li>Fusions are run, which passes them to an executor that performs shape inference, requests an instantiated fusion from the specification's thread-safe store, and launches them. Launch logic eventually defers to device-specific logic.</li>\n<li>Fusions not previously instantiated are compiled. Compilation is device-specific and arg-specific. Compilation logic eventually defers to device-specific logic.</li>\n<li>If the fusion could not be run because fusion on the requested device is disabled or shape inference fails a fallback is invoked.</li>\n</ul>\n<p>This flow can be thought of as PyTorch IR -&gt; Device-Independent Fusion Logic -&gt; Device-Specific Fusion Logic. The current upstream logic is, by contrast, PyTorch IR -&gt; Device-Specific Logic -&gt; Device-Independent Logic, which results in needless code duplication and lack of conceptual clarity. That was my mistake when splitting the fuser off from the rest of the jit and our reviews since then have been incredibly helpful in understanding why the approach in this PR is better.</p>\n<p>This PR does not only move code around. It also fixes few couple bugs and makes some logical/code changes.</p>\n<p>Bug fixes:</p>\n<ul>\n<li>thread-safety is improved with caches preventing concurrent access</li>\n<li>the nvrtc version is now reviewed to determine the appropriate compute architecture to compile for, fixing a bug that would cause runtime errors if a user's nvrtc didn't support the compute architecture their gpu reported</li>\n<li>an issue with DeviceGuard not setting the device properly and failing silently is worked-around (<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> mentioned he was reviewing the dynamic registration DeviceGuard uses, which may resolve the issue)</li>\n</ul>\n<p>Code/Logical changes:</p>\n<ul>\n<li>\"const\" now appears many more places (note: I cast const away in operator.h because of some obscure build issues -- I think we should be able to fix this and will take a look while this goes through testing)</li>\n<li>The new flow allowed some redundant code to be removed (AnnotatedGraph is gone, for example, and the more straightforward flow eliminated duplication of effort elsewhere)</li>\n<li>Fallback logic is now also invoked if a fusion is requested on a device that cannot handle fusions</li>\n<li>Use of macros to determine which files are compiled is reduced (though they may come back if the Windows build is unhappy)</li>\n<li>There is no more \"common\" code or folder, the device-independent logic being at the forefront of the fuser replaces and improves upon the goal of sharing code</li>\n</ul>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> who I promised naming rights to<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a> who correctly pointed out that the device-independent logic should be the bulk of what the fuser is doing<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> who contributed to the design of this architecture</p>", "body_text": "This PR principally redesigns the fuser's logical flow to be hierarchical, with device-independent logic directing (relatively little) device-specific logic. This design is based on reviews of XLA, TVM, internal design review at NVIDIA and discussions with fuser owners at Facebook. To further vet the design I have begun developing the next significant PR (extended fusion logic) on top of this architecture and it has made the work significantly easier. This PR also improves fuser modularity, which should make it easier for others to contribute to. Unfortunately, this PR is large and its nature has made breaking it into smaller pieces challenging. Future PRs should be smaller.\nThe fusion flow is now:\n\nFusions are \"registered\" and \"upfront compilation\" occurs. The fusion specifications, which includes the graph, go into a thread-safe device-independent cache. Upfront compilation generates some information used later during shape inference.\nFusions are run, which passes them to an executor that performs shape inference, requests an instantiated fusion from the specification's thread-safe store, and launches them. Launch logic eventually defers to device-specific logic.\nFusions not previously instantiated are compiled. Compilation is device-specific and arg-specific. Compilation logic eventually defers to device-specific logic.\nIf the fusion could not be run because fusion on the requested device is disabled or shape inference fails a fallback is invoked.\n\nThis flow can be thought of as PyTorch IR -> Device-Independent Fusion Logic -> Device-Specific Fusion Logic. The current upstream logic is, by contrast, PyTorch IR -> Device-Specific Logic -> Device-Independent Logic, which results in needless code duplication and lack of conceptual clarity. That was my mistake when splitting the fuser off from the rest of the jit and our reviews since then have been incredibly helpful in understanding why the approach in this PR is better.\nThis PR does not only move code around. It also fixes few couple bugs and makes some logical/code changes.\nBug fixes:\n\nthread-safety is improved with caches preventing concurrent access\nthe nvrtc version is now reviewed to determine the appropriate compute architecture to compile for, fixing a bug that would cause runtime errors if a user's nvrtc didn't support the compute architecture their gpu reported\nan issue with DeviceGuard not setting the device properly and failing silently is worked-around (@ezyang mentioned he was reviewing the dynamic registration DeviceGuard uses, which may resolve the issue)\n\nCode/Logical changes:\n\n\"const\" now appears many more places (note: I cast const away in operator.h because of some obscure build issues -- I think we should be able to fix this and will take a look while this goes through testing)\nThe new flow allowed some redundant code to be removed (AnnotatedGraph is gone, for example, and the more straightforward flow eliminated duplication of effort elsewhere)\nFallback logic is now also invoked if a fusion is requested on a device that cannot handle fusions\nUse of macros to determine which files are compiled is reduced (though they may come back if the Windows build is unhappy)\nThere is no more \"common\" code or folder, the device-independent logic being at the forefront of the fuser replaces and improves upon the goal of sharing code\n\n@apaszke who I promised naming rights to\n@zdevito who correctly pointed out that the device-independent logic should be the bulk of what the fuser is doing\n@ngimel who contributed to the design of this architecture", "body": "This PR principally redesigns the fuser's logical flow to be hierarchical, with device-independent logic directing (relatively little) device-specific logic. This design is based on reviews of XLA, TVM, internal design review at NVIDIA and discussions with fuser owners at Facebook. To further vet the design I have begun developing the next significant PR (extended fusion logic) on top of this architecture and it has made the work significantly easier. This PR also improves fuser modularity, which should make it easier for others to contribute to. Unfortunately, this PR is large and its nature has made breaking it into smaller pieces challenging. Future PRs should be smaller.\r\n\r\nThe fusion flow is now:\r\n\r\n- Fusions are \"registered\" and \"upfront compilation\" occurs. The fusion specifications, which includes the graph, go into a thread-safe device-independent cache. Upfront compilation generates some information used later during shape inference. \r\n- Fusions are run, which passes them to an executor that performs shape inference, requests an instantiated fusion from the specification's thread-safe store, and launches them. Launch logic eventually defers to device-specific logic.\r\n- Fusions not previously instantiated are compiled. Compilation is device-specific and arg-specific. Compilation logic eventually defers to device-specific logic.\r\n- If the fusion could not be run because fusion on the requested device is disabled or shape inference fails a fallback is invoked.\r\n\r\nThis flow can be thought of as PyTorch IR -> Device-Independent Fusion Logic -> Device-Specific Fusion Logic. The current upstream logic is, by contrast, PyTorch IR -> Device-Specific Logic -> Device-Independent Logic, which results in needless code duplication and lack of conceptual clarity. That was my mistake when splitting the fuser off from the rest of the jit and our reviews since then have been incredibly helpful in understanding why the approach in this PR is better.\r\n\r\nThis PR does not only move code around. It also fixes few couple bugs and makes some logical/code changes.\r\n\r\nBug fixes:\r\n- thread-safety is improved with caches preventing concurrent access\r\n- the nvrtc version is now reviewed to determine the appropriate compute architecture to compile for, fixing a bug that would cause runtime errors if a user's nvrtc didn't support the compute architecture their gpu reported\r\n- an issue with DeviceGuard not setting the device properly and failing silently is worked-around (@ezyang mentioned he was reviewing the dynamic registration DeviceGuard uses, which may resolve the issue)\r\n\r\nCode/Logical changes:\r\n- \"const\" now appears many more places (note: I cast const away in operator.h because of some obscure build issues -- I think we should be able to fix this and will take a look while this goes through testing)\r\n- The new flow allowed some redundant code to be removed (AnnotatedGraph is gone, for example, and the more straightforward flow eliminated duplication of effort elsewhere)\r\n- Fallback logic is now also invoked if a fusion is requested on a device that cannot handle fusions\r\n- Use of macros to determine which files are compiled is reduced (though they may come back if the Windows build is unhappy)\r\n- There is no more \"common\" code or folder, the device-independent logic being at the forefront of the fuser replaces and improves upon the goal of sharing code\r\n\r\n@apaszke who I promised naming rights to\r\n@zdevito who correctly pointed out that the device-independent logic should be the bulk of what the fuser is doing\r\n@ngimel who contributed to the design of this architecture"}