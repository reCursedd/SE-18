{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/228263108", "pull_request_review_id": 168496295, "id": 228263108, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyODI2MzEwOA==", "diff_hunk": "@@ -0,0 +1,361 @@\n+#include \"torch/csrc/jit/fuser/executor.h\"\n+\n+#include \"ATen/ATen.h\"\n+#include \"ATen/ExpandUtils.h\"\n+#include \"c10/util/Optional.h\"\n+#include \"torch/csrc/utils/functional.h\"\n+#include \"torch/csrc/jit/stack.h\"\n+#include \"torch/csrc/jit/fuser/config.h\"\n+#include \"torch/csrc/jit/fuser/interface.h\"\n+#include \"torch/csrc/jit/fuser/kernel_cache.h\"\n+#include \"torch/csrc/jit/fuser/kernel_spec.h\"\n+#include \"torch/csrc/jit/fuser/compiler.h\"\n+#include \"torch/csrc/jit/fuser/tensor_info.h\"\n+\n+#include <vector>\n+#include <tuple>\n+#include <stdexcept>\n+#include <algorithm>\n+#include <map>\n+#include <iostream> // TODO: remove, debugging only\n+\n+namespace torch { namespace jit { namespace fuser {\n+\n+// Returns the \"map size\" for this run, which is the common size for all\n+// intermediate tensors.\n+static c10::optional<std::vector<int64_t>> getMapSize(\n+  const KernelSpec& spec\n+, at::TensorList args\n+, at::IntList arg_subset) {\n+  \n+  int64_t dim_after_broadcast = 0;\n+  for (const auto arg_idx : arg_subset) {\n+    dim_after_broadcast = std::max(dim_after_broadcast, args[arg_idx].dim());\n+  }\n+  // TODO: this keeps reallocating map_size at every iteration, but we know\n+  // exactly how much storage do we need, so this could be fixed in-place at\n+  // every step. We're just missing a few functions for ATen, but the fix\n+  // should be straightforward.\n+  // Note: left unitialized since empty shape is broadcastable to any shape\n+  std::vector<int64_t> map_size;\n+  for (size_t i = 0; i < arg_subset.size(); ++i) {\n+    auto& arg = args.at(arg_subset[i]);\n+    auto& chunk_desc = spec.inputChunks().at(arg_subset[i]);\n+    if (chunk_desc.nSubTensors() == 1) {\n+      try {\n+        map_size = at::infer_size(map_size, arg.sizes());\n+      } catch (...) {\n+        return c10::nullopt;\n+      }\n+    } else {\n+      auto tensor_sizes = arg.sizes().vec();\n+      const auto num_chunks = chunk_desc.nSubTensors();\n+      const auto dim = at::maybe_wrap_dim(chunk_desc.dim(), tensor_sizes.size());\n+      if (tensor_sizes[dim] % num_chunks != 0) {\n+        return c10::nullopt;\n+      }\n+      tensor_sizes[dim] /= num_chunks;\n+      try {\n+        map_size = at::infer_size(map_size, tensor_sizes);\n+      } catch (...) {\n+        return c10::nullopt;\n+      }\n+    }\n+  }\n+\n+  return {map_size};\n+}\n+\n+// Tries to determine a map size for the instantiated kernel (see above)\n+static c10::optional<std::vector<int64_t>> canRunKernel(\n+  const KernelSpec& spec\n+, at::TensorList args) {\n+  // Short-circuits on size mismatch\n+  AT_CHECK(\n+    args.size() == spec.inputChunks().size()\n+  , \"Expected \", spec.inputChunks().size(), \" arguments, but got \", args.size());\n+\n+  c10::optional<std::vector<int64_t>> map_size;\n+  for (const auto& broadcast_group : spec.inputBroadcastGroups()) {\n+    if (!map_size) {\n+      map_size = getMapSize(spec, args, broadcast_group);\n+      if (!map_size) return c10::nullopt;\n+    } else {\n+      const auto group_map_size = getMapSize(spec, args, broadcast_group);\n+      // Note: this checks that group_map_size is defined AND equal to map_size\n+      if (map_size != group_map_size) return c10::nullopt;\n+    }\n+  }\n+\n+  return map_size;\n+}\n+\n+// Arguments are expanded to a common shape, referred to as the \"map size,\"\n+// (see above). \n+// Note: Arguments are mutated by this call, although map_size is restored\n+// to its original value.\n+static void expandArgs(\n+  const KernelSpec& spec\n+, std::vector<at::Tensor>& args\n+, std::vector<int64_t>& map_size) {\n+  for (size_t i = 0; i < args.size(); ++i) {\n+    auto& arg = args[i];\n+    const auto& pdesc = spec.inputChunks()[i];\n+    if (pdesc.nSubTensors() == 1) {\n+      if (arg.sizes().equals(map_size)) continue;\n+      arg = arg.expand(map_size);\n+    } else {\n+      map_size.at(pdesc.dim()) *= pdesc.nSubTensors();\n+      if (!arg.sizes().equals(map_size)) {\n+        arg = arg.expand(map_size);\n+      }\n+      map_size.at(pdesc.dim()) /= pdesc.nSubTensors();\n+    }\n+  }\n+}\n+\n+// Note: assumes that inputs are 32-bit addressable\n+static uint32_t computeNumel(const at::ArrayRef<int64_t>& sizes) {\n+  uint32_t result = 1;\n+\n+  // Short-circuits if scalar tensor\n+  if (sizes.size() == 0) return 1;", "path": "torch/csrc/jit/fuser/executor.cpp", "position": null, "original_position": 122, "commit_id": "445f1d937372bebb3a4f7717b5d4de33c77401e6", "original_commit_id": "d5339380c73db30184dd27b47fef610df1073d2e", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "body": "Yep. Removed.", "created_at": "2018-10-25T17:22:32Z", "updated_at": "2018-11-23T15:53:35Z", "html_url": "https://github.com/pytorch/pytorch/pull/13108#discussion_r228263108", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13108", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/228263108"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13108#discussion_r228263108"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13108"}}, "body_html": "<p>Yep. Removed.</p>", "body_text": "Yep. Removed.", "in_reply_to_id": 228102343}