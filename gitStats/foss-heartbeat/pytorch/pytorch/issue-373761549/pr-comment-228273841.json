{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/228273841", "pull_request_review_id": 168509838, "id": 228273841, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyODI3Mzg0MQ==", "diff_hunk": "@@ -0,0 +1,53 @@\n+#pragma once\n+\n+#include \"torch/csrc/WindowsTorchApiMacro.h\"\n+#include \"torch/csrc/jit/assertions.h\"\n+#include \"torch/csrc/jit/fuser/tensor_desc.h\"\n+\n+#include <memory>\n+#include <cstdint>\n+#include <vector>\n+\n+namespace torch { namespace jit { namespace fuser {\n+\n+// Descriptor for chunk-ing an input tensor into subtensors\n+// OR concat-ing an output tensor from subtensors\n+struct TORCH_API PartitionDesc {\n+  PartitionDesc()", "path": "torch/csrc/jit/fuser/partition_desc.h", "position": 20, "original_position": 16, "commit_id": "445f1d937372bebb3a4f7717b5d4de33c77401e6", "original_commit_id": "d5339380c73db30184dd27b47fef610df1073d2e", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "body": "That's a fine way to think of them but the current code uses the default constructor for \"pass-through\" inputs and outputs that don't participate in concat/chunk operations. This can be seen in codegen.cpp, for example. I think we can simplify the shape inference more than is done in this PR but I don't want to extend it. The extended fusion logic requires a shape inference cleanup, anyway.", "created_at": "2018-10-25T17:53:32Z", "updated_at": "2018-11-23T15:53:36Z", "html_url": "https://github.com/pytorch/pytorch/pull/13108#discussion_r228273841", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/13108", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/228273841"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/13108#discussion_r228273841"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/13108"}}, "body_html": "<p>That's a fine way to think of them but the current code uses the default constructor for \"pass-through\" inputs and outputs that don't participate in concat/chunk operations. This can be seen in codegen.cpp, for example. I think we can simplify the shape inference more than is done in this PR but I don't want to extend it. The extended fusion logic requires a shape inference cleanup, anyway.</p>", "body_text": "That's a fine way to think of them but the current code uses the default constructor for \"pass-through\" inputs and outputs that don't participate in concat/chunk operations. This can be seen in codegen.cpp, for example. I think we can simplify the shape inference more than is done in this PR but I don't want to extend it. The extended fusion logic requires a shape inference cleanup, anyway.", "in_reply_to_id": 228105340}