{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/350739675", "html_url": "https://github.com/pytorch/pytorch/issues/4085#issuecomment-350739675", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4085", "id": 350739675, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDczOTY3NQ==", "user": {"login": "josecabjim", "id": 28218638, "node_id": "MDQ6VXNlcjI4MjE4NjM4", "avatar_url": "https://avatars0.githubusercontent.com/u/28218638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/josecabjim", "html_url": "https://github.com/josecabjim", "followers_url": "https://api.github.com/users/josecabjim/followers", "following_url": "https://api.github.com/users/josecabjim/following{/other_user}", "gists_url": "https://api.github.com/users/josecabjim/gists{/gist_id}", "starred_url": "https://api.github.com/users/josecabjim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/josecabjim/subscriptions", "organizations_url": "https://api.github.com/users/josecabjim/orgs", "repos_url": "https://api.github.com/users/josecabjim/repos", "events_url": "https://api.github.com/users/josecabjim/events{/privacy}", "received_events_url": "https://api.github.com/users/josecabjim/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-11T14:29:30Z", "updated_at": "2017-12-11T14:31:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think these are sensible suggestions. The current implementation is stripped of optional formatting on flow features, and whether we add alternatives I guess depends on how often these appear to be useful. Here are some comments on three proposals.</p>\n<p><strong>Normalised input</strong></p>\n<p>I think the normalisation of the flow with respect to image dimensions is mainly handy for learning and applying flow features independent of image dimensions. It is common for optical flow to be learnt at scales lower than full resolution for efficiency and practical reasons (eg. <a href=\"https://arxiv.org/pdf/1711.06045.pdf\" rel=\"nofollow\">here</a>). The convention that flow features are normalised avoids the need to know the image dimensions on which a flow field was computed, and only requires the image dimensions of the image on which we would like to use it.</p>\n<p><strong>Optical flow sample</strong></p>\n<p>I'm not entirely sure I understand the issue here. To modify a normalised flow grid it is possible to use up/downscaling modules that will interpolate flow values (eg. with bilinear interpolation). As to the mapping from [-1, 1] to absolute grid values the simplest would be to create an grid with linspace and onto which we can add the normalised flow. Your issue, however, sounds like a conflict with the current convention, so if the adaptation of the current sampler to your repetitively leads to boilerplate code I agree we could include other behaviours outside the current standard.</p>\n<p><strong>Border behaviour</strong></p>\n<p>I think the behaviour described would not be desired for low-level vision tasks, as creating holes in warped reconstructions is likely to disrupt the learning of transformations more aggresively than replacing holes with border values. Having said that, if there are cases where this could be beneficial this could be added as a <code>padding_mode = None</code>, where values outside the image would produce a <code>NaN</code> output.</p>", "body_text": "I think these are sensible suggestions. The current implementation is stripped of optional formatting on flow features, and whether we add alternatives I guess depends on how often these appear to be useful. Here are some comments on three proposals.\nNormalised input\nI think the normalisation of the flow with respect to image dimensions is mainly handy for learning and applying flow features independent of image dimensions. It is common for optical flow to be learnt at scales lower than full resolution for efficiency and practical reasons (eg. here). The convention that flow features are normalised avoids the need to know the image dimensions on which a flow field was computed, and only requires the image dimensions of the image on which we would like to use it.\nOptical flow sample\nI'm not entirely sure I understand the issue here. To modify a normalised flow grid it is possible to use up/downscaling modules that will interpolate flow values (eg. with bilinear interpolation). As to the mapping from [-1, 1] to absolute grid values the simplest would be to create an grid with linspace and onto which we can add the normalised flow. Your issue, however, sounds like a conflict with the current convention, so if the adaptation of the current sampler to your repetitively leads to boilerplate code I agree we could include other behaviours outside the current standard.\nBorder behaviour\nI think the behaviour described would not be desired for low-level vision tasks, as creating holes in warped reconstructions is likely to disrupt the learning of transformations more aggresively than replacing holes with border values. Having said that, if there are cases where this could be beneficial this could be added as a padding_mode = None, where values outside the image would produce a NaN output.", "body": "I think these are sensible suggestions. The current implementation is stripped of optional formatting on flow features, and whether we add alternatives I guess depends on how often these appear to be useful. Here are some comments on three proposals.\r\n\r\n**Normalised input**\r\n\r\nI think the normalisation of the flow with respect to image dimensions is mainly handy for learning and applying flow features independent of image dimensions. It is common for optical flow to be learnt at scales lower than full resolution for efficiency and practical reasons (eg. [here](https://arxiv.org/pdf/1711.06045.pdf)). The convention that flow features are normalised avoids the need to know the image dimensions on which a flow field was computed, and only requires the image dimensions of the image on which we would like to use it.\r\n\r\n**Optical flow sample**\r\n\r\nI'm not entirely sure I understand the issue here. To modify a normalised flow grid it is possible to use up/downscaling modules that will interpolate flow values (eg. with bilinear interpolation). As to the mapping from [-1, 1] to absolute grid values the simplest would be to create an grid with linspace and onto which we can add the normalised flow. Your issue, however, sounds like a conflict with the current convention, so if the adaptation of the current sampler to your repetitively leads to boilerplate code I agree we could include other behaviours outside the current standard.\r\n\r\n**Border behaviour**\r\n\r\nI think the behaviour described would not be desired for low-level vision tasks, as creating holes in warped reconstructions is likely to disrupt the learning of transformations more aggresively than replacing holes with border values. Having said that, if there are cases where this could be beneficial this could be added as a `padding_mode = None`, where values outside the image would produce a `NaN` output."}