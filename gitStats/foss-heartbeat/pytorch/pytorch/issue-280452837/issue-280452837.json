{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4085", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4085/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4085/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4085/events", "html_url": "https://github.com/pytorch/pytorch/issues/4085", "id": 280452837, "node_id": "MDU6SXNzdWUyODA0NTI4Mzc=", "number": 4085, "title": "GridSampler behaviours", "user": {"login": "ClementPinard", "id": 4380424, "node_id": "MDQ6VXNlcjQzODA0MjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4380424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ClementPinard", "html_url": "https://github.com/ClementPinard", "followers_url": "https://api.github.com/users/ClementPinard/followers", "following_url": "https://api.github.com/users/ClementPinard/following{/other_user}", "gists_url": "https://api.github.com/users/ClementPinard/gists{/gist_id}", "starred_url": "https://api.github.com/users/ClementPinard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ClementPinard/subscriptions", "organizations_url": "https://api.github.com/users/ClementPinard/orgs", "repos_url": "https://api.github.com/users/ClementPinard/repos", "events_url": "https://api.github.com/users/ClementPinard/events{/privacy}", "received_events_url": "https://api.github.com/users/ClementPinard/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 806617721, "node_id": "MDU6TGFiZWw4MDY2MTc3MjE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/cudnn", "name": "cudnn", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-08T11:03:36Z", "updated_at": "2018-01-16T17:26:15Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>This is in continuity with <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"255218649\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/2625\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/2625/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/2625\">#2625</a></p>\n<p>Here are some observations with grid sample, that I think could be worth discussing :</p>\n<h3>Normalized input</h3>\n<p>I figure that [-1, 1] bounding was enforced by original cuDNN API, but I cannot help noticing that most of the time, when not using grid generator, I have a pixel unit grid <code>[0, H-1]x[0, W-1]</code> which I have to normalize, and which is inverted back to pixel unit (see <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/SpatialGridSamplerBilinear.cu#L54\">here</a> and <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialGridSamplerBilinear.c#L71\">here</a> ). Isn't it some kind of waste of computation ? maybe we can add an option to specify whether the input is normalized [-1, 1] or not [0, {H or W} -1]</p>\n<h3>Optical Flow Sample</h3>\n<p>As discussed here <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"280201744\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/NVIDIA/flownet2-pytorch/issues/3\" data-hovercard-type=\"issue\" data-hovercard-url=\"/NVIDIA/flownet2-pytorch/issues/3/hovercard\" href=\"https://github.com/NVIDIA/flownet2-pytorch/issues/3\">NVIDIA/flownet2-pytorch#3</a> , resampling from an optical flow map is subotpimal. Assuming flow is normalized to fit nicely in a [-1, 1] normalized grid, You have to convert flow to grid. This can be done with affine grid generator, but you then have to generate an identity <code>theta</code> matrix, i.e. generate a <code>Bx3x2</code> zeros <code>theta</code> volatile Variable, fill  <code>theta[:,0,0]</code> and <code>theta[:,1,1]</code> with ones, devolatilize the grid and add it to your normalized flow.<br>\nAlong with dramatically simplyfying the workflow, would adding a <code>residual</code> option (or whatever its name could be) to implicitely add identity to the grid be a good solution ? (or maybe add a new <code>flow_sample</code> function ? )</p>\n<h3>Border behaviour</h3>\n<p>This suggestion is admittedly only cosmetic as the resulting gradient would be the same as when 'same' padding is applied (from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"272596908\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/3599\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/3599/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/3599\">#3599</a>), so obviously not essential. It could be nice to have zero padding, but no interpolation between 0s and actual values. See an example <a href=\"https://discuss.pytorch.org/t/spatial-transformer-networks-boundary-grid-interpolation-behaviour/8891/2\" rel=\"nofollow\">here</a> . As soon as grid value is outside [-1,1], output 0 as if point was far outside the input (and thus no interpolation between void and the border)<br>\nThis also could be beneficial for classifier associated with a pixel-wise loss for this particular resampled feature map. Classifier like 0 values for irrelevant data points, Pixel-wise loss likes non-zero gradients for valid values only<br>\nThis is semantically different from 'same' padding so I assume it would not be easy just to add another mode to <code>padding_mode</code> option.</p>", "body_text": "This is in continuity with #2625\nHere are some observations with grid sample, that I think could be worth discussing :\nNormalized input\nI figure that [-1, 1] bounding was enforced by original cuDNN API, but I cannot help noticing that most of the time, when not using grid generator, I have a pixel unit grid [0, H-1]x[0, W-1] which I have to normalize, and which is inverted back to pixel unit (see here and here ). Isn't it some kind of waste of computation ? maybe we can add an option to specify whether the input is normalized [-1, 1] or not [0, {H or W} -1]\nOptical Flow Sample\nAs discussed here NVIDIA/flownet2-pytorch#3 , resampling from an optical flow map is subotpimal. Assuming flow is normalized to fit nicely in a [-1, 1] normalized grid, You have to convert flow to grid. This can be done with affine grid generator, but you then have to generate an identity theta matrix, i.e. generate a Bx3x2 zeros theta volatile Variable, fill  theta[:,0,0] and theta[:,1,1] with ones, devolatilize the grid and add it to your normalized flow.\nAlong with dramatically simplyfying the workflow, would adding a residual option (or whatever its name could be) to implicitely add identity to the grid be a good solution ? (or maybe add a new flow_sample function ? )\nBorder behaviour\nThis suggestion is admittedly only cosmetic as the resulting gradient would be the same as when 'same' padding is applied (from #3599), so obviously not essential. It could be nice to have zero padding, but no interpolation between 0s and actual values. See an example here . As soon as grid value is outside [-1,1], output 0 as if point was far outside the input (and thus no interpolation between void and the border)\nThis also could be beneficial for classifier associated with a pixel-wise loss for this particular resampled feature map. Classifier like 0 values for irrelevant data points, Pixel-wise loss likes non-zero gradients for valid values only\nThis is semantically different from 'same' padding so I assume it would not be easy just to add another mode to padding_mode option.", "body": "This is in continuity with #2625\r\n\r\nHere are some observations with grid sample, that I think could be worth discussing : \r\n\r\n### Normalized input\r\nI figure that [-1, 1] bounding was enforced by original cuDNN API, but I cannot help noticing that most of the time, when not using grid generator, I have a pixel unit grid `[0, H-1]x[0, W-1]` which I have to normalize, and which is inverted back to pixel unit (see [here](https://github.com/pytorch/pytorch/blob/master/aten/src/THCUNN/SpatialGridSamplerBilinear.cu#L54) and [here](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialGridSamplerBilinear.c#L71) ). Isn't it some kind of waste of computation ? maybe we can add an option to specify whether the input is normalized [-1, 1] or not [0, {H or W} -1]\r\n\r\n\r\n### Optical Flow Sample\r\nAs discussed here NVIDIA/flownet2-pytorch#3 , resampling from an optical flow map is subotpimal. Assuming flow is normalized to fit nicely in a [-1, 1] normalized grid, You have to convert flow to grid. This can be done with affine grid generator, but you then have to generate an identity `theta` matrix, i.e. generate a `Bx3x2` zeros `theta` volatile Variable, fill  `theta[:,0,0]` and `theta[:,1,1]` with ones, devolatilize the grid and add it to your normalized flow.\r\nAlong with dramatically simplyfying the workflow, would adding a `residual` option (or whatever its name could be) to implicitely add identity to the grid be a good solution ? (or maybe add a new `flow_sample` function ? )\r\n \r\n### Border behaviour\r\nThis suggestion is admittedly only cosmetic as the resulting gradient would be the same as when 'same' padding is applied (from #3599), so obviously not essential. It could be nice to have zero padding, but no interpolation between 0s and actual values. See an example [here](https://discuss.pytorch.org/t/spatial-transformer-networks-boundary-grid-interpolation-behaviour/8891/2) . As soon as grid value is outside [-1,1], output 0 as if point was far outside the input (and thus no interpolation between void and the border)\r\nThis also could be beneficial for classifier associated with a pixel-wise loss for this particular resampled feature map. Classifier like 0 values for irrelevant data points, Pixel-wise loss likes non-zero gradients for valid values only\r\nThis is semantically different from 'same' padding so I assume it would not be easy just to add another mode to `padding_mode` option."}