{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5826", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5826/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5826/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5826/events", "html_url": "https://github.com/pytorch/pytorch/issues/5826", "id": 305773258, "node_id": "MDU6SXNzdWUzMDU3NzMyNTg=", "number": 5826, "title": "UserWarning: tensor1/other is not broadcastable to self, but they have the same number of elements. Falling back to deprecated pointwise behavior", "user": {"login": "ProGamerGov", "id": 10626398, "node_id": "MDQ6VXNlcjEwNjI2Mzk4", "avatar_url": "https://avatars1.githubusercontent.com/u/10626398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ProGamerGov", "html_url": "https://github.com/ProGamerGov", "followers_url": "https://api.github.com/users/ProGamerGov/followers", "following_url": "https://api.github.com/users/ProGamerGov/following{/other_user}", "gists_url": "https://api.github.com/users/ProGamerGov/gists{/gist_id}", "starred_url": "https://api.github.com/users/ProGamerGov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ProGamerGov/subscriptions", "organizations_url": "https://api.github.com/users/ProGamerGov/orgs", "repos_url": "https://api.github.com/users/ProGamerGov/repos", "events_url": "https://api.github.com/users/ProGamerGov/events{/privacy}", "received_events_url": "https://api.github.com/users/ProGamerGov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 545367190, "node_id": "MDU6TGFiZWw1NDUzNjcxOTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/awaiting%20response", "name": "awaiting response", "color": "5319e7", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-03-16T01:29:28Z", "updated_at": "2018-04-05T17:46:13Z", "closed_at": null, "author_association": "NONE", "body_html": "<ul>\n<li>\n<p>OS: Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-1038-aws x86_64)</p>\n</li>\n<li>\n<p>PyTorch version:  0.4.0a0+7f864bb (Source), and 0.3.1 (Pip).</p>\n</li>\n<li>\n<p>How you installed PyTorch (conda, pip, source): Both Pip and Source</p>\n</li>\n<li>\n<p>Python version: Python2 and Python3</p>\n</li>\n<li>\n<p>CUDA/cuDNN version: Cuda 9, cuDNN v7</p>\n</li>\n<li>\n<p>GPU models and configuration: Tesla K80</p>\n</li>\n<li>\n<p>GCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)</p>\n</li>\n</ul>\n<p>When using the Pip version (Python2 &amp; 3), I am getting these errors when using the <code>torch.legacy.optim</code> L-BFGS and Adam optimizers:</p>\n<p>Running optimization with ADAM:</p>\n<pre><code>/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/adam.py:65: UserWarning: tensor1 is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n  x.addcdiv_(-stepSize, state['m'], state['denom'])\n</code></pre>\n<p>Running optimization with L-BFGS:</p>\n<pre><code>('&lt;optim.lbfgs&gt;', 'creating recyclable direction/step/history buffers')\n/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/lbfgs.py:197: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n  x.add_(t, d)\n</code></pre>\n<p>This is my feval function:</p>\n<pre><code>num_calls = [0]\ndef feval(x):\n  num_calls[0] += 1\n  net.updateOutput(x.cuda())\n  grad = net.updateGradInput(x.cuda(), dy.cuda())\n  loss = 0\n  for mod in content_losses:\n    loss = loss + mod.loss\n  for mod in style_losses:\n    loss = loss + mod.loss\n  return loss, grad.view(grad.nelement())\n</code></pre>\n<p>And this is how I run it:</p>\n<pre><code>optim_state = None\nif params.optimizer == 'lbfgs':\n  optim_state = {\n    \"maxIter\": params.num_iterations,\n    \"verbose\": True,\n    \"tolX\": -1,\n    \"tolFun\": -1,\n  }\n  if params.lbfgs_num_correction &gt; 0:\n    optim_state.nCorrection = params.lbfgs_num_correction\nelif params.optimizer == 'adam':\n    optim_state = {\n      \"learningRate\": params.learning_rate,\n    }\n\n# Run optimization.\nif params.optimizer == 'lbfgs':\n  print(\"Running optimization with L-BFGS\")\n  x, losses = optim.lbfgs(feval, img, optim_state)\nelif params.optimizer == 'adam':\n  print(\"Running optimization with ADAM\")\n  for t in xrange(params.num_iterations):\n    x, losses = optim.adam(feval, img, optim_state)\n</code></pre>\n<p>If I run the above code with the latest Github version, I get this error which happens in a completely different area:</p>\n<pre><code>Traceback (most recent call last):\n  File \"test2.py\", line 124, in &lt;module&gt;\n    net.updateOutput(content_image_caffe)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py\", line 36, in updateOutput\n    currentOutput = module.updateOutput(currentOutput)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/SpatialConvolution.py\", line 84, in updateOutput\n    self._viewWeight()\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/SpatialConvolution.py\", line 75, in _viewWeight\n    self.gradWeight = self.gradWeight.view(self.nOutputPlane, self.nInputPlane * self.kH * self.kW)\nRuntimeError: invalid argument 2: size '[64 x 27]' is invalid for input with 0 elements at /home/ubuntu/pytorch/aten/src/TH/THStorage.c:41\n</code></pre>\n<p>So I am not sure if the above error is still related to the Pip issues, if something got changed/broken, etc...</p>", "body_text": "OS: Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-1038-aws x86_64)\n\n\nPyTorch version:  0.4.0a0+7f864bb (Source), and 0.3.1 (Pip).\n\n\nHow you installed PyTorch (conda, pip, source): Both Pip and Source\n\n\nPython version: Python2 and Python3\n\n\nCUDA/cuDNN version: Cuda 9, cuDNN v7\n\n\nGPU models and configuration: Tesla K80\n\n\nGCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)\n\n\nWhen using the Pip version (Python2 & 3), I am getting these errors when using the torch.legacy.optim L-BFGS and Adam optimizers:\nRunning optimization with ADAM:\n/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/adam.py:65: UserWarning: tensor1 is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n  x.addcdiv_(-stepSize, state['m'], state['denom'])\n\nRunning optimization with L-BFGS:\n('<optim.lbfgs>', 'creating recyclable direction/step/history buffers')\n/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/lbfgs.py:197: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n  x.add_(t, d)\n\nThis is my feval function:\nnum_calls = [0]\ndef feval(x):\n  num_calls[0] += 1\n  net.updateOutput(x.cuda())\n  grad = net.updateGradInput(x.cuda(), dy.cuda())\n  loss = 0\n  for mod in content_losses:\n    loss = loss + mod.loss\n  for mod in style_losses:\n    loss = loss + mod.loss\n  return loss, grad.view(grad.nelement())\n\nAnd this is how I run it:\noptim_state = None\nif params.optimizer == 'lbfgs':\n  optim_state = {\n    \"maxIter\": params.num_iterations,\n    \"verbose\": True,\n    \"tolX\": -1,\n    \"tolFun\": -1,\n  }\n  if params.lbfgs_num_correction > 0:\n    optim_state.nCorrection = params.lbfgs_num_correction\nelif params.optimizer == 'adam':\n    optim_state = {\n      \"learningRate\": params.learning_rate,\n    }\n\n# Run optimization.\nif params.optimizer == 'lbfgs':\n  print(\"Running optimization with L-BFGS\")\n  x, losses = optim.lbfgs(feval, img, optim_state)\nelif params.optimizer == 'adam':\n  print(\"Running optimization with ADAM\")\n  for t in xrange(params.num_iterations):\n    x, losses = optim.adam(feval, img, optim_state)\n\nIf I run the above code with the latest Github version, I get this error which happens in a completely different area:\nTraceback (most recent call last):\n  File \"test2.py\", line 124, in <module>\n    net.updateOutput(content_image_caffe)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py\", line 36, in updateOutput\n    currentOutput = module.updateOutput(currentOutput)\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/SpatialConvolution.py\", line 84, in updateOutput\n    self._viewWeight()\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/SpatialConvolution.py\", line 75, in _viewWeight\n    self.gradWeight = self.gradWeight.view(self.nOutputPlane, self.nInputPlane * self.kH * self.kW)\nRuntimeError: invalid argument 2: size '[64 x 27]' is invalid for input with 0 elements at /home/ubuntu/pytorch/aten/src/TH/THStorage.c:41\n\nSo I am not sure if the above error is still related to the Pip issues, if something got changed/broken, etc...", "body": "- OS: Ubuntu 16.04.3 LTS (GNU/Linux 4.4.0-1038-aws x86_64)\r\n- PyTorch version:  0.4.0a0+7f864bb (Source), and 0.3.1 (Pip).\r\n\r\n- How you installed PyTorch (conda, pip, source): Both Pip and Source\r\n- Python version: Python2 and Python3\r\n- CUDA/cuDNN version: Cuda 9, cuDNN v7\r\n- GPU models and configuration: Tesla K80        \r\n- GCC version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)\r\n\r\n\r\nWhen using the Pip version (Python2 & 3), I am getting these errors when using the `torch.legacy.optim` L-BFGS and Adam optimizers: \r\n\r\n\r\nRunning optimization with ADAM:\r\n\r\n```\r\n/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/adam.py:65: UserWarning: tensor1 is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\r\n  x.addcdiv_(-stepSize, state['m'], state['denom'])\r\n```\r\n \r\nRunning optimization with L-BFGS:\r\n\r\n```\r\n('<optim.lbfgs>', 'creating recyclable direction/step/history buffers')\r\n/usr/local/lib/python2.7/dist-packages/torch/legacy/optim/lbfgs.py:197: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\r\n  x.add_(t, d)\r\n```\r\n\r\nThis is my feval function: \r\n\r\n```\r\nnum_calls = [0]\r\ndef feval(x):\r\n  num_calls[0] += 1\r\n  net.updateOutput(x.cuda())\r\n  grad = net.updateGradInput(x.cuda(), dy.cuda())\r\n  loss = 0\r\n  for mod in content_losses:\r\n    loss = loss + mod.loss\r\n  for mod in style_losses:\r\n    loss = loss + mod.loss\r\n  return loss, grad.view(grad.nelement())\r\n```\r\n\r\nAnd this is how I run it: \r\n\r\n\r\n```\r\noptim_state = None\r\nif params.optimizer == 'lbfgs':\r\n  optim_state = {\r\n    \"maxIter\": params.num_iterations,\r\n    \"verbose\": True,\r\n    \"tolX\": -1,\r\n    \"tolFun\": -1,\r\n  }\r\n  if params.lbfgs_num_correction > 0:\r\n    optim_state.nCorrection = params.lbfgs_num_correction\r\nelif params.optimizer == 'adam':\r\n    optim_state = {\r\n      \"learningRate\": params.learning_rate,\r\n    }\r\n\r\n# Run optimization.\r\nif params.optimizer == 'lbfgs':\r\n  print(\"Running optimization with L-BFGS\")\r\n  x, losses = optim.lbfgs(feval, img, optim_state)\r\nelif params.optimizer == 'adam':\r\n  print(\"Running optimization with ADAM\")\r\n  for t in xrange(params.num_iterations):\r\n    x, losses = optim.adam(feval, img, optim_state)\r\n```\r\n\r\nIf I run the above code with the latest Github version, I get this error which happens in a completely different area: \r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test2.py\", line 124, in <module>\r\n    net.updateOutput(content_image_caffe)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/Sequential.py\", line 36, in updateOutput\r\n    currentOutput = module.updateOutput(currentOutput)\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/SpatialConvolution.py\", line 84, in updateOutput\r\n    self._viewWeight()\r\n  File \"/usr/local/lib/python2.7/dist-packages/torch/legacy/nn/SpatialConvolution.py\", line 75, in _viewWeight\r\n    self.gradWeight = self.gradWeight.view(self.nOutputPlane, self.nInputPlane * self.kH * self.kW)\r\nRuntimeError: invalid argument 2: size '[64 x 27]' is invalid for input with 0 elements at /home/ubuntu/pytorch/aten/src/TH/THStorage.c:41\r\n```\r\n\r\nSo I am not sure if the above error is still related to the Pip issues, if something got changed/broken, etc... "}