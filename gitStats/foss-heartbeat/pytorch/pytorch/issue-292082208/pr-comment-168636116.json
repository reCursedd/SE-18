{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168636116", "pull_request_review_id": 97038714, "id": 168636116, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2ODYzNjExNg==", "diff_hunk": "@@ -130,31 +136,60 @@ def __init__(self, module, device_ids=None, output_device=None, dim=0):\n         else:\n             self._module_copies = [self.module]\n \n-        # Split parameters into buckets that will coalesce reductions\n-        # TODO: different types need different buckets\n-        t = None\n-        for p in self.module.parameters():\n-            tp = type(p.data)\n-            if t is not None and t is not tp:\n-                raise ValueError(\"DistributedDataParallel requires all parameters' data to be of the same type\")\n-            t = tp\n+        # Split parameters into type buckets so that parameter sync (broadcast)\n+        # can operates on mixed parameter types. (e.g. mixed half and float)\n+        self.param_type_buckets = \\\n+            defaultdict(lambda: [[] for _ in range(len(self.device_ids))])\n \n+        for device_idx, module in enumerate(self._module_copies):\n+            for p in module.parameters():\n+                tp = p.type()\n+                if tp == torch.cuda.HalfTensor and \\\n+                        dist._backend != dist.dist_backend.NCCL and \\\n+                        dist._backend != dist.dist_backend.GLOO:\n+                    raise RuntimeError(\"DistributedDataParallel currently only \"\n+                                       \"supports half precision parameters \"\n+                                       \"with NCCL backend\")\n+                # Add the parameter into the type bucket\n+                self.param_type_buckets[tp][device_idx].append(p)\n+\n+        # TODO, adding mixed precision support in NCCL reduction code path\n+        # This is because NCCL backend doesn't support multiple reduction\n+        # bucket\n+        if len(self.param_type_buckets) > 1 and \\\n+                dist._backend == dist.dist_backend.NCCL:\n+            raise RuntimeError(\"DistributedDataParallel currently doesn't \"\n+                               \"support mixed precision type for NCCL backend\")\n+\n+        # Split parameters into buckets that will coalesce reductions\n+        #\n+        # Note that previously we have already splitted parameters by the type.\n+        # Here, for each type, we further split each type of parameters into\n+        # reduction buckets so that each bucket will only have a single type\n+        # of parameters. Therefore subsequent all-reduce can be successful since\n+        # the reduction operation needs to operate on the same kind of data type\n         self.bucket_sizes = []\n         self.bucket_map = {}\n+\n         # Currently NCCL backend only supports single reduction thread/bucket\n         if dist._backend == dist.dist_backend.NCCL:\n             bucket_bytes_cap = float('inf')\n         else:\n             bucket_bytes_cap = 1 * MB\n-        bucket_bytes = bucket_bytes_cap  # to init the first bucket immediately\n-        for param_tuple in zip(*map(lambda m: m.parameters(), self._module_copies)):\n-            if param_tuple[0].requires_grad:\n+\n+        for tp in self.param_type_buckets:\n+            # to init the first bucket immediately for each type\n+            bucket_bytes = bucket_bytes_cap\n+            for param_idx, param in enumerate(self.param_type_buckets[tp][0]):\n+                if not param.requires_grad:\n+                    continue\n                 if bucket_bytes >= bucket_bytes_cap:\n                     self.bucket_sizes.append(0)\n                     bucket_bytes = 0\n-                for p in param_tuple:\n-                    self.bucket_map[p] = len(self.bucket_sizes) - 1\n-                bucket_bytes += p.numel() * p.element_size()\n+                for dev_idx in range(len(self.device_ids)):\n+                    dev_param = self.param_type_buckets[tp][dev_idx][param_idx]\n+                    self.bucket_map[dev_param] = len(self.bucket_sizes) - 1\n+                bucket_bytes += param.numel() * param.element_size()\n                 self.bucket_sizes[-1] += 1", "path": "torch/nn/parallel/distributed.py", "position": null, "original_position": 108, "commit_id": "0a6d068ab9c8084ebc38a0f08e84929c68374cd5", "original_commit_id": "e83e35c2bf2acd0e35cb9e2a6b3a4b56152375bc", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Actually can't we just use `list(take_tensors(module.parameters()))` for this? This would remove a lot of code in here and simplify DDP a lot", "created_at": "2018-02-15T23:06:58Z", "updated_at": "2018-11-23T15:39:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/4891#discussion_r168636116", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4891", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/168636116"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4891#discussion_r168636116"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4891"}}, "body_html": "<p>Actually can't we just use <code>list(take_tensors(module.parameters()))</code> for this? This would remove a lot of code in here and simplify DDP a lot</p>", "body_text": "Actually can't we just use list(take_tensors(module.parameters())) for this? This would remove a lot of code in here and simplify DDP a lot"}