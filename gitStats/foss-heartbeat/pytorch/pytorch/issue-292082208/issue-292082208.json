{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4891", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4891/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4891/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4891/events", "html_url": "https://github.com/pytorch/pytorch/pull/4891", "id": 292082208, "node_id": "MDExOlB1bGxSZXF1ZXN0MTY1NDk4NDUx", "number": 4891, "title": "Added mixed-precision support in distributed training", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-01-27T02:27:44Z", "updated_at": "2018-11-23T15:39:46Z", "closed_at": "2018-02-21T13:29:40Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/4891", "html_url": "https://github.com/pytorch/pytorch/pull/4891", "diff_url": "https://github.com/pytorch/pytorch/pull/4891.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/4891.patch"}, "body_html": "<p>This PR added mixed-precision support in distributed training.</p>\n<p>Basically, this is done by</p>\n<ol>\n<li>bucketing parameters by data types so that intra-node broadcast can operate on different data types.</li>\n<li>bucketing parameters by data types so that gradient all reduction can operate on different data types.</li>\n</ol>\n<p>Note that the NCCL backend currently only supports a single reduction bucket, so this support will be further added with the other PR that makes NCCL backend a separate code path.</p>\n<p>Half-precision support is only enabled for NCCL and GLOO backend.</p>\n<p>Also added the option to the constructor to either sync or not sync the module buffers as an option since for ResNet we don't need to sync buffer and can hit the same accuracy.</p>\n<p>Tested by running the distributed training on DGX1. Bucketing are also tested by printing out the value.</p>\n<p>Tested mixed prevision for both Nccl and Gloo as well.</p>", "body_text": "This PR added mixed-precision support in distributed training.\nBasically, this is done by\n\nbucketing parameters by data types so that intra-node broadcast can operate on different data types.\nbucketing parameters by data types so that gradient all reduction can operate on different data types.\n\nNote that the NCCL backend currently only supports a single reduction bucket, so this support will be further added with the other PR that makes NCCL backend a separate code path.\nHalf-precision support is only enabled for NCCL and GLOO backend.\nAlso added the option to the constructor to either sync or not sync the module buffers as an option since for ResNet we don't need to sync buffer and can hit the same accuracy.\nTested by running the distributed training on DGX1. Bucketing are also tested by printing out the value.\nTested mixed prevision for both Nccl and Gloo as well.", "body": "This PR added mixed-precision support in distributed training.  \r\n\r\nBasically, this is done by\r\n\r\n1. bucketing parameters by data types so that intra-node broadcast can operate on different data types.\r\n2. bucketing parameters by data types so that gradient all reduction can operate on different data types.\r\n\r\nNote that the NCCL backend currently only supports a single reduction bucket, so this support will be further added with the other PR that makes NCCL backend a separate code path.\r\n\r\nHalf-precision support is only enabled for NCCL and GLOO backend.\r\n\r\nAlso added the option to the constructor to either sync or not sync the module buffers as an option since for ResNet we don't need to sync buffer and can hit the same accuracy.\r\n\r\nTested by running the distributed training on DGX1. Bucketing are also tested by printing out the value.\r\n\r\nTested mixed prevision for both Nccl and Gloo as well."}