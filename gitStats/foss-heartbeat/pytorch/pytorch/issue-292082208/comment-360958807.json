{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/360958807", "html_url": "https://github.com/pytorch/pytorch/pull/4891#issuecomment-360958807", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4891", "id": 360958807, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDk1ODgwNw==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-27T04:21:09Z", "updated_at": "2018-01-27T04:21:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8120856\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/teng-li\">@teng-li</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22205833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/csarofeen\">@csarofeen</a> has a simple distributed module that supports mixed precision <a href=\"https://github.com/csarofeen/examples/blob/dist_fp16/imagenet/distributed.py\">https://github.com/csarofeen/examples/blob/dist_fp16/imagenet/distributed.py</a>, he was planning to push it to pytorch core soon. It is just a few lines of code, much easier to follow than DistributedDataParallel. Like yours here, it all-reduces all the parameters at the end of the iteration, and unlike DistributedDataParallel it does not have the limitation that all the parameters have to be updated at each iteration. It does not sync buffers and you are right, it is a good idea to have an option to sync or not sync buffers. We also tested it with resnet on DGX1.<br>\nAnd thank you for your work with nccl backend!</p>", "body_text": "@teng-li, @csarofeen has a simple distributed module that supports mixed precision https://github.com/csarofeen/examples/blob/dist_fp16/imagenet/distributed.py, he was planning to push it to pytorch core soon. It is just a few lines of code, much easier to follow than DistributedDataParallel. Like yours here, it all-reduces all the parameters at the end of the iteration, and unlike DistributedDataParallel it does not have the limitation that all the parameters have to be updated at each iteration. It does not sync buffers and you are right, it is a good idea to have an option to sync or not sync buffers. We also tested it with resnet on DGX1.\nAnd thank you for your work with nccl backend!", "body": "@teng-li, @csarofeen has a simple distributed module that supports mixed precision https://github.com/csarofeen/examples/blob/dist_fp16/imagenet/distributed.py, he was planning to push it to pytorch core soon. It is just a few lines of code, much easier to follow than DistributedDataParallel. Like yours here, it all-reduces all the parameters at the end of the iteration, and unlike DistributedDataParallel it does not have the limitation that all the parameters have to be updated at each iteration. It does not sync buffers and you are right, it is a good idea to have an option to sync or not sync buffers. We also tested it with resnet on DGX1.\r\nAnd thank you for your work with nccl backend!"}