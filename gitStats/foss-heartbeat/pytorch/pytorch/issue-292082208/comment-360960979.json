{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/360960979", "html_url": "https://github.com/pytorch/pytorch/pull/4891#issuecomment-360960979", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4891", "id": 360960979, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MDk2MDk3OQ==", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-27T05:12:12Z", "updated_at": "2018-01-27T05:44:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a> Thanks for the previous code review. Yes, I did see <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22205833\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/csarofeen\">@csarofeen</a> 's DDP code and it was a lot simpler. But our current DistributedDataParallel model has also covered other important use cases that can be beneficial with other backends such as gloo. And with the current DDP implementation, if we use single GPU binding and launch a DDP process per GPU, I didn't see much perf degradation by binding a single GPU to our current DDP implementation with multiple processes compared to Christian's DDP either. Please see</p>\n<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"291843910\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/4870\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/4870/hovercard\" href=\"https://github.com/pytorch/pytorch/pull/4870\">#4870</a></p>\n<p>for the perf comparisons with Christian's simpler version of DDP.  Also the current DDP  implementation also covers the single process multiple GPU case, which is another use case for users other than multi-process use case. So I guess it would probably be Ok to also add this mixed-precision support into the current DDP so that other use cases (such as gloo on ethernet with perf advantage due to the multi-threading reduction) can be covered as well.</p>\n<p>CC: <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>", "body_text": "@ngimel Thanks for the previous code review. Yes, I did see @csarofeen 's DDP code and it was a lot simpler. But our current DistributedDataParallel model has also covered other important use cases that can be beneficial with other backends such as gloo. And with the current DDP implementation, if we use single GPU binding and launch a DDP process per GPU, I didn't see much perf degradation by binding a single GPU to our current DDP implementation with multiple processes compared to Christian's DDP either. Please see\n#4870\nfor the perf comparisons with Christian's simpler version of DDP.  Also the current DDP  implementation also covers the single process multiple GPU case, which is another use case for users other than multi-process use case. So I guess it would probably be Ok to also add this mixed-precision support into the current DDP so that other use cases (such as gloo on ethernet with perf advantage due to the multi-threading reduction) can be covered as well.\nCC: @apaszke @soumith", "body": "@ngimel Thanks for the previous code review. Yes, I did see @csarofeen 's DDP code and it was a lot simpler. But our current DistributedDataParallel model has also covered other important use cases that can be beneficial with other backends such as gloo. And with the current DDP implementation, if we use single GPU binding and launch a DDP process per GPU, I didn't see much perf degradation by binding a single GPU to our current DDP implementation with multiple processes compared to Christian's DDP either. Please see \r\n\r\nhttps://github.com/pytorch/pytorch/pull/4870 \r\n\r\nfor the perf comparisons with Christian's simpler version of DDP.  Also the current DDP  implementation also covers the single process multiple GPU case, which is another use case for users other than multi-process use case. So I guess it would probably be Ok to also add this mixed-precision support into the current DDP so that other use cases (such as gloo on ethernet with perf advantage due to the multi-threading reduction) can be covered as well.\r\n\r\nCC: @apaszke @soumith "}