{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166139357", "pull_request_review_id": 94172698, "id": 166139357, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjEzOTM1Nw==", "diff_hunk": "@@ -117,14 +122,34 @@ def __init__(self, module, device_ids=None, output_device=None, dim=0):\n         else:\n             self._module_copies = [self.module]\n \n+        # Split parameters into type buckets so that parameter sync (broadcast)\n+        # can operates on mixed parameter types. (e.g. mixed half and float)\n+        self.param_type_buckets = {}\n+        for device_idx, module in enumerate(self._module_copies):\n+            for p in module.parameters():\n+                tp = type(p.data)\n+                if tp == torch.cuda.HalfTensor and \\\n+                        dist._backend != dist.dist_backend.NCCL:\n+                    raise RuntimeError(\"DistributedDataParallel currently only \"\n+                                       \"supports half precision parameters \"\n+                                       \"with NCCL backend\")\n+                if tp not in self.param_type_buckets:\n+                    self.param_type_buckets[tp] = \\\n+                        [[] for _ in range(len(self.device_ids))]\n+                # Add the parameter into the type bucket\n+                self.param_type_buckets[tp][device_idx].append(p)\n+\n         # Split parameters into buckets that will coalesce reductions\n-        # TODO: different types need different buckets\n-        t = None\n-        for p in self.module.parameters():\n-            tp = type(p.data)\n-            if t is not None and t is not tp:\n-                raise ValueError(\"DistributedDataParallel requires all parameters' data to be of the same type\")\n-            t = tp\n+        #\n+        # Note that the NCCL backend currently only supports a single reduction\n+        # bucket, so instead of splitting different Tensor types (half, float,\n+        # double, etc) into separate buckets, which will form multipel buckets,\n+        # we will split the parameters into reduction buckets regardless of\n+        # the data types here.", "path": "torch/nn/parallel/distributed.py", "position": null, "original_position": 62, "commit_id": "0a6d068ab9c8084ebc38a0f08e84929c68374cd5", "original_commit_id": "c2c0cc2fd6a88940c16be4b6cc7c60746e10a7f6", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "I don't think this bucketing strategy is good. Consider what will happen if you were to have parameters that are float, then half, then float again, etc. You could easily divide them into separate buckets per type, but now we'll be doing 2x more reductions that we could have", "created_at": "2018-02-05T22:56:57Z", "updated_at": "2018-11-23T15:39:07Z", "html_url": "https://github.com/pytorch/pytorch/pull/4891#discussion_r166139357", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4891", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166139357"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4891#discussion_r166139357"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4891"}}, "body_html": "<p>I don't think this bucketing strategy is good. Consider what will happen if you were to have parameters that are float, then half, then float again, etc. You could easily divide them into separate buckets per type, but now we'll be doing 2x more reductions that we could have</p>", "body_text": "I don't think this bucketing strategy is good. Consider what will happen if you were to have parameters that are float, then half, then float again, etc. You could easily divide them into separate buckets per type, but now we'll be doing 2x more reductions that we could have"}