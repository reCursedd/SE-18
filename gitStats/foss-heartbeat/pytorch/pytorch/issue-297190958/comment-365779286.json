{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/365779286", "html_url": "https://github.com/pytorch/pytorch/issues/5239#issuecomment-365779286", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5239", "id": 365779286, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NTc3OTI4Ng==", "user": {"login": "shiranD", "id": 9203906, "node_id": "MDQ6VXNlcjkyMDM5MDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9203906?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shiranD", "html_url": "https://github.com/shiranD", "followers_url": "https://api.github.com/users/shiranD/followers", "following_url": "https://api.github.com/users/shiranD/following{/other_user}", "gists_url": "https://api.github.com/users/shiranD/gists{/gist_id}", "starred_url": "https://api.github.com/users/shiranD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shiranD/subscriptions", "organizations_url": "https://api.github.com/users/shiranD/orgs", "repos_url": "https://api.github.com/users/shiranD/repos", "events_url": "https://api.github.com/users/shiranD/events{/privacy}", "received_events_url": "https://api.github.com/users/shiranD/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-14T23:27:20Z", "updated_at": "2018-02-14T23:27:20Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Wow! This is really good stuff that I learn from you.\nI now know that I will try one with a pure gpu not using the grad_penalty and another run with it (but using the cpu rnn as an object)\nHowever, when you mentioned a simple loss, did you mean just the Wasserstein loss and the original loss which is derived in Goodfellow paper?\nI mean, reading the Wass paper really convinced me the Wass loss makes a lot of sense. What is your experience with it?\n\nFrom: Tongzhou Wang &lt;notifications@github.com&gt;\nReply-To: pytorch/pytorch &lt;reply@reply.github.com&gt;\nDate: Wednesday, February 14, 2018 at 3:19 PM\nTo: pytorch/pytorch &lt;pytorch@noreply.github.com&gt;\nCc: Shiran Dudy &lt;dudy@ohsu.edu&gt;, Mention &lt;mention@noreply.github.com&gt;\nSubject: Re: [pytorch/pytorch] A bug that appears when running with CUDA and does not when using cpu (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"297190958\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5239\" href=\"https://github.com/pytorch/pytorch/issues/5239\">#5239</a>)\n\n\n<a class=\"user-mention\" href=\"https://github.com/shiranD\">@shiranD</a>&lt;<a href=\"https://github.com/shirand\">https://github.com/shirand</a>&gt; Yes, you can still use a GPU. That flag controls whether some operations are dispatched to our GPU implementation (THNN) or NVIDIA's GPU implementation (CUDNN). Usually the second is faster, that's why we dispatch to CUDNN by default. However, in your case, CUDNN RNN (which is considered as a single autograd op in PyTorch) doesn't support double backward, but the THNN RNN (which is just a series of autograd operations) does. So it works, albeit some potential slow downs.\n\nAbout the last sentence in my comment above, personally and empirically, I'm not sure how much gradient penalty helps. There are some intuition around it, such as smoothing D around the manifold etc., and some people have been reporting that it reduces sensitivity to hyperparameters a bit in certain z=&gt;img GAN settings. But I haven't found it to very effective in non z=&gt;img setting, such as simply adversarial loss. Considering that sequence data is a completely different domain that image, I'm just not very certain that whether it will help.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub&lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"297190958\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/5239\" href=\"https://github.com/pytorch/pytorch/issues/5239#issuecomment-365777683\">#5239 (comment)</a>&gt;, or mute the thread&lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AIxwwtz6cpfOKnWcPTYACHOKQzveoeqNks5tU2nlgaJpZM4SFu0B\">https://github.com/notifications/unsubscribe-auth/AIxwwtz6cpfOKnWcPTYACHOKQzveoeqNks5tU2nlgaJpZM4SFu0B</a>&gt;.</div>", "body_text": "Wow! This is really good stuff that I learn from you.\nI now know that I will try one with a pure gpu not using the grad_penalty and another run with it (but using the cpu rnn as an object)\nHowever, when you mentioned a simple loss, did you mean just the Wasserstein loss and the original loss which is derived in Goodfellow paper?\nI mean, reading the Wass paper really convinced me the Wass loss makes a lot of sense. What is your experience with it?\n\nFrom: Tongzhou Wang <notifications@github.com>\nReply-To: pytorch/pytorch <reply@reply.github.com>\nDate: Wednesday, February 14, 2018 at 3:19 PM\nTo: pytorch/pytorch <pytorch@noreply.github.com>\nCc: Shiran Dudy <dudy@ohsu.edu>, Mention <mention@noreply.github.com>\nSubject: Re: [pytorch/pytorch] A bug that appears when running with CUDA and does not when using cpu (#5239)\n\n\n@shiranD<https://github.com/shirand> Yes, you can still use a GPU. That flag controls whether some operations are dispatched to our GPU implementation (THNN) or NVIDIA's GPU implementation (CUDNN). Usually the second is faster, that's why we dispatch to CUDNN by default. However, in your case, CUDNN RNN (which is considered as a single autograd op in PyTorch) doesn't support double backward, but the THNN RNN (which is just a series of autograd operations) does. So it works, albeit some potential slow downs.\n\nAbout the last sentence in my comment above, personally and empirically, I'm not sure how much gradient penalty helps. There are some intuition around it, such as smoothing D around the manifold etc., and some people have been reporting that it reduces sensitivity to hyperparameters a bit in certain z=>img GAN settings. But I haven't found it to very effective in non z=>img setting, such as simply adversarial loss. Considering that sequence data is a completely different domain that image, I'm just not very certain that whether it will help.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<#5239 (comment)>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AIxwwtz6cpfOKnWcPTYACHOKQzveoeqNks5tU2nlgaJpZM4SFu0B>.", "body": "Wow! This is really good stuff that I learn from you.\r\nI now know that I will try one with a pure gpu not using the grad_penalty and another run with it (but using the cpu rnn as an object)\r\nHowever, when you mentioned a simple loss, did you mean just the Wasserstein loss and the original loss which is derived in Goodfellow paper?\r\nI mean, reading the Wass paper really convinced me the Wass loss makes a lot of sense. What is your experience with it?\r\n\r\nFrom: Tongzhou Wang <notifications@github.com>\r\nReply-To: pytorch/pytorch <reply@reply.github.com>\r\nDate: Wednesday, February 14, 2018 at 3:19 PM\r\nTo: pytorch/pytorch <pytorch@noreply.github.com>\r\nCc: Shiran Dudy <dudy@ohsu.edu>, Mention <mention@noreply.github.com>\r\nSubject: Re: [pytorch/pytorch] A bug that appears when running with CUDA and does not when using cpu (#5239)\r\n\r\n\r\n@shiranD<https://github.com/shirand> Yes, you can still use a GPU. That flag controls whether some operations are dispatched to our GPU implementation (THNN) or NVIDIA's GPU implementation (CUDNN). Usually the second is faster, that's why we dispatch to CUDNN by default. However, in your case, CUDNN RNN (which is considered as a single autograd op in PyTorch) doesn't support double backward, but the THNN RNN (which is just a series of autograd operations) does. So it works, albeit some potential slow downs.\r\n\r\nAbout the last sentence in my comment above, personally and empirically, I'm not sure how much gradient penalty helps. There are some intuition around it, such as smoothing D around the manifold etc., and some people have been reporting that it reduces sensitivity to hyperparameters a bit in certain z=>img GAN settings. But I haven't found it to very effective in non z=>img setting, such as simply adversarial loss. Considering that sequence data is a completely different domain that image, I'm just not very certain that whether it will help.\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/pytorch/pytorch/issues/5239#issuecomment-365777683>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AIxwwtz6cpfOKnWcPTYACHOKQzveoeqNks5tU2nlgaJpZM4SFu0B>.\r\n"}