{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5239", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5239/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5239/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5239/events", "html_url": "https://github.com/pytorch/pytorch/issues/5239", "id": 297190958, "node_id": "MDU6SXNzdWUyOTcxOTA5NTg=", "number": 5239, "title": "A bug that appears when running with CUDA and does not when using cpu", "user": {"login": "shiranD", "id": 9203906, "node_id": "MDQ6VXNlcjkyMDM5MDY=", "avatar_url": "https://avatars3.githubusercontent.com/u/9203906?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shiranD", "html_url": "https://github.com/shiranD", "followers_url": "https://api.github.com/users/shiranD/followers", "following_url": "https://api.github.com/users/shiranD/following{/other_user}", "gists_url": "https://api.github.com/users/shiranD/gists{/gist_id}", "starred_url": "https://api.github.com/users/shiranD/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shiranD/subscriptions", "organizations_url": "https://api.github.com/users/shiranD/orgs", "repos_url": "https://api.github.com/users/shiranD/repos", "events_url": "https://api.github.com/users/shiranD/events{/privacy}", "received_events_url": "https://api.github.com/users/shiranD/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2018-02-14T18:00:39Z", "updated_at": "2018-02-14T23:43:50Z", "closed_at": "2018-02-14T23:43:32Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nWhen I run a snippet like</p>\n<p>def calc_gradient_penalty(netD, real_data, fake_data):</p>\n<pre><code>wrdseq, batch, embd = real_data.shape\nalpha = torch.rand(wrdseq, batch, embd)\nalpha = alpha.expand(real_data.shape)\nalpha = alpha.cuda(gpu) if use_cuda else alpha\ninterpolates = alpha * real_data + ((1 - alpha) * fake_data)\n\nif use_cuda:\n    interpolates = interpolates.cuda(gpu)\ninterpolates = autograd.Variable(interpolates, requires_grad=True)\n\nhiddenD = netD.init_hidden(train_batch_size)\ndisc_interpolates, _ = netD(interpolates, hiddenD)\n\ngradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                          grad_outputs=torch.ones(disc_interpolates.shape).cuda(gpu) if use_cuda else torch.ones(\n                              disc_interpolates.size()),\n                          create_graph=True, retain_graph=True, only_inputs=True)[0]\n\ngradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\nreturn gradient_penalty\n</code></pre>\n<p>gradient_penalty = calc_gradient_penalty(netD, _data.data, _fake.data)<br>\ngradient_penalty.backward()`</p>\n<p>with CUDA I get:<br>\nCudnnRNNLegacyBackward is not differentiable twice<br>\nbut with cpu there is no error.<br>\nWhat should I do to fix it?<br>\nIt seems to be related to this cpp script: ./pytorch/torch/csrc/autograd/python_function.cpp<br>\nHow can I make CudnnRNNLegacyBackward differentiable twice since I assume that the equivalent object under cpu can do that as I get no errors</p>\n<p>Would appreciate your help so deeply<br>\nShiran</p>", "body_text": "Hi,\nWhen I run a snippet like\ndef calc_gradient_penalty(netD, real_data, fake_data):\nwrdseq, batch, embd = real_data.shape\nalpha = torch.rand(wrdseq, batch, embd)\nalpha = alpha.expand(real_data.shape)\nalpha = alpha.cuda(gpu) if use_cuda else alpha\ninterpolates = alpha * real_data + ((1 - alpha) * fake_data)\n\nif use_cuda:\n    interpolates = interpolates.cuda(gpu)\ninterpolates = autograd.Variable(interpolates, requires_grad=True)\n\nhiddenD = netD.init_hidden(train_batch_size)\ndisc_interpolates, _ = netD(interpolates, hiddenD)\n\ngradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                          grad_outputs=torch.ones(disc_interpolates.shape).cuda(gpu) if use_cuda else torch.ones(\n                              disc_interpolates.size()),\n                          create_graph=True, retain_graph=True, only_inputs=True)[0]\n\ngradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\nreturn gradient_penalty\n\ngradient_penalty = calc_gradient_penalty(netD, _data.data, _fake.data)\ngradient_penalty.backward()`\nwith CUDA I get:\nCudnnRNNLegacyBackward is not differentiable twice\nbut with cpu there is no error.\nWhat should I do to fix it?\nIt seems to be related to this cpp script: ./pytorch/torch/csrc/autograd/python_function.cpp\nHow can I make CudnnRNNLegacyBackward differentiable twice since I assume that the equivalent object under cpu can do that as I get no errors\nWould appreciate your help so deeply\nShiran", "body": "Hi,\r\nWhen I run a snippet like \r\n\r\ndef calc_gradient_penalty(netD, real_data, fake_data):\r\n\r\n    wrdseq, batch, embd = real_data.shape\r\n    alpha = torch.rand(wrdseq, batch, embd)\r\n    alpha = alpha.expand(real_data.shape)\r\n    alpha = alpha.cuda(gpu) if use_cuda else alpha\r\n    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\r\n\r\n    if use_cuda:\r\n        interpolates = interpolates.cuda(gpu)\r\n    interpolates = autograd.Variable(interpolates, requires_grad=True)\r\n\r\n    hiddenD = netD.init_hidden(train_batch_size)\r\n    disc_interpolates, _ = netD(interpolates, hiddenD)\r\n\r\n    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\r\n                              grad_outputs=torch.ones(disc_interpolates.shape).cuda(gpu) if use_cuda else torch.ones(\r\n                                  disc_interpolates.size()),\r\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\r\n\r\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\r\n    return gradient_penalty\r\n\r\ngradient_penalty = calc_gradient_penalty(netD, _data.data, _fake.data)\r\ngradient_penalty.backward()`\r\n\r\nwith CUDA I get:\r\nCudnnRNNLegacyBackward is not differentiable twice\r\nbut with cpu there is no error.\r\nWhat should I do to fix it? \r\nIt seems to be related to this cpp script: ./pytorch/torch/csrc/autograd/python_function.cpp\r\nHow can I make CudnnRNNLegacyBackward differentiable twice since I assume that the equivalent object under cpu can do that as I get no errors\r\n\r\nWould appreciate your help so deeply\r\nShiran"}