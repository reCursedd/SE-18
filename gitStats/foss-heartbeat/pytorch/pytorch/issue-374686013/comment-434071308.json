{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/434071308", "html_url": "https://github.com/pytorch/pytorch/issues/13214#issuecomment-434071308", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/13214", "id": 434071308, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDA3MTMwOA==", "user": {"login": "sailordiary", "id": 6279310, "node_id": "MDQ6VXNlcjYyNzkzMTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/6279310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sailordiary", "html_url": "https://github.com/sailordiary", "followers_url": "https://api.github.com/users/sailordiary/followers", "following_url": "https://api.github.com/users/sailordiary/following{/other_user}", "gists_url": "https://api.github.com/users/sailordiary/gists{/gist_id}", "starred_url": "https://api.github.com/users/sailordiary/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sailordiary/subscriptions", "organizations_url": "https://api.github.com/users/sailordiary/orgs", "repos_url": "https://api.github.com/users/sailordiary/repos", "events_url": "https://api.github.com/users/sailordiary/events{/privacy}", "received_events_url": "https://api.github.com/users/sailordiary/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-29T20:40:53Z", "updated_at": "2018-10-29T20:42:41Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a> After trying to create a MWE, I think I found the cause of the problem.</p>\n<pre><code># Run this program with CUDA_VISIBLE_DEVICES=0,1\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n\n    def forward(self, x, lens):\n        self.lstm.flatten_parameters()\n        lens = torch.tensor(lens, dtype=torch.int64, device=torch.device('cpu'))\n        x = torch.nn.utils.rnn.pack_padded_sequence(x, lens, batch_first=True)\n        out, _ = self.lstm(x)\n        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n\n        return out\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.lstm = LSTM(512, 512, 3)\n\n    def forward(self, x, x_lens):\n        x = self.lstm(x, x_lens)\n        return x\n\ndef train(model):\n    inputs = torch.randn(64, 245, 512).cuda() # (B, T, C)\n    lens = list(range(64, 0, -1))\n    outputs = model(inputs, lens)\n    \nmodel = Net()\nmodel = nn.DataParallel(model)\nmodel = model.cuda()\ntrain(model)\n</code></pre>\n<p>This time, after simplification, it gives a very illuminating error message:</p>\n<blockquote>\n<p>RuntimeError: Expected <code>len(lengths)</code> to be equal to batch_size, but got 64 (batch_size=32)</p>\n</blockquote>\n<p>I had incorrectly assumed that the <code>lens</code> parameter will be automatically distributed but apparently it won't. I also tried directly inputting a CPU LongTensor, rather than creating it inside the forward calls (i. e. <code>lens = torch.LongTensor(list(range(64, 0, -1)))</code>) but it didn't work either. What should I do to make this work in a multi-GPU environment?</p>\n<p>P. S. It is still not clear why my original program displays a different message (RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor), though. The gists of the programs are indentical. I will let you know if I do find out.</p>", "body_text": "@zou3519 After trying to create a MWE, I think I found the cause of the problem.\n# Run this program with CUDA_VISIBLE_DEVICES=0,1\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n\n    def forward(self, x, lens):\n        self.lstm.flatten_parameters()\n        lens = torch.tensor(lens, dtype=torch.int64, device=torch.device('cpu'))\n        x = torch.nn.utils.rnn.pack_padded_sequence(x, lens, batch_first=True)\n        out, _ = self.lstm(x)\n        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n\n        return out\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.lstm = LSTM(512, 512, 3)\n\n    def forward(self, x, x_lens):\n        x = self.lstm(x, x_lens)\n        return x\n\ndef train(model):\n    inputs = torch.randn(64, 245, 512).cuda() # (B, T, C)\n    lens = list(range(64, 0, -1))\n    outputs = model(inputs, lens)\n    \nmodel = Net()\nmodel = nn.DataParallel(model)\nmodel = model.cuda()\ntrain(model)\n\nThis time, after simplification, it gives a very illuminating error message:\n\nRuntimeError: Expected len(lengths) to be equal to batch_size, but got 64 (batch_size=32)\n\nI had incorrectly assumed that the lens parameter will be automatically distributed but apparently it won't. I also tried directly inputting a CPU LongTensor, rather than creating it inside the forward calls (i. e. lens = torch.LongTensor(list(range(64, 0, -1)))) but it didn't work either. What should I do to make this work in a multi-GPU environment?\nP. S. It is still not clear why my original program displays a different message (RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor), though. The gists of the programs are indentical. I will let you know if I do find out.", "body": "@zou3519 After trying to create a MWE, I think I found the cause of the problem.\r\n```\r\n# Run this program with CUDA_VISIBLE_DEVICES=0,1\r\nclass LSTM(nn.Module):\r\n    def __init__(self, input_size, hidden_size, num_layers):\r\n        super(LSTM, self).__init__()\r\n        self.hidden_size = hidden_size\r\n        self.num_layers = num_layers\r\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\r\n\r\n    def forward(self, x, lens):\r\n        self.lstm.flatten_parameters()\r\n        lens = torch.tensor(lens, dtype=torch.int64, device=torch.device('cpu'))\r\n        x = torch.nn.utils.rnn.pack_padded_sequence(x, lens, batch_first=True)\r\n        out, _ = self.lstm(x)\r\n        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\r\n\r\n        return out\r\n\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.lstm = LSTM(512, 512, 3)\r\n\r\n    def forward(self, x, x_lens):\r\n        x = self.lstm(x, x_lens)\r\n        return x\r\n\r\ndef train(model):\r\n    inputs = torch.randn(64, 245, 512).cuda() # (B, T, C)\r\n    lens = list(range(64, 0, -1))\r\n    outputs = model(inputs, lens)\r\n    \r\nmodel = Net()\r\nmodel = nn.DataParallel(model)\r\nmodel = model.cuda()\r\ntrain(model)\r\n```\r\nThis time, after simplification, it gives a very illuminating error message:\r\n> RuntimeError: Expected `len(lengths)` to be equal to batch_size, but got 64 (batch_size=32)\r\n\r\nI had incorrectly assumed that the `lens` parameter will be automatically distributed but apparently it won't. I also tried directly inputting a CPU LongTensor, rather than creating it inside the forward calls (i. e. `lens = torch.LongTensor(list(range(64, 0, -1)))`) but it didn't work either. What should I do to make this work in a multi-GPU environment?\r\n\r\nP. S. It is still not clear why my original program displays a different message (RuntimeError: 'lengths' argument should be a 1D CPU int64 tensor), though. The gists of the programs are indentical. I will let you know if I do find out."}