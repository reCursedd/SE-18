{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13214", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13214/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13214/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13214/events", "html_url": "https://github.com/pytorch/pytorch/issues/13214", "id": 374686013, "node_id": "MDU6SXNzdWUzNzQ2ODYwMTM=", "number": 13214, "title": "torch.nn.utils.rnn.pack_padded_sequence not working in multi-GPU environments", "user": {"login": "sailordiary", "id": 6279310, "node_id": "MDQ6VXNlcjYyNzkzMTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/6279310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sailordiary", "html_url": "https://github.com/sailordiary", "followers_url": "https://api.github.com/users/sailordiary/followers", "following_url": "https://api.github.com/users/sailordiary/following{/other_user}", "gists_url": "https://api.github.com/users/sailordiary/gists{/gist_id}", "starred_url": "https://api.github.com/users/sailordiary/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sailordiary/subscriptions", "organizations_url": "https://api.github.com/users/sailordiary/orgs", "repos_url": "https://api.github.com/users/sailordiary/repos", "events_url": "https://api.github.com/users/sailordiary/events{/privacy}", "received_events_url": "https://api.github.com/users/sailordiary/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-10-27T20:32:13Z", "updated_at": "2018-10-29T21:06:18Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n<p>torch.nn.utils.rnn.pack_padded_sequence not working properly</p>\n<h2>To Reproduce</h2>\n<p>Steps to reproduce the behavior:</p>\n<pre><code>    # lens is a Python list which contains the lengths of each sample in decreasing order\n    def forward(self, x, lens):\n        x_lens = torch.tensor(lens, dtype=torch.int64, device=torch.device('cpu'))\n        print (x_lens, x_lens.size(), x_lens.type())\n        x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True)\n        out, _ = self.lstm(x_packed)\n        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        out = self.fc(out)\n\n        return out\n</code></pre>\n<h2>Expected behavior</h2>\n<p>Above is an excerpt from a longer program, which upon running gives the error message \"'lengths' argument should be a 1D CPU int64 tensor\" (<a href=\"https://github.com/pytorch/pytorch/blob/4d62eef505770f1c866fa2205a5a05ab47865dda/aten/src/ATen/native/PackedSequence.cpp#L9\">link</a> to source).</p>\n<p>However, when I print the tensor, it clearly shows up as a torch.LongTensor on the CPU. As you can see, I have tried everything I can to ensure that this is the case, but the problem persists. Does this have anything with the recent C++ porting?</p>\n<p>Here is the output:</p>\n<pre><code> tensor([228, 228, 214, 212, 209, 208, 207, 203, 202, 200, 198, 197, 187, 187,\n        182, 178, 175, 175, 174, 173, 173, 172, 169, 167, 162, 161, 161, 160,\n        158, 157, 157, 157, 156, 155, 154, 152, 152, 152, 151, 151, 151, 150,\n        149, 149, 148, 147, 146, 145, 145, 145, 141, 140, 140, 140, 138, 137,\n        136, 135, 134, 134, 134, 134, 132, 131, 130, 130, 129, 129, 128, 128,\n        127, 125, 123, 123, 122, 121, 121, 120, 120, 118, 117, 116, 116, 110,\n        110, 106, 104, 102, 100, 100,  97,  97,  96,  95,  92,  91,  87,  85,\n         82,  82], device='cpu') torch.Size([100]) torch.LongTensor\nTraceback (most recent call last):\n  File \"main_CTC.py\", line 320, in &lt;module&gt;\n    main()\n  ... (_omitted_)\n  File \"/home/sailordiary/tools/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 477, in __call\n__\n    result = self.forward(*input, **kwargs)\n  File \"/home/sailordiary/code/******/ctc.py\", line 174, in forward\n    x = self.lstm(x, x_lens)\n  File \"/home/sailordiary/tools/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 477, in __call\n__\n    result = self.forward(*input, **kwargs)\n  File \"/home/sailordiary/code/******/ctc.py\", line 121, in forward\n    x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True)\n  File \"/home/sailordiary/tools/anaconda3/lib/python3.6/site-packages/torch/nn/utils/rnn.py\", line 148, in pack_padded\n_sequence\n    return PackedSequence(torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first))\nRuntimeError: 'lengths' argument should be a 1D CPU int64 tensor\n</code></pre>\n<h2>Environment</h2>\n<p>PyTorch version: 1.0.0.dev20181027<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 8.0.61</p>\n<p>OS: Ubuntu 16.04.3 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.3.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 8.0.44<br>\nGPU models and configuration:<br>\nGPU 0: TITAN Xp<br>\nGPU 1: TITAN Xp<br>\nGPU 2: TITAN Xp<br>\nGPU 3: TITAN Xp</p>\n<p>Nvidia driver version: 384.130<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip] Could not collect<br>\n[conda] cuda80                    1.0                  h205658b_0    pytorch<br>\n[conda] magma-cuda80              2.3.0                         1    pytorch<br>\n[conda] pytorch-ignite            0.1.0                     <br>\n[conda] pytorch-nightly           1.0.0.dev20181027 py3.6_cuda8.0.61_cudnn7.1.2_0  [cuda80]  pytorch<br>\n[conda] torchfile                 0.1.0                     </p>\n<h2>Additional context</h2>\n<p>None.</p>", "body_text": "\ud83d\udc1b Bug\ntorch.nn.utils.rnn.pack_padded_sequence not working properly\nTo Reproduce\nSteps to reproduce the behavior:\n    # lens is a Python list which contains the lengths of each sample in decreasing order\n    def forward(self, x, lens):\n        x_lens = torch.tensor(lens, dtype=torch.int64, device=torch.device('cpu'))\n        print (x_lens, x_lens.size(), x_lens.type())\n        x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True)\n        out, _ = self.lstm(x_packed)\n        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n        out = self.fc(out)\n\n        return out\n\nExpected behavior\nAbove is an excerpt from a longer program, which upon running gives the error message \"'lengths' argument should be a 1D CPU int64 tensor\" (link to source).\nHowever, when I print the tensor, it clearly shows up as a torch.LongTensor on the CPU. As you can see, I have tried everything I can to ensure that this is the case, but the problem persists. Does this have anything with the recent C++ porting?\nHere is the output:\n tensor([228, 228, 214, 212, 209, 208, 207, 203, 202, 200, 198, 197, 187, 187,\n        182, 178, 175, 175, 174, 173, 173, 172, 169, 167, 162, 161, 161, 160,\n        158, 157, 157, 157, 156, 155, 154, 152, 152, 152, 151, 151, 151, 150,\n        149, 149, 148, 147, 146, 145, 145, 145, 141, 140, 140, 140, 138, 137,\n        136, 135, 134, 134, 134, 134, 132, 131, 130, 130, 129, 129, 128, 128,\n        127, 125, 123, 123, 122, 121, 121, 120, 120, 118, 117, 116, 116, 110,\n        110, 106, 104, 102, 100, 100,  97,  97,  96,  95,  92,  91,  87,  85,\n         82,  82], device='cpu') torch.Size([100]) torch.LongTensor\nTraceback (most recent call last):\n  File \"main_CTC.py\", line 320, in <module>\n    main()\n  ... (_omitted_)\n  File \"/home/sailordiary/tools/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 477, in __call\n__\n    result = self.forward(*input, **kwargs)\n  File \"/home/sailordiary/code/******/ctc.py\", line 174, in forward\n    x = self.lstm(x, x_lens)\n  File \"/home/sailordiary/tools/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 477, in __call\n__\n    result = self.forward(*input, **kwargs)\n  File \"/home/sailordiary/code/******/ctc.py\", line 121, in forward\n    x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True)\n  File \"/home/sailordiary/tools/anaconda3/lib/python3.6/site-packages/torch/nn/utils/rnn.py\", line 148, in pack_padded\n_sequence\n    return PackedSequence(torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first))\nRuntimeError: 'lengths' argument should be a 1D CPU int64 tensor\n\nEnvironment\nPyTorch version: 1.0.0.dev20181027\nIs debug build: No\nCUDA used to build PyTorch: 8.0.61\nOS: Ubuntu 16.04.3 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.3.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 8.0.44\nGPU models and configuration:\nGPU 0: TITAN Xp\nGPU 1: TITAN Xp\nGPU 2: TITAN Xp\nGPU 3: TITAN Xp\nNvidia driver version: 384.130\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip] Could not collect\n[conda] cuda80                    1.0                  h205658b_0    pytorch\n[conda] magma-cuda80              2.3.0                         1    pytorch\n[conda] pytorch-ignite            0.1.0                     \n[conda] pytorch-nightly           1.0.0.dev20181027 py3.6_cuda8.0.61_cudnn7.1.2_0  [cuda80]  pytorch\n[conda] torchfile                 0.1.0                     \nAdditional context\nNone.", "body": "## \ud83d\udc1b Bug\r\n\r\ntorch.nn.utils.rnn.pack_padded_sequence not working properly\r\n\r\n## To Reproduce\r\n\r\nSteps to reproduce the behavior:\r\n\r\n```\r\n    # lens is a Python list which contains the lengths of each sample in decreasing order\r\n    def forward(self, x, lens):\r\n        x_lens = torch.tensor(lens, dtype=torch.int64, device=torch.device('cpu'))\r\n        print (x_lens, x_lens.size(), x_lens.type())\r\n        x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True)\r\n        out, _ = self.lstm(x_packed)\r\n        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\r\n        out = self.fc(out)\r\n\r\n        return out\r\n```\r\n\r\n## Expected behavior\r\n\r\nAbove is an excerpt from a longer program, which upon running gives the error message \"'lengths' argument should be a 1D CPU int64 tensor\" ([link](https://github.com/pytorch/pytorch/blob/4d62eef505770f1c866fa2205a5a05ab47865dda/aten/src/ATen/native/PackedSequence.cpp#L9) to source).\r\n\r\nHowever, when I print the tensor, it clearly shows up as a torch.LongTensor on the CPU. As you can see, I have tried everything I can to ensure that this is the case, but the problem persists. Does this have anything with the recent C++ porting?\r\n\r\nHere is the output:\r\n\r\n```\r\n tensor([228, 228, 214, 212, 209, 208, 207, 203, 202, 200, 198, 197, 187, 187,\r\n        182, 178, 175, 175, 174, 173, 173, 172, 169, 167, 162, 161, 161, 160,\r\n        158, 157, 157, 157, 156, 155, 154, 152, 152, 152, 151, 151, 151, 150,\r\n        149, 149, 148, 147, 146, 145, 145, 145, 141, 140, 140, 140, 138, 137,\r\n        136, 135, 134, 134, 134, 134, 132, 131, 130, 130, 129, 129, 128, 128,\r\n        127, 125, 123, 123, 122, 121, 121, 120, 120, 118, 117, 116, 116, 110,\r\n        110, 106, 104, 102, 100, 100,  97,  97,  96,  95,  92,  91,  87,  85,\r\n         82,  82], device='cpu') torch.Size([100]) torch.LongTensor\r\nTraceback (most recent call last):\r\n  File \"main_CTC.py\", line 320, in <module>\r\n    main()\r\n  ... (_omitted_)\r\n  File \"/home/sailordiary/tools/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 477, in __call\r\n__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/sailordiary/code/******/ctc.py\", line 174, in forward\r\n    x = self.lstm(x, x_lens)\r\n  File \"/home/sailordiary/tools/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 477, in __call\r\n__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/sailordiary/code/******/ctc.py\", line 121, in forward\r\n    x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True)\r\n  File \"/home/sailordiary/tools/anaconda3/lib/python3.6/site-packages/torch/nn/utils/rnn.py\", line 148, in pack_padded\r\n_sequence\r\n    return PackedSequence(torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first))\r\nRuntimeError: 'lengths' argument should be a 1D CPU int64 tensor\r\n```\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0.dev20181027\r\nIs debug build: No\r\nCUDA used to build PyTorch: 8.0.61\r\n\r\nOS: Ubuntu 16.04.3 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.3.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 8.0.44\r\nGPU models and configuration:\r\nGPU 0: TITAN Xp\r\nGPU 1: TITAN Xp\r\nGPU 2: TITAN Xp\r\nGPU 3: TITAN Xp\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] Could not collect\r\n[conda] cuda80                    1.0                  h205658b_0    pytorch\r\n[conda] magma-cuda80              2.3.0                         1    pytorch\r\n[conda] pytorch-ignite            0.1.0                     <pip>\r\n[conda] pytorch-nightly           1.0.0.dev20181027 py3.6_cuda8.0.61_cudnn7.1.2_0  [cuda80]  pytorch\r\n[conda] torchfile                 0.1.0                     <pip>\r\n\r\n## Additional context\r\n\r\nNone."}