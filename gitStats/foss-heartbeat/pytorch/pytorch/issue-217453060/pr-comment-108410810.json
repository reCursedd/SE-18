{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/108410810", "pull_request_review_id": 29438546, "id": 108410810, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwODQxMDgxMA==", "diff_hunk": "@@ -591,4 +591,22 @@ def backward(self, grad_output):\n         return grad_input\n \n \n+class Cumsum(Function):\n+\n+    def __init__(self, dim):\n+        super(Cumsum, self).__init__()\n+        self.dim = dim\n+\n+    def forward(self, input):\n+        return torch.cumsum(input, dim=self.dim)\n+\n+    def backward(self, grad_output):\n+        grad_sum = torch.sum(grad_output, dim=self.dim)\n+\n+        grad_input = torch.cumsum(-grad_output, dim=self.dim)\n+        grad_input += grad_sum.expand_as(grad_input)", "path": "torch/autograd/_functions/tensor.py", "position": null, "original_position": 17, "commit_id": "a00ad072fe5f8699c153aa422cfdcdc2b2c03e3d", "original_commit_id": "4d0c91cdcdb9e120bfda4343ff518b3647941a4c", "user": {"login": "bunelr", "id": 3354626, "node_id": "MDQ6VXNlcjMzNTQ2MjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3354626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bunelr", "html_url": "https://github.com/bunelr", "followers_url": "https://api.github.com/users/bunelr/followers", "following_url": "https://api.github.com/users/bunelr/following{/other_user}", "gists_url": "https://api.github.com/users/bunelr/gists{/gist_id}", "starred_url": "https://api.github.com/users/bunelr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bunelr/subscriptions", "organizations_url": "https://api.github.com/users/bunelr/orgs", "repos_url": "https://api.github.com/users/bunelr/repos", "events_url": "https://api.github.com/users/bunelr/events{/privacy}", "received_events_url": "https://api.github.com/users/bunelr/received_events", "type": "User", "site_admin": false}, "body": "But this would allow a useless summation over the `grad_output` tensor.\r\n\r\nWhat if we:\r\n- select the last element according to self.dim\r\n- clone it so that it doesn't share the memory anymore\r\n- then expand it and subtract it\r\n\r\nThis would mean removing one summation over the whole tensor, at the cost of a clone of the reduced tensor, which is smaller?", "created_at": "2017-03-28T12:51:31Z", "updated_at": "2018-11-23T15:32:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/1122#discussion_r108410810", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1122", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/108410810"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1122#discussion_r108410810"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1122"}}, "body_html": "<p>But this would allow a useless summation over the <code>grad_output</code> tensor.</p>\n<p>What if we:</p>\n<ul>\n<li>select the last element according to self.dim</li>\n<li>clone it so that it doesn't share the memory anymore</li>\n<li>then expand it and subtract it</li>\n</ul>\n<p>This would mean removing one summation over the whole tensor, at the cost of a clone of the reduced tensor, which is smaller?</p>", "body_text": "But this would allow a useless summation over the grad_output tensor.\nWhat if we:\n\nselect the last element according to self.dim\nclone it so that it doesn't share the memory anymore\nthen expand it and subtract it\n\nThis would mean removing one summation over the whole tensor, at the cost of a clone of the reduced tensor, which is smaller?", "in_reply_to_id": 108367134}