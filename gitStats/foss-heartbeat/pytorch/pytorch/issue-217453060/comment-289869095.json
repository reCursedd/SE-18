{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/289869095", "html_url": "https://github.com/pytorch/pytorch/pull/1122#issuecomment-289869095", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1122", "id": 289869095, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTg2OTA5NQ==", "user": {"login": "bunelr", "id": 3354626, "node_id": "MDQ6VXNlcjMzNTQ2MjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3354626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bunelr", "html_url": "https://github.com/bunelr", "followers_url": "https://api.github.com/users/bunelr/followers", "following_url": "https://api.github.com/users/bunelr/following{/other_user}", "gists_url": "https://api.github.com/users/bunelr/gists{/gist_id}", "starred_url": "https://api.github.com/users/bunelr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bunelr/subscriptions", "organizations_url": "https://api.github.com/users/bunelr/orgs", "repos_url": "https://api.github.com/users/bunelr/repos", "events_url": "https://api.github.com/users/bunelr/events{/privacy}", "received_events_url": "https://api.github.com/users/bunelr/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-28T18:53:48Z", "updated_at": "2017-03-28T18:53:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15841449\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ngimel\">@ngimel</a><br>\nThis approach would indeed be preferable as its a more straightforward implementation, however it is lacking a reverse operation over the grad_input to bring back the elements in their original order now that their gradient has been computed.</p>\n<p>I have tested it and it seems to be slower, especially when doing the cumsum along a dimension that is not the first one.</p>\n<p>Benchmarking code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> time\n\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Function, Variable\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CumsumClone</span>(<span class=\"pl-e\">Function</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">dim</span>):\n        <span class=\"pl-c1\">super</span>(CumsumClone, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.dim <span class=\"pl-k\">=</span> dim\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-k\">return</span> torch.cumsum(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dim)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        grad_input <span class=\"pl-k\">=</span> torch.cumsum(<span class=\"pl-k\">-</span>grad_output, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dim)\n\n        end_idx <span class=\"pl-k\">=</span> grad_input.size(<span class=\"pl-c1\">self</span>.dim) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n        grad_sum <span class=\"pl-k\">=</span> grad_input.select(<span class=\"pl-c1\">self</span>.dim, end_idx)\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(grad_output.size()) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span>:\n            grad_input <span class=\"pl-k\">-=</span> grad_sum\n        <span class=\"pl-k\">else</span>:\n            grad_input <span class=\"pl-k\">-=</span> grad_sum.unsqueeze(<span class=\"pl-c1\">self</span>.dim).expand_as(grad_input)\n        grad_input <span class=\"pl-k\">+=</span> grad_output\n        <span class=\"pl-k\">return</span> grad_input\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CumsumExplicitSum</span>(<span class=\"pl-e\">Function</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">dim</span>):\n        <span class=\"pl-c1\">super</span>(CumsumExplicitSum, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.dim <span class=\"pl-k\">=</span> dim\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-k\">return</span> torch.cumsum(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dim)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        grad_sum <span class=\"pl-k\">=</span> torch.sum(grad_output, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dim)\n\n        grad_input <span class=\"pl-k\">=</span> torch.cumsum(<span class=\"pl-k\">-</span>grad_output, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dim)\n        grad_input <span class=\"pl-k\">+=</span> grad_sum.expand_as(grad_input)\n        grad_input <span class=\"pl-k\">+=</span> grad_output\n        <span class=\"pl-k\">return</span> grad_input\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">CumsumReverse</span>(<span class=\"pl-e\">Function</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">dim</span>):\n        <span class=\"pl-c1\">super</span>(CumsumReverse, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.dim <span class=\"pl-k\">=</span> dim\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        <span class=\"pl-k\">return</span> torch.cumsum(<span class=\"pl-c1\">input</span>, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dim)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n        end_idx <span class=\"pl-k\">=</span> grad_output.size(<span class=\"pl-c1\">self</span>.dim) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n        indices <span class=\"pl-k\">=</span> torch.LongTensor(<span class=\"pl-c1\">range</span>(end_idx, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n        grad_input <span class=\"pl-k\">=</span> torch.index_select(grad_output, <span class=\"pl-c1\">self</span>.dim, indices)\n        grad_input <span class=\"pl-k\">=</span> torch.cumsum(grad_input, <span class=\"pl-c1\">self</span>.dim)\n        grad_input <span class=\"pl-k\">=</span> torch.index_select(grad_input, <span class=\"pl-c1\">self</span>.dim, indices)\n        <span class=\"pl-k\">return</span> grad_input\n\nto_test <span class=\"pl-k\">=</span> [CumsumClone, CumsumExplicitSum,  CumsumReverse]\n\nnb_runs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nsize <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span> )\ninp <span class=\"pl-k\">=</span> Variable(torch.randn(size).double(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ngrad_out <span class=\"pl-k\">=</span> torch.randn(size).double()\n\ndim_to_test <span class=\"pl-k\">=</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(size))\n<span class=\"pl-k\">for</span> dim <span class=\"pl-k\">in</span> dim_to_test:\n    <span class=\"pl-k\">for</span> cumsum_cls <span class=\"pl-k\">in</span> to_test:\n        start <span class=\"pl-k\">=</span> time.time()\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(nb_runs):\n            cum_sum_func <span class=\"pl-k\">=</span> cumsum_cls(dim)\n            out <span class=\"pl-k\">=</span> cum_sum_func(inp)\n            grad_in <span class=\"pl-k\">=</span> out.backward(grad_out)\n        end <span class=\"pl-k\">=</span> time.time()\n        time.sleep(<span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Cumsum along dim <span class=\"pl-c1\">{}</span>, Average time for <span class=\"pl-c1\">{}</span>:<span class=\"pl-cce\">\\t</span><span class=\"pl-c1\">{}</span>s per pass<span class=\"pl-pds\">\"</span></span>.format(dim,\n                                                                               cumsum_cls.<span class=\"pl-c1\">__name__</span>,\n                                                                               (end <span class=\"pl-k\">-</span> start) <span class=\"pl-k\">/</span> nb_runs))\n    <span class=\"pl-c1\">print</span>()\n</pre></div>\n<p>Benchmarking results (desktop):</p>\n<pre><code>$ python bench.py \nCumsum along dim 0, Average time for CumsumClone:\t0.01940817356109619s per pass\nCumsum along dim 0, Average time for CumsumExplicitSum:\t0.022341022491455077s per pass\nCumsum along dim 0, Average time for CumsumReverse:\t0.01978654384613037s per pass\n\nCumsum along dim 1, Average time for CumsumClone:\t0.019124305248260497s per pass\nCumsum along dim 1, Average time for CumsumExplicitSum:\t0.021526150703430176s per pass\nCumsum along dim 1, Average time for CumsumReverse:\t0.02462069034576416s per pass\n\nCumsum along dim 2, Average time for CumsumClone:\t0.012899844646453858s per pass\nCumsum along dim 2, Average time for CumsumExplicitSum:\t0.015139245986938476s per pass\nCumsum along dim 2, Average time for CumsumReverse:\t0.022597653865814207s per pass\n\n</code></pre>\n<p>If I rerun it several times, the trends I see are:<br>\n-&gt; Clone is marginally faster than ExplicitSum (can be opposite on certains runs but this holds as a trend)<br>\n-&gt; When doing the cumsum over the 0-th dimension, Reverse is in the same ballpark as the other two.<br>\n-&gt; When doing the cumsum over another dimension, Reverse usually becomes slower than the other two</p>\n<p>Additionally, if I do the same test over a 1D vector of size 10000, I get the following result:</p>\n<pre><code>$ python bench.py \nCumsum along dim 0, Average time for CumsumClone:\t0.0003379058837890625s per pass\nCumsum along dim 0, Average time for CumsumExplicitSum:\t0.0003282761573791504s per pass\nCumsum along dim 0, Average time for CumsumReverse:\t0.0019441533088684082s per pass\n</code></pre>\n<p>Therefore I don't think the Reverse implementation should be used.</p>", "body_text": "@ngimel\nThis approach would indeed be preferable as its a more straightforward implementation, however it is lacking a reverse operation over the grad_input to bring back the elements in their original order now that their gradient has been computed.\nI have tested it and it seems to be slower, especially when doing the cumsum along a dimension that is not the first one.\nBenchmarking code:\nimport torch\nimport time\n\nfrom torch.autograd import Function, Variable\n\nclass CumsumClone(Function):\n\n    def __init__(self, dim):\n        super(CumsumClone, self).__init__()\n        self.dim = dim\n\n    def forward(self, input):\n        return torch.cumsum(input, dim=self.dim)\n\n    def backward(self, grad_output):\n        grad_input = torch.cumsum(-grad_output, dim=self.dim)\n\n        end_idx = grad_input.size(self.dim) - 1\n        grad_sum = grad_input.select(self.dim, end_idx)\n        if len(grad_output.size()) == 1:\n            grad_input -= grad_sum\n        else:\n            grad_input -= grad_sum.unsqueeze(self.dim).expand_as(grad_input)\n        grad_input += grad_output\n        return grad_input\n\nclass CumsumExplicitSum(Function):\n\n    def __init__(self, dim):\n        super(CumsumExplicitSum, self).__init__()\n        self.dim = dim\n\n    def forward(self, input):\n        return torch.cumsum(input, dim=self.dim)\n\n    def backward(self, grad_output):\n        grad_sum = torch.sum(grad_output, dim=self.dim)\n\n        grad_input = torch.cumsum(-grad_output, dim=self.dim)\n        grad_input += grad_sum.expand_as(grad_input)\n        grad_input += grad_output\n        return grad_input\n\nclass CumsumReverse(Function):\n\n    def __init__(self, dim):\n        super(CumsumReverse, self).__init__()\n        self.dim = dim\n\n    def forward(self, input):\n        return torch.cumsum(input, dim=self.dim)\n\n    def backward(self, grad_output):\n        end_idx = grad_output.size(self.dim) - 1\n        indices = torch.LongTensor(range(end_idx, -1, -1))\n        grad_input = torch.index_select(grad_output, self.dim, indices)\n        grad_input = torch.cumsum(grad_input, self.dim)\n        grad_input = torch.index_select(grad_input, self.dim, indices)\n        return grad_input\n\nto_test = [CumsumClone, CumsumExplicitSum,  CumsumReverse]\n\nnb_runs = 100\nsize = (100, 100, 100 )\ninp = Variable(torch.randn(size).double(), requires_grad=True)\ngrad_out = torch.randn(size).double()\n\ndim_to_test = range(len(size))\nfor dim in dim_to_test:\n    for cumsum_cls in to_test:\n        start = time.time()\n        for i in range(nb_runs):\n            cum_sum_func = cumsum_cls(dim)\n            out = cum_sum_func(inp)\n            grad_in = out.backward(grad_out)\n        end = time.time()\n        time.sleep(1)\n        print(\"Cumsum along dim {}, Average time for {}:\\t{}s per pass\".format(dim,\n                                                                               cumsum_cls.__name__,\n                                                                               (end - start) / nb_runs))\n    print()\n\nBenchmarking results (desktop):\n$ python bench.py \nCumsum along dim 0, Average time for CumsumClone:\t0.01940817356109619s per pass\nCumsum along dim 0, Average time for CumsumExplicitSum:\t0.022341022491455077s per pass\nCumsum along dim 0, Average time for CumsumReverse:\t0.01978654384613037s per pass\n\nCumsum along dim 1, Average time for CumsumClone:\t0.019124305248260497s per pass\nCumsum along dim 1, Average time for CumsumExplicitSum:\t0.021526150703430176s per pass\nCumsum along dim 1, Average time for CumsumReverse:\t0.02462069034576416s per pass\n\nCumsum along dim 2, Average time for CumsumClone:\t0.012899844646453858s per pass\nCumsum along dim 2, Average time for CumsumExplicitSum:\t0.015139245986938476s per pass\nCumsum along dim 2, Average time for CumsumReverse:\t0.022597653865814207s per pass\n\n\nIf I rerun it several times, the trends I see are:\n-> Clone is marginally faster than ExplicitSum (can be opposite on certains runs but this holds as a trend)\n-> When doing the cumsum over the 0-th dimension, Reverse is in the same ballpark as the other two.\n-> When doing the cumsum over another dimension, Reverse usually becomes slower than the other two\nAdditionally, if I do the same test over a 1D vector of size 10000, I get the following result:\n$ python bench.py \nCumsum along dim 0, Average time for CumsumClone:\t0.0003379058837890625s per pass\nCumsum along dim 0, Average time for CumsumExplicitSum:\t0.0003282761573791504s per pass\nCumsum along dim 0, Average time for CumsumReverse:\t0.0019441533088684082s per pass\n\nTherefore I don't think the Reverse implementation should be used.", "body": "@ngimel \r\nThis approach would indeed be preferable as its a more straightforward implementation, however it is lacking a reverse operation over the grad_input to bring back the elements in their original order now that their gradient has been computed.\r\n\r\nI have tested it and it seems to be slower, especially when doing the cumsum along a dimension that is not the first one.\r\n\r\n\r\nBenchmarking code:\r\n\r\n```python\r\nimport torch\r\nimport time\r\n\r\nfrom torch.autograd import Function, Variable\r\n\r\nclass CumsumClone(Function):\r\n\r\n    def __init__(self, dim):\r\n        super(CumsumClone, self).__init__()\r\n        self.dim = dim\r\n\r\n    def forward(self, input):\r\n        return torch.cumsum(input, dim=self.dim)\r\n\r\n    def backward(self, grad_output):\r\n        grad_input = torch.cumsum(-grad_output, dim=self.dim)\r\n\r\n        end_idx = grad_input.size(self.dim) - 1\r\n        grad_sum = grad_input.select(self.dim, end_idx)\r\n        if len(grad_output.size()) == 1:\r\n            grad_input -= grad_sum\r\n        else:\r\n            grad_input -= grad_sum.unsqueeze(self.dim).expand_as(grad_input)\r\n        grad_input += grad_output\r\n        return grad_input\r\n\r\nclass CumsumExplicitSum(Function):\r\n\r\n    def __init__(self, dim):\r\n        super(CumsumExplicitSum, self).__init__()\r\n        self.dim = dim\r\n\r\n    def forward(self, input):\r\n        return torch.cumsum(input, dim=self.dim)\r\n\r\n    def backward(self, grad_output):\r\n        grad_sum = torch.sum(grad_output, dim=self.dim)\r\n\r\n        grad_input = torch.cumsum(-grad_output, dim=self.dim)\r\n        grad_input += grad_sum.expand_as(grad_input)\r\n        grad_input += grad_output\r\n        return grad_input\r\n\r\nclass CumsumReverse(Function):\r\n\r\n    def __init__(self, dim):\r\n        super(CumsumReverse, self).__init__()\r\n        self.dim = dim\r\n\r\n    def forward(self, input):\r\n        return torch.cumsum(input, dim=self.dim)\r\n\r\n    def backward(self, grad_output):\r\n        end_idx = grad_output.size(self.dim) - 1\r\n        indices = torch.LongTensor(range(end_idx, -1, -1))\r\n        grad_input = torch.index_select(grad_output, self.dim, indices)\r\n        grad_input = torch.cumsum(grad_input, self.dim)\r\n        grad_input = torch.index_select(grad_input, self.dim, indices)\r\n        return grad_input\r\n\r\nto_test = [CumsumClone, CumsumExplicitSum,  CumsumReverse]\r\n\r\nnb_runs = 100\r\nsize = (100, 100, 100 )\r\ninp = Variable(torch.randn(size).double(), requires_grad=True)\r\ngrad_out = torch.randn(size).double()\r\n\r\ndim_to_test = range(len(size))\r\nfor dim in dim_to_test:\r\n    for cumsum_cls in to_test:\r\n        start = time.time()\r\n        for i in range(nb_runs):\r\n            cum_sum_func = cumsum_cls(dim)\r\n            out = cum_sum_func(inp)\r\n            grad_in = out.backward(grad_out)\r\n        end = time.time()\r\n        time.sleep(1)\r\n        print(\"Cumsum along dim {}, Average time for {}:\\t{}s per pass\".format(dim,\r\n                                                                               cumsum_cls.__name__,\r\n                                                                               (end - start) / nb_runs))\r\n    print()\r\n\r\n```\r\n\r\nBenchmarking results (desktop): \r\n\r\n```\r\n$ python bench.py \r\nCumsum along dim 0, Average time for CumsumClone:\t0.01940817356109619s per pass\r\nCumsum along dim 0, Average time for CumsumExplicitSum:\t0.022341022491455077s per pass\r\nCumsum along dim 0, Average time for CumsumReverse:\t0.01978654384613037s per pass\r\n\r\nCumsum along dim 1, Average time for CumsumClone:\t0.019124305248260497s per pass\r\nCumsum along dim 1, Average time for CumsumExplicitSum:\t0.021526150703430176s per pass\r\nCumsum along dim 1, Average time for CumsumReverse:\t0.02462069034576416s per pass\r\n\r\nCumsum along dim 2, Average time for CumsumClone:\t0.012899844646453858s per pass\r\nCumsum along dim 2, Average time for CumsumExplicitSum:\t0.015139245986938476s per pass\r\nCumsum along dim 2, Average time for CumsumReverse:\t0.022597653865814207s per pass\r\n\r\n```\r\n\r\nIf I rerun it several times, the trends I see are:\r\n-> Clone is marginally faster than ExplicitSum (can be opposite on certains runs but this holds as a trend)\r\n-> When doing the cumsum over the 0-th dimension, Reverse is in the same ballpark as the other two.\r\n-> When doing the cumsum over another dimension, Reverse usually becomes slower than the other two\r\n\r\n\r\nAdditionally, if I do the same test over a 1D vector of size 10000, I get the following result:\r\n```\r\n$ python bench.py \r\nCumsum along dim 0, Average time for CumsumClone:\t0.0003379058837890625s per pass\r\nCumsum along dim 0, Average time for CumsumExplicitSum:\t0.0003282761573791504s per pass\r\nCumsum along dim 0, Average time for CumsumReverse:\t0.0019441533088684082s per pass\r\n```\r\n\r\nTherefore I don't think the Reverse implementation should be used."}