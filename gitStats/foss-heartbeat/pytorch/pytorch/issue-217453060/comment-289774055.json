{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/289774055", "html_url": "https://github.com/pytorch/pytorch/pull/1122#issuecomment-289774055", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1122", "id": 289774055, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTc3NDA1NQ==", "user": {"login": "bunelr", "id": 3354626, "node_id": "MDQ6VXNlcjMzNTQ2MjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3354626?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bunelr", "html_url": "https://github.com/bunelr", "followers_url": "https://api.github.com/users/bunelr/followers", "following_url": "https://api.github.com/users/bunelr/following{/other_user}", "gists_url": "https://api.github.com/users/bunelr/gists{/gist_id}", "starred_url": "https://api.github.com/users/bunelr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bunelr/subscriptions", "organizations_url": "https://api.github.com/users/bunelr/orgs", "repos_url": "https://api.github.com/users/bunelr/repos", "events_url": "https://api.github.com/users/bunelr/events{/privacy}", "received_events_url": "https://api.github.com/users/bunelr/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-28T13:44:21Z", "updated_at": "2017-03-28T13:44:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Added the method for<code>Variable</code>, as well as the associated test.</p>\n<p>With regards to the implementation, i did a quick time measurement on the one doing the sum and the one cloning the end of the cumsum, the results are as follows:</p>\n<ul>\n<li>With an explicit sum: 0.021312472820281984</li>\n<li>With cloning of the end of the cumsum: 0.019439733028411864</li>\n</ul>\n<p>Difference is small but it's there so i put in the fastest implementation.</p>\n<p>For reference, measurement was done with this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd._functions <span class=\"pl-k\">import</span> Cumsum\n\n<span class=\"pl-k\">import</span> time\n\nnb_runs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nsize <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>)\ninp <span class=\"pl-k\">=</span> Variable(torch.randn(size).double(), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\ngrad_out <span class=\"pl-k\">=</span> torch.randn(size).double()\n\nstart <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(nb_runs):\n    cum_sum_func <span class=\"pl-k\">=</span> Cumsum(<span class=\"pl-c1\">0</span>)\n    out <span class=\"pl-k\">=</span> cum_sum_func(inp)\n    out.backward(grad_out)\nend <span class=\"pl-k\">=</span> time.time()\n<span class=\"pl-c1\">print</span>((end<span class=\"pl-k\">-</span>start) <span class=\"pl-k\">/</span> nb_runs)</pre></div>", "body_text": "Added the method forVariable, as well as the associated test.\nWith regards to the implementation, i did a quick time measurement on the one doing the sum and the one cloning the end of the cumsum, the results are as follows:\n\nWith an explicit sum: 0.021312472820281984\nWith cloning of the end of the cumsum: 0.019439733028411864\n\nDifference is small but it's there so i put in the fastest implementation.\nFor reference, measurement was done with this:\nimport torch\nfrom torch.autograd._functions import Cumsum\n\nimport time\n\nnb_runs = 100\nsize = (1000, 1000)\ninp = Variable(torch.randn(size).double(), requires_grad=True)\ngrad_out = torch.randn(size).double()\n\nstart = time.time()\nfor i in range(nb_runs):\n    cum_sum_func = Cumsum(0)\n    out = cum_sum_func(inp)\n    out.backward(grad_out)\nend = time.time()\nprint((end-start) / nb_runs)", "body": "Added the method for`Variable`, as well as the associated test.\r\n\r\nWith regards to the implementation, i did a quick time measurement on the one doing the sum and the one cloning the end of the cumsum, the results are as follows:\r\n\r\n- With an explicit sum: 0.021312472820281984\r\n- With cloning of the end of the cumsum: 0.019439733028411864\r\n\r\nDifference is small but it's there so i put in the fastest implementation.\r\n\r\n\r\nFor reference, measurement was done with this:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd._functions import Cumsum\r\n\r\nimport time\r\n\r\nnb_runs = 100\r\nsize = (1000, 1000)\r\ninp = Variable(torch.randn(size).double(), requires_grad=True)\r\ngrad_out = torch.randn(size).double()\r\n\r\nstart = time.time()\r\nfor i in range(nb_runs):\r\n    cum_sum_func = Cumsum(0)\r\n    out = cum_sum_func(inp)\r\n    out.backward(grad_out)\r\nend = time.time()\r\nprint((end-start) / nb_runs)\r\n```"}