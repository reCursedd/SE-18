{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/334231461", "html_url": "https://github.com/pytorch/pytorch/issues/2895#issuecomment-334231461", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2895", "id": 334231461, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDIzMTQ2MQ==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-04T17:34:16Z", "updated_at": "2017-10-04T17:34:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Reserve size returned by cudnn is indeed larger for GRU in this example (864000000 vs 736000000). I could not reproduce overall higher mem usage (see my output below), but in my case only 10 MB showed up as used, so must be something wrong with my pynvml. However, even properly working nvml is not the right tool to check memory usage, because memory cached in mem allocator that can be freed when needed will show up as used in nvml.</p>\n<pre><code>workspace size 13169920\nreserve size 864000000\nEpoch 0 - GRU - 2279370 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nworkspace size 13169920\nreserve size 864000000\nEpoch 1 - GRU - 2279370 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nworkspace size 13169920\nreserve size 864000000\nEpoch 2 - GRU - 2279370 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nroot@79c48f5ef1c6:/workspace/ALL/playground# vim rnnmem.py \nroot@79c48f5ef1c6:/workspace/ALL/playground# python rnnmem.py \nworkspace size 17730560\nreserve size 736000000\nEpoch 0 - LSTM - 3038090 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nworkspace size 17730560\nreserve size 736000000\nEpoch 1 - LSTM - 3038090 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nworkspace size 17730560\nreserve size 736000000\nEpoch 2 - LSTM - 3038090 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\n</code></pre>", "body_text": "Reserve size returned by cudnn is indeed larger for GRU in this example (864000000 vs 736000000). I could not reproduce overall higher mem usage (see my output below), but in my case only 10 MB showed up as used, so must be something wrong with my pynvml. However, even properly working nvml is not the right tool to check memory usage, because memory cached in mem allocator that can be freed when needed will show up as used in nvml.\nworkspace size 13169920\nreserve size 864000000\nEpoch 0 - GRU - 2279370 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nworkspace size 13169920\nreserve size 864000000\nEpoch 1 - GRU - 2279370 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nworkspace size 13169920\nreserve size 864000000\nEpoch 2 - GRU - 2279370 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nroot@79c48f5ef1c6:/workspace/ALL/playground# vim rnnmem.py \nroot@79c48f5ef1c6:/workspace/ALL/playground# python rnnmem.py \nworkspace size 17730560\nreserve size 736000000\nEpoch 0 - LSTM - 3038090 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nworkspace size 17730560\nreserve size 736000000\nEpoch 1 - LSTM - 3038090 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\nworkspace size 17730560\nreserve size 736000000\nEpoch 2 - LSTM - 3038090 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total", "body": "Reserve size returned by cudnn is indeed larger for GRU in this example (864000000 vs 736000000). I could not reproduce overall higher mem usage (see my output below), but in my case only 10 MB showed up as used, so must be something wrong with my pynvml. However, even properly working nvml is not the right tool to check memory usage, because memory cached in mem allocator that can be freed when needed will show up as used in nvml. \r\n```\r\nworkspace size 13169920\r\nreserve size 864000000\r\nEpoch 0 - GRU - 2279370 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\r\nworkspace size 13169920\r\nreserve size 864000000\r\nEpoch 1 - GRU - 2279370 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\r\nworkspace size 13169920\r\nreserve size 864000000\r\nEpoch 2 - GRU - 2279370 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\r\nroot@79c48f5ef1c6:/workspace/ALL/playground# vim rnnmem.py \r\nroot@79c48f5ef1c6:/workspace/ALL/playground# python rnnmem.py \r\nworkspace size 17730560\r\nreserve size 736000000\r\nEpoch 0 - LSTM - 3038090 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\r\nworkspace size 17730560\r\nreserve size 736000000\r\nEpoch 1 - LSTM - 3038090 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\r\nworkspace size 17730560\r\nreserve size 736000000\r\nEpoch 2 - LSTM - 3038090 params ::: b'Tesla P100-SXM2-16GB': 16266.25 MB free, 10.0 MB used, 16276.25 MB total\r\n```\r\n"}