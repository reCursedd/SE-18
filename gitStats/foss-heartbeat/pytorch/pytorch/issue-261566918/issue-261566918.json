{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2895", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2895/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2895/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2895/events", "html_url": "https://github.com/pytorch/pytorch/issues/2895", "id": 261566918, "node_id": "MDU6SXNzdWUyNjE1NjY5MTg=", "number": 2895, "title": "cudnn RNN memory consumption is not coherent", "user": {"login": "stefbraun", "id": 13469638, "node_id": "MDQ6VXNlcjEzNDY5NjM4", "avatar_url": "https://avatars0.githubusercontent.com/u/13469638?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefbraun", "html_url": "https://github.com/stefbraun", "followers_url": "https://api.github.com/users/stefbraun/followers", "following_url": "https://api.github.com/users/stefbraun/following{/other_user}", "gists_url": "https://api.github.com/users/stefbraun/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefbraun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefbraun/subscriptions", "organizations_url": "https://api.github.com/users/stefbraun/orgs", "repos_url": "https://api.github.com/users/stefbraun/repos", "events_url": "https://api.github.com/users/stefbraun/events{/privacy}", "received_events_url": "https://api.github.com/users/stefbraun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-09-29T08:25:29Z", "updated_at": "2017-10-05T05:16:14Z", "closed_at": "2017-10-05T05:16:14Z", "author_association": "NONE", "body_html": "<p>Hi, I am experimenting with cudnn RNNs and i found that in terms of memory consumption, the order across cell types is as following (low memory usage to high memory usage):<br>\n<code>RNN</code> &lt; <strong><code>LSTM</code></strong> &lt; <code>GRU</code> (measured).</p>\n<p>As GRU has less parameters than LSTM, it would be expected that the order is<br>\n<code>RNN</code> &lt; <code>GRU</code> &lt; <strong><code>LSTM</code></strong> (expected).</p>\n<p>The script attached at the end enables reproduction (see output below). Two questions arise:</p>\n<ol>\n<li>Why does GRU consume more memory although it has less parameters than LSTM?</li>\n<li>Why is more memory consumed in the 2nd and 3rd epoch?</li>\n</ol>\n<pre><code>Epoch 0 - RNN - 761930 params ::: GeForce GTX 980 Ti: 5153.75 MB free, 924.5625 MB used, 6078.3125 MB total\nEpoch 1 - RNN - 761930 params ::: GeForce GTX 980 Ti: 5150.75 MB free, 927.5625 MB used, 6078.3125 MB total\nEpoch 2 - RNN - 761930 params ::: GeForce GTX 980 Ti: 5150.75 MB free, 927.5625 MB used, 6078.3125 MB total\n</code></pre>\n<pre><code>Epoch 0 - GRU - 2279370 params ::: GeForce GTX 980 Ti: 3910.125 MB free, 2168.1875 MB used, 6078.3125 MB total\nEpoch 1 - GRU - 2279370 params ::: GeForce GTX 980 Ti: 3086.125 MB free, 2992.1875 MB used, 6078.3125 MB total\nEpoch 2 - GRU - 2279370 params ::: GeForce GTX 980 Ti: 3086.125 MB free, 2992.1875 MB used, 6078.3125 MB total\n</code></pre>\n<pre><code>Epoch 0 - LSTM - 3038090 params ::: GeForce GTX 980 Ti: 4133.5 MB free, 1944.8125 MB used, 6078.3125 MB total\nEpoch 1 - LSTM - 3038090 params ::: GeForce GTX 980 Ti: 3431.5 MB free, 2646.8125 MB used, 6078.3125 MB total\nEpoch 2 - LSTM - 3038090 params ::: GeForce GTX 980 Ti: 3431.5 MB free, 2646.8125 MB used, 6078.3125 MB total\n</code></pre>\n<pre><code>from timeit import default_timer as timer\nimport torch.cuda as cutorch\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\nfrom pynvml import *\n\n# nvml\nnvmlInit()\nfor i in range(nvmlDeviceGetCount()):\n    handle = nvmlDeviceGetHandleByIndex(i)\n\n# Set seeds\nnp.random.seed(11)\ntorch.manual_seed(11)\ntorch.cuda.manual_seed(11)\n\n# Parameters\nbatch_size = 25\nmax_length = 1000\nfeatures = 123\nnum_classes = 10\n\narch = 'LSTM'\nnum_layers = 4\nhidden_size = 320\n\nepochs = 3\n# Toy data\nbatch = torch.rand(max_length, batch_size, features)\nlabels = torch.from_numpy((np.random.randint(low=0, high=num_classes - 1, size=batch_size, dtype=np.int32)))\n\n# Network\nclass rnn(nn.Module):\n    def __init__(self, features, num_layers, hidden_size, arch, num_classes):\n        super(rnn, self).__init__()\n        if arch == 'RNN':\n            self.rnn = nn.RNN(input_size=features, num_layers=num_layers, hidden_size=hidden_size, bias=True)\n        if arch == 'GRU':\n            self.rnn = nn.GRU(input_size=features, num_layers=num_layers, hidden_size=hidden_size, bias=True)\n        if arch == 'LSTM':\n            self.rnn = nn.LSTM(input_size=features, num_layers=num_layers, hidden_size=hidden_size, bias=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n    def forward(self, x):\n        x, _ = self.rnn(x)\n        x = self.fc(x[-1,:,:])# RNN\n        return x\n\n# Net, optimizer\nnet = rnn(features, num_layers, hidden_size, arch=arch, num_classes = num_classes).cuda()\noptimizer = optim.Adam(net.parameters())\ncriterion = nn.CrossEntropyLoss()\n\n# Parameter count\nparams = 0\nfor param in list(net.parameters()):\n    sizes = 1\n    for el in param.size():\n        sizes = sizes * el\n    params += sizes\n\nfor epoch in range(epochs):\n    batch_var = Variable(batch).cuda()\n    labels_var = Variable(labels).cuda()\n    optimizer.zero_grad()\n    output = net(batch_var)\n    loss = criterion(output, labels_var.long())\n    loss.backward()\n    optimizer.step()\n\n    meminfo = nvmlDeviceGetMemoryInfo(handle)\n    print('Epoch {} - {} - {} params ::: {}: {} MB free, {} MB used, {} MB total'.format(epoch, arch,params,\n        nvmlDeviceGetName(handle),\n        meminfo.free / 1024. ** 2, meminfo.used / 1024. ** 2, meminfo.total / 1024. ** 2))\n</code></pre>", "body_text": "Hi, I am experimenting with cudnn RNNs and i found that in terms of memory consumption, the order across cell types is as following (low memory usage to high memory usage):\nRNN < LSTM < GRU (measured).\nAs GRU has less parameters than LSTM, it would be expected that the order is\nRNN < GRU < LSTM (expected).\nThe script attached at the end enables reproduction (see output below). Two questions arise:\n\nWhy does GRU consume more memory although it has less parameters than LSTM?\nWhy is more memory consumed in the 2nd and 3rd epoch?\n\nEpoch 0 - RNN - 761930 params ::: GeForce GTX 980 Ti: 5153.75 MB free, 924.5625 MB used, 6078.3125 MB total\nEpoch 1 - RNN - 761930 params ::: GeForce GTX 980 Ti: 5150.75 MB free, 927.5625 MB used, 6078.3125 MB total\nEpoch 2 - RNN - 761930 params ::: GeForce GTX 980 Ti: 5150.75 MB free, 927.5625 MB used, 6078.3125 MB total\n\nEpoch 0 - GRU - 2279370 params ::: GeForce GTX 980 Ti: 3910.125 MB free, 2168.1875 MB used, 6078.3125 MB total\nEpoch 1 - GRU - 2279370 params ::: GeForce GTX 980 Ti: 3086.125 MB free, 2992.1875 MB used, 6078.3125 MB total\nEpoch 2 - GRU - 2279370 params ::: GeForce GTX 980 Ti: 3086.125 MB free, 2992.1875 MB used, 6078.3125 MB total\n\nEpoch 0 - LSTM - 3038090 params ::: GeForce GTX 980 Ti: 4133.5 MB free, 1944.8125 MB used, 6078.3125 MB total\nEpoch 1 - LSTM - 3038090 params ::: GeForce GTX 980 Ti: 3431.5 MB free, 2646.8125 MB used, 6078.3125 MB total\nEpoch 2 - LSTM - 3038090 params ::: GeForce GTX 980 Ti: 3431.5 MB free, 2646.8125 MB used, 6078.3125 MB total\n\nfrom timeit import default_timer as timer\nimport torch.cuda as cutorch\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\nfrom pynvml import *\n\n# nvml\nnvmlInit()\nfor i in range(nvmlDeviceGetCount()):\n    handle = nvmlDeviceGetHandleByIndex(i)\n\n# Set seeds\nnp.random.seed(11)\ntorch.manual_seed(11)\ntorch.cuda.manual_seed(11)\n\n# Parameters\nbatch_size = 25\nmax_length = 1000\nfeatures = 123\nnum_classes = 10\n\narch = 'LSTM'\nnum_layers = 4\nhidden_size = 320\n\nepochs = 3\n# Toy data\nbatch = torch.rand(max_length, batch_size, features)\nlabels = torch.from_numpy((np.random.randint(low=0, high=num_classes - 1, size=batch_size, dtype=np.int32)))\n\n# Network\nclass rnn(nn.Module):\n    def __init__(self, features, num_layers, hidden_size, arch, num_classes):\n        super(rnn, self).__init__()\n        if arch == 'RNN':\n            self.rnn = nn.RNN(input_size=features, num_layers=num_layers, hidden_size=hidden_size, bias=True)\n        if arch == 'GRU':\n            self.rnn = nn.GRU(input_size=features, num_layers=num_layers, hidden_size=hidden_size, bias=True)\n        if arch == 'LSTM':\n            self.rnn = nn.LSTM(input_size=features, num_layers=num_layers, hidden_size=hidden_size, bias=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n    def forward(self, x):\n        x, _ = self.rnn(x)\n        x = self.fc(x[-1,:,:])# RNN\n        return x\n\n# Net, optimizer\nnet = rnn(features, num_layers, hidden_size, arch=arch, num_classes = num_classes).cuda()\noptimizer = optim.Adam(net.parameters())\ncriterion = nn.CrossEntropyLoss()\n\n# Parameter count\nparams = 0\nfor param in list(net.parameters()):\n    sizes = 1\n    for el in param.size():\n        sizes = sizes * el\n    params += sizes\n\nfor epoch in range(epochs):\n    batch_var = Variable(batch).cuda()\n    labels_var = Variable(labels).cuda()\n    optimizer.zero_grad()\n    output = net(batch_var)\n    loss = criterion(output, labels_var.long())\n    loss.backward()\n    optimizer.step()\n\n    meminfo = nvmlDeviceGetMemoryInfo(handle)\n    print('Epoch {} - {} - {} params ::: {}: {} MB free, {} MB used, {} MB total'.format(epoch, arch,params,\n        nvmlDeviceGetName(handle),\n        meminfo.free / 1024. ** 2, meminfo.used / 1024. ** 2, meminfo.total / 1024. ** 2))", "body": "Hi, I am experimenting with cudnn RNNs and i found that in terms of memory consumption, the order across cell types is as following (low memory usage to high memory usage):\r\n`RNN` < **`LSTM`** < `GRU` (measured).\r\n\r\nAs GRU has less parameters than LSTM, it would be expected that the order is\r\n`RNN` < `GRU` < **`LSTM`** (expected).\r\n\r\nThe script attached at the end enables reproduction (see output below). Two questions arise:\r\n1. Why does GRU consume more memory although it has less parameters than LSTM?\r\n2. Why is more memory consumed in the 2nd and 3rd epoch?\r\n```\r\nEpoch 0 - RNN - 761930 params ::: GeForce GTX 980 Ti: 5153.75 MB free, 924.5625 MB used, 6078.3125 MB total\r\nEpoch 1 - RNN - 761930 params ::: GeForce GTX 980 Ti: 5150.75 MB free, 927.5625 MB used, 6078.3125 MB total\r\nEpoch 2 - RNN - 761930 params ::: GeForce GTX 980 Ti: 5150.75 MB free, 927.5625 MB used, 6078.3125 MB total\r\n```\r\n```\r\nEpoch 0 - GRU - 2279370 params ::: GeForce GTX 980 Ti: 3910.125 MB free, 2168.1875 MB used, 6078.3125 MB total\r\nEpoch 1 - GRU - 2279370 params ::: GeForce GTX 980 Ti: 3086.125 MB free, 2992.1875 MB used, 6078.3125 MB total\r\nEpoch 2 - GRU - 2279370 params ::: GeForce GTX 980 Ti: 3086.125 MB free, 2992.1875 MB used, 6078.3125 MB total\r\n```\r\n```\r\nEpoch 0 - LSTM - 3038090 params ::: GeForce GTX 980 Ti: 4133.5 MB free, 1944.8125 MB used, 6078.3125 MB total\r\nEpoch 1 - LSTM - 3038090 params ::: GeForce GTX 980 Ti: 3431.5 MB free, 2646.8125 MB used, 6078.3125 MB total\r\nEpoch 2 - LSTM - 3038090 params ::: GeForce GTX 980 Ti: 3431.5 MB free, 2646.8125 MB used, 6078.3125 MB total\r\n```\r\n```\r\nfrom timeit import default_timer as timer\r\nimport torch.cuda as cutorch\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nfrom torch.autograd import Variable\r\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\r\nfrom pynvml import *\r\n\r\n# nvml\r\nnvmlInit()\r\nfor i in range(nvmlDeviceGetCount()):\r\n    handle = nvmlDeviceGetHandleByIndex(i)\r\n\r\n# Set seeds\r\nnp.random.seed(11)\r\ntorch.manual_seed(11)\r\ntorch.cuda.manual_seed(11)\r\n\r\n# Parameters\r\nbatch_size = 25\r\nmax_length = 1000\r\nfeatures = 123\r\nnum_classes = 10\r\n\r\narch = 'LSTM'\r\nnum_layers = 4\r\nhidden_size = 320\r\n\r\nepochs = 3\r\n# Toy data\r\nbatch = torch.rand(max_length, batch_size, features)\r\nlabels = torch.from_numpy((np.random.randint(low=0, high=num_classes - 1, size=batch_size, dtype=np.int32)))\r\n\r\n# Network\r\nclass rnn(nn.Module):\r\n    def __init__(self, features, num_layers, hidden_size, arch, num_classes):\r\n        super(rnn, self).__init__()\r\n        if arch == 'RNN':\r\n            self.rnn = nn.RNN(input_size=features, num_layers=num_layers, hidden_size=hidden_size, bias=True)\r\n        if arch == 'GRU':\r\n            self.rnn = nn.GRU(input_size=features, num_layers=num_layers, hidden_size=hidden_size, bias=True)\r\n        if arch == 'LSTM':\r\n            self.rnn = nn.LSTM(input_size=features, num_layers=num_layers, hidden_size=hidden_size, bias=True)\r\n        self.fc = nn.Linear(hidden_size, num_classes)\r\n    def forward(self, x):\r\n        x, _ = self.rnn(x)\r\n        x = self.fc(x[-1,:,:])# RNN\r\n        return x\r\n\r\n# Net, optimizer\r\nnet = rnn(features, num_layers, hidden_size, arch=arch, num_classes = num_classes).cuda()\r\noptimizer = optim.Adam(net.parameters())\r\ncriterion = nn.CrossEntropyLoss()\r\n\r\n# Parameter count\r\nparams = 0\r\nfor param in list(net.parameters()):\r\n    sizes = 1\r\n    for el in param.size():\r\n        sizes = sizes * el\r\n    params += sizes\r\n\r\nfor epoch in range(epochs):\r\n    batch_var = Variable(batch).cuda()\r\n    labels_var = Variable(labels).cuda()\r\n    optimizer.zero_grad()\r\n    output = net(batch_var)\r\n    loss = criterion(output, labels_var.long())\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n    meminfo = nvmlDeviceGetMemoryInfo(handle)\r\n    print('Epoch {} - {} - {} params ::: {}: {} MB free, {} MB used, {} MB total'.format(epoch, arch,params,\r\n        nvmlDeviceGetName(handle),\r\n        meminfo.free / 1024. ** 2, meminfo.used / 1024. ** 2, meminfo.total / 1024. ** 2))\r\n```\r\n"}