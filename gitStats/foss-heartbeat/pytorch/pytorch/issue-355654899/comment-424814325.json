{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424814325", "html_url": "https://github.com/pytorch/pytorch/issues/11089#issuecomment-424814325", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11089", "id": 424814325, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDgxNDMyNQ==", "user": {"login": "syed-ahmed", "id": 8906225, "node_id": "MDQ6VXNlcjg5MDYyMjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/8906225?v=4", "gravatar_id": "", "url": "https://api.github.com/users/syed-ahmed", "html_url": "https://github.com/syed-ahmed", "followers_url": "https://api.github.com/users/syed-ahmed/followers", "following_url": "https://api.github.com/users/syed-ahmed/following{/other_user}", "gists_url": "https://api.github.com/users/syed-ahmed/gists{/gist_id}", "starred_url": "https://api.github.com/users/syed-ahmed/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/syed-ahmed/subscriptions", "organizations_url": "https://api.github.com/users/syed-ahmed/orgs", "repos_url": "https://api.github.com/users/syed-ahmed/repos", "events_url": "https://api.github.com/users/syed-ahmed/events{/privacy}", "received_events_url": "https://api.github.com/users/syed-ahmed/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T18:04:57Z", "updated_at": "2018-09-26T18:04:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349\">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349</a>. Also, I have added CUDA 10 guards to <code>CuFFTPlanCache.h</code> through this commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa/hovercard\" href=\"https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa\"><tt>ffbac7d</tt></a>. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.</p>\n<p>Please let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue.</p>", "body_text": "As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349. Also, I have added CUDA 10 guards to CuFFTPlanCache.h through this commit ffbac7d. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.\nPlease let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue.", "body": "As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349. Also, I have added CUDA 10 guards to `CuFFTPlanCache.h` through this commit https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.\r\n\r\nPlease let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue. "}