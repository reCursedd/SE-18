{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424815386", "html_url": "https://github.com/pytorch/pytorch/issues/11089#issuecomment-424815386", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11089", "id": 424815386, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDgxNTM4Ng==", "user": {"login": "isaacgerg", "id": 11971499, "node_id": "MDQ6VXNlcjExOTcxNDk5", "avatar_url": "https://avatars0.githubusercontent.com/u/11971499?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isaacgerg", "html_url": "https://github.com/isaacgerg", "followers_url": "https://api.github.com/users/isaacgerg/followers", "following_url": "https://api.github.com/users/isaacgerg/following{/other_user}", "gists_url": "https://api.github.com/users/isaacgerg/gists{/gist_id}", "starred_url": "https://api.github.com/users/isaacgerg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isaacgerg/subscriptions", "organizations_url": "https://api.github.com/users/isaacgerg/orgs", "repos_url": "https://api.github.com/users/isaacgerg/repos", "events_url": "https://api.github.com/users/isaacgerg/events{/privacy}", "received_events_url": "https://api.github.com/users/isaacgerg/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T18:08:10Z", "updated_at": "2018-09-26T18:08:10Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Thank you. I am okay if you close.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Sep 26, 2018 at 2:06 PM Syed Tousif Ahmed ***@***.***&gt; wrote:\n As of CUDA 10 release last week, the bug has been fixed in cuFFT and I\n have updated the note here:\n <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349\">https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349</a>.\n Also, I have added CUDA 10 guards to CuFFTPlanCache.h through this commit\n <a href=\"https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa\" class=\"commit-link\"><tt>ffbac7d</tt></a>\n &lt;<a href=\"https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa\" class=\"commit-link\"><tt>ffbac7d</tt></a>&gt;.\n Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array\n should grow as intended without failing at the 1024th plan. I have tested\n this commit by building pytorch with CUDA 10 and running the test suite\n successfully.\n\n Please let me know if there are any other queries regarding this issue.\n Otherwise, I think we are good to close this issue.\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"355654899\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/11089\" href=\"https://github.com/pytorch/pytorch/issues/11089#issuecomment-424814325\">#11089 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ALarq3NS9GQNTWmIN_QbJUI53VsUR847ks5ue8IRgaJpZM4WTx52\">https://github.com/notifications/unsubscribe-auth/ALarq3NS9GQNTWmIN_QbJUI53VsUR847ks5ue8IRgaJpZM4WTx52</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Thank you. I am okay if you close.\n\u2026\nOn Wed, Sep 26, 2018 at 2:06 PM Syed Tousif Ahmed ***@***.***> wrote:\n As of CUDA 10 release last week, the bug has been fixed in cuFFT and I\n have updated the note here:\n https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349.\n Also, I have added CUDA 10 guards to CuFFTPlanCache.h through this commit\n ffbac7d\n <ffbac7d>.\n Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array\n should grow as intended without failing at the 1024th plan. I have tested\n this commit by building pytorch with CUDA 10 and running the test suite\n successfully.\n\n Please let me know if there are any other queries regarding this issue.\n Otherwise, I think we are good to close this issue.\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#11089 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ALarq3NS9GQNTWmIN_QbJUI53VsUR847ks5ue8IRgaJpZM4WTx52>\n .", "body": "Thank you. I am okay if you close.\n\nOn Wed, Sep 26, 2018 at 2:06 PM Syed Tousif Ahmed <notifications@github.com>\nwrote:\n\n> As of CUDA 10 release last week, the bug has been fixed in cuFFT and I\n> have updated the note here:\n> https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349.\n> Also, I have added CUDA 10 guards to CuFFTPlanCache.h through this commit\n> ffbac7d\n> <https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa>.\n> Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array\n> should grow as intended without failing at the 1024th plan. I have tested\n> this commit by building pytorch with CUDA 10 and running the test suite\n> successfully.\n>\n> Please let me know if there are any other queries regarding this issue.\n> Otherwise, I think we are good to close this issue.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/issues/11089#issuecomment-424814325>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ALarq3NS9GQNTWmIN_QbJUI53VsUR847ks5ue8IRgaJpZM4WTx52>\n> .\n>\n"}