{"url": "https://api.github.com/repos/pytorch/pytorch/issues/6953", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/6953/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/6953/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/6953/events", "html_url": "https://github.com/pytorch/pytorch/pull/6953", "id": 317703569, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg0MDkzNzIz", "number": 6953, "title": "Changes incorrect \"overlappingIndices\" call to correct \"maybeOverlappingIndices\"", "user": {"login": "mruberry", "id": 38511765, "node_id": "MDQ6VXNlcjM4NTExNzY1", "avatar_url": "https://avatars3.githubusercontent.com/u/38511765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mruberry", "html_url": "https://github.com/mruberry", "followers_url": "https://api.github.com/users/mruberry/followers", "following_url": "https://api.github.com/users/mruberry/following{/other_user}", "gists_url": "https://api.github.com/users/mruberry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mruberry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mruberry/subscriptions", "organizations_url": "https://api.github.com/users/mruberry/orgs", "repos_url": "https://api.github.com/users/mruberry/repos", "events_url": "https://api.github.com/users/mruberry/events{/privacy}", "received_events_url": "https://api.github.com/users/mruberry/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-04-25T16:30:38Z", "updated_at": "2018-04-26T16:07:42Z", "closed_at": "2018-04-26T01:07:14Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/6953", "html_url": "https://github.com/pytorch/pytorch/pull/6953", "diff_url": "https://github.com/pytorch/pytorch/pull/6953.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/6953.patch"}, "body_html": "<p>THE PROBLEM</p>\n<p>The current overlappingIndices() is meant to detect if a tensor defines multiple valid indices for the same data element. There are two significant issues with this function:</p>\n<p>(1) The algorithm it attempts to implement cannot do this.</p>\n<p>(2) That algorithm is not implemented correctly.</p>\n<p>This call is used by pointwiseApply() and scatter(). If a tensor is readable/writable and detected as overlapped these algorithms will create a non-overlapped copy of it to work on. When tensors are improperly identified as overlapped this causese extra work. If tensors are improperly identified as non-overlapped then this would cause the operations to exhibit unexpected behavior.</p>\n<p>For example,</p>\n<p>ref = torch.arange(0, 32 * 5).view(4, 8, 5).cuda().double()<br>\np = ref[:,:,::2]<br>\np += 1</p>\n<p>Results in a call to pointwiseApply1, which detects p as an overlapped tensor (it is not), causing a call to pointwiseApply2 that copies it into a non-overlapped temporary, and then another call to pointwiseApply2 later that copies it back to the original tensor. If, however, the original tensor is given dimensions of (4, 8, 4), instead, it is correctly detected as non-overlapped and only a single pointwiseApply1 call is made. (This is due to an \"off-by-one\" error in overlappingIndices().)</p>\n<p>DISCUSSION + FIX</p>\n<p>The algorithm that overlappingIndices() attempts to implement tests for a sufficient but not necessary condition of a tensor to be non-overlapping. That is, if its algorithm were implemented properly then it would be a conservative check that would ensure all overlapped tensors were copied (as desired), but also that some non-overlapped tensors were copied too.</p>\n<p>The algorithm can be thought of as trying to test whether the dimensions can be ordered like \"nesting dolls,\" with each dimension fitting within the next one larger than it. If this is true then the tensor is non-overlapping, but if it's false the tensor may or may not be overlapped. For example, a tensor with dims (2, 3) and strides (4, 3) cannot be \"nested,\" but is non-overlapping. (The tensor looks like [[0, 3, 6], [4, 7, 10]].)</p>\n<p>The algorithm is currently implemented improperly, as can be seen in the example above. The tensor p has dimensions [4, 8, 3] and strides [40, 5, 2]. This confuses the current implementation, which thinks the innermost dimension needs a stride of 6, which is incorrect. The first row is [0, 2, 4] and the next row begins with 5. The current implementation also improperly implemented its sorting behavior. (qsort comparators require -1, 0, and 1, not true/false return values.)</p>\n<p>Fixing the existing algorithm is straightforward (and what this PR does, see below), but it is important to note that the algorithm never performed as intended, so its name and the documentation around it has been updated, too. A natural question is if it's possible to write an efficient overlappingIndices(), and I believe the answer is \"no.\" Disambiguating overlapping from non-overlapping tensors is equivalent to finding a nonzero solution to a linear Diophantine equation with restricted coefficients, that is, an equation of the form x_0s_0 + x_1s_1 ... = 0 where s_X is the stride in dimension X and x_X is an integer from [-size_X + 1, size_X - 1].</p>\n<p>Another note is that the CPU does not perform this check. For example, if we run:</p>\n<p>a = torch.FloatTensor([[0,1], [10, 11]])<br>\nb = torch.FloatTensor([[0,0],[0,0]])<br>\nb = b.set_(a.storage(), storage_offset=0, size=a.size(), stride=(1,1))<br>\nb += 1</p>\n<p>Then b is [[1, 3], [3, 11]] because the operation is applied twice to the second element of the original tensor. This causes no warning.</p>\n<p>Since the CPU does not perform a similar check, another question is whether the GPU code should remove its check. While it may seem that writing to overlapping tensors is an error state, running test_cuda.py reveals 171 instances of possibly overlapped tensors being copied by pointwiseApply(). (The prior incorrect version has 176 copies.) Allowing writing to overlapped tensors on the GPU may violate assumptions about memory accesses, too. In fairness, these assumptions may be violated on the CPU already.</p>\n<p>Leaving the CPU vs GPU behavior question for the future, this fix corrects the current intended GPU behavior. This means that there will be fewer unnecessary copies and no chance of an overlapped tensor sneaking through on the GPU. The CPU behavior remains unchanged. The fix also adds a test to test_cuda.py to ensure that overlapped tensors on the GPU are written to as expected.</p>\n<p>Automated testing: test_cuda.py (OK), run_test.py (no regression from PyTorch/Master)</p>", "body_text": "THE PROBLEM\nThe current overlappingIndices() is meant to detect if a tensor defines multiple valid indices for the same data element. There are two significant issues with this function:\n(1) The algorithm it attempts to implement cannot do this.\n(2) That algorithm is not implemented correctly.\nThis call is used by pointwiseApply() and scatter(). If a tensor is readable/writable and detected as overlapped these algorithms will create a non-overlapped copy of it to work on. When tensors are improperly identified as overlapped this causese extra work. If tensors are improperly identified as non-overlapped then this would cause the operations to exhibit unexpected behavior.\nFor example,\nref = torch.arange(0, 32 * 5).view(4, 8, 5).cuda().double()\np = ref[:,:,::2]\np += 1\nResults in a call to pointwiseApply1, which detects p as an overlapped tensor (it is not), causing a call to pointwiseApply2 that copies it into a non-overlapped temporary, and then another call to pointwiseApply2 later that copies it back to the original tensor. If, however, the original tensor is given dimensions of (4, 8, 4), instead, it is correctly detected as non-overlapped and only a single pointwiseApply1 call is made. (This is due to an \"off-by-one\" error in overlappingIndices().)\nDISCUSSION + FIX\nThe algorithm that overlappingIndices() attempts to implement tests for a sufficient but not necessary condition of a tensor to be non-overlapping. That is, if its algorithm were implemented properly then it would be a conservative check that would ensure all overlapped tensors were copied (as desired), but also that some non-overlapped tensors were copied too.\nThe algorithm can be thought of as trying to test whether the dimensions can be ordered like \"nesting dolls,\" with each dimension fitting within the next one larger than it. If this is true then the tensor is non-overlapping, but if it's false the tensor may or may not be overlapped. For example, a tensor with dims (2, 3) and strides (4, 3) cannot be \"nested,\" but is non-overlapping. (The tensor looks like [[0, 3, 6], [4, 7, 10]].)\nThe algorithm is currently implemented improperly, as can be seen in the example above. The tensor p has dimensions [4, 8, 3] and strides [40, 5, 2]. This confuses the current implementation, which thinks the innermost dimension needs a stride of 6, which is incorrect. The first row is [0, 2, 4] and the next row begins with 5. The current implementation also improperly implemented its sorting behavior. (qsort comparators require -1, 0, and 1, not true/false return values.)\nFixing the existing algorithm is straightforward (and what this PR does, see below), but it is important to note that the algorithm never performed as intended, so its name and the documentation around it has been updated, too. A natural question is if it's possible to write an efficient overlappingIndices(), and I believe the answer is \"no.\" Disambiguating overlapping from non-overlapping tensors is equivalent to finding a nonzero solution to a linear Diophantine equation with restricted coefficients, that is, an equation of the form x_0s_0 + x_1s_1 ... = 0 where s_X is the stride in dimension X and x_X is an integer from [-size_X + 1, size_X - 1].\nAnother note is that the CPU does not perform this check. For example, if we run:\na = torch.FloatTensor([[0,1], [10, 11]])\nb = torch.FloatTensor([[0,0],[0,0]])\nb = b.set_(a.storage(), storage_offset=0, size=a.size(), stride=(1,1))\nb += 1\nThen b is [[1, 3], [3, 11]] because the operation is applied twice to the second element of the original tensor. This causes no warning.\nSince the CPU does not perform a similar check, another question is whether the GPU code should remove its check. While it may seem that writing to overlapping tensors is an error state, running test_cuda.py reveals 171 instances of possibly overlapped tensors being copied by pointwiseApply(). (The prior incorrect version has 176 copies.) Allowing writing to overlapped tensors on the GPU may violate assumptions about memory accesses, too. In fairness, these assumptions may be violated on the CPU already.\nLeaving the CPU vs GPU behavior question for the future, this fix corrects the current intended GPU behavior. This means that there will be fewer unnecessary copies and no chance of an overlapped tensor sneaking through on the GPU. The CPU behavior remains unchanged. The fix also adds a test to test_cuda.py to ensure that overlapped tensors on the GPU are written to as expected.\nAutomated testing: test_cuda.py (OK), run_test.py (no regression from PyTorch/Master)", "body": "THE PROBLEM\r\n\r\nThe current overlappingIndices() is meant to detect if a tensor defines multiple valid indices for the same data element. There are two significant issues with this function:\r\n\r\n(1) The algorithm it attempts to implement cannot do this.\r\n\r\n(2) That algorithm is not implemented correctly.\r\n\r\nThis call is used by pointwiseApply() and scatter(). If a tensor is readable/writable and detected as overlapped these algorithms will create a non-overlapped copy of it to work on. When tensors are improperly identified as overlapped this causese extra work. If tensors are improperly identified as non-overlapped then this would cause the operations to exhibit unexpected behavior.\r\n\r\nFor example,\r\n\r\nref = torch.arange(0, 32 * 5).view(4, 8, 5).cuda().double()\r\np = ref[:,:,::2]\r\np += 1\r\n\r\nResults in a call to pointwiseApply1, which detects p as an overlapped tensor (it is not), causing a call to pointwiseApply2 that copies it into a non-overlapped temporary, and then another call to pointwiseApply2 later that copies it back to the original tensor. If, however, the original tensor is given dimensions of (4, 8, 4), instead, it is correctly detected as non-overlapped and only a single pointwiseApply1 call is made. (This is due to an \"off-by-one\" error in overlappingIndices().) \r\n\r\nDISCUSSION + FIX\r\n\r\nThe algorithm that overlappingIndices() attempts to implement tests for a sufficient but not necessary condition of a tensor to be non-overlapping. That is, if its algorithm were implemented properly then it would be a conservative check that would ensure all overlapped tensors were copied (as desired), but also that some non-overlapped tensors were copied too.\r\n\r\nThe algorithm can be thought of as trying to test whether the dimensions can be ordered like \"nesting dolls,\" with each dimension fitting within the next one larger than it. If this is true then the tensor is non-overlapping, but if it's false the tensor may or may not be overlapped. For example, a tensor with dims (2, 3) and strides (4, 3) cannot be \"nested,\" but is non-overlapping. (The tensor looks like [[0, 3, 6], [4, 7, 10]].)\r\n\r\nThe algorithm is currently implemented improperly, as can be seen in the example above. The tensor p has dimensions [4, 8, 3] and strides [40, 5, 2]. This confuses the current implementation, which thinks the innermost dimension needs a stride of 6, which is incorrect. The first row is [0, 2, 4] and the next row begins with 5. The current implementation also improperly implemented its sorting behavior. (qsort comparators require -1, 0, and 1, not true/false return values.)\r\n\r\nFixing the existing algorithm is straightforward (and what this PR does, see below), but it is important to note that the algorithm never performed as intended, so its name and the documentation around it has been updated, too. A natural question is if it's possible to write an efficient overlappingIndices(), and I believe the answer is \"no.\" Disambiguating overlapping from non-overlapping tensors is equivalent to finding a nonzero solution to a linear Diophantine equation with restricted coefficients, that is, an equation of the form x_0s_0 + x_1s_1 ... = 0 where s_X is the stride in dimension X and x_X is an integer from [-size_X + 1, size_X - 1].\r\n\r\nAnother note is that the CPU does not perform this check. For example, if we run:\r\n\r\na = torch.FloatTensor([[0,1], [10, 11]])\r\nb = torch.FloatTensor([[0,0],[0,0]])\r\nb = b.set_(a.storage(), storage_offset=0, size=a.size(), stride=(1,1))\r\nb += 1\r\n\r\nThen b is [[1, 3], [3, 11]] because the operation is applied twice to the second element of the original tensor. This causes no warning.\r\n\r\nSince the CPU does not perform a similar check, another question is whether the GPU code should remove its check. While it may seem that writing to overlapping tensors is an error state, running test_cuda.py reveals 171 instances of possibly overlapped tensors being copied by pointwiseApply(). (The prior incorrect version has 176 copies.) Allowing writing to overlapped tensors on the GPU may violate assumptions about memory accesses, too. In fairness, these assumptions may be violated on the CPU already.\r\n\r\nLeaving the CPU vs GPU behavior question for the future, this fix corrects the current intended GPU behavior. This means that there will be fewer unnecessary copies and no chance of an overlapped tensor sneaking through on the GPU. The CPU behavior remains unchanged. The fix also adds a test to test_cuda.py to ensure that overlapped tensors on the GPU are written to as expected.\r\n\r\nAutomated testing: test_cuda.py (OK), run_test.py (no regression from PyTorch/Master)"}