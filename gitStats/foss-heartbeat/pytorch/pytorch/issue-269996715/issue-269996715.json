{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3394", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3394/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3394/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3394/events", "html_url": "https://github.com/pytorch/pytorch/issues/3394", "id": 269996715, "node_id": "MDU6SXNzdWUyNjk5OTY3MTU=", "number": 3394, "title": "Memory leak from Function.save_for_backward() when looping over batch", "user": {"login": "pbloem", "id": 1104629, "node_id": "MDQ6VXNlcjExMDQ2Mjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1104629?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pbloem", "html_url": "https://github.com/pbloem", "followers_url": "https://api.github.com/users/pbloem/followers", "following_url": "https://api.github.com/users/pbloem/following{/other_user}", "gists_url": "https://api.github.com/users/pbloem/gists{/gist_id}", "starred_url": "https://api.github.com/users/pbloem/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pbloem/subscriptions", "organizations_url": "https://api.github.com/users/pbloem/orgs", "repos_url": "https://api.github.com/users/pbloem/repos", "events_url": "https://api.github.com/users/pbloem/events{/privacy}", "received_events_url": "https://api.github.com/users/pbloem/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-31T15:12:30Z", "updated_at": "2017-11-01T10:00:54Z", "closed_at": "2017-11-01T10:00:54Z", "author_association": "NONE", "body_html": "<p>Due to some quirks of my model, I'm forced to loop over individual instances in a batch and apply a function to each individually. I ran into a memory leak, as illustrated by the following minimal example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> tqdm <span class=\"pl-k\">import</span> trange\n\n<span class=\"pl-c1\">CUDA</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\nB <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n<span class=\"pl-c1\">BIG</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">512</span>\n\ncriterion <span class=\"pl-k\">=</span> nn.MSELoss()\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Mult</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">autograd</span>.<span class=\"pl-e\">Function</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">big</span>, <span class=\"pl-smi\">vector</span>):\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Comment this and problems go away</span>\n        <span class=\"pl-c1\">self</span>.save_for_backward(big)\n\n        <span class=\"pl-k\">return</span> vector <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2.0</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">backward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">grad_output</span>):\n\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">iteration</span>():\n    mult <span class=\"pl-k\">=</span> Mult()\n\n    big <span class=\"pl-k\">=</span> torch.zeros(B, <span class=\"pl-c1\">BIG</span>, <span class=\"pl-c1\">BIG</span>)\n    x <span class=\"pl-k\">=</span> torch.rand(B, <span class=\"pl-c1\">32</span>)\n    y <span class=\"pl-k\">=</span> torch.zeros(B, <span class=\"pl-c1\">32</span>)\n    target <span class=\"pl-k\">=</span> x.clone()\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">CUDA</span>:\n        big, x, y, target <span class=\"pl-k\">=</span> big.cuda(), x.cuda(), y.cuda(), target.cuda()\n    big, x, y, target <span class=\"pl-k\">=</span> Variable(big), Variable(x, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), Variable(y), Variable(target)\n\n    <span class=\"pl-k\">for</span> b <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(B):\n      y[b, :] <span class=\"pl-k\">=</span> mult(big[b, :, :], x[b, :])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Use this instead of the above, and problems go away</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> y = mult(big, x)</span>\n\n    loss <span class=\"pl-k\">=</span> criterion(y, target)\n\n    <span class=\"pl-k\">del</span> mult\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> trange(<span class=\"pl-c1\">int</span>(<span class=\"pl-c1\">10e7</span>)):\n    iteration()\n</pre></div>\n<p>On both my laptop (OSX, CPU) and a cluster node (RedHat, TitanX GPU) the memory use skyrockets with a few iterations. Since every torch object is created in the iteration() function the memory use should stay roughly stable from one iteration to the next.</p>\n<p>Some things I noticed:</p>\n<ul>\n<li>The problem goes away when you apply the function to the whole batch in one go.</li>\n<li>The problem also goes away when you don't store anything for backward.</li>\n<li>Recovering the tensor 'big' in backward(), or actually using it in the computation graph makes no difference.</li>\n<li>The problem happens on both the CPU and the GPU.</li>\n<li><a href=\"https://discuss.pytorch.org/t/autograd-for-sparse-matmul-getting-either-cuda-memory-leak-or-buffers-have-already-been-freed-error/3806\" rel=\"nofollow\">A possibly related issue</a></li>\n</ul>\n<p>I think I've reached the limits of my understanding of pytorch. Did I misunderstand the Function API, or is this a genuine memory leak? If it is a bug, is there a simple workaround I could use?</p>", "body_text": "Due to some quirks of my model, I'm forced to loop over individual instances in a batch and apply a function to each individually. I ran into a memory leak, as illustrated by the following minimal example:\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom tqdm import trange\n\nCUDA = False\n\nB = 2\nBIG = 512\n\ncriterion = nn.MSELoss()\n\nclass Mult(torch.autograd.Function):\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, big, vector):\n\n        # Comment this and problems go away\n        self.save_for_backward(big)\n\n        return vector * 2.0\n\n    def backward(self, grad_output):\n\n        return None, None\n\ndef iteration():\n    mult = Mult()\n\n    big = torch.zeros(B, BIG, BIG)\n    x = torch.rand(B, 32)\n    y = torch.zeros(B, 32)\n    target = x.clone()\n\n    if CUDA:\n        big, x, y, target = big.cuda(), x.cuda(), y.cuda(), target.cuda()\n    big, x, y, target = Variable(big), Variable(x, requires_grad=True), Variable(y), Variable(target)\n\n    for b in range(B):\n      y[b, :] = mult(big[b, :, :], x[b, :])\n\n    # Use this instead of the above, and problems go away\n    # y = mult(big, x)\n\n    loss = criterion(y, target)\n\n    del mult\n\nfor i in trange(int(10e7)):\n    iteration()\n\nOn both my laptop (OSX, CPU) and a cluster node (RedHat, TitanX GPU) the memory use skyrockets with a few iterations. Since every torch object is created in the iteration() function the memory use should stay roughly stable from one iteration to the next.\nSome things I noticed:\n\nThe problem goes away when you apply the function to the whole batch in one go.\nThe problem also goes away when you don't store anything for backward.\nRecovering the tensor 'big' in backward(), or actually using it in the computation graph makes no difference.\nThe problem happens on both the CPU and the GPU.\nA possibly related issue\n\nI think I've reached the limits of my understanding of pytorch. Did I misunderstand the Function API, or is this a genuine memory leak? If it is a bug, is there a simple workaround I could use?", "body": "Due to some quirks of my model, I'm forced to loop over individual instances in a batch and apply a function to each individually. I ran into a memory leak, as illustrated by the following minimal example:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\nfrom torch import nn\r\nfrom tqdm import trange\r\n\r\nCUDA = False\r\n\r\nB = 2\r\nBIG = 512\r\n\r\ncriterion = nn.MSELoss()\r\n\r\nclass Mult(torch.autograd.Function):\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n    def forward(self, big, vector):\r\n\r\n        # Comment this and problems go away\r\n        self.save_for_backward(big)\r\n\r\n        return vector * 2.0\r\n\r\n    def backward(self, grad_output):\r\n\r\n        return None, None\r\n\r\ndef iteration():\r\n    mult = Mult()\r\n\r\n    big = torch.zeros(B, BIG, BIG)\r\n    x = torch.rand(B, 32)\r\n    y = torch.zeros(B, 32)\r\n    target = x.clone()\r\n\r\n    if CUDA:\r\n        big, x, y, target = big.cuda(), x.cuda(), y.cuda(), target.cuda()\r\n    big, x, y, target = Variable(big), Variable(x, requires_grad=True), Variable(y), Variable(target)\r\n\r\n    for b in range(B):\r\n      y[b, :] = mult(big[b, :, :], x[b, :])\r\n\r\n    # Use this instead of the above, and problems go away\r\n    # y = mult(big, x)\r\n\r\n    loss = criterion(y, target)\r\n\r\n    del mult\r\n\r\nfor i in trange(int(10e7)):\r\n    iteration()\r\n\r\n```\r\nOn both my laptop (OSX, CPU) and a cluster node (RedHat, TitanX GPU) the memory use skyrockets with a few iterations. Since every torch object is created in the iteration() function the memory use should stay roughly stable from one iteration to the next.\r\n\r\nSome things I noticed:\r\n\r\n* The problem goes away when you apply the function to the whole batch in one go.\r\n* The problem also goes away when you don't store anything for backward.\r\n* Recovering the tensor 'big' in backward(), or actually using it in the computation graph makes no difference.\r\n* The problem happens on both the CPU and the GPU.\r\n* [A possibly related issue](https://discuss.pytorch.org/t/autograd-for-sparse-matmul-getting-either-cuda-memory-leak-or-buffers-have-already-been-freed-error/3806)\r\n\r\nI think I've reached the limits of my understanding of pytorch. Did I misunderstand the Function API, or is this a genuine memory leak? If it is a bug, is there a simple workaround I could use?\r\n\r\n\r\n"}