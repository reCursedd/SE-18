{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8818", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8818/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8818/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8818/events", "html_url": "https://github.com/pytorch/pytorch/issues/8818", "id": 335060161, "node_id": "MDU6SXNzdWUzMzUwNjAxNjE=", "number": 8818, "title": "MPI causing job to hang --- unresponsive to external (termination) signals", "user": {"login": "MidoAssran", "id": 7530871, "node_id": "MDQ6VXNlcjc1MzA4NzE=", "avatar_url": "https://avatars2.githubusercontent.com/u/7530871?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MidoAssran", "html_url": "https://github.com/MidoAssran", "followers_url": "https://api.github.com/users/MidoAssran/followers", "following_url": "https://api.github.com/users/MidoAssran/following{/other_user}", "gists_url": "https://api.github.com/users/MidoAssran/gists{/gist_id}", "starred_url": "https://api.github.com/users/MidoAssran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MidoAssran/subscriptions", "organizations_url": "https://api.github.com/users/MidoAssran/orgs", "repos_url": "https://api.github.com/users/MidoAssran/repos", "events_url": "https://api.github.com/users/MidoAssran/events{/privacy}", "received_events_url": "https://api.github.com/users/MidoAssran/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-06-23T02:32:03Z", "updated_at": "2018-06-23T04:03:51Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>When running <code>torch.distributed</code> with the MPI backend in a cluster, the job fails to exit and hangs unresponsively --- essentially any node in the cluster that the runs the script becomes unresponsive when you try to terminate the job. This varies from run to run, but it happens often enough that it is problematic.</p>\n<p>Here is a minimal reproduction script. High level steps:</p>\n<ol>\n<li>initialize multiple DataLoader workers</li>\n<li>communicate model parameters</li>\n<li>update model parameters</li>\n<li>run forward-backward pass</li>\n</ol>\n<h2>Code example</h2>\n<p>For a single machine with <code>$world_size</code> GPUs, run</p>\n<p><code>mpirun -n $world_size python issues.py</code></p>\n<p>issues.py:</p>\n<pre><code>import os\nimport signal\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torch.multiprocessing as mp\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\n# path to dataset (REPLACE WITH YOUR OWN LOCAL PATH TO DATASET)\nTRAIN_DIRECTORY = '/datasets/imagenet_full_size/train'\n\n\ndef main():\n\n    signal.signal(signal.SIGTERM, SIGTERMHandler)\n\n    # initialize torch distributed mpi backend\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '40101'\n    dist.init_process_group(backend='mpi')\n\n    # run each task on a single GPU\n    torch.cuda.set_device(dist.get_rank())\n\n    # seed for reproducibility\n    torch.manual_seed(37)\n    torch.cuda.manual_seed(37)\n    torch.backends.cudnn.deterministic = True\n\n    model = models.resnet50()\n    model.cuda()\n    model.train()\n    criterion = torch.nn.CrossEntropyLoss().cuda()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9,\n                                weight_decay=0.0001, nesterov=True)\n\n    cudnn.benchmark = True\n\n    # initialize a dataloader with 4 worker processes\n    loader, sampler = make_dataloader(TRAIN_DIRECTORY)\n\n    for i, (input, target) in enumerate(loader):\n\n        print('itr {}'.format(i))\n\n        input = input.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # communicate model parameters\n        comm_buffer = _flatten_tensors(list(model.parameters())).detach_()\n        send_recieve(comm_buffer)\n        comm_buffer = _unflatten_tensors(comm_buffer, list(model.parameters()))\n        with torch.no_grad():\n            for p, e in zip(model.parameters(), comm_buffer):\n                p.data.copy_(e)\n\n        # forward/backward pass\n        output = model(input)\n        loss = criterion(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        print('* @Prec1 {prec1:.3f}\\t@Prec5 {prec5:.3f}'\n              .format(prec1=prec1.item(), prec5=prec5.item()))\n\n\ndef send_recieve(buffer):\n\n    buffer.mul_(0.5)\n\n    # ring communication\n    destination = (dist.get_rank() + 1) % dist.get_world_size()\n    source = dist.get_rank() - 1 if dist.get_rank() != 0 else dist.get_world_size() - 1\n\n    # non-blocking send message\n    send_buffer = buffer.clone()\n    req = dist.isend(tensor=send_buffer, dst=destination)\n    out_msg = (req, send_buffer)  # keep in scope\n\n    # blocking receive\n    receive = buffer.clone()\n    dist.recv(receive, src=source)\n\n    # update buffer\n    buffer.add_(receive)\n\n    # wait for neighbours to receive message\n    out_msg[0].wait()\n\n\ndef make_dataloader(train_directory):\n\"\"\" Create distributed dataloader \"\"\"\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    train_dataset = datasets.ImageFolder(train_directory, transforms.Compose([\n                        transforms.RandomResizedCrop(224),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.ToTensor(),\n                        normalize]))\n\n    # sampler produces indices used to assign data samples to each agent\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n                        dataset=train_dataset,\n                        num_replicas=dist.get_world_size(),\n                        rank=dist.get_rank())\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=32,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True, sampler=train_sampler)\n\n    return train_loader, train_sampler\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\" Computes the precision@k for the specified values of k \"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef _flatten_tensors(tensors):\n    if len(tensors) == 1:\n        return tensors[0].view(-1)\n    flat = torch.cat([t.view(-1) for t in tensors], dim=0)\n    return flat\n\n\ndef _unflatten_tensors(flat, tensors):\n    outputs = []\n    offset = 0\n    for tensor in tensors:\n        numel = tensor.numel()\n        outputs.append(flat.narrow(0, offset, numel).view_as(tensor))\n        offset += numel\n    return tuple(outputs)\n\n\ndef SIGTERMHandler(signum, frame):\n    \"\"\"\n    Ignore SIGTERM preemption signal (doesn't stop preemption);\n    instead SIGUSR1 will be moved up and handled accordingly\n    \"\"\"\n    print('Received SIGTERM')\n\n\nif __name__ == '__main__':\n    mp.set_start_method('forkserver')\n    main()\n\n</code></pre>\n<h2>System Info</h2>\n<p>PyTorch (built from source) version: 0.5.0a0+f8c18e0<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010<br>\nCMake version: version 3.11.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nGPU[0-8]: V100 (NVIDIA voltas)</p>\n<p>Nvidia driver version: 384.81<br>\ncuDNN version: cudnn/v7.0-cuda.9.0</p>\n<p>Versions of relevant libraries:</p>\n<p>openmpi/3.0.0/gcc.5.4.0<br>\nNCCL/2.2.12-1-cuda.9.0</p>\n<p>[pip] numpy (1.14.3)<br>\n[pip] torch (0.5.0a0+f8c18e0)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] magma-cuda90   2.3.0       1         pytorch<br>\n[conda] torch                     0.5.0   <br>\n[conda] torchvision            0.2.1   </p>", "body_text": "Issue description\nWhen running torch.distributed with the MPI backend in a cluster, the job fails to exit and hangs unresponsively --- essentially any node in the cluster that the runs the script becomes unresponsive when you try to terminate the job. This varies from run to run, but it happens often enough that it is problematic.\nHere is a minimal reproduction script. High level steps:\n\ninitialize multiple DataLoader workers\ncommunicate model parameters\nupdate model parameters\nrun forward-backward pass\n\nCode example\nFor a single machine with $world_size GPUs, run\nmpirun -n $world_size python issues.py\nissues.py:\nimport os\nimport signal\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torch.multiprocessing as mp\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\n# path to dataset (REPLACE WITH YOUR OWN LOCAL PATH TO DATASET)\nTRAIN_DIRECTORY = '/datasets/imagenet_full_size/train'\n\n\ndef main():\n\n    signal.signal(signal.SIGTERM, SIGTERMHandler)\n\n    # initialize torch distributed mpi backend\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '40101'\n    dist.init_process_group(backend='mpi')\n\n    # run each task on a single GPU\n    torch.cuda.set_device(dist.get_rank())\n\n    # seed for reproducibility\n    torch.manual_seed(37)\n    torch.cuda.manual_seed(37)\n    torch.backends.cudnn.deterministic = True\n\n    model = models.resnet50()\n    model.cuda()\n    model.train()\n    criterion = torch.nn.CrossEntropyLoss().cuda()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9,\n                                weight_decay=0.0001, nesterov=True)\n\n    cudnn.benchmark = True\n\n    # initialize a dataloader with 4 worker processes\n    loader, sampler = make_dataloader(TRAIN_DIRECTORY)\n\n    for i, (input, target) in enumerate(loader):\n\n        print('itr {}'.format(i))\n\n        input = input.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # communicate model parameters\n        comm_buffer = _flatten_tensors(list(model.parameters())).detach_()\n        send_recieve(comm_buffer)\n        comm_buffer = _unflatten_tensors(comm_buffer, list(model.parameters()))\n        with torch.no_grad():\n            for p, e in zip(model.parameters(), comm_buffer):\n                p.data.copy_(e)\n\n        # forward/backward pass\n        output = model(input)\n        loss = criterion(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        print('* @Prec1 {prec1:.3f}\\t@Prec5 {prec5:.3f}'\n              .format(prec1=prec1.item(), prec5=prec5.item()))\n\n\ndef send_recieve(buffer):\n\n    buffer.mul_(0.5)\n\n    # ring communication\n    destination = (dist.get_rank() + 1) % dist.get_world_size()\n    source = dist.get_rank() - 1 if dist.get_rank() != 0 else dist.get_world_size() - 1\n\n    # non-blocking send message\n    send_buffer = buffer.clone()\n    req = dist.isend(tensor=send_buffer, dst=destination)\n    out_msg = (req, send_buffer)  # keep in scope\n\n    # blocking receive\n    receive = buffer.clone()\n    dist.recv(receive, src=source)\n\n    # update buffer\n    buffer.add_(receive)\n\n    # wait for neighbours to receive message\n    out_msg[0].wait()\n\n\ndef make_dataloader(train_directory):\n\"\"\" Create distributed dataloader \"\"\"\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    train_dataset = datasets.ImageFolder(train_directory, transforms.Compose([\n                        transforms.RandomResizedCrop(224),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.ToTensor(),\n                        normalize]))\n\n    # sampler produces indices used to assign data samples to each agent\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n                        dataset=train_dataset,\n                        num_replicas=dist.get_world_size(),\n                        rank=dist.get_rank())\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=32,\n        shuffle=False,\n        num_workers=4,\n        pin_memory=True, sampler=train_sampler)\n\n    return train_loader, train_sampler\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\" Computes the precision@k for the specified values of k \"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef _flatten_tensors(tensors):\n    if len(tensors) == 1:\n        return tensors[0].view(-1)\n    flat = torch.cat([t.view(-1) for t in tensors], dim=0)\n    return flat\n\n\ndef _unflatten_tensors(flat, tensors):\n    outputs = []\n    offset = 0\n    for tensor in tensors:\n        numel = tensor.numel()\n        outputs.append(flat.narrow(0, offset, numel).view_as(tensor))\n        offset += numel\n    return tuple(outputs)\n\n\ndef SIGTERMHandler(signum, frame):\n    \"\"\"\n    Ignore SIGTERM preemption signal (doesn't stop preemption);\n    instead SIGUSR1 will be moved up and handled accordingly\n    \"\"\"\n    print('Received SIGTERM')\n\n\nif __name__ == '__main__':\n    mp.set_start_method('forkserver')\n    main()\n\n\nSystem Info\nPyTorch (built from source) version: 0.5.0a0+f8c18e0\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010\nCMake version: version 3.11.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU[0-8]: V100 (NVIDIA voltas)\nNvidia driver version: 384.81\ncuDNN version: cudnn/v7.0-cuda.9.0\nVersions of relevant libraries:\nopenmpi/3.0.0/gcc.5.4.0\nNCCL/2.2.12-1-cuda.9.0\n[pip] numpy (1.14.3)\n[pip] torch (0.5.0a0+f8c18e0)\n[pip] torchvision (0.2.1)\n[conda] magma-cuda90   2.3.0       1         pytorch\n[conda] torch                     0.5.0   \n[conda] torchvision            0.2.1", "body": "## Issue description\r\nWhen running `torch.distributed` with the MPI backend in a cluster, the job fails to exit and hangs unresponsively --- essentially any node in the cluster that the runs the script becomes unresponsive when you try to terminate the job. This varies from run to run, but it happens often enough that it is problematic.\r\n\r\nHere is a minimal reproduction script. High level steps:\r\n\r\n1.  initialize multiple DataLoader workers\r\n2. communicate model parameters\r\n3. update model parameters\r\n4. run forward-backward pass\r\n\r\n## Code example\r\n\r\nFor a single machine with `$world_size` GPUs, run\r\n\r\n`mpirun -n $world_size python issues.py`\r\n\r\nissues.py:\r\n\r\n```\r\nimport os\r\nimport signal\r\n\r\nimport torch\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.distributed as dist\r\nimport torch.utils.data\r\nimport torch.utils.data.distributed\r\nimport torch.multiprocessing as mp\r\nimport torchvision.datasets as datasets\r\nimport torchvision.models as models\r\nimport torchvision.transforms as transforms\r\n\r\n# path to dataset (REPLACE WITH YOUR OWN LOCAL PATH TO DATASET)\r\nTRAIN_DIRECTORY = '/datasets/imagenet_full_size/train'\r\n\r\n\r\ndef main():\r\n\r\n    signal.signal(signal.SIGTERM, SIGTERMHandler)\r\n\r\n    # initialize torch distributed mpi backend\r\n    os.environ['MASTER_ADDR'] = 'localhost'\r\n    os.environ['MASTER_PORT'] = '40101'\r\n    dist.init_process_group(backend='mpi')\r\n\r\n    # run each task on a single GPU\r\n    torch.cuda.set_device(dist.get_rank())\r\n\r\n    # seed for reproducibility\r\n    torch.manual_seed(37)\r\n    torch.cuda.manual_seed(37)\r\n    torch.backends.cudnn.deterministic = True\r\n\r\n    model = models.resnet50()\r\n    model.cuda()\r\n    model.train()\r\n    criterion = torch.nn.CrossEntropyLoss().cuda()\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9,\r\n                                weight_decay=0.0001, nesterov=True)\r\n\r\n    cudnn.benchmark = True\r\n\r\n    # initialize a dataloader with 4 worker processes\r\n    loader, sampler = make_dataloader(TRAIN_DIRECTORY)\r\n\r\n    for i, (input, target) in enumerate(loader):\r\n\r\n        print('itr {}'.format(i))\r\n\r\n        input = input.cuda(non_blocking=True)\r\n        target = target.cuda(non_blocking=True)\r\n\r\n        # communicate model parameters\r\n        comm_buffer = _flatten_tensors(list(model.parameters())).detach_()\r\n        send_recieve(comm_buffer)\r\n        comm_buffer = _unflatten_tensors(comm_buffer, list(model.parameters()))\r\n        with torch.no_grad():\r\n            for p, e in zip(model.parameters(), comm_buffer):\r\n                p.data.copy_(e)\r\n\r\n        # forward/backward pass\r\n        output = model(input)\r\n        loss = criterion(output, target)\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\r\n        print('* @Prec1 {prec1:.3f}\\t@Prec5 {prec5:.3f}'\r\n              .format(prec1=prec1.item(), prec5=prec5.item()))\r\n\r\n\r\ndef send_recieve(buffer):\r\n\r\n    buffer.mul_(0.5)\r\n\r\n    # ring communication\r\n    destination = (dist.get_rank() + 1) % dist.get_world_size()\r\n    source = dist.get_rank() - 1 if dist.get_rank() != 0 else dist.get_world_size() - 1\r\n\r\n    # non-blocking send message\r\n    send_buffer = buffer.clone()\r\n    req = dist.isend(tensor=send_buffer, dst=destination)\r\n    out_msg = (req, send_buffer)  # keep in scope\r\n\r\n    # blocking receive\r\n    receive = buffer.clone()\r\n    dist.recv(receive, src=source)\r\n\r\n    # update buffer\r\n    buffer.add_(receive)\r\n\r\n    # wait for neighbours to receive message\r\n    out_msg[0].wait()\r\n\r\n\r\ndef make_dataloader(train_directory):\r\n\"\"\" Create distributed dataloader \"\"\"\r\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n                                     std=[0.229, 0.224, 0.225])\r\n\r\n    train_dataset = datasets.ImageFolder(train_directory, transforms.Compose([\r\n                        transforms.RandomResizedCrop(224),\r\n                        transforms.RandomHorizontalFlip(),\r\n                        transforms.ToTensor(),\r\n                        normalize]))\r\n\r\n    # sampler produces indices used to assign data samples to each agent\r\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\r\n                        dataset=train_dataset,\r\n                        num_replicas=dist.get_world_size(),\r\n                        rank=dist.get_rank())\r\n\r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset, batch_size=32,\r\n        shuffle=False,\r\n        num_workers=4,\r\n        pin_memory=True, sampler=train_sampler)\r\n\r\n    return train_loader, train_sampler\r\n\r\n\r\ndef accuracy(output, target, topk=(1,)):\r\n    \"\"\" Computes the precision@k for the specified values of k \"\"\"\r\n    with torch.no_grad():\r\n        maxk = max(topk)\r\n        batch_size = target.size(0)\r\n\r\n        _, pred = output.topk(maxk, 1, True, True)\r\n        pred = pred.t()\r\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\r\n\r\n        res = []\r\n        for k in topk:\r\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\r\n            res.append(correct_k.mul_(100.0 / batch_size))\r\n        return res\r\n\r\n\r\ndef _flatten_tensors(tensors):\r\n    if len(tensors) == 1:\r\n        return tensors[0].view(-1)\r\n    flat = torch.cat([t.view(-1) for t in tensors], dim=0)\r\n    return flat\r\n\r\n\r\ndef _unflatten_tensors(flat, tensors):\r\n    outputs = []\r\n    offset = 0\r\n    for tensor in tensors:\r\n        numel = tensor.numel()\r\n        outputs.append(flat.narrow(0, offset, numel).view_as(tensor))\r\n        offset += numel\r\n    return tuple(outputs)\r\n\r\n\r\ndef SIGTERMHandler(signum, frame):\r\n    \"\"\"\r\n    Ignore SIGTERM preemption signal (doesn't stop preemption);\r\n    instead SIGUSR1 will be moved up and handled accordingly\r\n    \"\"\"\r\n    print('Received SIGTERM')\r\n\r\n\r\nif __name__ == '__main__':\r\n    mp.set_start_method('forkserver')\r\n    main()\r\n\r\n```\r\n\r\n## System Info\r\n\r\nPyTorch (built from source) version: 0.5.0a0+f8c18e0\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.5.0-12ubuntu1~16.04) 5.5.0 20171010\r\nCMake version: version 3.11.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration:\r\nGPU[0-8]: V100 (NVIDIA voltas)\r\n\r\nNvidia driver version: 384.81\r\ncuDNN version: cudnn/v7.0-cuda.9.0\r\n\r\nVersions of relevant libraries:\r\n\r\nopenmpi/3.0.0/gcc.5.4.0\r\nNCCL/2.2.12-1-cuda.9.0\r\n\r\n[pip] numpy (1.14.3)\r\n[pip] torch (0.5.0a0+f8c18e0)\r\n[pip] torchvision (0.2.1)\r\n[conda] magma-cuda90   2.3.0       1         pytorch\r\n[conda] torch                     0.5.0   <pip>\r\n[conda] torchvision            0.2.1   <pip>"}