{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1100", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1100/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1100/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1100/events", "html_url": "https://github.com/pytorch/pytorch/issues/1100", "id": 216955914, "node_id": "MDU6SXNzdWUyMTY5NTU5MTQ=", "number": 1100, "title": "Feature Request: Support `.numpy()` with CUDA Tensor", "user": {"login": "zuoxingdong", "id": 18168681, "node_id": "MDQ6VXNlcjE4MTY4Njgx", "avatar_url": "https://avatars0.githubusercontent.com/u/18168681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuoxingdong", "html_url": "https://github.com/zuoxingdong", "followers_url": "https://api.github.com/users/zuoxingdong/followers", "following_url": "https://api.github.com/users/zuoxingdong/following{/other_user}", "gists_url": "https://api.github.com/users/zuoxingdong/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuoxingdong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuoxingdong/subscriptions", "organizations_url": "https://api.github.com/users/zuoxingdong/orgs", "repos_url": "https://api.github.com/users/zuoxingdong/repos", "events_url": "https://api.github.com/users/zuoxingdong/events{/privacy}", "received_events_url": "https://api.github.com/users/zuoxingdong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-03-25T07:05:55Z", "updated_at": "2017-03-25T07:46:59Z", "closed_at": "2017-03-25T07:46:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When using sending Tensor to GPU by <code>.cuda()</code>, if we want to retrieve the array to numpy, we have to firstly send it back to CPU by <code>var.data.cpu().numpy()</code>.</p>\n<p>In this case, it has to write a if-else condition for a generic code for both CPU and GPU.</p>\n<pre><code>if Tensor in CUDA:\n    var.data.cpu().numpy()\nelse:\n   var.data.numpy()\n</code></pre>\n<p>It is better to support <code>numpy()</code> with both CPU Tensor and CUDA Tensor. So that it is not necessary to write these if-else condition</p>", "body_text": "When using sending Tensor to GPU by .cuda(), if we want to retrieve the array to numpy, we have to firstly send it back to CPU by var.data.cpu().numpy().\nIn this case, it has to write a if-else condition for a generic code for both CPU and GPU.\nif Tensor in CUDA:\n    var.data.cpu().numpy()\nelse:\n   var.data.numpy()\n\nIt is better to support numpy() with both CPU Tensor and CUDA Tensor. So that it is not necessary to write these if-else condition", "body": "When using sending Tensor to GPU by `.cuda()`, if we want to retrieve the array to numpy, we have to firstly send it back to CPU by `var.data.cpu().numpy()`. \r\n\r\nIn this case, it has to write a if-else condition for a generic code for both CPU and GPU. \r\n```\r\nif Tensor in CUDA:\r\n    var.data.cpu().numpy()\r\nelse:\r\n   var.data.numpy()\r\n```\r\n\r\nIt is better to support `numpy()` with both CPU Tensor and CUDA Tensor. So that it is not necessary to write these if-else condition"}