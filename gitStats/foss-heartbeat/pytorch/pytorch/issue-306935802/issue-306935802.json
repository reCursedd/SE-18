{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5908", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5908/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5908/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5908/events", "html_url": "https://github.com/pytorch/pytorch/issues/5908", "id": 306935802, "node_id": "MDU6SXNzdWUzMDY5MzU4MDI=", "number": 5908, "title": "[feature request] Add training={True|False} kwarg to nn.RNN.forward(), nn.Sequential.forward()", "user": {"login": "Fenugreek", "id": 3323801, "node_id": "MDQ6VXNlcjMzMjM4MDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3323801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Fenugreek", "html_url": "https://github.com/Fenugreek", "followers_url": "https://api.github.com/users/Fenugreek/followers", "following_url": "https://api.github.com/users/Fenugreek/following{/other_user}", "gists_url": "https://api.github.com/users/Fenugreek/gists{/gist_id}", "starred_url": "https://api.github.com/users/Fenugreek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Fenugreek/subscriptions", "organizations_url": "https://api.github.com/users/Fenugreek/orgs", "repos_url": "https://api.github.com/users/Fenugreek/repos", "events_url": "https://api.github.com/users/Fenugreek/events{/privacy}", "received_events_url": "https://api.github.com/users/Fenugreek/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-03-20T16:13:52Z", "updated_at": "2018-03-20T17:09:51Z", "closed_at": "2018-03-20T17:09:51Z", "author_association": "NONE", "body_html": "<p>Both the RNN Module subclass and Sequential Module subclass support dropout layers during construction, but it is currently bit of a gotcha to switch the dropout off when in evaluation mode.</p>\n<p>Can we have a simpler and more visible way to switch off dropout when not training, by for example having a <code>training=True</code> (default) kwarg to the <code>.forward()</code> method, similar to the kwarg in <code>nn.functional.dropout()</code>?</p>\n<p>Currently, for the RNN subclasses one has to do the following:</p>\n<pre><code>class myNN(nn.Module):\n\n    def __init__(self, n_layers=2, dropout=0.3):\n\n        super(myNN, self).__init__()\n\tself.gru = nn.GRU(100, 500, n_layers, dropout=dropout)\n        ...\n\n    def forward(self, inputs, hidden, training=True):\n\n\tself.gru.training = training\n        output = self.gru(inputs, hidden)\n        ...\n</code></pre>\n<p>And for the Sequential subclass, I think one has to explicitly reference all the layer(s) in the module list and set their <code>.training</code> attributes to <code>False</code>.</p>", "body_text": "Both the RNN Module subclass and Sequential Module subclass support dropout layers during construction, but it is currently bit of a gotcha to switch the dropout off when in evaluation mode.\nCan we have a simpler and more visible way to switch off dropout when not training, by for example having a training=True (default) kwarg to the .forward() method, similar to the kwarg in nn.functional.dropout()?\nCurrently, for the RNN subclasses one has to do the following:\nclass myNN(nn.Module):\n\n    def __init__(self, n_layers=2, dropout=0.3):\n\n        super(myNN, self).__init__()\n\tself.gru = nn.GRU(100, 500, n_layers, dropout=dropout)\n        ...\n\n    def forward(self, inputs, hidden, training=True):\n\n\tself.gru.training = training\n        output = self.gru(inputs, hidden)\n        ...\n\nAnd for the Sequential subclass, I think one has to explicitly reference all the layer(s) in the module list and set their .training attributes to False.", "body": "Both the RNN Module subclass and Sequential Module subclass support dropout layers during construction, but it is currently bit of a gotcha to switch the dropout off when in evaluation mode.\r\n\r\nCan we have a simpler and more visible way to switch off dropout when not training, by for example having a `training=True` (default) kwarg to the `.forward()` method, similar to the kwarg in `nn.functional.dropout()`?\r\n\r\nCurrently, for the RNN subclasses one has to do the following:\r\n\r\n```\r\nclass myNN(nn.Module):\r\n\r\n    def __init__(self, n_layers=2, dropout=0.3):\r\n\r\n        super(myNN, self).__init__()\r\n\tself.gru = nn.GRU(100, 500, n_layers, dropout=dropout)\r\n        ...\r\n\r\n    def forward(self, inputs, hidden, training=True):\r\n\r\n\tself.gru.training = training\r\n        output = self.gru(inputs, hidden)\r\n        ...\r\n```\r\n\r\nAnd for the Sequential subclass, I think one has to explicitly reference all the layer(s) in the module list and set their `.training` attributes to `False`."}