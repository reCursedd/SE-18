{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422974056", "html_url": "https://github.com/pytorch/pytorch/pull/11758#issuecomment-422974056", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11758", "id": 422974056, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjk3NDA1Ng==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-19T22:03:58Z", "updated_at": "2018-09-19T22:03:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9796\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/resistor\">@resistor</a> - on element-wise operations we also have <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cpu/vml.h\">vml.h</a> that defines a set of functions that can act as an interface to Intel's VML, or a loop + SLEEF if that's not available. Intel VML is much faster and also parallelizes (but of course doesn't let you merge operations). It's also worth checking whether some parts here can be gutted and replaced by Ops within MKL/MKL-DNN/iDeep etc.</p>\n<p>Having said this, in my mind, none of this is required for this and there's still value in dumping and simplifying code from TH/THNN and putting it into native.</p>", "body_text": "@resistor - on element-wise operations we also have vml.h that defines a set of functions that can act as an interface to Intel's VML, or a loop + SLEEF if that's not available. Intel VML is much faster and also parallelizes (but of course doesn't let you merge operations). It's also worth checking whether some parts here can be gutted and replaced by Ops within MKL/MKL-DNN/iDeep etc.\nHaving said this, in my mind, none of this is required for this and there's still value in dumping and simplifying code from TH/THNN and putting it into native.", "body": "@resistor - on element-wise operations we also have [vml.h](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/cpu/vml.h) that defines a set of functions that can act as an interface to Intel's VML, or a loop + SLEEF if that's not available. Intel VML is much faster and also parallelizes (but of course doesn't let you merge operations). It's also worth checking whether some parts here can be gutted and replaced by Ops within MKL/MKL-DNN/iDeep etc. \r\n\r\nHaving said this, in my mind, none of this is required for this and there's still value in dumping and simplifying code from TH/THNN and putting it into native."}