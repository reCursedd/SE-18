{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422603660", "html_url": "https://github.com/pytorch/pytorch/pull/11758#issuecomment-422603660", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11758", "id": 422603660, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjYwMzY2MA==", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-19T00:25:50Z", "updated_at": "2018-09-19T00:25:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> sure, I took a closer look at the forward implementation at CPU when weight.numel() = 1, and it looks the same to me. Here's the previous implementation:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/3da8d71d7de6f2eb3c9bbe395e38dc2fdf2784b0/aten/src/THNN/generic/PReLU.c#L5-L23\">pytorch/aten/src/THNN/generic/PReLU.c</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 5 to 23\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/3da8d71d7de6f2eb3c9bbe395e38dc2fdf2784b0\">3da8d71</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L5\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"5\"></td>\n          <td id=\"LC5\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">void</span> <span class=\"pl-en\">THNN_</span>(PReLU_updateOutput)( </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L6\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"6\"></td>\n          <td id=\"LC6\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           THNNState *state, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L7\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"7\"></td>\n          <td id=\"LC7\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           THTensor *input, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L8\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"8\"></td>\n          <td id=\"LC8\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           THTensor *output, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L9\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"9\"></td>\n          <td id=\"LC9\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           THTensor *weight) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L10\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"10\"></td>\n          <td id=\"LC10\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> { </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L11\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"11\"></td>\n          <td id=\"LC11\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">THTensor_</span>(resizeAs)(output, input); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L12\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"12\"></td>\n          <td id=\"LC12\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">int64_t</span> nOutputPlane = <span class=\"pl-c1\">THTensor_</span>(numel)(weight); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L13\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"13\"></td>\n          <td id=\"LC13\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">  </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L14\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"14\"></td>\n          <td id=\"LC14\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">if</span> (nOutputPlane == <span class=\"pl-c1\">1</span>) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L15\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"15\"></td>\n          <td id=\"LC15\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   { </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L16\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"16\"></td>\n          <td id=\"LC16\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c\"><span class=\"pl-c\">//</span> handle shared parameter case</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L17\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"17\"></td>\n          <td id=\"LC17\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c1\">scalar_t</span> w = *weight-&gt;<span class=\"pl-smi\">data</span>&lt;<span class=\"pl-c1\">scalar_t</span>&gt;(); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L18\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"18\"></td>\n          <td id=\"LC18\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c1\">TH_TENSOR_APPLY2</span>(<span class=\"pl-c1\">scalar_t</span>, output, <span class=\"pl-c1\">scalar_t</span>, input, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L19\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"19\"></td>\n          <td id=\"LC19\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           <span class=\"pl-k\">const</span> <span class=\"pl-c1\">scalar_t</span> r = (*input_data &gt; <span class=\"pl-c1\">0</span>) ? <span class=\"pl-c1\">1</span> : w; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L20\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"20\"></td>\n          <td id=\"LC20\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">           *output_data = *input_data * r; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L21\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"21\"></td>\n          <td id=\"LC21\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     ); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L22\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"22\"></td>\n          <td id=\"LC22\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">return</span>; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L23\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"23\"></td>\n          <td id=\"LC23\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   } </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>", "body_text": "@ezyang sure, I took a closer look at the forward implementation at CPU when weight.numel() = 1, and it looks the same to me. Here's the previous implementation:\n\n  \n    \n      pytorch/aten/src/THNN/generic/PReLU.c\n    \n    \n        Lines 5 to 23\n      in\n      3da8d71\n    \n    \n    \n    \n\n        \n          \n           void THNN_(PReLU_updateOutput)( \n        \n\n        \n          \n                     THNNState *state, \n        \n\n        \n          \n                     THTensor *input, \n        \n\n        \n          \n                     THTensor *output, \n        \n\n        \n          \n                     THTensor *weight) \n        \n\n        \n          \n           { \n        \n\n        \n          \n             THTensor_(resizeAs)(output, input); \n        \n\n        \n          \n             int64_t nOutputPlane = THTensor_(numel)(weight); \n        \n\n        \n          \n            \n        \n\n        \n          \n             if (nOutputPlane == 1) \n        \n\n        \n          \n             { \n        \n\n        \n          \n               // handle shared parameter case \n        \n\n        \n          \n               scalar_t w = *weight->data<scalar_t>(); \n        \n\n        \n          \n               TH_TENSOR_APPLY2(scalar_t, output, scalar_t, input, \n        \n\n        \n          \n                     const scalar_t r = (*input_data > 0) ? 1 : w; \n        \n\n        \n          \n                     *output_data = *input_data * r; \n        \n\n        \n          \n               ); \n        \n\n        \n          \n               return; \n        \n\n        \n          \n             }", "body": "@ezyang sure, I took a closer look at the forward implementation at CPU when weight.numel() = 1, and it looks the same to me. Here's the previous implementation: \r\nhttps://github.com/pytorch/pytorch/blob/3da8d71d7de6f2eb3c9bbe395e38dc2fdf2784b0/aten/src/THNN/generic/PReLU.c#L5-L23"}