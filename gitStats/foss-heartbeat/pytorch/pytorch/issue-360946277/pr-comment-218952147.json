{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218952147", "pull_request_review_id": 157007181, "id": 218952147, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxODk1MjE0Nw==", "diff_hunk": "@@ -2,9 +2,239 @@\n #include \"ATen/NativeFunctions.h\"\n #include \"ATen/Dispatch.h\"\n #include \"ATen/cuda/CUDAApplyUtils.cuh\"\n+#include \"ATen/cuda/detail/IndexUtils.cuh\"\n+\n \n namespace at { namespace native {\n \n+// -----------------------------------\n+// prelu forward\n+// -----------------------------------\n+template <typename scalar_t>\n+void prelu_cuda_kernel_share_weights(\n+  const Tensor& input,\n+  Tensor& result,\n+  const scalar_t* weight_data) {\n+\n+  at::cuda::CUDA_tensor_apply2<scalar_t, scalar_t>(\n+    input,\n+    result,\n+    [=] __device__ (\n+      const scalar_t& input_val,\n+      scalar_t& result_val) {\n+        result_val = (input_val > 0) ? input_val : *weight_data * input_val;\n+  });\n+}\n+\n+template <typename scalar_t>\n+__global__ void prelu_cuda_kernel_multi_weights(\n+  scalar_t* result_data,\n+  const scalar_t* input_data,\n+  const scalar_t* weight_data,\n+  int64_t input_stride0,\n+  int64_t input_stride1,\n+  int64_t input_numel) {\n+\n+  int64_t linearId = blockIdx.x * blockDim.x + threadIdx.x;\n+  if (linearId >= input_numel) return;\n+\n+  // multiply values at each channel with weight[channel_index]\n+  int64_t channel = (linearId % input_stride0) / input_stride1;\n+  scalar_t input_data_val = input_data[linearId];\n+  result_data[linearId] = (input_data_val > 0) ? input_data_val : weight_data[channel] * input_data_val;\n+}\n+\n+Tensor prelu_cuda(const Tensor& self, const Tensor& weight_) {\n+  AT_CHECK(self.is_cuda());\n+  AT_CHECK(weight_.is_cuda());\n+\n+  auto input = self.contiguous();\n+  auto weight = weight_.contiguous();\n+\n+  AT_CHECK(input.is_contiguous());\n+  AT_CHECK(weight.is_contiguous());\n+\n+  int64_t weight_num = weight.numel();\n+  Tensor result = at::empty_like(input);\n+  auto strides = input.strides();\n+\n+  // case1: shared weight for all channels\n+  if (weight_num == 1) {\n+    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"prelu_cuda\", [&] {\n+      prelu_cuda_kernel_share_weights<scalar_t>(\n+        input,\n+        result,\n+        weight.data<scalar_t>());\n+    });\n+  }\n+  else { // case2: multiple weights, one for each channel\n+    int64_t input_ndim = input.dim();\n+    AT_CHECK(input_ndim > 0, \"Not allow zero-dim input tensor.\");\n+\n+    int64_t channel_size = 1; // channel_size default to 1\n+    int64_t input_dim0_size = 1, input_stride0 = 1, input_stride1 = 1;\n+\n+    if (input_ndim > 1) {\n+      channel_size = input.size(1); // channel is the 2nd dim of input\n+      input_dim0_size = input.size(0);\n+      input_stride0 = strides[0];\n+      input_stride1 = strides[1];\n+    }\n+    AT_CHECK(channel_size == weight_num,\n+      \"Mismatch of parameter numbers and input channel size. Found parameter numbers = %d, and channel size = %d.\",\n+      weight_num, channel_size);\n+\n+    // config to run cuda kernel\n+    int64_t input_numel = input.numel();\n+    const dim3 block = dim3(std::min(static_cast<int64_t>(cuda::getApplyBlock().x), input_numel));\n+    dim3 grid;\n+    int curDevice = -1;\n+    cudaGetDevice(&curDevice);\n+    cudaStream_t stream = at::cuda::getCurrentCUDAStream(curDevice);\n+    AT_CHECK(cuda::getApplyGrid(input_numel, grid, curDevice), \"prelu: input too large or too many dimensions\");\n+\n+    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"prelu_cuda\", [&] {\n+      prelu_cuda_kernel_multi_weights<scalar_t>\n+      <<<grid, block, 0, stream>>>(\n+        result.data<scalar_t>(),\n+        input.data<scalar_t>(),\n+        weight.data<scalar_t>(),\n+        input_stride0,\n+        input_stride1,\n+        input_numel);\n+    });\n+  }\n+  return result;\n+}\n+\n+// -----------------------------------\n+// prelu backward\n+// -----------------------------------\n+template <typename scalar_t>\n+void prelu_cuda_backward_kernel_share_weights(\n+  const Tensor& input,\n+  const Tensor& grad_out,\n+  Tensor& input_grad,\n+  Tensor& weight_grad_collector,\n+  const scalar_t* weight_data) {\n+\n+  at::cuda::CUDA_tensor_apply4<scalar_t, scalar_t, scalar_t, scalar_t>(\n+    input,\n+    grad_out,\n+    input_grad,\n+    weight_grad_collector,\n+    [=] __device__ (\n+      const scalar_t& input_val,\n+      const scalar_t& grad_out_val,\n+      scalar_t& input_grad_val,\n+      scalar_t& weight_grad_collector_val) {\n+        input_grad_val = (input_val > 0) ? grad_out_val : *weight_data * grad_out_val;\n+        weight_grad_collector_val = (input_val > 0) ? scalar_t(0) : input_val * grad_out_val;\n+  });\n+}\n+\n+template <typename scalar_t>\n+__global__ void prelu_cuda_backward_kernel_multi_weights(\n+  const scalar_t* input_data,\n+  const scalar_t* weight_data,\n+  const scalar_t* grad_out_data,\n+  scalar_t* input_grad_data,\n+  scalar_t* weight_grad_collector,\n+  int64_t input_stride0,\n+  int64_t input_stride1,\n+  int64_t input_numel) {\n+\n+  int64_t linearId = blockIdx.x * blockDim.x + threadIdx.x;\n+  if (linearId >= input_numel) return;\n+  int64_t channel = (linearId % input_stride0) / input_stride1;\n+  scalar_t input_data_val = input_data[linearId];\n+  scalar_t grad_out_data_val = grad_out_data[linearId];\n+  input_grad_data[linearId] = (input_data_val > 0) ? grad_out_data_val : weight_data[channel] * grad_out_data_val;\n+  weight_grad_collector[linearId] = (input_data_val > 0) ? scalar_t(0) : input_data_val * grad_out_data_val;\n+}\n+\n+std::tuple<Tensor, Tensor> prelu_backward_cuda(const Tensor& grad_out_, const Tensor& self, const Tensor& weight_) {\n+  AT_CHECK(grad_out_.is_cuda());\n+  AT_CHECK(self.is_cuda());\n+  AT_CHECK(weight_.is_cuda());\n+\n+  auto input = self.contiguous();\n+  auto grad_out = grad_out_.contiguous();\n+  auto weight = weight_.contiguous();\n+\n+  AT_CHECK(input.is_contiguous());\n+  AT_CHECK(weight.is_contiguous());\n+  AT_CHECK(grad_out.is_contiguous());\n+\n+  int64_t weight_num = weight.numel();\n+  auto strides = input.strides();\n+  auto dims = input.dim();\n+  Tensor input_grad = at::empty_like(input);\n+  Tensor weight_grad = at::empty_like(weight);\n+  Tensor weight_grad_collector = at::empty_like(input);\n+  // case1: shared parameter for all channels\n+  if (weight_num == 1) {\n+    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"prelu_backward_cuda\", [&] {\n+      prelu_cuda_backward_kernel_share_weights<scalar_t>(\n+        input,\n+        grad_out,\n+        input_grad,\n+        weight_grad_collector,\n+        weight.data<scalar_t>());\n+    });\n+    weight_grad.fill_(weight_grad_collector.sum());\n+  }\n+  else { // case2: multiple parameters, one for each channel\n+    int64_t input_ndim = input.dim();\n+    AT_CHECK(input_ndim > 0, \"Not allow zero-dim input tensor.\");\n+\n+    int64_t channel_size = 1; // channel_size default to 1\n+    int64_t input_dim0_size = 1, input_stride0 = 1, input_stride1 = 1;\n+\n+    if (input_ndim > 1) {\n+      channel_size = input.size(1); // channel is the 2nd dim of input\n+      input_dim0_size = input.size(0);\n+      input_stride0 = strides[0];\n+      input_stride1 = strides[1];\n+    }\n+    AT_CHECK(channel_size == weight_num,\n+      \"Mismatch of parameter numbers and input channel size. Found parameter numbers = %d, and channel size = %d.\",\n+      weight_num, channel_size);\n+\n+    // config to run cuda kernel\n+    int64_t input_numel = input.numel();\n+    const dim3 block = dim3(std::min(static_cast<int64_t>(cuda::getApplyBlock().x), input_numel));\n+    dim3 grid;\n+    int curDevice = -1;\n+    cudaGetDevice(&curDevice);\n+    cudaStream_t stream = at::cuda::getCurrentCUDAStream(curDevice);\n+    AT_CHECK(cuda::getApplyGrid(input_numel, grid, curDevice), \"prelu_backward_cuda: input too large or too many dimensions\");\n+\n+    AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"prelu_backward_cuda\", [&] {\n+      prelu_cuda_backward_kernel_multi_weights<scalar_t>\n+      <<<grid, block, 0, stream>>>(\n+        input.data<scalar_t>(),\n+        weight.data<scalar_t>(),\n+        grad_out.data<scalar_t>(),\n+        input_grad.data<scalar_t>(),\n+        weight_grad_collector.data<scalar_t>(),\n+        input_stride0,\n+        input_stride1,\n+        input_numel);\n+    });\n+    // update weight_grad", "path": "aten/src/ATen/native/cuda/Activation.cu", "position": 224, "original_position": 224, "commit_id": "b4b5ae0ce41bd2cf83d23c5fea005ee05f92bc1e", "original_commit_id": "da2fbf4b263c9dd3dc9418d583ebaf219da901c8", "user": {"login": "weiyangfb", "id": 38509346, "node_id": "MDQ6VXNlcjM4NTA5MzQ2", "avatar_url": "https://avatars1.githubusercontent.com/u/38509346?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weiyangfb", "html_url": "https://github.com/weiyangfb", "followers_url": "https://api.github.com/users/weiyangfb/followers", "following_url": "https://api.github.com/users/weiyangfb/following{/other_user}", "gists_url": "https://api.github.com/users/weiyangfb/gists{/gist_id}", "starred_url": "https://api.github.com/users/weiyangfb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weiyangfb/subscriptions", "organizations_url": "https://api.github.com/users/weiyangfb/orgs", "repos_url": "https://api.github.com/users/weiyangfb/repos", "events_url": "https://api.github.com/users/weiyangfb/events{/privacy}", "received_events_url": "https://api.github.com/users/weiyangfb/received_events", "type": "User", "site_admin": false}, "body": "is there a better way to do the reduction on weight_grad? @SsnL ", "created_at": "2018-09-19T20:27:50Z", "updated_at": "2018-11-23T15:51:42Z", "html_url": "https://github.com/pytorch/pytorch/pull/11758#discussion_r218952147", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11758", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218952147"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11758#discussion_r218952147"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11758"}}, "body_html": "<p>is there a better way to do the reduction on weight_grad? <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5674597\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SsnL\">@SsnL</a></p>", "body_text": "is there a better way to do the reduction on weight_grad? @SsnL"}