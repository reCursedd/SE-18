{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422965671", "html_url": "https://github.com/pytorch/pytorch/pull/11758#issuecomment-422965671", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11758", "id": 422965671, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjk2NTY3MQ==", "user": {"login": "cpuhrsch", "id": 1716488, "node_id": "MDQ6VXNlcjE3MTY0ODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/1716488?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cpuhrsch", "html_url": "https://github.com/cpuhrsch", "followers_url": "https://api.github.com/users/cpuhrsch/followers", "following_url": "https://api.github.com/users/cpuhrsch/following{/other_user}", "gists_url": "https://api.github.com/users/cpuhrsch/gists{/gist_id}", "starred_url": "https://api.github.com/users/cpuhrsch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cpuhrsch/subscriptions", "organizations_url": "https://api.github.com/users/cpuhrsch/orgs", "repos_url": "https://api.github.com/users/cpuhrsch/repos", "events_url": "https://api.github.com/users/cpuhrsch/events{/privacy}", "received_events_url": "https://api.github.com/users/cpuhrsch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-19T21:31:37Z", "updated_at": "2018-09-19T21:31:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9796\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/resistor\">@resistor</a> - the only issue is that this won't be compiled with -mavx/-mavx2 etc. if it doesn't live within native/cpu for the OSS version, where we rely on a dispatch to not assume avx/avx2 capabilities for any CPU. But we can easily move it into there and use the autovectorization. Having said that, just because the compiler vectorizes it doesn't mean it's faster, because it might try to maintain invariants we don't care about. For example, we might want to tradeoff adding one number at a time with adding 8 floats to 8 adders at a time. This won't be the same results, but it's much faster and will still be reproducible.</p>", "body_text": "@resistor - the only issue is that this won't be compiled with -mavx/-mavx2 etc. if it doesn't live within native/cpu for the OSS version, where we rely on a dispatch to not assume avx/avx2 capabilities for any CPU. But we can easily move it into there and use the autovectorization. Having said that, just because the compiler vectorizes it doesn't mean it's faster, because it might try to maintain invariants we don't care about. For example, we might want to tradeoff adding one number at a time with adding 8 floats to 8 adders at a time. This won't be the same results, but it's much faster and will still be reproducible.", "body": "@resistor - the only issue is that this won't be compiled with -mavx/-mavx2 etc. if it doesn't live within native/cpu for the OSS version, where we rely on a dispatch to not assume avx/avx2 capabilities for any CPU. But we can easily move it into there and use the autovectorization. Having said that, just because the compiler vectorizes it doesn't mean it's faster, because it might try to maintain invariants we don't care about. For example, we might want to tradeoff adding one number at a time with adding 8 floats to 8 adders at a time. This won't be the same results, but it's much faster and will still be reproducible."}