{"url": "https://api.github.com/repos/pytorch/pytorch/issues/2298", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/2298/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/2298/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/2298/events", "html_url": "https://github.com/pytorch/pytorch/issues/2298", "id": 248150728, "node_id": "MDU6SXNzdWUyNDgxNTA3Mjg=", "number": 2298, "title": "BCEWithLogitsLoss computes wrong gradient?", "user": {"login": "amrsharaf", "id": 1218410, "node_id": "MDQ6VXNlcjEyMTg0MTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1218410?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amrsharaf", "html_url": "https://github.com/amrsharaf", "followers_url": "https://api.github.com/users/amrsharaf/followers", "following_url": "https://api.github.com/users/amrsharaf/following{/other_user}", "gists_url": "https://api.github.com/users/amrsharaf/gists{/gist_id}", "starred_url": "https://api.github.com/users/amrsharaf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amrsharaf/subscriptions", "organizations_url": "https://api.github.com/users/amrsharaf/orgs", "repos_url": "https://api.github.com/users/amrsharaf/repos", "events_url": "https://api.github.com/users/amrsharaf/events{/privacy}", "received_events_url": "https://api.github.com/users/amrsharaf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-08-05T01:59:22Z", "updated_at": "2017-08-05T05:46:17Z", "closed_at": "2017-08-05T05:46:17Z", "author_association": "NONE", "body_html": "<p>Here are two identical pieces of code using BCELoss and BCEWithLogitsLoss, the loss is similar, but the gradient is different, the correct gradient should be 0.5:</p>\n<pre><code>def main():\n    print('BCELoss():')\n    criterion = nn.BCELoss()\n    w = Variable(torch.Tensor([0.0]), requires_grad=True)\n    x = Variable(torch.Tensor([1]), requires_grad=True)\n    s = w * x\n    loss = criterion(F.sigmoid(s), Variable(torch.Tensor([0.0])))\n    print('loss: ', loss)\n    loss.backward()\n    print('w.grad: ', w.grad)\n    print('==============')\n    print('BCEWithLogitsLoss():')\n    criterion = nn.BCEWithLogitsLoss()\n    w = Variable(torch.Tensor([0.0]), requires_grad=True)\n    x = Variable(torch.Tensor([1]), requires_grad=True)\n    s = w * x\n    loss = criterion(s, Variable(torch.Tensor([0.0])))\n    print('loss: ', loss)\n    loss.backward()\n    print('w.grad: ', w.grad)\n    print('==============')\n</code></pre>\n<p>output:</p>\n<blockquote>\n<p>BCELoss():<br>\nloss:  Variable containing:<br>\n0.6931<br>\n[torch.FloatTensor of size 1]</p>\n<p>w.grad:  Variable containing:<br>\n0.5000<br>\n[torch.FloatTensor of size 1]</p>\n<p>==============<br>\nBCEWithLogitsLoss():<br>\nloss:  Variable containing:<br>\n0.6931<br>\n[torch.FloatTensor of size 1]</p>\n<p>w.grad:  Variable containing:<br>\n0<br>\n[torch.FloatTensor of size 1]</p>\n</blockquote>", "body_text": "Here are two identical pieces of code using BCELoss and BCEWithLogitsLoss, the loss is similar, but the gradient is different, the correct gradient should be 0.5:\ndef main():\n    print('BCELoss():')\n    criterion = nn.BCELoss()\n    w = Variable(torch.Tensor([0.0]), requires_grad=True)\n    x = Variable(torch.Tensor([1]), requires_grad=True)\n    s = w * x\n    loss = criterion(F.sigmoid(s), Variable(torch.Tensor([0.0])))\n    print('loss: ', loss)\n    loss.backward()\n    print('w.grad: ', w.grad)\n    print('==============')\n    print('BCEWithLogitsLoss():')\n    criterion = nn.BCEWithLogitsLoss()\n    w = Variable(torch.Tensor([0.0]), requires_grad=True)\n    x = Variable(torch.Tensor([1]), requires_grad=True)\n    s = w * x\n    loss = criterion(s, Variable(torch.Tensor([0.0])))\n    print('loss: ', loss)\n    loss.backward()\n    print('w.grad: ', w.grad)\n    print('==============')\n\noutput:\n\nBCELoss():\nloss:  Variable containing:\n0.6931\n[torch.FloatTensor of size 1]\nw.grad:  Variable containing:\n0.5000\n[torch.FloatTensor of size 1]\n==============\nBCEWithLogitsLoss():\nloss:  Variable containing:\n0.6931\n[torch.FloatTensor of size 1]\nw.grad:  Variable containing:\n0\n[torch.FloatTensor of size 1]", "body": "Here are two identical pieces of code using BCELoss and BCEWithLogitsLoss, the loss is similar, but the gradient is different, the correct gradient should be 0.5:\r\n\r\n```\r\ndef main():\r\n    print('BCELoss():')\r\n    criterion = nn.BCELoss()\r\n    w = Variable(torch.Tensor([0.0]), requires_grad=True)\r\n    x = Variable(torch.Tensor([1]), requires_grad=True)\r\n    s = w * x\r\n    loss = criterion(F.sigmoid(s), Variable(torch.Tensor([0.0])))\r\n    print('loss: ', loss)\r\n    loss.backward()\r\n    print('w.grad: ', w.grad)\r\n    print('==============')\r\n    print('BCEWithLogitsLoss():')\r\n    criterion = nn.BCEWithLogitsLoss()\r\n    w = Variable(torch.Tensor([0.0]), requires_grad=True)\r\n    x = Variable(torch.Tensor([1]), requires_grad=True)\r\n    s = w * x\r\n    loss = criterion(s, Variable(torch.Tensor([0.0])))\r\n    print('loss: ', loss)\r\n    loss.backward()\r\n    print('w.grad: ', w.grad)\r\n    print('==============')\r\n```\r\n\r\noutput:\r\n\r\n> BCELoss():\r\n> loss:  Variable containing:\r\n>  0.6931\r\n> [torch.FloatTensor of size 1]\r\n> \r\n> w.grad:  Variable containing:\r\n>  0.5000\r\n> [torch.FloatTensor of size 1]\r\n> \r\n> ==============\r\n> BCEWithLogitsLoss():\r\n> loss:  Variable containing:\r\n>  0.6931\r\n> [torch.FloatTensor of size 1]\r\n> \r\n> w.grad:  Variable containing:\r\n>  0\r\n> [torch.FloatTensor of size 1]"}