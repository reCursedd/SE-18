{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/425463531", "html_url": "https://github.com/pytorch/pytorch/pull/11903#issuecomment-425463531", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11903", "id": 425463531, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTQ2MzUzMQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-28T14:58:44Z", "updated_at": "2018-09-28T14:58:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Before:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> CUDAFloatType.cpp</span>\n\nTensor <span class=\"pl-en\">CUDAFloatType::_cudnn_init_dropout_state</span>(<span class=\"pl-k\">double</span> dropout, <span class=\"pl-k\">bool</span> train, <span class=\"pl-c1\">int64_t</span> dropout_seed) <span class=\"pl-k\">const</span> {\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> DeviceGuard omitted</span>\n    <span class=\"pl-k\">const</span> <span class=\"pl-k\">auto</span>&amp; self_ty = *<span class=\"pl-c1\">this</span>;\n    (<span class=\"pl-k\">void</span>)self_ty;\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">at::native::_cudnn_init_dropout_state</span>(<span class=\"pl-c\"><span class=\"pl-c\">/*</span> actuals <span class=\"pl-c\">*/</span></span> self_ty, dropout, train, dropout_seed);\n}</pre></div>\n<p>After:</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> CUDAFloatType.cpp</span>\n\nTensor <span class=\"pl-en\">CUDAFloatType::_cudnn_init_dropout_state</span>(<span class=\"pl-k\">double</span> dropout, <span class=\"pl-k\">bool</span> train, <span class=\"pl-c1\">int64_t</span> dropout_seed) <span class=\"pl-k\">const</span> {\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> DeviceGuard omitted</span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">at::native::_cudnn_init_dropout_state</span>(<span class=\"pl-c\"><span class=\"pl-c\">/*</span> actuals <span class=\"pl-c\">*/</span></span> *<span class=\"pl-c1\">this</span>, dropout, train, dropout_seed);\n}</pre></div>", "body_text": "Before:\n// CUDAFloatType.cpp\n\nTensor CUDAFloatType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed) const {\n    // DeviceGuard omitted\n    const auto& self_ty = *this;\n    (void)self_ty;\n    return at::native::_cudnn_init_dropout_state(/* actuals */ self_ty, dropout, train, dropout_seed);\n}\nAfter:\n// CUDAFloatType.cpp\n\nTensor CUDAFloatType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed) const {\n    // DeviceGuard omitted\n    return at::native::_cudnn_init_dropout_state(/* actuals */ *this, dropout, train, dropout_seed);\n}", "body": "Before:\r\n```cpp\r\n// CUDAFloatType.cpp\r\n\r\nTensor CUDAFloatType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed) const {\r\n    // DeviceGuard omitted\r\n    const auto& self_ty = *this;\r\n    (void)self_ty;\r\n    return at::native::_cudnn_init_dropout_state(/* actuals */ self_ty, dropout, train, dropout_seed);\r\n}\r\n```\r\nAfter:\r\n```cpp\r\n// CUDAFloatType.cpp\r\n\r\nTensor CUDAFloatType::_cudnn_init_dropout_state(double dropout, bool train, int64_t dropout_seed) const {\r\n    // DeviceGuard omitted\r\n    return at::native::_cudnn_init_dropout_state(/* actuals */ *this, dropout, train, dropout_seed);\r\n}\r\n```"}