{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201761678", "pull_request_review_id": 136033207, "id": 201761678, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMTc2MTY3OA==", "diff_hunk": "@@ -182,11 +185,58 @@ struct Parser {\n       L.expect(end);\n     return List<T>::create(r, elements);\n   }\n+\n   Const parseConst() {\n     auto range = L.cur().range;\n     auto t = L.expect(TK_NUMBER);\n     return Const::create(t.range, t.text());\n   }\n+\n+  bool isCharCount(char c, const std::string& str, size_t start, int len) {\n+    //count checks from [start, start + len) \n+    return start + len <= str.size() && std::count(str.begin() + start, str.begin() + start + len, c) == len; \n+  }\n+\n+  std::string parseLexedString(const std::string &str, size_t start) {", "path": "torch/csrc/jit/script/parser.h", "position": null, "original_position": 26, "commit_id": "e905658d61f902d3c9b75bf08d2504eb7e7b3e6f", "original_commit_id": "b2decb1ecffef6120bd76460622caa004bf0bac0", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This is a bit confusing. Why do we concatenate adjacent string literals in lexer, but we do it in a way that requires cleanup later? Either leave them separate in the lexer and do all the work in the parser, or emit nice and clean STRINGLITERAL tokens.", "created_at": "2018-07-11T16:36:45Z", "updated_at": "2018-11-23T15:47:08Z", "html_url": "https://github.com/pytorch/pytorch/pull/9324#discussion_r201761678", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9324", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/201761678"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9324#discussion_r201761678"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9324"}}, "body_html": "<p>This is a bit confusing. Why do we concatenate adjacent string literals in lexer, but we do it in a way that requires cleanup later? Either leave them separate in the lexer and do all the work in the parser, or emit nice and clean STRINGLITERAL tokens.</p>", "body_text": "This is a bit confusing. Why do we concatenate adjacent string literals in lexer, but we do it in a way that requires cleanup later? Either leave them separate in the lexer and do all the work in the parser, or emit nice and clean STRINGLITERAL tokens."}