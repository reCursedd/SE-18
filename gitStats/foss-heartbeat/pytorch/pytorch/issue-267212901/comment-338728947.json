{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/338728947", "html_url": "https://github.com/pytorch/pytorch/issues/3202#issuecomment-338728947", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3202", "id": 338728947, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODcyODk0Nw==", "user": {"login": "elistevens", "id": 138016, "node_id": "MDQ6VXNlcjEzODAxNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/138016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elistevens", "html_url": "https://github.com/elistevens", "followers_url": "https://api.github.com/users/elistevens/followers", "following_url": "https://api.github.com/users/elistevens/following{/other_user}", "gists_url": "https://api.github.com/users/elistevens/gists{/gist_id}", "starred_url": "https://api.github.com/users/elistevens/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elistevens/subscriptions", "organizations_url": "https://api.github.com/users/elistevens/orgs", "repos_url": "https://api.github.com/users/elistevens/repos", "events_url": "https://api.github.com/users/elistevens/events{/privacy}", "received_events_url": "https://api.github.com/users/elistevens/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-23T17:08:19Z", "updated_at": "2017-10-23T17:08:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Speaking only as an end-use with no particular insight into how difficult it would be to code:</p>\n<p>These suggestions make me feel like end users will end up having to think more about device semantics, not less. I'd like to see the API move away from having the tensor type imply the storage location entirely. Have a context manager that sets cpu vs. cuda, default device, etc. Something like:</p>\n<pre><code>_tensorDefaults_stack = []\n@contextmanager\ndef setTensorDefaults(location=torch.cpu_tensors, device=None, dtype=torch.float, pin_memory=None):\n    assert location != torch.cuda_tensors or device is not None\n    assert device is None or pin_memory is None\n    _tensorDefaults_stack.append((location, device, dtype, pin_memory))\n    yield\n    _tensorDefaults_stack.pop()\n\nwith setTensorDefauts(location=torch.cuda_tensors, device=3):\n    x = torch.ones((5, ,5))\n    y = torch.Tensor([1,2,3,4,5]) # uses whatever is at the top of the _tensorDefaults_stack\n</code></pre>\n<p>Then once this exists, <code>DataLoader</code> class wraps its calls to the <code>Dataset</code> with an appropriate context, as do Modules. I think that this results in everything Just Working(tm).</p>", "body_text": "Speaking only as an end-use with no particular insight into how difficult it would be to code:\nThese suggestions make me feel like end users will end up having to think more about device semantics, not less. I'd like to see the API move away from having the tensor type imply the storage location entirely. Have a context manager that sets cpu vs. cuda, default device, etc. Something like:\n_tensorDefaults_stack = []\n@contextmanager\ndef setTensorDefaults(location=torch.cpu_tensors, device=None, dtype=torch.float, pin_memory=None):\n    assert location != torch.cuda_tensors or device is not None\n    assert device is None or pin_memory is None\n    _tensorDefaults_stack.append((location, device, dtype, pin_memory))\n    yield\n    _tensorDefaults_stack.pop()\n\nwith setTensorDefauts(location=torch.cuda_tensors, device=3):\n    x = torch.ones((5, ,5))\n    y = torch.Tensor([1,2,3,4,5]) # uses whatever is at the top of the _tensorDefaults_stack\n\nThen once this exists, DataLoader class wraps its calls to the Dataset with an appropriate context, as do Modules. I think that this results in everything Just Working(tm).", "body": "Speaking only as an end-use with no particular insight into how difficult it would be to code:\r\n\r\nThese suggestions make me feel like end users will end up having to think more about device semantics, not less. I'd like to see the API move away from having the tensor type imply the storage location entirely. Have a context manager that sets cpu vs. cuda, default device, etc. Something like:\r\n\r\n```\r\n_tensorDefaults_stack = []\r\n@contextmanager\r\ndef setTensorDefaults(location=torch.cpu_tensors, device=None, dtype=torch.float, pin_memory=None):\r\n    assert location != torch.cuda_tensors or device is not None\r\n    assert device is None or pin_memory is None\r\n    _tensorDefaults_stack.append((location, device, dtype, pin_memory))\r\n    yield\r\n    _tensorDefaults_stack.pop()\r\n\r\nwith setTensorDefauts(location=torch.cuda_tensors, device=3):\r\n    x = torch.ones((5, ,5))\r\n    y = torch.Tensor([1,2,3,4,5]) # uses whatever is at the top of the _tensorDefaults_stack\r\n```\r\n\r\nThen once this exists, `DataLoader` class wraps its calls to the `Dataset` with an appropriate context, as do Modules. I think that this results in everything Just Working(tm)."}