{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/338686591", "html_url": "https://github.com/pytorch/pytorch/issues/3202#issuecomment-338686591", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3202", "id": 338686591, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODY4NjU5MQ==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-23T14:56:35Z", "updated_at": "2017-10-23T14:56:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=263228\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/LMescheder\">@LMescheder</a> one thing I like about <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>'s <code>Add device-aware dtype</code> suggestion is that<br>\nit makes it explicit and obvious when something is happening on cuda, rather than requiring users to know that non-negative device numbers refer to cuda.</p>\n<p>Maybe this is a bad idea, but what about also taking an optional <code>device</code> kw in every function that takes a <code>dtype</code> but to have both <code>cuda</code> and non-<code>cuda</code> dtypes?  So passing <code>dtype=torch.cuda.float</code> would give you a cuda float tensor on the current GPU, passing <code>dtype=torch.float</code> would give you a CPU float, and passing <code>dtype=torch.cuda.float, device=5</code> would give you a cuda tensor on GPU 5.  Obviously we'd need error checking to check things like non-<code>cuda</code> devices don't have non-negative <code>device</code>s specified (which is kind of silly because <em>if</em> the device is specified it tells you everything), but it does seem to make the common case of not specifying a device more clear.</p>", "body_text": "@LMescheder one thing I like about @apaszke's Add device-aware dtype suggestion is that\nit makes it explicit and obvious when something is happening on cuda, rather than requiring users to know that non-negative device numbers refer to cuda.\nMaybe this is a bad idea, but what about also taking an optional device kw in every function that takes a dtype but to have both cuda and non-cuda dtypes?  So passing dtype=torch.cuda.float would give you a cuda float tensor on the current GPU, passing dtype=torch.float would give you a CPU float, and passing dtype=torch.cuda.float, device=5 would give you a cuda tensor on GPU 5.  Obviously we'd need error checking to check things like non-cuda devices don't have non-negative devices specified (which is kind of silly because if the device is specified it tells you everything), but it does seem to make the common case of not specifying a device more clear.", "body": "@LMescheder one thing I like about @apaszke's `Add device-aware dtype` suggestion is that\r\n it makes it explicit and obvious when something is happening on cuda, rather than requiring users to know that non-negative device numbers refer to cuda.\r\n\r\nMaybe this is a bad idea, but what about also taking an optional `device` kw in every function that takes a `dtype` but to have both `cuda` and non-`cuda` dtypes?  So passing `dtype=torch.cuda.float` would give you a cuda float tensor on the current GPU, passing `dtype=torch.float` would give you a CPU float, and passing `dtype=torch.cuda.float, device=5` would give you a cuda tensor on GPU 5.  Obviously we'd need error checking to check things like non-`cuda` devices don't have non-negative `device`s specified (which is kind of silly because _if_ the device is specified it tells you everything), but it does seem to make the common case of not specifying a device more clear."}