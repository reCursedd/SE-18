{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/338491514", "html_url": "https://github.com/pytorch/pytorch/issues/3202#issuecomment-338491514", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3202", "id": 338491514, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODQ5MTUxNA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-22T16:46:41Z", "updated_at": "2017-10-22T16:46:41Z", "author_association": "MEMBER", "body_html": "<p>I agree we should figure out a way to make writing device-agnostic code easier. I don't think fakecuda is a good idea, it's too magical and has global side effects. Here's a list of options I see, along with some comments. In general I'm not 100% happy with any of those, but I don't have any better ideas right now.</p>\n<h4>Add device-aware dtype</h4>\n<p>Example:</p>\n<div class=\"highlight highlight-source-python\"><pre>dtype <span class=\"pl-k\">=</span> torch.cuda.float\ntorch.ones(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype) <span class=\"pl-c\"><span class=\"pl-c\">#</span> Returns on the current GPU</span></pre></div>\n<p>I think that sooner or later we should add <code>dtype</code> arg to <code>torch</code> functions anyway, so it seems like a good enough extension.</p>\n<h4><code>get_array_module()</code></h4>\n<p>I think I'm fine with this option too, but I'm afraid you would either need a single global module captured inside functions, or use it everywhere inside. In general it can be simulated using the <code>dtype</code> approach using partials (of course this is an inefficient implementation, but you get the idea):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch <span class=\"pl-k\">as</span> _torch\n\ndtype <span class=\"pl-k\">=</span> <span class=\"pl-c1\">...</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> pick CUDA or CPU dtype</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">DeviceAgnosticModule</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getattribute__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">name</span>):\n        obj <span class=\"pl-k\">=</span> <span class=\"pl-c1\">getattr</span>(torch, name)\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">callable</span>(obj):\n            <span class=\"pl-k\">return</span> obj\n        <span class=\"pl-k\">return</span> functools.partial(obj, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype)</pre></div>\n<h4><code>cast</code></h4>\n<p>This is fine, except that you have to call it a lot and it makes the code super verbose. A similar alternative that people sometimes use now is this:</p>\n<div class=\"highlight highlight-source-python\"><pre>dtype <span class=\"pl-k\">=</span> torch.cuda.FloatTensor\nx <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>).type(dtype))</pre></div>\n<h4><code>new</code></h4>\n<p>This is really the best option, as it will give you a tensor of exactly the same type and on exactly the same device as the source. This is usually the desired behaviour, which is why we use it all over the place in the core library. On the other hand, it feels a bit unnatural, and is annoying when you do more complicated things, because you first construct the tensor and only then fill it with the data. <code>ones_like</code> and <code>zeros_like</code> helps a lot with that</p>", "body_text": "I agree we should figure out a way to make writing device-agnostic code easier. I don't think fakecuda is a good idea, it's too magical and has global side effects. Here's a list of options I see, along with some comments. In general I'm not 100% happy with any of those, but I don't have any better ideas right now.\nAdd device-aware dtype\nExample:\ndtype = torch.cuda.float\ntorch.ones(5, 5, dtype=dtype) # Returns on the current GPU\nI think that sooner or later we should add dtype arg to torch functions anyway, so it seems like a good enough extension.\nget_array_module()\nI think I'm fine with this option too, but I'm afraid you would either need a single global module captured inside functions, or use it everywhere inside. In general it can be simulated using the dtype approach using partials (of course this is an inefficient implementation, but you get the idea):\nimport torch as _torch\n\ndtype = ... # pick CUDA or CPU dtype\nclass DeviceAgnosticModule(object):\n    def __getattribute__(self, name):\n        obj = getattr(torch, name)\n        if not callable(obj):\n            return obj\n        return functools.partial(obj, dtype=dtype)\ncast\nThis is fine, except that you have to call it a lot and it makes the code super verbose. A similar alternative that people sometimes use now is this:\ndtype = torch.cuda.FloatTensor\nx = Variable(torch.randn(5, 5).type(dtype))\nnew\nThis is really the best option, as it will give you a tensor of exactly the same type and on exactly the same device as the source. This is usually the desired behaviour, which is why we use it all over the place in the core library. On the other hand, it feels a bit unnatural, and is annoying when you do more complicated things, because you first construct the tensor and only then fill it with the data. ones_like and zeros_like helps a lot with that", "body": "I agree we should figure out a way to make writing device-agnostic code easier. I don't think fakecuda is a good idea, it's too magical and has global side effects. Here's a list of options I see, along with some comments. In general I'm not 100% happy with any of those, but I don't have any better ideas right now.\r\n\r\n#### Add device-aware dtype\r\n\r\nExample:\r\n```python\r\ndtype = torch.cuda.float\r\ntorch.ones(5, 5, dtype=dtype) # Returns on the current GPU\r\n```\r\n\r\nI think that sooner or later we should add `dtype` arg to `torch` functions anyway, so it seems like a good enough extension.\r\n\r\n#### `get_array_module()`\r\n\r\nI think I'm fine with this option too, but I'm afraid you would either need a single global module captured inside functions, or use it everywhere inside. In general it can be simulated using the `dtype` approach using partials (of course this is an inefficient implementation, but you get the idea):\r\n\r\n```python\r\nimport torch as _torch\r\n\r\ndtype = ... # pick CUDA or CPU dtype\r\nclass DeviceAgnosticModule(object):\r\n    def __getattribute__(self, name):\r\n        obj = getattr(torch, name)\r\n        if not callable(obj):\r\n            return obj\r\n        return functools.partial(obj, dtype=dtype)\r\n```\r\n\r\n#### `cast`\r\n\r\nThis is fine, except that you have to call it a lot and it makes the code super verbose. A similar alternative that people sometimes use now is this:\r\n```python\r\ndtype = torch.cuda.FloatTensor\r\nx = Variable(torch.randn(5, 5).type(dtype))\r\n```\r\n\r\n#### `new`\r\n\r\nThis is really the best option, as it will give you a tensor of exactly the same type and on exactly the same device as the source. This is usually the desired behaviour, which is why we use it all over the place in the core library. On the other hand, it feels a bit unnatural, and is annoying when you do more complicated things, because you first construct the tensor and only then fill it with the data. `ones_like` and `zeros_like` helps a lot with that"}