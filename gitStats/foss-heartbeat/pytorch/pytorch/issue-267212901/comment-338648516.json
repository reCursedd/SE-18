{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/338648516", "html_url": "https://github.com/pytorch/pytorch/issues/3202#issuecomment-338648516", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3202", "id": 338648516, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODY0ODUxNg==", "user": {"login": "LMescheder", "id": 263228, "node_id": "MDQ6VXNlcjI2MzIyOA==", "avatar_url": "https://avatars0.githubusercontent.com/u/263228?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LMescheder", "html_url": "https://github.com/LMescheder", "followers_url": "https://api.github.com/users/LMescheder/followers", "following_url": "https://api.github.com/users/LMescheder/following{/other_user}", "gists_url": "https://api.github.com/users/LMescheder/gists{/gist_id}", "starred_url": "https://api.github.com/users/LMescheder/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LMescheder/subscriptions", "organizations_url": "https://api.github.com/users/LMescheder/orgs", "repos_url": "https://api.github.com/users/LMescheder/repos", "events_url": "https://api.github.com/users/LMescheder/events{/privacy}", "received_events_url": "https://api.github.com/users/LMescheder/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-23T12:50:48Z", "updated_at": "2017-10-23T12:50:48Z", "author_association": "NONE", "body_html": "<p>It would indeed be amazing if PyTorch could make it simpler to write device agnostic code.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> I really like your first suggestion.<br>\nJust a quick note from my (applied) perspective: maybe it would be possible to split up the <code>dtype</code> and the <code>device</code> into two attributes? First of all, I think this would make it easier to handle cases where tensors are on different gpus and second of all, it would allow to write code like this:<br>\n<code>torch.ones((5, 5), dtype=torch.float, device=x.device) # Returns on same device as x</code><br>\nDon't know how practical this is though.</p>", "body_text": "It would indeed be amazing if PyTorch could make it simpler to write device agnostic code.\n@apaszke I really like your first suggestion.\nJust a quick note from my (applied) perspective: maybe it would be possible to split up the dtype and the device into two attributes? First of all, I think this would make it easier to handle cases where tensors are on different gpus and second of all, it would allow to write code like this:\ntorch.ones((5, 5), dtype=torch.float, device=x.device) # Returns on same device as x\nDon't know how practical this is though.", "body": "It would indeed be amazing if PyTorch could make it simpler to write device agnostic code.\r\n\r\n@apaszke I really like your first suggestion.\r\nJust a quick note from my (applied) perspective: maybe it would be possible to split up the `dtype` and the `device` into two attributes? First of all, I think this would make it easier to handle cases where tensors are on different gpus and second of all, it would allow to write code like this:\r\n``\r\ntorch.ones((5, 5), dtype=torch.float, device=x.device) # Returns on same device as x\r\n``\r\nDon't know how practical this is though."}