{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/338801243", "html_url": "https://github.com/pytorch/pytorch/issues/3202#issuecomment-338801243", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3202", "id": 338801243, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODgwMTI0Mw==", "user": {"login": "t-vi", "id": 20787943, "node_id": "MDQ6VXNlcjIwNzg3OTQz", "avatar_url": "https://avatars2.githubusercontent.com/u/20787943?v=4", "gravatar_id": "", "url": "https://api.github.com/users/t-vi", "html_url": "https://github.com/t-vi", "followers_url": "https://api.github.com/users/t-vi/followers", "following_url": "https://api.github.com/users/t-vi/following{/other_user}", "gists_url": "https://api.github.com/users/t-vi/gists{/gist_id}", "starred_url": "https://api.github.com/users/t-vi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/t-vi/subscriptions", "organizations_url": "https://api.github.com/users/t-vi/orgs", "repos_url": "https://api.github.com/users/t-vi/repos", "events_url": "https://api.github.com/users/t-vi/events{/privacy}", "received_events_url": "https://api.github.com/users/t-vi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-23T21:24:35Z", "updated_at": "2017-10-23T21:24:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm not an expert on any of this, but about the only two things that bug me about using <code>new</code> is when I have to cast are</p>\n<ol>\n<li>when I don't have a prototypical tensor at hand (e.g. in a module it feels unnatural to use <code>self.param.data.new(...)...</code> so it might be neat to have a \"canonical tensor\" in modules that reacts to .cuda() and whose <code>.new</code> is exposed as <code>module.newtensor</code> or so.</li>\n<li>when I want a different type <code>my_float_tensor.new().long().resize(x,y).do_something</code> is just a bit much.</li>\n</ol>\n<p>Other than that, I think using it is cool, I think that there is not much advantage in<br>\n<code>torch.ones((5, 5), dtype=torch.float, device=x.device) </code><br>\nover<br>\n<code>torch.ones((5,5), out=x.new())</code></p>\n<p>But this is only from my limited experience.</p>", "body_text": "I'm not an expert on any of this, but about the only two things that bug me about using new is when I have to cast are\n\nwhen I don't have a prototypical tensor at hand (e.g. in a module it feels unnatural to use self.param.data.new(...)... so it might be neat to have a \"canonical tensor\" in modules that reacts to .cuda() and whose .new is exposed as module.newtensor or so.\nwhen I want a different type my_float_tensor.new().long().resize(x,y).do_something is just a bit much.\n\nOther than that, I think using it is cool, I think that there is not much advantage in\ntorch.ones((5, 5), dtype=torch.float, device=x.device) \nover\ntorch.ones((5,5), out=x.new())\nBut this is only from my limited experience.", "body": "I'm not an expert on any of this, but about the only two things that bug me about using `new` is when I have to cast are\r\n1. when I don't have a prototypical tensor at hand (e.g. in a module it feels unnatural to use `self.param.data.new(...)...` so it might be neat to have a \"canonical tensor\" in modules that reacts to .cuda() and whose `.new` is exposed as `module.newtensor` or so.\r\n2. when I want a different type `my_float_tensor.new().long().resize(x,y).do_something` is just a bit much.\r\n\r\nOther than that, I think using it is cool, I think that there is not much advantage in\r\n`torch.ones((5, 5), dtype=torch.float, device=x.device) `\r\nover\r\n`torch.ones((5,5), out=x.new())`\r\n\r\nBut this is only from my limited experience."}