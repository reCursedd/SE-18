{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/125103369", "pull_request_review_id": 47465073, "id": 125103369, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNTEwMzM2OQ==", "diff_hunk": "@@ -0,0 +1,114 @@\n+\"\"\"\n+Weight Normalization from https://arxiv.org/abs/1602.07868\n+\"\"\"\n+import torch.utils.hooks as hooks\n+from torch.nn.parameter import Parameter\n+\n+\n+class WeightNorm(object):\n+    def __init__(self, name, dim):\n+        self.name = name\n+        self.dim = dim\n+\n+    def compute_weight(self, module):\n+        g = getattr(module, self.name + '_g')\n+        v = getattr(module, self.name + '_v')\n+        return v * (g / self.norm(v))\n+\n+    def norm(self, p):\n+        \"\"\"Computes the norm over all dimensions except dim\"\"\"\n+        if self.dim is None:\n+            return p.norm()\n+        if self.dim != 0:\n+            p = p.transpose(0, self.dim)\n+        output_size = (p.size(0),) + (1,) * (p.dim() - 1)\n+        p = p.contiguous().view(p.size(0), -1).norm(dim=1).view(*output_size)\n+        if self.dim != 0:\n+            p = p.transpose(0, self.dim)\n+        return p\n+\n+    @staticmethod\n+    def apply(module, name, dim):\n+        fn = WeightNorm(name, dim)\n+\n+        weight = getattr(module, name)\n+\n+        # remove w from parameter list\n+        del module._parameters[name]\n+\n+        # add g and v as new parameters and express w as g/||v|| * v\n+        module.register_parameter(name + '_g', Parameter(fn.norm(weight).data))\n+        module.register_parameter(name + '_v', Parameter(weight.data))\n+        setattr(module, name, fn.compute_weight(module))\n+\n+        handle = hooks.RemovableHandle(module._forward_pre_hooks)\n+        module._forward_pre_hooks[handle.id] = fn\n+        fn.handle = handle\n+\n+        return fn\n+\n+    def remove(self, module):\n+        weight = self.compute_weight(module)\n+\n+        self.handle.remove()\n+        delattr(module, self.name)\n+        del module._parameters[self.name + '_g']\n+        del module._parameters[self.name + '_v']\n+        module.register_parameter(self.name, Parameter(weight.data))\n+\n+    def __call__(self, module, inputs):\n+        setattr(module, self.name, self.compute_weight(module))\n+\n+\n+def weight_norm(module, name='weight', dim=0):\n+    \"\"\"Applies weight normalization to a parameter in the given module.\n+\n+    Weight normalization is a reparameterization that decouples the magnitude\n+    of a weight tensor from its direction. This replaces the parameter specified\n+    by `name` (e.g. \"weight\") with two parameters: one specifying the magnitude\n+    (e.g. \"weight_g\") and one specifying the direction (e.g. \"weight_v\").\n+    The module's forward function is wrapped to first recompute the weight\n+    tensor from the magnitude and direction.\n+\n+    The magnitude parameter (\"weight_g\") contains weight.size(dim) elements.\n+    By default, weight is normalized per activation in an `nn.Linear` layer and\n+    per output-plane in a convolutional layer. If `dim` is `None`, the entire", "path": "torch/nn/utils/weight_norm.py", "position": null, "original_position": 75, "commit_id": "54f77f8150e0d1d57172278a88eaadf6b06be46c", "original_commit_id": "3f38d6bfa11222aa32304eab1866c4086d26b330", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Because by default, each output channel is normalized separately. (This is how its specified in the paper and how it's implemented in nn.WeightNorm). In most of our module's this corresponds to dim `0`.", "created_at": "2017-06-30T18:32:18Z", "updated_at": "2018-11-23T15:34:01Z", "html_url": "https://github.com/pytorch/pytorch/pull/1945#discussion_r125103369", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1945", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/125103369"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1945#discussion_r125103369"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1945"}}, "body_html": "<p>Because by default, each output channel is normalized separately. (This is how its specified in the paper and how it's implemented in nn.WeightNorm). In most of our module's this corresponds to dim <code>0</code>.</p>", "body_text": "Because by default, each output channel is normalized separately. (This is how its specified in the paper and how it's implemented in nn.WeightNorm). In most of our module's this corresponds to dim 0.", "in_reply_to_id": 125102622}