{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/125103622", "pull_request_review_id": 47465369, "id": 125103622, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNTEwMzYyMg==", "diff_hunk": "@@ -0,0 +1,114 @@\n+\"\"\"\n+Weight Normalization from https://arxiv.org/abs/1602.07868\n+\"\"\"\n+import torch.utils.hooks as hooks\n+from torch.nn.parameter import Parameter\n+\n+\n+class WeightNorm(object):\n+    def __init__(self, name, dim):\n+        self.name = name\n+        self.dim = dim\n+\n+    def compute_weight(self, module):\n+        g = getattr(module, self.name + '_g')\n+        v = getattr(module, self.name + '_v')\n+        return v * (g / self.norm(v))\n+\n+    def norm(self, p):\n+        \"\"\"Computes the norm over all dimensions except dim\"\"\"\n+        if self.dim is None:\n+            return p.norm()\n+        if self.dim != 0:\n+            p = p.transpose(0, self.dim)\n+        output_size = (p.size(0),) + (1,) * (p.dim() - 1)\n+        p = p.contiguous().view(p.size(0), -1).norm(dim=1).view(*output_size)\n+        if self.dim != 0:\n+            p = p.transpose(0, self.dim)\n+        return p\n+\n+    @staticmethod\n+    def apply(module, name, dim):\n+        fn = WeightNorm(name, dim)\n+\n+        weight = getattr(module, name)\n+\n+        # remove w from parameter list\n+        del module._parameters[name]\n+\n+        # add g and v as new parameters and express w as g/||v|| * v\n+        module.register_parameter(name + '_g', Parameter(fn.norm(weight).data))\n+        module.register_parameter(name + '_v', Parameter(weight.data))\n+        setattr(module, name, fn.compute_weight(module))\n+\n+        handle = hooks.RemovableHandle(module._forward_pre_hooks)\n+        module._forward_pre_hooks[handle.id] = fn\n+        fn.handle = handle\n+\n+        return fn\n+\n+    def remove(self, module):\n+        weight = self.compute_weight(module)\n+\n+        self.handle.remove()\n+        delattr(module, self.name)\n+        del module._parameters[self.name + '_g']\n+        del module._parameters[self.name + '_v']\n+        module.register_parameter(self.name, Parameter(weight.data))\n+\n+    def __call__(self, module, inputs):\n+        setattr(module, self.name, self.compute_weight(module))\n+\n+\n+def weight_norm(module, name='weight', dim=0):\n+    \"\"\"Applies weight normalization to a parameter in the given module.\n+\n+    Weight normalization is a reparameterization that decouples the magnitude\n+    of a weight tensor from its direction. This replaces the parameter specified\n+    by `name` (e.g. \"weight\") with two parameters: one specifying the magnitude\n+    (e.g. \"weight_g\") and one specifying the direction (e.g. \"weight_v\").\n+    The module's forward function is wrapped to first recompute the weight\n+    tensor from the magnitude and direction.\n+\n+    The magnitude parameter (\"weight_g\") contains weight.size(dim) elements.\n+    By default, weight is normalized per activation in an `nn.Linear` layer and\n+    per output-plane in a convolutional layer. If `dim` is `None`, the entire\n+    weight tensor is normalized.\n+\n+    See https://arxiv.org/abs/1602.07868\n+\n+    Args:\n+        module (nn.Module): containing module\n+        name (str, optional): name of weight parameter\n+        dim (int, optional): output dimension of weight parameter\n+\n+    Example::\n+\n+        >>> m = weight_norm(nn.Linear(20, 40))", "path": "torch/nn/utils/weight_norm.py", "position": null, "original_position": 87, "commit_id": "54f77f8150e0d1d57172278a88eaadf6b06be46c", "original_commit_id": "3f38d6bfa11222aa32304eab1866c4086d26b330", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Why would you weight norm bias...?", "created_at": "2017-06-30T18:33:45Z", "updated_at": "2018-11-23T15:34:01Z", "html_url": "https://github.com/pytorch/pytorch/pull/1945#discussion_r125103622", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1945", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/125103622"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1945#discussion_r125103622"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1945"}}, "body_html": "<p>Why would you weight norm bias...?</p>", "body_text": "Why would you weight norm bias...?", "in_reply_to_id": 125102388}