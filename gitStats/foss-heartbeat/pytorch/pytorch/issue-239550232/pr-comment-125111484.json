{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/125111484", "pull_request_review_id": 47474415, "id": 125111484, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyNTExMTQ4NA==", "diff_hunk": "@@ -0,0 +1,114 @@\n+\"\"\"\n+Weight Normalization from https://arxiv.org/abs/1602.07868\n+\"\"\"\n+import torch.utils.hooks as hooks\n+from torch.nn.parameter import Parameter\n+\n+\n+class WeightNorm(object):\n+    def __init__(self, name, dim):\n+        self.name = name\n+        self.dim = dim\n+\n+    def compute_weight(self, module):\n+        g = getattr(module, self.name + '_g')\n+        v = getattr(module, self.name + '_v')\n+        return v * (g / self.norm(v))\n+\n+    def norm(self, p):\n+        \"\"\"Computes the norm over all dimensions except dim\"\"\"\n+        if self.dim is None:\n+            return p.norm()\n+        if self.dim != 0:\n+            p = p.transpose(0, self.dim)\n+        output_size = (p.size(0),) + (1,) * (p.dim() - 1)\n+        p = p.contiguous().view(p.size(0), -1).norm(dim=1).view(*output_size)\n+        if self.dim != 0:\n+            p = p.transpose(0, self.dim)\n+        return p\n+\n+    @staticmethod\n+    def apply(module, name, dim):\n+        fn = WeightNorm(name, dim)\n+\n+        weight = getattr(module, name)\n+\n+        # remove w from parameter list\n+        del module._parameters[name]\n+\n+        # add g and v as new parameters and express w as g/||v|| * v\n+        module.register_parameter(name + '_g', Parameter(fn.norm(weight).data))\n+        module.register_parameter(name + '_v', Parameter(weight.data))\n+        setattr(module, name, fn.compute_weight(module))\n+\n+        handle = hooks.RemovableHandle(module._forward_pre_hooks)\n+        module._forward_pre_hooks[handle.id] = fn\n+        fn.handle = handle\n+\n+        return fn\n+\n+    def remove(self, module):\n+        weight = self.compute_weight(module)\n+\n+        self.handle.remove()\n+        delattr(module, self.name)\n+        del module._parameters[self.name + '_g']\n+        del module._parameters[self.name + '_v']\n+        module.register_parameter(self.name, Parameter(weight.data))\n+\n+    def __call__(self, module, inputs):\n+        setattr(module, self.name, self.compute_weight(module))\n+\n+\n+def weight_norm(module, name='weight', dim=0):\n+    \"\"\"Applies weight normalization to a parameter in the given module.\n+\n+    Weight normalization is a reparameterization that decouples the magnitude\n+    of a weight tensor from its direction. This replaces the parameter specified\n+    by `name` (e.g. \"weight\") with two parameters: one specifying the magnitude\n+    (e.g. \"weight_g\") and one specifying the direction (e.g. \"weight_v\").\n+    The module's forward function is wrapped to first recompute the weight\n+    tensor from the magnitude and direction.\n+\n+    The magnitude parameter (\"weight_g\") contains weight.size(dim) elements.\n+    By default, weight is normalized per activation in an `nn.Linear` layer and\n+    per output-plane in a convolutional layer. If `dim` is `None`, the entire\n+    weight tensor is normalized.\n+\n+    See https://arxiv.org/abs/1602.07868\n+\n+    Args:\n+        module (nn.Module): containing module\n+        name (str, optional): name of weight parameter\n+        dim (int, optional): output dimension of weight parameter\n+\n+    Example::\n+\n+        >>> m = weight_norm(nn.Linear(20, 40))", "path": "torch/nn/utils/weight_norm.py", "position": null, "original_position": 87, "commit_id": "54f77f8150e0d1d57172278a88eaadf6b06be46c", "original_commit_id": "3f38d6bfa11222aa32304eab1866c4086d26b330", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "I documented the return type. I'm not a huge fan of adding examples of weight normalizing something like bias (unless someone has showed that it actually makes sense). (Bilinear takes in two inputs, but only has one weight tensor)\r\n\r\nIf you have two parameters, you need two calls to `weight_norm`. Either:\r\n\r\n```\r\nm = weight_norm(weight_norm(m, 'weight1'), 'weight2')\r\n```\r\n\r\nor\r\n```\r\nm = ...\r\nweight_norm(m, 'weight1')\r\nweight_norm(m, 'weight2')\r\n```\r\n\r\nLike most of our other mutating methods (e.g. `.cuda()`) on modules, `weight_norm` returns the passed in module as a convenience.", "created_at": "2017-06-30T19:14:57Z", "updated_at": "2018-11-23T15:34:01Z", "html_url": "https://github.com/pytorch/pytorch/pull/1945#discussion_r125111484", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1945", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/125111484"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1945#discussion_r125111484"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1945"}}, "body_html": "<p>I documented the return type. I'm not a huge fan of adding examples of weight normalizing something like bias (unless someone has showed that it actually makes sense). (Bilinear takes in two inputs, but only has one weight tensor)</p>\n<p>If you have two parameters, you need two calls to <code>weight_norm</code>. Either:</p>\n<pre><code>m = weight_norm(weight_norm(m, 'weight1'), 'weight2')\n</code></pre>\n<p>or</p>\n<pre><code>m = ...\nweight_norm(m, 'weight1')\nweight_norm(m, 'weight2')\n</code></pre>\n<p>Like most of our other mutating methods (e.g. <code>.cuda()</code>) on modules, <code>weight_norm</code> returns the passed in module as a convenience.</p>", "body_text": "I documented the return type. I'm not a huge fan of adding examples of weight normalizing something like bias (unless someone has showed that it actually makes sense). (Bilinear takes in two inputs, but only has one weight tensor)\nIf you have two parameters, you need two calls to weight_norm. Either:\nm = weight_norm(weight_norm(m, 'weight1'), 'weight2')\n\nor\nm = ...\nweight_norm(m, 'weight1')\nweight_norm(m, 'weight2')\n\nLike most of our other mutating methods (e.g. .cuda()) on modules, weight_norm returns the passed in module as a convenience.", "in_reply_to_id": 125102388}