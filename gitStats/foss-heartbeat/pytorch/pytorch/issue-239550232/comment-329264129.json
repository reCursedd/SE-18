{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/329264129", "html_url": "https://github.com/pytorch/pytorch/pull/1945#issuecomment-329264129", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1945", "id": 329264129, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTI2NDEyOQ==", "user": {"login": "xwuaustin", "id": 31604955, "node_id": "MDQ6VXNlcjMxNjA0OTU1", "avatar_url": "https://avatars1.githubusercontent.com/u/31604955?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xwuaustin", "html_url": "https://github.com/xwuaustin", "followers_url": "https://api.github.com/users/xwuaustin/followers", "following_url": "https://api.github.com/users/xwuaustin/following{/other_user}", "gists_url": "https://api.github.com/users/xwuaustin/gists{/gist_id}", "starred_url": "https://api.github.com/users/xwuaustin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xwuaustin/subscriptions", "organizations_url": "https://api.github.com/users/xwuaustin/orgs", "repos_url": "https://api.github.com/users/xwuaustin/repos", "events_url": "https://api.github.com/users/xwuaustin/events{/privacy}", "received_events_url": "https://api.github.com/users/xwuaustin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-13T18:57:51Z", "updated_at": "2017-09-26T02:18:45Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3375899\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/rtqichen\">@rtqichen</a>  Thanks for contribution for this code.<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=24615343\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hanzhanggit\">@hanzhanggit</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> <a class=\"user-mention\" data-hovercard-type=\"organization\" data-hovercard-url=\"/orgs/ALL/hovercard\" href=\"https://github.com/ALL\">@ALL</a><br>\nHi everyone who read this post. I have some questions regarding to weight_norm. It would be great if you can help.<br>\nI tried to implement the weight_norm for each convolution and linear layer (check the code here <a href=\"https://github.com/xwuaustin/weight_norm/blob/master/cifar10_tutorial_weightNorm.py\">https://github.com/xwuaustin/weight_norm/blob/master/cifar10_tutorial_weightNorm.py</a> ).  However, the training loss in CIFAR-10 seems no difference to the original setting (see the picture below) at the first 2 epochs.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/31604955/30393122-1044c77a-988d-11e7-9ec5-05cb6bb3417f.png\"><img src=\"https://user-images.githubusercontent.com/31604955/30393122-1044c77a-988d-11e7-9ec5-05cb6bb3417f.png\" width=\"500\" style=\"max-width:100%;\"></a></p>\n<p>Now questions:</p>\n<h3>1. Is there something wrong with the code I modified? I used the code from cifar10_tutorial in pyTorch. All I did is to add the weight norm at each layer.</h3>\n<p>import torch.nn.utils.weight_norm as weightNorm<br>\nclass Net(nn.Module):<br>\ndef <strong>init</strong>(self):<br>\nsuper(Net, self).<strong>init</strong>()<br>\n### we use weight normalization after each convolutions and linear transfrom<br>\n<strong>self.conv1 = weightNorm(nn.Conv2d(3, 6, 5),name = \"weight\")</strong><br>\n#print (self.conv1._parameters.keys())<br>\nself.pool = nn.MaxPool2d(2, 2)<br>\n<strong>self.conv2 =weightNorm(nn.Conv2d(6, 16, 5),name = \"weight\")</strong><br>\n<strong>self.fc1 = weightNorm(nn.Linear(16 * 5 * 5, 120),name = \"weight\")</strong><br>\n<strong>self.fc2 = weightNorm(nn.Linear(120, 84),name = \"weight\")</strong><br>\n<strong>self.fc3 = weightNorm(nn.Linear(84, 10),name = \"weight\")</strong></p>\n<h3>2 Is the update of the weights and bias, namely 'weight_g', 'weight_v',  using the formulation:</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/31604955/30393546-7eea33a8-988e-11e7-8aae-5effb41e5df5.jpg\"><img src=\"https://user-images.githubusercontent.com/31604955/30393546-7eea33a8-988e-11e7-8aae-5effb41e5df5.jpg\" width=\"500\" style=\"max-width:100%;\"></a></p>\n<h3>3. Can we do the initialization as the paper suggested?</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/31604955/30393581-a4889fa0-988e-11e7-99d0-ea30ba976a0f.jpg\"><img src=\"https://user-images.githubusercontent.com/31604955/30393581-a4889fa0-988e-11e7-99d0-ea30ba976a0f.jpg\" width=\"500\" style=\"max-width:100%;\"></a></p>\n<p>Thanks. Looking for your response. :)</p>\n<p>Answer: I think I can answer this myself.<br>\nThe first question: the way to use weight_norm should be fine. I use minibatch 128, then WN works better.<br>\nThe second question: the update does follow what the paper suggested<br>\nThe third question: we could do the initialization by defining a function say weight_init and then do net.apply(weight_init). There are references for weight initialization.</p>", "body_text": "@rtqichen  Thanks for contribution for this code.\n@hanzhanggit @soumith @colesbury @ALL\nHi everyone who read this post. I have some questions regarding to weight_norm. It would be great if you can help.\nI tried to implement the weight_norm for each convolution and linear layer (check the code here https://github.com/xwuaustin/weight_norm/blob/master/cifar10_tutorial_weightNorm.py ).  However, the training loss in CIFAR-10 seems no difference to the original setting (see the picture below) at the first 2 epochs.\n\nNow questions:\n1. Is there something wrong with the code I modified? I used the code from cifar10_tutorial in pyTorch. All I did is to add the weight norm at each layer.\nimport torch.nn.utils.weight_norm as weightNorm\nclass Net(nn.Module):\ndef init(self):\nsuper(Net, self).init()\n### we use weight normalization after each convolutions and linear transfrom\nself.conv1 = weightNorm(nn.Conv2d(3, 6, 5),name = \"weight\")\n#print (self.conv1._parameters.keys())\nself.pool = nn.MaxPool2d(2, 2)\nself.conv2 =weightNorm(nn.Conv2d(6, 16, 5),name = \"weight\")\nself.fc1 = weightNorm(nn.Linear(16 * 5 * 5, 120),name = \"weight\")\nself.fc2 = weightNorm(nn.Linear(120, 84),name = \"weight\")\nself.fc3 = weightNorm(nn.Linear(84, 10),name = \"weight\")\n2 Is the update of the weights and bias, namely 'weight_g', 'weight_v',  using the formulation:\n\n3. Can we do the initialization as the paper suggested?\n\nThanks. Looking for your response. :)\nAnswer: I think I can answer this myself.\nThe first question: the way to use weight_norm should be fine. I use minibatch 128, then WN works better.\nThe second question: the update does follow what the paper suggested\nThe third question: we could do the initialization by defining a function say weight_init and then do net.apply(weight_init). There are references for weight initialization.", "body": "@rtqichen  Thanks for contribution for this code. \r\n@hanzhanggit @soumith @colesbury @all  \r\nHi everyone who read this post. I have some questions regarding to weight_norm. It would be great if you can help. \r\nI tried to implement the weight_norm for each convolution and linear layer (check the code here https://github.com/xwuaustin/weight_norm/blob/master/cifar10_tutorial_weightNorm.py ).  However, the training loss in CIFAR-10 seems no difference to the original setting (see the picture below) at the first 2 epochs.  \r\n<img src=\"https://user-images.githubusercontent.com/31604955/30393122-1044c77a-988d-11e7-9ec5-05cb6bb3417f.png\"  width=500>\r\n\r\nNow questions:\r\n### 1. Is there something wrong with the code I modified? I used the code from cifar10_tutorial in pyTorch. All I did is to add the weight norm at each layer.\r\n\r\nimport torch.nn.utils.weight_norm as weightNorm\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        ### we use weight normalization after each convolutions and linear transfrom\r\n        **self.conv1 = weightNorm(nn.Conv2d(3, 6, 5),name = \"weight\")**\r\n        #print (self.conv1._parameters.keys())\r\n        self.pool = nn.MaxPool2d(2, 2)\r\n        **self.conv2 =weightNorm(nn.Conv2d(6, 16, 5),name = \"weight\")**\r\n        **self.fc1 = weightNorm(nn.Linear(16 * 5 * 5, 120),name = \"weight\")**\r\n        **self.fc2 = weightNorm(nn.Linear(120, 84),name = \"weight\")**\r\n        **self.fc3 = weightNorm(nn.Linear(84, 10),name = \"weight\")**\r\n\r\n\r\n### 2 Is the update of the weights and bias, namely 'weight_g', 'weight_v',  using the formulation:\r\n<img src=\"https://user-images.githubusercontent.com/31604955/30393546-7eea33a8-988e-11e7-8aae-5effb41e5df5.jpg\" width=500>\r\n\r\n### 3. Can we do the initialization as the paper suggested? \r\n<img src=\"https://user-images.githubusercontent.com/31604955/30393581-a4889fa0-988e-11e7-99d0-ea30ba976a0f.jpg\" width=500>\r\n\r\nThanks. Looking for your response. :) \r\n\r\n\r\nAnswer: I think I can answer this myself. \r\nThe first question: the way to use weight_norm should be fine. I use minibatch 128, then WN works better. \r\nThe second question: the update does follow what the paper suggested\r\nThe third question: we could do the initialization by defining a function say weight_init and then do net.apply(weight_init). There are references for weight initialization. \r\n\r\n"}