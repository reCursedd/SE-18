{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159984022", "pull_request_review_id": 87028352, "id": 159984022, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1OTk4NDAyMg==", "diff_hunk": "@@ -125,160 +116,100 @@\n RECORD_ATTRIBUTE = CodeTemplate(\"\"\"\\\n setattr(n, jit::stringToSymbol(\"${name}\"), ${name});\"\"\")\n \n-CONDITIONAL = CodeTemplate(\"\"\"\\\n-if (${cond}) {\n-  ${statements}\n-}\n-\"\"\")\n-\n-FUNCTION_PROTOTYPE = CodeTemplate(\"\"\"\\\n-${name}(${typed_args})\"\"\")\n-\n-BUFFER_DECLARATION = CodeTemplate(\"\"\"\\\n-auto ${name} = tensor();\n-auto& ${name}_ = static_cast<VariableImpl*>(${name}.get())->data;\"\"\")\n-\n-template_path = os.path.join(os.path.dirname(__file__), 'templates')\n \n-VARIABLE_TYPE_H = CodeTemplate.from_file(template_path + '/VariableType.h')\n-VARIABLE_TYPE_CPP = CodeTemplate.from_file(template_path + '/VariableType.cpp')\n-PY_VARIABLE_METHODS_CPP = CodeTemplate.from_file(template_path + '/python_variable_methods.cpp')\n-PY_VARIABLE_DISPATCH_H = CodeTemplate.from_file(template_path + '/python_variable_methods_dispatch.h')\n-PY_NN_FUNCTIONS_CPP = CodeTemplate.from_file(template_path + '/python_nn_functions.cpp')\n-PY_NN_FUNCTIONS_H = CodeTemplate.from_file(template_path + '/python_nn_functions.h')\n-PY_NN_DISPATCH_H = CodeTemplate.from_file(template_path + '/python_nn_functions_dispatch.h')\n-\n-derivatives_path = os.path.join(os.path.dirname(__file__), 'derivatives.yaml')\n-deprecated_path = os.path.join(os.path.dirname(__file__), 'deprecated.yaml')\n-\n-# Functions with these return types delegate completely to the underlying\n-# base at::Type\n-FALLTHROUGH_RETURN_TYPES = {'int64_t', 'void*', 'bool', 'IntList'}\n-FALLTHROUGH_FUNCTIONS = {\n-    'arange', 'eye', 'linspace', 'logspace', 'tensor', 'ones', 'ones_like',\n-    'rand', 'randn', 'randperm', 'range', 'tensor', 'zeros',\n-    'zeros_like', 'set_', '_indices', '_values',\n-    # these are only implemented on integral types\n-    '__and__', '__iand__', '__ilshift__', '__ior__', '__irshift__', '__ixor__',\n-    '__lshift__', '__or__', '__rshift__', '__xor__',\n-}\n-VIEW_FUNCTIONS = {\n-    'alias', 'as_strided', 'expand', 'narrow', 'permute', 'select', 'slice',\n-    'squeeze', 't', 'transpose', 'unfold', 'unsqueeze', 'view',\n-}\n-MANUAL_IMPLEMENTATIONS = {\n-    'contiguous', 'resize_', 'resize_as_'\n-}\n-# These functions require manual Python bindings or are not exposed to Python\n-SKIP_PYTHON_BINDINGS = [\n-    'alias', 'contiguous', 'clamp.*', 'is_cuda', 'is_sparse', 'size', 'stride', 'slice_dim',\n-    '.*_backward'\n-]\n-# These functions we don't want to record for tracing, because we always want\n-# to trace their constituent parts.  This is a temporary hack in lieue\n-# of proper scopes, where subsequent compilation passes can ask for the unfolding\n-# on demand.  Only concrete ATen methods can be disabled this way; it will have\n-# NO EFFECT otherwise.\n-DONT_RECORD_TRACE = {'convolution', 'conv1d', 'conv2d', 'conv3d',\n-                     'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d'}\n+def gen_variable_type(out, aten_declarations):\n+    \"\"\"VariableType.h and VariableType.cpp body\n \n-# Matches \"foo\" in \"foo, bar\" but not \"foobar\". Used to search for the\n-# occurence of a parameter in the derivative formula\n-IDENT_REGEX = r'(^|\\W){}($|\\W)'\n+    This is the at::Type subclass for differentiable tensors. The\n+    implementation of each function dispatches to the base tensor type to\n+    compute the output. The grad_fn is attached to differentiable functions.\n+    \"\"\"\n \n+    type_declarations = []\n+    type_definitions = []\n \n-def format_return_type(returns):\n-    if len(returns) == 0:\n-        return 'void'\n-    elif len(returns) == 1:\n-        return returns[0]['type']\n-    else:\n-        return_types = [r['type'] for r in returns]\n-        return 'std::tuple<{}>'.format(','.join(return_types))\n+    for declaration in aten_declarations:\n+        if declaration['name'].endswith('_out'):\n+            # TODO: implement _out functions\n+            continue\n+        type_declarations.append(METHOD_DECLARATION.substitute(declaration))\n+        if declaration['name'] not in MANUAL_IMPLEMENTATIONS:\n+            type_definitions.append(emit_method_definition(declaration))\n \n+    env = {\n+        'type_derived_method_declarations': type_declarations,\n+        'type_derived_method_definitions': type_definitions,\n+    }\n+    write(out, 'VariableType.h', VARIABLE_TYPE_H, env)\n+    write(out, 'VariableType.cpp', VARIABLE_TYPE_CPP, env)\n \n-# TODO: Use a real parser here; this will get bamboozled\n-# by signatures that contain things like std::array<bool, 2> (note the space)\n-def split_name_params(prototype):\n-    name, params = re.match('(\\w+)\\((.*)\\)', prototype).groups()\n-    return name, params.split(', ')\n \n+def emit_method_definition(declaration):\n+    body = emit_body(declaration)\n+    return METHOD_DEFINITION.substitute(declaration, type_definition_body=body)\n \n-def uses_grad(func):\n-    if func is None:\n-        return False\n-    for derivative in func['derivatives']:\n-        formula = derivative['formula']\n-        if re.search(IDENT_REGEX.format('grad'), formula):\n-            return True\n-    return False\n \n+def emit_body(declaration):\n+    strategy = dispatch_strategy(declaration)\n \n-def dispatch_strategy(declaration):\n-    \"\"\"How are we going to call the underlying implementation of a\n-    declaration?  There are three strategies:\n+    arguments = declaration['arguments']\n+    returns = declaration['returns']\n+    func = declaration['derivative']\n+    name = declaration['name']\n+    inplace = declaration['inplace']\n \n-        - use_derived: we want to call the implementation on CPUDoubleType\n-          (or a similar, derived Type instance).  Because these derived\n-          instances deal in Tensors, not Variables (it's a completely different\n-          object, so it doesn't dispatch back to VariableType), code on\n-          this dispatch path needs to wrap/unwrap tensors.  If the\n-          derived implementation takes and returns tensors, the\n-          implementation is usually differentiable (although we also use\n-          the derived dispatch path for non-differentiable functions\n-          that we still want to dispatch on the derived Type instance;\n-          e.g., size())\n+    base_name = name[:-1] if inplace else name\n+    is_view = base_name in VIEW_FUNCTIONS\n \n-        - use_type: we want to call the implementation on Type, because\n-          it is implemented concretely, and the functions it invokes will\n-          get dispatched back to VariableType (which will ensure that they\n-          are differentiable.)\n+    # These exclude things like BoolTensor, int64_t, and Scalar\n+    def is_differentiable(arg):\n+        if 'Tensor' not in arg['type']:\n+            return False\n+        if arg['dynamic_type'] in {'IndexTensor', 'BoolTensor'}:\n+            return False\n+        return True\n \n-        - unimplemented: we don't have an underlying implementation, so\n-          we will immediately error if they call this method on VariableType.\n-    \"\"\"\n-    def use_derived(option):\n-        return (option['return_type'] in FALLTHROUGH_RETURN_TYPES or\n-                option['name'] in FALLTHROUGH_FUNCTIONS or\n-                option['name'] in MANUAL_IMPLEMENTATIONS or\n-                # TODO: Now that NN is represented explicitly in\n-                # derivatives.yaml, get rid of this test soon.  We can't get\n-                # rid of it /quite/ yet because we need to add NYI entries\n-                # to derivatives.yaml, but actually they'll get real entries\n-                # in https://github.com/pytorch/pytorch/pull/4116 so I am\n-                # too lazy to delete it now.\n-                option['name'].endswith('_backward') or\n-                option.get('derivative') is not None)\n-\n-    if use_derived(declaration):\n-        return 'use_derived'\n-    elif not declaration['abstract']:\n-        # This applies a heuristic: if the function is concrete (we\n-        # don't have to override it) and we didn't declare it in\n-        # derivatives.yaml, we'll assume that it is actually implemented\n-        # out of differentiable functions.  (This assumption might not\n-        # hold, but then you'll see gradcheck fail.)\n-        return 'use_type'\n-    else:\n-        return 'unimplemented'\n+    differentiable_inputs = list(filter(is_differentiable, arguments))\n+    differentiable_outputs = list(filter(is_differentiable, returns))\n \n+    if uses_grad(func):\n+        # If we only use `grad` and not `grads[i]` in the derivative than\n+        # assume that only the first output is differentiable. TODO: remove\n+        # this heuristic.\n+        differentiable_outputs = differentiable_outputs[:1]\n \n-def create_variable_type(top_env, aten_declarations):\n-    \"\"\"VariableType.h and VariableType.cpp body\n+    requires_derivative = (\n+        name not in DONT_REQUIRE_DERIVATIVE and\n+        len(differentiable_inputs) > 0 and len(differentiable_outputs) > 0 and\n+        strategy == 'use_derived')", "path": "tools/autograd/gen_variable_type.py", "position": 372, "original_position": 369, "commit_id": "e135e7863dad9e6c6fc43085459880f5ad3d0126", "original_commit_id": "202fd3eb9a2f59e7d1460f03beb5e8026164210d", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "differently named tuples can be equal:\r\n\r\n```python\r\nfrom collections import namedtuple\r\nVALUE1 = namedtuple('Value1', [])()\r\nVALUE2 = namedtuple('Value2', [])()\r\nprint(VALUE1 == VALUE2)  # True (!!!)\r\n```\r\n  ", "created_at": "2018-01-05T21:28:34Z", "updated_at": "2018-11-23T15:37:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/4487#discussion_r159984022", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4487", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159984022"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4487#discussion_r159984022"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4487"}}, "body_html": "<p>differently named tuples can be equal:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> collections <span class=\"pl-k\">import</span> namedtuple\n<span class=\"pl-c1\">VALUE1</span> <span class=\"pl-k\">=</span> namedtuple(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Value1<span class=\"pl-pds\">'</span></span>, [])()\n<span class=\"pl-c1\">VALUE2</span> <span class=\"pl-k\">=</span> namedtuple(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Value2<span class=\"pl-pds\">'</span></span>, [])()\n<span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">VALUE1</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">VALUE2</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> True (!!!)</span></pre></div>", "body_text": "differently named tuples can be equal:\nfrom collections import namedtuple\nVALUE1 = namedtuple('Value1', [])()\nVALUE2 = namedtuple('Value2', [])()\nprint(VALUE1 == VALUE2)  # True (!!!)", "in_reply_to_id": 159940584}