{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159902276", "pull_request_review_id": 86930846, "id": 159902276, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1OTkwMjI3Ng==", "diff_hunk": "@@ -15,106 +16,96 @@\n #       - The function's output is not differentiable\n #       - The function has no data dependency on its input\n #\n-#     These are currently called \"fallthrough\" functions, although\n-#     they are not entirely fallthrough; for example, if the function\n-#     in question returns a tensor, we have to wrap it in a\n-#     (does not require grad) variable to abide by the API contract\n-#     of Variable.\n-#\n #   - Some function don't need a backwards implementation because they\n-#     are implement as a composition of other (differentiable) ATen\n+#     are implemented as a composition of other (differentiable) ATen\n #     functions.  These are dispatched directly to the Type superclass,\n #     which will in turn dispatch back to VariableType for its\n #     differentiable subcomponents.\n+#\n+from __future__ import print_function\n+import sys\n+from .utils import CodeTemplate, nested_dict\n+from .gen_autograd import VIEW_FUNCTIONS, template_path, write\n+from .gen_autograd_functions import uses_grad\n+\n+VARIABLE_TYPE_H = CodeTemplate.from_file(template_path + '/VariableType.h')\n+VARIABLE_TYPE_CPP = CodeTemplate.from_file(template_path + '/VariableType.cpp')\n+\n+# These functions are written manually in templates/VariableType.cpp\n+MANUAL_IMPLEMENTATIONS = {\n+    'contiguous', 'resize_', 'resize_as_'\n+}\n+\n+# These functions we don't want to record for tracing, because we always want\n+# to trace their constituent parts.  This is a temporary hack in lieue\n+# of proper scopes, where subsequent compilation passes can ask for the unfolding\n+# on demand.  Only concrete ATen methods can be disabled this way; it will have\n+# NO EFFECT otherwise.\n+DONT_RECORD_TRACE = {\n+    'convolution', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d',\n+    'conv_transpose2d', 'conv_transpose3d',\n+}\n \n-import argparse\n-import copy\n-import os\n-import re\n-import yaml\n-from collections import defaultdict\n-from .utils import CodeTemplate, write, nested_dict, YamlLoader\n+# These functions are not worth profiling because they are very cheap and may\n+# be called very often.\n+DONT_PROFILE = {\n+    'data_ptr', 'get_device', 'is_contiguous', 'is_cuda', 'is_distributed',\n+    'is_same_size', 'is_set_to', 'is_signed', 'is_sparse', 'numel',\n+    'size', 'storage_offset', 'stride',\n+}\n \n+DONT_REQUIRE_DERIVATIVE = {", "path": "tools/autograd/gen_variable_type.py", "position": null, "original_position": 69, "commit_id": "e135e7863dad9e6c6fc43085459880f5ad3d0126", "original_commit_id": "202fd3eb9a2f59e7d1460f03beb5e8026164210d", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "can you add a comment saying what happens to those functions (do we still generate them? is their backward just an error? are they guaranteed to return things that don't require grad?)?", "created_at": "2018-01-05T15:28:04Z", "updated_at": "2018-11-23T15:37:50Z", "html_url": "https://github.com/pytorch/pytorch/pull/4487#discussion_r159902276", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4487", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159902276"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4487#discussion_r159902276"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4487"}}, "body_html": "<p>can you add a comment saying what happens to those functions (do we still generate them? is their backward just an error? are they guaranteed to return things that don't require grad?)?</p>", "body_text": "can you add a comment saying what happens to those functions (do we still generate them? is their backward just an error? are they guaranteed to return things that don't require grad?)?"}