{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159941964", "pull_request_review_id": 86978670, "id": 159941964, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1OTk0MTk2NA==", "diff_hunk": "@@ -2236,36 +2236,6 @@ class dont_convert(tuple):\n     ('det', lambda: random_fullrank_matrix_distinct_singular_value(S), (), 'distinct_postive_s', (), [skipIfNoLapack]),\n     ('svd', lambda: random_fullrank_matrix_distinct_singular_value(S), (), '', (), [skipIfNoLapack]),\n     ('gesv', (S, S), ((S, S),), '', (), [skipIfNoLapack]),\n-    ('eq', (S, S, S), ((S, S, S),)),", "path": "test/test_autograd.py", "position": 4, "original_position": 4, "commit_id": "e135e7863dad9e6c6fc43085459880f5ad3d0126", "original_commit_id": "202fd3eb9a2f59e7d1460f03beb5e8026164210d", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "We should fix this, but I'm not sure how yet. I don't think we should ignore outputs that don't require_grad. I think we should be checking that the gradient is actually zero. Otherwise we miss things that should have gradient definitions, but are erroneously marked as not requiring grad.", "created_at": "2018-01-05T18:01:42Z", "updated_at": "2018-11-23T15:37:51Z", "html_url": "https://github.com/pytorch/pytorch/pull/4487#discussion_r159941964", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4487", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/159941964"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4487#discussion_r159941964"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4487"}}, "body_html": "<p>We should fix this, but I'm not sure how yet. I don't think we should ignore outputs that don't require_grad. I think we should be checking that the gradient is actually zero. Otherwise we miss things that should have gradient definitions, but are erroneously marked as not requiring grad.</p>", "body_text": "We should fix this, but I'm not sure how yet. I don't think we should ignore outputs that don't require_grad. I think we should be checking that the gradient is actually zero. Otherwise we miss things that should have gradient definitions, but are erroneously marked as not requiring grad.", "in_reply_to_id": 159885604}