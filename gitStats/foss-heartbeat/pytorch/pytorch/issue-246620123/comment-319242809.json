{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/319242809", "html_url": "https://github.com/pytorch/pytorch/issues/2258#issuecomment-319242809", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/2258", "id": 319242809, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTI0MjgwOQ==", "user": {"login": "alexsax", "id": 5157485, "node_id": "MDQ6VXNlcjUxNTc0ODU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5157485?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexsax", "html_url": "https://github.com/alexsax", "followers_url": "https://api.github.com/users/alexsax/followers", "following_url": "https://api.github.com/users/alexsax/following{/other_user}", "gists_url": "https://api.github.com/users/alexsax/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexsax/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexsax/subscriptions", "organizations_url": "https://api.github.com/users/alexsax/orgs", "repos_url": "https://api.github.com/users/alexsax/repos", "events_url": "https://api.github.com/users/alexsax/events{/privacy}", "received_events_url": "https://api.github.com/users/alexsax/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-01T01:37:04Z", "updated_at": "2017-08-01T01:37:04Z", "author_association": "NONE", "body_html": "<p>I've actually used that one! It's a great library but does something different than what I'm proposing.</p>\n<p>I'm proposing a wrapper which encapsulates the typical training procedure so that I don't have to rewrite the 'glue' code over and over again. Similar to the <code>train</code> function in <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim\">tf-slim</a>. PyTorch already has a skeletal implementation of a training loop in <code>torch.utils.trainer</code>. It's not sufficiently fully-featured or flexible for anything except the most basic use cases, but I'd like to extend it. I'm proposing to extend this into something very generic which still able to provide some of the more common requirements (e.g. checkpointing, saving the best model) for off-the-shelf use.<br>\nSo if you wanted to run something totally weird, every 3 epochs, you should be able to do it with:</p>\n<pre><code>plug = PluginFactory((3, 'epoch'), fn=myWeirdFunction)\ntrainer.register(plug)\n</code></pre>\n<p>And checkpointing with a complex save function could be a one-liner!</p>\n<pre><code>trainer.register(Saver((1, 'epoch'), filename='checkpoint.pth.tar', save_function=myCrazySaveFunction)\n</code></pre>\n<p>Then there's a separate part that concerns visual logging, which is the niche that Tensorboard fills. The visualization could be powered with TensorBoard via a <code>TensorboardLogger</code> class which would log statistics out to TensorBoard periodically. Tf-slim has a great tool, summaries, which do exactly that (in TF). However, TensoBoard has a limited range of plots and isn't readily extensible. That makes it hard to interact with it using python.</p>\n<p>Visdom, which is also from Facebook, allows more customization and chart types. In particular, it allows images and videos, which means that users could view CNN filters or sample episodes from an A3C model.</p>\n<p>Sorry that I didn't communicate that clearly before! In short, since PyTorch already has the trainer, it might as well be fully featured! As for Visdom v. Tensorboard, someone else might want to incorporate the TensorBoard plugins at a future time, but I think that Visdom fits in better with the \"Extensions without Pain\" and \"Python First\" parts of PyTorch.</p>", "body_text": "I've actually used that one! It's a great library but does something different than what I'm proposing.\nI'm proposing a wrapper which encapsulates the typical training procedure so that I don't have to rewrite the 'glue' code over and over again. Similar to the train function in tf-slim. PyTorch already has a skeletal implementation of a training loop in torch.utils.trainer. It's not sufficiently fully-featured or flexible for anything except the most basic use cases, but I'd like to extend it. I'm proposing to extend this into something very generic which still able to provide some of the more common requirements (e.g. checkpointing, saving the best model) for off-the-shelf use.\nSo if you wanted to run something totally weird, every 3 epochs, you should be able to do it with:\nplug = PluginFactory((3, 'epoch'), fn=myWeirdFunction)\ntrainer.register(plug)\n\nAnd checkpointing with a complex save function could be a one-liner!\ntrainer.register(Saver((1, 'epoch'), filename='checkpoint.pth.tar', save_function=myCrazySaveFunction)\n\nThen there's a separate part that concerns visual logging, which is the niche that Tensorboard fills. The visualization could be powered with TensorBoard via a TensorboardLogger class which would log statistics out to TensorBoard periodically. Tf-slim has a great tool, summaries, which do exactly that (in TF). However, TensoBoard has a limited range of plots and isn't readily extensible. That makes it hard to interact with it using python.\nVisdom, which is also from Facebook, allows more customization and chart types. In particular, it allows images and videos, which means that users could view CNN filters or sample episodes from an A3C model.\nSorry that I didn't communicate that clearly before! In short, since PyTorch already has the trainer, it might as well be fully featured! As for Visdom v. Tensorboard, someone else might want to incorporate the TensorBoard plugins at a future time, but I think that Visdom fits in better with the \"Extensions without Pain\" and \"Python First\" parts of PyTorch.", "body": "I've actually used that one! It's a great library but does something different than what I'm proposing.  \r\n\r\nI'm proposing a wrapper which encapsulates the typical training procedure so that I don't have to rewrite the 'glue' code over and over again. Similar to the `train` function in [tf-slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim). PyTorch already has a skeletal implementation of a training loop in `torch.utils.trainer`. It's not sufficiently fully-featured or flexible for anything except the most basic use cases, but I'd like to extend it. I'm proposing to extend this into something very generic which still able to provide some of the more common requirements (e.g. checkpointing, saving the best model) for off-the-shelf use. \r\nSo if you wanted to run something totally weird, every 3 epochs, you should be able to do it with:\r\n```\r\nplug = PluginFactory((3, 'epoch'), fn=myWeirdFunction)\r\ntrainer.register(plug)\r\n```\r\n\r\nAnd checkpointing with a complex save function could be a one-liner!\r\n```\r\ntrainer.register(Saver((1, 'epoch'), filename='checkpoint.pth.tar', save_function=myCrazySaveFunction)\r\n```\r\n\r\n\r\nThen there's a separate part that concerns visual logging, which is the niche that Tensorboard fills. The visualization could be powered with TensorBoard via a `TensorboardLogger` class which would log statistics out to TensorBoard periodically. Tf-slim has a great tool, summaries, which do exactly that (in TF). However, TensoBoard has a limited range of plots and isn't readily extensible. That makes it hard to interact with it using python. \r\n\r\nVisdom, which is also from Facebook, allows more customization and chart types. In particular, it allows images and videos, which means that users could view CNN filters or sample episodes from an A3C model. \r\n\r\nSorry that I didn't communicate that clearly before! In short, since PyTorch already has the trainer, it might as well be fully featured! As for Visdom v. Tensorboard, someone else might want to incorporate the TensorBoard plugins at a future time, but I think that Visdom fits in better with the \"Extensions without Pain\" and \"Python First\" parts of PyTorch. \r\n\r\n\r\n\r\n"}