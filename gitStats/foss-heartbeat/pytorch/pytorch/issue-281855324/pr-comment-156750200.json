{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156750200", "pull_request_review_id": 83279891, "id": 156750200, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1Njc1MDIwMA==", "diff_hunk": "@@ -92,6 +92,53 @@ def __iter__(self):\n     def __len__(self):\n         return self.num_samples\n \n+class EnlargeLabelShufflingSampler(Sampler):\n+    \"\"\"\n+        label shuffling technique aimed to deal with imbalanced class problem\n+        without replacement, manipulated by indices.\n+        All classes are enlarged to the same amount, so classes can be trained equally.\n+        argument:\n+        indices: indices of labels of the whole dataset\n+        \"\"\"\n+    \n+    def __init__(self, indices):\n+        # mapping between label index and sorted label index\n+        sorted_labels = sorted(enumerate(indices), key=lambda x: x[1])\n+        uniq_labels = set(sorted_labels)\n+        count = 1\n+        count_of_each_label = []\n+        tmp = -1\n+        # get count of each label\n+        for (x, y) in sorted_labels:\n+            if y == tmp:\n+                count += 1\n+            else:\n+                if tmp != -1:\n+                    count_of_each_label.append(count)\n+                    count = 1\n+            tmp = y\n+        count_of_each_label.append(count)\n+        # get the largest count among all classes. used to enlarge every class to the same amount\n+        largest = int(np.amax(count_of_each_label))\n+        self.count_of_each_label = count_of_each_label\n+        self.enlarged_index = []\n+        \n+        # preidx used for find the mapping beginning of arg \"sorted_labels\"\n+        preidx = 0\n+        for x in range(len(self.count_of_each_label)):\n+            idxes = np.remainder(t.randperm(largest).numpy(), self.count_of_each_label[x]) + preidx\n+            for y in idxes:\n+                self.enlarged_index.append(sorted_labels[y][0])\n+            \n+            preidx += int(self.count_of_each_label[x])\n+\n+    def __iter__(self):\n+        shuffle(self.enlarged_index)\n+        return iter(self.enlarged_index)\n+    \n+    def __len__(self):\n+        return np.amax(self.count_of_each_label)*len(self.count_of_each_label)", "path": "torch/utils/data/sampler.py", "position": null, "original_position": 49, "commit_id": "08354a2e6327a92a0919747d15f0fcc5578c2ae6", "original_commit_id": "6d1f9e4f68a191a9c7b44c6a212b48677b1e0bac", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "body": "Numpy is not a hard dependency of torch, and is not even imported in this file so I believe this will be an error", "created_at": "2017-12-13T18:58:00Z", "updated_at": "2018-11-23T15:37:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/4153#discussion_r156750200", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4153", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/156750200"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4153#discussion_r156750200"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4153"}}, "body_html": "<p>Numpy is not a hard dependency of torch, and is not even imported in this file so I believe this will be an error</p>", "body_text": "Numpy is not a hard dependency of torch, and is not even imported in this file so I believe this will be an error"}