{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424575451", "html_url": "https://github.com/pytorch/pytorch/pull/11984#issuecomment-424575451", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11984", "id": 424575451, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDU3NTQ1MQ==", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T03:49:16Z", "updated_at": "2018-09-26T03:49:16Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>I'm a bit concerned about adding the numba installation to your test runner, rather than folding the numba package into your test image. I've pinned to installed version to attempt to mitigate the risk of a dependency on an external package, but this still adds ~400mb of external package access to the test run.</p>\n</blockquote>\n<p>Well, here's another crazy idea. We don't <em>necessarily</em> need to test against numba; we just need to test against something that supports cuda_array_interface. There's no reason we couldn't make PyTorch support cuda_array_interface. So what we could do is (1) add read-in support for cuda_array_interface to PyTorch, and then (2) test that going to a dummy object with cuda_array_interface and then back again works. Then as long as we don't have correlated errors between the decoder and encoder, you have a reasonably robust test.</p>", "body_text": "I'm a bit concerned about adding the numba installation to your test runner, rather than folding the numba package into your test image. I've pinned to installed version to attempt to mitigate the risk of a dependency on an external package, but this still adds ~400mb of external package access to the test run.\n\nWell, here's another crazy idea. We don't necessarily need to test against numba; we just need to test against something that supports cuda_array_interface. There's no reason we couldn't make PyTorch support cuda_array_interface. So what we could do is (1) add read-in support for cuda_array_interface to PyTorch, and then (2) test that going to a dummy object with cuda_array_interface and then back again works. Then as long as we don't have correlated errors between the decoder and encoder, you have a reasonably robust test.", "body": "> I'm a bit concerned about adding the numba installation to your test runner, rather than folding the numba package into your test image. I've pinned to installed version to attempt to mitigate the risk of a dependency on an external package, but this still adds ~400mb of external package access to the test run.\r\n\r\nWell, here's another crazy idea. We don't *necessarily* need to test against numba; we just need to test against something that supports cuda_array_interface. There's no reason we couldn't make PyTorch support cuda_array_interface. So what we could do is (1) add read-in support for cuda_array_interface to PyTorch, and then (2) test that going to a dummy object with cuda_array_interface and then back again works. Then as long as we don't have correlated errors between the decoder and encoder, you have a reasonably robust test."}