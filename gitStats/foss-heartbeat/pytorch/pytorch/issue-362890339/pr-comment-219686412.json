{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/219686412", "pull_request_review_id": 157926742, "id": 219686412, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxOTY4NjQxMg==", "diff_hunk": "@@ -441,4 +441,52 @@ def __array_wrap__(self, array):\n             array = array.astype('uint8')\n         return torch.from_numpy(array)\n \n+    @property\n+    def __cuda_array_interface__(self):\n+        \"\"\"Array view description for cuda tensors.\n+\n+        See:\n+        https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html\n+        \"\"\"\n+\n+        # raise AttributeError for unsupported tensors, so that\n+        # hasattr(cpu_tensor, \"__cuda_array_interface__\") is False.\n+        if not self.device.type == \"cuda\":\n+            raise AttributeError(\n+                \"Not on cuda device, use Tensor.cuda() first: %r\" %\n+                self.device\n+            )\n+\n+        if self.is_sparse:\n+            raise AttributeError(\n+                \"Can't convert sparse tensor, use Tensor.to_dense() \"\n+                \"to convert to a dense tensor first.\"\n+            )\n+\n+        # RuntimeError, matching tensor.__array__() behavior.\n+        if self.requires_grad:\n+            raise RuntimeError(\n+                \"Can't get __cuda_array_interface__ on Variable that requires grad. \"\n+                \"Use var.detach().__cuda_array_interface__ instead.\"", "path": "torch/tensor.py", "position": null, "original_position": 30, "commit_id": "2f79b64368e4d93d217385d8109c5b94753d6641", "original_commit_id": "e7c7a45c620cd3d8f76e3f3906e557d7737d0c12", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "If our user blindly follows this advice without knowing what detach() actually does, they might come calling us when their code didn't work, because this advice didn't solve the fundamental problem which is that you can't autograd past PyTorch. More robust advice would be to say, \"If gradients are truly not required, you can get a Variable which doesn't require grad by saying var.detach()\". And I wouldn't advise the user to use __cuda_array_interface__, because in all likelihood this call is happening in code out of their control.", "created_at": "2018-09-23T02:06:21Z", "updated_at": "2018-11-23T15:51:49Z", "html_url": "https://github.com/pytorch/pytorch/pull/11984#discussion_r219686412", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11984", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/219686412"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11984#discussion_r219686412"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11984"}}, "body_html": "<p>If our user blindly follows this advice without knowing what detach() actually does, they might come calling us when their code didn't work, because this advice didn't solve the fundamental problem which is that you can't autograd past PyTorch. More robust advice would be to say, \"If gradients are truly not required, you can get a Variable which doesn't require grad by saying var.detach()\". And I wouldn't advise the user to use <strong>cuda_array_interface</strong>, because in all likelihood this call is happening in code out of their control.</p>", "body_text": "If our user blindly follows this advice without knowing what detach() actually does, they might come calling us when their code didn't work, because this advice didn't solve the fundamental problem which is that you can't autograd past PyTorch. More robust advice would be to say, \"If gradients are truly not required, you can get a Variable which doesn't require grad by saying var.detach()\". And I wouldn't advise the user to use cuda_array_interface, because in all likelihood this call is happening in code out of their control."}