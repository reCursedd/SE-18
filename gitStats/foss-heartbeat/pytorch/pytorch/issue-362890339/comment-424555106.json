{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424555106", "html_url": "https://github.com/pytorch/pytorch/pull/11984#issuecomment-424555106", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11984", "id": 424555106, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDU1NTEwNg==", "user": {"login": "asford", "id": 282792, "node_id": "MDQ6VXNlcjI4Mjc5Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/282792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asford", "html_url": "https://github.com/asford", "followers_url": "https://api.github.com/users/asford/followers", "following_url": "https://api.github.com/users/asford/following{/other_user}", "gists_url": "https://api.github.com/users/asford/gists{/gist_id}", "starred_url": "https://api.github.com/users/asford/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asford/subscriptions", "organizations_url": "https://api.github.com/users/asford/orgs", "repos_url": "https://api.github.com/users/asford/repos", "events_url": "https://api.github.com/users/asford/events{/privacy}", "received_events_url": "https://api.github.com/users/asford/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T01:34:13Z", "updated_at": "2018-09-26T01:34:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a></p>\n<p>This is ready for another round of review when you get the chance. I've:</p>\n<ol>\n<li>\n<p>Expanded the test suite to cover baseline numba integration and added numba installation to xenial-cuda9 py2 &amp; py3 test configurations.</p>\n<p>This <em>could</em> be expanded into a full end-to-end test, covering dispatch of a jit-compiled numba function over input torch.Tensors and/or a full test of autograd integration via numba-jitted forward/backward passes. I'm not entirely sure about the project's philosopy of what should live in the test layer vs examples, and I'd be happy to hear your thoughts.</p>\n</li>\n<li>\n<p>Tidied up the <strong>cuda_array_interface</strong> <code>dir</code> behavior.</p>\n</li>\n<li>\n<p>Tidied up the <strong>cuda_array_interface</strong> error messages.</p>\n</li>\n</ol>\n<p>I'm a <em>bit</em> concerned about adding the numba installation to your test runner, rather than folding the numba package into your test image. I've pinned to installed version to attempt to mitigate the risk of a dependency on an external package, but this still adds ~400mb of external package access to the test run.</p>", "body_text": "@ezyang\nThis is ready for another round of review when you get the chance. I've:\n\n\nExpanded the test suite to cover baseline numba integration and added numba installation to xenial-cuda9 py2 & py3 test configurations.\nThis could be expanded into a full end-to-end test, covering dispatch of a jit-compiled numba function over input torch.Tensors and/or a full test of autograd integration via numba-jitted forward/backward passes. I'm not entirely sure about the project's philosopy of what should live in the test layer vs examples, and I'd be happy to hear your thoughts.\n\n\nTidied up the cuda_array_interface dir behavior.\n\n\nTidied up the cuda_array_interface error messages.\n\n\nI'm a bit concerned about adding the numba installation to your test runner, rather than folding the numba package into your test image. I've pinned to installed version to attempt to mitigate the risk of a dependency on an external package, but this still adds ~400mb of external package access to the test run.", "body": "@ezyang\r\n\r\nThis is ready for another round of review when you get the chance. I've:\r\n\r\n1. Expanded the test suite to cover baseline numba integration and added numba installation to xenial-cuda9 py2 & py3 test configurations.\r\n\r\n    This _could_ be expanded into a full end-to-end test, covering dispatch of a jit-compiled numba function over input torch.Tensors and/or a full test of autograd integration via numba-jitted forward/backward passes. I'm not entirely sure about the project's philosopy of what should live in the test layer vs examples, and I'd be happy to hear your thoughts.\r\n    \r\n2. Tidied up the __cuda_array_interface__ `dir` behavior.\r\n2. Tidied up the __cuda_array_interface__ error messages.\r\n\r\nI'm a _bit_ concerned about adding the numba installation to your test runner, rather than folding the numba package into your test image. I've pinned to installed version to attempt to mitigate the risk of a dependency on an external package, but this still adds ~400mb of external package access to the test run."}