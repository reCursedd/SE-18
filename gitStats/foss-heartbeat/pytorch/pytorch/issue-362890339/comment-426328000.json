{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/426328000", "html_url": "https://github.com/pytorch/pytorch/pull/11984#issuecomment-426328000", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11984", "id": 426328000, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjMyODAwMA==", "user": {"login": "asford", "id": 282792, "node_id": "MDQ6VXNlcjI4Mjc5Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/282792?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asford", "html_url": "https://github.com/asford", "followers_url": "https://api.github.com/users/asford/followers", "following_url": "https://api.github.com/users/asford/following{/other_user}", "gists_url": "https://api.github.com/users/asford/gists{/gist_id}", "starred_url": "https://api.github.com/users/asford/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asford/subscriptions", "organizations_url": "https://api.github.com/users/asford/orgs", "repos_url": "https://api.github.com/users/asford/repos", "events_url": "https://api.github.com/users/asford/events{/privacy}", "received_events_url": "https://api.github.com/users/asford/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-02T15:54:11Z", "updated_at": "2018-10-02T15:54:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a></p>\n<p>I'm comfortable with either solution you've proposed, mainly trying to avoid having a drive-by contribution that makes your testing system more fragile. I've been burned a few times by having dependencies on large external downloads in CI and think it's ideal to avoid it if possible.</p>\n<p>Having this run as a nightly may be best if you already have the infrastructure in place for that. Are the pytorch-examples run on a periodic basis as part of your CI?</p>\n<p>Having the ability to read data from numba seems generally useful. You can approximate this now by creating a numbabased view of the \"to\" tensor and then copy data via numba from the \"from\" data buffer, but executing this in torch is more natural. Given that it both improves the testing situation and improves the interface I'm in favor of this approach.</p>\n<p>Adding read support for <code>__cuda_array_interface__</code> components makes this feature <em>very</em> similar to the existing dlpack utilities. I'm going to quickly review that implementation to make sure I'm not adding a bunch of duplicate logic.</p>", "body_text": "@ezyang\nI'm comfortable with either solution you've proposed, mainly trying to avoid having a drive-by contribution that makes your testing system more fragile. I've been burned a few times by having dependencies on large external downloads in CI and think it's ideal to avoid it if possible.\nHaving this run as a nightly may be best if you already have the infrastructure in place for that. Are the pytorch-examples run on a periodic basis as part of your CI?\nHaving the ability to read data from numba seems generally useful. You can approximate this now by creating a numbabased view of the \"to\" tensor and then copy data via numba from the \"from\" data buffer, but executing this in torch is more natural. Given that it both improves the testing situation and improves the interface I'm in favor of this approach.\nAdding read support for __cuda_array_interface__ components makes this feature very similar to the existing dlpack utilities. I'm going to quickly review that implementation to make sure I'm not adding a bunch of duplicate logic.", "body": "@ezyang \r\n\r\nI'm comfortable with either solution you've proposed, mainly trying to avoid having a drive-by contribution that makes your testing system more fragile. I've been burned a few times by having dependencies on large external downloads in CI and think it's ideal to avoid it if possible. \r\n\r\nHaving this run as a nightly may be best if you already have the infrastructure in place for that. Are the pytorch-examples run on a periodic basis as part of your CI?\r\n\r\nHaving the ability to read data from numba seems generally useful. You can approximate this now by creating a numbabased view of the \"to\" tensor and then copy data via numba from the \"from\" data buffer, but executing this in torch is more natural. Given that it both improves the testing situation and improves the interface I'm in favor of this approach.\r\n\r\nAdding read support for `__cuda_array_interface__` components makes this feature _very_ similar to the existing dlpack utilities. I'm going to quickly review that implementation to make sure I'm not adding a bunch of duplicate logic."}