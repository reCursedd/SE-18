{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/331048633", "html_url": "https://github.com/pytorch/pytorch/issues/264#issuecomment-331048633", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/264", "id": 331048633, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTA0ODYzMw==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-21T04:35:09Z", "updated_at": "2017-09-21T04:44:15Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5652049\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zou3519\">@zou3519</a> let's do the following:</p>\n<p>Introduce a <code>sum</code> keyword argument for the losses.<br>\nWhen <code>sum=True</code>, existing behavior of the summed up losses is preserved.<br>\nWhen <code>sum=False</code>, we will instead return per-sample losses, i.e. losses that are not summed over the mini-batch.</p>\n<p>Start with tackling three losses:</p>\n<ul>\n<li>NLLLoss</li>\n<li>CrossEntropyLoss (this only needs python changes, its implemented as LogSoftmax  + NLLLoss)</li>\n<li>MSELoss</li>\n</ul>\n<p>Remember that when <code>sum=False</code>, the gradient wrt the loss, i.e. <code>gradOutput</code> is not implicit anymore. In general, if the loss is a scalar output, we assume that the <code>gradOutput</code> is the value <code>1</code>.<br>\nIf the loss is not a scalar output, the gradOutput will be of the same dimensionality and shape as the output of the loss function.</p>\n<p>What does this mean for this task?<br>\nYou will have to introduce <code>gradOutput</code> as an argument to the <code>updateGradInput</code> functions of the loss functions in THNN/THCUNN.<br>\nFor example, here:<br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/MSECriterion.c#L28\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/MSECriterion.c#L28</a><br>\n<a href=\"https://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/THNN.h#L290\">https://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/THNN.h#L290</a></p>\n<p>The relevant C code for both these losses are in MSECriterion.c and ClassNLLCriterion.c (similarly, .cu files for THCUNN).</p>\n<p>While making these modifications, <a href=\"http://pytorch.org/docs/master/notes/extending.html?highlight=gradcheck\" rel=\"nofollow\">gradcheck</a> / unit tests are your best friend.</p>\n<p>Let's discuss IRL if you have any questions about the math and stuff.</p>\n<p>Basically you will be changing the updateOutput and updateGradInput functions.</p>\n<p>Another super-easy sanity check is to implement the same loss as simple autograd operations and comparing the output / gradInput of this reference implementation against your modified implementation.</p>\n<p>For example, MSELoss with <code>sum=False</code> can be implemented as:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">MSELoss_ref</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">sum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n   <span class=\"pl-k\">return</span> (<span class=\"pl-c1\">input</span> <span class=\"pl-k\">-</span> target)<span class=\"pl-k\">**</span><span class=\"pl-c1\">2</span></pre></div>", "body_text": "@zou3519 let's do the following:\nIntroduce a sum keyword argument for the losses.\nWhen sum=True, existing behavior of the summed up losses is preserved.\nWhen sum=False, we will instead return per-sample losses, i.e. losses that are not summed over the mini-batch.\nStart with tackling three losses:\n\nNLLLoss\nCrossEntropyLoss (this only needs python changes, its implemented as LogSoftmax  + NLLLoss)\nMSELoss\n\nRemember that when sum=False, the gradient wrt the loss, i.e. gradOutput is not implicit anymore. In general, if the loss is a scalar output, we assume that the gradOutput is the value 1.\nIf the loss is not a scalar output, the gradOutput will be of the same dimensionality and shape as the output of the loss function.\nWhat does this mean for this task?\nYou will have to introduce gradOutput as an argument to the updateGradInput functions of the loss functions in THNN/THCUNN.\nFor example, here:\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/MSECriterion.c#L28\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/THNN.h#L290\nThe relevant C code for both these losses are in MSECriterion.c and ClassNLLCriterion.c (similarly, .cu files for THCUNN).\nWhile making these modifications, gradcheck / unit tests are your best friend.\nLet's discuss IRL if you have any questions about the math and stuff.\nBasically you will be changing the updateOutput and updateGradInput functions.\nAnother super-easy sanity check is to implement the same loss as simple autograd operations and comparing the output / gradInput of this reference implementation against your modified implementation.\nFor example, MSELoss with sum=False can be implemented as:\ndef MSELoss_ref(input, target, size_average=False, sum=False):\n   return (input - target)**2", "body": "@zou3519 let's do the following:\r\n\r\nIntroduce a `sum` keyword argument for the losses.\r\nWhen `sum=True`, existing behavior of the summed up losses is preserved.\r\nWhen `sum=False`, we will instead return per-sample losses, i.e. losses that are not summed over the mini-batch.\r\n\r\nStart with tackling three losses:\r\n- NLLLoss\r\n- CrossEntropyLoss (this only needs python changes, its implemented as LogSoftmax  + NLLLoss)\r\n- MSELoss\r\n\r\nRemember that when `sum=False`, the gradient wrt the loss, i.e. `gradOutput` is not implicit anymore. In general, if the loss is a scalar output, we assume that the `gradOutput` is the value `1`.\r\nIf the loss is not a scalar output, the gradOutput will be of the same dimensionality and shape as the output of the loss function.\r\n\r\nWhat does this mean for this task?\r\nYou will have to introduce `gradOutput` as an argument to the `updateGradInput` functions of the loss functions in THNN/THCUNN.\r\nFor example, here:\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/MSECriterion.c#L28\r\nhttps://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/THNN.h#L290\r\n\r\nThe relevant C code for both these losses are in MSECriterion.c and ClassNLLCriterion.c (similarly, .cu files for THCUNN).\r\n\r\nWhile making these modifications, [gradcheck](http://pytorch.org/docs/master/notes/extending.html?highlight=gradcheck) / unit tests are your best friend.\r\n\r\nLet's discuss IRL if you have any questions about the math and stuff.\r\n\r\nBasically you will be changing the updateOutput and updateGradInput functions.\r\n\r\nAnother super-easy sanity check is to implement the same loss as simple autograd operations and comparing the output / gradInput of this reference implementation against your modified implementation.\r\n\r\nFor example, MSELoss with `sum=False` can be implemented as:\r\n\r\n```python\r\ndef MSELoss_ref(input, target, size_average=False, sum=False):\r\n   return (input - target)**2\r\n```"}