{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189477180", "pull_request_review_id": 121662537, "id": 189477180, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4OTQ3NzE4MA==", "diff_hunk": "@@ -59,10 +49,13 @@ def contiguous(input):\n     return input\n \n \n+# `input` is input to `fn`\n+# `target` is the Tensors wrt whom Jacobians are calculated\n def get_numerical_jacobian(fn, input, target, eps=1e-3):\n     # To be able to use .view(-1) input must be contiguous\n     input = contiguous(input)\n-    target = contiguous(target)\n+    if not all(t.is_contiguous() for t in iter_tensors(target)):\n+        raise RuntimeError(\"target must only contain contiguous Tensors\")", "path": "torch/autograd/gradcheck.py", "position": null, "original_position": 36, "commit_id": "2ba2b50122d1c270dbe99b96c449969ca5c36a5d", "original_commit_id": "2c26aa58f8ef86c3526c794b239558c30f5dddf0", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "body": "The version here is WIP. But when `target` is not `input`, e.g., in some tests we have `get_numerical_jacobian(module, input, module_params)`, then making `target` contiguous is a copy, and changing values in it has no effect on the resulting gradients, and thus the numerical result would be 0.\r\n\r\nThe current gradgradcheck is broken because of this. It only passed because we had the same `contiguous(inputs)` in calculating analytical grads.", "created_at": "2018-05-21T00:55:27Z", "updated_at": "2018-11-23T15:44:23Z", "html_url": "https://github.com/pytorch/pytorch/pull/7710#discussion_r189477180", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/7710", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/189477180"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/7710#discussion_r189477180"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/7710"}}, "body_html": "<p>The version here is WIP. But when <code>target</code> is not <code>input</code>, e.g., in some tests we have <code>get_numerical_jacobian(module, input, module_params)</code>, then making <code>target</code> contiguous is a copy, and changing values in it has no effect on the resulting gradients, and thus the numerical result would be 0.</p>\n<p>The current gradgradcheck is broken because of this. It only passed because we had the same <code>contiguous(inputs)</code> in calculating analytical grads.</p>", "body_text": "The version here is WIP. But when target is not input, e.g., in some tests we have get_numerical_jacobian(module, input, module_params), then making target contiguous is a copy, and changing values in it has no effect on the resulting gradients, and thus the numerical result would be 0.\nThe current gradgradcheck is broken because of this. It only passed because we had the same contiguous(inputs) in calculating analytical grads.", "in_reply_to_id": 189468334}