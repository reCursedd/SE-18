{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14253", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14253/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14253/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14253/events", "html_url": "https://github.com/pytorch/pytorch/pull/14253", "id": 382891576, "node_id": "MDExOlB1bGxSZXF1ZXN0MjMyNDkyODQy", "number": 14253, "title": "Option to preserve bitwise accuracy of gradient checkpointed vs non-checkpointed dropout", "user": {"login": "mcarilli", "id": 7799218, "node_id": "MDQ6VXNlcjc3OTkyMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/7799218?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mcarilli", "html_url": "https://github.com/mcarilli", "followers_url": "https://api.github.com/users/mcarilli/followers", "following_url": "https://api.github.com/users/mcarilli/following{/other_user}", "gists_url": "https://api.github.com/users/mcarilli/gists{/gist_id}", "starred_url": "https://api.github.com/users/mcarilli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mcarilli/subscriptions", "organizations_url": "https://api.github.com/users/mcarilli/orgs", "repos_url": "https://api.github.com/users/mcarilli/repos", "events_url": "https://api.github.com/users/mcarilli/events{/privacy}", "received_events_url": "https://api.github.com/users/mcarilli/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-20T23:33:00Z", "updated_at": "2018-11-23T16:11:20Z", "closed_at": "2018-11-23T16:11:20Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/14253", "html_url": "https://github.com/pytorch/pytorch/pull/14253", "diff_url": "https://github.com/pytorch/pytorch/pull/14253.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/14253.patch"}, "body_html": "<p>This issue was noticed, and fix proposed, by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9101033\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/raulpuric\">@raulpuric</a>.</p>\n<p>Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward.  This can result in the RNG state advancing more than it would without checkpointing, which can cause checkpoints that include dropout invocations to lose end-to-end bitwise accuracy as compared to non-checkpointed passes.</p>\n<p>The present PR contains optional logic to juggle the RNG states such that checkpointed passes containing dropout achieve bitwise accuracy with non-checkpointed equivalents.**  The user requests this behavior by supplying <code>preserve_rng_state=True</code> to <code>torch.utils.checkpoint</code> or <code>torch.utils.checkpoint_sequential</code>.</p>\n<p>Currently, <code>preserve_rng_state=True</code> may incur a moderate performance hit because restoring MTGP states can be expensive.  However, restoring Philox states is dirt cheap, so <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8906225\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/syed-ahmed\">@syed-ahmed</a>'s <a href=\"https://github.com/pytorch/pytorch/pull/13070#discussion_r235179882\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/pytorch/pytorch/pull/13070/hovercard\">RNG refactor</a>, once merged, will make this option more or less free.</p>\n<p>I'm a little wary of the <a href=\"https://github.com/pytorch/pytorch/pull/14253/files#diff-58da227fc9b1d56752b7dfad90428fe0R75\">def checkpoint(function, *args, preserve_rng_state=False):</a> argument-passing method (specifically, putting a kwarg after a variable argument list).  Python 3 seems happy with it.<br>\nEdit:  It appears Python 2.7 is NOT happy with a <a href=\"https://travis-ci.org/pytorch/pytorch/builds/457706518?utm_source=github_status&amp;utm_medium=notification\" rel=\"nofollow\">kwarg after *args</a>.  <code>preserve_rng_state</code> also needs to be communicated in a way that doesn't break any existing usage.  I'm open to suggestions (a global flag perhaps)?</p>\n<p>**Batchnorm may still be an issue, but that's a battle for another day.</p>", "body_text": "This issue was noticed, and fix proposed, by @raulpuric.\nCheckpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward.  This can result in the RNG state advancing more than it would without checkpointing, which can cause checkpoints that include dropout invocations to lose end-to-end bitwise accuracy as compared to non-checkpointed passes.\nThe present PR contains optional logic to juggle the RNG states such that checkpointed passes containing dropout achieve bitwise accuracy with non-checkpointed equivalents.**  The user requests this behavior by supplying preserve_rng_state=True to torch.utils.checkpoint or torch.utils.checkpoint_sequential.\nCurrently, preserve_rng_state=True may incur a moderate performance hit because restoring MTGP states can be expensive.  However, restoring Philox states is dirt cheap, so @syed-ahmed's RNG refactor, once merged, will make this option more or less free.\nI'm a little wary of the def checkpoint(function, *args, preserve_rng_state=False): argument-passing method (specifically, putting a kwarg after a variable argument list).  Python 3 seems happy with it.\nEdit:  It appears Python 2.7 is NOT happy with a kwarg after *args.  preserve_rng_state also needs to be communicated in a way that doesn't break any existing usage.  I'm open to suggestions (a global flag perhaps)?\n**Batchnorm may still be an issue, but that's a battle for another day.", "body": "This issue was noticed, and fix proposed, by @raulpuric.\r\n\r\nCheckpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward.  This can result in the RNG state advancing more than it would without checkpointing, which can cause checkpoints that include dropout invocations to lose end-to-end bitwise accuracy as compared to non-checkpointed passes.  \r\n\r\nThe present PR contains optional logic to juggle the RNG states such that checkpointed passes containing dropout achieve bitwise accuracy with non-checkpointed equivalents.**  The user requests this behavior by supplying `preserve_rng_state=True` to `torch.utils.checkpoint` or `torch.utils.checkpoint_sequential`.\r\n\r\nCurrently, `preserve_rng_state=True` may incur a moderate performance hit because restoring MTGP states can be expensive.  However, restoring Philox states is dirt cheap, so @syed-ahmed's [RNG refactor](https://github.com/pytorch/pytorch/pull/13070#discussion_r235179882), once merged, will make this option more or less free.\r\n\r\nI'm a little wary of the [def checkpoint(function, *args, preserve_rng_state=False):](https://github.com/pytorch/pytorch/pull/14253/files#diff-58da227fc9b1d56752b7dfad90428fe0R75) argument-passing method (specifically, putting a kwarg after a variable argument list).  Python 3 seems happy with it.\r\nEdit:  It appears Python 2.7 is NOT happy with a [kwarg after *args](https://travis-ci.org/pytorch/pytorch/builds/457706518?utm_source=github_status&utm_medium=notification).  `preserve_rng_state` also needs to be communicated in a way that doesn't break any existing usage.  I'm open to suggestions (a global flag perhaps)?\r\n\r\n\r\n**Batchnorm may still be an issue, but that's a battle for another day."}