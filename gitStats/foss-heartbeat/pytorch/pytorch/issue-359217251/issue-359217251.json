{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11545", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11545/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11545/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11545/events", "html_url": "https://github.com/pytorch/pytorch/pull/11545", "id": 359217251, "node_id": "MDExOlB1bGxSZXF1ZXN0MjE0NzYxMzgz", "number": 11545, "title": "Improve tracer warnings", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-09-11T20:46:47Z", "updated_at": "2018-09-12T05:11:52Z", "closed_at": "2018-09-12T05:11:52Z", "author_association": "MEMBER", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11545", "html_url": "https://github.com/pytorch/pytorch/pull/11545", "diff_url": "https://github.com/pytorch/pytorch/pull/11545.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11545.patch"}, "body_html": "<p>Also, fix a performance bug in <code>ensureUnique</code>. Previously it formatted the warning string even though we weren't tracing, so all that work would <em>always</em> happen in the hot path and be for nothing.</p>\n<p>A sample of how the new warnings look like:</p>\n<pre><code>tmp.py:4: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Pytho\nn values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  int(x)\ntmp.py:5: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this fun\nction to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might caus\ne the trace to be incorrect.\n  torch.tensor([1.])\ntmp.py:6: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator add_. This might cause t\nhe trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other ha\nnd, if all other views use the same memory, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n  torch.split(y, 2, dim=1)[0].add_(2)\n\n</code></pre>", "body_text": "Also, fix a performance bug in ensureUnique. Previously it formatted the warning string even though we weren't tracing, so all that work would always happen in the hot path and be for nothing.\nA sample of how the new warnings look like:\ntmp.py:4: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Pytho\nn values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  int(x)\ntmp.py:5: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this fun\nction to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might caus\ne the trace to be incorrect.\n  torch.tensor([1.])\ntmp.py:6: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator add_. This might cause t\nhe trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other ha\nnd, if all other views use the same memory, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n  torch.split(y, 2, dim=1)[0].add_(2)", "body": "Also, fix a performance bug in `ensureUnique`. Previously it formatted the warning string even though we weren't tracing, so all that work would *always* happen in the hot path and be for nothing.\r\n\r\nA sample of how the new warnings look like:\r\n```\r\ntmp.py:4: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Pytho\r\nn values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  int(x)\r\ntmp.py:5: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this fun\r\nction to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might caus\r\ne the trace to be incorrect.\r\n  torch.tensor([1.])\r\ntmp.py:6: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator add_. This might cause t\r\nhe trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other ha\r\nnd, if all other views use the same memory, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\r\n  torch.split(y, 2, dim=1)[0].add_(2)\r\n\r\n```"}