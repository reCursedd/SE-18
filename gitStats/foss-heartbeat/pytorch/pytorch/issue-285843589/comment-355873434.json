{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355873434", "html_url": "https://github.com/pytorch/pytorch/issues/4466#issuecomment-355873434", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4466", "id": 355873434, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTg3MzQzNA==", "user": {"login": "sighingnow", "id": 7144772, "node_id": "MDQ6VXNlcjcxNDQ3NzI=", "avatar_url": "https://avatars2.githubusercontent.com/u/7144772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sighingnow", "html_url": "https://github.com/sighingnow", "followers_url": "https://api.github.com/users/sighingnow/followers", "following_url": "https://api.github.com/users/sighingnow/following{/other_user}", "gists_url": "https://api.github.com/users/sighingnow/gists{/gist_id}", "starred_url": "https://api.github.com/users/sighingnow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sighingnow/subscriptions", "organizations_url": "https://api.github.com/users/sighingnow/orgs", "repos_url": "https://api.github.com/users/sighingnow/repos", "events_url": "https://api.github.com/users/sighingnow/events{/privacy}", "received_events_url": "https://api.github.com/users/sighingnow/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-08T02:20:12Z", "updated_at": "2018-01-08T02:20:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When dispatch the module methods to variables, if the <code>self</code> argument comes second, the first two arguments need to be swapped.</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/35abc4efa2d08ef2e9b7d978089fbd98b8d14187/torch/csrc/Module.cpp#L222-L224\">pytorch/torch/csrc/Module.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 222 to 224\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/35abc4efa2d08ef2e9b7d978089fbd98b8d14187\">35abc4e</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L222\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"222\"></td>\n          <td id=\"LC222\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">//</span> On Variables, swap the first two arguments if the 'self' argument comes</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L223\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"223\"></td>\n          <td id=\"LC223\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">//</span> second. This handles the deprecated torch.addxx signatures. For example,</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L224\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"224\"></td>\n          <td id=\"LC224\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">//</span> torch.addmm(1, var, 2, a, b) -&gt; var.addmm(1, 2, a, b)</span> </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>We could use <code>polygamma</code> as</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> x <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> v <span class=\"pl-k\">=</span> torch.autograd.Variable(x, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.polygamma(<span class=\"pl-c1\">1</span>, x) <span class=\"pl-c\"><span class=\"pl-c\">#</span> works</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.polygamma(<span class=\"pl-c1\">1</span>, v) <span class=\"pl-c\"><span class=\"pl-c\">#</span> doesn't work</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> torch.polygamma(v, <span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> works</span></pre></div>\n<p>We should call <code>IMPLEMENT_STATELESS</code> for <code>polygamma</code> rather than <code>IMPLEMENT_STATELESS</code>. I will open a PR for that.</p>", "body_text": "When dispatch the module methods to variables, if the self argument comes second, the first two arguments need to be swapped.\n\n  \n    \n      pytorch/torch/csrc/Module.cpp\n    \n    \n        Lines 222 to 224\n      in\n      35abc4e\n    \n    \n    \n    \n\n        \n          \n           // On Variables, swap the first two arguments if the 'self' argument comes \n        \n\n        \n          \n           // second. This handles the deprecated torch.addxx signatures. For example, \n        \n\n        \n          \n           // torch.addmm(1, var, 2, a, b) -> var.addmm(1, 2, a, b) \n        \n    \n  \n\n\nWe could use polygamma as\n>>> x = torch.ones(1)\n>>> v = torch.autograd.Variable(x, requires_grad=True)\n\n>>> torch.polygamma(1, x) # works\n>>> torch.polygamma(1, v) # doesn't work\n>>> torch.polygamma(v, 1) # works\nWe should call IMPLEMENT_STATELESS for polygamma rather than IMPLEMENT_STATELESS. I will open a PR for that.", "body": "When dispatch the module methods to variables, if the `self` argument comes second, the first two arguments need to be swapped.\r\n\r\nhttps://github.com/pytorch/pytorch/blob/35abc4efa2d08ef2e9b7d978089fbd98b8d14187/torch/csrc/Module.cpp#L222-L224\r\n\r\nWe could use `polygamma` as\r\n\r\n```python\r\n>>> x = torch.ones(1)\r\n>>> v = torch.autograd.Variable(x, requires_grad=True)\r\n\r\n>>> torch.polygamma(1, x) # works\r\n>>> torch.polygamma(1, v) # doesn't work\r\n>>> torch.polygamma(v, 1) # works\r\n```\r\n\r\nWe should call `IMPLEMENT_STATELESS` for `polygamma` rather than `IMPLEMENT_STATELESS`. I will open a PR for that."}