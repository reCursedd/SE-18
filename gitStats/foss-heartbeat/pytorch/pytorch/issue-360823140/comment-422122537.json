{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/422122537", "html_url": "https://github.com/pytorch/pytorch/issues/11752#issuecomment-422122537", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/11752", "id": 422122537, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjEyMjUzNw==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-17T18:32:27Z", "updated_at": "2018-09-17T18:32:27Z", "author_association": "MEMBER", "body_html": "<p>I think this boils down to <code>log_softmax</code>.</p>\n<div class=\"highlight highlight-source-python\"><pre>y_small <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float32)\ny_big <span class=\"pl-k\">=</span> y_small <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1e8</span>\n<span class=\"pl-c1\">print</span>(torch.nn.functional.log_softmax(y_small, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n<span class=\"pl-c1\">print</span>(torch.nn.functional.log_softmax(y_big, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))</pre></div>\n<p>yields</p>\n<div class=\"highlight highlight-source-python\"><pre>tensor([[<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6931</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6931</span>]])\ntensor([[<span class=\"pl-c1\">0</span>., <span class=\"pl-c1\">0</span>.]])</pre></div>\n<p>I'm not yet sure where the problem is (I couldn't find anything suspicious from a quick look at <a href=\"https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L44-L60\">the CPU code</a>), but for information it works as expected if we cast the tensors to double precision</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">print</span>(torch.nn.functional.log_softmax(y_small.double(), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n<span class=\"pl-c1\">print</span>(torch.nn.functional.log_softmax(y_big.double(), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))</pre></div>\n<p>gives</p>\n<div class=\"highlight highlight-source-python\"><pre>tensor([[<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6931</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6931</span>]], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float64)\ntensor([[<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6931</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6931</span>]], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float64)</pre></div>", "body_text": "I think this boils down to log_softmax.\ny_small = torch.ones(1, 2, dtype=torch.float32)\ny_big = y_small * 1e8\nprint(torch.nn.functional.log_softmax(y_small, -1))\nprint(torch.nn.functional.log_softmax(y_big, -1))\nyields\ntensor([[-0.6931, -0.6931]])\ntensor([[0., 0.]])\nI'm not yet sure where the problem is (I couldn't find anything suspicious from a quick look at the CPU code), but for information it works as expected if we cast the tensors to double precision\nprint(torch.nn.functional.log_softmax(y_small.double(), -1))\nprint(torch.nn.functional.log_softmax(y_big.double(), -1))\ngives\ntensor([[-0.6931, -0.6931]], dtype=torch.float64)\ntensor([[-0.6931, -0.6931]], dtype=torch.float64)", "body": "I think this boils down to `log_softmax`.\r\n\r\n```python\r\ny_small = torch.ones(1, 2, dtype=torch.float32)\r\ny_big = y_small * 1e8\r\nprint(torch.nn.functional.log_softmax(y_small, -1))\r\nprint(torch.nn.functional.log_softmax(y_big, -1))\r\n```\r\nyields\r\n```python\r\ntensor([[-0.6931, -0.6931]])\r\ntensor([[0., 0.]])\r\n```\r\n\r\nI'm not yet sure where the problem is (I couldn't find anything suspicious from a quick look at [the CPU code](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SoftMax.cpp#L44-L60)), but for information it works as expected if we cast the tensors to double precision\r\n```python\r\nprint(torch.nn.functional.log_softmax(y_small.double(), -1))\r\nprint(torch.nn.functional.log_softmax(y_big.double(), -1))\r\n```\r\ngives\r\n```python\r\ntensor([[-0.6931, -0.6931]], dtype=torch.float64)\r\ntensor([[-0.6931, -0.6931]], dtype=torch.float64)\r\n```"}