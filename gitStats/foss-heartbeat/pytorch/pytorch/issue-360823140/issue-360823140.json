{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11752", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11752/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11752/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11752/events", "html_url": "https://github.com/pytorch/pytorch/issues/11752", "id": 360823140, "node_id": "MDU6SXNzdWUzNjA4MjMxNDA=", "number": 11752, "title": "nn.CrossEntropyLoss() yields wrong output for big logits", "user": {"login": "fab-jul", "id": 3440923, "node_id": "MDQ6VXNlcjM0NDA5MjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/3440923?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fab-jul", "html_url": "https://github.com/fab-jul", "followers_url": "https://api.github.com/users/fab-jul/followers", "following_url": "https://api.github.com/users/fab-jul/following{/other_user}", "gists_url": "https://api.github.com/users/fab-jul/gists{/gist_id}", "starred_url": "https://api.github.com/users/fab-jul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fab-jul/subscriptions", "organizations_url": "https://api.github.com/users/fab-jul/orgs", "repos_url": "https://api.github.com/users/fab-jul/repos", "events_url": "https://api.github.com/users/fab-jul/events{/privacy}", "received_events_url": "https://api.github.com/users/fab-jul/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2018-09-17T11:20:15Z", "updated_at": "2018-09-19T19:31:18Z", "closed_at": "2018-09-19T19:31:18Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p><code>nn.CrossEntropyLoss()</code> yields incorrect results for big logits.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n\ncrit <span class=\"pl-k\">=</span> nn.CrossEntropyLoss()\n\ntarget <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.long)\n\ny_small <span class=\"pl-k\">=</span> torch.ones(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float32)\ny_big <span class=\"pl-k\">=</span> y_small <span class=\"pl-k\">*</span> <span class=\"pl-c1\">1e8</span>\n\nloss_small <span class=\"pl-k\">=</span> crit(y_small, target)\nloss_big <span class=\"pl-k\">=</span> crit(y_big, target)\nloss_fixed <span class=\"pl-k\">=</span> crit(y_big <span class=\"pl-k\">-</span> y_big.max(), target)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss_small = <span class=\"pl-c1\">{}</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>\n      <span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss_big   = <span class=\"pl-c1\">{}</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>\n      <span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss_fixed = <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(\n        loss_small, loss_big, loss_fixed))</pre></div>\n<p><code>loss_small</code> should be equal to <code>loss_big</code>, as both <code>y_small</code> and <code>y_big</code> contain logits that correspond to the same distribution. However, the output of the above is:</p>\n<pre><code>loss_small = 0.6931471824645996\nloss_big   = 0.0\nloss_fixed = 0.6931471824645996\n</code></pre>\n<p>Note that the output is correct (<code>loss_fixed == loss_small</code>) if I apply the trick <code>softmax(x) = softmax(x - c)</code> for some <code>c</code>, in particular, for <code>c=max(x)</code>.</p>\n<p>This happens on CPU and GPU.</p>\n<h2>System Info</h2>\n<p>PyTorch version: 0.4.1<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Debian GNU/Linux 8 (jessie)<br>\nGCC version: (Debian 4.9.2-10+deb8u1) 4.9.2<br>\nCMake version: version 3.0.2</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nGPU 0: TITAN Xp</p>\n<p>Nvidia driver version: 384.130<br>\ncuDNN version: Could not collect</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.14.3)<br>\n[pip] torch (0.4.1)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] pytorch                   0.4.1           py36_cuda9.0.176_cudnn7.1.2_1    pytorch<br>\n[conda] torchvision               0.2.1                    py36_1    pytorch</p>", "body_text": "Issue description\nnn.CrossEntropyLoss() yields incorrect results for big logits.\nCode example\nimport torch\nfrom torch import nn\n\ncrit = nn.CrossEntropyLoss()\n\ntarget = torch.ones(1, dtype=torch.long)\n\ny_small = torch.ones(1, 2, dtype=torch.float32)\ny_big = y_small * 1e8\n\nloss_small = crit(y_small, target)\nloss_big = crit(y_big, target)\nloss_fixed = crit(y_big - y_big.max(), target)\n\nprint('loss_small = {}\\n'\n      'loss_big   = {}\\n'\n      'loss_fixed = {}'.format(\n        loss_small, loss_big, loss_fixed))\nloss_small should be equal to loss_big, as both y_small and y_big contain logits that correspond to the same distribution. However, the output of the above is:\nloss_small = 0.6931471824645996\nloss_big   = 0.0\nloss_fixed = 0.6931471824645996\n\nNote that the output is correct (loss_fixed == loss_small) if I apply the trick softmax(x) = softmax(x - c) for some c, in particular, for c=max(x).\nThis happens on CPU and GPU.\nSystem Info\nPyTorch version: 0.4.1\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Debian GNU/Linux 8 (jessie)\nGCC version: (Debian 4.9.2-10+deb8u1) 4.9.2\nCMake version: version 3.0.2\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0: TITAN Xp\nNvidia driver version: 384.130\ncuDNN version: Could not collect\nVersions of relevant libraries:\n[pip] numpy (1.14.3)\n[pip] torch (0.4.1)\n[pip] torchvision (0.2.1)\n[conda] pytorch                   0.4.1           py36_cuda9.0.176_cudnn7.1.2_1    pytorch\n[conda] torchvision               0.2.1                    py36_1    pytorch", "body": "## Issue description\r\n\r\n`nn.CrossEntropyLoss()` yields incorrect results for big logits.\r\n\r\n## Code example\r\n\r\n``` python\r\nimport torch\r\nfrom torch import nn\r\n\r\ncrit = nn.CrossEntropyLoss()\r\n\r\ntarget = torch.ones(1, dtype=torch.long)\r\n\r\ny_small = torch.ones(1, 2, dtype=torch.float32)\r\ny_big = y_small * 1e8\r\n\r\nloss_small = crit(y_small, target)\r\nloss_big = crit(y_big, target)\r\nloss_fixed = crit(y_big - y_big.max(), target)\r\n\r\nprint('loss_small = {}\\n'\r\n      'loss_big   = {}\\n'\r\n      'loss_fixed = {}'.format(\r\n        loss_small, loss_big, loss_fixed))\r\n```\r\n\r\n`loss_small` should be equal to `loss_big`, as both `y_small` and `y_big` contain logits that correspond to the same distribution. However, the output of the above is:\r\n\r\n```\r\nloss_small = 0.6931471824645996\r\nloss_big   = 0.0\r\nloss_fixed = 0.6931471824645996\r\n```\r\n\r\nNote that the output is correct (`loss_fixed == loss_small`) if I apply the trick `softmax(x) = softmax(x - c)` for some `c`, in particular, for `c=max(x)`.\r\n\r\nThis happens on CPU and GPU.\r\n\r\n## System Info\r\n\r\nPyTorch version: 0.4.1\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Debian GNU/Linux 8 (jessie)\r\nGCC version: (Debian 4.9.2-10+deb8u1) 4.9.2\r\nCMake version: version 3.0.2\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: TITAN Xp\r\n\r\nNvidia driver version: 384.130\r\ncuDNN version: Could not collect\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.14.3)\r\n[pip] torch (0.4.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch                   0.4.1           py36_cuda9.0.176_cudnn7.1.2_1    pytorch\r\n[conda] torchvision               0.2.1                    py36_1    pytorch\r\n"}