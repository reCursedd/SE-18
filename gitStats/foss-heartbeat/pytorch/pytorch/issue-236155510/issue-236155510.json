{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1811", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1811/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1811/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1811/events", "html_url": "https://github.com/pytorch/pytorch/issues/1811", "id": 236155510, "node_id": "MDU6SXNzdWUyMzYxNTU1MTA=", "number": 1811, "title": "Variable-length mini-batches in bidirectional RNNs", "user": {"login": "root20", "id": 8732466, "node_id": "MDQ6VXNlcjg3MzI0NjY=", "avatar_url": "https://avatars1.githubusercontent.com/u/8732466?v=4", "gravatar_id": "", "url": "https://api.github.com/users/root20", "html_url": "https://github.com/root20", "followers_url": "https://api.github.com/users/root20/followers", "following_url": "https://api.github.com/users/root20/following{/other_user}", "gists_url": "https://api.github.com/users/root20/gists{/gist_id}", "starred_url": "https://api.github.com/users/root20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/root20/subscriptions", "organizations_url": "https://api.github.com/users/root20/orgs", "repos_url": "https://api.github.com/users/root20/repos", "events_url": "https://api.github.com/users/root20/events{/privacy}", "received_events_url": "https://api.github.com/users/root20/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-06-15T11:13:14Z", "updated_at": "2017-06-16T02:13:52Z", "closed_at": "2017-06-16T02:13:37Z", "author_association": "NONE", "body_html": "<p>As per my understanding, PackedSequence enabled use of variable-length mini-batches without worrying about gradients flowing through the zero-padded frames.<br>\nI am curious about if it is also valid in bidirectional RNNs. (bi-RNN)<br>\nIf it performs reversal of the sequences, I think it should exclude zero-paddings.<br>\nIf we include zero-paddings during reversal, in the output of the bi-RNN, zero-paddings will be concatenated to the non-paddings in some short sequences in a mini-batch.<br>\nThis behaviour is different with the case when we use bi-RNNs with mini-batch of size 1.</p>\n<p>Example of wrong case (where \u25cf is non-paddng frame; \u25cb is zero-padding)&gt;<br>\n(before reversal)<br>\n\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\u25cb\u25cb<br>\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb<br>\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb<br>\n(after reversal)<br>\n\u25cb\u25cb\u25cb\u25cb\u25cf\u25cf\u25cf\u25cf<br>\n\u25cb\u25cb\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf<br>\n\u25cb\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf</p>\n<p>Example of correct case&gt;<br>\n(before reversal)<br>\n\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\u25cb\u25cb<br>\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb<br>\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb<br>\n(after reversal)<br>\n\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\u25cb\u25cb<br>\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb<br>\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb</p>\n<p>I don't really know how the bi-RNNs are implemented, but I could not see the part that takes lengths informations to handle this problem.</p>\n<p>Thank you.</p>", "body_text": "As per my understanding, PackedSequence enabled use of variable-length mini-batches without worrying about gradients flowing through the zero-padded frames.\nI am curious about if it is also valid in bidirectional RNNs. (bi-RNN)\nIf it performs reversal of the sequences, I think it should exclude zero-paddings.\nIf we include zero-paddings during reversal, in the output of the bi-RNN, zero-paddings will be concatenated to the non-paddings in some short sequences in a mini-batch.\nThis behaviour is different with the case when we use bi-RNNs with mini-batch of size 1.\nExample of wrong case (where \u25cf is non-paddng frame; \u25cb is zero-padding)>\n(before reversal)\n\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\u25cb\u25cb\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\n(after reversal)\n\u25cb\u25cb\u25cb\u25cb\u25cf\u25cf\u25cf\u25cf\n\u25cb\u25cb\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n\u25cb\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\nExample of correct case>\n(before reversal)\n\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\u25cb\u25cb\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\n(after reversal)\n\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\u25cb\u25cb\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\nI don't really know how the bi-RNNs are implemented, but I could not see the part that takes lengths informations to handle this problem.\nThank you.", "body": "As per my understanding, PackedSequence enabled use of variable-length mini-batches without worrying about gradients flowing through the zero-padded frames.\r\nI am curious about if it is also valid in bidirectional RNNs. (bi-RNN)\r\nIf it performs reversal of the sequences, I think it should exclude zero-paddings.\r\nIf we include zero-paddings during reversal, in the output of the bi-RNN, zero-paddings will be concatenated to the non-paddings in some short sequences in a mini-batch.\r\nThis behaviour is different with the case when we use bi-RNNs with mini-batch of size 1.\r\n\r\nExample of wrong case (where \u25cf is non-paddng frame; \u25cb is zero-padding)>\r\n(before reversal)\r\n\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\u25cb\u25cb\r\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\r\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\r\n(after reversal)\r\n\u25cb\u25cb\u25cb\u25cb\u25cf\u25cf\u25cf\u25cf\r\n\u25cb\u25cb\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\r\n\u25cb\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\r\n\r\nExample of correct case> \r\n(before reversal)\r\n\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\u25cb\u25cb\r\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\r\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\r\n(after reversal)\r\n\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\u25cb\u25cb\r\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\u25cb\r\n\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cb\r\n\r\nI don't really know how the bi-RNNs are implemented, but I could not see the part that takes lengths informations to handle this problem.\r\n\r\nThank you.\r\n"}