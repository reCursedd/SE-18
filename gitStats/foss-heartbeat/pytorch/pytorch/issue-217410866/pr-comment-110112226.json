{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110112226", "pull_request_review_id": 31254325, "id": 110112226, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMDExMjIyNg==", "diff_hunk": "@@ -0,0 +1,963 @@\n+#ifndef THC_GENERIC_FILE\n+#define THC_GENERIC_FILE \"generic/FusedRNNKernel.cu\"\n+#else\n+#include <cstdarg>\n+\n+#include \"../common.h\"\n+\n+#define DATATYPE TensorUtils<THCTensor>::DataType\n+\n+//factor will be 3 for GRU and 4 for LSTM\n+void THNN_(FusedRNNAssertSizes)(THCState *state, int factor, int count ...)\n+{\n+  va_list list;\n+  va_start(list, count);\n+  THCTensor *input = va_arg(list, THCTensor*);\n+  THCTensor *hidden = va_arg(list, THCTensor*);\n+  THArgCheck(THCTensor_(nElement)(state, input) ==\n+\t     THCTensor_(nElement)(state, hidden),\n+\t     3, \"Input and Hidden tensor sizes should be the same.\");\n+\n+  THAssertMsg(TensorUtils<THCTensor>::getDims(state, input) <= MAX_CUTORCH_DIMS,\n+\t\t \"Tensor dimension is too large.\");\n+\n+  THAssertMsg(TensorUtils<THCTensor>::getDims(state, hidden) <= MAX_CUTORCH_DIMS,\n+\t\t \"Tensor dimension is too large.\");\n+\n+  for (int arg=2; arg < count; ++arg){\n+    THCTensor *tens = va_arg(list, THCTensor*);\n+    THArgCheck(THCTensor_(nElement)(state, input) ==\n+\t       THCTensor_(nElement)(state, tens)*factor,\n+\t       3, \"A pointwise tensor was not the right size, should have 1/%u the elements of input/hidden tensor.\", arg, factor);\n+    THAssertMsg(TensorUtils<THCTensor>::getDims(state, tens) <= MAX_CUTORCH_DIMS,\n+\t\t \"Tensor dimension is too large.\");\n+  }\n+\n+  va_end(list);\n+}\n+\n+int THNN_(minIndexType)(THCState *state, int count, ...)\n+{\n+  va_list list;\n+  va_start(list, count);\n+\n+  int maxDim = -2;\n+  for (int arg=0; arg < count; ++arg){\n+    THCTensor* tens = va_arg(list, THCTensor*);\n+    if(THCTensor_(isContiguous)(state, tens)) continue;\n+    int tensdims = TensorUtils<THCTensor>::getDims(state, tens);\n+    maxDim = (( tensdims> maxDim) ? tensdims : maxDim);\n+  }\n+\n+  va_end(list);\n+  return maxDim;\n+}\n+\n+bool THNN_(canUse32BitIndexMath)(THCState *state, int count, ...)\n+{\n+  va_list list;\n+  va_start(list, count);\n+\n+  for (int arg=0; arg < count; ++arg){\n+    THCTensor *tens = va_arg(list, THCTensor*);\n+    if (!TensorUtils<THCTensor>::canUse32BitIndexMath(state, tens)){\n+\tva_end(list);\n+\treturn false;\n+      }\n+  }\n+  va_end(list);\n+  return true;\n+}\n+\n+#define DEVICE_LINEAR_GET(D_TENSOR, INDEX)\t\t\t\t\\\n+  D_TENSOR.data[IndexToOffset<T, IndexType, Dims>::get(INDEX, D_TENSOR)]\n+\n+#define H2F(input) __half2float(input)\n+#define F2H(input) __float2half(input)\n+\n+template <typename T,\n+\t  typename IndexType,\n+\t  int Dims>\n+#if __CUDA_ARCH__ >= 350\n+__launch_bounds__(32 * 16, 4)\n+#endif\n+__global__ void\n+  THNN_(GRUForward)(TensorInfo<T, IndexType> Input,\n+\t\t    TensorInfo<T, IndexType> Hidden,\n+\t\t    TensorInfo<T, IndexType> Bias1,\n+\t\t    TensorInfo<T, IndexType> Bias2,\n+\t\t    TensorInfo<T, IndexType> _hx,\n+\t\t    TensorInfo<T, IndexType> _hy,\n+\t\t    IndexType hsz,\n+\t\t    IndexType totalElements)\n+{\n+  for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x)\n+    {\n+\n+      IndexType offset = (linearIndex/hsz)*3*hsz+linearIndex%hsz;\n+\n+      T* ir = &DEVICE_LINEAR_GET(Input, offset+0*hsz);\n+      T* ii = &DEVICE_LINEAR_GET(Input, offset+1*hsz);\n+      T* in = &DEVICE_LINEAR_GET(Input, offset+2*hsz);\n+\n+      T* hr = &DEVICE_LINEAR_GET(Hidden,offset+0*hsz);\n+      T* hi = &DEVICE_LINEAR_GET(Hidden,offset+1*hsz);\n+      T hn = DEVICE_LINEAR_GET(Hidden,  offset+2*hsz);\n+\n+      T hx = DEVICE_LINEAR_GET(_hx, linearIndex);\n+\n+      T* hy = &DEVICE_LINEAR_GET(_hy, linearIndex);\n+\n+      bool has_bias = (Bias1.data != NULL);\n+\n+      T b1r, b1i, b1n, b2r, b2i, b2n;\n+\n+      if(has_bias){\n+\tb1r = DEVICE_LINEAR_GET(Bias1, linearIndex%hsz+0*hsz);\n+\tb1i = DEVICE_LINEAR_GET(Bias1, linearIndex%hsz+1*hsz);\n+\tb1n = DEVICE_LINEAR_GET(Bias1, linearIndex%hsz+2*hsz);\n+\n+\tb2r = DEVICE_LINEAR_GET(Bias2, linearIndex%hsz+0*hsz);\n+\tb2i = DEVICE_LINEAR_GET(Bias2, linearIndex%hsz+1*hsz);\n+\tb2n = DEVICE_LINEAR_GET(Bias2, linearIndex%hsz+2*hsz);\n+      }else{\n+#ifndef THC_REAL_IS_HALF\n+\tb1r = 0.0; b1i = 0.0; b1n = 0.0;\n+\tb2r = 0.0; b2i = 0.0; b2n = 0.0;\n+#else\n+\tb1r = F2H(0.0); b1i = F2H(0.0); b1n = F2H(0.0);\n+\tb2r = F2H(0.0); b2i = F2H(0.0); b2n = F2H(0.0);\n+#endif\n+      }\n+\n+\n+#ifndef THC_REAL_IS_HALF\n+\n+      T rg, ig, ng;\n+\n+      rg = *ir + *hr + b1r + b2r;\n+      ig = *ii + *hi + b1i + b2i;\n+\n+      TensorSigmoidOp<real>()(&rg, &rg);\n+      TensorSigmoidOp<real>()(&ig, &ig);\n+      ng = *in + b1n + rg * (hn + b2n);\n+      ng = THCNumerics<T>::tanh(ng);\n+      *hy = ng + ig * (hx - ng);\n+\n+      //SAVE FOR BACKWARDS\n+      *ir = rg;\n+      *ii = ig;\n+      *in = ng;\n+      *hr = hx;\n+      *hi = hn + b2n;\n+#else\n+\n+      float rg, ig, ng;\n+\n+      rg = H2F(*ir) + H2F(*hr) + H2F(b1r) + H2F(b2r);\n+      ig = H2F(*ii) + H2F(*hi) + H2F(b1i) + H2F(b2i);\n+\n+      TensorSigmoidOp<float>()(&rg, &rg);\n+      TensorSigmoidOp<float>()(&ig, &ig);\n+      ng = H2F(*in) + H2F(b1n) + rg*( H2F(hn)+H2F(b2n) );\n+      ng = THCNumerics<float>::tanh(ng);\n+      *hy = F2H( ng + ig * ( H2F(hx)-ng ) );\n+\n+      //SAVE FOR BACKWARDS\n+      *ir = F2H(rg);\n+      *ii = F2H(ig);\n+      *in = F2H(ng);\n+      *hr = hx;\n+      *hi = F2H( H2F(hn) + H2F(b2n) );\n+\n+#endif\n+    }\n+}\n+\n+template <typename T,\n+\t  typename IndexType,\n+\t  int Dims>\n+#if __CUDA_ARCH__ >= 350\n+__launch_bounds__(32 * 16, 4)\n+#endif\n+__global__ void\n+  THNN_(GRUBackward)(TensorInfo<T, IndexType> input,\n+\t\t     TensorInfo<T, IndexType> hidden,\n+\t\t     TensorInfo<T, IndexType> gradoutput,\n+\t\t     TensorInfo<T, IndexType> gradinput,\n+\t\t     IndexType hsz,\n+\t\t     IndexType totalElements)\n+{\n+  for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+    IndexType offset = (linearIndex/hsz)*3*hsz+linearIndex%hsz;;\n+\n+    //will return input grads here\n+    T* rg = &DEVICE_LINEAR_GET(input, offset+0*hsz);\n+    T* ig = &DEVICE_LINEAR_GET(input, offset+1*hsz);\n+    T* ng = &DEVICE_LINEAR_GET(input, offset+2*hsz);\n+    //will return hidden grads here\n+    T* hx = &DEVICE_LINEAR_GET(hidden, offset+0*hsz);\n+    T* hn = &DEVICE_LINEAR_GET(hidden, offset+1*hsz);\n+    T* oghn=&DEVICE_LINEAR_GET(hidden, offset+2*hsz);\n+\n+    T* gi = &DEVICE_LINEAR_GET(gradinput, linearIndex);\n+\n+    T* go = &DEVICE_LINEAR_GET(gradoutput, linearIndex);\n+\n+#ifndef THC_REAL_IS_HALF\n+    T gig = (*go)*(*hx-*ng)*( 1-(*ig) )*(*ig);\n+    T ghx = (*go)*(*ig);\n+    T gin = (*go)*(1-*ig)*( 1-(*ng)*(*ng) );\n+    T ghn = (gin) * (*rg);\n+    T grg = (gin)*(*hn)*( 1-(*rg) )*(*rg);\n+\n+    *gi = ghx;\n+\n+    *rg = grg;\n+    *ig = gig;\n+    *ng = gin;\n+\n+    *hx = grg;\n+    *hn = gig;\n+    *oghn = ghn;\n+#else\n+    float gig = H2F(*go)*( H2F(*hx)-H2F(*ng) )*( 1-H2F(*ig) )*H2F(*ig);\n+    float ghx = H2F(*go)*H2F(*ig);\n+    float gin = H2F(*go)*( 1-H2F(*ig) )*( 1-H2F(*ng)*H2F(*ng) );\n+    float ghn = H2F(gin) * H2F(*rg);\n+    float grg = H2F(gin)*H2F(*hn)*( 1-H2F(*rg) )*H2F(*rg);\n+\n+    *gi = F2H(ghx);\n+\n+    *rg = F2H(grg);\n+    *ig = F2H(gig);\n+    *ng = F2H(gin);\n+\n+    *hx = F2H(grg);\n+    *hn = F2H(gig);\n+    *oghn = F2H(ghn);\n+#endif\n+  }\n+}\n+\n+template <typename T,\n+\t  typename IndexType,\n+\t  int Dims>\n+#if __CUDA_ARCH__ >= 350\n+__launch_bounds__(32 * 16, 4)\n+#endif\n+__global__ void\n+  THNN_(LSTMForward)(TensorInfo<T, IndexType> input,\n+\t\t    TensorInfo<T, IndexType> hidden,\n+\t\t    TensorInfo<T, IndexType> bias1,\n+\t\t    TensorInfo<T, IndexType> bias2,\n+\t\t    TensorInfo<T, IndexType> _cx,\n+\t\t    TensorInfo<T, IndexType> _hy,\n+\t\t    TensorInfo<T, IndexType> _cy,\n+\t\t    IndexType hsz,\n+\t\t    IndexType totalElements)\n+{\n+\n+    for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x)\n+    {\n+\n+      IndexType offset = (linearIndex/hsz)*4*hsz+linearIndex%hsz;\n+\n+      T* iig = &DEVICE_LINEAR_GET(input, offset+0*hsz);\n+      T* ifg = &DEVICE_LINEAR_GET(input, offset+1*hsz);\n+      T* icg = &DEVICE_LINEAR_GET(input, offset+2*hsz);\n+      T* iog = &DEVICE_LINEAR_GET(input, offset+3*hsz);\n+\n+      T hig = DEVICE_LINEAR_GET(hidden, offset+0*hsz);\n+      T hfg = DEVICE_LINEAR_GET(hidden, offset+1*hsz);\n+      T hcg = DEVICE_LINEAR_GET(hidden,  offset+2*hsz);\n+      T hog = DEVICE_LINEAR_GET(hidden,  offset+3*hsz);\n+\n+      T cx = DEVICE_LINEAR_GET(_cx, linearIndex);\n+\n+      T* hy = &DEVICE_LINEAR_GET(_hy, linearIndex);\n+      T* cy = &DEVICE_LINEAR_GET(_cy, linearIndex);\n+\n+      bool has_bias = (bias1.data != NULL);\n+\n+      T b1i, b1f, b1c, b1o;\n+      T b2i, b2f, b2c, b2o;\n+\n+      if(has_bias){\n+\tb1i = DEVICE_LINEAR_GET(bias1, linearIndex%hsz+0*hsz);\n+\tb1f = DEVICE_LINEAR_GET(bias1, linearIndex%hsz+1*hsz);\n+\tb1c = DEVICE_LINEAR_GET(bias1, linearIndex%hsz+2*hsz);\n+\tb1o = DEVICE_LINEAR_GET(bias1, linearIndex%hsz+3*hsz);\n+\n+\tb2i = DEVICE_LINEAR_GET(bias2, linearIndex%hsz+0*hsz);\n+\tb2f = DEVICE_LINEAR_GET(bias2, linearIndex%hsz+1*hsz);\n+\tb2c = DEVICE_LINEAR_GET(bias2, linearIndex%hsz+2*hsz);\n+\tb2o = DEVICE_LINEAR_GET(bias2, linearIndex%hsz+3*hsz);\n+\n+      }else{\n+#ifndef THC_REAL_IS_HALF\n+\tb1i = 0.0; b1f = 0.0; b1c = 0.0; b1o = 0.0;\n+\tb2i = 0.0; b2f = 0.0; b2c = 0.0; b2o = 0.0;\n+#else\n+\tb1i = F2H(0.0); b1f = F2H(0.0); b1c = F2H(0.0); b1o = F2H(0.0);\n+\tb2i = F2H(0.0); b2f = F2H(0.0); b2c = F2H(0.0); b2o = F2H(0.0);\n+#endif\n+      }\n+\n+#ifndef THC_REAL_IS_HALF\n+      T ig, fg, cg, og;\n+\n+      ig = *iig + hig + b1i + b2i;\n+      fg = *ifg + hfg + b1f + b2f;\n+      cg = *icg + hcg + b1c + b2c;\n+      og = *iog + hog + b1o + b2o;\n+\n+      TensorSigmoidOp<real>()(&ig, &ig);\n+      TensorSigmoidOp<real>()(&fg, &fg);\n+      cg = THCNumerics<T>::tanh(cg);\n+      TensorSigmoidOp<real>()(&og, &og);\n+\n+      *cy = (fg * cx) + (ig * cg);\n+      *hy = og * THCNumerics<T>::tanh(*cy);\n+\n+      *iig = ig;\n+      *ifg = fg;\n+      *icg = cg;\n+      *iog = og;\n+#else\n+      float ig, fg, cg, og;\n+      float f_hy, f_cy;\n+\n+      ig = H2F(*iig) + H2F(hig) + H2F(b1i) + H2F(b2i);\n+      fg = H2F(*ifg) + H2F(hfg) + H2F(b1f) + H2F(b2f);\n+      cg = H2F(*icg) + H2F(hcg) + H2F(b1c) + H2F(b2c);\n+      og = H2F(*iog) + H2F(hog) + H2F(b1o) + H2F(b2o);\n+\n+      TensorSigmoidOp<float>()(&ig, &ig);\n+      TensorSigmoidOp<float>()(&fg, &fg);\n+      cg = THCNumerics<float>::tanh(cg);\n+      TensorSigmoidOp<float>()(&og, &og);\n+\n+      f_cy = (fg * H2F(cx) ) + (ig * cg);\n+      f_hy = og * THCNumerics<float>::tanh(f_cy);\n+\n+      *hy = F2H(f_hy);\n+      *cy = F2H(f_cy);\n+\n+      //SAVE FOR BACKWARDS\n+      //Also need cy and cx but can be saved easily in python\n+      *iig = F2H(ig);\n+      *ifg = F2H(fg);\n+      *icg = F2H(cg);\n+      *iog = F2H(og);\n+#endif\n+    }\n+}\n+\n+template <typename T,\n+\t  typename IndexType,\n+\t  int Dims>\n+#if __CUDA_ARCH__ >= 350\n+__launch_bounds__(32 * 16, 4)\n+#endif\n+__global__ void\n+  THNN_(LSTMBackward)(TensorInfo<T, IndexType> input,\n+\t\t      TensorInfo<T, IndexType> hidden,\n+\t\t      TensorInfo<T, IndexType> _cx,\n+\t\t      TensorInfo<T, IndexType> _cy,\n+\t\t      TensorInfo<T, IndexType> gradoutput,\n+\t\t      TensorInfo<T, IndexType> gradoutputcell,\n+\t\t      TensorInfo<T, IndexType> gradinput,\n+\t\t      IndexType hsz,\n+\t\t      IndexType totalElements)\n+{\n+  for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n+       linearIndex < totalElements;\n+       linearIndex += gridDim.x * blockDim.x) {\n+    IndexType offset = (linearIndex/hsz)*4*hsz+linearIndex%hsz;\n+\n+    T ig = DEVICE_LINEAR_GET(input, offset+0*hsz);\n+    T fg = DEVICE_LINEAR_GET(input, offset+1*hsz);\n+    T cg = DEVICE_LINEAR_GET(input, offset+2*hsz);\n+    T og = DEVICE_LINEAR_GET(input, offset+3*hsz);\n+\n+    T* ih = &DEVICE_LINEAR_GET(hidden, offset+0*hsz);\n+    T* fh = &DEVICE_LINEAR_GET(hidden, offset+1*hsz);\n+    T* ch = &DEVICE_LINEAR_GET(hidden, offset+2*hsz);\n+    T* oh = &DEVICE_LINEAR_GET(hidden, offset+3*hsz);\n+\n+    //will return hidden grads here\n+    T cx = DEVICE_LINEAR_GET(_cx, linearIndex);\n+    T cy = DEVICE_LINEAR_GET(_cy, linearIndex);\n+\n+    T* gi = &DEVICE_LINEAR_GET(gradinput, linearIndex);\n+\n+    T go = DEVICE_LINEAR_GET(gradoutput, linearIndex);\n+    T goc= DEVICE_LINEAR_GET(gradoutputcell, linearIndex);\n+#ifndef THC_REAL_IS_HALF\n+    T gcx = THCNumerics<T>::tanh(cy);\n+\n+    T gog = go * gcx;\n+    gcx = go * og * ( 1 - gcx*gcx) + goc;\n+\n+    T gig = gcx * cg;\n+    T gfg = gcx * cx;\n+    T gcg = gcx * ig;\n+\n+    gcx = gcx * fg;\n+\n+    gig = gig * (1-ig) * ig;\n+    gfg = gfg * (1-fg) * fg;\n+    gcg = gcg * (1-cg*cg);\n+    gog = gog * (1-og) * og;\n+\n+    *ih = gig;\n+    *fh = gfg;\n+    *ch = gcg;\n+    *oh = gog;\n+\n+    *gi = gcx;\n+#else\n+    float gcx = THCNumerics<float>::tanh(H2F(cy));\n+    float gog = H2F(go) * gcx;\n+    gcx = H2F(go) * H2F(og) * ( 1 - gcx*gcx) + H2F(goc);\n+\n+    float gcg = gcx * H2F(fg);\n+    float gfg = gcx * H2F(cg);\n+    float gig = gcx * H2F(cx);\n+\n+    gog = gog * ( (1-H2F(og))*H2F(og) );\n+    gcg = gcg * (1-H2F(cg)*H2F(cg));\n+    gfg = gfg * ( (1-H2F(fg))*H2F(fg) );\n+    gig = gig * ( (1-H2F(ig))*H2F(ig) );\n+\n+    *ih = F2H(gig);\n+    *fh = F2H(gfg);\n+    *ch = F2H(gcg);\n+    *oh = F2H(gog);\n+\n+    *gi = F2H(gcx);\n+#endif\n+  }\n+}\n+\n+// *********** START Generate specializations *************** //\n+#define EXPAND_FUNCTION(ITYPE, DIM)\t\t\t\t\t\\\n+  template __global__ void THNN_(GRUForward)<DATATYPE, ITYPE, DIM>\t\\\n+    (TensorInfo<DATATYPE, ITYPE> inputI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> hiddenI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> bias1I,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> bias2I,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> hxI,\t\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> hyI,\t\t\t\t\t\\\n+     ITYPE hsz,\t\t\t\t\t\t\t\t\\\n+     ITYPE totalElements);\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+  template void __global__ THNN_(GRUBackward)<DATATYPE, ITYPE, DIM>\t\\\n+    (TensorInfo<DATATYPE, ITYPE> inputI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> hiddenI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> gradoutputI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> gradinputI,\t\t\t\t\\\n+     ITYPE hsz,\t\t\t\t\t\t\t\t\\\n+     ITYPE totalElements);\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+  template void __global__ THNN_(LSTMForward)<DATATYPE, ITYPE, DIM>\t\\\n+    (TensorInfo<DATATYPE, ITYPE> inputI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> hiddenI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> bias1I,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> bias2I,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> cxI,\t\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> hyI,\t\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> cyI,\t\t\t\t\t\\\n+     ITYPE hsz,\t\t\t\t\t\t\t\t\\\n+     ITYPE totalElements);\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+  template void __global__ THNN_(LSTMBackward)<DATATYPE, ITYPE, DIM>\t\\\n+    (TensorInfo<DATATYPE, ITYPE> inputI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> hiddenI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> cxI,\t\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> cyI,\t\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> gradoutputI,\t\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> gradoutputcellI,\t\t\t\\\n+     TensorInfo<DATATYPE, ITYPE> gradinputI,\t\t\t\t\\\n+     ITYPE hsz,\t\t\t\t\t\t\t\t\\\n+     ITYPE totalElements);\t\t\t\t\t\t\\\n+\n+\n+#define EXPAND_DIM(ITYPE)\t\t\t\t\\\n+  EXPAND_FUNCTION(ITYPE, -2)\t\t\t\t\\\n+  EXPAND_FUNCTION(ITYPE, -1)                     \t\\\n+  EXPAND_FUNCTION(ITYPE, 1)                      \t\\\n+  EXPAND_FUNCTION(ITYPE, 2)                      \t\\\n+\n+\n+#define EXPAND_TYPE                                     \\\n+  EXPAND_DIM(unsigned int)\t\t\t\t\\\n+  EXPAND_DIM(unsigned long)\t\t\t\t\\\n+\n+\n+EXPAND_TYPE\n+\n+// ************ END generating specializations ************** //\n+\n+// ************ START Create actual function calls ********** //\n+#define FILL_FUNCTION(ITYPE, DIM, FUNCTION) FUNCTION(ITYPE, DIM)\n+\n+#define FILL_DIM(ITYPE, DIM, FUNCTION)\t\t\\\n+  switch (DIM) {\t\t\t\t\\\n+  case -2:\t\t\t\t\t\\\n+    FILL_FUNCTION(ITYPE, -2, FUNCTION);\t\t\\\n+    break;\t\t\t\t\t\\\n+  case 1:\t\t\t\t\t\\\n+    FILL_FUNCTION(ITYPE, 1, FUNCTION);\t\t\\\n+    break;\t\t\t\t\t\\\n+  case 2:\t\t\t\t\t\\\n+    FILL_FUNCTION(ITYPE, 2, FUNCTION);\t\t\\\n+    break;\t\t\t\t\t\\\n+  default:\t\t\t\t\t\\\n+    FILL_FUNCTION(ITYPE, -1, FUNCTION);\t\t\\\n+    break;\t\t\t\t\t\\\n+  }\n+\n+#define LSTM_FORWARD(ITYPE, DIM) THNN_(LSTMForward)\t\t\t\\\n+  <DATATYPE, ITYPE, DIM>\t\t\t\t\t\t\\\n+  <<<grid, block, 0, THCState_getCurrentStream(state)>>>\t\t\\\n+  (inputI, hiddenI,\t\t\t\t\t\t\t\\\n+   bias1I, bias2I, cxI, hyI, cyI,\t\t\t\t\t\\\n+   hid_size, totalElements);\n+  \n+#define LSTM_BACKWARD(ITYPE, DIM) THNN_(LSTMBackward)\t\t\t\\\n+  <DATATYPE, ITYPE, DIM>\t\t\t\t\t\t\\\n+  <<<grid, block, 0, THCState_getCurrentStream(state)>>>\t\t\\\n+  (inputI, hiddenI, cxI, cyI,\t\t\t\t\t\t\\\n+   gradoutI, gradoutcI, gradinI,\t\t\t\t\t\\\n+   hid_size, totalElements);\n+\n+#define GRU_FORWARD(ITYPE, DIM) THNN_(GRUForward)<DATATYPE, ITYPE, DIM>\t\\\n+  <<<grid, block, 0, THCState_getCurrentStream(state)>>>\t\t\\\n+  (inputI, hiddenI, bias1I, bias2I, hxI, hyI,\t\t\t\t\\\n+   hid_size, totalElements);\n+\n+#define GRU_BACKWARD(ITYPE, DIM) THNN_(GRUBackward)\t\t\t\\\n+  <DATATYPE, ITYPE, DIM>\t\t\t\t\t\t\\\n+  <<<grid, block, 0, THCState_getCurrentStream(state)>>>\t\t\\\n+  (inputI, hiddenI, gradoutI, gradinI, hid_size, totalElements);\t\n+\n+// ************ END Create actual function calls ************ //\n+\n+void THNN_(LSTMFused_updateOutput)(\n+          THCState *state,\n+          THCTensor *input,\n+\t  THCTensor *hidden,\n+\t  THCTensor *bias1,\n+\t  THCTensor *bias2,\n+\t  THCTensor *cx,\n+\t  THCTensor *hy,\n+\t  THCTensor *cy)\n+{\n+  THCTensor_(resizeAs)(state, hy, cx);\n+  THCTensor_(resizeAs)(state, cy, cx);\n+  THNN_(FusedRNNAssertSizes)(state, 4, 5, input, hidden, hy, cy, cx);\n+\n+  bool has_bias = (bias1!=NULL);\n+  bool canUse32bi;\n+  int maxDim;\n+\n+  if(has_bias){\n+    THCUNN_assertSameGPU(state, 7, input, hidden, bias1, bias2, hy, cy, cx);\n+    canUse32bi = THNN_(canUse32BitIndexMath)\n+      (state, 7, input, hidden, bias1, bias2, hy, cy, cx);\n+    maxDim = THNN_(minIndexType)\n+      (state, 7, input, hidden, bias1, bias2, hy, cy, cx);\n+\n+    TensorInfo<DATATYPE, unsigned long> tmphi =\n+      getTensorInfo<THCTensor, unsigned long>(state, cx);\n+\n+    unsigned long tmp_hid = tmphi.sizes[tmphi.dims-1];\n+\n+    THAssertMsg( tmp_hid*4 == THCTensor_(nElement)(state, bias1) &&\n+\t\t tmp_hid*4 == THCTensor_(nElement)(state, bias2),\n+\t\t \"Bias in pointwise operation is an incorrect size, must be 4 x feature size.\");\n+  }else{\n+    THCUNN_assertSameGPU(state, 5, input, hidden, hy, cy, cx);\n+    canUse32bi = THNN_(canUse32BitIndexMath)\n+      (state, 5, input, hidden, hy, cy, cx);\n+    maxDim = THNN_(minIndexType)\n+      (state, 5, input, hidden, hy, cy, cx);\n+  }\n+\n+  const dim3 block = getApplyBlock();\n+  //const dim3 block(32, 32);\n+  ptrdiff_t totalElements = TensorUtils<THCTensor>::getNumElements(state, cx);\n+\n+  dim3 grid;\n+ \n+  THAssertMsg(getApplyGrid(state, totalElements, grid),\n+\t      \"Could not get grid size for pointwise apply\");\n+  if(canUse32bi){\n+    TensorInfo<DATATYPE, unsigned int> inputI =\n+      getTensorInfo<THCTensor, unsigned int>(state, input);\n+    TensorInfo<DATATYPE, unsigned int> hiddenI =\n+      getTensorInfo<THCTensor, unsigned int>(state, hidden);\n+    TensorInfo<DATATYPE, unsigned int> cxI =\n+      getTensorInfo<THCTensor, unsigned int>(state, cx);\n+    TensorInfo<DATATYPE, unsigned int> hyI =\n+      getTensorInfo<THCTensor, unsigned int>(state, hy);\n+    TensorInfo<DATATYPE, unsigned int> cyI =\n+      getTensorInfo<THCTensor, unsigned int>(state, cy);\n+\n+    unsigned int hid_size = cxI.sizes[cxI.dims-1];\n+\n+    inputI.collapseDims();\n+    hiddenI.collapseDims();\n+    cxI.collapseDims();\n+    hyI.collapseDims();\n+    cyI.collapseDims();\n+\n+    unsigned int zero[1] = {0};\n+    TensorInfo<DATATYPE, unsigned int> nullinfo =\n+      TensorInfo<DATATYPE, unsigned int>(NULL, 1, zero, zero);\n+    TensorInfo<DATATYPE, unsigned int> bias1I = nullinfo;\n+    TensorInfo<DATATYPE, unsigned int> bias2I = nullinfo;\n+\n+    if(has_bias){\n+      bias1I = getTensorInfo<THCTensor, unsigned int>(state, bias1);\n+      bias2I = getTensorInfo<THCTensor, unsigned int>(state, bias2);\n+      bias1I.collapseDims();\n+      bias2I.collapseDims();\n+    }\n+\n+    FILL_DIM(unsigned int, maxDim, LSTM_FORWARD);\n+\n+  }else{\n+\n+    TensorInfo<DATATYPE, unsigned long> inputI =\n+      getTensorInfo<THCTensor, unsigned long>(state, input);\n+    TensorInfo<DATATYPE, unsigned long> hiddenI =\n+      getTensorInfo<THCTensor, unsigned long>(state, hidden);\n+    TensorInfo<DATATYPE, unsigned long> cxI =\n+      getTensorInfo<THCTensor, unsigned long>(state, cx);\n+    TensorInfo<DATATYPE, unsigned long> hyI =\n+      getTensorInfo<THCTensor, unsigned long>(state, hy);\n+    TensorInfo<DATATYPE, unsigned long> cyI =\n+      getTensorInfo<THCTensor, unsigned long>(state, cy);\n+\n+    unsigned long hid_size = cxI.sizes[cxI.dims-1];\n+\n+    inputI.collapseDims();\n+    hiddenI.collapseDims();\n+    cxI.collapseDims();\n+    hyI.collapseDims();\n+    cyI.collapseDims();\n+\n+    unsigned long zero[1] = {0};\n+    TensorInfo<DATATYPE, unsigned long> nullinfo =\n+      TensorInfo<DATATYPE, unsigned long>(NULL, 1, zero, zero);\n+    TensorInfo<DATATYPE, unsigned long> bias1I = nullinfo;\n+    TensorInfo<DATATYPE, unsigned long> bias2I = nullinfo;\n+\n+    if(has_bias){\n+      bias1I = getTensorInfo<THCTensor, unsigned long>(state, bias1);\n+      bias2I = getTensorInfo<THCTensor, unsigned long>(state, bias2);\n+      bias1I.collapseDims();\n+      bias2I.collapseDims();\n+    }\n+\n+    FILL_DIM(unsigned long, maxDim, LSTM_FORWARD);\n+  }\n+  THCudaCheck(cudaGetLastError());\n+}\n+\n+void THNN_(LSTMFused_updateGradInput)(\n+          THCState *state,\n+          THCTensor *input,\n+          THCTensor *hidden,\n+\t  THCTensor *cx,\n+\t  THCTensor *cy,\n+          THCTensor *gradOutput,\n+          THCTensor *gradOutputCell,\n+          THCTensor *gradInput)\n+{\n+  THCTensor_(resizeAs)(state, gradInput, gradOutput);\n+  THCUNN_assertSameGPU(state, 7, input, hidden, cx, cy,\n+\t\t       gradOutput, gradOutputCell, gradInput);\n+  THNN_(FusedRNNAssertSizes)\n+    (state, 4, 7, input, hidden, cx, cy,\n+     gradOutput, gradOutputCell, gradInput);\n+  bool canUse32bi = THNN_(canUse32BitIndexMath)\n+    (state, 7, input, hidden, cx, cy,\n+     gradOutput, gradOutputCell, gradInput);\n+  int maxDim = THNN_(minIndexType)\n+    (state, 7, input, hidden, cx, cy,\n+     gradOutput, gradOutputCell, gradInput);\n+\n+  const dim3 block = getApplyBlock();\n+\n+  ptrdiff_t totalElements = TensorUtils<THCTensor>::getNumElements(state, gradOutput);\n+\n+  dim3 grid;\n+\n+  THAssertMsg(getApplyGrid(state, totalElements, grid),\n+\t      \"Could not get grid size for pointwise apply\");\n+\n+  if(canUse32bi){\n+    TensorInfo<DATATYPE, unsigned int> inputI =\n+      getTensorInfo<THCTensor, unsigned int>(state, input);\n+    TensorInfo<DATATYPE, unsigned int> hiddenI =\n+      getTensorInfo<THCTensor, unsigned int>(state, hidden);\n+    TensorInfo<DATATYPE, unsigned int> cxI =\n+      getTensorInfo<THCTensor, unsigned int>(state, cx);\n+    TensorInfo<DATATYPE, unsigned int> cyI =\n+      getTensorInfo<THCTensor, unsigned int>(state, cy);\n+    TensorInfo<DATATYPE, unsigned int> gradoutI =\n+      getTensorInfo<THCTensor, unsigned int>(state, gradOutput);\n+    TensorInfo<DATATYPE, unsigned int> gradoutcI =\n+      getTensorInfo<THCTensor, unsigned int>(state, gradOutputCell);\n+    TensorInfo<DATATYPE, unsigned int> gradinI =\n+      getTensorInfo<THCTensor, unsigned int>(state, gradInput);\n+\n+    unsigned int hid_size = gradoutI.sizes[gradoutI.dims-1];\n+\n+    inputI.collapseDims();\n+    hiddenI.collapseDims();\n+    cxI.collapseDims();\n+    cyI.collapseDims();\n+    gradoutI.collapseDims();\n+    gradoutcI.collapseDims();\n+    gradinI.collapseDims();\n+\n+    FILL_DIM(unsigned int, maxDim, LSTM_BACKWARD);\n+\n+  }else{\n+    TensorInfo<DATATYPE, unsigned long> inputI =\n+      getTensorInfo<THCTensor, unsigned long>(state, input);\n+    TensorInfo<DATATYPE, unsigned long> hiddenI =\n+      getTensorInfo<THCTensor, unsigned long>(state, hidden);\n+    TensorInfo<DATATYPE, unsigned long> cxI =\n+      getTensorInfo<THCTensor, unsigned long>(state, cx);\n+    TensorInfo<DATATYPE, unsigned long> cyI =\n+      getTensorInfo<THCTensor, unsigned long>(state, cy);\n+    TensorInfo<DATATYPE, unsigned long> gradoutI =\n+      getTensorInfo<THCTensor, unsigned long>(state, gradOutput);\n+    TensorInfo<DATATYPE, unsigned long> gradoutcI =\n+      getTensorInfo<THCTensor, unsigned long>(state, gradOutputCell);\n+    TensorInfo<DATATYPE, unsigned long> gradinI =\n+      getTensorInfo<THCTensor, unsigned long>(state, gradInput);\n+\n+    unsigned long hid_size = gradoutI.sizes[gradoutI.dims-1];\n+\n+    inputI.collapseDims();\n+    hiddenI.collapseDims();\n+    cxI.collapseDims();\n+    cyI.collapseDims();\n+    gradoutI.collapseDims();\n+    gradoutcI.collapseDims();\n+    gradinI.collapseDims();\n+\n+    FILL_DIM(unsigned long, maxDim, LSTM_BACKWARD);\n+\n+  }\n+  THCudaCheck(cudaGetLastError());\n+}\n+\n+void THNN_(GRUFused_updateOutput)(\n+          THCState *state,\n+          THCTensor *input,\n+\t  THCTensor *hidden,\n+\t  THCTensor *bias1,\n+\t  THCTensor *bias2,\n+\t  THCTensor *hx,\n+\t  THCTensor *hy)\n+{\n+  THCTensor_(resizeAs)(state, hy, hx);\n+  THNN_(FusedRNNAssertSizes)(state, 3, 4, input, hidden, hx, hy);\n+\n+  bool has_bias = (bias1!=NULL);\n+  bool canUse32bi;\n+  int maxDim;\n+\n+  if(has_bias){\n+\n+    THCUNN_assertSameGPU\n+      (state, 6, input, hidden, hx, hy, bias1, bias2);\n+    canUse32bi = THNN_(canUse32BitIndexMath)\n+      (state, 6, input, hidden, hx, hy, bias1, bias2);\n+    maxDim = THNN_(minIndexType)\n+      (state, 6, input, hidden, hx, hy, bias1, bias2);\n+\n+    TensorInfo<DATATYPE, unsigned long> tmphi =\n+      getTensorInfo<THCTensor, unsigned long>(state, hx);\n+    unsigned long tmp_hid = tmphi.sizes[tmphi.dims-1];\n+    \n+    THAssertMsg( tmp_hid*3 == THCTensor_(nElement)(state, bias1) &&\n+\t\t tmp_hid*3 == THCTensor_(nElement)(state, bias2),\n+\t\t \"Bias in pointwise operation is an incorrect size, must be 3 x feature size.\");\n+  }else{\n+    THCUNN_assertSameGPU\n+      (state, 4, input, hidden, hx, hy);\n+    canUse32bi = THNN_(canUse32BitIndexMath)\n+      (state, 4, input, hidden, hx, hy);\n+    maxDim = THNN_(minIndexType)\n+      (state, 4, input, hidden, hx, hy);\n+  }\n+\n+  const dim3 block = getApplyBlock();\n+  //const dim3 block(32, 32);\n+  ptrdiff_t totalElements = TensorUtils<THCTensor>::getNumElements(state, hx);\n+\n+  dim3 grid;\n+\n+  THAssertMsg(getApplyGrid(state, totalElements, grid),\n+\t      \"Could not get grid size for pointwise apply\");\n+  if(canUse32bi){\n+    TensorInfo<DATATYPE, unsigned int> inputI =\n+      getTensorInfo<THCTensor, unsigned int>(state, input);\n+    TensorInfo<DATATYPE, unsigned int> hiddenI =\n+      getTensorInfo<THCTensor, unsigned int>(state, hidden);\n+    TensorInfo<DATATYPE, unsigned int> hxI =\n+      getTensorInfo<THCTensor, unsigned int>(state, hx);\n+    TensorInfo<DATATYPE, unsigned int> hyI =\n+      getTensorInfo<THCTensor, unsigned int>(state, hy);\n+\n+    unsigned int hid_size = hxI.sizes[hxI.dims-1];\n+\n+    inputI.collapseDims();\n+    hiddenI.collapseDims();\n+    hyI.collapseDims();\n+    hxI.collapseDims();\n+\n+    unsigned int zero[1] = {0};\n+    TensorInfo<DATATYPE, unsigned int> nullinfo =\n+      TensorInfo<DATATYPE, unsigned int>(NULL, 1, zero, zero);\n+    TensorInfo<DATATYPE, unsigned int> bias1I = nullinfo;\n+    TensorInfo<DATATYPE, unsigned int> bias2I = nullinfo;\n+\n+    if(has_bias){\n+      bias1I = getTensorInfo<THCTensor, unsigned int>(state, bias1);\n+      bias2I = getTensorInfo<THCTensor, unsigned int>(state, bias2);\n+      bias1I.collapseDims();\n+      bias2I.collapseDims();\n+    }\n+\n+    FILL_DIM(unsigned int, maxDim, GRU_FORWARD);\n+      \n+  }else{\n+\n+    TensorInfo<DATATYPE, unsigned long> inputI =", "path": "torch/lib/THCUNN/generic/FusedRNNKernel.cu", "position": null, "original_position": 852, "commit_id": "4b269de7bd2dc272edfb456696a1552bf575bace", "original_commit_id": "c058a4c0cceccc54a2e0cdf67ecfe47b90bb0d3d", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "There's a lot of duplication in these cases (in all top level functions), and it seems that the only difference is `unsigned long` vs `unsigned int`. Why not separate them into a templated functions and instantiate different versions in each case?", "created_at": "2017-04-06T09:07:14Z", "updated_at": "2018-11-23T15:33:02Z", "html_url": "https://github.com/pytorch/pytorch/pull/1119#discussion_r110112226", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/1119", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/110112226"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/1119#discussion_r110112226"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/1119"}}, "body_html": "<p>There's a lot of duplication in these cases (in all top level functions), and it seems that the only difference is <code>unsigned long</code> vs <code>unsigned int</code>. Why not separate them into a templated functions and instantiate different versions in each case?</p>", "body_text": "There's a lot of duplication in these cases (in all top level functions), and it seems that the only difference is unsigned long vs unsigned int. Why not separate them into a templated functions and instantiate different versions in each case?"}