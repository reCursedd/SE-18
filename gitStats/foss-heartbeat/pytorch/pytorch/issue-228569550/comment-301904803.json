{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/301904803", "html_url": "https://github.com/pytorch/pytorch/issues/1552#issuecomment-301904803", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1552", "id": 301904803, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTkwNDgwMw==", "user": {"login": "lijunzh", "id": 9093413, "node_id": "MDQ6VXNlcjkwOTM0MTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/9093413?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lijunzh", "html_url": "https://github.com/lijunzh", "followers_url": "https://api.github.com/users/lijunzh/followers", "following_url": "https://api.github.com/users/lijunzh/following{/other_user}", "gists_url": "https://api.github.com/users/lijunzh/gists{/gist_id}", "starred_url": "https://api.github.com/users/lijunzh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lijunzh/subscriptions", "organizations_url": "https://api.github.com/users/lijunzh/orgs", "repos_url": "https://api.github.com/users/lijunzh/repos", "events_url": "https://api.github.com/users/lijunzh/events{/privacy}", "received_events_url": "https://api.github.com/users/lijunzh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-16T20:25:29Z", "updated_at": "2017-05-16T20:25:29Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10166968\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kvrd18\">@kvrd18</a> Thanks a lot for answering my question. I didn't made it clear in my original post. I actually only need to repeatedly shift the Tensor to some off-grid point. For example, given 1-D signal sampled at integer point (0, 1, 2, etc.), we want the interpolated values at non-integer points (0.2, 1.4, 2.9, etc). This is an interpolation problem in general.</p>\n<p><a href=\"http://pytorch.org/docs/nn#torch.nn.UpsamplingBilinear2d\" rel=\"nofollow\">UpSampling</a> should solve the problem with maybe an unnecessary cost. One can easily upsample the tensor high enough and pick the sample point as needed. However, it will be nice if the those <strong>Interpolators</strong> (nearest, linear, bilinear, cubic spline etc.) are available directly in the PyTorch API just like what's offered in <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.interp.html\" rel=\"nofollow\">Numpy</a>/<a href=\"https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html\" rel=\"nofollow\">Scipy</a> and <a href=\"https://www.mathworks.com/help/matlab/interpolation.html;jsessionid=2e3857153578f1c032555a80441c\" rel=\"nofollow\">MATLAB</a>. Maybe it does not make too much sense for neural networks to have this kind of functions, but it will help when you use PyTorch as a general numpy GPU support.</p>\n<p>I do have some ideas about implementing this and would like to ask if people in PyTorch community if it is a good idea to include those features. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a>  Forgive me to tag you guys directly if you are not interested in this, but I wanna hear your opinions. Do you guys think PyTorch should support more Numpy features and make it more of a general purpose GPU computing module or it mostly intend to be a neural network tool that complete with <a href=\"https://www.tensorflow.org\" rel=\"nofollow\">TensorFlow</a> with dynamic computational graph.  Thanks a lot.</p>", "body_text": "@kvrd18 Thanks a lot for answering my question. I didn't made it clear in my original post. I actually only need to repeatedly shift the Tensor to some off-grid point. For example, given 1-D signal sampled at integer point (0, 1, 2, etc.), we want the interpolated values at non-integer points (0.2, 1.4, 2.9, etc). This is an interpolation problem in general.\nUpSampling should solve the problem with maybe an unnecessary cost. One can easily upsample the tensor high enough and pick the sample point as needed. However, it will be nice if the those Interpolators (nearest, linear, bilinear, cubic spline etc.) are available directly in the PyTorch API just like what's offered in Numpy/Scipy and MATLAB. Maybe it does not make too much sense for neural networks to have this kind of functions, but it will help when you use PyTorch as a general numpy GPU support.\nI do have some ideas about implementing this and would like to ask if people in PyTorch community if it is a good idea to include those features. @apaszke and @soumith  Forgive me to tag you guys directly if you are not interested in this, but I wanna hear your opinions. Do you guys think PyTorch should support more Numpy features and make it more of a general purpose GPU computing module or it mostly intend to be a neural network tool that complete with TensorFlow with dynamic computational graph.  Thanks a lot.", "body": "@kvrd18 Thanks a lot for answering my question. I didn't made it clear in my original post. I actually only need to repeatedly shift the Tensor to some off-grid point. For example, given 1-D signal sampled at integer point (0, 1, 2, etc.), we want the interpolated values at non-integer points (0.2, 1.4, 2.9, etc). This is an interpolation problem in general. \r\n\r\n[UpSampling](http://pytorch.org/docs/nn#torch.nn.UpsamplingBilinear2d) should solve the problem with maybe an unnecessary cost. One can easily upsample the tensor high enough and pick the sample point as needed. However, it will be nice if the those **Interpolators** (nearest, linear, bilinear, cubic spline etc.) are available directly in the PyTorch API just like what's offered in [Numpy](https://docs.scipy.org/doc/numpy/reference/generated/numpy.interp.html)/[Scipy](https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html) and [MATLAB](https://www.mathworks.com/help/matlab/interpolation.html;jsessionid=2e3857153578f1c032555a80441c). Maybe it does not make too much sense for neural networks to have this kind of functions, but it will help when you use PyTorch as a general numpy GPU support. \r\n\r\nI do have some ideas about implementing this and would like to ask if people in PyTorch community if it is a good idea to include those features. @apaszke and @soumith  Forgive me to tag you guys directly if you are not interested in this, but I wanna hear your opinions. Do you guys think PyTorch should support more Numpy features and make it more of a general purpose GPU computing module or it mostly intend to be a neural network tool that complete with [TensorFlow](https://www.tensorflow.org) with dynamic computational graph.  Thanks a lot.\r\n\r\n"}