{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12817", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12817/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12817/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12817/events", "html_url": "https://github.com/pytorch/pytorch/issues/12817", "id": 371471365, "node_id": "MDU6SXNzdWUzNzE0NzEzNjU=", "number": 12817, "title": "`CUDNN_STATUS_BAD_PARAM` reported during traced model execution", "user": {"login": "jatentaki", "id": 22394841, "node_id": "MDQ6VXNlcjIyMzk0ODQx", "avatar_url": "https://avatars1.githubusercontent.com/u/22394841?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jatentaki", "html_url": "https://github.com/jatentaki", "followers_url": "https://api.github.com/users/jatentaki/followers", "following_url": "https://api.github.com/users/jatentaki/following{/other_user}", "gists_url": "https://api.github.com/users/jatentaki/gists{/gist_id}", "starred_url": "https://api.github.com/users/jatentaki/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jatentaki/subscriptions", "organizations_url": "https://api.github.com/users/jatentaki/orgs", "repos_url": "https://api.github.com/users/jatentaki/repos", "events_url": "https://api.github.com/users/jatentaki/events{/privacy}", "received_events_url": "https://api.github.com/users/jatentaki/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-10-18T10:49:39Z", "updated_at": "2018-10-26T15:56:39Z", "closed_at": null, "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"bug\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f41b.png\">\ud83d\udc1b</g-emoji> Bug</h2>\n\n<p>I have a model which behaves correctly \"as it is\" and during tracing, but the traced one fails during convolution. The backtrace is as follows:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/torch_localize-0.0.1-py3.6.egg/torch_localize/localize.py\", line 14, in wrapped\n    return method(self, *args, **kwargs)\n  File \"unet.py\", line 274, in forward\n    y = self.conv1(y)\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 475, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 465, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/se3cnn-0.0.0-py3.6.egg/se3cnn/batchnorm.py\", line 274, in forward\n    return torch.nn.functional.conv3d(input, kernel, bias=bias, **self.kwargs)\nRuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM\n</code></pre>\n<p>During tracing <code>self.kwargs</code> is consistently empty so this is surprising. I still believe this is my programming error, but the runtime error definitely could be more telling (what kind of <code>BAD_PARAM</code>?).</p>\n<h2>To Reproduce</h2>\n<p>Unfortunately I couldn't boil the offending code down to a reasonable size, but it is pretty clear that there is just missing error handling hook which would translate the <code>cuDNN</code> runtime errors into something more human-digestible.</p>\n<h2>Environment</h2>\n<p>PyTorch version: 1.0.0.dev20181015<br>\nIs debug build: No<br>\nCUDA used to build PyTorch: 9.0.176</p>\n<p>OS: Ubuntu 16.04.4 LTS<br>\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609<br>\nCMake version: version 3.5.1</p>\n<p>Python version: 3.6<br>\nIs CUDA available: Yes<br>\nCUDA runtime version: 9.0.176<br>\nGPU models and configuration:<br>\nGPU 0: TITAN X (Pascal)<br>\nGPU 1: TITAN X (Pascal)<br>\nGPU 2: TITAN X (Pascal)<br>\nGPU 3: TITAN X (Pascal)</p>\n<p>Nvidia driver version: 390.87<br>\ncuDNN version: Probably one of the following:<br>\n/usr/local/MATLAB/R2018a/bin/glnxa64/libcudnn.so.7.0.3</p>\n<p>Versions of relevant libraries:<br>\n[pip] numpy (1.15.2)<br>\n[pip] torch (1.0.0.dev20181015)<br>\n[pip] torch-localize (0.0.1)<br>\n[pip] torchvision (0.2.1)<br>\n[conda] pytorch-nightly           1.0.0.dev20181015 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch<br>\n[conda] torchvision               0.2.1                     </p>", "body_text": "\ud83d\udc1b Bug\n\nI have a model which behaves correctly \"as it is\" and during tracing, but the traced one fails during convolution. The backtrace is as follows:\nTraceback (most recent call last):\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/torch_localize-0.0.1-py3.6.egg/torch_localize/localize.py\", line 14, in wrapped\n    return method(self, *args, **kwargs)\n  File \"unet.py\", line 274, in forward\n    y = self.conv1(y)\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 475, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 465, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/se3cnn-0.0.0-py3.6.egg/se3cnn/batchnorm.py\", line 274, in forward\n    return torch.nn.functional.conv3d(input, kernel, bias=bias, **self.kwargs)\nRuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM\n\nDuring tracing self.kwargs is consistently empty so this is surprising. I still believe this is my programming error, but the runtime error definitely could be more telling (what kind of BAD_PARAM?).\nTo Reproduce\nUnfortunately I couldn't boil the offending code down to a reasonable size, but it is pretty clear that there is just missing error handling hook which would translate the cuDNN runtime errors into something more human-digestible.\nEnvironment\nPyTorch version: 1.0.0.dev20181015\nIs debug build: No\nCUDA used to build PyTorch: 9.0.176\nOS: Ubuntu 16.04.4 LTS\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCMake version: version 3.5.1\nPython version: 3.6\nIs CUDA available: Yes\nCUDA runtime version: 9.0.176\nGPU models and configuration:\nGPU 0: TITAN X (Pascal)\nGPU 1: TITAN X (Pascal)\nGPU 2: TITAN X (Pascal)\nGPU 3: TITAN X (Pascal)\nNvidia driver version: 390.87\ncuDNN version: Probably one of the following:\n/usr/local/MATLAB/R2018a/bin/glnxa64/libcudnn.so.7.0.3\nVersions of relevant libraries:\n[pip] numpy (1.15.2)\n[pip] torch (1.0.0.dev20181015)\n[pip] torch-localize (0.0.1)\n[pip] torchvision (0.2.1)\n[conda] pytorch-nightly           1.0.0.dev20181015 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\n[conda] torchvision               0.2.1", "body": "## \ud83d\udc1b Bug\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nI have a model which behaves correctly \"as it is\" and during tracing, but the traced one fails during convolution. The backtrace is as follows:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/torch_localize-0.0.1-py3.6.egg/torch_localize/localize.py\", line 14, in wrapped\r\n    return method(self, *args, **kwargs)\r\n  File \"unet.py\", line 274, in forward\r\n    y = self.conv1(y)\r\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 475, in __call__\r\n    result = self._slow_forward(*input, **kwargs)\r\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 465, in _slow_forward\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/cvlabdata2/home/tyszkiew/miniconda3/envs/se3use/lib/python3.6/site-packages/se3cnn-0.0.0-py3.6.egg/se3cnn/batchnorm.py\", line 274, in forward\r\n    return torch.nn.functional.conv3d(input, kernel, bias=bias, **self.kwargs)\r\nRuntimeError: cuDNN error: CUDNN_STATUS_BAD_PARAM\r\n```\r\nDuring tracing `self.kwargs` is consistently empty so this is surprising. I still believe this is my programming error, but the runtime error definitely could be more telling (what kind of `BAD_PARAM`?).\r\n## To Reproduce\r\n\r\nUnfortunately I couldn't boil the offending code down to a reasonable size, but it is pretty clear that there is just missing error handling hook which would translate the `cuDNN` runtime errors into something more human-digestible.\r\n\r\n## Environment\r\n\r\nPyTorch version: 1.0.0.dev20181015\r\nIs debug build: No\r\nCUDA used to build PyTorch: 9.0.176\r\n\r\nOS: Ubuntu 16.04.4 LTS\r\nGCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCMake version: version 3.5.1\r\n\r\nPython version: 3.6\r\nIs CUDA available: Yes\r\nCUDA runtime version: 9.0.176\r\nGPU models and configuration: \r\nGPU 0: TITAN X (Pascal)\r\nGPU 1: TITAN X (Pascal)\r\nGPU 2: TITAN X (Pascal)\r\nGPU 3: TITAN X (Pascal)\r\n\r\nNvidia driver version: 390.87\r\ncuDNN version: Probably one of the following:\r\n/usr/local/MATLAB/R2018a/bin/glnxa64/libcudnn.so.7.0.3\r\n\r\nVersions of relevant libraries:\r\n[pip] numpy (1.15.2)\r\n[pip] torch (1.0.0.dev20181015)\r\n[pip] torch-localize (0.0.1)\r\n[pip] torchvision (0.2.1)\r\n[conda] pytorch-nightly           1.0.0.dev20181015 py3.6_cuda9.0.176_cudnn7.1.2_0    pytorch\r\n[conda] torchvision               0.2.1                     <pip>\r\n"}