{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/248483559", "html_url": "https://github.com/pytorch/pytorch/issues/43#issuecomment-248483559", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/43", "id": 248483559, "node_id": "MDEyOklzc3VlQ29tbWVudDI0ODQ4MzU1OQ==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-21T01:18:19Z", "updated_at": "2016-09-21T01:18:19Z", "author_association": "MEMBER", "body_html": "<p>Another weird thing is that Conv2d defaults to expect a FloatTensor, but LogSoftmax defaults to expecting a double tensor?<br>\nThis works fine:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> autograd.Variable(torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">5</span>))\nm <span class=\"pl-k\">=</span> nn.LogSoftmax()\nm(<span class=\"pl-c1\">input</span>)</pre></div>\n<p>But this errors out</p>\n<div class=\"highlight highlight-source-python\"><pre>m <span class=\"pl-k\">=</span> nn.Conv2d(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">32</span>, (<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>))\n<span class=\"pl-c1\">input</span> <span class=\"pl-k\">=</span> autograd.Variable(torch.randn(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">10</span>))\nm(<span class=\"pl-c1\">input</span>)</pre></div>", "body_text": "Another weird thing is that Conv2d defaults to expect a FloatTensor, but LogSoftmax defaults to expecting a double tensor?\nThis works fine:\ninput = autograd.Variable(torch.randn(3, 5))\nm = nn.LogSoftmax()\nm(input)\nBut this errors out\nm = nn.Conv2d(16, 32, (3, 3))\ninput = autograd.Variable(torch.randn(3, 16, 10, 10))\nm(input)", "body": "Another weird thing is that Conv2d defaults to expect a FloatTensor, but LogSoftmax defaults to expecting a double tensor?\nThis works fine:\n\n``` python\ninput = autograd.Variable(torch.randn(3, 5))\nm = nn.LogSoftmax()\nm(input)\n```\n\nBut this errors out\n\n``` python\nm = nn.Conv2d(16, 32, (3, 3))\ninput = autograd.Variable(torch.randn(3, 16, 10, 10))\nm(input)\n```\n"}