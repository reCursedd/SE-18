{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/424820689", "html_url": "https://github.com/pytorch/pytorch/issues/9170#issuecomment-424820689", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9170", "id": 424820689, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNDgyMDY4OQ==", "user": {"login": "itaiyesh", "id": 18677140, "node_id": "MDQ6VXNlcjE4Njc3MTQw", "avatar_url": "https://avatars3.githubusercontent.com/u/18677140?v=4", "gravatar_id": "", "url": "https://api.github.com/users/itaiyesh", "html_url": "https://github.com/itaiyesh", "followers_url": "https://api.github.com/users/itaiyesh/followers", "following_url": "https://api.github.com/users/itaiyesh/following{/other_user}", "gists_url": "https://api.github.com/users/itaiyesh/gists{/gist_id}", "starred_url": "https://api.github.com/users/itaiyesh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/itaiyesh/subscriptions", "organizations_url": "https://api.github.com/users/itaiyesh/orgs", "repos_url": "https://api.github.com/users/itaiyesh/repos", "events_url": "https://api.github.com/users/itaiyesh/events{/privacy}", "received_events_url": "https://api.github.com/users/itaiyesh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-26T18:24:20Z", "updated_at": "2018-09-26T18:24:20Z", "author_association": "NONE", "body_html": "<p>Also take into consideration that training on larger batch sizes results in worse generalization. If your training on various batch sizes results in sort of the same loss, but accuracy differs drastically - it could be attributed to this fact.</p>", "body_text": "Also take into consideration that training on larger batch sizes results in worse generalization. If your training on various batch sizes results in sort of the same loss, but accuracy differs drastically - it could be attributed to this fact.", "body": "Also take into consideration that training on larger batch sizes results in worse generalization. If your training on various batch sizes results in sort of the same loss, but accuracy differs drastically - it could be attributed to this fact."}