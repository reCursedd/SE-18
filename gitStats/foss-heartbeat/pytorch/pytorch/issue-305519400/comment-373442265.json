{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/373442265", "html_url": "https://github.com/pytorch/pytorch/issues/5807#issuecomment-373442265", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5807", "id": 373442265, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzQ0MjI2NQ==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-15T16:40:13Z", "updated_at": "2018-03-15T16:40:35Z", "author_association": "MEMBER", "body_html": "<p>Can you run:</p>\n<pre><code>strace python -c \"import torch; torch.cuda.is_available()\" 2&gt;stderr.log\n</code></pre>\n<p>And post stderr.log (to a gist or something)</p>\n<p>This will trace which system calls are being made. My suspicion is that CUDA is JIT-compiling the CUDA kernels. We try to avoid the runtime compilation by distributing kernels compiled for recent architectures (including Pascal), but it might not be fully working in 0.3.0.post4. JIT-compilation can be quite slow, especially if your nv cache directory (default: <code>~/.nv/ComputeCache</code>) is on network storage.</p>\n<p>You may also want to try PyTorch 0.3.1</p>", "body_text": "Can you run:\nstrace python -c \"import torch; torch.cuda.is_available()\" 2>stderr.log\n\nAnd post stderr.log (to a gist or something)\nThis will trace which system calls are being made. My suspicion is that CUDA is JIT-compiling the CUDA kernels. We try to avoid the runtime compilation by distributing kernels compiled for recent architectures (including Pascal), but it might not be fully working in 0.3.0.post4. JIT-compilation can be quite slow, especially if your nv cache directory (default: ~/.nv/ComputeCache) is on network storage.\nYou may also want to try PyTorch 0.3.1", "body": "Can you run:\r\n\r\n```\r\nstrace python -c \"import torch; torch.cuda.is_available()\" 2>stderr.log\r\n```\r\n\r\nAnd post stderr.log (to a gist or something)\r\n\r\nThis will trace which system calls are being made. My suspicion is that CUDA is JIT-compiling the CUDA kernels. We try to avoid the runtime compilation by distributing kernels compiled for recent architectures (including Pascal), but it might not be fully working in 0.3.0.post4. JIT-compilation can be quite slow, especially if your nv cache directory (default: `~/.nv/ComputeCache`) is on network storage.\r\n\r\nYou may also want to try PyTorch 0.3.1\r\n\r\n"}