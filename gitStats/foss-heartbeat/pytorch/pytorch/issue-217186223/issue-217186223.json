{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1112", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1112/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1112/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1112/events", "html_url": "https://github.com/pytorch/pytorch/issues/1112", "id": 217186223, "node_id": "MDU6SXNzdWUyMTcxODYyMjM=", "number": 1112, "title": "Feature Request: Add API to anneal LR and WD of optimizer", "user": {"login": "IanChen83", "id": 5383244, "node_id": "MDQ6VXNlcjUzODMyNDQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/5383244?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IanChen83", "html_url": "https://github.com/IanChen83", "followers_url": "https://api.github.com/users/IanChen83/followers", "following_url": "https://api.github.com/users/IanChen83/following{/other_user}", "gists_url": "https://api.github.com/users/IanChen83/gists{/gist_id}", "starred_url": "https://api.github.com/users/IanChen83/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IanChen83/subscriptions", "organizations_url": "https://api.github.com/users/IanChen83/orgs", "repos_url": "https://api.github.com/users/IanChen83/repos", "events_url": "https://api.github.com/users/IanChen83/events{/privacy}", "received_events_url": "https://api.github.com/users/IanChen83/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131849, "node_id": "MDU6TGFiZWw0MjQxMzE4NDk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/enhancement", "name": "enhancement", "color": "84b6eb", "default": true}, {"id": 443484050, "node_id": "MDU6TGFiZWw0NDM0ODQwNTA=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/medium%20priority", "name": "medium priority", "color": "fbca04", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-27T09:29:06Z", "updated_at": "2017-04-02T19:31:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>In <a href=\"https://github.com/pytorch/examples/blob/master/imagenet/main.py#L266-L270\">PyTorch ImageNet example</a>, the approach to anneal LR is to change internal parameters, which is defined in <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py#L28\">here</a>. An API to handle hyper parameter state might be better.</p>\n<p>Possible solutions might be</p>\n<ol>\n<li>A member function to directly change internal members</li>\n<li>A annealing scheme function <code>(epoch) -&gt; {lr, wd, ...}</code> as an argument when initializing the opimizers</li>\n</ol>", "body_text": "In PyTorch ImageNet example, the approach to anneal LR is to change internal parameters, which is defined in here. An API to handle hyper parameter state might be better.\nPossible solutions might be\n\nA member function to directly change internal members\nA annealing scheme function (epoch) -> {lr, wd, ...} as an argument when initializing the opimizers", "body": "In [PyTorch ImageNet example](https://github.com/pytorch/examples/blob/master/imagenet/main.py#L266-L270), the approach to anneal LR is to change internal parameters, which is defined in [here](https://github.com/pytorch/pytorch/blob/master/torch/optim/optimizer.py#L28). An API to handle hyper parameter state might be better.\r\n\r\nPossible solutions might be\r\n1. A member function to directly change internal members\r\n2. A annealing scheme function `(epoch) -> {lr, wd, ...}` as an argument when initializing the opimizers"}