{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/403770305", "html_url": "https://github.com/pytorch/pytorch/issues/5660#issuecomment-403770305", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5660", "id": 403770305, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMzc3MDMwNQ==", "user": {"login": "chuong98", "id": 15174756, "node_id": "MDQ6VXNlcjE1MTc0NzU2", "avatar_url": "https://avatars1.githubusercontent.com/u/15174756?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chuong98", "html_url": "https://github.com/chuong98", "followers_url": "https://api.github.com/users/chuong98/followers", "following_url": "https://api.github.com/users/chuong98/following{/other_user}", "gists_url": "https://api.github.com/users/chuong98/gists{/gist_id}", "starred_url": "https://api.github.com/users/chuong98/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chuong98/subscriptions", "organizations_url": "https://api.github.com/users/chuong98/orgs", "repos_url": "https://api.github.com/users/chuong98/repos", "events_url": "https://api.github.com/users/chuong98/events{/privacy}", "received_events_url": "https://api.github.com/users/chuong98/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-10T10:04:43Z", "updated_at": "2018-07-10T10:06:27Z", "author_association": "NONE", "body_html": "<p>I follow <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1735272\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/velikodniy\">@velikodniy</a> to add the Weighted BCEloss, where the weights can be computed dynamically for each batch:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">weighted_binary_cross_entropy</span>(<span class=\"pl-smi\">sigmoid_x</span>, <span class=\"pl-smi\">targets</span>, <span class=\"pl-smi\">pos_weight</span>, <span class=\"pl-smi\">weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.</span>\n<span class=\"pl-s\">        targets: true value, one-hot-like vector of size [N,C]</span>\n<span class=\"pl-s\">        pos_weight: Weight for postive sample</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> (targets.size() <span class=\"pl-k\">==</span> sigmoid_x.size()):\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Target size (<span class=\"pl-c1\">{}</span>) must be the same as input size (<span class=\"pl-c1\">{}</span>)<span class=\"pl-pds\">\"</span></span>.format(targets.size(), sigmoid_x.size()))\n\n    loss <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>pos_weight<span class=\"pl-k\">*</span> targets <span class=\"pl-k\">*</span> sigmoid_x.log() <span class=\"pl-k\">-</span> (<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>targets)<span class=\"pl-k\">*</span>(<span class=\"pl-c1\">1</span><span class=\"pl-k\">-</span>sigmoid_x).log()\n\n    <span class=\"pl-k\">if</span> weight <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        loss <span class=\"pl-k\">=</span> loss <span class=\"pl-k\">*</span> weight\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-v\">reduce</span>:\n        <span class=\"pl-k\">return</span> loss\n    <span class=\"pl-k\">elif</span> size_average:\n        <span class=\"pl-k\">return</span> loss.mean()\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> loss.sum()</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">WeightedBCELoss</span>(<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">pos_weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">PosWeightIsDynamic</span><span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">WeightIsDynamic</span><span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">        Args:</span>\n<span class=\"pl-s\">            pos_weight = Weight for postive samples. Size [1,C]</span>\n<span class=\"pl-s\">            weight = Weight for Each class. Size [1,C]</span>\n<span class=\"pl-s\">            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.</span>\n<span class=\"pl-s\">            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n\n        <span class=\"pl-c1\">self</span>.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight<span class=\"pl-pds\">'</span></span>, weight)\n        <span class=\"pl-c1\">self</span>.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pos_weight<span class=\"pl-pds\">'</span></span>, pos_weight)\n        <span class=\"pl-c1\">self</span>.size_average <span class=\"pl-k\">=</span> size_average\n        <span class=\"pl-c1\">self</span>.reduce <span class=\"pl-k\">=</span> <span class=\"pl-v\">reduce</span>\n        <span class=\"pl-c1\">self</span>.PosWeightIsDynamic <span class=\"pl-k\">=</span> PosWeightIsDynamic\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight</span>\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.PosWeightIsDynamic:\n            positive_counts <span class=\"pl-k\">=</span> target.sum(<span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n            nBatch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(target)\n            <span class=\"pl-c1\">self</span>.pos_weight <span class=\"pl-k\">=</span> (nBatch <span class=\"pl-k\">-</span> positive_counts)<span class=\"pl-k\">/</span>(positive_counts <span class=\"pl-k\">+</span><span class=\"pl-c1\">1e-5</span>)\n\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight</span>\n            <span class=\"pl-k\">return</span> weighted_binary_cross_entropy(<span class=\"pl-c1\">input</span>, target,\n                                                 <span class=\"pl-c1\">self</span>.pos_weight,\n                                                 <span class=\"pl-v\">weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.weight,\n                                                 <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.size_average,\n                                                 <span class=\"pl-v\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.reduce)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">return</span> weighted_binary_cross_entropy(<span class=\"pl-c1\">input</span>, target,\n                                                 <span class=\"pl-c1\">self</span>.pos_weight,\n                                                 <span class=\"pl-v\">weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                                 <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.size_average,\n                                                 <span class=\"pl-v\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.reduce)</pre></div>", "body_text": "I follow @velikodniy to add the Weighted BCEloss, where the weights can be computed dynamically for each batch:\ndef weighted_binary_cross_entropy(sigmoid_x, targets, pos_weight, weight=None, size_average=True, reduce=True):\n    \"\"\"\n    Args:\n        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n        targets: true value, one-hot-like vector of size [N,C]\n        pos_weight: Weight for postive sample\n    \"\"\"\n    if not (targets.size() == sigmoid_x.size()):\n        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n\n    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n\n    if weight is not None:\n        loss = loss * weight\n\n    if not reduce:\n        return loss\n    elif size_average:\n        return loss.mean()\n    else:\n        return loss.sum()\nclass WeightedBCELoss(Module):\n    def __init__(self, pos_weight=1, weight=None, PosWeightIsDynamic= False, WeightIsDynamic= False, size_average=True, reduce=True):\n        \"\"\"\n        Args:\n            pos_weight = Weight for postive samples. Size [1,C]\n            weight = Weight for Each class. Size [1,C]\n            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n        \"\"\"\n        super().__init__()\n\n        self.register_buffer('weight', weight)\n        self.register_buffer('pos_weight', pos_weight)\n        self.size_average = size_average\n        self.reduce = reduce\n        self.PosWeightIsDynamic = PosWeightIsDynamic\n\n    def forward(self, input, target):\n        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n        if self.PosWeightIsDynamic:\n            positive_counts = target.sum(dim=0)\n            nBatch = len(target)\n            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n\n        if self.weight is not None:\n            # weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=self.weight,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)\n        else:\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=None,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)", "body": "I follow @velikodniy to add the Weighted BCEloss, where the weights can be computed dynamically for each batch:\r\n``` python\r\ndef weighted_binary_cross_entropy(sigmoid_x, targets, pos_weight, weight=None, size_average=True, reduce=True):\r\n    \"\"\"\r\n    Args:\r\n        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\r\n        targets: true value, one-hot-like vector of size [N,C]\r\n        pos_weight: Weight for postive sample\r\n    \"\"\"\r\n    if not (targets.size() == sigmoid_x.size()):\r\n        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\r\n\r\n    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\r\n\r\n    if weight is not None:\r\n        loss = loss * weight\r\n\r\n    if not reduce:\r\n        return loss\r\n    elif size_average:\r\n        return loss.mean()\r\n    else:\r\n        return loss.sum()\r\n```\r\n``` python\r\nclass WeightedBCELoss(Module):\r\n    def __init__(self, pos_weight=1, weight=None, PosWeightIsDynamic= False, WeightIsDynamic= False, size_average=True, reduce=True):\r\n        \"\"\"\r\n        Args:\r\n            pos_weight = Weight for postive samples. Size [1,C]\r\n            weight = Weight for Each class. Size [1,C]\r\n            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\r\n            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\r\n        \"\"\"\r\n        super().__init__()\r\n\r\n        self.register_buffer('weight', weight)\r\n        self.register_buffer('pos_weight', pos_weight)\r\n        self.size_average = size_average\r\n        self.reduce = reduce\r\n        self.PosWeightIsDynamic = PosWeightIsDynamic\r\n\r\n    def forward(self, input, target):\r\n        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\r\n        if self.PosWeightIsDynamic:\r\n            positive_counts = target.sum(dim=0)\r\n            nBatch = len(target)\r\n            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\r\n\r\n        if self.weight is not None:\r\n            # weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight\r\n            return weighted_binary_cross_entropy(input, target,\r\n                                                 self.pos_weight,\r\n                                                 weight=self.weight,\r\n                                                 size_average=self.size_average,\r\n                                                 reduce=self.reduce)\r\n        else:\r\n            return weighted_binary_cross_entropy(input, target,\r\n                                                 self.pos_weight,\r\n                                                 weight=None,\r\n                                                 size_average=self.size_average,\r\n                                                 reduce=self.reduce)\r\n```"}