{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5660", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5660/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5660/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5660/events", "html_url": "https://github.com/pytorch/pytorch/issues/5660", "id": 303821364, "node_id": "MDU6SXNzdWUzMDM4MjEzNjQ=", "number": 5660, "title": "BCELoss with weights for labels (like weighted_cross_entropy_with_logits in TF)", "user": {"login": "velikodniy", "id": 1735272, "node_id": "MDQ6VXNlcjE3MzUyNzI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1735272?v=4", "gravatar_id": "", "url": "https://api.github.com/users/velikodniy", "html_url": "https://github.com/velikodniy", "followers_url": "https://api.github.com/users/velikodniy/followers", "following_url": "https://api.github.com/users/velikodniy/following{/other_user}", "gists_url": "https://api.github.com/users/velikodniy/gists{/gist_id}", "starred_url": "https://api.github.com/users/velikodniy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/velikodniy/subscriptions", "organizations_url": "https://api.github.com/users/velikodniy/orgs", "repos_url": "https://api.github.com/users/velikodniy/repos", "events_url": "https://api.github.com/users/velikodniy/events{/privacy}", "received_events_url": "https://api.github.com/users/velikodniy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-03-09T12:11:09Z", "updated_at": "2018-07-10T10:06:27Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>I've implemented an analog of <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\" rel=\"nofollow\">weighted_cross_entropy_with_logits</a> in my current project. It's useful for working with imbalanced datasets. I want to add it to PyTorch but I'm in doubt if it is really needed for others.</p>\n<p>For example, my implementation:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">weighted_binary_cross_entropy_with_logits</span>(<span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">targets</span>, <span class=\"pl-smi\">pos_weight</span>, <span class=\"pl-smi\">weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> (targets.size() <span class=\"pl-k\">==</span> logits.size()):\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Target size (<span class=\"pl-c1\">{}</span>) must be the same as input size (<span class=\"pl-c1\">{}</span>)<span class=\"pl-pds\">\"</span></span>.format(targets.size(), logits.size()))\n\n    max_val <span class=\"pl-k\">=</span> (<span class=\"pl-k\">-</span>logits).clamp(<span class=\"pl-v\">min</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    log_weight <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">+</span> (pos_weight <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">*</span> targets\n    loss <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> targets) <span class=\"pl-k\">*</span> logits <span class=\"pl-k\">+</span> log_weight <span class=\"pl-k\">*</span> (((<span class=\"pl-k\">-</span>max_val).exp() <span class=\"pl-k\">+</span> (<span class=\"pl-k\">-</span>logits <span class=\"pl-k\">-</span> max_val).exp()).log() <span class=\"pl-k\">+</span> max_val)\n\n    <span class=\"pl-k\">if</span> weight <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        loss <span class=\"pl-k\">=</span> loss <span class=\"pl-k\">*</span> weight\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-v\">reduce</span>:\n        <span class=\"pl-k\">return</span> loss\n    <span class=\"pl-k\">elif</span> size_average:\n        <span class=\"pl-k\">return</span> loss.mean()\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-k\">return</span> loss.sum()\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">WeightedBCEWithLogitsLoss</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">pos_weight</span>, <span class=\"pl-smi\">weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight<span class=\"pl-pds\">'</span></span>, weight)\n        <span class=\"pl-c1\">self</span>.register_buffer(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pos_weight<span class=\"pl-pds\">'</span></span>, pos_weight)\n        <span class=\"pl-c1\">self</span>.size_average <span class=\"pl-k\">=</span> size_average\n        <span class=\"pl-c1\">self</span>.reduce <span class=\"pl-k\">=</span> <span class=\"pl-v\">reduce</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">target</span>):\n        pos_weight <span class=\"pl-k\">=</span> Variable(<span class=\"pl-c1\">self</span>.pos_weight) <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(<span class=\"pl-c1\">self</span>.pos_weight, Variable) <span class=\"pl-k\">else</span> <span class=\"pl-c1\">self</span>.pos_weight\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.weight <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n            weight <span class=\"pl-k\">=</span> Variable(<span class=\"pl-c1\">self</span>.weight) <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(<span class=\"pl-c1\">self</span>.weight, Variable) <span class=\"pl-k\">else</span> <span class=\"pl-c1\">self</span>.weight\n            <span class=\"pl-k\">return</span> weighted_binary_cross_entropy_with_logits(<span class=\"pl-c1\">input</span>, target,\n                                                             pos_weight,\n                                                             <span class=\"pl-v\">weight</span><span class=\"pl-k\">=</span>weight\n                                                             <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.size_average,\n                                                             <span class=\"pl-v\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.reduce)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-k\">return</span> weighted_binary_cross_entropy_with_logits(<span class=\"pl-c1\">input</span>, target,\n                                                             pos_weight,\n                                                             <span class=\"pl-v\">weight</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                                             <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.size_average,\n                                                             <span class=\"pl-v\">reduce</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.reduce)</pre></div>\n<p>(Of course, tests and WeightedBCELoss should be written too.)</p>", "body_text": "I've implemented an analog of weighted_cross_entropy_with_logits in my current project. It's useful for working with imbalanced datasets. I want to add it to PyTorch but I'm in doubt if it is really needed for others.\nFor example, my implementation:\ndef weighted_binary_cross_entropy_with_logits(logits, targets, pos_weight, weight=None, size_average=True, reduce=True):\n    if not (targets.size() == logits.size()):\n        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), logits.size()))\n\n    max_val = (-logits).clamp(min=0)\n    log_weight = 1 + (pos_weight - 1) * targets\n    loss = (1 - targets) * logits + log_weight * (((-max_val).exp() + (-logits - max_val).exp()).log() + max_val)\n\n    if weight is not None:\n        loss = loss * weight\n\n    if not reduce:\n        return loss\n    elif size_average:\n        return loss.mean()\n    else:\n        return loss.sum()\n\nclass WeightedBCEWithLogitsLoss(torch.nn.Module):\n    def __init__(self, pos_weight, weight=None, size_average=True, reduce=True):\n        super().__init__()\n        self.register_buffer('weight', weight)\n        self.register_buffer('pos_weight', pos_weight)\n        self.size_average = size_average\n        self.reduce = reduce\n\n    def forward(self, input, target):\n        pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n        if self.weight is not None:\n            weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight\n            return weighted_binary_cross_entropy_with_logits(input, target,\n                                                             pos_weight,\n                                                             weight=weight\n                                                             size_average=self.size_average,\n                                                             reduce=self.reduce)\n        else:\n            return weighted_binary_cross_entropy_with_logits(input, target,\n                                                             pos_weight,\n                                                             weight=None,\n                                                             size_average=self.size_average,\n                                                             reduce=self.reduce)\n(Of course, tests and WeightedBCELoss should be written too.)", "body": "I've implemented an analog of [weighted_cross_entropy_with_logits](https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits) in my current project. It's useful for working with imbalanced datasets. I want to add it to PyTorch but I'm in doubt if it is really needed for others.\r\n\r\nFor example, my implementation:\r\n```python\r\ndef weighted_binary_cross_entropy_with_logits(logits, targets, pos_weight, weight=None, size_average=True, reduce=True):\r\n    if not (targets.size() == logits.size()):\r\n        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), logits.size()))\r\n\r\n    max_val = (-logits).clamp(min=0)\r\n    log_weight = 1 + (pos_weight - 1) * targets\r\n    loss = (1 - targets) * logits + log_weight * (((-max_val).exp() + (-logits - max_val).exp()).log() + max_val)\r\n\r\n    if weight is not None:\r\n        loss = loss * weight\r\n\r\n    if not reduce:\r\n        return loss\r\n    elif size_average:\r\n        return loss.mean()\r\n    else:\r\n        return loss.sum()\r\n\r\nclass WeightedBCEWithLogitsLoss(torch.nn.Module):\r\n    def __init__(self, pos_weight, weight=None, size_average=True, reduce=True):\r\n        super().__init__()\r\n        self.register_buffer('weight', weight)\r\n        self.register_buffer('pos_weight', pos_weight)\r\n        self.size_average = size_average\r\n        self.reduce = reduce\r\n\r\n    def forward(self, input, target):\r\n        pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\r\n        if self.weight is not None:\r\n            weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight\r\n            return weighted_binary_cross_entropy_with_logits(input, target,\r\n                                                             pos_weight,\r\n                                                             weight=weight\r\n                                                             size_average=self.size_average,\r\n                                                             reduce=self.reduce)\r\n        else:\r\n            return weighted_binary_cross_entropy_with_logits(input, target,\r\n                                                             pos_weight,\r\n                                                             weight=None,\r\n                                                             size_average=self.size_average,\r\n                                                             reduce=self.reduce)\r\n```\r\n\r\n(Of course, tests and WeightedBCELoss should be written too.)"}