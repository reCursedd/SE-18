{"url": "https://api.github.com/repos/pytorch/pytorch/issues/5315", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/5315/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/5315/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/5315/events", "html_url": "https://github.com/pytorch/pytorch/issues/5315", "id": 298715621, "node_id": "MDU6SXNzdWUyOTg3MTU2MjE=", "number": 5315, "title": "ONNX export does not support parallel model converted using nn.DataParallel(model) and will throw error message \"untraced buffer\"", "user": {"login": "xiaoyongzhu", "id": 1185090, "node_id": "MDQ6VXNlcjExODUwOTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/1185090?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xiaoyongzhu", "html_url": "https://github.com/xiaoyongzhu", "followers_url": "https://api.github.com/users/xiaoyongzhu/followers", "following_url": "https://api.github.com/users/xiaoyongzhu/following{/other_user}", "gists_url": "https://api.github.com/users/xiaoyongzhu/gists{/gist_id}", "starred_url": "https://api.github.com/users/xiaoyongzhu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xiaoyongzhu/subscriptions", "organizations_url": "https://api.github.com/users/xiaoyongzhu/orgs", "repos_url": "https://api.github.com/users/xiaoyongzhu/repos", "events_url": "https://api.github.com/users/xiaoyongzhu/events{/privacy}", "received_events_url": "https://api.github.com/users/xiaoyongzhu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 693805995, "node_id": "MDU6TGFiZWw2OTM4MDU5OTU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/onnx", "name": "onnx", "color": "e99695", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-02-20T19:18:25Z", "updated_at": "2018-11-06T09:54:24Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Looks like ONNX support does not support dataparallel models (in particular, model that is converted after nn.DataParallel(model).</p>\n<p>Here is a simple piece of code to repro the issue:</p>\n<pre><code>import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.nn.init as init\nimport time\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics.ranking import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport multiprocessing\n\nmodel = models.densenet.densenet121(pretrained=True)\nmodel = nn.DataParallel(model)\ndummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\nmodel = model.cuda()\ntorch.onnx.export(model, dummy_input, \"test.onnx\", verbose=False, export_params=True)\n</code></pre>\n<p>The error message is:</p>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-4-514806a4333a&gt; in &lt;module&gt;()\n    386 dummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\n    387 model = model.cuda()\n--&gt; 388 torch.onnx.export(model, dummy_input, \"test.onnx\", verbose=False, export_params=True)\n    389 \n    390 #model.load_state_dict(chkpt['state_dict'])\n\n~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in export(model, args, f, export_params, verbose, training)\n     73             only, so you will generally not need to set this to True.\n     74     \"\"\"\n---&gt; 75     _export(model, args, f, export_params, verbose, training)\n     76 \n     77 \n\n~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in _export(model, args, f, export_params, verbose, training)\n    120                            \"something weird is happening in your model!\")\n    121 \n--&gt; 122     _optimize_trace(trace)\n    123     if verbose:\n    124         print(trace)\n\n~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in _optimize_trace(trace)\n     79     torch._C._jit_pass_peephole(trace)\n     80     torch._C._jit_pass_lint(trace)\n---&gt; 81     torch._C._jit_pass_onnx(trace)\n     82     torch._C._jit_pass_lint(trace)\n     83     torch._C._jit_pass_onnx_peephole(trace)\n\nRuntimeError: untraced buffer\n</code></pre>\n<p>However if we remove the <code>model = nn.DataParallel(model)</code> line, i.e. change the code to the following:</p>\n<pre><code>model = models.densenet.densenet121(pretrained=True)\ndummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\nmodel = model.cuda()\ntorch.onnx.export(model, dummy_input, \"test.onnx\", verbose=False, export_params=True)\n</code></pre>\n<p>It at least exports the model.</p>\n<p>I hope either we fix this bug (i.e. also support the model that is converted using nn.DataParallel(model), or provide an explicit error message saying multiple GPU model is not supported.</p>\n<p>BTW - for those who still want to export such a model, here is the workaround:<br>\n<a href=\"https://stackoverflow.com/questions/44230907/keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict\" rel=\"nofollow\">https://stackoverflow.com/questions/44230907/keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict</a><br>\ni.e. still loading the single GPU model and when loading the weights, change the dict name to exclude the string \"module\".</p>\n<p>When submitting a bug report, please include the following information (where relevant):</p>\n<ul>\n<li>OS: Linux</li>\n<li>PyTorch version: 0.3.1</li>\n<li>How you installed PyTorch (conda, pip, source): pip</li>\n<li>Python version: 3.5</li>\n<li>CUDA/cuDNN version: 8.0/cuDNN 6</li>\n<li>GPU models and configuration: Tesla K80</li>\n<li>GCC version (if compiling from source): N/A</li>\n</ul>", "body_text": "Looks like ONNX support does not support dataparallel models (in particular, model that is converted after nn.DataParallel(model).\nHere is a simple piece of code to repro the issue:\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.nn.init as init\nimport time\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics.ranking import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nimport multiprocessing\n\nmodel = models.densenet.densenet121(pretrained=True)\nmodel = nn.DataParallel(model)\ndummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\nmodel = model.cuda()\ntorch.onnx.export(model, dummy_input, \"test.onnx\", verbose=False, export_params=True)\n\nThe error message is:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-4-514806a4333a> in <module>()\n    386 dummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\n    387 model = model.cuda()\n--> 388 torch.onnx.export(model, dummy_input, \"test.onnx\", verbose=False, export_params=True)\n    389 \n    390 #model.load_state_dict(chkpt['state_dict'])\n\n~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in export(model, args, f, export_params, verbose, training)\n     73             only, so you will generally not need to set this to True.\n     74     \"\"\"\n---> 75     _export(model, args, f, export_params, verbose, training)\n     76 \n     77 \n\n~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in _export(model, args, f, export_params, verbose, training)\n    120                            \"something weird is happening in your model!\")\n    121 \n--> 122     _optimize_trace(trace)\n    123     if verbose:\n    124         print(trace)\n\n~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in _optimize_trace(trace)\n     79     torch._C._jit_pass_peephole(trace)\n     80     torch._C._jit_pass_lint(trace)\n---> 81     torch._C._jit_pass_onnx(trace)\n     82     torch._C._jit_pass_lint(trace)\n     83     torch._C._jit_pass_onnx_peephole(trace)\n\nRuntimeError: untraced buffer\n\nHowever if we remove the model = nn.DataParallel(model) line, i.e. change the code to the following:\nmodel = models.densenet.densenet121(pretrained=True)\ndummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\nmodel = model.cuda()\ntorch.onnx.export(model, dummy_input, \"test.onnx\", verbose=False, export_params=True)\n\nIt at least exports the model.\nI hope either we fix this bug (i.e. also support the model that is converted using nn.DataParallel(model), or provide an explicit error message saying multiple GPU model is not supported.\nBTW - for those who still want to export such a model, here is the workaround:\nhttps://stackoverflow.com/questions/44230907/keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict\ni.e. still loading the single GPU model and when loading the weights, change the dict name to exclude the string \"module\".\nWhen submitting a bug report, please include the following information (where relevant):\n\nOS: Linux\nPyTorch version: 0.3.1\nHow you installed PyTorch (conda, pip, source): pip\nPython version: 3.5\nCUDA/cuDNN version: 8.0/cuDNN 6\nGPU models and configuration: Tesla K80\nGCC version (if compiling from source): N/A", "body": "Looks like ONNX support does not support dataparallel models (in particular, model that is converted after nn.DataParallel(model).\r\n\r\nHere is a simple piece of code to repro the issue:\r\n```\r\nimport os\r\nimport sys\r\nimport numpy as np\r\nimport pandas as pd\r\nimport torch\r\nimport torchvision.models as models\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nimport torch.nn.init as init\r\nimport time\r\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\r\nfrom torch.autograd import Variable\r\nimport torchvision.transforms as transforms\r\nfrom torch.utils.data import DataLoader, Dataset\r\nfrom sklearn.metrics.ranking import roc_auc_score\r\nfrom sklearn.model_selection import train_test_split\r\nfrom PIL import Image\r\nimport multiprocessing\r\n\r\nmodel = models.densenet.densenet121(pretrained=True)\r\nmodel = nn.DataParallel(model)\r\ndummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\r\nmodel = model.cuda()\r\ntorch.onnx.export(model, dummy_input, \"test.onnx\", verbose=False, export_params=True)\r\n```\r\nThe error message is:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-4-514806a4333a> in <module>()\r\n    386 dummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\r\n    387 model = model.cuda()\r\n--> 388 torch.onnx.export(model, dummy_input, \"test.onnx\", verbose=False, export_params=True)\r\n    389 \r\n    390 #model.load_state_dict(chkpt['state_dict'])\r\n\r\n~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in export(model, args, f, export_params, verbose, training)\r\n     73             only, so you will generally not need to set this to True.\r\n     74     \"\"\"\r\n---> 75     _export(model, args, f, export_params, verbose, training)\r\n     76 \r\n     77 \r\n\r\n~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in _export(model, args, f, export_params, verbose, training)\r\n    120                            \"something weird is happening in your model!\")\r\n    121 \r\n--> 122     _optimize_trace(trace)\r\n    123     if verbose:\r\n    124         print(trace)\r\n\r\n~/python3env/lib/python3.5/site-packages/torch/onnx/__init__.py in _optimize_trace(trace)\r\n     79     torch._C._jit_pass_peephole(trace)\r\n     80     torch._C._jit_pass_lint(trace)\r\n---> 81     torch._C._jit_pass_onnx(trace)\r\n     82     torch._C._jit_pass_lint(trace)\r\n     83     torch._C._jit_pass_onnx_peephole(trace)\r\n\r\nRuntimeError: untraced buffer\r\n```\r\n\r\nHowever if we remove the `model = nn.DataParallel(model)` line, i.e. change the code to the following:\r\n```\r\nmodel = models.densenet.densenet121(pretrained=True)\r\ndummy_input = Variable(torch.randn(10, 3, 224, 224)).cuda()\r\nmodel = model.cuda()\r\ntorch.onnx.export(model, dummy_input, \"test.onnx\", verbose=False, export_params=True)\r\n```\r\nIt at least exports the model.\r\n\r\nI hope either we fix this bug (i.e. also support the model that is converted using nn.DataParallel(model), or provide an explicit error message saying multiple GPU model is not supported.\r\n\r\nBTW - for those who still want to export such a model, here is the workaround:\r\nhttps://stackoverflow.com/questions/44230907/keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict\r\ni.e. still loading the single GPU model and when loading the weights, change the dict name to exclude the string \"module\".\r\n\r\n\r\nWhen submitting a bug report, please include the following information (where relevant):\r\n- OS: Linux\r\n- PyTorch version: 0.3.1\r\n- How you installed PyTorch (conda, pip, source): pip\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 8.0/cuDNN 6\r\n- GPU models and configuration: Tesla K80\r\n- GCC version (if compiling from source): N/A\r\n\r\n"}