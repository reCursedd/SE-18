{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193266182", "pull_request_review_id": 126204484, "id": 193266182, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5MzI2NjE4Mg==", "diff_hunk": "@@ -98,6 +98,15 @@ non-Variable arguments::\n             # Gradients of non-Tensor arguments to forward must be None.\n             return grad_output * ctx.constant, None\n \n+.. note::\n+    To implement custom double backward (i.e., second order derivatives) for a\n+    custom :class:`~torch.autograd.function`, e.g., named ``MyFunction``, we can\n+    define another custom  :class:`~torch.autograd.function`, e.g., named\n+    ``MyFunctionBackward``, where the ``MyFunctionBackward.forward`` method\n+    implements the backward pass of ``MyFunction`` and is called in\n+    ``MyFunction.backward`` method, and ``MyFunctionBackward.backward`` method\n+    implements the custom double backward computation for ``MyFunction``.", "path": "docs/source/notes/extending.rst", "position": null, "original_position": 11, "commit_id": "84be0f3863530ea64eb2bc1b0a43ddde76f68996", "original_commit_id": "0c82813af2bb9aa93011c2891f4d8a1064e4ed5e", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "This seems like a needlessly complicated description. Basically, just point out that the inputs to `backward` are honest to goodness variables, so if you implement `backward` using differentiable operations (e.g., `MyFunctionBackward` invocation), higher order derivatives will work.", "created_at": "2018-06-06T01:06:15Z", "updated_at": "2018-11-23T15:45:00Z", "html_url": "https://github.com/pytorch/pytorch/pull/8166#discussion_r193266182", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8166", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/193266182"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8166#discussion_r193266182"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8166"}}, "body_html": "<p>This seems like a needlessly complicated description. Basically, just point out that the inputs to <code>backward</code> are honest to goodness variables, so if you implement <code>backward</code> using differentiable operations (e.g., <code>MyFunctionBackward</code> invocation), higher order derivatives will work.</p>", "body_text": "This seems like a needlessly complicated description. Basically, just point out that the inputs to backward are honest to goodness variables, so if you implement backward using differentiable operations (e.g., MyFunctionBackward invocation), higher order derivatives will work."}