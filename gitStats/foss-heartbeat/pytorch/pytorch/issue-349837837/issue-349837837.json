{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10450", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10450/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10450/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10450/events", "html_url": "https://github.com/pytorch/pytorch/issues/10450", "id": 349837837, "node_id": "MDU6SXNzdWUzNDk4Mzc4Mzc=", "number": 10450, "title": "Verbose exception on deleting multiprocessed `DataLoaderIter`", "user": {"login": "kl2792", "id": 7944969, "node_id": "MDQ6VXNlcjc5NDQ5Njk=", "avatar_url": "https://avatars3.githubusercontent.com/u/7944969?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kl2792", "html_url": "https://github.com/kl2792", "followers_url": "https://api.github.com/users/kl2792/followers", "following_url": "https://api.github.com/users/kl2792/following{/other_user}", "gists_url": "https://api.github.com/users/kl2792/gists{/gist_id}", "starred_url": "https://api.github.com/users/kl2792/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kl2792/subscriptions", "organizations_url": "https://api.github.com/users/kl2792/orgs", "repos_url": "https://api.github.com/users/kl2792/repos", "events_url": "https://api.github.com/users/kl2792/events{/privacy}", "received_events_url": "https://api.github.com/users/kl2792/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-12T19:33:05Z", "updated_at": "2018-08-13T17:30:28Z", "closed_at": "2018-08-13T17:30:28Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>Ignored exception ConnectionResetError when deleting <code>DataLoaderIter</code> on every training epoch when using num_workers &gt; 0.</p>\n<h2>Code example</h2>\n<p>Reproduction: cannot create code for minimal reproduction at the moment. I get this error at the end of every epoch when using the torchbearer training package, and my model uses GPU.</p>\n<p>Stack trace:</p>\n<pre><code>Exception ignored in: &lt;bound method _DataLoaderIter.__del__ of &lt;torch.utils.data.dataloader._DataLoaderIter object at 0x2aab643fe278&gt;&gt;\nTraceback (most recent call last):\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n    self._shutdown_workers()\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n    self.worker_result_queue.get()\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n    return _ForkingPickler.loads(res)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n    with _resource_sharer.get_connection(self._id) as conn:\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n    c = Client(address, authkey=process.current_process().authkey)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n    answer_challenge(c, authkey)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n    response = connection.recv_bytes(256)        # reject large message\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nConnectionResetError: [Errno 104] Connection reset by peer\n</code></pre>\n<h2>System Info</h2>\n<ul>\n<li>PyTorch or Caffe2:  pytorch</li>\n<li>How you installed PyTorch (conda, pip, source): conda</li>\n<li>Build command you used (if compiling from source):</li>\n<li>OS: linux</li>\n<li>PyTorch version: 0.4.1</li>\n<li>Python version: 3.6</li>\n<li>CUDA/cuDNN version: 9.0</li>\n<li>GPU models and configuration: see code</li>\n<li>GCC version (if compiling from source):</li>\n<li>CMake version:</li>\n<li>Versions of any other relevant libraries:</li>\n</ul>", "body_text": "Issue description\nIgnored exception ConnectionResetError when deleting DataLoaderIter on every training epoch when using num_workers > 0.\nCode example\nReproduction: cannot create code for minimal reproduction at the moment. I get this error at the end of every epoch when using the torchbearer training package, and my model uses GPU.\nStack trace:\nException ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x2aab643fe278>>\nTraceback (most recent call last):\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n    self._shutdown_workers()\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n    self.worker_result_queue.get()\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n    return _ForkingPickler.loads(res)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n    fd = df.detach()\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n    with _resource_sharer.get_connection(self._id) as conn:\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n    c = Client(address, authkey=process.current_process().authkey)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n    answer_challenge(c, authkey)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n    response = connection.recv_bytes(256)        # reject large message\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nSystem Info\n\nPyTorch or Caffe2:  pytorch\nHow you installed PyTorch (conda, pip, source): conda\nBuild command you used (if compiling from source):\nOS: linux\nPyTorch version: 0.4.1\nPython version: 3.6\nCUDA/cuDNN version: 9.0\nGPU models and configuration: see code\nGCC version (if compiling from source):\nCMake version:\nVersions of any other relevant libraries:", "body": "## Issue description\r\n\r\nIgnored exception ConnectionResetError when deleting `DataLoaderIter` on every training epoch when using num_workers > 0.\r\n\r\n## Code example\r\n\r\nReproduction: cannot create code for minimal reproduction at the moment. I get this error at the end of every epoch when using the torchbearer training package, and my model uses GPU.\r\n\r\nStack trace:\r\n```\r\nException ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x2aab643fe278>>\r\nTraceback (most recent call last):\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\r\n    self._shutdown_workers()\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\r\n    self.worker_result_queue.get()\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\r\n    return _ForkingPickler.loads(res)\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\r\n    fd = df.detach()\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\r\n    with _resource_sharer.get_connection(self._id) as conn:\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\r\n    c = Client(address, authkey=process.current_process().authkey)\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\r\n    answer_challenge(c, authkey)\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\r\n    response = connection.recv_bytes(256)        # reject large message\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/rigel/katt/users/kl2792/.anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\n```\r\n\r\n## System Info\r\n- PyTorch or Caffe2:  pytorch\r\n- How you installed PyTorch (conda, pip, source): conda\r\n- Build command you used (if compiling from source):\r\n- OS: linux \r\n- PyTorch version: 0.4.1\r\n- Python version: 3.6\r\n- CUDA/cuDNN version: 9.0\r\n- GPU models and configuration: see code\r\n- GCC version (if compiling from source):\r\n- CMake version:\r\n- Versions of any other relevant libraries:\r\n"}