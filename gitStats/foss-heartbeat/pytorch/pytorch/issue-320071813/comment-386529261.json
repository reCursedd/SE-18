{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/386529261", "html_url": "https://github.com/pytorch/pytorch/issues/7261#issuecomment-386529261", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7261", "id": 386529261, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NjUyOTI2MQ==", "user": {"login": "mfuntowicz", "id": 2241520, "node_id": "MDQ6VXNlcjIyNDE1MjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2241520?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mfuntowicz", "html_url": "https://github.com/mfuntowicz", "followers_url": "https://api.github.com/users/mfuntowicz/followers", "following_url": "https://api.github.com/users/mfuntowicz/following{/other_user}", "gists_url": "https://api.github.com/users/mfuntowicz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mfuntowicz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mfuntowicz/subscriptions", "organizations_url": "https://api.github.com/users/mfuntowicz/orgs", "repos_url": "https://api.github.com/users/mfuntowicz/repos", "events_url": "https://api.github.com/users/mfuntowicz/events{/privacy}", "received_events_url": "https://api.github.com/users/mfuntowicz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-04T07:53:04Z", "updated_at": "2018-05-04T08:02:52Z", "author_association": "NONE", "body_html": "<p>Please, find the script below. The discriminator network is the same that I'm using in my project.</p>\n<p>On the same machine described at the beginning of the issue, at each epoch the GPU memory increases by ~2Mb (Nvidia-smi visual stats). If you let this script run for a long time, then it gives you out of memory.</p>\n<p>Removing the spectral_norm() calls in _ResidualDownSamplingBlock stabilizes memory consumption.</p>\n<p>Let me know if I can do anything else.<br>\nMorgan</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> argparse <span class=\"pl-k\">import</span> ArgumentParser\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.backends.cudnn <span class=\"pl-k\">as</span> cudnn\n<span class=\"pl-k\">import</span> torch.cuda <span class=\"pl-k\">as</span> cuda\n\n<span class=\"pl-k\">from</span> torch.nn <span class=\"pl-k\">import</span> Sequential, Conv2d, ReLU, Linear, Module, AvgPool2d, BatchNorm2d\n<span class=\"pl-k\">from</span> torch.nn.functional <span class=\"pl-k\">import</span> binary_cross_entropy_with_logits, avg_pool2d\n<span class=\"pl-k\">from</span> torch.nn.utils <span class=\"pl-k\">import</span> spectral_norm\n<span class=\"pl-k\">from</span> torch.optim <span class=\"pl-k\">import</span> Adam\n\n__author__ <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Morgan Funtowicz<span class=\"pl-pds\">'</span></span>\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Flatten</span>(<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        x <span class=\"pl-k\">=</span> x.view(x.size(<span class=\"pl-c1\">0</span>), <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-k\">return</span> x\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">_ResidualDownSamplingBlock</span>(<span class=\"pl-e\">Module</span>):\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">n_in</span>, <span class=\"pl-smi\">n_out</span>, <span class=\"pl-smi\">ksize</span>, <span class=\"pl-smi\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-smi\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n        <span class=\"pl-c1\">super</span>().<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>._f <span class=\"pl-k\">=</span> Sequential(\n            ReLU(),\n            spectral_norm(Conv2d(n_in, n_out, ksize, stride, padding)),\n            ReLU(<span class=\"pl-c1\">True</span>),\n            spectral_norm(Conv2d(n_out, n_out, ksize, stride, padding)),\n            AvgPool2d(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\n        )\n        <span class=\"pl-c1\">self</span>._sc <span class=\"pl-k\">=</span> spectral_norm(Conv2d(n_in, n_out, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>))\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        <span class=\"pl-k\">return</span> avg_pool2d(<span class=\"pl-c1\">self</span>._sc(x), <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>._f(x)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Ensure Tensor are allocated as FloatTensor</span>\n    cudnn.benchmark <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n    torch.set_default_tensor_type(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch.FloatTensor<span class=\"pl-pds\">'</span></span>)\n    torch.set_default_dtype(torch.float32)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Parse provided arguments</span>\n    args_parser <span class=\"pl-k\">=</span> ArgumentParser()\n    args_parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-d<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dest</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>device<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Device to use for training (-1 = CPU)<span class=\"pl-pds\">'</span></span>)\n    args_parser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>-b<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">16</span>, <span class=\"pl-v\">dest</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Size of the minibatch<span class=\"pl-pds\">'</span></span>)\n\n    args <span class=\"pl-k\">=</span> args_parser.parse_args()\n    args.gpu <span class=\"pl-k\">=</span> args.device <span class=\"pl-k\">&gt;=</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> cuda.is_available()\n    device <span class=\"pl-k\">=</span> torch.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cuda:<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> args.device <span class=\"pl-k\">if</span> args.gpu <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cpu<span class=\"pl-pds\">\"</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the model &amp; Optimizer</span>\n    model <span class=\"pl-k\">=</span> Sequential(\n        _ResidualDownSamplingBlock(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>),\n        _ResidualDownSamplingBlock(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>),\n        _ResidualDownSamplingBlock(<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>),\n        _ResidualDownSamplingBlock(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>),\n        _ResidualDownSamplingBlock(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>),\n        _ResidualDownSamplingBlock(<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>),\n        BatchNorm2d(<span class=\"pl-c1\">64</span>), ReLU(<span class=\"pl-c1\">True</span>), Flatten(),\n        spectral_norm(Linear(<span class=\"pl-c1\">256</span>, <span class=\"pl-c1\">1</span>))\n    ).to(device)\n\n    opt <span class=\"pl-k\">=</span> Adam(model.parameters())\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Train</span>\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">20000</span>):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Starting epoch <span class=\"pl-c1\">%d</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> epoch)\n\n        x, y <span class=\"pl-k\">=</span> torch.randn((args.batch, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>), <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>device), torch.rand((args.batch, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>device)\n        y_hat <span class=\"pl-k\">=</span> model(x)\n\n        opt.zero_grad()\n        loss <span class=\"pl-k\">=</span> binary_cross_entropy_with_logits(y_hat, y)\n        loss.backward()\n        opt.step()</pre></div>", "body_text": "Please, find the script below. The discriminator network is the same that I'm using in my project.\nOn the same machine described at the beginning of the issue, at each epoch the GPU memory increases by ~2Mb (Nvidia-smi visual stats). If you let this script run for a long time, then it gives you out of memory.\nRemoving the spectral_norm() calls in _ResidualDownSamplingBlock stabilizes memory consumption.\nLet me know if I can do anything else.\nMorgan\nfrom argparse import ArgumentParser\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.cuda as cuda\n\nfrom torch.nn import Sequential, Conv2d, ReLU, Linear, Module, AvgPool2d, BatchNorm2d\nfrom torch.nn.functional import binary_cross_entropy_with_logits, avg_pool2d\nfrom torch.nn.utils import spectral_norm\nfrom torch.optim import Adam\n\n__author__ = 'Morgan Funtowicz'\n\n\nclass Flatten(Module):\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        return x\n\n\nclass _ResidualDownSamplingBlock(Module):\n\n    def __init__(self, n_in, n_out, ksize, stride=1, padding=1):\n        super().__init__()\n        self._f = Sequential(\n            ReLU(),\n            spectral_norm(Conv2d(n_in, n_out, ksize, stride, padding)),\n            ReLU(True),\n            spectral_norm(Conv2d(n_out, n_out, ksize, stride, padding)),\n            AvgPool2d(2, 2)\n        )\n        self._sc = spectral_norm(Conv2d(n_in, n_out, 1, padding=0))\n\n    def forward(self, x):\n        return avg_pool2d(self._sc(x), 2, 2) + self._f(x)\n\n\nif __name__ == '__main__':\n    # Ensure Tensor are allocated as FloatTensor\n    cudnn.benchmark = True\n    torch.set_default_tensor_type('torch.FloatTensor')\n    torch.set_default_dtype(torch.float32)\n\n    # Parse provided arguments\n    args_parser = ArgumentParser()\n    args_parser.add_argument('-d', type=int, default=-1, dest='device', help='Device to use for training (-1 = CPU)')\n    args_parser.add_argument('-b', type=int, default=-16, dest='batch', help='Size of the minibatch')\n\n    args = args_parser.parse_args()\n    args.gpu = args.device >= 0 and cuda.is_available()\n    device = torch.device(\"cuda:%d\" % args.device if args.gpu else \"cpu\")\n\n    # Define the model & Optimizer\n    model = Sequential(\n        _ResidualDownSamplingBlock(3, 64, ksize=3),\n        _ResidualDownSamplingBlock(64, 64, ksize=3),\n        _ResidualDownSamplingBlock(64, 128, ksize=3),\n        _ResidualDownSamplingBlock(128, 128, ksize=3),\n        _ResidualDownSamplingBlock(128, 128, ksize=3),\n        _ResidualDownSamplingBlock(128, 64, ksize=3),\n        BatchNorm2d(64), ReLU(True), Flatten(),\n        spectral_norm(Linear(256, 1))\n    ).to(device)\n\n    opt = Adam(model.parameters())\n\n    # Train\n    for epoch in range(20000):\n        print('Starting epoch %d' % epoch)\n\n        x, y = torch.randn((args.batch, 3, 128, 128), device=device), torch.rand((args.batch, 1), device=device)\n        y_hat = model(x)\n\n        opt.zero_grad()\n        loss = binary_cross_entropy_with_logits(y_hat, y)\n        loss.backward()\n        opt.step()", "body": "Please, find the script below. The discriminator network is the same that I'm using in my project. \r\n\r\nOn the same machine described at the beginning of the issue, at each epoch the GPU memory increases by ~2Mb (Nvidia-smi visual stats). If you let this script run for a long time, then it gives you out of memory.\r\n\r\nRemoving the spectral_norm() calls in _ResidualDownSamplingBlock stabilizes memory consumption.\r\n\r\nLet me know if I can do anything else.\r\nMorgan\r\n\r\n```python\r\nfrom argparse import ArgumentParser\r\n\r\nimport torch\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.cuda as cuda\r\n\r\nfrom torch.nn import Sequential, Conv2d, ReLU, Linear, Module, AvgPool2d, BatchNorm2d\r\nfrom torch.nn.functional import binary_cross_entropy_with_logits, avg_pool2d\r\nfrom torch.nn.utils import spectral_norm\r\nfrom torch.optim import Adam\r\n\r\n__author__ = 'Morgan Funtowicz'\r\n\r\n\r\nclass Flatten(Module):\r\n    def forward(self, x):\r\n        x = x.view(x.size(0), -1)\r\n        return x\r\n\r\n\r\nclass _ResidualDownSamplingBlock(Module):\r\n\r\n    def __init__(self, n_in, n_out, ksize, stride=1, padding=1):\r\n        super().__init__()\r\n        self._f = Sequential(\r\n            ReLU(),\r\n            spectral_norm(Conv2d(n_in, n_out, ksize, stride, padding)),\r\n            ReLU(True),\r\n            spectral_norm(Conv2d(n_out, n_out, ksize, stride, padding)),\r\n            AvgPool2d(2, 2)\r\n        )\r\n        self._sc = spectral_norm(Conv2d(n_in, n_out, 1, padding=0))\r\n\r\n    def forward(self, x):\r\n        return avg_pool2d(self._sc(x), 2, 2) + self._f(x)\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Ensure Tensor are allocated as FloatTensor\r\n    cudnn.benchmark = True\r\n    torch.set_default_tensor_type('torch.FloatTensor')\r\n    torch.set_default_dtype(torch.float32)\r\n\r\n    # Parse provided arguments\r\n    args_parser = ArgumentParser()\r\n    args_parser.add_argument('-d', type=int, default=-1, dest='device', help='Device to use for training (-1 = CPU)')\r\n    args_parser.add_argument('-b', type=int, default=-16, dest='batch', help='Size of the minibatch')\r\n\r\n    args = args_parser.parse_args()\r\n    args.gpu = args.device >= 0 and cuda.is_available()\r\n    device = torch.device(\"cuda:%d\" % args.device if args.gpu else \"cpu\")\r\n\r\n    # Define the model & Optimizer\r\n    model = Sequential(\r\n        _ResidualDownSamplingBlock(3, 64, ksize=3),\r\n        _ResidualDownSamplingBlock(64, 64, ksize=3),\r\n        _ResidualDownSamplingBlock(64, 128, ksize=3),\r\n        _ResidualDownSamplingBlock(128, 128, ksize=3),\r\n        _ResidualDownSamplingBlock(128, 128, ksize=3),\r\n        _ResidualDownSamplingBlock(128, 64, ksize=3),\r\n        BatchNorm2d(64), ReLU(True), Flatten(),\r\n        spectral_norm(Linear(256, 1))\r\n    ).to(device)\r\n\r\n    opt = Adam(model.parameters())\r\n\r\n    # Train\r\n    for epoch in range(20000):\r\n        print('Starting epoch %d' % epoch)\r\n\r\n        x, y = torch.randn((args.batch, 3, 128, 128), device=device), torch.rand((args.batch, 1), device=device)\r\n        y_hat = model(x)\r\n\r\n        opt.zero_grad()\r\n        loss = binary_cross_entropy_with_logits(y_hat, y)\r\n        loss.backward()\r\n        opt.step()\r\n```"}