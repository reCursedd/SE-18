{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199532181", "pull_request_review_id": 133668612, "id": 199532181, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5OTUzMjE4MQ==", "diff_hunk": "@@ -0,0 +1,375 @@\n+import copy\n+\n+import torch\n+from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors, \\\n+    _take_tensors\n+\n+from torch.cuda.comm import broadcast_coalesced\n+from torch.cuda import nccl\n+import torch.distributed.c10d as c10d\n+\n+from ..modules import Module\n+from .replicate import replicate\n+from .scatter_gather import scatter_kwargs, gather\n+from .parallel_apply import parallel_apply\n+\n+\n+class _DistributedDataParallelC10d(Module):\n+    r\"\"\"Implements distributed data parallelism that is based on c10d at the\n+    module level.\n+\n+    Currently this module is EXPERIMENTAL ONLY and should not be\n+    used by normal users. Instead, please use DistributedDataParallel.\n+\n+    This container parallelizes the application of the given module by\n+    splitting the input across the specified devices by chunking in the batch\n+    dimension. The module is replicated on each machine and each device, and\n+    each such replica handles a portion of the input. During the backwards\n+    pass, gradients from each node are averaged.\n+\n+    The batch size should be larger than the number of GPUs used locally. It\n+    should also be an integer multiple of the number of GPUs so that each chunk\n+    is the same size (so that each GPU processes the same number of samples).\n+\n+    See also: :ref:`distributed-basics` and :ref:`cuda-nn-dataparallel-instead`.\n+    The same constraints on input as in :class:`torch.nn.DataParallel` apply.\n+\n+    Creation of this class requires the c10d process group to be already\n+    initialized. This class will basically operate on the provided c10d\n+    process group.\n+\n+    .. warning::\n+        This module works only with the ``gloo`` and ``nccl`` process groups.\n+\n+    .. warning::\n+        Constructor, forward method, and differentiation of the output (or a\n+        function of the output of this module) is a distributed synchronization\n+        point. Take that into account in case different processes might be\n+        executing different code.\n+\n+    .. warning::\n+        This module assumes all parameters are registered in the model by the\n+        time it is created. No parameters should be added nor removed later.\n+        Same applies to buffers.\n+\n+    -- warning::\n+        This module assumes all parameters are registered in the model of each\n+        distributed processes are in the same order. The module itself will\n+        conduct gradient all-reduction following the reverse order of the\n+        registered parameters of the model. In other wise, it is users'\n+        responsibility to ensure that each distributed process has the exact\n+        same model and thus the exact parameter registeration order.\n+\n+    .. warning::\n+        This module assumes all buffers and gradients are dense.\n+\n+    .. warning::\n+        This module doesn't work with :func:`torch.autograd.grad` (i.e. it will\n+        only work if gradients are to be accumulated in ``.grad`` attributes of\n+        parameters).\n+\n+    .. warning::\n+        If you plan on using this module with a ``nccl`` process group or\n+        a ``gloo`` process group (that uses Infiniband), together with a\n+        DataLoader that uses multiple workers, please change the multiprocessing\n+        start method to ``forkserver`` (Python 3 only) or ``spawn``.\n+        Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe,\n+        and you will likely experience deadlocks if you don't change this\n+        setting.\n+\n+    .. note::\n+        Parameters are never broadcast between processes. The module performs\n+        an all-reduce step on gradients and assumes that they will be modified\n+        by the optimizer in all processes in the same way. Buffers\n+        (e.g. BatchNorm stats) are broadcast from the module in process of rank\n+        0, to all other replicas in the system in every iteration.\n+\n+    .. warning::\n+        Forward and backward hooks defined on :attr:`module` and its submodules\n+        won't be invoked anymore, unless the hooks are initialized in the\n+        :meth:`forward` method.\n+\n+    Args:\n+        module: module to be parallelized\n+        process_group: the c10d process group to be used for distributed data\n+                       all-reduction\n+        device_ids: CUDA devices (default: all devices)\n+        output_device: device location of output (default: device_ids[0])\n+        broadcast_buffers: flag that enables syncing (broadcasting) buffers of\n+                           the module at beginning of the forward function.\n+                           (default: True)\n+        bucket_cap_mb: DistributedDataParallelC10d will bucket parameters into\n+                       multiple buckets so that gradient reduction of each\n+                       bucket can potentially overlap with backward computation.\n+                       bucket_cap_mb controls the bucket size in MegaBytes (MB)\n+                       (default: 25)\n+\n+    Attributes:\n+        module (Module): the module to be parallelized\n+\n+    Example::\n+        >>> store = torch.distributed.c10d.FileStore(\"/tmp/tempfile.txt\")\n+        >>> pg = torch.distributed.c10d.ProcessGroupGloo(store, rank, world_size)\n+        >>> net = torch.nn._DistributedDataParallelC10d(model, pg)\n+    \"\"\"\n+    def __init__(self, module, process_group, device_ids=None,\n+                 output_device=None, dim=0, broadcast_buffers=True,\n+                 bucket_cap_mb=25):\n+\n+        super(_DistributedDataParallelC10d, self).__init__()\n+\n+        # Use all devices by default\n+        if device_ids is None:\n+            device_ids = list(range(torch.cuda.device_count()))\n+\n+        if output_device is None:\n+            output_device = device_ids[0]\n+\n+        self.dim = dim\n+        self.module = module\n+        self.process_group = process_group\n+        self.device_ids = device_ids\n+        self.output_device = output_device\n+        self.broadcast_buffers = broadcast_buffers\n+\n+        MB = 1024 * 1024\n+\n+        # used for intra-node param sync and inter-node sync as well\n+        self.broadcast_bucket_size = 25 * MB\n+\n+        # Sync params and buffers\n+        module_states = list(self.module.state_dict().values())\n+        if len(module_states) > 0:\n+            self._dist_broadcast_coalesced(module_states,\n+                                           self.broadcast_bucket_size)\n+\n+        if len(device_ids) > 1:\n+            # TODO: we don't need to replicate params in here. they're always going to\n+            # be broadcasted using larger blocks in broadcast_coalesced, so it might be\n+            # better to not pollute the caches with these small blocks\n+            self._module_copies = replicate(self.module, self.device_ids, detach=True)\n+            self._module_copies[0] = self.module\n+\n+            for module_copy in self._module_copies[1:]:\n+                for param, copy_param in zip(self.module.parameters(), module_copy.parameters()):\n+                    copy_param.requires_grad = param.requires_grad\n+\n+        else:\n+            self._module_copies = [self.module]\n+\n+        self.modules_params_data = [[] for _ in range(len(self.device_ids))]\n+        self.modules_buffers_data = [[] for _ in range(len(self.device_ids))]\n+\n+        for dev_idx, module in enumerate(self._module_copies):\n+            self.modules_params_data[dev_idx] = [p.data for p in module.parameters()]\n+            self.modules_buffers_data[dev_idx] = [b.data for b in module._all_buffers()]\n+\n+        bucket_bytes_cap = bucket_cap_mb * MB\n+\n+        # This is a triply-nested list where the \"dimensions\" are: devices, buckets, bucket_elems\n+        param_buckets = []\n+        # Split the parameters into buckets and by types as well\n+        param_buckets = [list(_take_tensors(m.parameters(), bucket_bytes_cap)) for m in self._module_copies]\n+\n+        self.bucket_sizes = []\n+        self.bucket_map = {}\n+\n+        # We transpose param_buckets, so the loop is over buckets.\n+        # param_buckets_tuple is a doubly-nested list with \"dims\": devices, bucket_elems\n+        for bucket_idx, param_buckets_tuple in enumerate(zip(*param_buckets)):\n+            self.bucket_sizes.append(0)\n+            # Now, we transpose again, so we iterate over bucket_elems, but getting tuples\n+            # of params from each device.\n+            for idx, param_tuple in enumerate(zip(*param_buckets_tuple)):\n+                if not param_tuple[0].requires_grad:\n+                    continue\n+                for p in param_tuple:\n+                    self.bucket_map[p] = (bucket_idx, idx)\n+                self.bucket_sizes[bucket_idx] += 1\n+\n+        self.buckets = [[[None for _ in range(self.bucket_sizes[i])]\n+                        for _ in range(len(self.device_ids))] for i in range(len(self.bucket_sizes))]\n+        # The number of params ready in each bucket\n+        self.buckets_ready_size = [[0 for _ in range(len(self.device_ids))] for i in range(len(self.bucket_sizes))]\n+\n+        # coalesced bucket for only device 0\n+        self.buckets_coalesced = [[] for _ in range(len(self.bucket_sizes))]\n+        # We will always reduce the bucket following the reverse order\n+        # that is, alway reduces following the order of: n - 1, n - 2, ..., 0\n+        self.next_bucket = len(self.bucket_sizes) - 1\n+        self.ready_buckets_not_reduced = set()\n+        self.reduction_works = [None for _ in range(len(self.bucket_sizes))]\n+\n+        self.devs_ready = [0 for _ in range(len(self.bucket_sizes))]\n+\n+        # default stream tracking to launch nccl reduce kernels\n+        self.default_streams = []\n+        for dev_id in self.device_ids:\n+            with torch.cuda.device(dev_id):\n+                self.default_streams.append(torch.cuda.current_stream())\n+\n+        self._register_grad_hooks()\n+\n+    def __getstate__(self):\n+        attrs = copy.copy(self.__dict__)\n+        del attrs['_grad_accs']\n+        return attrs\n+\n+    def __setstate__(self, state):\n+        super(_DistributedDataParallelC10d, self).__setstate__(state)\n+        self._register_grad_hooks()\n+\n+    def forward(self, *inputs, **kwargs):\n+        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n+        self._sync_params()\n+        if len(self.device_ids) == 1:\n+            return self.module(*inputs[0], **kwargs[0])\n+        outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)\n+        return self.gather(outputs, self.output_device)\n+\n+    def scatter(self, inputs, kwargs, device_ids):\n+        return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n+\n+    def parallel_apply(self, replicas, inputs, kwargs):\n+        return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n+\n+    def gather(self, outputs, output_device):\n+        return gather(outputs, output_device, dim=self.dim)\n+\n+    def train(self, mode=True):\n+        super(_DistributedDataParallelC10d, self).train(mode)\n+        for module in self._module_copies[1:]:\n+            module.train(mode)\n+\n+    def _dist_broadcast_coalesced(self, tensors, buffer_size):\n+        for tensors in _take_tensors(tensors, buffer_size):\n+            flat_tensors = _flatten_dense_tensors(tensors)\n+            c10d.broadcast(flat_tensors, 0, self.process_group).wait()\n+            for tensor, synced in zip(tensors, _unflatten_dense_tensors(flat_tensors, tensors)):\n+                tensor.copy_(synced)\n+\n+    def _sync_params(self):\n+        if len(self.device_ids) > 1:\n+            # intra-node parameter sync\n+            result = broadcast_coalesced(self.modules_params_data[0],\n+                                         self.device_ids,\n+                                         self.broadcast_bucket_size)\n+            for tensors, module_params_data in zip(result[1:], self.modules_params_data[1:]):\n+                for tensor, param_data in zip(tensors, module_params_data):\n+                    param_data.set_(tensor)\n+\n+        # module buffer sync\n+        if self.broadcast_buffers:\n+            if len(self.modules_buffers_data[0]) > 0:\n+                # cross-node buffer sync\n+                self._dist_broadcast_coalesced(self.modules_buffers_data[0],\n+                                               self.broadcast_bucket_size)\n+                if len(self.device_ids) > 1:\n+                    # intra-node buffer sync\n+                    result = broadcast_coalesced(self.modules_buffers_data[0],\n+                                                 self.device_ids,\n+                                                 self.broadcast_bucket_size)\n+                    for tensors, module_buffers_data in zip(result[1:], self.modules_buffers_data[1:]):\n+                        for tensor, buffer_data in zip(tensors, module_buffers_data):\n+                            buffer_data.set_(tensor)\n+\n+    def _register_grad_hooks(self):\n+        self._grad_accs = []  # need to keep them in scope\n+        for device_idx, module in enumerate(self._module_copies):\n+            for p in module.parameters():\n+                if p.requires_grad:\n+                    p_tmp = p.expand_as(p)\n+                    grad_acc = p_tmp.grad_fn.next_functions[0][0]\n+                    grad_acc.register_hook(self._make_param_hook(p, device_idx))\n+                    self._grad_accs.append(grad_acc)\n+\n+    def _make_param_hook(self, param, device_idx):\n+        bucket_idx, bucket_offset = self.bucket_map[param]\n+\n+        def distributed_data_parallel_hook(*unused):\n+            if param.grad.requires_grad:\n+                raise RuntimeError(\"DistributedDataParallelC10d only works \"\n+                                   \"with gradients that don't require grad\")\n+            bucket = self.buckets[bucket_idx][device_idx]\n+            bucket[bucket_offset] = param.grad.data\n+            self.buckets_ready_size[bucket_idx][device_idx] += 1\n+\n+            # We can flush these and save memory for replicas\n+            if device_idx > 0:\n+                param.grad = None\n+                param.data.set_()\n+\n+            # Current device's bucket is full\n+            if self.buckets_ready_size[bucket_idx][device_idx] == self.bucket_sizes[bucket_idx]:\n+                self.devs_ready[bucket_idx] += 1\n+                if self.devs_ready[bucket_idx] < len(self.device_ids):\n+                    return\n+\n+                # Now all devices's buckets with index: bucket_idx are ready\n+                if bucket_idx == self.next_bucket:\n+                    self._queue_reduction(bucket_idx)\n+                    self.next_bucket -= 1\n+                    # Now reduce anything that is ready but not yet reduced\n+                    if len(self.ready_buckets_not_reduced) > 0:\n+                        sorted_todo = sorted(self.ready_buckets_not_reduced, reverse=True)", "path": "torch/nn/parallel/distributed_c10d.py", "position": 314, "original_position": 314, "commit_id": "25389601de502db312299893f3c69bbe00f4ffca", "original_commit_id": "25389601de502db312299893f3c69bbe00f4ffca", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "FWIW this introduces a O(n^2 log n) complexity where n is the number of buckets. It would be much simpler and faster to keep an array of True/False flags for each bucket that tells you if it's ready or not. Then, once you get here, you start at `self.next_bucket` and iterate over the array until you reach a False, or it ends.", "created_at": "2018-07-02T15:17:53Z", "updated_at": "2018-11-23T15:46:39Z", "html_url": "https://github.com/pytorch/pytorch/pull/8584#discussion_r199532181", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8584", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/199532181"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8584#discussion_r199532181"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8584"}}, "body_html": "<p>FWIW this introduces a O(n^2 log n) complexity where n is the number of buckets. It would be much simpler and faster to keep an array of True/False flags for each bucket that tells you if it's ready or not. Then, once you get here, you start at <code>self.next_bucket</code> and iterate over the array until you reach a False, or it ends.</p>", "body_text": "FWIW this introduces a O(n^2 log n) complexity where n is the number of buckets. It would be much simpler and faster to keep an array of True/False flags for each bucket that tells you if it's ready or not. Then, once you get here, you start at self.next_bucket and iterate over the array until you reach a False, or it ends."}