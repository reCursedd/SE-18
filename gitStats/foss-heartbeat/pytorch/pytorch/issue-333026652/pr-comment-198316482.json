{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/198316482", "pull_request_review_id": 132226701, "id": 198316482, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5ODMxNjQ4Mg==", "diff_hunk": "@@ -0,0 +1,366 @@\n+import copy\n+\n+import torch\n+from torch.autograd import Variable\n+from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors, \\\n+    _take_tensors\n+\n+from torch.cuda.comm import broadcast_coalesced\n+from torch.cuda import nccl\n+import torch.distributed.c10d as c10d\n+\n+from ..modules import Module\n+from .replicate import replicate\n+from .scatter_gather import scatter_kwargs, gather\n+from .parallel_apply import parallel_apply\n+\n+\n+class DistributedDataParallelC10d(Module):\n+    r\"\"\"Implements distributed data parallelism that is based on c10d at the\n+    module level.\n+\n+    Currently this module is EXPERIMENTAL ONLY and should not be\n+    used by normal users. Instead, please use DistributedDataParallel.\n+\n+    This container parallelizes the application of the given module by\n+    splitting the input across the specified devices by chunking in the batch\n+    dimension. The module is replicated on each machine and each device, and\n+    each such replica handles a portion of the input. During the backwards\n+    pass, gradients from each node are averaged.\n+\n+    The batch size should be larger than the number of GPUs used locally. It\n+    should also be an integer multiple of the number of GPUs so that each chunk\n+    is the same size (so that each GPU processes the same number of samples).\n+\n+    See also: :ref:`distributed-basics` and :ref:`cuda-nn-dataparallel-instead`.\n+    The same constraints on input as in :class:`torch.nn.DataParallel` apply.\n+\n+    Creation of this class requires the c10d process group to be already\n+    initialized. This class will basically operate on the provided c10d\n+    process group.\n+\n+    .. warning::\n+        This module works only with the ``gloo`` and ``nccl`` process groups.\n+\n+    .. warning::\n+        Constructor, forward method, and differentiation of the output (or a\n+        function of the output of this module) is a distributed synchronization\n+        point. Take that into account in case different processes might be\n+        executing different code.\n+\n+    .. warning::\n+        This module assumes all parameters are registered in the model by the\n+        time it is created. No parameters should be added nor removed later.\n+        Same applies to buffers.\n+\n+    -- warning::\n+        This module assumes all parameters are registered in the model of each\n+        distributed processes are in the same order. The module itself will\n+        conduct gradient all-reduction following the reverse order of the\n+        registered parameters of the model. In other wise, it is users'\n+        responsibility to ensure that each distributed process has the exact\n+        same model and thus the exact parameter registeration order.\n+\n+    .. warning::\n+        This module assumes all buffers and gradients are dense.\n+\n+    .. warning::\n+        This module doesn't work with :func:`torch.autograd.grad` (i.e. it will\n+        only work if gradients are to be accumulated in ``.grad`` attributes of\n+        parameters).\n+\n+    .. warning::\n+        If you plan on using this module with a ``nccl`` process group or\n+        a ``gloo`` process group (that uses Infiniband), together with a\n+        DataLoader that uses multiple workers, please change the multiprocessing\n+        start method to ``forkserver`` (Python 3 only) or ``spawn``.\n+        Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe,\n+        and you will likely experience deadlocks if you don't change this\n+        setting.\n+\n+    .. note::\n+        Parameters are never broadcast between processes. The module performs\n+        an all-reduce step on gradients and assumes that they will be modified\n+        by the optimizer in all processes in the same way. Buffers\n+        (e.g. BatchNorm stats) are broadcast from the module in process of rank\n+        0, to all other replicas in the system in every iteration.\n+\n+    .. warning::\n+        Forward and backward hooks defined on :attr:`module` and its submodules\n+        won't be invoked anymore, unless the hooks are initialized in the\n+        :meth:`forward` method.\n+\n+    Args:\n+        module: module to be parallelized\n+        process_group: the c10d process group to be used for distributed data\n+                       all-reduction\n+        device_ids: CUDA devices (default: all devices)\n+        output_device: device location of output (default: device_ids[0])\n+        broadcast_buffers: flag that enables syncing (broadcasting) buffers of\n+                           the module at beginning of the forward function.\n+                           (default: True)\n+        bucket_cap_mb: DistributedDataParallelC10d will bucket parameters into\n+                       multiple buckets so that gradient reduction of each\n+                       bucket can potentially overlap with backward computation.\n+                       bucket_cap_mb controls the bucket size in MegaBytes (MB)\n+                       (default: 25)\n+\n+    Attributes:\n+        module (Module): the module to be parallelized\n+\n+    Example::\n+        >>> store = torch.distributed.c10d.FileStore(\"/tmp/tempfile.txt\")\n+        >>> pg = torch.distributed.c10d.ProcessGroupGloo(store, rank, world_size)\n+        >>> net = torch.nn.DistributedDataParallel(model, pg)\n+    \"\"\"\n+    def __init__(self, module, process_group, device_ids=None,\n+                 output_device=None, dim=0, broadcast_buffers=True,\n+                 bucket_cap_mb=25):\n+\n+        super(DistributedDataParallelC10d, self).__init__()\n+\n+        # Use all devices by default\n+        if device_ids is None:\n+            device_ids = list(range(torch.cuda.device_count()))\n+\n+        if output_device is None:\n+            output_device = device_ids[0]\n+\n+        self.dim = dim\n+        self.module = module\n+        self.process_group = process_group\n+        self.device_ids = device_ids\n+        self.output_device = output_device\n+        self.broadcast_buffers = broadcast_buffers\n+\n+        MB = 1024 * 1024\n+\n+        # used for intra-node param sync and inter-node sync as well\n+        self.broadcast_bucket_size = 25 * MB\n+\n+        # Sync params and buffers\n+        module_states = list(self.module.state_dict().values())\n+        if len(module_states) > 0:\n+            self._dist_broadcast_coalesced(module_states,\n+                                           self.broadcast_bucket_size)\n+\n+        if len(device_ids) > 1:\n+            # TODO: we don't need to replicate params in here. they're always going to\n+            # be broadcasted using larger blocks in broadcast_coalesced, so it might be\n+            # better to not pollute the caches with these small blocks\n+            self._module_copies = replicate(self.module, self.device_ids, detach=True)\n+            self._module_copies[0] = self.module\n+\n+            for module_copy in self._module_copies[1:]:\n+                for param, copy_param in zip(self.module.parameters(), module_copy.parameters()):\n+                    copy_param.requires_grad = param.requires_grad\n+\n+        else:\n+            self._module_copies = [self.module]\n+\n+        self.modules_params_data = [[] for _ in range(len(self.device_ids))]\n+        self.modules_buffers_data = [[] for _ in range(len(self.device_ids))]\n+\n+        for dev_idx, module in enumerate(self._module_copies):\n+            self.modules_params_data[dev_idx] = [p.data for p in module.parameters()]\n+            self.modules_buffers_data[dev_idx] = [b.data for b in module._all_buffers()]\n+\n+        bucket_bytes_cap = bucket_cap_mb * MB\n+\n+        # This is a triply-nested list where the \"dimensions\" are: devices, buckets, bucket_elems\n+        param_buckets = []\n+        # Split the parameters into buckets and by types as well\n+        for dev_idx, module in enumerate(self._module_copies):\n+            param_buckets.append(list(_take_tensors(module.parameters(), bucket_bytes_cap)))", "path": "torch/nn/parallel/distributed_c10d.py", "position": null, "original_position": 174, "commit_id": "25389601de502db312299893f3c69bbe00f4ffca", "original_commit_id": "084f926f539a57dda7ffb25bd17988304d116d19", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "Done", "created_at": "2018-06-26T22:24:43Z", "updated_at": "2018-11-23T15:46:24Z", "html_url": "https://github.com/pytorch/pytorch/pull/8584#discussion_r198316482", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8584", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/198316482"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8584#discussion_r198316482"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8584"}}, "body_html": "<p>Done</p>", "body_text": "Done", "in_reply_to_id": 198268076}