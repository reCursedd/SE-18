{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197571449", "pull_request_review_id": 131349730, "id": 197571449, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NzU3MTQ0OQ==", "diff_hunk": "@@ -0,0 +1,352 @@\n+import copy\n+\n+import torch\n+from torch.autograd import Variable\n+from torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors, \\\n+    _take_tensors\n+\n+from torch.cuda.comm import broadcast_coalesced\n+from torch.cuda import nccl\n+import torch.distributed.c10d as c10d\n+\n+from ..modules import Module\n+from .replicate import replicate\n+from .scatter_gather import scatter_kwargs, gather\n+from .parallel_apply import parallel_apply\n+\n+\n+class DistributedDataParallelC10d(Module):\n+    r\"\"\"Implements distributed data parallelism that is based on c10d at the\n+    module level.\n+\n+    Currently this module is EXPERIMENTAL ONLY and should not be\n+    used by normal users. Instead, please use DistributedDataParallel.\n+\n+    This container parallelizes the application of the given module by\n+    splitting the input across the specified devices by chunking in the batch\n+    dimension. The module is replicated on each machine and each device, and\n+    each such replica handles a portion of the input. During the backwards\n+    pass, gradients from each node are averaged.\n+\n+    The batch size should be larger than the number of GPUs used locally. It\n+    should also be an integer multiple of the number of GPUs so that each chunk\n+    is the same size (so that each GPU processes the same number of samples).\n+\n+    See also: :ref:`distributed-basics` and :ref:`cuda-nn-dataparallel-instead`.\n+    The same constraints on input as in :class:`torch.nn.DataParallel` apply.\n+\n+    Creation of this class requires the c10d process group to be already\n+    initialized. This class will basically operate on the provided c10d\n+    process group.\n+\n+    .. warning::\n+        This module works only with the ``gloo`` and ``nccl`` process groups.\n+\n+    .. warning::\n+        Constructor, forward method, and differentiation of the output (or a\n+        function of the output of this module) is a distributed synchronization\n+        point. Take that into account in case different processes might be\n+        executing different code.\n+\n+    .. warning::\n+        This module assumes all parameters are registered in the model by the\n+        time it is created. No parameters should be added nor removed later.\n+        Same applies to buffers.\n+\n+    -- warning::\n+        This module assumes all parameters are registered in the model of each\n+        distributed processes are in the same order. The module itself will\n+        conduct gradient all-reduction following the reverse order of the\n+        registered parameters of the model. In other wise, it is users'\n+        responsibility to ensure that each distributed process has the exact\n+        same model and thus the exact parameter registeration order.\n+\n+    .. warning::\n+        This module assumes all buffers and gradients are dense.\n+\n+    .. warning::\n+        This module doesn't work with :func:`torch.autograd.grad` (i.e. it will\n+        only work if gradients are to be accumulated in ``.grad`` attributes of\n+        parameters).\n+\n+    .. warning::\n+        If you plan on using this module with a ``nccl`` process group or\n+        a ``gloo`` process group (that uses Infiniband), together with a\n+        DataLoader that uses multiple workers, please change the multiprocessing\n+        start method to ``forkserver`` (Python 3 only) or ``spawn``.\n+        Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe,\n+        and you will likely experience deadlocks if you don't change this\n+        setting.\n+\n+    .. note::\n+        Parameters are never broadcast between processes. The module performs\n+        an all-reduce step on gradients and assumes that they will be modified\n+        by the optimizer in all processes in the same way. Buffers\n+        (e.g. BatchNorm stats) are broadcast from the module in process of rank\n+        0, to all other replicas in the system in every iteration.\n+\n+    .. warning::\n+        Forward and backward hooks defined on :attr:`module` and its submodules\n+        won't be invoked anymore, unless the hooks are initialized in the\n+        :meth:`forward` method.\n+\n+    Args:\n+        module: module to be parallelized\n+        process_group: the c10d process group to be used for distributed data\n+                       all-reduction\n+        device_ids: CUDA devices (default: all devices)\n+        output_device: device location of output (default: device_ids[0])\n+        broadcast_buffers: flag that enables syncing (broadcasting) buffers of\n+                           the module at beginning of the forward function.\n+                           (default: True)\n+\n+    Attributes:\n+        module (Module): the module to be parallelized\n+\n+    Example::\n+        >>> store = torch.distributed.c10d.FileStore(\"/tmp/tempfile.txt\")\n+        >>> pg = torch.distributed.c10d.ProcessGroupGloo(store, rank, world_size)\n+        >>> net = torch.nn.DistributedDataParallel(model, pg)\n+    \"\"\"\n+    def __init__(self, module, process_group, device_ids=None,\n+                 output_device=None, dim=0, broadcast_buffers=True):\n+\n+        super(DistributedDataParallelC10d, self).__init__()\n+\n+        # Use all devices by default\n+        if device_ids is None:\n+            device_ids = list(range(torch.cuda.device_count()))\n+\n+        if output_device is None:\n+            output_device = device_ids[0]\n+\n+        self.dim = dim\n+        self.module = module\n+        self.process_group = process_group\n+        self.device_ids = device_ids\n+        self.output_device = output_device\n+        self.broadcast_buffers = broadcast_buffers\n+\n+        MB = 1024 * 1024\n+\n+        # used for intra-node param sync and inter-node sync as well\n+        self.broadcast_bucket_size = 25 * MB\n+\n+        # Sync params and buffers\n+        module_states = list(self.module.state_dict().values())\n+        if len(module_states) > 0:\n+            self._dist_broadcast_coalesced(module_states,\n+                                           self.broadcast_bucket_size)\n+\n+        if len(device_ids) > 1:\n+            # TODO: we don't need to replicate params in here. they're always going to\n+            # be broadcasted using larger blocks in broadcast_coalesced, so it might be\n+            # better to not pollute the caches with these small blocks\n+            self._module_copies = replicate(self.module, self.device_ids, detach=True)\n+            self._module_copies[0] = self.module\n+\n+            for module_copy in self._module_copies[1:]:\n+                for param, copy_param in zip(self.module.parameters(), module_copy.parameters()):\n+                    copy_param.requires_grad = param.requires_grad\n+\n+        else:\n+            self._module_copies = [self.module]\n+\n+        bucket_bytes_cap = 25 * MB\n+\n+        # This is a triply-nested list where the \"dimensions\" are: devices, buckets, bucket_elems\n+        param_buckets = []\n+        # Split the parameters into buckets and by types as well\n+        for dev_idx, module in enumerate(self._module_copies):\n+            param_buckets.append(list(_take_tensors(module.parameters(), bucket_bytes_cap)))\n+\n+        self.bucket_sizes = []\n+        self.bucket_map = {}\n+\n+        # We transpose param_buckets, so the loop is over buckets.\n+        # param_buckets_tuple is a doubly-nested list with \"dims\": devices, bucket_elems\n+        for bucket_idx, param_buckets_tuple in enumerate(zip(*param_buckets)):\n+            self.bucket_sizes.append(0)\n+            # Now, we transpose again, so we iterate over bucket_elems, but getting tuples\n+            # of params from each device.\n+            for idx, param_tuple in enumerate(zip(*param_buckets_tuple)):\n+                if not param_tuple[0].requires_grad:\n+                    continue\n+                for p in param_tuple:\n+                    self.bucket_map[p] = bucket_idx\n+                self.bucket_sizes[bucket_idx] += 1\n+\n+        self.buckets = [[[] for _ in range(len(self.device_ids))] for _ in range(len(self.bucket_sizes))]\n+        # coalesced bucket for only device 0\n+        self.buckets_coalesced = [[] for _ in range(len(self.bucket_sizes))]\n+\n+        # We will always reduce the bucket following the reverse order\n+        self.bucket_order = list(range(len(self.bucket_sizes)))\n+        self.bucket_order.reverse()\n+        self.buckets_to_reduce = copy.copy(self.bucket_order)\n+\n+        self.buckets_ready = set()\n+        self.reduction_works = []\n+\n+        self.devs_ready = [0 for _ in range(len(self.bucket_sizes))]\n+\n+        # default stream tracking to launch nccl reduce kernels\n+        self.default_streams = []\n+        for dev_id in self.device_ids:\n+            with torch.cuda.device(dev_id):\n+                self.default_streams.append(torch.cuda.current_stream())\n+\n+        self._register_grad_hooks()\n+\n+    def __getstate__(self):\n+        attrs = copy.copy(self.__dict__)\n+        return attrs\n+\n+    def __setstate__(self, state):\n+        super(DistributedDataParallelC10d, self).__setstate__(state)\n+        self._register_grad_hooks()\n+\n+    def forward(self, *inputs, **kwargs):\n+        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n+        self._sync_params()\n+        if len(self.device_ids) == 1:\n+            return self.module(*inputs[0], **kwargs[0])\n+        outputs = self.parallel_apply(self._module_copies[:len(inputs)], inputs, kwargs)\n+        return self.gather(outputs, self.output_device)\n+\n+    def scatter(self, inputs, kwargs, device_ids):\n+        return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n+\n+    def parallel_apply(self, replicas, inputs, kwargs):\n+        return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n+\n+    def gather(self, outputs, output_device):\n+        return gather(outputs, output_device, dim=self.dim)\n+\n+    def train(self, mode=True):\n+        super(DistributedDataParallelC10d, self).train(mode)\n+        for module in self._module_copies[1:]:\n+            module.train(mode)\n+\n+    def _dist_broadcast_coalesced(self, tensors, buffer_size):\n+        for tensors in _take_tensors(tensors, buffer_size):\n+            flat_tensors = _flatten_dense_tensors(tensors)\n+            work = c10d.broadcast(flat_tensors, 0, self.process_group)\n+            work.wait()", "path": "torch/nn/parallel/distributed_c10d.py", "position": null, "original_position": 235, "commit_id": "25389601de502db312299893f3c69bbe00f4ffca", "original_commit_id": "39a6c4123c6fa9eb9e56cb827f1a041c63797329", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "body": "Ok", "created_at": "2018-06-22T21:15:55Z", "updated_at": "2018-11-23T15:46:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/8584#discussion_r197571449", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8584", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/197571449"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8584#discussion_r197571449"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8584"}}, "body_html": "<p>Ok</p>", "body_text": "Ok", "in_reply_to_id": 197407030}