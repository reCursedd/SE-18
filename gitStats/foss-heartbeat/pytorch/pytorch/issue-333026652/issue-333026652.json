{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8584", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8584/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8584/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8584/events", "html_url": "https://github.com/pytorch/pytorch/pull/8584", "id": 333026652, "node_id": "MDExOlB1bGxSZXF1ZXN0MTk1MzU2MTY5", "number": 8584, "title": "[c10d] Distributed Data Parallel Module Implementation", "user": {"login": "teng-li", "id": 8120856, "node_id": "MDQ6VXNlcjgxMjA4NTY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8120856?v=4", "gravatar_id": "", "url": "https://api.github.com/users/teng-li", "html_url": "https://github.com/teng-li", "followers_url": "https://api.github.com/users/teng-li/followers", "following_url": "https://api.github.com/users/teng-li/following{/other_user}", "gists_url": "https://api.github.com/users/teng-li/gists{/gist_id}", "starred_url": "https://api.github.com/users/teng-li/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/teng-li/subscriptions", "organizations_url": "https://api.github.com/users/teng-li/orgs", "repos_url": "https://api.github.com/users/teng-li/repos", "events_url": "https://api.github.com/users/teng-li/events{/privacy}", "received_events_url": "https://api.github.com/users/teng-li/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953883, "node_id": "MDU6TGFiZWw2Nzk5NTM4ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/distributed", "name": "distributed", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-06-17T01:12:42Z", "updated_at": "2018-11-23T15:46:39Z", "closed_at": "2018-06-29T00:26:34Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/8584", "html_url": "https://github.com/pytorch/pytorch/pull/8584", "diff_url": "https://github.com/pytorch/pytorch/pull/8584.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/8584.patch"}, "body_html": "<p>This is an initial implementation of Distributed Data Parallel module for c10d GLOO and NCCL backend.</p>\n<p>Have done performance testing and made sure that both single GPU / process and multi-GPU / process are able to overlap communication with BW computation</p>\n<p>The idea is, DDP will bucket parameters and do all reduce in the reverse order of the bucket. Since all C10D ops are async ops, no more dedicated thread is needed and we simply queue the all-reduce kernels once the bucket is ready following the deterministic reduction order.</p>\n<p>Tested with 8 nodes 64 GPUs, ResNet 50, hit the required accuracy within 90 epochs</p>", "body_text": "This is an initial implementation of Distributed Data Parallel module for c10d GLOO and NCCL backend.\nHave done performance testing and made sure that both single GPU / process and multi-GPU / process are able to overlap communication with BW computation\nThe idea is, DDP will bucket parameters and do all reduce in the reverse order of the bucket. Since all C10D ops are async ops, no more dedicated thread is needed and we simply queue the all-reduce kernels once the bucket is ready following the deterministic reduction order.\nTested with 8 nodes 64 GPUs, ResNet 50, hit the required accuracy within 90 epochs", "body": "This is an initial implementation of Distributed Data Parallel module for c10d GLOO and NCCL backend.  \r\n\r\nHave done performance testing and made sure that both single GPU / process and multi-GPU / process are able to overlap communication with BW computation\r\n\r\nThe idea is, DDP will bucket parameters and do all reduce in the reverse order of the bucket. Since all C10D ops are async ops, no more dedicated thread is needed and we simply queue the all-reduce kernels once the bucket is ready following the deterministic reduction order.\r\n\r\nTested with 8 nodes 64 GPUs, ResNet 50, hit the required accuracy within 90 epochs"}