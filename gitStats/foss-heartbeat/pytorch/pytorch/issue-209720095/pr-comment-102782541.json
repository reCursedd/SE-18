{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/102782541", "pull_request_review_id": 23550185, "id": 102782541, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMjc4MjU0MQ==", "diff_hunk": "@@ -0,0 +1,240 @@\n+import numpy as np\n+import torch\n+from torch.autograd import Variable\n+\n+\n+def uniform(tensor, a=0, b=1):\n+    \"\"\"Fills the input Tensor or Variable with values drawn from a uniform U(a,b)\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        a: the lower bound of the uniform distribution\n+        b: the upper bound of the uniform distribution\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.uniform(w)\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        uniform(tensor.data, a=a, b=b)\n+        return tensor\n+    else:\n+        return tensor.uniform_(a, b)\n+\n+\n+def normal(tensor, mean=0, std=1):\n+    \"\"\"Fills the input Tensor or Variable with values drawn from a normal distribution with the given mean and std\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        mean: the mean of the normal distribution\n+        std: the standard deviation of the normal distribution\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.normal(w)\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        normal(tensor.data, mean=mean, std=std)\n+        return tensor\n+    else:\n+        return tensor.normal_(mean, std)\n+\n+\n+def constant(tensor, val):\n+    \"\"\"Fills the input Tensor or Variable with the value `val`\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        val: the value to fill the tensor with\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.constant(w)\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        constant(tensor.data, val)\n+        return tensor\n+    else:\n+        return tensor.fill_(val)\n+\n+\n+def _calculate_fan_in_and_fan_out(tensor):\n+    if tensor.ndimension() < 2:\n+        raise ValueError(\"fan in and fan out can not be computed for tensor of size \", tensor.size())\n+\n+    if tensor.ndimension() == 2:  # Linear\n+        fan_in = tensor.size(1)\n+        fan_out = tensor.size(0)\n+    else:\n+        num_input_fmaps = tensor.size(1)\n+        num_output_fmaps = tensor.size(0)\n+        receptive_field_size = np.prod(tensor.numpy().shape[2:])\n+        fan_in = num_input_fmaps * receptive_field_size\n+        fan_out = num_output_fmaps * receptive_field_size\n+\n+    return fan_in, fan_out\n+\n+\n+def xavier_uniform(tensor, gain=1):\n+    \"\"\"Fills the input Tensor or Variable with values according to the method described in \"Understanding the difficulty of training\n+       deep feedforward neural networks\" - Glorot, X. and Bengio, Y., using a uniform distribution.\n+\n+       The resulting tensor will have values sampled from U(-a, a) where a = gain * sqrt(2/(fan_in + fan_out))\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        gain: an optional scaling factor to be applied\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.xavier_uniform(w, gain=np.sqrt(2.0))\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        xavier_uniform(tensor.data, gain=gain)\n+        return tensor\n+    else:\n+        fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n+        std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n+        a = np.sqrt(3.0) * std\n+        return tensor.uniform_(-a, a)\n+\n+\n+def xavier_normal(tensor, gain=1):\n+    \"\"\"Fills the input Tensor or Variable with values according to the method described in \"Understanding the difficulty of training\n+       deep feedforward neural networks\" - Glorot, X. and Bengio, Y., using a normal distribution.\n+\n+       The resulting tensor will have values sampled from normal distribution with mean=0 and\n+       std = gain * sqrt(2/(fan_in + fan_out))\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        gain: an optional scaling factor to be applied\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.xavier_normal(w, gain=np.sqrt(2.0))\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        xavier_normal(tensor.data, gain=gain)\n+        return tensor\n+    else:\n+        fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n+        std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n+        return tensor.normal_(0, std)\n+\n+\n+def kaiming_uniform(tensor, gain=1):", "path": "torch/nn/init.py", "position": null, "original_position": 127, "commit_id": "7d979075cc3167a956b14cd3cfd05a56fde264fa", "original_commit_id": "6c88e7dd4785e16b1d26766c09b12808bc7e2f82", "user": {"login": "alykhantejani", "id": 687194, "node_id": "MDQ6VXNlcjY4NzE5NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/687194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alykhantejani", "html_url": "https://github.com/alykhantejani", "followers_url": "https://api.github.com/users/alykhantejani/followers", "following_url": "https://api.github.com/users/alykhantejani/following{/other_user}", "gists_url": "https://api.github.com/users/alykhantejani/gists{/gist_id}", "starred_url": "https://api.github.com/users/alykhantejani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alykhantejani/subscriptions", "organizations_url": "https://api.github.com/users/alykhantejani/orgs", "repos_url": "https://api.github.com/users/alykhantejani/repos", "events_url": "https://api.github.com/users/alykhantejani/events{/privacy}", "received_events_url": "https://api.github.com/users/alykhantejani/received_events", "type": "User", "site_admin": false}, "body": "It's not explicitly described in the paper, and in that particular work they use a Gaussian distribution. However, I believe the condition of the initialization scheme is that the weights be symmetrically distributed around zero, so a uniform distribution is also a valid choice with `std = gain * np.sqrt(1.0/n)` ", "created_at": "2017-02-23T18:27:54Z", "updated_at": "2018-11-23T15:32:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/833#discussion_r102782541", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/833", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/102782541"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/833#discussion_r102782541"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/833"}}, "body_html": "<p>It's not explicitly described in the paper, and in that particular work they use a Gaussian distribution. However, I believe the condition of the initialization scheme is that the weights be symmetrically distributed around zero, so a uniform distribution is also a valid choice with <code>std = gain * np.sqrt(1.0/n)</code></p>", "body_text": "It's not explicitly described in the paper, and in that particular work they use a Gaussian distribution. However, I believe the condition of the initialization scheme is that the weights be symmetrically distributed around zero, so a uniform distribution is also a valid choice with std = gain * np.sqrt(1.0/n)", "in_reply_to_id": 102768794}