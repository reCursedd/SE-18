{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/102783732", "pull_request_review_id": 23551367, "id": 102783732, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwMjc4MzczMg==", "diff_hunk": "@@ -0,0 +1,240 @@\n+import numpy as np\n+import torch\n+from torch.autograd import Variable\n+\n+\n+def uniform(tensor, a=0, b=1):\n+    \"\"\"Fills the input Tensor or Variable with values drawn from a uniform U(a,b)\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        a: the lower bound of the uniform distribution\n+        b: the upper bound of the uniform distribution\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.uniform(w)\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        uniform(tensor.data, a=a, b=b)\n+        return tensor\n+    else:\n+        return tensor.uniform_(a, b)\n+\n+\n+def normal(tensor, mean=0, std=1):\n+    \"\"\"Fills the input Tensor or Variable with values drawn from a normal distribution with the given mean and std\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        mean: the mean of the normal distribution\n+        std: the standard deviation of the normal distribution\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.normal(w)\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        normal(tensor.data, mean=mean, std=std)\n+        return tensor\n+    else:\n+        return tensor.normal_(mean, std)\n+\n+\n+def constant(tensor, val):\n+    \"\"\"Fills the input Tensor or Variable with the value `val`\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        val: the value to fill the tensor with\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.constant(w)\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        constant(tensor.data, val)\n+        return tensor\n+    else:\n+        return tensor.fill_(val)\n+\n+\n+def _calculate_fan_in_and_fan_out(tensor):\n+    if tensor.ndimension() < 2:\n+        raise ValueError(\"fan in and fan out can not be computed for tensor of size \", tensor.size())\n+\n+    if tensor.ndimension() == 2:  # Linear\n+        fan_in = tensor.size(1)\n+        fan_out = tensor.size(0)\n+    else:\n+        num_input_fmaps = tensor.size(1)\n+        num_output_fmaps = tensor.size(0)\n+        receptive_field_size = np.prod(tensor.numpy().shape[2:])\n+        fan_in = num_input_fmaps * receptive_field_size\n+        fan_out = num_output_fmaps * receptive_field_size\n+\n+    return fan_in, fan_out\n+\n+\n+def xavier_uniform(tensor, gain=1):\n+    \"\"\"Fills the input Tensor or Variable with values according to the method described in \"Understanding the difficulty of training\n+       deep feedforward neural networks\" - Glorot, X. and Bengio, Y., using a uniform distribution.\n+\n+       The resulting tensor will have values sampled from U(-a, a) where a = gain * sqrt(2/(fan_in + fan_out))\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        gain: an optional scaling factor to be applied\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.xavier_uniform(w, gain=np.sqrt(2.0))\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        xavier_uniform(tensor.data, gain=gain)\n+        return tensor\n+    else:\n+        fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n+        std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n+        a = np.sqrt(3.0) * std\n+        return tensor.uniform_(-a, a)\n+\n+\n+def xavier_normal(tensor, gain=1):\n+    \"\"\"Fills the input Tensor or Variable with values according to the method described in \"Understanding the difficulty of training\n+       deep feedforward neural networks\" - Glorot, X. and Bengio, Y., using a normal distribution.\n+\n+       The resulting tensor will have values sampled from normal distribution with mean=0 and\n+       std = gain * sqrt(2/(fan_in + fan_out))\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        gain: an optional scaling factor to be applied\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.xavier_normal(w, gain=np.sqrt(2.0))\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        xavier_normal(tensor.data, gain=gain)\n+        return tensor\n+    else:\n+        fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n+        std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n+        return tensor.normal_(0, std)\n+\n+\n+def kaiming_uniform(tensor, gain=1):\n+    \"\"\"Fills the input Tensor or Variable with values according to the method described in \"Delving deep into rectifiers: Surpassing\n+       human-level performance on ImageNet classification\" - He, K. et al using a uniform distribution.\n+\n+       The resulting tensor will have values sampled from U(-a, a) where a = gain * sqrt(1/(fan_in))\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        gain: an optional scaling factor to be applied\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.kaiming_uniform(w, gain=np.sqrt(2.0))\n+    \"\"\"\n+\n+    if isinstance(tensor, Variable):\n+        kaiming_uniform(tensor.data, gain=gain)\n+        return tensor\n+    else:\n+        fan_in, _ = _calculate_fan_in_and_fan_out(tensor)\n+        std = gain * np.sqrt(1.0 / fan_in)\n+        a = np.sqrt(3.0) * std\n+        return tensor.uniform_(-a, a)\n+\n+\n+def kaiming_normal(tensor, gain=1):\n+    \"\"\"Fills the input Tensor or Variable with values according to the method described in \"Delving deep into rectifiers:\n+       Surpassing human-level performance on ImageNet classification\" - He, K. et al using a normal distribution.\n+\n+       The resulting tensor will have values sampled from normal distribution with mean=0 and\n+       std = gain * sqrt(1/(fan_in))\n+\n+    Args:\n+        tensor: a n-dimension torch.Tensor\n+        gain: an optional scaling factor to be applied\n+\n+    Examples:\n+        >>> w = torch.Tensor(3, 5)\n+        >>> nninit.kaiming_normal(w, gain=np.sqrt(2.0))\n+    \"\"\"\n+    if isinstance(tensor, Variable):\n+        kaiming_normal(tensor.data, gain=gain)\n+        return tensor\n+    else:\n+        fan_in, _ = _calculate_fan_in_and_fan_out(tensor)\n+        std = gain * np.sqrt(1.0 / fan_in)", "path": "torch/nn/init.py", "position": null, "original_position": 172, "commit_id": "7d979075cc3167a956b14cd3cfd05a56fde264fa", "original_commit_id": "6c88e7dd4785e16b1d26766c09b12808bc7e2f82", "user": {"login": "alykhantejani", "id": 687194, "node_id": "MDQ6VXNlcjY4NzE5NA==", "avatar_url": "https://avatars1.githubusercontent.com/u/687194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alykhantejani", "html_url": "https://github.com/alykhantejani", "followers_url": "https://api.github.com/users/alykhantejani/followers", "following_url": "https://api.github.com/users/alykhantejani/following{/other_user}", "gists_url": "https://api.github.com/users/alykhantejani/gists{/gist_id}", "starred_url": "https://api.github.com/users/alykhantejani/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alykhantejani/subscriptions", "organizations_url": "https://api.github.com/users/alykhantejani/orgs", "repos_url": "https://api.github.com/users/alykhantejani/repos", "events_url": "https://api.github.com/users/alykhantejani/events{/privacy}", "received_events_url": "https://api.github.com/users/alykhantejani/received_events", "type": "User", "site_admin": false}, "body": "@colesbury I think the default gain should probably be set to `np.sqrt(2.0)`, as `1` isn't very useful.\r\n\r\n`sqrt(2.0/n)` is valid for ReLU, but for leaky ReLU variants it should be `sqrt(2/n*(1 + a^2))`, which can be controlled via the `gain` parameter. For example see discussions in section 2 of [Kaiming's paper](https://arxiv.org/pdf/1502.01852.pdf) for a discussion of PReLU\r\n\r\n@szagoruyko I think this is why other frameworks (e.g. Lasagne) also implement this with a `gain` param", "created_at": "2017-02-23T18:32:53Z", "updated_at": "2018-11-23T15:32:31Z", "html_url": "https://github.com/pytorch/pytorch/pull/833#discussion_r102783732", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/833", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/102783732"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/833#discussion_r102783732"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/833"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> I think the default gain should probably be set to <code>np.sqrt(2.0)</code>, as <code>1</code> isn't very useful.</p>\n<p><code>sqrt(2.0/n)</code> is valid for ReLU, but for leaky ReLU variants it should be <code>sqrt(2/n*(1 + a^2))</code>, which can be controlled via the <code>gain</code> parameter. For example see discussions in section 2 of <a href=\"https://arxiv.org/pdf/1502.01852.pdf\" rel=\"nofollow\">Kaiming's paper</a> for a discussion of PReLU</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4953728\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/szagoruyko\">@szagoruyko</a> I think this is why other frameworks (e.g. Lasagne) also implement this with a <code>gain</code> param</p>", "body_text": "@colesbury I think the default gain should probably be set to np.sqrt(2.0), as 1 isn't very useful.\nsqrt(2.0/n) is valid for ReLU, but for leaky ReLU variants it should be sqrt(2/n*(1 + a^2)), which can be controlled via the gain parameter. For example see discussions in section 2 of Kaiming's paper for a discussion of PReLU\n@szagoruyko I think this is why other frameworks (e.g. Lasagne) also implement this with a gain param", "in_reply_to_id": 102692475}