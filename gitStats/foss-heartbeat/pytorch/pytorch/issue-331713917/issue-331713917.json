{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8392", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8392/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8392/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8392/events", "html_url": "https://github.com/pytorch/pytorch/issues/8392", "id": 331713917, "node_id": "MDU6SXNzdWUzMzE3MTM5MTc=", "number": 8392, "title": "[ONNX] Export fails when the same instance of a module appears multiple times in a module list", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-06-12T19:03:01Z", "updated_at": "2018-10-22T15:11:10Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Example:</p>\n<pre><code>import torch\n\nclass SomeModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(4, 4))\n\n    def forward(self, x):\n        return torch.mm(x, self.param)\n\nclass MultiModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.num_mods = 3\n        self.modlist = torch.nn.ModuleList([SomeModule()] * self.num_mods)\n\n    def forward(self, x):\n        for i in range(self.num_mods):\n            x = self.modlist[i](x)\n        return x\n\nmm = MultiModule()\nimport io\nf = io.BytesIO()\ntorch.onnx.export(mm, (torch.rand(5, 4),), f, verbose=True)\n</code></pre>\n<p>Output:</p>\n<pre><code>graph(%0 : Float(5, 4)\n      %1 : Float(4, 4)) {\n  %2 : Dynamic = onnx::Constant[value={0}](), scope: MultiModule/SomeModule\n  %3 : Float(5, 4) = onnx::Gemm[alpha=1, beta=0, broadcast=1](%0, %1, %2), scope: MultiModule/SomeModule\n  %4 : Dynamic = onnx::Constant[value={0}](), scope: MultiModule/SomeModule\n  %5 : Float(5, 4) = onnx::Gemm[alpha=1, beta=0, broadcast=1](%3, %1, %4), scope: MultiModule/SomeModule\n  %6 : Dynamic = onnx::Constant[value={0}](), scope: MultiModule/SomeModule\n  %7 : Float(5, 4) = onnx::Gemm[alpha=1, beta=0, broadcast=1](%5, %1, %6), scope: MultiModule/SomeModule\n  return (%7);\n}\n\nTraceback (most recent call last):\n  File \"multimodule.py\", line 25, in &lt;module&gt;\n    torch.onnx.export(mm, (torch.rand(5, 4),), f, verbose=True)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/__init__.py\", line 31, in export\n    return utils.export(*args, **kwargs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 87, in export\n    operator_export_type=operator_export_type)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 208, in _export\n    proto, export_map = graph.export(params, _onnx_opset_version, defer_weight_export, operator_export_type)\nRuntimeError: torch/csrc/jit/export.cpp:296: encodeBlock: Assertion `b-&gt;inputs().size() &gt;= num_initializers` failed.\n</code></pre>", "body_text": "Example:\nimport torch\n\nclass SomeModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(4, 4))\n\n    def forward(self, x):\n        return torch.mm(x, self.param)\n\nclass MultiModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.num_mods = 3\n        self.modlist = torch.nn.ModuleList([SomeModule()] * self.num_mods)\n\n    def forward(self, x):\n        for i in range(self.num_mods):\n            x = self.modlist[i](x)\n        return x\n\nmm = MultiModule()\nimport io\nf = io.BytesIO()\ntorch.onnx.export(mm, (torch.rand(5, 4),), f, verbose=True)\n\nOutput:\ngraph(%0 : Float(5, 4)\n      %1 : Float(4, 4)) {\n  %2 : Dynamic = onnx::Constant[value={0}](), scope: MultiModule/SomeModule\n  %3 : Float(5, 4) = onnx::Gemm[alpha=1, beta=0, broadcast=1](%0, %1, %2), scope: MultiModule/SomeModule\n  %4 : Dynamic = onnx::Constant[value={0}](), scope: MultiModule/SomeModule\n  %5 : Float(5, 4) = onnx::Gemm[alpha=1, beta=0, broadcast=1](%3, %1, %4), scope: MultiModule/SomeModule\n  %6 : Dynamic = onnx::Constant[value={0}](), scope: MultiModule/SomeModule\n  %7 : Float(5, 4) = onnx::Gemm[alpha=1, beta=0, broadcast=1](%5, %1, %6), scope: MultiModule/SomeModule\n  return (%7);\n}\n\nTraceback (most recent call last):\n  File \"multimodule.py\", line 25, in <module>\n    torch.onnx.export(mm, (torch.rand(5, 4),), f, verbose=True)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/__init__.py\", line 31, in export\n    return utils.export(*args, **kwargs)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 87, in export\n    operator_export_type=operator_export_type)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 208, in _export\n    proto, export_map = graph.export(params, _onnx_opset_version, defer_weight_export, operator_export_type)\nRuntimeError: torch/csrc/jit/export.cpp:296: encodeBlock: Assertion `b->inputs().size() >= num_initializers` failed.", "body": "Example:\r\n\r\n```\r\nimport torch\r\n\r\nclass SomeModule(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.param = torch.nn.Parameter(torch.rand(4, 4))\r\n\r\n    def forward(self, x):\r\n        return torch.mm(x, self.param)\r\n\r\nclass MultiModule(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.num_mods = 3\r\n        self.modlist = torch.nn.ModuleList([SomeModule()] * self.num_mods)\r\n\r\n    def forward(self, x):\r\n        for i in range(self.num_mods):\r\n            x = self.modlist[i](x)\r\n        return x\r\n\r\nmm = MultiModule()\r\nimport io\r\nf = io.BytesIO()\r\ntorch.onnx.export(mm, (torch.rand(5, 4),), f, verbose=True)\r\n```\r\n\r\n\r\nOutput:\r\n\r\n\r\n```\r\ngraph(%0 : Float(5, 4)\r\n      %1 : Float(4, 4)) {\r\n  %2 : Dynamic = onnx::Constant[value={0}](), scope: MultiModule/SomeModule\r\n  %3 : Float(5, 4) = onnx::Gemm[alpha=1, beta=0, broadcast=1](%0, %1, %2), scope: MultiModule/SomeModule\r\n  %4 : Dynamic = onnx::Constant[value={0}](), scope: MultiModule/SomeModule\r\n  %5 : Float(5, 4) = onnx::Gemm[alpha=1, beta=0, broadcast=1](%3, %1, %4), scope: MultiModule/SomeModule\r\n  %6 : Dynamic = onnx::Constant[value={0}](), scope: MultiModule/SomeModule\r\n  %7 : Float(5, 4) = onnx::Gemm[alpha=1, beta=0, broadcast=1](%5, %1, %6), scope: MultiModule/SomeModule\r\n  return (%7);\r\n}\r\n\r\nTraceback (most recent call last):\r\n  File \"multimodule.py\", line 25, in <module>\r\n    torch.onnx.export(mm, (torch.rand(5, 4),), f, verbose=True)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/__init__.py\", line 31, in export\r\n    return utils.export(*args, **kwargs)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 87, in export\r\n    operator_export_type=operator_export_type)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/onnx/utils.py\", line 208, in _export\r\n    proto, export_map = graph.export(params, _onnx_opset_version, defer_weight_export, operator_export_type)\r\nRuntimeError: torch/csrc/jit/export.cpp:296: encodeBlock: Assertion `b->inputs().size() >= num_initializers` failed.\r\n```"}