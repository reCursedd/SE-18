{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/177947943", "pull_request_review_id": 107918795, "id": 177947943, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3Nzk0Nzk0Mw==", "diff_hunk": "@@ -88,32 +94,27 @@ def grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=Fal\n     ``grad_outputs`` should be a sequence of length matching ``output``\n     containing the pre-computed gradients w.r.t. each of the outputs. If an\n     output doesn't require_grad, then the gradient can be ``None``).\n-    Gradients can be given as Tensors when one doesn't need the graph of the\n-    derivative, or as Variables, in which case the graph will be created.\n \n     If ``only_inputs`` is ``True``, the function will only return a list of gradients\n     w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining\n     leaves will still be computed, and will be accumulated into their ``.grad``\n     attribute.\n \n     Arguments:\n-        outputs (sequence of Variable): outputs of the differentiated function.\n-        inputs (sequence of Variable): Inputs w.r.t. which the gradient will be\n+        outputs (sequence of Tensor): outputs of the differentiated function.\n+        inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be\n             returned (and not accumulated into ``.grad``).\n         grad_outputs (sequence of Tensor): Gradients w.r.t. each output.\n-            Any tensors will be automatically converted to Variables that are\n-            volatile unless ``create_graph`` is ``True``. None values can be\n-            specified for scalar Variables or ones that don't require grad.\n-            If a None value would be acceptable for all grad_variables, then\n-            this argument is optional.\n+            None values can be specified for scalar Tensors or ones that don't require\n+            grad. If a None value would be acceptable for all grad_tensors, then this\n+            argument is optional.\n         retain_graph (bool, optional): If ``False``, the graph used to compute the grad", "path": "torch/autograd/__init__.py", "position": null, "original_position": 123, "commit_id": "79ce483a3f633cbdcb88167d640b9539c9878dd6", "original_commit_id": "deeee8056e348ebbaac97fb2a8d8f701ea38fed3", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "The retain_graph default is specified.", "created_at": "2018-03-29T03:51:14Z", "updated_at": "2018-11-23T15:41:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/5907#discussion_r177947943", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5907", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/177947943"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5907#discussion_r177947943"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5907"}}, "body_html": "<p>The retain_graph default is specified.</p>", "body_text": "The retain_graph default is specified.", "in_reply_to_id": 177913966}