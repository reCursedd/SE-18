{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175815342", "pull_request_review_id": 105407427, "id": 175815342, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTgxNTM0Mg==", "diff_hunk": "@@ -88,32 +86,27 @@ def grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=Fal\n     ``grad_outputs`` should be a sequence of length matching ``output``\n     containing the pre-computed gradients w.r.t. each of the outputs. If an\n     output doesn't require_grad, then the gradient can be ``None``).\n-    Gradients can be given as Tensors when one doesn't need the graph of the\n-    derivative, or as Variables, in which case the graph will be created.\n \n     If ``only_inputs`` is ``True``, the function will only return a list of gradients\n     w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining\n     leaves will still be computed, and will be accumulated into their ``.grad``\n     attribute.\n \n     Arguments:\n-        outputs (sequence of Variable): outputs of the differentiated function.\n-        inputs (sequence of Variable): Inputs w.r.t. which the gradient will be\n+        outputs (sequence of Tensor): outputs of the differentiated function.\n+        inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be\n             returned (and not accumulated into ``.grad``).\n         grad_outputs (sequence of Tensor): Gradients w.r.t. each output.\n-            Any tensors will be automatically converted to Variables that are\n-            volatile unless ``create_graph`` is ``True``. None values can be\n-            specified for scalar Variables or ones that don't require grad.\n-            If a None value would be acceptable for all grad_variables, then\n-            this argument is optional.\n+            None values can be specified for scalar Tensors or ones that don't require\n+            grad. If a None value would be acceptable for all grad_tensors, then this\n+            argument is optional.\n         retain_graph (bool, optional): If ``False``, the graph used to compute the grad\n             will be freed. Note that in nearly all cases setting this option to ``True``\n             is not needed and often can be worked around in a much more efficient\n             way. Defaults to the value of ``create_graph``.\n         create_graph (bool, optional): If ``True``, graph of the derivative will\n             be constructed, allowing to compute higher order derivative products.\n-            Defaults to ``False``, unless ``grad_variables`` contains at least one\n-            non-volatile Variable.", "path": "torch/autograd/__init__.py", "position": 131, "original_position": 122, "commit_id": "79ce483a3f633cbdcb88167d640b9539c9878dd6", "original_commit_id": "6967fac5488ffe795d2c2869e268d3a469fd3bf0", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "@colesbury is this still true? I don't think it is but I'm not 100% sure:\r\n\r\nI haven't been able to find code otherwise, and tested the following:\r\n```\r\nimport torch\r\nimport torch.autograd as autograd\r\nx = torch.randn((2, 2), requires_grad=True)\r\nout = x ** 2\r\n\r\n# Is a graph for the derivatives created?\r\ngrad_output = torch.ones((2, 2), requires_grad=False)\r\nout2 = autograd.grad(out, [x], [grad_output], create_graph=True)\r\nout2[0].backward()  # yes\r\n\r\n# Is a graph for the derivatives created?\r\ngrad_output = torch.ones((2, 2), requires_grad=True)\r\nout2 = autograd.grad(out, [x], [grad_output], create_graph=False)\r\nout2.requires_grad  # false, so no\r\n```", "created_at": "2018-03-20T15:39:47Z", "updated_at": "2018-11-23T15:40:59Z", "html_url": "https://github.com/pytorch/pytorch/pull/5907#discussion_r175815342", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5907", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/175815342"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5907#discussion_r175815342"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5907"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> is this still true? I don't think it is but I'm not 100% sure:</p>\n<p>I haven't been able to find code otherwise, and tested the following:</p>\n<pre><code>import torch\nimport torch.autograd as autograd\nx = torch.randn((2, 2), requires_grad=True)\nout = x ** 2\n\n# Is a graph for the derivatives created?\ngrad_output = torch.ones((2, 2), requires_grad=False)\nout2 = autograd.grad(out, [x], [grad_output], create_graph=True)\nout2[0].backward()  # yes\n\n# Is a graph for the derivatives created?\ngrad_output = torch.ones((2, 2), requires_grad=True)\nout2 = autograd.grad(out, [x], [grad_output], create_graph=False)\nout2.requires_grad  # false, so no\n</code></pre>", "body_text": "@colesbury is this still true? I don't think it is but I'm not 100% sure:\nI haven't been able to find code otherwise, and tested the following:\nimport torch\nimport torch.autograd as autograd\nx = torch.randn((2, 2), requires_grad=True)\nout = x ** 2\n\n# Is a graph for the derivatives created?\ngrad_output = torch.ones((2, 2), requires_grad=False)\nout2 = autograd.grad(out, [x], [grad_output], create_graph=True)\nout2[0].backward()  # yes\n\n# Is a graph for the derivatives created?\ngrad_output = torch.ones((2, 2), requires_grad=True)\nout2 = autograd.grad(out, [x], [grad_output], create_graph=False)\nout2.requires_grad  # false, so no"}