{"url": "https://api.github.com/repos/pytorch/pytorch/issues/13659", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/13659/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/13659/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/13659/events", "html_url": "https://github.com/pytorch/pytorch/issues/13659", "id": 378169510, "node_id": "MDU6SXNzdWUzNzgxNjk1MTA=", "number": 13659, "title": "something about dropout", "user": {"login": "douyh", "id": 34704737, "node_id": "MDQ6VXNlcjM0NzA0NzM3", "avatar_url": "https://avatars1.githubusercontent.com/u/34704737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/douyh", "html_url": "https://github.com/douyh", "followers_url": "https://api.github.com/users/douyh/followers", "following_url": "https://api.github.com/users/douyh/following{/other_user}", "gists_url": "https://api.github.com/users/douyh/gists{/gist_id}", "starred_url": "https://api.github.com/users/douyh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/douyh/subscriptions", "organizations_url": "https://api.github.com/users/douyh/orgs", "repos_url": "https://api.github.com/users/douyh/repos", "events_url": "https://api.github.com/users/douyh/events{/privacy}", "received_events_url": "https://api.github.com/users/douyh/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-07T07:22:06Z", "updated_at": "2018-11-07T21:21:43Z", "closed_at": "2018-11-07T21:21:42Z", "author_association": "NONE", "body_html": "<h2><g-emoji class=\"g-emoji\" alias=\"question\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/2753.png\">\u2753</g-emoji> Questions and Help</h2>\n<h3>Please note that this issue tracker is not a help form and this issue will be closed.</h3>\n<p>When I use nn.Dropout() in my network, I am confused by the ouput of training loss and validation loss.<br>\nI use<code>self.drop = nn.Dropout(0.7)</code>  when I initialize my network, and I add <code>x = self.drop(x)</code> between 2 fc layers.<br>\nHowever, when I print my loss, the loss on validation set is MUCH HIGHER than training loss from the FIRST BATCH. It seems to be divided by a factor 0.3.<br>\nIn the first 100 batches, my training loss is 4.7, at the same time, my validating loss is 15.1.<br>\nWhat happened?<br>\nI use <code>model.eva()</code> before test on validation set and <code>model.train()</code> before training.</p>", "body_text": "\u2753 Questions and Help\nPlease note that this issue tracker is not a help form and this issue will be closed.\nWhen I use nn.Dropout() in my network, I am confused by the ouput of training loss and validation loss.\nI useself.drop = nn.Dropout(0.7)  when I initialize my network, and I add x = self.drop(x) between 2 fc layers.\nHowever, when I print my loss, the loss on validation set is MUCH HIGHER than training loss from the FIRST BATCH. It seems to be divided by a factor 0.3.\nIn the first 100 batches, my training loss is 4.7, at the same time, my validating loss is 15.1.\nWhat happened?\nI use model.eva() before test on validation set and model.train() before training.", "body": "## \u2753 Questions and Help\r\n\r\n### Please note that this issue tracker is not a help form and this issue will be closed.\r\n\r\nWhen I use nn.Dropout() in my network, I am confused by the ouput of training loss and validation loss.\r\nI use```self.drop = nn.Dropout(0.7)```  when I initialize my network, and I add ```x = self.drop(x)``` between 2 fc layers.\r\nHowever, when I print my loss, the loss on validation set is MUCH HIGHER than training loss from the FIRST BATCH. It seems to be divided by a factor 0.3.\r\nIn the first 100 batches, my training loss is 4.7, at the same time, my validating loss is 15.1.\r\nWhat happened?\r\nI use ```model.eva()``` before test on validation set and ```model.train()``` before training.\r\n"}