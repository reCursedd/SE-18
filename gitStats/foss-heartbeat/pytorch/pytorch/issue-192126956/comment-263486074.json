{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/263486074", "html_url": "https://github.com/pytorch/pytorch/issues/263#issuecomment-263486074", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/263", "id": 263486074, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzQ4NjA3NA==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-29T06:15:59Z", "updated_at": "2016-11-29T06:20:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Yes, pseudorandom. Indeed it's possible that this scheme will produce biased dropout, for two reasons:</p>\n<ol>\n<li>The composition of the torch PRNG with the cudnn dropout random leads to something biased, e.g. if one is the inverse of the other. This situation is unlikely, but hard to say since the curand PRNG is not open source AFAIK.</li>\n<li>The cudnn dropout PRNG iteration 1 is correlated between all/random seeds. I would hope that's not the case but can't check that either.</li>\n</ol>\n<p>I'm sure that <code>torch.IntTensor.random()</code> is deterministic w.r.t. multi-threaded code in Lua-Torch (each thread gets its own PRNG state); I'm not sure about the current state in PyTorch but I would hope so because if it's not then it's hard to achieve determinism in general (e.g. with non-cudnn dropout). cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a></p>\n<p><code>philox(seed, key)</code>et al are based on pseudo-random functions, so if on each iteration you need N random numbers, you do something like <code>x_k = philox(seed, count + k)</code>, and on completion <code>count += N</code>. What's nice is that (a) the only state is <code>count</code>, and (b) it's deterministic even if you change the number of threads. The Random123 PRNGs have been tested to be uncorrelated between values of <code>key</code> (and less rigorously between values of <code>seed</code> aka streams).</p>\n<p>In fact, if you pick a strong enough PRF (e.g. a cryptographic hash), than the values should be uncorrelated (up to computational indistinguishability) for <em>any</em> values of <code>seed</code> and <code>key</code>, which makes your life even easier because you can do something like <code>x_k = AES(seed &lt;&lt; 64 &amp; iter_key &lt;&lt; 32 &amp; k)</code>. All you need to do is give a different <code>iter_key</code> each time. But you might only trust a cryptographic hash for this and it may be too computationally expensive.</p>", "body_text": "Yes, pseudorandom. Indeed it's possible that this scheme will produce biased dropout, for two reasons:\n\nThe composition of the torch PRNG with the cudnn dropout random leads to something biased, e.g. if one is the inverse of the other. This situation is unlikely, but hard to say since the curand PRNG is not open source AFAIK.\nThe cudnn dropout PRNG iteration 1 is correlated between all/random seeds. I would hope that's not the case but can't check that either.\n\nI'm sure that torch.IntTensor.random() is deterministic w.r.t. multi-threaded code in Lua-Torch (each thread gets its own PRNG state); I'm not sure about the current state in PyTorch but I would hope so because if it's not then it's hard to achieve determinism in general (e.g. with non-cudnn dropout). cc @soumith\nphilox(seed, key)et al are based on pseudo-random functions, so if on each iteration you need N random numbers, you do something like x_k = philox(seed, count + k), and on completion count += N. What's nice is that (a) the only state is count, and (b) it's deterministic even if you change the number of threads. The Random123 PRNGs have been tested to be uncorrelated between values of key (and less rigorously between values of seed aka streams).\nIn fact, if you pick a strong enough PRF (e.g. a cryptographic hash), than the values should be uncorrelated (up to computational indistinguishability) for any values of seed and key, which makes your life even easier because you can do something like x_k = AES(seed << 64 & iter_key << 32 & k). All you need to do is give a different iter_key each time. But you might only trust a cryptographic hash for this and it may be too computationally expensive.", "body": "Yes, pseudorandom. Indeed it's possible that this scheme will produce biased dropout, for two reasons:\r\n1) The composition of the torch PRNG with the cudnn dropout random leads to something biased, e.g. if one is the inverse of the other. This situation is unlikely, but hard to say since the curand PRNG is not open source AFAIK.\r\n2) The cudnn dropout PRNG iteration 1 is correlated between all/random seeds. I would hope that's not the case but can't check that either.\r\n\r\nI'm sure that `torch.IntTensor.random()` is deterministic w.r.t. multi-threaded code in Lua-Torch (each thread gets its own PRNG state); I'm not sure about the current state in PyTorch but I would hope so because if it's not then it's hard to achieve determinism in general (e.g. with non-cudnn dropout). cc @soumith \r\n\r\n`philox(seed, key)`et al are based on pseudo-random functions, so if on each iteration you need N random numbers, you do something like `x_k = philox(seed, count + k)`, and on completion `count += N`. What's nice is that (a) the only state is `count`, and (b) it's deterministic even if you change the number of threads. The Random123 PRNGs have been tested to be uncorrelated between values of `key` (and less rigorously between values of `seed` aka streams).\r\n\r\nIn fact, if you pick a strong enough PRF (e.g. a cryptographic hash), than the values should be uncorrelated (up to computational indistinguishability) for *any* values of `seed` and `key`, which makes your life even easier because you can do something like `x_k = AES(seed << 64 & iter_key << 32 & k)`. All you need to do is give a different `iter_key` each time. But you might only trust a cryptographic hash for this and it may be too computationally expensive.\r\n"}