{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/263663979", "html_url": "https://github.com/pytorch/pytorch/issues/263#issuecomment-263663979", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/263", "id": 263663979, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzY2Mzk3OQ==", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-29T18:58:21Z", "updated_at": "2016-11-29T18:58:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for pointing me at the curand device implementation, I didn't know it was available.</p>\n<p>The term \"stateless\" is a bit of a misnomer in PyTorch nn. The idea is that an autograd computation is executed via <em>functions</em> which only persist for one forward-backward computation. It's the function that picks which backend (e.g. cudnn) to use. At a higher level we have <em>modules</em> which persist and which can maintain state between multiple passes, but<br>\n(a) Modules are supposed to be unaware of the particular backend (e.g. cudnn)<br>\n(b) The control flow is that module calls <code>output = function(input)</code>. There is no API for a function to pass back state to the module.</p>\n<p>One workaround for these restrictions is to cache state in the backend. That's what we do for the algorithms cache, but it's a dirty hack (according to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> who wrote it). In the case of RNNs, it would suffer from exactly those nondeterminism problems you mentioned, because if multiple threads all execute RNNs, they will each grab the same buffer from the cache (possibly getting duplicate random numbers).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> suggested a slightly more palatable workaround, which is to store a <code>dropout_state</code> tensor (or more general <code>buffers</code> dict) int the module that gets passed to the function on each iteration. Cudnn can store the dropout state in there. I think this might be the best compromise; if you'd like I can send a PR with this change.</p>\n<p>Thanks for taking a look at the pytorch RNN code, I appreciate it.</p>\n<p>Adam</p>\n<p>P.S. I'm happy to end the Philox discussion, but w.r.t. what I've just described, the benefit of Philox could be that if on each iteration you know you need <code>N</code> random numbers, then the module just keeps track of which iteration its on, and the backend can then infer <code>count = iter * N</code>.</p>", "body_text": "Thanks for pointing me at the curand device implementation, I didn't know it was available.\nThe term \"stateless\" is a bit of a misnomer in PyTorch nn. The idea is that an autograd computation is executed via functions which only persist for one forward-backward computation. It's the function that picks which backend (e.g. cudnn) to use. At a higher level we have modules which persist and which can maintain state between multiple passes, but\n(a) Modules are supposed to be unaware of the particular backend (e.g. cudnn)\n(b) The control flow is that module calls output = function(input). There is no API for a function to pass back state to the module.\nOne workaround for these restrictions is to cache state in the backend. That's what we do for the algorithms cache, but it's a dirty hack (according to @colesbury who wrote it). In the case of RNNs, it would suffer from exactly those nondeterminism problems you mentioned, because if multiple threads all execute RNNs, they will each grab the same buffer from the cache (possibly getting duplicate random numbers).\n@colesbury suggested a slightly more palatable workaround, which is to store a dropout_state tensor (or more general buffers dict) int the module that gets passed to the function on each iteration. Cudnn can store the dropout state in there. I think this might be the best compromise; if you'd like I can send a PR with this change.\nThanks for taking a look at the pytorch RNN code, I appreciate it.\nAdam\nP.S. I'm happy to end the Philox discussion, but w.r.t. what I've just described, the benefit of Philox could be that if on each iteration you know you need N random numbers, then the module just keeps track of which iteration its on, and the backend can then infer count = iter * N.", "body": "Thanks for pointing me at the curand device implementation, I didn't know it was available.\r\n\r\nThe term \"stateless\" is a bit of a misnomer in PyTorch nn. The idea is that an autograd computation is executed via *functions* which only persist for one forward-backward computation. It's the function that picks which backend (e.g. cudnn) to use. At a higher level we have *modules* which persist and which can maintain state between multiple passes, but\r\n(a) Modules are supposed to be unaware of the particular backend (e.g. cudnn)\r\n(b) The control flow is that module calls `output = function(input)`. There is no API for a function to pass back state to the module.\r\n\r\nOne workaround for these restrictions is to cache state in the backend. That's what we do for the algorithms cache, but it's a dirty hack (according to @colesbury who wrote it). In the case of RNNs, it would suffer from exactly those nondeterminism problems you mentioned, because if multiple threads all execute RNNs, they will each grab the same buffer from the cache (possibly getting duplicate random numbers).\r\n\r\n@colesbury suggested a slightly more palatable workaround, which is to store a `dropout_state` tensor (or more general `buffers` dict) int the module that gets passed to the function on each iteration. Cudnn can store the dropout state in there. I think this might be the best compromise; if you'd like I can send a PR with this change.\r\n\r\nThanks for taking a look at the pytorch RNN code, I appreciate it.\r\n\r\nAdam\r\n\r\nP.S. I'm happy to end the Philox discussion, but w.r.t. what I've just described, the benefit of Philox could be that if on each iteration you know you need `N` random numbers, then the module just keeps track of which iteration its on, and the backend can then infer `count = iter * N`."}