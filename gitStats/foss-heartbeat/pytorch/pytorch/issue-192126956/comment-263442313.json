{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/263442313", "html_url": "https://github.com/pytorch/pytorch/issues/263#issuecomment-263442313", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/263", "id": 263442313, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzQ0MjMxMw==", "user": {"login": "ngimel", "id": 15841449, "node_id": "MDQ6VXNlcjE1ODQxNDQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/15841449?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ngimel", "html_url": "https://github.com/ngimel", "followers_url": "https://api.github.com/users/ngimel/followers", "following_url": "https://api.github.com/users/ngimel/following{/other_user}", "gists_url": "https://api.github.com/users/ngimel/gists{/gist_id}", "starred_url": "https://api.github.com/users/ngimel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ngimel/subscriptions", "organizations_url": "https://api.github.com/users/ngimel/orgs", "repos_url": "https://api.github.com/users/ngimel/repos", "events_url": "https://api.github.com/users/ngimel/events{/privacy}", "received_events_url": "https://api.github.com/users/ngimel/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-29T00:47:13Z", "updated_at": "2016-11-29T00:47:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Each new seed is pseudorandom;-) \"Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.\", you are just making this sin a little bit graver. Is torch.IntTensor.random() also deterministic with multi-threaded operation? E.g. different threads controlling different GPUs, generating randoms on the cpu in the process, some other threads loading data and generating some other randoms for permutations and random crops etc. Would it still be deterministic?<br>\nNote that philox still saves a counter for each generator, so either you are saving these counters for each rng (and for device-side rng generation the number of rng's is ~number of resident threads), or you are reseeding it each time, as you are doing now (with the same caveats of sequences not being guaranteed to be uncorrelated).</p>", "body_text": "Each new seed is pseudorandom;-) \"Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.\", you are just making this sin a little bit graver. Is torch.IntTensor.random() also deterministic with multi-threaded operation? E.g. different threads controlling different GPUs, generating randoms on the cpu in the process, some other threads loading data and generating some other randoms for permutations and random crops etc. Would it still be deterministic?\nNote that philox still saves a counter for each generator, so either you are saving these counters for each rng (and for device-side rng generation the number of rng's is ~number of resident threads), or you are reseeding it each time, as you are doing now (with the same caveats of sequences not being guaranteed to be uncorrelated).", "body": "Each new seed is pseudorandom;-) \"Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.\", you are just making this sin a little bit graver. Is torch.IntTensor.random() also deterministic with multi-threaded operation? E.g. different threads controlling different GPUs, generating randoms on the cpu in the process, some other threads loading data and generating some other randoms for permutations and random crops etc. Would it still be deterministic? \r\nNote that philox still saves a counter for each generator, so either you are saving these counters for each rng (and for device-side rng generation the number of rng's is ~number of resident threads), or you are reseeding it each time, as you are doing now (with the same caveats of sequences not being guaranteed to be uncorrelated). "}