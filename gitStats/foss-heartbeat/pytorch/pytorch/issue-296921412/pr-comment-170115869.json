{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170115869", "pull_request_review_id": 98747664, "id": 170115869, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDExNTg2OQ==", "diff_hunk": "@@ -0,0 +1,261 @@\n+#include \"python_tensor.h\"\n+\n+#include <structmember.h>\n+#include <mutex>\n+#include <pybind11/pybind11.h>\n+#include <sstream>\n+\n+#include \"torch/csrc/assertions.h\"\n+#include \"torch/csrc/Exceptions.h\"\n+#include \"torch/csrc/autograd/variable.h\"\n+#include \"torch/csrc/autograd/python_variable.h\"\n+#include \"torch/csrc/autograd/generated/VariableType.h\"\n+#include \"torch/csrc/utils/python_strings.h\"\n+#include \"torch/csrc/utils/tensor_new.h\"\n+#include \"torch/csrc/utils/tensor_types.h\"\n+\n+namespace torch { namespace tensor {\n+\n+using namespace at;\n+using namespace torch::autograd;\n+\n+struct PyTensorType {\n+  PyTypeObject py_type;\n+  at::Type* aten_type;\n+  bool is_cuda;\n+  bool is_sparse;\n+  bool is_default;\n+  char name[64];\n+};\n+\n+static_assert(std::is_standard_layout<PyTensorType>::value, \"PyTensorType must be standard layout\");\n+\n+static PyTensorType* default_tensor_type;\n+static std::once_flag init_cuda_flag;\n+\n+static void py_bind_tensor_types(const std::vector<PyTensorType>& tensor_types);\n+\n+static PyObject* Tensor_new(PyTypeObject *type, PyObject *args, PyObject *kwargs) {\n+  HANDLE_TH_ERRORS\n+  auto& tensor_type = *((PyTensorType*)type);\n+  if (!tensor_type.aten_type) {\n+    throw TypeError(\"type %s not available\", tensor_type.name);\n+  }\n+  if (tensor_type.is_cuda) {\n+    std::call_once(init_cuda_flag, []() {\n+      pybind11::module::import(\"torch.cuda\").attr(\"init\")();\n+    });\n+  }\n+  return THPVariable_Wrap(torch::utils::legacy_tensor_ctor(*tensor_type.aten_type, args, kwargs));\n+  END_HANDLE_TH_ERRORS\n+}\n+\n+static PyObject* Tensor_instancecheck(PyTensorType* self, PyObject* arg) {\n+  HANDLE_TH_ERRORS\n+  if (THPVariable_Check(arg)) {\n+    if (self->is_default) {\n+      Py_RETURN_TRUE;\n+    }\n+    auto& var = ((THPVariable*)arg)->cdata;\n+    if (&var.type() == self->aten_type) {\n+      Py_RETURN_TRUE;\n+    }\n+  }\n+  Py_RETURN_FALSE;\n+  END_HANDLE_TH_ERRORS\n+}\n+\n+static struct PyMethodDef metaclass_methods[] = {\n+  {\"__instancecheck__\", (PyCFunction)Tensor_instancecheck, METH_O, NULL},\n+  {NULL}\n+};\n+\n+static struct PyMemberDef metaclass_members[] = {\n+  {(char*)\"is_cuda\", T_BOOL, offsetof(PyTensorType, is_cuda), READONLY, NULL},\n+  {(char*)\"is_sparse\", T_BOOL, offsetof(PyTensorType, is_sparse), READONLY, NULL},\n+  {NULL}\n+};\n+\n+static PyTypeObject metaclass;\n+\n+static void py_initialize_metaclass(PyTypeObject& metaclass) {\n+  ((PyObject*)&metaclass)->ob_refcnt = 1;\n+  metaclass.tp_basicsize = sizeof(PyTypeObject);\n+  metaclass.tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE;\n+  metaclass.tp_methods = metaclass_methods;\n+  metaclass.tp_members = metaclass_members;\n+  metaclass.tp_name = \"torch.tensortype\";\n+  metaclass.tp_base = &PyType_Type;\n+  if (PyType_Ready(&metaclass) < 0) {\n+    throw python_error();\n+  }\n+}\n+\n+static void py_initialize_tensor_type(PyTypeObject& type, const char* name, PyObject* tp_dict) {\n+  // NOTE: we don't use he typical static declaration of PyTypeObject because", "path": "torch/csrc/tensor/python_tensor.cpp", "position": null, "original_position": 95, "commit_id": "e22441c7d8edd20473033ec8d50e657604b33d02", "original_commit_id": "5670cadc6d42a86a8a9267253ddd2c6f400cfa9e", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "we don't use he -> we don't use the.", "created_at": "2018-02-22T22:37:25Z", "updated_at": "2018-11-23T15:39:53Z", "html_url": "https://github.com/pytorch/pytorch/pull/5225#discussion_r170115869", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5225", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170115869"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5225#discussion_r170115869"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5225"}}, "body_html": "<p>we don't use he -&gt; we don't use the.</p>", "body_text": "we don't use he -> we don't use the."}