{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170120096", "pull_request_review_id": 98747664, "id": 170120096, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MDEyMDA5Ng==", "diff_hunk": "@@ -0,0 +1,261 @@\n+#include \"python_tensor.h\"\n+\n+#include <structmember.h>\n+#include <mutex>\n+#include <pybind11/pybind11.h>\n+#include <sstream>\n+\n+#include \"torch/csrc/assertions.h\"\n+#include \"torch/csrc/Exceptions.h\"\n+#include \"torch/csrc/autograd/variable.h\"\n+#include \"torch/csrc/autograd/python_variable.h\"\n+#include \"torch/csrc/autograd/generated/VariableType.h\"\n+#include \"torch/csrc/utils/python_strings.h\"\n+#include \"torch/csrc/utils/tensor_new.h\"\n+#include \"torch/csrc/utils/tensor_types.h\"\n+\n+namespace torch { namespace tensor {\n+\n+using namespace at;\n+using namespace torch::autograd;\n+\n+struct PyTensorType {\n+  PyTypeObject py_type;\n+  at::Type* aten_type;\n+  bool is_cuda;\n+  bool is_sparse;\n+  bool is_default;\n+  char name[64];\n+};\n+\n+static_assert(std::is_standard_layout<PyTensorType>::value, \"PyTensorType must be standard layout\");\n+\n+static PyTensorType* default_tensor_type;\n+static std::once_flag init_cuda_flag;\n+\n+static void py_bind_tensor_types(const std::vector<PyTensorType>& tensor_types);\n+\n+static PyObject* Tensor_new(PyTypeObject *type, PyObject *args, PyObject *kwargs) {\n+  HANDLE_TH_ERRORS\n+  auto& tensor_type = *((PyTensorType*)type);\n+  if (!tensor_type.aten_type) {\n+    throw TypeError(\"type %s not available\", tensor_type.name);\n+  }\n+  if (tensor_type.is_cuda) {\n+    std::call_once(init_cuda_flag, []() {\n+      pybind11::module::import(\"torch.cuda\").attr(\"init\")();\n+    });\n+  }\n+  return THPVariable_Wrap(torch::utils::legacy_tensor_ctor(*tensor_type.aten_type, args, kwargs));\n+  END_HANDLE_TH_ERRORS\n+}\n+\n+static PyObject* Tensor_instancecheck(PyTensorType* self, PyObject* arg) {\n+  HANDLE_TH_ERRORS\n+  if (THPVariable_Check(arg)) {\n+    if (self->is_default) {\n+      Py_RETURN_TRUE;\n+    }\n+    auto& var = ((THPVariable*)arg)->cdata;\n+    if (&var.type() == self->aten_type) {\n+      Py_RETURN_TRUE;\n+    }\n+  }\n+  Py_RETURN_FALSE;\n+  END_HANDLE_TH_ERRORS\n+}\n+\n+static struct PyMethodDef metaclass_methods[] = {\n+  {\"__instancecheck__\", (PyCFunction)Tensor_instancecheck, METH_O, NULL},\n+  {NULL}\n+};\n+\n+static struct PyMemberDef metaclass_members[] = {\n+  {(char*)\"is_cuda\", T_BOOL, offsetof(PyTensorType, is_cuda), READONLY, NULL},\n+  {(char*)\"is_sparse\", T_BOOL, offsetof(PyTensorType, is_sparse), READONLY, NULL},\n+  {NULL}\n+};\n+\n+static PyTypeObject metaclass;\n+\n+static void py_initialize_metaclass(PyTypeObject& metaclass) {\n+  ((PyObject*)&metaclass)->ob_refcnt = 1;\n+  metaclass.tp_basicsize = sizeof(PyTypeObject);\n+  metaclass.tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE;\n+  metaclass.tp_methods = metaclass_methods;\n+  metaclass.tp_members = metaclass_members;\n+  metaclass.tp_name = \"torch.tensortype\";\n+  metaclass.tp_base = &PyType_Type;\n+  if (PyType_Ready(&metaclass) < 0) {\n+    throw python_error();\n+  }\n+}\n+\n+static void py_initialize_tensor_type(PyTypeObject& type, const char* name, PyObject* tp_dict) {\n+  // NOTE: we don't use he typical static declaration of PyTypeObject because\n+  // we need to initialize as many types as there are VariableType instances.\n+  // The typical PyVarObject_HEAD_INIT(NULL, 0) is described in the Python\n+  // documentation: it initializes the refcnt to 1 and the other object header\n+  // fields to zero.\n+  memset(&type, 0, sizeof(PyTypeObject));\n+  ((PyObject*)&type)->ob_refcnt = 1;\n+  ((PyObject*)&type)->ob_type = &metaclass;\n+  type.tp_basicsize = sizeof(PyTensorType);\n+  type.tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE;\n+  type.tp_name = name;\n+  type.tp_new = Tensor_new;\n+  if (PyType_Ready(&type) < 0) {\n+    throw python_error();\n+  }\n+  if (PyDict_Merge(type.tp_dict, tp_dict, 0) < 0) {\n+    throw python_error();\n+  }\n+}\n+\n+static const char* get_module(Backend backend) {\n+  switch (backend) {\n+    case kCPU: return \"torch\";\n+    case kCUDA: return \"torch.cuda\";\n+    case kSparseCPU: return \"torch.sparse\";\n+    case kSparseCUDA: return \"torch.cuda.sparse\";\n+    default: runtime_error(\"invalid backend: %s\", toString(backend));\n+  }\n+}\n+\n+static std::string get_name(Backend backend, ScalarType scalarType) {\n+  std::ostringstream ss;\n+  ss << get_module(backend) << \".\" << at::toString(scalarType) << \"Tensor\";\n+  return ss.str();\n+}\n+\n+static void set_type(PyTensorType& type_obj, Backend backend, ScalarType scalarType) {\n+  auto baseType = globalContext().type_registry[static_cast<int>(backend)][static_cast<int>(scalarType)].get();\n+  type_obj.aten_type = baseType ? torch::autograd::VariableType::getType(*baseType) : nullptr;\n+  type_obj.is_cuda = backend == kCUDA || backend == kSparseCUDA;\n+  type_obj.is_sparse = backend == kSparseCPU || backend == kSparseCUDA;\n+}\n+\n+static void set_name(PyTensorType& type_obj, const std::string& name) {\n+  size_t n = sizeof(type_obj.name);\n+  strncpy(type_obj.name, name.c_str(), n);\n+  type_obj.name[n - 1] = '\\0';\n+}\n+\n+static PyObject* get_variable_dict() {\n+  auto autograd = THPObjectPtr(PyImport_ImportModule(\"torch.autograd\"));\n+  if (!autograd) throw python_error();\n+\n+  auto variable_class = THPObjectPtr(PyObject_GetAttrString(autograd.get(), \"Variable\"));\n+  if (!variable_class) throw python_error();\n+\n+  return ((PyTypeObject*)variable_class.get())->tp_dict;\n+}\n+\n+static std::vector<PyTensorType> tensor_types;\n+\n+static void initialize_aten_types(std::vector<PyTensorType>& tensor_types) {\n+  // includes CUDA types even when PyTorch is not built with CUDA\n+  auto declared_types = torch::utils::all_declared_types();\n+  tensor_types.resize(declared_types.size() + 1);\n+\n+  for (size_t i = 0, end = declared_types.size(); i != end; i++) {\n+    auto& tensor_type = tensor_types[i];\n+    Backend backend = declared_types[i].first;\n+    ScalarType scalar_type = declared_types[i].second;\n+    set_type(tensor_type, backend, scalar_type);\n+    set_name(tensor_type, get_name(backend, scalar_type));\n+  }\n+\n+  set_type(tensor_types.back(), kCPU, kFloat);\n+  set_name(tensor_types.back(), \"torch.Tensor\");\n+  tensor_types.back().is_default = true;\n+}\n+\n+void initialize_python_bindings(PyObject* module) {\n+  // Initialize the at::Type* pointers, name, and properties of the PyTensorType\n+  // vector. After this call, the vector must not be resized.\n+  initialize_aten_types(tensor_types);\n+\n+  // Initialize the Python metaclass for the torch.Tensor, torch.FloatTensor,\n+  // etc. types. The metaclass handles __instancecheck__ checks and binds the\n+  // propeties is_cuda and is_sparse on the type objects.\n+  py_initialize_metaclass(metaclass);\n+\n+  // Get the tp_dict of the Variable Python class. We copy function definitions\n+  // onto each Tensor type object so that they can be accessed via e.g.\n+  // `torch.Tensor.add`.\n+  PyObject* var_dict = get_variable_dict();\n+\n+  // Initialize each Python type object torch.FloatTensor, torch.DoubleTensor,\n+  // etc. and the \"default\" type object torch.Tensor.\n+  for (auto& tensor_type : tensor_types) {\n+    py_initialize_tensor_type(tensor_type.py_type, tensor_type.name, var_dict);\n+  }\n+\n+  // The type object for torch.Tensor is at the end.\n+  default_tensor_type = &tensor_types.back();\n+\n+  // Add the type objects to their corresponding modules. e.g. torch.FloatTensor\n+  // is added to the `torch` module as `FloatTensor`. Also add all the type\n+  // objects to the set torch._tensor_classes.\n+  py_bind_tensor_types(tensor_types);\n+}\n+\n+static void py_bind_tensor_types(const std::vector<PyTensorType>& tensor_types) {\n+  auto torch_module = THPObjectPtr(PyImport_ImportModule(\"torch\"));\n+  if (!torch_module) throw python_error();\n+\n+  auto tensor_classes = THPObjectPtr(PyObject_GetAttrString(torch_module.get(), \"_tensor_classes\"));\n+  if (!tensor_classes) throw python_error();\n+\n+  for (auto& tensor_type : tensor_types) {\n+    auto name = std::string(tensor_type.name);\n+    auto idx = name.rfind(\".\");\n+    auto type_name = name.substr(idx + 1);\n+    auto module_name = name.substr(0, idx);\n+\n+    auto module_obj = THPObjectPtr(PyImport_ImportModule(module_name.c_str()));\n+    if (!module_obj) throw python_error();\n+\n+    PyObject* type_obj = (PyObject*)&tensor_type;\n+    Py_INCREF(type_obj);\n+    PyModule_AddObject(module_obj.get(), type_name.c_str(), type_obj);\n+\n+    if (PySet_Add(tensor_classes.get(), type_obj) < 0) {\n+      throw python_error();\n+    }\n+  }\n+}\n+\n+static bool PyTensorType_Check(PyObject* obj) {\n+  auto it = std::find_if(tensor_types.begin(), tensor_types.end(),\n+    [obj](const PyTensorType& x) {\n+      return (PyObject*)&x == obj;\n+    });\n+  return it != tensor_types.end();\n+}\n+\n+static at::Type* THPDefaultATenType;\n+\n+void set_default_tensor_type(const at::Type& type) {\n+  set_type(*default_tensor_type, type.backend(), type.scalarType());\n+  THPDefaultATenType = default_tensor_type->aten_type;\n+}\n+\n+void py_set_default_tensor_type(PyObject* obj) {\n+  if (!PyTensorType_Check(obj)) {\n+    throw TypeError(\"invalid type object\");\n+  }\n+  auto type = (PyTensorType*)obj;\n+  if (!type->aten_type) {\n+    throw TypeError(\"invalid type object\");", "path": "torch/csrc/tensor/python_tensor.cpp", "position": null, "original_position": 251, "commit_id": "e22441c7d8edd20473033ec8d50e657604b33d02", "original_commit_id": "5670cadc6d42a86a8a9267253ddd2c6f400cfa9e", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "I think you need an error message as above, with special casing for good cuda error messages.  This can really happen because we create all of the \"declared types\".", "created_at": "2018-02-22T22:56:42Z", "updated_at": "2018-11-23T15:39:54Z", "html_url": "https://github.com/pytorch/pytorch/pull/5225#discussion_r170120096", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5225", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/170120096"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5225#discussion_r170120096"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5225"}}, "body_html": "<p>I think you need an error message as above, with special casing for good cuda error messages.  This can really happen because we create all of the \"declared types\".</p>", "body_text": "I think you need an error message as above, with special casing for good cuda error messages.  This can really happen because we create all of the \"declared types\"."}