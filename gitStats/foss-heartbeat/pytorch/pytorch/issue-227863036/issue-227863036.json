{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1536", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1536/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1536/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1536/events", "html_url": "https://github.com/pytorch/pytorch/issues/1536", "id": 227863036, "node_id": "MDU6SXNzdWUyMjc4NjMwMzY=", "number": 1536, "title": "LSTM memory leak?", "user": {"login": "kennysong", "id": 2199875, "node_id": "MDQ6VXNlcjIxOTk4NzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/2199875?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kennysong", "html_url": "https://github.com/kennysong", "followers_url": "https://api.github.com/users/kennysong/followers", "following_url": "https://api.github.com/users/kennysong/following{/other_user}", "gists_url": "https://api.github.com/users/kennysong/gists{/gist_id}", "starred_url": "https://api.github.com/users/kennysong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kennysong/subscriptions", "organizations_url": "https://api.github.com/users/kennysong/orgs", "repos_url": "https://api.github.com/users/kennysong/repos", "events_url": "https://api.github.com/users/kennysong/events{/privacy}", "received_events_url": "https://api.github.com/users/kennysong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-05-11T02:40:19Z", "updated_at": "2017-07-25T07:56:53Z", "closed_at": "2017-05-14T14:27:22Z", "author_association": "NONE", "body_html": "<p>The following script consumes about 1 GB of memory in 100 iterations, and continually increases memory usage. It's a stripped down version of a larger file to isolate the potential leak.</p>\n<p>Commenting out lines 33-34 stabilizes memory usage to 30 MB.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">PolicyNet</span>(<span class=\"pl-e\">torch</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">layers</span>):\n        <span class=\"pl-c1\">super</span>(PolicyNet, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.lstm <span class=\"pl-k\">=</span> torch.nn.LSTMCell(layers[<span class=\"pl-c1\">0</span>], layers[<span class=\"pl-c1\">1</span>])\n        <span class=\"pl-c1\">self</span>.linear <span class=\"pl-k\">=</span> torch.nn.Linear(layers[<span class=\"pl-c1\">1</span>], layers[<span class=\"pl-c1\">2</span>])\n        <span class=\"pl-c1\">self</span>.layers <span class=\"pl-k\">=</span> layers\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">h0</span>, <span class=\"pl-smi\">c0</span>):\n        h1, c1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.lstm(x, (h0, c0))\n        o1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear(h1)\n        <span class=\"pl-k\">return</span> o1, h1, c1\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">leak</span>(<span class=\"pl-smi\">policy_net</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Prepare for one forward pass, with the batch containing the entire episode</span>\n    h_n_batch <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">32</span>))\n    c_n_batch <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">32</span>))\n    softmax <span class=\"pl-k\">=</span> torch.nn.Softmax()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Batch input to LSTM has size [num_agents, episode_len, lstm_input_size]</span>\n    input_batch <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">5</span>))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Do a forward pass, and fill sum_log_probs with sum(log(p)) for each time-step</span>\n    sum_log_probs <span class=\"pl-k\">=</span> Variable(torch.zeros(<span class=\"pl-c1\">1000</span>))\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>):\n        o_n, h_n_batch, c_n_batch <span class=\"pl-k\">=</span> policy_net(input_batch[i], h_n_batch, c_n_batch)\n        dist <span class=\"pl-k\">=</span> softmax(o_n)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>## Comment out L33-34 to stop memory leak ###</span>\n        <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n            sum_log_probs[j] <span class=\"pl-k\">=</span> sum_log_probs[j] <span class=\"pl-k\">+</span> torch.log(dist[j, <span class=\"pl-c1\">0</span>])\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    policy_net <span class=\"pl-k\">=</span> PolicyNet([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">3</span>])\n\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10000</span>):\n        leak(policy_net)\n        <span class=\"pl-c1\">print</span>(i)</pre></div>\n<p>This is for version <code>0.1.12_2</code> on macOS 10.12.4.</p>", "body_text": "The following script consumes about 1 GB of memory in 100 iterations, and continually increases memory usage. It's a stripped down version of a larger file to isolate the potential leak.\nCommenting out lines 33-34 stabilizes memory usage to 30 MB.\nimport numpy as np\nimport torch\n\nfrom torch.autograd import Variable\n\nclass PolicyNet(torch.nn.Module):\n    def __init__(self, layers):\n        super(PolicyNet, self).__init__()\n        self.lstm = torch.nn.LSTMCell(layers[0], layers[1])\n        self.linear = torch.nn.Linear(layers[1], layers[2])\n        self.layers = layers\n\n    def forward(self, x, h0, c0):\n        h1, c1 = self.lstm(x, (h0, c0))\n        o1 = self.linear(h1)\n        return o1, h1, c1\n\ndef leak(policy_net):\n    # Prepare for one forward pass, with the batch containing the entire episode\n    h_n_batch = Variable(torch.zeros(1000, 32))\n    c_n_batch = Variable(torch.zeros(1000, 32))\n    softmax = torch.nn.Softmax()\n\n    # Batch input to LSTM has size [num_agents, episode_len, lstm_input_size]\n    input_batch = Variable(torch.zeros(2, 1000, 5))\n\n    # Do a forward pass, and fill sum_log_probs with sum(log(p)) for each time-step\n    sum_log_probs = Variable(torch.zeros(1000))\n    for i in range(2):\n        o_n, h_n_batch, c_n_batch = policy_net(input_batch[i], h_n_batch, c_n_batch)\n        dist = softmax(o_n)\n        ### Comment out L33-34 to stop memory leak ###\n        for j in range(1000):\n            sum_log_probs[j] = sum_log_probs[j] + torch.log(dist[j, 0])\n\nif __name__ == '__main__':\n    policy_net = PolicyNet([5, 32, 3])\n\n    for i in range(10000):\n        leak(policy_net)\n        print(i)\nThis is for version 0.1.12_2 on macOS 10.12.4.", "body": "The following script consumes about 1 GB of memory in 100 iterations, and continually increases memory usage. It's a stripped down version of a larger file to isolate the potential leak.\r\n\r\nCommenting out lines 33-34 stabilizes memory usage to 30 MB.\r\n\r\n```python\r\nimport numpy as np\r\nimport torch\r\n\r\nfrom torch.autograd import Variable\r\n\r\nclass PolicyNet(torch.nn.Module):\r\n    def __init__(self, layers):\r\n        super(PolicyNet, self).__init__()\r\n        self.lstm = torch.nn.LSTMCell(layers[0], layers[1])\r\n        self.linear = torch.nn.Linear(layers[1], layers[2])\r\n        self.layers = layers\r\n\r\n    def forward(self, x, h0, c0):\r\n        h1, c1 = self.lstm(x, (h0, c0))\r\n        o1 = self.linear(h1)\r\n        return o1, h1, c1\r\n\r\ndef leak(policy_net):\r\n    # Prepare for one forward pass, with the batch containing the entire episode\r\n    h_n_batch = Variable(torch.zeros(1000, 32))\r\n    c_n_batch = Variable(torch.zeros(1000, 32))\r\n    softmax = torch.nn.Softmax()\r\n\r\n    # Batch input to LSTM has size [num_agents, episode_len, lstm_input_size]\r\n    input_batch = Variable(torch.zeros(2, 1000, 5))\r\n\r\n    # Do a forward pass, and fill sum_log_probs with sum(log(p)) for each time-step\r\n    sum_log_probs = Variable(torch.zeros(1000))\r\n    for i in range(2):\r\n        o_n, h_n_batch, c_n_batch = policy_net(input_batch[i], h_n_batch, c_n_batch)\r\n        dist = softmax(o_n)\r\n        ### Comment out L33-34 to stop memory leak ###\r\n        for j in range(1000):\r\n            sum_log_probs[j] = sum_log_probs[j] + torch.log(dist[j, 0])\r\n\r\nif __name__ == '__main__':\r\n    policy_net = PolicyNet([5, 32, 3])\r\n\r\n    for i in range(10000):\r\n        leak(policy_net)\r\n        print(i)\r\n```\r\n\r\nThis is for version `0.1.12_2` on macOS 10.12.4."}