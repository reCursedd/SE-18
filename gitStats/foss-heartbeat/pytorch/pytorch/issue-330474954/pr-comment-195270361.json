{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195270361", "pull_request_review_id": 128603976, "id": 195270361, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NTI3MDM2MQ==", "diff_hunk": "@@ -0,0 +1,149 @@\n+#ifndef CAFFE2_OPERATORS_REDUCE_OPS_H_\n+#define CAFFE2_OPERATORS_REDUCE_OPS_H_\n+\n+#include <algorithm>\n+#include <functional>\n+#include <vector>\n+\n+#include \"caffe2/core/context.h\"\n+#include \"caffe2/core/operator.h\"\n+#include \"caffe2/core/types.h\"\n+#include \"caffe2/utils/math.h\"\n+\n+namespace caffe2 {\n+\n+template <typename InputTypes, class Context, class Expander>\n+class ExpandOp final : public Operator<Context> {\n+ public:\n+  USE_OPERATOR_CONTEXT_FUNCTIONS;\n+\n+  ExpandOp(const OperatorDef& operator_def, Workspace* ws)\n+      : Operator<Context>(operator_def, ws) {}\n+\n+  bool RunOnDevice() override {\n+    return DispatchHelper<InputTypes>::call(this, Input(0));\n+  }\n+ template <typename T>\n+  bool DoRunWithType() {\n+    const auto& X = Input(0);\n+    const auto& Y_shape_tensor = Input(1);\n+\tstd::vector<int> shape_dims(Y_shape_tensor.size());\n+\tcontext_.template Copy<int, Context, CPUContext>(Y_shape_tensor.size(), Y_shape_tensor.template data<int>(), shape_dims.data());\n+    const int* Y_shape = Y_shape_tensor.template data<int>();\n+    auto* Y = Output(0);\n+\n+\tconst int ndim = shape_dims.size();\n+    const std::vector<int> X_dims(X.dims().cbegin(), X.dims().cend());\n+    std::vector<int> Y_dims;\n+    Y_dims.reserve(std::max(ndim, X.ndim()));\n+    // ndim, X.ndim() might equal to 0\n+    std::cout << \"loginfoo \" << ndim << \" \" << X.ndim() << std::endl;\n+    for (int i = ndim - 1, j = X.ndim() - 1; i >= 0 || j >= 0; i--, j--) {", "path": "caffe2/operators/expand_op.h", "position": null, "original_position": 41, "commit_id": "da691c3616829781c2d7d017cdfb1f468f5c55b9", "original_commit_id": "f7e3c112b9c77889378f26b1812505c7e262f9a1", "user": {"login": "bddppq", "id": 9300575, "node_id": "MDQ6VXNlcjkzMDA1NzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/9300575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bddppq", "html_url": "https://github.com/bddppq", "followers_url": "https://api.github.com/users/bddppq/followers", "following_url": "https://api.github.com/users/bddppq/following{/other_user}", "gists_url": "https://api.github.com/users/bddppq/gists{/gist_id}", "starred_url": "https://api.github.com/users/bddppq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bddppq/subscriptions", "organizations_url": "https://api.github.com/users/bddppq/orgs", "repos_url": "https://api.github.com/users/bddppq/repos", "events_url": "https://api.github.com/users/bddppq/events{/privacy}", "received_events_url": "https://api.github.com/users/bddppq/received_events", "type": "User", "site_admin": false}, "body": "looks to me this is quite similar to https://github.com/pytorch/pytorch/blob/d1bdb3b10ae789729e8aef48d85b4835f9bdfd51/caffe2/utils/math_cpu.cc#L876, could you refactor to use that util function? it's better to keep the broadcast implementation the same across all the places.", "created_at": "2018-06-14T00:00:05Z", "updated_at": "2018-11-23T15:45:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/8263#discussion_r195270361", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/8263", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/195270361"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/8263#discussion_r195270361"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/8263"}}, "body_html": "<p>looks to me this is quite similar to <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/d1bdb3b10ae789729e8aef48d85b4835f9bdfd51/caffe2/utils/math_cpu.cc#L876\">pytorch/caffe2/utils/math_cpu.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 876\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/d1bdb3b10ae789729e8aef48d85b4835f9bdfd51\">d1bdb3b</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L876\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"876\"></td>\n          <td id=\"LC876\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">void</span> <span class=\"pl-en\">BroadcastImpl</span>( </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n, could you refactor to use that util function? it's better to keep the broadcast implementation the same across all the places.</p>", "body_text": "looks to me this is quite similar to \n  \n    \n      pytorch/caffe2/utils/math_cpu.cc\n    \n    \n         Line 876\n      in\n      d1bdb3b\n    \n    \n    \n    \n\n        \n          \n           void BroadcastImpl( \n        \n    \n  \n\n, could you refactor to use that util function? it's better to keep the broadcast implementation the same across all the places."}