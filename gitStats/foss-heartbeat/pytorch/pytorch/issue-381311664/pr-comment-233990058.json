{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233990058", "pull_request_review_id": 175533495, "id": 233990058, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMzk5MDA1OA==", "diff_hunk": "@@ -833,57 +834,83 @@ TEST(DataLoaderTest, RespectsTimeout) {\n   ASSERT_LT(duration.count(), 1);\n }\n \n+struct Barrier {\n+  void wait(size_t target) {\n+    std::unique_lock<std::mutex> lock(mutex_);\n+    ++counter_;\n+    cv_.wait(lock, [this, target] { return this->counter_ == target; });\n+    cv_.notify_all();\n+  }\n+\n+  size_t counter_{0};\n+  std::condition_variable cv_;\n+  std::mutex mutex_;\n+};\n+\n+namespace ordering_test {\n namespace {\n-std::atomic<size_t> ordering_test_counter{0};\n-std::condition_variable ordering_test_cv;\n-std::mutex ordering_test_mutex;\n-const std::array<size_t, 4> ordering_test_order = {3, 1, 0, 2};\n-std::atomic<size_t> ordering_test_index{0};\n+const size_t kNumberOfWorkers = 10;\n+const std::vector<size_t> kOrderInWhichWorkersReturnTheirBatch =\n+    {3, 7, 0, 5, 4, 8, 2, 1, 9, 6};\n } // namespace\n \n-struct OrderingTestDataset : datasets::BatchDataset<DummyDataset, int> {\n-  OrderingTestDataset() = default;\n+struct Dataset : datasets::BatchDataset<Dataset, size_t> {\n+  Dataset() = default;\n \n   // This copy constructor will be called when we copy the dataset into a\n   // particular thread.\n-  OrderingTestDataset(const OrderingTestDataset& other)\n-      : id(ordering_test_counter++) {}\n-\n-  OrderingTestDataset(OrderingTestDataset&& other) noexcept = default;\n-  OrderingTestDataset& operator=(const OrderingTestDataset& other) = delete;\n-  OrderingTestDataset& operator=(OrderingTestDataset&& other) noexcept = delete;\n-\n-  int get_batch(torch::ArrayRef<size_t> indices) override {\n-    std::unique_lock<std::mutex> lock(ordering_test_mutex);\n-    // block until order.at(index) == my_thread_id (until it's this thread's\n-    // turn)\n-    ordering_test_cv.wait(lock, [this] {\n-      return ordering_test_order.at(ordering_test_index.load()) == this->id;\n-    });\n-    // Make one step in the order.\n-    ++ordering_test_index;\n+  Dataset(const Dataset& other) {\n+    static std::atomic<size_t> counter{0};\n+    thread_id_ = counter.fetch_add(1);\n+  }\n+\n+  Dataset(Dataset&& other) noexcept = default;\n+  Dataset& operator=(const Dataset& other) = delete;\n+  Dataset& operator=(Dataset&& other) noexcept = delete;\n+\n+  size_t get_batch(torch::ArrayRef<size_t> indices) override {\n+    static Barrier barrier;\n+    static auto order_iterator = kOrderInWhichWorkersReturnTheirBatch.begin();\n+    static std::condition_variable cv;\n+    static std::mutex mutex;\n+\n+    // Wait for all threads to get an index batch and arrive here.\n+    barrier.wait(kNumberOfWorkers);\n+\n+    std::unique_lock<std::mutex> lock(mutex);\n+    cv.wait(lock, [this] { return *order_iterator == this->thread_id_; });\n+    ++order_iterator;\n     lock.unlock();\n-    // Wake up the other threads to check if it's their turn to return.\n-    ordering_test_cv.notify_all();\n+    cv.notify_all();\n \n-    return id;\n+    return indices.front();", "path": "test/cpp/api/dataloader.cpp", "position": 132, "original_position": 87, "commit_id": "5580beff9774c75bcf0e178627d70e842a68a136", "original_commit_id": "b22ae9b293dc62ce41df41e0868c20dde86f9491", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "ok, that's a good comment to have :) (Does `enforce_ordering` work when `batch_size` is not one?)", "created_at": "2018-11-15T20:06:20Z", "updated_at": "2018-11-23T15:54:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/14038#discussion_r233990058", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/14038", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/233990058"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/14038#discussion_r233990058"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/14038"}}, "body_html": "<p>ok, that's a good comment to have :) (Does <code>enforce_ordering</code> work when <code>batch_size</code> is not one?)</p>", "body_text": "ok, that's a good comment to have :) (Does enforce_ordering work when batch_size is not one?)", "in_reply_to_id": 233986274}