{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234002085", "pull_request_review_id": 175548688, "id": 234002085, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNDAwMjA4NQ==", "diff_hunk": "@@ -833,57 +834,83 @@ TEST(DataLoaderTest, RespectsTimeout) {\n   ASSERT_LT(duration.count(), 1);\n }\n \n+struct Barrier {\n+  void wait(size_t target) {\n+    std::unique_lock<std::mutex> lock(mutex_);\n+    ++counter_;\n+    cv_.wait(lock, [this, target] { return this->counter_ == target; });\n+    cv_.notify_all();\n+  }\n+\n+  size_t counter_{0};\n+  std::condition_variable cv_;\n+  std::mutex mutex_;\n+};\n+\n+namespace ordering_test {\n namespace {\n-std::atomic<size_t> ordering_test_counter{0};\n-std::condition_variable ordering_test_cv;\n-std::mutex ordering_test_mutex;\n-const std::array<size_t, 4> ordering_test_order = {3, 1, 0, 2};\n-std::atomic<size_t> ordering_test_index{0};\n+const size_t kNumberOfWorkers = 10;\n+const std::vector<size_t> kOrderInWhichWorkersReturnTheirBatch =\n+    {3, 7, 0, 5, 4, 8, 2, 1, 9, 6};\n } // namespace\n \n-struct OrderingTestDataset : datasets::BatchDataset<DummyDataset, int> {\n-  OrderingTestDataset() = default;\n+struct Dataset : datasets::BatchDataset<Dataset, size_t> {\n+  Dataset() = default;\n \n   // This copy constructor will be called when we copy the dataset into a\n   // particular thread.\n-  OrderingTestDataset(const OrderingTestDataset& other)\n-      : id(ordering_test_counter++) {}\n-\n-  OrderingTestDataset(OrderingTestDataset&& other) noexcept = default;\n-  OrderingTestDataset& operator=(const OrderingTestDataset& other) = delete;\n-  OrderingTestDataset& operator=(OrderingTestDataset&& other) noexcept = delete;\n-\n-  int get_batch(torch::ArrayRef<size_t> indices) override {\n-    std::unique_lock<std::mutex> lock(ordering_test_mutex);\n-    // block until order.at(index) == my_thread_id (until it's this thread's\n-    // turn)\n-    ordering_test_cv.wait(lock, [this] {\n-      return ordering_test_order.at(ordering_test_index.load()) == this->id;\n-    });\n-    // Make one step in the order.\n-    ++ordering_test_index;\n+  Dataset(const Dataset& other) {\n+    static std::atomic<size_t> counter{0};\n+    thread_id_ = counter.fetch_add(1);\n+  }\n+\n+  Dataset(Dataset&& other) noexcept = default;\n+  Dataset& operator=(const Dataset& other) = delete;\n+  Dataset& operator=(Dataset&& other) noexcept = delete;\n+\n+  size_t get_batch(torch::ArrayRef<size_t> indices) override {\n+    static Barrier barrier;\n+    static auto order_iterator = kOrderInWhichWorkersReturnTheirBatch.begin();\n+    static std::condition_variable cv;\n+    static std::mutex mutex;\n+\n+    // Wait for all threads to get an index batch and arrive here.\n+    barrier.wait(kNumberOfWorkers);", "path": "test/cpp/api/dataloader.cpp", "position": null, "original_position": 76, "commit_id": "5580beff9774c75bcf0e178627d70e842a68a136", "original_commit_id": "b22ae9b293dc62ce41df41e0868c20dde86f9491", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "The first paragraph is correct. About the second paragraph: your observations are true, and the reason I do this is purely to simulate a deterministic *but unsorted* order in which threads are unblocked, to test that the ordering really takes place (otherwise maybe threads wold return in order, in which case the ordering functionality wouldn't really be tested). IRL threads could fetch multiple batches and that would be fine", "created_at": "2018-11-15T20:47:57Z", "updated_at": "2018-11-23T15:54:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/14038#discussion_r234002085", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/14038", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/234002085"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/14038#discussion_r234002085"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/14038"}}, "body_html": "<p>The first paragraph is correct. About the second paragraph: your observations are true, and the reason I do this is purely to simulate a deterministic <em>but unsorted</em> order in which threads are unblocked, to test that the ordering really takes place (otherwise maybe threads wold return in order, in which case the ordering functionality wouldn't really be tested). IRL threads could fetch multiple batches and that would be fine</p>", "body_text": "The first paragraph is correct. About the second paragraph: your observations are true, and the reason I do this is purely to simulate a deterministic but unsorted order in which threads are unblocked, to test that the ordering really takes place (otherwise maybe threads wold return in order, in which case the ordering functionality wouldn't really be tested). IRL threads could fetch multiple batches and that would be fine", "in_reply_to_id": 233989451}