{"url": "https://api.github.com/repos/pytorch/pytorch/issues/14038", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/14038/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/14038/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/14038/events", "html_url": "https://github.com/pytorch/pytorch/pull/14038", "id": 381311664, "node_id": "MDExOlB1bGxSZXF1ZXN0MjMxMzEyMzM4", "number": 14038, "title": "Fix DataLoaderTest.EnforcesOrderingAmongThreadsWhenConfigured", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-15T19:44:31Z", "updated_at": "2018-11-23T15:54:56Z", "closed_at": "2018-11-16T01:32:13Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/14038", "html_url": "https://github.com/pytorch/pytorch/pull/14038", "diff_url": "https://github.com/pytorch/pytorch/pull/14038.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/14038.patch"}, "body_html": "<p>I think this will be it. So for one, the previous test was bullshit because it was returning the thread id instead of the sample index (which is the thing whose ordering is enforced). Just turning up the number of threads to 10 from 4 made this very obvious. I also think there is a race condition, which may or may not have surfaced, in that there was nothing stopping one worker to get multiple batches, which would screw with the whole ordering logic. I've added a barrier struct such that workers wait for all workers to be in the <code>get_batch</code> function before actually doing something.</p>\n<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #14002.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"380944021\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/14002\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/14002/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/14002\">#14002</a></p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a></p>", "body_text": "I think this will be it. So for one, the previous test was bullshit because it was returning the thread id instead of the sample index (which is the thing whose ordering is enforced). Just turning up the number of threads to 10 from 4 made this very obvious. I also think there is a race condition, which may or may not have surfaced, in that there was nothing stopping one worker to get multiple batches, which would screw with the whole ordering logic. I've added a barrier struct such that workers wait for all workers to be in the get_batch function before actually doing something.\nFixes #14002\n@ezyang", "body": "I think this will be it. So for one, the previous test was bullshit because it was returning the thread id instead of the sample index (which is the thing whose ordering is enforced). Just turning up the number of threads to 10 from 4 made this very obvious. I also think there is a race condition, which may or may not have surfaced, in that there was nothing stopping one worker to get multiple batches, which would screw with the whole ordering logic. I've added a barrier struct such that workers wait for all workers to be in the `get_batch` function before actually doing something.\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/14002\r\n\r\n@ezyang "}