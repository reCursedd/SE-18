{"url": "https://api.github.com/repos/pytorch/pytorch/issues/12136", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/12136/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/12136/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/12136/events", "html_url": "https://github.com/pytorch/pytorch/issues/12136", "id": 364468103, "node_id": "MDU6SXNzdWUzNjQ0NjgxMDM=", "number": 12136, "title": "[Caffe2] GAN No Gradients in Generator", "user": {"login": "kabibubi", "id": 38755008, "node_id": "MDQ6VXNlcjM4NzU1MDA4", "avatar_url": "https://avatars2.githubusercontent.com/u/38755008?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kabibubi", "html_url": "https://github.com/kabibubi", "followers_url": "https://api.github.com/users/kabibubi/followers", "following_url": "https://api.github.com/users/kabibubi/following{/other_user}", "gists_url": "https://api.github.com/users/kabibubi/gists{/gist_id}", "starred_url": "https://api.github.com/users/kabibubi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kabibubi/subscriptions", "organizations_url": "https://api.github.com/users/kabibubi/orgs", "repos_url": "https://api.github.com/users/kabibubi/repos", "events_url": "https://api.github.com/users/kabibubi/events{/privacy}", "received_events_url": "https://api.github.com/users/kabibubi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-09-27T13:08:25Z", "updated_at": "2018-09-27T13:37:39Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>Hi everyone,<br>\nIn the process of my Bachelor Degree I am building a DCGAN in caffe2. After following all the tutorials I have figured out a architecture. But when I run my Code the Generator does not update at all. I made some debugging and found out, that the Generator's layers do not receive any Gradients at all.<br>\nEven my tutor can't find the source of the issue.</p>\n<p>I am grateful for any help! =)</p>\n<p>Here is my Code:</p>\n<h1>-<em>- coding: utf-8 -</em>-</h1>\n<p>from <strong>future</strong> import absolute_import<br>\nfrom <strong>future</strong> import division<br>\nfrom <strong>future</strong> import print_function<br>\nfrom <strong>future</strong> import unicode_literals</p>\n<p>import os<br>\nimport shutil<br>\n#import pickle as pkl<br>\nimport numpy as np</p>\n<p>from matplotlib import pyplot#, image<br>\nfrom caffe2.python import brew, core, workspace, model_helper, optimizer, visualize, memonger, utils<br>\nfrom caffe2.proto import caffe2_pb2</p>\n<p>###############################################################################</p>\n<h1>Data Import</h1>\n<p>###############################################################################</p>\n<p>workspace.ResetWorkspace()</p>\n<p>def DownloadResource(url, path):<br>\n'''Downloads resources from s3 by url and unzips them to the provided path'''<br>\nimport requests, StringIO, zipfile<br>\nprint(\"Downloading... {} to {}\".format(url, path))<br>\nr = requests.get(url, stream=True)<br>\nz = zipfile.ZipFile(StringIO.StringIO(r.content))<br>\nz.extractall(path)<br>\nprint(\"Completed download and extraction.\")</p>\n<p>current_folder = os.getcwd()<br>\nprint(\"The current folder is {}\".format(current_folder) )<br>\ndata_folder = os.path.join(current_folder, 'tutorial_data', 'mnist')<br>\nroot_folder = os.path.join(current_folder, 'tutorial_files', 'tutorial_mnist')<br>\ndb_missing = False</p>\n<p>if not os.path.exists(data_folder):<br>\nos.makedirs(data_folder)<br>\nprint(\"Your data folder was not found!! This was generated: {}\".format(data_folder))</p>\n<h1>Look for existing database: lmdb</h1>\n<p>if os.path.exists(os.path.join(data_folder,\"mnist-train-nchw-lmdb\")):<br>\nprint(\"lmdb train db found!\")<br>\nelse:<br>\ndb_missing = True</p>\n<p>if os.path.exists(os.path.join(data_folder,\"mnist-test-nchw-lmdb\")):<br>\nprint(\"lmdb test db found!\")<br>\nelse:<br>\ndb_missing = True</p>\n<h1>attempt the download of the db if either was missing</h1>\n<p>if db_missing:<br>\nprint(\"one or both of the MNIST lmbd dbs not found!!\")<br>\ndb_url = \"<a href=\"http://download.caffe2.ai/databases/mnist-lmdb.zip\" rel=\"nofollow\">http://download.caffe2.ai/databases/mnist-lmdb.zip</a>\"<br>\ntry:<br>\nDownloadResource(db_url, data_folder)<br>\nexcept Exception as ex:<br>\nprint(\"Failed to download dataset. Please download it manually from {}\".format(db_url))<br>\nprint(\"Unzip it and place the two database folders here: {}\".format(data_folder))<br>\nraise ex</p>\n<p>if os.path.exists(root_folder):<br>\nprint(\"Looks like you ran this before, so we need to cleanup those old files...\")<br>\nshutil.rmtree(root_folder)</p>\n<p>os.makedirs(root_folder)<br>\nworkspace.ResetWorkspace(root_folder)</p>\n<p>print(\"training data folder:\" + data_folder)<br>\nprint(\"workspace root folder:\" + root_folder)</p>\n<p>###############################################################################</p>\n<h1>Memory Management</h1>\n<p>###############################################################################</p>\n<h1>Kann zur Performanceoptimierung sp\u00e4ter eingesetzt werden.</h1>\n<p>def optimize_gradient_memory(model, loss):<br>\nmodel.net._net = memonger.share_grad_blobs(<br>\nmodel.net,<br>\nloss,<br>\nset(model.param_to_grad.values()),<br>\n# Due to memonger internals, we need a namescope here. Let's make one up; we'll need it later!<br>\nnamescope=\"memonger_needs_a_namescope\",<br>\nshare_activations=False)</p>\n<p>###############################################################################</p>\n<h1>Global Parameters</h1>\n<p>###############################################################################</p>\n<p>device_option = caffe2_pb2.DeviceOption(device_type=caffe2_pb2.CUDA)</p>\n<h1>Gr\u00f6\u00dfere Batchsize oder learning_rate f\u00fchrt zu loss_d = nan</h1>\n<p>batch_size = 32<br>\narg_scope = {\"order\": \"NCHW\"}<br>\nlearning_rate = 0.00003</p>\n<h1>For Leaky ReLU</h1>\n<p>alpha = 0.1</p>\n<h1>Create labels for D_Real. Only ones. Soft and noisy.</h1>\n<p>label_real = np.random.rand(batch_size, 2048, 1, 1).astype('float32') * 0.1 + 0.9</p>\n<h1>Create labels for D_Fake. Only zeros. Soft and noisy</h1>\n<p>label_fake = np.random.rand(batch_size, 2048, 1, 1).astype('float32') * 0.1</p>\n<h1>Create labels for G. Only ones.</h1>\n<p>label_g = np.ones(batch_size, dtype=np.int32)</p>\n<h1>Dummy Blobs for evading deadlock between G and d_fake.</h1>\n<h1>For G</h1>\n<p>dummyblob_g = np.ones(batch_size)<br>\ndummyblob_d_loss_fake = np.ones(1)</p>\n<h1>For D</h1>\n<p>dummyblob_d = np.ones(batch_size<em>28</em>28).reshape(batch_size, 1, 28, 28)</p>\n<h1>Noise Data Input for the Generator</h1>\n<p>noise = np.random.randn(batch_size, 100).astype(np.float32)</p>\n<h1>Insert all relevant data to workspace</h1>\n<p>with core.DeviceScope(device_option):<br>\nworkspace.FeedBlob(\"tanh\", dummyblob_d.astype(np.float32))<br>\nworkspace.FeedBlob(\"sigmoid_d_real\", dummyblob_d.astype(np.float32))<br>\nworkspace.FeedBlob(\"sigmoid_d_fake\", dummyblob_g.astype(np.float32))<br>\nworkspace.FeedBlob(\"d_loss_fake\", dummyblob_d_loss_fake.astype(np.float32))<br>\nworkspace.FeedBlob(\"label_g\", label_g)<br>\nworkspace.FeedBlob(\"label_fake\", label_fake)<br>\nworkspace.FeedBlob(\"label_real\", label_real)<br>\nworkspace.FeedBlob(\"noise\", noise)</p>\n<p>###############################################################################</p>\n<h1>Define Models</h1>\n<p>###############################################################################</p>\n<h1>Manage Input for D_real</h1>\n<p>def AddInput(model, batch_size, db, db_type):<br>\n# load the data<br>\ndata_uint8, label = model.TensorProtosDBInput(<br>\n[], [\"data_uint8\", \"label\"], batch_size = batch_size,<br>\ndb=db, db_type=db_type)<br>\n# cast the data to float<br>\ndata = model.Cast(data_uint8, \"data\", to=core.DataType.FLOAT)<br>\n# scale data from [0,255] to [-1,1]<br>\ndata = model.Scale(data, data, scale=float(1./256 * 2 - 1))<br>\n# Caffe2 ist sehr generisch! Tats\u00e4chlich wird hier (fast) alles als<br>\n# Operator behandelt. Dabei auch der ganz simple input! Die Backpropagation<br>\n# welche die Werte der hidden Layers updated hat hier jedoch nichts verloren.<br>\n# Deswegen muss caffe2, auf Grund seiner generischen Bauweise, noch einmal<br>\n# extra gesagt werden, dass hier bitte keine updates erfolgen sollen.<br>\n# Darum: StopGradient<br>\ndata = model.StopGradient(data, data)</p>\n<pre><code>return data\n</code></pre>\n<p>###############################################################################</p>\n<h1>Define Generator</h1>\n<p>def GenModel(model, data):<br>\n# Input Layer: 1x1x100<br>\n# data is noise<br>\nfc1 = brew.fc(model, data, \"fc1\", dim_in=100, dim_out=4096)<br>\nreshape, oldshape = model.net.Reshape(<br>\n[\"fc1\"],<br>\n[\"reshape\", \"oldshape\"],<br>\nshape = (batch_size, 1024, 2, 2))<br>\nrelu0_g = brew.relu(model, reshape, \"relu0_g\")</p>\n<pre><code># Deconv Block 1: 256x2x2 \ndeconv1 = brew.conv_transpose(\nmodel,\nrelu0_g, \n\"deconv1\", \ndim_in=1024, \ndim_out=512, \nkernel=2, \nstride=2\n)\n\nbatchnorm1_g = brew.spatial_bn(\nmodel, \ndeconv1, \n\"batchnorm1_g\", \n512, \nepsilon=1e-5, \nmomentum=0.9,\nis_test=False        \n)    \n\nrelu1_g = brew.relu(model, batchnorm1_g, \"relu1_g\")\n\n# Deconv Block 2: 128x4x4\ndeconv2 = brew.conv_transpose(\nmodel,\nrelu1_g, \n\"deconv2\", \ndim_in=512, \ndim_out=256, \nkernel=4, \nstride=1\n)\n\nbatchnorm2_g = brew.spatial_bn(\nmodel, \ndeconv2, \n\"batchnorm2_g\", \n256, \nepsilon=1e-5, \nmomentum=0.9,\nis_test=False        \n)\n\nrelu2_g = brew.relu(model, batchnorm2_g, 'relu2_g')\n\n# Deconv Block 3: 64x7x7\ndeconv3 = brew.conv_transpose(\nmodel,\nrelu2_g, \n\"deconv3\", \ndim_in=256, \ndim_out=128, \nkernel=2, \nstride=2\n)\n\nbatchnorm3_g = brew.spatial_bn(\nmodel, \ndeconv3, \n\"batchnorm3_g\", \n128, \nepsilon=1e-5, \nmomentum=0.9,\nis_test=False        \n)    \n\nrelu3_g = brew.relu(model, batchnorm3_g, 'relu3_g')\n\n# Deconv Block 4: 32x14x14 -&gt; 1x28x28\ndeconv4 = brew.conv_transpose(\nmodel,\nrelu3_g, \n\"deconv4\", \ndim_in=128, \ndim_out=1, \nkernel=2, \nstride=2\n)\n\nbatchnorm4_g = brew.spatial_bn(\nmodel, \ndeconv4, \n\"batchnorm4_g\", \n1, \nepsilon=1e-5, \nmomentum=0.9,\nis_test=False        \n)\n\ntanh = brew.tanh(model, batchnorm4_g, \"tanh\")\n    \nreturn tanh\n</code></pre>\n<p>###############################################################################</p>\n<h1>Define Discriminator Real</h1>\n<p>def DisModel_real(model, data):<br>\n# convblock 1 28x28 -&gt; 14x14<br>\nconv1_d = brew.conv(model, data, \"conv1_d\", dim_in=1, dim_out=128, kernel=2, stride=2, pad=0)<br>\nbatchnorm1_d = brew.spatial_bn(<br>\nmodel,<br>\nconv1_d,<br>\n\"batchnorm1_d\",<br>\n128,<br>\nepsilon=1e-5,<br>\nmomentum=0.9,<br>\nis_test=False<br>\n)</p>\n<pre><code>lrelu1_d = model.net.LeakyRelu(batchnorm1_d, \"lrelu1_d\", alpha=alpha)\n\n\n# convblock 2 14x14 -&gt; 7x7\nconv2_d = brew.conv(model, lrelu1_d, \"conv2_d\", dim_in=128, dim_out=256, kernel=2, stride=2, pad=0)\nbatchnorm2_d = brew.spatial_bn(\n    model, \n    conv2_d, \n    \"batchnorm2_d\", \n    256, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu2_d = model.net.LeakyRelu(batchnorm2_d, \"lrelu2_d\", alpha=alpha)\n\n# convblock 3 7x7 -&gt; 4x4\nconv3_d = brew.conv(model, lrelu2_d, \"conv3_d\", dim_in=256, dim_out=512, kernel=1, stride=2, pad=0)\nbatchnorm3_d = brew.spatial_bn(\n    model, \n    conv3_d, \n    \"batchnorm3_d\", \n    512, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu3_d = model.net.LeakyRelu(batchnorm3_d, \"lrelu3_d\", alpha=alpha)\n\n# convblock 4 4x4 -&gt; 2x2\nconv4_d = brew.conv(model, lrelu3_d, \"conv4_d\", dim_in=512, dim_out=512, kernel=2, stride=2, pad=0)\nbatchnorm4_d = brew.spatial_bn(\n    model, \n    conv4_d, \n    \"batchnorm4_d\", \n    512, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu4_d = model.net.LeakyRelu(batchnorm4_d, \"lrelu4_d\", alpha=alpha)\n\n# Flatten 512x2x2 -&gt; 2048x1x1    \nreshape_d, oldshape_d = model.net.Reshape(\n[\"lrelu4_d\"], \n[\"reshape_d\", \"oldshape_d\"], \nshape=(batch_size,2048,1,1))    \n\nsigmoid_real = model.net.Sigmoid(reshape_d, \"sigmoid_d_real\")\n\nreturn sigmoid_real\n</code></pre>\n<p>###############################################################################</p>\n<h1>Define Discriminator Fake</h1>\n<p>def DisModel_fake(model, data):<br>\n# convblock 1 28x28 -&gt; 14x14<br>\nconv1_d = brew.conv(model, data, \"conv1_d\", dim_in=1, dim_out=128, kernel=2, stride=2, pad=0)<br>\nbatchnorm1_d = brew.spatial_bn(<br>\nmodel,<br>\nconv1_d,<br>\n\"batchnorm1_d\",<br>\n128,<br>\nepsilon=1e-5,<br>\nmomentum=0.9,<br>\nis_test=False<br>\n)</p>\n<pre><code>lrelu1_d = model.net.LeakyRelu(batchnorm1_d, \"lrelu1_d\", alpha=alpha)\n\n\n# convblock 2 14x14 -&gt; 7x7\nconv2_d = brew.conv(model, lrelu1_d, \"conv2_d\", dim_in=128, dim_out=256, kernel=2, stride=2, pad=0)\nbatchnorm2_d = brew.spatial_bn(\n    model, \n    conv2_d, \n    \"batchnorm2_d\", \n    256, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu2_d = model.net.LeakyRelu(batchnorm2_d, \"lrelu2_d\", alpha=alpha)\n\n# convblock 3 7x7 -&gt; 4x4\nconv3_d = brew.conv(model, lrelu2_d, \"conv3_d\", dim_in=256, dim_out=512, kernel=1, stride=2, pad=0)\nbatchnorm3_d = brew.spatial_bn(\n    model, \n    conv3_d, \n    \"batchnorm3_d\", \n    512, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu3_d = model.net.LeakyRelu(batchnorm3_d, \"lrelu3_d\", alpha=alpha)\n\n# convblock 4 4x4 -&gt; 2x2\nconv4_d = brew.conv(model, lrelu3_d, \"conv4_d\", dim_in=512, dim_out=512, kernel=2, stride=2, pad=0)\nbatchnorm4_d = brew.spatial_bn(\n    model, \n    conv4_d, \n    \"batchnorm4_d\", \n    512, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu4_d = model.net.LeakyRelu(batchnorm4_d, \"lrelu4_d\", alpha=alpha)\n\n# Flatten 512x2x2 -&gt; 2048x1x1    \nreshape_d, oldshape_d = model.net.Reshape(\n[\"lrelu4_d\"], \n[\"reshape_d\", \"oldshape_d\"], \nshape=(batch_size,2048,1,1))\n\nsigmoid_fake = model.net.Sigmoid(reshape_d, \"sigmoid_d_fake\")\n\nreturn sigmoid_fake\n</code></pre>\n<p>###############################################################################</p>\n<h1>Define Training</h1>\n<p>###############################################################################</p>\n<h1>Training Operators for D</h1>\n<p>def AddTrainingOperators_D(model_r, model_f, sigmoid_r, sigmoid_f, lr = learning_rate):<br>\nxent_fake = model_f.net.SigmoidCrossEntropyWithLogits([sigmoid_f, \"label_fake\"], 'xent_fake')<br>\nd_loss_fake = model_f.net.AveragedLoss(xent_fake, \"d_loss_fake\")<br>\nxent_real = model_r.net.SigmoidCrossEntropyWithLogits([sigmoid_r, \"label_real\"], 'xent_real')<br>\nd_loss_real = model_r.net.AveragedLoss(xent_real, \"d_loss_real\")<br>\nd_loss_total_r = model_r.net.Add([\"d_loss_real\", \"d_loss_fake\"], \"d_loss_total_r\")<br>\nd_loss_total_f = model_f.net.Add([\"d_loss_real\", \"d_loss_fake\"], \"d_loss_total_f\")<br>\nmodel_r.AddGradientOperators([d_loss_total_r])<br>\nmodel_f.AddGradientOperators([d_loss_total_f])<br>\noptimizer.build_adam(model_f, lr)<br>\noptimizer.build_adam(model_r, lr)</p>\n<p>###############################################################################</p>\n<h1>Training Operators for G</h1>\n<p>def AddTrainingOperators_Gen(model, fake_sigmoid, learning_rate=learning_rate):<br>\nxent = model.net.LabelCrossEntropy([fake_sigmoid, \"label_g\"], 'xent_g')<br>\n# compute the expected loss with the help of xent.<br>\nloss_g = model.net.AveragedLoss(xent, \"loss_g\")<br>\n# use the average loss to to add gradient operators to the model<br>\nmodel.AddGradientOperators([loss_g])<br>\n# Use adam<br>\noptimizer.build_adam(model, learning_rate)</p>\n<p>###############################################################################</p>\n<h1>Create Models</h1>\n<p>###############################################################################</p>\n<h1>Create G</h1>\n<p>generator = model_helper.ModelHelper(name=\"mnist_gen\")</p>\n<h1>Create D_real</h1>\n<p>discriminator_real = model_helper.ModelHelper(<br>\nname=\"mnist_dis_real\", arg_scope=arg_scope)</p>\n<h1>Create D_fake</h1>\n<p>discriminator_fake = model_helper.ModelHelper(<br>\nname=\"mnist_dis_fake\", arg_scope=arg_scope)</p>\n<h1>Apply net-definitions</h1>\n<p>with core.DeviceScope(device_option):<br>\n# Get Data<br>\ndata = AddInput(<br>\ndiscriminator_real, batch_size=batch_size,<br>\ndb=os.path.join(data_folder, 'mnist-test-nchw-lmdb'),<br>\ndb_type='lmdb')<br>\n# With Data from noise vector<br>\ntanh_gen = GenModel(generator, \"noise\")<br>\n# Only filled with data from MNIST.<br>\ntrue_sigmoid = DisModel_real(discriminator_real, data)<br>\n# Only filled with data from G.<br>\nfake_sigmoid = DisModel_fake(discriminator_fake, tanh_gen)<br>\n# Add Trainingsoperators<br>\n# For G<br>\nAddTrainingOperators_Gen(generator, fake_sigmoid, learning_rate)<br>\n# For D<br>\nAddTrainingOperators_D(discriminator_real, discriminator_fake, true_sigmoid, fake_sigmoid, learning_rate)</p>\n<p>###############################################################################<br>\n#Initialize the network<br>\n###############################################################################</p>\n<p>workspace.RunNetOnce(discriminator_real.param_init_net)<br>\nworkspace.CreateNet(discriminator_real.net)<br>\nworkspace.RunNetOnce(generator.param_init_net)<br>\nworkspace.CreateNet(generator.net)<br>\nworkspace.RunNetOnce(discriminator_fake.param_init_net)<br>\nworkspace.CreateNet(discriminator_fake.net)</p>\n<p>print(workspace.Blobs())</p>\n<p>###############################################################################</p>\n<h1>Run the training procedure</h1>\n<p>###############################################################################</p>\n<h1>times the training will be running</h1>\n<p>epochs = 10<br>\nsteps = int(600/ batch_size) # MNIST size / batch_size -&gt; 1 epoch</p>\n<h1>Containers for plotting progress</h1>\n<p>loss_d = np.zeros(steps)<br>\nloss_g = np.zeros(steps)<br>\nimages = np.empty((batch_size,1,28,28), np.float32)</p>\n<p>print(\"Total Number of Runs: {}\".format(epochs * steps))</p>\n<p>for e in range (epochs):<br>\n# Zum Debuggen G Output als print. Bug: G updatet nicht. Werte bleiben gleich.<br>\ntanh_out = workspace.FetchBlob('tanh')<br>\n#print(tanh_out[0][0][0])<br>\nfor i in range(steps):<br>\n# Train D<br>\nworkspace.RunNet(discriminator_real.net)<br>\nworkspace.RunNet(discriminator_fake.net)</p>\n<pre><code>    # Noise Data Input for the Generator \n    noise = np.random.randn(batch_size, 100).astype(np.float32)\n    workspace.FeedBlob(\"noise\", noise)\n    \n    # Train G    \n    workspace.RunNet(generator.net)\n    \n    loss_d[i] = workspace.FetchBlob(\"d_loss_total_r\")\n    loss_g[i] = workspace.FetchBlob(\"loss_g\")\n\n    # Nach dem Debugging wieder dekommentieren. \n    \"\"\"if (i % 50) == 0:\n        print(\"Round: {}\".format(i * (e+1)))\n        print(\"LOSS D\")\n        print(workspace.FetchBlob(\"d_loss_total_r\"))\n        print(\"LOSS G\")\n        print(workspace.FetchBlob(\"loss_g\"))   \"\"\"     \n</code></pre>\n<p>###############################################################################</p>\n<h1>After the execution is done, let's plot the values.</h1>\n<p>print(\"Final D loss: {}\".format(workspace.FetchBlob(\"d_loss_total_r\")))<br>\nprint(\"Final G loss: {}\".format(workspace.FetchBlob(\"loss_g\")))</p>\n<p>pyplot.plot(loss_d, 'b')<br>\npyplot.plot(loss_g, 'r')<br>\npyplot.title(\"Summary of Training Run\")<br>\npyplot.xlabel(\"Iteration\")<br>\npyplot.legend(('Loss_d', 'Loss_g'), loc='upper right')</p>\n<h1>Plot G Results</h1>\n<h1>Use visualize module to show the examples from the last batch that was fed to the model</h1>\n<p>tanh_out = workspace.FetchBlob('tanh')<br>\npyplot.figure()<br>\npyplot.title(\"Last Batch from G\")<br>\n_ = visualize.NCHW.ShowMultiple(tanh_out)</p>\n<h1>Nur f\u00fcr das Debugging auskommentiert. Sp\u00e4ter wieder einf\u00fcgen!</h1>\n<p>#pyplot.show()<br>\n###############################################################################</p>\n<p>And here are the networks blobs.</p>\n<pre><code>[u'AdamOptimizer_1_lr_gpu0', u'AdamOptimizer_2_lr_gpu0', u'lrelu4_d_grad_dims', u'batchnorm1_d', u'batchnorm1_d_b', u'batchnorm1_d_b_first_moment', u'batchnorm1_d_b_grad', u'batchnorm1_d_b_second_moment', u'batchnorm1_d_grad', u'batchnorm1_d_riv', u'batchnorm1_d_rm', u'batchnorm1_d_s', u'batchnorm1_d_s_first_moment', u'batchnorm1_d_s_grad', u'batchnorm1_d_s_second_moment', u'batchnorm1_d_siv', u'batchnorm1_d_sm', u'batchnorm1_g', u'batchnorm1_g_b', u'batchnorm1_g_riv', u'batchnorm1_g_rm', u'batchnorm1_g_s', u'batchnorm1_g_siv', u'batchnorm1_g_sm', u'batchnorm2_d', u'batchnorm2_d_b', u'batchnorm2_d_b_first_moment', u'batchnorm2_d_b_grad', u'batchnorm2_d_b_second_moment', u'batchnorm2_d_grad', u'batchnorm2_d_riv', u'batchnorm2_d_rm', u'batchnorm2_d_s', u'batchnorm2_d_s_first_moment', u'batchnorm2_d_s_grad', u'batchnorm2_d_s_second_moment', u'batchnorm2_d_siv', u'batchnorm2_d_sm', u'batchnorm2_g', u'batchnorm2_g_b', u'batchnorm2_g_riv', u'batchnorm2_g_rm', u'batchnorm2_g_s', u'batchnorm2_g_siv', u'batchnorm2_g_sm', u'batchnorm3_d', u'batchnorm3_d_b', u'batchnorm3_d_b_first_moment', u'batchnorm3_d_b_grad', u'batchnorm3_d_b_second_moment', u'batchnorm3_d_grad', u'batchnorm3_d_riv', u'batchnorm3_d_rm', u'batchnorm3_d_s', u'batchnorm3_d_s_first_moment', u'batchnorm3_d_s_grad', u'batchnorm3_d_s_second_moment', u'batchnorm3_d_siv', u'batchnorm3_d_sm', u'batchnorm3_g', u'batchnorm3_g_b', u'batchnorm3_g_riv', u'batchnorm3_g_rm', u'batchnorm3_g_s', u'batchnorm3_g_siv', u'batchnorm3_g_sm', u'batchnorm4_d', u'batchnorm4_d_b', u'batchnorm4_d_b_first_moment', u'batchnorm4_d_b_grad', u'batchnorm4_d_b_second_moment', u'batchnorm4_d_grad', u'batchnorm4_d_riv', u'batchnorm4_d_rm', u'batchnorm4_d_s', u'batchnorm4_d_s_first_moment', u'batchnorm4_d_s_grad', u'batchnorm4_d_s_second_moment', u'batchnorm4_d_siv', u'batchnorm4_d_sm', u'batchnorm4_g', u'batchnorm4_g_b', u'batchnorm4_g_riv', u'batchnorm4_g_rm', u'batchnorm4_g_s', u'batchnorm4_g_siv', u'batchnorm4_g_sm', u'conv1_d', u'conv1_d_b', u'conv1_d_b_first_moment', u'conv1_d_b_grad', u'conv1_d_b_second_moment', u'conv1_d_grad', u'conv1_d_w', u'conv1_d_w_first_moment', u'conv1_d_w_grad', u'conv1_d_w_second_moment', u'conv2_d', u'conv2_d_b', u'conv2_d_b_first_moment', u'conv2_d_b_grad', u'conv2_d_b_second_moment', u'conv2_d_grad', u'conv2_d_w', u'conv2_d_w_first_moment', u'conv2_d_w_grad', u'conv2_d_w_second_moment', u'conv3_d', u'conv3_d_b', u'conv3_d_b_first_moment', u'conv3_d_b_grad', u'conv3_d_b_second_moment', u'conv3_d_grad', u'conv3_d_w', u'conv3_d_w_first_moment', u'conv3_d_w_grad', u'conv3_d_w_second_moment', u'conv4_d', u'conv4_d_b', u'conv4_d_b_first_moment', u'conv4_d_b_grad', u'conv4_d_b_second_moment', u'conv4_d_grad', u'conv4_d_w', u'conv4_d_w_first_moment', u'conv4_d_w_grad', u'conv4_d_w_second_moment', u'd_loss_fake', u'd_loss_fake_grad', u'd_loss_real', u'd_loss_real_grad', u'd_loss_total_f', u'd_loss_total_f_autogen_grad', u'd_loss_total_r', u'd_loss_total_r_autogen_grad', u'data', u'data_grad', u'data_uint8', u'dbreader/home/kanani/workspace/tutorial_data/mnist/mnist-test-nchw-lmdb', u'deconv1', u'deconv1_b', u'deconv1_w', u'deconv2', u'deconv2_b', u'deconv2_w', u'deconv3', u'deconv3_b', u'deconv3_w', u'deconv4', u'deconv4_b', u'deconv4_w', u'fc1', u'fc1_b', u'fc1_w', u'iteration_mutex', u'label', u'label_fake', u'label_g', u'label_real', u'loss_g', u'loss_g_autogen_grad', u'lrelu1_d', u'lrelu1_d_grad', u'lrelu2_d', u'lrelu2_d_grad', u'lrelu3_d', u'lrelu3_d_grad', u'lrelu4_d', u'lrelu4_d_grad', u'noise', u'oldshape', u'oldshape_d', u'optimizer_iteration', u'relu0_g', u'relu1_g', u'relu2_g', u'relu3_g', u'reshape', u'reshape_d', u'reshape_d_grad', u'sigmoid_d_fake', u'sigmoid_d_fake_grad', u'sigmoid_d_real', u'sigmoid_d_real_grad', u'tanh', u'tanh_grad', u'xent_fake', u'xent_fake_grad', u'xent_g', u'xent_g_grad', u'xent_real', u'xent_real_grad']\n</code></pre>\n<p>Thanks everyone! =)</p>", "body_text": "Hi everyone,\nIn the process of my Bachelor Degree I am building a DCGAN in caffe2. After following all the tutorials I have figured out a architecture. But when I run my Code the Generator does not update at all. I made some debugging and found out, that the Generator's layers do not receive any Gradients at all.\nEven my tutor can't find the source of the issue.\nI am grateful for any help! =)\nHere is my Code:\n-- coding: utf-8 --\nfrom future import absolute_import\nfrom future import division\nfrom future import print_function\nfrom future import unicode_literals\nimport os\nimport shutil\n#import pickle as pkl\nimport numpy as np\nfrom matplotlib import pyplot#, image\nfrom caffe2.python import brew, core, workspace, model_helper, optimizer, visualize, memonger, utils\nfrom caffe2.proto import caffe2_pb2\n###############################################################################\nData Import\n###############################################################################\nworkspace.ResetWorkspace()\ndef DownloadResource(url, path):\n'''Downloads resources from s3 by url and unzips them to the provided path'''\nimport requests, StringIO, zipfile\nprint(\"Downloading... {} to {}\".format(url, path))\nr = requests.get(url, stream=True)\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\nz.extractall(path)\nprint(\"Completed download and extraction.\")\ncurrent_folder = os.getcwd()\nprint(\"The current folder is {}\".format(current_folder) )\ndata_folder = os.path.join(current_folder, 'tutorial_data', 'mnist')\nroot_folder = os.path.join(current_folder, 'tutorial_files', 'tutorial_mnist')\ndb_missing = False\nif not os.path.exists(data_folder):\nos.makedirs(data_folder)\nprint(\"Your data folder was not found!! This was generated: {}\".format(data_folder))\nLook for existing database: lmdb\nif os.path.exists(os.path.join(data_folder,\"mnist-train-nchw-lmdb\")):\nprint(\"lmdb train db found!\")\nelse:\ndb_missing = True\nif os.path.exists(os.path.join(data_folder,\"mnist-test-nchw-lmdb\")):\nprint(\"lmdb test db found!\")\nelse:\ndb_missing = True\nattempt the download of the db if either was missing\nif db_missing:\nprint(\"one or both of the MNIST lmbd dbs not found!!\")\ndb_url = \"http://download.caffe2.ai/databases/mnist-lmdb.zip\"\ntry:\nDownloadResource(db_url, data_folder)\nexcept Exception as ex:\nprint(\"Failed to download dataset. Please download it manually from {}\".format(db_url))\nprint(\"Unzip it and place the two database folders here: {}\".format(data_folder))\nraise ex\nif os.path.exists(root_folder):\nprint(\"Looks like you ran this before, so we need to cleanup those old files...\")\nshutil.rmtree(root_folder)\nos.makedirs(root_folder)\nworkspace.ResetWorkspace(root_folder)\nprint(\"training data folder:\" + data_folder)\nprint(\"workspace root folder:\" + root_folder)\n###############################################################################\nMemory Management\n###############################################################################\nKann zur Performanceoptimierung sp\u00e4ter eingesetzt werden.\ndef optimize_gradient_memory(model, loss):\nmodel.net._net = memonger.share_grad_blobs(\nmodel.net,\nloss,\nset(model.param_to_grad.values()),\n# Due to memonger internals, we need a namescope here. Let's make one up; we'll need it later!\nnamescope=\"memonger_needs_a_namescope\",\nshare_activations=False)\n###############################################################################\nGlobal Parameters\n###############################################################################\ndevice_option = caffe2_pb2.DeviceOption(device_type=caffe2_pb2.CUDA)\nGr\u00f6\u00dfere Batchsize oder learning_rate f\u00fchrt zu loss_d = nan\nbatch_size = 32\narg_scope = {\"order\": \"NCHW\"}\nlearning_rate = 0.00003\nFor Leaky ReLU\nalpha = 0.1\nCreate labels for D_Real. Only ones. Soft and noisy.\nlabel_real = np.random.rand(batch_size, 2048, 1, 1).astype('float32') * 0.1 + 0.9\nCreate labels for D_Fake. Only zeros. Soft and noisy\nlabel_fake = np.random.rand(batch_size, 2048, 1, 1).astype('float32') * 0.1\nCreate labels for G. Only ones.\nlabel_g = np.ones(batch_size, dtype=np.int32)\nDummy Blobs for evading deadlock between G and d_fake.\nFor G\ndummyblob_g = np.ones(batch_size)\ndummyblob_d_loss_fake = np.ones(1)\nFor D\ndummyblob_d = np.ones(batch_size2828).reshape(batch_size, 1, 28, 28)\nNoise Data Input for the Generator\nnoise = np.random.randn(batch_size, 100).astype(np.float32)\nInsert all relevant data to workspace\nwith core.DeviceScope(device_option):\nworkspace.FeedBlob(\"tanh\", dummyblob_d.astype(np.float32))\nworkspace.FeedBlob(\"sigmoid_d_real\", dummyblob_d.astype(np.float32))\nworkspace.FeedBlob(\"sigmoid_d_fake\", dummyblob_g.astype(np.float32))\nworkspace.FeedBlob(\"d_loss_fake\", dummyblob_d_loss_fake.astype(np.float32))\nworkspace.FeedBlob(\"label_g\", label_g)\nworkspace.FeedBlob(\"label_fake\", label_fake)\nworkspace.FeedBlob(\"label_real\", label_real)\nworkspace.FeedBlob(\"noise\", noise)\n###############################################################################\nDefine Models\n###############################################################################\nManage Input for D_real\ndef AddInput(model, batch_size, db, db_type):\n# load the data\ndata_uint8, label = model.TensorProtosDBInput(\n[], [\"data_uint8\", \"label\"], batch_size = batch_size,\ndb=db, db_type=db_type)\n# cast the data to float\ndata = model.Cast(data_uint8, \"data\", to=core.DataType.FLOAT)\n# scale data from [0,255] to [-1,1]\ndata = model.Scale(data, data, scale=float(1./256 * 2 - 1))\n# Caffe2 ist sehr generisch! Tats\u00e4chlich wird hier (fast) alles als\n# Operator behandelt. Dabei auch der ganz simple input! Die Backpropagation\n# welche die Werte der hidden Layers updated hat hier jedoch nichts verloren.\n# Deswegen muss caffe2, auf Grund seiner generischen Bauweise, noch einmal\n# extra gesagt werden, dass hier bitte keine updates erfolgen sollen.\n# Darum: StopGradient\ndata = model.StopGradient(data, data)\nreturn data\n\n###############################################################################\nDefine Generator\ndef GenModel(model, data):\n# Input Layer: 1x1x100\n# data is noise\nfc1 = brew.fc(model, data, \"fc1\", dim_in=100, dim_out=4096)\nreshape, oldshape = model.net.Reshape(\n[\"fc1\"],\n[\"reshape\", \"oldshape\"],\nshape = (batch_size, 1024, 2, 2))\nrelu0_g = brew.relu(model, reshape, \"relu0_g\")\n# Deconv Block 1: 256x2x2 \ndeconv1 = brew.conv_transpose(\nmodel,\nrelu0_g, \n\"deconv1\", \ndim_in=1024, \ndim_out=512, \nkernel=2, \nstride=2\n)\n\nbatchnorm1_g = brew.spatial_bn(\nmodel, \ndeconv1, \n\"batchnorm1_g\", \n512, \nepsilon=1e-5, \nmomentum=0.9,\nis_test=False        \n)    \n\nrelu1_g = brew.relu(model, batchnorm1_g, \"relu1_g\")\n\n# Deconv Block 2: 128x4x4\ndeconv2 = brew.conv_transpose(\nmodel,\nrelu1_g, \n\"deconv2\", \ndim_in=512, \ndim_out=256, \nkernel=4, \nstride=1\n)\n\nbatchnorm2_g = brew.spatial_bn(\nmodel, \ndeconv2, \n\"batchnorm2_g\", \n256, \nepsilon=1e-5, \nmomentum=0.9,\nis_test=False        \n)\n\nrelu2_g = brew.relu(model, batchnorm2_g, 'relu2_g')\n\n# Deconv Block 3: 64x7x7\ndeconv3 = brew.conv_transpose(\nmodel,\nrelu2_g, \n\"deconv3\", \ndim_in=256, \ndim_out=128, \nkernel=2, \nstride=2\n)\n\nbatchnorm3_g = brew.spatial_bn(\nmodel, \ndeconv3, \n\"batchnorm3_g\", \n128, \nepsilon=1e-5, \nmomentum=0.9,\nis_test=False        \n)    \n\nrelu3_g = brew.relu(model, batchnorm3_g, 'relu3_g')\n\n# Deconv Block 4: 32x14x14 -> 1x28x28\ndeconv4 = brew.conv_transpose(\nmodel,\nrelu3_g, \n\"deconv4\", \ndim_in=128, \ndim_out=1, \nkernel=2, \nstride=2\n)\n\nbatchnorm4_g = brew.spatial_bn(\nmodel, \ndeconv4, \n\"batchnorm4_g\", \n1, \nepsilon=1e-5, \nmomentum=0.9,\nis_test=False        \n)\n\ntanh = brew.tanh(model, batchnorm4_g, \"tanh\")\n    \nreturn tanh\n\n###############################################################################\nDefine Discriminator Real\ndef DisModel_real(model, data):\n# convblock 1 28x28 -> 14x14\nconv1_d = brew.conv(model, data, \"conv1_d\", dim_in=1, dim_out=128, kernel=2, stride=2, pad=0)\nbatchnorm1_d = brew.spatial_bn(\nmodel,\nconv1_d,\n\"batchnorm1_d\",\n128,\nepsilon=1e-5,\nmomentum=0.9,\nis_test=False\n)\nlrelu1_d = model.net.LeakyRelu(batchnorm1_d, \"lrelu1_d\", alpha=alpha)\n\n\n# convblock 2 14x14 -> 7x7\nconv2_d = brew.conv(model, lrelu1_d, \"conv2_d\", dim_in=128, dim_out=256, kernel=2, stride=2, pad=0)\nbatchnorm2_d = brew.spatial_bn(\n    model, \n    conv2_d, \n    \"batchnorm2_d\", \n    256, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu2_d = model.net.LeakyRelu(batchnorm2_d, \"lrelu2_d\", alpha=alpha)\n\n# convblock 3 7x7 -> 4x4\nconv3_d = brew.conv(model, lrelu2_d, \"conv3_d\", dim_in=256, dim_out=512, kernel=1, stride=2, pad=0)\nbatchnorm3_d = brew.spatial_bn(\n    model, \n    conv3_d, \n    \"batchnorm3_d\", \n    512, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu3_d = model.net.LeakyRelu(batchnorm3_d, \"lrelu3_d\", alpha=alpha)\n\n# convblock 4 4x4 -> 2x2\nconv4_d = brew.conv(model, lrelu3_d, \"conv4_d\", dim_in=512, dim_out=512, kernel=2, stride=2, pad=0)\nbatchnorm4_d = brew.spatial_bn(\n    model, \n    conv4_d, \n    \"batchnorm4_d\", \n    512, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu4_d = model.net.LeakyRelu(batchnorm4_d, \"lrelu4_d\", alpha=alpha)\n\n# Flatten 512x2x2 -> 2048x1x1    \nreshape_d, oldshape_d = model.net.Reshape(\n[\"lrelu4_d\"], \n[\"reshape_d\", \"oldshape_d\"], \nshape=(batch_size,2048,1,1))    \n\nsigmoid_real = model.net.Sigmoid(reshape_d, \"sigmoid_d_real\")\n\nreturn sigmoid_real\n\n###############################################################################\nDefine Discriminator Fake\ndef DisModel_fake(model, data):\n# convblock 1 28x28 -> 14x14\nconv1_d = brew.conv(model, data, \"conv1_d\", dim_in=1, dim_out=128, kernel=2, stride=2, pad=0)\nbatchnorm1_d = brew.spatial_bn(\nmodel,\nconv1_d,\n\"batchnorm1_d\",\n128,\nepsilon=1e-5,\nmomentum=0.9,\nis_test=False\n)\nlrelu1_d = model.net.LeakyRelu(batchnorm1_d, \"lrelu1_d\", alpha=alpha)\n\n\n# convblock 2 14x14 -> 7x7\nconv2_d = brew.conv(model, lrelu1_d, \"conv2_d\", dim_in=128, dim_out=256, kernel=2, stride=2, pad=0)\nbatchnorm2_d = brew.spatial_bn(\n    model, \n    conv2_d, \n    \"batchnorm2_d\", \n    256, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu2_d = model.net.LeakyRelu(batchnorm2_d, \"lrelu2_d\", alpha=alpha)\n\n# convblock 3 7x7 -> 4x4\nconv3_d = brew.conv(model, lrelu2_d, \"conv3_d\", dim_in=256, dim_out=512, kernel=1, stride=2, pad=0)\nbatchnorm3_d = brew.spatial_bn(\n    model, \n    conv3_d, \n    \"batchnorm3_d\", \n    512, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu3_d = model.net.LeakyRelu(batchnorm3_d, \"lrelu3_d\", alpha=alpha)\n\n# convblock 4 4x4 -> 2x2\nconv4_d = brew.conv(model, lrelu3_d, \"conv4_d\", dim_in=512, dim_out=512, kernel=2, stride=2, pad=0)\nbatchnorm4_d = brew.spatial_bn(\n    model, \n    conv4_d, \n    \"batchnorm4_d\", \n    512, \n    epsilon=1e-5, \n    momentum=0.9,\n    is_test=False\n    )\nlrelu4_d = model.net.LeakyRelu(batchnorm4_d, \"lrelu4_d\", alpha=alpha)\n\n# Flatten 512x2x2 -> 2048x1x1    \nreshape_d, oldshape_d = model.net.Reshape(\n[\"lrelu4_d\"], \n[\"reshape_d\", \"oldshape_d\"], \nshape=(batch_size,2048,1,1))\n\nsigmoid_fake = model.net.Sigmoid(reshape_d, \"sigmoid_d_fake\")\n\nreturn sigmoid_fake\n\n###############################################################################\nDefine Training\n###############################################################################\nTraining Operators for D\ndef AddTrainingOperators_D(model_r, model_f, sigmoid_r, sigmoid_f, lr = learning_rate):\nxent_fake = model_f.net.SigmoidCrossEntropyWithLogits([sigmoid_f, \"label_fake\"], 'xent_fake')\nd_loss_fake = model_f.net.AveragedLoss(xent_fake, \"d_loss_fake\")\nxent_real = model_r.net.SigmoidCrossEntropyWithLogits([sigmoid_r, \"label_real\"], 'xent_real')\nd_loss_real = model_r.net.AveragedLoss(xent_real, \"d_loss_real\")\nd_loss_total_r = model_r.net.Add([\"d_loss_real\", \"d_loss_fake\"], \"d_loss_total_r\")\nd_loss_total_f = model_f.net.Add([\"d_loss_real\", \"d_loss_fake\"], \"d_loss_total_f\")\nmodel_r.AddGradientOperators([d_loss_total_r])\nmodel_f.AddGradientOperators([d_loss_total_f])\noptimizer.build_adam(model_f, lr)\noptimizer.build_adam(model_r, lr)\n###############################################################################\nTraining Operators for G\ndef AddTrainingOperators_Gen(model, fake_sigmoid, learning_rate=learning_rate):\nxent = model.net.LabelCrossEntropy([fake_sigmoid, \"label_g\"], 'xent_g')\n# compute the expected loss with the help of xent.\nloss_g = model.net.AveragedLoss(xent, \"loss_g\")\n# use the average loss to to add gradient operators to the model\nmodel.AddGradientOperators([loss_g])\n# Use adam\noptimizer.build_adam(model, learning_rate)\n###############################################################################\nCreate Models\n###############################################################################\nCreate G\ngenerator = model_helper.ModelHelper(name=\"mnist_gen\")\nCreate D_real\ndiscriminator_real = model_helper.ModelHelper(\nname=\"mnist_dis_real\", arg_scope=arg_scope)\nCreate D_fake\ndiscriminator_fake = model_helper.ModelHelper(\nname=\"mnist_dis_fake\", arg_scope=arg_scope)\nApply net-definitions\nwith core.DeviceScope(device_option):\n# Get Data\ndata = AddInput(\ndiscriminator_real, batch_size=batch_size,\ndb=os.path.join(data_folder, 'mnist-test-nchw-lmdb'),\ndb_type='lmdb')\n# With Data from noise vector\ntanh_gen = GenModel(generator, \"noise\")\n# Only filled with data from MNIST.\ntrue_sigmoid = DisModel_real(discriminator_real, data)\n# Only filled with data from G.\nfake_sigmoid = DisModel_fake(discriminator_fake, tanh_gen)\n# Add Trainingsoperators\n# For G\nAddTrainingOperators_Gen(generator, fake_sigmoid, learning_rate)\n# For D\nAddTrainingOperators_D(discriminator_real, discriminator_fake, true_sigmoid, fake_sigmoid, learning_rate)\n###############################################################################\n#Initialize the network\n###############################################################################\nworkspace.RunNetOnce(discriminator_real.param_init_net)\nworkspace.CreateNet(discriminator_real.net)\nworkspace.RunNetOnce(generator.param_init_net)\nworkspace.CreateNet(generator.net)\nworkspace.RunNetOnce(discriminator_fake.param_init_net)\nworkspace.CreateNet(discriminator_fake.net)\nprint(workspace.Blobs())\n###############################################################################\nRun the training procedure\n###############################################################################\ntimes the training will be running\nepochs = 10\nsteps = int(600/ batch_size) # MNIST size / batch_size -> 1 epoch\nContainers for plotting progress\nloss_d = np.zeros(steps)\nloss_g = np.zeros(steps)\nimages = np.empty((batch_size,1,28,28), np.float32)\nprint(\"Total Number of Runs: {}\".format(epochs * steps))\nfor e in range (epochs):\n# Zum Debuggen G Output als print. Bug: G updatet nicht. Werte bleiben gleich.\ntanh_out = workspace.FetchBlob('tanh')\n#print(tanh_out[0][0][0])\nfor i in range(steps):\n# Train D\nworkspace.RunNet(discriminator_real.net)\nworkspace.RunNet(discriminator_fake.net)\n    # Noise Data Input for the Generator \n    noise = np.random.randn(batch_size, 100).astype(np.float32)\n    workspace.FeedBlob(\"noise\", noise)\n    \n    # Train G    \n    workspace.RunNet(generator.net)\n    \n    loss_d[i] = workspace.FetchBlob(\"d_loss_total_r\")\n    loss_g[i] = workspace.FetchBlob(\"loss_g\")\n\n    # Nach dem Debugging wieder dekommentieren. \n    \"\"\"if (i % 50) == 0:\n        print(\"Round: {}\".format(i * (e+1)))\n        print(\"LOSS D\")\n        print(workspace.FetchBlob(\"d_loss_total_r\"))\n        print(\"LOSS G\")\n        print(workspace.FetchBlob(\"loss_g\"))   \"\"\"     \n\n###############################################################################\nAfter the execution is done, let's plot the values.\nprint(\"Final D loss: {}\".format(workspace.FetchBlob(\"d_loss_total_r\")))\nprint(\"Final G loss: {}\".format(workspace.FetchBlob(\"loss_g\")))\npyplot.plot(loss_d, 'b')\npyplot.plot(loss_g, 'r')\npyplot.title(\"Summary of Training Run\")\npyplot.xlabel(\"Iteration\")\npyplot.legend(('Loss_d', 'Loss_g'), loc='upper right')\nPlot G Results\nUse visualize module to show the examples from the last batch that was fed to the model\ntanh_out = workspace.FetchBlob('tanh')\npyplot.figure()\npyplot.title(\"Last Batch from G\")\n_ = visualize.NCHW.ShowMultiple(tanh_out)\nNur f\u00fcr das Debugging auskommentiert. Sp\u00e4ter wieder einf\u00fcgen!\n#pyplot.show()\n###############################################################################\nAnd here are the networks blobs.\n[u'AdamOptimizer_1_lr_gpu0', u'AdamOptimizer_2_lr_gpu0', u'lrelu4_d_grad_dims', u'batchnorm1_d', u'batchnorm1_d_b', u'batchnorm1_d_b_first_moment', u'batchnorm1_d_b_grad', u'batchnorm1_d_b_second_moment', u'batchnorm1_d_grad', u'batchnorm1_d_riv', u'batchnorm1_d_rm', u'batchnorm1_d_s', u'batchnorm1_d_s_first_moment', u'batchnorm1_d_s_grad', u'batchnorm1_d_s_second_moment', u'batchnorm1_d_siv', u'batchnorm1_d_sm', u'batchnorm1_g', u'batchnorm1_g_b', u'batchnorm1_g_riv', u'batchnorm1_g_rm', u'batchnorm1_g_s', u'batchnorm1_g_siv', u'batchnorm1_g_sm', u'batchnorm2_d', u'batchnorm2_d_b', u'batchnorm2_d_b_first_moment', u'batchnorm2_d_b_grad', u'batchnorm2_d_b_second_moment', u'batchnorm2_d_grad', u'batchnorm2_d_riv', u'batchnorm2_d_rm', u'batchnorm2_d_s', u'batchnorm2_d_s_first_moment', u'batchnorm2_d_s_grad', u'batchnorm2_d_s_second_moment', u'batchnorm2_d_siv', u'batchnorm2_d_sm', u'batchnorm2_g', u'batchnorm2_g_b', u'batchnorm2_g_riv', u'batchnorm2_g_rm', u'batchnorm2_g_s', u'batchnorm2_g_siv', u'batchnorm2_g_sm', u'batchnorm3_d', u'batchnorm3_d_b', u'batchnorm3_d_b_first_moment', u'batchnorm3_d_b_grad', u'batchnorm3_d_b_second_moment', u'batchnorm3_d_grad', u'batchnorm3_d_riv', u'batchnorm3_d_rm', u'batchnorm3_d_s', u'batchnorm3_d_s_first_moment', u'batchnorm3_d_s_grad', u'batchnorm3_d_s_second_moment', u'batchnorm3_d_siv', u'batchnorm3_d_sm', u'batchnorm3_g', u'batchnorm3_g_b', u'batchnorm3_g_riv', u'batchnorm3_g_rm', u'batchnorm3_g_s', u'batchnorm3_g_siv', u'batchnorm3_g_sm', u'batchnorm4_d', u'batchnorm4_d_b', u'batchnorm4_d_b_first_moment', u'batchnorm4_d_b_grad', u'batchnorm4_d_b_second_moment', u'batchnorm4_d_grad', u'batchnorm4_d_riv', u'batchnorm4_d_rm', u'batchnorm4_d_s', u'batchnorm4_d_s_first_moment', u'batchnorm4_d_s_grad', u'batchnorm4_d_s_second_moment', u'batchnorm4_d_siv', u'batchnorm4_d_sm', u'batchnorm4_g', u'batchnorm4_g_b', u'batchnorm4_g_riv', u'batchnorm4_g_rm', u'batchnorm4_g_s', u'batchnorm4_g_siv', u'batchnorm4_g_sm', u'conv1_d', u'conv1_d_b', u'conv1_d_b_first_moment', u'conv1_d_b_grad', u'conv1_d_b_second_moment', u'conv1_d_grad', u'conv1_d_w', u'conv1_d_w_first_moment', u'conv1_d_w_grad', u'conv1_d_w_second_moment', u'conv2_d', u'conv2_d_b', u'conv2_d_b_first_moment', u'conv2_d_b_grad', u'conv2_d_b_second_moment', u'conv2_d_grad', u'conv2_d_w', u'conv2_d_w_first_moment', u'conv2_d_w_grad', u'conv2_d_w_second_moment', u'conv3_d', u'conv3_d_b', u'conv3_d_b_first_moment', u'conv3_d_b_grad', u'conv3_d_b_second_moment', u'conv3_d_grad', u'conv3_d_w', u'conv3_d_w_first_moment', u'conv3_d_w_grad', u'conv3_d_w_second_moment', u'conv4_d', u'conv4_d_b', u'conv4_d_b_first_moment', u'conv4_d_b_grad', u'conv4_d_b_second_moment', u'conv4_d_grad', u'conv4_d_w', u'conv4_d_w_first_moment', u'conv4_d_w_grad', u'conv4_d_w_second_moment', u'd_loss_fake', u'd_loss_fake_grad', u'd_loss_real', u'd_loss_real_grad', u'd_loss_total_f', u'd_loss_total_f_autogen_grad', u'd_loss_total_r', u'd_loss_total_r_autogen_grad', u'data', u'data_grad', u'data_uint8', u'dbreader/home/kanani/workspace/tutorial_data/mnist/mnist-test-nchw-lmdb', u'deconv1', u'deconv1_b', u'deconv1_w', u'deconv2', u'deconv2_b', u'deconv2_w', u'deconv3', u'deconv3_b', u'deconv3_w', u'deconv4', u'deconv4_b', u'deconv4_w', u'fc1', u'fc1_b', u'fc1_w', u'iteration_mutex', u'label', u'label_fake', u'label_g', u'label_real', u'loss_g', u'loss_g_autogen_grad', u'lrelu1_d', u'lrelu1_d_grad', u'lrelu2_d', u'lrelu2_d_grad', u'lrelu3_d', u'lrelu3_d_grad', u'lrelu4_d', u'lrelu4_d_grad', u'noise', u'oldshape', u'oldshape_d', u'optimizer_iteration', u'relu0_g', u'relu1_g', u'relu2_g', u'relu3_g', u'reshape', u'reshape_d', u'reshape_d_grad', u'sigmoid_d_fake', u'sigmoid_d_fake_grad', u'sigmoid_d_real', u'sigmoid_d_real_grad', u'tanh', u'tanh_grad', u'xent_fake', u'xent_fake_grad', u'xent_g', u'xent_g_grad', u'xent_real', u'xent_real_grad']\n\nThanks everyone! =)", "body": "Hi everyone,\r\nIn the process of my Bachelor Degree I am building a DCGAN in caffe2. After following all the tutorials I have figured out a architecture. But when I run my Code the Generator does not update at all. I made some debugging and found out, that the Generator's layers do not receive any Gradients at all.\r\nEven my tutor can't find the source of the issue.\r\n\r\nI am grateful for any help! =)\r\n\r\nHere is my Code:\r\n\r\n# -*- coding: utf-8 -*-\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import unicode_literals\r\n\r\nimport os\r\nimport shutil\r\n#import pickle as pkl\r\nimport numpy as np\r\n\r\nfrom matplotlib import pyplot#, image\r\nfrom caffe2.python import brew, core, workspace, model_helper, optimizer, visualize, memonger, utils\r\nfrom caffe2.proto import caffe2_pb2\r\n\r\n###############################################################################\r\n# Data Import\r\n###############################################################################\r\n\r\nworkspace.ResetWorkspace()\r\n\r\ndef DownloadResource(url, path):\r\n    '''Downloads resources from s3 by url and unzips them to the provided path'''\r\n    import requests, StringIO, zipfile\r\n    print(\"Downloading... {} to {}\".format(url, path))\r\n    r = requests.get(url, stream=True)\r\n    z = zipfile.ZipFile(StringIO.StringIO(r.content))\r\n    z.extractall(path)\r\n    print(\"Completed download and extraction.\")\r\n\r\ncurrent_folder = os.getcwd()\r\nprint(\"The current folder is {}\".format(current_folder) )\r\ndata_folder = os.path.join(current_folder, 'tutorial_data', 'mnist')\r\nroot_folder = os.path.join(current_folder, 'tutorial_files', 'tutorial_mnist')\r\ndb_missing = False\r\n\r\nif not os.path.exists(data_folder):\r\n    os.makedirs(data_folder)   \r\n    print(\"Your data folder was not found!! This was generated: {}\".format(data_folder))\r\n    \r\n# Look for existing database: lmdb\r\nif os.path.exists(os.path.join(data_folder,\"mnist-train-nchw-lmdb\")):\r\n    print(\"lmdb train db found!\")\r\nelse:\r\n    db_missing = True\r\n    \r\nif os.path.exists(os.path.join(data_folder,\"mnist-test-nchw-lmdb\")):\r\n    print(\"lmdb test db found!\")\r\nelse:\r\n    db_missing = True\r\n    \r\n# attempt the download of the db if either was missing\r\nif db_missing:\r\n    print(\"one or both of the MNIST lmbd dbs not found!!\")\r\n    db_url = \"http://download.caffe2.ai/databases/mnist-lmdb.zip\"\r\n    try:\r\n        DownloadResource(db_url, data_folder)\r\n    except Exception as ex:\r\n        print(\"Failed to download dataset. Please download it manually from {}\".format(db_url))\r\n        print(\"Unzip it and place the two database folders here: {}\".format(data_folder))\r\n        raise ex\r\n        \r\nif os.path.exists(root_folder):\r\n    print(\"Looks like you ran this before, so we need to cleanup those old files...\")\r\n    shutil.rmtree(root_folder)\r\n\r\nos.makedirs(root_folder)\r\nworkspace.ResetWorkspace(root_folder)\r\n\r\nprint(\"training data folder:\" + data_folder)\r\nprint(\"workspace root folder:\" + root_folder)\r\n\r\n###############################################################################\r\n# Memory Management\r\n###############################################################################\r\n\r\n# Kann zur Performanceoptimierung sp\u00e4ter eingesetzt werden. \r\n\r\ndef optimize_gradient_memory(model, loss):\r\n    model.net._net = memonger.share_grad_blobs(\r\n        model.net,\r\n        loss,\r\n        set(model.param_to_grad.values()),\r\n        # Due to memonger internals, we need a namescope here. Let's make one up; we'll need it later!\r\n        namescope=\"memonger_needs_a_namescope\",\r\n        share_activations=False)\r\n\r\n###############################################################################\r\n# Global Parameters\r\n###############################################################################\r\n\r\ndevice_option = caffe2_pb2.DeviceOption(device_type=caffe2_pb2.CUDA)\r\n# Gr\u00f6\u00dfere Batchsize oder learning_rate f\u00fchrt zu loss_d = nan\r\nbatch_size = 32\r\narg_scope = {\"order\": \"NCHW\"}\r\nlearning_rate = 0.00003\r\n# For Leaky ReLU\r\nalpha = 0.1\r\n\r\n# Create labels for D_Real. Only ones. Soft and noisy. \r\nlabel_real = np.random.rand(batch_size, 2048, 1, 1).astype('float32') * 0.1 + 0.9\r\n# Create labels for D_Fake. Only zeros. Soft and noisy\r\nlabel_fake = np.random.rand(batch_size, 2048, 1, 1).astype('float32') * 0.1\r\n# Create labels for G. Only ones.\r\nlabel_g = np.ones(batch_size, dtype=np.int32)\r\n\r\n# Dummy Blobs for evading deadlock between G and d_fake. \r\n# For G\r\ndummyblob_g = np.ones(batch_size)\r\ndummyblob_d_loss_fake = np.ones(1)\r\n# For D\r\ndummyblob_d = np.ones(batch_size*28*28).reshape(batch_size, 1, 28, 28)\r\n\r\n# Noise Data Input for the Generator \r\nnoise = np.random.randn(batch_size, 100).astype(np.float32)\r\n\r\n# Insert all relevant data to workspace\r\nwith core.DeviceScope(device_option):\r\n    workspace.FeedBlob(\"tanh\", dummyblob_d.astype(np.float32))  \r\n    workspace.FeedBlob(\"sigmoid_d_real\", dummyblob_d.astype(np.float32))\r\n    workspace.FeedBlob(\"sigmoid_d_fake\", dummyblob_g.astype(np.float32))\r\n    workspace.FeedBlob(\"d_loss_fake\", dummyblob_d_loss_fake.astype(np.float32))\r\n    workspace.FeedBlob(\"label_g\", label_g)\r\n    workspace.FeedBlob(\"label_fake\", label_fake)\r\n    workspace.FeedBlob(\"label_real\", label_real)\r\n    workspace.FeedBlob(\"noise\", noise)\r\n\r\n###############################################################################\r\n# Define Models\r\n###############################################################################\r\n\r\n# Manage Input for D_real\r\n\r\ndef AddInput(model, batch_size, db, db_type):\r\n    # load the data\r\n    data_uint8, label = model.TensorProtosDBInput(\r\n        [], [\"data_uint8\", \"label\"], batch_size = batch_size,\r\n        db=db, db_type=db_type)\r\n    # cast the data to float\r\n    data = model.Cast(data_uint8, \"data\", to=core.DataType.FLOAT)\r\n    # scale data from [0,255] to [-1,1]\r\n    data = model.Scale(data, data, scale=float(1./256 * 2 - 1))\r\n    # Caffe2 ist sehr generisch! Tats\u00e4chlich wird hier (fast) alles als\r\n    # Operator behandelt. Dabei auch der ganz simple input! Die Backpropagation\r\n    # welche die Werte der hidden Layers updated hat hier jedoch nichts verloren.\r\n    # Deswegen muss caffe2, auf Grund seiner generischen Bauweise, noch einmal\r\n    # extra gesagt werden, dass hier bitte keine updates erfolgen sollen. \r\n    # Darum: StopGradient\r\n    data = model.StopGradient(data, data)          \r\n    \r\n    return data\r\n    \r\n###############################################################################\r\n\r\n# Define Generator \r\n \r\ndef GenModel(model, data):\r\n    # Input Layer: 1x1x100\r\n    # data is noise\r\n    fc1 = brew.fc(model, data, \"fc1\", dim_in=100, dim_out=4096)\r\n    reshape, oldshape = model.net.Reshape(\r\n    [\"fc1\"],\r\n    [\"reshape\", \"oldshape\"], \r\n    shape = (batch_size, 1024, 2, 2))\r\n    relu0_g = brew.relu(model, reshape, \"relu0_g\")\r\n    \r\n    # Deconv Block 1: 256x2x2 \r\n    deconv1 = brew.conv_transpose(\r\n    model,\r\n    relu0_g, \r\n    \"deconv1\", \r\n    dim_in=1024, \r\n    dim_out=512, \r\n    kernel=2, \r\n    stride=2\r\n    )\r\n\r\n    batchnorm1_g = brew.spatial_bn(\r\n    model, \r\n    deconv1, \r\n    \"batchnorm1_g\", \r\n    512, \r\n    epsilon=1e-5, \r\n    momentum=0.9,\r\n    is_test=False        \r\n    )    \r\n    \r\n    relu1_g = brew.relu(model, batchnorm1_g, \"relu1_g\")\r\n    \r\n    # Deconv Block 2: 128x4x4\r\n    deconv2 = brew.conv_transpose(\r\n    model,\r\n    relu1_g, \r\n    \"deconv2\", \r\n    dim_in=512, \r\n    dim_out=256, \r\n    kernel=4, \r\n    stride=1\r\n    )\r\n\r\n    batchnorm2_g = brew.spatial_bn(\r\n    model, \r\n    deconv2, \r\n    \"batchnorm2_g\", \r\n    256, \r\n    epsilon=1e-5, \r\n    momentum=0.9,\r\n    is_test=False        \r\n    )\r\n    \r\n    relu2_g = brew.relu(model, batchnorm2_g, 'relu2_g')\r\n\r\n    # Deconv Block 3: 64x7x7\r\n    deconv3 = brew.conv_transpose(\r\n    model,\r\n    relu2_g, \r\n    \"deconv3\", \r\n    dim_in=256, \r\n    dim_out=128, \r\n    kernel=2, \r\n    stride=2\r\n    )\r\n    \r\n    batchnorm3_g = brew.spatial_bn(\r\n    model, \r\n    deconv3, \r\n    \"batchnorm3_g\", \r\n    128, \r\n    epsilon=1e-5, \r\n    momentum=0.9,\r\n    is_test=False        \r\n    )    \r\n    \r\n    relu3_g = brew.relu(model, batchnorm3_g, 'relu3_g')\r\n    \r\n    # Deconv Block 4: 32x14x14 -> 1x28x28\r\n    deconv4 = brew.conv_transpose(\r\n    model,\r\n    relu3_g, \r\n    \"deconv4\", \r\n    dim_in=128, \r\n    dim_out=1, \r\n    kernel=2, \r\n    stride=2\r\n    )\r\n    \r\n    batchnorm4_g = brew.spatial_bn(\r\n    model, \r\n    deconv4, \r\n    \"batchnorm4_g\", \r\n    1, \r\n    epsilon=1e-5, \r\n    momentum=0.9,\r\n    is_test=False        \r\n    )\r\n    \r\n    tanh = brew.tanh(model, batchnorm4_g, \"tanh\")\r\n        \r\n    return tanh\r\n\r\n###############################################################################\r\n\r\n# Define Discriminator Real\r\n\r\ndef DisModel_real(model, data):\r\n    # convblock 1 28x28 -> 14x14\r\n    conv1_d = brew.conv(model, data, \"conv1_d\", dim_in=1, dim_out=128, kernel=2, stride=2, pad=0)\r\n    batchnorm1_d = brew.spatial_bn(\r\n        model, \r\n        conv1_d, \r\n        \"batchnorm1_d\", \r\n        128, \r\n        epsilon=1e-5, \r\n        momentum=0.9,\r\n        is_test=False        \r\n        )\r\n\r\n    lrelu1_d = model.net.LeakyRelu(batchnorm1_d, \"lrelu1_d\", alpha=alpha)\r\n    \r\n    \r\n    # convblock 2 14x14 -> 7x7\r\n    conv2_d = brew.conv(model, lrelu1_d, \"conv2_d\", dim_in=128, dim_out=256, kernel=2, stride=2, pad=0)\r\n    batchnorm2_d = brew.spatial_bn(\r\n        model, \r\n        conv2_d, \r\n        \"batchnorm2_d\", \r\n        256, \r\n        epsilon=1e-5, \r\n        momentum=0.9,\r\n        is_test=False\r\n        )\r\n    lrelu2_d = model.net.LeakyRelu(batchnorm2_d, \"lrelu2_d\", alpha=alpha)\r\n    \r\n    # convblock 3 7x7 -> 4x4\r\n    conv3_d = brew.conv(model, lrelu2_d, \"conv3_d\", dim_in=256, dim_out=512, kernel=1, stride=2, pad=0)\r\n    batchnorm3_d = brew.spatial_bn(\r\n        model, \r\n        conv3_d, \r\n        \"batchnorm3_d\", \r\n        512, \r\n        epsilon=1e-5, \r\n        momentum=0.9,\r\n        is_test=False\r\n        )\r\n    lrelu3_d = model.net.LeakyRelu(batchnorm3_d, \"lrelu3_d\", alpha=alpha)\r\n    \r\n    # convblock 4 4x4 -> 2x2\r\n    conv4_d = brew.conv(model, lrelu3_d, \"conv4_d\", dim_in=512, dim_out=512, kernel=2, stride=2, pad=0)\r\n    batchnorm4_d = brew.spatial_bn(\r\n        model, \r\n        conv4_d, \r\n        \"batchnorm4_d\", \r\n        512, \r\n        epsilon=1e-5, \r\n        momentum=0.9,\r\n        is_test=False\r\n        )\r\n    lrelu4_d = model.net.LeakyRelu(batchnorm4_d, \"lrelu4_d\", alpha=alpha)\r\n    \r\n    # Flatten 512x2x2 -> 2048x1x1    \r\n    reshape_d, oldshape_d = model.net.Reshape(\r\n    [\"lrelu4_d\"], \r\n    [\"reshape_d\", \"oldshape_d\"], \r\n    shape=(batch_size,2048,1,1))    \r\n    \r\n    sigmoid_real = model.net.Sigmoid(reshape_d, \"sigmoid_d_real\")\r\n\r\n    return sigmoid_real\r\n###############################################################################\r\n\r\n# Define Discriminator Fake\r\n\r\ndef DisModel_fake(model, data):\r\n    # convblock 1 28x28 -> 14x14\r\n    conv1_d = brew.conv(model, data, \"conv1_d\", dim_in=1, dim_out=128, kernel=2, stride=2, pad=0)\r\n    batchnorm1_d = brew.spatial_bn(\r\n        model, \r\n        conv1_d, \r\n        \"batchnorm1_d\", \r\n        128, \r\n        epsilon=1e-5, \r\n        momentum=0.9,\r\n        is_test=False        \r\n        )\r\n\r\n    lrelu1_d = model.net.LeakyRelu(batchnorm1_d, \"lrelu1_d\", alpha=alpha)\r\n    \r\n    \r\n    # convblock 2 14x14 -> 7x7\r\n    conv2_d = brew.conv(model, lrelu1_d, \"conv2_d\", dim_in=128, dim_out=256, kernel=2, stride=2, pad=0)\r\n    batchnorm2_d = brew.spatial_bn(\r\n        model, \r\n        conv2_d, \r\n        \"batchnorm2_d\", \r\n        256, \r\n        epsilon=1e-5, \r\n        momentum=0.9,\r\n        is_test=False\r\n        )\r\n    lrelu2_d = model.net.LeakyRelu(batchnorm2_d, \"lrelu2_d\", alpha=alpha)\r\n    \r\n    # convblock 3 7x7 -> 4x4\r\n    conv3_d = brew.conv(model, lrelu2_d, \"conv3_d\", dim_in=256, dim_out=512, kernel=1, stride=2, pad=0)\r\n    batchnorm3_d = brew.spatial_bn(\r\n        model, \r\n        conv3_d, \r\n        \"batchnorm3_d\", \r\n        512, \r\n        epsilon=1e-5, \r\n        momentum=0.9,\r\n        is_test=False\r\n        )\r\n    lrelu3_d = model.net.LeakyRelu(batchnorm3_d, \"lrelu3_d\", alpha=alpha)\r\n    \r\n    # convblock 4 4x4 -> 2x2\r\n    conv4_d = brew.conv(model, lrelu3_d, \"conv4_d\", dim_in=512, dim_out=512, kernel=2, stride=2, pad=0)\r\n    batchnorm4_d = brew.spatial_bn(\r\n        model, \r\n        conv4_d, \r\n        \"batchnorm4_d\", \r\n        512, \r\n        epsilon=1e-5, \r\n        momentum=0.9,\r\n        is_test=False\r\n        )\r\n    lrelu4_d = model.net.LeakyRelu(batchnorm4_d, \"lrelu4_d\", alpha=alpha)\r\n    \r\n    # Flatten 512x2x2 -> 2048x1x1    \r\n    reshape_d, oldshape_d = model.net.Reshape(\r\n    [\"lrelu4_d\"], \r\n    [\"reshape_d\", \"oldshape_d\"], \r\n    shape=(batch_size,2048,1,1))\r\n    \r\n    sigmoid_fake = model.net.Sigmoid(reshape_d, \"sigmoid_d_fake\")\r\n\r\n    return sigmoid_fake\r\n    \r\n###############################################################################\r\n# Define Training\r\n###############################################################################\r\n\r\n# Training Operators for D\r\n\r\ndef AddTrainingOperators_D(model_r, model_f, sigmoid_r, sigmoid_f, lr = learning_rate):\r\n    xent_fake = model_f.net.SigmoidCrossEntropyWithLogits([sigmoid_f, \"label_fake\"], 'xent_fake')\r\n    d_loss_fake = model_f.net.AveragedLoss(xent_fake, \"d_loss_fake\")    \r\n    xent_real = model_r.net.SigmoidCrossEntropyWithLogits([sigmoid_r, \"label_real\"], 'xent_real')\r\n    d_loss_real = model_r.net.AveragedLoss(xent_real, \"d_loss_real\")\r\n    d_loss_total_r = model_r.net.Add([\"d_loss_real\", \"d_loss_fake\"], \"d_loss_total_r\")\r\n    d_loss_total_f = model_f.net.Add([\"d_loss_real\", \"d_loss_fake\"], \"d_loss_total_f\")\r\n    model_r.AddGradientOperators([d_loss_total_r])\r\n    model_f.AddGradientOperators([d_loss_total_f])\r\n    optimizer.build_adam(model_f, lr)\r\n    optimizer.build_adam(model_r, lr)\r\n    \r\n\r\n###############################################################################\r\n\r\n# Training Operators for G\r\n        \r\ndef AddTrainingOperators_Gen(model, fake_sigmoid, learning_rate=learning_rate):\r\n    xent = model.net.LabelCrossEntropy([fake_sigmoid, \"label_g\"], 'xent_g')\r\n    # compute the expected loss with the help of xent.\r\n    loss_g = model.net.AveragedLoss(xent, \"loss_g\")\r\n    # use the average loss to to add gradient operators to the model\r\n    model.AddGradientOperators([loss_g])\r\n    # Use adam\r\n    optimizer.build_adam(model, learning_rate)\r\n        \r\n###############################################################################        \r\n# Create Models\r\n###############################################################################\r\n\r\n# Create G\r\ngenerator = model_helper.ModelHelper(name=\"mnist_gen\")\r\n\r\n# Create D_real\r\ndiscriminator_real = model_helper.ModelHelper(\r\n    name=\"mnist_dis_real\", arg_scope=arg_scope)\r\n\r\n# Create D_fake\r\ndiscriminator_fake = model_helper.ModelHelper(\r\n    name=\"mnist_dis_fake\", arg_scope=arg_scope)\r\n\r\n# Apply net-definitions\r\nwith core.DeviceScope(device_option):\r\n    # Get Data\r\n    data = AddInput(\r\n        discriminator_real, batch_size=batch_size,\r\n        db=os.path.join(data_folder, 'mnist-test-nchw-lmdb'),\r\n        db_type='lmdb')\r\n    # With Data from noise vector\r\n    tanh_gen = GenModel(generator, \"noise\")\r\n    # Only filled with data from MNIST.\r\n    true_sigmoid = DisModel_real(discriminator_real, data)\r\n    # Only filled with data from G. \r\n    fake_sigmoid = DisModel_fake(discriminator_fake, tanh_gen)\r\n    # Add Trainingsoperators\r\n    # For G\r\n    AddTrainingOperators_Gen(generator, fake_sigmoid, learning_rate)\r\n    # For D\r\n    AddTrainingOperators_D(discriminator_real, discriminator_fake, true_sigmoid, fake_sigmoid, learning_rate)\r\n\r\n###############################################################################\r\n#Initialize the network\r\n###############################################################################\r\n\r\nworkspace.RunNetOnce(discriminator_real.param_init_net)\r\nworkspace.CreateNet(discriminator_real.net)\r\nworkspace.RunNetOnce(generator.param_init_net)\r\nworkspace.CreateNet(generator.net)\r\nworkspace.RunNetOnce(discriminator_fake.param_init_net)\r\nworkspace.CreateNet(discriminator_fake.net)\r\n\r\nprint(workspace.Blobs())\r\n\r\n###############################################################################\r\n# Run the training procedure\r\n###############################################################################\r\n\r\n# times the training will be running\r\nepochs = 10\r\nsteps = int(600/ batch_size) # MNIST size / batch_size -> 1 epoch\r\n\r\n# Containers for plotting progress\r\nloss_d = np.zeros(steps)\r\nloss_g = np.zeros(steps)\r\nimages = np.empty((batch_size,1,28,28), np.float32)\r\n\r\nprint(\"Total Number of Runs: {}\".format(epochs * steps))\r\n\r\nfor e in range (epochs):\r\n    # Zum Debuggen G Output als print. Bug: G updatet nicht. Werte bleiben gleich.\r\n    tanh_out = workspace.FetchBlob('tanh')\r\n    #print(tanh_out[0][0][0])\r\n    for i in range(steps):\r\n        # Train D\r\n        workspace.RunNet(discriminator_real.net)\r\n        workspace.RunNet(discriminator_fake.net)\r\n        \r\n        # Noise Data Input for the Generator \r\n        noise = np.random.randn(batch_size, 100).astype(np.float32)\r\n        workspace.FeedBlob(\"noise\", noise)\r\n        \r\n        # Train G    \r\n        workspace.RunNet(generator.net)\r\n        \r\n        loss_d[i] = workspace.FetchBlob(\"d_loss_total_r\")\r\n        loss_g[i] = workspace.FetchBlob(\"loss_g\")\r\n\r\n        # Nach dem Debugging wieder dekommentieren. \r\n        \"\"\"if (i % 50) == 0:\r\n            print(\"Round: {}\".format(i * (e+1)))\r\n            print(\"LOSS D\")\r\n            print(workspace.FetchBlob(\"d_loss_total_r\"))\r\n            print(\"LOSS G\")\r\n            print(workspace.FetchBlob(\"loss_g\"))   \"\"\"     \r\n    \r\n###############################################################################\r\n\r\n# After the execution is done, let's plot the values.\r\n\r\nprint(\"Final D loss: {}\".format(workspace.FetchBlob(\"d_loss_total_r\")))\r\nprint(\"Final G loss: {}\".format(workspace.FetchBlob(\"loss_g\")))\r\n\r\npyplot.plot(loss_d, 'b')\r\npyplot.plot(loss_g, 'r')\r\npyplot.title(\"Summary of Training Run\")\r\npyplot.xlabel(\"Iteration\")\r\npyplot.legend(('Loss_d', 'Loss_g'), loc='upper right')\r\n\r\n# Plot G Results\r\n# Use visualize module to show the examples from the last batch that was fed to the model\r\ntanh_out = workspace.FetchBlob('tanh')\r\npyplot.figure()\r\npyplot.title(\"Last Batch from G\")\r\n_ = visualize.NCHW.ShowMultiple(tanh_out)\r\n\r\n# Nur f\u00fcr das Debugging auskommentiert. Sp\u00e4ter wieder einf\u00fcgen!\r\n#pyplot.show()\r\n############################################################################### \r\n\r\nAnd here are the networks blobs.\r\n\r\n    [u'AdamOptimizer_1_lr_gpu0', u'AdamOptimizer_2_lr_gpu0', u'lrelu4_d_grad_dims', u'batchnorm1_d', u'batchnorm1_d_b', u'batchnorm1_d_b_first_moment', u'batchnorm1_d_b_grad', u'batchnorm1_d_b_second_moment', u'batchnorm1_d_grad', u'batchnorm1_d_riv', u'batchnorm1_d_rm', u'batchnorm1_d_s', u'batchnorm1_d_s_first_moment', u'batchnorm1_d_s_grad', u'batchnorm1_d_s_second_moment', u'batchnorm1_d_siv', u'batchnorm1_d_sm', u'batchnorm1_g', u'batchnorm1_g_b', u'batchnorm1_g_riv', u'batchnorm1_g_rm', u'batchnorm1_g_s', u'batchnorm1_g_siv', u'batchnorm1_g_sm', u'batchnorm2_d', u'batchnorm2_d_b', u'batchnorm2_d_b_first_moment', u'batchnorm2_d_b_grad', u'batchnorm2_d_b_second_moment', u'batchnorm2_d_grad', u'batchnorm2_d_riv', u'batchnorm2_d_rm', u'batchnorm2_d_s', u'batchnorm2_d_s_first_moment', u'batchnorm2_d_s_grad', u'batchnorm2_d_s_second_moment', u'batchnorm2_d_siv', u'batchnorm2_d_sm', u'batchnorm2_g', u'batchnorm2_g_b', u'batchnorm2_g_riv', u'batchnorm2_g_rm', u'batchnorm2_g_s', u'batchnorm2_g_siv', u'batchnorm2_g_sm', u'batchnorm3_d', u'batchnorm3_d_b', u'batchnorm3_d_b_first_moment', u'batchnorm3_d_b_grad', u'batchnorm3_d_b_second_moment', u'batchnorm3_d_grad', u'batchnorm3_d_riv', u'batchnorm3_d_rm', u'batchnorm3_d_s', u'batchnorm3_d_s_first_moment', u'batchnorm3_d_s_grad', u'batchnorm3_d_s_second_moment', u'batchnorm3_d_siv', u'batchnorm3_d_sm', u'batchnorm3_g', u'batchnorm3_g_b', u'batchnorm3_g_riv', u'batchnorm3_g_rm', u'batchnorm3_g_s', u'batchnorm3_g_siv', u'batchnorm3_g_sm', u'batchnorm4_d', u'batchnorm4_d_b', u'batchnorm4_d_b_first_moment', u'batchnorm4_d_b_grad', u'batchnorm4_d_b_second_moment', u'batchnorm4_d_grad', u'batchnorm4_d_riv', u'batchnorm4_d_rm', u'batchnorm4_d_s', u'batchnorm4_d_s_first_moment', u'batchnorm4_d_s_grad', u'batchnorm4_d_s_second_moment', u'batchnorm4_d_siv', u'batchnorm4_d_sm', u'batchnorm4_g', u'batchnorm4_g_b', u'batchnorm4_g_riv', u'batchnorm4_g_rm', u'batchnorm4_g_s', u'batchnorm4_g_siv', u'batchnorm4_g_sm', u'conv1_d', u'conv1_d_b', u'conv1_d_b_first_moment', u'conv1_d_b_grad', u'conv1_d_b_second_moment', u'conv1_d_grad', u'conv1_d_w', u'conv1_d_w_first_moment', u'conv1_d_w_grad', u'conv1_d_w_second_moment', u'conv2_d', u'conv2_d_b', u'conv2_d_b_first_moment', u'conv2_d_b_grad', u'conv2_d_b_second_moment', u'conv2_d_grad', u'conv2_d_w', u'conv2_d_w_first_moment', u'conv2_d_w_grad', u'conv2_d_w_second_moment', u'conv3_d', u'conv3_d_b', u'conv3_d_b_first_moment', u'conv3_d_b_grad', u'conv3_d_b_second_moment', u'conv3_d_grad', u'conv3_d_w', u'conv3_d_w_first_moment', u'conv3_d_w_grad', u'conv3_d_w_second_moment', u'conv4_d', u'conv4_d_b', u'conv4_d_b_first_moment', u'conv4_d_b_grad', u'conv4_d_b_second_moment', u'conv4_d_grad', u'conv4_d_w', u'conv4_d_w_first_moment', u'conv4_d_w_grad', u'conv4_d_w_second_moment', u'd_loss_fake', u'd_loss_fake_grad', u'd_loss_real', u'd_loss_real_grad', u'd_loss_total_f', u'd_loss_total_f_autogen_grad', u'd_loss_total_r', u'd_loss_total_r_autogen_grad', u'data', u'data_grad', u'data_uint8', u'dbreader/home/kanani/workspace/tutorial_data/mnist/mnist-test-nchw-lmdb', u'deconv1', u'deconv1_b', u'deconv1_w', u'deconv2', u'deconv2_b', u'deconv2_w', u'deconv3', u'deconv3_b', u'deconv3_w', u'deconv4', u'deconv4_b', u'deconv4_w', u'fc1', u'fc1_b', u'fc1_w', u'iteration_mutex', u'label', u'label_fake', u'label_g', u'label_real', u'loss_g', u'loss_g_autogen_grad', u'lrelu1_d', u'lrelu1_d_grad', u'lrelu2_d', u'lrelu2_d_grad', u'lrelu3_d', u'lrelu3_d_grad', u'lrelu4_d', u'lrelu4_d_grad', u'noise', u'oldshape', u'oldshape_d', u'optimizer_iteration', u'relu0_g', u'relu1_g', u'relu2_g', u'relu3_g', u'reshape', u'reshape_d', u'reshape_d_grad', u'sigmoid_d_fake', u'sigmoid_d_fake_grad', u'sigmoid_d_real', u'sigmoid_d_real_grad', u'tanh', u'tanh_grad', u'xent_fake', u'xent_fake_grad', u'xent_g', u'xent_g_grad', u'xent_real', u'xent_real_grad']\r\n\r\nThanks everyone! =)\r\n"}