{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1737", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1737/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1737/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1737/events", "html_url": "https://github.com/pytorch/pytorch/issues/1737", "id": 233913605, "node_id": "MDU6SXNzdWUyMzM5MTM2MDU=", "number": 1737, "title": "RNN CUDNN backend OOM issue", "user": {"login": "imisra", "id": 3059768, "node_id": "MDQ6VXNlcjMwNTk3Njg=", "avatar_url": "https://avatars0.githubusercontent.com/u/3059768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/imisra", "html_url": "https://github.com/imisra", "followers_url": "https://api.github.com/users/imisra/followers", "following_url": "https://api.github.com/users/imisra/following{/other_user}", "gists_url": "https://api.github.com/users/imisra/gists{/gist_id}", "starred_url": "https://api.github.com/users/imisra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/imisra/subscriptions", "organizations_url": "https://api.github.com/users/imisra/orgs", "repos_url": "https://api.github.com/users/imisra/repos", "events_url": "https://api.github.com/users/imisra/events{/privacy}", "received_events_url": "https://api.github.com/users/imisra/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-06-06T14:36:25Z", "updated_at": "2017-06-06T16:34:19Z", "closed_at": "2017-06-06T15:05:44Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I think I have stumbled upon something weird with the CUDNN backend for RNN. I am using CUDNN v5 on Cent OS 7.3.1.</p>\n<pre><code>torch.version.__version__ = e1d257bc6d472ee297df1719bf344bae359dbeaa\n</code></pre>\n<p>I have discussed this with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1310570\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/soumith\">@soumith</a> as well.<br>\nThe code snippet for reproducing is below. Enabling the cudnn backend increases the memory used linearly (goes OOM eventually). Disabling the backend results in expected behavior.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> unicode_literals\n\n<span class=\"pl-k\">import</span> torch\ntorch.backends.cudnn.enabled <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n<span class=\"pl-k\">import</span> torch.cuda\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">import</span> gc\n\n<span class=\"pl-c1\">print</span>(torch.version.<span class=\"pl-c1\">__version__</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_num_tensors</span>():\n    ctr <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n    <span class=\"pl-k\">for</span> obj <span class=\"pl-k\">in</span> gc.get_objects():\n        <span class=\"pl-k\">if</span> torch.is_tensor(obj):\n            ctr <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-k\">return</span> ctr\n\n\nwordvec_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">300</span>\nhidden_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">256</span>\nrnn_num_layers <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nvocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nrnn_dropout <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.5</span>\n\nmodel <span class=\"pl-k\">=</span> nn.LSTM(wordvec_dim, hidden_dim, rnn_num_layers,\n                           <span class=\"pl-v\">dropout</span><span class=\"pl-k\">=</span>rnn_dropout, <span class=\"pl-v\">batch_first</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> set training mode</span>\nmodel.cuda()\nmodel.train()\n\nencoded <span class=\"pl-k\">=</span> Variable(torch.FloatTensor(batch_size, <span class=\"pl-c1\">1</span>, wordvec_dim))\nencoded <span class=\"pl-k\">=</span> encoded.cuda()\n\nh0 <span class=\"pl-k\">=</span> Variable(torch.zeros(rnn_num_layers, batch_size, hidden_dim))\nc0 <span class=\"pl-k\">=</span> Variable(torch.zeros(rnn_num_layers, batch_size, hidden_dim))\nh <span class=\"pl-k\">=</span> h0.cuda()\nc <span class=\"pl-k\">=</span> c0.cuda()\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Start:<span class=\"pl-pds\">'</span></span>, get_num_tensors())\nnum_forward_passes <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\n<span class=\"pl-k\">for</span> _i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_forward_passes):\n    output, (h, c) <span class=\"pl-k\">=</span> model(encoded, (h, c))\n    <span class=\"pl-c1\">print</span>(_i, get_num_tensors())\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>End:<span class=\"pl-pds\">'</span></span>, get_num_tensors())</pre></div>\n<p>Output <em>with</em> cudnn enabled</p>\n<pre><code>e1d257bc6d472ee297df1719bf344bae359dbeaa\nStart: 9\n0 16\n1 22\n2 28\n3 34\n4 40\n5 46\n6 52\n7 58\n8 64\n9 70\nEnd: 70\n</code></pre>\n<p>Output without cudnn</p>\n<pre><code>e1d257bc6d472ee297df1719bf344bae359dbeaa\nStart: 9\n0 10\n1 10\n2 10\n3 10\n4 10\n5 10\n6 10\n7 10\n8 10\n9 10\nEnd: 10\n</code></pre>", "body_text": "Hi,\nI think I have stumbled upon something weird with the CUDNN backend for RNN. I am using CUDNN v5 on Cent OS 7.3.1.\ntorch.version.__version__ = e1d257bc6d472ee297df1719bf344bae359dbeaa\n\nI have discussed this with @soumith as well.\nThe code snippet for reproducing is below. Enabling the cudnn backend increases the memory used linearly (goes OOM eventually). Disabling the backend results in expected behavior.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport torch\ntorch.backends.cudnn.enabled = False\nimport torch.cuda\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport gc\n\nprint(torch.version.__version__)\n\n\ndef get_num_tensors():\n    ctr = 0\n    for obj in gc.get_objects():\n        if torch.is_tensor(obj):\n            ctr += 1\n    return ctr\n\n\nwordvec_dim = 300\nhidden_dim = 256\nrnn_num_layers = 1\nbatch_size = 10\nvocab_size = 100\nrnn_dropout = 0.5\n\nmodel = nn.LSTM(wordvec_dim, hidden_dim, rnn_num_layers,\n                           dropout=rnn_dropout, batch_first=True)\n# set training mode\nmodel.cuda()\nmodel.train()\n\nencoded = Variable(torch.FloatTensor(batch_size, 1, wordvec_dim))\nencoded = encoded.cuda()\n\nh0 = Variable(torch.zeros(rnn_num_layers, batch_size, hidden_dim))\nc0 = Variable(torch.zeros(rnn_num_layers, batch_size, hidden_dim))\nh = h0.cuda()\nc = c0.cuda()\n\nprint('Start:', get_num_tensors())\nnum_forward_passes = 10\n\nfor _i in range(num_forward_passes):\n    output, (h, c) = model(encoded, (h, c))\n    print(_i, get_num_tensors())\n\nprint('End:', get_num_tensors())\nOutput with cudnn enabled\ne1d257bc6d472ee297df1719bf344bae359dbeaa\nStart: 9\n0 16\n1 22\n2 28\n3 34\n4 40\n5 46\n6 52\n7 58\n8 64\n9 70\nEnd: 70\n\nOutput without cudnn\ne1d257bc6d472ee297df1719bf344bae359dbeaa\nStart: 9\n0 10\n1 10\n2 10\n3 10\n4 10\n5 10\n6 10\n7 10\n8 10\n9 10\nEnd: 10", "body": "Hi,\r\n\r\nI think I have stumbled upon something weird with the CUDNN backend for RNN. I am using CUDNN v5 on Cent OS 7.3.1. \r\n```\r\ntorch.version.__version__ = e1d257bc6d472ee297df1719bf344bae359dbeaa\r\n```\r\nI have discussed this with @soumith as well.\r\nThe code snippet for reproducing is below. Enabling the cudnn backend increases the memory used linearly (goes OOM eventually). Disabling the backend results in expected behavior.\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom __future__ import unicode_literals\r\n\r\nimport torch\r\ntorch.backends.cudnn.enabled = False\r\nimport torch.cuda\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport gc\r\n\r\nprint(torch.version.__version__)\r\n\r\n\r\ndef get_num_tensors():\r\n    ctr = 0\r\n    for obj in gc.get_objects():\r\n        if torch.is_tensor(obj):\r\n            ctr += 1\r\n    return ctr\r\n\r\n\r\nwordvec_dim = 300\r\nhidden_dim = 256\r\nrnn_num_layers = 1\r\nbatch_size = 10\r\nvocab_size = 100\r\nrnn_dropout = 0.5\r\n\r\nmodel = nn.LSTM(wordvec_dim, hidden_dim, rnn_num_layers,\r\n                           dropout=rnn_dropout, batch_first=True)\r\n# set training mode\r\nmodel.cuda()\r\nmodel.train()\r\n\r\nencoded = Variable(torch.FloatTensor(batch_size, 1, wordvec_dim))\r\nencoded = encoded.cuda()\r\n\r\nh0 = Variable(torch.zeros(rnn_num_layers, batch_size, hidden_dim))\r\nc0 = Variable(torch.zeros(rnn_num_layers, batch_size, hidden_dim))\r\nh = h0.cuda()\r\nc = c0.cuda()\r\n\r\nprint('Start:', get_num_tensors())\r\nnum_forward_passes = 10\r\n\r\nfor _i in range(num_forward_passes):\r\n    output, (h, c) = model(encoded, (h, c))\r\n    print(_i, get_num_tensors())\r\n\r\nprint('End:', get_num_tensors())\r\n```\r\n\r\nOutput *with* cudnn enabled\r\n```\r\ne1d257bc6d472ee297df1719bf344bae359dbeaa\r\nStart: 9\r\n0 16\r\n1 22\r\n2 28\r\n3 34\r\n4 40\r\n5 46\r\n6 52\r\n7 58\r\n8 64\r\n9 70\r\nEnd: 70\r\n```\r\n\r\nOutput without cudnn\r\n```\r\ne1d257bc6d472ee297df1719bf344bae359dbeaa\r\nStart: 9\r\n0 10\r\n1 10\r\n2 10\r\n3 10\r\n4 10\r\n5 10\r\n6 10\r\n7 10\r\n8 10\r\n9 10\r\nEnd: 10\r\n```"}