{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/371892478", "html_url": "https://github.com/pytorch/pytorch/pull/5647#issuecomment-371892478", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5647", "id": 371892478, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTg5MjQ3OA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-09T17:56:16Z", "updated_at": "2018-03-09T17:56:39Z", "author_association": "MEMBER", "body_html": "<p>Thanks for the review! It's just something that was blocking my JIT work and I wanted to play around with ATen codegen anyway, so I came up with this. I won't mind closing this PR in favor of a better solution if it's going to be implemented soon. Alternatively, I can take up the task too if you can explain the plan to me (we can have a VC). Nevertheless, here are some thoughts and questions I have:</p>\n<p><strong>Removing <code>at::Scalar</code>?</strong></p>\n<p>How exactly would relocatable tensors behave? While I agree that having <code>at::Scalar</code>s undergo some changes would be good, I don't think that removing them altogether is a good idea. It doesn't make sense to allow them to hold tensors, but the tagged float/integral union is useful when writing C++ code, because you don't need to worry about overflows. If you make all scalars taken by your interface <code>double</code>s you end up in Lua land, and I remember there were some issues with that.</p>\n<p><strong>Why do we need to dispatch scalars differently in so many places?</strong></p>\n<p>So here's a bit of a rationale behind this PR (which basically adds the <code>x.dim() == 0</code> dispatch to more places). In general we want only scalars to be polymorphic, so we need to handle them specially. However, doing so in the <code>s_</code> methods (as we do today) is pointless, because they are only reached through the broadacsting methods. This means that by the time they get there they are <em>never</em> scalars anymore, so we'll skip the check anyway and proceed to type checks that will fail.</p>\n<p>Ok, so we established that if we see a scalar in <code>Tensor::add</code> (tensor overload), then we want to dispatch to a different method than the regular <code>s_</code> one (for which we <em>need</em> to broadcast, or we break the <code>s_</code> invariant). So far so good, this is simple to implement.</p>\n<p>But then, you come across autograd, which treats the <code>Scalar</code> overloads in a way that prohibits differentiation wrt. scalars. This is why the whole \"reverse <code>zero_dim_dispatch</code>\" business is needed - you need to fix up the scalar type of the scalar tensor, and use the <code>Tensor</code> overload to correctly record the history.</p>", "body_text": "Thanks for the review! It's just something that was blocking my JIT work and I wanted to play around with ATen codegen anyway, so I came up with this. I won't mind closing this PR in favor of a better solution if it's going to be implemented soon. Alternatively, I can take up the task too if you can explain the plan to me (we can have a VC). Nevertheless, here are some thoughts and questions I have:\nRemoving at::Scalar?\nHow exactly would relocatable tensors behave? While I agree that having at::Scalars undergo some changes would be good, I don't think that removing them altogether is a good idea. It doesn't make sense to allow them to hold tensors, but the tagged float/integral union is useful when writing C++ code, because you don't need to worry about overflows. If you make all scalars taken by your interface doubles you end up in Lua land, and I remember there were some issues with that.\nWhy do we need to dispatch scalars differently in so many places?\nSo here's a bit of a rationale behind this PR (which basically adds the x.dim() == 0 dispatch to more places). In general we want only scalars to be polymorphic, so we need to handle them specially. However, doing so in the s_ methods (as we do today) is pointless, because they are only reached through the broadacsting methods. This means that by the time they get there they are never scalars anymore, so we'll skip the check anyway and proceed to type checks that will fail.\nOk, so we established that if we see a scalar in Tensor::add (tensor overload), then we want to dispatch to a different method than the regular s_ one (for which we need to broadcast, or we break the s_ invariant). So far so good, this is simple to implement.\nBut then, you come across autograd, which treats the Scalar overloads in a way that prohibits differentiation wrt. scalars. This is why the whole \"reverse zero_dim_dispatch\" business is needed - you need to fix up the scalar type of the scalar tensor, and use the Tensor overload to correctly record the history.", "body": "Thanks for the review! It's just something that was blocking my JIT work and I wanted to play around with ATen codegen anyway, so I came up with this. I won't mind closing this PR in favor of a better solution if it's going to be implemented soon. Alternatively, I can take up the task too if you can explain the plan to me (we can have a VC). Nevertheless, here are some thoughts and questions I have:\r\n\r\n**Removing `at::Scalar`?**\r\n\r\nHow exactly would relocatable tensors behave? While I agree that having `at::Scalar`s undergo some changes would be good, I don't think that removing them altogether is a good idea. It doesn't make sense to allow them to hold tensors, but the tagged float/integral union is useful when writing C++ code, because you don't need to worry about overflows. If you make all scalars taken by your interface `double`s you end up in Lua land, and I remember there were some issues with that.\r\n\r\n**Why do we need to dispatch scalars differently in so many places?**\r\n\r\nSo here's a bit of a rationale behind this PR (which basically adds the `x.dim() == 0` dispatch to more places). In general we want only scalars to be polymorphic, so we need to handle them specially. However, doing so in the `s_` methods (as we do today) is pointless, because they are only reached through the broadacsting methods. This means that by the time they get there they are *never* scalars anymore, so we'll skip the check anyway and proceed to type checks that will fail.\r\n\r\nOk, so we established that if we see a scalar in `Tensor::add` (tensor overload), then we want to dispatch to a different method than the regular `s_` one (for which we *need* to broadcast, or we break the `s_` invariant). So far so good, this is simple to implement.\r\n\r\nBut then, you come across autograd, which treats the `Scalar` overloads in a way that prohibits differentiation wrt. scalars. This is why the whole \"reverse `zero_dim_dispatch`\" business is needed - you need to fix up the scalar type of the scalar tensor, and use the `Tensor` overload to correctly record the history."}