{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/371973952", "html_url": "https://github.com/pytorch/pytorch/pull/5647#issuecomment-371973952", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/5647", "id": 371973952, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTk3Mzk1Mg==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-09T23:24:55Z", "updated_at": "2018-03-09T23:24:55Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>won't mind closing this PR in favor of a better solution if it's going to be implemented soon. Alternatively, I can take up the task too if you can explain the plan to me (we can have a VC).</p>\n</blockquote>\n<p>It's not on my short term roadmap.</p>\n<blockquote>\n<p>How exactly would relocatable tensors behave? While I agree that having at::Scalars undergo some changes would be good, I don't think that removing them altogether is a good idea. It doesn't make sense to allow them to hold tensors, but the tagged float/integral union is useful when writing C++ code, because you don't need to worry about overflows. If you make all scalars taken by your interface doubles you end up in Lua land, and I remember there were some issues with that.</p>\n</blockquote>\n<p>Why can't they just construct a tensor of the proper type (i.e. move the scalar constructors to tensors, and have them create a CPU tensor of the appropriate type)?</p>\n<blockquote>\n<p>In general we want only scalars to be polymorphic, so we need to handle them specially.</p>\n</blockquote>\n<p>I don't think we necessarily only want scalars to be polymorphic (I'm not saying we necessarily do either), it's more like the TH/THC interfaces are written in a way that can be polymorphic and they tend to be faster than the tensor equivalents as well.</p>\n<blockquote>\n<p>However, doing so in the s_ methods (as we do today) is pointless, because they are only reached through the broadacsting methods. This means that by the time they get there they are never scalars anymore, so we'll skip the check anyway and proceed to type checks that will fail.</p>\n</blockquote>\n<p>Well they can be scalars if everything is a scalar, but sure the overall point stands.</p>\n<blockquote>\n<p>Ok, so we established that if we see a scalar in Tensor::add (tensor overload), then we want to dispatch to a different method than the regular s_ one (for which we need to broadcast, or we break the s_ invariant). So far so good, this is simple to implement.</p>\n</blockquote>\n<blockquote>\n<p>But then, you come across autograd, which treats the Scalar overloads in a way that prohibits differentiation wrt. scalars. This is why the whole \"reverse zero_dim_dispatch\" business is needed - you need to fix up the scalar type of the scalar tensor, and use the Tensor overload to correctly record the history.</p>\n</blockquote>\n<p>What about this (just some brainstorming <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=655866\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/colesbury\">@colesbury</a> and I did): instead of doing this zero-dim dispatch, you do \"if tensor has one unexpanded element\", then you grab the data item and turn it into a scalar.  This seems to work (I changed the generated code to do it), except that you don't record the implicit type conversion so your grad_input for the scalar can have the wrong type.</p>\n<p>But that just sort of gets us back to the original expand problem; we originally wrote the each autograd functions to take that into account, which was a burden, so we recorded.  We could do another generation step with type conversions like we do with the <code>s_</code> functions, but that seems a bit silly here; the rules for type conversion are simpler than for expand, because we know the type of the grad_input needs to be the same as the input and we can just record that automatically and do the conversion implicitly.</p>\n<p>Note that this problem already could exist with native functions; there's nothing stopping someone from writing a mixed-type function there, and they'd have to be careful to get the type right in the derivative specification, which is non-obvious.  And then type-conversion functions like <code>cuda</code>, <code>type</code>, etc. don't need an explicit definition in derivatives.yaml, they just work implicitly.</p>", "body_text": "won't mind closing this PR in favor of a better solution if it's going to be implemented soon. Alternatively, I can take up the task too if you can explain the plan to me (we can have a VC).\n\nIt's not on my short term roadmap.\n\nHow exactly would relocatable tensors behave? While I agree that having at::Scalars undergo some changes would be good, I don't think that removing them altogether is a good idea. It doesn't make sense to allow them to hold tensors, but the tagged float/integral union is useful when writing C++ code, because you don't need to worry about overflows. If you make all scalars taken by your interface doubles you end up in Lua land, and I remember there were some issues with that.\n\nWhy can't they just construct a tensor of the proper type (i.e. move the scalar constructors to tensors, and have them create a CPU tensor of the appropriate type)?\n\nIn general we want only scalars to be polymorphic, so we need to handle them specially.\n\nI don't think we necessarily only want scalars to be polymorphic (I'm not saying we necessarily do either), it's more like the TH/THC interfaces are written in a way that can be polymorphic and they tend to be faster than the tensor equivalents as well.\n\nHowever, doing so in the s_ methods (as we do today) is pointless, because they are only reached through the broadacsting methods. This means that by the time they get there they are never scalars anymore, so we'll skip the check anyway and proceed to type checks that will fail.\n\nWell they can be scalars if everything is a scalar, but sure the overall point stands.\n\nOk, so we established that if we see a scalar in Tensor::add (tensor overload), then we want to dispatch to a different method than the regular s_ one (for which we need to broadcast, or we break the s_ invariant). So far so good, this is simple to implement.\n\n\nBut then, you come across autograd, which treats the Scalar overloads in a way that prohibits differentiation wrt. scalars. This is why the whole \"reverse zero_dim_dispatch\" business is needed - you need to fix up the scalar type of the scalar tensor, and use the Tensor overload to correctly record the history.\n\nWhat about this (just some brainstorming @colesbury and I did): instead of doing this zero-dim dispatch, you do \"if tensor has one unexpanded element\", then you grab the data item and turn it into a scalar.  This seems to work (I changed the generated code to do it), except that you don't record the implicit type conversion so your grad_input for the scalar can have the wrong type.\nBut that just sort of gets us back to the original expand problem; we originally wrote the each autograd functions to take that into account, which was a burden, so we recorded.  We could do another generation step with type conversions like we do with the s_ functions, but that seems a bit silly here; the rules for type conversion are simpler than for expand, because we know the type of the grad_input needs to be the same as the input and we can just record that automatically and do the conversion implicitly.\nNote that this problem already could exist with native functions; there's nothing stopping someone from writing a mixed-type function there, and they'd have to be careful to get the type right in the derivative specification, which is non-obvious.  And then type-conversion functions like cuda, type, etc. don't need an explicit definition in derivatives.yaml, they just work implicitly.", "body": "> won't mind closing this PR in favor of a better solution if it's going to be implemented soon. Alternatively, I can take up the task too if you can explain the plan to me (we can have a VC).\r\n\r\nIt's not on my short term roadmap.\r\n\r\n> How exactly would relocatable tensors behave? While I agree that having at::Scalars undergo some changes would be good, I don't think that removing them altogether is a good idea. It doesn't make sense to allow them to hold tensors, but the tagged float/integral union is useful when writing C++ code, because you don't need to worry about overflows. If you make all scalars taken by your interface doubles you end up in Lua land, and I remember there were some issues with that.\r\n\r\nWhy can't they just construct a tensor of the proper type (i.e. move the scalar constructors to tensors, and have them create a CPU tensor of the appropriate type)?\r\n\r\n> In general we want only scalars to be polymorphic, so we need to handle them specially.\r\n\r\nI don't think we necessarily only want scalars to be polymorphic (I'm not saying we necessarily do either), it's more like the TH/THC interfaces are written in a way that can be polymorphic and they tend to be faster than the tensor equivalents as well.\r\n\r\n> However, doing so in the s_ methods (as we do today) is pointless, because they are only reached through the broadacsting methods. This means that by the time they get there they are never scalars anymore, so we'll skip the check anyway and proceed to type checks that will fail.\r\n\r\nWell they can be scalars if everything is a scalar, but sure the overall point stands.\r\n\r\n> Ok, so we established that if we see a scalar in Tensor::add (tensor overload), then we want to dispatch to a different method than the regular s_ one (for which we need to broadcast, or we break the s_ invariant). So far so good, this is simple to implement.\r\n\r\n> But then, you come across autograd, which treats the Scalar overloads in a way that prohibits differentiation wrt. scalars. This is why the whole \"reverse zero_dim_dispatch\" business is needed - you need to fix up the scalar type of the scalar tensor, and use the Tensor overload to correctly record the history.\r\n\r\nWhat about this (just some brainstorming @colesbury and I did): instead of doing this zero-dim dispatch, you do \"if tensor has one unexpanded element\", then you grab the data item and turn it into a scalar.  This seems to work (I changed the generated code to do it), except that you don't record the implicit type conversion so your grad_input for the scalar can have the wrong type.\r\n\r\nBut that just sort of gets us back to the original expand problem; we originally wrote the each autograd functions to take that into account, which was a burden, so we recorded.  We could do another generation step with type conversions like we do with the `s_` functions, but that seems a bit silly here; the rules for type conversion are simpler than for expand, because we know the type of the grad_input needs to be the same as the input and we can just record that automatically and do the conversion implicitly.\r\n\r\nNote that this problem already could exist with native functions; there's nothing stopping someone from writing a mixed-type function there, and they'd have to be careful to get the type right in the derivative specification, which is non-obvious.  And then type-conversion functions like `cuda`, `type`, etc. don't need an explicit definition in derivatives.yaml, they just work implicitly."}