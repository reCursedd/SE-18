{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7504", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7504/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7504/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7504/events", "html_url": "https://github.com/pytorch/pytorch/issues/7504", "id": 322386096, "node_id": "MDU6SXNzdWUzMjIzODYwOTY=", "number": 7504, "title": "[jit][script] literals don't interact well with gpu tensors", "user": {"login": "zdevito", "id": 370202, "node_id": "MDQ6VXNlcjM3MDIwMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/370202?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zdevito", "html_url": "https://github.com/zdevito", "followers_url": "https://api.github.com/users/zdevito/followers", "following_url": "https://api.github.com/users/zdevito/following{/other_user}", "gists_url": "https://api.github.com/users/zdevito/gists{/gist_id}", "starred_url": "https://api.github.com/users/zdevito/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zdevito/subscriptions", "organizations_url": "https://api.github.com/users/zdevito/orgs", "repos_url": "https://api.github.com/users/zdevito/repos", "events_url": "https://api.github.com/users/zdevito/events{/privacy}", "received_events_url": "https://api.github.com/users/zdevito/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-11T18:08:07Z", "updated_at": "2018-09-19T19:06:27Z", "closed_at": "2018-09-19T19:06:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Literals are always allocated on the CPU, meaning that simple expressions break when the input tensors are on the GPU, e.g.:</p>\n<pre><code>@torch.jit.script\ndef foo(a):\n    return 1 + a # fails for GPU tensor\n</code></pre>\n<p>If we added types for non-tensors numbers then we would be able to patch the graph to ensure things would work, e.g:</p>\n<pre><code>one = to_backend_as(1, a)\nreturn one + a\n</code></pre>\n<p>Alternatively, we can scalar tensors like 1 have this behavior universally in the operator library.</p>\n<p>Steps to resolve this issue:</p>\n<ol>\n<li>Introduce <code>float</code> and <code>int</code> types into type.h, and change the frontend to assign these types to constant literals like <code>1</code>.</li>\n<li>Modify emitSimpleExpr to allow float/int to work on normal arithmetic operators like +, and ensure the returned value has the float/int type as well.</li>\n<li>Modify <code>emitSimpleExpr</code> and <code>tryMatchSchema</code> in compiler.cpp to add an implicit conversion from int/float into a tensor type (see \"// check input types\" for where this occurs).  Given an <code>int</code> or <code>float</code> type, this implicit conversion should insert code that converts the float/int value <code>v</code> into a tensor on the correct backend for the op by finding the first Tensor argument to the op <code>t</code> and then inserting a call to move <code>v</code> to the same device as <code>t</code>.</li>\n</ol>", "body_text": "Literals are always allocated on the CPU, meaning that simple expressions break when the input tensors are on the GPU, e.g.:\n@torch.jit.script\ndef foo(a):\n    return 1 + a # fails for GPU tensor\n\nIf we added types for non-tensors numbers then we would be able to patch the graph to ensure things would work, e.g:\none = to_backend_as(1, a)\nreturn one + a\n\nAlternatively, we can scalar tensors like 1 have this behavior universally in the operator library.\nSteps to resolve this issue:\n\nIntroduce float and int types into type.h, and change the frontend to assign these types to constant literals like 1.\nModify emitSimpleExpr to allow float/int to work on normal arithmetic operators like +, and ensure the returned value has the float/int type as well.\nModify emitSimpleExpr and tryMatchSchema in compiler.cpp to add an implicit conversion from int/float into a tensor type (see \"// check input types\" for where this occurs).  Given an int or float type, this implicit conversion should insert code that converts the float/int value v into a tensor on the correct backend for the op by finding the first Tensor argument to the op t and then inserting a call to move v to the same device as t.", "body": "Literals are always allocated on the CPU, meaning that simple expressions break when the input tensors are on the GPU, e.g.:\r\n\r\n```\r\n@torch.jit.script\r\ndef foo(a):\r\n    return 1 + a # fails for GPU tensor\r\n```\r\nIf we added types for non-tensors numbers then we would be able to patch the graph to ensure things would work, e.g:\r\n\r\n```\r\none = to_backend_as(1, a)\r\nreturn one + a\r\n```\r\n\r\nAlternatively, we can scalar tensors like 1 have this behavior universally in the operator library.\r\n\r\nSteps to resolve this issue:\r\n1. Introduce `float` and `int` types into type.h, and change the frontend to assign these types to constant literals like `1`.\r\n2. Modify emitSimpleExpr to allow float/int to work on normal arithmetic operators like +, and ensure the returned value has the float/int type as well.\r\n2. Modify `emitSimpleExpr` and `tryMatchSchema` in compiler.cpp to add an implicit conversion from int/float into a tensor type (see \"// check input types\" for where this occurs).  Given an `int` or `float` type, this implicit conversion should insert code that converts the float/int value `v` into a tensor on the correct backend for the op by finding the first Tensor argument to the op `t` and then inserting a call to move `v` to the same device as `t`. "}