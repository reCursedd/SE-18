{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/308310267", "html_url": "https://github.com/pytorch/pytorch/issues/1787#issuecomment-308310267", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1787", "id": 308310267, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODMxMDI2Nw==", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-14T03:33:37Z", "updated_at": "2017-06-14T03:33:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Actually, it's probably better to deal with the expansion directly in the autograd Functions; if we do the expansion in variable.py, the tensor functions will never actually do the expansion and we'd need to take some care to deal with error messages, pointwise fallback, etc. correctly.  If we save the sizes of the input tensors and we know the operation (in forward), it should be possible to do the equivalent of Expand.backward in the Functions themselves.</p>", "body_text": "Actually, it's probably better to deal with the expansion directly in the autograd Functions; if we do the expansion in variable.py, the tensor functions will never actually do the expansion and we'd need to take some care to deal with error messages, pointwise fallback, etc. correctly.  If we save the sizes of the input tensors and we know the operation (in forward), it should be possible to do the equivalent of Expand.backward in the Functions themselves.", "body": "Actually, it's probably better to deal with the expansion directly in the autograd Functions; if we do the expansion in variable.py, the tensor functions will never actually do the expansion and we'd need to take some care to deal with error messages, pointwise fallback, etc. correctly.  If we save the sizes of the input tensors and we know the operation (in forward), it should be possible to do the equivalent of Expand.backward in the Functions themselves."}