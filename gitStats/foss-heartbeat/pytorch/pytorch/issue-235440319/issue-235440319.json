{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1787", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1787/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1787/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1787/events", "html_url": "https://github.com/pytorch/pytorch/issues/1787", "id": 235440319, "node_id": "MDU6SXNzdWUyMzU0NDAzMTk=", "number": 1787, "title": "autograd doesn't follow broadcasting", "user": {"login": "szagoruyko", "id": 4953728, "node_id": "MDQ6VXNlcjQ5NTM3Mjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/4953728?v=4", "gravatar_id": "", "url": "https://api.github.com/users/szagoruyko", "html_url": "https://github.com/szagoruyko", "followers_url": "https://api.github.com/users/szagoruyko/followers", "following_url": "https://api.github.com/users/szagoruyko/following{/other_user}", "gists_url": "https://api.github.com/users/szagoruyko/gists{/gist_id}", "starred_url": "https://api.github.com/users/szagoruyko/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/szagoruyko/subscriptions", "organizations_url": "https://api.github.com/users/szagoruyko/orgs", "repos_url": "https://api.github.com/users/szagoruyko/repos", "events_url": "https://api.github.com/users/szagoruyko/events{/privacy}", "received_events_url": "https://api.github.com/users/szagoruyko/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-06-13T05:32:16Z", "updated_at": "2017-06-19T16:05:01Z", "closed_at": "2017-06-19T16:05:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Gradient shape is not adjusted to follow broadcasting and autograd fails. Backwards of these two fail, for example:</p>\n<ul>\n<li><code>torch.randn(1) * torch.randn(5,4)</code></li>\n<li><code>torch.randn(4,1) * torch.randn(1,4)</code></li>\n</ul>\n<p>grad/variable shape mismatch:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> no broadcasting</span>\n\na <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nb <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">4</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n(a.expand_as(b) <span class=\"pl-k\">*</span> b).sum().backward()\n\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>a:<span class=\"pl-pds\">'</span></span>, a.size(), a.grad.size()\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>b:<span class=\"pl-pds\">'</span></span>, b.size(), b.grad.size()\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> broadcasting failure case</span>\n\na <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">1</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nb <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">5</span>,<span class=\"pl-c1\">4</span>), <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n(a <span class=\"pl-k\">*</span> b).sum().backward()\n\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>a:<span class=\"pl-pds\">'</span></span>, a.size(), a.grad.size() <span class=\"pl-c\"><span class=\"pl-c\">#</span> &lt;-- grad size is different with tensor here</span>\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>b:<span class=\"pl-pds\">'</span></span>, b.size(), b.grad.size()</pre></div>", "body_text": "Gradient shape is not adjusted to follow broadcasting and autograd fails. Backwards of these two fail, for example:\n\ntorch.randn(1) * torch.randn(5,4)\ntorch.randn(4,1) * torch.randn(1,4)\n\ngrad/variable shape mismatch:\nimport torch\nfrom torch.autograd import Variable\n\n# no broadcasting\n\na = Variable(torch.randn(1), requires_grad=True)\nb = Variable(torch.randn(5,4), requires_grad=True)\n\n(a.expand_as(b) * b).sum().backward()\n\nprint 'a:', a.size(), a.grad.size()\nprint 'b:', b.size(), b.grad.size()\n\n\n# broadcasting failure case\n\na = Variable(torch.randn(1), requires_grad=True)\nb = Variable(torch.randn(5,4), requires_grad=True)\n\n(a * b).sum().backward()\n\nprint 'a:', a.size(), a.grad.size() # <-- grad size is different with tensor here\nprint 'b:', b.size(), b.grad.size()", "body": "Gradient shape is not adjusted to follow broadcasting and autograd fails. Backwards of these two fail, for example:\r\n\r\n* ```torch.randn(1) * torch.randn(5,4)```\r\n* ```torch.randn(4,1) * torch.randn(1,4)```\r\n\r\ngrad/variable shape mismatch:\r\n\r\n```python\r\nimport torch\r\nfrom torch.autograd import Variable\r\n\r\n# no broadcasting\r\n\r\na = Variable(torch.randn(1), requires_grad=True)\r\nb = Variable(torch.randn(5,4), requires_grad=True)\r\n\r\n(a.expand_as(b) * b).sum().backward()\r\n\r\nprint 'a:', a.size(), a.grad.size()\r\nprint 'b:', b.size(), b.grad.size()\r\n\r\n\r\n# broadcasting failure case\r\n\r\na = Variable(torch.randn(1), requires_grad=True)\r\nb = Variable(torch.randn(5,4), requires_grad=True)\r\n\r\n(a * b).sum().backward()\r\n\r\nprint 'a:', a.size(), a.grad.size() # <-- grad size is different with tensor here\r\nprint 'b:', b.size(), b.grad.size()\r\n```"}