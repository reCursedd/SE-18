{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/340932286", "html_url": "https://github.com/pytorch/pytorch/issues/3354#issuecomment-340932286", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3354", "id": 340932286, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDkzMjI4Ng==", "user": {"login": "gokceneraslan", "id": 1140359, "node_id": "MDQ6VXNlcjExNDAzNTk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1140359?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gokceneraslan", "html_url": "https://github.com/gokceneraslan", "followers_url": "https://api.github.com/users/gokceneraslan/followers", "following_url": "https://api.github.com/users/gokceneraslan/following{/other_user}", "gists_url": "https://api.github.com/users/gokceneraslan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gokceneraslan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gokceneraslan/subscriptions", "organizations_url": "https://api.github.com/users/gokceneraslan/orgs", "repos_url": "https://api.github.com/users/gokceneraslan/repos", "events_url": "https://api.github.com/users/gokceneraslan/events{/privacy}", "received_events_url": "https://api.github.com/users/gokceneraslan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-31T23:01:20Z", "updated_at": "2017-10-31T23:01:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I've simplified the script a bit:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> pandas <span class=\"pl-k\">as</span> pd\n<span class=\"pl-k\">import</span> plotnine <span class=\"pl-k\">as</span> p9\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n<span class=\"pl-k\">from</span> torch.utils.data <span class=\"pl-k\">import</span> TensorDataset, DataLoader, Dataset\n\nnp.random.seed(<span class=\"pl-c1\">555</span>)\ntorch.manual_seed(<span class=\"pl-c1\">555</span>)\n\nnum_sample <span class=\"pl-k\">=</span> <span class=\"pl-c1\">300</span>\nnum_feat <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\nnum_out <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\n\nX <span class=\"pl-k\">=</span> np.random.normal(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0.5</span>, (num_sample, num_feat)).astype(np.float32)\nW <span class=\"pl-k\">=</span> np.random.normal(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0.5</span>, (num_feat, num_out)).astype(np.float32)\nb <span class=\"pl-k\">=</span> np.random.normal(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0.5</span>, (<span class=\"pl-c1\">1</span>, num_out)).astype(np.float32)\nY <span class=\"pl-k\">=</span> np.exp(np.dot(X, W) <span class=\"pl-k\">+</span> b)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>### training with shuffle</span>\n\nds <span class=\"pl-k\">=</span> TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(Y).float())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> train torch models with and without shuffling</span>\nmodel1 <span class=\"pl-k\">=</span> torch.nn.Linear(num_feat, num_out)\nopt1 <span class=\"pl-k\">=</span> torch.optim.RMSprop(model1.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\nloss1 <span class=\"pl-k\">=</span> torch.nn.MSELoss()\n\ntrain_hist_shuf <span class=\"pl-k\">=</span> []\n\n<span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">300</span>):\n\n    train_batch_losses <span class=\"pl-k\">=</span> []\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> shuffle dataset</span>\n    idx <span class=\"pl-k\">=</span> torch.randperm(<span class=\"pl-c1\">len</span>(ds))\n    ds <span class=\"pl-k\">=</span> TensorDataset(<span class=\"pl-k\">*</span>ds[idx])\n\n    <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">int</span>(np.ceil(<span class=\"pl-c1\">len</span>(ds)<span class=\"pl-k\">/</span>batch_size))):\n        x, y <span class=\"pl-k\">=</span> ds[batch<span class=\"pl-k\">*</span>batch_size:(batch<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">*</span>batch_size]\n\n        x_var, y_var <span class=\"pl-k\">=</span> Variable(x), Variable(y)\n        pred <span class=\"pl-k\">=</span> model1(x_var)\n        l <span class=\"pl-k\">=</span> loss1(pred, y_var)\n        train_batch_losses.append(l.data[<span class=\"pl-c1\">0</span>])\n\n        opt1.zero_grad()\n        l.backward()\n        opt1.step()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> save mean of all batch errors within the epoch</span>\n    train_hist_shuf.append(np.array(train_batch_losses).mean())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>### now no shuffling</span>\n\nds <span class=\"pl-k\">=</span> TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(Y).float())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> train torch models with and without shuffling</span>\nmodel2 <span class=\"pl-k\">=</span> torch.nn.Linear(num_feat, num_out)\nopt2 <span class=\"pl-k\">=</span> torch.optim.RMSprop(model2.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>)\nloss2 <span class=\"pl-k\">=</span> torch.nn.MSELoss()\n\ntrain_hist_noshuf <span class=\"pl-k\">=</span> []\n\n<span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">300</span>):\n    train_batch_losses <span class=\"pl-k\">=</span> []\n\n    <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">int</span>(np.ceil(<span class=\"pl-c1\">len</span>(ds)<span class=\"pl-k\">/</span>batch_size))):\n        x, y <span class=\"pl-k\">=</span> ds[batch<span class=\"pl-k\">*</span>batch_size:(batch<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">*</span>batch_size]\n\n        x_var, y_var <span class=\"pl-k\">=</span> Variable(x), Variable(y)\n        pred <span class=\"pl-k\">=</span> model2(x_var)\n        l <span class=\"pl-k\">=</span> loss2(pred, y_var)\n        train_batch_losses.append(l.data[<span class=\"pl-c1\">0</span>])\n\n        opt2.zero_grad()\n        l.backward()\n        opt2.step()\n\n    train_hist_noshuf.append(np.array(train_batch_losses).mean())\n\n(p9.ggplot(pd.DataFrame({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch_train_shuf<span class=\"pl-pds\">'</span></span>: train_hist_shuf,\n                         <span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch_train_noshuf<span class=\"pl-pds\">'</span></span>: train_hist_noshuf,\n                         <span class=\"pl-s\"><span class=\"pl-pds\">'</span>epochs<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(train_hist_shuf))}),\n            p9.aes(<span class=\"pl-v\">x</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>epochs<span class=\"pl-pds\">'</span></span>)) <span class=\"pl-k\">+</span>\n  p9.geom_path(p9.aes(<span class=\"pl-v\">y</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch_train_shuf<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">color</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>\"loss (shuffled)\"<span class=\"pl-pds\">'</span></span>)) <span class=\"pl-k\">+</span>\n  p9.geom_path(p9.aes(<span class=\"pl-v\">y</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>torch_train_noshuf<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">color</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>\"loss (not shuffled)\"<span class=\"pl-pds\">'</span></span>)) <span class=\"pl-k\">+</span>\n  p9.labs(<span class=\"pl-v\">color</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loss<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">+</span>\n  p9.theme_minimal()).save(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shuffle.png<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">width</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">6</span>,\n                           <span class=\"pl-v\">height</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">dpi</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>)\n</pre></div>\n<p>Here is the resulting figure:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1140359/32252805-8f0f6bac-be97-11e7-8f93-7666aa7ecaad.png\"><img src=\"https://user-images.githubusercontent.com/1140359/32252805-8f0f6bac-be97-11e7-8f93-7666aa7ecaad.png\" alt=\"shuffle\" style=\"max-width:100%;\"></a></p>", "body_text": "I've simplified the script a bit:\nimport numpy as np\nimport pandas as pd\nimport plotnine as p9\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\n\nnp.random.seed(555)\ntorch.manual_seed(555)\n\nnum_sample = 300\nnum_feat = 10\nnum_out = 10\n\nbatch_size = 32\n\nX = np.random.normal(0, 0.5, (num_sample, num_feat)).astype(np.float32)\nW = np.random.normal(0, 0.5, (num_feat, num_out)).astype(np.float32)\nb = np.random.normal(0, 0.5, (1, num_out)).astype(np.float32)\nY = np.exp(np.dot(X, W) + b)\n\n#### training with shuffle\n\nds = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(Y).float())\n\n# train torch models with and without shuffling\nmodel1 = torch.nn.Linear(num_feat, num_out)\nopt1 = torch.optim.RMSprop(model1.parameters(), lr=0.01)\nloss1 = torch.nn.MSELoss()\n\ntrain_hist_shuf = []\n\nfor epoch in range(300):\n\n    train_batch_losses = []\n\n    # shuffle dataset\n    idx = torch.randperm(len(ds))\n    ds = TensorDataset(*ds[idx])\n\n    for batch in range(int(np.ceil(len(ds)/batch_size))):\n        x, y = ds[batch*batch_size:(batch+1)*batch_size]\n\n        x_var, y_var = Variable(x), Variable(y)\n        pred = model1(x_var)\n        l = loss1(pred, y_var)\n        train_batch_losses.append(l.data[0])\n\n        opt1.zero_grad()\n        l.backward()\n        opt1.step()\n\n    # save mean of all batch errors within the epoch\n    train_hist_shuf.append(np.array(train_batch_losses).mean())\n\n#### now no shuffling\n\nds = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(Y).float())\n\n# train torch models with and without shuffling\nmodel2 = torch.nn.Linear(num_feat, num_out)\nopt2 = torch.optim.RMSprop(model2.parameters(), lr=0.01)\nloss2 = torch.nn.MSELoss()\n\ntrain_hist_noshuf = []\n\nfor epoch in range(300):\n    train_batch_losses = []\n\n    for batch in range(int(np.ceil(len(ds)/batch_size))):\n        x, y = ds[batch*batch_size:(batch+1)*batch_size]\n\n        x_var, y_var = Variable(x), Variable(y)\n        pred = model2(x_var)\n        l = loss2(pred, y_var)\n        train_batch_losses.append(l.data[0])\n\n        opt2.zero_grad()\n        l.backward()\n        opt2.step()\n\n    train_hist_noshuf.append(np.array(train_batch_losses).mean())\n\n(p9.ggplot(pd.DataFrame({'torch_train_shuf': train_hist_shuf,\n                         'torch_train_noshuf': train_hist_noshuf,\n                         'epochs': range(len(train_hist_shuf))}),\n            p9.aes(x='epochs')) +\n  p9.geom_path(p9.aes(y='torch_train_shuf', color='\"loss (shuffled)\"')) +\n  p9.geom_path(p9.aes(y='torch_train_noshuf', color='\"loss (not shuffled)\"')) +\n  p9.labs(color='Loss', y=' ') +\n  p9.theme_minimal()).save('shuffle.png', width=6,\n                           height=4, dpi=200)\n\nHere is the resulting figure:", "body": "I've simplified the script a bit:\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport plotnine as p9\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\r\n\r\nnp.random.seed(555)\r\ntorch.manual_seed(555)\r\n\r\nnum_sample = 300\r\nnum_feat = 10\r\nnum_out = 10\r\n\r\nbatch_size = 32\r\n\r\nX = np.random.normal(0, 0.5, (num_sample, num_feat)).astype(np.float32)\r\nW = np.random.normal(0, 0.5, (num_feat, num_out)).astype(np.float32)\r\nb = np.random.normal(0, 0.5, (1, num_out)).astype(np.float32)\r\nY = np.exp(np.dot(X, W) + b)\r\n\r\n#### training with shuffle\r\n\r\nds = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(Y).float())\r\n\r\n# train torch models with and without shuffling\r\nmodel1 = torch.nn.Linear(num_feat, num_out)\r\nopt1 = torch.optim.RMSprop(model1.parameters(), lr=0.01)\r\nloss1 = torch.nn.MSELoss()\r\n\r\ntrain_hist_shuf = []\r\n\r\nfor epoch in range(300):\r\n\r\n    train_batch_losses = []\r\n\r\n    # shuffle dataset\r\n    idx = torch.randperm(len(ds))\r\n    ds = TensorDataset(*ds[idx])\r\n\r\n    for batch in range(int(np.ceil(len(ds)/batch_size))):\r\n        x, y = ds[batch*batch_size:(batch+1)*batch_size]\r\n\r\n        x_var, y_var = Variable(x), Variable(y)\r\n        pred = model1(x_var)\r\n        l = loss1(pred, y_var)\r\n        train_batch_losses.append(l.data[0])\r\n\r\n        opt1.zero_grad()\r\n        l.backward()\r\n        opt1.step()\r\n\r\n    # save mean of all batch errors within the epoch\r\n    train_hist_shuf.append(np.array(train_batch_losses).mean())\r\n\r\n#### now no shuffling\r\n\r\nds = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(Y).float())\r\n\r\n# train torch models with and without shuffling\r\nmodel2 = torch.nn.Linear(num_feat, num_out)\r\nopt2 = torch.optim.RMSprop(model2.parameters(), lr=0.01)\r\nloss2 = torch.nn.MSELoss()\r\n\r\ntrain_hist_noshuf = []\r\n\r\nfor epoch in range(300):\r\n    train_batch_losses = []\r\n\r\n    for batch in range(int(np.ceil(len(ds)/batch_size))):\r\n        x, y = ds[batch*batch_size:(batch+1)*batch_size]\r\n\r\n        x_var, y_var = Variable(x), Variable(y)\r\n        pred = model2(x_var)\r\n        l = loss2(pred, y_var)\r\n        train_batch_losses.append(l.data[0])\r\n\r\n        opt2.zero_grad()\r\n        l.backward()\r\n        opt2.step()\r\n\r\n    train_hist_noshuf.append(np.array(train_batch_losses).mean())\r\n\r\n(p9.ggplot(pd.DataFrame({'torch_train_shuf': train_hist_shuf,\r\n                         'torch_train_noshuf': train_hist_noshuf,\r\n                         'epochs': range(len(train_hist_shuf))}),\r\n            p9.aes(x='epochs')) +\r\n  p9.geom_path(p9.aes(y='torch_train_shuf', color='\"loss (shuffled)\"')) +\r\n  p9.geom_path(p9.aes(y='torch_train_noshuf', color='\"loss (not shuffled)\"')) +\r\n  p9.labs(color='Loss', y=' ') +\r\n  p9.theme_minimal()).save('shuffle.png', width=6,\r\n                           height=4, dpi=200)\r\n\r\n```\r\n\r\nHere is the resulting figure:\r\n\r\n![shuffle](https://user-images.githubusercontent.com/1140359/32252805-8f0f6bac-be97-11e7-8f93-7666aa7ecaad.png)"}