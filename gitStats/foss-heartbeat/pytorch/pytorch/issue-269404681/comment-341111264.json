{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/341111264", "html_url": "https://github.com/pytorch/pytorch/issues/3354#issuecomment-341111264", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/3354", "id": 341111264, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTExMTI2NA==", "user": {"login": "soumith", "id": 1310570, "node_id": "MDQ6VXNlcjEzMTA1NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1310570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soumith", "html_url": "https://github.com/soumith", "followers_url": "https://api.github.com/users/soumith/followers", "following_url": "https://api.github.com/users/soumith/following{/other_user}", "gists_url": "https://api.github.com/users/soumith/gists{/gist_id}", "starred_url": "https://api.github.com/users/soumith/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soumith/subscriptions", "organizations_url": "https://api.github.com/users/soumith/orgs", "repos_url": "https://api.github.com/users/soumith/repos", "events_url": "https://api.github.com/users/soumith/events{/privacy}", "received_events_url": "https://api.github.com/users/soumith/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-01T13:50:03Z", "updated_at": "2017-11-01T13:50:03Z", "author_association": "MEMBER", "body_html": "<p>This is still expected for the following counter-intuitive reason:</p>\n<p>When you do not shuffle across epochs, the model is never seeing the same sample again until it sees all other samples. For example, if you have 100 samples in your dataset, the model sees sample 13 with a frequency of 100, at the 13th step in every epoch.</p>\n<p>So, for a randomly distributed dataset, the momentum effect over some sequence of samples is roughly the same.</p>\n<p>However, if you shuffle the dataset after every epoch, then there is a higher chance that the frequency of seeing sample 13 is less than 100. i.e. you might see sample 13 in the last iteration of epoch0 and again in the first iteration of epoch1. This will have implications on optimization methods which use gradients and momentum, and the overall training loss might actually take a hit. For example think of a worst-case scenario where: epoch0 - [....., i0, i1, i2, i3], epoch1 - [i3, i2, i1, i0, ...]. In this case, the model can ever so slightly overfit to this local sequence of i0-i3 and when you sample i15 in the next iteration, the loss on it will be higher (because of that temporary overfitting). This happens with pure SGD, but is amplified in momentum-based methods even further.</p>", "body_text": "This is still expected for the following counter-intuitive reason:\nWhen you do not shuffle across epochs, the model is never seeing the same sample again until it sees all other samples. For example, if you have 100 samples in your dataset, the model sees sample 13 with a frequency of 100, at the 13th step in every epoch.\nSo, for a randomly distributed dataset, the momentum effect over some sequence of samples is roughly the same.\nHowever, if you shuffle the dataset after every epoch, then there is a higher chance that the frequency of seeing sample 13 is less than 100. i.e. you might see sample 13 in the last iteration of epoch0 and again in the first iteration of epoch1. This will have implications on optimization methods which use gradients and momentum, and the overall training loss might actually take a hit. For example think of a worst-case scenario where: epoch0 - [....., i0, i1, i2, i3], epoch1 - [i3, i2, i1, i0, ...]. In this case, the model can ever so slightly overfit to this local sequence of i0-i3 and when you sample i15 in the next iteration, the loss on it will be higher (because of that temporary overfitting). This happens with pure SGD, but is amplified in momentum-based methods even further.", "body": "This is still expected for the following counter-intuitive reason:\r\n\r\nWhen you do not shuffle across epochs, the model is never seeing the same sample again until it sees all other samples. For example, if you have 100 samples in your dataset, the model sees sample 13 with a frequency of 100, at the 13th step in every epoch.\r\n\r\nSo, for a randomly distributed dataset, the momentum effect over some sequence of samples is roughly the same.\r\n\r\nHowever, if you shuffle the dataset after every epoch, then there is a higher chance that the frequency of seeing sample 13 is less than 100. i.e. you might see sample 13 in the last iteration of epoch0 and again in the first iteration of epoch1. This will have implications on optimization methods which use gradients and momentum, and the overall training loss might actually take a hit. For example think of a worst-case scenario where: epoch0 - [....., i0, i1, i2, i3], epoch1 - [i3, i2, i1, i0, ...]. In this case, the model can ever so slightly overfit to this local sequence of i0-i3 and when you sample i15 in the next iteration, the loss on it will be higher (because of that temporary overfitting). This happens with pure SGD, but is amplified in momentum-based methods even further.\r\n"}