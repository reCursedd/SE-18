{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/349623230", "html_url": "https://github.com/pytorch/pytorch/issues/4054#issuecomment-349623230", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4054", "id": 349623230, "node_id": "MDEyOklzc3VlQ29tbWVudDM0OTYyMzIzMA==", "user": {"login": "maciejkula", "id": 2392579, "node_id": "MDQ6VXNlcjIzOTI1Nzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/2392579?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maciejkula", "html_url": "https://github.com/maciejkula", "followers_url": "https://api.github.com/users/maciejkula/followers", "following_url": "https://api.github.com/users/maciejkula/following{/other_user}", "gists_url": "https://api.github.com/users/maciejkula/gists{/gist_id}", "starred_url": "https://api.github.com/users/maciejkula/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maciejkula/subscriptions", "organizations_url": "https://api.github.com/users/maciejkula/orgs", "repos_url": "https://api.github.com/users/maciejkula/repos", "events_url": "https://api.github.com/users/maciejkula/events{/privacy}", "received_events_url": "https://api.github.com/users/maciejkula/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-06T12:20:08Z", "updated_at": "2017-12-06T12:20:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thank you for your suggestion.</p>\n<p>Just to clarify (and improve my mental model): repeat is necessarily a sharing-breaking operation because it allocates a new buffer and repeatedly copies the source. In that sense, I would argue that an implementation of <code>repeat</code> should be agnostic to whether the input tensor is shared or not; after all, allocating a new tensor is always necessary. In fact looking at the <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/tensor.py#L274\">code</a> suggests that this is indeed the intention.</p>\n<p>Given this reasoning, I see two possibilities:</p>\n<ol>\n<li>The error is thrown to make it abundantly obvious to the user that sharing is broken, and this is the intention of the code.</li>\n<li>The intention of the code <em>is</em> to allocate a new tensor and copy, but there is a bug in the implementation.</li>\n</ol>", "body_text": "Thank you for your suggestion.\nJust to clarify (and improve my mental model): repeat is necessarily a sharing-breaking operation because it allocates a new buffer and repeatedly copies the source. In that sense, I would argue that an implementation of repeat should be agnostic to whether the input tensor is shared or not; after all, allocating a new tensor is always necessary. In fact looking at the code suggests that this is indeed the intention.\nGiven this reasoning, I see two possibilities:\n\nThe error is thrown to make it abundantly obvious to the user that sharing is broken, and this is the intention of the code.\nThe intention of the code is to allocate a new tensor and copy, but there is a bug in the implementation.", "body": "Thank you for your suggestion.\r\n\r\nJust to clarify (and improve my mental model): repeat is necessarily a sharing-breaking operation because it allocates a new buffer and repeatedly copies the source. In that sense, I would argue that an implementation of `repeat` should be agnostic to whether the input tensor is shared or not; after all, allocating a new tensor is always necessary. In fact looking at the [code](https://github.com/pytorch/pytorch/blob/master/torch/tensor.py#L274) suggests that this is indeed the intention.\r\n\r\nGiven this reasoning, I see two possibilities:\r\n1. The error is thrown to make it abundantly obvious to the user that sharing is broken, and this is the intention of the code.\r\n2. The intention of the code _is_ to allocate a new tensor and copy, but there is a bug in the implementation."}