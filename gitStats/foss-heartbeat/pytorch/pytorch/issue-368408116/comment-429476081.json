{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/429476081", "html_url": "https://github.com/pytorch/pytorch/pull/12502#issuecomment-429476081", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/12502", "id": 429476081, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTQ3NjA4MQ==", "user": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-12T22:07:23Z", "updated_at": "2018-10-12T22:07:23Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>but I can't see why shouldn't it behave as if you executed the code without that block later.</p>\n</blockquote>\n<p>I don't think this is correct. What you are saying implies that it is possible to do</p>\n<pre><code>with torch.no_grad():\n  y = net(x)\n\ntorch.autograd.grad(y, x)\n</code></pre>\n<p>But operations done in <code>no_grad()</code> block should not keep any buffers or should be tracked by autograd history. We should make view ops consistent with other ops, so its outputs should just share version counter with base but not track any other autograd stuff. I am quite certain that most of the time this is the expected behavior from users. Also, anecdotally, I use PyTorch in my research and I certainly would expect so.</p>\n<p>It was never made clear that what view operation should be tracked and what should not. Previously we basically only have <code>.detach()</code> that is a non-differentiable view. Now we realize that there is <code>no_grad()</code>, and, in future, view ops whose outputs are naturally non-differentiable, e.g., <code>sparse_tensor.indices()</code>. So I believe it is good to have a mechanism like this. IMO, it makes everything easier to understand with a very clear rule.</p>\n<p>Moreover, I much more prefer fixing things in a general way and make the rules clear on what each thing should behave like, than apply local patches that makes the code base difficult to navigate around and understand why something happens (both for new comers and for already core devs). For one, I really really don't want to apply a manual patch on <code>sparse_tensor.indices()</code> just to make some op sharing storage with input, especially when all the information is there already in the yaml/codegen scripts (it is a view op; its output is not differentiable).</p>\n<p>The conclusion we arrived last time, I think, is to treat the output as if there is a detach() after the view operation. Probably I remembered wrong. But given the above thoughts, I still think that this patch is the correct thing to do.</p>", "body_text": "but I can't see why shouldn't it behave as if you executed the code without that block later.\n\nI don't think this is correct. What you are saying implies that it is possible to do\nwith torch.no_grad():\n  y = net(x)\n\ntorch.autograd.grad(y, x)\n\nBut operations done in no_grad() block should not keep any buffers or should be tracked by autograd history. We should make view ops consistent with other ops, so its outputs should just share version counter with base but not track any other autograd stuff. I am quite certain that most of the time this is the expected behavior from users. Also, anecdotally, I use PyTorch in my research and I certainly would expect so.\nIt was never made clear that what view operation should be tracked and what should not. Previously we basically only have .detach() that is a non-differentiable view. Now we realize that there is no_grad(), and, in future, view ops whose outputs are naturally non-differentiable, e.g., sparse_tensor.indices(). So I believe it is good to have a mechanism like this. IMO, it makes everything easier to understand with a very clear rule.\nMoreover, I much more prefer fixing things in a general way and make the rules clear on what each thing should behave like, than apply local patches that makes the code base difficult to navigate around and understand why something happens (both for new comers and for already core devs). For one, I really really don't want to apply a manual patch on sparse_tensor.indices() just to make some op sharing storage with input, especially when all the information is there already in the yaml/codegen scripts (it is a view op; its output is not differentiable).\nThe conclusion we arrived last time, I think, is to treat the output as if there is a detach() after the view operation. Probably I remembered wrong. But given the above thoughts, I still think that this patch is the correct thing to do.", "body": "> but I can't see why shouldn't it behave as if you executed the code without that block later. \r\n\r\nI don't think this is correct. What you are saying implies that it is possible to do\r\n```\r\nwith torch.no_grad():\r\n  y = net(x)\r\n\r\ntorch.autograd.grad(y, x)\r\n```\r\nBut operations done in `no_grad()` block should not keep any buffers or should be tracked by autograd history. We should make view ops consistent with other ops, so its outputs should just share version counter with base but not track any other autograd stuff. I am quite certain that most of the time this is the expected behavior from users. Also, anecdotally, I use PyTorch in my research and I certainly would expect so.\r\n\r\nIt was never made clear that what view operation should be tracked and what should not. Previously we basically only have `.detach()` that is a non-differentiable view. Now we realize that there is `no_grad()`, and, in future, view ops whose outputs are naturally non-differentiable, e.g., `sparse_tensor.indices()`. So I believe it is good to have a mechanism like this. IMO, it makes everything easier to understand with a very clear rule.\r\n\r\nMoreover, I much more prefer fixing things in a general way and make the rules clear on what each thing should behave like, than apply local patches that makes the code base difficult to navigate around and understand why something happens (both for new comers and for already core devs). For one, I really really don't want to apply a manual patch on `sparse_tensor.indices()` just to make some op sharing storage with input, especially when all the information is there already in the yaml/codegen scripts (it is a view op; its output is not differentiable).\r\n\r\nThe conclusion we arrived last time, I think, is to treat the output as if there is a detach() after the view operation. Probably I remembered wrong. But given the above thoughts, I still think that this patch is the correct thing to do."}