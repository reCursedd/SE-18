{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/224850542", "pull_request_review_id": 164325019, "id": 224850542, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNDg1MDU0Mg==", "diff_hunk": "@@ -369,15 +374,76 @@ struct TORCH_API Variable::Impl : public at::TensorImpl {\n };\n \n //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n-//                          Variable::ViewImpl\n+//                     Variable::DifferentiableViewImpl\n //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-/// A Variable that is a view on another Variable. The base and view share the\n-/// same version_counter. The grad_fn field of the Variable may become stale\n-/// due to in-place modifications of the shared data. Accesses should go\n-/// through get_grad_fn(). All other fields are always valid.\n-struct TORCH_API Variable::ViewImpl : public Variable::Impl {\n-  ViewImpl(Variable base, at::Tensor data, Edge gradient_edge);\n+/// NOTE [ Autograd View Variables ]\n+///\n+/// Many operations return Variable that shares storage with an input Variable.\n+/// The returned Variable is called a **view** Variable on the input **base**\n+/// Variable.\n+///\n+/// In PyTorch, we have two types of views: differentiable views, and\n+/// non-differentiable views. In either type, to support proper version\n+/// checking, the base and view Variables must always share the same\n+/// version_counter.\n+///\n+///\n+/// Differentiable Views\n+/// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Differentiable views are the view variables where you want gradients to flow\n+/// back to the base variables. Out-of-place operations on views are quite\n+/// straightforward, but in-place ones are very tricky. Even if the base\n+/// variable may not require grad when we create the view, we still need to\n+/// track the view relation because future in-place ops may require back-proping\n+/// through it. For example, we need to support\n+///\n+///   (1) in-place operation on view, e.g.,\n+///\n+///     # Have:\n+///     #   base.requires_grad = False\n+///     #   var.requires_grad = True\n+///     base[1] = var  # i.e., base[1].copy_(var)\n+///     torch.autograd.grad(base.sum(), var)  <- should return an all ones tensor\n+///\n+///   (2) in-place operation on base after view is created, e.g.,\n+///\n+///     # Have:\n+///     #   base.requires_grad = False\n+///     #   var.requires_grad = True\n+///     view = base[1]\n+///     base.copy_(var)\n+///     torch.autograd.grad(view.sum(), var)  <- should return a tensor with\n+///                                              var[1] filled with all ones and\n+///                                              zeros everywhere else\n+///\n+/// Variable::DifferentiableViewImpl is created to support gradient tracking of\n+/// such **in-place** operations. In particular,\n+///   + if an in-place op is done on base, the grad_fn field of the view may\n+///     become stale. So accesses should always go through get_grad_fn(), which\n+///     reconstructs an updated grad_fn if the version_counter has incremented.\n+///     All other fields are always valid.\n+///   + if an in-place op is done on view, in rebase_history() of view, which is\n+///     called after every in-place op in VariableType.cpp, the grad_fn of base\n+///     is updated.\n+///\n+///\n+/// Non-Differentiable Views\n+/// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// In certain cases, although function outputs share storage with inputs, they\n+/// will **never** require gradient history tracking. Instead of registering the\n+/// view relation via DifferentiableViewImpl in autograd, the views will be\n+/// using usual Varaible::Impl and just share the version counters with the base", "path": "torch/csrc/autograd/variable.h", "position": null, "original_position": 114, "commit_id": "574fb2e632ad46890e7c5283c7ede1e3264140e6", "original_commit_id": "f0e65850d85eb85462bc645c254a5ce133297560", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "nit Variable", "created_at": "2018-10-12T16:49:47Z", "updated_at": "2018-11-23T15:52:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/12502#discussion_r224850542", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12502", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/224850542"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12502#discussion_r224850542"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12502"}}, "body_html": "<p>nit Variable</p>", "body_text": "nit Variable"}