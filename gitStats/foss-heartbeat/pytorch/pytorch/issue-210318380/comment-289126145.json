{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/289126145", "html_url": "https://github.com/pytorch/pytorch/pull/857#issuecomment-289126145", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/857", "id": 289126145, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTEyNjE0NQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-24T19:48:04Z", "updated_at": "2017-03-24T19:48:04Z", "author_association": "MEMBER", "body_html": "<p>Actually I just realized that I forgot to synchronize after creating the input, so the addr time was skewed by the CPU copy. This is a new script, and it seems that addr is consistently faster 1.4x faster than expand + add <g-emoji class=\"g-emoji\" alias=\"confused\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f615.png\">\ud83d\ude15</g-emoji></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch                                                             \n<span class=\"pl-k\">import</span> time                                                              \n                                                                         \nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>                                                          \nfeatures <span class=\"pl-k\">=</span> <span class=\"pl-c1\">128</span>                                                           \n                                                                         \n<span class=\"pl-k\">for</span> batch_size <span class=\"pl-k\">in</span> (<span class=\"pl-c1\">2</span> <span class=\"pl-k\">**</span> i <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">11</span>)):                        \n    <span class=\"pl-k\">for</span> features <span class=\"pl-k\">in</span> (<span class=\"pl-c1\">2</span> <span class=\"pl-k\">**</span> i <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">11</span>)):                      \n        x <span class=\"pl-k\">=</span> torch.randn(features).cuda()                                 \n        y <span class=\"pl-k\">=</span> torch.zeros(batch_size, features).cuda()                     \n        torch.cuda.synchronize()                                         \n                                                                         \n        s <span class=\"pl-k\">=</span> time.perf_counter()                                          \n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):                                            \n            add_buffer <span class=\"pl-k\">=</span> x.new(batch_size).fill_(<span class=\"pl-c1\">1</span>)                      \n            y.addr_(add_buffer, x)                                       \n        torch.cuda.synchronize()                                         \n        addr_t <span class=\"pl-k\">=</span> time.perf_counter() <span class=\"pl-k\">-</span> s                                 \n                                                                         \n        s <span class=\"pl-k\">=</span> time.perf_counter()                                          \n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):                                            \n            y.add_(x.expand_as(y))                                       \n        torch.cuda.synchronize()                                         \n        expand_t <span class=\"pl-k\">=</span> time.perf_counter() <span class=\"pl-k\">-</span> s                               \n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">{}</span><span class=\"pl-cce\">\\t</span><span class=\"pl-c1\">{}</span><span class=\"pl-cce\">\\t</span><span class=\"pl-c1\">{<span class=\"pl-k\">:.4f</span>}</span><span class=\"pl-cce\">\\t</span><span class=\"pl-c1\">{<span class=\"pl-k\">:.4f</span>}</span><span class=\"pl-cce\">\\t</span><span class=\"pl-c1\">{<span class=\"pl-k\">:.2f</span>}</span><span class=\"pl-pds\">'</span></span>.format(                   \n              batch_size, features, addr_t, expand_t, addr_t <span class=\"pl-k\">/</span> expand_t))</pre></div>", "body_text": "Actually I just realized that I forgot to synchronize after creating the input, so the addr time was skewed by the CPU copy. This is a new script, and it seems that addr is consistently faster 1.4x faster than expand + add \ud83d\ude15\nimport torch                                                             \nimport time                                                              \n                                                                         \nbatch_size = 32                                                          \nfeatures = 128                                                           \n                                                                         \nfor batch_size in (2 ** i for i in range(5, 11)):                        \n    for features in (2 ** i for i in range(5, 11)):                      \n        x = torch.randn(features).cuda()                                 \n        y = torch.zeros(batch_size, features).cuda()                     \n        torch.cuda.synchronize()                                         \n                                                                         \n        s = time.perf_counter()                                          \n        for i in range(1000):                                            \n            add_buffer = x.new(batch_size).fill_(1)                      \n            y.addr_(add_buffer, x)                                       \n        torch.cuda.synchronize()                                         \n        addr_t = time.perf_counter() - s                                 \n                                                                         \n        s = time.perf_counter()                                          \n        for i in range(1000):                                            \n            y.add_(x.expand_as(y))                                       \n        torch.cuda.synchronize()                                         \n        expand_t = time.perf_counter() - s                               \n        print('{}\\t{}\\t{:.4f}\\t{:.4f}\\t{:.2f}'.format(                   \n              batch_size, features, addr_t, expand_t, addr_t / expand_t))", "body": "Actually I just realized that I forgot to synchronize after creating the input, so the addr time was skewed by the CPU copy. This is a new script, and it seems that addr is consistently faster 1.4x faster than expand + add \ud83d\ude15 \r\n\r\n```python\r\nimport torch                                                             \r\nimport time                                                              \r\n                                                                         \r\nbatch_size = 32                                                          \r\nfeatures = 128                                                           \r\n                                                                         \r\nfor batch_size in (2 ** i for i in range(5, 11)):                        \r\n    for features in (2 ** i for i in range(5, 11)):                      \r\n        x = torch.randn(features).cuda()                                 \r\n        y = torch.zeros(batch_size, features).cuda()                     \r\n        torch.cuda.synchronize()                                         \r\n                                                                         \r\n        s = time.perf_counter()                                          \r\n        for i in range(1000):                                            \r\n            add_buffer = x.new(batch_size).fill_(1)                      \r\n            y.addr_(add_buffer, x)                                       \r\n        torch.cuda.synchronize()                                         \r\n        addr_t = time.perf_counter() - s                                 \r\n                                                                         \r\n        s = time.perf_counter()                                          \r\n        for i in range(1000):                                            \r\n            y.add_(x.expand_as(y))                                       \r\n        torch.cuda.synchronize()                                         \r\n        expand_t = time.perf_counter() - s                               \r\n        print('{}\\t{}\\t{:.4f}\\t{:.4f}\\t{:.2f}'.format(                   \r\n              batch_size, features, addr_t, expand_t, addr_t / expand_t))\r\n```"}