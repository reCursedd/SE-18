{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7503", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7503/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7503/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7503/events", "html_url": "https://github.com/pytorch/pytorch/pull/7503", "id": 322371227, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg3NTIxMTE0", "number": 7503, "title": "[Caffe2]  Fix of the performance issue of IDEEP", "user": {"login": "yinghai", "id": 1100089, "node_id": "MDQ6VXNlcjExMDAwODk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1100089?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yinghai", "html_url": "https://github.com/yinghai", "followers_url": "https://api.github.com/users/yinghai/followers", "following_url": "https://api.github.com/users/yinghai/following{/other_user}", "gists_url": "https://api.github.com/users/yinghai/gists{/gist_id}", "starred_url": "https://api.github.com/users/yinghai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yinghai/subscriptions", "organizations_url": "https://api.github.com/users/yinghai/orgs", "repos_url": "https://api.github.com/users/yinghai/repos", "events_url": "https://api.github.com/users/yinghai/events{/privacy}", "received_events_url": "https://api.github.com/users/yinghai/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 890282107, "node_id": "MDU6TGFiZWw4OTAyODIxMDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/caffe2", "name": "caffe2", "color": "210aa8", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-05-11T17:14:59Z", "updated_at": "2018-11-23T15:43:57Z", "closed_at": "2018-05-11T20:43:41Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/7503", "html_url": "https://github.com/pytorch/pytorch/pull/7503", "diff_url": "https://github.com/pytorch/pytorch/pull/7503.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/7503.patch"}, "body_html": "<p>The key performance issue of IDEEP is that the <code>conv</code> op always reshape the weights at each run (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"321826469\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/intel/ideep/issues/7\" data-hovercard-type=\"issue\" data-hovercard-url=\"/intel/ideep/issues/7/hovercard\" href=\"https://github.com/intel/ideep/issues/7\">intel/ideep#7</a>), creating an overhead. When batch size is small, the overhead is relatively huge and easily observable. Even when the batch size is large, the overhead is still not negligible, which makes IDEEP runs always slower than the MKLML ops.</p>\n<p>The fix here is to cache the transformed weights, following the same approach as the MKLML ops. Performance is greatly improved according to my resnet50 run on 12-core Broadwell, beating MKLML ops slightly with both small and large batch size.</p>\n<p>Performance numbers (FPS) on resnet50 with 12 core 2.5GHz Broadwell socket. The larger the better.</p>\n<table>\n<thead>\n<tr>\n<th>Device</th>\n<th>1-thread/batch-128</th>\n<th>12-thread/batch-128</th>\n<th>1-thread/batch-1</th>\n<th>12-thread/batch-1</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>MKLML</td>\n<td>7.93</td>\n<td>58.42</td>\n<td>7.40</td>\n<td>49.72</td>\n</tr>\n<tr>\n<td>IDEEP</td>\n<td>8.27</td>\n<td>62.76</td>\n<td>7.93</td>\n<td>49.65</td>\n</tr>\n</tbody>\n</table>\n<p>Test command</p>\n<pre><code> KMP_AFFINITY=granularity=fine,compact,1 OMP_NUM_THREADS=12 MKL_NUM_THREADS=12 numactl -m 0 -N 0 python test_ideep_net.py --model resnet50 --batch_size 128 --layer_wise_benchmark --device {IDEEP|MKL}\n</code></pre>\n<p><strong>Note</strong> that <code>FC</code> ops potentially have the same issue, but right now from profiling I don't see it popping out. And there is follow-ups in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"321826469\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/intel/ideep/issues/7\" data-hovercard-type=\"issue\" data-hovercard-url=\"/intel/ideep/issues/7/hovercard\" href=\"https://github.com/intel/ideep/issues/7\">intel/ideep#7</a> to make sure we don't transform the weights in MKLDNN for that op.</p>", "body_text": "The key performance issue of IDEEP is that the conv op always reshape the weights at each run (intel/ideep#7), creating an overhead. When batch size is small, the overhead is relatively huge and easily observable. Even when the batch size is large, the overhead is still not negligible, which makes IDEEP runs always slower than the MKLML ops.\nThe fix here is to cache the transformed weights, following the same approach as the MKLML ops. Performance is greatly improved according to my resnet50 run on 12-core Broadwell, beating MKLML ops slightly with both small and large batch size.\nPerformance numbers (FPS) on resnet50 with 12 core 2.5GHz Broadwell socket. The larger the better.\n\n\n\nDevice\n1-thread/batch-128\n12-thread/batch-128\n1-thread/batch-1\n12-thread/batch-1\n\n\n\n\nMKLML\n7.93\n58.42\n7.40\n49.72\n\n\nIDEEP\n8.27\n62.76\n7.93\n49.65\n\n\n\nTest command\n KMP_AFFINITY=granularity=fine,compact,1 OMP_NUM_THREADS=12 MKL_NUM_THREADS=12 numactl -m 0 -N 0 python test_ideep_net.py --model resnet50 --batch_size 128 --layer_wise_benchmark --device {IDEEP|MKL}\n\nNote that FC ops potentially have the same issue, but right now from profiling I don't see it popping out. And there is follow-ups in intel/ideep#7 to make sure we don't transform the weights in MKLDNN for that op.", "body": "The key performance issue of IDEEP is that the `conv` op always reshape the weights at each run (intel/ideep#7), creating an overhead. When batch size is small, the overhead is relatively huge and easily observable. Even when the batch size is large, the overhead is still not negligible, which makes IDEEP runs always slower than the MKLML ops. \r\n\r\nThe fix here is to cache the transformed weights, following the same approach as the MKLML ops. Performance is greatly improved according to my resnet50 run on 12-core Broadwell, beating MKLML ops slightly with both small and large batch size. \r\n\r\nPerformance numbers (FPS) on resnet50 with 12 core 2.5GHz Broadwell socket. The larger the better. \r\n\r\nDevice|1-thread/batch-128 | 12-thread/batch-128|1-thread/batch-1 | 12-thread/batch-1\r\n-------|---------|----------|---------|----------\r\nMKLML|7.93|58.42|7.40|49.72\r\nIDEEP|8.27|62.76|7.93|49.65\r\n\r\nTest command\r\n```\r\n KMP_AFFINITY=granularity=fine,compact,1 OMP_NUM_THREADS=12 MKL_NUM_THREADS=12 numactl -m 0 -N 0 python test_ideep_net.py --model resnet50 --batch_size 128 --layer_wise_benchmark --device {IDEEP|MKL}\r\n```\r\n\r\n**Note** that `FC` ops potentially have the same issue, but right now from profiling I don't see it popping out. And there is follow-ups in intel/ideep#7 to make sure we don't transform the weights in MKLDNN for that op. "}