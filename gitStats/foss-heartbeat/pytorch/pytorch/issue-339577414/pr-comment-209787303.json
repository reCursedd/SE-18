{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209787303", "pull_request_review_id": 145869400, "id": 209787303, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTc4NzMwMw==", "diff_hunk": "@@ -1072,14 +1125,56 @@ def do_test(t):\n \n         do_test(self.SparseTensor())\n \n+    def _test_resize_shape(self, x_i, x_v, x_size, y_i, y_v, y_size):\n+        x = torch.sparse_coo_tensor(torch.zeros(x_i), torch.randn(x_v), torch.Size(x_size))\n+        x_dense = x.to_dense()\n+        y = torch.sparse_coo_tensor(torch.zeros(y_i), torch.randn(y_v), torch.Size(y_size))\n+        y_dense = y.to_dense()\n+        x.resize_as_(y)\n+        x_dense.resize_as_(y_dense)\n+        self.assertEqual(x.shape, y.shape)\n+        self.assertEqual(x._sparseDims(), y._sparseDims())\n+        self.assertEqual(x._denseDims(), y._denseDims())", "path": "test/test_sparse.py", "position": 98, "original_position": 92, "commit_id": "f7b1f23e8f85e8484934de71c9708933421009d0", "original_commit_id": "33e4eed872a19a5ba8b81a2f2e7b9d48fb51b41f", "user": {"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}, "body": "Copying from a collapsed comment:\r\n\r\n@gchanan:\r\nis the expectation that these resize_as_ calls behave equivalently as for dense tensors? If so, you should check this. FYI when I check this I get that case 2, 3, and 5 fail (caveat: I may not be checking this correctly, I just hacked something up). It's probably also better to do randn rather than ones for the values, since using ones everywhere can cover up issues.\r\n\r\n@yf225:\r\nThere seems to be two issues if we do the value check against a similarly resized dense tensors for equivalence:\r\n\r\n1. When we expand the size of the `values` tensor, since the newly allocated region is uninitialized (and not always set to zero), we can't guarantee that they have the same value in both the sparse tensor and the dense tensor we are comparing against.\r\n\r\n2. The memory layout of a sparse tensor's `values` and the equivalent dense tensor is different, and the same resize operation on them can give different results. For example:\r\n\r\n```\r\n>>> x_i, x_v, x_size = [1, 1], [1, 2, 3], [2, 2, 3]\r\n>>> y_i, y_v, y_size = [1, 1], [1, 2], [2, 2]\r\n>>> x = torch.sparse_coo_tensor(torch.zeros(x_i), torch.randn(x_v), torch.Size(x_size))\r\n>>> print(x)\r\ntorch.sparse.FloatTensor of size (2,2,3) with indices:\r\ntensor([[0]])\r\nand values:\r\ntensor([[[ 0.0832, -0.1464, -0.6159],\r\n         [ 0.0932,  1.3642, -0.2350]]])\r\n>>> x_dense = x.to_dense()\r\n>>> print(x_dense)\r\ntensor([[[ 0.0832, -0.1464, -0.6159],\r\n         [ 0.0932,  1.3642, -0.2350]],\r\n\r\n        [[ 0.0000,  0.0000,  0.0000],\r\n         [ 0.0000,  0.0000,  0.0000]]])\r\n>>> y = torch.sparse_coo_tensor(torch.zeros(y_i), torch.randn(y_v), torch.Size(y_size))\r\n>>> y_dense = y.to_dense()\r\n>>> x.resize_as_(y)\r\ntorch.sparse.FloatTensor of size (2,2) with indices:\r\ntensor([[0]])\r\nand values:\r\ntensor([[ 0.0832, -0.1464]])\r\n>>> x.to_dense()  # The second row doesn't have nonzero values, as expected\r\ntensor([[ 0.0832, -0.1464],\r\n        [ 0.0000,  0.0000]])\r\n>>> x_dense.resize_as_(y_dense)  # Notice that the second row has nonzero values\r\ntensor([[ 0.0832, -0.1464],\r\n        [-0.6159,  0.0932]])\r\n```\r\n\r\nHere `x.resize_as_(y)` is actually equivalent to `z=torch.zeros(y_dense.shape); z[0]=x_dense.resize_(y_dense.shape)[0]`, as we need to mask out any changes that are not directly affecting the nnz values.\r\n\r\nI think to address the 2nd issue we can perform the resize test on a sparse tensor in which the `values` has the same size as the overall sparse tensor, (e.g. `torch.sparse_coo_tensor(torch.zeros([1, 1]), torch.randn([1, 2, 3]), torch.Size([1, 2, 3]))`), so that we don't need to mask out the zero regions when checking for the correctness of `resize`. Although strictly speaking this is a reduction in coverage and only sufficient because we know how the sparse resize is implemented currently.\r\n\r\nFor the 1st issue, we could probably set the \"data\" region to some special value and set everywhere else to 0, so that we can do the equivalence comparison between the two expanded tensors.", "created_at": "2018-08-13T23:12:17Z", "updated_at": "2018-11-23T15:49:14Z", "html_url": "https://github.com/pytorch/pytorch/pull/9279#discussion_r209787303", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9279", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209787303"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9279#discussion_r209787303"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9279"}}, "body_html": "<p>Copying from a collapsed comment:</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a>:<br>\nis the expectation that these resize_as_ calls behave equivalently as for dense tensors? If so, you should check this. FYI when I check this I get that case 2, 3, and 5 fail (caveat: I may not be checking this correctly, I just hacked something up). It's probably also better to do randn rather than ones for the values, since using ones everywhere can cover up issues.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4063635\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yf225\">@yf225</a>:<br>\nThere seems to be two issues if we do the value check against a similarly resized dense tensors for equivalence:</p>\n<ol>\n<li>\n<p>When we expand the size of the <code>values</code> tensor, since the newly allocated region is uninitialized (and not always set to zero), we can't guarantee that they have the same value in both the sparse tensor and the dense tensor we are comparing against.</p>\n</li>\n<li>\n<p>The memory layout of a sparse tensor's <code>values</code> and the equivalent dense tensor is different, and the same resize operation on them can give different results. For example:</p>\n</li>\n</ol>\n<pre><code>&gt;&gt;&gt; x_i, x_v, x_size = [1, 1], [1, 2, 3], [2, 2, 3]\n&gt;&gt;&gt; y_i, y_v, y_size = [1, 1], [1, 2], [2, 2]\n&gt;&gt;&gt; x = torch.sparse_coo_tensor(torch.zeros(x_i), torch.randn(x_v), torch.Size(x_size))\n&gt;&gt;&gt; print(x)\ntorch.sparse.FloatTensor of size (2,2,3) with indices:\ntensor([[0]])\nand values:\ntensor([[[ 0.0832, -0.1464, -0.6159],\n         [ 0.0932,  1.3642, -0.2350]]])\n&gt;&gt;&gt; x_dense = x.to_dense()\n&gt;&gt;&gt; print(x_dense)\ntensor([[[ 0.0832, -0.1464, -0.6159],\n         [ 0.0932,  1.3642, -0.2350]],\n\n        [[ 0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000]]])\n&gt;&gt;&gt; y = torch.sparse_coo_tensor(torch.zeros(y_i), torch.randn(y_v), torch.Size(y_size))\n&gt;&gt;&gt; y_dense = y.to_dense()\n&gt;&gt;&gt; x.resize_as_(y)\ntorch.sparse.FloatTensor of size (2,2) with indices:\ntensor([[0]])\nand values:\ntensor([[ 0.0832, -0.1464]])\n&gt;&gt;&gt; x.to_dense()  # The second row doesn't have nonzero values, as expected\ntensor([[ 0.0832, -0.1464],\n        [ 0.0000,  0.0000]])\n&gt;&gt;&gt; x_dense.resize_as_(y_dense)  # Notice that the second row has nonzero values\ntensor([[ 0.0832, -0.1464],\n        [-0.6159,  0.0932]])\n</code></pre>\n<p>Here <code>x.resize_as_(y)</code> is actually equivalent to <code>z=torch.zeros(y_dense.shape); z[0]=x_dense.resize_(y_dense.shape)[0]</code>, as we need to mask out any changes that are not directly affecting the nnz values.</p>\n<p>I think to address the 2nd issue we can perform the resize test on a sparse tensor in which the <code>values</code> has the same size as the overall sparse tensor, (e.g. <code>torch.sparse_coo_tensor(torch.zeros([1, 1]), torch.randn([1, 2, 3]), torch.Size([1, 2, 3]))</code>), so that we don't need to mask out the zero regions when checking for the correctness of <code>resize</code>. Although strictly speaking this is a reduction in coverage and only sufficient because we know how the sparse resize is implemented currently.</p>\n<p>For the 1st issue, we could probably set the \"data\" region to some special value and set everywhere else to 0, so that we can do the equivalence comparison between the two expanded tensors.</p>", "body_text": "Copying from a collapsed comment:\n@gchanan:\nis the expectation that these resize_as_ calls behave equivalently as for dense tensors? If so, you should check this. FYI when I check this I get that case 2, 3, and 5 fail (caveat: I may not be checking this correctly, I just hacked something up). It's probably also better to do randn rather than ones for the values, since using ones everywhere can cover up issues.\n@yf225:\nThere seems to be two issues if we do the value check against a similarly resized dense tensors for equivalence:\n\n\nWhen we expand the size of the values tensor, since the newly allocated region is uninitialized (and not always set to zero), we can't guarantee that they have the same value in both the sparse tensor and the dense tensor we are comparing against.\n\n\nThe memory layout of a sparse tensor's values and the equivalent dense tensor is different, and the same resize operation on them can give different results. For example:\n\n\n>>> x_i, x_v, x_size = [1, 1], [1, 2, 3], [2, 2, 3]\n>>> y_i, y_v, y_size = [1, 1], [1, 2], [2, 2]\n>>> x = torch.sparse_coo_tensor(torch.zeros(x_i), torch.randn(x_v), torch.Size(x_size))\n>>> print(x)\ntorch.sparse.FloatTensor of size (2,2,3) with indices:\ntensor([[0]])\nand values:\ntensor([[[ 0.0832, -0.1464, -0.6159],\n         [ 0.0932,  1.3642, -0.2350]]])\n>>> x_dense = x.to_dense()\n>>> print(x_dense)\ntensor([[[ 0.0832, -0.1464, -0.6159],\n         [ 0.0932,  1.3642, -0.2350]],\n\n        [[ 0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000]]])\n>>> y = torch.sparse_coo_tensor(torch.zeros(y_i), torch.randn(y_v), torch.Size(y_size))\n>>> y_dense = y.to_dense()\n>>> x.resize_as_(y)\ntorch.sparse.FloatTensor of size (2,2) with indices:\ntensor([[0]])\nand values:\ntensor([[ 0.0832, -0.1464]])\n>>> x.to_dense()  # The second row doesn't have nonzero values, as expected\ntensor([[ 0.0832, -0.1464],\n        [ 0.0000,  0.0000]])\n>>> x_dense.resize_as_(y_dense)  # Notice that the second row has nonzero values\ntensor([[ 0.0832, -0.1464],\n        [-0.6159,  0.0932]])\n\nHere x.resize_as_(y) is actually equivalent to z=torch.zeros(y_dense.shape); z[0]=x_dense.resize_(y_dense.shape)[0], as we need to mask out any changes that are not directly affecting the nnz values.\nI think to address the 2nd issue we can perform the resize test on a sparse tensor in which the values has the same size as the overall sparse tensor, (e.g. torch.sparse_coo_tensor(torch.zeros([1, 1]), torch.randn([1, 2, 3]), torch.Size([1, 2, 3]))), so that we don't need to mask out the zero regions when checking for the correctness of resize. Although strictly speaking this is a reduction in coverage and only sufficient because we know how the sparse resize is implemented currently.\nFor the 1st issue, we could probably set the \"data\" region to some special value and set everywhere else to 0, so that we can do the equivalence comparison between the two expanded tensors."}