{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204580790", "pull_request_review_id": 139678316, "id": 204580790, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDU4MDc5MA==", "diff_hunk": "@@ -148,79 +176,104 @@ SparseTensor new_with_tensor_and_size_sparse(const LongTensor& indices, const Te\n   }\n \n   const SparseType& dtype = values.type().toSparse();\n+\n+#ifndef USE_TH_SIZE_ZERO_DIM\n   // NB: This used to be dims, but mumble TH handling zero-sized tensors\n   // incorrectly\n   if (indices.numel() == 0 && values.numel() == 0) {\n     return new_with_size_sparse(dtype, sizes);\n   }\n+#endif\n \n   int64_t sparseDims = indices.size(0);\n   int64_t denseDims = values.dim() - 1;\n-  AT_CHECK(sizes.size() == sparseDims + denseDims, \"number of dimensions must be sparseDims (\", sparseDims, \") + denseDims (\", denseDims, \"), but got \", sizes);\n-\n-  LongTensor max_indices = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n-  LongTensor cpu_max_indices;\n-  if (max_indices.is_cuda()) {\n-    cpu_max_indices = at::CPU(kLong).copy(max_indices);\n-  } else {\n-    cpu_max_indices = max_indices;\n-  }\n-  auto cpu_max_indices_accessor = cpu_max_indices.accessor<int64_t, 1>();\n-  for (int64_t d = 0; d < sparseDims; d++) {\n-    // NB: This used to sync ndim times to access each entry; now we copy\n-    // everything to CPU first and then access it.\n-    int64_t max_index_in_dim = cpu_max_indices_accessor[d];\n-    int64_t dim_size = sizes[static_cast<size_t>(d)];\n-    AT_CHECK(max_index_in_dim < dim_size,\n-             \"sizes is inconsistent with indices: for dim \", d, \", size is \", dim_size, \" but found index \", max_index_in_dim);\n+  AT_CHECK(sizes.size() == sparseDims + denseDims, \"number of dimensions must be sparseDims (\", sparseDims, \") + denseDims (\", denseDims, \"), but got \", sizes.size());\n+\n+  // Check to make sure all indices are within the boundaries of `sizes`\n+  if (indices.numel() > 0) {\n+    LongTensor max_indices = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n+    LongTensor cpu_max_indices;\n+    if (max_indices.is_cuda()) {\n+      cpu_max_indices = at::CPU(kLong).copy(max_indices);\n+    } else {\n+      cpu_max_indices = max_indices;\n+    }\n+    auto cpu_max_indices_accessor = cpu_max_indices.accessor<int64_t, 1>();\n+    for (int64_t d = 0; d < sparseDims; d++) {\n+      // NB: This used to sync ndim times to access each entry; now we copy\n+      // everything to CPU first and then access it.\n+      int64_t max_index_in_dim = cpu_max_indices_accessor[d];\n+      int64_t dim_size = sizes[static_cast<size_t>(d)];\n+      AT_CHECK(max_index_in_dim < dim_size,\n+               \"sizes is inconsistent with indices: for dim \", d, \", size is \", dim_size, \" but found index \", max_index_in_dim);\n+    }\n   }\n+\n+#ifndef USE_TH_SIZE_ZERO_DIM\n   for (int64_t d = 0; d < denseDims; d++) {\n     int64_t values_size = values.size(d+1);\n     int64_t specified_size = sizes[static_cast<size_t>(sparseDims + d)];\n     AT_CHECK(values_size <= specified_size,\n              \"values and sizes are inconsistent: sizes[\", d + sparseDims, \"] is \", specified_size,\n              \" but values.size(\", d + 1, \") is \", values_size);\n   }\n+#endif\n   return _new_with_dims_and_tensor_sparse(dtype, sparseDims, denseDims, sizes, indices, values);\n }\n \n // NB: Deleted newWithSizeNd variants\n \n SparseTensor clone_sparse(const SparseTensor& self) {\n+#ifndef USE_TH_SIZE_ZERO_DIM\n   SparseTensor other = new_sparse(self.type());\n-  _raw_resize_sparse(other, self._sparseDims(), self._denseDims(), self.sizes());\n+  _raw_resize_sparse_legacy(other, self._sparseDims(), self._denseDims(), self.sizes());\n   // NB: This seems to preserve the size of the UN-narrowed indices and\n   // values.  Veeery interesting.\n   _copy_into_sparse(other, _get_sparse_impl(self)->indices(), _get_sparse_impl(self)->values());\n   _get_sparse_impl(other)->set_coalesced(self.is_coalesced());\n   _get_sparse_impl(other)->set_nnz(self._nnz());\n   return other;\n+#else\n+  SparseTensor other = new_with_dims_and_size_sparse(self.type(), self._sparseDims(), self._denseDims(), self.sizes());\n+  _copy_into_sparse(other, _get_sparse_impl(self)->indices(), _get_sparse_impl(self)->values());\n+  _get_sparse_impl(other)->set_coalesced(self.is_coalesced());\n+  return other;\n+#endif\n }\n \n /******************************************************************************\n  * reshaping methods\n  ******************************************************************************/\n \n-/*\n-// We should implement a utility function which: (1) sets nnz and (2) resizes\n-// indices/values to hold enough space to fit nnz, if nnz is larger than\n-// the previous amount.  This ensures that we maintain the nnz invariant.\n-void _resize_nnz_(const SparseTensor& self, int64_t nnz) {\n+// TODO: remove this function when USE_TH_SIZE_ZERO_DIM is enabled by default\n+void resize_sparse(const SparseTensor& self, ArrayRef<int64_t> size) {\n+  _raw_resize_sparse_legacy(self, size.size(), 0, size);\n }\n-*/\n \n-void resize_sparse(const SparseTensor& self, ArrayRef<int64_t> size) {\n-  _raw_resize_sparse(self, size.size(), 0, size);\n+// TODO: remove this function when USE_TH_SIZE_ZERO_DIM is enabled by default\n+SparseTensor& raw_resize_sparse_legacy_(SparseTensor& self, ArrayRef<int64_t> size, int64_t sparseDims, int64_t denseDims) {\n+  if (sparseDims == -1) {\n+    sparseDims = self._indices().size(0);\n+  }\n+  if (denseDims == -1) {\n+    denseDims = self._values().dim() - 1;\n+  }\n+  _raw_resize_sparse_legacy(self, sparseDims, denseDims, size);\n+  return self;\n }\n \n-SparseTensor& raw_resize_sparse_(SparseTensor& self, ArrayRef<int64_t> size, int64_t sparseDims, int64_t denseDims) {\n+SparseTensor& sparse_resize_and_clear_(SparseTensor& self, ArrayRef<int64_t> size, int64_t sparseDims, int64_t denseDims) {\n   if (sparseDims == -1) {", "path": "aten/src/ATen/native/sparse/SparseTensor.cpp", "position": null, "original_position": 214, "commit_id": "f7b1f23e8f85e8484934de71c9708933421009d0", "original_commit_id": "5c83f2537455f5a108e8ac356844b4e99adc664c", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "do we still need this behavior?", "created_at": "2018-07-23T23:09:58Z", "updated_at": "2018-11-23T15:47:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/9279#discussion_r204580790", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9279", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204580790"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9279#discussion_r204580790"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9279"}}, "body_html": "<p>do we still need this behavior?</p>", "body_text": "do we still need this behavior?"}