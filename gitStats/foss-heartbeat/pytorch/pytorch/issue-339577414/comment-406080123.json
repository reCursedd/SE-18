{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/406080123", "html_url": "https://github.com/pytorch/pytorch/pull/9279#issuecomment-406080123", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/9279", "id": 406080123, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNjA4MDEyMw==", "user": {"login": "yf225", "id": 4063635, "node_id": "MDQ6VXNlcjQwNjM2MzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/4063635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yf225", "html_url": "https://github.com/yf225", "followers_url": "https://api.github.com/users/yf225/followers", "following_url": "https://api.github.com/users/yf225/following{/other_user}", "gists_url": "https://api.github.com/users/yf225/gists{/gist_id}", "starred_url": "https://api.github.com/users/yf225/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yf225/subscriptions", "organizations_url": "https://api.github.com/users/yf225/orgs", "repos_url": "https://api.github.com/users/yf225/repos", "events_url": "https://api.github.com/users/yf225/events{/privacy}", "received_events_url": "https://api.github.com/users/yf225/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-18T21:28:18Z", "updated_at": "2018-07-18T21:28:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3768583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gchanan\">@gchanan</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> On a high level, I made the following changes:</p>\n<ul>\n<li>\n<p>Got rid of <code>set_indices</code> / <code>set_values</code> / <code>set_nnz</code> in SparseTensorImpl.h, and re-route all the call sites that call these functions to use <code>set_indices_and_values_unsafe</code>, which will set <code>indices</code> and <code>values</code> accordingly, set <code>nnz</code> properly based on the dimensions of <code>indices</code> and <code>values</code>, and also perform checks for all invariants. This way <code>_nnz</code> is guaranteed to be the actual number of non-zero elements in the sparse tensor.<br>\nNote that we don't use <code>_nnz</code> to do narrowing on the sparse tensor anymore, but instead we let the caller do the narrowing on the underlying <code>indices</code> and <code>values</code> tensors, and then put them into the sparse tensor by calling <code>set_indices_and_values_unsafe</code>.</p>\n</li>\n<li>\n<p>There is no <code>sparse_raw_resize_</code> function anymore, instead there is <code>sparse_resize_and_empty_</code> function which resizes the sparse tensor and also resets the underlying <code>indices</code> and <code>values</code> tensors to empty. The reason we do this is that it's difficult to preserve the dim invariants when we change the sparse tensor size but it doesn't agree with the dims of the existing <code>indices</code> or <code>values</code> (in which case we could resize the data tensors as well, but it's not a safe operation and should be avoided). Instead, we should let the caller decide whether/how they want to keep the data tensors before resizing.</p>\n</li>\n<li>\n<p><code>tensor = torch.sparse_coo_tensor([], [], torch.Size([]))</code> is not supported anymore, because it breaks our assumption that a sparse tensor with empty size should have one element in values (i.e. it should be a scalar sparse tensor). If we need to create an empty sparse tensor, we should use torch.sparse.*Tensor() instead.</p>\n</li>\n</ul>\n<p>After zero-dim size is enabled by default, we can search <code>USE_TH_SIZE_ZERO_DIM</code> in the code and remove all code that is marked as legacy.</p>", "body_text": "@gchanan @ezyang On a high level, I made the following changes:\n\n\nGot rid of set_indices / set_values / set_nnz in SparseTensorImpl.h, and re-route all the call sites that call these functions to use set_indices_and_values_unsafe, which will set indices and values accordingly, set nnz properly based on the dimensions of indices and values, and also perform checks for all invariants. This way _nnz is guaranteed to be the actual number of non-zero elements in the sparse tensor.\nNote that we don't use _nnz to do narrowing on the sparse tensor anymore, but instead we let the caller do the narrowing on the underlying indices and values tensors, and then put them into the sparse tensor by calling set_indices_and_values_unsafe.\n\n\nThere is no sparse_raw_resize_ function anymore, instead there is sparse_resize_and_empty_ function which resizes the sparse tensor and also resets the underlying indices and values tensors to empty. The reason we do this is that it's difficult to preserve the dim invariants when we change the sparse tensor size but it doesn't agree with the dims of the existing indices or values (in which case we could resize the data tensors as well, but it's not a safe operation and should be avoided). Instead, we should let the caller decide whether/how they want to keep the data tensors before resizing.\n\n\ntensor = torch.sparse_coo_tensor([], [], torch.Size([])) is not supported anymore, because it breaks our assumption that a sparse tensor with empty size should have one element in values (i.e. it should be a scalar sparse tensor). If we need to create an empty sparse tensor, we should use torch.sparse.*Tensor() instead.\n\n\nAfter zero-dim size is enabled by default, we can search USE_TH_SIZE_ZERO_DIM in the code and remove all code that is marked as legacy.", "body": "@gchanan @ezyang On a high level, I made the following changes:\r\n\r\n- Got rid of `set_indices` / `set_values` / `set_nnz` in SparseTensorImpl.h, and re-route all the call sites that call these functions to use `set_indices_and_values_unsafe`, which will set `indices` and `values` accordingly, set `nnz` properly based on the dimensions of `indices` and `values`, and also perform checks for all invariants. This way `_nnz` is guaranteed to be the actual number of non-zero elements in the sparse tensor.\r\nNote that we don't use `_nnz` to do narrowing on the sparse tensor anymore, but instead we let the caller do the narrowing on the underlying `indices` and `values` tensors, and then put them into the sparse tensor by calling `set_indices_and_values_unsafe`.\r\n\r\n- There is no `sparse_raw_resize_` function anymore, instead there is `sparse_resize_and_empty_` function which resizes the sparse tensor and also resets the underlying `indices` and `values` tensors to empty. The reason we do this is that it's difficult to preserve the dim invariants when we change the sparse tensor size but it doesn't agree with the dims of the existing `indices` or `values` (in which case we could resize the data tensors as well, but it's not a safe operation and should be avoided). Instead, we should let the caller decide whether/how they want to keep the data tensors before resizing.\r\n\r\n- `tensor = torch.sparse_coo_tensor([], [], torch.Size([]))` is not supported anymore, because it breaks our assumption that a sparse tensor with empty size should have one element in values (i.e. it should be a scalar sparse tensor). If we need to create an empty sparse tensor, we should use torch.sparse.*Tensor() instead.\r\n\r\nAfter zero-dim size is enabled by default, we can search `USE_TH_SIZE_ZERO_DIM` in the code and remove all code that is marked as legacy."}