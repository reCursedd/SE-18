{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205283807", "pull_request_review_id": 140511935, "id": 205283807, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNTI4MzgwNw==", "diff_hunk": "@@ -148,79 +176,97 @@ SparseTensor new_with_tensor_and_size_sparse(const LongTensor& indices, const Te\n   }\n \n   const SparseType& dtype = values.type().toSparse();\n+\n+#ifndef USE_TH_SIZE_ZERO_DIM\n   // NB: This used to be dims, but mumble TH handling zero-sized tensors\n   // incorrectly\n   if (indices.numel() == 0 && values.numel() == 0) {\n     return new_with_size_sparse(dtype, sizes);\n   }\n+#endif\n \n   int64_t sparseDims = indices.size(0);\n   int64_t denseDims = values.dim() - 1;\n-  AT_CHECK(sizes.size() == sparseDims + denseDims, \"number of dimensions must be sparseDims (\", sparseDims, \") + denseDims (\", denseDims, \"), but got \", sizes);\n-\n-  LongTensor max_indices = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n-  LongTensor cpu_max_indices;\n-  if (max_indices.is_cuda()) {\n-    cpu_max_indices = at::CPU(kLong).copy(max_indices);\n-  } else {\n-    cpu_max_indices = max_indices;\n-  }\n-  auto cpu_max_indices_accessor = cpu_max_indices.accessor<int64_t, 1>();\n-  for (int64_t d = 0; d < sparseDims; d++) {\n-    // NB: This used to sync ndim times to access each entry; now we copy\n-    // everything to CPU first and then access it.\n-    int64_t max_index_in_dim = cpu_max_indices_accessor[d];\n-    int64_t dim_size = sizes[static_cast<size_t>(d)];\n-    AT_CHECK(max_index_in_dim < dim_size,\n-             \"sizes is inconsistent with indices: for dim \", d, \", size is \", dim_size, \" but found index \", max_index_in_dim);\n+  AT_CHECK(sizes.size() == sparseDims + denseDims, \"number of dimensions must be sparseDims (\", sparseDims, \") + denseDims (\", denseDims, \"), but got \", sizes.size());\n+\n+  // Check to make sure all indices are within the boundaries of `sizes`\n+  if (indices.numel() > 0) {\n+    LongTensor max_indices = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n+    LongTensor cpu_max_indices;\n+    if (max_indices.is_cuda()) {\n+      cpu_max_indices = at::CPU(kLong).copy(max_indices);\n+    } else {\n+      cpu_max_indices = max_indices;\n+    }\n+    auto cpu_max_indices_accessor = cpu_max_indices.accessor<int64_t, 1>();\n+    for (int64_t d = 0; d < sparseDims; d++) {\n+      // NB: This used to sync ndim times to access each entry; now we copy\n+      // everything to CPU first and then access it.\n+      int64_t max_index_in_dim = cpu_max_indices_accessor[d];\n+      int64_t dim_size = sizes[static_cast<size_t>(d)];\n+      AT_CHECK(max_index_in_dim < dim_size,\n+               \"sizes is inconsistent with indices: for dim \", d, \", size is \", dim_size, \" but found index \", max_index_in_dim);\n+    }\n   }\n+\n+#ifndef USE_TH_SIZE_ZERO_DIM", "path": "aten/src/ATen/native/sparse/SparseTensor.cpp", "position": null, "original_position": 150, "commit_id": "f7b1f23e8f85e8484934de71c9708933421009d0", "original_commit_id": "48cbd37ac3ec1bd1cea7f9087ddb68872b481daa", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "why does this not apply if not USE_TH_SIZE_ZERO_DIM?", "created_at": "2018-07-25T22:40:58Z", "updated_at": "2018-11-23T15:48:08Z", "html_url": "https://github.com/pytorch/pytorch/pull/9279#discussion_r205283807", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9279", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/205283807"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9279#discussion_r205283807"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9279"}}, "body_html": "<p>why does this not apply if not USE_TH_SIZE_ZERO_DIM?</p>", "body_text": "why does this not apply if not USE_TH_SIZE_ZERO_DIM?"}