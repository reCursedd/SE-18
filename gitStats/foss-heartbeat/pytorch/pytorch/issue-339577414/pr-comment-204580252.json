{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204580252", "pull_request_review_id": 139678316, "id": 204580252, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNDU4MDI1Mg==", "diff_hunk": "@@ -90,32 +92,55 @@ SparseTensor new_with_tensor_sparse(const LongTensor& indices, const Tensor& val\n   int64_t denseDims = values.dim() - 1;\n \n   std::vector<int64_t> computed_sizes(sparseDims + denseDims);\n-  // NB: It used to keepdim. I think that was wrong.\n-  LongTensor computed_indices_sizes = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n-  computed_indices_sizes.add_(1); // len = max_index + 1\n-  LongTensor cpu_computed_indices_sizes;\n-  if (computed_indices_sizes.is_cuda()) {\n-    cpu_computed_indices_sizes = at::CPU(kLong).tensor(computed_indices_sizes.sizes());\n-    cpu_computed_indices_sizes.copy_(computed_indices_sizes);\n+  if (indices.numel() > 0) {\n+    // NB: It used to keepdim. I think that was wrong.\n+    LongTensor computed_indices_sizes = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n+    computed_indices_sizes.add_(1); // len = max_index + 1\n+    LongTensor cpu_computed_indices_sizes;\n+    if (computed_indices_sizes.is_cuda()) {\n+      cpu_computed_indices_sizes = at::CPU(kLong).tensor(computed_indices_sizes.sizes());\n+      cpu_computed_indices_sizes.copy_(computed_indices_sizes);\n+    } else {\n+      cpu_computed_indices_sizes = computed_indices_sizes;\n+    }\n+    auto cpu_computed_indices_sizes_accessor = cpu_computed_indices_sizes.accessor<int64_t, 1>();\n+    for (int64_t d = 0; d < sparseDims; d++) {\n+      computed_sizes[static_cast<size_t>(d)] = cpu_computed_indices_sizes_accessor[d];\n+    }\n   } else {\n-    cpu_computed_indices_sizes = computed_indices_sizes;\n-  }\n-  auto cpu_computed_indices_sizes_accessor = cpu_computed_indices_sizes.accessor<int64_t, 1>();\n-  for (int64_t d = 0; d < sparseDims; d++) {\n-    computed_sizes[static_cast<size_t>(d)] = cpu_computed_indices_sizes_accessor[d];\n+    for (int64_t d = 0; d < sparseDims; d++) {\n+      computed_sizes[static_cast<size_t>(d)] = 0;", "path": "aten/src/ATen/native/sparse/SparseTensor.cpp", "position": null, "original_position": 46, "commit_id": "f7b1f23e8f85e8484934de71c9708933421009d0", "original_commit_id": "5c83f2537455f5a108e8ac356844b4e99adc664c", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "can you explain the logic here?  I don't get why all the sparse dimension sizes need to be 0 -- that's not the case usually, right?", "created_at": "2018-07-23T23:06:48Z", "updated_at": "2018-11-23T15:47:56Z", "html_url": "https://github.com/pytorch/pytorch/pull/9279#discussion_r204580252", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9279", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/204580252"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9279#discussion_r204580252"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9279"}}, "body_html": "<p>can you explain the logic here?  I don't get why all the sparse dimension sizes need to be 0 -- that's not the case usually, right?</p>", "body_text": "can you explain the logic here?  I don't get why all the sparse dimension sizes need to be 0 -- that's not the case usually, right?"}