{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202491710", "pull_request_review_id": 137208828, "id": 202491710, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwMjQ5MTcxMA==", "diff_hunk": "@@ -156,31 +179,39 @@ SparseTensor new_with_tensor_and_size_sparse(const LongTensor& indices, const Te\n \n   int64_t sparseDims = indices.size(0);\n   int64_t denseDims = values.dim() - 1;\n-  AT_CHECK(sizes.size() == sparseDims + denseDims, \"number of dimensions must be sparseDims (\", sparseDims, \") + denseDims (\", denseDims, \"), but got \", sizes);\n+  AT_CHECK(sizes.size() == sparseDims + denseDims, \"number of dimensions must be sparseDims (\", sparseDims, \") + denseDims (\", denseDims, \"), but got \", sizes.size());\n+\n+  // Check to make sure all indices are within the boundaries of `sizes`\n+  // NOTE: We don't need to check this for scalar sparse tensors, which\n+  // has sizes.size() == sparseDims == denseDims == 0\n+  if (sizes.size() > 0) {\n+    LongTensor max_indices = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n+    LongTensor cpu_max_indices;\n+    if (max_indices.is_cuda()) {\n+      cpu_max_indices = at::CPU(kLong).copy(max_indices);\n+    } else {\n+      cpu_max_indices = max_indices;\n+    }\n+    auto cpu_max_indices_accessor = cpu_max_indices.accessor<int64_t, 1>();\n+    for (int64_t d = 0; d < sparseDims; d++) {\n+      // NB: This used to sync ndim times to access each entry; now we copy\n+      // everything to CPU first and then access it.\n+      int64_t max_index_in_dim = cpu_max_indices_accessor[d];\n+      int64_t dim_size = sizes[static_cast<size_t>(d)];\n+      AT_CHECK(max_index_in_dim < dim_size,\n+               \"sizes is inconsistent with indices: for dim \", d, \", size is \", dim_size, \" but found index \", max_index_in_dim);\n+    }\n \n-  LongTensor max_indices = std::get</* values */ 0>(indices.max(/* dim */ 1, /* keepdim */ false));\n-  LongTensor cpu_max_indices;\n-  if (max_indices.is_cuda()) {\n-    cpu_max_indices = at::CPU(kLong).copy(max_indices);\n-  } else {\n-    cpu_max_indices = max_indices;\n-  }\n-  auto cpu_max_indices_accessor = cpu_max_indices.accessor<int64_t, 1>();\n-  for (int64_t d = 0; d < sparseDims; d++) {\n-    // NB: This used to sync ndim times to access each entry; now we copy\n-    // everything to CPU first and then access it.\n-    int64_t max_index_in_dim = cpu_max_indices_accessor[d];\n-    int64_t dim_size = sizes[static_cast<size_t>(d)];\n-    AT_CHECK(max_index_in_dim < dim_size,\n-             \"sizes is inconsistent with indices: for dim \", d, \", size is \", dim_size, \" but found index \", max_index_in_dim);\n-  }\n-  for (int64_t d = 0; d < denseDims; d++) {\n-    int64_t values_size = values.size(d+1);\n-    int64_t specified_size = sizes[static_cast<size_t>(sparseDims + d)];\n-    AT_CHECK(values_size <= specified_size,\n-             \"values and sizes are inconsistent: sizes[\", d + sparseDims, \"] is \", specified_size,\n-             \" but values.size(\", d + 1, \") is \", values_size);\n+    // Check to make sure the dense dimensions match between `values` and `sizes`\n+    for (int64_t d = 0; d < denseDims; d++) {", "path": "aten/src/ATen/native/sparse/SparseTensor.cpp", "position": null, "original_position": 100, "commit_id": "f7b1f23e8f85e8484934de71c9708933421009d0", "original_commit_id": "0a93ea0712a67fb3130dc093b4326352a9fff943", "user": {"login": "gchanan", "id": 3768583, "node_id": "MDQ6VXNlcjM3Njg1ODM=", "avatar_url": "https://avatars2.githubusercontent.com/u/3768583?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gchanan", "html_url": "https://github.com/gchanan", "followers_url": "https://api.github.com/users/gchanan/followers", "following_url": "https://api.github.com/users/gchanan/following{/other_user}", "gists_url": "https://api.github.com/users/gchanan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gchanan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gchanan/subscriptions", "organizations_url": "https://api.github.com/users/gchanan/orgs", "repos_url": "https://api.github.com/users/gchanan/repos", "events_url": "https://api.github.com/users/gchanan/events{/privacy}", "received_events_url": "https://api.github.com/users/gchanan/received_events", "type": "User", "site_admin": false}, "body": "does it make sense to unify all the checking with where you do it above (in SparseTensorImpl.cpp?)", "created_at": "2018-07-13T23:03:41Z", "updated_at": "2018-11-23T15:47:24Z", "html_url": "https://github.com/pytorch/pytorch/pull/9279#discussion_r202491710", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/9279", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/202491710"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/9279#discussion_r202491710"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/9279"}}, "body_html": "<p>does it make sense to unify all the checking with where you do it above (in SparseTensorImpl.cpp?)</p>", "body_text": "does it make sense to unify all the checking with where you do it above (in SparseTensorImpl.cpp?)"}