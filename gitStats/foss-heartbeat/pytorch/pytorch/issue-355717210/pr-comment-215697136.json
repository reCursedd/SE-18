{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215697136", "pull_request_review_id": 153020597, "id": 215697136, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxNTY5NzEzNg==", "diff_hunk": "@@ -0,0 +1,217 @@\n+#include \"ATen/ATen.h\"\n+#include <THC/THCTensorMathReduce.cuh>\n+#include <math.h>\n+\n+#include \"ATen/native/Distance.h\"\n+\n+\n+namespace at { namespace native {\n+\n+namespace {\n+\n+static const int warp_size = 32;\n+static const int forward_warps = 8;\n+\n+template <typename scalar_t>\n+static __forceinline__ __device__ scalar_t device_sqrt(scalar_t val);\n+\n+template <>\n+__forceinline__ __device__ float device_sqrt(float val) {\n+  return ::sqrtf(val);\n+}\n+\n+template <>\n+__forceinline__ __device__ double device_sqrt(double val) {\n+  return ::sqrt(val);\n+}\n+\n+template <typename scalar_t>\n+struct dists {\n+\n+  static __forceinline__ __device__ scalar_t sign(scalar_t val) {\n+    return (0 < val) - (val < 0);\n+  }\n+\n+  // Zero norm\n+  struct zero {\n+    static __forceinline__ __device__ void inc(scalar_t& agg, const scalar_t diff, const scalar_t p) { agg += diff != 0.0; }\n+    static __forceinline__ __device__ scalar_t finish(const scalar_t agg, const scalar_t p) { return agg; }\n+    static __forceinline__ __device__ void agg(scalar_t& update, const scalar_t other) { update += other; }\n+  };\n+\n+  // One norm\n+  struct one {\n+    static __forceinline__ __device__ void inc(scalar_t& agg, const scalar_t diff, const scalar_t p) { agg += diff; }\n+    static __forceinline__ __device__ scalar_t finish(const scalar_t agg, const scalar_t p) { return agg; }\n+    static __forceinline__ __device__ void agg(scalar_t& update, const scalar_t other) { update += other; }\n+    static __forceinline__ __device__ scalar_t backward(const scalar_t diff, const scalar_t grad, const scalar_t dist, const scalar_t p) { return grad * sign(diff); }\n+  };\n+\n+  // Special case backward when p is less than two\n+  struct lt_two {\n+    static __forceinline__ __device__ scalar_t backward(const scalar_t diff, const scalar_t grad, const scalar_t dist, const scalar_t p) { return dist == 0.0 ? 0 : sign(diff) * std::pow(std::abs(diff), p - 1) * grad / std::pow(dist, p - 1); }\n+  };\n+\n+  // Two norm\n+  struct two {\n+    static __forceinline__ __device__ void inc(scalar_t& agg, const scalar_t diff, const scalar_t p) { agg += diff * diff; }\n+    static __forceinline__ __device__ scalar_t finish(const scalar_t agg, const scalar_t p) { return device_sqrt<scalar_t>(agg); }\n+    static __forceinline__ __device__ void agg(scalar_t& update, const scalar_t other) { update += other; }\n+    static __forceinline__ __device__ scalar_t backward(const scalar_t diff, const scalar_t grad, const scalar_t dist, const scalar_t p) { return dist == 0.0 ? 0 : grad * diff / dist; }\n+  };\n+\n+  // General p norm\n+  struct p {\n+    static __forceinline__ __device__ void inc(scalar_t& agg, const scalar_t diff, const scalar_t p) { agg += std::pow(diff, p); }\n+    static __forceinline__ __device__ scalar_t finish(const scalar_t agg, const scalar_t p) { return std::pow(agg, static_cast<scalar_t>(1) / p); }\n+    static __forceinline__ __device__ void agg(scalar_t& update, const scalar_t other) { update += other; }\n+    static __forceinline__ __device__ scalar_t backward(const scalar_t diff, const scalar_t grad, const scalar_t dist, const scalar_t p) { return dist == 0.0 ? 0 : diff * std::pow(std::abs(diff), p - 2) * grad / std::pow(dist, p - 1); }\n+  };\n+\n+  // Inf norm\n+  struct inf {\n+    static __forceinline__ __device__ void inc(scalar_t& agg, const scalar_t diff, const scalar_t p) { if (diff > agg) { agg = diff; } }\n+    static __forceinline__ __device__ scalar_t finish(const scalar_t agg, const scalar_t p) { return agg; }\n+    static __forceinline__ __device__ void agg(scalar_t& update, const scalar_t other) { if (other > update) { update = other; } }\n+    static __forceinline__ __device__ scalar_t backward(const scalar_t diff, const scalar_t grad, const scalar_t dist, const scalar_t p) { return grad * sign(diff) * (std::abs(diff) == dist); }\n+  };\n+\n+};\n+\n+template <typename scalar_t, typename F>\n+__global__ static void pdist_kernel_cuda_impl(scalar_t * result, const scalar_t * self, const int64_t n, const int64_t m, const scalar_t p) {\n+  const int k = blockIdx.x;\n+  const int stride = blockDim.x;\n+\n+  float n2 = n - .5;\n+  // The -1 accounts for floating point truncation issues\n+  int64_t i = static_cast<int64_t>((n2 - device_sqrt<scalar_t>(n2 * n2 - 2 * k - 1)));\n+  int64_t j = k - n * i + i * (i + 1) / 2 + i + 1;\n+\n+  const scalar_t * const start = self + i * m;\n+  const scalar_t * const end = start + m;\n+  const scalar_t * a = start + threadIdx.x;\n+  const scalar_t * b = self + j * m + threadIdx.x;\n+  scalar_t agg = 0.0;\n+  for (; a < end; a += stride, b += stride) {\n+    F::inc(agg, std::abs(*a - *b), p);\n+  }\n+  \n+  // Reduce warps\n+  for (int offset = warp_size / 2; offset > 0; offset /= 2) {", "path": "aten/src/ATen/native/cuda/DistanceKernel.cu", "position": null, "original_position": 101, "commit_id": "32aafbf523153ab581e6029db69458ef42eee148", "original_commit_id": "27dcdb85fdf5cf2fae9cb3815c2bf997937662bd", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "Within the kernels you can use the built-in `warpSize`", "created_at": "2018-09-06T16:41:52Z", "updated_at": "2018-11-23T15:50:40Z", "html_url": "https://github.com/pytorch/pytorch/pull/11102#discussion_r215697136", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11102", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/215697136"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11102#discussion_r215697136"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11102"}}, "body_html": "<p>Within the kernels you can use the built-in <code>warpSize</code></p>", "body_text": "Within the kernels you can use the built-in warpSize"}