{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11102", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11102/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11102/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11102/events", "html_url": "https://github.com/pytorch/pytorch/pull/11102", "id": 355717210, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEyMTgxODI1", "number": 11102, "title": "Add GPU implementation of pdist", "user": {"login": "erikbrinkman", "id": 858926, "node_id": "MDQ6VXNlcjg1ODkyNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/858926?v=4", "gravatar_id": "", "url": "https://api.github.com/users/erikbrinkman", "html_url": "https://github.com/erikbrinkman", "followers_url": "https://api.github.com/users/erikbrinkman/followers", "following_url": "https://api.github.com/users/erikbrinkman/following{/other_user}", "gists_url": "https://api.github.com/users/erikbrinkman/gists{/gist_id}", "starred_url": "https://api.github.com/users/erikbrinkman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/erikbrinkman/subscriptions", "organizations_url": "https://api.github.com/users/erikbrinkman/orgs", "repos_url": "https://api.github.com/users/erikbrinkman/repos", "events_url": "https://api.github.com/users/erikbrinkman/events{/privacy}", "received_events_url": "https://api.github.com/users/erikbrinkman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-08-30T19:11:31Z", "updated_at": "2018-11-23T15:50:41Z", "closed_at": "2018-09-07T16:11:03Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11102", "html_url": "https://github.com/pytorch/pytorch/pull/11102", "diff_url": "https://github.com/pytorch/pytorch/pull/11102.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11102.patch"}, "body_html": "<p>Add the gpu kernel version.</p>\n<p>The parallelism I went with performs poorly when there are a large number of vectors, but they're all short, as I don't allocate the thread pool to wrap in that case.</p>\n<h2>Test Plan</h2>\n<pre><code>python -m unittest test_torch.TestTorch.test_pdist_{empty,scipy} test_nn.TestNN.test_pdist{,_zeros,_empty_row,_empty_col,_cpu_gradgrad_unimplemented,_cuda_gradgrad_unimplemented} test_jit.TestJitGenerated.test_nn_pdist\n</code></pre>\n<p>Current performance specs are a little underwhelming, I'm in the process of debugging.</p>\n<table>\n<thead>\n<tr>\n<th>size</th>\n<th>torch</th>\n<th>torch cuda</th>\n<th>scipy</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>16 x 16</td>\n<td>9.13 \u00b5s \u00b1 3.55 \u00b5s</td>\n<td>9.86 \u00b5s \u00b1 81.5 ns</td>\n<td>15.8 \u00b5s \u00b1 1.2 \u00b5s</td>\n</tr>\n<tr>\n<td>16 x 1024</td>\n<td>15 \u00b5s \u00b1 224 ns</td>\n<td>9.48 \u00b5s \u00b1 88.7 ns</td>\n<td>88.7 \u00b5s \u00b1 8.83 \u00b5s</td>\n</tr>\n<tr>\n<td>1024 x 16</td>\n<td>852 \u00b5s \u00b1 6.03 \u00b5s</td>\n<td>7.84 ms \u00b1 6.22 \u00b5s</td>\n<td>4.7 ms \u00b1 166 \u00b5s</td>\n</tr>\n<tr>\n<td>1024 x 1024</td>\n<td>34.1 ms \u00b1 803 \u00b5s</td>\n<td>11.5 ms \u00b1 6.24 \u00b5s</td>\n<td>273 ms \u00b1 6.7 ms</td>\n</tr>\n<tr>\n<td>2048 x 2048</td>\n<td>261 ms \u00b1 3.5 ms</td>\n<td>77.5 ms \u00b1 41.5 \u00b5s</td>\n<td>2.5 s \u00b1 97.6 ms</td>\n</tr>\n<tr>\n<td>4096 x 4096</td>\n<td>2.37 s \u00b1 154 ms</td>\n<td>636 ms \u00b1 2.97 \u00b5s</td>\n<td>25.9 s \u00b1 394 ms</td>\n</tr>\n</tbody>\n</table>", "body_text": "Add the gpu kernel version.\nThe parallelism I went with performs poorly when there are a large number of vectors, but they're all short, as I don't allocate the thread pool to wrap in that case.\nTest Plan\npython -m unittest test_torch.TestTorch.test_pdist_{empty,scipy} test_nn.TestNN.test_pdist{,_zeros,_empty_row,_empty_col,_cpu_gradgrad_unimplemented,_cuda_gradgrad_unimplemented} test_jit.TestJitGenerated.test_nn_pdist\n\nCurrent performance specs are a little underwhelming, I'm in the process of debugging.\n\n\n\nsize\ntorch\ntorch cuda\nscipy\n\n\n\n\n16 x 16\n9.13 \u00b5s \u00b1 3.55 \u00b5s\n9.86 \u00b5s \u00b1 81.5 ns\n15.8 \u00b5s \u00b1 1.2 \u00b5s\n\n\n16 x 1024\n15 \u00b5s \u00b1 224 ns\n9.48 \u00b5s \u00b1 88.7 ns\n88.7 \u00b5s \u00b1 8.83 \u00b5s\n\n\n1024 x 16\n852 \u00b5s \u00b1 6.03 \u00b5s\n7.84 ms \u00b1 6.22 \u00b5s\n4.7 ms \u00b1 166 \u00b5s\n\n\n1024 x 1024\n34.1 ms \u00b1 803 \u00b5s\n11.5 ms \u00b1 6.24 \u00b5s\n273 ms \u00b1 6.7 ms\n\n\n2048 x 2048\n261 ms \u00b1 3.5 ms\n77.5 ms \u00b1 41.5 \u00b5s\n2.5 s \u00b1 97.6 ms\n\n\n4096 x 4096\n2.37 s \u00b1 154 ms\n636 ms \u00b1 2.97 \u00b5s\n25.9 s \u00b1 394 ms", "body": "Add the gpu kernel version.\r\n\r\nThe parallelism I went with performs poorly when there are a large number of vectors, but they're all short, as I don't allocate the thread pool to wrap in that case.\r\n\r\nTest Plan\r\n---------\r\n```\r\npython -m unittest test_torch.TestTorch.test_pdist_{empty,scipy} test_nn.TestNN.test_pdist{,_zeros,_empty_row,_empty_col,_cpu_gradgrad_unimplemented,_cuda_gradgrad_unimplemented} test_jit.TestJitGenerated.test_nn_pdist\r\n```\r\n\r\nCurrent performance specs are a little underwhelming, I'm in the process of debugging.\r\n\r\nsize | torch | torch cuda | scipy\r\n-----|-------|------------|------\r\n16 x 16 | 9.13 \u00b5s \u00b1 3.55 \u00b5s | 9.86 \u00b5s \u00b1 81.5 ns | 15.8 \u00b5s \u00b1 1.2 \u00b5s\r\n16 x 1024 | 15 \u00b5s \u00b1 224 ns | 9.48 \u00b5s \u00b1 88.7 ns | 88.7 \u00b5s \u00b1 8.83 \u00b5s\r\n1024 x 16 | 852 \u00b5s \u00b1 6.03 \u00b5s | 7.84 ms \u00b1 6.22 \u00b5s | 4.7 ms \u00b1 166 \u00b5s\r\n1024 x 1024 | 34.1 ms \u00b1 803 \u00b5s | 11.5 ms \u00b1 6.24 \u00b5s | 273 ms \u00b1 6.7 ms\r\n2048 x 2048 | 261 ms \u00b1 3.5 ms | 77.5 ms \u00b1 41.5 \u00b5s | 2.5 s \u00b1 97.6 ms\r\n4096 x 4096 | 2.37 s \u00b1 154 ms | 636 ms \u00b1 2.97 \u00b5s | 25.9 s \u00b1 394 ms\r\n"}