{"url": "https://api.github.com/repos/pytorch/pytorch/issues/3882", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/3882/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/3882/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/3882/events", "html_url": "https://github.com/pytorch/pytorch/issues/3882", "id": 276820725, "node_id": "MDU6SXNzdWUyNzY4MjA3MjU=", "number": 3882, "title": " Leaking dataloader", "user": {"login": "Randl", "id": 3028543, "node_id": "MDQ6VXNlcjMwMjg1NDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/3028543?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Randl", "html_url": "https://github.com/Randl", "followers_url": "https://api.github.com/users/Randl/followers", "following_url": "https://api.github.com/users/Randl/following{/other_user}", "gists_url": "https://api.github.com/users/Randl/gists{/gist_id}", "starred_url": "https://api.github.com/users/Randl/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Randl/subscriptions", "organizations_url": "https://api.github.com/users/Randl/orgs", "repos_url": "https://api.github.com/users/Randl/repos", "events_url": "https://api.github.com/users/Randl/events{/privacy}", "received_events_url": "https://api.github.com/users/Randl/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "SsnL", "id": 5674597, "node_id": "MDQ6VXNlcjU2NzQ1OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5674597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SsnL", "html_url": "https://github.com/SsnL", "followers_url": "https://api.github.com/users/SsnL/followers", "following_url": "https://api.github.com/users/SsnL/following{/other_user}", "gists_url": "https://api.github.com/users/SsnL/gists{/gist_id}", "starred_url": "https://api.github.com/users/SsnL/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SsnL/subscriptions", "organizations_url": "https://api.github.com/users/SsnL/orgs", "repos_url": "https://api.github.com/users/SsnL/repos", "events_url": "https://api.github.com/users/SsnL/events{/privacy}", "received_events_url": "https://api.github.com/users/SsnL/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2017-11-26T12:21:49Z", "updated_at": "2017-12-19T15:42:49Z", "closed_at": "2017-12-19T15:42:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I\u2019ve met a strange memory leak when I tried to implement \u201cImproved Training of Wasserstein GANs\u201d. I\u2019m getting OOM in the middle of second epoch both on CPU and GPU. Memory usage seems to increase after each batch, the profiling of CPU version points on the for loop over dataloader. Here is the kinda minimal example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n\n<span class=\"pl-k\">import</span> argparse\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> random\n\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torch.backends.cudnn <span class=\"pl-k\">as</span> cudnn\n<span class=\"pl-k\">import</span> torch.nn.parallel\n<span class=\"pl-k\">import</span> torch.optim <span class=\"pl-k\">as</span> optim\n<span class=\"pl-k\">import</span> torch.utils.data\n<span class=\"pl-k\">import</span> torchvision.datasets <span class=\"pl-k\">as</span> dset\n<span class=\"pl-k\">import</span> torchvision.transforms <span class=\"pl-k\">as</span> transforms\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> autograd\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\nparser <span class=\"pl-k\">=</span> argparse.ArgumentParser()\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--dataroot<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">required</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>path to dataset<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--workers<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>number of data loading workers<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--batchSize<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input batch size<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--imageSize<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>the height / width of the input image to network<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--nz<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>size of the latent z vector<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--num_gen_filters<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span># of gen filters in first conv layer<span class=\"pl-pds\">'</span></span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 64</span>\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--num_disc_filters<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span># of discrim filters in first conv layer<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--niter<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">25</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>number of epochs to train for<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--lr<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">float</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-4</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning rate, default=0.0002<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--beta1<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">float</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>beta1 for adam. default=0.5<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--cuda<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">action</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>store_true<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>enables cuda<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--ngpu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>number of GPUs to use<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--netG<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>path to netG (to continue training)<span class=\"pl-pds\">\"</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--netD<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>path to netD (to continue training)<span class=\"pl-pds\">\"</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--outf<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>./result<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>folder to output images and model checkpoints<span class=\"pl-pds\">'</span></span>)\nparser.add_argument(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>--manualSeed<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">type</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>, <span class=\"pl-v\">default</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">help</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>manual seed<span class=\"pl-pds\">'</span></span>)\n\nopt <span class=\"pl-k\">=</span> parser.parse_args()\ntorch.backends.cudnn.benchmark <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Generator</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_gen_filters</span>, <span class=\"pl-smi\">nz</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-smi\">num_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>):\n        <span class=\"pl-c1\">super</span>(Generator, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.num_gen_filters <span class=\"pl-k\">=</span> num_gen_filters\n        <span class=\"pl-c1\">self</span>.preprocess <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.Linear(nz, <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> num_gen_filters),\n            nn.BatchNorm2d(<span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> num_gen_filters),\n            nn.ReLU(<span class=\"pl-c1\">True</span>),\n        )\n        <span class=\"pl-c1\">self</span>.block1 <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.ConvTranspose2d(<span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> num_gen_filters, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> num_gen_filters, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>),\n            nn.BatchNorm2d(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> num_gen_filters),\n            nn.ReLU(<span class=\"pl-c1\">True</span>),\n        )\n        <span class=\"pl-c1\">self</span>.block2 <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.ConvTranspose2d(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> num_gen_filters, num_gen_filters, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>),\n            nn.BatchNorm2d(num_gen_filters),\n            nn.ReLU(<span class=\"pl-c1\">True</span>),\n        )\n        <span class=\"pl-c1\">self</span>.deconv_out <span class=\"pl-k\">=</span> nn.ConvTranspose2d(num_gen_filters, num_channels, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n        <span class=\"pl-c1\">self</span>.tanh <span class=\"pl-k\">=</span> nn.Tanh()\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.preprocess(<span class=\"pl-c1\">input</span>)\n        output <span class=\"pl-k\">=</span> output.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.num_gen_filters, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>)\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.block1(output)\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.block2(output)\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.deconv_out(output)\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.tanh(output)\n        <span class=\"pl-k\">return</span> output.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>)\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Discriminator</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_disc_filters</span>, <span class=\"pl-smi\">num_channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>):\n        <span class=\"pl-c1\">super</span>(Discriminator, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.num_disc_filters <span class=\"pl-k\">=</span> num_disc_filters\n        <span class=\"pl-c1\">self</span>.main <span class=\"pl-k\">=</span> nn.Sequential(\n            nn.Conv2d(num_channels, num_disc_filters, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n            nn.LeakyReLU(),\n            nn.Conv2d(num_disc_filters, <span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> num_disc_filters, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n            nn.LeakyReLU(),\n            nn.Conv2d(<span class=\"pl-c1\">2</span> <span class=\"pl-k\">*</span> num_disc_filters, <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> num_disc_filters, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>),\n            nn.LeakyReLU(),\n        )\n\n        <span class=\"pl-c1\">self</span>.linear <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> num_disc_filters, <span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input</span>):\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.main(<span class=\"pl-c1\">input</span>)\n        output <span class=\"pl-k\">=</span> output.view(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">4</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.num_disc_filters)\n        output <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.linear(output)\n        <span class=\"pl-k\">return</span> output\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span class=\"pl-c1\">print</span>(opt)\n\n    nz <span class=\"pl-k\">=</span> opt.nz\n\n    <span class=\"pl-k\">try</span>:\n        os.makedirs(opt.outf)\n    <span class=\"pl-k\">except</span> <span class=\"pl-c1\">OSError</span>:\n        <span class=\"pl-k\">pass</span>\n\n    <span class=\"pl-k\">if</span> opt.manualSeed <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n        opt.manualSeed <span class=\"pl-k\">=</span> random.randint(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10000</span>)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Random Seed: <span class=\"pl-pds\">\"</span></span>, opt.manualSeed)\n    random.seed(opt.manualSeed)\n    torch.manual_seed(opt.manualSeed)\n    <span class=\"pl-k\">if</span> opt.cuda:\n        torch.cuda.manual_seed_all(opt.manualSeed)\n\n    cudnn.benchmark <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n    <span class=\"pl-k\">if</span> torch.cuda.is_available() <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> opt.cuda:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>WARNING: You have a CUDA device, so you should probably run with --cuda<span class=\"pl-pds\">\"</span></span>)\n\n    dataset <span class=\"pl-k\">=</span> dset.CIFAR10(<span class=\"pl-v\">root</span><span class=\"pl-k\">=</span>opt.dataroot, <span class=\"pl-v\">download</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">transform</span><span class=\"pl-k\">=</span>transforms.Compose(\n        [transforms.Resize(opt.imageSize), transforms.ToTensor(),\n         transforms.Normalize((<span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>), (<span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>)), ]))\n    dataloader <span class=\"pl-k\">=</span> torch.utils.data.DataLoader(dataset, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>opt.batchSize,\n                                             <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">num_workers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>(opt.workers))\n\n    netG <span class=\"pl-k\">=</span> Generator(opt.num_gen_filters, nz)\n    <span class=\"pl-c1\">print</span>(netG)\n\n    netD <span class=\"pl-k\">=</span> Discriminator(opt.num_disc_filters)\n    <span class=\"pl-c1\">print</span>(netD)\n\n    noise <span class=\"pl-k\">=</span> torch.FloatTensor(opt.batchSize, nz, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>)\n    fixed_noise <span class=\"pl-k\">=</span> torch.FloatTensor(opt.batchSize, nz).normal_(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-k\">if</span> opt.cuda:\n        netD.cuda()\n        netG.cuda()\n        noise, fixed_noise <span class=\"pl-k\">=</span> noise.cuda(), fixed_noise.cuda()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> setup optimizer</span>\n    optimizerD <span class=\"pl-k\">=</span> optim.Adam(netD.parameters(), <span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span>opt.lr, <span class=\"pl-v\">betas</span><span class=\"pl-k\">=</span>(opt.beta1, <span class=\"pl-c1\">0.9</span>))\n\n    <span class=\"pl-k\">for</span> epoch <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(opt.niter):\n        <span class=\"pl-k\">for</span> _, data <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(dataloader, <span class=\"pl-c1\">0</span>):\n            <span class=\"pl-k\">for</span> p <span class=\"pl-k\">in</span> netG.parameters():  <span class=\"pl-c\"><span class=\"pl-c\">#</span> reset requires_grad</span>\n                p.requires_grad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> they are set to False below in netG update</span>\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> train with real</span>\n            netD.zero_grad()\n            real_cpu, _ <span class=\"pl-k\">=</span> data\n            batch_size <span class=\"pl-k\">=</span> real_cpu.size(<span class=\"pl-c1\">0</span>)\n            <span class=\"pl-k\">if</span> opt.cuda:\n                real_cpu <span class=\"pl-k\">=</span> real_cpu.cuda()\n\n            noise.resize_(batch_size, nz).normal_(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>)\n            noisev <span class=\"pl-k\">=</span> Variable(noise, <span class=\"pl-v\">volatile</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n            fake <span class=\"pl-k\">=</span> Variable(netG(noisev).data)\n\n            interpolates <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.5</span> <span class=\"pl-k\">*</span> real_cpu <span class=\"pl-k\">+</span> <span class=\"pl-c1\">0.5</span> <span class=\"pl-k\">*</span> fake.data\n\n            <span class=\"pl-k\">if</span> opt.cuda:\n                interpolates <span class=\"pl-k\">=</span> interpolates.cuda()\n            interpolates <span class=\"pl-k\">=</span> autograd.Variable(interpolates, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n            disc_interpolates <span class=\"pl-k\">=</span> netD(interpolates)\n\n            gradients <span class=\"pl-k\">=</span> autograd.grad(<span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>disc_interpolates, <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>interpolates,\n                                      <span class=\"pl-v\">grad_outputs</span><span class=\"pl-k\">=</span>torch.ones(\n                                          disc_interpolates.size()).cuda() <span class=\"pl-k\">if</span> opt.cuda <span class=\"pl-k\">else</span> torch.ones(\n                                          disc_interpolates.size()), <span class=\"pl-v\">create_graph</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">only_inputs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)[<span class=\"pl-c1\">0</span>]\n\n            gradient_penalty <span class=\"pl-k\">=</span> ((gradients.norm(<span class=\"pl-c1\">2</span>, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">**</span> <span class=\"pl-c1\">2</span>).mean() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">10</span>\n\n            gradient_penalty.backward()\n            optimizerD.step()\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>\n<p>From the discussion on <a href=\"https://discuss.pytorch.org/t/leaking-dataloader/10418\" rel=\"nofollow\">https://discuss.pytorch.org/t/leaking-dataloader/10418</a> I've found out that the code runs fine on  torch 0.2.0_3, while I'm using <a href=\"https://github.com/pytorch/pytorch/tree/8ebf18b5b1d57ef16c24366649e720867f394a98\">https://github.com/pytorch/pytorch/tree/8ebf18b5b1d57ef16c24366649e720867f394a98</a> built from source, with CUDA 8 and cudnn7, so I supposed it should be some recently introduced bug</p>", "body_text": "I\u2019ve met a strange memory leak when I tried to implement \u201cImproved Training of Wasserstein GANs\u201d. I\u2019m getting OOM in the middle of second epoch both on CPU and GPU. Memory usage seems to increase after each batch, the profiling of CPU version points on the for loop over dataloader. Here is the kinda minimal example:\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport random\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch import autograd\nfrom torch import nn\nfrom torch.autograd import Variable\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--dataroot', required=True, help='path to dataset')\nparser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\nparser.add_argument('--batchSize', type=int, default=128, help='input batch size')\nparser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to network')\nparser.add_argument('--nz', type=int, default=128, help='size of the latent z vector')\nparser.add_argument('--num_gen_filters', type=int, default=32, help='# of gen filters in first conv layer')  # 64\nparser.add_argument('--num_disc_filters', type=int, default=32, help='# of discrim filters in first conv layer')\nparser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\nparser.add_argument('--lr', type=float, default=1e-4, help='learning rate, default=0.0002')\nparser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\nparser.add_argument('--cuda', action='store_true', default=False, help='enables cuda')\nparser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\nparser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\nparser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\nparser.add_argument('--outf', default='./result', help='folder to output images and model checkpoints')\nparser.add_argument('--manualSeed', type=int, default=1, help='manual seed')\n\nopt = parser.parse_args()\ntorch.backends.cudnn.benchmark = True\n\n\nclass Generator(nn.Module):\n    def __init__(self, num_gen_filters, nz=128, num_channels=3):\n        super(Generator, self).__init__()\n        self.num_gen_filters = num_gen_filters\n        self.preprocess = nn.Sequential(\n            nn.Linear(nz, 4 * 4 * 4 * num_gen_filters),\n            nn.BatchNorm2d(4 * 4 * 4 * num_gen_filters),\n            nn.ReLU(True),\n        )\n        self.block1 = nn.Sequential(\n            nn.ConvTranspose2d(4 * num_gen_filters, 2 * num_gen_filters, 2, stride=2),\n            nn.BatchNorm2d(2 * num_gen_filters),\n            nn.ReLU(True),\n        )\n        self.block2 = nn.Sequential(\n            nn.ConvTranspose2d(2 * num_gen_filters, num_gen_filters, 2, stride=2),\n            nn.BatchNorm2d(num_gen_filters),\n            nn.ReLU(True),\n        )\n        self.deconv_out = nn.ConvTranspose2d(num_gen_filters, num_channels, 2, stride=2)\n        self.tanh = nn.Tanh()\n\n    def forward(self, input):\n        output = self.preprocess(input)\n        output = output.view(-1, 4 * self.num_gen_filters, 4, 4)\n        output = self.block1(output)\n        output = self.block2(output)\n        output = self.deconv_out(output)\n        output = self.tanh(output)\n        return output.view(-1, 3, 32, 32)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_disc_filters, num_channels=3):\n        super(Discriminator, self).__init__()\n        self.num_disc_filters = num_disc_filters\n        self.main = nn.Sequential(\n            nn.Conv2d(num_channels, num_disc_filters, 3, 2, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(num_disc_filters, 2 * num_disc_filters, 3, 2, padding=1),\n            nn.LeakyReLU(),\n            nn.Conv2d(2 * num_disc_filters, 4 * num_disc_filters, 3, 2, padding=1),\n            nn.LeakyReLU(),\n        )\n\n        self.linear = nn.Linear(4 * 4 * 4 * num_disc_filters, 1)\n\n    def forward(self, input):\n        output = self.main(input)\n        output = output.view(-1, 4 * 4 * 4 * self.num_disc_filters)\n        output = self.linear(output)\n        return output\n\n\ndef main():\n    print(opt)\n\n    nz = opt.nz\n\n    try:\n        os.makedirs(opt.outf)\n    except OSError:\n        pass\n\n    if opt.manualSeed is None:\n        opt.manualSeed = random.randint(1, 10000)\n    print(\"Random Seed: \", opt.manualSeed)\n    random.seed(opt.manualSeed)\n    torch.manual_seed(opt.manualSeed)\n    if opt.cuda:\n        torch.cuda.manual_seed_all(opt.manualSeed)\n\n    cudnn.benchmark = True\n\n    if torch.cuda.is_available() and not opt.cuda:\n        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n\n    dataset = dset.CIFAR10(root=opt.dataroot, download=True, transform=transforms.Compose(\n        [transforms.Resize(opt.imageSize), transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]))\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n                                             shuffle=True, num_workers=int(opt.workers))\n\n    netG = Generator(opt.num_gen_filters, nz)\n    print(netG)\n\n    netD = Discriminator(opt.num_disc_filters)\n    print(netD)\n\n    noise = torch.FloatTensor(opt.batchSize, nz, 1, 1)\n    fixed_noise = torch.FloatTensor(opt.batchSize, nz).normal_(0, 1)\n\n    if opt.cuda:\n        netD.cuda()\n        netG.cuda()\n        noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n\n    # setup optimizer\n    optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.9))\n\n    for epoch in range(opt.niter):\n        for _, data in enumerate(dataloader, 0):\n            for p in netG.parameters():  # reset requires_grad\n                p.requires_grad = False  # they are set to False below in netG update\n\n            # train with real\n            netD.zero_grad()\n            real_cpu, _ = data\n            batch_size = real_cpu.size(0)\n            if opt.cuda:\n                real_cpu = real_cpu.cuda()\n\n            noise.resize_(batch_size, nz).normal_(0, 1)\n            noisev = Variable(noise, volatile=True)\n            fake = Variable(netG(noisev).data)\n\n            interpolates = 0.5 * real_cpu + 0.5 * fake.data\n\n            if opt.cuda:\n                interpolates = interpolates.cuda()\n            interpolates = autograd.Variable(interpolates, requires_grad=True)\n\n            disc_interpolates = netD(interpolates)\n\n            gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n                                      grad_outputs=torch.ones(\n                                          disc_interpolates.size()).cuda() if opt.cuda else torch.ones(\n                                          disc_interpolates.size()), create_graph=True, only_inputs=True)[0]\n\n            gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * 10\n\n            gradient_penalty.backward()\n            optimizerD.step()\n\n\nif __name__ == '__main__':\n    main()\nFrom the discussion on https://discuss.pytorch.org/t/leaking-dataloader/10418 I've found out that the code runs fine on  torch 0.2.0_3, while I'm using https://github.com/pytorch/pytorch/tree/8ebf18b5b1d57ef16c24366649e720867f394a98 built from source, with CUDA 8 and cudnn7, so I supposed it should be some recently introduced bug", "body": "I\u2019ve met a strange memory leak when I tried to implement \u201cImproved Training of Wasserstein GANs\u201d. I\u2019m getting OOM in the middle of second epoch both on CPU and GPU. Memory usage seems to increase after each batch, the profiling of CPU version points on the for loop over dataloader. Here is the kinda minimal example:\r\n```python\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport os\r\nimport random\r\n\r\nimport torch\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.nn.parallel\r\nimport torch.optim as optim\r\nimport torch.utils.data\r\nimport torchvision.datasets as dset\r\nimport torchvision.transforms as transforms\r\nfrom torch import autograd\r\nfrom torch import nn\r\nfrom torch.autograd import Variable\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--dataroot', required=True, help='path to dataset')\r\nparser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\r\nparser.add_argument('--batchSize', type=int, default=128, help='input batch size')\r\nparser.add_argument('--imageSize', type=int, default=32, help='the height / width of the input image to network')\r\nparser.add_argument('--nz', type=int, default=128, help='size of the latent z vector')\r\nparser.add_argument('--num_gen_filters', type=int, default=32, help='# of gen filters in first conv layer')  # 64\r\nparser.add_argument('--num_disc_filters', type=int, default=32, help='# of discrim filters in first conv layer')\r\nparser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\r\nparser.add_argument('--lr', type=float, default=1e-4, help='learning rate, default=0.0002')\r\nparser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\r\nparser.add_argument('--cuda', action='store_true', default=False, help='enables cuda')\r\nparser.add_argument('--ngpu', type=int, default=1, help='number of GPUs to use')\r\nparser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\r\nparser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\r\nparser.add_argument('--outf', default='./result', help='folder to output images and model checkpoints')\r\nparser.add_argument('--manualSeed', type=int, default=1, help='manual seed')\r\n\r\nopt = parser.parse_args()\r\ntorch.backends.cudnn.benchmark = True\r\n\r\n\r\nclass Generator(nn.Module):\r\n    def __init__(self, num_gen_filters, nz=128, num_channels=3):\r\n        super(Generator, self).__init__()\r\n        self.num_gen_filters = num_gen_filters\r\n        self.preprocess = nn.Sequential(\r\n            nn.Linear(nz, 4 * 4 * 4 * num_gen_filters),\r\n            nn.BatchNorm2d(4 * 4 * 4 * num_gen_filters),\r\n            nn.ReLU(True),\r\n        )\r\n        self.block1 = nn.Sequential(\r\n            nn.ConvTranspose2d(4 * num_gen_filters, 2 * num_gen_filters, 2, stride=2),\r\n            nn.BatchNorm2d(2 * num_gen_filters),\r\n            nn.ReLU(True),\r\n        )\r\n        self.block2 = nn.Sequential(\r\n            nn.ConvTranspose2d(2 * num_gen_filters, num_gen_filters, 2, stride=2),\r\n            nn.BatchNorm2d(num_gen_filters),\r\n            nn.ReLU(True),\r\n        )\r\n        self.deconv_out = nn.ConvTranspose2d(num_gen_filters, num_channels, 2, stride=2)\r\n        self.tanh = nn.Tanh()\r\n\r\n    def forward(self, input):\r\n        output = self.preprocess(input)\r\n        output = output.view(-1, 4 * self.num_gen_filters, 4, 4)\r\n        output = self.block1(output)\r\n        output = self.block2(output)\r\n        output = self.deconv_out(output)\r\n        output = self.tanh(output)\r\n        return output.view(-1, 3, 32, 32)\r\n\r\n\r\nclass Discriminator(nn.Module):\r\n    def __init__(self, num_disc_filters, num_channels=3):\r\n        super(Discriminator, self).__init__()\r\n        self.num_disc_filters = num_disc_filters\r\n        self.main = nn.Sequential(\r\n            nn.Conv2d(num_channels, num_disc_filters, 3, 2, padding=1),\r\n            nn.LeakyReLU(),\r\n            nn.Conv2d(num_disc_filters, 2 * num_disc_filters, 3, 2, padding=1),\r\n            nn.LeakyReLU(),\r\n            nn.Conv2d(2 * num_disc_filters, 4 * num_disc_filters, 3, 2, padding=1),\r\n            nn.LeakyReLU(),\r\n        )\r\n\r\n        self.linear = nn.Linear(4 * 4 * 4 * num_disc_filters, 1)\r\n\r\n    def forward(self, input):\r\n        output = self.main(input)\r\n        output = output.view(-1, 4 * 4 * 4 * self.num_disc_filters)\r\n        output = self.linear(output)\r\n        return output\r\n\r\n\r\ndef main():\r\n    print(opt)\r\n\r\n    nz = opt.nz\r\n\r\n    try:\r\n        os.makedirs(opt.outf)\r\n    except OSError:\r\n        pass\r\n\r\n    if opt.manualSeed is None:\r\n        opt.manualSeed = random.randint(1, 10000)\r\n    print(\"Random Seed: \", opt.manualSeed)\r\n    random.seed(opt.manualSeed)\r\n    torch.manual_seed(opt.manualSeed)\r\n    if opt.cuda:\r\n        torch.cuda.manual_seed_all(opt.manualSeed)\r\n\r\n    cudnn.benchmark = True\r\n\r\n    if torch.cuda.is_available() and not opt.cuda:\r\n        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\r\n\r\n    dataset = dset.CIFAR10(root=opt.dataroot, download=True, transform=transforms.Compose(\r\n        [transforms.Resize(opt.imageSize), transforms.ToTensor(),\r\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]))\r\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\r\n                                             shuffle=True, num_workers=int(opt.workers))\r\n\r\n    netG = Generator(opt.num_gen_filters, nz)\r\n    print(netG)\r\n\r\n    netD = Discriminator(opt.num_disc_filters)\r\n    print(netD)\r\n\r\n    noise = torch.FloatTensor(opt.batchSize, nz, 1, 1)\r\n    fixed_noise = torch.FloatTensor(opt.batchSize, nz).normal_(0, 1)\r\n\r\n    if opt.cuda:\r\n        netD.cuda()\r\n        netG.cuda()\r\n        noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\r\n\r\n    # setup optimizer\r\n    optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.9))\r\n\r\n    for epoch in range(opt.niter):\r\n        for _, data in enumerate(dataloader, 0):\r\n            for p in netG.parameters():  # reset requires_grad\r\n                p.requires_grad = False  # they are set to False below in netG update\r\n\r\n            # train with real\r\n            netD.zero_grad()\r\n            real_cpu, _ = data\r\n            batch_size = real_cpu.size(0)\r\n            if opt.cuda:\r\n                real_cpu = real_cpu.cuda()\r\n\r\n            noise.resize_(batch_size, nz).normal_(0, 1)\r\n            noisev = Variable(noise, volatile=True)\r\n            fake = Variable(netG(noisev).data)\r\n\r\n            interpolates = 0.5 * real_cpu + 0.5 * fake.data\r\n\r\n            if opt.cuda:\r\n                interpolates = interpolates.cuda()\r\n            interpolates = autograd.Variable(interpolates, requires_grad=True)\r\n\r\n            disc_interpolates = netD(interpolates)\r\n\r\n            gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\r\n                                      grad_outputs=torch.ones(\r\n                                          disc_interpolates.size()).cuda() if opt.cuda else torch.ones(\r\n                                          disc_interpolates.size()), create_graph=True, only_inputs=True)[0]\r\n\r\n            gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * 10\r\n\r\n            gradient_penalty.backward()\r\n            optimizerD.step()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n\r\nFrom the discussion on https://discuss.pytorch.org/t/leaking-dataloader/10418 I've found out that the code runs fine on  torch 0.2.0_3, while I'm using https://github.com/pytorch/pytorch/tree/8ebf18b5b1d57ef16c24366649e720867f394a98 built from source, with CUDA 8 and cudnn7, so I supposed it should be some recently introduced bug"}