{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7021", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7021/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7021/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7021/events", "html_url": "https://github.com/pytorch/pytorch/issues/7021", "id": 318256236, "node_id": "MDU6SXNzdWUzMTgyNTYyMzY=", "number": 7021, "title": "Backproping through Embedding in 0.4.0 fails", "user": {"login": "twoertwein", "id": 6618166, "node_id": "MDQ6VXNlcjY2MTgxNjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6618166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/twoertwein", "html_url": "https://github.com/twoertwein", "followers_url": "https://api.github.com/users/twoertwein/followers", "following_url": "https://api.github.com/users/twoertwein/following{/other_user}", "gists_url": "https://api.github.com/users/twoertwein/gists{/gist_id}", "starred_url": "https://api.github.com/users/twoertwein/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/twoertwein/subscriptions", "organizations_url": "https://api.github.com/users/twoertwein/orgs", "repos_url": "https://api.github.com/users/twoertwein/repos", "events_url": "https://api.github.com/users/twoertwein/events{/privacy}", "received_events_url": "https://api.github.com/users/twoertwein/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-04-27T02:23:16Z", "updated_at": "2018-05-24T12:07:50Z", "closed_at": "2018-04-27T12:40:28Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>torch.nn.Embedding seems to cause issues in PyTorch 0.4.0 when backproping through it.</p>\n<h2>Code example</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nembedding <span class=\"pl-k\">=</span> torch.nn.Embedding(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">3</span>)\nembedding(torch.LongTensor([<span class=\"pl-c1\">0</span>]).requires_grad_())[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>].backward()</pre></div>\n<p>RuntimeError: No grad accumulator for a saved leaf!</p>\n<p>The following works:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\nlinear <span class=\"pl-k\">=</span> torch.nn.Linear(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">3</span>)\nlinear(torch.rand(<span class=\"pl-c1\">10</span>).requires_grad_())[<span class=\"pl-c1\">0</span>].backward()</pre></div>", "body_text": "Issue description\ntorch.nn.Embedding seems to cause issues in PyTorch 0.4.0 when backproping through it.\nCode example\nimport torch\nembedding = torch.nn.Embedding(10, 3)\nembedding(torch.LongTensor([0]).requires_grad_())[0, 0].backward()\nRuntimeError: No grad accumulator for a saved leaf!\nThe following works:\nimport torch\nlinear = torch.nn.Linear(10, 3)\nlinear(torch.rand(10).requires_grad_())[0].backward()", "body": "## Issue description\r\ntorch.nn.Embedding seems to cause issues in PyTorch 0.4.0 when backproping through it.\r\n\r\n## Code example\r\n```py\r\nimport torch\r\nembedding = torch.nn.Embedding(10, 3)\r\nembedding(torch.LongTensor([0]).requires_grad_())[0, 0].backward()\r\n```\r\nRuntimeError: No grad accumulator for a saved leaf!\r\n\r\nThe following works:\r\n\r\n```py\r\nimport torch\r\nlinear = torch.nn.Linear(10, 3)\r\nlinear(torch.rand(10).requires_grad_())[0].backward()\r\n```"}