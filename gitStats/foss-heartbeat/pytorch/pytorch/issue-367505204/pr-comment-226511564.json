{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/226511564", "pull_request_review_id": 166352111, "id": 226511564, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyNjUxMTU2NA==", "diff_hunk": "@@ -234,7 +235,10 @@ def pad_sequence(sequences, batch_first=False, padding_value=0):\n     # in sequences are same and fetching those from sequences[0]\n     max_size = sequences[0].size()\n     trailing_dims = max_size[1:]\n-    max_len = max([s.size(0) for s in sequences])\n+    max_len = max((s.size(0) for s in sequences))", "path": "torch/nn/utils/rnn.py", "position": 22, "original_position": 22, "commit_id": "27c6d1c0c40a6144e4307c51bb1be83b3d175e59", "original_commit_id": "c67052bf653636ac066e39b5ae183613c75d6dd1", "user": {"login": "Darktex", "id": 890615, "node_id": "MDQ6VXNlcjg5MDYxNQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/890615?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Darktex", "html_url": "https://github.com/Darktex", "followers_url": "https://api.github.com/users/Darktex/followers", "following_url": "https://api.github.com/users/Darktex/following{/other_user}", "gists_url": "https://api.github.com/users/Darktex/gists{/gist_id}", "starred_url": "https://api.github.com/users/Darktex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Darktex/subscriptions", "organizations_url": "https://api.github.com/users/Darktex/orgs", "repos_url": "https://api.github.com/users/Darktex/repos", "events_url": "https://api.github.com/users/Darktex/events{/privacy}", "received_events_url": "https://api.github.com/users/Darktex/received_events", "type": "User", "site_admin": false}, "body": "It actually works!\r\n\r\n`q = [torch.tensor([2132, 4600, 6038]),\r\n torch.tensor([   1,    1, 6094, 1289,   56, 1284,    1]),\r\n torch.tensor([   1, 3038,    1])]`\r\n\r\n`qq = pad_sequence( q, \r\n                  batch_first=True, \r\n                  padding_value=0,\r\n                  min_length = 11,\r\n)`\r\n\r\nreturns `tensor([[2132, 4600, 6038,    0,    0,    0,    0,    0,    0,    0,    0],\r\n        [   1,    1, 6094, 1289,   56, 1284,    1,    0,    0,    0,    0],\r\n        [   1, 3038,    1,    0,    0,    0,    0,    0,    0,    0,    0]])`\r\nof shape `torch.Size([3, 11])`\r\n\r\nand `qq = pad_sequence( q, \r\n                  batch_first=False, \r\n                  padding_value=dataset.text_encoder.stoi['<pad>'],\r\n                  min_length = 11,\r\n                  )`\r\n\r\nreturns `tensor([[2132,    1,    1],\r\n        [4600,    1, 3038],\r\n        [6038, 6094,    1],\r\n        [   0, 1289,    0],\r\n        [   0,   56,    0],\r\n        [   0, 1284,    0],\r\n        [   0,    1,    0],\r\n        [   0,    0,    0],\r\n        [   0,    0,    0],\r\n        [   0,    0,    0],\r\n        [   0,    0,    0]])`\r\nof `shape torch.Size([11, 3])`\r\n", "created_at": "2018-10-19T01:26:44Z", "updated_at": "2018-11-23T15:53:17Z", "html_url": "https://github.com/pytorch/pytorch/pull/12427#discussion_r226511564", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/12427", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/226511564"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/12427#discussion_r226511564"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/12427"}}, "body_html": "<p>It actually works!</p>\n<p><code>q = [torch.tensor([2132, 4600, 6038]), torch.tensor([   1,    1, 6094, 1289,   56, 1284,    1]), torch.tensor([   1, 3038,    1])]</code></p>\n<p><code>qq = pad_sequence( q,  batch_first=True,  padding_value=0, min_length = 11, )</code></p>\n<p>returns <code>tensor([[2132, 4600, 6038,    0,    0,    0,    0,    0,    0,    0,    0], [   1,    1, 6094, 1289,   56, 1284,    1,    0,    0,    0,    0], [   1, 3038,    1,    0,    0,    0,    0,    0,    0,    0,    0]])</code><br>\nof shape <code>torch.Size([3, 11])</code></p>\n<p>and <code>qq = pad_sequence( q,  batch_first=False,  padding_value=dataset.text_encoder.stoi['&lt;pad&gt;'], min_length = 11, )</code></p>\n<p>returns <code>tensor([[2132,    1,    1], [4600,    1, 3038], [6038, 6094,    1], [   0, 1289,    0], [   0,   56,    0], [   0, 1284,    0], [   0,    1,    0], [   0,    0,    0], [   0,    0,    0], [   0,    0,    0], [   0,    0,    0]])</code><br>\nof <code>shape torch.Size([11, 3])</code></p>", "body_text": "It actually works!\nq = [torch.tensor([2132, 4600, 6038]), torch.tensor([   1,    1, 6094, 1289,   56, 1284,    1]), torch.tensor([   1, 3038,    1])]\nqq = pad_sequence( q,  batch_first=True,  padding_value=0, min_length = 11, )\nreturns tensor([[2132, 4600, 6038,    0,    0,    0,    0,    0,    0,    0,    0], [   1,    1, 6094, 1289,   56, 1284,    1,    0,    0,    0,    0], [   1, 3038,    1,    0,    0,    0,    0,    0,    0,    0,    0]])\nof shape torch.Size([3, 11])\nand qq = pad_sequence( q,  batch_first=False,  padding_value=dataset.text_encoder.stoi['<pad>'], min_length = 11, )\nreturns tensor([[2132,    1,    1], [4600,    1, 3038], [6038, 6094,    1], [   0, 1289,    0], [   0,   56,    0], [   0, 1284,    0], [   0,    1,    0], [   0,    0,    0], [   0,    0,    0], [   0,    0,    0], [   0,    0,    0]])\nof shape torch.Size([11, 3])", "in_reply_to_id": 223222683}