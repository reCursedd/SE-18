{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218187856", "pull_request_review_id": 156070987, "id": 218187856, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxODE4Nzg1Ng==", "diff_hunk": "@@ -47,7 +47,9 @@ def __init__(self, probs=None, logits=None, validate_args=None):\n         if probs is not None:\n             if probs.dim() < 1:\n                 raise ValueError(\"`probs` parameter must be at least one-dimensional.\")\n-            self.probs = probs / probs.sum(-1, keepdim=True)\n+            sum_probs = probs.detach().cpu().numpy().sum(-1, keepdims=True)", "path": "torch/distributions/categorical.py", "position": 5, "original_position": 5, "commit_id": "af2a1b8fbc273d5095dffcf7ba886bdb46851b6c", "original_commit_id": "af2a1b8fbc273d5095dffcf7ba886bdb46851b6c", "user": {"login": "zuoxingdong", "id": 18168681, "node_id": "MDQ6VXNlcjE4MTY4Njgx", "avatar_url": "https://avatars0.githubusercontent.com/u/18168681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zuoxingdong", "html_url": "https://github.com/zuoxingdong", "followers_url": "https://api.github.com/users/zuoxingdong/followers", "following_url": "https://api.github.com/users/zuoxingdong/following{/other_user}", "gists_url": "https://api.github.com/users/zuoxingdong/gists{/gist_id}", "starred_url": "https://api.github.com/users/zuoxingdong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zuoxingdong/subscriptions", "organizations_url": "https://api.github.com/users/zuoxingdong/orgs", "repos_url": "https://api.github.com/users/zuoxingdong/repos", "events_url": "https://api.github.com/users/zuoxingdong/events{/privacy}", "received_events_url": "https://api.github.com/users/zuoxingdong/received_events", "type": "User", "site_admin": false}, "body": "@ezyang Thanks for the comment. In this case, I would think to remove the 're-normalize' operation and let user to take responsibility for the validity of probabilities. Because with this line, it is really really slow in certain cases e.g. I am training policy with ES to solve RL tasks, the discrete (`Categorical`) environment is extremely slow compared with continuous tasks (`Normal`), because we have to create distribution for each time step. In addition, before creating categorical distribution, the most common use cases are either to take a softmax and feed probabilities in or feed the logits in. Thus, I would vote for removing this line of code as the violation of validity is rare but the degrading of speed is very high. What do you think ?", "created_at": "2018-09-17T18:58:12Z", "updated_at": "2018-11-23T15:51:30Z", "html_url": "https://github.com/pytorch/pytorch/pull/11749#discussion_r218187856", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/11749", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/218187856"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/11749#discussion_r218187856"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/11749"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13564\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ezyang\">@ezyang</a> Thanks for the comment. In this case, I would think to remove the 're-normalize' operation and let user to take responsibility for the validity of probabilities. Because with this line, it is really really slow in certain cases e.g. I am training policy with ES to solve RL tasks, the discrete (<code>Categorical</code>) environment is extremely slow compared with continuous tasks (<code>Normal</code>), because we have to create distribution for each time step. In addition, before creating categorical distribution, the most common use cases are either to take a softmax and feed probabilities in or feed the logits in. Thus, I would vote for removing this line of code as the violation of validity is rare but the degrading of speed is very high. What do you think ?</p>", "body_text": "@ezyang Thanks for the comment. In this case, I would think to remove the 're-normalize' operation and let user to take responsibility for the validity of probabilities. Because with this line, it is really really slow in certain cases e.g. I am training policy with ES to solve RL tasks, the discrete (Categorical) environment is extremely slow compared with continuous tasks (Normal), because we have to create distribution for each time step. In addition, before creating categorical distribution, the most common use cases are either to take a softmax and feed probabilities in or feed the logits in. Thus, I would vote for removing this line of code as the violation of validity is rare but the degrading of speed is very high. What do you think ?", "in_reply_to_id": 218055627}