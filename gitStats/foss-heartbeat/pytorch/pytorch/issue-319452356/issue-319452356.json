{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7172", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7172/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7172/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7172/events", "html_url": "https://github.com/pytorch/pytorch/issues/7172", "id": 319452356, "node_id": "MDU6SXNzdWUzMTk0NTIzNTY=", "number": 7172, "title": "L1Loss / torch.abs gradient discrepancy", "user": {"login": "griegler", "id": 6870057, "node_id": "MDQ6VXNlcjY4NzAwNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/6870057?v=4", "gravatar_id": "", "url": "https://api.github.com/users/griegler", "html_url": "https://github.com/griegler", "followers_url": "https://api.github.com/users/griegler/followers", "following_url": "https://api.github.com/users/griegler/following{/other_user}", "gists_url": "https://api.github.com/users/griegler/gists{/gist_id}", "starred_url": "https://api.github.com/users/griegler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/griegler/subscriptions", "organizations_url": "https://api.github.com/users/griegler/orgs", "repos_url": "https://api.github.com/users/griegler/repos", "events_url": "https://api.github.com/users/griegler/events{/privacy}", "received_events_url": "https://api.github.com/users/griegler/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-02T08:11:32Z", "updated_at": "2018-05-02T09:59:45Z", "closed_at": "2018-05-02T09:59:44Z", "author_association": "NONE", "body_html": "<p>Given the simple example below</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\ndevice <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>\na <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float64, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>device, <span class=\"pl-v\">requires_grad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nb <span class=\"pl-k\">=</span> torch.tensor([<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>torch.float64, <span class=\"pl-v\">device</span><span class=\"pl-k\">=</span>device)\nloss <span class=\"pl-k\">=</span> torch.nn.functional.l1_loss(a, b, <span class=\"pl-v\">size_average</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nloss.backward()\n<span class=\"pl-c1\">print</span>(a.grad)\n</pre></div>\n<p>The output/gradient is <code>tensor([-1.,  1.,  1.], dtype=torch.float64)</code>. However, if the same loss is implemented with <code>torch.abs(a - b).sum()</code>, then the output/gradient is <code>tensor([-1.,  0.,  1.], dtype=torch.float64)</code>.</p>\n<p>While any value in [-1,1] is correct in the sense of the subgradient, the latter one is in my opinion much more practical. Especially, if I want to mask values in a regression task (<code>ma * a; ma * b</code>), I do want zero gradients for those masked entries.</p>", "body_text": "Given the simple example below\nimport torch\ndevice = 'cpu'\na = torch.tensor([-1,0,1], dtype=torch.float64, device=device, requires_grad=True)\nb = torch.tensor([0,0,0], dtype=torch.float64, device=device)\nloss = torch.nn.functional.l1_loss(a, b, size_average=False)\nloss.backward()\nprint(a.grad)\n\nThe output/gradient is tensor([-1.,  1.,  1.], dtype=torch.float64). However, if the same loss is implemented with torch.abs(a - b).sum(), then the output/gradient is tensor([-1.,  0.,  1.], dtype=torch.float64).\nWhile any value in [-1,1] is correct in the sense of the subgradient, the latter one is in my opinion much more practical. Especially, if I want to mask values in a regression task (ma * a; ma * b), I do want zero gradients for those masked entries.", "body": "Given the simple example below\r\n\r\n```python\r\nimport torch\r\ndevice = 'cpu'\r\na = torch.tensor([-1,0,1], dtype=torch.float64, device=device, requires_grad=True)\r\nb = torch.tensor([0,0,0], dtype=torch.float64, device=device)\r\nloss = torch.nn.functional.l1_loss(a, b, size_average=False)\r\nloss.backward()\r\nprint(a.grad)\r\n\r\n```\r\nThe output/gradient is `tensor([-1.,  1.,  1.], dtype=torch.float64)`. However, if the same loss is implemented with `torch.abs(a - b).sum()`, then the output/gradient is `tensor([-1.,  0.,  1.], dtype=torch.float64)`.\r\n\r\nWhile any value in [-1,1] is correct in the sense of the subgradient, the latter one is in my opinion much more practical. Especially, if I want to mask values in a regression task (`ma * a; ma * b`), I do want zero gradients for those masked entries."}