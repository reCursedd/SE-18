{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/385926865", "html_url": "https://github.com/pytorch/pytorch/issues/7172#issuecomment-385926865", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/7172", "id": 385926865, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTkyNjg2NQ==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-02T09:59:44Z", "updated_at": "2018-05-02T09:59:44Z", "author_association": "MEMBER", "body_html": "<p>Thanks for the proposal!</p>\n<p>Masking should still give you the correct gradients, irrespectively of what the loss returns. I don't think we want to change this, both are mathematically correct (or rather both are incorrect <g-emoji class=\"g-emoji\" alias=\"smile\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f604.png\">\ud83d\ude04</g-emoji>, there's no derivative at 0). If we were to do it the other way, we'd loose a bit of backward compatibility, and would get issues from people who want to get a <code>1.</code> subgradient at that point.</p>", "body_text": "Thanks for the proposal!\nMasking should still give you the correct gradients, irrespectively of what the loss returns. I don't think we want to change this, both are mathematically correct (or rather both are incorrect \ud83d\ude04, there's no derivative at 0). If we were to do it the other way, we'd loose a bit of backward compatibility, and would get issues from people who want to get a 1. subgradient at that point.", "body": "Thanks for the proposal!\r\n\r\nMasking should still give you the correct gradients, irrespectively of what the loss returns. I don't think we want to change this, both are mathematically correct (or rather both are incorrect \ud83d\ude04, there's no derivative at 0). If we were to do it the other way, we'd loose a bit of backward compatibility, and would get issues from people who want to get a `1.` subgradient at that point."}