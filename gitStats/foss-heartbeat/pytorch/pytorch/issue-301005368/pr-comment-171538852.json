{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/171538852", "pull_request_review_id": 100406941, "id": 171538852, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3MTUzODg1Mg==", "diff_hunk": "@@ -32,7 +34,7 @@ class Threshold(Module):\n         >>> print(m(input))\n     \"\"\"\n \n-    def __init__(self, threshold, value, inplace=False):\n+    def __init__(self, threshold=0.0, value=0.0, inplace=False):", "path": "torch/nn/modules/activation.py", "position": null, "original_position": 14, "commit_id": "2f0f0e3ca3fd541c8f799feb805f28db88ea27fa", "original_commit_id": "db62804b96e6e6c0989174ae260106a6a0e06f54", "user": {"login": "pmitros", "id": 1427775, "node_id": "MDQ6VXNlcjE0Mjc3NzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1427775?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pmitros", "html_url": "https://github.com/pmitros", "followers_url": "https://api.github.com/users/pmitros/followers", "following_url": "https://api.github.com/users/pmitros/following{/other_user}", "gists_url": "https://api.github.com/users/pmitros/gists{/gist_id}", "starred_url": "https://api.github.com/users/pmitros/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pmitros/subscriptions", "organizations_url": "https://api.github.com/users/pmitros/orgs", "repos_url": "https://api.github.com/users/pmitros/repos", "events_url": "https://api.github.com/users/pmitros/events{/privacy}", "received_events_url": "https://api.github.com/users/pmitros/received_events", "type": "User", "site_admin": false}, "body": "I'm responding to points above, but all of this is likely academic at this point, so please feel free to skip.\r\n\r\nre: Points 2, 3.\r\n\r\nIn retrospect, that's clearly outside of the scope of this PR. It was a mistake on my part to raise them here. I'm glad to go into more depth if desired.\r\n\r\nre: `Threshold`.\r\n\r\nI think a key issue is that the terminology might be a little ambiguous. In classical neural networks literature, 'Threshold' generally referred to a step function:\r\n\r\n![image](https://user-images.githubusercontent.com/1427775/36841530-9054dcb2-1d16-11e8-8357-ad3eb8230d87.png)\r\n\r\n([source](http://staff.itee.uq.edu.au/janetw/cmc/chapters/Introduction/): a random text from 1997)\r\n\r\nOther [references](http://slideplayer.com/slide/4799311/) refer to *any* non-linear activation as a threshold function. At the time, thresholds generally were, as you stated, non-zero. A Google Images search 'threshold function deep learning' and similar brings up zero plots as used in pytorch, so this is likely to be a point-of-confusion.\r\n\r\nIdeally, the documentation might explain:\r\n\r\n* What the function does at-a-glance;\r\n* Points of confusion around terminology;\r\n* Why it does what it does; and\r\n* When it ought to be used\r\n\r\nI think that's especially important with a function like this one, where:\r\n\r\n* in networks with bias terms, `Threshold` might often behave just like a `ReLU` (perhaps with slightly different initial conditions)\r\n* there is a historical body of texts using the terminology differently\r\n\r\nThe target of the documentation are people who might not already be familiar with this.\r\n\r\nre: Plots with args\r\n\r\nMy goal with the plots was to give information-at-a-glance. With that, I prefer less clutter. If I were putting more time into this, I'd probably go in the other direction:\r\n\r\n* Shrink the plots a little bit;\r\n* Add x=0, y=0 axes (faint);\r\n* Reduce the number of labelled ticks; and\r\n* Possibly, remove title entirely, since it is clear from the context\r\n\r\nAlternatively, I might clearly label the parameters on the image (e.g. an arrow pointing to the threshold, labelling it as such).\r\n\r\nIf more clarity on how parameters influence things is desired, interactives where one can play with the parameters might be the next step.", "created_at": "2018-03-01T12:06:17Z", "updated_at": "2018-11-23T15:40:11Z", "html_url": "https://github.com/pytorch/pytorch/pull/5457#discussion_r171538852", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5457", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/171538852"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5457#discussion_r171538852"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5457"}}, "body_html": "<p>I'm responding to points above, but all of this is likely academic at this point, so please feel free to skip.</p>\n<p>re: Points 2, 3.</p>\n<p>In retrospect, that's clearly outside of the scope of this PR. It was a mistake on my part to raise them here. I'm glad to go into more depth if desired.</p>\n<p>re: <code>Threshold</code>.</p>\n<p>I think a key issue is that the terminology might be a little ambiguous. In classical neural networks literature, 'Threshold' generally referred to a step function:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/1427775/36841530-9054dcb2-1d16-11e8-8357-ad3eb8230d87.png\"><img src=\"https://user-images.githubusercontent.com/1427775/36841530-9054dcb2-1d16-11e8-8357-ad3eb8230d87.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>(<a href=\"http://staff.itee.uq.edu.au/janetw/cmc/chapters/Introduction/\" rel=\"nofollow\">source</a>: a random text from 1997)</p>\n<p>Other <a href=\"http://slideplayer.com/slide/4799311/\" rel=\"nofollow\">references</a> refer to <em>any</em> non-linear activation as a threshold function. At the time, thresholds generally were, as you stated, non-zero. A Google Images search 'threshold function deep learning' and similar brings up zero plots as used in pytorch, so this is likely to be a point-of-confusion.</p>\n<p>Ideally, the documentation might explain:</p>\n<ul>\n<li>What the function does at-a-glance;</li>\n<li>Points of confusion around terminology;</li>\n<li>Why it does what it does; and</li>\n<li>When it ought to be used</li>\n</ul>\n<p>I think that's especially important with a function like this one, where:</p>\n<ul>\n<li>in networks with bias terms, <code>Threshold</code> might often behave just like a <code>ReLU</code> (perhaps with slightly different initial conditions)</li>\n<li>there is a historical body of texts using the terminology differently</li>\n</ul>\n<p>The target of the documentation are people who might not already be familiar with this.</p>\n<p>re: Plots with args</p>\n<p>My goal with the plots was to give information-at-a-glance. With that, I prefer less clutter. If I were putting more time into this, I'd probably go in the other direction:</p>\n<ul>\n<li>Shrink the plots a little bit;</li>\n<li>Add x=0, y=0 axes (faint);</li>\n<li>Reduce the number of labelled ticks; and</li>\n<li>Possibly, remove title entirely, since it is clear from the context</li>\n</ul>\n<p>Alternatively, I might clearly label the parameters on the image (e.g. an arrow pointing to the threshold, labelling it as such).</p>\n<p>If more clarity on how parameters influence things is desired, interactives where one can play with the parameters might be the next step.</p>", "body_text": "I'm responding to points above, but all of this is likely academic at this point, so please feel free to skip.\nre: Points 2, 3.\nIn retrospect, that's clearly outside of the scope of this PR. It was a mistake on my part to raise them here. I'm glad to go into more depth if desired.\nre: Threshold.\nI think a key issue is that the terminology might be a little ambiguous. In classical neural networks literature, 'Threshold' generally referred to a step function:\n\n(source: a random text from 1997)\nOther references refer to any non-linear activation as a threshold function. At the time, thresholds generally were, as you stated, non-zero. A Google Images search 'threshold function deep learning' and similar brings up zero plots as used in pytorch, so this is likely to be a point-of-confusion.\nIdeally, the documentation might explain:\n\nWhat the function does at-a-glance;\nPoints of confusion around terminology;\nWhy it does what it does; and\nWhen it ought to be used\n\nI think that's especially important with a function like this one, where:\n\nin networks with bias terms, Threshold might often behave just like a ReLU (perhaps with slightly different initial conditions)\nthere is a historical body of texts using the terminology differently\n\nThe target of the documentation are people who might not already be familiar with this.\nre: Plots with args\nMy goal with the plots was to give information-at-a-glance. With that, I prefer less clutter. If I were putting more time into this, I'd probably go in the other direction:\n\nShrink the plots a little bit;\nAdd x=0, y=0 axes (faint);\nReduce the number of labelled ticks; and\nPossibly, remove title entirely, since it is clear from the context\n\nAlternatively, I might clearly label the parameters on the image (e.g. an arrow pointing to the threshold, labelling it as such).\nIf more clarity on how parameters influence things is desired, interactives where one can play with the parameters might be the next step.", "in_reply_to_id": 171328160}