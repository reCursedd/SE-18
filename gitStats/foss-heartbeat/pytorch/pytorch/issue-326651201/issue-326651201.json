{"url": "https://api.github.com/repos/pytorch/pytorch/issues/7860", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/7860/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/7860/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/7860/events", "html_url": "https://github.com/pytorch/pytorch/issues/7860", "id": 326651201, "node_id": "MDU6SXNzdWUzMjY2NTEyMDE=", "number": 7860, "title": ".data and .detach ", "user": {"login": "easonnie", "id": 11016329, "node_id": "MDQ6VXNlcjExMDE2MzI5", "avatar_url": "https://avatars0.githubusercontent.com/u/11016329?v=4", "gravatar_id": "", "url": "https://api.github.com/users/easonnie", "html_url": "https://github.com/easonnie", "followers_url": "https://api.github.com/users/easonnie/followers", "following_url": "https://api.github.com/users/easonnie/following{/other_user}", "gists_url": "https://api.github.com/users/easonnie/gists{/gist_id}", "starred_url": "https://api.github.com/users/easonnie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/easonnie/subscriptions", "organizations_url": "https://api.github.com/users/easonnie/orgs", "repos_url": "https://api.github.com/users/easonnie/repos", "events_url": "https://api.github.com/users/easonnie/events{/privacy}", "received_events_url": "https://api.github.com/users/easonnie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-05-25T20:18:34Z", "updated_at": "2018-05-25T20:40:13Z", "closed_at": "2018-05-25T20:39:56Z", "author_association": "NONE", "body_html": "<p>I try to play the <code>detach</code> function with the code below.</p>\n<pre><code>    print(torch.__version__)\n\n    a = torch.Tensor([1, 2])\n    a.requires_grad_(True)\n\n    b = torch.Tensor([2, 3])\n    b.requires_grad_(True)\n\n    y = torch.Tensor([1, 1])\n    y.requires_grad_(True)\n\n    x = a * b\n    x_detach = x.detach()\n\n    print(\"x = a * b (a = 2, b = 3)\")\n    print(\"x require gard:\", x.requires_grad)\n    print(\"x.detach() require gard:\", x_detach.requires_grad)\n\n    print('-' * 6, \"Before Fill\", '-' * 6)\n    print(\"x:\", x)\n    print(\"x.detach():\", x_detach)\n\n    print('-' * 6, \"After x.detach().zero_()\", '-' * 6)\n    x_detach.zero_()\n    print(\"x:\", x)\n    print(\"x.detach():\", x_detach)\n\n    print(\"out = (x * y).sum()\")\n    print(\"x.backward()\")\n    out = (x * y).sum()\n    out.backward()\n\n    print(\"out:\", out)\n    print(\"x.grad:\", x.grad)\n    print(\"y.grad:\", y.grad)\n    print(\"a.grad:\", a.grad)\n    print(\"b.grad:\", b.grad)\n</code></pre>\n<p>The output is:</p>\n<pre><code>0.4.0\nx = a * b (a = 2, b = 3)\nx require gard: True\nx.detach() require gard: False\n------ Before Fill ------\nx: tensor([ 2.,  6.])\nx.detach(): tensor([ 2.,  6.])\n------ After x.detach().zero_() ------\nx: tensor([ 0.,  0.])\nx.detach(): tensor([ 0.,  0.])\nout = (x * y).sum()\nx.backward()\nout: tensor(0.)\nx.grad: None\ny.grad: tensor([ 0.,  0.])\na.grad: tensor([ 2.,  3.])\nb.grad: tensor([ 1.,  2.])\n</code></pre>\n<p>The results seem to contradict the description below:</p>\n<p><code>.data</code> was the primary way to get the underlying <code>Tensor</code> from a <code>Variable</code>. After this merge, calling <code>y = x.data</code> still has similar semantics. So <code>y</code> will be a Tensor that shares the same data with <code>x</code>, is unrelated with the computation history of <code>x</code>, and has <code>requires_grad=False</code>.</p>\n<p>However, <code>.data</code> can be unsafe in some cases. Any changes on <code>x.data</code> wouldn't be tracked by <code>autograd</code>, and the computed gradients would be incorrect if <code>x</code> is needed in a backward pass. A safer alternative is to use <code>x.detach()</code>, which also returns a <code>Tensor</code> that shares data with <code>requires_grad=False</code>, but will have its in-place changes reported by <code>autograd</code> if <code>x</code> is needed in backward.</p>\n<p>Shouldn't it raise an Exception?</p>", "body_text": "I try to play the detach function with the code below.\n    print(torch.__version__)\n\n    a = torch.Tensor([1, 2])\n    a.requires_grad_(True)\n\n    b = torch.Tensor([2, 3])\n    b.requires_grad_(True)\n\n    y = torch.Tensor([1, 1])\n    y.requires_grad_(True)\n\n    x = a * b\n    x_detach = x.detach()\n\n    print(\"x = a * b (a = 2, b = 3)\")\n    print(\"x require gard:\", x.requires_grad)\n    print(\"x.detach() require gard:\", x_detach.requires_grad)\n\n    print('-' * 6, \"Before Fill\", '-' * 6)\n    print(\"x:\", x)\n    print(\"x.detach():\", x_detach)\n\n    print('-' * 6, \"After x.detach().zero_()\", '-' * 6)\n    x_detach.zero_()\n    print(\"x:\", x)\n    print(\"x.detach():\", x_detach)\n\n    print(\"out = (x * y).sum()\")\n    print(\"x.backward()\")\n    out = (x * y).sum()\n    out.backward()\n\n    print(\"out:\", out)\n    print(\"x.grad:\", x.grad)\n    print(\"y.grad:\", y.grad)\n    print(\"a.grad:\", a.grad)\n    print(\"b.grad:\", b.grad)\n\nThe output is:\n0.4.0\nx = a * b (a = 2, b = 3)\nx require gard: True\nx.detach() require gard: False\n------ Before Fill ------\nx: tensor([ 2.,  6.])\nx.detach(): tensor([ 2.,  6.])\n------ After x.detach().zero_() ------\nx: tensor([ 0.,  0.])\nx.detach(): tensor([ 0.,  0.])\nout = (x * y).sum()\nx.backward()\nout: tensor(0.)\nx.grad: None\ny.grad: tensor([ 0.,  0.])\na.grad: tensor([ 2.,  3.])\nb.grad: tensor([ 1.,  2.])\n\nThe results seem to contradict the description below:\n.data was the primary way to get the underlying Tensor from a Variable. After this merge, calling y = x.data still has similar semantics. So y will be a Tensor that shares the same data with x, is unrelated with the computation history of x, and has requires_grad=False.\nHowever, .data can be unsafe in some cases. Any changes on x.data wouldn't be tracked by autograd, and the computed gradients would be incorrect if x is needed in a backward pass. A safer alternative is to use x.detach(), which also returns a Tensor that shares data with requires_grad=False, but will have its in-place changes reported by autograd if x is needed in backward.\nShouldn't it raise an Exception?", "body": "I try to play the `detach` function with the code below.\r\n\r\n```\r\n    print(torch.__version__)\r\n\r\n    a = torch.Tensor([1, 2])\r\n    a.requires_grad_(True)\r\n\r\n    b = torch.Tensor([2, 3])\r\n    b.requires_grad_(True)\r\n\r\n    y = torch.Tensor([1, 1])\r\n    y.requires_grad_(True)\r\n\r\n    x = a * b\r\n    x_detach = x.detach()\r\n\r\n    print(\"x = a * b (a = 2, b = 3)\")\r\n    print(\"x require gard:\", x.requires_grad)\r\n    print(\"x.detach() require gard:\", x_detach.requires_grad)\r\n\r\n    print('-' * 6, \"Before Fill\", '-' * 6)\r\n    print(\"x:\", x)\r\n    print(\"x.detach():\", x_detach)\r\n\r\n    print('-' * 6, \"After x.detach().zero_()\", '-' * 6)\r\n    x_detach.zero_()\r\n    print(\"x:\", x)\r\n    print(\"x.detach():\", x_detach)\r\n\r\n    print(\"out = (x * y).sum()\")\r\n    print(\"x.backward()\")\r\n    out = (x * y).sum()\r\n    out.backward()\r\n\r\n    print(\"out:\", out)\r\n    print(\"x.grad:\", x.grad)\r\n    print(\"y.grad:\", y.grad)\r\n    print(\"a.grad:\", a.grad)\r\n    print(\"b.grad:\", b.grad)\r\n```\r\n\r\nThe output is:\r\n```\r\n0.4.0\r\nx = a * b (a = 2, b = 3)\r\nx require gard: True\r\nx.detach() require gard: False\r\n------ Before Fill ------\r\nx: tensor([ 2.,  6.])\r\nx.detach(): tensor([ 2.,  6.])\r\n------ After x.detach().zero_() ------\r\nx: tensor([ 0.,  0.])\r\nx.detach(): tensor([ 0.,  0.])\r\nout = (x * y).sum()\r\nx.backward()\r\nout: tensor(0.)\r\nx.grad: None\r\ny.grad: tensor([ 0.,  0.])\r\na.grad: tensor([ 2.,  3.])\r\nb.grad: tensor([ 1.,  2.])\r\n```\r\n\r\nThe results seem to contradict the description below:\r\n\r\n`.data` was the primary way to get the underlying `Tensor` from a `Variable`. After this merge, calling `y = x.data` still has similar semantics. So `y` will be a Tensor that shares the same data with `x`, is unrelated with the computation history of `x`, and has `requires_grad=False`.\r\n\r\nHowever, `.data` can be unsafe in some cases. Any changes on `x.data` wouldn't be tracked by `autograd`, and the computed gradients would be incorrect if `x` is needed in a backward pass. A safer alternative is to use `x.detach()`, which also returns a `Tensor` that shares data with `requires_grad=False`, but will have its in-place changes reported by `autograd` if `x` is needed in backward.\r\n\r\nShouldn't it raise an Exception?"}