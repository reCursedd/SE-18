{"url": "https://api.github.com/repos/pytorch/pytorch/issues/1120", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/1120/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/1120/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/1120/events", "html_url": "https://github.com/pytorch/pytorch/issues/1120", "id": 217418696, "node_id": "MDU6SXNzdWUyMTc0MTg2OTY=", "number": 1120, "title": "multiprocessing deadlocks", "user": {"login": "FuriouslyCurious", "id": 8412277, "node_id": "MDQ6VXNlcjg0MTIyNzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8412277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FuriouslyCurious", "html_url": "https://github.com/FuriouslyCurious", "followers_url": "https://api.github.com/users/FuriouslyCurious/followers", "following_url": "https://api.github.com/users/FuriouslyCurious/following{/other_user}", "gists_url": "https://api.github.com/users/FuriouslyCurious/gists{/gist_id}", "starred_url": "https://api.github.com/users/FuriouslyCurious/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FuriouslyCurious/subscriptions", "organizations_url": "https://api.github.com/users/FuriouslyCurious/orgs", "repos_url": "https://api.github.com/users/FuriouslyCurious/repos", "events_url": "https://api.github.com/users/FuriouslyCurious/events{/privacy}", "received_events_url": "https://api.github.com/users/FuriouslyCurious/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 424131847, "node_id": "MDU6TGFiZWw0MjQxMzE4NDc=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/bug", "name": "bug", "color": "b60205", "default": true}, {"id": 443484135, "node_id": "MDU6TGFiZWw0NDM0ODQxMzU=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/high%20priority", "name": "high priority", "color": "F22613", "default": false}, {"id": 553773019, "node_id": "MDU6TGFiZWw1NTM3NzMwMTk=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/needs-reproduction", "name": "needs-reproduction", "color": "e99695", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2017-03-28T01:04:03Z", "updated_at": "2017-04-22T20:49:45Z", "closed_at": "2017-03-28T13:42:46Z", "author_association": "NONE", "body_html": "<p>I am experimenting with Soumith's imagenet example, but it is crashing or deadlocking in several ways.  I have added a bunch of \"print\" statements to it to figure out where it is crashing, and here is the GIST of full script: (as you can see, there are almost no significant modifications to the original code.)  All code is running on 2x NVidia Titan X 12 GB cards with 96 GB RAM.</p>\n<p><a href=\"https://gist.github.com/FuriouslyCurious/81742b8126f07f919522a588147e6086\">https://gist.github.com/FuriouslyCurious/81742b8126f07f919522a588147e6086</a></p>\n<h3>Issue 1: transforms.Scale(512) fails in THCTensorMathBlas.cu:241</h3>\n<p>How to reproduce:</p>\n<ol>\n<li>Images are being fed with transforms.Scale(512)  or transforms.Scale(1024)</li>\n<li>Source images are 2048x2048.</li>\n<li>Workers &gt;= 1</li>\n<li>Batchsize &gt;= 2</li>\n<li>Script will crash on its own in few minutes</li>\n</ol>\n<p>Output</p>\n<pre><code> python train.py -a resnet18 -j 1 -b 2 /home/FC/data/P/\n=&gt; Parsing complete...\n=&gt; creating model 'resnet18'\n=&gt; Using CUDA DataParallel\n=&gt; Starting training images loading...\n=&gt; Starting validation images loading...\n=&gt; Loss criterion and optimizer setup\n=&gt; Starting training...\n=&gt; Training Epoch 0\nTraceback (most recent call last):\n  File \"train.py\", line 299, in &lt;module&gt;\n    main()\n  File \"train.py\", line 140, in main\n    train(train_loader, model, criterion, optimizer, epoch)\n  File \"train.py\", line 177, in train\n    output = model(input_var)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 202, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 92, in forward\n    outputs = self.parallel_apply(replicas, scattered, gpu_dicts)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 102, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 50, in parallel_apply\n    raise output\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 30, in _worker\n    output = module(*input, **kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 202, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torchvision-0.1.6-py3.5.egg/torchvision/models/resnet.py\", line 150, in forward\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 202, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/linear.py\", line 54, in forward\n    return self._backend.Linear()(input, self.weight, self.bias)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/_functions/linear.py\", line 10, in forward\n    output.addmm_(0, 1, input, weight.t())\nRuntimeError: size mismatch at /data/users/soumith/miniconda2/conda-bld/pytorch-cuda80-0.1.10_1488757768560/work/torch/lib/THC/generic/THCTensorMathBlas.cu:241\n</code></pre>\n<h3>Issue 2: Multiple worker threads deadlock in index_queue.get() and waiter.acquire()</h3>\n<p>How to reproduce:</p>\n<ol>\n<li>Images are being fed with default crop: transforms.RandomSizedCrop(224)</li>\n<li>Source images are 2048x2048.</li>\n<li>Workers &gt; 2</li>\n<li>Batchsize &gt; 40</li>\n<li>When you see GPU clock speed fall to resting MHz on NVidia-smi, script has deadlocked in waiter.acquire() and index_queue.get().  Abort the script manually.</li>\n</ol>\n<pre><code>python train.py -a resnet18 /home/FC/data/P\n=&gt; Parsing complete...\n=&gt; creating model 'resnet18'\n=&gt; Using CUDA DataParallel\n=&gt; Starting training images loading...\n=&gt; Starting validation images loading...\n=&gt; Loss criterion and optimizer setup\n=&gt; Starting training...\n=&gt; Training Epoch 0\n^CProcess Process-4:\nProcess Process-3:\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"train.py\", line 299, in &lt;module&gt;\n    main()\n  File \"train.py\", line 140, in main\n    train(train_loader, model, criterion, optimizer, epoch)\n  File \"train.py\", line 168, in train\n    for i, (input, target) in enumerate(train_loader):\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 168, in __next__\n    idx, batch = self.data_queue.get()\n  File \"/conda3/envs/idp/lib/python3.5/queue.py\", line 164, in get\n    self.not_empty.wait()\n  File \"/conda3/envs/idp/lib/python3.5/threading.py\", line 293, in wait\n    waiter.acquire()\nTraceback (most recent call last):\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n    self.run()\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n    r = index_queue.get()\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n    with self._rlock:\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n    return self._semlock.__enter__()\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n    self.run()\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n    r = index_queue.get()\n</code></pre>\n<h3>Issue 3: Single Worker thread hangs in threading.py:293 waiter.acquire()</h3>\n<p>How to reproduce:</p>\n<ol>\n<li>Images are being fed with NO crop or scale</li>\n<li>Source images are 2048x2048.</li>\n<li>Workers &gt;= 1</li>\n<li>Batchsize &gt;= 1</li>\n<li>When you see GPU clock speed fall to resting MHz on NVidia-smi, script has stalled in waiter.acquire().  Manually abort the script.</li>\n</ol>\n<pre><code>python train.py -a resnet152 -j 1 -b 1 /home/FC/data/P/\n=&gt; Parsing complete...\n=&gt; creating model 'resnet152'\n=&gt; Using CUDA DataParallel\n=&gt; Starting training images loading...\n=&gt; Starting validation images loading...\n=&gt; Loss criterion and optimizer setup\n=&gt; Starting training...\n=&gt; Training Epoch 0\n^CTraceback (most recent call last):\n  File \"train.py\", line 298, in &lt;module&gt;\n    main()\n  File \"train.py\", line 139, in main\n    train(train_loader, model, criterion, optimizer, epoch)\n  File \"train.py\", line 167, in train\n    for i, (input, target) in enumerate(train_loader):\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 168, in __next__\n    idx, batch = self.data_queue.get()\n  File \"/conda3/envs/idp/lib/python3.5/queue.py\", line 164, in get\n    self.not_empty.wait()\n  File \"/conda3/envs/idp/lib/python3.5/threading.py\", line 293, in wait\n    waiter.acquire()\nKeyboardInterrupt\n</code></pre>", "body_text": "I am experimenting with Soumith's imagenet example, but it is crashing or deadlocking in several ways.  I have added a bunch of \"print\" statements to it to figure out where it is crashing, and here is the GIST of full script: (as you can see, there are almost no significant modifications to the original code.)  All code is running on 2x NVidia Titan X 12 GB cards with 96 GB RAM.\nhttps://gist.github.com/FuriouslyCurious/81742b8126f07f919522a588147e6086\nIssue 1: transforms.Scale(512) fails in THCTensorMathBlas.cu:241\nHow to reproduce:\n\nImages are being fed with transforms.Scale(512)  or transforms.Scale(1024)\nSource images are 2048x2048.\nWorkers >= 1\nBatchsize >= 2\nScript will crash on its own in few minutes\n\nOutput\n python train.py -a resnet18 -j 1 -b 2 /home/FC/data/P/\n=> Parsing complete...\n=> creating model 'resnet18'\n=> Using CUDA DataParallel\n=> Starting training images loading...\n=> Starting validation images loading...\n=> Loss criterion and optimizer setup\n=> Starting training...\n=> Training Epoch 0\nTraceback (most recent call last):\n  File \"train.py\", line 299, in <module>\n    main()\n  File \"train.py\", line 140, in main\n    train(train_loader, model, criterion, optimizer, epoch)\n  File \"train.py\", line 177, in train\n    output = model(input_var)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 202, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 92, in forward\n    outputs = self.parallel_apply(replicas, scattered, gpu_dicts)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 102, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 50, in parallel_apply\n    raise output\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 30, in _worker\n    output = module(*input, **kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 202, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torchvision-0.1.6-py3.5.egg/torchvision/models/resnet.py\", line 150, in forward\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 202, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/linear.py\", line 54, in forward\n    return self._backend.Linear()(input, self.weight, self.bias)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/_functions/linear.py\", line 10, in forward\n    output.addmm_(0, 1, input, weight.t())\nRuntimeError: size mismatch at /data/users/soumith/miniconda2/conda-bld/pytorch-cuda80-0.1.10_1488757768560/work/torch/lib/THC/generic/THCTensorMathBlas.cu:241\n\nIssue 2: Multiple worker threads deadlock in index_queue.get() and waiter.acquire()\nHow to reproduce:\n\nImages are being fed with default crop: transforms.RandomSizedCrop(224)\nSource images are 2048x2048.\nWorkers > 2\nBatchsize > 40\nWhen you see GPU clock speed fall to resting MHz on NVidia-smi, script has deadlocked in waiter.acquire() and index_queue.get().  Abort the script manually.\n\npython train.py -a resnet18 /home/FC/data/P\n=> Parsing complete...\n=> creating model 'resnet18'\n=> Using CUDA DataParallel\n=> Starting training images loading...\n=> Starting validation images loading...\n=> Loss criterion and optimizer setup\n=> Starting training...\n=> Training Epoch 0\n^CProcess Process-4:\nProcess Process-3:\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"train.py\", line 299, in <module>\n    main()\n  File \"train.py\", line 140, in main\n    train(train_loader, model, criterion, optimizer, epoch)\n  File \"train.py\", line 168, in train\n    for i, (input, target) in enumerate(train_loader):\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 168, in __next__\n    idx, batch = self.data_queue.get()\n  File \"/conda3/envs/idp/lib/python3.5/queue.py\", line 164, in get\n    self.not_empty.wait()\n  File \"/conda3/envs/idp/lib/python3.5/threading.py\", line 293, in wait\n    waiter.acquire()\nTraceback (most recent call last):\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n    self.run()\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n    r = index_queue.get()\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n    with self._rlock:\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n    return self._semlock.__enter__()\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n    self.run()\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\n    r = index_queue.get()\n\nIssue 3: Single Worker thread hangs in threading.py:293 waiter.acquire()\nHow to reproduce:\n\nImages are being fed with NO crop or scale\nSource images are 2048x2048.\nWorkers >= 1\nBatchsize >= 1\nWhen you see GPU clock speed fall to resting MHz on NVidia-smi, script has stalled in waiter.acquire().  Manually abort the script.\n\npython train.py -a resnet152 -j 1 -b 1 /home/FC/data/P/\n=> Parsing complete...\n=> creating model 'resnet152'\n=> Using CUDA DataParallel\n=> Starting training images loading...\n=> Starting validation images loading...\n=> Loss criterion and optimizer setup\n=> Starting training...\n=> Training Epoch 0\n^CTraceback (most recent call last):\n  File \"train.py\", line 298, in <module>\n    main()\n  File \"train.py\", line 139, in main\n    train(train_loader, model, criterion, optimizer, epoch)\n  File \"train.py\", line 167, in train\n    for i, (input, target) in enumerate(train_loader):\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 168, in __next__\n    idx, batch = self.data_queue.get()\n  File \"/conda3/envs/idp/lib/python3.5/queue.py\", line 164, in get\n    self.not_empty.wait()\n  File \"/conda3/envs/idp/lib/python3.5/threading.py\", line 293, in wait\n    waiter.acquire()\nKeyboardInterrupt", "body": "I am experimenting with Soumith's imagenet example, but it is crashing or deadlocking in several ways.  I have added a bunch of \"print\" statements to it to figure out where it is crashing, and here is the GIST of full script: (as you can see, there are almost no significant modifications to the original code.)  All code is running on 2x NVidia Titan X 12 GB cards with 96 GB RAM. \r\n\r\n\r\nhttps://gist.github.com/FuriouslyCurious/81742b8126f07f919522a588147e6086\r\n\r\n### Issue 1: transforms.Scale(512) fails in THCTensorMathBlas.cu:241\r\n\r\nHow to reproduce:   \r\n1. Images are being fed with transforms.Scale(512)  or transforms.Scale(1024)  \r\n2. Source images are 2048x2048.\r\n3. Workers >= 1\r\n4. Batchsize >= 2\r\n5. Script will crash on its own in few minutes\r\n\r\nOutput\r\n```\r\n python train.py -a resnet18 -j 1 -b 2 /home/FC/data/P/\r\n=> Parsing complete...\r\n=> creating model 'resnet18'\r\n=> Using CUDA DataParallel\r\n=> Starting training images loading...\r\n=> Starting validation images loading...\r\n=> Loss criterion and optimizer setup\r\n=> Starting training...\r\n=> Training Epoch 0\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 299, in <module>\r\n    main()\r\n  File \"train.py\", line 140, in main\r\n    train(train_loader, model, criterion, optimizer, epoch)\r\n  File \"train.py\", line 177, in train\r\n    output = model(input_var)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 202, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 92, in forward\r\n    outputs = self.parallel_apply(replicas, scattered, gpu_dicts)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/data_parallel.py\", line 102, in parallel_apply\r\n    return parallel_apply(replicas, inputs, kwargs)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 50, in parallel_apply\r\n    raise output\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/parallel/parallel_apply.py\", line 30, in _worker\r\n    output = module(*input, **kwargs)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 202, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torchvision-0.1.6-py3.5.egg/torchvision/models/resnet.py\", line 150, in forward\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/module.py\", line 202, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/modules/linear.py\", line 54, in forward\r\n    return self._backend.Linear()(input, self.weight, self.bias)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/nn/_functions/linear.py\", line 10, in forward\r\n    output.addmm_(0, 1, input, weight.t())\r\nRuntimeError: size mismatch at /data/users/soumith/miniconda2/conda-bld/pytorch-cuda80-0.1.10_1488757768560/work/torch/lib/THC/generic/THCTensorMathBlas.cu:241\r\n```\r\n\r\n\r\n\r\n### Issue 2: Multiple worker threads deadlock in index_queue.get() and waiter.acquire()\r\n\r\nHow to reproduce:   \r\n1. Images are being fed with default crop: transforms.RandomSizedCrop(224) \r\n2. Source images are 2048x2048.\r\n3. Workers > 2\r\n4. Batchsize > 40\r\n5. When you see GPU clock speed fall to resting MHz on NVidia-smi, script has deadlocked in waiter.acquire() and index_queue.get().  Abort the script manually.\r\n\r\n```\r\npython train.py -a resnet18 /home/FC/data/P\r\n=> Parsing complete...\r\n=> creating model 'resnet18'\r\n=> Using CUDA DataParallel\r\n=> Starting training images loading...\r\n=> Starting validation images loading...\r\n=> Loss criterion and optimizer setup\r\n=> Starting training...\r\n=> Training Epoch 0\r\n^CProcess Process-4:\r\nProcess Process-3:\r\nTraceback (most recent call last):\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 299, in <module>\r\n    main()\r\n  File \"train.py\", line 140, in main\r\n    train(train_loader, model, criterion, optimizer, epoch)\r\n  File \"train.py\", line 168, in train\r\n    for i, (input, target) in enumerate(train_loader):\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 168, in __next__\r\n    idx, batch = self.data_queue.get()\r\n  File \"/conda3/envs/idp/lib/python3.5/queue.py\", line 164, in get\r\n    self.not_empty.wait()\r\n  File \"/conda3/envs/idp/lib/python3.5/threading.py\", line 293, in wait\r\n    waiter.acquire()\r\nTraceback (most recent call last):\r\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\r\n    self.run()\r\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\r\n    r = index_queue.get()\r\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/queues.py\", line 342, in get\r\n    with self._rlock:\r\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\r\n    return self._semlock.__enter__()\r\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\r\n    self.run()\r\n  File \"/conda3/envs/idp/lib/python3.5/multiprocessing/process.py\", line 93, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 26, in _worker_loop\r\n    r = index_queue.get()\r\n```\r\n\r\n\r\n### Issue 3: Single Worker thread hangs in threading.py:293 waiter.acquire()\r\n\r\nHow to reproduce:   \r\n1. Images are being fed with NO crop or scale\r\n2. Source images are 2048x2048.\r\n3. Workers >= 1\r\n4. Batchsize >= 1\r\n5. When you see GPU clock speed fall to resting MHz on NVidia-smi, script has stalled in waiter.acquire().  Manually abort the script.\r\n\r\n\r\n```\r\npython train.py -a resnet152 -j 1 -b 1 /home/FC/data/P/\r\n=> Parsing complete...\r\n=> creating model 'resnet152'\r\n=> Using CUDA DataParallel\r\n=> Starting training images loading...\r\n=> Starting validation images loading...\r\n=> Loss criterion and optimizer setup\r\n=> Starting training...\r\n=> Training Epoch 0\r\n^CTraceback (most recent call last):\r\n  File \"train.py\", line 298, in <module>\r\n    main()\r\n  File \"train.py\", line 139, in main\r\n    train(train_loader, model, criterion, optimizer, epoch)\r\n  File \"train.py\", line 167, in train\r\n    for i, (input, target) in enumerate(train_loader):\r\n  File \"/conda3/envs/idp/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 168, in __next__\r\n    idx, batch = self.data_queue.get()\r\n  File \"/conda3/envs/idp/lib/python3.5/queue.py\", line 164, in get\r\n    self.not_empty.wait()\r\n  File \"/conda3/envs/idp/lib/python3.5/threading.py\", line 293, in wait\r\n    waiter.acquire()\r\nKeyboardInterrupt\r\n```\r\n"}