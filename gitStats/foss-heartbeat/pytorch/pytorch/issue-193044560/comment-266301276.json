{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/266301276", "html_url": "https://github.com/pytorch/pytorch/pull/286#issuecomment-266301276", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/286", "id": 266301276, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NjMwMTI3Ng==", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-11T19:20:32Z", "updated_at": "2016-12-11T19:20:32Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">I benchmarked a one element copy and it was slightly faster than the\nprevious code.\n\nWe also can't avoid the IsSubtype calls if we want to properly support\nsubclasses of tensors or storages.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sun, Dec 11, 2016 at 8:12 PM Adam Paszke ***@***.***&gt; wrote:\n ***@***.**** commented on this pull request.\n\n\n\n\n ------------------------------\n\n\n\n\n In torch/csrc/copy_utils.h\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"193044560\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/286\" href=\"https://github.com/pytorch/pytorch/pull/286#pullrequestreview-12372344\">#286 (review)</a>&gt;:\n\n\n &gt; @@ -0,0 +1,88 @@\n\n +#ifndef THP_COPY_UTILS_H\n\n +#define THP_COPY_UTILS_H\n\n +\n\n +#include &lt;functional&gt;\n\n +#include &lt;vector&gt;\n\n +#include \"Types.h\"\n\n +\n\n +\n\n +typedef std::function&lt;void(PyObject*, PyObject*)&gt; THPCopyFunction;\n\n +struct THPCopyInfo {\n\n +  PyTypeObject* srcType;  // Python type of src tensor/storage\n\n +  THPCopyFunction copy;   // copy function\n\n +  bool async;             // true if copy implements an 'async' copy\n\n +};\n\n +typedef std::vector&lt;THPCopyInfo&gt; THPCopyList;\n\n\n\n Are you sure it's a good idea to keep the functions in a vector? We have 8\n tensor types and with CPU + CUDA + distributed we need 72 handlers. If we\n were to add OpenCL we'd arrive at 144.\n\n\n I'm a bit afraid of calling PyType_IsSubtype 100 times for every copy\n call.\n\n\n\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"193044560\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/286\" href=\"https://github.com/pytorch/pytorch/pull/286#pullrequestreview-12372344\">#286 (review)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAoB-vABI8f9c-xEUGa4x-1-1RcLJI2-ks5rHEsngaJpZM4LCPlO\">https://github.com/notifications/unsubscribe-auth/AAoB-vABI8f9c-xEUGa4x-1-1RcLJI2-ks5rHEsngaJpZM4LCPlO</a>&gt;\n .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "I benchmarked a one element copy and it was slightly faster than the\nprevious code.\n\nWe also can't avoid the IsSubtype calls if we want to properly support\nsubclasses of tensors or storages.\n\u2026\nOn Sun, Dec 11, 2016 at 8:12 PM Adam Paszke ***@***.***> wrote:\n ***@***.**** commented on this pull request.\n\n\n\n\n ------------------------------\n\n\n\n\n In torch/csrc/copy_utils.h\n <#286 (review)>:\n\n\n > @@ -0,0 +1,88 @@\n\n +#ifndef THP_COPY_UTILS_H\n\n +#define THP_COPY_UTILS_H\n\n +\n\n +#include <functional>\n\n +#include <vector>\n\n +#include \"Types.h\"\n\n +\n\n +\n\n +typedef std::function<void(PyObject*, PyObject*)> THPCopyFunction;\n\n +struct THPCopyInfo {\n\n +  PyTypeObject* srcType;  // Python type of src tensor/storage\n\n +  THPCopyFunction copy;   // copy function\n\n +  bool async;             // true if copy implements an 'async' copy\n\n +};\n\n +typedef std::vector<THPCopyInfo> THPCopyList;\n\n\n\n Are you sure it's a good idea to keep the functions in a vector? We have 8\n tensor types and with CPU + CUDA + distributed we need 72 handlers. If we\n were to add OpenCL we'd arrive at 144.\n\n\n I'm a bit afraid of calling PyType_IsSubtype 100 times for every copy\n call.\n\n\n\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#286 (review)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAoB-vABI8f9c-xEUGa4x-1-1RcLJI2-ks5rHEsngaJpZM4LCPlO>\n .", "body": "I benchmarked a one element copy and it was slightly faster than the\nprevious code.\n\nWe also can't avoid the IsSubtype calls if we want to properly support\nsubclasses of tensors or storages.\n\nOn Sun, Dec 11, 2016 at 8:12 PM Adam Paszke <notifications@github.com>\nwrote:\n\n> *@apaszke* commented on this pull request.\n>\n>\n>\n>\n> ------------------------------\n>\n>\n>\n>\n> In torch/csrc/copy_utils.h\n> <https://github.com/pytorch/pytorch/pull/286#pullrequestreview-12372344>:\n>\n>\n> > @@ -0,0 +1,88 @@\n>\n> +#ifndef THP_COPY_UTILS_H\n>\n> +#define THP_COPY_UTILS_H\n>\n> +\n>\n> +#include <functional>\n>\n> +#include <vector>\n>\n> +#include \"Types.h\"\n>\n> +\n>\n> +\n>\n> +typedef std::function<void(PyObject*, PyObject*)> THPCopyFunction;\n>\n> +struct THPCopyInfo {\n>\n> +  PyTypeObject* srcType;  // Python type of src tensor/storage\n>\n> +  THPCopyFunction copy;   // copy function\n>\n> +  bool async;             // true if copy implements an 'async' copy\n>\n> +};\n>\n> +typedef std::vector<THPCopyInfo> THPCopyList;\n>\n>\n>\n> Are you sure it's a good idea to keep the functions in a vector? We have 8\n> tensor types and with CPU + CUDA + distributed we need 72 handlers. If we\n> were to add OpenCL we'd arrive at 144.\n>\n>\n> I'm a bit afraid of calling PyType_IsSubtype 100 times for every copy\n> call.\n>\n>\n>\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/pytorch/pytorch/pull/286#pullrequestreview-12372344>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAoB-vABI8f9c-xEUGa4x-1-1RcLJI2-ks5rHEsngaJpZM4LCPlO>\n> .\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n"}