{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8860", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8860/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8860/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8860/events", "html_url": "https://github.com/pytorch/pytorch/issues/8860", "id": 335521322, "node_id": "MDU6SXNzdWUzMzU1MjEzMjI=", "number": 8860, "title": "NaN loss when using half precision", "user": {"login": "alexge233", "id": 6159747, "node_id": "MDQ6VXNlcjYxNTk3NDc=", "avatar_url": "https://avatars2.githubusercontent.com/u/6159747?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexge233", "html_url": "https://github.com/alexge233", "followers_url": "https://api.github.com/users/alexge233/followers", "following_url": "https://api.github.com/users/alexge233/following{/other_user}", "gists_url": "https://api.github.com/users/alexge233/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexge233/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexge233/subscriptions", "organizations_url": "https://api.github.com/users/alexge233/orgs", "repos_url": "https://api.github.com/users/alexge233/repos", "events_url": "https://api.github.com/users/alexge233/events{/privacy}", "received_events_url": "https://api.github.com/users/alexge233/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-06-25T18:39:06Z", "updated_at": "2018-09-11T12:03:36Z", "closed_at": "2018-06-28T13:53:11Z", "author_association": "NONE", "body_html": "<p>Let me start by saying I've googled the issue and understand others have also run into it.<br>\nI've tried with Adam optimizer and by setting epsilon to <code>1e-4</code> and <code>1e-3</code> but after a few iterations my loss is still <code>NaN</code>.<br>\nBecause I'm looking at very large datasets and <code>float</code> to <code>half</code> does make a difference, is there any other trick I can try in order to keep using <code>half</code>?<br>\nOne of the posts on the forum suggests (<a href=\"https://discuss.pytorch.org/t/adam-half-precision-nans/1765\" rel=\"nofollow\">https://discuss.pytorch.org/t/adam-half-precision-nans/1765</a>) to keep a copy of the weights in <code>float32</code> but it is unclear as to how to do that?<br>\nThe network is a copy of HappyNet, with 5 convolutional layers, 2 fully connected one, and a softmax output.</p>", "body_text": "Let me start by saying I've googled the issue and understand others have also run into it.\nI've tried with Adam optimizer and by setting epsilon to 1e-4 and 1e-3 but after a few iterations my loss is still NaN.\nBecause I'm looking at very large datasets and float to half does make a difference, is there any other trick I can try in order to keep using half?\nOne of the posts on the forum suggests (https://discuss.pytorch.org/t/adam-half-precision-nans/1765) to keep a copy of the weights in float32 but it is unclear as to how to do that?\nThe network is a copy of HappyNet, with 5 convolutional layers, 2 fully connected one, and a softmax output.", "body": "Let me start by saying I've googled the issue and understand others have also run into it.\r\nI've tried with Adam optimizer and by setting epsilon to `1e-4` and `1e-3` but after a few iterations my loss is still `NaN`.\r\nBecause I'm looking at very large datasets and `float` to `half` does make a difference, is there any other trick I can try in order to keep using `half`? \r\nOne of the posts on the forum suggests (https://discuss.pytorch.org/t/adam-half-precision-nans/1765) to keep a copy of the weights in `float32` but it is unclear as to how to do that?\r\nThe network is a copy of HappyNet, with 5 convolutional layers, 2 fully connected one, and a softmax output.\r\n"}