{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162158708", "pull_request_review_id": 89567315, "id": 162158708, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MjE1ODcwOA==", "diff_hunk": "@@ -213,72 +136,51 @@ PyObject *THPEngine_run_backward(THPEngine *self, PyObject *args, PyObject *kwar\n     }\n   }\n \n-  Engine::pre_callback_map callbacks;\n-  CallbackContext ctx;\n+  function_list output_edges;\n   if (inputs != NULL) {\n-    THPUtils_assert(PyTuple_Check(inputs), \"inputs argument has to be a tuple\");\n     int num_inputs = PyTuple_GET_SIZE(inputs);\n-    ctx.outputs = PyTuple_New(num_inputs);\n-    if (!ctx.outputs) return NULL;\n-    // First, find all relevant functions and fill ctx.output_map\n+    output_edges.reserve(num_inputs);\n     for (int i = 0; i < num_inputs; ++i) {\n       PyObject *input = PyTuple_GET_ITEM(inputs, i);\n       THPUtils_assert(THPVariable_Check(input),\n           \"all inputs have to be Variables, but got %s\", THPUtils_typename(input));\n       THPVariable *input_var = (THPVariable*)input;\n-      auto grad_fn = input_var->cdata.grad_fn();\n       int output_nr = input_var->cdata.output_nr();\n-      bool is_leaf = !grad_fn;\n-      if (is_leaf) {\n+      auto grad_fn = input_var->cdata.grad_fn();\n+      if (!grad_fn) {\n           grad_fn = input_var->cdata.get()->grad_accumulator.lock();\n       }\n       THPUtils_assert(input_var->cdata.requires_grad(),\n           \"One of the differentiated Variables does not require grad\");\n-      if (allow_unreachable && !grad_fn) continue;\n-      THPUtils_assert(grad_fn,\n-          \"One of the differentiated Variables appears to not have been used in the graph\");\n-      auto& fn_info = ctx.output_map[grad_fn];\n-      fn_info.first.emplace_back(output_nr, i);\n-      fn_info.second = is_leaf;\n-    }\n-    // Register callbacks that will gather the outputs\n-    for (auto& entry : ctx.output_map) {\n-      auto& fn_info = entry.second;\n-      callbacks.emplace(entry.first.get(), [&ctx, &fn_info](Function* _unused, variable_list& grads) {\n-        auto& saved_outputs = fn_info.first;\n-        bool is_leaf = fn_info.second;\n-        AutoGIL gil;\n-        for (auto& saved_out : saved_outputs) {\n-          PyTuple_SET_ITEM(ctx.outputs.get(), saved_out.second,\n-            THPVariable_Wrap(grads[saved_out.first]));\n-        }\n-        // Suppress grad accumulation.\n-        // If the variable is a leaf, the next function to execute\n-        // is a grad_accumulator.  But when inputs != NULL, we should\n-        // NOT accumulate, so terminate execution.\n-        return !is_leaf;\n-      });\n-    }\n-    // Disable execution for all unneeded functions\n-    if (only_inputs) {\n-      compute_partial_exec_callbacks(roots, ctx, callbacks, allow_unreachable);\n+      if (!grad_fn) {\n+        output_edges.emplace_back();\n+      } else {\n+        THPUtils_assert(grad_fn,\n+            \"One of the differentiated Variables appears to not have been used in the graph\");\n+        output_edges.emplace_back(grad_fn, output_nr);\n+      }\n     }\n   }\n \n+  variable_list outputs;\n   {\n     AutoNoGIL no_gil;\n-    engine.execute(roots, grads, keep_graph, create_graph, callbacks);\n+    outputs = engine.execute(roots, grads, keep_graph, create_graph, output_edges);\n   }\n \n-  if (ctx.outputs) {\n-    for (int i = 0; i < PyTuple_GET_SIZE(inputs); i++) {\n-      // XXX: initializing tuples with NULL pointers might be a CPython\n-      // implementation detail\n-      if (PyTuple_GET_ITEM(ctx.outputs.get(), i)) continue;\n-      Py_INCREF(Py_None);\n-      PyTuple_SET_ITEM(ctx.outputs.get(), i, Py_None);\n+  if (inputs != NULL) {\n+    int num_inputs = PyTuple_GET_SIZE(inputs);\n+    THPObjectPtr py_outputs {PyTuple_New(num_inputs)};\n+    if (!py_outputs) return NULL;\n+    for (int i = 0; i < num_inputs; i++) {\n+      if (outputs[i].defined()) {", "path": "torch/csrc/autograd/python_engine.cpp", "position": null, "original_position": 203, "commit_id": "f38d6a066f6f6ba1e21497df4f994b97bb052cb2", "original_commit_id": "5c81e3e03d169b1a45582e7fd035e40e18d08e1b", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Is it `None` when it's undefined?", "created_at": "2018-01-17T19:43:28Z", "updated_at": "2018-11-23T15:38:16Z", "html_url": "https://github.com/pytorch/pytorch/pull/4690#discussion_r162158708", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/4690", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/162158708"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/4690#discussion_r162158708"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/4690"}}, "body_html": "<p>Is it <code>None</code> when it's undefined?</p>", "body_text": "Is it None when it's undefined?", "in_reply_to_id": 162119062}