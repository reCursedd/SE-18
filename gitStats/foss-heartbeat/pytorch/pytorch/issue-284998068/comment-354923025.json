{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/354923025", "html_url": "https://github.com/pytorch/pytorch/issues/4396#issuecomment-354923025", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4396", "id": 354923025, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDkyMzAyNQ==", "user": {"login": "jekbradbury", "id": 11729078, "node_id": "MDQ6VXNlcjExNzI5MDc4", "avatar_url": "https://avatars2.githubusercontent.com/u/11729078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jekbradbury", "html_url": "https://github.com/jekbradbury", "followers_url": "https://api.github.com/users/jekbradbury/followers", "following_url": "https://api.github.com/users/jekbradbury/following{/other_user}", "gists_url": "https://api.github.com/users/jekbradbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/jekbradbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jekbradbury/subscriptions", "organizations_url": "https://api.github.com/users/jekbradbury/orgs", "repos_url": "https://api.github.com/users/jekbradbury/repos", "events_url": "https://api.github.com/users/jekbradbury/events{/privacy}", "received_events_url": "https://api.github.com/users/jekbradbury/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-03T02:00:32Z", "updated_at": "2018-01-03T02:00:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I may also be experiencing a related leak, though I haven't really triaged yet; I get an OOM after a few dozen training iterations when I annotate the first of these modules (inside a larger network; the second module shows the direct context where it's used):</p>\n<pre><code>@torch.jit.compile(nderivs=1)\nclass LayerNorm(nn.Module):\n\n    def __init__(self, d_model, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.zeros(d_model))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nclass ResidualBlock(nn.Module):\n\n    def __init__(self, layer, d_model, drop_ratio, pos=0):\n        super(ResidualBlock, self).__init__()\n        self.layer = layer\n        self.dropout = nn.Dropout(drop_ratio)\n        self.layernorm = LayerNorm(d_model)\n        self.pos = pos\n\n    def forward(self, *x):\n        return self.layernorm(x[self.pos] + self.dropout(self.layer(*x)))\n</code></pre>\n<p>I also have to explicitly specify the <code>super</code> call Python 2-style; if I use <code>super()</code> I get:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/jit/__init__.py\", line 171, in __init__\n    old_init(self, *args, **kwargs)\n  File \"/home/james/transformer/model.py\", line 191, in __init__\n    super().__init__()\nTypeError: super(type, obj): obj must be an instance or subtype of type\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"run.py\", line 464, in &lt;module&gt;\n    model = args.model(SRC, TRG, args)\n  File \"/home/james/transformer/model.py\", line 858, in __init__\n    self.encoder = Encoder(src, args)\n  File \"/home/james/transformer/model.py\", line 534, in __init__\n    [EncoderLayer(args) for i in range(args.n_layers)])\n  File \"/home/james/transformer/model.py\", line 534, in &lt;listcomp&gt;\n    [EncoderLayer(args) for i in range(args.n_layers)])\n  File \"/home/james/transformer/model.py\", line 464, in __init__\n    args.d_model, args.drop_ratio)\n  File \"/home/james/transformer/model.py\", line 207, in __init__\n    self.layernorm = LayerNorm(d_model)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/jit/__init__.py\", line 177, in __init__\n    \"\\n\\nOriginal error: {}\".format(str(e))), e)\n  File \"&lt;string&gt;\", line 3, in raise_from\nTypeError: torch.jit.compile must be used as a class decorator; using it on an already defined class is not valid.\n\nOriginal error: super(type, obj): obj must be an instance or subtype of type\n</code></pre>", "body_text": "I may also be experiencing a related leak, though I haven't really triaged yet; I get an OOM after a few dozen training iterations when I annotate the first of these modules (inside a larger network; the second module shows the direct context where it's used):\n@torch.jit.compile(nderivs=1)\nclass LayerNorm(nn.Module):\n\n    def __init__(self, d_model, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.zeros(d_model))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nclass ResidualBlock(nn.Module):\n\n    def __init__(self, layer, d_model, drop_ratio, pos=0):\n        super(ResidualBlock, self).__init__()\n        self.layer = layer\n        self.dropout = nn.Dropout(drop_ratio)\n        self.layernorm = LayerNorm(d_model)\n        self.pos = pos\n\n    def forward(self, *x):\n        return self.layernorm(x[self.pos] + self.dropout(self.layer(*x)))\n\nI also have to explicitly specify the super call Python 2-style; if I use super() I get:\nTraceback (most recent call last):\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/jit/__init__.py\", line 171, in __init__\n    old_init(self, *args, **kwargs)\n  File \"/home/james/transformer/model.py\", line 191, in __init__\n    super().__init__()\nTypeError: super(type, obj): obj must be an instance or subtype of type\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"run.py\", line 464, in <module>\n    model = args.model(SRC, TRG, args)\n  File \"/home/james/transformer/model.py\", line 858, in __init__\n    self.encoder = Encoder(src, args)\n  File \"/home/james/transformer/model.py\", line 534, in __init__\n    [EncoderLayer(args) for i in range(args.n_layers)])\n  File \"/home/james/transformer/model.py\", line 534, in <listcomp>\n    [EncoderLayer(args) for i in range(args.n_layers)])\n  File \"/home/james/transformer/model.py\", line 464, in __init__\n    args.d_model, args.drop_ratio)\n  File \"/home/james/transformer/model.py\", line 207, in __init__\n    self.layernorm = LayerNorm(d_model)\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/jit/__init__.py\", line 177, in __init__\n    \"\\n\\nOriginal error: {}\".format(str(e))), e)\n  File \"<string>\", line 3, in raise_from\nTypeError: torch.jit.compile must be used as a class decorator; using it on an already defined class is not valid.\n\nOriginal error: super(type, obj): obj must be an instance or subtype of type", "body": "I may also be experiencing a related leak, though I haven't really triaged yet; I get an OOM after a few dozen training iterations when I annotate the first of these modules (inside a larger network; the second module shows the direct context where it's used):\r\n```\r\n@torch.jit.compile(nderivs=1)\r\nclass LayerNorm(nn.Module):\r\n\r\n    def __init__(self, d_model, eps=1e-6):\r\n        super(LayerNorm, self).__init__()\r\n        self.gamma = nn.Parameter(torch.ones(d_model))\r\n        self.beta = nn.Parameter(torch.zeros(d_model))\r\n        self.eps = eps\r\n\r\n    def forward(self, x):\r\n        mean = x.mean(-1, keepdim=True)\r\n        std = x.std(-1, keepdim=True)\r\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\r\n\r\nclass ResidualBlock(nn.Module):\r\n\r\n    def __init__(self, layer, d_model, drop_ratio, pos=0):\r\n        super(ResidualBlock, self).__init__()\r\n        self.layer = layer\r\n        self.dropout = nn.Dropout(drop_ratio)\r\n        self.layernorm = LayerNorm(d_model)\r\n        self.pos = pos\r\n\r\n    def forward(self, *x):\r\n        return self.layernorm(x[self.pos] + self.dropout(self.layer(*x)))\r\n```\r\nI also have to explicitly specify the `super` call Python 2-style; if I use `super()` I get:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/jit/__init__.py\", line 171, in __init__\r\n    old_init(self, *args, **kwargs)\r\n  File \"/home/james/transformer/model.py\", line 191, in __init__\r\n    super().__init__()\r\nTypeError: super(type, obj): obj must be an instance or subtype of type\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 464, in <module>\r\n    model = args.model(SRC, TRG, args)\r\n  File \"/home/james/transformer/model.py\", line 858, in __init__\r\n    self.encoder = Encoder(src, args)\r\n  File \"/home/james/transformer/model.py\", line 534, in __init__\r\n    [EncoderLayer(args) for i in range(args.n_layers)])\r\n  File \"/home/james/transformer/model.py\", line 534, in <listcomp>\r\n    [EncoderLayer(args) for i in range(args.n_layers)])\r\n  File \"/home/james/transformer/model.py\", line 464, in __init__\r\n    args.d_model, args.drop_ratio)\r\n  File \"/home/james/transformer/model.py\", line 207, in __init__\r\n    self.layernorm = LayerNorm(d_model)\r\n  File \"/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/jit/__init__.py\", line 177, in __init__\r\n    \"\\n\\nOriginal error: {}\".format(str(e))), e)\r\n  File \"<string>\", line 3, in raise_from\r\nTypeError: torch.jit.compile must be used as a class decorator; using it on an already defined class is not valid.\r\n\r\nOriginal error: super(type, obj): obj must be an instance or subtype of type\r\n```"}