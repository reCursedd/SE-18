{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/355176087", "html_url": "https://github.com/pytorch/pytorch/issues/4396#issuecomment-355176087", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4396", "id": 355176087, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTE3NjA4Nw==", "user": {"login": "deepbrain", "id": 10003025, "node_id": "MDQ6VXNlcjEwMDAzMDI1", "avatar_url": "https://avatars3.githubusercontent.com/u/10003025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepbrain", "html_url": "https://github.com/deepbrain", "followers_url": "https://api.github.com/users/deepbrain/followers", "following_url": "https://api.github.com/users/deepbrain/following{/other_user}", "gists_url": "https://api.github.com/users/deepbrain/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepbrain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepbrain/subscriptions", "organizations_url": "https://api.github.com/users/deepbrain/orgs", "repos_url": "https://api.github.com/users/deepbrain/repos", "events_url": "https://api.github.com/users/deepbrain/events{/privacy}", "received_events_url": "https://api.github.com/users/deepbrain/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-04T01:33:46Z", "updated_at": "2018-01-04T01:33:46Z", "author_association": "NONE", "body_html": "<p>I can confirm that in my tests it is not leaking memory anymore. Also, I looked at how jit instruments the classes and figured out what I was doing incorrectly. Now it all seems to work well. I am getting a 20% speedup on inference in a relatively simple model and 5% speedup during the learning. This is on a very fast Intel CPU 8700K and python2. I am guessing that the speedup will be greater with more complex nets, slower CPUs and python 3.</p>\n<p>I am using 2 separate asynchronous threads for the data loading and got the GPU utilization up to 90% when running the compiled version, which is 10-20% higher than without JIT. There is still some room for JIT improvement I guess to get it closer to 100%.</p>\n<p>I found an issue with using a GPU with ID != 0. If I run all of the .cuda calls with GPU argument of 1, I get an error:</p>\n<p>RuntimeError: arguments are located on different GPUs at</p>\n<p>../pytorch/aten/src/THC/generic/THCTensorMathPairwise.cu:259</p>\n<p>It happens only with the JIT class decorations on.<br>\nLet me know if you can reproduce this, if not, I will create a simplified test case for you.</p>\n<p>I will report it in a separate issue for you to track.</p>\n<p>Thank you for the quick memory leak fix.</p>", "body_text": "I can confirm that in my tests it is not leaking memory anymore. Also, I looked at how jit instruments the classes and figured out what I was doing incorrectly. Now it all seems to work well. I am getting a 20% speedup on inference in a relatively simple model and 5% speedup during the learning. This is on a very fast Intel CPU 8700K and python2. I am guessing that the speedup will be greater with more complex nets, slower CPUs and python 3.\nI am using 2 separate asynchronous threads for the data loading and got the GPU utilization up to 90% when running the compiled version, which is 10-20% higher than without JIT. There is still some room for JIT improvement I guess to get it closer to 100%.\nI found an issue with using a GPU with ID != 0. If I run all of the .cuda calls with GPU argument of 1, I get an error:\nRuntimeError: arguments are located on different GPUs at\n../pytorch/aten/src/THC/generic/THCTensorMathPairwise.cu:259\nIt happens only with the JIT class decorations on.\nLet me know if you can reproduce this, if not, I will create a simplified test case for you.\nI will report it in a separate issue for you to track.\nThank you for the quick memory leak fix.", "body": "I can confirm that in my tests it is not leaking memory anymore. Also, I looked at how jit instruments the classes and figured out what I was doing incorrectly. Now it all seems to work well. I am getting a 20% speedup on inference in a relatively simple model and 5% speedup during the learning. This is on a very fast Intel CPU 8700K and python2. I am guessing that the speedup will be greater with more complex nets, slower CPUs and python 3. \r\n\r\nI am using 2 separate asynchronous threads for the data loading and got the GPU utilization up to 90% when running the compiled version, which is 10-20% higher than without JIT. There is still some room for JIT improvement I guess to get it closer to 100%.\r\n\r\nI found an issue with using a GPU with ID != 0. If I run all of the .cuda calls with GPU argument of 1, I get an error:\r\n\r\nRuntimeError: arguments are located on different GPUs at \r\n\r\n ../pytorch/aten/src/THC/generic/THCTensorMathPairwise.cu:259\r\n\r\nIt happens only with the JIT class decorations on.\r\nLet me know if you can reproduce this, if not, I will create a simplified test case for you.\r\n\r\nI will report it in a separate issue for you to track.\r\n\r\nThank you for the quick memory leak fix.\r\n"}