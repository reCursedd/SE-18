{"url": "https://api.github.com/repos/pytorch/pytorch/issues/4396", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/4396/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/4396/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/4396/events", "html_url": "https://github.com/pytorch/pytorch/issues/4396", "id": 284998068, "node_id": "MDU6SXNzdWUyODQ5OTgwNjg=", "number": 4396, "title": "JIT leaks memory in nets with dropout layer ", "user": {"login": "deepbrain", "id": 10003025, "node_id": "MDQ6VXNlcjEwMDAzMDI1", "avatar_url": "https://avatars3.githubusercontent.com/u/10003025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepbrain", "html_url": "https://github.com/deepbrain", "followers_url": "https://api.github.com/users/deepbrain/followers", "following_url": "https://api.github.com/users/deepbrain/following{/other_user}", "gists_url": "https://api.github.com/users/deepbrain/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepbrain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepbrain/subscriptions", "organizations_url": "https://api.github.com/users/deepbrain/orgs", "repos_url": "https://api.github.com/users/deepbrain/repos", "events_url": "https://api.github.com/users/deepbrain/events{/privacy}", "received_events_url": "https://api.github.com/users/deepbrain/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-12-28T22:03:11Z", "updated_at": "2018-01-04T01:33:47Z", "closed_at": "2018-01-03T18:44:50Z", "author_association": "NONE", "body_html": "<p>if I add a dropout layer in FC layers, a JIT compiled forward pass of a net starts to seriously leak memory in the backward pass. I made a simple test case below. When it runs, you will see a rapidly increasing GPU/CPU memory usage in nvidia-smi/top and it will crash with an out of memory exception soon. Without the dropout layer or the backward pass, it works fine.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">from</span> torch <span class=\"pl-k\">import</span> jit\n<span class=\"pl-k\">import</span> torch.nn <span class=\"pl-k\">as</span> nn\n<span class=\"pl-k\">from</span> torch.autograd <span class=\"pl-k\">import</span> Variable\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">TestNet</span>(<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">Module</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(TestNet, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.net1 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">200</span>)\n        <span class=\"pl-c1\">self</span>.net2 <span class=\"pl-k\">=</span> nn.Linear(<span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">self</span>.sigmoid <span class=\"pl-k\">=</span> nn.Sigmoid()\n        <span class=\"pl-c1\">self</span>.ReLU <span class=\"pl-k\">=</span> nn.ReLU(<span class=\"pl-v\">inplace</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        <span class=\"pl-c1\">self</span>.drop <span class=\"pl-k\">=</span> nn.Dropout(<span class=\"pl-c1\">0.5</span>)\n               \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">V</span>):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.sigmoid(<span class=\"pl-c1\">self</span>.net2(<span class=\"pl-c1\">self</span>.drop(<span class=\"pl-c1\">self</span>.ReLU(<span class=\"pl-c1\">self</span>.net1(V))))).squeeze() \n\n\nuse_cuda <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\nnet <span class=\"pl-k\">=</span> TestNet()\ncriterion <span class=\"pl-k\">=</span> nn.BCELoss()\n<span class=\"pl-k\">if</span> use_cuda:\n    net.cuda()\n    criterion.cuda()\n    V <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>)).cuda()\n    label <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>)).cuda()\n<span class=\"pl-k\">else</span>:\n    V <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">100</span>))\n    label <span class=\"pl-k\">=</span> Variable(torch.randn(<span class=\"pl-c1\">100</span>))\n\nnet.train()\nfwd <span class=\"pl-k\">=</span> jit.compile(net.forward)\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1000000</span>):\n    r <span class=\"pl-k\">=</span> fwd(V)\n    err <span class=\"pl-k\">=</span> criterion(r, label)\n    err.backward()   \n</pre></div>", "body_text": "if I add a dropout layer in FC layers, a JIT compiled forward pass of a net starts to seriously leak memory in the backward pass. I made a simple test case below. When it runs, you will see a rapidly increasing GPU/CPU memory usage in nvidia-smi/top and it will crash with an out of memory exception soon. Without the dropout layer or the backward pass, it works fine.\nimport torch\nfrom torch import jit\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass TestNet(nn.Module):\n    def __init__(self):\n        super(TestNet, self).__init__()\n        self.net1 = nn.Linear(100, 200)\n        self.net2 = nn.Linear(200, 1)\n        self.sigmoid = nn.Sigmoid()\n        self.ReLU = nn.ReLU(inplace=False)\n        self.drop = nn.Dropout(0.5)\n               \n    def forward(self, V):\n        return self.sigmoid(self.net2(self.drop(self.ReLU(self.net1(V))))).squeeze() \n\n\nuse_cuda = True\nnet = TestNet()\ncriterion = nn.BCELoss()\nif use_cuda:\n    net.cuda()\n    criterion.cuda()\n    V = Variable(torch.randn(100, 100)).cuda()\n    label = Variable(torch.randn(100)).cuda()\nelse:\n    V = Variable(torch.randn(100, 100))\n    label = Variable(torch.randn(100))\n\nnet.train()\nfwd = jit.compile(net.forward)\nfor i in range(0,1000000):\n    r = fwd(V)\n    err = criterion(r, label)\n    err.backward()", "body": "if I add a dropout layer in FC layers, a JIT compiled forward pass of a net starts to seriously leak memory in the backward pass. I made a simple test case below. When it runs, you will see a rapidly increasing GPU/CPU memory usage in nvidia-smi/top and it will crash with an out of memory exception soon. Without the dropout layer or the backward pass, it works fine. \r\n\r\n```python\r\nimport torch\r\nfrom torch import jit\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\n\r\nclass TestNet(nn.Module):\r\n    def __init__(self):\r\n        super(TestNet, self).__init__()\r\n        self.net1 = nn.Linear(100, 200)\r\n        self.net2 = nn.Linear(200, 1)\r\n        self.sigmoid = nn.Sigmoid()\r\n        self.ReLU = nn.ReLU(inplace=False)\r\n        self.drop = nn.Dropout(0.5)\r\n               \r\n    def forward(self, V):\r\n        return self.sigmoid(self.net2(self.drop(self.ReLU(self.net1(V))))).squeeze() \r\n\r\n\r\nuse_cuda = True\r\nnet = TestNet()\r\ncriterion = nn.BCELoss()\r\nif use_cuda:\r\n    net.cuda()\r\n    criterion.cuda()\r\n    V = Variable(torch.randn(100, 100)).cuda()\r\n    label = Variable(torch.randn(100)).cuda()\r\nelse:\r\n    V = Variable(torch.randn(100, 100))\r\n    label = Variable(torch.randn(100))\r\n\r\nnet.train()\r\nfwd = jit.compile(net.forward)\r\nfor i in range(0,1000000):\r\n    r = fwd(V)\r\n    err = criterion(r, label)\r\n    err.backward()   \r\n\r\n```\r\n\r\n"}