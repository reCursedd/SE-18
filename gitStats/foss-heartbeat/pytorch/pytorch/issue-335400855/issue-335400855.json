{"url": "https://api.github.com/repos/pytorch/pytorch/issues/8847", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/8847/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/8847/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/8847/events", "html_url": "https://github.com/pytorch/pytorch/issues/8847", "id": 335400855, "node_id": "MDU6SXNzdWUzMzU0MDA4NTU=", "number": 8847, "title": "Runtime Error thrown when using Optimizer in a Pytorch Function: element 0 of tensors does not require grad and does not have a grad_fn", "user": {"login": "sbarratt", "id": 8305177, "node_id": "MDQ6VXNlcjgzMDUxNzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/8305177?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sbarratt", "html_url": "https://github.com/sbarratt", "followers_url": "https://api.github.com/users/sbarratt/followers", "following_url": "https://api.github.com/users/sbarratt/following{/other_user}", "gists_url": "https://api.github.com/users/sbarratt/gists{/gist_id}", "starred_url": "https://api.github.com/users/sbarratt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sbarratt/subscriptions", "organizations_url": "https://api.github.com/users/sbarratt/orgs", "repos_url": "https://api.github.com/users/sbarratt/repos", "events_url": "https://api.github.com/users/sbarratt/events{/privacy}", "received_events_url": "https://api.github.com/users/sbarratt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 443483881, "node_id": "MDU6TGFiZWw0NDM0ODM4ODE=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/todo", "name": "todo", "color": "c2e0c6", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-06-25T13:11:18Z", "updated_at": "2018-07-03T09:54:06Z", "closed_at": "2018-07-03T09:54:06Z", "author_association": "NONE", "body_html": "<h2>Issue description</h2>\n<p>When I am trying to run an optimizer on a separate variable inside a pytorch function, pytorch throws an error, <code>element 0 of tensors does not require grad and does not have a grad_fn</code>.</p>\n<h2>Code example</h2>\n<pre><code>import numpy as np\n\nfrom torch.autograd import Function, Variable\nimport torch\n\nN, n = 100, 2\nX = np.random.randn(N, n)\ny = np.random.randint(0,2,size=N)\nXv = Variable(torch.Tensor(X), requires_grad=True)\nyv = Variable(torch.Tensor(y), requires_grad=True)\n\ndef solve_logistic_regression(X, y, lamb):\n    N, n = X.shape\n    theta = Variable(torch.ones(n), requires_grad=True)\n    optimizer = torch.optim.LBFGS([theta], lr=.8)\n    def closure():\n        optimizer.zero_grad()\n        pi = 1./(1.+torch.exp(-X.mm(theta.unsqueeze(-1))))\n        loss = 1./N*torch.nn.BCELoss()(pi.squeeze(), y) + lamb/2*torch.norm(theta[:-1])**2\n        print (loss.item())\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n    return theta\n    \nclass LogisticRegression(Function):\n    @staticmethod\n    def forward(ctx, X, y, lamb):\n        theta = solve_logistic_regression(X, y, lamb)\n        return 0\n        \n    @staticmethod\n    def backward(ctx, grad_output):\n        return None, None, None\nlr = LogisticRegression.apply\n\n# this works\nsolve_logistic_regression(Xv.detach(),yv.detach(),.1)\n\n# this doesn't work\nlr(Xv.detach(),yv.detach(),.1)\n</code></pre>\n<h2>Error</h2>\n<pre><code>---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-26-618cf3a429a3&gt; in &lt;module&gt;()\n----&gt; 1 lr(Xv.detach(),yv.detach(),.1)\n\n&lt;ipython-input-24-5b3d43f3448b&gt; in forward(ctx, X, y, lamb)\n     16     @staticmethod\n     17     def forward(ctx, X, y, lamb):\n---&gt; 18         theta = solve_logistic_regression(X, y, lamb)\n     19         return 0\n     20 \n\n&lt;ipython-input-24-5b3d43f3448b&gt; in solve_logistic_regression(X, y, lamb)\n     10         loss.backward()\n     11         return loss\n---&gt; 12     optimizer.step(closure)\n     13     return theta\n     14 \n\n~/anaconda/lib/python3.6/site-packages/torch/optim/lbfgs.py in step(self, closure)\n    101 \n    102         # evaluate initial f(x) and df/dx\n--&gt; 103         orig_loss = closure()\n    104         loss = float(orig_loss)\n    105         current_evals = 1\n\n&lt;ipython-input-24-5b3d43f3448b&gt; in closure()\n      8         loss = 1./N*torch.nn.BCELoss()(pi.squeeze(), y) + lamb/2*torch.norm(theta[:-1])**2\n      9         print (loss.item())\n---&gt; 10         loss.backward()\n     11         return loss\n     12     optimizer.step(closure)\n\n~/anaconda/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\n     91                 products. Defaults to ``False``.\n     92         \"\"\"\n---&gt; 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n     94 \n     95     def register_hook(self, hook):\n\n~/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     87     Variable._execution_engine.run_backward(\n     88         tensors, grad_tensors, retain_graph, create_graph,\n---&gt; 89         allow_unreachable=True)  # allow_unreachable flag\n     90 \n     91 \n\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n</code></pre>\n<h2>System Info</h2>\n<pre><code>PyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.12.6\nGCC version: Could not collect\nCMake version: version 3.6.3\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.4)\n[pip3] numpydoc (0.6.0)\n[pip3] torch (0.4.0)\n[pip3] torchaudio (0.1)\n[pip3] torchvision (0.2.0)\n[conda] pytorch                   0.3.0           py36_cuda0.0_cudnn0.0h57b1bc9_4    pytorch\n[conda] torch                     0.4.0                     &lt;pip&gt;\n[conda] torch                     0.2.0+cd9b272             &lt;pip&gt;\n[conda] torchaudio                0.1                       &lt;pip&gt;\n[conda] torchvision               0.2.0            py36hf5eb7ec_1    pytorch\n</code></pre>", "body_text": "Issue description\nWhen I am trying to run an optimizer on a separate variable inside a pytorch function, pytorch throws an error, element 0 of tensors does not require grad and does not have a grad_fn.\nCode example\nimport numpy as np\n\nfrom torch.autograd import Function, Variable\nimport torch\n\nN, n = 100, 2\nX = np.random.randn(N, n)\ny = np.random.randint(0,2,size=N)\nXv = Variable(torch.Tensor(X), requires_grad=True)\nyv = Variable(torch.Tensor(y), requires_grad=True)\n\ndef solve_logistic_regression(X, y, lamb):\n    N, n = X.shape\n    theta = Variable(torch.ones(n), requires_grad=True)\n    optimizer = torch.optim.LBFGS([theta], lr=.8)\n    def closure():\n        optimizer.zero_grad()\n        pi = 1./(1.+torch.exp(-X.mm(theta.unsqueeze(-1))))\n        loss = 1./N*torch.nn.BCELoss()(pi.squeeze(), y) + lamb/2*torch.norm(theta[:-1])**2\n        print (loss.item())\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n    return theta\n    \nclass LogisticRegression(Function):\n    @staticmethod\n    def forward(ctx, X, y, lamb):\n        theta = solve_logistic_regression(X, y, lamb)\n        return 0\n        \n    @staticmethod\n    def backward(ctx, grad_output):\n        return None, None, None\nlr = LogisticRegression.apply\n\n# this works\nsolve_logistic_regression(Xv.detach(),yv.detach(),.1)\n\n# this doesn't work\nlr(Xv.detach(),yv.detach(),.1)\n\nError\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-26-618cf3a429a3> in <module>()\n----> 1 lr(Xv.detach(),yv.detach(),.1)\n\n<ipython-input-24-5b3d43f3448b> in forward(ctx, X, y, lamb)\n     16     @staticmethod\n     17     def forward(ctx, X, y, lamb):\n---> 18         theta = solve_logistic_regression(X, y, lamb)\n     19         return 0\n     20 \n\n<ipython-input-24-5b3d43f3448b> in solve_logistic_regression(X, y, lamb)\n     10         loss.backward()\n     11         return loss\n---> 12     optimizer.step(closure)\n     13     return theta\n     14 \n\n~/anaconda/lib/python3.6/site-packages/torch/optim/lbfgs.py in step(self, closure)\n    101 \n    102         # evaluate initial f(x) and df/dx\n--> 103         orig_loss = closure()\n    104         loss = float(orig_loss)\n    105         current_evals = 1\n\n<ipython-input-24-5b3d43f3448b> in closure()\n      8         loss = 1./N*torch.nn.BCELoss()(pi.squeeze(), y) + lamb/2*torch.norm(theta[:-1])**2\n      9         print (loss.item())\n---> 10         loss.backward()\n     11         return loss\n     12     optimizer.step(closure)\n\n~/anaconda/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\n     91                 products. Defaults to ``False``.\n     92         \"\"\"\n---> 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n     94 \n     95     def register_hook(self, hook):\n\n~/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     87     Variable._execution_engine.run_backward(\n     88         tensors, grad_tensors, retain_graph, create_graph,\n---> 89         allow_unreachable=True)  # allow_unreachable flag\n     90 \n     91 \n\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n\nSystem Info\nPyTorch version: 0.4.0\nIs debug build: No\nCUDA used to build PyTorch: None\n\nOS: Mac OSX 10.12.6\nGCC version: Could not collect\nCMake version: version 3.6.3\n\nPython version: 3.6\nIs CUDA available: No\nCUDA runtime version: No CUDA\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\n\nVersions of relevant libraries:\n[pip3] numpy (1.14.4)\n[pip3] numpydoc (0.6.0)\n[pip3] torch (0.4.0)\n[pip3] torchaudio (0.1)\n[pip3] torchvision (0.2.0)\n[conda] pytorch                   0.3.0           py36_cuda0.0_cudnn0.0h57b1bc9_4    pytorch\n[conda] torch                     0.4.0                     <pip>\n[conda] torch                     0.2.0+cd9b272             <pip>\n[conda] torchaudio                0.1                       <pip>\n[conda] torchvision               0.2.0            py36hf5eb7ec_1    pytorch", "body": "## Issue description\r\n\r\nWhen I am trying to run an optimizer on a separate variable inside a pytorch function, pytorch throws an error, ```element 0 of tensors does not require grad and does not have a grad_fn```.\r\n\r\n## Code example\r\n\r\n```\r\nimport numpy as np\r\n\r\nfrom torch.autograd import Function, Variable\r\nimport torch\r\n\r\nN, n = 100, 2\r\nX = np.random.randn(N, n)\r\ny = np.random.randint(0,2,size=N)\r\nXv = Variable(torch.Tensor(X), requires_grad=True)\r\nyv = Variable(torch.Tensor(y), requires_grad=True)\r\n\r\ndef solve_logistic_regression(X, y, lamb):\r\n    N, n = X.shape\r\n    theta = Variable(torch.ones(n), requires_grad=True)\r\n    optimizer = torch.optim.LBFGS([theta], lr=.8)\r\n    def closure():\r\n        optimizer.zero_grad()\r\n        pi = 1./(1.+torch.exp(-X.mm(theta.unsqueeze(-1))))\r\n        loss = 1./N*torch.nn.BCELoss()(pi.squeeze(), y) + lamb/2*torch.norm(theta[:-1])**2\r\n        print (loss.item())\r\n        loss.backward()\r\n        return loss\r\n    optimizer.step(closure)\r\n    return theta\r\n    \r\nclass LogisticRegression(Function):\r\n    @staticmethod\r\n    def forward(ctx, X, y, lamb):\r\n        theta = solve_logistic_regression(X, y, lamb)\r\n        return 0\r\n        \r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        return None, None, None\r\nlr = LogisticRegression.apply\r\n\r\n# this works\r\nsolve_logistic_regression(Xv.detach(),yv.detach(),.1)\r\n\r\n# this doesn't work\r\nlr(Xv.detach(),yv.detach(),.1)\r\n```\r\n\r\n## Error\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-26-618cf3a429a3> in <module>()\r\n----> 1 lr(Xv.detach(),yv.detach(),.1)\r\n\r\n<ipython-input-24-5b3d43f3448b> in forward(ctx, X, y, lamb)\r\n     16     @staticmethod\r\n     17     def forward(ctx, X, y, lamb):\r\n---> 18         theta = solve_logistic_regression(X, y, lamb)\r\n     19         return 0\r\n     20 \r\n\r\n<ipython-input-24-5b3d43f3448b> in solve_logistic_regression(X, y, lamb)\r\n     10         loss.backward()\r\n     11         return loss\r\n---> 12     optimizer.step(closure)\r\n     13     return theta\r\n     14 \r\n\r\n~/anaconda/lib/python3.6/site-packages/torch/optim/lbfgs.py in step(self, closure)\r\n    101 \r\n    102         # evaluate initial f(x) and df/dx\r\n--> 103         orig_loss = closure()\r\n    104         loss = float(orig_loss)\r\n    105         current_evals = 1\r\n\r\n<ipython-input-24-5b3d43f3448b> in closure()\r\n      8         loss = 1./N*torch.nn.BCELoss()(pi.squeeze(), y) + lamb/2*torch.norm(theta[:-1])**2\r\n      9         print (loss.item())\r\n---> 10         loss.backward()\r\n     11         return loss\r\n     12     optimizer.step(closure)\r\n\r\n~/anaconda/lib/python3.6/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n     91                 products. Defaults to ``False``.\r\n     92         \"\"\"\r\n---> 93         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n     94 \r\n     95     def register_hook(self, hook):\r\n\r\n~/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n     87     Variable._execution_engine.run_backward(\r\n     88         tensors, grad_tensors, retain_graph, create_graph,\r\n---> 89         allow_unreachable=True)  # allow_unreachable flag\r\n     90 \r\n     91 \r\n\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```\r\n\r\n## System Info\r\n\r\n```\r\nPyTorch version: 0.4.0\r\nIs debug build: No\r\nCUDA used to build PyTorch: None\r\n\r\nOS: Mac OSX 10.12.6\r\nGCC version: Could not collect\r\nCMake version: version 3.6.3\r\n\r\nPython version: 3.6\r\nIs CUDA available: No\r\nCUDA runtime version: No CUDA\r\nGPU models and configuration: No CUDA\r\nNvidia driver version: No CUDA\r\ncuDNN version: No CUDA\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy (1.14.4)\r\n[pip3] numpydoc (0.6.0)\r\n[pip3] torch (0.4.0)\r\n[pip3] torchaudio (0.1)\r\n[pip3] torchvision (0.2.0)\r\n[conda] pytorch                   0.3.0           py36_cuda0.0_cudnn0.0h57b1bc9_4    pytorch\r\n[conda] torch                     0.4.0                     <pip>\r\n[conda] torch                     0.2.0+cd9b272             <pip>\r\n[conda] torchaudio                0.1                       <pip>\r\n[conda] torchvision               0.2.0            py36hf5eb7ec_1    pytorch\r\n```\r\n"}