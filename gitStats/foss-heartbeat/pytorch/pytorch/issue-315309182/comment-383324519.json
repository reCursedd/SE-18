{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/383324519", "html_url": "https://github.com/pytorch/pytorch/issues/6694#issuecomment-383324519", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/6694", "id": 383324519, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzMyNDUxOQ==", "user": {"login": "HMEIatJHU", "id": 19693633, "node_id": "MDQ6VXNlcjE5NjkzNjMz", "avatar_url": "https://avatars1.githubusercontent.com/u/19693633?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HMEIatJHU", "html_url": "https://github.com/HMEIatJHU", "followers_url": "https://api.github.com/users/HMEIatJHU/followers", "following_url": "https://api.github.com/users/HMEIatJHU/following{/other_user}", "gists_url": "https://api.github.com/users/HMEIatJHU/gists{/gist_id}", "starred_url": "https://api.github.com/users/HMEIatJHU/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HMEIatJHU/subscriptions", "organizations_url": "https://api.github.com/users/HMEIatJHU/orgs", "repos_url": "https://api.github.com/users/HMEIatJHU/repos", "events_url": "https://api.github.com/users/HMEIatJHU/events{/privacy}", "received_events_url": "https://api.github.com/users/HMEIatJHU/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-21T19:40:43Z", "updated_at": "2018-04-21T19:41:05Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9110200\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fmassa\">@fmassa</a> Thanks!</p>\n<p>I tried a simple script as below on 3 different versions of PyTorch on Ubuntu 1404:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> timeit\n\ntorch.manual_seed(<span class=\"pl-c1\">123456</span>)\n\ndim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\nnumber <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">use_index</span>(<span class=\"pl-smi\">a</span>, <span class=\"pl-smi\">b</span>, <span class=\"pl-smi\">c</span>):\n\n    ind0 <span class=\"pl-k\">=</span> (a <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0.0</span>) <span class=\"pl-k\">&amp;</span> (b <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1.0</span>)\n    ind1 <span class=\"pl-k\">=</span> (a <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0.0</span>) <span class=\"pl-k\">&amp;</span> (b <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1.0</span>)\n    ind <span class=\"pl-k\">=</span> ind0 <span class=\"pl-k\">&amp;</span> ind1\n    c[ind] <span class=\"pl-k\">=</span> a[ind] <span class=\"pl-k\">+</span> b[ind]\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">use_legacy_function</span>(<span class=\"pl-smi\">a</span>, <span class=\"pl-smi\">b</span>, <span class=\"pl-smi\">c</span>):\n\n    ind0 <span class=\"pl-k\">=</span> (a <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0.0</span>) <span class=\"pl-k\">&amp;</span> (b <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1.0</span>)\n    ind1 <span class=\"pl-k\">=</span> (a <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">0.0</span>) <span class=\"pl-k\">&amp;</span> (b <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1.0</span>)\n    ind <span class=\"pl-k\">=</span> ind0 <span class=\"pl-k\">&amp;</span> ind1\n    values <span class=\"pl-k\">=</span> a[ind] <span class=\"pl-k\">+</span> b[ind]\n    c.masked_scatter_(ind, values)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">wrapper</span>(<span class=\"pl-smi\">func</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">wrapper</span>():\n        <span class=\"pl-k\">return</span> func(<span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n    <span class=\"pl-k\">return</span> wrapper\n\n<span class=\"pl-k\">for</span> device <span class=\"pl-k\">in</span> [torch, torch.cuda]:\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time it on <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>gpu<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">if</span> device<span class=\"pl-k\">==</span>torch.cuda <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>cpu<span class=\"pl-pds\">'</span></span>))\n\n    a <span class=\"pl-k\">=</span> device.FloatTensor(dim, dim).normal_(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">1.0</span>)\n    b <span class=\"pl-k\">=</span> device.FloatTensor(dim, dim).exponential_(<span class=\"pl-c1\">1.0</span>)\n    c <span class=\"pl-k\">=</span> device.FloatTensor(dim, dim)\n\n    wrapped <span class=\"pl-k\">=</span> wrapper(use_index, a, b, c)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>use indexing, time is : <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(\n        timeit.timeit(wrapped, <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span>number)) )\n\n    wrapped <span class=\"pl-k\">=</span> wrapper(use_legacy_function, a, b, c)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>use legacy functions, time is : <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(\n        timeit.timeit(wrapped, <span class=\"pl-v\">number</span><span class=\"pl-k\">=</span>number)) )</pre></div>\n<p>Here are results with different versions:</p>\n<p>0.3.1.post2</p>\n<div class=\"highlight highlight-source-python\"><pre>time it on cpu\nuse indexing, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">8.527695072000142</span>\nuse legacy functions, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">8.785706020000134</span>\ntime it on gpu\nuse indexing, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">1.073753952000061</span>\nuse legacy functions, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">1.0569869740002105</span></pre></div>\n<p>0.4.0a0+a589180</p>\n<div class=\"highlight highlight-source-python\"><pre>time it on cpu\nuse indexing, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">7.870821144999923</span>\nuse legacy functions, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">7.257624652000004</span>\ntime it on gpu\nuse indexing, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">1.2752332359999627</span>\nuse legacy functions, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">1.2200424609999345</span></pre></div>\n<p>0.4.0a0+d564ecb</p>\n<div class=\"highlight highlight-source-python\"><pre>time it on cpu\nuse indexing, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">7.808169143999976</span>\nuse legacy functions, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">7.182136361000062</span>\ntime it on gpu\nuse indexing, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">1.1385018550000723</span>\nuse legacy functions, time <span class=\"pl-k\">is</span> : <span class=\"pl-c1\">1.0596150669999815</span></pre></div>", "body_text": "@fmassa Thanks!\nI tried a simple script as below on 3 different versions of PyTorch on Ubuntu 1404:\nimport torch\nimport timeit\n\ntorch.manual_seed(123456)\n\ndim = 1000\nnumber = 1000\n\ndef use_index(a, b, c):\n\n    ind0 = (a > 0.0) & (b < 1.0)\n    ind1 = (a < 0.0) & (b > 1.0)\n    ind = ind0 & ind1\n    c[ind] = a[ind] + b[ind]\n\ndef use_legacy_function(a, b, c):\n\n    ind0 = (a > 0.0) & (b < 1.0)\n    ind1 = (a < 0.0) & (b > 1.0)\n    ind = ind0 & ind1\n    values = a[ind] + b[ind]\n    c.masked_scatter_(ind, values)\n\ndef wrapper(func, *args, **kwargs):\n    def wrapper():\n        return func(*args, **kwargs)\n    return wrapper\n\nfor device in [torch, torch.cuda]:\n\n    print(\"time it on {}\".format('gpu' if device==torch.cuda else 'cpu'))\n\n    a = device.FloatTensor(dim, dim).normal_(0.0, 1.0)\n    b = device.FloatTensor(dim, dim).exponential_(1.0)\n    c = device.FloatTensor(dim, dim)\n\n    wrapped = wrapper(use_index, a, b, c)\n    print(\"use indexing, time is : {}\".format(\n        timeit.timeit(wrapped, number=number)) )\n\n    wrapped = wrapper(use_legacy_function, a, b, c)\n    print(\"use legacy functions, time is : {}\".format(\n        timeit.timeit(wrapped, number=number)) )\nHere are results with different versions:\n0.3.1.post2\ntime it on cpu\nuse indexing, time is : 8.527695072000142\nuse legacy functions, time is : 8.785706020000134\ntime it on gpu\nuse indexing, time is : 1.073753952000061\nuse legacy functions, time is : 1.0569869740002105\n0.4.0a0+a589180\ntime it on cpu\nuse indexing, time is : 7.870821144999923\nuse legacy functions, time is : 7.257624652000004\ntime it on gpu\nuse indexing, time is : 1.2752332359999627\nuse legacy functions, time is : 1.2200424609999345\n0.4.0a0+d564ecb\ntime it on cpu\nuse indexing, time is : 7.808169143999976\nuse legacy functions, time is : 7.182136361000062\ntime it on gpu\nuse indexing, time is : 1.1385018550000723\nuse legacy functions, time is : 1.0596150669999815", "body": "@fmassa Thanks!  \r\n\r\nI tried a simple script as below on 3 different versions of PyTorch on Ubuntu 1404: \r\n```python \r\nimport torch\r\nimport timeit\r\n\r\ntorch.manual_seed(123456)\r\n\r\ndim = 1000\r\nnumber = 1000\r\n\r\ndef use_index(a, b, c):\r\n\r\n    ind0 = (a > 0.0) & (b < 1.0)\r\n    ind1 = (a < 0.0) & (b > 1.0)\r\n    ind = ind0 & ind1\r\n    c[ind] = a[ind] + b[ind]\r\n\r\ndef use_legacy_function(a, b, c):\r\n\r\n    ind0 = (a > 0.0) & (b < 1.0)\r\n    ind1 = (a < 0.0) & (b > 1.0)\r\n    ind = ind0 & ind1\r\n    values = a[ind] + b[ind]\r\n    c.masked_scatter_(ind, values)\r\n\r\ndef wrapper(func, *args, **kwargs):\r\n    def wrapper():\r\n        return func(*args, **kwargs)\r\n    return wrapper\r\n\r\nfor device in [torch, torch.cuda]:\r\n\r\n    print(\"time it on {}\".format('gpu' if device==torch.cuda else 'cpu'))\r\n\r\n    a = device.FloatTensor(dim, dim).normal_(0.0, 1.0)\r\n    b = device.FloatTensor(dim, dim).exponential_(1.0)\r\n    c = device.FloatTensor(dim, dim)\r\n\r\n    wrapped = wrapper(use_index, a, b, c)\r\n    print(\"use indexing, time is : {}\".format(\r\n        timeit.timeit(wrapped, number=number)) )\r\n\r\n    wrapped = wrapper(use_legacy_function, a, b, c)\r\n    print(\"use legacy functions, time is : {}\".format(\r\n        timeit.timeit(wrapped, number=number)) )\r\n```\r\n\r\nHere are results with different versions: \r\n\r\n0.3.1.post2\r\n```python\r\ntime it on cpu\r\nuse indexing, time is : 8.527695072000142\r\nuse legacy functions, time is : 8.785706020000134\r\ntime it on gpu\r\nuse indexing, time is : 1.073753952000061\r\nuse legacy functions, time is : 1.0569869740002105\r\n```\r\n\r\n0.4.0a0+a589180\r\n```python \r\ntime it on cpu\r\nuse indexing, time is : 7.870821144999923\r\nuse legacy functions, time is : 7.257624652000004\r\ntime it on gpu\r\nuse indexing, time is : 1.2752332359999627\r\nuse legacy functions, time is : 1.2200424609999345\r\n```\r\n\r\n0.4.0a0+d564ecb\r\n```python \r\ntime it on cpu\r\nuse indexing, time is : 7.808169143999976\r\nuse legacy functions, time is : 7.182136361000062\r\ntime it on gpu\r\nuse indexing, time is : 1.1385018550000723\r\nuse legacy functions, time is : 1.0596150669999815\r\n```\r\n"}