{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10590", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10590/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10590/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10590/events", "html_url": "https://github.com/pytorch/pytorch/issues/10590", "id": 351340121, "node_id": "MDU6SXNzdWUzNTEzNDAxMjE=", "number": 10590, "title": "grad_fn printout bug for torch::jit::(anonymous namespace)::ExecutionPlanAutogradFunction", "user": {"login": "jamesr66a", "id": 4685384, "node_id": "MDQ6VXNlcjQ2ODUzODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4685384?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamesr66a", "html_url": "https://github.com/jamesr66a", "followers_url": "https://api.github.com/users/jamesr66a/followers", "following_url": "https://api.github.com/users/jamesr66a/following{/other_user}", "gists_url": "https://api.github.com/users/jamesr66a/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamesr66a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamesr66a/subscriptions", "organizations_url": "https://api.github.com/users/jamesr66a/orgs", "repos_url": "https://api.github.com/users/jamesr66a/repos", "events_url": "https://api.github.com/users/jamesr66a/events{/privacy}", "received_events_url": "https://api.github.com/users/jamesr66a/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-16T19:36:31Z", "updated_at": "2018-08-19T07:51:39Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Code:</p>\n<pre><code>class MILSTMCell(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, bias=True):\n        super(MILSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = torch.nn.Parameter(torch.Tensor(4 * hidden_size, input_size))\n        self.weight_hh = torch.nn.Parameter(torch.Tensor(4 * hidden_size, hidden_size))\n        if bias:\n            self.bias = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n        else:\n            self.register_parameter(\"bias\", None)\n        self.alpha = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n        self.beta_h = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n        self.beta_i = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    # TODO: type annotations for traced modules\n    def forward(self, x, hx, cx):\n        # get prev_t, cell_t from states\n        Wx = F.linear(x, self.weight_ih)\n        Uz = F.linear(hx, self.weight_hh)\n\n        # Section 2.1 in https://arxiv.org/pdf/1606.06630.pdf\n        gates = self.alpha * Wx * Uz + self.beta_i * Wx + self.beta_h * Uz + self.bias\n\n        # Same as LSTMCell after this point\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n        ingate = F.sigmoid(ingate)\n        forgetgate = F.sigmoid(forgetgate)\n        cellgate = F.tanh(cellgate)\n        outgate = F.sigmoid(outgate)\n\n        cy = (forgetgate * cx) + (ingate * cellgate)\n        hy = outgate * F.tanh(cy)\n\n        return hy, cy\n\n\nclass MILSTMEncoder(torch.jit.ScriptModule):\n    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0.0):\n        super(MILSTMEncoder, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n        self.initial_hidden = torch.nn.Parameter(torch.randn(hidden_size))\n        self.initial_cell = torch.nn.Parameter(torch.randn(hidden_size))\n\n        self.milstm = torch.jit.trace(torch.rand(5, hidden_size), self.initial_hidden.repeat(5, 1), self.initial_cell.repeat(5, 1))(MILSTMCell(hidden_size, hidden_size))\n\n    @torch.jit.script_method\n    def forward(self, input, seq_lens):\n        input = self.embedding(input)\n        hidden = self.initial_hidden.unsqueeze(0).repeat([input.size(1), 1])\n        cell = self.initial_cell.unsqueeze(0).repeat([input.size(1), 1])\n        outputs = torch.zeros([0, input.size(1), input.size(2)])\n        for i in range(input.size(0)):\n            hidden, cell = self.milstm(input[i], hidden, cell)\n            outputs = torch.cat((outputs, hidden.unsqueeze(0)), dim=0)\n\n        return outputs, hidden\n\nmilstm = MILSTMEncoder(C, embedding)\n\ninputs = torch.LongTensor(20, 5).random_(0, 10000)\nseq_lens = torch.LongTensor(-np.sort(-np.random.randint(low=1, high=20, size=(5,))))\nouts = milstm(inputs, seq_lens)\nfor out in outs:\n    print(out)\n</code></pre>\n<p>When we try to print out the second output we get:</p>\n<pre><code>Traceback (most recent call last):\n  File \"refactored_seq2seq.py\", line 277, in &lt;module&gt;\n    print(out)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/tensor.py\", line 60, in __repr__\n    return torch._tensor_str._str(self)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/_tensor_str.py\", line 259, in _str\n    if self.grad_fn is not None:\nTypeError: Don't know how to create Python object for N5torch3jit12_GLOBAL__N_129ExecutionPlanAutogradFunctionE\n</code></pre>\n<p>This doesn't happen if we first call <code>detach()</code> on the Tensor. Maybe having a Python binding for <code>torch::jit::(anonymous namespace)::ExecutionPlanAutogradFunction</code> will fix this?</p>", "body_text": "Code:\nclass MILSTMCell(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, bias=True):\n        super(MILSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.bias = bias\n        self.weight_ih = torch.nn.Parameter(torch.Tensor(4 * hidden_size, input_size))\n        self.weight_hh = torch.nn.Parameter(torch.Tensor(4 * hidden_size, hidden_size))\n        if bias:\n            self.bias = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n        else:\n            self.register_parameter(\"bias\", None)\n        self.alpha = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n        self.beta_h = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n        self.beta_i = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.hidden_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, stdv)\n\n    # TODO: type annotations for traced modules\n    def forward(self, x, hx, cx):\n        # get prev_t, cell_t from states\n        Wx = F.linear(x, self.weight_ih)\n        Uz = F.linear(hx, self.weight_hh)\n\n        # Section 2.1 in https://arxiv.org/pdf/1606.06630.pdf\n        gates = self.alpha * Wx * Uz + self.beta_i * Wx + self.beta_h * Uz + self.bias\n\n        # Same as LSTMCell after this point\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n        ingate = F.sigmoid(ingate)\n        forgetgate = F.sigmoid(forgetgate)\n        cellgate = F.tanh(cellgate)\n        outgate = F.sigmoid(outgate)\n\n        cy = (forgetgate * cx) + (ingate * cellgate)\n        hy = outgate * F.tanh(cy)\n\n        return hy, cy\n\n\nclass MILSTMEncoder(torch.jit.ScriptModule):\n    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0.0):\n        super(MILSTMEncoder, self).__init__()\n        self.n_layers = n_layers\n        self.hidden_size = hidden_size\n        self.embedding = embedding\n        self.initial_hidden = torch.nn.Parameter(torch.randn(hidden_size))\n        self.initial_cell = torch.nn.Parameter(torch.randn(hidden_size))\n\n        self.milstm = torch.jit.trace(torch.rand(5, hidden_size), self.initial_hidden.repeat(5, 1), self.initial_cell.repeat(5, 1))(MILSTMCell(hidden_size, hidden_size))\n\n    @torch.jit.script_method\n    def forward(self, input, seq_lens):\n        input = self.embedding(input)\n        hidden = self.initial_hidden.unsqueeze(0).repeat([input.size(1), 1])\n        cell = self.initial_cell.unsqueeze(0).repeat([input.size(1), 1])\n        outputs = torch.zeros([0, input.size(1), input.size(2)])\n        for i in range(input.size(0)):\n            hidden, cell = self.milstm(input[i], hidden, cell)\n            outputs = torch.cat((outputs, hidden.unsqueeze(0)), dim=0)\n\n        return outputs, hidden\n\nmilstm = MILSTMEncoder(C, embedding)\n\ninputs = torch.LongTensor(20, 5).random_(0, 10000)\nseq_lens = torch.LongTensor(-np.sort(-np.random.randint(low=1, high=20, size=(5,))))\nouts = milstm(inputs, seq_lens)\nfor out in outs:\n    print(out)\n\nWhen we try to print out the second output we get:\nTraceback (most recent call last):\n  File \"refactored_seq2seq.py\", line 277, in <module>\n    print(out)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/tensor.py\", line 60, in __repr__\n    return torch._tensor_str._str(self)\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/_tensor_str.py\", line 259, in _str\n    if self.grad_fn is not None:\nTypeError: Don't know how to create Python object for N5torch3jit12_GLOBAL__N_129ExecutionPlanAutogradFunctionE\n\nThis doesn't happen if we first call detach() on the Tensor. Maybe having a Python binding for torch::jit::(anonymous namespace)::ExecutionPlanAutogradFunction will fix this?", "body": "Code:\r\n\r\n```\r\nclass MILSTMCell(torch.nn.Module):\r\n    def __init__(self, input_size, hidden_size, bias=True):\r\n        super(MILSTMCell, self).__init__()\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.bias = bias\r\n        self.weight_ih = torch.nn.Parameter(torch.Tensor(4 * hidden_size, input_size))\r\n        self.weight_hh = torch.nn.Parameter(torch.Tensor(4 * hidden_size, hidden_size))\r\n        if bias:\r\n            self.bias = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\r\n        else:\r\n            self.register_parameter(\"bias\", None)\r\n        self.alpha = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\r\n        self.beta_h = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\r\n        self.beta_i = torch.nn.Parameter(torch.Tensor(4 * hidden_size))\r\n        self.reset_parameters()\r\n\r\n    def reset_parameters(self):\r\n        stdv = 1.0 / math.sqrt(self.hidden_size)\r\n        for weight in self.parameters():\r\n            weight.data.uniform_(-stdv, stdv)\r\n\r\n    # TODO: type annotations for traced modules\r\n    def forward(self, x, hx, cx):\r\n        # get prev_t, cell_t from states\r\n        Wx = F.linear(x, self.weight_ih)\r\n        Uz = F.linear(hx, self.weight_hh)\r\n\r\n        # Section 2.1 in https://arxiv.org/pdf/1606.06630.pdf\r\n        gates = self.alpha * Wx * Uz + self.beta_i * Wx + self.beta_h * Uz + self.bias\r\n\r\n        # Same as LSTMCell after this point\r\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\r\n\r\n        ingate = F.sigmoid(ingate)\r\n        forgetgate = F.sigmoid(forgetgate)\r\n        cellgate = F.tanh(cellgate)\r\n        outgate = F.sigmoid(outgate)\r\n\r\n        cy = (forgetgate * cx) + (ingate * cellgate)\r\n        hy = outgate * F.tanh(cy)\r\n\r\n        return hy, cy\r\n\r\n\r\nclass MILSTMEncoder(torch.jit.ScriptModule):\r\n    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0.0):\r\n        super(MILSTMEncoder, self).__init__()\r\n        self.n_layers = n_layers\r\n        self.hidden_size = hidden_size\r\n        self.embedding = embedding\r\n        self.initial_hidden = torch.nn.Parameter(torch.randn(hidden_size))\r\n        self.initial_cell = torch.nn.Parameter(torch.randn(hidden_size))\r\n\r\n        self.milstm = torch.jit.trace(torch.rand(5, hidden_size), self.initial_hidden.repeat(5, 1), self.initial_cell.repeat(5, 1))(MILSTMCell(hidden_size, hidden_size))\r\n\r\n    @torch.jit.script_method\r\n    def forward(self, input, seq_lens):\r\n        input = self.embedding(input)\r\n        hidden = self.initial_hidden.unsqueeze(0).repeat([input.size(1), 1])\r\n        cell = self.initial_cell.unsqueeze(0).repeat([input.size(1), 1])\r\n        outputs = torch.zeros([0, input.size(1), input.size(2)])\r\n        for i in range(input.size(0)):\r\n            hidden, cell = self.milstm(input[i], hidden, cell)\r\n            outputs = torch.cat((outputs, hidden.unsqueeze(0)), dim=0)\r\n\r\n        return outputs, hidden\r\n\r\nmilstm = MILSTMEncoder(C, embedding)\r\n\r\ninputs = torch.LongTensor(20, 5).random_(0, 10000)\r\nseq_lens = torch.LongTensor(-np.sort(-np.random.randint(low=1, high=20, size=(5,))))\r\nouts = milstm(inputs, seq_lens)\r\nfor out in outs:\r\n    print(out)\r\n```\r\n\r\nWhen we try to print out the second output we get:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"refactored_seq2seq.py\", line 277, in <module>\r\n    print(out)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/tensor.py\", line 60, in __repr__\r\n    return torch._tensor_str._str(self)\r\n  File \"/Users/jamesreed/onnx-fairseq/pytorch/torch/_tensor_str.py\", line 259, in _str\r\n    if self.grad_fn is not None:\r\nTypeError: Don't know how to create Python object for N5torch3jit12_GLOBAL__N_129ExecutionPlanAutogradFunctionE\r\n```\r\n\r\nThis doesn't happen if we first call `detach()` on the Tensor. Maybe having a Python binding for `torch::jit::(anonymous namespace)::ExecutionPlanAutogradFunction` will fix this?"}