{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167535962", "pull_request_review_id": 95763349, "id": 167535962, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzUzNTk2Mg==", "diff_hunk": "@@ -1,329 +1,615 @@\n #pragma once\n \n-// A wrapper around at::Tensor to represent autograd Variables. Variables\n-// can be implicitly converted to an at::Tensor.\n+#include <Python.h>\n \n-#include <mutex>\n+#include \"torch/csrc/autograd/edge.h\"\n+#include \"torch/csrc/autograd/function_hook.h\"\n+#include \"torch/csrc/autograd/variable_version.h\"\n+#include \"torch/csrc/utils/auto_unique_ptr.h\"\n+\n+#include <ATen/ATen.h>\n+\n+#include <list>\n #include <memory>\n+#include <mutex>\n+#include <stdexcept>\n+#include <string>\n #include <vector>\n-#include <functional>\n-#include <ATen/ATen.h>\n \n-#include \"torch/csrc/assertions.h\"\n-#include \"torch/csrc/jit/ir.h\"\n-#include \"torch/csrc/jit/tracer_state.h\"\n-#include \"torch/csrc/autograd/function_hook.h\"\n-#include \"torch/csrc/utils/auto_unique_ptr.h\"\n-#include \"torch/csrc/autograd/variable_version.h\"\n-#include \"torch/csrc/autograd/edge.h\"\n-#include \"torch/csrc/Types.h\"\n+namespace torch {\n+namespace autograd {\n+struct Function;\n+} // namespace autograd\n+namespace jit { namespace tracer {\n+// Has to be forward declared because tracer_state.h has a dependency on\n+// variable.h.\n+struct ValueTracingStateElem;\n+using ValueTracingState = std::list<ValueTracingStateElem>;\n+}} // namespace jit::tracer\n+} // namespace torch\n \n namespace torch { namespace autograd {\n \n-using at::Tensor;\n-struct VariableImpl;\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+///                                Variable\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// A `Variable` augments a `Tensor` with the ability to interact in our\n+/// autograd machinery. Conceptually, `Variable`s travel along `Edge`s between\n+/// `Function`s in the autograd graph. A `Variable` can either be a leaf, like a\n+/// weight in a neural network, or an interior variable, when it is the result\n+/// of an operation between variables. Every `Variable` also stores another\n+/// `Variable` (recursively) called its `grad` (gradient). If the variable is a", "path": "torch/csrc/autograd/variable.h", "position": null, "original_position": 56, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "b2eb9e59dd9969c5c8c9d21036652c3e0a281de9", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "it's rarely recursive, since `.grad`s are usually not leaves, and so they don't get gradient saved by default (`.grad.grad` is not a very useful value - it's not the same as taking the second-order derivatives)", "created_at": "2018-02-12T12:05:11Z", "updated_at": "2018-11-23T15:39:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167535962", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167535962"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167535962"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>it's rarely recursive, since <code>.grad</code>s are usually not leaves, and so they don't get gradient saved by default (<code>.grad.grad</code> is not a very useful value - it's not the same as taking the second-order derivatives)</p>", "body_text": "it's rarely recursive, since .grads are usually not leaves, and so they don't get gradient saved by default (.grad.grad is not a very useful value - it's not the same as taking the second-order derivatives)"}