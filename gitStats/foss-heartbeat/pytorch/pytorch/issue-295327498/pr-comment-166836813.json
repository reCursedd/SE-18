{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166836813", "pull_request_review_id": 94961640, "id": 166836813, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NjgzNjgxMw==", "diff_hunk": "@@ -1,150 +1,250 @@\n #pragma once\n \n-// A wrapper around at::Tensor to represent autograd Variables. Variables\n-// can be implicitly converted to an at::Tensor.\n+#include <Python.h>\n \n-#include <mutex>\n-#include <memory>\n-#include <vector>\n-#include <functional>\n-#include <ATen/ATen.h>\n-\n-#include \"torch/csrc/assertions.h\"\n-#include \"torch/csrc/jit/ir.h\"\n-#include \"torch/csrc/jit/tracer_state.h\"\n+#include \"torch/csrc/autograd/edge.h\"\n #include \"torch/csrc/autograd/function_hook.h\"\n-#include \"torch/csrc/utils/auto_unique_ptr.h\"\n #include \"torch/csrc/autograd/variable_version.h\"\n-#include \"torch/csrc/autograd/edge.h\"\n-#include \"torch/csrc/Types.h\"\n+#include \"torch/csrc/jit/tracer_state.h\"\n+#include \"torch/csrc/utils/auto_unique_ptr.h\"\n+\n+#include <ATen/ATen.h>\n+\n+#include <list>\n+#include <memory>\n+#include <mutex>\n+#include <stdexcept>\n+#include <string>\n+#include <vector>\n \n namespace torch { namespace autograd {\n \n-using at::Tensor;\n-struct VariableImpl;\n+struct Function;\n+\n+//===----------------------------------------------------------------------===//\n+//                                Variable\n+//===----------------------------------------------------------------------===//\n+\n+/// A `Variable` augments a `Tensor` with the ability to interact in our\n+/// autograd machinery. `Variable` inherits from `Tensor` and may be converted\n+/// to and from `Tensor` implicitly.\n+class Variable : public at::Tensor {\n+ public:\n+  /// Creates a Variable that is a *view* of another (*base*) variable.\n+  /// The `gradient_edge` is an optional (gradient_function, input_number) pair.\n+  static Variable\n+  as_view(Variable base, at::Tensor data, Edge gradient_edge = Edge());\n+\n+  Variable() = default;\n+  Variable(at::Tensor data, bool requires_grad);\n+  Variable(at::Tensor data, Edge gradient_edge);\n+\n+  // \"Downcasts\" a `Tensor` into a `Variable`. Only call this on tensors you\n+  // know are Variables.\n+  /*implicit*/ Variable(at::Tensor const& rhs) : at::Tensor(rhs) {}\n+  /*implicit*/ Variable(at::Tensor&& rhs) noexcept\n+      : at::Tensor(std::move(rhs)) {}\n+\n+  // NOTE: Assignment operators to Tensor come for free from the constructors.\n+\n+  /// Compare this `Variable` to another `Variable` (or `Tensor`) via\n+  /// pointer-equality.\n+  bool is_same(const Variable& other) const noexcept {\n+    return this->pImpl == other.pImpl;\n+  }\n+\n+  void set_name(const std::string& name);\n+  const std::string& name() const noexcept;\n+\n+  /// Get the gradient function of the `Variable`. If this is a leaf variable,\n+  /// the pointer returned will be null.\n+  const std::shared_ptr<Function>& grad_fn() const;\n+\n+  /// Get the raw gradient function pointer, whatever it currently is.\n+  Function* grad_fn_ptr() const;\n \n-struct Variable : public at::Tensor {\n-  inline Variable(VariableImpl * self, bool retain);\n-  Variable() : Tensor() {}\n-  Variable(const Variable & rhs) : Tensor(rhs) {}\n-  Variable(Variable && rhs) noexcept : Tensor(std::move(rhs)) {}\n+  /// Set the gradient accumulator of the `Variable`. This is only applicable\n+  /// to leaf variables. Interior variables should call `set_gradient_edge()`.\n+  void set_grad_accumulator(std::weak_ptr<Function> grad_accumulator);\n \n-  // Implicitly casts a Tensor to a Variable. This should only be called on\n-  // Tensors which you know are actually Variables.\n-  /*implicit*/ Variable(Tensor const & rhs) : Tensor(rhs) {}\n-  /*implicit*/ Variable(Tensor && rhs) noexcept : Tensor(std::move(rhs)) {}\n+  /// Attempt to get a pointer to the gradient accumulator of the `Variable`,\n+  /// if it still exists. If the gradient accumulator function has been\n+  /// destroyed, returns a `nullptr`.\n+  std::shared_ptr<Function> try_get_grad_accumulator() const;\n \n+  /// Get the gradient accumulator of the `Variable` if it has one, or else\n+  /// create one on the fly and return it.\n+  std::shared_ptr<Function> grad_accumulator() const;\n+\n+  /// Set the gradient edge -- i.e. `grad_fn` and `input_nr` -- of the\n+  /// `Variable`.\n+  /// NOTE: This will always set the `grad_fn`, even if this is a leaf\n+  /// variable, and never the `grad_accumulator`. For the latter, use\n+  /// `set_grad_accumulator`. This allows late construction of an interior\n+  /// `Variable`.\n+  void set_gradient_edge(Edge&& edge) noexcept;\n+\n+  /// Return the \"canonical\" gradient edge of this `Variable`, i.e. either the\n+  /// gradient function if this is an interior `Variable`, or the gradient\n+  /// accumulator otherwise. If the `Variable` is interior, the returned `Edge`\n+  /// will store the input index of the `Function` to which this variable is\n+  /// connected in its `input_nr` field. For leaves, the `input_nr` is always\n+  /// zero. Note that `set_gradient_edge` and `gradient_edge` are not\n+  /// symmetric. You must use `set_gradient_edge` to set the `grad_fn` and\n+  /// `set_grad_accumulator` to set the accumulator.\n   Edge gradient_edge() const {\n     // If grad_fn is null (as is the case for a leaf node), we instead\n-    // interpret the gradient function to be a grad accumulator,\n-    // which will accumulate its inputs into the grad property of the\n-    // variable. These nodes get suppressed in some situations,\n-    // see \"suppress grad accumulation\" below. Note that only variables which\n-    // have `requires_grad = True` can have grad accumulators.\n+    // interpret the gradient function to be a gradient accumulator, which will\n+    // accumulate its inputs into the grad property of the variable. These\n+    // nodes get suppressed in some situations, see \"suppress gradient\n+    // accumulation\" below. Note that only variables which have `requires_grad =\n+    // True` can have gradient accumulators.\n     if (const auto& gradient = grad_fn()) {\n       return Edge(gradient, output_nr());\n     } else {\n       return Edge(grad_accumulator(), 0);\n     }\n   }\n \n-  inline VariableImpl* get() const;\n+  /// Return the input index of the gradient `Function` to which this `Variable`\n+  /// is connected.\n+  int output_nr() const noexcept;\n \n-  inline const Tensor & data() const;\n-  inline       Tensor & data();\n+  void set_requires_grad(bool requires_grad) noexcept;\n+  bool requires_grad() const noexcept;\n \n-  inline Tensor opt_data() const;\n+  PyObject* pyobj() const noexcept;\n+  void set_pyobj(PyObject* pyobj) noexcept;\n \n-  inline const Variable & grad() const;\n-  inline       Variable & grad();\n+  /// Set the type of the underlying `Tensor`.\n+  void set_type(at::Type*) noexcept;", "path": "torch/csrc/autograd/variable.h", "position": null, "original_position": 156, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "088215bd4f82e8f84d8a56bbc553e0e71b2afcf5", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "I noticed this in code review because previously there was no setter for type, and now there is. That should have indicated that something is up.\r\n\r\nIn fact, the back story is that `set_type` is exposed *solely* to support type-changing of Variable (documented at:\r\n\r\n```\r\n// XXX: This is a hack to access private TensorImpl::type_\r\n// http://bloglitb.blogspot.com/2011/12/access-to-private-members-safer.html\r\n// This is currently needed because module.float() changes the type of the\r\n// data field of each variable. We should fix this and not allow changing the\r\n// type of var.data.\r\n```\r\n\r\nSo, there should (1) be a very clear indication that `set_type` is really exposed for OO reasons, but not as a legitimate thing you should actually do with a Tensor (unless we revisit the design); at least prefixing it with an underscore but also a comment and (2) a clear reference to the portion of code which actually makes use of it, and the reason why (namely, to support parameter identity-preserving, inplace type casting of modules). You can use the `Note [Foo]` convention to do this.", "created_at": "2018-02-08T05:39:30Z", "updated_at": "2018-11-23T15:39:22Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r166836813", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166836813"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r166836813"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>I noticed this in code review because previously there was no setter for type, and now there is. That should have indicated that something is up.</p>\n<p>In fact, the back story is that <code>set_type</code> is exposed <em>solely</em> to support type-changing of Variable (documented at:</p>\n<pre><code>// XXX: This is a hack to access private TensorImpl::type_\n// http://bloglitb.blogspot.com/2011/12/access-to-private-members-safer.html\n// This is currently needed because module.float() changes the type of the\n// data field of each variable. We should fix this and not allow changing the\n// type of var.data.\n</code></pre>\n<p>So, there should (1) be a very clear indication that <code>set_type</code> is really exposed for OO reasons, but not as a legitimate thing you should actually do with a Tensor (unless we revisit the design); at least prefixing it with an underscore but also a comment and (2) a clear reference to the portion of code which actually makes use of it, and the reason why (namely, to support parameter identity-preserving, inplace type casting of modules). You can use the <code>Note [Foo]</code> convention to do this.</p>", "body_text": "I noticed this in code review because previously there was no setter for type, and now there is. That should have indicated that something is up.\nIn fact, the back story is that set_type is exposed solely to support type-changing of Variable (documented at:\n// XXX: This is a hack to access private TensorImpl::type_\n// http://bloglitb.blogspot.com/2011/12/access-to-private-members-safer.html\n// This is currently needed because module.float() changes the type of the\n// data field of each variable. We should fix this and not allow changing the\n// type of var.data.\n\nSo, there should (1) be a very clear indication that set_type is really exposed for OO reasons, but not as a legitimate thing you should actually do with a Tensor (unless we revisit the design); at least prefixing it with an underscore but also a comment and (2) a clear reference to the portion of code which actually makes use of it, and the reason why (namely, to support parameter identity-preserving, inplace type casting of modules). You can use the Note [Foo] convention to do this."}