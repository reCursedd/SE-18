{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166796158", "pull_request_review_id": 94915243, "id": 166796158, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2Njc5NjE1OA==", "diff_hunk": "@@ -1,150 +1,255 @@\n #pragma once\n \n-// A wrapper around at::Tensor to represent autograd Variables. Variables\n-// can be implicitly converted to an at::Tensor.\n+#include <Python.h>\n \n-#include <mutex>\n-#include <memory>\n-#include <vector>\n-#include <functional>\n-#include <ATen/ATen.h>\n-\n-#include \"torch/csrc/assertions.h\"\n-#include \"torch/csrc/jit/ir.h\"\n-#include \"torch/csrc/jit/tracer_state.h\"\n+#include \"torch/csrc/autograd/edge.h\"\n #include \"torch/csrc/autograd/function_hook.h\"\n-#include \"torch/csrc/utils/auto_unique_ptr.h\"\n #include \"torch/csrc/autograd/variable_version.h\"\n-#include \"torch/csrc/autograd/edge.h\"\n-#include \"torch/csrc/Types.h\"\n+#include \"torch/csrc/jit/tracer_state.h\"\n+#include \"torch/csrc/utils/auto_unique_ptr.h\"\n+\n+#include <ATen/Scalar.h>\n+#include <ATen/ScalarType.h>\n+#include <ATen/Storage.h>\n+#include <ATen/Tensor.h>\n+#include <ATen/TensorImpl.h>\n+#include <ATen/Type.h>\n+\n+#include <list>\n+#include <memory>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n \n namespace torch { namespace autograd {\n \n-using at::Tensor;\n-struct VariableImpl;\n+struct Function;\n+\n+//===----------------------------------------------------------------------===//\n+//                                Variable\n+//===----------------------------------------------------------------------===//\n+\n+/// A `Variable` augments a `Tensor` with the ability to interact in our\n+/// autograd machinery. `Variable` inherits from `Tensor` and may be converted\n+/// to and from `Tensor` implicitly.\n+class Variable : public at::Tensor {\n+ public:\n+  /// Creates a Variable that is a *view* of another (*base*) variable.\n+  /// The `gradient_edge` is an optional (gradient_function, input_number) pair.\n+  static Variable\n+  as_view(Variable base, at::Tensor data, Edge gradient_edge = Edge());\n+\n+  Variable() = default;\n+  Variable(at::Tensor data, bool requires_grad);\n+  Variable(at::Tensor data, Edge gradient_edge);\n+\n+  // \"Downcasts\" a `Tensor` into a `Variable`. Only call this on tensors you\n+  // know are Variables.\n+  /*implicit*/ Variable(at::Tensor const& rhs) : at::Tensor(rhs) {}\n+  /*implicit*/ Variable(at::Tensor&& rhs) noexcept\n+      : at::Tensor(std::move(rhs)) {}\n+\n+  // NOTE: Assignment operators to Tensor come for free from the constructors.\n+\n+  /// Compare this `Variable` to another `Variable` (or `Tensor`) via\n+  /// pointer-equality.\n+  bool operator==(const Variable& other) const noexcept {\n+    return this->pImpl == other.pImpl;\n+  }\n+\n+  bool operator!=(const Variable& other) const noexcept {\n+    return !(*this == other);\n+  }\n+\n+  void set_name(const std::string& name);\n+  const std::string& name() const noexcept;\n+\n+  /// Get the gradient function of the `Variable`. If this is a leaf variable,\n+  /// the pointer returned will be null.\n+  const std::shared_ptr<Function>& grad_fn() const;\n+\n+  /// Set the gradient accumulator of the `Variable`. This is only applicable\n+  /// to leaf variables. Interior variables should call `set_gradient_edge()`.\n+  void set_grad_accumulator(std::weak_ptr<Function> grad_accumulator);\n \n-struct Variable : public at::Tensor {\n-  inline Variable(VariableImpl * self, bool retain);\n-  Variable() : Tensor() {}\n-  Variable(const Variable & rhs) : Tensor(rhs) {}\n-  Variable(Variable && rhs) noexcept : Tensor(std::move(rhs)) {}\n+  /// Attempt to get a pointer to the gradient accumulator of the `Variable`,\n+  /// if it still exists. If the gradient accumulator function has been\n+  /// destroyed, returns a `nullptr`.\n+  std::shared_ptr<Function> try_get_grad_accumulator() const;\n \n-  // Implicitly casts a Tensor to a Variable. This should only be called on\n-  // Tensors which you know are actually Variables.\n-  /*implicit*/ Variable(Tensor const & rhs) : Tensor(rhs) {}\n-  /*implicit*/ Variable(Tensor && rhs) noexcept : Tensor(std::move(rhs)) {}\n+  /// Get the gradient accumulator of the `Variable` if it has one, or else\n+  /// create one on the fly and return it.\n+  std::shared_ptr<Function> grad_accumulator() const;\n \n+  /// Set the gradient edge -- i.e. `grad_fn` and `input_nr` -- of the\n+  /// `Variable`.\n+  /// NOTE: This will always set the `grad_fn`, even if this is a leaf\n+  /// variable, and never the `grad_accumulator`. For the latter, use\n+  /// `set_grad_accumulator`. This allows late construction of an interior\n+  /// `Variable`.\n+  void set_gradient_edge(Edge&& edge);\n+\n+  /// Return the \"canonical\" gradient edge of this `Variable`, i.e. either the\n+  /// gradient function if this is an interior `Variable`, or the gradient\n+  /// accumulator otherwise. If the `Variable` is interior, the returned `Edge`\n+  /// will store the input index of the `Function` to which this variable is\n+  /// connected in its `input_nr` field. For leaves, the `input_nr` is always\n+  /// zero. Note that `set_gradient_edge` and `gradient_edge` are not\n+  /// symmetric. You must use `set_gradient_edge` to set the `grad_fn` and\n+  /// `set_grad_accumulator` to set the accumulator.\n   Edge gradient_edge() const {\n     // If grad_fn is null (as is the case for a leaf node), we instead\n-    // interpret the gradient function to be a grad accumulator,\n-    // which will accumulate its inputs into the grad property of the\n-    // variable. These nodes get suppressed in some situations,\n-    // see \"suppress grad accumulation\" below. Note that only variables which\n-    // have `requires_grad = True` can have grad accumulators.\n+    // interpret the gradient function to be a gradient accumulator, which will\n+    // accumulate its inputs into the grad property of the variable. These\n+    // nodes get suppressed in some situations, see \"suppress gradient\n+    // accumulation\" below. Note that only variables which have `requires_grad =\n+    // True` can have gradient accumulators.\n     if (const auto& gradient = grad_fn()) {\n       return Edge(gradient, output_nr());\n     } else {\n       return Edge(grad_accumulator(), 0);\n     }\n   }\n \n-  inline VariableImpl* get() const;\n+  /// Return the input index of the gradient `Function` to which this `Variable`\n+  /// is connected.\n+  int output_nr() const noexcept;", "path": "torch/csrc/autograd/variable.h", "position": null, "original_position": 147, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "2770506f74aa5bd4f0a6c4a3dee541a6e8e4250c", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "The advantage of marking these declarations inline in the header is that you get compilation errors (instead of runtime or linker errors) if you miss the definition below.", "created_at": "2018-02-08T00:00:00Z", "updated_at": "2018-11-23T15:39:20Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r166796158", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166796158"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r166796158"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>The advantage of marking these declarations inline in the header is that you get compilation errors (instead of runtime or linker errors) if you miss the definition below.</p>", "body_text": "The advantage of marking these declarations inline in the header is that you get compilation errors (instead of runtime or linker errors) if you miss the definition below."}