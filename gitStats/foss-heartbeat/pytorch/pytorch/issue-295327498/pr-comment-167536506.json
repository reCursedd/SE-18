{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167536506", "pull_request_review_id": 95763349, "id": 167536506, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzUzNjUwNg==", "diff_hunk": "@@ -1,329 +1,615 @@\n #pragma once\n \n-// A wrapper around at::Tensor to represent autograd Variables. Variables\n-// can be implicitly converted to an at::Tensor.\n+#include <Python.h>\n \n-#include <mutex>\n+#include \"torch/csrc/autograd/edge.h\"\n+#include \"torch/csrc/autograd/function_hook.h\"\n+#include \"torch/csrc/autograd/variable_version.h\"\n+#include \"torch/csrc/utils/auto_unique_ptr.h\"\n+\n+#include <ATen/ATen.h>\n+\n+#include <list>\n #include <memory>\n+#include <mutex>\n+#include <stdexcept>\n+#include <string>\n #include <vector>\n-#include <functional>\n-#include <ATen/ATen.h>\n \n-#include \"torch/csrc/assertions.h\"\n-#include \"torch/csrc/jit/ir.h\"\n-#include \"torch/csrc/jit/tracer_state.h\"\n-#include \"torch/csrc/autograd/function_hook.h\"\n-#include \"torch/csrc/utils/auto_unique_ptr.h\"\n-#include \"torch/csrc/autograd/variable_version.h\"\n-#include \"torch/csrc/autograd/edge.h\"\n-#include \"torch/csrc/Types.h\"\n+namespace torch {\n+namespace autograd {\n+struct Function;\n+} // namespace autograd\n+namespace jit { namespace tracer {\n+// Has to be forward declared because tracer_state.h has a dependency on\n+// variable.h.\n+struct ValueTracingStateElem;\n+using ValueTracingState = std::list<ValueTracingStateElem>;\n+}} // namespace jit::tracer\n+} // namespace torch\n \n namespace torch { namespace autograd {\n \n-using at::Tensor;\n-struct VariableImpl;\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+///                                Variable\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// A `Variable` augments a `Tensor` with the ability to interact in our\n+/// autograd machinery. Conceptually, `Variable`s travel along `Edge`s between\n+/// `Function`s in the autograd graph. A `Variable` can either be a leaf, like a\n+/// weight in a neural network, or an interior variable, when it is the result\n+/// of an operation between variables. Every `Variable` also stores another\n+/// `Variable` (recursively) called its `grad` (gradient). If the variable is a\n+/// leaf, its gradient will be accumulated into this variable.\n+///\n+///                             Gradient Edges\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Furthermore, `Variable`s have the notion of a `gradient_edge`, which is the\n+/// edge in the autograd graph that connects the variable to a particular input\n+/// of the gradient function that will be invoked with the variable during the\n+/// backward pass. More precisely, this gradient function can be one of two\n+/// things:\n+/// 1. A `grad_fn`, if the variable is in the interior of the graph. This is the\n+///    gradient of the function that produced the variable.\n+/// 2. A `grad_accumulator`, if the variable is a leaf, which accumulates a\n+///    scalar gradient value into its `grad` variable.\n+///\n+///                               Versioning\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Another major feature of `Variable`s are *versions*. Versions are\n+/// incremented when an in-place mutation of a variable occurs. Versions are\n+/// useful when constructing `SavedVariable`s, which take a snapshot of a\n+/// `Variable` at a certain version. You can retrieve a `Variable`'s version\n+/// through its `current_version()` method.\n+///\n+///                                Views\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// It is possible for a  `Variable` to be a *view* of another `Variable`, in\n+/// which case it tracks that `Variable`'s data and autograd history. Beyond\n+/// construction, the interface of a view is identical to that of a regular\n+/// `Variable`. You can determine whether `Variable` is in fact a view by\n+/// probing its `is_view()` method.\n+///\n+///                               Interface\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// `Variable` inherits from `Tensor` and thus its API is a superset of that of\n+/// `Tensor`. This means you can perform all the usual mathematical and other\n+/// operations you can perform on `Tensor`s also on `Variable`s. Furthermore,\n+/// `Variable` and `Tensor` actually convert implicitly between each other. You\n+/// can thus call functions defined on `Tensor`s also with `Variable`s. Besides\n+/// the constructor of `Variable` that converts to it from `Tensor`, you can use\n+/// the `make_variable` free functions to create variables. To create views,", "path": "torch/csrc/autograd/variable.h", "position": null, "original_position": 95, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "b2eb9e59dd9969c5c8c9d21036652c3e0a281de9", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This implies that you can just use the \"constructor that converts a Tensor into a Variable or `make_variable`\", but they are not really equivalent - one is unsafe and *expects an upcasted Variable*, the other one expects a *raw tensor* and will wrap it in a Variable", "created_at": "2018-02-12T12:07:01Z", "updated_at": "2018-11-23T15:39:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167536506", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167536506"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167536506"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>This implies that you can just use the \"constructor that converts a Tensor into a Variable or <code>make_variable</code>\", but they are not really equivalent - one is unsafe and <em>expects an upcasted Variable</em>, the other one expects a <em>raw tensor</em> and will wrap it in a Variable</p>", "body_text": "This implies that you can just use the \"constructor that converts a Tensor into a Variable or make_variable\", but they are not really equivalent - one is unsafe and expects an upcasted Variable, the other one expects a raw tensor and will wrap it in a Variable"}