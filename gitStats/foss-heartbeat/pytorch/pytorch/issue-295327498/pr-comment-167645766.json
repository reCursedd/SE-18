{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167645766", "pull_request_review_id": 95894265, "id": 167645766, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzY0NTc2Ng==", "diff_hunk": "@@ -106,52 +105,71 @@ VariableViewImpl::VariableViewImpl(Variable base_, at::Tensor data_, int output_\n   attr_version = version_counter.current_version();\n }\n \n-std::shared_ptr<Function>& VariableViewImpl::get_grad_fn() {\n+std::shared_ptr<Function>& Variable::ViewImpl::get_grad_fn() {\n   std::lock_guard<std::mutex> lock(mutex);\n-  if (!_grad_fn && !base.requires_grad()) {\n-    return _grad_fn;\n+  if (!grad_fn && !base.requires_grad()) {\n+    return grad_fn;\n   }\n   auto current_version = version_counter.current_version();\n   if (attr_version != current_version) {\n     TORCH_ASSERT(output_nr == 0);\n     auto fn = std::make_shared<generated::AsStridedBackward>();\n-    fn->self_geometry = TensorGeometry(base);\n+    fn->self_geometry = at::TensorGeometry(base);\n     fn->size = sizes();\n     fn->stride = strides();\n     fn->storage_offset = data.storage_offset();\n     fn->set_next_functions(get_next_functions(base));\n     fn->num_inputs = 1;\n-    _grad_fn = std::move(fn);\n+    grad_fn = std::move(fn);\n     attr_version = current_version;\n   }\n-  return _grad_fn;\n+  return grad_fn;\n }\n \n-void VariableViewImpl::rebase_history(int output_nr, std::shared_ptr<Function> grad_fn) {\n-  TORCH_ASSERT(output_nr == 0);\n-  TORCH_ASSERT(grad_fn);\n-  TORCH_ASSERTM(grad_fn->num_inputs == 1, \"Functions which modify views in-place must return a single Variable\");\n-  this->output_nr = output_nr;\n-  base.output_nr() = 0;\n-  base.get()->_grad_fn = std::make_shared<CopySlices>(\n-      base, TensorGeometry(data), std::move(grad_fn));\n-  get_grad_fn();  // trigger an update to the view's grad_fn\n+void Variable::ViewImpl::rebase_history(Edge gradient_edge) {\n+  TORCH_ASSERT(gradient_edge.input_nr == 0);\n+  TORCH_ASSERT(gradient_edge.function);\n+  TORCH_ASSERTM(\n+      gradient_edge.function->num_inputs == 1,\n+      \"Functions which modify views in-place must return a single Variable\");\n+  this->output_nr = gradient_edge.input_nr;\n+  auto copy_slices = std::make_shared<CopySlices>(\n+      base, at::TensorGeometry(data), std::move(gradient_edge.function));\n+  base.set_gradient_edge({std::move(copy_slices), 0});\n+  get_grad_fn(); // trigger an update to the view's grad_fn\n+}\n+\n+void Variable::rebase_history(Edge gradient_edge) {\n+  TORCH_ASSERT(gradient_edge.function != nullptr);\n+  if (is_view()) {\n+    auto& impl = static_cast<Variable::ViewImpl&>(*get());\n+    impl.rebase_history(std::move(gradient_edge));\n+  } else {\n+    set_gradient_edge(std::move(gradient_edge));\n+  }\n }\n \n Variable Variable::detach() const {\n-  Variable detached = make_variable(data());\n-  detached.version_counter() = version_counter();\n+  auto detached = make_variable(data(), /*requires_grad=*/false);\n+  detached.set_version(version_counter());", "path": "torch/csrc/autograd/variable.cpp", "position": null, "original_position": 209, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "b2eb9e59dd9969c5c8c9d21036652c3e0a281de9", "user": {"login": "colesbury", "id": 655866, "node_id": "MDQ6VXNlcjY1NTg2Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/655866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/colesbury", "html_url": "https://github.com/colesbury", "followers_url": "https://api.github.com/users/colesbury/followers", "following_url": "https://api.github.com/users/colesbury/following{/other_user}", "gists_url": "https://api.github.com/users/colesbury/gists{/gist_id}", "starred_url": "https://api.github.com/users/colesbury/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/colesbury/subscriptions", "organizations_url": "https://api.github.com/users/colesbury/orgs", "repos_url": "https://api.github.com/users/colesbury/repos", "events_url": "https://api.github.com/users/colesbury/events{/privacy}", "received_events_url": "https://api.github.com/users/colesbury/received_events", "type": "User", "site_admin": false}, "body": "We share the version counter when we create a detached Variable since they share the same data. That way, if you modify the detached variable, you'll still get the error message in backwards if it messes up a SavedVariable.\r\n\r\nIs that what you mean by \"why do we detach the version counter as well\"?", "created_at": "2018-02-12T18:32:11Z", "updated_at": "2018-11-23T15:39:29Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167645766", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167645766"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167645766"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>We share the version counter when we create a detached Variable since they share the same data. That way, if you modify the detached variable, you'll still get the error message in backwards if it messes up a SavedVariable.</p>\n<p>Is that what you mean by \"why do we detach the version counter as well\"?</p>", "body_text": "We share the version counter when we create a detached Variable since they share the same data. That way, if you modify the detached variable, you'll still get the error message in backwards if it messes up a SavedVariable.\nIs that what you mean by \"why do we detach the version counter as well\"?", "in_reply_to_id": 167538810}