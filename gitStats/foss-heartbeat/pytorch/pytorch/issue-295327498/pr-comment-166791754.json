{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166791754", "pull_request_review_id": 94906614, "id": 166791754, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2Njc5MTc1NA==", "diff_hunk": "@@ -1,46 +1,68 @@\n #pragma once\n \n-#include <mutex>\n-#include <memory>\n-#include <functional>\n-#include <ATen/ATen.h>\n-\n-#include \"torch/csrc/jit/tracer_state.h\"\n-#include \"torch/csrc/autograd/variable.h\"\n #include \"torch/csrc/autograd/variable_version.h\"\n-#include \"torch/csrc/Types.h\"\n \n-namespace torch { namespace autograd {\n+#include <ATen/Tensor.h>\n+\n+#include <cstdint>\n+#include <list>\n+#include <memory>\n \n+namespace torch {\n+namespace autograd {\n+class Variable;\n struct Function;\n+} // namespace autograd\n+namespace jit { namespace tracer {\n+struct ValueTracingStateElem;\n+using ValueTracingState = std::list<ValueTracingStateElem>;\n+}} // namespace jit::tracer\n+} // namespace torch\n \n-extern const char* ERR_BACKWARD_TWICE;\n+namespace torch { namespace autograd {\n \n-struct SavedVariable {\n-  SavedVariable()\n-    : data()\n-    , has_grad_fn(false)\n-    , version()\n-    , requires_grad(false)\n-    , expected_version(-1) {}\n+extern const char* ERR_BACKWARD_TWICE;\n \n+/// A snapshot of a variable at a certain version. A `SavedVariable` stores\n+/// enough information to reconstruct a variable from a certain point in time.\n+class SavedVariable {\n+ public:\n+  SavedVariable() = default;\n   SavedVariable(const Variable& variable, bool is_output);\n+  SavedVariable(SavedVariable&&) = default;\n+  SavedVariable& operator=(SavedVariable&&) = default;\n+\n+  // Must be defined externally to avoid it being inlined by the compiler,\n+  // which would require it to see the definition of ValueTracingState.\n+  ~SavedVariable();\n+\n+  /// Reconstructs the saved variable. Pass `saved_for` as the gradient\n+  /// function if constructing the `SavedVariable` with it would have caused a\n+  /// circular reference.\n+  Variable unpack(std::shared_ptr<Function> saved_for = nullptr) const;\n+\n+  void reset_data() {\n+    return data_.reset();\n+  }\n+\n+ private:\n+  bool was_default_constructed_ = true;\n+  at::Tensor data_;\n \n-  at::Tensor data;\n   // The gradient function associated with this node. If has_grad_fn\n   // is false, then this is a leaf node. Note that the grad_fn is not saved if\n   // it would create a circular reference. In that case, the grad_fn must be\n   // passed in to the unpack function when reconstructing the Variable.\n-  bool has_grad_fn;\n-  std::shared_ptr<Function> _grad_fn;\n-  std::weak_ptr<Function> grad_accumulator;\n-  SavedVersion version;\n-  bool requires_grad;\n-  int expected_version;\n-  int output_nr;\n-  std::unique_ptr<jit::tracer::ValueTracingState> tracing_state;\n-\n-  Variable unpack(std::shared_ptr<Function> saved_for=nullptr) const;\n-};\n+  std::shared_ptr<Function> grad_fn_;\n+  bool has_grad_fn_ = false;", "path": "torch/csrc/autograd/saved_variable.h", "position": null, "original_position": 86, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "2770506f74aa5bd4f0a6c4a3dee541a6e8e4250c", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Can you compress those bools in one place? Right now the struct will likely have 1B for a bool, 7B for padding, 8B for grad_fn, 1B for bool, 7B for padding, ...", "created_at": "2018-02-07T23:35:37Z", "updated_at": "2018-11-23T15:39:19Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r166791754", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166791754"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r166791754"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>Can you compress those bools in one place? Right now the struct will likely have 1B for a bool, 7B for padding, 8B for grad_fn, 1B for bool, 7B for padding, ...</p>", "body_text": "Can you compress those bools in one place? Right now the struct will likely have 1B for a bool, 7B for padding, 8B for grad_fn, 1B for bool, 7B for padding, ..."}