{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167533956", "pull_request_review_id": 95763349, "id": 167533956, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzUzMzk1Ng==", "diff_hunk": "@@ -1,46 +1,59 @@\n #pragma once\n \n-#include <mutex>\n-#include <memory>\n-#include <functional>\n+#include \"torch/csrc/autograd/variable_version.h\"\n+#include \"torch/csrc/jit/tracer_state.h\"\n+\n #include <ATen/ATen.h>\n \n-#include \"torch/csrc/jit/tracer_state.h\"\n-#include \"torch/csrc/autograd/variable.h\"\n-#include \"torch/csrc/autograd/variable_version.h\"\n-#include \"torch/csrc/Types.h\"\n+#include <cstdint>\n+#include <list>\n+#include <memory>\n \n namespace torch { namespace autograd {\n \n+struct Variable;\n struct Function;\n \n extern const char* ERR_BACKWARD_TWICE;\n \n-struct SavedVariable {\n-  SavedVariable()\n-    : data()\n-    , has_grad_fn(false)\n-    , version()\n-    , requires_grad(false)\n-    , expected_version(-1) {}\n-\n+/// A snapshot of a variable at a certain version. A `SavedVariable` stores\n+/// enough information to reconstruct a variable from a certain point in time.\n+class SavedVariable {\n+ public:\n+  SavedVariable() = default;\n   SavedVariable(const Variable& variable, bool is_output);\n+  SavedVariable(SavedVariable&&) = default;\n+  SavedVariable& operator=(SavedVariable&&) = default;\n+\n+  // Must be defined externally to avoid it being inlined by the compiler,\n+  // which would require it to see the definition of ValueTracingState.\n+  ~SavedVariable();\n+\n+  /// Reconstructs the saved variable. Pass `saved_for` as the gradient\n+  /// function if constructing the `SavedVariable` with it would have caused a\n+  /// circular reference.\n+  Variable unpack(std::shared_ptr<Function> saved_for = nullptr) const;\n+\n+  void reset_data() {\n+    return data_.reset();\n+  }\n+\n+ private:\n+  at::Tensor data_;\n \n-  at::Tensor data;\n   // The gradient function associated with this node. If has_grad_fn\n   // is false, then this is a leaf node. Note that the grad_fn is not saved if\n   // it would create a circular reference. In that case, the grad_fn must be\n   // passed in to the unpack function when reconstructing the Variable.\n-  bool has_grad_fn;\n-  std::shared_ptr<Function> _grad_fn;\n-  std::weak_ptr<Function> grad_accumulator;\n-  SavedVersion version;\n-  bool requires_grad;\n-  int expected_version;\n-  int output_nr;\n-  std::unique_ptr<jit::tracer::ValueTracingState> tracing_state;\n-\n-  Variable unpack(std::shared_ptr<Function> saved_for=nullptr) const;\n+  std::shared_ptr<Function> grad_fn_;\n+  std::weak_ptr<Function> grad_accumulator_;\n+  std::unique_ptr<jit::tracer::ValueTracingState> tracing_state_;\n+  VariableVersion version_counter_;\n+\n+  uint32_t saved_version_ = 0;\n+  uint32_t output_nr_ = 0;\n+  bool was_default_constructed_ = true;\n+  bool requires_grad_ = false;\n+  bool has_grad_fn_ = false;", "path": "torch/csrc/autograd/saved_variable.h", "position": null, "original_position": 83, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "b2eb9e59dd9969c5c8c9d21036652c3e0a281de9", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "Same here (requires_grad + has_grad_fn). These fields should be always initialized in a constructor.", "created_at": "2018-02-12T11:56:40Z", "updated_at": "2018-11-23T15:39:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167533956", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167533956"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167533956"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>Same here (requires_grad + has_grad_fn). These fields should be always initialized in a constructor.</p>", "body_text": "Same here (requires_grad + has_grad_fn). These fields should be always initialized in a constructor."}