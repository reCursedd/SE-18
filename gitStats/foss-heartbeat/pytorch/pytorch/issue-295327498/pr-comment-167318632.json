{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167318632", "pull_request_review_id": 95529200, "id": 167318632, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzMxODYzMg==", "diff_hunk": "@@ -153,177 +253,162 @@ struct VariableViewImpl : public VariableImpl {\n \n   // Called after in-place modifications. Modifies the grad_fn of the base\n   // Variable.\n-  void rebase_history(int output_nr, std::shared_ptr<Function> grad_fn);\n+  void rebase_history(Edge gradient_edge);\n \n   // The base Variable (never a view)\n   Variable base;\n \n   // The value of the version_counter at the time grad_fn was created. The\n-  // _grad_fn field is stale if attr_version != version_counter.current_version()\n-  int attr_version;\n+  // grad_fn field is stale if attr_version !=\n+  // version_counter.current_version()\n+  uint32_t attr_version;\n };\n \n-inline Variable make_variable(at::Tensor data, bool requires_grad=false) {\n-  if (!data.defined()) {\n-    return Variable();\n-  }\n+//===----------------------------------------------------------------------===//\n+//                        Variable Implementation\n+//===----------------------------------------------------------------------===//\n \n-#ifndef WITH_SCALARS\n-  if (data.dim() == 0) {\n-    // don't expose 0-dim tensors to Variable API.\n-    data = data.as_strided_({1}, {1});\n-  }\n-#endif\n+inline Variable::Variable(Variable::Impl* self, bool retain)\n+    : at::Tensor(self, retain) {}\n \n-  return Variable(new VariableImpl(std::move(data), requires_grad), false);\n+inline const std::shared_ptr<Function>& Variable::grad_fn() const {\n+  return get()->get_grad_fn();\n }\n \n-inline Variable make_variable(at::Tensor data, int output_nr, std::shared_ptr<Function> grad_fn) {\n-  if (!data.defined()) {\n-    return Variable();\n-  }\n+inline Function* Variable::grad_fn_ptr() const {\n+  return get()->grad_fn.get();\n+}\n \n-#ifndef WITH_SCALARS\n-  if (data.dim() == 0) {\n-    // don't expose 0-dim tensors to Variable API.\n-    data = data.as_strided_({1}, {1});\n-  }\n-#endif\n+inline void Variable::set_grad_accumulator(\n+    std::weak_ptr<Function> grad_accumulator) {\n+  get()->grad_accumulator = std::move(grad_accumulator);\n+}\n \n-  return Variable(new VariableImpl(std::move(data), false, output_nr, std::move(grad_fn)), false);\n+inline std::shared_ptr<Function> Variable::try_get_grad_accumulator() const {\n+  return get()->grad_accumulator.lock();\n }\n \n-Variable make_variable(at::Tensor data, std::shared_ptr<Function> grad_fn);\n+inline std::shared_ptr<Function> Variable::grad_accumulator() const {\n+  return get()->get_grad_accumulator();\n+}\n \n-inline Variable make_variable_view(Variable base, at::Tensor data, int output_nr=0,\n-                                   std::shared_ptr<Function> grad_fn=nullptr) {\n-  if (!data.defined()) {\n-    return Variable();\n-  }\n+inline void Variable::set_gradient_edge(Edge&& edge) noexcept {\n+  get()->grad_fn = std::move(edge.function);\n+  get()->output_nr = edge.input_nr;\n+}\n \n-#ifndef WITH_SCALARS\n-  if (data.dim() == 0) {\n-    // don't expose 0-dim tensors to Variable API.\n-    data = data.as_strided_({1}, {1});\n-  }\n-#endif\n+inline int Variable::output_nr() const noexcept {\n+  return get()->output_nr;\n+}\n \n-  return Variable(new VariableViewImpl(std::move(base), std::move(data), output_nr, std::move(grad_fn)), false);\n+inline void Variable::set_requires_grad(bool requires_grad) noexcept {\n+  get()->requires_grad = requires_grad;\n }\n \n+inline bool Variable::requires_grad() const noexcept {\n+  return get()->requires_grad || get()->grad_fn ||\n+      (is_view() && base().requires_grad());\n+}\n \n-inline Variable::Variable(VariableImpl * self, bool retain) : Tensor(self, retain) {\n+inline void Variable::set_pyobj(PyObject* pyobj) noexcept {\n+  get()->pyobj = pyobj;\n }\n \n-inline VariableImpl* Variable::get() const {\n-  return static_cast<VariableImpl*>(pImpl);\n+inline PyObject* Variable::pyobj() const noexcept {\n+  return get()->pyobj;\n }\n \n-inline const Tensor & Variable::data() const {\n-  return get()->data;\n+inline void Variable::set_type(at::Type* new_type) noexcept {\n+  pImpl->setType(new_type);\n }\n-inline Tensor & Variable::data() {\n+\n+inline void Variable::reset_grad() noexcept {\n+  get()->grad.reset();\n+}\n+\n+inline const at::Tensor& Variable::data() const noexcept {\n   return get()->data;\n }\n \n-inline Tensor Variable::opt_data() const {\n-  if (!defined()) {\n-    return Tensor();\n-  }\n-  return data();\n+inline at::Tensor& Variable::data() noexcept {\n+  return get()->data;\n }\n \n-inline const Variable & Variable::grad() const {\n+inline const Variable& Variable::grad() const noexcept {\n   return get()->grad;\n }\n-inline Variable & Variable::grad() {\n+\n+inline Variable& Variable::grad() noexcept {\n   return get()->grad;\n }\n \n-inline bool Variable::is_leaf() const {\n-  return get()->_grad_fn == nullptr;\n+inline bool Variable::is_leaf() const noexcept {\n+  return get()->grad_fn == nullptr;\n }\n \n-inline const std::shared_ptr<Function>& Variable::grad_fn() const {\n-  return get()->get_grad_fn();\n-};\n-inline void Variable::rebase_history(int output_nr, std::shared_ptr<Function> grad_fn) {\n-  TORCH_ASSERT(grad_fn);\n-  if (is_view()) {\n-    auto& impl = static_cast<VariableViewImpl&>(*get());\n-    impl.rebase_history(output_nr, std::move(grad_fn));\n-  } else {\n-    get()->output_nr = output_nr;\n-    get()->_grad_fn = std::move(grad_fn);\n-  }\n+inline void Variable::add_hook(std::shared_ptr<FunctionPreHook> hook) {\n+  get()->hooks.push_back(std::move(hook));\n }\n-inline std::shared_ptr<Function> Variable::grad_accumulator() const {\n-  return get()->get_grad_accumulator();\n-};\n \n-inline const std::vector<std::shared_ptr<FunctionPreHook>>& Variable::hooks() const {\n-  return get()->hooks;\n-};\n-inline std::vector<std::shared_ptr<FunctionPreHook>>& Variable::hooks() {\n+inline const std::vector<std::shared_ptr<FunctionPreHook>>& Variable::hooks()\n+    const noexcept {\n   return get()->hooks;\n-};\n+}\n \n-inline auto_unique_ptr<jit::tracer::ValueTracingState>& Variable::tracing_state() const {\n-  return get()->tracing_state;\n-};\n+inline void Variable::clear_hooks() {\n+  get()->hooks.clear();\n+}\n \n-inline int Variable::current_version() const {\n-  return get()->version_counter.current_version();\n+inline void Variable::set_tracing_state(\n+    jit::tracer::ValueTracingState* new_tracing_state) {\n+  get()->tracing_state.reset(new_tracing_state);\n }\n \n-inline VariableVersion& Variable::version_counter() const {\n-  return get()->version_counter;\n+inline jit::tracer::ValueTracingState& Variable::tracing_state() const\n+    noexcept {\n+  return *get()->tracing_state;", "path": "torch/csrc/autograd/variable.h", "position": null, "original_position": 537, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "088215bd4f82e8f84d8a56bbc553e0e71b2afcf5", "user": {"login": "goldsborough", "id": 6429851, "node_id": "MDQ6VXNlcjY0Mjk4NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/6429851?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goldsborough", "html_url": "https://github.com/goldsborough", "followers_url": "https://api.github.com/users/goldsborough/followers", "following_url": "https://api.github.com/users/goldsborough/following{/other_user}", "gists_url": "https://api.github.com/users/goldsborough/gists{/gist_id}", "starred_url": "https://api.github.com/users/goldsborough/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goldsborough/subscriptions", "organizations_url": "https://api.github.com/users/goldsborough/orgs", "repos_url": "https://api.github.com/users/goldsborough/repos", "events_url": "https://api.github.com/users/goldsborough/events{/privacy}", "received_events_url": "https://api.github.com/users/goldsborough/received_events", "type": "User", "site_admin": false}, "body": "Actually the `tracing_state` is an `auto_unique_ptr` so it will get constructed on dereference", "created_at": "2018-02-09T19:08:07Z", "updated_at": "2018-11-23T15:39:26Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167318632", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167318632"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167318632"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>Actually the <code>tracing_state</code> is an <code>auto_unique_ptr</code> so it will get constructed on dereference</p>", "body_text": "Actually the tracing_state is an auto_unique_ptr so it will get constructed on dereference", "in_reply_to_id": 166841301}