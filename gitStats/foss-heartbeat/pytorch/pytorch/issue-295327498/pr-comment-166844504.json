{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166844504", "pull_request_review_id": 94970292, "id": 166844504, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2Njg0NDUwNA==", "diff_hunk": "@@ -1,150 +1,255 @@\n #pragma once\n \n-// A wrapper around at::Tensor to represent autograd Variables. Variables\n-// can be implicitly converted to an at::Tensor.\n+#include <Python.h>\n \n-#include <mutex>\n-#include <memory>\n-#include <vector>\n-#include <functional>\n-#include <ATen/ATen.h>\n-\n-#include \"torch/csrc/assertions.h\"\n-#include \"torch/csrc/jit/ir.h\"\n-#include \"torch/csrc/jit/tracer_state.h\"\n+#include \"torch/csrc/autograd/edge.h\"\n #include \"torch/csrc/autograd/function_hook.h\"\n-#include \"torch/csrc/utils/auto_unique_ptr.h\"\n #include \"torch/csrc/autograd/variable_version.h\"\n-#include \"torch/csrc/autograd/edge.h\"\n-#include \"torch/csrc/Types.h\"\n+#include \"torch/csrc/jit/tracer_state.h\"\n+#include \"torch/csrc/utils/auto_unique_ptr.h\"\n+\n+#include <ATen/Scalar.h>\n+#include <ATen/ScalarType.h>\n+#include <ATen/Storage.h>\n+#include <ATen/Tensor.h>\n+#include <ATen/TensorImpl.h>\n+#include <ATen/Type.h>\n+\n+#include <list>\n+#include <memory>\n+#include <mutex>\n+#include <string>\n+#include <vector>\n \n namespace torch { namespace autograd {\n \n-using at::Tensor;\n-struct VariableImpl;\n+struct Function;\n+\n+//===----------------------------------------------------------------------===//\n+//                                Variable\n+//===----------------------------------------------------------------------===//\n+\n+/// A `Variable` augments a `Tensor` with the ability to interact in our\n+/// autograd machinery. `Variable` inherits from `Tensor` and may be converted\n+/// to and from `Tensor` implicitly.\n+class Variable : public at::Tensor {\n+ public:\n+  /// Creates a Variable that is a *view* of another (*base*) variable.\n+  /// The `gradient_edge` is an optional (gradient_function, input_number) pair.\n+  static Variable\n+  as_view(Variable base, at::Tensor data, Edge gradient_edge = Edge());\n+\n+  Variable() = default;\n+  Variable(at::Tensor data, bool requires_grad);\n+  Variable(at::Tensor data, Edge gradient_edge);\n+\n+  // \"Downcasts\" a `Tensor` into a `Variable`. Only call this on tensors you\n+  // know are Variables.\n+  /*implicit*/ Variable(at::Tensor const& rhs) : at::Tensor(rhs) {}\n+  /*implicit*/ Variable(at::Tensor&& rhs) noexcept\n+      : at::Tensor(std::move(rhs)) {}", "path": "torch/csrc/autograd/variable.h", "position": 145, "original_position": 66, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "2770506f74aa5bd4f0a6c4a3dee541a6e8e4250c", "user": {"login": "ezyang", "id": 13564, "node_id": "MDQ6VXNlcjEzNTY0", "avatar_url": "https://avatars0.githubusercontent.com/u/13564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezyang", "html_url": "https://github.com/ezyang", "followers_url": "https://api.github.com/users/ezyang/followers", "following_url": "https://api.github.com/users/ezyang/following{/other_user}", "gists_url": "https://api.github.com/users/ezyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezyang/subscriptions", "organizations_url": "https://api.github.com/users/ezyang/orgs", "repos_url": "https://api.github.com/users/ezyang/repos", "events_url": "https://api.github.com/users/ezyang/events{/privacy}", "received_events_url": "https://api.github.com/users/ezyang/received_events", "type": "User", "site_admin": false}, "body": "The `make_` convention is not without precedent in C++. For example, you don't have a constructor for both `T*` and the perfect forwarding version of `std::shared_ptr`, you have a constructor and `std::make_shared` (in this case, you really don't want an overload because they interact poorly with perfect forwarding (see Effective Modern C++ for more details ;)). I do understand static functions are common in pure-OO codebases like Java, where top-level functions are not actually a thing, but the prevailing style in PyTorch is to prefer plain old functions over static methods.", "created_at": "2018-02-08T06:49:56Z", "updated_at": "2018-11-23T15:39:23Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r166844504", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/166844504"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r166844504"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>The <code>make_</code> convention is not without precedent in C++. For example, you don't have a constructor for both <code>T*</code> and the perfect forwarding version of <code>std::shared_ptr</code>, you have a constructor and <code>std::make_shared</code> (in this case, you really don't want an overload because they interact poorly with perfect forwarding (see Effective Modern C++ for more details ;)). I do understand static functions are common in pure-OO codebases like Java, where top-level functions are not actually a thing, but the prevailing style in PyTorch is to prefer plain old functions over static methods.</p>", "body_text": "The make_ convention is not without precedent in C++. For example, you don't have a constructor for both T* and the perfect forwarding version of std::shared_ptr, you have a constructor and std::make_shared (in this case, you really don't want an overload because they interact poorly with perfect forwarding (see Effective Modern C++ for more details ;)). I do understand static functions are common in pure-OO codebases like Java, where top-level functions are not actually a thing, but the prevailing style in PyTorch is to prefer plain old functions over static methods.", "in_reply_to_id": 166789793}