{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167538036", "pull_request_review_id": 95763349, "id": 167538036, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2NzUzODAzNg==", "diff_hunk": "@@ -1,329 +1,615 @@\n #pragma once\n \n-// A wrapper around at::Tensor to represent autograd Variables. Variables\n-// can be implicitly converted to an at::Tensor.\n+#include <Python.h>\n \n-#include <mutex>\n+#include \"torch/csrc/autograd/edge.h\"\n+#include \"torch/csrc/autograd/function_hook.h\"\n+#include \"torch/csrc/autograd/variable_version.h\"\n+#include \"torch/csrc/utils/auto_unique_ptr.h\"\n+\n+#include <ATen/ATen.h>\n+\n+#include <list>\n #include <memory>\n+#include <mutex>\n+#include <stdexcept>\n+#include <string>\n #include <vector>\n-#include <functional>\n-#include <ATen/ATen.h>\n \n-#include \"torch/csrc/assertions.h\"\n-#include \"torch/csrc/jit/ir.h\"\n-#include \"torch/csrc/jit/tracer_state.h\"\n-#include \"torch/csrc/autograd/function_hook.h\"\n-#include \"torch/csrc/utils/auto_unique_ptr.h\"\n-#include \"torch/csrc/autograd/variable_version.h\"\n-#include \"torch/csrc/autograd/edge.h\"\n-#include \"torch/csrc/Types.h\"\n+namespace torch {\n+namespace autograd {\n+struct Function;\n+} // namespace autograd\n+namespace jit { namespace tracer {\n+// Has to be forward declared because tracer_state.h has a dependency on\n+// variable.h.\n+struct ValueTracingStateElem;\n+using ValueTracingState = std::list<ValueTracingStateElem>;\n+}} // namespace jit::tracer\n+} // namespace torch\n \n namespace torch { namespace autograd {\n \n-using at::Tensor;\n-struct VariableImpl;\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+///                                Variable\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// A `Variable` augments a `Tensor` with the ability to interact in our\n+/// autograd machinery. Conceptually, `Variable`s travel along `Edge`s between\n+/// `Function`s in the autograd graph. A `Variable` can either be a leaf, like a\n+/// weight in a neural network, or an interior variable, when it is the result\n+/// of an operation between variables. Every `Variable` also stores another\n+/// `Variable` (recursively) called its `grad` (gradient). If the variable is a\n+/// leaf, its gradient will be accumulated into this variable.\n+///\n+///                             Gradient Edges\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Furthermore, `Variable`s have the notion of a `gradient_edge`, which is the\n+/// edge in the autograd graph that connects the variable to a particular input\n+/// of the gradient function that will be invoked with the variable during the\n+/// backward pass. More precisely, this gradient function can be one of two\n+/// things:\n+/// 1. A `grad_fn`, if the variable is in the interior of the graph. This is the\n+///    gradient of the function that produced the variable.\n+/// 2. A `grad_accumulator`, if the variable is a leaf, which accumulates a\n+///    scalar gradient value into its `grad` variable.\n+///\n+///                               Versioning\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// Another major feature of `Variable`s are *versions*. Versions are\n+/// incremented when an in-place mutation of a variable occurs. Versions are\n+/// useful when constructing `SavedVariable`s, which take a snapshot of a\n+/// `Variable` at a certain version. You can retrieve a `Variable`'s version\n+/// through its `current_version()` method.\n+///\n+///                                Views\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// It is possible for a  `Variable` to be a *view* of another `Variable`, in\n+/// which case it tracks that `Variable`'s data and autograd history. Beyond\n+/// construction, the interface of a view is identical to that of a regular\n+/// `Variable`. You can determine whether `Variable` is in fact a view by\n+/// probing its `is_view()` method.\n+///\n+///                               Interface\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+/// `Variable` inherits from `Tensor` and thus its API is a superset of that of\n+/// `Tensor`. This means you can perform all the usual mathematical and other\n+/// operations you can perform on `Tensor`s also on `Variable`s. Furthermore,\n+/// `Variable` and `Tensor` actually convert implicitly between each other. You\n+/// can thus call functions defined on `Tensor`s also with `Variable`s. Besides\n+/// the constructor of `Variable` that converts to it from `Tensor`, you can use\n+/// the `make_variable` free functions to create variables. To create views,\n+/// use `make_variable_view` instead.\n+///~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n struct Variable : public at::Tensor {\n-  inline Variable(VariableImpl * self, bool retain);\n-  Variable() : Tensor() {}\n-  Variable(const Variable & rhs) : Tensor(rhs) {}\n-  Variable(Variable && rhs) noexcept : Tensor(std::move(rhs)) {}\n-\n-  // Implicitly casts a Tensor to a Variable. This should only be called on\n-  // Tensors which you know are actually Variables.\n-  /*implicit*/ Variable(Tensor const & rhs) : Tensor(rhs) {}\n-  /*implicit*/ Variable(Tensor && rhs) noexcept : Tensor(std::move(rhs)) {}\n+  /// Default constructor.\n+  Variable() = default;\n+\n+  // Factory Functions\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  // NOTE: These factory functions have to be friends to access the\n+  // `Variable::Impl`. As a side effect, it allows us to keep them in the class.\n+\n+  /// Creates a `Variable` that is a *view* of another (*base*) variable.\n+  /// The `gradient_edge` is an optional (gradient_function, input_number) pair.\n+  friend Variable\n+  make_variable_view(Variable base, at::Tensor data, Edge gradient_edge);\n+\n+  /// Creates a `Variable` from the given `Tensor`. `requires_grad` should be set\n+  /// only for leaves, and determines whether the `Variable` will accumulate\n+  /// gradients.\n+  friend Variable make_variable(at::Tensor data, bool requires_grad);\n+\n+  /// Creates a `Variable` from the given `Tensor` and specify a `gradient_edge`,\n+  /// i.e. a (function, input_nr) pair specifying the function in the autograd\n+  /// graph, and what particular input of that function, this variable is\n+  /// connected to.\n+  friend Variable make_variable(at::Tensor data, Edge gradient_edge);\n+\n+  // Tensor Conversions\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  // \"Downcasts\" a `Tensor` into a `Variable`. Only call this on tensors you\n+  // know are Variables.\n+  /*implicit*/ Variable(at::Tensor const& rhs) : at::Tensor(rhs) {}\n+  /*implicit*/ Variable(at::Tensor&& rhs) noexcept\n+      : at::Tensor(std::move(rhs)) {}\n+\n+  // NOTE: Assignment operators to Tensor come for free from the constructors.\n+\n+  /// Downcasts the `Tensor` reference to a `Variable` reference. If compiling\n+  /// in DEBUG mode and the tensor's dynamic type is not in fact `Variable`,\n+  /// throws a `std::runtime_error` exception.\n+  /// NOTE: Has to be a friend function because runtime type information is\n+  /// available only for `TensorImpl`/`Impl` and not the `Tensor`/`Variable`\n+  /// classes, as the latter are not polymorphic classes (`Tensor` has no\n+  /// virtual methods).\n+  friend Variable& as_variable_ref(at::Tensor& tensor);\n+\n+  const at::Tensor& data() const noexcept;\n+  at::Tensor& data() noexcept;\n+\n+  // Gradient Function and Edges\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  /// Gets the gradient function of the `Variable`. If this is a leaf variable,\n+  /// the pointer returned will be null.\n+  const std::shared_ptr<Function>& grad_fn() const;\n+\n+  /// Gets the raw gradient function pointer, whatever it currently is.\n+  Function* grad_fn_unsafe() const;\n+\n+  /// Sets the gradient accumulator of the `Variable`. This is only applicable\n+  /// to leaf variables. Interior variables should call `set_gradient_edge()`.\n+  void set_grad_accumulator(std::weak_ptr<Function> grad_accumulator);\n+\n+  /// Attempts to get a pointer to the gradient accumulator of the `Variable`,\n+  /// if it still exists. If the gradient accumulator function has been\n+  /// destroyed, returns a `nullptr`.\n+  std::shared_ptr<Function> try_get_grad_accumulator() const;\n+\n+  /// Gets the gradient accumulator of the `Variable` if it has one, or else\n+  /// create one on the fly and return it.\n+  std::shared_ptr<Function> grad_accumulator() const;\n \n+  /// Sets the gradient edge -- i.e. `grad_fn` and `input_nr` -- of the\n+  /// `Variable`.\n+  /// NOTE: This will always set the `grad_fn`, even if this is a leaf\n+  /// variable, and never the `grad_accumulator`. For the latter, use\n+  /// `set_grad_accumulator`. This allows late construction of an interior\n+  /// `Variable`.\n+  void set_gradient_edge(Edge&& edge) noexcept;\n+\n+  /// Returns the \"canonical\" gradient edge of this `Variable`, i.e. either the\n+  /// gradient function if this is an interior `Variable`, or the gradient\n+  /// accumulator otherwise. If the `Variable` is interior, the returned `Edge`\n+  /// will store the input index of the `Function` to which this variable is\n+  /// connected in its `input_nr` field. For leaves, the `input_nr` is always\n+  /// zero. Note that `set_gradient_edge` and `gradient_edge` are not\n+  /// symmetric. You must use `set_gradient_edge` to set the `grad_fn` and\n+  /// `set_grad_accumulator` to set the accumulator.\n   Edge gradient_edge() const {\n     // If grad_fn is null (as is the case for a leaf node), we instead\n-    // interpret the gradient function to be a grad accumulator,\n-    // which will accumulate its inputs into the grad property of the\n-    // variable. These nodes get suppressed in some situations,\n-    // see \"suppress grad accumulation\" below. Note that only variables which\n-    // have `requires_grad = True` can have grad accumulators.\n+    // interpret the gradient function to be a gradient accumulator, which will\n+    // accumulate its inputs into the grad property of the variable. These\n+    // nodes get suppressed in some situations, see \"suppress gradient\n+    // accumulation\" below. Note that only variables which have `requires_grad =\n+    // True` can have gradient accumulators.\n     if (const auto& gradient = grad_fn()) {\n       return Edge(gradient, output_nr());\n     } else {\n       return Edge(grad_accumulator(), 0);\n     }\n   }\n \n-  inline VariableImpl* get() const;\n+  /// Returns the input index of the gradient `Function` to which this `Variable`\n+  /// is connected.\n+  uint32_t output_nr() const noexcept;\n \n-  inline const Tensor & data() const;\n-  inline       Tensor & data();\n+  /// True if this `Variable` is a leaf and thus does not have a `grad_fn`.\n+  bool is_leaf() const noexcept;\n \n-  inline Tensor opt_data() const;\n+  // The Grad Variable\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-  inline const Variable & grad() const;\n-  inline       Variable & grad();\n+  /// Accesses the gradient `Variable` of this `Variable`.\n+  const Variable& grad() const noexcept;\n+  Variable& grad() noexcept;\n+  void reset_grad() noexcept;\n \n-  inline bool is_leaf() const;\n+  /// Sets the `requires_grad` property of `Variable`. This should be true for\n+  /// leaf variables that want to accumulate gradients, and false for all other\n+  /// variables.\n+  void set_requires_grad(bool requires_grad) noexcept;\n+  bool requires_grad() const noexcept;\n \n-  inline const std::shared_ptr<Function>& grad_fn() const;\n+  // Versions\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-  // Updates the grad_fn of an existing Variable. Called after in-place modifications.\n-  // XXX: this should be called only _after_ the version counter is implemented.\n-  inline void rebase_history(int output_nr, std::shared_ptr<Function> grad_fn);\n+  /// Increments the version count of this `Variable`.\n+  void bump_version() noexcept;\n+  void set_version(const VariableVersion& version) noexcept;\n \n-  std::shared_ptr<Function> grad_accumulator() const;\n+  /// Retrieves this `Variable`s version counter.\n+  const VariableVersion& version_counter() const noexcept;\n+\n+  /// Retrieves the current value of the `Variable`'s version counter.\n+  /// Equivalent to calling `version_counter().current_version()`.\n+  uint32_t current_version() const noexcept;\n+\n+  // Autograd Graph Interaction\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  /// Update the grad_fn of an existing Variable. Called after in-place\n+  /// modifications.\n+  void rebase_history(Edge gradient_edge);\n+\n+  /// Returns a copy of this `Variable` that is detached from its autograd graph\n+  /// and has a blank version. This method is OK to call if the `Variable` is a\n+  /// view.\n   Variable detach() const;\n+\n+  /// Like `detach()`, but removes this `Variable` in-place. This method may\n+  /// only be called on non-view `Variable`s. You can use `is_view()` to check\n+  /// this. If this `Variable` is a view, throws an `std::runtime_error()`.\n   void detach_();\n \n-  inline const std::vector<std::shared_ptr<FunctionPreHook>>& hooks() const;\n-  inline       std::vector<std::shared_ptr<FunctionPreHook>>& hooks();\n+  // Hooks\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  void add_hook(std::shared_ptr<FunctionPreHook> hook);\n+  const std::vector<std::shared_ptr<FunctionPreHook>>& hooks() const noexcept;\n+  void clear_hooks();\n \n-  inline auto_unique_ptr<jit::tracer::ValueTracingState>& tracing_state() const;\n+  // JIT Tracing\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-  inline int current_version() const;\n+  void set_tracing_state(jit::tracer::ValueTracingState* new_tracing_state);\n+  jit::tracer::ValueTracingState& tracing_state() const noexcept;\n+\n+  /// Returns true if the `Variable`'s tracing state is not null.\n+  bool has_tracing_state() const noexcept;\n+\n+  // View Variables\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  /// Returns true if this `Variable` is a view of another `Variable`.\n+  bool is_view() const noexcept;\n+\n+  /// Returns the `Variable` that this `Variable` is a view of. If this\n+  /// `Variable` is not a view, throw a `std::runtime_error`.\n+  const Variable& base() const;\n+\n+  // Miscellaneous\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  /// Compares this `Variable` to another `Variable` (or `Tensor`) via\n+  /// pointer-equality.\n+  bool is_same(const Variable& other) const noexcept {\n+    return this->pImpl == other.pImpl;\n+  }\n \n-  inline VariableVersion& version_counter() const;\n+  void set_name(const std::string& name);\n+  const std::string& name() const noexcept;\n \n-  inline const int& output_nr() const;\n-  inline       int& output_nr();\n+  PyObject* pyobj() const noexcept;\n+  void set_pyobj(PyObject* pyobj) noexcept;\n \n-  inline bool requires_grad() const;\n+  // Hacks!\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n-  inline bool is_view() const;\n-  inline Variable& base() const;\n+  /// Sets the type of the underlying `Tensor`. Used for a bad (hopefully)\n+  /// temporary hack in python_variable.h. If removed, also remove the `using\n+  /// at::TensorImpl::type_;` in `Variable::Impl`.\n+  void temporary_hack_set_type(at::Type*) noexcept;\n \n-  inline const std::string& name() const;\n-  inline       std::string& name();\n+ private:\n+  /// Private implementation struct of the `Variable`. This struct declaration\n+  /// and the `get()` method which exposes it shall forever remain private and\n+  /// never be exposed to the public interface of this class.\n+  struct Impl;\n+  struct ViewImpl;\n \n-  inline Variable & operator=(Variable && rhs) &;\n-  inline Variable & operator=(const Variable & rhs) &;\n-  inline Variable & operator=(Tensor && rhs) &;\n-  inline Variable & operator=(const Tensor & rhs) &;\n+  // Private Methods\n+  //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+  Variable(Variable::Impl* self, bool retain);\n+  Impl* get() const noexcept;\n };\n \n-struct VariableImpl : public at::TensorImpl {\n-public:\n-  VariableImpl(at::Tensor data, bool requires_grad=false, int output_nr=0,\n-               std::shared_ptr<Function> grad_fn=nullptr);\n-  virtual ~VariableImpl();\n-  virtual const char * toString() const override;\n-  virtual at::IntList sizes() const override;\n-  virtual at::IntList strides() const override;\n-  virtual int64_t dim() const override;\n-  virtual at::Scalar localScalar() override;\n-  virtual void * unsafeGetTH(bool retain) override;\n-  virtual std::unique_ptr<at::Storage> storage() override;\n-  static const char * typeString();\n-\n-public:\n+//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+//                            Variable::Impl\n+//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+struct Variable::Impl : public at::TensorImpl {\n+  explicit Impl(\n+      at::Tensor data_,\n+      bool requires_grad_ = false,\n+      Edge edge = Edge());\n+\n+  virtual ~Impl();\n+\n+  const char* toString() const override;\n+  at::IntList sizes() const override;\n+  at::IntList strides() const override;\n+  int64_t dim() const override;\n+  at::Scalar localScalar() override;\n+  void* unsafeGetTH(bool retain) override;\n+  std::unique_ptr<at::Storage> storage() override;\n+  static const char* typeString();\n+\n   std::shared_ptr<Function> get_grad_accumulator();\n-  virtual std::shared_ptr<Function>& get_grad_fn() { return _grad_fn; }\n+  virtual std::shared_ptr<Function>& get_grad_fn() {\n+    return grad_fn;\n+  }\n \n+  // Make this field public so we can access it from `Variable`. Part of\n+  // temporary_hack_set_type.\n+  using at::TensorImpl::type_;\n+\n+  std::string name;\n   at::Tensor data;\n+\n   Variable grad;\n-  std::shared_ptr<Function> _grad_fn;\n+  std::shared_ptr<Function> grad_fn;\n+  std::weak_ptr<Function> grad_accumulator;\n+\n   VariableVersion version_counter;\n   std::vector<std::shared_ptr<FunctionPreHook>> hooks;\n-  std::weak_ptr<Function> grad_accumulator;\n-  // Mutex to ensure that concurrent read operations that modify internal state\n-  // are still thread-safe. Used by get_grad_fn and get_grad_accumulator.\n-  std::mutex mutex;\n-  bool _requires_grad;  // only meaningful on leaf variables (must be false otherwise)\n+\n+  bool requires_grad; // only meaningful on leaf variables (must be false\n+                      // otherwise)\n   bool is_view;\n   // The \"output number\" of this variable; e.g., if this variable\n   // was the second output of a function, then output_nr == 1.\n   // We use this to make sure we can setup the backwards trace\n   // correctly when this variable is passed to another function.\n-  int output_nr;\n-  PyObject *pyobj;  // weak reference\n+  uint32_t output_nr;\n+  PyObject* pyobj; // weak reference\n \n-  std::string name;\n+  // Mutex to ensure that concurrent read operations that modify internal\n+  // state are still thread-safe. Used by get_grad_fn and\n+  // get_grad_accumulator.\n+  std::mutex mutex;\n \n   // For use in torch::jit::tracer\n   auto_unique_ptr<jit::tracer::ValueTracingState> tracing_state;\n-  friend struct VariableType;\n };\n \n-// A Variable that is a view on another Variable. The base and view share the\n-// same version_counter. The _grad_fn field of the Variable may become stale\n-// due to in-place modifications of the shared data. Accesses should go through\n-// get_grad_fn(). All other fields are always valid.\n-struct VariableViewImpl : public VariableImpl {\n-  VariableViewImpl(Variable base, at::Tensor data, int output_nr, std::shared_ptr<Function> grad_fn);\n+//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+//                          Variable::ViewImpl\n+//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+/// A Variable that is a view on another Variable. The base and view share the\n+/// same version_counter. The grad_fn field of the Variable may become stale\n+/// due to in-place modifications of the shared data. Accesses should go\n+/// through get_grad_fn(). All other fields are always valid.\n+struct Variable::ViewImpl : public Variable::Impl {\n+  ViewImpl(Variable base_, at::Tensor data_, Edge gradient_edge);\n \n-  // Gets the up-to-date grad_fn. If the shared data or base was modified, we\n-  // re-create the grad_fn to express the up-to-date view relationship between\n-  // this and the base Variable.\n+  /// Gets the up-to-date grad_fn. If the shared data or base was modified, we\n+  /// re-create the grad_fn to express the up-to-date view relationship between\n+  /// this and the base Variable.\n   virtual std::shared_ptr<Function>& get_grad_fn() override;\n \n-  // Called after in-place modifications. Modifies the grad_fn of the base\n-  // Variable.\n-  void rebase_history(int output_nr, std::shared_ptr<Function> grad_fn);\n+  /// Called after in-place modifications. Modifies the grad_fn of the base\n+  /// Variable.\n+  void rebase_history(Edge gradient_edge);\n \n-  // The base Variable (never a view)\n+  /// The base `Variable` (never a view).\n   Variable base;\n \n-  // The value of the version_counter at the time grad_fn was created. The\n-  // _grad_fn field is stale if attr_version != version_counter.current_version()\n-  int attr_version;\n+  /// The value of the version_counter at the time grad_fn was created. The\n+  /// grad_fn field is stale if attr_version !=\n+  /// version_counter.current_version().\n+  uint32_t attr_version;\n };\n \n-inline Variable make_variable(at::Tensor data, bool requires_grad=false) {\n-  if (!data.defined()) {\n-    return Variable();\n-  }\n+//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+//                        Variable Implementation\n+//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n \n+namespace detail {\n+inline at::Tensor handle_scalars(at::Tensor& data) {\n #ifndef WITH_SCALARS\n   if (data.dim() == 0) {\n-    // don't expose 0-dim tensors to Variable API.\n-    data = data.as_strided_({1}, {1});\n+    // Don't expose 0-dim tensors to Variable API.\n+    return data.as_strided_({1}, {1});\n   }\n #endif\n+  return data;\n+}\n+} // namespace detail\n+\n+// Factory Functions\n+//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+inline Variable make_variable_view(\n+    Variable base,\n+    at::Tensor data,\n+    Edge gradient_edge = Edge()) {\n+  if (data.defined()) {\n+    data = detail::handle_scalars(data);\n+    auto impl = new Variable::ViewImpl(\n+        std::move(base), std::move(data), std::move(gradient_edge));\n+    return Variable(impl, /*retain=*/false);\n+  }\n+  return Variable();\n+}\n \n-  return Variable(new VariableImpl(std::move(data), requires_grad), false);\n+inline Variable make_variable(at::Tensor data, bool requires_grad) {\n+  if (data.defined()) {\n+    auto impl = new Variable::Impl(detail::handle_scalars(data), requires_grad);\n+    return Variable(impl, /*retain=*/false);\n+  }\n+  return Variable();\n }\n \n-inline Variable make_variable(at::Tensor data, int output_nr, std::shared_ptr<Function> grad_fn) {\n-  if (!data.defined()) {\n-    return Variable();\n+inline Variable make_variable(at::Tensor data, Edge gradient_edge) {\n+  if (data.defined()) {\n+    auto impl = new Variable::Impl(\n+        detail::handle_scalars(data), false, std::move(gradient_edge));\n+    return Variable(impl, /*retain=*/false);\n   }\n+  return Variable();\n+}\n \n-#ifndef WITH_SCALARS\n-  if (data.dim() == 0) {\n-    // don't expose 0-dim tensors to Variable API.\n-    data = data.as_strided_({1}, {1});\n+// Tensor Conversion\n+//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+inline Variable& as_variable_ref(at::Tensor& tensor) {\n+#ifdef DEBUG\n+  // dynamic_cast will return a nullptr if the `TensorImpl`'s dynamic type is\n+  // not `Variable::Impl`.\n+  if (dynamic_cast<Variable::Impl*>(tensor.get()) == nullptr) {", "path": "torch/csrc/autograd/variable.h", "position": 564, "original_position": 558, "commit_id": "eba1e177795cddcd1971096dbccdcd81e0dcd773", "original_commit_id": "b2eb9e59dd9969c5c8c9d21036652c3e0a281de9", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "body": "This can be a `ViewImpl` too right?", "created_at": "2018-02-12T12:14:47Z", "updated_at": "2018-11-23T15:39:28Z", "html_url": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167538036", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/5127", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/167538036"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/5127#discussion_r167538036"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/5127"}}, "body_html": "<p>This can be a <code>ViewImpl</code> too right?</p>", "body_text": "This can be a ViewImpl too right?"}