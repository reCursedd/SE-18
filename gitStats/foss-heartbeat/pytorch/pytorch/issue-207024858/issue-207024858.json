{"url": "https://api.github.com/repos/pytorch/pytorch/issues/721", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/721/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/721/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/721/events", "html_url": "https://github.com/pytorch/pytorch/pull/721", "id": 207024858, "node_id": "MDExOlB1bGxSZXF1ZXN0MTA1Nzc3MDU0", "number": 721, "title": "fix serialization bug for large files", "user": {"login": "adamlerer", "id": 5702157, "node_id": "MDQ6VXNlcjU3MDIxNTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/5702157?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adamlerer", "html_url": "https://github.com/adamlerer", "followers_url": "https://api.github.com/users/adamlerer/followers", "following_url": "https://api.github.com/users/adamlerer/following{/other_user}", "gists_url": "https://api.github.com/users/adamlerer/gists{/gist_id}", "starred_url": "https://api.github.com/users/adamlerer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adamlerer/subscriptions", "organizations_url": "https://api.github.com/users/adamlerer/orgs", "repos_url": "https://api.github.com/users/adamlerer/repos", "events_url": "https://api.github.com/users/adamlerer/events{/privacy}", "received_events_url": "https://api.github.com/users/adamlerer/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-02-12T02:53:44Z", "updated_at": "2017-02-12T18:13:04Z", "closed_at": "2017-02-12T18:13:04Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/721", "html_url": "https://github.com/pytorch/pytorch/pull/721", "diff_url": "https://github.com/pytorch/pytorch/pull/721.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/721.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #717.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"206939335\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/717\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/717/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/717\">#717</a></p>\n<p>read(2) and write(2) are not guaranteed to write arbitrarily large sizes, and at least on my system (CentOS7) they read/write a maximum of 2GB. This writes in a loop until the whole tensor is written.</p>\n<p>Test:</p>\n<pre><code>import torch\nt = torch.IntTensor(600000000).fill_(42)\nt[0]=100\nt[-1]=101\nIn [9]: torch.save(t, \"foo.pt\")\nu = torch.load('foo.pt')\nprint(u[:5])\nprint(u[-5:])\n\n 100\n  42\n  42\n  42\n  42\n[torch.IntTensor of size 5]\n\n  42\n  42\n  42\n  42\n 101\n[torch.IntTensor of size 5]\n</code></pre>\n<p>This may also <span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #718.\">fix</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"206939646\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/718\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/718/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/718\">#718</a> but I haven't written any tests for big-endian.</p>", "body_text": "Fixes #717\nread(2) and write(2) are not guaranteed to write arbitrarily large sizes, and at least on my system (CentOS7) they read/write a maximum of 2GB. This writes in a loop until the whole tensor is written.\nTest:\nimport torch\nt = torch.IntTensor(600000000).fill_(42)\nt[0]=100\nt[-1]=101\nIn [9]: torch.save(t, \"foo.pt\")\nu = torch.load('foo.pt')\nprint(u[:5])\nprint(u[-5:])\n\n 100\n  42\n  42\n  42\n  42\n[torch.IntTensor of size 5]\n\n  42\n  42\n  42\n  42\n 101\n[torch.IntTensor of size 5]\n\nThis may also fix #718 but I haven't written any tests for big-endian.", "body": "Fixes https://github.com/pytorch/pytorch/issues/717\r\n\r\nread(2) and write(2) are not guaranteed to write arbitrarily large sizes, and at least on my system (CentOS7) they read/write a maximum of 2GB. This writes in a loop until the whole tensor is written.\r\n\r\nTest:\r\n```\r\nimport torch\r\nt = torch.IntTensor(600000000).fill_(42)\r\nt[0]=100\r\nt[-1]=101\r\nIn [9]: torch.save(t, \"foo.pt\")\r\nu = torch.load('foo.pt')\r\nprint(u[:5])\r\nprint(u[-5:])\r\n\r\n 100\r\n  42\r\n  42\r\n  42\r\n  42\r\n[torch.IntTensor of size 5]\r\n\r\n  42\r\n  42\r\n  42\r\n  42\r\n 101\r\n[torch.IntTensor of size 5]\r\n```\r\n\r\nThis may also fix https://github.com/pytorch/pytorch/issues/718 but I haven't written any tests for big-endian."}