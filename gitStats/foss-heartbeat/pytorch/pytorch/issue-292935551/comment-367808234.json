{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/367808234", "html_url": "https://github.com/pytorch/pytorch/issues/4946#issuecomment-367808234", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/4946", "id": 367808234, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzgwODIzNA==", "user": {"login": "fmassa", "id": 9110200, "node_id": "MDQ6VXNlcjkxMTAyMDA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9110200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fmassa", "html_url": "https://github.com/fmassa", "followers_url": "https://api.github.com/users/fmassa/followers", "following_url": "https://api.github.com/users/fmassa/following{/other_user}", "gists_url": "https://api.github.com/users/fmassa/gists{/gist_id}", "starred_url": "https://api.github.com/users/fmassa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fmassa/subscriptions", "organizations_url": "https://api.github.com/users/fmassa/orgs", "repos_url": "https://api.github.com/users/fmassa/repos", "events_url": "https://api.github.com/users/fmassa/events{/privacy}", "received_events_url": "https://api.github.com/users/fmassa/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-22T20:16:30Z", "updated_at": "2018-02-22T20:16:30Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=975964\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/varunagrawal\">@varunagrawal</a> The current ROI pooling that is present in pytorch is broken. The extension is written in C++ using ATen, and uses pybind11 to expose it to python automatically.<br>\nSo, it looks like</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> roi_align.cpp</span>\nat::Tensor <span class=\"pl-en\">ROIAlign_forward</span>(<span class=\"pl-k\">const</span> at::Tensor&amp; input,\n                            <span class=\"pl-k\">const</span> at::Tensor&amp; rois,\n                            <span class=\"pl-k\">const</span> <span class=\"pl-k\">float</span> spatial_scale,\n                            <span class=\"pl-k\">const</span> <span class=\"pl-k\">int</span> pooled_height,\n                            <span class=\"pl-k\">const</span> <span class=\"pl-k\">int</span> pooled_width,\n                            <span class=\"pl-k\">const</span> <span class=\"pl-k\">int</span> sampling_ratio) {\n <span class=\"pl-c\"><span class=\"pl-c\">//</span> implementation goes here</span>\n}\n\n<span class=\"pl-en\">PYBIND11_MODULE</span>(library, m) {\n  m.<span class=\"pl-c1\">def</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>roi_align_forward<span class=\"pl-pds\">\"</span></span>, &amp;ROIAlign_forward, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ROIAlign_forward<span class=\"pl-pds\">\"</span></span>);\n}</pre></div>\n<p>and on the python side, we have</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> torch.utils.cpp_extension <span class=\"pl-k\">import</span> load\n_C <span class=\"pl-k\">=</span> load(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>library<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>roi_align.cpp<span class=\"pl-pds\">'</span></span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> can call _C.roi_align_forward straight away.</span></pre></div>", "body_text": "@varunagrawal The current ROI pooling that is present in pytorch is broken. The extension is written in C++ using ATen, and uses pybind11 to expose it to python automatically.\nSo, it looks like\n// roi_align.cpp\nat::Tensor ROIAlign_forward(const at::Tensor& input,\n                            const at::Tensor& rois,\n                            const float spatial_scale,\n                            const int pooled_height,\n                            const int pooled_width,\n                            const int sampling_ratio) {\n // implementation goes here\n}\n\nPYBIND11_MODULE(library, m) {\n  m.def(\"roi_align_forward\", &ROIAlign_forward, \"ROIAlign_forward\");\n}\nand on the python side, we have\nfrom torch.utils.cpp_extension import load\n_C = load('library', ['roi_align.cpp'])\n\n# can call _C.roi_align_forward straight away.", "body": "@varunagrawal The current ROI pooling that is present in pytorch is broken. The extension is written in C++ using ATen, and uses pybind11 to expose it to python automatically.\r\nSo, it looks like\r\n```c++\r\n// roi_align.cpp\r\nat::Tensor ROIAlign_forward(const at::Tensor& input,\r\n                            const at::Tensor& rois,\r\n                            const float spatial_scale,\r\n                            const int pooled_height,\r\n                            const int pooled_width,\r\n                            const int sampling_ratio) {\r\n // implementation goes here\r\n}\r\n\r\nPYBIND11_MODULE(library, m) {\r\n  m.def(\"roi_align_forward\", &ROIAlign_forward, \"ROIAlign_forward\");\r\n}\r\n```\r\n\r\nand on the python side, we have\r\n```python\r\nfrom torch.utils.cpp_extension import load\r\n_C = load('library', ['roi_align.cpp'])\r\n\r\n# can call _C.roi_align_forward straight away.\r\n```"}