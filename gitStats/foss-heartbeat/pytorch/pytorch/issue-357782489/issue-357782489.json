{"url": "https://api.github.com/repos/pytorch/pytorch/issues/11341", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/11341/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/11341/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/11341/events", "html_url": "https://github.com/pytorch/pytorch/pull/11341", "id": 357782489, "node_id": "MDExOlB1bGxSZXF1ZXN0MjEzNzE2NTM5", "number": 11341, "title": "Add .expand() method to distribution classes", "user": {"login": "neerajprad", "id": 1762463, "node_id": "MDQ6VXNlcjE3NjI0NjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1762463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neerajprad", "html_url": "https://github.com/neerajprad", "followers_url": "https://api.github.com/users/neerajprad/followers", "following_url": "https://api.github.com/users/neerajprad/following{/other_user}", "gists_url": "https://api.github.com/users/neerajprad/gists{/gist_id}", "starred_url": "https://api.github.com/users/neerajprad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neerajprad/subscriptions", "organizations_url": "https://api.github.com/users/neerajprad/orgs", "repos_url": "https://api.github.com/users/neerajprad/repos", "events_url": "https://api.github.com/users/neerajprad/events{/privacy}", "received_events_url": "https://api.github.com/users/neerajprad/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2018-09-06T18:55:37Z", "updated_at": "2018-11-23T15:50:46Z", "closed_at": "2018-09-11T13:57:51Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/11341", "html_url": "https://github.com/pytorch/pytorch/pull/11341", "diff_url": "https://github.com/pytorch/pytorch/pull/11341.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/11341.patch"}, "body_html": "<p>This adds a <code>.expand</code> method for distributions that is akin to the <code>torch.Tensor.expand</code> method for tensors. It returns a new distribution instance with batch dimensions expanded to the desired <code>batch_shape</code>. Since this calls <code>torch.Tensor.expand</code> on the distribution's parameters, it does not allocate new memory for the expanded distribution instance's parameters.</p>\n<p>e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> d <span class=\"pl-k\">=</span> dist.Normal(torch.zeros(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">1</span>), torch.ones(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">1</span>))\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> d.sample().shape\n  torch.Size([<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">1</span>])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> d.expand([<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10</span>]).sample().shape\n  torch.Size([<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">10</span>])</pre></div>\n<h3>Motivation</h3>\n<p>We have already been using the <code>.expand</code> method in Pyro in our <a href=\"https://github.com/uber/pyro/blob/dev/pyro/distributions/torch.py#L10\">patch</a> of <code>torch.distributions</code>. We use this in our models to enable dynamic broadcasting. This has also been requested by a few users on the distributions slack, and we believe will be useful to the larger community.</p>\n<p>Note that currently, there is no convenient and efficient way to expand distribution instances:</p>\n<ul>\n<li>Many distributions use <code>TransformedDistribution</code> (or wrap over another distribution instance. e.g. <code>OneHotCategorical</code> uses a <code>Categorical</code> instance) under the hood, or have lazy parameters. This makes it difficult to collect all the relevant parameters, broadcast them and construct new instances.</li>\n<li>In the few cases where this is even possible, the resulting implementation would be inefficient since we will go through a lot of broadcasting and args validation logic in <code>__init__.py</code> that can be avoided.</li>\n</ul>\n<p>The <code>.expand</code> method allows for a safe and efficient way to expand distribution instances. Additionally, this bypasses <code>__init__.py</code> (using <code>__new__</code> and populating relevant attributes) since we do not need to do any broadcasting or args validation (which was already done when the instance was first created). This can result in significant savings as compared to constructing new instances via <code>__init__</code> (that said, the <code>sample</code> and <code>log_prob</code> methods will probably be the rate determining steps in many applications).</p>\n<p>e.g.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> a <span class=\"pl-k\">=</span> dist.Bernoulli(torch.ones([<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">1</span>]), <span class=\"pl-v\">validate_args</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">%</span>timeit a.expand([<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">100</span>])\n<span class=\"pl-c1\">15.2</span> \u00b5s \u00b1 <span class=\"pl-c1\">224</span> ns per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100000</span> loops each)\n\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">%</span>timeit dist.Bernoulli(torch.ones([<span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">100</span>]), <span class=\"pl-v\">validate_args</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c1\">11.8</span> ms \u00b1 <span class=\"pl-c1\">153</span> \u00b5s per loop (mean \u00b1 std. dev. of <span class=\"pl-c1\">7</span> runs, <span class=\"pl-c1\">100</span> loops each)</pre></div>\n<p>cc. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=648532\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fritzo\">@fritzo</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23639302\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vishwakftw\">@vishwakftw</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1093846\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alicanb\">@alicanb</a></p>", "body_text": "This adds a .expand method for distributions that is akin to the torch.Tensor.expand method for tensors. It returns a new distribution instance with batch dimensions expanded to the desired batch_shape. Since this calls torch.Tensor.expand on the distribution's parameters, it does not allocate new memory for the expanded distribution instance's parameters.\ne.g.\n>>> d = dist.Normal(torch.zeros(100, 1), torch.ones(100, 1))\n>>> d.sample().shape\n  torch.Size([100, 1])\n>>> d.expand([100, 10]).sample().shape\n  torch.Size([100, 10])\nMotivation\nWe have already been using the .expand method in Pyro in our patch of torch.distributions. We use this in our models to enable dynamic broadcasting. This has also been requested by a few users on the distributions slack, and we believe will be useful to the larger community.\nNote that currently, there is no convenient and efficient way to expand distribution instances:\n\nMany distributions use TransformedDistribution (or wrap over another distribution instance. e.g. OneHotCategorical uses a Categorical instance) under the hood, or have lazy parameters. This makes it difficult to collect all the relevant parameters, broadcast them and construct new instances.\nIn the few cases where this is even possible, the resulting implementation would be inefficient since we will go through a lot of broadcasting and args validation logic in __init__.py that can be avoided.\n\nThe .expand method allows for a safe and efficient way to expand distribution instances. Additionally, this bypasses __init__.py (using __new__ and populating relevant attributes) since we do not need to do any broadcasting or args validation (which was already done when the instance was first created). This can result in significant savings as compared to constructing new instances via __init__ (that said, the sample and log_prob methods will probably be the rate determining steps in many applications).\ne.g.\n>>> a = dist.Bernoulli(torch.ones([10000, 1]), validate_args=True)\n\n>>> %timeit a.expand([10000, 100])\n15.2 \u00b5s \u00b1 224 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n>>> %timeit dist.Bernoulli(torch.ones([10000, 100]), validate_args=True)\n11.8 ms \u00b1 153 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\ncc. @fritzo, @apaszke, @vishwakftw, @alicanb", "body": "This adds a `.expand` method for distributions that is akin to the `torch.Tensor.expand` method for tensors. It returns a new distribution instance with batch dimensions expanded to the desired `batch_shape`. Since this calls `torch.Tensor.expand` on the distribution's parameters, it does not allocate new memory for the expanded distribution instance's parameters. \r\n\r\ne.g.\r\n```python\r\n>>> d = dist.Normal(torch.zeros(100, 1), torch.ones(100, 1))\r\n>>> d.sample().shape\r\n  torch.Size([100, 1])\r\n>>> d.expand([100, 10]).sample().shape\r\n  torch.Size([100, 10])\r\n```\r\n\r\n### Motivation\r\n\r\nWe have already been using the `.expand` method in Pyro in our [patch](https://github.com/uber/pyro/blob/dev/pyro/distributions/torch.py#L10) of `torch.distributions`. We use this in our models to enable dynamic broadcasting. This has also been requested by a few users on the distributions slack, and we believe will be useful to the larger community. \r\n\r\nNote that currently, there is no convenient and efficient way to expand distribution instances:\r\n - Many distributions use `TransformedDistribution` (or wrap over another distribution instance. e.g. `OneHotCategorical` uses a `Categorical` instance) under the hood, or have lazy parameters. This makes it difficult to collect all the relevant parameters, broadcast them and construct new instances.\r\n - In the few cases where this is even possible, the resulting implementation would be inefficient since we will go through a lot of broadcasting and args validation logic in `__init__.py` that can be avoided.\r\n\r\nThe `.expand` method allows for a safe and efficient way to expand distribution instances. Additionally, this bypasses `__init__.py` (using `__new__` and populating relevant attributes) since we do not need to do any broadcasting or args validation (which was already done when the instance was first created). This can result in significant savings as compared to constructing new instances via `__init__` (that said, the `sample` and `log_prob` methods will probably be the rate determining steps in many applications).\r\n\r\ne.g.\r\n```python\r\n>>> a = dist.Bernoulli(torch.ones([10000, 1]), validate_args=True)\r\n\r\n>>> %timeit a.expand([10000, 100])\r\n15.2 \u00b5s \u00b1 224 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\r\n\r\n>>> %timeit dist.Bernoulli(torch.ones([10000, 100]), validate_args=True)\r\n11.8 ms \u00b1 153 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\r\n```\r\n\r\ncc. @fritzo, @apaszke, @vishwakftw, @alicanb "}