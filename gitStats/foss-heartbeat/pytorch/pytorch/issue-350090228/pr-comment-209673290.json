{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209673290", "pull_request_review_id": 145732020, "id": 209673290, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTY3MzI5MA==", "diff_hunk": "@@ -236,12 +236,35 @@ struct GraphFuser {\n     return true;\n   }\n \n+  // XXX: this is O(n) where n is the number of outputs of the FusionGroup\n+  bool isFusedConcatOutput(Value * producer) {\n+    JIT_ASSERT(producer->node()->kind() == prim::FusionGroup);\n+    auto * fusion_group = producer->node();\n+\n+    // Find the output index\n+    auto outputs = fusion_group->outputs();\n+    auto it = std::find(outputs.begin(), outputs.end(), producer);\n+    JIT_ASSERT(it != outputs.end());\n+    int64_t output_index = it - outputs.begin();", "path": "torch/csrc/jit/passes/graph_fuser.cpp", "position": null, "original_position": 13, "commit_id": "ac93d291c8d2d9d2067da71faeffab9a3a85fa7c", "original_commit_id": "fb571a337de415b424f63cf424a8d450f68435a0", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "Yes, we can have both concatenated and regular outputs from a fusion group:\r\n```\r\n\r\nIn [1]: import torch\r\n   ...: @torch.jit.script\r\n   ...: def fn(x, y, z):\r\n   ...:     x1 = x + y\r\n   ...:     y1 = x - y\r\n   ...:     w = torch.cat([x1, y1])\r\n   ...:     return w, x1, y1\r\n   ...:\r\n   ...: x = torch.randn(2, 2, dtype=torch.float, device='cpu')\r\n   ...: y = torch.randn(2, 2, dtype=torch.float, device='cpu')\r\n   ...: z = torch.randn(4, 2, dtype=torch.float, device='cpu')\r\n   ...: fn(x, y, z)\r\n   ...: fn.graph_for(x, y, z)\r\n   ...:\r\n   ...:\r\nOut[1]:\r\ngraph(%x : Float(2, 2)\r\n      %y : Float(2, 2)\r\n      %z : Float(4, 2)) {\r\n  %3 : int = prim::Constant[value=1]()\r\n  %y1 : Float(2, 2) = aten::sub(%x, %y, %3)\r\n  %w : Float(4, 2), %x1 : Float(2, 2) = prim::FusionGroup_0[device=-1](%y1, %x, %y)\r\n  %8 : int = prim::Constant[value=0]()\r\n  return (%w, %x1, %y1);\r\n}\r\nwith prim::FusionGroup_0 = graph(%1 : Float(2, 2)\r\n      %3 : Float(2, 2)\r\n      %4 : Float(2, 2)) {\r\n  %5 : int = prim::Constant[value=1]()\r\n  %x1 : Float(2, 2) = aten::add(%3, %4, %5)\r\n  %w : Float(4, 2) = prim::FusedConcat[dim=0](%x1, %1)\r\n  return (%w, %x1);\r\n}\r\n```\r\n\r\nand yes, we can have multiple concats fused as well:\r\n```\r\n\r\nIn [6]: import torch\r\n   ...: @torch.jit.script\r\n   ...: def fn(x, y):\r\n   ...:     x1 = x + y\r\n   ...:     y1 = x + 2*y\r\n   ...:     x2 = y + 3*x\r\n   ...:     y2 = y + 4*x\r\n   ...:     w1 = torch.cat([x1, y2])\r\n   ...:     w2 = torch.cat([x2, y2])\r\n   ...:     return w1, w2\r\n   ...:\r\n   ...: x = torch.randn(2, 2, dtype=torch.float, device='cpu')\r\n   ...: y = torch.randn(2, 2, dtype=torch.float, device='cpu')\r\n   ...: fn(x, y)\r\n   ...: fn.graph_for(x, y)\r\n   ...:\r\n   ...:\r\nOut[6]:\r\ngraph(%x : Float(2, 2)\r\n      %y : Float(2, 2)) {\r\n  %2 : int = prim::Constant[value=1]()\r\n  %4 : int = prim::Constant[value=3]()\r\n  %5 : Float(2, 2) = aten::mul(%4, %x)\r\n  %8 : int = prim::Constant[value=4]()\r\n  %9 : Float(2, 2) = aten::mul(%8, %x)\r\n  %13 : int = prim::Constant[value=0]()\r\n  %w2 : Float(4, 2), %28 : Float(4, 2) = prim::FusionGroup_0[device=-1](%y, %5, %9, %x)\r\n  return (%28, %w2);\r\n}\r\nwith prim::FusionGroup_0 = graph(%3 : Float(2, 2)\r\n      %4 : Float(2, 2)\r\n      %9 : Float(2, 2)\r\n      %13 : Float(2, 2)) {\r\n  %16 : int = prim::Constant[value=1]()\r\n  %14 : int = prim::Constant[value=1]()\r\n  %x1 : Float(2, 2) = aten::add(%13, %3, %14)\r\n  %12 : int = prim::Constant[value=1]()\r\n  %10 : int = prim::Constant[value=1]()\r\n  %y2 : Float(2, 2) = aten::add(%3, %9, %10)\r\n  %w1 : Float(4, 2) = prim::FusedConcat[dim=0](%x1, %y2)\r\n  %5 : int = prim::Constant[value=1]()\r\n  %x2 : Float(2, 2) = aten::add(%3, %4, %5)\r\n  %w2 : Float(4, 2) = prim::FusedConcat[dim=0](%x2, %y2)\r\n  return (%w2, %w1);\r\n}\r\n```", "created_at": "2018-08-13T16:23:45Z", "updated_at": "2018-11-23T15:49:09Z", "html_url": "https://github.com/pytorch/pytorch/pull/10466#discussion_r209673290", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10466", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209673290"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10466#discussion_r209673290"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10466"}}, "body_html": "<p>Yes, we can have both concatenated and regular outputs from a fusion group:</p>\n<pre><code>\nIn [1]: import torch\n   ...: @torch.jit.script\n   ...: def fn(x, y, z):\n   ...:     x1 = x + y\n   ...:     y1 = x - y\n   ...:     w = torch.cat([x1, y1])\n   ...:     return w, x1, y1\n   ...:\n   ...: x = torch.randn(2, 2, dtype=torch.float, device='cpu')\n   ...: y = torch.randn(2, 2, dtype=torch.float, device='cpu')\n   ...: z = torch.randn(4, 2, dtype=torch.float, device='cpu')\n   ...: fn(x, y, z)\n   ...: fn.graph_for(x, y, z)\n   ...:\n   ...:\nOut[1]:\ngraph(%x : Float(2, 2)\n      %y : Float(2, 2)\n      %z : Float(4, 2)) {\n  %3 : int = prim::Constant[value=1]()\n  %y1 : Float(2, 2) = aten::sub(%x, %y, %3)\n  %w : Float(4, 2), %x1 : Float(2, 2) = prim::FusionGroup_0[device=-1](%y1, %x, %y)\n  %8 : int = prim::Constant[value=0]()\n  return (%w, %x1, %y1);\n}\nwith prim::FusionGroup_0 = graph(%1 : Float(2, 2)\n      %3 : Float(2, 2)\n      %4 : Float(2, 2)) {\n  %5 : int = prim::Constant[value=1]()\n  %x1 : Float(2, 2) = aten::add(%3, %4, %5)\n  %w : Float(4, 2) = prim::FusedConcat[dim=0](%x1, %1)\n  return (%w, %x1);\n}\n</code></pre>\n<p>and yes, we can have multiple concats fused as well:</p>\n<pre><code>\nIn [6]: import torch\n   ...: @torch.jit.script\n   ...: def fn(x, y):\n   ...:     x1 = x + y\n   ...:     y1 = x + 2*y\n   ...:     x2 = y + 3*x\n   ...:     y2 = y + 4*x\n   ...:     w1 = torch.cat([x1, y2])\n   ...:     w2 = torch.cat([x2, y2])\n   ...:     return w1, w2\n   ...:\n   ...: x = torch.randn(2, 2, dtype=torch.float, device='cpu')\n   ...: y = torch.randn(2, 2, dtype=torch.float, device='cpu')\n   ...: fn(x, y)\n   ...: fn.graph_for(x, y)\n   ...:\n   ...:\nOut[6]:\ngraph(%x : Float(2, 2)\n      %y : Float(2, 2)) {\n  %2 : int = prim::Constant[value=1]()\n  %4 : int = prim::Constant[value=3]()\n  %5 : Float(2, 2) = aten::mul(%4, %x)\n  %8 : int = prim::Constant[value=4]()\n  %9 : Float(2, 2) = aten::mul(%8, %x)\n  %13 : int = prim::Constant[value=0]()\n  %w2 : Float(4, 2), %28 : Float(4, 2) = prim::FusionGroup_0[device=-1](%y, %5, %9, %x)\n  return (%28, %w2);\n}\nwith prim::FusionGroup_0 = graph(%3 : Float(2, 2)\n      %4 : Float(2, 2)\n      %9 : Float(2, 2)\n      %13 : Float(2, 2)) {\n  %16 : int = prim::Constant[value=1]()\n  %14 : int = prim::Constant[value=1]()\n  %x1 : Float(2, 2) = aten::add(%13, %3, %14)\n  %12 : int = prim::Constant[value=1]()\n  %10 : int = prim::Constant[value=1]()\n  %y2 : Float(2, 2) = aten::add(%3, %9, %10)\n  %w1 : Float(4, 2) = prim::FusedConcat[dim=0](%x1, %y2)\n  %5 : int = prim::Constant[value=1]()\n  %x2 : Float(2, 2) = aten::add(%3, %4, %5)\n  %w2 : Float(4, 2) = prim::FusedConcat[dim=0](%x2, %y2)\n  return (%w2, %w1);\n}\n</code></pre>", "body_text": "Yes, we can have both concatenated and regular outputs from a fusion group:\n\nIn [1]: import torch\n   ...: @torch.jit.script\n   ...: def fn(x, y, z):\n   ...:     x1 = x + y\n   ...:     y1 = x - y\n   ...:     w = torch.cat([x1, y1])\n   ...:     return w, x1, y1\n   ...:\n   ...: x = torch.randn(2, 2, dtype=torch.float, device='cpu')\n   ...: y = torch.randn(2, 2, dtype=torch.float, device='cpu')\n   ...: z = torch.randn(4, 2, dtype=torch.float, device='cpu')\n   ...: fn(x, y, z)\n   ...: fn.graph_for(x, y, z)\n   ...:\n   ...:\nOut[1]:\ngraph(%x : Float(2, 2)\n      %y : Float(2, 2)\n      %z : Float(4, 2)) {\n  %3 : int = prim::Constant[value=1]()\n  %y1 : Float(2, 2) = aten::sub(%x, %y, %3)\n  %w : Float(4, 2), %x1 : Float(2, 2) = prim::FusionGroup_0[device=-1](%y1, %x, %y)\n  %8 : int = prim::Constant[value=0]()\n  return (%w, %x1, %y1);\n}\nwith prim::FusionGroup_0 = graph(%1 : Float(2, 2)\n      %3 : Float(2, 2)\n      %4 : Float(2, 2)) {\n  %5 : int = prim::Constant[value=1]()\n  %x1 : Float(2, 2) = aten::add(%3, %4, %5)\n  %w : Float(4, 2) = prim::FusedConcat[dim=0](%x1, %1)\n  return (%w, %x1);\n}\n\nand yes, we can have multiple concats fused as well:\n\nIn [6]: import torch\n   ...: @torch.jit.script\n   ...: def fn(x, y):\n   ...:     x1 = x + y\n   ...:     y1 = x + 2*y\n   ...:     x2 = y + 3*x\n   ...:     y2 = y + 4*x\n   ...:     w1 = torch.cat([x1, y2])\n   ...:     w2 = torch.cat([x2, y2])\n   ...:     return w1, w2\n   ...:\n   ...: x = torch.randn(2, 2, dtype=torch.float, device='cpu')\n   ...: y = torch.randn(2, 2, dtype=torch.float, device='cpu')\n   ...: fn(x, y)\n   ...: fn.graph_for(x, y)\n   ...:\n   ...:\nOut[6]:\ngraph(%x : Float(2, 2)\n      %y : Float(2, 2)) {\n  %2 : int = prim::Constant[value=1]()\n  %4 : int = prim::Constant[value=3]()\n  %5 : Float(2, 2) = aten::mul(%4, %x)\n  %8 : int = prim::Constant[value=4]()\n  %9 : Float(2, 2) = aten::mul(%8, %x)\n  %13 : int = prim::Constant[value=0]()\n  %w2 : Float(4, 2), %28 : Float(4, 2) = prim::FusionGroup_0[device=-1](%y, %5, %9, %x)\n  return (%28, %w2);\n}\nwith prim::FusionGroup_0 = graph(%3 : Float(2, 2)\n      %4 : Float(2, 2)\n      %9 : Float(2, 2)\n      %13 : Float(2, 2)) {\n  %16 : int = prim::Constant[value=1]()\n  %14 : int = prim::Constant[value=1]()\n  %x1 : Float(2, 2) = aten::add(%13, %3, %14)\n  %12 : int = prim::Constant[value=1]()\n  %10 : int = prim::Constant[value=1]()\n  %y2 : Float(2, 2) = aten::add(%3, %9, %10)\n  %w1 : Float(4, 2) = prim::FusedConcat[dim=0](%x1, %y2)\n  %5 : int = prim::Constant[value=1]()\n  %x2 : Float(2, 2) = aten::add(%3, %4, %5)\n  %w2 : Float(4, 2) = prim::FusedConcat[dim=0](%x2, %y2)\n  return (%w2, %w1);\n}", "in_reply_to_id": 209670446}