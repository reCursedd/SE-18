{"url": "https://api.github.com/repos/pytorch/pytorch/issues/10466", "repository_url": "https://api.github.com/repos/pytorch/pytorch", "labels_url": "https://api.github.com/repos/pytorch/pytorch/issues/10466/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/pytorch/issues/10466/comments", "events_url": "https://api.github.com/repos/pytorch/pytorch/issues/10466/events", "html_url": "https://github.com/pytorch/pytorch/pull/10466", "id": 350090228, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA4MDI0MjMz", "number": 10466, "title": "[jit] Fix prim::FusedConcat bug", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 679953983, "node_id": "MDU6TGFiZWw2Nzk5NTM5ODM=", "url": "https://api.github.com/repos/pytorch/pytorch/labels/jit", "name": "jit", "color": "c5def5", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-08-13T15:56:22Z", "updated_at": "2018-11-23T15:49:15Z", "closed_at": "2018-08-14T04:10:20Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/pytorch/pytorch/pulls/10466", "html_url": "https://github.com/pytorch/pytorch/pull/10466", "diff_url": "https://github.com/pytorch/pytorch/pull/10466.diff", "patch_url": "https://github.com/pytorch/pytorch/pull/10466.patch"}, "body_html": "<p><span class=\"issue-keyword tooltipped tooltipped-se\" aria-label=\"This pull request closes issue #10456.\">Fixes</span> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"350042418\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/pytorch/pytorch/issues/10456\" data-hovercard-type=\"issue\" data-hovercard-url=\"/pytorch/pytorch/issues/10456/hovercard\" href=\"https://github.com/pytorch/pytorch/issues/10456\">#10456</a></p>\n<p>The graph fuser was fusing together groups with prim::FusedConcat (the producer) with other ops (the consumer) if the consumer is fusable. For example,</p>\n<pre><code>import torch\n@torch.jit.script\ndef fn(x, y, z):\n    x1 = x + y\n    y1 = x - y\n    w = torch.cat([x1, y1])\n    return w + z\n\nx = torch.randn(2, 2, dtype=torch.float, device='cpu')\ny = torch.randn(2, 2, dtype=torch.float, device='cpu')\nz = torch.randn(4, 2, dtype=torch.float, device='cpu')\nfn(x, y, z)\nfn.graph_for(x, y, z)\n</code></pre>\n<p>produced the following graph:</p>\n<pre><code>graph(%x : Float(2, 2)\n      %y : Float(2, 2)\n      %z : Float(4, 2)) {\n  %3 : int = prim::Constant[value=1]()\n  %y1 : Float(2, 2) = aten::sub(%x, %y, %3)\n  %8 : int = prim::Constant[value=0]()\n  %14 : Float(4, 2) = prim::FusionGroup_0[device=-1](%z, %y1, %x, %y)\n  return (%14);\n}\nwith prim::FusionGroup_0 = graph(%1 : Float(4, 2)\n      %5 : Float(2, 2)\n      %7 : Float(2, 2)\n      %8 : Float(2, 2)) {\n  %11 : int = prim::Constant[value=1]()\n  %9 : int = prim::Constant[value=1]()\n  %x1 : Float(2, 2) = aten::add(%7, %8, %9)\n  %w : Float(4, 2) = prim::FusedConcat[dim=0](%x1, %5)\n  %2 : int = prim::Constant[value=1]()\n  %3 : Float(4, 2) = aten::add(%w, %1, %2)\n  return (%3);\n}\n</code></pre>\n<p>this is a problem because it violates two invariants:</p>\n<ol>\n<li>all inputs to the FusionGroup must have the same size</li>\n<li>prim::FusedConcat's output must not be used inside the FusionGroup</li>\n</ol>\n<p>This PR fixes this problem by checking if the output to a FusionGroup came from a prim::FusedConcat node when deciding whether to fuse the consumer and producer.<br>\nIf the producer is a value that came from a prim::FusedConcat node in a FusionGroup, then consumer &amp; producer do not get fused.</p>\n<p>cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4583066\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/apaszke\">@apaszke</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=370202\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zdevito\">@zdevito</a></p>", "body_text": "Fixes #10456\nThe graph fuser was fusing together groups with prim::FusedConcat (the producer) with other ops (the consumer) if the consumer is fusable. For example,\nimport torch\n@torch.jit.script\ndef fn(x, y, z):\n    x1 = x + y\n    y1 = x - y\n    w = torch.cat([x1, y1])\n    return w + z\n\nx = torch.randn(2, 2, dtype=torch.float, device='cpu')\ny = torch.randn(2, 2, dtype=torch.float, device='cpu')\nz = torch.randn(4, 2, dtype=torch.float, device='cpu')\nfn(x, y, z)\nfn.graph_for(x, y, z)\n\nproduced the following graph:\ngraph(%x : Float(2, 2)\n      %y : Float(2, 2)\n      %z : Float(4, 2)) {\n  %3 : int = prim::Constant[value=1]()\n  %y1 : Float(2, 2) = aten::sub(%x, %y, %3)\n  %8 : int = prim::Constant[value=0]()\n  %14 : Float(4, 2) = prim::FusionGroup_0[device=-1](%z, %y1, %x, %y)\n  return (%14);\n}\nwith prim::FusionGroup_0 = graph(%1 : Float(4, 2)\n      %5 : Float(2, 2)\n      %7 : Float(2, 2)\n      %8 : Float(2, 2)) {\n  %11 : int = prim::Constant[value=1]()\n  %9 : int = prim::Constant[value=1]()\n  %x1 : Float(2, 2) = aten::add(%7, %8, %9)\n  %w : Float(4, 2) = prim::FusedConcat[dim=0](%x1, %5)\n  %2 : int = prim::Constant[value=1]()\n  %3 : Float(4, 2) = aten::add(%w, %1, %2)\n  return (%3);\n}\n\nthis is a problem because it violates two invariants:\n\nall inputs to the FusionGroup must have the same size\nprim::FusedConcat's output must not be used inside the FusionGroup\n\nThis PR fixes this problem by checking if the output to a FusionGroup came from a prim::FusedConcat node when deciding whether to fuse the consumer and producer.\nIf the producer is a value that came from a prim::FusedConcat node in a FusionGroup, then consumer & producer do not get fused.\ncc @apaszke @zdevito", "body": "Fixes #10456\r\n\r\nThe graph fuser was fusing together groups with prim::FusedConcat (the producer) with other ops (the consumer) if the consumer is fusable. For example,\r\n\r\n```\r\nimport torch\r\n@torch.jit.script\r\ndef fn(x, y, z):\r\n    x1 = x + y\r\n    y1 = x - y\r\n    w = torch.cat([x1, y1])\r\n    return w + z\r\n\r\nx = torch.randn(2, 2, dtype=torch.float, device='cpu')\r\ny = torch.randn(2, 2, dtype=torch.float, device='cpu')\r\nz = torch.randn(4, 2, dtype=torch.float, device='cpu')\r\nfn(x, y, z)\r\nfn.graph_for(x, y, z)\r\n```\r\nproduced the following graph:\r\n```\r\ngraph(%x : Float(2, 2)\r\n      %y : Float(2, 2)\r\n      %z : Float(4, 2)) {\r\n  %3 : int = prim::Constant[value=1]()\r\n  %y1 : Float(2, 2) = aten::sub(%x, %y, %3)\r\n  %8 : int = prim::Constant[value=0]()\r\n  %14 : Float(4, 2) = prim::FusionGroup_0[device=-1](%z, %y1, %x, %y)\r\n  return (%14);\r\n}\r\nwith prim::FusionGroup_0 = graph(%1 : Float(4, 2)\r\n      %5 : Float(2, 2)\r\n      %7 : Float(2, 2)\r\n      %8 : Float(2, 2)) {\r\n  %11 : int = prim::Constant[value=1]()\r\n  %9 : int = prim::Constant[value=1]()\r\n  %x1 : Float(2, 2) = aten::add(%7, %8, %9)\r\n  %w : Float(4, 2) = prim::FusedConcat[dim=0](%x1, %5)\r\n  %2 : int = prim::Constant[value=1]()\r\n  %3 : Float(4, 2) = aten::add(%w, %1, %2)\r\n  return (%3);\r\n}\r\n```\r\n\r\nthis is a problem because it violates two invariants:\r\n1) all inputs to the FusionGroup must have the same size\r\n2) prim::FusedConcat's output must not be used inside the FusionGroup\r\n\r\nThis PR fixes this problem by checking if the output to a FusionGroup came from a prim::FusedConcat node when deciding whether to fuse the consumer and producer.\r\nIf the producer is a value that came from a prim::FusedConcat node in a FusionGroup, then consumer & producer do not get fused.\r\n\r\ncc @apaszke @zdevito "}