{"url": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209820446", "pull_request_review_id": 145907108, "id": 209820446, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTgyMDQ0Ng==", "diff_hunk": "@@ -0,0 +1,17 @@\n+graph(%0 : Float(2, 2)\n+      %1 : Float(2, 2)\n+      %2 : Float(4, 2)) {\n+  %3 : int = prim::Constant[value=1]()\n+  %4 : Float(2, 2) = aten::sub(%0, %1, %3)", "path": "test/expect/TestJit.test_concat_fusion_invariant_cuda.expect", "position": 5, "original_position": 5, "commit_id": "ac93d291c8d2d9d2067da71faeffab9a3a85fa7c", "original_commit_id": "ac93d291c8d2d9d2067da71faeffab9a3a85fa7c", "user": {"login": "zou3519", "id": 5652049, "node_id": "MDQ6VXNlcjU2NTIwNDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5652049?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zou3519", "html_url": "https://github.com/zou3519", "followers_url": "https://api.github.com/users/zou3519/followers", "following_url": "https://api.github.com/users/zou3519/following{/other_user}", "gists_url": "https://api.github.com/users/zou3519/gists{/gist_id}", "starred_url": "https://api.github.com/users/zou3519/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zou3519/subscriptions", "organizations_url": "https://api.github.com/users/zou3519/orgs", "repos_url": "https://api.github.com/users/zou3519/repos", "events_url": "https://api.github.com/users/zou3519/events{/privacy}", "received_events_url": "https://api.github.com/users/zou3519/received_events", "type": "User", "site_admin": false}, "body": "we special case aten::add(Tensor, Tensor, Scalar) in the graph fuser to fuse it. We should do the same thing for sub but we don't do that right now: https://github.com/pytorch/pytorch/blob/f6eb966fd2ae22d1a96678f595b11cffa06e7fd0/torch/csrc/jit/passes/graph_fuser.cpp#L162-L164", "created_at": "2018-08-14T03:37:25Z", "updated_at": "2018-11-23T15:49:15Z", "html_url": "https://github.com/pytorch/pytorch/pull/10466#discussion_r209820446", "pull_request_url": "https://api.github.com/repos/pytorch/pytorch/pulls/10466", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/comments/209820446"}, "html": {"href": "https://github.com/pytorch/pytorch/pull/10466#discussion_r209820446"}, "pull_request": {"href": "https://api.github.com/repos/pytorch/pytorch/pulls/10466"}}, "body_html": "<p>we special case aten::add(Tensor, Tensor, Scalar) in the graph fuser to fuse it. We should do the same thing for sub but we don't do that right now: <div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/pytorch/pytorch/blob/f6eb966fd2ae22d1a96678f595b11cffa06e7fd0/torch/csrc/jit/passes/graph_fuser.cpp#L162-L164\">pytorch/torch/csrc/jit/passes/graph_fuser.cpp</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 162 to 164\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/pytorch/pytorch/commit/f6eb966fd2ae22d1a96678f595b11cffa06e7fd0\">f6eb966</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L162\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"162\"></td>\n          <td id=\"LC162\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> (node-&gt;<span class=\"pl-c1\">matches</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>aten::add(Tensor self, Tensor other, *, Scalar alpha) -&gt; Tensor<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c\"><span class=\"pl-c\">/*</span>const=<span class=\"pl-c\">*/</span></span>attr::alpha)) { </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L163\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"163\"></td>\n          <td id=\"LC163\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   std::vector&lt;Value*&gt; inputs {node-&gt;<span class=\"pl-c1\">namedInput</span>(attr::self), node-&gt;<span class=\"pl-c1\">namedInput</span>(attr::other)}; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L164\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"164\"></td>\n          <td id=\"LC164\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">return</span> <span class=\"pl-c1\">areTensorsOfSameShape</span>(inputs) &amp;&amp; <span class=\"pl-c1\">haveSupportedType</span>(inputs); </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>", "body_text": "we special case aten::add(Tensor, Tensor, Scalar) in the graph fuser to fuse it. We should do the same thing for sub but we don't do that right now: \n  \n    \n      pytorch/torch/csrc/jit/passes/graph_fuser.cpp\n    \n    \n        Lines 162 to 164\n      in\n      f6eb966\n    \n    \n    \n    \n\n        \n          \n           if (node->matches(\"aten::add(Tensor self, Tensor other, *, Scalar alpha) -> Tensor\", /*const=*/attr::alpha)) { \n        \n\n        \n          \n             std::vector<Value*> inputs {node->namedInput(attr::self), node->namedInput(attr::other)}; \n        \n\n        \n          \n             return areTensorsOfSameShape(inputs) && haveSupportedType(inputs);", "in_reply_to_id": 209771371}