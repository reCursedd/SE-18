{"url": "https://api.github.com/repos/pytorch/pytorch/issues/comments/295443250", "html_url": "https://github.com/pytorch/pytorch/pull/1305#issuecomment-295443250", "issue_url": "https://api.github.com/repos/pytorch/pytorch/issues/1305", "id": 295443250, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTQ0MzI1MA==", "user": {"login": "apaszke", "id": 4583066, "node_id": "MDQ6VXNlcjQ1ODMwNjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4583066?v=4", "gravatar_id": "", "url": "https://api.github.com/users/apaszke", "html_url": "https://github.com/apaszke", "followers_url": "https://api.github.com/users/apaszke/followers", "following_url": "https://api.github.com/users/apaszke/following{/other_user}", "gists_url": "https://api.github.com/users/apaszke/gists{/gist_id}", "starred_url": "https://api.github.com/users/apaszke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/apaszke/subscriptions", "organizations_url": "https://api.github.com/users/apaszke/orgs", "repos_url": "https://api.github.com/users/apaszke/repos", "events_url": "https://api.github.com/users/apaszke/events{/privacy}", "received_events_url": "https://api.github.com/users/apaszke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-19T21:07:38Z", "updated_at": "2017-04-19T21:07:38Z", "author_association": "MEMBER", "body_html": "<p>I wonder if it wouldn't be simpler and less magical to offer that deferred sparse weight decay as an alternative kind of an optimizer. The user would create the regular one with weight_decay=0 and then another one <code>wd = optim.DeferredWD(model.parameters())</code>. It seems more explicit, otherwise it might be easy to forget about <code>flush()</code> because right now the user doesn't really need to be aware which gradients are sparse.</p>", "body_text": "I wonder if it wouldn't be simpler and less magical to offer that deferred sparse weight decay as an alternative kind of an optimizer. The user would create the regular one with weight_decay=0 and then another one wd = optim.DeferredWD(model.parameters()). It seems more explicit, otherwise it might be easy to forget about flush() because right now the user doesn't really need to be aware which gradients are sparse.", "body": "I wonder if it wouldn't be simpler and less magical to offer that deferred sparse weight decay as an alternative kind of an optimizer. The user would create the regular one with weight_decay=0 and then another one `wd = optim.DeferredWD(model.parameters())`. It seems more explicit, otherwise it might be easy to forget about `flush()` because right now the user doesn't really need to be aware which gradients are sparse."}